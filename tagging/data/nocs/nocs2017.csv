title,abstract,full_text
Achieving Lightweight Multicast in Asynchronous NoCs Using a Continuous-Time Multi-Way Read Buffer.,"Multicast communication (1-to-many) is common in parallel architectures and emerging areas such as neuromorphic computing. However, there is very limited research in supporting multicast in asynchronous NoCs. This paper proposes a new parallel multicast asynchronous NoC with 2D-mesh topology. To the best of our knowledge, this is the first general-purpose asynchronous NoC to support multicast in 2D meshes. A critical feature of this NoC is the use of a new continuous-time replication strategy, where the flits of a multicast packet are routed through the distinct outputs of the router according to each output's own rate, in parallel and in continuous time. This unique asynchronous continuous-time replication, not discretized to clock cycles, can handle subtle variations in network congestion and exploit ""sub-cycle"" differentials in operating speeds. A new continuous-time multi-way read (CMR) buffer is proposed to enable this replication strategy. Only a single CMR buffer is used per input port, with multiple independent read pointers, which are accessed by different outputs. For diverse multicast benchmarks, the new parallel multicast network achieved significant latency and throughput gains over a serial baseline. Moderate energy overhead was seen for one benchmark with a small multicast portion, but major reductions were achieved for higher amounts of multicast. Interestingly, consistent latency improvements were observed for unicast, in spite of the extra instrumentation. Experiments on isolated multicast packet transmissions also showed over an order-of-magnitude improvement in delivery time.","Achieving Lightweight Multicast in Asynchronous NoCs Using a Continuous-Time Multi-Way Read Buffer Kshitij Bhardwaj Dept. of Computer Science Columbia University New York, NY 10027 kbhardwaj@cs.columbia.edu Weiwei Jiang Dept. of Computer Science Columbia University New York, NY 10027 wj2159@columbia.edu Steven M. Nowick Dept. of Computer Science Columbia University New York, NY 10027 nowick@cs.columbia.edu ABSTRACT Multicast communication (1-to-many) is common in parallel architectures and emerging areas such as neuromorphic computing. However, there is very limited research in supporting multicast in asynchronous NoCs. This paper proposes a new parallel multicast asynchronous NoC with 2D-mesh topology. To the best of our knowledge, this is the first general-purpose asynchronous NoC to support multicast in 2D meshes. A critical feature of this NoC is the use of a new continuous-time replication strategy, where the flits of a multicast packet are routed through the distinct outputs of the router according to each output’s own rate, in parallel and in continuous time. This unique asynchronous continuous-time replication, not discretized to clock cycles, can handle subtle variations in network congestion and exploit “sub-cycle” differentials in operating speeds. A new continuous-time multi-way read (CMR) buffer is proposed to enable this replication strategy. Only a single CMR buffer is used per input port, with multiple independent read pointers, which are accessed by different outputs. For diverse multicast benchmarks, the new parallel multicast network achieved significant latency and throughput gains over a serial baseline. Moderate energy overhead was seen for one benchmark with a small multicast portion, but major reductions were achieved for higher amounts of multicast. Interestingly, consistent latency improvements were observed for unicast, in spite of the extra instrumentation. Experiments on isolated multicast packet transmissions also showed over an order-of-magnitude improvement in delivery time. CCS CONCEPTS • Computer systems organization → Interconnection architectures; 1 INTRODUCTION Networks-on-chip (NoCs) have become the standard for communication in many-core processors. The system performance and power not only depends on the computing efficiency but are also governed by the on-chip interconnects [1]. In the last decade, there has been significant NoC research devoted to optimizing performance and power, improving fault-tolerance and achieving guaranteed service [2], [3]. However, most of the focus has been on unicast (i.e. This work was partially supported by NSF Grant CCF-1527796. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130221 one-to-one) traffic. A key challenge for modern NoCs is to efficiently support new traffic patterns, common in advanced parallel architectures. Multicast (i.e. one-to-many) is one such communication pattern that has recently seen a growing interest [4]. Multicast is defined as sending the same packet from a single source to an arbitrary subset of destinations. It is widely-used in parallel computing domain [5]: (i) in cache coherence protocols, to send write invalidates to multiple cores, e.g. in HyperTransport and Token Coherence protocols, 14% and 52.4% of the injected traffic is multicast [6], (ii) in shared operand networks, to deliver operands to multiple instructions, and (iii) in multi-threaded applications, to notify barrier synchronization to multiple processors. Multicast is also gaining importance with new technologies replacing the traditional electrical wires in NoCs. Multicast and broadcast (i.e. one-to-all) are inherent forms of communication in emerging NoC technologies: wireless [7], surface-wave [8], photonic [9], and CDMA [10]. Finally, 1-to-many and 1-to-all patterns are also common in emerging NoC application areas, such as neuromorphic computing. Recently, an application-specific asynchronous NoC has been used to handle multicast in hardware implementation of spiking neural networks (SNNs) [11]. In recent years, there has also been increasing interest in designing asynchronous NoCs [12]. Asynchronous NoCs are the building blocks of globally-asynchronous locally-synchronous (GALS) systems as well as fully-clockless systems. These NoCs eliminate global clock management and are therefore free from clock skew overheads and clock tree switching power, and do not require any explicit clock-gating circuitry. A number of asynchronous NoCs have been proposed that show significant area and power reductions, compared to synchronous NoCs, while maintaining or improving latency and throughput [13], [14]. A recent asynchronous NoC router from our group, synthesized in an advanced 14nm FinFET library showed 55% less area and 28% reduction in latency in a head-tohead comparison with a state-of-the-art synchronous router used in high-end AMD processors [15]. There is also increasing commercial uptake of asynchronous NoCs. Several advanced GALS systems have been proposed: (i) IBM’s TrueNorth neuromorphic chip uses a fully-asynchronous NoC to integrate 4096 synchronous neurosynaptic cores that model 1M neurons and 256M synapses [16], and (ii) STMicroelectronics’ many-core “STHORM” system connects 4 synchronous clusters with 16 processors each, using a fully-asynchronous NoC [17]. Related work. There has been significant research devoted to performing multicast in synchronous NoCs [18], [19], [5], [20]. These techniques can be divided into two categories: serial pathbased multicast and parallel tree-based multicast. In path-based, a multicast packet is serially routed from the source to its first destination, from there to the next, and so on [18], [19]. This technique is simple but can incur significant latency overheads for large number of destinations. Tree-based multicast is more widely-used, where a packet is first routed on a common path from the source towards all destinations. When this common path ends, the packet is replicated. The new copies also follow a recursive tree approach, replicating multiple times to reach the destinations [5], [21] [20], [6]. Several recent works use the tree approach for high-performance multicast, but these can still incur significant cost overheads. Early tree-based approaches used multiple unicast packets to set up paths for a multicast packet. This preconfiguration phase can be expensive in terms of network latency, extra congestion and power [5], [21]. Recent approaches avoid setup entirely and dynamically compute the multicast tree paths based on the destinations. However, these approaches lead to complex router designs due to highlycustomized route computation, multiple virtual channels per port, and turn prohibitions to avoid deadlocks [20], [22]. An alternative low-overhead technique achieves this dynamic multicast using only one buffer per port, but was introduced for off-chip networks and can have performance overheads as replication of multicast packets in routers takes multiple clock cycles [23]. A recent NoC achieves full broadcast in 2 cycles for an 8x8 2D mesh [6], using a chipwide broadcast, followed by dropping packets at non-destinations. However, this technique can have significant power overheads for multicasts with a small or moderate number of destinations. Contributions. The focus of this paper is on supporting an efficient parallel multicast capability in asynchronous NoCs. This feature has been largely missing and, given the recent interest and exploitation of asynchronous and GALS NoCs, fulfills a critical architectural need to enable advanced computing systems. The potential cost benefits of asynchronous NoCs [14], [15] also suggest opportunities for low-overhead multicast solutions. Recently, an initial asynchronous NoC with multicast support, targeting a Mesh-of-Trees (MoT) topology, was introduced [24]. In contrast, the new work uses a different protocol, and addresses the much more challenging problem of multicast in 2D-mesh topology, with higher radix routers, greater parallelism and more complex routing. The first contribution is a lightweight tree-based multicast using an asynchronous NoC with 2D-mesh topology. The new approach constructs multicast tree paths dynamically, without any extra setup phase. The proposed NoC uses a two-phase handshaking protocol and single-rail bundled data encoding [12], which have been shown to be essential to obtain high performance and energy efficiency [14], [15]. This work builds on these earlier designs, but with significant enhancements to support multicast. To the best of our knowledge, this is the first general-purpose asynchronous NoC to support multicast in 2D-mesh topologies. The second contribution is a novel continuous-time replication strategy to support parallel multicast in the new asynchronous NoC. In this strategy, the flits of a multicast packet are routed through the distinct outputs of the router according to each output’s own rate, in parallel, and in continuous time. Unlike synchronous, this unique asynchronous approach, not discretized to clock cycles, can provide significant performance benefits by accommodating subtle variations in network congestion and exploit “sub-cycle” differences in interface operating rates. Third, a new continuous-time multi-way read (CMR) buffer is introduced to enable the above replication strategy. The CMR buffer is a low-latency asynchronous circular FIFO with multiple decoupled read controls. The buffer is used to store the flits of each multicast packet, which can then be accessed by multiple outputs in parallel using independent read pointers. Only a single shared CMR buffer is used per input port, compared to multiple buffers in several recent synchronous multicast routers [20], [21].1 Moreover, the CMR buffer is a standalone unit which is expected to be useful for a wide variety of applications. Fourth, to achieve further latency optimization, an expensive input buffering operation is eliminated from the router’s forward critical path. In particular, route computation is initiated in parallel with the buffering of a packet header. In contrast, asynchronous routers generally perform buffering of the header and route computation serially [14], [15], [13], resulting in a latency bottleneck. To ensure energy efficiency, the route computation unit is deactivated for the remaining flits of the packet. Finally, the new network was implemented and extensively evaluated. Highly-diverse synthetic multicast benchmarks were created with traffic patterns common in cache coherence protocols and in spiking neural networks. Compared to a unicast-based serial multicast baseline, The new parallel multicast network achieved up to 96.8% improvements in network latency, with 25.6 to 170.2% higher throughput. While the new network showed 28.7% higher energy for one benchmark with small amount of multicast, it obtained a 2.7 to 57.2% energy reduction for benchmarks with moderate-tohigh multicast portions. Experiments were also conducted on a single isolated multicast packet to evaluate critical delivery time and energy; the new NoC showed almost 37× lower latency than the serial baseline. Interestingly, moderate improvements in latency were also obtained for unicast. 2 BACKGROUND This section reviews background on asynchronous communication, and a previous 2D-mesh asynchronous NoC from our group. This NoC forms the foundation of the new work and is used as a baseline serial unicast-based multicast network. Asynchronous communication. In asynchronous design, a handshaking protocol is required to synchronize a sender and a receiver [12]. Two common handshaking protocols are: (a) a fourphase (RZ) protocol, and (b) two-phase (NRZ) protocol. In four-phase, req/ack wires are initially low; a sequence of asserting then deasserting each of these wires, in turn, constitutes a complete transaction. In two-phase, there is no return to zero; a single toggle on req, followed by a toggle on ack, form one transaction. In this paper, a two-phase protocol is used, to achieve higher throughput, since it only involves one roundtrip communication per transaction, compared to two roundtrips for 4-phase. This paper also uses single-rail bundled data encoding [14], rather than delay-insensitive encoding [13], for higher coding efficiency and lower power. The former has almost the same coding efficiency as synchronous, with two components: a synchronous-style data channel and an accompanying bundling req signal, which transitions only after all data bits are stable. 1 One synchronous off-chip multicast network features the use of a single shared input buffer, operating at the granularity of a packet [23]. However, unlike the new work, it does not operate in continuous time and uses buffers with a single read port. Therefore, distinct output ports cannot read the buffer in parallel and require complex arbitration mechanism to gain access. The result is a serial replication of a multicast packet, performed in multiple clock cycles, which has significant performance overheads. Figure 1: Baseline IPM micro-architecture Baseline asynchronous NoC. An existing high-performance asynchronous NoC forms the baseline for the new work [14], targeting a 2D mesh topology with 5-port routers. Each router has two main components: input port modules (IPMs) and output port modules (OPMs), connected using a shared crossbar. Each IPM has one input channel and four output channels connected to the OPMs. The IPM performs route computation on an incoming packet and selects the correct OPM for routing. Each OPM has four input channels and a single output channel. The OPM arbitrates between packets from the four IPMs and the winner is forwarded on the output channel. For high performance, capture-pass registers, i.e. normally-transparent single-latch registers are used throughout the router, both in the IPM and OPM, which provide very low latency as no synchronization to a latch enable is required to open these registers [12]. The micro-architecture of the baseline IPM is shown in Figure 1. There are four components: Circular FIFO Buffer, Route Computation Unit, four Request Generators, and Internal Ack Generator. The Circular Buffer stores the incoming flits. This buffer was proposed as an auxiliary unit in [14] but was not used in its IPM. However, the baseline in this paper uses this buffer. The Route Computation Unit uses XY routing to select the correct Request Generator based on the destination address in the packet header, which activates the correct OPM for the packet and generates an output request for each flit. The Internal Ack Generator observes completion of handshaking between the IPM and the OPM for each flit, and advances the read pointer in the Circular Buffer. The baseline IPM has a simple operation. A header flit first arrives on the input channel and is immediately stored in the Circular Buffer. Next, three operations are performed in parallel: (a) the flit (DataX) from the buffer is speculatively broadcast to all OPMs, (b) the bundled ReqX is also broadcast to all the Request Generators, which then toggles the corresponding output requests, and (c) the Route Computation Unit receives the address field (X/Y coordinates of destination) and selects the one correct Request Generator. This Request Generator activates the correct OPM by asserting PathEnabled, which persists for the packet lifetime. Once the flit has been routed and an ack received from the OPM, the Internal Ack Generator then toggles AckX to read the next flit from the buffer. Similar operations are performed for body and tail flits. Finally, the routing of the tail by the OPM (TailPassed) deasserts PathEnabled, releasing the OPM’s arbiter, and completing the packet’s routing. The Circular FIFO is a low-latency, area- and energy-efficient buffer. This buffer uses single latch-based registers, where each latch register can hold a distinct item. In contrast, synchronous Figure 2: Baseline circular FIFO designs typically use D flipflop-based registers with significant overheads, or pulse-mode single-latch designs with challenging two-sided timing constraints.2 Interestingly, the use of capturepass registers in this FIFO also improves forward latency. As shown in Figure 2 the buffer has one write interface and one read interface. After a flit is written in the tail cell, the write interface for the cell generates a Full request to the the read interface, indicating that the cell has been written, followed by an Ackout on the input channel completing the handshaking operation, and the write pointer is incremented. The read interface then reads the selected flit, which is forwarded to all the OPMs along with its req. After AckX is received from the OPM, the read interface advances its read pointer. Serial multicast baseline. The baseline network handles multicast using a serial unicast-based approach. For each multicast packet, the source NI creates multiple unicast copies destined to different destinations, which are then injected and routed serially. As shown in Section 5, this serial multicast can have severe performance and power overheads. In this paper, the baseline network will be significantly enhanced to support parallel multicast. Only IPM of the baseline is modified: since the IPM performs route computation and selects the required output ports, it must be extended to support replication capability, which is essential for parallel multicast. In contrast, the OPM remains the same as baseline as it still arbitrates between multiple packets and does not make any routing decisions. 3 OVERVIEW OF NEW APPROACH This section introduces the new asynchronous NoC with tree-based multicast: its novel replication strategy, router micro-architecture, operation and ability to avoid deadlocks are covered in-depth, before presenting the detailed designs. 3.1 Tree-based parallel multicast Figure 3 shows a simple view of the new router micro-architecture and illustrates how it supports tree-based multicast. This figure will also be used to highlight several important features in the next subsections. The figure only shows one IPM in detail with new enhancements for parallel multicast, connected to all four OPMs using a partial crossbar. The IPM consists of a single CMR buffer, 2 A synchronous 2D circular FIFO has been proposed for low-voltage operation which, similar to us, uses single-latch registers; however, it can incur considerable overheads [25]. This FIFO comprises many buffer lanes, where each lane is a latch-based shift register. Due to the two dimensions, this FIFO can have significant area and power overheads. It is also expensive in terms of latency due to multiple latches in the shift registers. A 1D version of this FIFO, which is not shown in the paper, will be a circular FIFO with each lane consisting of only one latch register. However, the operation of this 1D FIFO is still aligned to clock cycles and requires complex two-sided timing constraints for pulse-mode operation, unlike the proposed approach. which connects both to the input channel and to four OPMs through the crossbar; and a parallel Route Compute Unit that also connects to the input channel and identifies the correct OPMs for routing. As shown in the given example, the multicast packet is replicated and forwarded to two output channels in parallel based on the route computation. These new copies will again follow a tree-based path through the network, performing multiple replications until they reach the destinations. 3.2 New continuous-time replication strategy The new replication strategy routes the set of flits in a multicast packet through the distinct output ports in parallel, and at each output’s own rate. Unlike synchronous schemes, this replication and forwarding is performed in continuous time and not discretized to clock cycles. In more detail, the flits of a multicast packet stored in sequence in a single shared input buffer are read by multiple OPMs. The OPMs read these flits independently and at their own rates using decoupled read pointers, not bound to any discrete clock cycle. If an output port gets congested, others can continue reading the flits of the stored packet. Due to the continuous-time read operation, the congested output can resume reading the next flits as soon as it becomes available rather than waiting for the next clock cycle. Hence, unlike synchronous, this design can handle subtle variations in network congestion. Each OPM stalls once it has read all the currently stored flits. A stored packet becomes stale after all the required outputs have read all its flits, including the tail. Figure 3 shows the top-level view of the continuous-time multiway read (CMR) buffer used to implement the new replication strategy. The buffer has a single write port to store new incoming flits, and four read ports, which can be accessed by multiple OPMs in parallel using decoupled read pointers. The CMR buffer is an extension of the baseline single read-ported buffer [14], but with the new multiple read capability and distributed read control. 3.3 Route computation and buffering policy To remove the overhead of buffering, route computation on the header is performed in parallel, using a special small dedicated Route Compute Unit. When a header flit arrives, it is rapidly forked on two paths: to Route Compute and to the CMR Buffer. The former stores the header, in a one-place buffer, and holds it for the packet lifetime. Bit string addressing is used, which is a common efficient multicast encoding [5], [20], [6]. The address field has a single bit for each node in the network. The bit is 1 if the node is a destination, otherwise it is 0. Route computation is performed only on the packet header; after address decode, for energy efficiency, the unit is disabled. XY routing is used, to forward the multicast flit on the appropriate output ports, which ensures a deadlock-free protocol. The IPM’s buffering policy is specifically designed for the continuous-time replication strategy. A new packet can only be buffered when the previously-stored packet has been completely routed. The advantage of this packet-based buffering policy is to allow each OPM to freely read stored flits without safety violations. In particular, the Route Compute Unit only opens after a complete packet has been processed. If multiple flits of the next packet have already arrived, the unit will not sample its header address. A special synchronization is enforced, to ensure all OPM’s have read the tail flit before sending the final upstream acknowledgment. The buffer size is therefore set equal to the worst-case packet size handled by the NoC. While the buffer can be under-utilized for shorter Figure 3: New router architecture and multicast operation packets, an extra buffer can also be added on the input channel to increase capacity. This approach also simplifies the flow control, since the buffer will never risk overflow. 3.4 Simulation of multicast routing A simple multicast scenario is considered in Figure 3, assuming a 3-flit multicast packet. The header first arrives on the input channel, intended for outputs 0 and 1. Two operations are next performed in parallel: the header is stored in the CMR buffer, and route computation is used to select OPMs 0 and 1. After the header is written, an ack is sent to the upstream router while OPMs 0 and 1 read the stored header in parallel using their individual read pointers. Due to the decoupled write and read operations of the CMR buffer, multiple concurrent operations are performed: (a) a new body flit arrives on the input channel, which is written to the buffer followed by an ack to the upstream router, and (b) the header wins arbitration in each OPM and is sent out on both output channels. The body flit can be read by each OPM at its own rate, and the tail flit can be written. To highlight the continuous-time decoupled read operations, assume OPM 1 is temporarily congested, while OPM 0 is free. OPM 0 can then read the body flit by advancing its read pointer, and then the tail flit, while OPM 1 remains quiescent. OPM 0 then stalls, since it has read the complete packet. Finally, as soon as OPM 1 becomes free, it reads the body and tail flit. Since the tail flit has been read by both outputs, the Route Compute Unit is synchronized, and the final ack is sent upstream. 3.5 Resource-dependent deadlock avoidance The new replication strategy not only has a simple operation but also avoids resource-dependent deadlocks within a router. A potential deadlock may occur due to a cyclic dependency between multi-flit multicast packets arriving at different inputs. This dependency arises when these multicast packets arrive almost simultaneously and are intended for the same output ports but acquire different subsets of these outputs. A deadlock can occur if a conservative multicast routing protocol is followed, where no body flits can be routed until the header has been routed through all required outputs.3 The new replication strategy eliminates this deadlock, since it allows a packet to be routed through any OPM, and the OPM is released, regardless of the status of other OPMs. Figure 4 shows a simple example of a resource-dependent deadlock in a conservative multicast routing protocol. Without loss of 3 Deadlock will even occur in a more relaxed protocol, where body and tail flits can be sent freely on each OPM, but no OPM can be released until full multicast is complete. Figure 4: Cyclic dependency between multicast packets generality, a four-port router is considered. The router receives two multi-flit multicast packets: A and B almost simultaneously on east and west IPMs. Both these packets are intended for north and south OPMs. Assuming packet A header occupies the north OPM, while the packet B header occupies the south OPM. A circular dependency has occured: packet A requires access to north for its complete routing, but is occupied by B, similarly, B requires access to south, which is occupied by A. Since, a conservative protocol is followed and none of the headers have been completely routed, the remaining flits are stuck and a deadlock has occurred. The new replication strategy breaks this cycle. The packets can be completely routed through each acquired OPM, releasing these outputs for the other packet to acquire. In the scenario of Figure 4, packet A will be completely routed through south, releasing this OPM for B to acquire. Similarly, B will release north for A. Hence, both packets are completely routed through the required outputs. 4 NEW IPM DESIGN DETAILS The detailed design of the new IPM is now presented, which supports the continuous-time replication strategy, along with the details on its route computation unit and the CMR buffer. 4.1 New IPM structure and operation Structure. As shown in the Figure 5, the IPM has one input channel that connects to the upstream router and four output channels towards the OPMs through a crossbar. There are three components in the new IPM: a Route Computation Unit (RCU), the CMR buffer and four Address Modifier Units (AMUs) on each output direction. The RCU stores only the header addressing and selects the correct OPMs for routing. The CMR buffer, however, stores all the flits, which can be accessed by the OPMs in parallel using its four decoupled read ports. Finally, an AMU is present at each output of the CMR buffer, which masks some of the bits of the header bit-string addressing corresponding to destinations not reachable through the corresponding OPM. Operation. When a new packet header arrives, it is stored in the CMR buffer and in parallel its address is stored in a small buffer in the RCU to start route computation. After the header is stored in the CMR buffer, it is speculatively broadcast to all read interfaces. The write interface, next, generates an Ackout on the input channel, which is used to (a) advance the buffer’s write pointer, and (b) close the buffer in the RCU, disabling it for the remainder of the packet to save energy. A similar write protocol is followed for the body flits until the tail arrives and is stored. The Ackout for the tail is sent only after the tail has been read by all the correct OPMs of the buffer. This Ackout also re-activates the RCU. Read operations on the buffer are performed in parallel to the write. The header is first speculatively read out of all the read interfaces, with AMU address modifications, and sent to all the OPMs along with Reqouts. After the RCU finishes the route computation, the correct OPMs are selected using PathEnabled, which are also Figure 5: New IPM micro-architecture with CMR buffer Figure 6: New route computation unit architecture used by the read interfaces to throttle the copies on the wrong paths. Each read interface on the correct paths receives an Ackin from its OPM after the header has been routed through the OPM, and advances its individual read pointer. Similar read operations are performed for the body and tail flits, where each OPM on the correct path reads these flits independently. 4.2 New route computation unit (RCU) Structure. The RCU has one input channel and four output channels connected to the OPMs through the crossbar (Figure 6). It has three main components: an address register, a route computation logic, and an OPM selector. Operation. The RCU operates only for the headers. When a packet header arrives on the input channel, its address field is stored in a normally-transparent latch register with the correct phase of its bundling request. The head predictor controls this register: it is closed right after an Ackout is sent on the input channel for the header and reopens after the Ackout for tail, anticipating the next header. The route computation logic selects the correct routes based on a network partitioning approach. Four 64-bit partition bit-strings are used corresponding to each output direction which have a bit for every node: 1 if the node is reachable through this direction using XY routing, 0 otherwise. A bit-wise AND is performed between the destination bit-string and each of these partition strings: if the result is non-zero then that direction is selected for routing as there is at least one destination reachable through this direction. Finally, the OPM selector generates the correct PathEnabled signals for the packet lifetime, which are deasserted after the appropriate Tailpasseds are received. 4.3 New CMR buffer Structure of CMR buffer. As shown in Figure 5, the CMR buffer consists of one write interface and four decoupled read interfaces. The buffer can handle up to 5-flit packets. The write interface has one input channel and 5 output channels, corresponding to each cell of its storage unit. The input Datain is broadcast to all the storage cells with its handshaking signals connected to the write interface control. Each cell has an output datapath and a bundling CellFull broadcast to all the read interfaces. The write interface control selects the correct storage cell using a 1-hot write pointer. This control also gets 2-phase CellEmpty signals from each read interface, which provide the status of the read operations on individual cells, and the Tail flags from its datapath to determine which cell has the tail flit. Each read interface is connected to the write interface and to an OPM through the AMU on datapath. This interface receives the data along with the bundled CellFull requests from each cell of the write interface and sends back 2-phase CellEmpty status signals. Each read interface has its own control and a 1-hot read pointer, which selects the correct cell to read using a MUX. The read interface also gets PathEnabled signals from the RCU to determine if the interface is on the correct path or not. The read interface control generates Reqouts to the OPM and receives Ackin. Operation of CMR buffer. A packet header first arrives on the input channel and is written to a cell pointed by the write interface. Next, the bundling CellFull request for this cell along with the data are speculatively broadcast to all the read interfaces. This broadcast is followed by an Ackout on the input channel, which then increments the write pointer. A similar write protocol is followed for the body and tail flits. However, the Ackout for the tail is generated only after it has been read by all the read interfaces (i.e. toggle on all the CellEmpty signals), both speculatively and non-speculatively. The read interfaces of the buffer have a uniform operation for every flit. A cell is read if the read pointer is pointing to it, and its CellFull signal is toggled. In this case, the cell data is read out along with its Reqout. There are two types of reads: correct if the interface is on the right path, or incorrect. With a correct read, an Ackin is received from the OPM, which then increments the read pointer and toggles the corresponding CellEmpty. However, with an incorrect read, no Ackin is received. This case is detected using the PathEnabled signals from the RCU, and the speculative Reqout is canceled by toggling it again. This second toggle on Reqout also increments the read pointer and toggles the corresponding CellEmpty signal. Due to this uniform operation for both speculative and non-speculative reads, all the read pointers point to exactly the same location after all the flits of a packet are read, leading to an overall simple operation. 5 EXPERIMENTAL RESULTS The new parallel multicast and the baseline serial multicast networks are implemented at post-mapped, pre-layout level. The experimental framework and node- and network-level evaluations for diverse benchmarks are now presented. 5.1 Experimental framework Experimental setup. Two 8x8 2D-mesh networks, new and baseline, are implemented in Nangate 45 nm technology. Router nodes are first technology mapped to the Nangate standard cell library using the Cadence Virtuoso tool. Next, the Spectre simulator is used to extract gate-level models, i.e. determine rise/fall delays for every I/O path of each gate, at a typical process corner. The channel length and delay are borrowed from the Freescale PowerPC e200z7 floorplan: 1 mm and 100 ps [6]. The networks are implemented in Structural Verilog using the extracted models. An asynchronous NoC simulator, previously developed for unicast, was extended to support multicast. The simulator uses a Programming Language Interface (PLI) to connect a technology mapped network to a C-based test environment. This environment was significantly enhanced to support injection and validation of routing of multicast packets. Packet headers are injected following an exponential distribution. Long warmup and measurement phases are also used based on a procedure similar to [26]. In addition, asynchronous network energy is measured using two steps: (a) annotate the precise switching activity of every wire during a benchmark run, (b) run Synopsys PrimeTime using the recorded activity to compute energy. Benchmarks. Most of the experiments are conducted on 8 synthetic benchmarks. There are 3 unicast benchmarks [26]: (1) uniform random; (2) bit complement; and (3) hotspot10, which is similar 5.2 Node-level results to uniform random but the middle four routers have 10% higher probability of receiving packets than others. There are 5 multicast benchmarks: (4) multicast5 and (5) multicast10, where 5% and 10%, respectively, of total injected packets are multicast to random destinations and rest of the traffic is uniform random unicast, similar to cache coherence traffic [5]; (6) multicast-static, where 16 sources are statically fixed to do random multicast, while remaining perform uniform random unicast; (7) all-multicast and (8) all-broadcast, where all sources only perform random multicast or only broadcast. These two patterns are common in spiking neural networks, where the former is seen in random neural network (RNDC) and the latter in a Hopfield network [27]. Node-level area. The new router has almost 2× more area (54 083 µm2 ) than the baseline router (24 962 µm2 ). This overhead is expected and primarily due to the new parallel multicast capability. However, this area overhead will be amortized when considering the total network-level area, including link area.4 As shown later, the simpler baseline incurs significant performance and energy overheads for multicast traffic, justifying the need for this parallel multicast capability. It it also useful to get a rough area comparison of the new asynchronous router with a state-of-the-art single-cycle synchronous multicast router. The latter has an area of 227 230 µm2 , using 6 VCs per port and 10 total buffer slots, each with a 64-bit datapath [22]. The total buffering of this router is similar to our new router (one 5-slot buffer per port with a 128-bit datapath). Therefore, the two routers are estimated to have similar buffer area, which typically dominates the router area. Both the routers use 45 nm technology, but the synchronous one is laid out in a different process and also uses lookahead optimization, which should not have considerable impact on area. Overall, the new asynchronous router is estimated to have roughly 4× lower area than the synchronous one. 4 It would be interesting to also analyze how this area overhead will scale with adding VCs. The area overhead of the new router with VCs over the baseline with VCs will be similar to the current 2× overhead, which does not include VCs in both the designs. This outcome is due to the use of multi-switch architecture with replicated crossbars for these projected VC-based routers, rather than the multi-stage architecture with shared crossbar, which is less efficient for asynchronous designs [15]. Since the switch area is the dominating factor in multi-switch architectures, the areas of the two routers will scale linearly w.r.t. the switch area as the numbers of VCs are increased. Figure 7: Network latency results for unicast traffic Node-level latency. Node latency is measured for only header. Header latency is critical, since these flits incur the most overhead setting up the path, and they typically define the packet latency. The baseline router performs buffering and route computation serially. In contrast, the new router performs buffering in parallel with route computation. This parallelism significantly improves latency for the new router, recouping any overheads due to a more complex route computation that handles multicast addressing. As a result, latency (693 ps) of new router is 16% lower than baseline (833 ps). 5.3 Network-level results Latency, throughput and energy results are now presented for new and baseline networks. Benchmarks are considered in three categories: unicast, mix of unicast and multicast, and only multicast/broadcast. Unicast benchmarks show the impact of the new parallel multicast capability on basic unicast performance and energy. The mix of two types of traffic is common in parallel computing and therefore also evaluated. Finally, all multicast/broadcast represents an extreme scenario, common in emerging applications like neuromorphic computing. Two packet sizes are considered: multi-flit (5-flit) and single-flit. The former represents a general size, while the latter is common for control packets, e.g. write-invalidates. Network latency for multi-flit packets. Figure 7 shows the average latency results for all three unicast benchmarks. These results are measured for header at 25% of saturation load of baseline, which is high enough to show interesting benchmark characteristics with network largely uncongested. The new network shows moderate latency reductions from 6.1% (uniform random) to 14% (hotspot10). This important result shows that the new network not only supports parallel multicast but also improves unicast latency. Figure 8 shows the average latency results for all three benchmarks with unicast/multicast mix. These results show the variation of average latency with injection rate, covering a complete trajectory from zero-load till saturation. The latency for a multicast packet is measured from the time of injection of header (or first header for serial baseline) to the arrival of the last header at its destination. There are three important trends: (i) latency magnitudes, (ii) shape of curves, and (iii) points of saturation. The baseline incurs significant latency overheads: for each benchmark, the gap between the curves increases significantly as injection rate increases. Also, note the steep rise in latency for baseline as the number of multicast packets injected increase. The baseline curves are steeper for a higher degree of multicast (multicast10 and multicast-static). Finally, for each benchmark, the baseline saturates very early compared to the new network, showing the effectiveness of parallel multicast. Zero-load latency is measured for extreme cases: all-multicast and all-broadcast. Even at zero-load, with a single packet injected from each source, the latency for serial baseline is severely degraded (all-multicast: 78176 ps, all-broadcast: 307108 ps), showing that the baseline is not a practical solution to support traffic comprising Figure 8: Network latency for multicast/unicast mix benchmarks Figure 9: Saturation throughput results only multicast and broadcast. In contrast, the new network handle this traffic well (all-multicast: 7872 ps, all-broadcast: 9744 ps) with 89.8% and 96.8% respective latency improvements. Saturation throughput for multi-flit packets. Figure 9 shows the throughput results for all benchmarks. For unicast, the new network incurs an overhead of 13.3% (hotspot10) to 30.9% (uniform random). This overhead is the result of a longer delay in the new design in acknowledging to the upstream router after a flit is stored in the current router. However, for multicast, the new network shows significant improvements: 25.6% (multicast5) to 88.2% (multicast-static) and 170.2% (all-broadcast) higher throughput. In summary, more gains are seen for benchmarks with a higher multicast amount as the parallel multicast capability provides a higher threshold for saturation than serial. Total network energy for multi-flit packets. Figure 10 shows total network energy results for all benchmarks. For unicast, the new network incurs 2× overhead over the serial baseline due to the extra instrumentation for parallel multicast. However, for multicast benchmarks, the routing of several unicasts for each multicast in serial baseline leads to higher resource utilization than the new network. Interestingly, this low utilization in the new network overcomes the complexity overhead as multicast amount increases, showing a 28.7% degradation for multicast5 to 2.7-57.2% lower energy than baseline for benchmarks with higher multicast portion. Overall, in terms of multi-objective design space exploration for multicast traffic, the new network has a clear advantage over baseline. As one important example, for mixed traffic like multicast10, the new network shows significant improvements in latency and throughput, with slightly lower energy, but with an expected area overhead. Single-flit packets. A limited set of experiments measures performance of single-flit packets. This case is important to consider the impact of the new design when there are no body flits to exploit the continuous-time replication strategy. Two benchmarks Benchmarks Baseline New Baseline New Network Latency (ps) shortmulticast typicalmulticast extrememulticast 5492 1486 5492 1486 120695 11010 115318 6252 242051 11010 235560 6252 broadcast shortmulticast Corner Source 246752 1.72 11808 2.75 Center Source 239316 1.72 7049 2.75 Network Energy (nJ) typicalmulticast extrememulticast 8.15 5.73 5.72 5.40 15.30 6.71 10.31 6.63 broadcast 16.01 6.71 10.56 6.63 Table 1: Isolated multicast case study: network latency and energy Figure 10: Total network energy results are considered: uniform random and multicast10. For the unicast uniform random, as expected, the new network achieves 11.6% lower latency (5635 ps) than baseline (6376 ps). In terms of throughput, the new network (0.18 GFps) incurs a 57% degradation over the baseline (0.42 GFps) due to the conservative write protocol performed at a packet granularity. However, for multicast10, the efficient parallel multicast capability results in 33.6% lower latency than baseline with almost the same throughput: new (5059 ps, 0.39 GFps) and baseline (7629 ps, 0.37 GFps). Isolated multicast case study. Finally, limited experiments are conducted to measure latency and energy of an isolated multicast packet. These results are critical to give insight into the actual delivery time and energy of a single multicast packet, not averaged with other traffic. A head-to-head comparison is performed, while varying the number of destinations and the location of the packet source. Four multicast benchmarks are created with a single 5-flit packet: (i) short-multicast: to 2 immediate neighboring destinations, (ii) typical-multicast: to 30 destinations uniformly distributed across network, (iii) extreme-multicast: to 62 destinations, and (iv) broadcast. Two different source locations are considered: corner and center, covering a range of possible multicasts. As shown in Table 1, new network achieves 72.9% (short-multicast) to 95.2% (broadcast) lower latency for a corner source. The absolute latency for both networks increases with the longest path length: 2 hops in broadcast. The poor baseline results (and up to 37× worse than new for center source) are due to the long delay from injection of first header to the arrival of last header. For energy comparison, for both sources, the new network incurs 59.8% overhead over baseline for short-multicast. In contrast, in case of the corner source, for hops in short-multicast, 13 hops in typical/extreme multicast and 14 typical-multicast and broadcast, with more destinations, the new network achieves 29.6% to 58% lower energy, respectively, due to the lighter traffic in parallel multicast. Similar results are shown for the center source. 6 CONCLUSION This paper proposes a new parallel multicast asynchronous NoC with 2D-mesh topology. A new continuous-time replication strategy is proposed, where the flits of a multicast packet are routed through the distinct outputs of the router according to each output’s own rate, in parallel and in continuous time. A new continuous-time multi-way read (CMR) buffer is also proposed to enable this replication strategy. For multicast, the new parallel multicast network achieved up to 96.8% better network latency and 25.6-170.2% gain in saturation throughput over a serial baseline, with 2.7-57.2% reduction in energy for benchmarks with high multicast portion. "
Adaptive Manycore Architectures for Big Data Computing.,"This work presents a cross-layer design of an adaptive manycore architecture to address the computational needs of emerging big data applications within the technological constraints of power and reliability. From the circuits end, we present links with reconfigurable repeaters that allow single-cycle traversals across multiple hops, creating fast single-cycle paths on demand. At the microarchitecture end, we present a router with bi-directional links, unified virtual channel (VC) structure, and the ability to perform self-monitoring and self-configuration around faults. We present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources in order to realize this vision. We provide concrete learning algorithms for core and NoC reconfiguration; and dynamic power management to improve the performance, energy-efficiency, and reliability over static designs to meet the demands of big data computing. We also discuss future challenges to push the state-of-the-art on fully adaptive manycore architectures.","Adaptive Manycore Architectures for Big Data Computing Special Session Paper Janardhan Rao Doppa School of Electrical Engineering and Computer Science Washington State University jana@eecs.wsu.edu Ryan Gary Kim Department of Electrical and Computer Engineering Carnegie Mellon University rgkim@cmu.edu Mihailo Isakov and Michel A. Kinsy Department of Electrical and Computer Engineering Boston University mihailo@bu.edu,kinsy@bu.edu HyoukJun Kwon and Tushar Krishna School of Electrical and Computer Engineering Georgia Institute of Technology hyoukjun@gatech.edu, tushar@ece.gatech.edu ABSTRACT This work presents a cross-layer design of an adaptive manycore architecture to address the computational needs of emerging big data applications within the technological constraints of power and reliability. From the circuits end, we present links with reconfigurable repeaters that allow single-cycle traversals across multiple hops, creating fast single-cycle paths on demand. At the microarchitecture end, we present a router with bi-directional links, unified virtual channel (VC) structure, and the ability to perform self-monitoring and self-configuration around faults. We present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources in order to realize this vision. We provide concrete learning algorithms for core and NoC reconfiguration; and dynamic power management to improve the performance, energyefficiency, and reliability over static designs to meet the demands of big data computing. We also discuss future challenges to push the state-of-the-art on fully adaptive manycore architectures. CCS CONCEPTS • Computer systems organization → Interconnection architectures; Fault-tolerant network topologies; • Hardware → Power and energy ; • Computing methodologies → Machine learning; KEYWORDS Adaptive manycore architectures, Big data computing, Interconnect networks, Power management, Machine learning. ACM format: Janardhan Rao Doppa, Ryan Gary Kim, Mihailo Isakov and Michel A. Kinsy, and HyoukJun Kwon and Tushar Krishna. 2017. Adaptive Manycore Architectures for Big Data Computing. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130236 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130236 1 INTRODUCTION Computing systems design is at an inflection point today. Emerging big data applications such as machine learning, graph processing, image processing, databases, etc. are often massively multi-threaded with extremely high computation demands. This has led to the emergence of many-core architectures with hundreds to thousands of cores [1, 4, 7, 9]. On the technology end, however, chips today are highly power-constrained (due to the end of Dennard voltage scaling) and face reliability and process variation challenges (due to sub-nm technology nodes). To address the performance, energy-efficiency, and reliability needs of big data computing systems, we envision self-aware manycore architectures that automatically adapt their behavior to accommodate the dynamic needs of applications/users, performance and energy constraints, and resource availability. This adaptivity can span all the way from the application (e.g., task mapping/scheduling) to the core (e.g., DVFS/power-gating) to the interconnect fabric (e.g., DVFS/routing). We argue that machine learning techniques can be leveraged to reason about and manage the on-chip resources to achieve the desired performance, energy, and reliability trade-offs. To achieve this vision, we need (a) a highly adaptive and configurable microarchitecture substrate that exposes control knobs to system software, and (b) machine learning algorithms that enable the system software to learn how to vary these control knobs to achieve system-level goals. Together, this can enable manycore systems to adapt to conditions seen during run-time (e.g., application characteristics, process-variations, aging components), while accommodating dynamic user constraints. This paper brings together three bodies of work spanning circuits, microarchitecture, and machine learning algorithms to realize the vision of adaptive manycore architectures: • First, in Section 2, we present the microarchitecture of a highly configurable router called RAIN that provides finegrained control of its resources (VC buffers and links) for efficiency, while providing protection against faults with mechanisms for self-monitoring and self-configuring to create deadlock-free routes around unreliable components of the chip. • Second, in Section 3, we present novel link circuits called SMART that provide fine-grained control of link repeaters, to enable the creation of single-cycle long-range on-chip links NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J. R. Doppa et al. on demand. This helps reduce latency. Running SMART links at lower frequencies also helps reduce energy. • Finally, in Section 4, we present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources to realize this vision. We provide concrete algorithms for NoC reconfiguration and dynamic power management to improve the performance and energy-efficiency over static designs. We demonstrate the benefits of these adaptive schemes over baseline static schemes, making a case for adaptive self-aware manycore architectures. We also discuss exciting open research problems in this space in Section 5. 2 CONFIGURABLE ROUTER MICRO ARCHITECTURE FOR ADAPTIVE ROUTING Adaptive manycore systems require on-chip networks that can dynamically reconfigure themselves to application traffic needs, e.g., high-bandwidth and user-specified constraints, e.g., hard realtime. Furthermore, these NoCs should make every effort to avoid compute resource from becoming disconnected, or deadlocks, due to defective routers and faulty links. To this end, we propose RAIN (Resilient Adaptive Intelligent Network-on-Chip). RAIN makes conventional NoC routers highly configurable and resilient by augmenting conventional wormhole routers with four additional features: (1) bidirectional physical links, (2) a common pool of virtual channels (VCs), (3) a routing cost table that keeps track of latencies associated with previous routing decisions, and (4) an intelligence unit which contains a monitoring module (Mo) and a reconfiguration module (Re). Figure 1 shows the RAIN router architecture. RAIN is able to accommodate userspecified constraints and coordination across traffic paths. We also develop algorithms for fault resiliency at the on-chip network level through self-monitoring and self-configuration. Figure 1: Resilient Adaptive Intelligent Network-on-Chip Micro-Architecture 2.1 RAIN Architecture Design Approach Bidirectional physical links for efficient high-performance In [16], Cho et al. introduced a bandwidth-adaptive network where the link bisection bandwidth can adapt to changing network conditions using local state information. The general design approach for bandwidth-adaptive networks is to merge unidirectional links between network node pairs into a set of bidirectional links. Each new bidirectional link can be configured to deliver packets in either direction. The links can be driven from any one of the nodes connected to it. There are local arbitration logic and tristate buffers to ensure that two nodes do not simultaneously drive the same wire. Figure 2 illustrates how the egress buffers’ occupancy rate can be used to locally arbitrate the link bandwidths. First, there is sensing, followed by a reconfiguration step. Figure 2: Local bandwidth arbitration using egress buffer pressures The main problem with local decision making in a bidirectional link router system is the susceptibility to head-of-line-blocking effects. Figure 3 depicts a 2D-mesh network case using XY dimensional order routing on four flows (A, B, C, and D). Based on the pressure between nodes (1, 0) and (1, 1), a local decision is made to allocate three links from (1, 0) to (1, 1). Yet, due to the sharing of the bandwidth resources among flows A, C and D between (1, 2) and (1, 3), only one third of the allocated bandwidth between (1, 0) and (1, 1) can reach Des t inat ionA . This local greedy approach can lead to the throttling or starvation of other flows, in this case flow B. Figure 3: Illustration of head-of-line effect due to local decision making. The RAIN architecture counters the head-of-line-blocking effects by allowing the coordination of direction changes and the collective arbitration of multiple links. Figure 4 illustrates the coordinated scheme. The regional information is used to biased the local decision to enable complementary effects of the distributed bandwidth adaptation. It is worth noting that this part of the approach does not require a separate network. The inter-arbiter communication logic consists of an additional three bits and can be bundled with the credit wires of the original bidirectional link architecture. The RAIN design uses two networks for resiliency. Unified Virtual Channel Structure Instead of having a set of virtual channels strictly associated to a given port as seen in the conventional router, the RAIN router has a pool of virtual channels that can be shared among the ports. It follows the unified virtual channel structure approach of the ViChaR [5] router architecture. With this approach, virtual channels are not statically partitioned and fixed to input ports, rather they are communal resources dynamically managed by the reconfiguration module. This approach prevents a faulty buffer from impacting any particular port or rendering a port unusable. Furthermore, Adaptive Manycore Architectures for Big Data Computing NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 4: Autonomous globally state aware polymorphism the unified VC design lends itself extremely well to the use of the bidirectional links in the architecture. It allows for the link direction switching to be coupled with buffer space reallocation. With more available VCs to select from, link direction switching is more efficient. Self-monitoring and Self-reconfiguration The RAIN router monitors its: (1) routing costs of packets, (2) buffer allocation and utilization, and (3) link and buffer operating states. First, there is a learning phase where the router collects network state information, then learns and forecasts communication patterns. Second, there is a monitoring phase to validate the information learned in the first phase. In the final phase, routing tables are updated. The collection of network state information uses an augmented credit message format. Besides the conventional credit information (CR), free buffer spaces (FB), header flit arrival time (AT), route computation latency (RC), virtual channel allocation latency (VA), and switch allocation latency (SA) information on downstream routers are sent back [15]. Figure 5 shows the information pieces added to the credit message. Instead of sharing the network link bandwidth with program data, a secondary bufferless network is created to route the augmented credit messages [29]. This credit network sends two types of messages: one contains the credit information when the header and the body flits are passing through the router and the second has the routing state information when the tail flit passes through the router. Tables shown in Figure 5 are stored in the Routing Cost Table. The secondary noninterfering bufferless network adds extra resiliency to the router. It guarantees that the network is monitored and state information are collected when in the presence of main network failures. A router can identify a faulty buffer, link, or router by examining FB, AT, RC, VA, and SA collected data against expected values. Figure 5: Augmented credit message. The reconfiguration is done in three places: the routing cost table, physical link direction, and virtual channel association to physical links. To ensure a deadlock-free network reconfiguration, a topology checker algorithm is run in the intelligence unit (cf., Figure 1 (4)) to determine if a change in link direction or node availability will affect a cut-element. a cut-element is an element whose removal breaks network connectivity and an element may be a vertex or an edge. The intelligence unit builds a network connectivity map and marks all cut-elements using a fully distributed depth-first search algorithm [18, 19]. This approach assumes that the on-chip network has been initially converted to a channel dependency graph [13] even with link direction changes. 2.2 Experimental Results Figure 6: Transpose benchmark saturation results. Figure 7: Synthetic Aperture Radar (SAR) saturation results. To test the efficiency of our router design, flows from both synthetic benchmarks and real applications (e.g, Shuffle and SAR Image formation) are used. Synthetic Aperture Radar (SAR) is a radar technique for emulating the effects of a large-aperture physical radar, whose construction is not feasible, with a smaller aperture (antenna) radar. The 2-D FFT image formation algorithm in SAR is computationally very demanding and generally classified as a highperformance computing application. The application was profiled and the inter-module communication was simulated. Injection rate is correlated to image size. The network is an 8 × 8 2D-Mesh. The router has a pool of 32 virtual channels and 4 slots per virtual channel. The Heracles [14] RTL simulator is used for all the experiments. Heracles’ injector cores are used to create network traffic. Figures NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J. R. Doppa et al. 6 and 7 show the Transpose and the SAR benchmarks latency results, respectively. For the conventional router (Conv), the unified virtual channel design (Pool), and the unified virtual channel plus bidirectional physical links with local decision making (Local), the Adaptive Dimensional Order Routing (AD-DOR) is used for routing. For the globally-aware design, the neural network based predictive routing [15] is used (ML). 3 CONFIGURABLE LINKS CIRCUITS FOR ADAPTIVE CONNECTIONS The fundamental equation for the latency of a packet in a NoC is as follows [17] H h=1 TP = H · (tr + tw ) + Ts + tc (h ) (1) It has a fixed component for router (tr ) + link (tw ) delay, which gets multiplied by the number of hops H ; a constant serialization delay Ts for multi-flit packets equal to the number of flits minus one, i.e., ( ⌈L/b ⌉ − 1), where L is the packet length and b is the link bandwidth; and a variable delay depending on contention at every hop (tc (h )). A decade of research in NoCs [3, 10, 23] coupled with technology scaling, has enabled microarchitectures with single-cycle routers (i.e., tr =1) and single-cycle links connecting adjacent routers (i.e., tw =1). This is the state-of-the-art today. However, network latency still goes up linearly with H . As core counts increase, H inevitably increases (linearly with k in a k × k mesh). As we add hundreds to thousands of cores on a chip [1, 4, 7, 9] for the big-data era, high hop counts will lead to horrendous on-chip network traversal latency and energy creating a stumbling block to core count scaling. We propose to design NoCs with adaptive link circuits, that can reconfigure to create single-cycle connections between any two cores. In this section, we first present an analysis of multi-mm link circuits to demonstrate why this is feasible from a circuit point of view. Next, we present the micro-architecture of our configurable interconnect fabric called SMART [25] that enables single-cycle traversals across multiple-hops. We then present a flow control scheme to setup SMART paths dynamically. Finally, we demonstrate how SMART can be leveraged to perform efficient on-chip power management. Figure 8: Delay of Repeated Wires. Repeater Spacing = 1mm, Wire Spacing ∼3.DRCm i n 3.1 Single-Cycle Multi-mm On-Chip Links On-chip wires are laid out as multi-bit buses between a driver (single/multi-stage inverter) and a receiver (clocked latch/flip-flop). Wire delay depends on the effective resistance (R) times capacitance (C) values. Both R and C go up linearly with the wire’s length. The C includes capacitance to ground, and capacitance between wires (i.e., coupling capacitance). We did a design-space exploration on wire-delay using a commercial 45nm technology node and commercial CAD tools to see how fast on-chip wires are. We observed that wires can enable 13 mm signaling at a GHz by adding repeaters (inverters or buffers) at regular intervals and increasing wire spacing to (to reduce coupling capacitance), as shown in Figure 8. This length can be increased further if custom repeaters were to be designed [6] or circuit techniques like crossover and shielding used to lower crosstalk [30]. Moreover, though wires are not becoming any faster, since on-chip clock frequencies have plateaued, and chip sizes remain fairly constant due to yields, we can conclude that wires are fast enough to provide single-cycle communication between any two cores in today’s and future technologies. Figure 9: SMART Control Path and Datapath. 3.2 Datapath: Reconfigurable Repeaters Though on-chip wires are fast enough for single-cycle communication, laying out dedicated all-to-all wires on-chip is infeasible due to area and power constraints. Instead, many-core architectures use a NoC with short-distance links, each controlled by a router. We propose to create single-cycle long-distance wires by connecting multiple short-distance links with reconfigurable repeaters. Each repeater either operates in a buffer mode, latching the incoming signal like a conventional clocked receiver, or in a bypass mode, forwarding it to the next repeater without latching like a conventional repeater. Thus we can send signals multiple-mm on-chip by setting intermediate repeaters to act in bypass modes, and the destination to act in a buffer mode. The micro-architecture of our proposed single-cycle multi-hop datapath is shown in Figure 9(b). We call Adaptive Manycore Architectures for Big Data Computing NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea this SMART (Single-cycle Multi-hop Asynchronous Repeated Traversal) [6, 25]. The reconfigurable repeaters are embedded within each router. 3.3 Control Path: Single-cycle Reconfiguration SMART paths can be setup in myriad ways. If the application’s communication pattern is completely known in advance, SMART paths can be circuit-switched and created by configuring the repeaters right before running the application [6]. If the application’s communication pattern is dynamic, SMART paths can be setup over additional control wires, which are shown in Figure 9(a). In the first cycle, the winner of the switch at Router R0 requests a SMART path of length 3 over its control wires. If there is no contention, it gets the full-path, and can perform a 3-hop traversal in the next cycle, as shown in Figure 9(b) for the blue flit. In case of contention, however, each router prioritizes its own local flits over bypassing flits, and sets the repeater to buffer mode, and sends its own flit out instead. This is shown in Figure 9(c) where the blue flit has to stop, and the pink flit uses the link between Routers 1 and 2 in Cycle 2. Figure 10: Performance of SMART with a Shared L2 Design. SMART paths are thus opportunistic. This is because the underlying datapath does not have all-to-all connections. Thus flits may get partial bypass paths in case of contention. However, most modern applications do not have heavy traffic over the NoC, as most memory requests get filtered by L1 caches. Figure 10 thus shows that SMART provides 49-52% latency reduction on average with PARSEC applications over a shared distributed L2 design (which is highly sensitive to NoC latency). In summary, SMART optimizes network latency as follows: H h=1 TP = ⌈(H /HPC )⌉ · (tr + tw ) + Ts + tc (h ) (2) where HPC stands for number of Hops Per Cycle, and depends on contention. We reduce the effective number of hops to ⌈(H /HPC )⌉ . The maximum value of HPC, is known as HPCma x and depends on the wire delay, tile size, and clock frequency. 3.4 SMART with DVFS As discussed above, the maximum distance (in hops) that SMART can achieve is HPCma x . SMART can provide an additional runtime knob for reducing energy without any loss of performance - reduce the clock frequency. In conventional DVFS, this helps lower energy, but at the cost of increased latency. In SMART, however, a lower Figure 11: SMART NoC with DVFS - Adaptive HPCma x . clock frequency can help increase HPCma x which can reduce latency in cycles, countering the overall increase in clock period [28]. Figure 11(a)-(c) illustrates this idea. There would be an optimal frequency that can provide an overall reduction in EDP. We sweep the design-space with different values of link frequency and plot the overall NoC delay vs. energy in Figure 11(d). The number next to each configuration represents the frequency multiplier. Traditional DVFS (MESH-F2) lowers energy but increases delay. Uniform frequency scaling associated with router voltage scaling (SMARTR2L2 and SMART-R4L4) improves energy, however increases delay. Running the router (R) at a high-frequency, and links (L) at a lower frequency (SMART-R1L2 and SMARTR1L4), enable 14% reduction in latency and 52% reduction in energy. We can leverage application level behavior, as we discuss later in Section 4, to enhance the DVFS policy further. 4 MACHINE LEARNING FOR ADAPTIVE CONTROL In this section, we first describe our vision for self-aware manycore architectures and associated challenges. Subsequently, we provide some candidate machine learning solutions to realize this vision and a concrete instantiation for dynamic power management to illustrate the main ideas. 4.1 Self-Aware Manycore Architectures Vision In today’s manycore systems, the behavior of the system relies on many control knobs that control different aspects of the processor. Through careful manipulation of these knobs, we can dynamically adapt different components of the manycore system depending on the situation at hand. We envision that manycore computing systems should automatically adapt their behavior to accommodate the dynamic needs of applications/users, performance and energy constraints, and resource availability. To achieve this goal, we need online learning algorithms that enable the system to learn how to vary these control knobs so that the system can adapt to conditions seen during run-time (e.g., application characteristics, process-variations, and aging) while accommodating dynamic user constraints. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J. R. Doppa et al. Core Adaptation We can dynamically adapt the voltage and frequency associated with each core using learned power management policies to optimize the energy consumption subject to performance constraints. Similarly, we can reconfigure the cores/accelerators depending on the workload to improve the performance. Interconnect Adaptation We can dynamically adapt the voltage and frequency of network elements to optimize the energy consumption subject to performance constraints. Similarly, we can reconfigure the interconnection network on-the-fly to improve the performance and energy-efficiency of the system. Application Adaptation Task mapping and task scheduling will directly impact the performance and energy consumption of the system. Therefore, it is important to learn policies that can score different types of cores and accelerators based on their suitability for a given task. Additionally, task scheduling policies should optimally consider and use all available resources. 4.2 Candidate Machine Learning Solutions Core and NoC Reconfiguration The space of physically feasible core and NoC reconfiguration designs is combinatorial in nature. Our goal is to find the design that minimizes a given cost function O . Machine-learning techniques can enable the problem-solver (a computational search procedure) to make intelligent search decisions to achieve computational efficiency for finding (near-) optimal solutions over non-learning based algorithms [8]. The STAGE algorithm [2] is very appropriate to solve this problem. STAGE learns an evaluation function based on the data of already explored designs, which is used to guide the search towards high-quality designs. The main advantage of STAGE over popular algorithms such as simulated annealing (SA) and Integer Linear Programming is that it tries to learn the structure of the solution space, and uses this information in a clever way to improve both convergence time and the quality of the solution. As the system size increases, this aspect of STAGE is very advantageous to (1) improve the design-validate cycle before mass manufacturing; and (2) dynamically adapt the designs for new application workloads. Figure 12: Performance comparison among STAGE, SA, and GA for 3D NoC design optimization. In a recent work, we undertook a comparative performance analysis between STAGE, SA, and genetic algorithm (GA) to design a TSV-enabled 3D NoC architecture [22]. Fig. 12 shows the communication cost of the optimized network from the STAGE, SA, and GA algorithms as a function of time. We can see that STAGE uncovers high-quality designs very fast (within 5 minutes). On the other hand, SA and GA reach Ob e s t more gradually compared to STAGE, and even after 50 minutes, their respective Ob e s t does not reach the same solution as STAGE. We conjecture that with the increase in the design space due to large system sizes and emerging technologies (e.g., Monolithic 3D integration), STAGE will be even more efficient than SA and GA. Adaptive Control Reinforcement Learning (RL) and Imitation Learning (IL) are two popular machine learning approaches for learning control policies [27]. IL is considered to be an exponentially better framework than RL for learning sequential decision-making policies, but assumes the availability of a good Oracle (or expert) policy to drive the learning process. At a very high-level, the difference between IL and RL is the same as the difference between supervised learning and exploratory learning. In the supervised setting, the learner is provided with the best action for a given state. In the exploratory setting, the learner only receives weak supervision in the form of immediate costs and needs to explore different actions at each state, observe the corresponding costs, and learn from past experiences to figure out the best action for a given state. From a complexity perspective, when it is possible to learn a good approximation of the expert, the amount of data and time required to learn an expert policy is polynomial (quadratic or less) in the time horizon (i.e., number of decision steps). However, near-optimal RL is intractable for large state spaces. For large system sizes where the state space grows exponentially with the number of cores, RL methods may not scale well. To efficiently create control policies offline for different application workloads, IL is a better choice if we can construct a good oracle policy. We provide a concrete IL methodology for dynamic power management and show its effectiveness [21]. RL formulations can be employed for online learning, but we advocate the use of more recent algorithms that take a policy search view instead of Q-learning [27]. 4.3 Dynamic Power Management: A Case Study Problem Description The design of high-performance manycore chips is dominated by power and thermal constraints. Voltage-Frequency Islands (VFI) has emerged as an efficient and scalable power management strategy [26]. In such designs, effective VFI clustering techniques allow cores and network elements (routers and links) that behave similarly to share the same Voltage/Frequency (V/F) values without significant performance penalties. Naturally, with time-varying workloads, we can dynamically fine-tune the V/F levels of VFIs to further reduce the energy dissipation with minimal performance degradation. For applications with highly varying workloads, machine learning (ML) methods are suitable to fine-tune the V/F levels within VFIs. Optimization Objective Consider a manycore system with n cores. Without loss of generality, let us assume that there exist k VFIs. The dynamic VFI (DVFI) Adaptive Manycore Architectures for Big Data Computing NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea control policy π , at each control epoch t (where N is the total number of epochs), takes the current system state st and generates the V/F allocation for all k VFIs: π : st → {V1 /F1 , V2 /F2 , · · · , Vk /Fk } (3) Given a VFI-enabled manycore architecture, an application, and a maximum allowable performance penalty, our objective is to create a DVFI control policy π ∗ that minimizes the energy dissipation within the user-specified maximum allowable performance penalty. Imitation Learning Methodology For our DVFI control problem, the expert corresponds to an Oracle controller that provides the supervision on how to make good control decisions for V/F tuning. There are three main challenges in applying the IL framework to learn DVFI control policies: 1) Oracle construction, 2) fast and accurate decision-making, and 3) learning robust control policies. We discuss and provide the corresponding solutions below. a) Oracle Construction. In traditional IL, expert demonstrations are used as the Oracle policy in the IL process. Unfortunately, we don’t have any training data for the DVFI control problem. For DVFI-enabled systems, we define the Oracle policy as the controller that selects the V/F level for each VFI that minimizes the overall power consumption within some performance constraint. Since the learning process is offline, we access the future system states and perform a look-ahead search to find the best joint V/F allocation for all VFIs. This is accomplished by running the application with different V/F assignments to optimize the global performance (i.e., EDP of the system). To overcome the computational challenge and closely approximate optimality, our key insight is to perform local optimization followed by aggregation for global optimization. First, we compute the optimal V/F (which minimizes EDP) for each VFI, at each control epoch, for m different execution time penalties (e.g., 0%, 5%, and 10% for m=3). This gives us m different V/F assignments for each control epoch. Second, for every n control epochs, we compute the best V/F decisions by performing an exhaustive search over all possible combinations of local optima from the first step (mn ). Note that it is easy to find a small m that works well in practice, but both the quality and computation time of the Oracle depends on n . Figure 13: Illustration of DVFI decision-making as a structured prediction task b) Fast and Accurate Decision-Making. We formulate the problem of DVFI control decision-making as a structured output prediction task [11, 12]. This is the task of mapping from an input structured object (a graph with features on the nodes and edges) to an output structured object (a graph with labels on the nodes and edges). Figure 13 illustrates the structured prediction task corresponding to the DVFI control decision-making. The structured input graph contains a node for each VFI and edges corresponding to interVFI traffic density. The structured output graph captures the V/F allocation (node labels) for each VFI and the structural dependencies between different input and output variables. The main challenge in DVFI control is to choose the best V/F from the large space of all possible V/F assignments (Lk , where k is the number of VFIs and L is the number of V/F levels for each VFI). This is especially challenging in our DVFI control problem: we are trying to predict the joint V/F allocation for all VFIs to save energy, but it is useless if the computation for making the prediction consumes more energy than the energy saved. Therefore, we want a fast and accurate predictor whose energy overhead is miniscule when compared to the overall energy savings due to DVFI control. To address the above-mentioned challenge, we learn pseudoindependent structured controllers to achieve efficiency without losing accuracy. Specifically, we learn k controllers, one controller for each VFI. These controllers are pseudo-independent in the sense that each controller predicts the V/F allocation for only a single VFI but has the context of previous predictions from all controllers and the structural dependency information of the other VFIs when making predictions. Intuitively, the different controllers are trying to help each other by supplying additional contextual information. Figure 14: Illustration of Learning with DAgger c) Learning Robust Control Policies. Our goal is to learn a controller that closely follows the Oracle in terms of V/F allocation. Unlike standard supervised learning problems that assume IID (Independent and Identically Distributed) input examples, our controller learning problem is Non-IID because the next state depends on the decision of the controller at the previous state. Therefore, controllers learned via exact imitation can be prone to error propagation: errors in the previous state may result in a next state that is very different from the distribution of states the learner has seen during the training, and contributes to more errors. To address the error-propagation problem associated with exact imitation training, we can employ an advanced imitation learning approach called DAgger [24]. The key idea behind DAgger is to generate additional training data so that the learner is able to learn how to recover from mistakes (see Figure 14). Experimental Results In a recent work, we showed the effectiveness of the above IL methodology when compared to the prior approaches for power management [21]. Fig. 15 shows the computational overhead for learning each DVFI policy and the full-system energy dissipation for IL, RL, and a feedback-based (FB) DVFI control policy [20]. Here, each RL and FB marker represents the results of a benchmark normalized with respect to IL. The IL policy is able to outperform every benchmark while requiring significantly less computational overhead than RL. Since there is no learning involved in FB, the NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J. R. Doppa et al. [3] Amit Kumar et al. 2007. Express Virtual Channels: Towards the Ideal Interconnection Fabric. In ISCA ’07. 150–161. https://doi.org/10.1145/1250662.1250681 [4] Boris Grot et al. 2011. Kilo-NOC: A Heterogeneous Network-on-chip Architecture for Scalability and Service Guarantees. In ISCA ’11. 401–412. https://doi.org/10. 1145/2000064.2000112 [5] Chrysostomos A. Nicopoulos et al. 2006. ViChaR: A Dynamic Virtual Channel Regulator for Network-on-Chip Routers. In MICRO 39. 333–346. https://doi.org/ 10.1109/MICRO.2006.50 [6] Chia-Hsin O. Chen et al. 2013. SMART: A single-cycle reconfigurable NoC for SoC applications. In DATE ’13. 338–343. https://doi.org/10.7873/DATE.2013.080 [7] Daniel Johnson et al. 2011. Rigel: A 1,024-Core Single-Chip Accelerator Architecture. IEEE Micro 31, 4 (July 2011), 30–41. https://doi.org/10.1109/MM.2011.40 [8] F.A. Rezaur Rahman Chowdhury et al. 2017. Select-and-Evaluate: A Learning Framework for Large-Scale Knowledge Graph Search. JMLR 80 (2017). [9] George Kurian et al. 2010. ATAC: A 1000-core Cache-coherent Processor with On-chip Optical Network. In PACT ’10. 477–488. https://doi.org/10.1145/1854273. 1854332 [10] Hiroki Matsutani et al. 2009. Prediction router: Yet another low latency on-chip router architecture. In HPCA ’09. 367–378. https://doi.org/10.1109/HPCA.2009. 4798274 [11] Janardhan R. Doppa et al. 2014. HC-search: A Learning Framework for Searchbased Structured Prediction. JAIR 50, 1 (May 2014), 369–407. [12] Janardhan R. Doppa et al. 2014. Structured Prediction via Output Space Search. JMLR 15, 1 (Jan. 2014), 1317–1350. [13] Michel A. Kinsy et al. 2009. Application-aware Deadlock-free Oblivious Routing. In ISCA ’09. 208–219. https://doi.org/10.1145/1555754.1555782 [14] Michel A. Kinsy et al. 2013. Heracles: A Tool for Fast RTL-based Design Space Exploration of Multicore Processors. In FPGA ’13. 125–134. https://doi.org/10. 1145/2435264.2435287 [15] Michel A. Kinsy et al. 2017. PreNoc: Neural Network Based Predictive Routing for Network-on-Chip Architectures. In GLSVLSI ’17. 65–70. https://doi.org/10. 1145/3060403.3060406 [16] Myong Hyon Cho et al. 2009. Oblivious Routing in On-Chip Bandwidth-Adaptive Networks. In PACT ’09. 181–190. https://doi.org/10.1109/PACT.2009.41 [17] Natalie E. Jerger et al. 2017. On-Chip Networks, Second Edition. Synthesis Lectures on Computer Architecture (2017). [18] Pengju Ren et al. 2016. A Deadlock-Free and Connectivity-Guaranteed Methodology for Achieving Fault-Tolerance in On-Chip Networks. IEEE TC 65, 2 (Feb. 2016), 353–366. https://doi.org/10.1109/TC.2015.2425887 [19] Pengju Ren et al. 2016. Fault-Aware Load-Balancing Routing for 2D-Mesh and Torus On-Chip Network Topologies. IEEE TC 65, 3 (March 2016), 873–887. https://doi.org/10.1109/TC.2015.2439276 [20] Ryan G. Kim et al. 2016. Wireless NoC and Dynamic VFI Codesign: Energy Efficiency Without Performance Penalty. IEEE TVLSI 24, 7 (2016), 2488–2501. [21] Ryan G. Kim et al. 2017. Imitation Learning for Dynamic VFI Control in LargeScale Manycore Systems. IEEE TVLSI 25, 9 (Sept 2017), 2458–2471. https://doi. org/10.1109/TVLSI.2017.2700726 [22] Sourav Das et al. 2017. Design-Space Exploration and Optimization of an EnergyEfficient and Reliable 3-D Small-World Network-on-Chip. IEEE TCAD 36, 5 (May 2017), 719–732. https://doi.org/10.1109/TCAD.2016.2604288 [23] Sunghyun Park et al. 2012. Approaching the theoretical limits of a mesh NoC with a 16-node chip prototype in 45nm SOI. In DAC ’12. 398–405. [24] Stéphane Ross et al. 2011. A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. In AISTATS ’11, Vol. 15. Fort Lauderdale, FL, USA, 627–635. [25] Tushar Krishna et al. 2013. Breaking the on-chip latency barrier using SMART. In HPCA ’13. 378–389. https://doi.org/10.1109/HPCA.2013.6522334 [26] Umit Y. Ogras et al. 2009. Design and Management of Voltage-frequency Island Partitioned Networks-on-chip. IEEE TVLSI 17, 3 (2009), 330–341. [27] Hwisung Jung and Massoud Pedram. 2010. Supervised Learning Based Power Management for Multicore Processors. IEEE TCAD 29, 9 (Sept. 2010), 1395–1408. [28] Monodeep Kar and Tushar Krishna. 2017. A Case for Low Frequency Single Cycle Multi Hop NoCs for Energy Efficiency and High Performance. In ICCAD ’17. IEEE. [29] Michel A. Kinsy and Srinivas Devadas. 2014. Low-overhead hard real-time aware interconnect network router. In HPEC ’14. 1–6. https://doi.org/10.1109/HPEC. 2014.7040976 [30] Jan M. Rabaey and Anantha Chandrakasan. 2002. Digital Integrated Circuits: A Design Perspective. Prentice Hall Pub. Figure 15: Comparison of several DVFI policies (IL, RL, and a feedback-based controller (FB)) in computational overhead to learn the policy and full-system energy dissipation. computation time is negligible. The performance gap between IL methodology relative to other approaches will grow with the system size and complexity of decision-making. 5 CONCLUSIONS AND FUTURE CHALLENGES In this work, with the focus on the on-chip network, we present an illustrative design methodology for adaptive manycore architectures to address the computational needs of emerging big data applications and the technological constraints of power and reliability. It is a cross-layer design approach that spans circuits, microarchitecture, and machine learning algorithms for intelligent and autonomous runtime adaptation. Although the research community has made progress in exploring self-aware adaptive architectures, many important research questions remain open. Ultimately, the design of adaptive manycore architectures requires a hardware-software co-design approach for maximum benefits. In the future, adaptivity should not only expand to all components of the manycore architecture, but also operate in a holistic manner to achieve the global objectives of the system. This will involve not only highly adaptable components across the entire system (much like the ones presented in this paper), but also a detailed understanding of how each control decision or joint control decisions (e.g., DVFI, communication routing, task management, resource allocation) affects any of the system-level objectives (e.g., power, energy, latency, execution-time, and reliability constraints). Some important research gaps include (1) dynamic allocation and reconfiguration of computing resources depending on the needs of program (e.g., the amount of parallelism in the program); (2) automatic hardware-level approximation to meet both program and system goals (e.g., execution time budget, power constraints, and resiliency) without the programming complexity of current manycore systems; (3) methodologies to enable programmers to succinctly specify the execution context along with the computational/algorithmic components of programs; and (4) innovations in control and learning techniques to handle this large complex state-action space under dynamically changing constraints with negligible computational overhead. "
On the Accuracy of Stochastic Delay Bound for Network on Chip.,"Delay bound guarantee in network on chip (NoC) is important for hard real-time applications, and deterministic network calculus (DNC) is a effective tool for delay bound modeling. But for soft real-time applications, delay bound derivation using DNC is often over-pessimistic, resulting in too much chip area (e.g., router buffer) and power consumption; stochastic network calculus (SNC), on the contrary, improves the delay bound accuracy by providing stochastic service curves. Existing service models assume that contention takes place as long as there exist contention flows from different input channels requesting the same output channel. These models only consider flow paths in flows contention analyzing. We have observed that, beyond flow path contentions, the arrival rate also has deep influence on the flow contention in NoC, consequently affecting delay bound. In this paper, we further analyze the intrinsic factors affecting the flow contention, and propose a stochastic analytic model of per-flow delay bound to improve the calculation accuracy, according to both path and arrival rate. Within this model, the end-to-end delay bound is evaluated based on SNC. Experimental results show that our proposed model is both effective and accurate.",Packet 1 (t) 2 (t) (t) 1 (t) 1F 2F 1R Packet (a) 2R Lu 1 (t) Jiang 1 (b) 1 2 (t) Cycle IP0 IP1 IP2 IP3 NI R0 NI R1 NI R2 NI R3 IP4 IP5 NI IP6 IP7 NI R4 NI R5 NI R6 NI R7 IP8 IP9 IP10 IP11 NI R8 NI R9 NI R10 NI R11 IP NI Intellectual Property Network Interface IP12 IP13 IP14 IP15 R Router NI 12 NI R13 NI R14 NI R15 Packet Packet 1F 2F (a) 1F 2F (b) 1F 2F (c) 3F 0 4 8 1 5 9 2 6 2F 10 1F 12 13 14 (a) 3 7 11 15 3F 4F 0 4 8 5F 1F 1 5 9 2 6 2F 10 3 7 11 12 13 14 15 (b) 1F R9 RR 2F R5 R1 R2 3F 
Distributed and Dynamic Shared-Buffer Router for High-Performance Interconnect.,"Most Network-on-Chip routers dedicate a set of buffers to the input and/or output ports. This design decision leads to buffer underutilization especially when running applications with non-uniform traffic patterns. In order to maximize resource usage for performance and energy gains, we present a synchronous and elastic buffer implementation of a router architecture called Roundabout with intrinsic resource sharing. Roundabout is inspired by real-life traffic roundabouts and consists of lanes shared by multiple input and output ports. Roundabout offers performance improvement of 61% for uniform traffic pattern and up to 88% for non-uniform traffic pattern over the Hermes router, a typical input buffered router. In terms of power, it consumes 24% less than the Hermes router. Roundabout provides a highly parametric architecture that can produce different router configurations with varying topological trade-offs for performance gains without sacrificing area.","Distributed and Dynamic Shared-Buffer Router for High-Performance Interconnect Charles Effiong, Gilles Sassatelli, Abdoulaye Gamatie Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier (LIRMM), Centre National de la Recherche Scientifique (CNRS), Université de Montpellier, France [firstname.lastname@lirmm.fr] ABSTRACT Most Network-on-Chip routers dedicate a set of buffers to the input and/or output ports. This design decision leads to buffer underutilization especially when running applications with non-uniform traffic patterns. In order to maximize resource usage for performance and energy gains, we present a synchronous and elastic buffer implementation of a router architecture called Roundabout with intrinsic resource sharing. Roundabout is inspired by real-life traffic roundabouts and consists of lanes shared by multiple input and output ports. Roundabout offers performance improvement of 61% for uniform traffic pattern and up to 88% for non-uniform traffic pattern over the Hermes router, a typical input buffered router. In terms of power, it consumes 24% less than the Hermes router. Roundabout provides a highly parametric architecture that can produce different router configurations with varying topological trade-offs for performance gains without sacrificing area. CCS CONCEPTS • Hardware → Network on chip; KEYWORDS Network-on-Chip, Router, Buffer sharing, High performance ACM format: Charles Effiong, Gilles Sassatelli, Abdoulaye Gamatie. 2017. Distributed and Dynamic Shared-Buffer Router for High-Performance Interconnect. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130223 1 INTRODUCTION Networks-on-Chip (NoCs) have emerged as a mature alternative interconnect for manycore architectures compared to traditional bus and crossbar interconnects [9]. Although numerous architectures have been proposed in the literature, typical NoC routers consist of input/output buffers used to temporarily store packets that cannot proceed to their outputs due to possible network contentions. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130223 Buffers in a typical NoC router are tightly coupled to the input/output ports and therefore can only be exploited by data-flows using those ports at any point in time. It has been observed that for such NoC routers a large number of buffers are left unutilized (i.e., idle or underutilized) even at high load, while other buffers are busy, especially while running applications with non-uniform traffic patterns and/or bursty behaviors [16, 17]. Unutilized buffers can therefore lead to high performance degradation. Router architectures capable of fully utilizing their buffering resources for energy efficiency and performance gains are desired [17]. The Rotary [1] router share similar roundabout/ring-like concept with the proposed router. However, it utilizes a combination of virtual cut-through (VCT) and bubble flow-control. The main reason for that choice is that it is extremely challenging to employ wormhole flow control in ring-like router architectures because of its proneness to deadlocks [5]. On the other hand, VCT requires large buffers which occupy chip area and consumes significant power [13, 17]. In order to maximize buffer resource utilization and to provide low-latency support for applications with diverse traffic characteristics, we consider the concept of Roundabout router [6, 7]. This router is inspired by a real-life multi-lane traffic roundabouts, which consists of lanes shared by input/output ports in order to maximize resource utilization. In this paper, a synchronous implementation of Roundabout based on elastic buffers [2] is devised and evaluated based on 45nm CMOS technology. This implementation leads to over 80% area reduction and much simpler design compared to the asynchronous implementation [7]. We provide detailed comparison of Roundabout against the Hermes [12] router, which is a typical input buffered wormhole router, Rotary [1] and virtual-channel single cycle routers [3, 14]. For similar buffer count, Roundabout provides a performance improvement of 61% over the Hermes [12] for uniform traffic pattern and up to 88% for non-uniform traffic pattern. Our main contributions can be summarized as follows: • We expose in detail a synchronous implementation of a roundabout-inspired router called Roundabout [7], with inherent buffer resource sharing. • We carry out a comprehensive analysis of Roundabout in terms of performance, area and power consumption. A performance improvement of 61% and up to 88% is reported respectively for uniform and non uniform traffic patterns over the Hermes router. In terms of power, it consumes 24% • We also evaluate the impact of resource utilization on perforless than the Hermes router. mance and power consumption in the router. The scalability of Roundabout is assessed by using multiple lanes and additional buffering. Roundabout is shown to be more scalable than the Hermes router. • We explore various Roundabout topologies and we show that its router concept offers a highly parametric architecture that can produce different router configurations with varying topological trade-offs for performance gains without sacrificing area. The remainder of this paper is organized as follows. Section 2 describes related work. Roundabout is introduced in Section 3, while its implementation is described in Section 4. Section 5 presents Roundabout evaluation, while Section 6 gives concluding remarks and perspectives. 2 RELATED WORK Several techniques have been proposed in the literature to improve traditional router performance and reduce power consumption. Bufferless routing completely remove buffers from the router, which saves router area and power [10, 13]. However, at high traffic loads, bufferless routers suffer from performance degradation and energy increase compared to IBR. Indeed, packets are frequently deflected further from their destination due to network contention. Increasing deflections consumes router link bandwidth and increases energy per packet. Unlike bufferless router, packets are stored in idle buffers when contention occurs in IBR. Thus, saving link bandwidth [13]. On the other hand, router architectures with buffer sharing have been proposed for better resource utilization. A router architecture with distributed shared-buffer has been proposed in [15]. It provides a higher network throughput compared to an input buffered router. However, such a performance improvement comes at the expense of more power dissipation and area due to the additional crossbar and complex arbitration scheme [15]. Roundabout uses distributed buffering and improves performance over input buffered router without sacrificing area and power. The RoShaQ15 router with shared buffer bypass technique for reducing zero-load latency has been proposed in [17]. RoShaQ15 however requires an additional crossbar for allocating the shared queue which incurs a higher power and introduces additional area overhead. Roundabout does not use explicit crossbar and input buffers. Hence, Roundabout avoids the additional area and power dissipation associated with crossbars and input buffers. The Rotary [1] router, optimized for torus topologies, shares a similar roundabout concept as our proposed router. It combines bubble (i.e., data spacers) and VCT features to avoid deadlocks. Contrary to Rotary router that uses bubbles for ensuring deadlock-freeness, Roundabout uses wormhole and relies on a carefully designed cyclefree internal topology. 3 THE ROUNDABOUT ROUTER We briefly recall the basic principles of the Roundabout router as introduced in our previous work [7]. 3.1 Roundabout architecture Roundabout router is inspired by real-life multi-lane roundabouts where cars go on a lane and then switch to high priority lane should they miss their exit. Fig. 1 shows a 4-lane high-level Roundabout 2 Figure 1: A 4-lane Roundabout architecture architecture. Here, we use architecture and topology interchangeably to refer to the manner in which the lanes are arranged in the router. The lanes in Roundabout are partitioned into primary and secondary lanes. In Fig. 1, Lane 0 and Lane 1 are the primary lanes, while Lane 2 and Lane 3 are the secondary lanes. Packets on the secondary lanes are given priority to access shared resources whenever requests for a shared resource originate simultaneously from both lanes. The input ports are distributed only to the primary lanes, while the secondary lanes are exploited only whenever congestion occur. In the topology shown in Fig. 1, Lane 0 hosts the local and west input ports, while Lane 1 hosts the east, south and north input ports. In the Roundabout architecture, the input ports are carefully distributed to the lanes in such a way as to avoid deadlocks. Deciding how the input ports are distributed to the primary lanes is inspired by XY-Routing algorithm, where data is first routed along the x-direction and then along the y-direction to the receiver. In XY-routing, a packet from the north input port can only be destined for south or local output port, while a packet from the south input port can only be destined for the north or local input port. Hence, distributing the south, north and east input ports to Lane 1 does not produce deadlock in the router. Packets from the input ports may be temporarily blocked (due to contentions on the lane); however, they will eventually advance to their desired output port. Therefore, no packet is permanently blocked on the lane. A similar argument can be made for packets from the west and local input ports distributed to Lane 0. When a packet flowing on a primary lane is blocked or not granted access to the output port, it switches to a secondary lane via the switch link. A scenario where a packet switches to a secondary lane, via a switch link, because its path is blocked is shown in Fig. 2(b). In Fig. 2(b), a packet from the east input port destined for the local output port switches to a secondary lane because the path is being used by another packet. This design choice avoids queuing packets on the lane (if possible) whenever multiple flows compete for shortest path resources, which increases router resource utilization and packet throughput. Fig. 2(c) shows a scenario Figure 2: Packet flow scenarios: (a) Packet entry (b) Lane switching caused by blocked path (c) Lane switching when packet output port is busy. PX →Y : a packet from X input port and destined for Y output port. (d) Low traffic: a packet from west switches lane and uses secondary lane resources. (e)-(f ) More lane resources are being used. where a packet from the local input port and destined for the west output port switches to a secondary lane because the west output port is occupied by a packet flowing from east input port to west output port. Packets can only make forward progress (i.e. continue on a given lane or switch to a secondary lane) if the desired lane resource is available. Otherwise, they are queued on the lane. The output port is granted in a round-robin manner if two simultaneous request for a given output port originate from either two primary or two secondary lanes. More generally, the main properties of Roundabout can be summarized as follows: i The router can have N lanes (where N ≥ 2). The lanes are partitioned into primary and secondary lanes with an arbitrary lane count in each. ii The output ports are connected to both primary and secondary lanes, while the input ports are only connected to the primary lanes. iii Packets can switch from primary to secondary lanes when either their path is blocked or their output is unavailable. As shown in Fig. 2(d-f ) almost all the lane resources can be used when network load is high. This version of Roundabout supports XY and YX-Routing. Roundabout offers different topological trade-offs, in terms of the number of lanes, the input ports distribution on the primary lanes and the parallelism level on both the primary and secondary lanes. We explore such trade-offs in Section 5. 3.2 Deadlock avoidance One of the main challenge with designing communication architecture with shared resources is deadlock-freeness. This is because resource sharing often introduces cyclic-dependencies which leads to deadlock as discussed in Section 2. Our goal in this subsection is to show that the router and corresponding networks are deadlockfree. In order to assess Roundabout deadlock-freeness, we used the well-known theoretical model for deadlock avoidance proposed by Duato [5]. This model relies on building a dependency diagram of the shared network resources and identifying cyclic dependencies. Deadlock can potentially occur if cyclic dependencies exist 3 Figure 3: (a) Router channel dependency graph (b) Four packet flows in a 2x2 mesh network. P X Y →Z : X is packet number, Y is packet source node and Z is packet destination node between shared resources. The network architecture and the routing algorithm are key inputs for building the channel dependency graph (CDG). Fig. 3(a) shows the CDG for the proposed router. A node on the CDG represents channels, whereas an edge represents the dependency between them. To build this graph, we consider the router architecture described in Section 3.1 and XY-Routing algorithm. The router is deadlock free since there are no cyclic dependencies in the CDG. Since deadlock-freeness at the router level does not automatically corresponds to deadlock-freeness at the network-level, we followed a similar method to assess networklevel deadlock-freeness. Here, we provide an informal example on a 2x2 mesh topology network (for simplicity) to show that mesh networks composed of Roundabout routers are deadlock-free. Consider the network in Fig. 3(b) with transpose traffic pattern. Here, we assumed that the packets, i.e. P 0 , P 1 , P 2 , and P 3 were injected into the network at the same time. No deadlock occurs in this network and all packets will eventually get to their destinations. Notice that P 1 destined for node R2 waits just before the east input because P 3 is currently occupying the lane. However, after sometime P 1 can advance and make its way to the local port of node R2 when P 3 is completely transmitted. The packets can successfully make forward progress and deadlocks do not occur. 4 ROUNDABOUT IMPLEMENTATION Roundabout heavily relies on effective handshaking between individual blocks assembled in lanes. Though the implementation could be based on conventional FIFO queues, we chose the efficient elastic buffer (EB) flow control. EB flow-control eliminates the need for explicit input and/or output buffers at the router by using existing pipelined flip-flops in the channels to implement elastic FIFOs. EB can be implemented using either edge-triggered registers or latches [2]. The flip-flop based implementation is more favorable for timing analysis. However, the latch based implementation is preferred here because of its area, delay and power benefits over the flip-flop-based counterpart [2]. Fig. 4a shows a latch based implementation of an EB, which is composed of master and slave latches controlled by a logic. The control logic drives the enable signal of the latches independently, making it possible for both latches to store separate data [11] [2]. A possible implementation of the control logic is given in [11]. EB channels use consecutive EBs to form a distributed FIFO. The EBs control access to the next shared lane resource (i.e. south output controller buffer). For this purpose, the arbiter operates in a FCFS manner or round-robin if request from both controllers are simultaneously asserted. The arbiter is implemented using FSM with combinational output (Mealy FSM), hence it can process requests as soon as they arrive without waiting for clock edge. 5 EVALUATION 5.1 Router evaluation In this section, we evaluate the Roundabout router and show the benefits of our proposal against the Hermes router. 5.1.1 Performance and Area Evaluation. For Roundabout area and head-flit latency evaluation, we consider the router version shown in Fig. 1. The router was synthesized with Cadence Design Compiler using a 45nm standard CMOS cell library. Table 1: Performance of 34-bits Roundabout routers. Head-flit latency (cycles) Min Max 2 6 Operating freq. (MHz) 650 Total area (mm2 ) 0.03 In Table 1, the result for the min and max head-flit latencies are displayed. The min and max head-flit latencies correspond to latencies incurred when the head-flit takes a short path (e.g. west input to local output port) and a long path (e.g. west input to north output port) on a given lane in the router. Thanks to the short router pipeline, the head-flit latency for the router is only 2 clock cycles for short path (a cycle each for input and output controllers). Output port arbitration is performed in the same cycle when output request is sent by output controllers. Compared to most typical NoC router, the router pipeline for a short path is twice less. As an example, the Hermes [12] router has a router pipeline of 5 clock cycles which is only one clock cycle less than the pipeline cycle period for the longest path on a given lane in Roundabout. The 4 or 5 stage pipeline in typical routers is often due to series of steps such as buffer allocation, route computation, switch allocation, switch traversal and link traversal (often requiring a cycle each) needed to route a packet. Roundabout distributed architecture provides a much simpler design (no centralized crossbar) and shorter pipeline stages to route a packet. The router is clocked at 500MH z and has a small area overhead as displayed. The max operating frequency of the design is reported in Table 1. In order to investigate the impact of lane switching (i.e. switching from primary to secondary lanes) on the router on performance, we compare router configurations with and without lane switching. Unlike the Rotary [10] router which incurs power and performance overheads caused by allowing packets to make multiple turns in lanes as discussed in Section 2, Roundabout avoids these overheads by allowing packets to immediately switch to secondary lanes via the switch link. Fig. 6(a) - (d) shows the considered packet flow scenarios, where only primary lanes are used in Fig. 6(a) and (b) i.e. no lane switching, while packets switch lanes in Fig. 6(c) and (d). Table 2 shows the (a) Latch based EB (b) Synchronous elasticization Figure 4: Elastic buffer (EB) implementation communicate using a ready/valid handshake [11]. When the valid signal is asserted, the upstream EB is sending a valid data. Similarly, when the ready signal is asserted, the downstream EB has at least one more empty slot for accepting a new data. As shown in Fig. 4b, synchronous blocks can be made elastic by adding an elastic wrapper around them without introducing additional latency [8]. We adhere to this observation in our design. 4.1 Packet format The router employs wormhole flow control. Roundabout packets consist of N-flits of 32 data-bits, encoded with the begin-of-packet (BoP) and end-of-packet (EoP) information. A packet has three types of flits: head, body and tail. The head flit encodes the destination address and output port. The body/tail flits contain actual data. Resources allocated by the head-flit are deallocated after the tail flit is transmitted. 4.2 Controllers As depicted in Fig. 5, the input controller block consists of an EB and a path computation (PC) block. The area overhead of this block is minimal. When a flit arrives at the input port of the router, it is forwarded to the PC block. The PC block is responsible for computing the packet output port using XY-routing. If the flit is a header-flit, the PC block decodes the packet destination address encoded in the flit and uses this information to compute the packet output port in the current router. The output port routing information is encoded in the header-flit of the packet before it is transmitted. The other flits follow the path already reserved by the header-flit. The output controller is also displayed in Fig. 5. It consists of an elastic-buffer, an output logic block and a demux. When a flit is received, it is forwarded to the output logic block. Depending on the packet output information encoded in the packet header and the output status, the block selects one of two possible paths to forward the packet. The path controller is similar to the output controller in terms of functionality. It forwards a packet to the lane if the packet path is not blocked. Otherwise, it switches the packet to a secondary lane. More information about the controllers can be found in [7]. The entire pipeline for lane 0 is shown in Fig. 5. The blocks (i.e. input, output, lane controllers) make up the individual pipeline stages and communicate via ready/valid handshake protocol. Each block incurs only a cycle each, hence the entire pipeline is only 6 clock cycles for the longest path. Notice that arbitration between the local lane controller and the local input controller is required to 4 Figure 5: Roundabout Lane 0 pipeline Figure 6: Packet from east port to north port: (a) without lane switching, (c) with lane switching. Packet from east to south: (b) without lane switching (d) with lane switching. Table 2: Impact of lane switching on packet latency Without lane switching With lane switching Scenario-A Scenario-B Scenario-C Scenario-D 11 cycles 15 cycles 18 cycles 15 cycles Figure 7: Power consumption based on paths taken in the router corresponding latency incurred for transmitting a packet for each scenario. Here, we make two important observations: (i) taking longer routes in the router does not necessarily incur significant latency overhead. As an example, scenario-B incurred only a small additional latency of 4 clock cycles when compared to scenarioA where the packet took a short route. This observation is also valid for all the other scenarios. (ii) similarly, lane switching does not incur significant packet latency overhead. On the contrary, lane switching avoids additional latency incurred when packets are 5 Figure 8: Blocked packet caused by different arrival times queued and improve the router throughput since switching lanes frees up the router resource for use by other packets.The network packet latency at low-load is expected to be minimal since only few packet will switch lanes. In the Hermes router, the latency for flowing a similar number of flits (10 flit packet) from any input port to an output port (assuming the packet is not blocked) takes 15 clock cycles. Therefore, lane switching does not significantly impair Roundabout performance. 5.1.2 Power estimation. We carried out Roundabout power estimation of the 4 lanes router at Gate level using Synopsys PrimeTime PX and the 45nm CMOS cell library. Fig. 7 shows the power results for different packet throughputs depending on the paths (i.e. shortest, short, long, longest) taken by packets in the router. Shortest, short, long, longest paths correspond to a path from west input to local, south, east and north output ports respectively. The power results are obtained when the router links are operating at their maximum throughput. This plot shows one of the interesting benefits of the resource adaptive features of Roundabout, where the power consumption does not solely depend on the packet throughput but also on the path taken by packets in the router. Hence, less power is consumed for similar packet throughputs when shorter paths are utilized in the router, i.e. under light load. Additionally, it is visible from the plot that the power consumption increases proportionally with traffic, which is a desirable feature achieved thanks to the traffic-proportional utilization of lanes and buffers. 5.2 Topology exploration We have presented a four-lane Roundabout architecture in Section 3. The resource sharing feature allows packets to switch lanes and use secondary lane resources when contention occurs on the lane. In the topology shown in Fig. 1, lane switching frees-up the primary lane resources for use by incoming packets (assuming the secondary lane is free and packets can exit). However, packets cannot always switch lanes when contention occurs on the primary lanes. Fig 8 (a) Performance of Roundabout routers (b) Area overhead of Roundabout routers Figure 9: Performance and area-overhead of Roundabout routers (H: Hermes) (a) Comparison w.r.t. different buffer counts (b) Comparison w.r.t. different traffics and buffer counts Figure 10: Performance comparison for Roundabout and Hermes routers. XB denotes baseline router with additional X buffers shows three different scenarios where a packet with a later arrival time (i.e. P1) is temporarily blocked because the lane is occupied by an earlier packet (i.e. P0 ). In the scenarios, the switch links are not available for use by the blocked packets. This situation occurs because the parallelism level on the primary lanes is limited. Therefore the router cannot always support five concurrent dataflows supported in typical router architecture like Hermes [12]. Table 3: Roundabout configurations (P/S denotes the ratio of primary to secondary lanes. PL denotes parallelism level.) Config. C0 C1 C2 C3 C4 C5 Lane depth 2 3 3 4 3 2 PL 2 5 4 3 3 3 P/S (%) No. of lanes 50 4 56 44 33 33 44 9 6 A possible solution to mitigate performance loss caused by temporarily blocked packet is through the use of additional buffers. Additional buffers can be added to the lanes to improve packet throughput. However, this will incur significant area overhead and latency since packets will have to traverse many buffers assembled on the lanes. Roundabout is a highly parametric architecture template that can produce different router configurations i.e. with varying topological trade-offs in terms of number of primary/secondary lanes, input port distribution on the primary lanes and the parallelism level on the lanes, etc. Since Roundabout can have N-number of lanes, we have chosen to explore 9 lanes version of the router allowing for high level of parallelism on the primary lanes and a good number of shared secondary lanes. Table 3 shows the properties of the different 9 lanes versions of Roundabout. Note that configuration C0 is the 4 lane architecture in Fig. 1, while others are 9 lanes. Roundabout routers can have a maximum of five primary lanes indicated by the parallelism level column in Table 3. A level of 5 means the router has maximum parallelism on the primary lanes (i.e., a lane associated with each input port), while the secondary lanes are shared among packets from input ports. Such a router (e.g., C1) configuration in Table 3 behaves like a typical input buffered router at low traffic, while the secondary lanes are exploited at medium and high traffic. Another parameter denoted Depth relates to the lane connectivity: in a router of Depth D a packet can at most be routed on D lanes before leaving the router. For network-level performance exploration, we consider a 4x4mesh network of Roundabout routers. We use a packet length of 10 flits, with flit size of 32 data bits for our simulations. In our VHDL testbench, processing blocks attached to the routers local ports serve as both producers and consumers of packets. In our simulation model, each block keeps sending packets until a stable network latency is reached. Fig.9a depicts the performance of different router configurations for uniform traffic pattern. It shows that having a high-level of parallelism on primary lanes is rewarding, since it mitigates performance loss caused by temporarily blocked packets and avoids unnecessary contentions on the lanes. As displayed in Fig.9a, the zero-load packet latency is also improved for configurations with higher parallelism level. Router configurations with higher level of parallelism outperform those with less parallelism for similar or even lesser lane depth. Fig. 9b shows the corresponding area overhead for the router configurations. It is observed that the performance of the routers does not solely depend on the area, but on their topological parameters. In order to assess the scalability of Roundabout, we consider the same versions with additional buffers and compare the result to that of their baseline. The additional buffers are evenly distributed on the lanes as shown in Fig. 1 where the grey lines represent these buffers. Fig. 10a shows the performance of the routers with additional buffers for uniform traffic pattern. In the plot, zeroadditional-buffers represent the routers baseline configurations. The network saturation throughput is increased with additional buffers for Roundabout routers. The figure also shows the corresponding performance of the Hermes router baseline and with additional buffers. We consider a configuration with a total of 80 buffers for the Hermes baseline configuration, i.e., 16 slot buffer for each input port FIFO queue. Similarly to Roundabout, the Hermes router uses wormhole flow-control and XY-routing. It employs credit-based buffer management scheme. The baseline configurations for C1, C2, C3, C4, C5 have a total of 80, 94, 68, 102, 86, and 88 buffers respectively. We observe that Roundabout offers better scalability than Hermes router. Table 4: Comparison of Hermes and Roundabout Router Hermes Roundabout (C1) Area (mm2 ) 0.049 0.052 Power (mW) 3.6 Min Max 2.6 2.9 The Roundabout configuration C1 is selected for comparing with Hermes [12]. Both routers have a buffer count of 80 buffers for a fair comparison. Table 4 shows the area results for both routers. The Hermes router has an area-overhead that is 5.7% smaller when 7 compared to the Roundabout router. Roundabout additional area is due to its distributed design, requiring several mux and arbiters to control access to shared resources in the router. Roundabout in turn consumes less power when compared to Hermes due to its input buffers [17]. We display the power results for both the shortest (i.e. min power) and the longest (i.e. max power) path on a given lane in Roundabout. The power results were obtained for a single link operation when the routers were operated at similar frequency. Fig. 10b shows the performance of both routers. We also display the performance of the 4 lanes Roundabout. As shown in the plot, the Hermes router outperforms the Roundabout C0 router, offering better network saturation throughput. As explained earlier, the Hermes router can always support up to five concurrent data-flows (for different source-destination pairs) regardless of the packet arrival time. Conversely, packets for different source-destination pair may compete for channel resources in the Roundabout C0 router. This observation motivated the 9 lanes versions of Roundabout. The 9 lanes Roundabout (i.e. C1_baseline) provides a performance improvement of over 60% compared to Hermes baseline (i.e. H_baseline). In Fig. 10b, Roundabout with additional buffers significantly outperform the Hermes routers (with additional buffers) by up 87% We simulated the baseline routers using transpose and hotspot traffic patterns. In transpose traffic pattern, each node communicates only with destination node with the upper and lower halves of its own address. In hotspot traffic pattern, all nodes communicate with a specific node i.e. the hotspot node. This creates a higher network contention when compared to the transpose and uniform traffic. It is observed that the network saturation throughput for Roundabout is improved by 61% and 88% for transpose and hotspot traffic respectively when compared to the Hermes router, which confirms the intrinsic ability of Roundabout to support specific traffic patterns by means of dynamically allocating buffer resources whenever needed. Table 5: Comparison of Roundabout (C1) and Rotary. (Sat: saturation) Router Rotary [1] C1 Head-flit-lat (cycles) 4 2 Sat. Flow (%) control 115 VCT/bubble 50 Wormhole Network Topology 2D-Torus Mesh Table 5 shows the comparison of Rotary [1] and Roundabout (C1). The head-flit travel time for Roundabout is twice less than that of Rotary since it uses shorter router pipeline requiring only two cycles. Table 5 also displays the network saturation for both routers for similar network size. The network saturation threshold for Rotary outperforms that of Roundabout. However, this performance improvement comes at a significantly high area and power cost associated with the use of large buffers. Besides, Rotary uses Torus network which provides shorter paths between network nodes. Packets in the Rotary router are allowed to make multiple turns before using any available output port. This could lead to significant dynamic power consumption, caused by an increase in router switching activities, and could also incur a significant increase in packet latency especially under high loads. Conversely, packets in Roundabout can adaptively switch lanes to free up the router resource for use by other packets. Table 6: Network saturation throughput (%) of virtual-channel (VC) based routers and Roundabout (C1) for uniform traffic pattern. Packet length RIVR [4] 4 70% 8 54% 12 48% 16 45% IVR-SC [14] 71% 56% 49% 46% FOVR-LS [3] 61% 53% 51% 47% C1 57% 53% 50% 47% Table 6 shows the performance comparison of Roundabout with state of the art virtual channel (VC) based routers for similar mesh network topology and buffer count. The BIVR [4] router represents typical VC routers with 5 pipeline stages, while the IVR-SC [14] and FOVR-LS [3] are single cycle routers requiring only one clock cycle for a single flit to travel-through the router. In general, the network no-load latency for FOVR-LS is lower than that of Roundabout for equal packet length, while the network no-load latency for IVR-SC is only marginally lower than that of Roundabout. As shown in Table 6, the network-saturation threshold for Roundabout is highly competitive to that of the VC routers. We expect Roundabout to provide improved performance for non-uniform traffic due to its shared-buffer and dynamic resource allocation features. Figure 11: Power consumption of Roundabout C1 Fig. 11 shows the power consumption of Roundabout C1 under several traffic scenarios sorted by aggregated throughput. The power results were obtained for varying router path and throughput. Rerouting denotes that path taken by a packet is not minimal due to other packets utilizing shortest path resources. Similar to the observation made for configuration C0 (i.e., 4 lanes router), the power consumption depends on the path taken in the router. Therefore, less power is consumed for similar packet throughputs when shorter paths are utilized in the router. Note rerouting may arise at any aggregated throughput without resulting in a contention. 6 CONCLUSION AND PERSPECTIVES We have presented a synchronous and elastic buffer based implementation of Roundabout [7], a router architecture that allows multiple input ports to share buffer resources for energy efficiency 8 and performance gains. Roundabout dynamically allocates buffer resources depending on network congestion. Thus, dynamic power consumed by buffers is kept relatively low especially at low loads. The router is implemented using 45nm CMOS process technology and compared against the Hermes router, a typical input buffered router. Roundabout offers performance improvement of 61% over the Hermes input buffered router for uniform/random traffic pattern and up to 88% for non-uniform traffic pattern where dynamic buffer resource allocation plays a key-role. Thus, the dynamic resource allocation feature gives Roundabout an edge over the Hermes router since the buffering resources can be dynamically allocated to meet high traffic demands. In terms of power, it consumes 24% less than the Hermes router. As illustrated in the evaluation, Roundabout provides a highly parametric architecture that can produce different router configurations with varying topological trade-offs for performance gains without sacrificing area. Future work will aim at further exploring the router architecture for performance improvement. In addition, estimating the router performance when considering real-world application workloads is an important perspective to the current study. We also plan to provide support for virtual-channels. "
SMART - A Scalable Mapping And Routing Technique for Power-Gating in NoC Routers.,"Reducing the size of the technology increases leakage power in Network-on-Chip (NoC) routers drastically. Power-gating, particularly in NoC routers, is one of the most efficient approaches for alleviating the leakage power. Although applying power-gating techniques alleviates NoC power consumption due to high proportion of idleness in NoC routers, since the timing behavior of packets is irregular, even in low injection rates, performance overhead in power-gated routers is significant. In this paper, we present SMART, a Scalable Mapping And Routing Technique, with virtually no area overhead on the network. It improves the irregularity of the timing behavior of packets in order to mitigate leakage power and lighten the imposed performance overhead. SMART employs a special deterministic routing algorithm, which reduces number of packets encounter power-gated routers. It establishes a dedicated path between each source-destination pair to maximize using powered-on routers, which roughly halves the number of wake-ups. Additionally, in order to maximize the efficiency of the proposed routing algorithm, SMART provides an exclusive mapping for each communication task graph. In proposed mapping, all cores should be arranged with a special layout suited for the proposed routing, which helps us to minimize the number of hops. Furthermore, we modify the predictor of conventional power-gating technique to reduce energy overhead of inconsistent wake-ups. Experimental results on SPLASH-2 benchmarks indicate that the proposed technique can save 21.9% of static power, and reduce the latency overhead by 42.9% compared with the conventional power-gating technique.","SMART: A Scalable Mapping And Routing Technique for Power-Gating in NoC Routers Hossein Farrokhbakht Department of Computer Engineering Sharif University of Technology Tehran, Iran hfarrokhbakht@ce.sharif.edu Hadi Mardani Kamali Department of Computer Engineering Sharif University of Technology Tehran, Iran mardani@ce.sharif.edu Shaahin Hessabi Department of Computer Engineering Sharif University of Technology Tehran, Iran hessabi@sharif.edu ABSTRACT Reducing the size of the technology increases leakage power in Network-on-Chip (NoC) routers drastically. Power-gating, particularly in NoC routers, is one of the most efficient approaches for alleviating the leakage power. Although applying power-gating techniques alleviates NoC power consumption due to high proportion of idleness in NoC routers, since the timing behavior of packets is irregular, even in low injection rates, performance overhead in power-gated routers is significant. In this paper, we present SMART, a Scalable Mapping And Routing Technique, with virtually no area overhead on the network. It improves the irregularity of the timing behavior of packets in order to mitigate leakage power and lighten the imposed performance overhead. SMART employs a special deterministic routing algorithm, which reduces number of packets encounter power-gated routers. It establishes a dedicated path between each source-destination pair to maximize using powered-on routers, which roughly halves the number of wake-ups. Additionally, in order to maximize the efficiency of the proposed routing algorithm, SMART provides an exclusive mapping for each communication task graph. In proposed mapping, all cores should be arranged with a special layout suited for the proposed routing, which helps us to minimize the number of hops. Furthermore, we modify the predictor of conventional power-gating technique to reduce energy overhead of inconsistent wake-ups. Experimental results on SPLASH-2 benchmarks indicate that the proposed technique can save 21.9% of static power, and reduce the latency overhead by 42.9% compared with the conventional power-gating technique. KEYWORDS Power-gating, Network-on-Chip, Mapping, Routing ACM Format: Hossein Farrokhbakht, Hadi Mardani Kamali, and Shaahin Hessabi. 2017. SMART: A Scalable Mapping And Routing Technique for Power-Gating in NoC Routers. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130231 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130231 1 INTRODUCTION Due to lack of scalability in bus-based interconnection networks in MPSoCs, Network-on-Chip (NoC) has been developed as a highthroughput, cost-effective, and especially scalable interconnection network, which is more suited for many-core systems with high traffic volume. By reducing the size of technology, power consumption in current NoC designs has become a big concern which should be considered meticulously. On-chip networks may contribute up to 35% of chip’s total power [1]. Also, shrinking the size of transistors down to nano-scale drastically increases the proportion of static power consumption. For instance, power analysis in [2] shows that the static power dissipation increases quickly, from 11.2% at 65nm to 33.6% at 32nm. There are other examples [3–5] which depicts the increasing rate of power consumption from a technology node to a smaller one. Furthermore, although on-chip routers have been designed for peak (i.e., close to saturation point) load, network utilization is relatively low in many-thread real applications [6]. As an illustration, our investigation shows that router utilization in SPLASH-2 benchmarks [7] is, on average, less than 20%. Therefore, high power consumption, as well as high probability of idleness in NoC routers, motivate us to focus on solutions for reducing static power in on-chip routers. Although there are several possible power saving techniques in circuit-level designs [8], power-gating technique is an efficient technique for reducing static power [9]. Despite the aforementioned advantages of power-gating technique, employing it in NoC routers encounters several challenges. First, all packets must wait for wake-up when they encounter a power-gated router, which significantly increases the average network latency. In addition, increasing the number of power-gated routers between all source-destination pairs, which is more probable in low injection rates, exacerbates the latency. Another nonnegligible drawback in power-gated NoC routers is power overhead of wake-ups. Each wake-up imposes a significant power to total power consumption. Accordingly, the idleness time interval should be large enough to compensate for the wake-up power overhead. Although network utilization is low in many-thread real applications, since the timing behavior of packets is irregular, there is no deterministic pattern for specifying large idle time intervals [10], [11]. Additionally, emerging many-core systems act as a disincentive to proposing power-gating techniques, in the sense that increasing the size of the networks increases hop counts between all source-destination pairs. Therefore, encountering several powergated routers makes an inevitable cumulative latency due to required wake-ups in routers. So, in order to propose an efficient and highperformance power-gating technique, we have to provide a technique to avoid several suspensions due to power-gated routers. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea H. Farrokhbakht et al. Several power-gating techniques have been proposed so far, which have to do with their attempts for reducing performance overhead imposed by power-gated NoC routers. For instance, employing an early wake-up technique, partially [12] or fully [13], decreases the slack time of the blocked packets behind the power-gated routers. Although all proposed techniques commonly demonstrate the efficiency of power-gating technique for on-chip networks [2], [4], [14], intermittent packet arrival is the main obstacle to provide an optimum power-gating technique. In this paper, we present a Scalable Mapping And Routing Technique (SMART) to reduce the power consumption of NoC routers. The idea behind the proposed technique is to increase the utilization rate of certain part of NoC to provide higher idleness periods of the remaining parts, which, consequently, increases the power-gating opportunity, and reduces the number of wake-ups. The proposed technique reduces irregularity of timing behavior of the packets by employing a special deterministic routing algorithm which establishes only one deterministic and dedicated path between each source-destination pair. In fact, we determine only one bidirectional path between any pairs in the network, in order to receive the packets from the path which has been dedicated for sending, and also send the packets from the very same path which has been dedicated for receiving. By doing so, each packet encounters around half of power-gated routers in its transmission compared to Dimension order Routing (DoR) algorithm (XY or YX). Furthermore, an exclusive mapping structure is implemented in SMART, which maximizes the efficiency of our proposed routing algorithm. The proposed mapping localizes the cores with more communication with a specific layout, which reforms the distribution of packets suited for the proposed routing algorithm. It significantly reduces power consumption, and alleviates the performance overhead imposed by power-gating technique. Additionally, we modify the conventional predictor due to changing the distribution of packets in the network, which decreases average packet latency significantly. Experimental results using SPLASH-2 benchmarks show that using our integrated routing and mapping mechanism improves latency overhead by 39.3% in comparison with conventional power-gating technique. In addition, amendment on the conventional predictor decreases average packet latency by 5.9%. Thanks to its simplicity and lightweight structure, SMART has only around 2.1% power overhead, which is virtually negligible. Overall, in comparison with conventional optimized power-gating techniques, SMART saves static power around 21.9%, decreases average packet latency by 42.9%, compared to the conventional power-gating. The rest of the paper is organized as follows. In Section II, more background on power-gating, also all prerequisites and challenges of this technique are considered which motivates devising a more desirable approach. Section III expounds the infrastructure of SMART and investigates all details and challenges in this approach. Section IV depicts our experimental results. Section V summarizes the related work. Finally, Section VI concludes the paper. 2 BACKGROUND, MOTIVATION, AND CHALLENGES Increasing the number of cores in MPSoCs, and consequently emerging many-core systems is the main factor in improving performance, especially for communication-intensive applications. Meanwhile, NoC, as a new on-chip interconnection infrastructure, is designed to support high traffic volume while avoiding imposed performance bottleneck. On the other hand, scaling down the technology and decreasing supply voltage increase the portion of static power in chip’s total power. While NoC may contribute up to 35% of chip’s total power [1], shrinking the technology size from 45nm to 22nm increases the contribution of leakage power from 43% to 63% [5]. Accordingly, static power of on-chip networks has become a dominant factor in chip’s total power. Therefore, implementing powerefficient and high-performance techniques for mitigating the portion of leakage power, especially in on-chip networks, is inevitable. As its name implies, power-gating technique is accomplished by disconnecting unutilized (idle) components of architecture from the corresponding power supply. Therefore, it is obvious that the ratio of idleness in components plays fundamental role in efficiency of this technique. Although NoC is designed for handling peak-load traffic, our analysis on SPLASH-2 benchmarks on an 8 × 8 mesh network shows that, on average, routers are idle for 79.9% of time during simulation. Therefore, concentrating on the efficiency of power-gating technique is promising enough to propose a novel technique which can significantly reduce leakage power. However, the proposed power-gating technique must minimize the performance overhead imposed by it. All power-gating techniques in NoC routers are accomplished in two different granularity levels: Coarse-Grained and Fine-Grained. 2.1 Coarse-Grained Power-Gating In some schemes, implemented in [13] and [15], the power control, which is accomplished by a power switch (e.g. high threshold transistor), is located between the core and its power/ground IO cells. In fact, there is some power/ground ring cells around all circuits, which are responsible for supplying power for the circuits. Therefore, the power supply connected to circuit can be controlled instantaneously. In order to apply coarse-grained power-gating method for NoC routers, all flit queues, buffers, output registers, and crossbars should be cut-off. So, by using a simple monitoring controller, the idleness of all modules should be checked to find the suitable period of time for power-gating the router. In addition, all adjacent routers must be informed by the power-gated router to avoid sending packets. Since all queues and buffers are cut-off in coarse-grained power-gating, the received packets from neighbors will be lost in the network. Therefore, establishing a handshaking mechanism between routers is necessary in order to avoid lossy transmissions. Handshaking is bidirectional between power-gated routers and neighbors. Power-gated routers must inform all neighbors by asserting a notifier signal, and adjacent routers tag all corresponding output ports as disabled ports. On the other hand, all adjacent routers need triggering signals to inform the power-gated routers when it is necessary to make them available (power-on). Fig. 1 is a simple illustration of a power-gating handshaking mechanism between a power-gated router and its neighbor. Power-gate (PG) signal is responsible for notifying neighbors of power-gated routers, and the neighbors can trigger them by using wake-up (WU) signal. SMART: A Scalable Mapping And Routing Technique for ... NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 1. Power Gating of On-Chip Routers. 2.2 Fine-Grained Power-Gating In some other schemes, such as [16] and [17], power-gating is carried out on some network sub-modules, such as buffers and Virtual Channels. For instance, power-gating in [16] is applied on different router sub-modules, like VC buffers, output latches, and routing multiplexers. Unlike coarse-grained, fine-grained power-gating encounters complex wake-up mechanism. In fact, since power-gating is applied on different modules, different scenarios should be considered for wake-ups. For example, although there are several eligible sub-modules for applying power-gating in [16], different and complicated scenarios for waking up limits the number of sub-modules that can be power-gated. 2.3 Cumulative Wake-Up Latency Different conditions may be occurred in a power-gated router: (1) Injection Packet, in which the power-gated router is going to inject a packet to the network, (2) Ejection Packet, in which the destination of the received packet is the current power-gated router, and (3) Intermediate Packet, where the received packet should be transmitted to one of the neighbors. All aforementioned conditions necessitate the power-gated router to wake-up. A typical wake-up needs around 8 cycles in a 2 GHz architecture [12]. Furthermore, a packet might encounter several power-gated routers in transmissions, which necessitate all power-gated routers to wake-up. In this case a considerable latency may be inflicted upon the network, which is called Cumulative Wake-Up Latency. In addition, the issue is aggravated when the network utilization is low. That is because in this case, the majority of the routers has been power gated due to low network utilization and therefore a packet encounters more power-gated routers along its path to destination. This condition motivates us to propose a technique in order to reduce the number of packets encounter power-gated routers, and also increase the utilization of the current power-on routers, which decreases cumulative wake-up latency dramatically. 2.4 Break-Even Time For each wake-up event, significant power will be imposed to total power consumption. It is obvious that the quantity of cycles where router is power-gated should be large enough against to compensate Figure 2. Packet Distribution Encountering a Power-Gated Router in SPLASH-2 traces [7]. for the wake-up power overhead. So, the minimum number of consecutive cycles, in which the router is power-gated, which is called Break-Even Time (BET), must be large enough to compensate power overhead of each wake-up. Investigation shows that the minimum suitable BET is 10 cycles [12]. In other words, power overhead of each wake-up is more than power compensation of power-gating technique for BET less than 10. As was mentioned before, we observed that routers are idle for 79.9% of time, but 75.8% of these idle periods violate BET requirement. Therefore, a predictor is necessary in order to find suitable time intervals for power-gating, which pass BET prerequisite. 2.5 Key Observation Our investigation on types of the packets in SPLASH-2 benchmarks illustrates that 73.5% of all packets on average, which encounter a power-gated router in a mesh network, are intermediate packets, i.e. the packets whose sources and destinations are not the current nodes, and the remaining proportion is divided between injection packets and ejection packets. Fig. 2 depicts the distribution of the packets encounter a power-gated router in SPLASH-2 benchmarks. Note that this proportionately is valid for all minimal-based routing algorithm. On the other hand, encountering power-gated routers is a significant contributor for power consumption in on-chip routers. Therefore, it is obvious that if we could employ an specific deterministic routing algorithm, which decreases the number of encountering power-gated routers, we can dramatically alleviate power-consumption in NoC routers. So, we engage a deterministic routing algorithm which can re-use the receiving path for sending in each source-destination pair. it can significantly reduce the number of encountered power-gated routers. Additionally, this routing algorithm can increase the utilization of some parts of the network in comparison with other parts, if we employ an special mapping for each communication task graph (CTG). It helps us to achieve a network with less irregularity. Having a network with less irregularity in the timing behavior of the packets can provide more large idle time intervals, and consequently SMART can achieve more gain compared to conventional power-gating technique. Also, if we could propose an exclusive mapping for each CTG, with an special and distinctive layout, which is compatible with the proposed routing algorithm, we can minimize the number of hops for the packets, leads us to attain more efficient power-gating technique. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea H. Farrokhbakht et al. 3 PROPOSED METHOD: SMART As it was mentioned in Section II. E, high proportion of packets in real application benchmarks (73.5%) which encounters power-gated routers, is intermediate packets that must traverses the current router. Although, all routers in a network are powered-on in the beginning of the packet traversal, most of them will be power-gated in lower injection rates. When a packet encounters a power-gated router, the router must be powered-on. Accordingly, if we decrease the number of packets encounter power-gated routers, we can provide a more efficient power-gating technique by (1) reducing the number of powered-on routers for each packet traversal, which consequently mitigates power consumption, and (2) decreasing the cumulative latency due to wake-ups by decreasing cases in which a packet encounters a power-gated router, which reduces the imposed performance overhead in a conventional power-gating technique. As its name implies, we employ an integrated mapping and routing mechanism in SMART as well as a reformed predictor architecture, which helps not only reducing power consumption in NoC routers, but also compensating for the imposed performance overhead. The proposed routing algorithm routes the packets with a simple, non-overhead, and deterministic structure, which slightly decreases the number of packets encounter power-gated routers. The proposed exclusive mapping mechanism provides an arrangement suited for the proposed routing algorithm, which is able to maximize the efficiency of our approach. Additionally, the integrated routing and mapping mechanism leads us to change the predictor logic to achieve more throughput with less power consumption. 3.1 XYX: Pseudo Circuit-Switching Routing Algorithm Not only most of the proposed power-gating implementations [13, 15–17], but also most of the on-chip implementations have used mesh topology with XY routing in their architectures. XY routing uses different paths for each source-destination pair, while both rows and columns are different. Fig. 3a illustrates different routes for some source-destination pairs. It is obvious that if source and destination are located in a row or in a column the routes are same. Our proposed routing algorithm, XYX, chooses a common path for both direction in each source-destination pair. In other words, if we use XY routing for packet traversal from node1 to node2, we will choose YX routing for traversing from node2 to node1, and vice versa. Fig. 3b demonstrates the common routes for packet traversal based on XYX. As an instance, according to XYX, which is similar to the allocating a route in circuit-switching structure, the route for packet traversal from (4,5) to (6,7) is XY, and the route for sending packets from (6,7) to (4,5) is YX. By using XYX, we re-use the powered-on routers in the mentioned route for both directions, which increases the utilization of the routers located in the chosen path. Also, as it can be seen, if two nodes are located in a row or in a column, the routes in XY and XYX is similar. Using XYX increases the utilization of some paths more efficiently, especially when traffic is high between nodes. Furthermore, since there is less uniformity in distribution of packet traversal in real application benchmarks, our routing algorithm can provide more efficiency in utilization of the network resources. Although XYX orients the packets and changes the distribution of them in the network, (a) (b) Figure 3. Utilized routers in (a) XY routing (b) XYX. the exclusive mapping perfects the results obtained from SMART. In fact, XYX alongside the mapping mechanism makes some routes more utilized in regions with high traffic, and provides some regions with more idleness. Accordingly, since we increase the utilization of some routers in some specific paths, the number of packets encounter power-gated routers will be decreased, and consequently, we can reduce the accumulated latency imposed by wake-ups. Also, using XYX increases the idleness of some routers in the network, which decreases power consumption on average. XYX is a deterministic routing algorithm whose structure automatically eliminates some turns, which proves that there is no deadlock in this algorithm. Since we use YX in a direction that XY has been used in opposite direction, two turns in each cycle (clockwise and counter-clockwise) will be eliminated. Fig. 4a illustrates all possible routes in XYX in a 3 × 3 mesh network, which proves that there is no possible cycle in XYX routing algorithm, and consequently no additional virtual channel (VC) is needed to handle probable deadlock. Fig. 4b depicts all possible turns in XYX routing algorithm. 3.2 Pseudo-Spiral: Exclusive Mapping for XYX Routing Although XYX can avoid some wake-ups in some power-gated routers by re-using the routes with powered-on routers, we can increase more power-gated routers in the network by using an exclusive mapping mechanism. According to XYX, if we engage XY for sending packets from node1 to node2, we will use YX for sending in opposite direction, i.e. form node2 to node1. So, overall pattern of the routing packets is L-shape, which is illustrated in Fig. 5a. Using mapping mechanism localizes the cores with more communication. Localizing the cores with more communication dramatically decrease number of hops between each source-destination pair. Consequently, decreasing the number of hops between each source-destination reduces the number of intermediate routers. So, it can reduce probability of encountering power-gated routers for each packet. So, engaging a mapping mechanism, by means of profiling techniques (offline profiling techniques), can considerably improve the efficiency of SMART. In order to increase the efficiency of the exclusive mapping mechanism alongside XYX, we must engage an exclusive L-shape-based SMART: A Scalable Mapping And Routing Technique for ... NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Table 1: Key Parameters and gem5 Configurations for SPLASH-2 Traces (a) (b) Figure 4. Automatic Deadlock Avoidance in XYX Routing Algorithm. (a) a 3 × 3 Sample Mesh Network with XYX Routing Algorithm (b) Automatic Prohibited Turns in XYX which Guarantees Deadlock Avoidance. (a) (b) Figure 5. Reciprocal Relationship between Our Proposed Routing (XYX) and Mapping (Pseudo-Spiral) in a 8 × 8 network (a) L-shape structure in XYX (b) L-shape-based mapping (Pseudo-Spiral). mapping mechanism suited for XYX. Fig. 5b illustrates overall layout of the mapping, which has a pseudo-spiral structure. Using this pseudo-spiral mapping structure orchestrates the mapping and the routing integration, which improves the efficiency of XYX as well as decreasing the number of hops for each packet traversal. In order to provide an efficient mapping mechanism, we need offline profiling on CTG. Using offline profiling helps us to make an extrapolation from the traffic volume between nodes. Having an offline profiling form each CTG allows us to arrange the nodes with more communication in a pseudo-spiral structure. In order to show the efficiency of the profiling, we investigate application-driven workloads for NoCs, i.e. SPLASH-2, which shows that using this pseudo-spiral mapping structure alongside XYX routing algorithm can considerably improve the efficiency of SMART. 3.3 SMART Predictor Predictor of conventional power-gating techniques looks for 4 consecutive idle cycles in a router to infer that the router will be idle in the 10 next cycles. Due to our XYX routing algorithm as well as pseudo-spiral mapping mechanism, the irregularity of the network is reformed. Consequently, we change the overall infrastructure of predictor module. As it can be seen in Fig. 5b, it is obvious that SMART improves the irregularity of the network due to its L-shape structure. In fact, the traffic is more dense in upper triangle in mesh, and the idleness of nodes located in lower triangle is more probable. Accordingly, SMART predictor waits for 10 consecutive idle cycles in the first four L-shapes to predict that the router will be idle in the 10 next cycles. Similar to conventional power-gating techniques, the number of waiting cycles for other remaining L-shapes is 4. Reforming the predictor considerably decreases useless wake-ups, and consequently reduces power consumption in SMART. 4 EXPERIMENTAL RESULTS AND ANALYSIS In order to evaluate SMART, we simulate all scenarios by means of gem5 [18], alongside with Ruby memory model, and integrated modified BOOKSIM [19]. We accomplished our evaluation with 9 SPLASH-2 traces using gem5 in syscall emulation mode. Additionally, in order to model leakage and dynamic power, we integrated DSENT [3] with BOOKSIM for 45nm process. All key parameters and gem5 configurations are demonstrated in Table 1. Note that, the traces were gathered from 10 million cycles after fullsystem initialization (initialization of threads and caches). All traffic weights were obtained by evaluating the number of flits per each source-destination pair. In order to avoid extreme large latency due to network saturation, 1,000 cycles were capped during simulation. We compare the results obtained from SMART with different implemented designs: (1) No-PG as a baseline architecture without any power-gating technique. (2) Conv-PG which is the simplest type of power-gating, in which the router must be powered-off immediately after idle detection. (3) ConvOpt-PG waits 4 consecutive cycles to detect idleness, and uses early wake-up to partially hide wake-up latency [12]. (4) Power Punch Signal as a high-efficient power-gating technique implemented by Chen et al. [13]. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea H. Farrokhbakht et al. In the rest of this section, firstly we present the impact of SMART on power and energy. After that we analyze the impact of SMART on performance. Then, we evaluate the effectiveness of SMART predictor compared to conventional predictor. 4.1 Impact on Power and Energy Fig. 6 illustrated normalized (to No-PG) values of power consumption in all aforementioned techniques as well as SMART. As it can be seen the total power consumption in each router consists of dynamic power, leakage power, and overhead of power gating (wake-ups overhead and power-gating controller overhead). SMART considerably reduces number of packets encounter power-gated routers, and decreases the irregularity of the packet traversal in the network, whereby SMART alleviates 41.3% of static power consumption in comparison with No-PG, while Power Punch and ConvOpt-PG can save 26.2% and 25.4%, respectively. Note that overhead of powergating should be accounted as a part of static power. In fact, in order to provide a fair comparison between different power-gating techniques we should consider sum of power-gating overhead and static power as total static power. It is obvious that SMART advantage against Power Punch returns to close to zero overhead of the proposed technique. Two main contributors of overhead power is the controller of power-gating technique and wake-ups. Since we have accomplished the proposed technique via an integrated routing and mapping mechanism, the overhead of controller is almost zero, and all overhead power returns to wake-ups, which is negligible in SMART. Fig. 7 depicts total static energy consumption in all mentioned power-gating techniques. As it can be seen, SMART is able to save 40.7% of static energy compared to No-PG, while ConvOpt-PG and Power Punch save 22% and 25.3%, respectively. As it was mentioned before, SMART reduces the irregularity of timing behavior of packets, which helps increasing the utilization of resources in NoC. Accordingly, we achieved considerable improvement in total energy consumption in SMART against other schemes. 4.2 Impact on Performance SMART not only provides an efficient power-gating technique, which significantly alleviates power consumption compared to other schemes, but also has negligible packet latency compared to No-PG thanks to decreasing the irregularity in timing behavior of packets. Fig. 8 demonstrates the average packet latency in all power-gating techniques. Note that No-PG refers to baseline architecture without applying any power-gating technique. So, it is obvious that since all routers are powered-on and it has no latency for wake-up, No-PG must have the minimum packet latency compared to other schemes. In contrast with No-PG, Conv-PG is the simplest power-gating technique which suffers from lots of wake-ups. So, it has maximum overhead (123.1% against No-PG) in average packet latency. It can be accounted as an evidence for the irregularity in timing behavior of packets in real application benchmarks, which is the main obstacle for the efficiency of power-gating schemes. By using the integrated mapping and routing mechanism as well as the amended predictor, SMART has only 27.3% overhead in average packet latency. Although Power Punch provides an efficient power-gating technique with only 13.8% overhead in packet latency, the simplicity of SMART helps us to achieve 20.4% improvement in power consumption compared to Power Punch. Note that Power Punch has tried to eliminate wake-up latency by using a signaling mechanism. The efficiency of its signaling mechanism almost eliminates the performance overhead, and consequently minimizes the packet latency compared to other competitors. However, Power Punch has just focused on the packet latency, and it has no solution for improving power reduction. Applying power-gating technique imposes time overhead on total execution time. As it can be seen in Fig. 9, SMART behaves similarly for different benchmark traces. Also, Unlike Conv-PG, SMART has negligible sensitivity in some benchmark traces, such as reytrace, fft, and barnes. According to Fig. 9, the Conv-PG, ConvOpt-PG, Power Punch, and SMART impose 7.9%, 4%, 0.7%, and 1.8% execution time overhead, respectively. Reducing number of packets encounter power-gated routers is the main target of the integrated mapping and routing mechanism, which provides an efficient power-gating technique with considerable performance compared to Conv-PG. Using same path between each source-destination pair for packet traversal, which is referred in Fig. 3, roughly halves the number of packets encounter power-gated routers. Fig. 10 illustrates the number of encountering power-gated routers for all power-gating schemes. As it can be seen, compared to Conv-PG, SMART decreases 60.4% number of encountering powergated routers, which is the main factor for significant improvement in average packet latency. It should be noted that, although SMART encounters less power-gated routers on average compared to Power Punch, since the required number of cycles for waking up is fewer in Power Punch rather than SMART, it imposes less overhead on average packet latency compared to SMART. 4.3 Impact of SMART Predictor We implement an integrated mapping and routing mechanism in SMART, which helps us to decrease the irregularity in timing behavior of packets. As it was mentioned before, Fig. 5 demonstrates that the proposed integrated mechanism congests the packets in the first L-shapes. So, it is obvious that reforming the timing behavior of packets may have contradiction with conventional predictor. As it was mentioned earlier, we changed the required number of idle cycles from 4 to 10 for the first four L-shapes to decrease BET violation due to the proposed integrated routing and mapping mechanism. Fig. 11a depicts the BET violation in SPLASH-2 benchmark for both conventional and SMART predictor. As it can be seen, SMART predictor decrease 47.3% of BET violation compared to conventional predictor. Reducing the percentage of BET violation provides more suitable intervals for power-gating, and consequently it improves the average packet latency significantly. Additionally, Fig. 11b shows that using SMART predictor mitigates 74.6% of the energy overhead imposed by conventional predictor on the integrated routing and mapping mechanism. 5 RELATED WORK Several coarse-grained power-gating techniques has been successfully implemented so far. Two extra bypassing injection and ejection ports have been added in NoRD [4] to avoid wake-ups for injection, SMART: A Scalable Mapping And Routing Technique for ... NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 6. Power Breakdown in Routers. Figure 7. Static Energy Consumption. Figure 9. Normalized Execution Time. Figure 8. Average Packet Latency. Figure 10. Normalized Encountered Power-gated Routers. ejection, and forwarding packets in power-gated routers. However, detouring the packets to neighbors in power-gated routers not only increases the packet latency, but also it is not scalable for large networks. Router Parking [2] is able to power-gate a set of routers which are connected to a powered-off core. Also, the mechanism of power-gating is traffic-aware, however, its routing algorithm is complicated, which increases the overhead of controller. A controlling mechanism has been accomplished in Power Punch [13] to decrease the required number of cycles for wake-up by informing the routers before packet arrival. Although Power Punch provides a considerable performance among power-gating schemes, no strategy for the irregularity in timing behavior of packets is still a big concern in this technique. A simple mechanism for bypassing straight packets has been implemented in TooT [15]. High proportion of straight packets in the network enables TooT to provide an efficient and lightweight mechanism for power-gating technique, however, the irregularity in timing behavior of the packets is a major concern in average packet latency. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea H. Farrokhbakht et al. by 27.3% and 49.9%, respectively, compared to conventional powergating technique. "
Energy and Area Efficient Near Field Inductive Coupling - A Case Study on 3D NoC.,"Near Field Inductive Coupling (NFIC) enables design of energy efficient and robust three-dimensional (3D) manycore systems. The associated design challenges and the trade-offs of the NFIC-based vertical links depend on achievable data-rates, energy and area overheads. In this work, we propose a holistic design flow that explores optimum energy and area efficient NFIC-link design as a communication backbone in a 3D manycore chip. Moreover, the design framework employs statistical link analysis to select optimum NFIC link configuration. The proposed NFIC-link design is significantly more efficient in terms of energy efficiency and area overhead compared to state-of-the-art counterpart. Energy efficiency and resiliency of NFIC-links are exploited in the context of a 3D NoC design. We demonstrate that overall reliability of the NFIC-enabled 3D NoC is significantly better compared to a conventional stand-alone TSV-based architecture.","Energy and Area Efficient Near Field Inductive Coupling: A Case  Study on 3D NoC   Srinivasan Gopal, Sourav Das, Deukhyoun Heo, Partha Pratim Pande  {sgopal, sdas, dheo, pande}@eecs.wsu.edu  School of EECS, Washington State University, Pullman, WA-99163  ABSTRACT  Near Field Inductive Coupling (NFIC) enables design of energy  efficient and robust three-dimensional (3D) manycore systems.  The associated design challenges and the trade-offs of the NFICbased vertical links depend on achievable data-rates, energy and  area overheads. In this work, we propose a holistic design flow that  explores optimum energy and area efficient NFIC-link design as a  communication backbone in a 3D manycore chip. Moreover, the  design framework employs statistical link analysis to select  optimum NFIC link configuration. The proposed NFIC-link design  is significantly more efficient in terms of energy efficiency and  area overhead compared to state-of-the-art counterpart. Energy  efficiency and resiliency of NFIC-links are exploited in the context  of a 3D NoC design. We demonstrate that overall reliability of the  NFIC-enabled 3D NoC is significantly better compared to a  conventional stand-alone TSV-based architecture.  CCS CONCEPTS  • Hardware →  Integrated circuits →  3D integrated circuits  •  Networks →  Network types →  Network on chip  KEYWORDS  NFIC; 3D NoC; 3D IC; Proximity Communication; Geometric  Programming; Optimization; Low power; Robust NoC design.  ACM format:  S. Gopal, S. Das, D. Heo, P. P. Pande. 2017. Energy and Area  Efficient Near Field Inductive Coupling: A Case Study on 3D NoC.  In Proc. of ACM NOCS, Seoul, October 2017, 8 pages.  hps://doi.org/10.1145/3130218.3130224   1 INTRODUCTION  3D integration, a breakthrough technology to achieve “More  Moore and More Than Moore,” provides numerous benefits such  as better performance, lower power consumption, and higher  bandwidth by vertical interconnects and 3D stacking. These  vertical interconnects enable the design of high performance 3D  Network-on-Chip (NoC) as a communication backbone for  Permission to make digital or hard copies of all or part of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full  citation on the first page. Copyrights for components of this work owned by others  than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.  NOCS  '17, October  19–20,  2017,  Seoul,  Republic  of  Korea   © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-49840/17/10…$15.00 https://doi.org/10.1145/3130218.3130224   massive manycore platforms. 3D integration provides higher  device density, wide inter-die bandwidth, and heterogeneous  integration [1] [2]. Higher device density continues “More Moore”  and enables more functionality and/or smaller form factor. The  heterogeneous integration of multiple planar dies fabricated with  different technologies achieves a complete System-on-Chip (SoC)  composed of digital logic, DRAM, analog and RF circuits, and  sensors [2][3].   On the other hand, NoC is an enabling solution for integrating  large numbers of embedded cores in a single die. 3D NoC  architectures combine the benefits of the two emerging  technologies, namely, 3D integrated circuits (3D IC) and NoC to  offer an unprecedented performance gain even beyond Moore’s  law regime. With an additional degree of freedom introduced by  the vertical integration, architectures that were impossible or  prohibitive due to wiring constraints in planar ICs are now  feasible, and many 3D implementations can outperform their twodimensional (2D) counterparts [2]. The anticipated performance  gain of 3D NoC-enabled manycore chips will be compromised due  to the possible failures of through-silicon vias (TSVs) that are  predominantly used as vertical interconnects in 3D ICs [3][4][5].   Alternatively, NFIC link is another 3D integration technique  that offers attractive benefits over conventional TSVs [6]. High  speed NFIC link comprises of transmitter (Tx), NFIC channel and  receiver (Rx). TSVs suffer from bandwidth limitations due to their  inherent low-pass frequency response and exhibit large bandwidth  reduction with misalignment errors [2]. In contrast, NFIC links can  afford higher bandwidth compared to TSVs.  Furthermore, NFIC  links are highly resilient to misalignment. Current state-of-the-art  research shows that NFIC links outperform TSV implementations  in terms of energy-per-bit metric, and can be expected to scale far  more competitively with more advanced technology nodes [7].  These advantages can be achieved by avoiding  increased  fabrication cost associated with the TSV-based 3D integration.  This work introduces a holistic design methodology for NFIC  links by considering the necessary speed-power-area trade-offs.  We develop a speed-power-area co-optimization engine (SPACE)  to find optimum NFIC link configuration at a given data rate and  communication range. SPACE  is developed by combining  statistical link analysis and convex optimization for the first time  in 3D NFIC design space. We demonstrate that an efficient NFIC  link design is accomplished by employing receiver equalization  architecture with reduced link power and inductor area.   To exploit the advantages of the NFIC-based physical layer, we  undertake a detailed performance evaluation of a 3D NoC  incorporating NFIC links. We demonstrate that by incorporating  NFIC links, it is possible to design robust and energy-efficient 3D  NOCS’17, October 2017, Seoul, Korea  S. Gopal et al.  Figure 1: Overall work-flow diagram for this work and the physical layer and NoC architecture co-design approach.  NoC architectures. The overall achievable performance of the NoC  architecture degrades marginally even with 50% misalignment  between the transmitter and receiver  inductors. We also  demonstrate that by replacing heavily-utilized TSV-based vertical  links (VLs) with NFIC, the overall reliability of the system is  enhanced. Fig. 1 shows the overall work-flow diagram of physical  layer-NoC architecture co-design techniques.  2 RELATED WORKS  TSV is the most mature and popular technology enabling 3D  integration. However, the data rate per unit TSV channel is limited,  and parallel TSVs are combined to obtain a high aggregate data  rate [10]. Parallel TSV bundle introduces area overhead due to  pitch and keep-out-zone (KOZ) requirements for obtaining high  yield [11]. Often, redundant TSVs are employed to enhance  robustness, contributing to further increase in overall chip area  [11]. TSVs require mechanical alignment with high precision  involving  special  fabrication process,  resulting  in high  manufacturing cost [7]. TSV improves data rate with technology  scaling. For instance, IBM has achieved 6Gbps data rate using 45  nm CMOS fully-depleted silicon-on-insulator process with high  interconnect density [12].  In recent technologies, proximity communications based on  NFIC and capacitive coupling channels achieve excellent bit error  rate (BER) at high speed and low power [13][14]. Non-return-tozero (NRZ) data can be directly communicated using contactless  channels formed by capacitive coupling or NFIC without carrier  modulation [16]. Capacitive coupling is employed for short range  communications, restricting them to face-to-face 3D integration  applications [14]. On the contrary, long range multi-layer interchip communications can be realized using an NFIC channel.   From the NoC design perspective, simple and regular 3D meshbased architectures have been investigated in considerable details  [2][15]. To take advantage of the vertical short distance inherent  in 3D integration, several TSV-bus-based NoC architectures have  also been proposed [15][17]. However, it is well known that the  main bottleneck of mesh-based networks arises from multi-hop  communication. Recently, design of irregular 3D NoCs has been  proposed that incorporate random connectivity in horizontal  planes, while the vertical links (VLs) are designed with NFICs [6].  In this context, a small-world network-enabled 3D NoC  architecture (3D SWNoC) was proposed in [5], which outperforms  2  other 3D NoCs. In our work, we consider this 3D SWNoC  architecture as a testbed to analyze the performance of the  proposed energy efficient NFIC-based VLs. It is to be noted that the  NFIC-based physical layer design methodologies presented in this  work are independent of NoC architectures, and hence, can be  explored to design any other 3D NoC as well.  3 3D NFIC LINK OPTIMIZATION   In this section, we present the design methodology for NFIC  links. To characterize an NFIC based contactless data transmission,  link performance is analyzed based on the channel transfer  function and the associated design trade-offs.  3.1 Analysis of NFIC Link Trade-offs  NFIC channel gain is proportional to the mutual coupling  inductance (M=k√LTLR ), as shown in Fig. 2(a) [14]. It depends on  the geometric mean of transmitter and receiver inductors with  coupling coefficient (k) as the proportionality constant. Transmit  current (IT) induces voltage (VR) in the receiver inductor through  mutual coupling. Received signal strength depends on the amount  of coupled electromagnetic flux between the pair of inductors. Fig.  2(a) shows the high frequency NFIC channel model with parasitic  resistors and capacitors. The transmitter inductor (LT) parasitic  resistor-capacitor pair is given by (RT, CT), while the receiver  inductor (LR) parasitic pair is given by (RR, CR).   The challenges of implementing an energy-efficient NFIC link  are governed by the intertwined trade-offs between channel gain,  power, data rate, area and communication range. This interdependency among various parameters is shown in Fig. 2(a) and is  summarized below:  • An increase in transmit current (IT) could boost channel gain  and reduce BER, resulting in increased power consumption.  • An  increase  in receiver pre-amplifier (pre-amp) gainbandwidth could enhance sensitivity to recover signal from  noise for improved BER, adding to link power overhead.  • Channel gain is directly proportional to inductor diameter  and inversely proportional to communication range. To  extend the communication range, an increase in the inductor  diameter translates to an increased link area.  The adverse effect of increasing inductor diameter for high  data rates is inter symbol interference (ISI). The maximum  •        Energy and Area Efficient Near Field Inductive Coupling: A Case Study on 3D NoC  NOCS’17, October 2017, Seoul, S. Korea  Figure 2 (a) High frequency model of 3D NFIC channel with inter-twined multi-variable trade-offs (b) Two stage SPACE  PRBS Generator @ 32Gbps TX NFIC Channel S-paramter network RX DCD = 5% UI RJ= 5% UI ΔIT Swing W/o DFE W/o DFE BER = 10-12 BER = 10-11 BER = 10-10 time (ps) ) V ( e g a t l o V σn  = 3mV Voff = 3mV RJ= 5% UI DFE Coefficients W/ DFE W/ DFE BER = 10-12 BER = 10-11 BER = 10-10 time (ps) (a) ) V ( e g a t l o V (b) Figure 3: (a) Statistical link BER simulation set-up (b) BER  contours without (W/o) and with (W/) DFE  co-optimization engine (SPACE) for the NFIC link is shown in Fig.  2(b). In a 3D NFIC link, the communication channel design directly  impacts the energy, area and bandwidth of the link.   Implementation of an efficient 3D NFIC link is governed by the  generation of a large database of possible inductors using a 3D  electromagnetic (EM) field solver, which generates NFIC channel  model. Channel model is incorporated into SPICE simulations to  evaluate the complete link performance. Failing to meet link  specification in the above process calls for an expensive redesign  time through an iterative process.   In this context, the proposed SPACE-based optimization  simplifies the process by employing two powerful techniques (i)  Geometric Programming-based convex optimization for fast  evaluation of the metric J-1 introduced in (1) to find a set of local  optima and (ii) Statistical link analysis to find the global optimum  by using real channel models. SPACE filters out the large set of  local optima and hence reduces the EM solver’s computational  time. At the same time, SPACE also preserves the accuracy of NFIC  channel by using statistical link analysis.  3.3 Convex Optimization for Local Optima  signal bandwidth is limited by self-resonance frequency (SRF)  of the inductors to avoid excessive ringing in the received  voltage pulse causing ISI [16].  To find optimum NFIC link parameters, we introduce unified  link performance metric (J-1). The J-1 captures the effects of speed,  power and area for a given communication range.  Hence, our aim  Geometric programming (GP)-based convex optimization for  is to maximize the metric (J-1), as shown in (1).  )�−1 × BWD ( Tbps inductor based circuits and high-speed links have been explored  mm2 )                         (1)                            [18][19]. Optimum NFIC link configuration is obtained using  geometric programming (GP). In this work, we adopt this method  (pJ/b)  =                                  (2)  to a multi-variable 3D IC environment, which includes transmitter,  BWD (Tbps/mm2) =  Data Rate (Tbps)                                (3)  receiver and the NFIC channel parameters for evaluating possible  This performance metric J-1 incorporates both energy and area  local optima design space. This method provides mapping from  parameters using energy efficiency (pJ/b) and bandwidth density  block-level parameters (Tx current swing, inductor geometry, Rx  pre-amp gain bandwidth) to NFIC link metrics, as shown in Fig.  (BWD) (Tbps/mm2) for different data rates. Maximizing the metric  2(b). This mapping facilitates a holistic 3D IC link evaluation  J-1 ensures high energy efficiency (high data rate with low power)  and high bandwidth density (achieving higher bandwidth with  lower area overhead).   J-1 =  �Energy  Energy bit bit Power (mW) Data Rate (Gbps) Area (mm2 ) ( pJ b 3.2 NFIC Link Optimization via SPACE  BER is a function of transmitter, receiver and channel  configurations. Multiple possible configurations satisfy link BER  specifications at a given data rate and communication range in a  3D IC. However, a power-area-optimized design at a given data  rate (speed) can only be accomplished by balancing the interdependency of these parameters. The proposed speed-power-area  D0-D7 SERIALIZER Pre-Driver Driver IT CLK/2 D0-D7 CLK/2 Hysteresis  Buffer DESERIALIZER CLK/2 DFE CLK/2 RTp CTp RTp RLp + VRx PreAmp CLp RLp M VRx A×VRx gm + VTH  VOUT Figure 4: Complete NFIC transceiver architecture with DFE  implementation and threshold modulation in the receiver.   3                NOCS’17, October 2017, Seoul, Korea  S. Gopal et al.  platform. Optimum NFIC link parameters are generated from Stage  1 of the optimization process that maximizes the metric J-1.  3.4 Statistical Analysis for Global Optimum  Stage 1 provides optimum link parameters to Stage 2 for the  NFIC physical design, as shown in Fig. 2(b). The 3D EM solver  generates scattering parameter (S-parameter) of the NFIC channel.  Statistical link BER simulation from Advanced Design Systems  (ADS) is employed on the S-parameter to estimate link BER [19].   The statistical link BER analysis incorporates random and  deterministic noise sources, distortion effects, along with  transmitter current swing, receiver sensitivity and aperture time.  The entire simulation set up is shown in Fig. 3(a). The main sources  of noise and jitter include 5%-unit interval (UI) duty cycle  distortion (DCD) in data and clock with random jitter (RJ) of 5% UI  at the Tx driver. We employ decision feedback equalization (DFE)  for ISI cancellation and link BER performance improvement [8].  This simulation incorporates DFE with noise and offset of 3mV and  5% UI random jitter at the receiver front end. The BER contour  plots shown in Fig. 3(b) demonstrate that DFE improves the link  BER performance with a ~ 67% larger eye opening at a BER of 1012, without additional Tx power or inductor area.    4 LINK CIRCUIT MODELING AND DESIGN  Accurate modeling of transceiver circuits is accomplished by  constant current density scaling techniques with normalized  transistor parameters [19][20]. The parameters are obtained from  device characterization in 28nm CMOS FD-SOI process with 330  GHz peak transition frequency (fT) at 0.4 mA/µm current density.  Fig. 4 shows the complete NFIC link transceiver architecture.  4.1 Transmitter Circuit   To determine transmitter power and delay models, the entire  chain of serializer, local clock buffering, pre-driver and output  driver are considered. Sizing is based on the optimal transmit  current swing that meets the minimum receiver eye opening  requirement for the specified BER. The major constraints in the  transmitter circuits are given by: (a) the maximum peak-to-peak  differential output swing, which is equal to the nominal supply  voltage, and (b) the 20%–80% transition time of the serialization  and driver circuits that is limited to one-third of a bit period to  avoid excessive ISI [19][20].  4.2 Receiver Circuit   The receiver total power includes pre-amp, hysteresis buffer,  DFE buffer and the deserializer. The receiver front-end constraints  are: (a) pre-amp gain and hysteresis buffer meeting the required  sensitivity, and (b) its bandwidth commensurate with the specified  data rate. With the above gain bandwidth requirements, Rx frontend power can be evaluated. The power of deserializer latches can  be evaluated from its settling time constraints. The decision  threshold of the hysteresis latch is made adaptive based on the  previous state of the output, as shown in Fig. 4 [8].   5 3D NoC ARCHITECTURE UNDER TEST  In this section, we briefly describe the testbed NoC architecture  and the routing algorithm adopted in this work. 3D NoC combines  the benefits of 3D-intergration and NoC paradigms to design  energy-efficient and high performance manycore systems.  However, conventional mesh-based NoCs employ multi-hop  communications, and thereby suffer from high network latency  and energy consumption [15]. In this context, a small-world  network inspired 3D NoC (3D SWNoC) balances both the local and  long-distant communications by selectively  incorporating a  limited number of long-range links. The 3D SWNoC outperforms  all other existing alternative architectures [5]. In this work, we  consider the 3D SWNoC as a suitable architecture for exploring the  benefits of NFIC-based link design.   The performance of any NoC significantly depends on the  routing algorithm for internode-data exchange. For irregular NoC  architectures, such as the small-world network, the topology  agnostic Adaptive Layered Shortest Path Routing (ALASH)  algorithm has proved to be suitable [21]. Hence, we employ the  ALASH-based routing in this work.  6 EXPERIMENTAL RESULTS  In this section, we first present the performance of the  proposed NFIC-link in terms of energy efficiency and bandwidth  density. We compare the performance of the proposed NFIC link  with a state-of-the-art counterpart. In the second stage, we  evaluate the performance of a suitable NoC architecture  incorporating the NFIC links. As outlined in Section V above, we  consider the 3D SWNoC as the testbed to evaluate the merits of the  NFIC-enabled VLs.   For NoC performance evaluation, we consider the energydelay-product (EDP) per message and the NoC reliability as the  relevant metrics. EDP is defined as the product of network latency  and energy consumption. The reliability metric corresponds to the  mean-time-to-failure (MTTF) of the vertical  links and  its  subsequent effects on the 3D NoC performance.   ) s t i n 2 1 U d e z i l a m r o N ( c i r t e M 1 0 0   65 nm CMOS   28 nm CMOS FD-SOI JOpt -1 [9]TVLSI 2016 (Normalized) ) s t i 2.4 n U 1.8 d e z i l a NFIC link (D/Z ~ 4) w/ increased coupling coefficient NFIC link (D/Z ~ 1.8) w/ increased Tx current drive NFIC link (D/Z ~ 2.6) w/ equalization 1.2 m r o N ( 0.6 c i r t e M 10 15 20 Data Rate (Gbps)                                                                                     (b)                                                                (c)  10 15 20 Data Rate (Gbps) 25 30 5 J 5 J (a)  25 30 1 0 0 4  Figure 5: (a) Energy efficiency vs. data rate and area, (b) SPACE optimization for global optimum data rate, and (c) J-1 vs. data rate for variable  D/Z configurations with BER = 10-12                                Energy and Area Efficient Near Field Inductive Coupling: A Case Study on 3D NoC  NOCS’17, October 2017, Seoul, S. Korea  Figure 6:  NRZ recovered eye diagrams with hysteresis: (a) without DFE correction; (b) with DFE correction and (c) misalignment tolerance and  link power overhead.  6.1 Experimental Setup   The proposed transceiver circuits of 3D NFIC link are designed  in 28-nm CMOS FD-SOI technology. The complete NFIC  transceiver link is designed in a Cadence Virtuoso environment.  The NFIC channel’s physical design uses Keysight’s ADS  Momentum tool to extract S-parameter multi-port models. These  models are directly incorporated into Cadence Virtuoso’s Spectre  Circuit simulator. NFIC  link performance  is evaluated by  measuring the eye diagrams of recovered NRZ transient signal  responses at the receiver and corresponding link power is  obtained.  To evaluate the performance of the NoC architecture, we  employ a cycle accurate NoC simulator that can simulate any  regular or irregular 3D architecture. Our system consists of 64  cores and 64 routers equally partitioned into four layers. The width  of each link is the same as the flit width, which is 32 bits. Each  packet consists of 64 flits. The NoC simulator uses routers  synthesized from an RTL level design in Synopsys™ Design Vision.  All router ports have a buffer depth of two flits and each router  port has four virtual channels. Energy dissipation of the network  routers was obtained from the synthesized netlist by running  Synopsys™ Prime Power, while the energy dissipated by planar  wireline links was obtained through HSPICE simulations by  considering their lengths.   In total, we consider four SPLASH-2 benchmarks, FFT, RADIX,  LU, and WATER [22], and four PARSEC benchmarks, DEDUP,  VIPS, FLUID, and CANNEAL [23]. These benchmarks vary widely  in traffic injection rates and communication characteristics. Hence,  they are of particular interest in this work.  6.2 3D NFIC Link Simulation Results  The SPACE methodology helps us to determine an optimum  data rate at which both energy efficiency and bandwidth density  can be maximized for a given technology. Energy efficiency is  plotted for different data rates and inductor areas in Fig. 5(a). The  optimum value of the unified metric J-1 defined in (1) is shown in  Fig. 5 (b) for different data rates. The global optimum value  (maximum) of J-1 is ~1.7 NU (normalized units of (pJ/b)1×Tbps/mm2). This value is obtained at 18 Gbps data rate with an  energy efficiency of 240 fJ/b and a bandwidth density of 40.8  Tbps/mm2 for a communication range of 12 µm in 28nm CMOS  FD-SOI process. Compared to our results, normalized J-1 metric of  state-of-the-art NFIC-based 3D NoC for the same communication  range of 12 µm is about ~0.1 NU with energy efficiency of 1.4 pJ/b  and normalized bandwidth density of 13.9 Tbps/mm2 at 8 Gbps in  65nm CMOS process [9].  Fig. 5 (c) shows the variation of the metric J-1 with data rate for  three possible channel configurations depending on their D/Z  ratios. Here D is the diameter of the inductor and Z is the vertical  communication range for a target BER of 10-12. The configuration  with reduced area (D/Z ~ 1.8) has reduced metric J-1 due to large  power consumption from the Tx current driver. Although the  configuration with increased coupling coefficient (D/Z ~ 4) helps in  significant energy savings (>80%), it comes at the cost of increased  chip area. The proposed NFIC link achieves simultaneous  reduction in power and area for D/Z ratio of 2.6. To characterize  the NFIC link, we also show the recovered NRZ signal eye  diagrams at the Rx front-end output without and with DFE in Fig.  6 (a) and (b), respectively. These eye diagrams show an improved  differential peak-to-peak voltage of 1.66V compared to 1.12V  without the DFE at 18 Gbps data rate.  6.3 Misalignment Tolerance of NFIC-links  One of the primary advantages of the NFIC link is that it can  tolerate certain amount of misalignment between the Tx and Rx  inductors. Fig. 6(c) shows the channel loss as a function of the  degree of misalignment. The parameter Δx/D represents the  percentage of misalignment, where Δx is the amount of  misalignment between the Tx and Rx inductors (Tx center axis to  Rx center axis), and D refers to the inductor diameter. From Fig.  6(c), we can see that as the amount of misalignment increases,  channel loss and link power overhead increase gradually. For a +/20% misalignment, channel loss is only ~ 3dB more, and associated  link power overhead is less than 20% to compensate for additional  channel loss.  Figure 7:  Normalized EDP (w. r. t. TSV_adjacent) of TSV- and NFIC-based 3D SWNoC for adjacent and bypass communication.  5            NOCS’17, October 2017, Seoul, Korea  S. Gopal et al.  6.4 Performance of 3D NoCs: TSV vs. NFIC  TSV_bypass,  The vertical links in the 3D SWNoC architecture act as longrange shortcuts and significantly affect overall achievable  performance. Vertical links can be placed in a point-to-point (P2P)  manner between any two adjacent layers. Alternatively, they can  bypass the intermediate layers to establish direct links between  non-adjacent layers. In this section, we evaluate the performance  of 3D SWNoC in these two scenarios using both conventional TSV-  and NFIC-based vertical links. The diameter, pitch and length of  the TSV are 2 μm, 6 μm and 15 μm respectively [27]. The TSV-based  architectures for these two cases are denoted by TSV_adjacent and  respectively. The  corresponding NFIC-based  architectures are denoted by NFIC_adjacent and NFIC_bypass,  respectively. Fig. 7 shows the normalized EDP profiles of different  TSV- and NFIC-enabled SWNoC architectures. For comparative  performance analysis, EDP values are normalized with respect to  TSV_adjacent for each benchmark.  From Fig. 7, it can be seen that for adjacent communication,  TSV is more energy efficient than NFIC. However, when we  consider long-range communications in the vertical direction  bypassing the routers in the adjacent layers, then both TSV and  NFIC reduce the EDP. However, the EDP improvement for the  NFIC-enabled design is relatively greater. This happens due to the  fact that bypassing routers enables reduction of hop count.  However, TSV-based links dissipate more energy than the NFICcounterparts for long-range communications. The NFIC-enabled  long-range links are 27.29% and 34.5% more energy-efficient  compared to TSVs, while communicating from layer 1 to layer 3  and for layer 1 to layer 4, respectively. Consequently, the NFICenabled 3D NoC outperforms TSV counterparts when the NoC is  designed efficiently to exploit the true benefits of long-range VLs.   6.5 Robustness of TSV- and NFIC-based NoCs  The NFIC-based communication can maintain a certain level of  performance even in the presence of misalignment between the  transmitter (Tx) and receiver (Rx) inductors.  For example, as  shown in Fig. 6(c), NFIC can tolerate 20%, 40% and 50%  misalignments from their center-axis by dissipating 16%, 20%, and  30% more power, respectively, while maintaining the same data-  and BER-rates of a perfectly aligned NFIC-link. On the contrary,  TSVs in a 3D NoC suffer from formation of voids, cracks,  misalignments, improper bonding, and so on [2] [4]. These issues  significantly reduce TSV-bandwidth, induce bonding- and timingfaults, and in the worst case, lead to failure of TSVs. Consequently,  network latency increases, and overall EDP per message increases  progressively with time.   To analyze the performance of 3D SWNoC in the presence of  vertical link failure, we consider two cases. For NFIC-based  communications, the misalignment between the Tx and Rx  inductors is considered. The amount of misalignment varied from  0% to 50% in a step of 10%. On the other hand, for TSV-enabled  NoC, we consider up to 50% TSV-enabled link failure, with an  increase of 10% failure at each step.   6.5.1 Performance of 3D NoC with NFIC-link  misalignment  To analyze the performance of NFIC-enabled 3D SWNoC in the  presence of misalignment between Tx and Rx inductors, Fig. 8(a)  and Fig. 8(b) show the normalized energy consumption per  message and EDP, respectively, for all the benchmarks considered  here. In this case, energy and EDP values are normalized with  respect to the NoC configuration having 0% misalignments  between the Tx and Rx inductors. From Fig. 8(a), it can be seen that  as the misalignment increases, energy consumption per message  also increases. For the highest amount of misalignment considered  (50%), energy consumption increases by only 4.1%, compared to the  perfectly aligned NFIC-enabled 3D SWNoC counterpart (marked  as 0%). It should be noted that even with misalignment, the NFICbased link is able to carry the data through it by dissipating  additional power. As discussed in Section 4(b), with any  misalignment for NFIC-based links, possible reduction in data rate  (a)  (b)  Figure 8: Normalized (a) energy and (b) EDP of 3D SWNoC with different amount of NFIC Tx- and Rx-inductor misalignment.  Energy and EDP are normalized with respect to the baseline NoC with 0% misalignment for the respective benchmark.  (a)  (b)  Figure 9: Normalized (a) energy and (b) EDP of 3D SWNoC with different amount of TSV-enabled VL failure. Energy per message  and EDP are normalized with respect to the baseline NoC architecture with 0% link failure for the respective benchmark.  6              Energy and Area Efficient Near Field Inductive Coupling: A Case Study on 3D NoC  NOCS’17, October 2017, Seoul, S. Korea  and degradation of BER are compensated by dissipating extra  power at the Tx and Rx circuits. Hence, network diameter (average  hop count) remains unchanged, and router energy-consumption  does not increase. Second, only a certain percentage of the energy  dissipated in the links increases for the traffic that passes through  the VLs. For the 64-core 3D NoC configuration considered in this  work, 63% and 37% of the total energy per message are dissipated  in the routers and the links, respectively. The number of VLs is  one-third of the total links. As an example, the amount of traffic  affected by the misalignment in the Tx and Rx inductors for the  CANNEAL benchmark is 31% of the total traffic carried by the  links. Consequently, the energy consumption per message  increases only 4.1% compared to the 0% misalignment counterpart.  The EDP profile, as can be seen in Fig. 8(b), also follows a  similar trend as the energy consumption profile of Fig. 8(a). As  mentioned above, with any kind of misalignment for NFIC-based  links, the possible reduction in data rate and degradation of BER  are compensated by dissipating extra power at the Tx and Rx  circuits. As a result, the network latency per message is unaltered  from 0%-misalignment NoC configurations. Consequently, the  normalized EDP of NFIC-based 3D NoC shows the same  characteristics of energy consumption profile.   6.5.2 Performance of 3D NoC with TSV-failure   In the presence of workload-induced stress, TSV material (in  general Cu) in a 3D IC undergoes electromigration. Hence, TSVs  fail due to the increase in resistance and overall delay. In addition,  due to the crosstalk noise from neighboring TSVs, the delay of the  affected TSV increases further [3][24]. The lifetime of TSVs is  termed as the mean-time-to-failure (MTTF) and popularly defined  as the time when TSV delay increases by 10% [26]. At this point,  TSV should not be used in data exchange. Consequently, in this  work, we study the performance degradation of 3D NoC with TSVenabled vertical link failure (multiple TSVs are placed in a bundle  to form a vertical link) for performance degradation analysis.  Figs. 9(a) and (b) show the normalized energy per message and  EDP of 3D SWNoC by varying the amount of vertical link failure.  For comparative performance analysis, the energy and EDP of each  configuration is normalized with respect to its baseline NoC  architecture (with 0% link failure).  In Fig. 9(a), it can be seen that as the amount of TSV failure  increases from 0%, energy consumption increases as well. For 50%  of total VL failure, on average, energy consumption per message  increases by 13.7% for all benchmarks considered in this work.  Failure of any TSV-enabled vertical link reduces available routing  resources, increases average network-diameter as data needs to  follow alternative paths, and hence, NoC performance degrades.   Similarly, in Fig. 9(b), it can be observed that EDP per message  increases with the increase of TSV failure. Average EDP increases  by 19% compared to the NoC configuration with 0% TSV failure.  With failure of VLs, messages need to follow alternative paths, and  hence the latency also increases. Increase in network latency and  energy consumption directly affect EDP, and hence, EDP degrades  with TSV failure. For the SWNoC considered here, it is inherently  robust against link failures. Thus, the performance degradation is  limited even in the presence of 50% vertical link failure. For other  3D NoCs, performance degradation is more severe [5].  6.6 Reliability Improvement for 3D NoCs  In this section, we analyze how the resiliency of NFIC links and  the robustness of small-world network can be combined to design  reliable 3D NoC architectures.   6.6.1 Reliability Metric: MTTF Distribution of VL  To measure the reliability of any 3D NoC, we consider the  MTTF distribution of TSV- or NFIC-enabled VLs as the relevant  metric. MTTF distribution indicates the percentage of total VLs  that have MTTF less than a particular value. Consequently, lower  value of MTTF, at any instant of time, indicates more reliable and  robust 3D NoC, which is expected to have longer lifetime  compared to an NoC configuration having higher value of MTTF.   6.6.2 Reliability Improvement with NFIC-based Design   The main reliability concern of NFIC-based vertical link design  is the misalignment of Tx and Rx inductors. However, as  mentioned above, that can be addressed by paying an additional  power overhead. We can exploit the resiliency of the NFICs to  design robust NoC architecture.   In this work, we aim to achieve maximum reliability of TSVbased 3D SWNoC architecture by introducing a given number of  NFIC-enabled VLs. In order to achieve this, we need to determine  the failure prone TSVs, and then replace them with NFIC-based  links. For ease of referencing, the hybrid TSV- and NFIC-enabled  NoC architecture is denoted NFIC_x%. Here, the term x% refers to  the percentage of NFIC-enabled VLs present in the NoC, while the  other (100- x)% VLs are designed with TSVs. To ensure maximum  reliability for a given budget of NFIC links, we follow the approach  of an efficient spare-vertical link allocation algorithm proposed in  [25]. The main idea of this algorithm is to explore the particular  vertical link with minimum MTTF, and replace it with spare links.  In our work, instead of allocating spares, we explore the TSVenabled vertical link with the lowest MTTF value, and redesign it  with NFIC-based vertical inks until the upper bound of the NFICbudget is reached.  (a)  (b)  Figure 10: MTTF occurrence distribution (cumulative) for (a) CANNEAL, (b) FFT benchmark with different percentage of TSVenabled VLs are replaced with NFIC-based (NFIC_x%) designs. The MTTF is normalized w.r.t. the lowest MTTF of the system.  7          NOCS’17, October 2017, Seoul, Korea  S. Gopal et al.  6.6.3 Evaluation of 3D NoC Reliability with NFIC-links  To evaluate the performance and reliability improvement of  TSV- and NFIC-enabled 3D SWNoC, Fig. 10(a) and Fig. 10(b) show  the MTTF distribution of the VLs for two benchmarks, viz.  CANNEAL and FFT, respectively, as examples. The horizontal axis  indicates the normalized MTTF values (normalized with respect to  the lowest value for the respective benchmarks).  In addition, some  of the VLs have very high MTTFs compared to others, and for ease  of representation, we have represented these MTTFs together in  the final column of each figure. This is because the low-MTTF part  of the distribution (the left-hand side of each graph having MTTFs  lower than 2~4) is most critical for determining the lifetime of the  3D NoC. If any link has low MTTF, it fails earlier than others,  negatively affecting the neighboring links by increasing their  workloads, and reduces the overall reliability.  From Figs. 10(a) and (b), it can be seen that as the number of  NFIC-based links increases (represented as the percentage of the  total links), the MTTF distribution of VLs shifts towards the right.  More specifically, the number of links having MTTFs lower than a  certain value decreases significantly. For example, for NFIC_25%  NoC architecture with CANNEAL benchmark, as shown in Fig.  10(a), none of the vertical links have MTTFs lower than 3 (in  normalized scale). This states that the time it takes for the first  vertical link to fail in the NFIC_25% architecture enhances by 200%  compared to the NFIC_0% NoC configuration. Similarly, the  percentage of links having normalized MTTF less than 4 reduces  from 31.5% to 6.25% compared to the same NFIC_0% counterpart.  For other benchmarks, similar shifts of the MTTF distribution are  also observed. However, the exact amount of shift depends on the  workload distribution among the VLs and nature of benchmarks.  The lifetime of NFIC-based links is higher compared to the TSV  counterparts. As a result, as the number of NFIC-based links  increases in the NoC for the same application and workload  distribution, the lifetime of VLs increases significantly. Hence, by  combing these findings with the results from Fig. 8(b) and Fig. 9(b),  it can be inferred that NFIC-links-enabled 3D NoC can maintain  lower EDP value for longer period of time compared to the sole  TSV-based counterparts.   7 CONCLUSIONS  The challenges of implementing an energy-area-efficient  NFIC-enabled vertical link to communicate at high data rates are  intertwined among different parameters, namely, channel gain,  power dissipation, achievable data rate, associated area overhead,  and communication range. In this work, we demonstrated a  holistic NFIC-based vertical  link design methodology that  considers all of the necessary trade-offs associated with these  parameters. The combined energy and area efficiency of the  proposed NFIC link represented by the metric J-1 is an order of  magnitude higher compared to another state-of-the-art NFIC link,  while operating at a higher data rate. Finally, the benefits of NFICbased vertical links are explored in the context of reliable, faulttolerant, and energy efficient 3D NoC design. By incorporating  SPLASH-2 and PARSEC benchmarks, we demonstrate that  addition of only 25% NFIC-enabled vertical links in a TSV-based  8  NoC design achieves as high as 200%~300% improvement in the  lifetime of the vertical link with the lowest MTTF.  ACKNOWLEDGMENTS  This work was supported in part by the US National Science  Foundation (NSF) grants CNS-1564014, CCF-1514269, and CCF1162202.  "
Synchoricity and NOCs could make Billion Gate Custom Hardware Centric SOCs Affordable.,"In this paper, we present a novel synchoros VLSI design scheme that discretizes space uniformly. Synchoros derives from the Greek word chóros for space. We propose raising the physical design abstraction to register transfer level by using coarse grain reconfigurable building blocks called SiLago blocks. SiLago blocks are hardened, synchoros and are used to create arbitrarily complex VLSI design instances by abutting them and not requiring any further logic and physical syntheses. SiLago blocks are interconnected by two levels of NOCs, regional and global. By configuring the SiLago blocks and the two levels of NOCs, it is possible to create implementation alternatives whose cost metrics can be evaluated with agility and post layout accuracy. This framework, called the SiLago framework includes a synthesis based design flow that allows end to end automation of multi-million gate functionality modeled as SDF in Simulink to be transformed into timing and DRC clean physical design in minutes, while exploring 100s of solutions. We benchmark the synthesis efficiency, and silicon and computational efficiencies against the conventional standard cell based tooling to show two orders improvement in accuracy and three orders improvement in synthesis while eliminating the need to verify at lower abstractions like RTL. The proposed solution is being extended to deal with system-level non-compile time functionalities. We also present arguments on how synchoricity could also contribute to eliminating the engineering cost of designing masks to lower the manufacturing cost.","Synchoricity and NOCs could  make Billion Gate  Custom Hardware Centric SOCs Affordable  (Invited Paper)  Ahmed Hemani  Dept. of Electronics, School of ICT, KTH  Kista, Sweden  hemani@kth.se  Syed Mohammed Asad Hassan Jafri  Dept. of Electronics, School of ICT, KTH  Kista, Sweden  jafri@kth.se  Shayesteh Masoumian  School of Electrical & Computer Engineering,  University of Tehran, Tehran, Iran  sh.masoumian@ut.ac.ir  ABSTRACT  In this paper, we present a novel synchoros VLSI design scheme  that discretizes space uniformly. Synchoros derives from the Greek  word chóros for space. We propose raising the physical design  abstraction to register transfer level by using coarse grain  reconfigurable building blocks called SiLago blocks. SiLago  blocks are hardened, synchoros and are used to create arbitrarily  complex VLSI design instances by abutting them and not requiring  any further logic and physical syntheses. SiLago blocks are  interconnected by two levels of NOCs, regional and global. By  configuring the SiLago blocks and the two levels of NOCs, it is  possible to create implementation alternatives whose cost metrics  can be evaluated with agility and post layout accuracy. This  framework, called the SiLago framework includes a synthesis  based design flow that allows end to end automation of multimillion gate functionality modeled as SDF in Simulink to be  transformed into timing and DRC clean physical design in minutes,  while exploring 100s of solutions. We benchmark the synthesis  efficiency, and silicon and computational efficiencies against the  conventional standard cell based tooling to show two orders  improvement in accuracy and three orders improvement in  synthesis while eliminating the need to verify at lower abstractions  like RTL. The proposed solution is being extended to deal with  system-level non-compile time functionalities. We also present  arguments on how synchoricity could also contribute to eliminating  the engineering cost of designing masks to lower the manufacturing  cost.  KEYWORDS  VLSI Design, NOCs, Coarse Grain Reconfiguration, Synchoricity,  ASICs, SOCs, High-level Synthesis, ESL  1 INTRODUCTION  Networks on Chip emerged as an alternative to bus as a systemlevel interconnect at the turn of the century [1, 2, 3]. Since then, it  has been richly researched and the technology has matured and  found its way into many commercial products. So far, NOC has  been used primarily as a system-level interconnect for composing  software centric, accelerator rich, multi-processor SOCs. In spite of  the enhanced reuse that the programmable interconnect like NOC  and the adoption of platform based design brings, the engineering  Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact  the Owner/Author.  NOCS '17, October 19–20, 2017, Seoul, Republic of Korea © 2017  Copyright is held by the owner/author(s).  ACM ISBN 978-1-4503-4984-0/17/10.  https://doi.org/10.1145/3130218.3132339  cost of implementing O(100) million gate SOCs has reached  O(100) million USDs as per [33] and shown in Fig. 1. We also note  that 90% of the cost shown in Fig. 1 is engineering cost and the  hardware design in SOCs is dominated by infrastructural hardware;  the functionality is often implemented as software. As argued by  Dally et. al. [7], this negatively impacts product innovation, both  because fewer venture capitalists are willing to risk such large  investments and the software heavy implementation style blocks  product categories that need the computational efficiency of ASIC  implementation; the latter impact is further aggravated by the  slowing benefits of technology scaling.   180 160 140 120 100 80 60 40 20 0 Masks &  Prototype Validation Software Development  & Test Hardware Design  Archictecture Verification & Layout 65 nm 45/40nm  28 nm  20 nm  Source: Dr. Handel Jones, IBS ”Factors for Success in System IC Business and Impact on  Business Model”, Q4 2012 Report Figure 1: The exponentially increasing cost of SOC design and  manufacturing  In this paper, we present a vision that has the potential to enable:  a) end-to-end automation of O(1) billion gate SOCs from functional  untimed models of system, down to timing-and-DRC-clean  physical design (GDSII), b) the implementation style would be  hardware centric, i.e., it would principally be parallel and spatially  distributed functional hardware and some infrastructural hardware,  and c) perhaps most critically, the engineering effort would be  comparable to programming to non-incrementally lower the  engineering cost; see section IV for some early evidence. Finally,  we also argue that such a method could contribute to lowering the  manufacturing cost as discussed in section VI.   The solution being proposed in this paper to realize this vision  requires orders-of-magnitude improvements in three aspects of  VLSI design automation, compared to the standard cell based  hardware synthesis:  1. Reduction in the design space that must be searched to create  a hardened physical design (GDSII).   2. Rapidly generate arbitrarily complex valid VLSI design  instances and implementation alternatives on them.   3. Ability to rapidly evaluate the cost metrics: area, performance  and energy of the implementation alternatives with post layout  accuracy.  In this paper, we show how the above three requirements are  achieved by the SiLago framework [4] that is inspired by the Lego  toy system and stands for Silicon Large Grain Objects called    NOCS 2017, Oct. 2017, Seoul, South Korea  A. Hemani et al.  SiLago blocks. SiLago blocks are synchoros, a new term that  derives from the Greek word for space chóros (pronounced as  khóros – χóρoς). Synchoros objects discretize space uniformly, the  way synchronous objects discretize  time uniformly. The  synchoricity and use of NOCs are at the heart of how the three  aforementioned required improvements are achieved. We explain  how this is done in the next sections.  2 SILAGO VLSI DESIGN CONCEPTS  In this section, we introduce the key VLSI design concepts in  the SiLago framework that contributes to fulfilling the three  requirements stated in the previous section.  2.1 Raise the abstraction of physical design  SiLago blocks implement micro-architecture level operations  and are 4-5 orders larger than the boolean level standard cells and  replaces them as the atomic building blocks to compose VLSI  systems. They are hardened, characterized with post layout data  [18] and the cost metrics exported to higher abstraction syntheses  tools. These tools automate generation of custom SiLago designs  from algorithm, application (hierarchy of algorithms) and system  (asynchronously concurrent and communication applications)  levels, hereafter collectively referred to as the higher abstractions.   SiLago effectively raises the abstraction of physical design to  RTL. To analyze the impact of this in comparison to the standard  cells that raises the abstraction of physical design to boolean or gate  level, consider the two VLSI design spaces shown in Fig. 2.  Standard cells, by raising the abstraction to gate level, reduced the  design space compared to full custom design (not shown) and this  has enabled automation from RTL. However, moving to higher  abstractions has proven harder because of the large design space to  be searched for complex O(10-100) million gate designs and the  inability to accurately predict cost metrics of implementation  alternatives at higher abstractions because physical synthesis has  not yet happened. We quantify this inaccuracy in section IV.  Consequently, the refinement from system-level down to RTL is  done manually inducing the costly verification step that is the major  cost component behind the unscalable O(100) million USD  engineering cost of making SOCs and its impacts as discussed in  the introductory paragraph of section I.  Figure 2: SiLago raises the physical design abstraction to RTL to  exponentially reduce the design space that must be searched.  Another problem with the standard cells as a physical design  platform is that it only partially raises the abstraction of physical  design to gate level because it only freezes the circuit level  decisions for logic but not wires. The position and dimension of the  wire segments connecting the standard cells and the buffers  necessary to maintain drive strength needs to be synthesized as part  of the physical synthesis phase. This also applies to the clock tree  and power grid that must be planned and designed in. All these  factors make the standard cell VLSI design space (Fig. 2a) too large  to be searched, too malleable and too unpredictable. Every point in  this space is an implementation alternative and further up an  alternative is from the physical level, the less accurately we know  its cost metrics.   SiLago by raising the physical design to RTL, exponentially  reduces the design space that the higher abstraction syntheses tools  must search as shown in Fig. 2b. This goes towards fulfilling the  first of the three requirements stated in section I. Because, the  SiLago bocks are hardened and characterized with post layout data,  the cost metrics of operations hosted by SiLago is also known with  the highest possible accuracy. However, if SiLago only hardens the  RTL SiLago blocks, SiLago will only partially raise the physical  design abstraction to RTL, the way standard cells partially raises  the abstraction to gate level as discussed above.   The ambition of SiLago is to completely raise the physical  design abstraction to RTL. Completeness implies that if a design is  refined down to RTL, the dimension and position of every  transistor and wire segment in the entire design and not just the  SiLago blocks is decided. This includes the wire segments of the  NOC wires that connect SiLago blocks, the clock tree, reset and  power grid and the necessary buffers needed for some of these  wires. This is achieved by use of NOCs and the synchoricity  property, as we elaborate in the sub-sections 2.3 & 2.4 after  introducing the pre-requisite background on SiLago regions next.  2.2 SiLago regions – Customization  SiLago regions are instruments of design time heterogeneity  and customization in the SiLago framework. Each region is  customized for a specific type of functionality in all aspects of its  architecture – computation, control, address generation,  interconnect, storage and access to it. This is reflected in region  specific SiLago blocks as shown in Fig. 3. The generality and  completeness of the SiLago framework lies in aggregation of the  highly customized region types.  Figure 3: The SiLago VLSI Design Concepts.  There are two categories of SiLago regions: infrastructural and  functional. Examples of infrastructural regions are Clock/Reset  generation, Power Management, Memory Control, System Control,  global NOC etc. Except for the global NOC, infrastructural regions  are populated with single or few instances of the region specific  SiLago blocks, see Fig. 3.   # of Solutions increases exponentially with abstraction A b r t s a c t i n o L e v e l Gates Physical SiLago Physical Design Platform Onetime RTL engineering effort Algoritims (b) Exponentially Reduced Design Space that  SiLago syntheses tools need to search System Application o o P u A t m n o e a b t i l s s i Standard Cells Gates Physical RTL  Algoritims (a) Large Design Space that must be   searched in Standard Cell based EDA System Application M a u n a l u A t o m a t d e u A t o m a t i n o I m o p s s i y b l D i f f i c u l t Standard Cells Onetime engineering effort Onetime engineering effort H i e h g r b A r t s c a t i n o s H i e h g r b A r t s c a t i n o s DRRA: Dynamically Reconfigurable Resouce Array. DSP/Dense Linear Algebra CGRA DiMArch: Distributed Memory Architecture. Streaming Scratchpad CGRA SWB: NOC Switch connected to each other directly or via NOC wires (buffered/pipelined) NIU: Region specific Network Interface Units abut and connects to NOC Switch N.B: To avoid clutter, not all NIUs, NOC wires, SWBs are labelled; they are color coded. G s r i e e n c L a p d i s n a e h f t o t r p c e o s h k c c n y t s k e c c o r c r r i i d n n a e s y e r p e n r t n e e e s t i n w l i t i i m 1. Region Specific SiLago Blocks; Regions are color coded 2. The diagram shows a custom SiLago design instance composed of instances of different region types. This is decided by the SiLago Syntheses flow depending on the functionality and constraints. NIU S W B NIU DRRA NIU DRRA DRRA DRRA SWB NIU NIU RISC  System Controller Program Memory Ethernet PLL/CGU  + PMU Data Memory NIU  TSVs 3D Memory  Control DiMArch DiMArch Standalone DiMArch DRRA DiMArch SWB Datapath System Path NOC Wires DiMArch DRRA O N C W i r e s 2                                                              Synchoricity and NOCs could  make Billion Gate Custom  Hardware Centric SOCs Affordable  Functional region types correspond to the dwarfs identified in  the Berkeley report on the Landscape of Parallel Computing [6] and  are implemented as domain specific CGRAs. Fig. 3 shows two  CGRAs, one for dense linear algebra/inner modem (DRRA) [9, 10]  and another for streaming scratchpad memory (DiMArch) [16].  CGRAs for protocol processing [17] and dynamic programming for  bioinformatics applications [8] have also been developed.   Depending on the set of applications and their constraints,  SiLago’s syntheses tools decide the number of instances of  different types of regions, the size of each region instance in terms  of number of Silago blocks and the floorplanning.  2.3 NOC based interconnect  NOCs in SiLago go beyond its traditional role as a chip level  interconnect. It also serves to cluster SiLago resources from fine  grained micro-architecture level to large grain task and application  level  to provide variations  in  function, capacity and  architecture/degree of parallelism that we call as implementation  alternatives as shown in Fig. 4.  SiLago designs have three levels of interconnect. The lowest  level of interconnect is intra SiLago blocks and is point to point and  hardwired. Though the interconnect is hardwired, the coarse grain  reconfigurability of the functionality within each SiLago block  enables the same SiLago block to function in different modes and  contributes to implementation alternatives as elaborated in sections  III.A and III.B.   The second level of interconnect is the inter SiLago blocks and  applies to functional CGRA based regions. This interconnect is  based on NOCs that are region specific and thus called regional  NOCs. The regional NOCs provide variations in terms of different  clusters of SiLago blocks and each SiLago block operating in  different mode enabled by its coarse grain reconfigurability. These  implementation alternatives vary from fine grain O(100K) to  medium grain O(1 million) gate datapath. In section III.A and III.B,  we show how rich this space can be.   The third level is inter-region interconnect and is also based on  NOCs, called global NOC elaborated in section III.C. The global  NOCs can create clusters across region instances and can provide  medium to large grain O(1-10 million) implementation alternatives  that we call as system-paths. A SOC is composed of multiple such  system-paths.  These implementation alternatives do not require change in the  underlying VLSI design, they are all created on the same VLSI  design instance, see Fig. 4.  Agile and efficient parallel coarse grain  reconfiguration creates different implementation alternatives, see  [14, 15]. SiLago also enables rapid generation of different design  instances with varying number and combination of SiLago blocks  and region instances. These instances are generated by abutment of  SiLago blocks. Abutment is enabled by synchoricity as elaborated  next.  SiLago Physical Design Platform Design Instance 1 Design Instance 2 .  .  . Composition by abutment generates different SiLago  Design Instances that are different VLSI Designs Implementation  Alternative 1 Implementation Alternative 2 .  .  . Configuration of Coarse Grain Reconfigurable SiLago blocks and  two levels of NOCs generates implementation alternatives on  the same VLSI Design Instance. These alternatives vary in  function, capacity and architecture/parallelism Figure 4: The hierarchical design space in SiLago. The leaf nodes  are the points in the higher abstraction design space in Fig. 2  2.4 Synchoricity based composition by abutment  As synchoricity is a fundamental property that enables  abutment, we begin by formally defining it:  NOCS 2017, Oct. 2017, Seoul, South Korea  Synchoros objects. A VLSI object A is synchoros if its  dimensions xA = K·uX , yA = L·uY, zA = M·uZ.  where uX, uY and uZ  are unit vectors and K, L and M are integers. Two objects A and B  are synchoros with respect to each other, if their respective  dimensions are equal, i.e., xA= xB, yA= yB, zA= zB. Further, the two  objects A and B are ratiochoros, if they are both synchoros and their  corresponding dimensions are rationally related.   Synchoricity is enforced using a virtual grid, see Fig. 3, that  models the unit vectors and corresponds to clock ticks in  synchronous systems. When the SiLago blocks are hardened, they  are hardened to occupy a contiguous number of cells. Region  specific SiLago blocks, potentially occupy different number of grid  cells as shown in Fig. 3. Henceforth, SiLago blocks imply being  hardened, synchoros and abut-able.   The abutment process implies that when the SiLago blocks are  placed on the grid, the corresponding interconnects of the  neighboring blocks connect to create a valid VLSI design without  any further logic or physical synthesis. This is how abutment is  implemented in SiLago:  1. All wires whose span go beyond that of a SiLago block are  divided into identical segments that are absorbed into SiLago  blocks. All wires imply functional NOC wires and infrastructural  wires like resets, clocks and power grid. SiLago does not allow  arbitrary point to point interconnect, as in ASICs. All wires whose  span is beyond SiLago have an architectural regularity that enables  them to be hardened in a parametric manner and enable the  abutment process.  2. As part of the hardening process, SiLago blocks brings out all  their interconnects, including the segments, on the periphery at the  right place and on the right metal layer, so that when the  neighboring SiLago blocks are placed and aligned on the grid, their  corresponding interconnects abut, see Fig. 6 for details. This  process not only enables rapid generation of valid VLSI design  instances, it also ensures that the segments of inter-SiLago block  wires are identical in their VLSI design in each SiLago block of the  same type. This is essential to ensure space invariance of cost  metrics of operations in the SiLago blocks. Otherwise, each SiLago  block will need to be characterized depending on its position and  make the engineering effort of characterization unscalable and  dependent on each design instance. This can be visually seen in the  micro-graph of two of design instances in Fig. 5, one composed  using conventional hierarchical synthesis and the other using  SiLago’s composition by abutment. Making wires predictable is  even more important than making logic predictable: “By 2018, the  cost of moving a 64 bit operand a mere 5 mm across the chip will  exceed  that of a floating-point operation  that uses  that  operand”[34].  (a) DRRA region instance composed by  commercial EDA’s hierarchical synthesis  flow results in irregular regional wires  resulting in space dependent cost metrics. (b) DRRA region instance composed by  SiLago’s composition by abutment scheme  results in regular regional wires resulting in  space invariant cost metrics. Figure 5: Comparing EDA’s hierarchical synthesis vs. SiLago  composition by abutment of a DRRA fabric instance  3. Arbitrary SiLago blocks cannot be neighbors, only a small  subset of SiLago block types can be neighbors. This is part of the  fabric’s topological definition, that is captured by a two  dimensional grammar. Every SiLago design instance, like the one  3                      NOCS 2017, Oct. 2017, Seoul, South Korea  A. Hemani et al.  shown in Fig. 3, is a sentence in this grammar. This restriction is  rooted in logic and architectural compatibility of neighbors that  have corresponding interconnects that can be abutted.  4. Neighbors that abut are not just architecturally compatible as  required by the previous point, they are also geometrically  compatible, i.e., synchoros. Synchoricity is natural because the  functional SiLago blocks are CGRA cells, they tend to have an  architectural regularity that naturally translates into equal sized  SiLago blocks. Additionally, in SiLago there are no arbitrary point  to point connections as it happens in ASICs. All SiLago blocks talk  to each other over regular programmable interconnect. This implies  that the wires are also amenable to physical design regularity.   Some CGRA cells could differ slightly because of differences  in supported modes and/or position in the fabric, for instance being  on the edge. This could result in different sizes. However, keeping  the size uniform enables parametric hardening and planning of the  functional and infrastructural wires that are regional, i.e., interSiLago block.  Note that, the standard pitch of standard cells was also driven  by similar motivation and even the limited enforcement of  synchoricity in one dimension, enabled impressive two orders gain  in productivity [31] compared to the-then state of the art full custom  macro based silicon compilers.  2.5 Composition by abutment of Clock and  Power Grid  The composition by abutment solution would not be complete  without it also applying to the infrastructural wires. In this subsection, we outline the scheme we have adopted for clock and  power grid to show how a scalable and valid clock and power nets  emerge by abutting SiLago blocks that absorbs hardened and  identical segments of these wires.  Clocks in SiLago have three levels. At the lowest level is the  clock tree that drives the flops in each SiLago block. Synthesis of  this local clock tree is generated by the commercial EDA tools and  is part of the hardened SiLago blocks.   The next level of clock tree is regional that feeds the local clock  trees of each SiLago block in a region instance. Fig. 6 shows the  details of how the regional clock tree is created by dividing it into  identical segments that are absorbed in Silago blocks and abutting  these SiLago blocks create a valid clock regional clock tree. Fig. 6  is also a good example of how composition by abutment process  works for other wires as well.  The Global Clock enters from an edge of the region via NIU Fragment of a region instance Each SiLago block has an identical fragment of the regional clock  tree composed of: 1. A buffer to maintain the edge, a technology imposed design  rule constraint. It also helps divide the regional clock tree into  identical equally loaded segments  2. A programmable delay line to minimize the skew between  entry points to the local clock trees by tapping into a delay  depending on the position of the SiLago block from the entry  point of global clock in the region 3. Has two entry points on top and left edge 4. Has two exit points on bottom and right edges 5. The corresponding exit and entry points allign enabling  abutment 6. Each SiLago block is surrounded by a small 2 micron halo and a  small wire (red colored) is created by the SiLago’s composition  by abutment script to connect the corresponding interconnects 7. Abutting the fragments of the regional clock tree creates an  electically and technologically valid regional clock tree 1 3 e n e n 2 i L y a e l D Local Clock Tree e n i L y a e l i L y a e l D Local Clock Tree e n i L y a e l 4 5 6 D Local Clock Tree D Local Clock Tree Figure 6: Conceptual Schematic showing how Regional Clock Tree  is created by abutting identical segments in each SiLago block  The regional clock tree is rapidly generated in SiLago by  abutment is regular as shown in Fig. 7b. This ensures space  4  invariance of cost metrics and is essential for the overall  composition by abutment scheme to work. For any arbitrary design  instance, the regional clock tree is created without any further VLSI  engineering effort other than what has gone into creation of the  SiLago block as a onetime engineering effort. The regional clock  tree generated has been validated with sign-off quality static timing  analysis and the switching capacitance is marginally lower than the  one generated by the hierarchical synthesis flow shown in Fig. 7a.  In contrast, the commercial EDA tools take 2-3 orders more  time to generate the regional clock tree and a new regional clock  tree has to be synthesized for every region instance.  Further, in  spite of the design being regular, the regional clock tree, as shown  in Fig. 7a, its routing and buffers are irregular. This violates the  space invariance requirement and is not amenable to composition  by abutment.  The third and the highest level of clock is the global clock that  is routed via NOC corridors, see Fig. 3. The global clock domain  and regional clock domain communicate on latency insensitive  basis by using a variant of GALS method called Globally  Ratiochronous and Locally Synchronous [5].  (a) Commercial EDA generated Regional  Clock Tree is irregular (b) SiLago’s composition by abutment  generated Regional Clock Tree is regular The leaf nodes are the Local Clock Trees.   The rest is the Regional Clock Tree Figure 7: Regional clock trees generated by commercial EDA vs.  SiLago’s composition by abutment scheme for the same set of leaf  nodes  Power Grid in SiLago is also generated by segmenting its  components and absorbing them into the hardened SiLago blocks  as shown in Fig. 8. Each SiLago block has its own power ring that  drives the power rails in it. This has been used for fine grain power  management [21, 22]. The rings are fed by a regular grid structure  of power stripes. The pitch of the grid is that of the smallest SiLago  block and positioned in the center as shown in Fig. 8. This allows  for a scalable scheme because as the size of the SiLago design  instance increases, the number of parallel paths that draws current  from the global outermost ring also increases.  Power Stripes a e r a e r o c l l e c d r a d n a t S s e p i r t S r e w o P Internal Power Rings Figure 8: Power Grid in SiLago is also created in a scalable manner  by abutting SiLago blocks with segments of Power Stripes  In conclusion, we propose to not just move to larger grain  hardened SiLago blocks compared to the standard cells, but we also  propose to adopt a stricter architectural and physical design  discipline to enable parametric hardening of all wires that in turn  enables composition by abutment to rapidly generate arbitrarily  complex and varied SiLago design instances. Synchoricity is  essential to keep the engineering effort to implement the  composition by abutment scheme and the space invariance of cost                            Synchoricity and NOCs could  make Billion Gate Custom  Hardware Centric SOCs Affordable  NOCS 2017, Oct. 2017, Seoul, South Korea  metrics scalable. Synchoricity is enabled by a regular microarchitecture and a regular interconnect scheme that we elaborate  next.  3 SILAGO MICRO-ARCHITECTURE LEVEL  FRAMEWORK  In this section, we present the micro-architecture level details of  DRRA, DiMArch CGRA fabrics with their own regional NOCs and  the global NOC that interconnects instances of DRRA and  DiMArch regions to show how they help achieve synchoricity and  generate implementation alternatives.  3.1 Dynamically Reconfigurable Resource Array  Dynamically Reconfigurable Resource Array (DRRA) is a  CGRA that is customized for dense linear algebra/DSP and a  fragment of it is schematically shown in Fig. 9. The key  differentiating factor of DRRA as a CGRA is its highly parallel and  spatially distributed customized control that contributes to its near  ASIC like efficiency [25]. Its rich control and scalable data [10],  control [9] and configuration interconnect [14, 15] has been  designed to implement complete sub-systems [4] rather than serve  as a configurable accelerators [12, 11, 13] made up of a sea of  arithmetic and register files. See [26] for a rigorous comparison to  other CGRAs and FPGAs. The DRRA fabric is composed of core  DRRA Cells that are interconnected by the regional NOCs.  1) The core DRRA Cell  The resources inside the core DRRA cell are a DPU, a register  file and a sequencer. They are hardwired to each other but the  resources themselves are configurable.  The DPU is customized for typical DSP operations. A few  variants of DPUs are available implementing different data flow  sub-graphs, with the same foot print and external interface.   The Register File has two read and write ports and each port  has an independent address generation unit (AGU). AGUs can  implement arbitrary two level affine functions that are typically  used in DSP like linear addressing, circular buffer, bit-reverse  addressing etc. Like DPUs, the AGUs come in a few variants  supporting different addressing modes. The DRRA AGUs can be  programmed in temporal sense by inserting run time configurable  number of clock ticks between reads and writes and between  repetition and start of streams. Coarse grain reconfigurable AGUs  provide hardwired FSM like efficiency and embody the ASIC like  parallel, spatially distributed customized control.  The Sequencer is a single cycle fetch decode execute type with  a small local sequencer memory. This makes its energy efficiency  comparable  to hardwired FSMs. The sequencers do have  branching, wait and iteration instructions but it is primarily  designed as a configuration unit that contributes to generation of  implementation alternatives at two levels. At the lowest level,  sequencer configures the local DPU mode and the four AGUs in  the local register file. At the next level, the sequencer configures  the local switch boxes of the regional NOC to control the clustering  of core DRRA cells.  These core DRRA cells are organized in a fabric glued by NOCs  that we describe next.  2)  The Sliding Window Circuit Switched NOC  The core DRRA cells are organized in N rows × 2 columns and  flanked on potentially both sides by a streaming scratchpad  memory CGRA  called DiMArch  (Distributed Memory  Architecture) described in the next sub-section B. The interconnect  that glues the fabric of core DRRA cells is a sliding window circuit  switched NOC. Each DPU and register file outputs to a bus that  straddles n columns to the left and n to the right creating a span of  2n+1 columns. Concurrent busses that spans 2n+1 columns  overlap with a stride of 1 column to create a sliding window  interconnect. At present, the span is 7 corresponding to n=3. This  is a design time decision and was driven by a) what can be  combinatorically glued in a single cycle in a specific technology,  b) sufficient resources to implement complex dataflow like radix-4  FFT butterfly and c) the overhead of interconnect with the  increasing span. The frequency of operation is also a design time  decision and is a trade-off between area and performance. Higher  frequencies are possible but like ASICs, SiLago relies on deeper,  parallel spatially distributed designs as opposed to the deeply  pipelined and centralized design style of processor centric SOCs.  Figure 9: Fragments of DRRA and DiMArch CGRA fabrics that  forms regions in SiLago designs  The output buses are intersected by the input buses with a  regional switchbox (RSWB in Fig. 9) to create a sliding window  circuit switched NOC that allows non-blocking, multi-casting  between DPUs/register files in the span of 7 columns. This forms  the basis for creating arbitrary datapaths as implementation  alternatives in terms of DPUs and register files by configuring this  regional NOC; the configuration happens in parallel spatially  distributed manner by sequencers in each DRRA cell that has  access to the local RSWB for DPU and Register file (see Fig. 9).  Note that the span of 7 columns restricts how far the input/output  of DPU or register file could reach in a single cycle. If this span is  not sufficient, it is possible to create a pipelined datapath that goes  beyond 7 columns. This this would also happen in ASICs.  The decision to have a circuit switched NOC is justified because  it is more efficient compared to packet switched NOC and it is  sufficient because the established paths are quasi-stationary to  implement a cluster of communicating DRRA core cells that  survives as long as the application persists. Besides the data NOC,  DRRA also has a control NOC [9] to enable creation of a hierarchy  of Control and Data Flow Graphs.  S R W B S R W B DPU DPU DPU DPU DPU DPU DPU SRAM SRAM SRAM SRAM SRAM SRAM SRAM SRAM SRAM SRAM Composite DiMArch Cell SRAM SRAM SRAM SRAM Reg. File DPU S R W B S R W B n e u q e S e c r Reg. File DPU S R W B S R W B n e u q e S e c r Reg. File DPU S W B S R W B n e u q e S e c r Reg. File DPU S W B S R W B n e u q e S e c r Reg. File DPU S W B S W B n e u q e S e c r Reg. File DPU S R W B S R W B n e u q e S e c r Reg. File DPU S R W B S R W B n e u q e S e c r S W W n e u q e B S S e c r S W W n e u q e B S S e c r S W W n e u q B e B S e c r S W W n e u q e B S S e c r S W W n e u q e B S S e c r S W W n e u q e B S S e c r S W W n e u q e B S S e c r Reg. FileR B Reg. FileR B Reg. FileS Reg. FileR B Reg. FileR B Reg. FileR B Reg. FileR B Regional Datapaths Input buses The sliding window Output buses.  Each DPU/Register file has an output bus  that spans 3 columns on each side to  create a 7 column span Regional Circuit Switch NOC (RSWB)  provides non-blocking communication  between every register file and DPU in  the 7 column sliding window; i.e., Each DPU/Register file can send/receive data  to 28 DPUs/Register Files including itself Composite DRRA Cell Circuit Switched  Data NOC Packet Switched  Control & Config NOC S R W B S R W B S R W B S R W B S R W B 5                      NOCS 2017, Oct. 2017, Seoul, South Korea  A. Hemani et al.  To implement synchoricity and composition by abutment, each  core DRRA cell absorbs the NOC wires spanning multiple columns  internally to create a composite DRRA cell as shown in Fig. 3. As  a result, the multi-column span of the sliding window DRRA NOC  wires are not created separately during global route, but gets  created by abutting the fragments of these wires absorbed in each  composite DRRA cell.   3) NOC enabled implementation alternatives in DRRA   The synchoricity and the composition by abutment enables  rapid creation of arbitrary DRRA region instance and for each such  instance, there is a rich possibility to create implementation  alternatives by configuring the DRRA cells and the sliding window  NOC in many different ways: 1) The DPUs can be chained to create  arbitrary data flow in terms of the atomic dataflow operations  supported by the DPUs. These chains need to be pipelined after 4  columns on each side and multiple such chains can co-exist and  feed each other in arbitrary manner. 2) The register files, including  split register files, can be chained to implement larger register files.  3) Chains of DPUs and register files can be clustered to create  arbitrary datapaths.  To get a feel for the rich design space that such NOC enabled  clustering creates, consider the variations in a chain of core DRRA  cells that can be created. Let us say each DPU supports m  computation modes and each register file has n addressing modes,  creating m×n possible combinations in which each core DRRA cell  can operate. In a 7× 2 fragment shown in Fig. 9, there are 14 core  DRRA cells and if they are daisy chained, the number of possible  datapaths that can result is (m×n)14. Typically m and n are between  10 and 20 and one can see that even for a small fragment of 7  columns and with just one possible way of chaining, it is possible  to create enormous number of variations. Many fragments of such  chains can co-exist and the m×n factor, in real life will be  significantly more rich with different sequence of operations in  each sequencer.  The design space of DRRA is made even more rich by  combining it with Distributed Memory Architecture CGRA fabric  that we introduce next.  3.2 Distributed Memory Architecture - DiMArch  DRRA provides a scalable computational engine to implement  dense linear algebra functionalities like DSP [25], Neural Networks  [27] that also have a need for large scratchpad buffers to hold  frames, packets, kernels and  intermediate  results. These  requirements are fulfilled by the Distributed Memory Architecture  (DiMArch) CGRA fabric [16], see Fig. 3 and Fig. 9.   DiMArch is also a region in the SiLago framework. It can be  instantiated as a standalone scratch pad memory to serve as shared  buffer between communicating applications or be instantiated as a  tightly coupled scratchpad memory along with DRRA as a  composite region. Fig. 3 shows DiMArch in both variants. A key  advantage of the composite DRRA+DiMArch region is that  increase in computational parallelism in DRRA can be matched  with increase in parallelism in access to the DiMArch. Also, the  proportion of scratchpad to computation can be customized  according to application and its requirements. This is not possible  in most multi-core based fabrics like [23, 20] and CGRAs [11, 13,  12], where the ratio of compute and scratchpad is locked at design  time and requires in-efficient and asymmetric access to SRAM  outside the cell/node.  1) The abut-able synchoros DiMArch Cell  The core of DiMArch cells are SRAM banks, at present we use  SRAM macros of size 2K×128-bit words. These banks are  6  organized in the form of a matrix as shown in Fig. 9 and glued by  two regional DiMArch NOCs: a circuit switched data NOC and  another that is a packet switched control and configuration NOC.  The latter is used to program the switches to create virtual SRAM  banks that is composed of multiple DiMArch cells.   The motivations for the circuit switched NOC are the same as  in the case of DRRA. However, in DiMArch, clustering SRAM  banks has no need for a sliding window because at any point in a  time, only one SRAM bank would be active in reading or writing  data in a cluster. In contrast, in DRRA, a DPU could be  simultaneously  fed by multiple DPUs/register  files  from  neighboring columns simultaneously. This is an example of domain  specific customization of NOCs. Similar customization of NOCs  has also been done for protocol processing and dynamic  programming CGRAs [17, 8].   The packet-switched NOC is used to program the AGUs and the  paths for the circuit-switched network. The motivation for using  packet-switched NOC is that control/configuration requires short  infrequent programming messages. Moreover, the generality of  packet-switched network allows the system controller to easily  access any node.  As in the case of composite DRRA cells, the DiMArch cells are  also made abut-able by segmenting DiMArch NOC wires and  absorbing the segments as part of the hardened synchoros  composite DiMArch cells as SiLago blocks. This makes DiMArch  and its NOCs in any arbitrary configuration, not just architectural  constructions but actual physical designs.  2) Implementation alternatives by clustering DRRA, DiMArch  Both the AGUs and the circuit switches in DiMArch data NOC  can be programmed in spatio-temporal sense. For instance, a circuit  switch can be programmed to stream for n cycles from east using a  specific address pattern and then seamlessly switch to north for the  next m cycles and so on. This enables creation of arbitrarily large  buffers with specific temporal rhythm. Such buffers can then be  clustered with clusters of DRRA cells in configurable degree of  parallelism between the two clusters to create arbitrary ASIC like  datapath with parallel and spatially distributed computation,  control, address generation and programmable interconnect. These  computing structures can a hierarchy of kernels to implement  complete sub-systems and not just speed up computation, as is done  in accelerators. As this section is on DiMArch, we focus on the  implementation alternatives that DiMArch and its regional NOCs  together with the global NOC enables in terms of three kinds of  data movement:  1. Local: Move data from one part of DiMArch to another part  in the same DiMArch region instance  2. Composite region: Move data between register files and  DiMArch partitions in a composite DRRA+DiMArch region  instance   3. Chip level: Move data over global circuit switched NOC  among 3D DRAM and DiMArch partitions in different region  instances.   The SiLago syntheses tools can when evaluating any such large  data transfer use cases combined with computation can evaluate  architectural alternatives to host them and evaluate their cost with  agility and accuracy that is simply not possible with standard cell  based design flows.  3.3 Quantification of DRRA, DiMArch   DRRA and DiMArch as CGRA fabrics have been implemented  in 40 nm node as the basis for SiLago physical design platform.  A  SiLago grid cell takes 100×100µm2 area and DRRA and DiMArch      Synchoricity and NOCs could  make Billion Gate Custom  Hardware Centric SOCs Affordable  cells occupy 4 and 6 grid cells respectively as shown in Fig. 3. Fig.  10 shows the area breakdown in terms of their respective  components. The regional NOCs in the DRRA and DiMArch bring  significant  flexibility  to create arbitrary  implementation  alternatives at a fairly moderate cost of ~20% as shown.  Sliding Window Circuit  Switched NOC + Control NOC Packet Switch  NOC Circuit Switch NOC Sequencer 22% DPU 14% 8% 8% 12% Sequencer  3% Sequencer 15 % Reg. File +  AGUs  49 % (a) Area breakdown of a  DRRA cell SRAM 72 % (b) Area breakdown of  a DiMArch cell SRAM 30% DRRA Sliding Window  Circuit Switchbox 3% DiMArch Circuit  Switchbox 4% DRRA Circuit  Switched Wires  DRRA (0.1%) Pipelined Circuit  Switched Wires  DiMArch 5% (c) Energy Breakdown a SiLago  CNN Design Instance Reg File  24% DPU 31% Figure 10: Area breakdown in DRRA and DiMArch Cells and  Energy breakdown of a SiLago Convolutional Neural Network  (CNN) Design  The energy breakdown in terms of components of DRRA and  DiMArch for a CNN is shown in Fig. 10c, shows that like area, the  energy cost of programmable NOC interconnect is fairly modest  (~15%). Given the versatility that the NOCs provide in in enabling  rapid generation of implementation alternatives whose cost metrics  are accurately known, we argue that this overhead is fully justified.  An ASIC macro would typically have hardwired point to point  interconnect that would be mostly wires with multiplexors that  would cost less in both area and energy compared to NOC.  However, we argue that NOC not only brings flexibility in an  architectural sense, it is the basis for interconnect regularity that is  essential for synchoricity and the abutment scheme.  3.4 Global NOC  To go beyond the complexity of datapaths that can be created  within a region with the help of regional DRRA and DiMArch  NOCs to implement systempaths as shown in Fig. 3 encompassing  O(10-100) million gates, SiLago has a Global NOC to create interregional clusters. A systempaths are envisioned as implementing  complex applications. A SiLago SOC is composed of multiple  interacting systempaths.  Global NOC has two parallel independent NOCs. One is a 32  bit wide packet switched control and configuration NOC that is  primarily used to initialize, control and configure region instances,  instantiate and terminate applications under the control of the  system controller, a RISC processor. The second global NOC is a  circuit switched 128 bit wide data NOC that is used for data transfer  at chip-level from 3D DRAM to DiMArch partitions and between  DiMArch partitions. It is also intended for off-chip data transfer via  Ethernet or some other modem, also realized as SiLago regions.  However, the data NOC is not intended to transfer data required in  the inner loops. This is an engineering guideline motivated by the  fact that data required in inner loop ought to be in the composite  regional DiMArch buffers for performance and efficiency regions.   The design decision to have two separate NOCs was made in  the interest of simplicity and efficiency for catering to two  significantly different interconnect requirements. The architectural  decision could be different than what we have made but the core  SiLago idea is that the NOCs should be composed of SiLago blocks  that are synchoros and NOCs of arbitrary dimension and capacity  should be possible to be composed by abutting the atomic NOC  SiLago blocks as shown in Fig. 3. There are three atomic NOC  SiLago blocks:   1. NOC switch (SWB in Fig. 3) that has logic for both packet  and circuit switching. The logic is sparse and takes only one SiLago  NOCS 2017, Oct. 2017, Seoul, South Korea  grid cell. However, for alignment, these blocks are stretched to 2×1  or 3×1 grid cells as two variants.   2. NOC wires: SiLago blocks come in two variants. One that  could be buffered (in the electrical sense to maintain drive strength)  or pipelined with flops. These blocks also come in 2×1 or 3×1 grid  cell variants.  3. Network Interface Units: (NIUs in Fig. 3) are region  specific and works as a gateway between the global NOC and the  regional NOC. NIUs for infrastructural resources are often tightly  integrated within and not instantiated separately. However, for the  functional regions, the NIUs are instantiated separately as shown in  Fig. 3.  Packet-Switch: The packet switch shown in Fig. 11a. contains  six full-duplex 32 bit channels. Five of which are conventional  channels that transfer data between East, West, North, South, and  the SiLago region instance directions. The sixth channel is used to  configure the local circuit-switch that is also encapsulated in the  same SiLago block. For the sake of simplicity, we have opted for  dimension order routing (DOR) with X as the preferred direction.  As DOR is meant for mesh topologies and not the arbitrary  topology of SiLago Global NOC, see Fig. 3, we have tweaked the  DOR algorithm. The adaption uses a 4-bit configuration register  that is configured at the startup by a special packet called the  configuration packet that identifies the active (North, South, East,  and West) channels. During routing, if the calculated X or Y  direction (for DOR) is inactive, the packet is routed in a direction  that takes it closer to the destination. If it is not possible, the packet  is routed in an arbitrary direction. This adaptation has not been  analyzed from the deadlock perspective.  (a) Packet switch Incoming packet Destination Packet_type Valid Data  (b) Circuit switch North Connection registers West Destination  selector Curent address connections N E S a o o s u e t r t s t t W h h Self Packet type East i M P C R C e o r c g n u o g n i f i U t i s w i t c h r e g s t i e r South (c) Packet fields for the packet switch Packet_type (2 bits) Destination (8 bits) Valid (1 bit) Data (16 bits) (d) Data field interperation depending on packet type Packet type Data field interperation PSw Config Active connection for north, south, east and west ports (4 MSBs) connections Region config Data sent directly to the region Csw Config 3 bits to configure the connections between 5 ports of the circuit switch PMU (Future) Figure 11: The dual Global NOC Scheme in SiLago GNOC  Each packet is 32-bit wide and the used packet fields are shown  in Fig. 11c. The two bit packet type field decides the four types of  payloads shown in Fig. 11d:  1) PSw config packet carries the  payload for the configuration register mentioned above for adapting  DOR for arbitrary topology, 2) Region config packets carry the  configware for DRRA, DiMArch cells in a region instance deciding  their interconnectivity, DPU modes, AGU configuration etc. 3)  CSw config packet configures the circuit switch by identifying how  each input will be connected to the output, and finally 4) the PMU  (power management unit) packets are not used at the moment but  we are in the process of adapting our previous work on fine grain  power management [21, 22].  Circuit-switch: The circuit switch provides predictable, low  power, and high performance data transfer in systempaths. The  circuit switch is shown in Fig. 11b. It provides full duplex  7                            NOCS 2017, Oct. 2017, Seoul, South Korea  A. Hemani et al.  connections that are configured during the systempath creation, by  the CSw packet payload carried by the packet switch. A circuit  switch can transfer a data packet between the regions either in a  single cycle or multiple cycles, in which case the NOC wires are  pipelined.  Network Interface Units: The NIUs besides serving their  conventional role as a gateway between the regional and global  NOCs, also serves a key role in the Globally Ratiochronous and  Locally Synchronous (GRLS) [5, 21, 22] clocking scheme adopted  in SiLago. All parts of global NOCs operate at the same clock  frequency, however the region instances could operate at a  rationally related clock frequencies. Global NOC, is also a valid  SiLago region. For this reason, GRLS synchronization interface is  present between every NIU and the associated NOC switch.  To conclude, we have so far shown how imposing synchoricity  on regular CGRA regions glued at two levels by NOCs raises the  physical design abstraction to RTL and we can generate the leaf  nodes in Fig. 4 that are also the points in higher abstraction space  in Fig. 2 and evaluate them with post layout accuracy. These were  the three requirements listed in section I. We next discuss how these  advantages can be exploited by a design flow to generate custom  functional hardware design with programming like effort.  4   SILAGO DESIGN FLOW  The present SiLago design flow, shown in Fig. 12, transforms  applications (hierarchy of algorithms) modeled as SDF in Simulink  to GDSII to meet the constraints on sampling rate and total latency.  This application level synthesis (ALS) relies on two, onetime  engineering efforts to build the SiLago physical design platform  and a library of function implementations as shown in Fig. 12.  4.1 Onetime engineering effort  The SiLago physical design platform development is based on  commercial EDA tools. These tools are used to harden, characterize  [18], make synchoros and abut-able the SiLago blocks. We have  outlined this is done in section II.E. This one-time engineering  effort results in  1) a set of SiLago blocks for DRRA, DiMArch,  Global NOC and system Controller (Leon), 2) post layout  characterization data that is exported to higher abstraction  syntheses tools [18] and 3) composition by abutment script.  The second onetime engineering effort is to build a library of  function  implementations – FIMPs. Each function has M  implementations, varying in architecture and degree of parallelism  and thereby in their cost-metrics. This is achieved by an in house  HLS tool called Vesyla [28]. These variations are the basis for the  design space exploration performed by the ALS. In effect, the FIMP  library raises the abstraction of physical design, one level higher,  to the algorithmic level.  4.2 SiLago Application Level Synthesis  The application level synthesis models applications as an SDF  graph. For each actor/function in SDF, it already knows the M  implementation alternatives and their costs from the FIMP library.  The ALS engine explores the design space at two levels. At the first  level, it explores in terms of most serial to most parallel SDF  schedules that are relative because the specific FIMP type has not  been selected yet. At the second level, for each SDF schedule, it  formulates a constraints satisfaction problem in terms of data  dependencies (from the SDF graph), resource dependency (from  the scheduled SDF graph) and cost of each solution coming from  the FIMP library. If there are L function/actors in the SDF model  and each function can be implemented in M different ways, there  are ML possible solutions. Each such solution represents a unique  combination of FIMP types, one for each actor. And each solution  8  can evaluated with certainty because the physical design for each  such solution is known and the functionality is compile time static  This enables automatically transforming complex O(multi-million)  gate applications to GDSII in minutes while exploring many  solutions. For the end user, the engineering effort is that of  modeling in Simulink, the rest is a onetime engineering effort that  is similar to standard cell library development and Synopsys’  DesignWare library development effort. These onetime efforts, we  note, are not factored in when calculating the SOC development  costs as is done in [33].  One Time Engineering Effort Parametric Pragma  Annotated  MATLAB Models of  functions FIMP = Function Implementation SiLago HLS FIMP Library M FIMPs SiLago Physical Design Platform Characterization Data Hardened, Synchoros Abut-able Blocks,  Scripts Logic & Physical  Synthesis Standard Cell Library Application Modeled  as SDF in Simulink L Algorithms c Constraints:  Sampling Rate,  Total Latency 1. Select Optimal Solution from ML solutions 2. Global Interconnect, buffers and control 3. Floorplanning Number and types of SiLago blocks + Mapping Compose GDSII Macro GDSII Macro Reports Figure 12: The SiLago Application Level Synthesis (ALS) flow.  4.3 Going beyond Compile Time Static ALS  We are in the process of enhancing the present design flow to  deal with non-compile time static functionalities and also to be able  to deal with system-level abstractions. This involves enhancements  on three fronts: modeling, architectural support and optimization  engine. For modelling, we are adopting Scenario Aware Dataflow  Graphs (SADFG) and Kahn Process Networks (KPNs) to deal with  non-compile time static functionalities. We are designing SiLago  blocks for control oriented functionality, FIFOs to connect KPN  processes and memory consistency logic to enable interacting  applications share memory safely. The essence of  these  architectural enhancements is to raise the abstraction of the  vocabulary spoken by hardware at physical design level. The  optimization engine will be hierarchical, applications will be  synthesized in varying degrees of parallelism and then system-level  synthesis will select the right mix. The optimization engine will use  profiling information to cope with non-compile time static  functionalities. With these enhancements, we also hope that  Moore’s EDA law [30] will be validated in moving to reusing large  grain objects to close the design productivity gap.  5   VALIDATION OF THE SILAGO METHOD  In this section, we share some early results that goes towards  validating the claim that the SiLago method can indeed fulfill the  orders of magnitude improvement in three aspects of VLSI design  automation listed in section 1.  5.1   Quantification of SiLago Benefits  To quantify the SiLago benefits, we took a number of  applications and processed them through the SiLago ALS. We also  synthesized the same application using commercial HLS tools by  synthesizing one algorithm at a time and manually refining the  interface between algorithms because HLS  tools do not  automatically generate the interface, see [4] for more details on this.  As can be seen in Fig. 13, SiLago outperforms standard cell based  HLS by 2 orders in accuracy of predicting energy, the most difficult  cost metric. Not only is SiLago more accurate, it is also more agile  as shown Fig. 13b, it is able to evaluate ~100 solutions in a few  minutes. The corresponding numbers for standard cell based HLS        Synchoricity and NOCs could  make Billion Gate Custom  Hardware Centric SOCs Affordable  NOCS 2017, Oct. 2017, Seoul, South Korea  are not reported as it takes unreasonably long time, O(months), to  synthesize ~100 JPEG encoders down to physical design. This two  orders improvement in accuracy is a direct consequence of SiLago  ALS having access to the post layout cost metrics, whereas  standard cell tools have to estimate them. This validates the third of  the three requirements in section I.   The three orders improvement in synthesis speed, again comes  from the fact that SiLago generates VLSI designs by abutting  without requiring any further logic and physical syntheses and  configuring SiLago blocks and NOCs to generate the design  alternatives; this validates the first and the second of the three  requirements in section I.  The exponentially reduced space of SiLago is further reduced  by the FIMP library as it virtually raises the physical design  abstraction to algorithmic level. A proof of this is shown in Fig. 13c  where one can see that the ALS explores space in three dimensions  in terms of number and types of FIMPs and not in number and types  of micro-architecture level blocks as the HLS tools do. This further  validates the first requirement.  Figure 13: Quantifying the benefits of SiLago ALS  We emphasize that while the speed and accuracy of synthesis  are important benefits but the machine translation of fairly complex  functionalities designed into a manufacturable design is the key  benefit because it provides correct by construction guarantee and  eliminates the need for functional verification, the major cost  component in SOC engineering cost. The accurate cost prediction  though does help in reducing the number of expensive iterations  through the synthesis runs.  5.2   The SiLago Overhead Analysis  The benefits of SiLago come with modest overhead in area and  energy that we quantify and justify in this section. The  quantification is shown in Fig. 13d and was found by retaining the  same degree of arithmetic and memory access level parallelism for  the standard cell based design flow for fair comparison. These  overhead primarily comes from underutilization of coarse grain  reconfigurable resources and enforcing synchoricity.   In some cases, the coarse grain reconfigurability turns out to  have an advantage over ASIC implementations. For CNN and Face  Recognition in Fig. 13d, SiLago [27] has better energy numbers  because  it exploits  its agile and efficient coarse grain  reconfigurability to dynamically morph into optimal microarchitecture for their widely varying computation, storage and  interconnect needs, whereas ASIC implementations can only be  optimized for a few critical layers. The multi-mode accelerator Fig.  13d is an industrial example [25], where once again SiLago’s area  numbers are better because the SiLago solution could reuse the  same silicon to map two time exclusive functionalities that the  ASIC design could not.  The overhead of SiLago are compared to ASICs, however  compared to software implementations, SiLago provides 3 orders  improvement  in performance and computational efficiency  compared to GPUs [27] and we note that the SiLago solutions are  generated with comparable engineering effort. All it takes to  generate a SiLago SOC is a programming like effort in Simulink or  similar abstraction and a complete manufacturable design can be  potentially generated. Note that, the way embedded systems build  automatically include a specific configuration of infrastructural  software like RTOS, drivers etc., SiLago can also include the  necessary infrastructural SiLago blocks for system control, clock  generation, power management etc. to generate a ready to  manufacture SOC; this has not been done but is eminently possible.   When standard cells were adopted three decades ago, the  penalty compared to full custom macro based design flow that it  replaced was much higher. [32] showed that the same NVidia GPU  design required 300% less power if implemented in custom  compared to standard cell based ASIC. There is also evidence to  show a significant improvement in silicon efficiency as well. In  spite of these overheads, standard cells have been accepted and the  irony is that today ASICs are the benchmark for efficiency in both  silicon and computational efficiencies. Standard cells became  acceptable long back because they showed ~70 % improvement in  logic and physical synthesis [31]. We argue that SiLago provides  significantly more, 2-3 orders improvement in engineering  efficiency and the SiLago designs are coarse grain reconfigurable  and thus retain the flexibility of software.  Another parallel we draw in justifying the overhead of  synchoros SiLago  is  the  synchronous digital  systems.  Synchronicity has enabled static timing analysis that brings  robustness to the VLSI engineering that has enabled design of  complex O(100) million gate digital designs. Synchronicity also  has an overhead that we have accepted because the discipline it  enforces enables coping with the complexity.  Figure 14: Synchronous systems underutilize clock period  We have quantified the overhead of synchronicity defined as the  percentage of clock period used by different paths. This  quantification is shown in Fig. 14 for two designs: Leon processor  and DRRA+DiMArch. As can be seen, the way synchoros coarse  grain reconfigurable SiLago underutilizes its architecture in spatial  dimension, the two synchronous designs underutilizes its clock  period in temporal dimension. To enable static analysis, we  assumed that all paths are equally likely to derive a weighted  average, shown in Fig. 14 as the orange line. The weighted average  shows that Leon and SiLago underutilizes their clock periods by  75% and 65% respectively. Assuming that ASICs utilize the silicon  100%, the average area underutilization by SiLago is 35% for the  examples shown in Fig. 13d. Both synchronicity and synchoricity  (d) Normalized Energy and Area overhead  of the Designs generated by the SiLago ALS 2.0 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Area Energy 0 0.5 X 107 1 1.5 2 2.5 3 3.5 3 X 106 4 4.5 0 50 100 150 a S m e p l I n t e r a v l Nr of  FIMPs 300 200 100 (c) Three Dimensional DSE plot of LTE  performed by the SiLago ALS (a) Energy Estimation Error (%) 100 101 102 103 3 6 9 % 2 2 3 % 2 8 2 % 8 3 . % 5 9 . % 8 1 . % 5 5 . % 1 8 5 % Standard cell based flow SiLago flow 4 6 % 7 3 % 6 2 . % 3 9 . % o # f u o S l i t n o a u a v e s l t d e 90 80 70 60 50 40 30 20 10 0 T i m e r u q e r d e f o r D E S i d n o c e S n s 225 200 175 150 125 100 75 50 25 0 (b) Agility of Design Space Exploration of  the SiLago Application Level Synthesis 40 30 20 10 0 2000 1500 1000 500 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 W h g e i z t e n v o A d a e r e g a U t i l i t i % o # f a P t h s Percentage of clock period SiLago cell 0 10 20 30 10000 8000 6000 4000 2000 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 1 0 W h g e i z t e n v o A d a e r e g a U t i l i t i % o # f a P t h s Percentage of clock period Leon 3 9                                                                                  NOCS 2017, Oct. 2017, Seoul, South Korea  A. Hemani et al.  have same order of underutilization of their respective domains -  time and space.  We argue that these overheads are justified because it enables  us to explore a more rich design space at higher abstractions and  cope with complexity that would not be possible with synchronicity  or synchoricity. Synchoricity also has the potential to reduce the  manufacturing cost as we argue next.  6 LOWERING MASK ENGINEERING COST  A key NRE cost in manufacturing chips is designing the mask  patterns that has become increasingly challenging in smaller  geometries because of multiple lithographic impairments that must  be factored in. SiLago based designs can virtually eliminate this  NRE cost because all SiLago designs are made up of a finite  number of SiLago block types and each SiLago block type can only  have finite number of neighbor types. If a lithographically corrected  component mask is made for each SiLago block type and for each  combination of neighbor types in a library as a one-time  engineering effort, the composite mask of an arbitrary SiLago  design instance can be created from such component masks. This  has not been validated yet that we hope to do in near future. If we  succeed, the case for adopting synchoros design style will be even  more compelling.  7 CONCLUSION  We have shown that synchoricity enables a parametric physical  design scheme that raises the abstraction of physical design to RTL  that enables rapid generation of arbitrary VLSI design instances by  abutment that does not require any further logic or physical  syntheses. NOCs and coarse grain reconfigurability enables  generation of  complex datapaths  and  systempaths  as  implementation alternatives. The cost metrics of these alternatives  can be evaluated with agility and post layout accuracy. An  application level synthesis flow has been shown to exploit these  properties to map moderately complex applications all the way  down to GDSII in minutes while exploring many solutions. This  flow can be extended to handle system-level functionality of  O(100-1000) million gates with an engineering effort for the end  user comparable to that of programming.  ACKNOWLEDGMENTS  This work has been supported by Vinnova, Sweden through  CREST II Project.  "
3D NoC-Enabled Heterogeneous Manycore Architectures for Accelerating CNN Training - Performance and Thermal Trade-offs.,"As deep learning technology is increasingly employed in diverse applications domains, the demand for computational power to enable these algorithms also increases. In this respect, high-performance three-dimensional (3D) heterogeneous manycore systems present a promising direction. However, deep learning on these systems pose several design challenges. First, the network-on-chip (NoC) must handle the traffic requirements of both CPU and GPU communications. Second, 3D system designs must address thermal issues resulting from high-power density. In this work, we propose a design methodology for a heterogeneous 3D NoC architecture that not only satisfies the traffic requirements of both CPUs and GPUs, but also reduces thermal hotspots. To this end, we target the training of two widely employed convolutional neural networks (CNN), namely, LeNet and CIFAR. By using our joint performance-thermal optimization methodology to create a 3D NoC for training CNNs, we reduce the maximum temperature by 22% while incurring only 5% full-system energy-delay-product degradation over a solely performance optimized 3D NoC. This demonstrates that, our design methodology achieves considerable temperature reduction with negligible loss in performance.","3D NoC-Enabled Heterogeneous Manycore Architectures for  Accelerating CNN Training: Performance and Thermal Trade-offs   B iresh Kumar Joardar*, Wonje Cho i*, Ryan Gary Kim† , Janardhan Rao Doppa*, Partha Pratim Pande*, Diana Marculescu† ,  Radu Marculescu†  *Schoo l o f EECS, Washing ton State Universi ty   †ECE Department, Carneg ie Mellon Universi ty   Pullman, WA 99164, U .S.A.   Pisburgh, PA 15213, U .S.A.  {bjoardar, wcho i1, jana, pande}@eecs.wsu.edu  {rgkim, d ianam, radum}@cmu.edu  ABSTRACT  As deep learning technology is increasingly employed in diverse  applications domains , the demand for computational power to  enable these algorithms also increases . In this respect, highperformance three-dimensional  (3D) heterogeneous manyco re  systems present a promising direction. However, deep learning on  these systems pose several design challenges . First, the networkon-chip (NoC) must handle the traffic requirements of both CPU  and GPU communications . Second, 3D system designs must  address thermal issues resulting from high-power density. In this  work, we propose a design methodology for a heterogeneous 3D  NoC architecture that not only satisfies the traffic requiremen t s  of both CPUs and GPUs , but also reduces thermal hotspots . To this  end, we target the training of two widely employed convolutiona l  neural networks (CNN) , namely, LeNet and CIFAR. By using our  joint performance-thermal optimization methodology to create a  3D NoC for training CNNs , we reduce the maximum temperatu re  by 22% while incurring only 5% full-system energy-delay-prod u c t  degradation over a solely performance optimized 3D NoC. This  demonstrates that, our design methodology achieves considerabl e  temperature reduction with negligible loss in performance.  CCS CONCEPTS  • Hardware → 3D integrated circuits; Network on Chip;  ermal optimization; • Computer systems organization →  Neural networks ; • eory of computation → Optimizatio n  with randomized search heuristics  KEYWORDS  Manycore, Heterogeneous NoC, CNN, 3D, Thermal Optimization  ACM format:  B. K . Joardar, W. Choi, R. G. K im, J. R. Doppa, P. P. Pande, D.  Marculescu,  and R. Marculescu. 2017 . 3D NoC-Enabled  Heterogeneous Manycore Architectures for Accelerating CNN  Training: Performance and Thermal Trade-offs . In Proceedings of  NOCS ’17, Seoul, South Korea, October 2017 (NOCS’17), 8 pages .  https ://doi.org/10 .1145 /3130218.3130219  Permis s ion to make d igital o r hard co p ies o f all o r part o f this wo rk fo r pers onal o r  clas s ro om us e is granted w ithout fee pro vid ed that cop ies are no t mad e o r d is tributed  fo r pro fit o r commercial advantage and that cop ies bear this no tice and the full  citation on the firs t page. Copy rights fo r components o f this wo rk owned by o thers  than ACM mus t be hono red . Abs tracting w ith cred it is permitted . To copy o therw is e,  o r republis h, to po s t on s ervers o r to red is tribute to lis ts , requires prio r s pecific  permis s ion and /o r a fee. Reques t permis s ions from Permis s ions@acm.o rg.  NOC S '17, Octo ber 19–20, 2017, Seoul, Republic o f Ko rea  © 2017 As s o ciation fo r Computing M achinery .  ACM ISBN 978-1-4503-4984- 0/17/ 10…$ 15.0 0  1 INTRODUCTION  Recent advances in deep learning technology are increasing l y  employed to automate data analysis tasks, e.g . , classification,  regression, feature extraction, and dimensionality reduction [1].  These deep learning applications have spanned diverse domains  including speech processing, computer vision, natural languag e  processing, and genomics . Subsequently, a significant amount of  effort has been put into developing software and hardware tools  that can handle progressively larger deep learning models and  datasets .  Consequently, current state-of-the-art research has led  to advances that allow us to run deep learning application s  without utilizing high performance computing clusters . In  particular, GPUs with many parallel processing cores have the  capacity to tremendous ly accelerate the training of deep neura l  networks . In this work, we consider the problem of designing  advanced manycore  architectures  to  efficiently  train  Convolutional Neural Networks (CNNs) , a popular class of deep  learning architectures used in computer vision and  image  processing. The CNN training process is highly data- and  compute-intensive  in nature with a high degree of data  parallelism; each iteration involves large number of complex  vector and matrix computations . Hence, large GPU-based systems  are the preferred platforms for training CNNs . Aside from the  high data parallelism, CNN training also involves high volumes of  data exchange between the CPUs and GPUs , mainly due to the  forwarding and storing of data between adjacent CNN layers . For  discrete GPU systems , this communication is carried out via offchip interconnects (e.g . , PCIe) that exhibit high data-transfe r  latency and power consumption [2]. A heterogeneous single chip  multiprocessor  (CMP) where  the CPUs and GPUs are  interconnected through the on-chip network would avoid such  expensive off-chip data transfers .   To reduce the performance bottlenecks arising out of the  planar interconnects , three-dimensional (3D) integrated circuits  ( IC) have been introduced [3 -6 ]. By stacking planar dies on top of  one another and connecting them with Through-Silicon Vias  (TSVs) , the communication  latency can be greatly reduced.  However, since 3D ICs have considerably higher power density  than their 2D counterparts , the manycore system must be  optimized by examining both the performance and the therma l  effects of the manycore system. In addition to 3D ICs , the  Network-on-Chip  (NoC) paradigm  has  emerged  as  a  revolutionary methodology for integrating many embedded cores  in a single die. Together, NoC architectures for 3D ICs offer an                              NOCS’17 , October 2017 , Seoul, South Korea  B. K . Joardar et al.  unprecedented performance gain beyond the Moore’s law regime.  I t has already been shown that 3D NoCs enable the design of a  low latency and high throughput communication backbone for  manycore chips [7 ].  Hence, we conjecture that the 3D NoC is a  suitable communication backbone to enable heterogeneo u s  manycore architectures for training CNNs . In this work, we  propose a design methodology to optimize the performance and  thermal characteristics of 3D NoC-enabled heterogeneo u s  manycore platforms . As a case study, we evaluate our proposed  methodology to create an optimized manycore platform to  perform the necessary training for deep learning applications . Our  major contributions are as follows :  • We undertake a comprehensive design and performance  evaluation of 3D heterogeneous NoC arch itectures tailormade to train CNNs for deep learning applications. To the  best of our know ledge, th is is the first eﬀort to design and  optim ize 3D NoCs specifically targeted for deep learning.   • We demonstrate the eﬀicacy of the proposed heterogeneou s  3D NoC in terms of performance-thermal trade-oﬀs to train  the CNNs for deep learning applications.   • We evaluate the role of CPU, GPU, and memory controller  (MC) placement in the proposed 3D NoC on performance  and temperature profiles.   The rest of this paper is organized as follows : Section II  discusses the prior work. Section I I I discusses the problem of  training CNNs on heterogeneous manycore platform. Section IV  formulates  the problem of designing a 3D heterogeneo u s  manycore  system  and describes  the proposed design  methodology.  In Section V, we present experimenta l l y  demonstrate  the effectiveness of  the proposed  thermal performance joint optimization methodology and the efficiency of  the optimized NoC architecture over traditional 3D mesh. Finally,  in Section VI , we conclude the paper by summarizing  the  contributions of this work.  2  RELATED WORKS  Deep  learning has seen great success in a wide-range of  applications  [1 ]. A NoC-enabled homogeneous manyco re  architecture targeting neural network applications has alread y  been explored [8]. The authors in [9] designed an accelerator that  sped up computations common in many machine  learni ng  algorithms to achieve higher performance and lower energ y  dissipation than a GPU-based system. Prior works on discrete  GPU platforms have  focused on  improving  the system  performance by enhancing their NoC architectures [10 ,11].   Data parallel applications like deep learning have been shown  to be best suited for an NoC-enabled heterogeneous CMP platform  consisting of CPUs and GPUs to reduce expensive off-chip CPUGPU data transfers [2 ]. Due to the differences in the thread-lev e l  parallelism of CPUs and GPUs , the NoC designed  for  heterogeneous systems should satisfy both CPU and GPU  communication constraints [12]. Hence, designing the 3D NoC for  the heterogeneous  systems  is more  complicated  than  homogeneous systems . Additionally, since GPU cores only  exchange data with a few shared memories , an NoC for  heterogeneous systems was proposed to efficiently handle many 2  to-few traffic patterns ( i.e. , many GPU cores communicating with  a few MCs) [10 ,11]. The shared memory resources  in a  heterogeneous system are often monopolized by the GPUs ,  resulting in high CPU memory access latency [13]. Conventiona l  2D NoC architectures , such as mesh, cannot handle many-to-few  traffic patterns and cannot fulfill the QoS requirements for both  CPU and GPU communications [14].  In recent years , designers have taken advantage of 3D IC’s  higher packing density and lower interconnect latency to improv e  the performance of manycore systems [3 -6]. However, most of the  existing 3D architecture  research  focuses on homogeneou s  manycore systems with stacked memory [5 ,15]. These existing  works mainly attempt to utilize the vertical interconnects to  reduce memory access  latency and  improve performanc e .  Advantages and challenges of designing 3D IC-based GPU  architectures have been explored [16]. These prior works have  principally focused on improving the GPU-memory throughpu t  and improving energy efficiency by using the benefits of 3D  integration. However, NoC designs for 3D heterogeneo u s  platforms have not been explored yet.   Unfortunately, due to the higher packing density of 3D ICs,  these systems suffer from thermal issues due to higher power  density. Several prior works have tried to address the therma l  issues in 3D systems . One of the popular methodologies for  reducing the peak temperature in a 3D architecture is to preven t  high power cores from being placed directly on top of one anothe r  [5 ]. Another method is to minimize the overlap between high  energy consuming regions using proper floor-planning [17].  Temperature-aware  task scheduling strategy has also been  proposed to address thermal issues in 3D systems [18].   In this work, we improve the state-of-the art by proposing a  design methodology for a 3D heterogeneous NoC architectu r e  that satisfies both CPU and GPU communication requiremen t s  while minimizing the peak temperature. As a case study, we  undertake a detailed performance evaluation of the proposed 3D  heterogeneous NoC to train CNNs for deep learning applications .  We provide a comprehensive analysis of the performance-the rma l  trade-offs for the proposed 3D NoC.  3  CONVOLUTIONAL NEURAL NETWORKS  A CNN consists of a combination of convolutional layers , pooling  layers , and fully connected layers . Convolution layers are the key  building blocks and perform the most expensive computations of  a CNN. Each set of weights is convolved across the inputs ,  computing the dot product between the weights and the inputs to  generate an activation map. Pooling layers do a spatial-down sampling of the input matrix by using an aggregation function,  e.g . , max or average. The fully connected layers connect all  neurons in a layer to every neuron in the previous layer. These  fully connected layers normally occur at the end of a CNN and  perform classification.   samples , {(𝑥𝑥1, 𝑦𝑦1 ), … , (𝑥𝑥𝑇𝑇 , 𝑦𝑦𝑇𝑇 )} , each sample 𝑥𝑥𝑘𝑘 is used as input  CNNs employ the backpropagation algorithm to train the  network and learn the weights . Given a training dataset of T  into the CNN with weights to obtain 𝑦𝑦�𝑘𝑘 ( forward pass) . I f 𝑦𝑦�𝑘𝑘 ≠       3D NoC-Enabled Heterogeneous Manycore Architectures   NOCS’17 , October 2017 , Seoul, South Korea  3  𝑦𝑦𝑘𝑘 , the weights are adjusted via stochastic gradient descent  (backward pass) . This procedure is repeated until the number of  prediction errors is minimized. During the forward and backward  passes , a lot of data parallelism exists and hence, execution can be  significantly accelerated using GPU cores .   In this work, we consider two widely employed CNN  architectures , namely, LeNet [19] and CIFAR [20]. As we  mentioned above, CNN architectures consist of multiple layers ,  and each layer involves unique computation and memory access  patterns . However, as shown in Fig. 1 , when training the CNN on  a heterogeneous system, greater than 80% of the total traffic is  between the memory controllers (MCs) and the processing cores  (MC-core) . Since the number of MCs is small, all CNN layers  exhibit many-to-few traffic patterns during training. Therefore ,  the NoC should be designed such that it efficiently handles this  many-to-few traffic patterns .    4 3D NoCs FOR DEEP LEARNING   A heterogeneous manycore architecture consists of multipl e  CPUs , GPUs , and MCs . A few MCs are shared among CPU and  GPU cores , and provide a unified virtual memory space for the  processors . Each MC incorporates a last level cache and a  mechanism to access the main memory. Each processing core in  the system maintains its own L1 cache and mainly communicat e s  with the few MCs . In the considered 3D architecture, the bottom  layer is the closest to the heat sink while the top most layer is the  furthest. Fig. 2  illustrates an example 3D heterogeneo u s  architecture with two layers .   4.1 Problem Formulation  In this 3D heterogeneous system, most of the traffic comes from  the processing core’s (CPU and GPU) L1 caches exchanging data  with the shared MCs [10]. As Fig. 1 illustrates , the deep learni ng  applications running on the proposed heterogeneous system  create many-to-few communications . Therefore, the NoC for  these heterogeneous manycore architectures should be well  equipped to handle many-to-few communication patterns [14].  Additionally, CPU and GPU cores have differing communicati o n  requirements . The CPU-MC communication is primarily latency  sensitive whereas the GPU-MC communication is throughpu t  sensitive [12]. CPUs require low memory access latency to avoid  execution time penalties . GPUs on the other hand require high  throughput to handle the large amount of data exchanges to and  from the MCs . The NoC for the heterogeneous platform should  satisfy both CPU and GPU communication requirements while  efficiently handling the many-to-few traffic patterns .   4.1.1 GPU Communication Objective.  To fulfill the GPU  communication requirements , the throughput of the NoC should  be maximized. This is accomplished by minimizing the mean link  bottlenecks [14]. The expected link utilization of link k (𝑈𝑈𝑘𝑘) and  utilization while ensuring the system is free from the bandwidth  the mean link utilization (𝑈𝑈�) for a NoC with R routers and L links  is given by:  (1 )  𝑈𝑈𝑘𝑘 = � ��𝑓𝑓𝑖𝑖𝑖𝑖 ⋅ 𝑝𝑝𝑖𝑖𝑖𝑖𝑘𝑘� 𝑈𝑈� = 1𝐿𝐿 � 𝑈𝑈𝑘𝑘 𝑅𝑅 𝑖𝑖=1 𝐿𝐿 𝑘𝑘=1 𝑅𝑅 𝑖𝑖 =1 where 𝑝𝑝𝑖𝑖𝑖𝑖𝑘𝑘 = 1 if nodes i and j communicate via link k, and 𝑝𝑝𝑖𝑖𝑖𝑖𝑘𝑘 = (2 )  0 otherwise, 𝑓𝑓𝑖𝑖𝑖𝑖  denotes the frequency of interaction between  node i and j. These 𝑓𝑓𝑖𝑖𝑖𝑖 values capture the many-to-few traffic  generated by training the CNN on the heterogeneous system.   Figure 1: Traffic characteristics for training two CNNs: (a) CIFAR (b ) LeNet. Each bar shows the distribution of traffic go ing  to and from the MCs (MC-core) and the traffic between any processing core (core-core). The traffic distribution is shown for  each layer of the CNN (conv - Convolution, pool - Pooling, full - Fully Connected). Since there are substantially more  processing cores than MCs, this shows that the traffic is heavily many-to-few in these applications.  100% 75% 50% 25% 0% c o n 1 v o o p l 1 c o n 2 v o o p l 2 c o n 3 v o o p l 3 f u l l 1 f u l l 1 o o p l 3 c o n 3 v o o p l 2 c o n 2 v o o p l 1 c o n 1 v % o f T r a f f i c MC- core core-cor e (a ) 100% 75% 50% 25% 0% c o n 1 v o o p l 1 c o n 2 v o o p l 2 c o n 3 v f u l l 1 f u l l 2 f u l l 2 f u l l 1 c o n 3 v o o p l 2 c o n 2 v o o p l 1 c o n 1 v % o f T r a f f i c MC- core core-cor e (b) Figure 2: Overview of the TSV-based 3D system considered  in this work. The system is divided into CPU, GPU, or MC  tiles. Tiles are interconnected via a planar link (intra-layer)  or a TSV (inter-layer). This figure is for illustration  purposes only; it is not optimized for any parameter.                          (3 )  𝐿𝐿𝑘𝑘=1 𝐶𝐶 𝑖𝑖 =1 𝑀𝑀 𝑖𝑖=1 𝜎𝜎 = � 1𝐿𝐿 � (𝑈𝑈𝑘𝑘 − 𝑈𝑈�)2 𝐻𝐻 = 1𝐶𝐶 ∗ 𝑀𝑀 � ��𝑟𝑟 ⋅ ℎ 𝑖𝑖𝑖𝑖 + 𝑑𝑑𝑖𝑖𝑖𝑖 � ⋅ 𝑓𝑓𝑖𝑖𝑖𝑖 NOCS’17 , October 2017 , Seoul, South Korea  deviation of link utilization (σ) in (3 ) along with 𝑈𝑈�.  To reduce bandwidth bottlenecks , we ensure the  link  utilization in an NoC is well balanced by minimizing the standard  In addition to fulfilling the GPU communication requirement s ,  the frequency of interactions , 𝑓𝑓 ) throughout the 3D NoC.  this naturally balances the many-to-few traffic (represented by  4.1.2 CPU Communication Objective.  To fulfill the CPU  communication  requirements ,  the CPU-MC communicati o n  latency should be minimized. The CPU-MC communicati o n  latency can be estimated using the average traffic-weighted hop  traffic-weighted hop count (H) between all CPUs and MCs . For a  system with C number of CPUs and M number of MCs , we can  find H between CPU and MC cores by the following equation:  where, 𝑟𝑟 is the number of router stages , ℎ 𝑖𝑖𝑖𝑖 is the number of links  (4 )  traversed from CPU i to MC j, and 𝑑𝑑𝑖𝑖𝑖𝑖 indicates the link length.   4.1.3 Thermal Objective.  One of the key challenges in 3D  integration  is the high-power density and  the resulting  temperature hotspots . Therefore, it is pertinent that the 3D  heterogeneous system design minimizes the peak temperature .  Cores that are further from the sink tend to have highe r  temperatures than those close to the sink. Therefore, cores  consuming higher power should be properly placed so that the  peak temperature is reduced without sacrificing performance. The  proposed 3D NoC should address these thermal constraints  inherent in any 3D system.  In order to quickly model the thermal properties of the  system, the fast approximation model from [21] can be utilized.  This model considers the vertical heat flow and horizontal heat  flow separately. When considering only the vertical heat flow, the  manycore system can be divided into N single-tile stacks , each  a single-tile stack n located at layer k from the sink (𝑇𝑇𝑛𝑛,𝑘𝑘) due to  with K layers , where N is the number of tiles on a single layer and  K is the total number of layers . The temperature of a core within  the vertical heat flow is given by:  where 𝑃𝑃𝑛𝑛,𝑖𝑖 is the power consumption of the core at layer i from  (5 )  the sink in single-tile stack n, 𝑅𝑅𝑖𝑖 is the thermal resistance in  vertical direction, and 𝑅𝑅𝑏𝑏 is the thermal resistance of the base  layer on which the dies are placed [21]. The values of 𝑅𝑅𝑖𝑖 and 𝑅𝑅𝑏𝑏  are extracted through the 3D-ICE model parameters [25].  When considering the horizontal heat flow, high-power tiles  should be spread out throughout the planar layer to avoid hotspot  difference at the same layer k (Δ𝑇𝑇(𝑘𝑘)) [21 ]:  clusters . This is represented through the maximum temperatu re  (6 )  Combining both horizontal and vertical heat flow models , our  objective becomes minimizing  the maximum  temperatu re  Δ𝑇𝑇 (𝑘𝑘) = max𝑛𝑛 𝑇𝑇𝑛𝑛,𝑘𝑘 − min𝑛𝑛 𝑇𝑇𝑛𝑛,𝑘𝑘  + 𝑅𝑅𝑏𝑏 � 𝑃𝑃𝑛𝑛,𝑖𝑖 𝑇𝑇𝑛𝑛,𝑘𝑘 = � �𝑃𝑃𝑛𝑛,𝑖𝑖 � 𝑅𝑅𝑖𝑖 𝑖𝑖𝑖𝑖 =1 � 𝑘𝑘𝑖𝑖 =1 𝑘𝑘𝑖𝑖=1 4  B. K . Joardar et al.  𝑇𝑇 = �maxn ,k 𝑇𝑇𝑛𝑛,𝑘𝑘� �max𝑘𝑘 Δ𝑇𝑇 (𝑘𝑘)�  𝐷𝐷 ∗ = 𝑀𝑀𝑀𝑀𝑀𝑀 (𝐷𝐷, 𝑀𝑀𝑂𝑂𝑂𝑂 = {Ū(𝑑𝑑), 𝜎𝜎 (𝑑𝑑), 𝐻𝐻(𝑑𝑑) , 𝑇𝑇 (𝑑𝑑)})  difference (6 ) . We define this objective as 𝑇𝑇:  calculated by (5 ) weighted by the maximum  temperatu re  We use 𝑇𝑇 as another objective in the performance-therma l multi(7 )  objective optimization (MOO) .  minimizes the mean link utilization (𝑈𝑈�) , standard deviation (𝜎𝜎),  4.1.4 Combined Optimization Objective.  At the end,  our aim is to find a 3D heterogeneous manycore design that  average traffic-weighted hop count between CPU and MC (H) and  temperature (T ) . We write our combined objective as follows :  where, 𝐷𝐷 ∗ is the set of Pareto optimal designs , D is the set of all  (8 )  possible 3D heterogeneous manycore system configurations ,  all objectives to evaluate a candidate design 𝑑𝑑 ∈ 𝐷𝐷 .  We also  MOO stands for the multi-objective solver, and OBJ is the set of  ensure that all 𝑑𝑑 ∈ 𝐷𝐷 are fully connected, i.e. , all source and  destination pair have at least one path between them.  A mesh-based 3D NoC architecture is ill-suited to achieve the  objectives discussed above, i.e. , efficiently handle many-to-few  communications , satisfy both CPU and GPU communicati o n  requirements ,  and  reduce  the peak  temperature.  In a  heterogeneous system, MCs can potentially become traffic  hotspots due to the many-to-few communication nature. Even for  a mesh-based 3D NoC architecture with optimized CPU, GPU, and  MC placements , there still exists a few links (connected to MCs)  that are heavily utilized when compared to the rest of the links  present in the NoC [14 ]. During high traffic, such links will  become bandwidth bottlenecks , negatively affecting the overal l  system performance. The presence of  these bandwidth  bottlenecks is due to the many-to-few communication patterns  and the inherent multi-hop nature of the mesh architecture, which  leads to high traffic aggregation at the intermediate routers and  links associated with MCs  4.2 Drawbacks of Mesh-based 3D NoCs   4.3 Proposed Design Methodology for 3D NoC  (3DHet)  To overcome the inherent  limitations of mesh-based NoC  architectures , we propose a design methodology to create a smallworld network enabled 3D heterogeneous NoC  architecture that: 1 ) optimizes both CPU and GPU communicati o n  requirements ; 2 ) balances the load of the 3D NoC under many-tofew traffic patterns ; and 3 ) minimizes the peak temperature of the  system. This design methodology focuses on the placement of the  CPUs , GPUs , MCs , and planar links . The goal of this design  methodology is to optimize the system along the three objectives  stated in Section IV.A.   Any MOO algorithm can be used to solve (8 ) , however, we use  Archived Multi-Objective Simulated Annealing (AMOSA) [22 ], an  MOO that has been shown to generate better solutions than many  popular MOO algorithms . AMOSA is a simulated annealing based  algorithm in which the optimization process is guided with the  help of an archive of solutions . In AMOSA, during each            3D NoC-Enabled Heterogeneous Manycore Architectures   NOCS’17 , October 2017 , Seoul, South Korea  d r o P i l a D E N e z m 0.73 0.7 0.67 0.64 Figure 3: Network EDP for 3DHet with different 𝒌𝒌𝒎𝒎𝒎𝒎𝒎𝒎  5 6 7 8 9 values, normalized with respect to 3DMeshperf .  kmax 𝑑𝑑̂ = 𝑎𝑎𝑟𝑟𝑎𝑎𝑚𝑚𝑚𝑚𝑚𝑚𝑑𝑑𝑑𝑑𝐷𝐷∗𝐸𝐸𝐷𝐷𝑃𝑃 (𝑑𝑑)  𝑠𝑠 . 𝑡𝑡 .   𝑇𝑇�𝑑𝑑̂ � ≤ 𝑇𝑇 ′  optimization step, a perturbation is created in one of the archive d  solutions to move the solution to a neighboring state. In this work,  since we are jointly optimizing the placement of the CPUs , GPUs ,  MCs , and the planar links , a neighboring state is defined by one  of the two actions : 1 ) swapping the locations of any two tiles  (CPUs , GPUs , or MCs) ; or 2 ) moving the location of a planar link  to any unoccupied location. TSV-based links connect the tiles in a  vertical layer giving rise to long range shortcuts needed in a smallworld network. Depending on the Pareto dominance conditions  of this new configuration and the current solutions , AMOSA then  configurations 𝐷𝐷 ∗ . Then, we pick the design solution that has the  updates the archive. After a specified number of iterations or after  AMOSA converges , we obtain a set of archived candidates NoC  best performance among the archive of solutions , given that it  satisfies the temperature constraint. In other words :  where 𝑑𝑑̂ is the chosen design that has the lowest energy-delay (9 )  product (EDP) among all solutions in the archive 𝐷𝐷 ∗ in (8 ) which  temperature in the NoC is 𝑇𝑇 ′ , it is decided by the designer’ s  satisfies the constraint  in (9 ) . The maximum allowed core  requirements . The temperature T (d) is determined through 3DICE simulation as explained later in Section V.A.  Since AMOSA does not enforce regular NoC connectivity, we  need  to  impose certain physical constraints on the 3D  heterogeneous manycore system design space. Therefore, we  generated 3DHet designs . 𝑘𝑘𝑎𝑎𝑎𝑎𝑎𝑎 = 2𝐿𝐿𝑅𝑅  specify the average number of ports per router (kavg) for the  where L denotes the total number of links in the NoC and R  denotes the number of routers . Also, we need to restrict the  maximum number of ports of a router (kmax) so that no particul a r  router becomes unrealistically large.   where 𝐿𝐿𝑚𝑚 is the number of links connected to the router associated  (11)  with core i. The MCs in a heterogeneous NoC are traffic hotspots  with heavy volumes of incoming and outgoing messages .  Increasing kmax allows the number of router ports attached to an  MC to increase, and hence, improves the bandwidth of the router  connected to the MC. However, high kmax values can lead to large  routers , which result in high network energy consumption. We  will investigate the effect of different kmax values on the EDP in  Section V.C.   𝐿𝐿𝑖𝑖 ≤ 𝑘𝑘𝑚𝑚𝑎𝑎𝑚𝑚, ∀ 𝑚𝑚  (10)  5  EXPERIMENTAL RESULTS  In this section, we evaluate the network and thermal performanc e  of the proposed 3DHet NoC.   5.1 Experimental Setup  We employ Gem5 -GPU [23 ], a heterogeneous  full system  simulator to obtain network and processor level information. We  modify the Garnet network within Gem5 -GPU to implement the  different NoC topologies . We also consider a three-stage router  architecture for all NoCs . The CPU operating clock frequency is  set at 2 .5 GHz while the GPU/Shader core clock frequency is 0 .7  GHz/1 .4 GHz. Our design methodology is valid for any router  architecture. The 3DHet architectures use Adaptive Layere d  Shortest Path (ALASH) routing [26]. The 3D mesh based NoCs use  XYZ-dimension order based routing. The memory system uses a  MESI two-level cache coherence protocol. Our methodology  explained in Section IV, is applicable for a system with any  number of CPUs , GPUs or MCs . In this work, we consider as an  example, a 36 -tile system comprised of 4 x86 CPUs , 8 MCs , and 24  NVIDIA Maxwell based GPUs arranged into a four-layer (3x3 per  layer) 3D architecture to demonstrate the effectiveness of our  optimization methodology. Each CPU and GPU have private L1  data and instruction caches . Each MC contains a 256KB s lice of  last level cache (LLC) . We employ GPUWattch [24] to obtain  detailed processor power profiles and 3D-ICE [25] to obtain the  thermal profile of the 3D systems under consideration. Due to the  high-power densities in a 3D IC, we incorporate micro-fluid based  cooling techniques to reduce the temperature of the cores .  5.2 Baseline Configurations  In this work, we provide two baseline configurations 3DMeshpe rf  and 3DMeshthe rm. The first configuration 3DMeshpe rf is a 3D meshthermal constraints , i.e. , 𝑇𝑇 ′ = ∞ for analytical purposes in (9).  based architecture optimized for performance without any  The second configuration 3DMeshthe rm is an optimized 3D mesh  architecture using the most aggressive thermal constraint for a  range is 63ᵒ𝐶𝐶 − 82ᵒ𝐶𝐶. Therefore, the values of 𝑇𝑇 ′ for 3DMeshpe rf  mesh-based design. From 3D-ICE simulations , we determine that  and 3DMeshthe rm are 82ᵒ𝐶𝐶 and 63ᵒ𝐶𝐶 respectively. Since we  the 3D mesh-based NoC architecture maximum temperatu r e  assume a 3D mesh topology, the optimization procedure focuses  only on tile placement and does not move any of the links . For the  performance evaluation, we consider two metrics : network  latency and EDP. The network EDP is defined as the product of  network latency and network energy consumption. I t unifies both  the terms into a single parameter.   5.3 Determining 3DHet Design Parameters  As described in Section IV.C, the 3DHet NoC designs obtained  using AMOSA have small-world-based connectivity. Since all  routers do not have the same number of ports , it is important to  enforce design constraints during the optimization process . In  kavg for 3DHet to be equal to that of a 3DMesh (𝑘𝑘𝑎𝑎𝑎𝑎𝑎𝑎 = 4 .17).  order to ensure fairness between 3DHet and 3DMesh NoCs , we set  Therefore, since we assign a router for each tile, the number of  5          NOCS’17 , October 2017 , Seoul, South Korea  B. K . Joardar et al.  d e z i l a m r P D E o N 1 0.9 0.8 0.7 0.6 0.5 EDP Tempera ture 90 80 70 60 50 e r u t a r e p m e T ) C ᵒ ( 3DHetmed 3DHet low Figure 4 : Network EDP for di fferent tempera ture constra ints .  3DHeth igh planar links in our 3DHet system is 48 . In order to determine the  optimal kmax, we find the set of candidate solutions using the  AMOSA-based methodology for kmax values varying from 5 to 9.  For each kmax, we select the solution that minimizes network EDP  𝑇𝑇 ′ = ∞) as shown in Fig. 3 . All values in Fig. 3 are normaliz e d  as defined by (9 ) with no temperature constraints imposed ( i.e.,  with respect to the network EDP of 3DMeshpe rf. Our results show  that as we increase kmax, the EDP decreases until kmax=7 . This  happens because for higher kmax values , more inter-rout e r  connections are allowed at each MC, leading to lower average hop  counts between the cores and the MCs . However, beyond kmax=7,  some routers become too large, leading to higher router energ y  without significant latency improvement, resulting in higher EDP.  Therefore, we design the 3DHet architecture with kmax=7 . In this  section, we present a detailed analysis to establish the necessary  performance and thermal trade-offs involving various 3D NoC  architectures considered here. To design the 3DHet architecture ,  we use the design parameters kavg and optimized kmax as  determined in Section V.C.  5.3.1 Thermal-Performance trade-oﬀs. To highlight the  performance-therma l  trade-off , we  consider multipl e  configurations with different thermal constraints . For various  maximum temperatures in our design space varies from 62ᵒ𝐶𝐶 to  80ᵒ𝐶𝐶 for both the applications . To establish the performanc e 3DHet NoC configurations , we observed that the range of  core temperatures as 𝑇𝑇 ′ = 62ᵒ𝐶𝐶, 71ᵒ𝐶𝐶, 80ᵒ𝐶𝐶 respectively as  thermal trade-offs we consider three representative designs from  this range. These three representative designs have maximum  obtained from 3D-ICE simulations . The three 3DHet NoC  architectures  corresponding  to  these  three maximum  temperatures are referred as 3DHet low, 3DHetmed, and 3DHeth igh  respectively. The thermal-performance tradeoffs can be gathere d  by comparing the EDP of the thermal-aware optimized designs ,  3DHet low, 3DHetmed and 3DHeth igh . Fig. 4 highlights  the  performance-therma l trade-offs for the CIFAR application. We  observe a similar trend for the LeNet application. We can see that  3DHeth igh , which has the least thermal constraints , shows the best  performance among all the NoC architectures considered here.  t e z d n u o a c i l m r p o o h N 1 0.9 0.8 0.7 3DHeth igh  Figure 6: Average inter-router hop count for different NoC  architectures with respect to 3DMeshperf .  3DMeshper f 3DHetper f 3DHetth erm Since 80ᵒ𝐶𝐶 is the upper limit on the temperature range, 3DHeth igh  effectively has no thermal optimization and is optimized for only  performance, thereby showing the best EDP among the three NoC  designs . This demonstrates that as we put tighter constraints on  temperature, the achievable performance degrades and 3DHet low  has the highest EDP among all the architectures considered here.   5.3.2 Comparative Performance evaluation.  For furthe r  analysis , we consider only two of the above mentioned 3DHet  architectures , namely 3DHeth igh and 3DHet low. 3DHeth igh exhibits  the best possible performance while 3DHet low exhibits the design  with the lowest maximum core temperature while maximizing  performance.  Since  is mainly optimized  for  performance, we refer it as 3DHetpe rf. Similarly, 3DHet low is  referred as 3DHet the rm.   We begin by investigating the performance of these two 3D  architectures and compare against the baseline NoC 3DMeshpe rf  while running both CIFAR and LeNet separately. Fig. 5 (a) and 5 (b)  show the network latency and network EDP for 3DMeshpe rf,  3DHetpe rf, and 3DHet the rm. From Fig. 5 (a) and 5 (b), we observe that  3DHetpe rf reduces the EDP by 35% and 29% over 3DMeshpe rf  executing CIFAR and LeNet respectively. 3DHet the rm shows a 25%  and 18% improvement in EDP compared to 3DMeshpe rf for both the  applications similarly.  The performance benefits of 3DHetpe rf with respect to its meshbased counterparts can be explained by considering the averag e  traffic-weighted  inter-router hop count. Fig. 6 shows that  3DHetpe rf and 3DHet the rm have lower average inter-router hop  count compared to the 3DMeshpe rf. Due to lower average hop  counts , 3DHet architectures achieve lower latency and energ y  consumption. We observe that 3DHetpe rf shows a 17 .5% reduction  in average hop count while 3DHetthe rm shows a 9 .5% reduction in  hop count over 3DMeshpe rf.   5.3.3 Thermal evaluation and analysis. To capture popula r  thermal management techniques for 3D architectures , e.g . , a nonstacking arrangement of high power cores in consecutive layers  [5], we create an architecture 3DHetnon -stac k by enforcing a  constraint to avoid stacking of high power consuming GPUs while  optimizing the NoC.   3DMeshpe rf 3DHetpe rf 3DHett he rm 3DMeshpe rf 3DHetpe rf 3DHett he rm d e z s e a i l u l m r a v o N 1 0 .75 0 .5 0 .25 0 (b) 1 0.75 0.5 0.25 0 d e z s e a i l u l m r a v o N (a ) 6  EDP Laten cy Figure 5: Network EDP and latency for 3DMeshperf , 3DHetperf , & 3DHetth erm. All values normalized with respect to 3DMeshperf .  La tenc y EDP                         3D NoC-Enabled Heterogeneous Manycore Architectures   NOCS’17 , October 2017 , Seoul, South Korea  7  Fig. 7 (a) shows the maximum core temperatures for the  different NoC configurations (3DMeshpe rf, 3DMeshthe rm, 3DHet pe rf,  3DHet the rm and 3DHetnon -stac k) . The following analysis pertains to  both CIFAR and LeNet applications . In all the NoCs considered in  Fig. 7 , we have adopted Reciprocal Design Symmetry (RDS) floorplanning [17] to reduce the direct overlap of core area as much as  possible. We observe from Fig. 7 (a) that 3DHetnon -stac k, where we  avoid stacking high power cores as much as possible, does not  reduce the core temperature as much as 3DHet the rm.We observe  that due to a high number of GPUs in a heterogeneous system, we  cannot avoid stacking of high power consuming GPUs in  consecutive layers completely. This results in 3DHetnon -stac k having  a few GPUs at the top layer to reduce stacking of GPUs causing  higher temperatures . On the other hand, 3DHetthe rm reduces  maximum core temperature by 15% over 3DHetnon -stac k and 24%  over 3DMeshpe rf since it does not place any GPU cores on the top  layer. Due to the performance-thermal  trade-off , 3DHet the rm’s  temperature  improvement  results in 11% EDP degradatio n  compared to the best-case performance observed in 3DHetpe rf  (shown in Fig. 7 (b) ) . We note that the EDP degradation of  3DHet the rm is negligible with respect to 3DHetnon -stac k. We have  shown that 3DHetthe rm reduces maximum core temperatu re  compared to the other NoC design choices . Therefore, we  conclude that 3DHet the rm achieves the best performance-the rma l  trade-off among all the architectures considered here.   Moreover, Fig 7 (a) shows that 3DMeshthe rm also achieves a 23%  reduction in maximum core temperature compared to 3DMeshpe rf  while sacrificing less than 15% performance as shown in Fig. 7 (b).  3DHet the rm achieves a similar reduction  in maximum core  temperature as 3DMeshpe rf while improv ing the EDP by 16% and  19% respectively for CIFAR and LeNet. This demonstrates that the  proposed 3D heterogeneous NoC improves both thermal and  performance profiles when compared to a mesh-based design.   As described in Section IV, GPU-MC communication  is  throughput-centric. To demonstrate that the proposed 3DHet therm  achieves better network throughput than the 3DMeshthe rm, we  present the Cumulative Distribution of link utilization for  3DHet the rm and 3DMeshthe rm running CIFAR in Fig. 8 . We observe  that more than 10% of links in 3DMeshthe rm have at least 2X highe r  link utilization than the mean. Moreover, some of the links are  utilized three times higher than the mean. During high traffic  instances , such links become bandwidth bottlenecks , leading to  performance degradation. On the other hand, in 3DHet the rm,  utilizations of most of the links are less than the mean link  utilization of the mesh. In 3DHet the rm the link utilization CDF  curve is shifted left compared to 3DMeshthe rm indicating that the  link utilization in 3DHetthe rm is better distributed. Thus , 3DHettherm  is relatively free from the bandwidth bottlenecks and achieve s  better system throughput. Similar trends are observed for LeNet.   Placement of cores and links play an important role in  determining  temperature and EDP. To better illustrate the  difference between 3DHetpe rf and 3DHet the rm, we show the  placements of cores and distribution of link in Fig. 9 . The link  distributions in both the NoCs are shown in Table 1 . For 3DHetpe rf,  majority of the links are spread over the middle two layers . For  3DHet the rm, the top two layers contain the majority of the links .  This is proportional to the number of MCs in each layer.  Additionally, it should be noted that in both 3DHet pe rf and  3DHet the rm, the bottom layer has the lowest number of links since  Figure 8 : CDF of 3DMeshtherm & 3DHettherm l ink u ti l iza tions .   100% 75% 50% 25% 0.5 1 1.5 2 2.5 3 3.5 4 P e r e c n t e g a o f i l k n s Link utilization w.r.t. 3DMeshtherm mean link utilization 3DMesht he rm 3DHett he rm CIFAR LeNet CIFAR LeNet Figure 7: Temperature profile and EDP for different NoCs: (a) Maximum system temperature and (b ) network EDP.  85 70 55 40 T e m p e r a t u r e ᵒ ( C ) 3DHetpe rf 3DMesht he rm 3DHett he rm 3DHetnon-st a ck 3DMeshpe rf (a ) 1 0.75 0.5 0.25 0 N o r m a i l e z d E D P 3DMeshpe rf 3DMesht he rm 3DHett he rm 3DHetpe rf 3DHetnon-st a ck (b) Figure 9: Tile placement and thermal map for (a) 3DHetperf                  (b ) 3DHetth erm (C: CPU core, G: GPU core, M: MC).  M M C C M C C M M G G M G M G G G M G G G G G G G G G G G G G G G G G G SINK G G G G G G M G M C C G M G G C M M G G G G MM G M C G G G G G G G G G SINK (a) 3DHetperf (b) 3DHettherm o L w T e m p H i g T h e m p Tab le 1: Link Distribution by Layer  No. , of Horizonta l l inks No. , of Vertica l l inks  3DHetperf  3DHettherm  3DHetperf  3DHettherm  Layer4  Layer3  Layer2  Layer1  8  18  19  3  22  19  5  2  9  18  18  9  9  18  18  9                                            NOCS’17 , October 2017 , Seoul, South Korea  B. K . Joardar et al.  3DMeshtherm 3DMeshp er f 3DHetp er f 3DHettherm "
JAMS - Jitter-Aware Message Scheduling for FlexRay Automotive Networks.,"FlexRay is becoming a popular in-vehicle communication protocol for the next generation x-by-wire applications such as drive-by-wire and steer-by-wire. The protocol supports both time-triggered and event-triggered transmissions. One of the important challenges with time-triggered transmissions is jitter, which is the unpredictable delay-induced deviation from the actual periodicity of a message. Failure to account for jitter can be catastrophic for time critical automotive applications. In this paper we propose a novel scheduling framework (JAMS) that handles both jitter affected time-triggered messages and high priority event-triggered messages to ensure message delivery while satisfying timing constraints. At design time, JAMS handles packing of multiple signals from Electronic Control Units (ECUs) into messages, and synthesizes a schedule using intelligent heuristics. At runtime, a Multi-Level Feedback Queue handles jitter affected time-triggered messages, and high priority event-triggered messages. We also propose a runtime scheduler that packs these messages into the FlexRay static segment slots depending on available slack. Experimental analysis indicates that JAMS improves the response time by 25.3% on average and up to 41% compared to the best-known prior work in the area.","JAMS: Jitter-Aware Message Scheduling for FlexRay   Automotive Networks  Special Session Paper  Vipin Kumar Kukkala*, Sudeep Pasricha*, Thomas Bradley†  *Department of Electrical and Computer Engineer ing  †Department of Mechanical Engineering  Colorado State University, Fort Collins, CO, U.S.A.  {vipin.kukkala, sudeep, thomas.bradley}@colostate.edu ABSTRACT  FlexRay is becoming a popular in-vehicle communication protocol  for the next generation x-by-wire applications such as drive-by-wire  and steer-by-wire. The protocol supports both time-triggered and  event-triggered transmissions. One of the important challenges with  time-triggered transmissions is jitter, which is the unpredictable delay-induced deviation from the actual periodicity of a message. Failure to account for jitter can be catastrophic for time critical automotive applications. In this paper we propose a novel scheduling framework (JAMS) that handles both jitter affected time-triggered messages and high priority event-triggered messages to ensure message  delivery while satisfying timing constraints. At design time, JAMS  handles packing of multiple signals from Electronic Control Units  (ECUs) into messages, and synthesizes a schedule using intelligent  heuristics. At runtime, a Multi-Level Feedback Queue handles jitter  affected time-triggered messages, and high priority event-triggered  messages. We also propose a runtime scheduler that packs these messages into the FlexRay static segment slots depending on available  slack. Experimental analysis indicates that JAMS improves the response time by 25.3% on average and up to 41% compared to the  best-known prior work in the area.    Categories and Subject Descriptors: C.3 [Special-purpose  and application-based systems]: Real-time and embedded systems;  C.4 [Performance of systems]: Fault tolerance   Keywords: Flexray, scheduling, jitter, automotive networks  1. INTRODUCTION   In today’s automobiles, Electronic Control Units (ECUs) are the  basic processing units that control different components in the vehicle. ECUs run various types of automotive applications such as the  anti-lock braking control, cruise control, transmission control, etc.  Most of these automotive applications have strict deadlines and latency constraints and thus they are classified as hard real-time applications [1]. In general, ECUs are distributed across the vehicle and  communicate with each other by exchanging signals. A signal can be  any raw data value or a control pulse that is packed into a message  and transmitted as a frame via the in-vehicle network. The most popPermission to mak e digital or hard copies of all or part of this work for personal or classroom u se is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than ACM mu st be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires pr ior specific permission and/or a fee. Request  permissions from Permissions@acm.org.  NOCS '17, October 19–20, 2017, Seoul, Republic of Korea  © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-4984-0/17/10…$15.00  https://doi .org/10.1145/3130218.3130234  ular and widely used in-vehicle network is the Controller Area Network (CAN) bus. CAN is a serial protocol which supports a maximum payload of 8 bytes and a typical transmission rate of up to 1   Mbps [2]. Some of the key features of the CAN bus include low cost,  a lightweight protocol, broadcast communication support, and support for message priorities and error handling [3], [4]. Other examples  of current in-vehicle networks include Local Interconnect Network  (LIN) and Media Oriented Systems Transport (MOST).  Recent advances in the automotive industry, such as the introduction of x-by-wire applications (throttle-by-wire, steer-by-wire, etc.),  have led to an increase in the complexity of automotive applications,  resulting in a huge demand for an efficient and reliable in-vehicle  communication system to satisfy the timing constraints of all the critical applications. This goal is difficult to achieve using the existing  CAN bus, which suffers from limited bandwidth and lack of timing  determinism. As a result, researchers have been actively exploring  other automotive communication solutions.    FlexRay has emerged in recent years as a potential solution that  overcomes the limitations of the existing CAN protocol and offers  added flexibility, higher data rates (at least 10× higher compared to  CAN [7]), and better timing determinism. FlexRay supports both  time-triggered and event-triggered transmissions. A major challenge  with time-triggered transmissions is jitter, which is a delay-induced  deviation from the actual periodicity of the message. There are various causes of jitter such as queuing of messages, delay in execution  of tasks, network congestion, noise, and other external disturbances.  In this work, we primarily focus on one of the more important sources  of jitter: delay in execution of tasks in ECUs. Jitter must be addressed  while designing schedules for time critical automotive applications  as failure to do so can severely affect system performance and also  be catastrophic in some cases (e.g., when the airbag deployment signal from the impact sensor to inflation module gets delayed due to  jitter). This creates a requirement for an effective jitter handling  mechanism that can be applied when designing and enforcing the  schedules for time-critical automotive applications.  In this paper, we propose a novel scheduling framework called  JAMS to handle both jitter effected time-triggered messages and high  priority event-triggered messages in a FlexRay based automotive system. JAMS combines design time schedule optimization with a  runtime jitter handling mechanism, to minimize the impact of jitter in  the FlexRay network. Our novel contributions in this paper are:   We propose a frame packing technique that packs the different  signals from an ECU into messages;   We develop a heuristic approach for the synthesis of design time  schedules for the FlexRay-based system;   We introduce a runtime scheduler that opportunistically packs the  jitter affected time-triggered and high priority event-triggered  messages in the FlexRay static segment slots;   We compare our JAMS framework with the best-known prior  works in the area and demonstrate its scalability.                    2. FLEXRAY OVERVIEW  FlexRay is an in-vehicle communication protocol designed for the  next generation x-by-wire automotive applications. It supports both  time-triggered and event-triggered transmissions. Figure 1 shows the  structure of the FlexRay protocol. According to the FlexRay specification [5], a communication cycle is one complete instance of a communication structure that repeats periodically. Each communication  cycle consists of a mandatory static segment, an optional dynamic  segment, an optional symbol window, and a mandatory network idle  time block.  Figure 1. Structure of FlexRay protocol  The static segment in FlexRay consists of multiple equal sized  slots called static segment slots that are used to transmit time-triggered and critical messages. The static segment enforces a Time Division Multiple Access (TDMA) media access scheme for the transmission of time-triggered messages, which results in a repetition of  the schedule periodically. In this TDMA scheme, each ECU is assigned a particular static segment slot and cycle number to transmit  its messages, which guarantees message delivery and time determinism. Each static segment slot incorporates one FlexRay frame, which  has three segments: header, payload, and trailer. The header segment  is 5-bytes long and consists of status bits, frame ID (FID), payload  length, header cyclic redundancy check (CRC), and cycle count. The  payload segment consists of actual data that has to be transmitted and  is up to 127 words (254 bytes) long. The trailer segment consists of  three 8-bit CRC fields to detect errors.   The dynamic segment consists of variable size slots called dynamic segment slots that are used to transmit event-triggered and low  priority messages. A dynamic segment slot consists of a variable  number of minislots (Figure 1), where each minislot is one microtick  (usually 1 µs) long. The dynamic segment enforces a Flexible Time  Division Multiple Access (FTDMA) media access scheme where  ECUs are assigned minislots according to their priorities. If an ECU  is selected to transmit a message, then it is assigned the required number of minislots depending on the size of the message, and hence the  length of a dynamic segment slot can vary in the dynamic segment  (Figure 1). During a transmission, all the other ECUs have to wait  until the one that is transmitting finishes. If an ECU chooses not to  transmit, then that ECU is assigned only one minislot and the next  ECU is assigned the subsequent minislot. The symbol window (SW)  is used for network maintenance and signaling for the starting of the  first communication cycle, while the network idle time (NIT) is used  to maintain synchronization between ECUs (Figure 1).  Inside a FlexRay ECU, messages are generated by a host processor  and sent to the Communication Host Interface (CHI) where the message data is packed into FlexRay frames. Each frame has a unique  frame ID (FID) that is equal to the slot ID in which the frame is transmitted [5]. CHI sends the qualified FlexRay frames to the Protocol  Engine (PE), which transmits the frames on to a physical FlexRay  bus, as shown in figure 2. A FlexRay frame is considered “qualified”  when the message data is available at the CHI before the beginning  of the allocated slot. Otherwise, a NULL frame is sent (by setting a  bit in the header segment of the FlexRay frame and setting all the data  bytes in the payload to zero). Jitter is one of the major reasons for  delay in the availability of message data at the CHI. Hence in this  paper we focus on a novel scheduling framework to overcome the  delays and performance loss due to jitter.  Figure 2. Message generation and transmission delay  3. BACKGROUND AND RELATED WORK  Many prior works in the area of FlexRay have focused on message  scheduling of both time-triggered and event-triggered messages. The  goal of these works is to generate message schedules by optimizing  parameters such as bandwidth, number of static segment slots used,  response time, etc., under strict timing constraints.  One of the popular techniques to maximize bandwidth utilization  is using frame packing, where multiple signals are packed into messages. In [6] two frame packing algorithms were proposed to optimize  bandwidth utilization. The authors in [7] proposed an Integer Linear  Programming (ILP) formulation to solve the frame packing problem  which requires multiple iterations with ILP to find the optimal solution. A Constraint Logic Programming (CLP) formulation and heuristic were presented for reliability-aware frame packing in [8], which  may require multiple re-transmissions of the packed frames to meet  reliability requirements. In [9], the frame packing problem is treated  as a one-dimension allocation problem and an ILP formulation and a  heuristic approach were proposed. The above-mentioned techniques  have a tradeoff between optimizing bandwidth utilization and minimizing the time to generate optimal frame packing. Our proposed  frame packing technique in this paper uses a fast heuristic approach  to generate near optimal results.  In case of automotive applications, most of the parameters such as  period, worst-case execution time, deadline etc., are known during  the design time, which facilitates the synthesis of highly optimized  design-time schedules. Several works explore the design-time scheduling of the static segment in FlexRay. One of the major objectives is  to minimize the number of slots allocated to ensure future extensibility of the system while maximizing bandwidth utilization. An ILP formulation with an objective to minimize the number of slots allocated  was proposed in [10], considering both message and task scheduling.  This was later extended in [12] by including support for multiple real  time operating systems and using ILP reduction techniques. In [11],  the message scheduling problem is transformed into a two dimensional bin-packing problem and an ILP formulation and a heuristic  approach were suggested for minimizing the number of allocated  slots. The authors in [14] and [15] proposed a CLP and ILP formulation respectively for jointly solving the problem of task and message  scheduling in FlexRay systems. A set of algorithms were proposed in  [17] that enable scheduling of event-triggered messages in time-triggered communication slots using a virtual communication layer. A  few other works solve the same problem with heuristics and variants  of ILP and CLP [8], [13], [16], [18], [19]. All of the above works lack  jitter awareness, which makes them unreliable for use in real-time  scenarios where jitter can significantly impact scheduling decisions.   Jitter in FlexRay based systems has largely been ignored and there  is limited literature on the topic. In [7] a jitter minimization technique  using an ILP formulation is proposed. In [20], the frequency of message transmission is changed for those messages that are likely to be  affected by jitter, to minimize message response time. However, in  both [7] and [20], it is assumed that the jitter value and number of  messages that are affected by jitter are known at design time, which  is unlikely in real world scenarios. As random jitter can affect any  message in the system, there is a need for a jitter handling mechanism  that can handle jitter more comprehensively at runtime. In this paper,  we propose a hybrid framework that combines design time schedule                  optimization with runtime jitter handling, to minimize the impact of  jitter in FlexRay.  4. PROBLEM DEFINITION    , (cid:1854)(cid:3036)(cid:3041) , (cid:1854)(cid:3036)(cid:3041) We consider a general scenario with multiple ECUs running various time critical automotive applications that are connected using a  FlexRay bus. Executing an application may result in the generation  of signals at an ECU, which may be required for another ECU executing a different application. These signals are packed into messages  and transmitted as FlexRay frames on the bus. In a typical automotive  system, messages can be categorized as: (a) time-triggered or periodic messages, and (b) event-triggered or aperiodic messages. Timetriggered messages are transmitted in the static segment of the  FlexRay cycle while the dynamic segment is used for transmitting  event-triggered messages. Every ECU or node in the system is capable of transmitting both types of messages in addition to the dedicated  connection to different sensors and actuators (henceforth the terms  ECU and node are used interchangeably). In this work, we primarily  focus on the challenging problem of scheduling time-triggered messages and high priority event-triggered messages in the FlexRay static  segment and ignore the scheduling of low-priority event-triggered  For each node n ∈ Ɲ, Sn = {(cid:1871)(cid:2869)(cid:3041) , (cid:1871)(cid:2870)(cid:3041) , …, (cid:1871)(cid:3012)(cid:3289)(cid:3041) } denotes the set of  messages in the FlexRay dynamic segment.  We consider an automotive system with the following inputs:   Ɲ represents the set of nodes, where Ɲ = {1, 2, 3, …, N};  Every signal (cid:1871)(cid:3036)(cid:3041) ∈ Sn, (i = 1, 2 …, Kn) is characterized by the    tuple {(cid:1868)(cid:3036)(cid:3041) , (cid:1856)(cid:3036)(cid:3041) }, where (cid:1868)(cid:3036)(cid:3041) , (cid:1856)(cid:3036)(cid:3041) signals transmitted from that node and Kn represents the maxiline, and data size (in bytes) of the signal (cid:1871)(cid:3036)(cid:3041) respectively;  mum number of signals in node n;  = {(cid:1865)(cid:2869)(cid:3041) , (cid:1865)(cid:2870)(cid:3041) , …, (cid:1865)(cid:3019)(cid:3289)(cid:3041) } in which every message (cid:1865)(cid:3037)(cid:3041) ∈ Mn, (j = 1,   denote the period, dead2, …, Rn) is characterized by the tuple {(cid:1868)(cid:3037)(cid:3041) , (cid:1856)(cid:3037)(cid:3041) , (cid:1854)(cid:3037)(cid:3041) }, where   After frame packing, every node maintains a set of messages Mn  (cid:1868)(cid:3037)(cid:3041) , (cid:1856)(cid:3037)(cid:3041) , (cid:1854)(cid:3037)(cid:3041) denotes the period, deadline and data size (in bytes)  of the message (cid:1865)(cid:3011)(cid:3041) respectively;  We assume the following definitions:  To transmit a message (cid:1865)(cid:3011)(cid:3041) on the FlexRay bus, it needs to be  Slot number or Slot identifier (slot ID): A number used to idenallocated a slot ID sl ∈ {1, 2, …, Nss} and a cycle number bc ∈  tify a specific slot within a communication cycle;   Cycle number: A number used to identify a specific communication cycle in the schedule;  If a message (cid:1865)(cid:3011)(cid:3041) is assigned to a particular slot and a cycle then  {0, 1, …, Cfx} where Nss and Cfx are the total number of static  segment slots in a cycle and total number of cycles, respectively.  This is referred to as message-to-slot assignment.  the source node n of that message should be allocated ownership  of that slot. This is known as ECU-to-slot assignment.  Problem Objective: For the above inputs, the goal of our work is  to: (i) maximize the bandwidth utilization on the Flexray bus with  frame packing, (ii) find an optimal design-time schedule (messageto-slot assignment, ECU-to-slot assignment) for the time-triggered  messages without violating timing constraints, (iii) minimize the effect of jitter on time-triggered messages and high priority event-triggered messages.        5. JAMS FRAMEWORK OVERVIEW  We propose the novel JAMS framework that enables jitter-aware  scheduling of time-triggered messages and collocation of high priority event-triggered messages in the static segment in a FlexRay-based  automotive system. Figure 3 shows an overview of the proposed  framework. At design time, JAMS packs different signals into messages during frame packing, and then synthesizes a schedule using a  design time scheduler. These frame packing and scheduling phases at  design time only consider time-triggered messages, as the arrival pattern of event-triggered messages is not known at design time. At  runtime, JAMS facilitates the handling of both jitter affected timetriggered messages and high priority event-triggered messages using  a multi-level feedback queue (MLFQ). The runtime scheduler in  JAMS packs these messages into the FlexRay static segment slots  depending on the available slack using the design time schedule and  the output of MLFQ. Each of these stages are discussed in detail in  the next four subsections.  Figure 3. Overview of JAMS framework  5.1 Design Time Frame Packing  Frame packing is the first step in the JAMS framework where multiple periodic signals in a node are grouped together into messages.  This step helps in maximizing bandwidth utilization, which not only  improves system performance but also enhances the extensibility of  the system by utilizing fewer slots than without frame packing. As  discussed in section III, several works have proposed techniques for  solving the frame-packing problem. However, many of these efforts  lack scalability. In order to overcome these shortcomings, we propose  a scalable frame packing heuristic to maximize bandwidth utilization  in a greedy manner.  Algorithm 1: Greedy frame packing  Inputs: Set of nodes (Ɲ), Set of time-triggered (periodic) signals in each  4:            for each signal  (cid:1871)(cid:3036)(cid:3041) in Sn do  node (Sn) , static segment slot s ize (bslot), framing overhead (Of)  1:     for each node n in Ɲ do  2:            Sort Sn in decrea sing order of data size  3:            Mn = { }  7:                   if signa l (cid:1871)(cid:3036)(cid:3041) is not packed then  8:                          Push (cid:1871)(cid:3036)(cid:3041) → current_message list  5:                   current_message = { }  9:                          utilization + =  (cid:1854)(cid:3036)(cid:3041) 6:                   utilization = 0   10:                        for each signal (cid:1871)(cid:3037)(cid:3041) in {Sn – (cid:1871)(cid:3036)(cid:3041) }do  11:                               if (cid:1868)(cid:3037)(cid:3041) == (cid:1868)(cid:3036) (cid:3041) then  12:                                      if signa l (cid:1871)(cid:3037)(cid:3041) is not packed then  13:                                             if utilization +  (cid:1854)(cid:3037)(cid:3041) + Of  < (cid:1854)(cid:3046)(cid:3039)(cid:3042)(cid:3047) then  14:                                                   Push (cid:1871)(cid:3037)(cid:3041) → current_message list  15:                                                   utilization + = (cid:1854)(cid:3037)(cid:3041) 16:                                             end if  17:                                      end if  18:                               end if  19:                        end for  20:                 end if  21:                 if current_message ≠ { } then  23:                 end if  22:                        Add current_message list to Mn  24:          end for  25:          Add Mn to M  26:   end for  Output: Set of messages in each node                           Algorithm 1 shows the pseudo code of the proposed heuristic  where the inputs are: a set of time-triggered (periodic) signals for  each node (Sn), static segment slot size (bslot), and the framing overhead (Of; calculated from [7]). According to the FlexRay protocol  specification [5], any slot in a given FlexRay cycle can be assigned  to at most one node, which restricts the packing of signals from different nodes into the same message. This makes the frame packing  problem solvable independently for different nodes as shown in step  1. In addition to this, signals with the same periods and from the same  node are prioritized for being grouped together, to minimize re-transmissions, which in turn minimizes the number of slots used. For any  give node n  Ɲ, the algorithm starts with sorting the set of signals  Sn in the decreasing order of bandwidth utilization (step 2). In step 3,  an empty set is created for storing the set of messages for that node  and all the signals in Sn are considered for packing as shown in step  4. In steps 5-19, every unpacked signal is packed with the other signals of the same period and the same node until utilization constraints  in step 13 are violated. The signal that caused the violation of utilization constraints is added to a new message after adding the previously  computed message to Mn. This is repeated until all the messages in  that node are packed. At the end, Mn computed for each node is added  to another set M, where M is a superset consisting of the set of messages in each node. The resulting messages will have their period be  the same as the signal period and their deadline will be equal to the  lowest signal deadline packed in that message. In this work, all the  messages have deadlines equal to their periods. The output of the algorithm is an optimal packing of signals into messages for each node.  5.2 Design Time Scheduling  In this subsection, we present a design time scheduling approach  for the previously generated time-triggered messages. As discussed  in section IV, for the transmission of any message over FlexRay, a  slot ID and cycle number needs to be assigned for both the message  and the source node of that message. A design time schedule consists  of such an assignment of slot IDs and cycle numbers to the messages  in the system. In creating this schedule, we aim to minimize the number of assigned static segment slots for efficient bandwidth utilization. In addition, we also take advantage of the cycle multiplexing in  FlexRay 3.0.1 in which multiple nodes can be assigned slots with the  same slot ID in different communication cycles, which is not possible  in FlexRay 2.1A. This helps to maximize the static segment utilization while using only minimal number of slots [11].   The fundamental basis for our proposed heuristic are the concepts  of message repetition and the slot ID utilization in a FlexRay based  system. As the time-triggered messages are periodic, their schedules  also repeat after a certain period. For any time-triggered message in  FlexRay, message repetition is the ratio of message period to the cycle time of the FlexRay as shown in equation (1):  (1/(cid:1870)(cid:1865)(cid:3037)(cid:3041)) of the available slots having that slot ID. Algorithm 2 shows  This number is usually an integer value as the FlexRay cycle time  is chosen to be the greatest common divisor of all the message periods  in the system. In addition, from [20] it is evident that any time-triggered message that is assigned a particular slot ID will use up  how this metric is used in JAMS to assign slots to the messages. After  initializing variables in step 1, all the time-triggered messages are  sorted in the increasing order of message periods. For each message  in the list, we check if adding the message to the current slot ID does  not violate the utilization constraint (step 4). If the maximum utilizafirst communication cycle for that message and all the c + ((cid:1861) ∗ (cid:1870)(cid:1865)(cid:3037)(cid:3041) )  tion is exceeded, then a new slot ID is allocated to that message and  the utilizations and slot variables are updated (steps 4-9). After a slot  ID has been assigned to a message, cycle numbers are determined  cycles (for i ∈ N0) away from the chosen cycle are added to the list  using steps 10 and, 11. The lowest cycle number c is chosen as the  (cid:1870)(cid:1865)(cid:3037)(cid:3041) = (cid:3043)(cid:3285)(cid:3289)(cid:3004)(cid:3281)(cid:3299)                                             (1)  of cycles for that message. Moreover, all the cycle numbers that were  added to the message are removed from the cycleList. This results in  a slot and cycle assignment to all the time-triggered messages in the  system and their corresponding source nodes.  Algorithm 2: JAMS design time scheduling  3:    for each message (cid:1865)(cid:3037)(cid:3041) in M do  Inputs: Set of all t ime-triggered messages in the system (M = {M 1, M2,  …, MN})  4:          if (utilprev + 1/(cid:1870)(cid:1865)(cid:3037)(cid:3041) ) > 1 then  1:    Initialize: utilcur, utilprev= 0; sloti nit, slotcur = 1; cycleList= {0,1,…, 63}  2:    Sort M in the increasing order of message periods  7:          utilcur + = 1/(cid:1870)(cid:1865)(cid:3037)(cid:3041)   5:               utilcur = utilprev = 0; slotcur + = 1; cycleList = {0,1,…., 63}  9:          (cid:1871)(cid:1864)(cid:1867)(cid:1872)(cid:3040)(cid:3285)(cid:3289) ← slotcur  6:          end if  10:        Push c , c +((cid:1861) ∗ (cid:1870)(cid:1865)(cid:3037)(cid:3041)) → list of cycles of (cid:1865)(cid:3037)(cid:3041) for i ∈ N0  11:        Remove c , c +((cid:1861) ∗ (cid:1870)(cid:1865)(cid:3037)(cid:3041)) from cycleList for i ∈ N0  8:          utilprev = utilcur  12:        Ass ign slot ownership((cid:1871)(cid:1864)(cid:1867)(cid:1872)(cid:3040)(cid:3285)(cid:3289) , list of cycles of (cid:1865)(cid:3037)(cid:3041) ) to node n  Output: s lot ID ((cid:1871)(cid:1864)(cid:1867)(cid:1872)(cid:3040)(cid:3285)(cid:3289) ) & set of communication cycles (list of cycles)  for each time-triggered message (cid:2195)(cid:2192)(cid:2196) , slot ownership of each node n.  13:  end for  5.3 Runtime Multi-level Feedback Queue  The schedule generated by the design time scheduler will only  guarantee latencies for time-triggered messages under ideal conditions. In realistic scenarios, various disturbances may interfere with  the regular operation on the FlexRay bus. One of the major disturbances that can severely impact the performance of schedules is jitter,  which is a serious threat for safety critical systems. Hence, we focus  on handling jitter at runtime using a runtime scheduler that re-schedules jitter affected time-triggered messages using the input from the  design time schedule and the output of a Multi-Level Feedback  Queue (MLFQ). Similarly, the high priority event-triggered messages  are also treated as jitter affected time-triggered messages during the  runtime scheduling, so that they can be scheduled in a timely manner  as part of the FlexRay static segment.  The MLFQ consists of two or more queues that have different priorities and are capable of exchanging messages between different levels using feedback connections (figure 3). The number of MLFQ  queues defines the number of levels; each level queue can have a different prioritization scheme and scheduling policy than other queues.  We considered an MLFQ consisting of three level queues as  shown in figure 3, with queue 1 (Q1) having the highest priority fol lowed by queue 2 (Q2) and queue 3 (Q3) with lower priorities. In  addition to prioritization between different level queues, we set up  priorities between different types of messages and within the messages of the same type. We prioritize time-triggered messages over  event-triggered ones and all other types of messages. Moreover,  within the time-triggered messages, we adapt a Rate Monotonic (RM)  policy to prioritize messages with high frequency of occurrence. In  case of a tie, priorities are assigned using a First Come First Serve  (FCFS) strategy. Event-triggered messages inherit the priority of their  generating node. In cases of multiple event-triggered messages from  the same node, an Earliest Deadline First (EDF) scheme is employed  to prioritize messages. These static priorities of the messages are used  to reorder the queues and promote messages to upper level queues. In  addition, there are two separate buffers that are used to handle jitter  affected time-triggered messages and high priority event-triggered  messages, which are later fed to the MLFQ (as shown in figure 3).  The operation of the MLFQ is depicted in figure 4. We begin by  checking the time-triggered message buffer for jitter-affected messages. If a time-triggered (TT) message is available, the ‘load TT  message’ function is executed which checks for a vacancy in queues  in the order Q1, Q2, and Q3 and, stores the TT message in the first                    available queue. If the TT message buffer is empty and an event-triggered (ET) message is available in the ET message buffer, the ‘load  ET message’ function is executed which checks for the vacancy in  queues in the order Q2, Q3 and Q1 and, stores the ET message in the  first available queue. In either case, when all the three queues are full,  the message is held in the corresponding buffer and the same function  is executed in the next clock cycle. Whenever there are no messages  available in both the buffers and, the reorder queue function is not  executed in the preceding clock cycle, messages in the queues are  reordered in the order of their priorities, by executing the ‘reorder  queues’ function. Otherwise, the queues are checked in the order Q1,  Q2 and Q3 by executing the ‘POP queue’ function. The conditions for  popping a queue are discussed in detail in the next subsection.  packed into the same frame. The scheme introduces two additional  segments in the payload segment of the FlexRay frame (figure 6). The  first is called AI or Append Indicator segment, which is the first segment in the payload. It is a 1-bit field indicating whether a message  is appended in the frame. The second is a custom header segment,  which consists of two fields: a type field and a length field. The type  is a 15-bit field used to specify different message types. It consists of  one bit each for payload preamble indicator, null frame indicator,  sync frame indicator and startup frame indicator which are defined in  [5] and an 11 bit frame ID (FID) field for specifying the FID of the  jitter affected message. The length field is 8-bit long and specifies the  data length of the jitter affected message in bytes. The length field in  the custom header along with the payload length field in the frame  header is used to find the start byte of the jitter affected message in  the payload segment. The regular operation of the FlexRay protocol  is not altered in any way by implementing these changes.  Figure 4. Full operat ions of the MLFQ  5.4 Runtime Scheduler  After handling the jitter-affected messages in the MLFQ as mentioned in the previous subsection, the next task is to re-schedule them  and meet their deadline constraints. To accomplish this, we introduce  a runtime scheduler that handles multiple inputs coming from the output of the MLFQ, and the design time generated schedule. In addition  to these, the runtime scheduler also has information on the available  slack in each of the static segment slots from the design time generated schedule. Thus, whenever there is a jitter-affected message available at the MLFQ, the runtime scheduler checks for two different  conditions in the next incoming slot: (i) whether there is sufficient  slack in the incoming slot to accommodate the available jitter-affected message and, (ii) whether the incoming slot is owned by the  sender node of the jitter-affected message. When these two conditions  are satisfied, the jitter-affected message from MLFQ is collocated  with the jitter unaffected message in the incoming slot, as shown in  figure 5. Similarly, when a high priority event-triggered message is  available at the MLFQ, the abovementioned conditions are checked  and the high priority event-triggered message is treated as the jitter  affected time-triggered message. This results in packing of two different message data in to the payload segment of one frame, which  unfortunately leads to two major challenges. Firstly, there is a requirement for a mechanism at the receiver to decode the payload and distinguish between the two messages. Secondly, the implicit addressing  scheme of TDMA is lost because of combining two different messages, as the receiving nodes will not be able to identify to which  specific node the message is meant for.  Figure 5. Packing of j itter affected messages during runtime  To overcome the abovementioned challenges, we propose a segmentation and addressing scheme to differentiate multiple messages  Figure 6. Updated frame format of the FlexRay frame using the proposed  segmentat ion and addressing scheme (sizes of individual segments are  shown in bits). The parts of the frame highlighted in gray represent our  modif ications.  6. EXPERIMENTS  6.1 Experimental Setup  In this section, we evaluate the performance of the JAMS framework by comparing it with Grouping based Message Scheduling  (GMSC [7]), Optimal Message Scheduling with Jitter minimization  (OMSC-JM [20]), Optimal Message Scheduling with FID minimization, (OMSC-FM [20]), and Policy based Message Scheduling  (PMSC [17]). GMSC [7] begins with frame packing of signals using  an ILP approach and tries to group these messages into selected slots.  After generating the corresponding groups, the allocation algorithm  proposed in [20] is used to generate the slot IDs and communication  cycles for which the response times are calculated. Based on the  available knowledge of worst-case jitter, slot allocations are modified. PMSC [17] uses a priority based runtime scheduler, which supports preemption based on the message arrival time and priority. Messages are scheduled based on the slot assignment generated using  heuristics. OMSC-JM [20] and OMSC-FM [20] use the frame packing technique in [7] and alter the message repetition to minimize the  effect of jitter. OMSC-JM and OMSC-FM differ by the weights associated with the objective function in the optimization problem.  OMSC-JM tries to minimize the effect of jitter by allocating more  slots and frequent message transmissions while OMSC-FM aims at  minimizing the number of allocated slots.  To evaluate JAMS and compare against the above-mentioned prior  works, a set of test cases was created using different combinations of  the number of nodes, number of signals in the system, and the periods  of the signals. For our initial set of experiments, we selected a test  case configuration representing a FlexRay 3.0.1 based system consisting of 4 nodes. The nodes exchanged 33 different signals with periods varying from 5 ms to 45 ms with data sizes ranging from 8 to  32 bytes. The FlexRay system considered for all the experiments has  a cycle duration of 5ms (tdc) with 62 static segment slots (Nss), with a  slot size of 42 bytes (bslot) and 64 communication cycles (Cfx). We  randomly sample jitter values from the range of 1–8 ms to modify the  arrival times of the messages originating from a randomly selected  jitter-affected node. The messages with a deadline under 25ms are  considered as high priority messages and the others as low priority                          messages. All the simulations are run on an Intel Core i7 3.6GHz  server with 16 GB RAM.  6.2 Response Time Analysis  Figures 7(a)-(b) illustrate the average response times of the high  priority and lower priority messages, respectively. The confidence interval on each bar represents the minimum and maximum average  response time of the message (with the deadline on x-axis) achieved  using the corresponding technique. The dashed horizontal lines show  the four different message deadlines in each figure. From figure 7(a),  it is evident that under many cases, GMSC exceeds the deadline of  the messages in the presence of jitter. This is because of its lack of  ability to handle random jitter. OMSC-FM results in high response  times in the presence of jitter as this technique allocates a minimal  number of slots resulting in long delays between any two instances of  a message transmission (e.g., when there is a delay in the message  arrival, the jitter-affected message has to wait for a longer duration to  be transmitted in the next allocated slot). Moreover, OMSC-FM does  not consider deadline constraints. OMSC-JM has relatively better response times (except in one case, where it misses a deadline because  of high jitter) as it assigns additional slots to the nodes and thus has  more number of available slots for the transmission of jitter-affected  messages. In PMSC, jitter has a strong impact on the high priority  messages (figure 7(a)), because of the type of frame packing involved. PMSC aims to use the entire static segment slot by packing  the signals that are larger than the slot size and uses EDF-based  preemption at the beginning of each slot. In the presence of jitter for  high priority messages, the arrival of these messages are delayed,  causing the node to wait for the next transmitting slot to preempt existing transmissions of low priority messages. This delay along with  jitter can sometimes exceed the message deadline and may lead to  missed deadlines, as shown in figure 7(a).  (a)  6.3 Scalability Analysis  To evaluate the effectiveness of JAMS for system configurations  with varying complexities, we analyzed it against the prior works by  selecting different combinations of number of nodes and number of  signals. Figures 8(a)-(c) plot the average response times of all the signals (y-axis) of the jitter-affected node under low, medium and high  jitter respectively (jitter values varied from 1–3 ms for low, 3 – 5 ms  for medium, and 6 – 8 ms for high), for different system configurations {p, q} where p denotes the number of nodes and q is the number  of signals (x-axis). The number on the top of each bar indicates the  number of signals that missed the deadline in that configuration. It  can be observed in figures 8(a)-(c) that the average response times of  all the signals in the jitter-affected node is less in almost all the cases  when using JAMS. JAMS does have an overhead in the average response time compared to the best average response time (using  OMSC-JM [20]) for the first configuration with 3 nodes and 21 signals, especially for messages with very low jitter.  (a)  (b)  (b)  Figure 7. Message deadlines vs average response time for (a) high priority  messages and (b) low priority messages; for GMSC [7], OMSC-JM [20],  OMSC-FM [20], PMSC [17], and our JAMS framework.  It is evident that our JAMS framework outperforms all the other  techniques and achieves lower response time for all the high priority  messages (figure 7(a)). In addition, JAMS is also able to minimize  the average response time of the low priority messages while meeting  their deadline constraints, as shown in figure 7(b).  (c)  Figure 8. Average response time of all signals for different configurations  (with number of missed deadlines shown on top of the bars) under (a) low,  (b) medium and, (c) high jitter condit ions, for GMSC [7], OMSC-JM [20],  OMSC-FM [20], PMSC [17], and our proposed JAMS framework.  6.4 Slot Allocation Analysis  Figure 9 illustrates the number of slots allocated for each configuration evaluated in the previous subsection, across the various frameworks. It can be seen that a higher number of slots are allocated with  OMSC-JM [20] than JAMS. This high allocation facilitates the frequent transmission of jitter-effected messages, which can lower response times. However, even with a slight advantage in response                                      Sangiovanni-Vincentelli, “Scheduling the FlexRay bus using optimization techniques”, in Proc. DAC, 2009.  [11] M. Lukasiewycz, M. Glaß, J. Teich and P. Milbredt, “Flexray  Schedule Optimization of  the Static Segment”,  in Proc.  CODES+ISSS, 2009.  [12] H.Zeng, M. Di Natale, A. Ghosal and A. Sangiovanni-Vincentelli, “Schedule Optimization of Time-Triggered Systems Communicating Over the FlexRay Static Segment”, in IEEE TII, 2011.  [13] M. Grenier, L. Havet and N. Navet, “Configuring the communication on FlexRay- the case of the static segment”, in Proc. ERTS,  2008.  [14] Z. Sun, H. Li, M. Yao and N. Li, “Scheduling Optimization  Techniques for FlexRay Using Constraint-Programming”, in Proc.  CPSCom, 2010.  [15] M. Lukasiewycz, R. Schneider, D. Goswami and S. Chakraborty,  “Modular Scheduling of Distributed Heterogeneous Time-Triggered  Automotive Systems”, in ASP-DAC, 2012.  [16] D. Goswami, M. Lukasiewycz, R. Schneider and S. Chakraborty,  “Time-triggered implementations of mixed- criticality automotive software”, in Proc. DATE, 2012.  [17] P. Mundhenk, F. Sagstetter, S. Steinhorst, M. Lukasiewycz and  S. Chakraborty, “Policy-based Message Scheduling Using FlexRay” ,  in Proc. CODES+ISSS, 2014.  [18] B. Tanasa, U.D. Bordoloi, P. Eles and Z. Peng, “Scheduling for  Fault-Tolerant Communication on the Static Segment of FlexRay” ,  in Proc. RTSS, 2010.  [19] Lange, F. Vasques, P. Portugal and R.S. de Oliveira, “Guaranteeing Real-Time Message Deadlines In The FlexRay Static Segment  Using a On-line Scheduling Approach”, in WFCS, 2014.  [20] K. Schmidt, E.G. Schmidt, “Optimal Message Scheduling for the  Static Segment of FlexRay”, in Proc. VTC, 2010.  time, OMSC-JM [20] still misses the deadlines for high jitter-affected  messages, unlike JAMS which has no deadline misses across all the  configurations. Also from figure 9, it is evident that OMSC-FM [20]  has a lower number of slots allocated in all the configurations. But  figures 8(a)-(c) illustrate that the average response time of the signals  impacted by jitter is higher using OMSC-FM [20] as compared to  JAMS because of the fewer availability of slots. In addition, OMSCFM [20] suffers from multiple deadline misses in comparison to  JAMS. From the results related to OMSC-JM and OMSC-FM in figures 8(a)-8(c) and 9, it is important to observe that there exists a  tradeoff between the number of allocated slots and the average response time. JAMS balances this tradeoff quite well. Thus with a minimal overhead in the number of allocated slots, JAMS surpasses all  the other techniques with no deadline misses and low response times.  Figure 9. Number of allocated slots for each conf igurat ion  7. CONCLUSION  We presented a novel scheduling framework for effectively handling jitter-affected time triggered and high-priority event-triggered  messages in a FlexRay based automotive system. Our framework utilizes both design time and run time scheduling to mitigate the effect  of jitter on the system. Our proposed JAMS framework has several  advantages compared to other scheduling approaches from prior  work, in terms of both response times and slot allocation. Our experimental analysis with the proposed framework indicates that it is  highly scalable and outperforms the best-known prior works in the  area. JAMS improves the response time by 25.3% on average and up  to 41% compared to the best-known prior work (OMSC-JM). Our  approach can also be extended to other time-triggered protocols with  minimal changes.  "
Rethinking NoCs for Spatial Neural Network Accelerators.,"Applications across image processing, speech recognition, and classification heavily rely on neural network-based algorithms that have demonstrated highly promising results in accuracy. However, such algorithms involve massive computations that are not manageable in general purpose processors. To cope with this challenge, spatial architecture-based accelerators, which consist of an array of hundreds of processing elements (PEs), have emerged. These accelerators achieve high throughput exploiting massive parallel computations over the PEs; however, most of them do not focus on on-chip data movement overhead, which increases with the degree of computational parallelism, and employ primitive networks-on-chip (NoC) such as buses, crossbars, and meshes. Such NoCs work for general purpose multicores, but lack scalability in area, power, latency, and throughput to use inside accelerators, as this work demonstrates. To this end, we propose a novel NoC generator that generates a network tailored for the traffic flows within a neural network, namely scatters, gathers and local communication, facilitating accelerator design. We build our NoC using an array of extremely lightweight microswitches that are energy- and area-efficient compared to traditional on-chip routers. We demonstrate the performance, area, and energy of our micro-switch based networks for convolutional neural network accelerators.","Rethinking NoCs for Spatial Neural Network Accelerators Hyoukjun Kwon Georgia Institute of Technology Atlanta, Georgia hyoukjun@gatech.edu Ananda Samajdar Georgia Institute of Technology Atlanta, Georgia anandsamajdar@gatech.edu Tushar Krishna Georgia Institute of Technology Atlanta, Georgia tushar@ece.gatech.edu ABSTRACT Applications across image processing, speech recognition, and classification heavily rely on neural network-based algorithms that have demonstrated highly promising results in accuracy. However, such algorithms involve massive computations that are not manageable in general purpose processors. To cope with this challenge, spatial architecture-based accelerators, which consist of an array of hundreds of processing elements (PEs), have emerged. These accelerators achieve high throughput exploiting massive parallel computations over the PEs; however, most of them do not focus on on-chip data movement overhead, which increases with the degree of computational parallelism, and employ primitive networks-onchip (NoC) such as buses, crossbars, and meshes. Such NoCs work for general purpose multicores, but lack scalability in area, power, latency, and throughput to use inside accelerators, as this work demonstrates. To this end, we propose a novel NoC generator that generates a network tailored for the traffic flows within a neural network, namely scatters, gathers and local communication, facilitating accelerator design. We build our NoC using an array of extremely lightweight microswitches that are energy- and areaefficient compared to traditional on-chip routers. We demonstrate the performance, area, and energy of our micro-switch based networks for convolutional neural network accelerators. CCS CONCEPTS • Computer systems organization → Interconnection architectures; Neural networks; ACM Format: Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna. 2017. Rethinking NoCs for Spatial Neural Network Accelerators. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130230 1 INTRODUCTION Neural network (NN) based algorithms, such as deep convolutional neural networks (CNN), have shown tremendous promise over the past few years in performing object detection, recognition, and classification across state-of-the-art benchmark suites [7, 15] at accuracies surpassing those of humans. Modern CNNs [14, 23] have tens of layers and millions of parameters. This adds tremendous Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130230 Figure 1: Architecture of a neural network accelerator. We generate the NoC between the global buffer and the PEs using novel latency, area, and energy-efficient microswitches. throughput and energy-efficiency challenges on the hardware substrate to efficiently load these parameters on-chip and perform millions of computations (multiply-accumulate). CPUs are unable to provide such parallelism, while GPUs can provide high parallelism and throughput but consume massive amounts of energy due to the frequent memory accesses. There has been flurry of research in the computer architecture domain for designing custom hardware accelerators for providing real-time processing of deep neural networks at stringent energy budgets. Most of these accelerators are spatial in nature, i.e., an array of interconnected processing elements (PEs) is used to provide parallelism [1, 3–5, 16]. The internal dataflow between the PEs is optimized to reuse parameters (input activations, weights, or output activations) that are shared by multiple neurons [3]. This reduces the number of memory accesses, thereby providing energyefficiency. The PEs are fed new parameters from an on-chip global buffer, as shown in Fig. 1. The microarchitecture of the PE (only compute, or compute with some local storage), and the nature of the dataflow between the PEs/global buffer to PEs is an area of active research currently and multiple alternate implementations have been demonstrated [1, 3– 5, 16]. However, there has been little research on the architecture or implementation of the network-on-chip (NoC) interconnecting the PEs to each other and to the global buffer. In a spatial NN accelerator, the NoC (Fig. 1) plays a key role, more so than in multi-cores, in realizing high-throughput. This is because most spatial accelerators operate in a dataflow style: a PE operation is triggered by data arrival, and the PE stalls if the next data to be processed is unavailable due to memory or NoC delay. Almost all NN accelerators have used specialized buses [3], or mesh-based NoCs [1, 4, 8], or crossbars [1], without a clear trade-off study on why one was picked over the other. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna This work provides a comprehensive analysis of the NoC within an array of spatial PEs that are accelerating a CNN. We first characterize traffic inside typical CNN accelerator implementations, in detail. We demonstrate that interconnecting tens of processing cores on a CMP or SoC is very different from interconnecting hundreds of tiny PEs from both a performance (latency and throughput) and cost (area and power) perspective. We also argue that the NoC cannot be completely custom and static, such as those in application specific embedded systems, since the traffic patterns vary based on the actual neural network being mapped. We conclude that the conventional NoCs (bus/crossbar/mesh/custom-trees) used in CMPs/SoCs today are not appropriate choices as they either limit the achievable throughput from the PE array, or add significant area and power penalties, proportional to that of the PEs themselves. We propose a new NoC design paradigm for NN accelerators using an array of reconfigurable micro-switches. The micro-switch array can be configured cycle by cycle to provide dedicated paths for three kinds of traffic that occurs in all CNN implementations: scatter (buffer to PE array - either via unicast, multicast or broadcast), local (PE to PE), and gather (PE array to buffer). We achieve three simultaneous goals: extremely low-area, energy-efficiency, and performance. The micro-switch array increases the performance of CNN implementations by 49% on average, compared to traditional (bus, tree, mesh, crossbar, hierarchical) NoCs. More importantly, it reduces average area and power by 48% and 39% respectively. In the same area, micro-switches can house 2.32X PEs than a mesh. The rest of the paper is organized as follows. Section 2 provides relevant background on NN accelerator dataflows. Section 3 motivates this work by analyzing traffic flows across NN accelerator implementations. Section 4 demonstrates our micro-switch array topology and microarchitecture. Section 5 presents evaluation results. Section 6 describes related work and Section 7 concludes. 2 BACKGROUND Neural networks are a rich class of algorithms that can be trained to model the behavior of complex mathematical functions. The structure of neural networks is modeled after the human brain with a large collection of “neurons"" connected together with “synapses"". In machine learning, a deep neural network (DNN) is built using multiple layers of neurons, each layer essentially acting as a feature extractor, with the final layer performing classification. 2.1 Convolutional Neural Networks (CNN) Convolutional Neural Networks (CNN) are a class of DNNs that are used widely for image processing. Each convolution layer receives inputs in the form of a raw image or input feature maps (the output of a previous convolution layer) and convolves it with a filter to produce an output feature map, as Fig. 2 illustrates. 2.2 Dataflows in CNN Accelerators There has been a surge in accelerators for CNN over the past few years [4–6, 8]. The dataflow graph for CNN computations can be mapped onto a PE array in multiple ways leading to different dataflow characteristics. We follow the taxonomy introduced in Eyeriss [6] that classifies CNN accelerator implementations into the following categories. Figure 2: Convolution operation performed by a CNN accelerator. Input activations convolves with filter weights in respective channel. The accelerator accumulates bias values and partial sums generated from different channels to obtain output activations. The accelerator repeats such calculations over entire input activation by sliding the filter by stride value until it generates all the output activations. • Weight Stationary (WS): In a WS accelerator, each PE fetches a unique weight element from the global buffer (GB) and retains it until the PE completes all calculations involving that weight. GB transfers input activations via a broadcast toward each PE. PEs may forward psums back to the GB (to be redistributed later), or accumulate them locally within the array. • Output Stationary (OS): An OS accelerator maps one output pixel on to one PE in every iteration. Each PE fetches both weights and input activations from global buffer and internally accumulates partial sums. When the accumulation completes, or output activations are generated, each PE sends the output activation to the global buffer. • Row Stationary (RS): A RS accelerator [6] maps a row of partial sum calculations on a column of the PE array, which facilitates data reuse of weight and input activations. Partial sums are accumulated by forwarding locally along the column, and the PEs at the top of the column send the final output activations to the global buffer. The dataflows in CNN accelerators remain fairly uniform within each layer to maintain uniform utilization across all PEs. 3 MOTIVATION 3.1 Traffic inside Neural Network Accelerators Given the highly parallel nature of the computation, most neural network accelerators - CNN [6, 8], RNN [10], SNN [1], - employ a multitude of compute units, which we refer to as processing elements (PE)s. Each PE contains some scratch pad memory and the compute logic. In addition to PEs there is also a larger on chip memory present in the accelerator, which we will refer to as the global buffer (GB). We assume the PE to be the most primitive building block of the accelerator - managing computation for one partial sum, a primitive output in CNNs. We identify that there are primarily three kinds of traffic flows in spatial accelerators: • Scatter: Scatter is data distribution from the GB to the PE array. Scatters can either be unicast or multicast, depending on the dataflow and the mapping of compute on PEs. • Gather: Gather is the traffic flow which occurs when multiple PEs send data to the GB at a given interval of time. Gather can either occur at the end of the computation, or in the middle of the computation due to insufficient number of PEs. Rethinking NoCs for Spatial Neural Network Accelerators NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 3: (a) Compute (Multiplications and Additions) vs. Communication (Scatter/Gather/Local) of each Alexnet layer [14] across different CNN implementations: weight stationary (WS), row stationary (RS), and output stationary (OS). (b) Average NoC bandwidth requirement for Alexnet vs. number of PEs Figure 4: Challenges with traditional NoCs for accelerators. (a) Latency of 64-PE WS CNN accelerator with increasing PE delay (b) Area, and (c) Power • Local: Local refers to the inter-PE communication traffic. It could be in the form of unicasts, multicasts or reductions. Fig. 3 (a) plots the total number of computations and communication flows (scatter/gather/local) within the convolution layers of AlexNet [14] for the accelerator implementations described in Section 2.2. It shows raw compute to communication ratio across typical CNN dataflows, and demonstrates that communication is critical in spatial CNN accelerators to get full throughput. The operation of most of such accelerators occurs in a dataflow style; a delay in communication would essentially lead to a stall. Because PEs are tiny compute units, they are incapable of exploiting ILP/TLP mechanisms for hiding delays, unlike conventional CPUs or GPUs. Fig. 3 (b) translates the raw communication into a bandwidth requirement as a function of the number of PEs. We simulate WS, OS and RS implementations and estimate the average traffic requirements across all the convolution layers of AlexNet. For all designs, scatter bandwidth (unicast for WS and OS, multicasts for RS) is extremely crucial. For WS architectures, the bandwidth required by gathers is significant. As the number of PEs increases, so does the bandwidth across all traffic flows. The traffic analysis demonstrates that compute is highly dependent on communication in CNN accelerators. They require an interconnect to support frequent scatters, local traffic, and gathers, to multiplex many neurons across finite PEs. A neural accelerator designer would decide the number of PEs (based on the area budget on-chip), and optimize a PE microarchitecture for the target application and its required accuracy. The total traffic bandwidth divided by the PE delay determines the network bandwidth requirement per cycle to sustain full-throughput, by Little’s Law. The NoC inside the accelerator needs to support this bandwidth. Next, we discuss the challenges with traditional NoCs for this purpose. 3.2 Traditional and Application-Specific NoCs Traditional NoCs such as buses, meshes, and crossbars are common across multicores today. Naturally, they have also found their way into multi-PE DNN accelerators. For instance, Eyeriss [3] and DNNWeaver [22] use buses, DianNao [4] and ShiDianNao [8] use meshes, and TrueNorth [1] uses crossbars and meshes in a hierarchical manner. However, these NoCs add scalability challenges when used inside accelerators, as we discuss in section 5. Application specific NoCs [19] generate NoCs in accordance with the application’s communication graph that is known apriori and are common in MPSoCs in the embedded domain. They may seem natural as NoCs inside such accelerators to tailor the NoC to the neural network dataflow. However, the traffic inside the accelerator is not static; it varies layer by layer [6], and is dependent on the mapping of the dataflow over PEs, and the input parameters, as Fig. 3(a) shows. Nevertheless, we also implemented a tree-based custom NoC inside the accelerator optimized for scatters and gathers. We perform a limit study with traditional NoCs with a WS dataflow. In this limit study, we assume no storage in PEs, which could mitigate the performance challenges of traditional NoCs at the cost of increased area and power in each PE. Performance. Fig. 4 (a) plots the runtime across the CONV1 layer of AlexNet for a weight-stationary accelerator with 64 PEs. We compare the performance of a mesh, and a multi-bus/multitree topology against an “ideal"" NoC which is a single-cycle zerocontention network. We make two observations. (1) With a 1-cycle PE, we observe that the mesh and a single bus or tree is 10× slower than the ideal. The reason is heavy contention at links near the GB. Assuming that the GB can sustain higher injection/ejection bandwidth, we also simulated NoCs with multiple buses/trees and found that even with 64 buses, the design is 2× slower than the ideal. (2) As the PE delay increases, normally the overall delay should increase as well, as we observe with the ideal. However, with the mesh or single bus/tree, the overall delay is almost constant demonstrating that the NoC is choked and is the bottleneck. Area. Fig. 4 (b) plots the area of traditional NoCs relative to the area of 64-PE array. All numbers are from RTL synthesis with in 15nm Nangate PDK [18]. The PE array is from Eyeriss [6]1 . The most telling observation is that traditional scalable networks like mesh routers, prevalent across multicores, consume significantly 1We thank the Eyeriss authors for sharing the RTL implementation of a PE. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna Figure 5: The connectivity of microswitch network for (a) scatter (unicast/multicast), (b) gather and (c) local traffic. We highlight top, middle, and bottom switches with blue, gray and green colors, respectively. within a GHz at 15nm, as we show later), enabling single-cycle communication inside the NoC. We call this MPCmax for maximum microswitches per cycle. 4.1 Topology Figure 6: The microarchitecture of three microswitches. higher area than even the compute PEs. This is primarily because routers in meshes are larger than a PE; utilizing a multicore mesh NoC inside an accelerator is thus not an efficient design choice as it would reduce the area available for the actual compute units, reducing overall throughput. Crossbars are known to scale horribly with number of nodes and this is also apparent from our area results. Buses and the custom tree are better in terms of area. Power. Fig. 4 (c) shows a similar trend with power. Traditional meshes and crossbars end up consuming more power than the entire compute array. This becomes worse as the number of PEs increases. Our conclusion from this study is two-fold: (1) Meshes are not scalable solutions as NoCs inside accelerators. From a performance perspective, they get throughput limited when handling scatters and gathers. From an area and power perspective, routers consume much higher area and power than PEs. (2) Buses and trees are effective for an area and power point of view, but they are non-configurable, which limit the performance of accelerators. That is, they are not flexible enough to support diverse demands of accelerators to support myriad CNN topologies, mappings, and input sizes. 4 MICROSWITCH NETWORK GENERATOR Accelerators achieve high throughput and energy efficiency in computation by distributing computations to tiny processing elements, exploiting massive parallelism. Similarly, we achieve high network throughput and energy efficiency in communication by distributing communication to tiny microswitches. A microswitch consists of a small combinational circuit and up to two FIFOs; in contrast to the building blocks of traditional NoCs such as mesh routers that house buffers, a crossbar, arbiters and control. We describe the microswitch architecture in Section 4.2. We design a NoC generator that aggregates multiple microswitches and connects them in our proposed topology to build a light-weight interconnect, that can be plugged into NN accelerators. Multiple microswitches can be traversed within a single-cycle, (24 switches For a N PE design, we use a N l oд (N ) microswitch array, as shown in Fig. 5. We divide the array into l oд (N ) levels, with N microswitches each. We numerically label the switches from the global buffer side from Level0 to Level l oд (N ) The microswitches in Level 0 are called top switches, Levels 1 to l oд (N ) − 1 are called middle switches, and Level l oд (N ) are called bottom switches. We layout our proposed topology over the array to efficiently handle three traffic flows that appears in any CNN implementation described in Section 3: scatter (unicast and multicast), gather, and local, as shown in Fig. 5. Scatter (unicast and multicast). For scatters, we construct a tree structure in a microswitch array, with the root at one of the top switches, and the leaves at the bottom switches, as shown in Fig. 5(a). This simulates the functionality of bus: delivering data to multiple destinations simultaneously within a cycle. Unlike a bus that broadcasts data to every PE, however, our design delivers data only to designated recipient PEs (i.e., a unicast or a multicast). Such selective data delivery enhances energy efficiency by suppressing redundant broadcasting; implementation is lightweight, comprising of two one-bit registers in each branching switch and control signal propagation wires whose width is 2 × (N − 1) when the number PEs is N. (i.e., N-1 one-bit registers and an 2(N-1)-bit wire). We discuss the control signal generation logic in detail in Section 4.5. Higher throughput from the global buffer is available by simply connecting to multiple top-switches, as we discuss later in this section. Gather. For gather, each PE has dedicated connections up to the top switches in Level 0, via bypass links within the middle and bottom switches, as shown in Fig. 5(b). This provides highbandwidth. Top switches send gather data towards one (or more) top switch connected to the global buffer’s I/O port. The top switch connected to the global buffer I/O port selects one of the incoming gather flits using a round-robin-based priority logic and sends the flit to the global buffer in a pipelined manner. Local. For PE to PE local traffic flows, we construct a bi-directional linear network using the bottom switches, as shown in Fig. 5(c). This network allows single-cycle traversals between any two PEs by controlling the microswitches appropriately. For example, if PE1 is communicating with PE2, PE3 with PE6, and PE7 with PE4, all of these can be supported simultaneously. We discuss this further in Section 4.4. The design thus minimizes the latency and maximizes the throughput of local traffic flows. The local traffic flow network is supported by the bottom switches using multiplexers and a FIFO, as discussed in Section 4.2. We manage the flow control using on/off Rethinking NoCs for Spatial Neural Network Accelerators NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea reverse signaling in which each bottom switch latches incoming local flit if the buffer in the next bottom switch is not available. Supporting Higher Bandwidth. The bandwidth of the scatter and gather networks is limited by the number of IO ports at the GB, which is logically visible from Fig. 5. The goal of a network architecture for accelerators has to be to make sure that the data delivery bandwidth does not become a bottleneck, which would lead to PEs stalling. As discussed earlier in Section 3.1, the required bandwidth depends not just on the traffic, but also one the delay and context state in each PE, which comes as an input to the microswitch NoC generator. We support higher bandwidth communication from the GB using wider channels and/or multiple parallel networks. 4.2 Microarchitecture We define the level of a microswitch as the number of layers between that microswitch and the global buffer, as described in Fig. 5. Because the traffic pattern for the top (the first layer from the global buffer), middle, and bottom level of microswitch array varies, we present three types of microswitches for each. Top switch. Top switches manage the gather and scatter (unicast and multicast), from PE to global buffer and vice versa respectively. Therefore, top switches contain two components: scatter and gather units, as shown in Fig. 6 (a). The scatter unit passes incoming flits to the branching nodes in the next level depending on the value of two one-bit control registers (one per scatter output port), determined by destinations of traversing flits. The traversal is completely bufferless, with flits branching to any one or both directions depending on the setup - unicast or multicast. The setup of the control registers is described in Section 4.5. The gather unit delivers incoming flits towards the global buffer I/O ports. There can be up to three gather flits entering a top microswitch, depending on its location, as Fig. 5(b) shows. A round-robin arbiter is used inside this unit. There is a an output FIFO after the arbiter to buffer the gather while it waits to win arbitration at the next microswitch. Middle switch. Middle switches, which belong to the level between the first and last level, manage scatter and gather traffic, as shown in Fig. 6 (b). The scatter unit is the same as that in top switches; the gather unit is just a wire that simply forwards incoming gather flits toward top switches. Such a gather unit minimizes the latency of gather flits. However, if the number of PEs increases, gather flits need to traverse more number of microswitches in the middle layer. Then, we need to insert pipeline latches to meet the operating clock frequency, which is managed by our generator. Our synthesis results using NanGate 15nm standard cell library [18] shows that flits can pass 24 microswitches within a cycle (MPCmax ) when the operating clock frequency is 1GHz, which allows 24 middle layers that cover 224 PEs, a number large enough to cover state-of-the-art neural network accelerators. Note that most neural network accelerators today operate with a clock frequency lower than 1GHz and employ less than 256 PEs [3–5, 16]. Thus our NoC can provide single-cycle traversals up to the top switches for gathers. Bottom switch. Bottom switches, which belong to the last level adjacent to PEs, manage scatter, gather, and local traffic. The scatter and gather units are wires. The local unit consists of a small number of components: three muxes, four demuxes, two FIFOs, and combinational logic that generates the mux/demux control signals, as shown in Fig. 6 (c). Although the number of components in a bottom switch is larger than that of components in top or middle switches, the overall overhead is not significant because the number of bottom switches increases linearly with the number of PEs. Local traffic units enable single-cycle multi-switch traversal, all the way from the source to the destination. Single-cycle multi-hop designs require extra control logic that introduces extra area and power overheads [13] to manage conflicts dynamically. We minimize such overheads by presetting microswitches to create multiple paths between PEs, as long as there are no conflicting links. We also allow flits to arbitrate for part of/the entire set of local links, like a bus. We still require buffers in bottom switches for two reasons. (1) the buffer at the destination PE may be full; as a result the flit on the local network needs to wait. (2) the maximum number of microswitches to be traversed may be greater than MPCmax . Recall that our synthesis results at 15nm demonstrate a MPCmax of 24 at 1GHz. We force the bypassing flits to be latched after traversing MPCmax microswitches. The network interface between a PE and a bottom switch inserts a one-hot encoded bit vector that represents the number of remaining traversals. This value decreases via a simple shift in each bottom switch during traversal, with the signal getting latched when all bits are zero. 4.3 Routing For scatters, the routing is predetermined by the microswitch control logic (Section 4.5). The control enables broadcasts, multicasts, and unicasts within a single cycle. For gathers, the route of all flows is fixed - from the PE to the GB. For local traffic, the NIC of source PEs inserts a one-hot bit vector representing the number of microswitches to traverse until the destination. 4.4 Flow Control The three kinds of traffic use different flow-control strategies, as determined by the switches they traverse. The overall goal is to provide single-cycle communication for all three traffic types, at the maximum possible throughput. Scatter. For scatters, we employ a customized cycle-by-cycle circuit switching technique that sets up unicast/multicast/broadcast paths that are valid for one cycle for each flit. This is done by the network controller, described later in Fig. 7. The global buffer maintains credits for the input buffers in the PEs, and performs a scatter only if all destination PEs have at least one free buffer. Gather. For gather traffic, since the traffic passes through unidirectional wires in the middle switches, no flow control is required here. Top switches, however, need a flow control for gathers, since an arbitration grant plus an empty FIFO slot in the next top switch is required before a flit can be dequeued. We use on/off back signaling to support this. Local. For local traffic, we support two schemes. (1) Static: the bottom switches are preset to enable multiple parallel circuit-switched connections between different PEs. This scheme depends on the mapping scheme across PEs and uses On/off back signaling between bottom switches. We discuss this scheme in Section 4.5. (2) Dynamic: part of or the entire set of local links can be arbitrated for and used like a bus. 4.5 Network Reconfiguration and Control A key property of our microswitch network is cycle-by-cycle reconfigurability. The reconfiguration is controlled by one-bit control NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna Figure 7: An example of scatter tree reconfiguration. (a) Control signal generation. The controller recursively tests a set of 2k consecutive bits in a destination bit vector if is not zero until it reaches level 0. If a test bit vector is not zero, the corresponding switch is active. Therefore, the parent node switch at the lower level is active as well to provide data to the child switch. Our control logic is based on such an observation. (b) Control signal mapping for a multicast scatter. For simplicity, we only show microswitches that belong to the scatter tree. The 2-bits in each microswitch is the control register value, one for each branch of the sub-tree. For example, if the control register values are 10, incoming scatter flit is forwarded to the upper subtree in the figure. (c) Local traffic control. Mode 1 (Static) - Control register manage flow control; if the value is zero, an incoming flit stops at the bottom switch, else it bypasses. For example., this mode allows PE0 to PE2, and PE2 to PE4 communication simultaneously. Mode 2 (Dynamic) - An arbiter selects one flit and grants the flit access through multiple switches exclusively. registers for muxes at each microswitch, to enable single-cycle traversals across the fabric over multiple microswitches. The top and middle switches can be configured for single-cycle scatters (unicast, multicasts, and broadcasts), and the bottom switches for singlecycle local traffic. Gather network uses conventional flow-control and delivers flits in a pipelined manner. 4.5.1 Control Signal Generation. Scatter Network. The reconfiguration for scatters is controlled by two one-bit control registers in each middle and top switch that are branching nodes in the tree we construct, as the example in Fig. 7 shows. The value of control registers indicates if an incoming data may flow toward their corresponding sub branches of a branching node in the tree. The network controller converts destination bits of a flit into control register values and sends the register values one cycle before the data flit traverses the scatter tree. The reconfiguration and the data flit traversal are pipelined so the controller inserts a data flit at every cycle. That is, while a data flit traverses the tree network, the controller generates and sends a control signal for the next data flit. Therefore, the control logic does not degrade the overall throughput. The controller receives a destination bit vector from the global buffer, which consists of N bits (N: number of PEs) that represents valid destinations, and generates a control signal that contains the value of control registers in branch switches of the scatter/broadcast tree. The control signal generation logic is based on the observation that each branching switch needs to send a flit toward a lower branch if the branch contains at least one of the valid destinations. That is, we can determine the control signal by examining two, four, and 2k consecutive bits in a destination bit vector for the level l oд (N ) − k , where N is the number of PEs and k is an integer between 0 and l oд (N ) . We provide an example in Fig. 7(a). The logic checks if an individual bit in the destination bit vector is nonzero; the results are the control signals for the last level. In the next step, the logic checks if consecutive two-bit values are nonzero; the results are the control signals for the next level. The logic repeats to double the size of test consecutive bits and check if each chunk is nonzero until the test bit size covers the half of the destination bit vector. If the number of PEs is not a power of two (i.e., number of PEs < 2k ), the logic regards the destination bit width as 2k and pads zeros for invalid destinations. Local Network. On the local network, we provide the ability to partition the set of local links into single-cycle circuit-switched paths between any two PEs (subject to the number of switches being less than MPCmax ). The local network configuration is done across larger time epochs rather than every cycle. For instance, for CNNs, this is done at the start of every convolutional layer. Since the network controller manages delivery of scatters, it also knows which PEs will communicate with which other PEs, and accordingly tries to provide neighbor-to-neighbor communication as much as possible, which can be supported in parallel, as shown in Fig. 7(c). Each bottom microswitch has 2-bits to determine whether incoming flits need to be forwarded to the next microswitch or stop. Flits that stop at a bottom switch are read by the appropriate PE if the destination matches. Thus the bottom switch allow the local links to form configurable buses of different lengths. If partitioning the bus statically is not possible for handling all local communication flows simultaneously, say for fully-connected layers of CNNs, some/all bottom switches operate in a forward mode and the local links behaves like a bus via dynamic arbitration, as shown in Fig. 7(c). We also enable part of the local links to operate like an arbitrated bus, and the remaining to be statically configured. This is all managed by the reconfiguration controller. 4.5.2 Control Signal Mapping. We utilize a separate control plane to configure each microswitch. Recall that each switch has a 2-bit configuration state. The number of bits in the control plane is a trade-off with reconfiguration time, and multiple implementations can exist. We support two: Dedicated. We use 2 × N l oдN wires, to enable cycle by cycle reconfiguration. As an energy optimization, the controller only sends bits to switches that need to update their configuration. A challenge with this design is that the configuration plane may become too wide at large PE counts. Ring. We also support an alternate design for the control plane where all switches are linked via a configuration ring (analogous to scan chains today) to carry a switch id and the 2-bit configuration. The controller sends configurations for each switch multiple cycles in advance, keeping the delay of traversing the ring in mind. This is possible since the dataflow is fixed after the mapping is complete. Rethinking NoCs for Spatial Neural Network Accelerators NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 8: Post-synthesis area/power estimation and layout on ASIC. The ASIC chip dimension is 440x440um. Network Bus, Custom tree, Crossbar, Mesh, Hierarchical Mesh (4 clusters, 1X or 2X BW at GB), Microswitch Language Bluespec System Verilog (BSV) [20] Technology 15nm NanGate PDK [18] Traffic Patterns WS (without local accumulation) and RS Application Alexnet (CNN) [14] Table 1: A summary of evaluation configuration Figure 9: (a) Total latency of each accelerator and NoC combination for entire Alexnet. (b) Throughput evaluation of mesh, bus, and microswitch network with 32 PEs and randomized synthetic traffic. 5 EVALUATIONS Table 1 presents our evaluation methodology and configurations. In addition to traditional NoCs, we also implement and evaluate the performance of hierarchical designs, which are popular in recent DNN accelerators [1, 5, 22]. We use a hierarchical mesh with four clusters (e.g., for 64 PEs, each cluster contains 16 PEs). 5.1 Area and power estimation Fig. 8 presents the post-synthesis area and power results of our micro-switch NoC compared to a traditional NoCs. The first stark observation is that the mesh adds too much overhead, both in terms of area, and power, compared to even the PE array. The crossbar area and power are reasonable at 32-64 PEs, but it shoots up at large PE counts. The mesh and crossbar consume 7.4X more power and 7.2X more area compared to the PE array at 256 PEs. The bus2 , tree and micro-switch array are the most scalable for area and power. On average, the micro-switch array consumes 47.8% lower area and 39.2% lower power than all baselines. Assuming a 512B SRAM in each PE [3], we find that a micro-switched based accelerator can house 2.32X more PEs than a mesh in the same area. 5.2 Throughput and latency Fig. 9 (a) presents the total latency for running Alexnet in WS and RS accelerators with 16, 32, and 64 PEs. Since multicast-scatter is dominant in weight-stationary traffic, as Fig. 3 shows, the bus and tree performs well with WS accelerators. However, as RS accelerators involve local traffic, the micro-switch network performs the best because it exploits the local traffic network between the bottom switches. Mesh performs the worst in every case because it needs to serialize all the scatter traffic. An optimization that clones a scatter flit in each router is feasible, but such an optimization demands more area and power. Considering the area and power 2 Note that this is a post-synthesis result that does not take into account the RC of the final bus layout. Thus the observed power consumption is somewhat under-evaluated. overhead of mesh is already prohibitively high (Section 5.1), it is not practical even if such an optimization were to be applied. The HMesh with 2X bandwidth at the global buffer performs better than the mesh, but is still worse than the bus and microswitch which have 1X bandwidth due to the lack of multicast support. ‘For RS traffic, the HMesh had worse performance because of mapping inefficiencies caused by the fixed size of clusters. In Fig. 9(b), we compare the performance of the networks with synthetic random scatter/gather traffic. The performance of the microswitch scatter network scales linearly without saturating as it guarantees single-cycle traversal to multiple destinations via the single-cycle multiple-hop network. The microswitch gather network saturates early due to heavy congestion at the link going into the GB, and we recommend using multiple gather networks or wider links at the top switches to enhance throughput. The bus and tree networks saturate very early. Fig. 10 shows the performance breakdown of the NoCs for running each layer of AlexNet. The micro-switch fabric provides the lowest runtime, a 49% savings on average across all NoCs, as it eliminates the scatter and/or gather bandwidth bottlenecks present in other NoCs. 5.3 Energy consumption Since a bus always broadcasts flits to the PE array, it requires more energy for each flit. The worst case of such an inefficiency is unicast that has only one destination but bus consumes energy for broadcast. More number of PEs aggravate the energy inefficiency of bus, as Fig. 11 (a) highlights. The amount of energy required for single flit traversal affects the overall energy consumption of entire computation. The total energy consumption for Alexnet convolution layers in Fig. 11 (b) shows the micro-switch NoC being the most efficient in terms of overall energy as it activates only the required minimal links for each flit traversal, for both scatters and gathers. In summary, we can observe that the micro-switch network performs well on all metrics - latency, throughput, area, power, and scalability, when used inside a neural network accelerator, while traditional NoCs fail on one or more of these fronts. 5.4 Bottom switch bypass for local traffic Depending on the operating clock frequency, the number of bottom micro-switches a local traffic flit can traverse within a cycle i.e., MPCmax , varies, as shown in Fig. 11 (c). The MPCmax value affects the throughput of local traffic network based on the sourcedestination pattern. If an accelerator design requires end-to-end local traffic, then the delay of such local traffic flits is the number of PEs divide by MPCmax . However, assuming that the neural network mapping algorithm did a good job mapping communication PEs close to each other, such a worst case would be rare, and we expect most local traversals to take a single-cycle leveraging the single-cycle over MPCmax -hops feature of our micro-switch array. For example, a PE in an RS accelerator requires partial sums to NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna Figure 10: Runtime of WS/RS accelerators for each Alexnet conv layer. The number below each group of bars represents the number of PEs. Figure 11: (a) Energy consumption for single flit traversal. (b) Total network energy for entire Alexnet convolution layers using an RS accelerator (c) MPCmax over clock frequency values. traverse to an adjacent PE in the same column of the PE array [3]. We can construct a column-first linear bottom switch network for it, and the number of bypass hops toward the destination is always one in such a configuration. This configuration works with either of the control logic we discussed in Section 4.5. 6 RELATED WORK NoCs in neural networks accelerators. Vainbrand, et al. [25] compared interconnection networks in neural network accelerators from a theoretical perspective and concluded that a multicast mesh is the most optimized solution. The authors remarked that tree-based networks are not a good option for inter-PE communication, which we address in this work by providing local links in bottom switches. Theocharides et al. [24] and Emery et al. [9] suggested that mesh is the best NoC in neural network accelerators because of its scalability and ease of reconfigurability. Diannao [4] and Shidiannao [8] relied on mesh-based interconnects for data transfer. Dadiannao [5] employed fat tree for scatter and gather and 3D mesh via hyperTransport 2.0 to distribute data among nodes. Eyeriss [3] uses separate buses for its scatters and gathers. Carrillo et al. [2] described H-NoC infrastructure in which the neural nodes are arranged in three layers, module, tile and cluster. In H-NoC, any traffic moving from a lower to upper level is reduced, which maintains the scalability of the network. Spinnaker [11] implemented SNNs using clusters of multiprocessor nodes with a 2D triangular mesh for inter-node and a 2D mesh for intra-node communication. IBM Truenorth [1] employed a 256×256 crossbar for traffic inside a neurocore and mesh for inter-core communication. Our microswitch based NoC can enhance all these designs by lowering the area and power of the NoC, enabling more PEs to be added. Lightweight and application-specific NoC design. NOC-out [17] designed a lightweight switch-based network and effectively addressed cache coherence traffic in many-core CMPs. Although both of NOC-Out and microswitch network are based on lightweight switches, microswitch network (1) exploits single-cycle multi-hop feature to implement multicast scatter, (2) allows multiple reduction points based on global buffer bandwidth, and (3) statically generate control signals to implement cycle-by-cycle circuit switching that eliminate the necessity of VC buffers, VC allocation, arbitration, credit management logic inside switches, which minimizes area and power. Ogras et al. [21] optimized general-purpose NoCs for selected applications by prioritizing application-specific traffic in selected long range paths. Kim et al. [12] employed a 2D mesh to address the multi-source-multi-destination traffic of augmentedreality applications in head-mounted displays with a scheduler for mapping the application over the NoC. 7 CONCLUSION We present a novel NoC design for neural network accelerators that consists of configurable light-weight micro-switches. The microswitch network is a scalable solution for all the four aspects latency, throughput, area, and energy - while traditional NoCs (bus/mesh/crossbar) only achieve scalability for some of them. We also provide a reconfiguration methodology to enable single-cycle paths over multiple micro-switches to support dynamism across neural network layers, mapping methodologies and input sizes. While our evaluations focused on neural network accelerators, we believe that the micro-switch fabric can be tuned for any accelerator built using a spatial array of hundreds of PEs. "
On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC.,"Many-core systems connected by 3D Network-on-Chips (NoC) are emerging as a promising computation engine for systems like cloud computing servers, big data systems, etc. Mapping applications at runtime to 3D NoCs is the key to maintain high throughput of the overall chip under a thermal/power constraint. However, the goals of optimizing both the communication latency and chip peak temperature are contradicting due to several reasons. Firstly, exploiting the vertical TSV links can accelerate communications, while low peak temperature prefers that the tasks to be mapped closer to the heat sink, instead of using the vertical links. Secondly, mapping tasks in close proximity can reduce communication latency, but at the cost of poor heat dissipation. To address these issues, in this paper, we propose an efficient runtime mapping algorithm to reduce both communication latency and overall application running time under thermal constraint. In essence, this algorithm first selects a 3D cuboid core region of a specific shape for each incoming application by setting the region's number of occupied vertical layers and its distance to the heat sink, in order to optimize its communication performance and peak temperature. Next, the exact locations of the core regions in the chip are determined, followed by a task-to-core mapping. The experimental results have confirmed that, compared to two recently proposed runtime mapping algorithms, our proposed approach can reduce the total running time by up to 48% and communication cost by up to 44%, with a low runtime overhead.","On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC Bing Li South China University of Technology, China l.b07@mail.scut.edu.cn Amit Kumar Singh Department of Electronics and Computer Science, University of Southampton, UK a.k.singh@soton.ac.uk ABSTRACT Many-core systems connected by 3D Network-on-Chips (NoC) are emerging as a promising computation engine for systems like cloud computing servers, big data systems, etc. Mapping applications at runtime to 3D NoCs is the key to maintain high throughput of the overall chip under a thermal/power constraint. However, the goals of optimizing both the communication latency and chip peak temperature are contradicting due to several reasons. Firstly, exploiting the vertical TSV links can accelerate communications, while low peak temperature prefers that the tasks to be mapped closer to the heat sink, instead of using the vertical links. Secondly, mapping tasks in close proximity can reduce communication latency, but at the cost of poor heat dissipation. To address these issues, in this paper, we propose an efficient runtime mapping algorithm to reduce both communication latency and overall application running time under thermal constraint. In essence, this algorithm first selects a 3D cuboid core region of a specific shape for each incoming application by setting the region’s number of occupied vertical layers and its distance to the heat sink, in order to optimize its communication performance and peak temperature. Next, the exact locations of the core regions in the chip are determined, followed by a task-to-core mapping. The experimental results have confirmed that, compared to two recently proposed runtime mapping algorithms, our proposed approach can reduce the total running time by up to 48% and communication cost by up to 44%, with a low runtime overhead. CCS CONCEPTS • Computer systems organization → Multicore architectures; Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130228 Xiaohang Wang South China University of Technology, China xiaohangwang@scut.edu.cn Terrence Mak Department of Electronics and Computer Science, University of Southampton, UK Shenzhen Institute of Advanced Technology and Guangzhou Institute of Advanced Technology, CAS, China KEYWORDS 3D NoC, Application mapping, Thermal management, On-chip Resource management ACM format: Bing Li, Xiaohang Wang, Amit Kumar Singh, and Terrence Mak. 2017. On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC. In Proceedings of NOCS ’17, Seoul, Republic of Korea, Oct. 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130228 1 INTRODUCTION Many-core systems have been widely used as an engine to provide sufficient computation power in cloud computing servers, big data systems, etc. In these systems, multiple applications with various workload characteristics arrive and leave the system at runtime. Mapping tasks to cores online is the key to improve system performance. 3-Dimensional (3D) integration can improve the system integration and reduce global wire length. Through-Silicon-Via (TSV) is one of the popular approaches among various 3D integration techniques [13, 16]. 3D networks-on-chip (NoC) adopting the TSV technique has lower network latency and power consumption, and higher bandwidth [18, 19]. However, as more dies stacked vertically, power density (W /m2 ) increases, and the length of heat conduction path increases, resulting in higher propagation delay and higher leakage power [4]. The major challenge of runtime application mapping in 3D NoC is to achieve the contradicting goals of optimizing communication and temperature, which requires to address the following two aspects: 1) Whether to exploit the vertical links or not leads to different communication and temperature behaviors. Lower communication latency requires that the tasks of an application to exploit the TSV connections as much as possible. That is, the tasks with high communication volume should be mapped crossing multiple vertically adjacent layers in the same column, as TSVs provide high bandwidth and shorter distance as in Fig. 1(a). On the other hand, lower temperature requires that the tasks to be mapped to the layer close to the heat sink as in Fig. 1(b), instead of crossing multiple layers. NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea B. Li et al. Figure 1: (a) Mapping tasks to exploit vertical wire for higher communication efficiency. (b) Mapping tasks close to heat sink for lower temperature. (c) Free cores scattered. (d) Free cores packed in close proximity. 2) For an application with some running applications, whether to map tasks in scattered or close proximity might lead to fragmentation or high temperature. A phenomenon called “fragmentation” often occurs causing free cores scattered (not forming a contiguous region). Fragmentation increases communication distance and latency for tasks of incoming application as they are mapped to noncontiguous free cores regions [17], as in Fig. 1(c). Mapping tasks in close proximity alleviates fragmentations. However, if we take thermal effect into consideration, mapping tasks in close proximity accumulates heat faster and tends to have high peak temperature as in Fig. 1(d), where the peak temperature is 2℃ higher than that in Fig. 1(c)1 . In this paper, we proposed an algorithm to address the above challenge, in order to optimize the communication and computation performances under a thermal constraint in 3D NoC systems. The algorithm has two steps. First, a 3D cuboid core region of a specific shape is selected for each application. Second, the exact locations of the core regions in the chip are determined, followed by a taskto-core mapping. The rest of paper is organized as follows. Related work is reviewed in Section 2. The problem is formulated in Section 3. The proposed runtime mapping algorithm is detailed in Section 4. Experimental results are evaluated in Section 5. Finally, Section 6 concludes the paper. 2 RELATED WORK Existing runtime application mapping algorithms in NoC can be classified into 3 categories: 1) communication, 2) temperature, and 3) both communication and temperature. Mapping algorithms in the first category focus on communication optimization, improving system throughput by reducing network latency [2, 7–9, 28]. Most of the algorithms in this category target 2D NoC systems as in [2, 7–9]. These approaches map communicating tasks to cores close to each other so that communication latency and power are reduced. For example, in [8], Mohammad el al. proposed a stochastic hill climbing algorithm that starts from a first node and maps the tasks to a set of nodes forming a contiguous region around it. A few of them target the 1 The Experimental setup is described in Section V.A Figure 2: An example of 3D mesh NoC architecture 3D NoC system as in [28]. In [28], Ziaeeziabari et al. proposed a latency-aware task mapping algorithm. It divides communications of a given application graph into low volume and high volume communication subgraphs. Then it maps application subgraphs one by one based on their total intra communications considering where the vertical channels are located in the network. However, these approaches do not consider thermal aspects. Mapping algorithms in the second category focus on temperature optimization [6, 11, 20, 25, 26]. In [6], Cui et al. proposed a B2T algorithm that maps all the tasks to the bottom layer which is close to the heat sink, followed by a second step which moves low-power tasks to the top layer. In [26], Zhu et al. proposed the TAPP (Temperature-Aware Partitioning and Placement) algorithm to reduce on-chip hotspots. TAPP spreads high-power cores and routers across the chip by performing a hierarchical bi-partitioning of the cores and concurrently conducting placement of the cores onto tiles, and achieves high efficiency and scalability. These algorithms ignore communication distance, which might lead to a higher network latency. In [11], Hamedani et al. explored the temperature constraints for thermal aware mapping of 3D networks on chip, focusing on the design of thermal management algorithm. Mapping algorithms in the third category focus on jointly optimizing communication latency and temperature [1, 5, 14, 15, 23, 27]. In [15], Mosayyebzadeh el al. proposed an algorithm using fuzzy logic to adjust the impact of heat emission capability, inter-task distance inside application, and distance from hot spot. It reduces the communication delay by mapping tasks with large communication volumes to cores close to each other. It also reduces power consumption and peak temperature by mapping tasks to cores that are close to the heat sink. However, this method does not fully exploit the vertical links to optimize latency. Further, all these approaches do not consider fragmentation [17], which might lead to high communication latency after running many applications. 3 PROBLEM FORMULATION 3.1 System Model In this work, we adopt the 3D NoC system model as that of [6]. Fig. 2 shows the architecture of a 3D mesh NoC with TSVs as the vertical links.The 3D NoC is modeled as a directed graph G (C , L), where C is the set of cores and L is the set of links connecting the cores. The cores of the system model support various voltage/frequency (V/F) levels. The deterministic XYZ routing is used. The layer which lies next to the heat sink is referred to as the bottom layer and the most distant one from the heat sink is referred to as the top layer. A centralized platform resource manager is designed to monitor the arrival of application, manage resources and perform application mapping. Table 1 summarizes the definition of symbols used in this paper. On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea Table 1: Symbol Definition Definition A 3D mesh NoC system X dimension (width) of the 3D NoC Y dimension (length) of the 3D NoC Z dimension (height) of the 3D NoC The set of cores in a 3D NoC system The set of links connecting the cores in a 3D NoC system The set of applications Application i The set of tasks of Ai The set of edges of Ai Edge e between two tasks A mapping function binding task t to core c The transmission time of data between two communicating tasks The traffic volume between two communicating tasks The Manhattan distance between two cores The power of the core c The thermal power capacity of the core c The overall running time of applications in the set S The average communication volume of each task in Ai point The corner index for Ai The number of occupied layers of an application The minimal distance to heat sink of an application The running time of an application The estimated running time of an application The estimated overall running time of a set applications The minimum overall running time of a set applications Symbol G (C , L) Gw G l Gh C L S Ai Ti e (i , j ) Ei M (t ) = c T T (e ) V (i , j ) D (i , j ) PM (c ) P (c ) σ (S ) CVi RP C Ii N O Li M D i RTi ERTi ˆσ (S ) ˆσ ∗ (S ) 3.3 The Thermal Power Capacity Model We adopted the thermal power capacity model (TPC) of [21]. As in [21], we define the TPC of a core as the maximum power the core can consume, given the power consumptions of other cores. It is used at runtime to estimate the power of a core given the power consumption of its neighboring eight cores with low computing cost. The TPC of each core can be determined offline. In the rest of the paper, we use PM (c ) and PM (x , y , z ) to denote the power capacity of the core c at the location (x , y , z ) interchangeably. The TPC of a core is bounded by the cooling capacity of the system, and the power consumption or temperature of other cores, i.e., thermal correlation. The TPC of a core c can be found as, PM (x , y , z ) = θ [P (x ± l1 , y ± l2 , z (2) where P (x ± l1 , y ± l2 , z ′) is the power consumption of the core c located at (x ± l1 , y ± l2 , z ′), which is thermally correlated with core c . The function θ (·) can also be found by linear regression, using the lasso method [10]. In particular, for each core at (x , y , z ) we only keep the coefficients of (1) adjacent cores and (2) those with the same Z coordinate as non-zero. That is, (x ± l1 , y ± l2 , z ′) with l1 , l2 = 0, 1, refering to cores that are neighboring to the core (x , y , z ), and z ′ = 0, 1, . . .Gh and z ′ (cid:44) z , refering to cores that have the same Z coordinate as core c . These cores have the highest thermal correlations with the core (x , y , z ). We set the coefficients of other cores to be 0, for core c . ′)] =  q αq · P (c ) 3.4 Problem Description The thermal- and communication-aware mapping problem is define as: Given a set of n applications in the system. Find a task-to-core mapping M (Ai ) for each application Ai in the 3D NoC system. Such that the overall running time of these applications is minimize while the system peak temperature is below threshold. Mathematically, the objective of our proposed algorithm is: min (3) subject to (4) where σ (S ) is the overall running time of the application set S , Al f and Af a are the finish time of the last application and the arrival time of the first application in the set, P (c ) is the power of a core and PM (c ) is the maximum power a core can consume which is computed from the TPC model. σ (S ) = Al f − Af a P (c ) ≤ PM (c ), ∀c ∈ C 4 THE PROPOSED THERMAL- AND COMMUNICATION-AWARE MAPPING ALGORITHM 4.1 Overview The proposed algorithm has two steps: (1) Finding a 3D cuboid core region of a specific shape for every application. (2) Determining the exact location of each application’s core region. 4.2 Finding the Shape of 3D Cuboid Core Regions 4.2.1 Characterizing the Shape of A Core Region. Two metrics, the minimal distance to heat sink (M D ) and the number of occupied 3.2 Application Model Each application is modeled as a directed graph A = (T , E ), where T is the set of tasks of the application and E is the set of directed edges representing data communication amongst the tasks. Each task ti ∈ T , where i = 1, 2 , ..., n , has a weight equal to its execution time. For Each edge e ∈ E , where j = 1, 2, ..., m, has a weight corresponding to data volume between the two communicating tasks, representing traffic. A mapping function M (t ) = c , for each t ∈ Ti , each c ∈ C binds tasks to the cores, such that task t is mapped to core c . Each edge e ∈ Ei has a weight of transmission time, after the two communicating tasks are mapped. The transmission time between two tasks i and j depends on the communication distance between the cores to which they are mapped, and their traffic volume. For each edge e = (i , j ), the transmission time can be modeled by Equation 1. T T (e ) = α · V (i , j ) + β · D (z , j ) (1) where V (i , j ) is the traffic volume between the two tasks i and j , and D (i , j ) is the distance between two cores to which the two tasks are mapped. α and β are regression coefficients of the linear regression model, which can be computed using the maximum likelihood method in [10]. The running time of each application i is the makespan of task graph, denoted as RTi . NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea B. Li et al. Figure 3: An example of minimal distance to heat sink (M D ) and the number of occupied layers (N O L) values of a core region. layers (N O L), are used to characterize the shape of a core region Layers closer to the heat sink have better cooling effect, which implies that cores in such layers can run with a higher V/F level and power consumption without violating the thermal constraint. Therefore, the distance to heat sink of a core region decides the peak temperature and computation performance. We use the minimal distance to heat sink defined below to reflect the average distance to heat sink of a core region. For example, the M D value of the core region for the 8-task application in Fig. 3 is 1. Definition 4.1. The minimal distance to heat sink (M D ) is the index of the lowest layer of Ai ’s core region. On the other hand, the number of occupied layers of a core region decides the number of TSV links in a core region corresponding to how many vertical links are used in a core region to accelerate communication. For an application whose number of tasks is fixed, a “taller” core region has more TSV links than a “shorter” one. Thus, a taller core region has lower network communication latency. That is, a larger “NOL” indicates a lower communication latency. For example, the N O L of the 8-task application in Fig. 3 is 2. Definition 4.2. The number of occupied layers value N O Li of application Ai ’s core region equals to its number of layers. 4.2.2 Estimation of an Application’s Running Time. The running time of an application depends on the execution time of each task and the communication latency. The execution time of each task can be modeled by the M D metric and the communication latency can be modeled by the N O L metric as discussed in Section 4.2.1. Consider the trade-off between model accuracy and algorithm runtime overhead, the running time of core region could be estimated by a linear regression model of M D and N O L as in Equation 5, ERTi = a0 + a1 × |Ai | + a2 × CVi + a3 × N O Li + a4 × M D i (5) where |Ai | is the number of tasks in application Ai , CVi is the average communication volume of each task in Ai , N O Li is the number of occupied layers of the core region, and M D i is the M D of the core region. To find the coefficients a0 , a1 , a2 , a3 , a4 , the maximum likelihood methods can be used [10]. In this step, a core region of a specific shape is selected for each application, by tuning its M D and N O L values according to its communication and computation demands. The branch-and-bound algorithm is used to find the best tree node corresponding to the best combination of shapes of core region for this set of application set. 4.2.3 Search tree definition. Assume n applications are to be mapped, a tree node is defined as a 2n-vector. A tree node is defined by Nm = ⟨N O L0 , M D 0 , . . ., N O Ln , M Dn ⟩ , corresponding to j the N O L and M D metrics of each application’s core region.The tree is grown by branching new nodes from the the root down to the leaves level by level, corresponding to setting the M D i and N O Li values for each application’s core region. Each non-leaf tree node N j is associated with ˆσ (S )m i n , indicating the minimal estimated running time of N j . The weight ˆσ (S ) of each leaf node can be computed as ˆσ (S ) = maxAi ∈S {ERTi } for all applications, that is, the time when the last application in set S finishes execution. The basic operations for the search tree include branch and cut. Branching of the search tree. If a tree node is a leaf node, all the applications are mapped. We compare the ˆσ (S ) of this tree node with ˆσ ∗ (S ). If this tree node is not a leaf node, a new tree node should be created. The M D i and N O Li of new application Ai are grounded by the following max and min N O Ls and M D s. (cid:25) (cid:24) = |Ai | Gw ×G l N O Lm i n i N O Lma x i M Dm i n i M Dma x i (6) i i i × N O Lma x i × M Dm i n i × M Dma x i N O Lm i n i = min {Gh , |Ai | } N O Lm i n i = Gh − N O Lma x = Gh − N O Lm i n i i (7) (8) (9) where |Ai | is number of tasks of Ai and Gw , G l and Gh are the width, length and height of the 3D NoC system, respectively. is the minimum number of occupied layers of a core region for an application such that the application’s tasks can be accommodated in the core region. N O Lma x is the upper limit of the core region’s number of occupied layers, which is bounded by the maximum number of vertical layers in the 3D NoC. M Dm i n are the maximum and minimum M D values of a core region. Once N O Lm i n are determined, they can be computed in a manner such that the core region can fit inside the 3D NoC. In total, for each non-leaf tree node, a maximum of tree nodes in the next and N O Lma x i and M Dma x i level can be created. Cutting. To reduce the search space and speed up the tree search process, some tree branches should be cut. For each of the newly created tree node, we check whether they should be discarded or not according to the following two cut rules. Rule 1) Cut infeasible nodes. An infeasible tree node refers to a setting of M D and N O L of the core regions that they do not fit into the 3D NoC, e.g., the number of cores of a region in a layer is larger than the total available cores in that layer. The number of available cores in each layer is maintained and updated at runtime to test the feasibility. Once some applications are mapped to run, the number of available cores in each layer is subtracted by the amount equal to the core count occupied by the applications in that layer. For the vertical direction, it is feasible if the sum of M D and N O L is less than or equal to Gh . Rule 2) Cut by node dominance. Once a new tree node N j is created, its ˆσ (S )m i n is compared with ˆσ ∗ (S ). If its ˆσ (S )m i n is larger than ˆσ ∗ (S ), which means the minimum overall application running time of N j is longer than the best overall running time found so far among all the tree nodes, the new tree node is discarded. Algorithm 1 shows how the search works. The tree nodes are stored in a working queue. A dummy root node is pushed to the queue at the start. In each iteration, new tree nodes are created by j j On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea Algorithm 1 Finding the Shape of Core Region for Each Application Input: S , the set of unmapped applications Output: M D i and N O Li values of each application Ai W Q : A working queue, initialized empty; N BN : The newly branched node BN : The best node during search; ˆσ ∗ (S ): A value keeps the minimum overall running time over all the tree nodes searched so far; while W Q is not empty do pop the top node Nq out of W Q ; if Nq is not a leaf node then branch new tree nodes; for each newly branched nodes N BN do if no cutting rules are met for N BN and W Q is not full then store N BN in W Q ; end else if ˆσ (S )q < ˆσ ∗ (S ) then ˆσ ∗ (S ) = ˆσ (S )q ; BN = Nq ; end i × M Dm i n i × N O Lma x i Figure 4: Four corners in one layer of a 3D NoC system assigning different M D i and N O Li values to a new application’s core region. In total N O Lm i n new tree nodes are created. For each of these tree nodes, if they do not meet the above two cutting rules, they are pushed to the queue. The length of the queue can be tuned to trade off the running speed and result optimality of the search algorithm. In this search process, applications with more traffic are possibly assigned with a region with larger N O L, while the computation intensive applications are possibly assigned with a region with smaller M D . × M Dma x i 4.3 Finding the Locations for the 3D Cuboid Core Regions In this step, the exact location of each core region in the chip is found. The goals in this step are to 1) keep the applications scattered to reduce peak temperature, and 2) reduce fragmentation. To achieve these goals, the core regions are placed at one of the four corners of the chip in a round-robin manner. Algorithm 2 shows how the location finding step works. First, the applications are sorted according to their number of tasks in a descending order, i.e., applications with more tasks are treated earlier. Next, the applications’ core regions are placed in a round-robin manner to one of the four corners as in Fig. ?? at each iteration. We use C Ii as the corner index for application Ai . Each corner is associated with a reference point, RP , indicating its start point coordinate. Starting from RP , we first scan along the Algorithm 2 Finding the Exact Location of Core Regions Input: S , the set of unmapped applications Output: M (Ai ) ∀Ai ∈ S , the task to core mapping for each application sort applications in S according to their number of tasks in a descending order; set C Ii as 1; for the core region of each Ai ∈ S do select RP , and scan along the X and Y directions according to the above four rules, with the constraint that it fits inside the 3D NoC; map the tasks of the application to the cores in that region; C Ii = C Ii % 4 + 1; end N O L i X dimension, then the Y dimension, until a region is found that has ⌈ |Ai | ⌉ free cores at layers M D i , . . ., M D i + N O Li , detailed as follows. (1) corner 1: RP = (0, 0, M D i ). Search location along x + direction, followed by y+ direction. (2) corner 2: RP = (Gw , G l , M D i ). Search location along x - direction, followed by y - direction. (3) corner 3: RP = (0, G l , M D i ). Search location along x - direction, followed by y+ direction. (4) corner 4: RP = (Gw , G l , M D i ). Search location along x + direction, followed by y - direction. Then, tasks of the application are mapped to each of the cores inside the free core region using existing mapping algorithms, for example, the one in [23]. This algorithm results in a contiguous free core region in the center of the chip as in Fig. ??. Therefore, fragmentation is alleviated. Besides, since the applications are mapped such that they are separated in the four corner, the peak temperature is also reduced. 4.4 Example There are 2 applications A0 and A1 in a 2×2×2 3D NoC system to be mapped. Fig. 4.4 (c) is a snapshot of a search tree for finding the shapes of core regions for A0 and A1 . Each tree node is represented by a 4-element vector < N O L0 , M D 0 , N O L1ThM D 1 >. A dummy root node is first created. For A0 , a new tree node N0 in level 1 is created from the root with M D 0 = 0 and N O L0 = 1. Since A0 has 3 tasks, it have 3 cores in layer 0. Next, a new node N1 in level 2 is created for A1 from N0 , with M D 1 = 0 and N O L1 = 2. Since A1 has 2 tasks, it has 1 core in both layers 0 and 1. Since the system has 4 cores in each layer, A0 and A1 fit into the system. Fig. 4.4(d) shows an infeasible tree node N2 in level 2. It requires 5 cores in layer 0 and thus exceeds the maximum number of available cores in layer 0. Thus, N1 is kept as the result of searching. Next, the locations of the applications’ core regions are found with the M D 0 , N O L0 , M D 1 , N O L1 shown in N1 . Since A0 has one more tasks than A1 , A0 is mapped first. Since M D 0 = 0 and N O L0 = 1, layer 0 needs 3 free cores for A0 . Starting from corner 1, i.e., RP is (0, 0, 0), then scan by X + direction and 3 available cores are found for A0 ’s core region. Now, for A1 , it needs 1 available core in both layers 0 and 1 for A1 . This time we start from corner 2, i.e., RP is (1, NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea B. Li et al. Table 2: Configurations of the Simulation Network Parameters Flit size 128 bits Latency Router 2 cycles, link 1 cycle Buffer depth 4 flits Routing algorithm XYZ routing 8 × 8 × 4 Baseline topology Random Benchmark Parameters Number of tasks [15, 45] Communication volume [10, 200] (Kbits) Degree of tasks [1, 15] Task number distribution Bimodal, uniform Configuration of the Extracting Trace Many-core Simulator Core Architecture 64 bit Alpha 21264 Baseline Frequency 3GHz Fetch/Decode/Commit size 4/4/4 ROB size 64 L1 D cache (private) 16KB, 2-way, 32B line 2 cycles, 2 ports, dual tags 32KB, 2-way 64B line, 2 cycles 64KB slice/core, 64B line 6 cycles, 2 ports Main memory size 2GB Task Graphs of Real Applications Barnes, Canneal, Raytrace, Dedup, Ferret, Freqmine Streamcluster, Fluidanimate, Swaptions, Blackscholes Hotspot Parameters Die size [mm] Specific heat capacity [ J/(m3 × K)] Resistivity [(m-K)/W] Layer 0 thickness [mm] Layer 1 thickness [mm] Layer 2 thickness [mm] Layer 3 thickness [mm] 0.5 × 0.5 1.75e6 0.01 0.10 0.12 0.14 0.16 L2 cache (shared) MESI protocol L1 I cache (private) a 8×8 NoC-based cycle accurate many-core simulator in [22]. The configuration of random benchmarks, real benchmarks, and the 3D NoC system are listed in Table 2. The temperature threshold is 60 ℃. It is acceptable to take any reasonable temperature threshold with a re-train thermal model. HotSpot [12] is used as the temperature simulator. We used different layer thickness to simulate the different heat transmit capacity, the Hotspot parameters are showed in Table 2. We compare our approach with the following two runtime mapping approaches, the Bottom-2-Top (B2T) method [6] and the fuzzy logic (FL) method [15]. The B2T scheme first maps all the tasks to the bottom layer in a 3D NoC to make the power consumption of cores in each vertical stack (cores with the same Z coordinate) identical as possible, and then moves low power tasks to the top layer to reduce the execution time [6]. FL defines three variables, i.e. heat transfer, distance from source core, and distance from hot spot and used rules to set the priorities of them. “Distance from Figure 5: (a) Two applications (A0 , A1 ) to be mapped. (b) The ranges of M D and N O L values of the two applications. (c) Search tree in step 1. (d) An infeasible tree node. (e) System after mapping. 1, 0) and an available core is found in each of the two layers for A1 . The locations of the the two core regions are shown in Fig. 4.4(e). 4.5 Cost Analysis The worse case complexity of the first step of the algorithm is O (F ), where F = min{ |W Q | , G 2n h } , and |W Q | is the maximum length of the working queue W Q . The complexity of the second O (∀Ai ∈S |Ai | 2 × |E | × |C | ), respectively. step includes the complexity of finding core region locations and that of mapping n applications, which are O (n × Gw × G l ) and Overall, the complexity of the whole algorithm is O (B ), where B = max{F , |Ai | 2 × |E | × |C | } .  ∀Ai ∈S 5 EXPERIMENTAL RESULT 5.1 Experimental Setup Experiments are performed on an event-driven C++ NoC simulator, with DSENT integrated as the power model. The simulator models the packet latency of the communications in a cycle accurate manner. The horizontal and vertical link bandwidth considered for simulation are 10 and 200. For example, to finish 200 units communication volume, horizontal link needs 20 cycles and vertical link needs 1 cycle. For processor-to-memory latency, we assumed the code and data are stored in the L1 cache and this latency is ignored considering it will be very small. The experiments are carried out on various 3D NoC systems. In the experiments, we used two kinds of benchmarks: benchmarks whose task graphs are randomly generated and real benchmark. The task graphs of the real applications are generated from the traces of SPLASH-2 [24] and PARSEC [3], which are collected by executing these applications in On Runtime Communication- and Thermal-aware Application Mapping in 3D NoC NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea source core” represents the distance inter tasks within the application. “Heat transfer” is the heat transfer capability of specific core. “Distance from hot spot” is the distance between the hottest core and specific core. During the experiment, we set the rules as considering “distance from source core” first, then “heat transfer”, and finally “distance from hot spot”. That is, FL maps tasks within application to cores of shorter distance first. If shorter distance cores are not available, then it uses the cores near the heat sink. If shorter distance cores and near heat sink cores are not available, it uses cores far from the hot spot. 5.2 Validating the Application Running Time Estimation Model ϵ = |RT − ERT | RT The error of each application’s running time estimation model is defined as follow, × 100% (10) where RT and ERT are the running times obtained from the simulator and the estimation model in Equation 5 for each application, respectively. The error of this estimation model is 4.82% on average, for the applications used in the experiments. Thus, the estimation is fairly accurate. Figure 6: Comparison with different sizes of 3D NoC Figure 7: Comparison with different number of tasks of applications 5.3 Performance Comparison 5.3.1 Experiments with Random Benchmarks. Experiments with Different Sizes of 3D NoC. To evaluate the proposed algorithm, we changed the size of 3D NoC to evaluate the overall running time and communication latency of the three algorithms. Fig. 6 shows that our proposed algorithm outperforms the other two when the network size increases. On average, the overall running times of B2T and FL are 1.39× and 1.42× over our proposed approach, respectively. Fig. 6 also shows that the communication latencies of B2T and FL are 1.55× and 1.24× over our proposed approach, respectively. The reason is that our algorithm can optimize the communication and computation performances for each application by selecting the appropriate M D and N O L values of its core region, according to its communication and computation demands. It can also reduce fragmentation and peak temperature by the core region location finding step. For every application, the B2T method makes maximum use of the layer that is close to the heat sink and thus it might lead to a scenario that all the cores close to the heat sink are occupied and the upcoming application has to be mapped to other layers. Further, B2T maps applications in close proximity, resulting in higher accumulated heat. Experiments with Different Number of Tasks. In this set of experiments, we changed the average number of tasks of the applications. Fig. 7 shows that, when the average number of tasks is large, e.g. 128, the overall running time of B2T and FL are 1.29× and 1.39× over our proposed approach, respectively. Fig. 6 also shows that the communication latencies of B2T and FL are 1.42× and 1.38× over our proposed approach, respectively. Experiments with Different Communication Volumes. In this set of experiments, we compared the performances with 4 different communication volumes: 50, 100, 150, 200 (Kbits). Fig. 8 shows that, when the communication volume is large, e.g. 200, the overall running time of B2T and FL are 1.38× and 1.25× over our proposed Figure 8: Comparison with different communication volumes Figure 9: Comparison with real benchmarks approach, respectively. Fig. 6 also shows that the communication latencies of B2T and FL are 1.47× and 1.13× over our proposed approach, respectively. 5.3.2 Experiments with Real Benchmarks. To evaluate the proposed framework, we compared the performances on real benchmarks with 4 different network sizes. Fig. 9 shows that, when the network size is large, e.g. 20 × 20 × 4, the overall running time of B2T and FL are 1.38× and 1.48× over our proposed approach, respectively. Fig. 9 also shows that the communication latencies of B2T and FL are 1.44× and 1.24× over our proposed approach, respectively. Fig. 9 also compares the peak temperature of different algorithms, showing that our algorithm reduces the peak temperature for B2T and FL by 3o C and 7℃. All of the three mapping algorithms are below the 60℃ thermal threshold. We also captured the normalized communication cost and running time of each real benchmark application , as shown in Fig. 10. Numbers 0 to 9 represent barnes, blackscholes, canneal, dedup, ferret, fluidanimate, freqmin, raytrace, streamcluster and swaptions respectively. 5.3.3 Runtime Overhead of the Proposed Algorithm. The average runtime overheads of B2T, FL and our algorithm are in the order of 3.5M, 5M and 4M cycles for one real benchmark, which are averaged NOCS ’17, Oct. 19–20, 2017, Seoul, Republic of Korea B. Li et al. [13] Guruprasad Katti, Michele Stucchi, Kristin De Meyer, and Wim Dehaene. 2010. Electrical modeling and characterization of through silicon via for threedimensional ICs. IEEE Trans. Electron Devices 57, 1 (2010), 256–262. [14] Jiayin Li, Meikang Qiu, Jian-Wei Niu, Laurence T Yang, Yongxin Zhu, and Zhong Ming. 2013. Thermal-aware task scheduling in 3D chip multiprocessor with real-time constrained workloads. ACM Trans. Embedded Computing Systems 12, 2 (2013), 24:1âĂŞ22. [15] Amin Mosayyebzadeh, Maziar Mehdizadeh Amiraski, and Shaahin Hessabi. 2016. Thermal and power aware task mapping on 3D network on chip. Computers & Electrical Engineering 51 (2016), 157–167. [16] Makoto Motoyoshi. 2009. Through silicon via (TSV). Proc. the IEEE 97, 1 (2009), 43–48. [17] Jim Ng, Xiaohang Wang, Amit Kumar Singh, and Terrence Mak. 2015. DeFrag: defragmentation for efficient runtime resource allocation in NoC-based manycore systems. In Proc. Int’l Conf. Parallel, Distributed and Network-Based Processing. 345–352. [18] Dongkook Park, Soumya Eachempati, Reetuparna Das, Asit K Mishra, Yuan Xie, Narayanan Vijaykrishnan, and Chita R Das. 2008. MIRA: a multi-layered on-chip interconnect router architecture. In ACM SIGARCH Computer Architecture News, Vol. 36. 251–261. [19] Vasilis F Pavlidis and Eby G Friedman. 2007. 3D topologies for networks-on-chip. IEEE Trans. Very Large Scale Integration Systems 15, 10 (2007), 1081–1090. [20] Chong Sun, Li Shang, and Robert P Dick. 2007. Three-dimensional multiprocessor system-on-chip thermal optimization. In Proc. IEEE/ACM Int’l Conf. Hardware/software Codesign and System Synthesis. 117–122. [21] Xiaohang Wang, Amit Kumar Singh, Bing Li, Yang Yang, Terrence Mak, and Hong Li. 2016. Bubble budgeting: Throughput optimization for dynamic workloads by exploiting dark cores in many core systems. In Proc. IEEE/ACM Int’l Symp. Networks-on-Chip. 1–8. [22] Xiaohang Wang, Mei Yang, Yingtao Jiang, Peng Liu, Masoud Daneshtalab, Maurizio Palesi, and Terrence Mak. 2014. On self-tuning networks-on-chip for dynamic network flow dominance adaptation. ACM Trans. Embedded Computing Systems 13, 2 (2014), 1–8. [23] Xiao-Hang Wang, Peng Liu, Mei Yang, Maurizio Palesi, Ying-Tao Jiang, and Michael C Huang. 2013. Energy efficient run-time incremental mapping for 3D networks-on-chip. J. Computer Science and Technology 28, 1 (2013), 54–71. [24] Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. 1995. The SPLASH-2 programs: Characterization and methodological considerations. In Proc. Annual International Symp. Computer Architecture. IEEE, 24–36. [25] Changyun Zhu, Zhenyu Gu, Li Shang, Robert P Dick, and Russ Joseph. 2008. Three-dimensional chip-multiprocessor run-time thermal management. IEEE Trans. Computer-Aided Design of Integrated Circuits and Systems 27, 8 (2008), 1479–1492. [26] Di Zhu, Lizhong Chen, Timothy M Pinkston, and Massoud Pedram. 2015. TAPP: temperature-aware application mapping for NoC-based many-core processors. In Proc. Design, Automation & Test in Europe Conf. & Exhibition. 1241–1244. [27] Zuomin Zhu, Vivek Chaturvedi, Amit Kumar Singh, Wei Zhang, and Yingnan Cui. 2017. Two-stage thermal-aware scheduling of task graphs on 3D multi-cores exploiting application and architecture characteristics. In Proc. Asia and South Pacific Design Automation Conf. 324–329. [28] Hesamedin Ziaeeziabari and Ahmad Patooghy. 2017. 3D-AMAP: a latency-aware task mapping onto 3D mesh-based NoCs with partially-filled TSVs. In Proc. Int’l Conf. Parallel, Distributed and Network-based Processing. 593–597. Figure 10: Comparison for each real benchmark by running each of the algorithms for fifty times with different real benchmark applications. The execution times of the benchmark applications are much longer than 4M cycles. Thus the overhead of the proposed algorithm is acceptable. 6 CONCLUSION In this paper, we proposed a runtime communication- and thermalaware mapping algorithm to optimize performance under the thermal constraint in 3D NoCs. Experimental results show that our proposed approach can reduce up to 48% overall running time compared to existing mapping algorithms. 7 ACKNOWLEDGEMENT This research program is supported by the Natural Science Foundation of China No. 61376024 and 61306024, Natural Science Foundation of Guangdong Province 2015A030313743, Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase), and the Science and Technology Research Grant of Guangdong Province No. 2016A010101011 and 2017A050501003. "
Addressing Extensibility and Fault Tolerance in CAN-based Automotive Systems.,"The design of automotive electronic systems needs to address a variety of important objectives, including safety, performance, fault tolerance, reliability, security, extensibility, etc. To obtain a feasible design, timing constraints must be satisfied and latencies of certain functional paths should not exceed their deadlines. From functionality perspective, soft errors caused by transient or intermittent faults need to be detected and recovered with fault tolerance techniques. Moreover, during the lifetime of a vehicle design or even the same car, updates are often needed to add new features or fix bugs in existing ones. It is therefore critical to improve the design extensibility for accommodating such updates without incurring major redesign and re-verification cost. In this work, we discuss the metrics for measuring latency, fault tolerance and extensibility, and present a simulated annealing based algorithm to search the design space with respect to them. Experimental results on industrial and synthetic examples demonstrate clear trade-offs among these objectives, and hence the importance of quantitatively analyzing such trade-offs and exploring the design space with automation tools.","Addressing Extensibility and Fault Tolerance in CAN-based Automotive Systems Special Session Paper Hengyi Liang University of California, Riverside Riverside, CA hlian010@ucr.edu Bowen Zheng University of California, Riverside Riverside, CA bzhen003@ucr.edu Zhilu Wang University of California, Riverside Riverside, CA zwang055@ucr.edu Qi Zhu University of California, Riverside Riverside, CA qzhu@ece.ucr.edu ABSTRACT The design of automotive electronic systems needs to address a variety of important objectives, including safety, performance, fault tolerance, reliability, security, extensibility, etc. To obtain a feasible design, timing constraints must be satis￿ed and latencies of certain functional paths should not exceed their deadlines. From functionality perspective, soft errors caused by transient or intermittent faults need to be detected and recovered with fault tolerance techniques. Moreover, during the lifetime of a vehicle design or even the same car, updates are often needed to add new features or ￿x bugs in existing ones. It is therefore critical to improve the design extensibility for accommodating such updates without incurring major redesign and re-veri￿cation cost. In this work, we discuss the metrics for measuring latency, fault tolerance and extensibility, and present a simulated annealing based algorithm to search the design space with respect to them. Experimental results on industrial and synthetic examples demonstrate clear trade-o￿s among these objectives, and hence the importance of quantitatively analyzing such trade-o￿s and exploring the design space with automation tools. CCS CONCEPTS • Computer systems organization → Embedded systems; Embedded software; ACM Format: Hengyi Liang, Zhilu Wang, Bowen Zheng, and Qi Zhu. 2017. Addressing Extensibility and Fault Tolerance in CAN-based Automotive Systems. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130233 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro￿t or commercial advantage and that copies bear this notice and the full citation on the ￿rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci￿c permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130233 1 INTRODUCTION The design of automotive electronic systems has become increasingly challenging due to large design space and stringent design requirements. The development of autonomous and semi-autonomous features, as well as vehicle connectivity functionality, requires more complex automotive software and hardware. In addition, the underlying architecture platform is shifting from the traditional federated architecture, where each function is deployed to one ECU (Electronic Control Unit) and provided as a black-box by a Tier-1 supplier, to the integrated architecture, where one function can be distributed over multiple ECUs and multiple functions can be supported by one ECU. Model-based design (MBD) methodology has been proposed to address the deign challenges in complex systems such as vehicles and avionic systems [13, 14]. In MBD, system functionality is ￿rst captured with formal or semi-formal models for early-stage analysis and validation. These functional models are then mapped onto an architectural platform (often also captured with models) for software or hardware implementation. For automotive electronic systems, this mapping/synthesis process involves generating software tasks from functional models (sometimes through another layer of runnables), allocating tasks onto ECUs connected with buses (such as CAN [7, 15, 22] or FlexRay [3, 16]), and scheduling the execution of tasks and the transmission of bus messages (Figure 1). During this process, a variety of design objectives, such as safety, performance, fault tolerance, reliability, security and extensibility, need to be addressed. Extensibility: A major challenge in vehicle design is to cope with software and hardware evolutions over the lifetime of a design or across multiple versions in the same product family or even for the same car. Updates such as adding new application software, reallocating some software among ECUs, or adding a new ECU are needed to ￿x bugs and provide new functionality. Due to the fast development of automotive applications, such updates (especially software updates) are expected to be more frequent. For instance, Tesla has already been able to carry out regular software updates over-the-air since version 8.1 [23]. However, small changes in software and hardware may cause big and unexpected changes in system timing and functionality. It is often necessary to re-verify and re-certify the entire system, which NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hengyi Liang, Zhilu Wang, Bowen Zheng, and Qi Zhu systems that are based on CAN, the prevalent bus protocol currently in vehicles. For tasks on the same ECU, the communication is through local memory and very fast. For tasks on di￿erent ECUs, the communication is through CAN bus messages/frames and the transmission time is much longer. Intuitively, maximizing extensibility or fault tolerance may lead to a more “balanced” task allocation and therefore more bus messages and longer path latencies. To quantitatively evaluate such trade-o￿s, we ￿rst de￿ne timing models for software tasks, messages and schedulability constraints, and metrics for extensibility and fault tolerance. We then optimize these metrics with a simulated annealing based approach, and conduct experiments with industrial and synthetic examples. The rest of the paper is organized as follows. In Section 2, we discuss previous work on extensibility and fault tolerance. In Section 3, we introduce our system models on timing/latency, extensibility and fault tolerance. In Section 4, we demonstrate the trade-o￿s among these objectives with an illustrating example, and then introduce our simulated annealing-based algorithm for optimizing them. We present experimental results and discuss our ￿ndings in Section 5, and conclude the paper in Section 6. 2 RELATED WORK In the literature, a number of studies have addressed robustness, scalability, ￿exibility and extensibility of real-time embedded system. The notions of these objectives could sometimes be obfuscated since they all relate to system’s capability of accommodating changes (which could come from variations or updates). For instance, in [25], scalability refers to how well a system can handle task execution time increases. In [1], ￿exibility describes system’s ability to add additional tasks without impeding existing ones. Various viewpoints and de￿nitions have also been proposed for system extensibility. In [25], Yerraballi et al. develop a method to ￿nd an optimal execution time scaling factor for all tasks in a given subset while ensuring system schedulability. In [21], novel de￿nitions of sustainability and extensibility for FlexRay-based communication systems are presented, which can then be combined with CAN-based system. In [10], although no formal de￿nition of extensibility is provided, an original approach utilizing contractbased design is proposed to negotiate among contracts for software updates. In this work, we adopt the task-level extensibility metric from [27], which measures how much task execution time can be increased without violating design constraints. Regarding fault tolerance, many error detection techniques, such as triple modular redundancy, watchdog timers and instruction signature checking, have been proposed [4, 5, 11, 12, 17–20]. For instance, in [12, 20], Izosimov et al. employ process re-execution and replication to tolerate transient faults and then extend their algorithm by checkpointing with rollback recovery. In [11], they develop a heuristic algorithm to trade-o￿ between hardware hardening and re-execution in software. In [4, 5], Burns et al. propose schedulability analysis and priority assignment with embedded error detection techniques. In our previous work [26], we formulate the impact of EOC and EED on system timing for di￿erent platform con￿gurations. An Figure 1: Mapping of software tasks onto ECUs connected with a CAN bus. The signals communicated between tasks are mapped to either local communication through memory or CAN messages. could lead to prohibitively expensive costs and undermine system availability and reliability. Therefore, it is important to improve the extensibility of designs so that future updates can be accommodated without incurring major redesign and re-veri￿cation cost. This is a challenging goal, especially due to the sharing and contention of software functions over limited computation and communication resources. Fault tolerance: Soft errors caused by transient or intermittent faults have become a major design concern, because of the continuous scaling of technology, high energy cosmic particles and radiation from the application environment [2, 24]. Fault tolerance techniques are greatly needed to detect and recover errors such as application crashes, illegal branches and silent data corruption. In this paper, similarly as in [8, 26], we focus on two categories of error detection techniques: embedded error detection (EED) and explicit output comparison (EOC). More speci￿cally, EED includes a variety of error detection techniques such as instruction signature checking, control ￿ow check (CFC) and watchdog timers [17]. EOC detects errors through explicit redundancy of task execution. For instance, the same program can be executed twice and output mismatch indicates occurred error(s) [8]. Choosing EOC or EED techniques for speci￿c tasks could signi￿cantly improve system’s ability to tolerate soft errors. In addition to extensibility and fault tolerance, there are often timing constraints that must be satis￿ed to ensure functional correctness and system safety, such as task execution deadlines, message transmission deadlines, and latency deadlines along functional paths. In this work, we discuss the metrics for measuring extensibility, fault tolerance and latency, optimize them in task allocation and scheduling, and analyze their trade-o￿s. We consider automotive Addressing Extensibility and Fault Tolerance in CAN-based Automotive SystemsNOCS ’17, October 19–20, 2017, Seoul, Republic of Korea MILP (mixed integer linear programming) model is then developed to explore task allocation and scheduling, together with the selections of error detection techniques for individual tasks. 3 SYSTEM MODEL In our system model, the CAN-based architectural platform includes a set of p ECUs E = {e1 , e2 , . . . , ep } connected through a CAN bus. The functional model is represented as a task graph G = {T , S } , where T = { 1 ,  2 , . . . ,  n } is the set of tasks and S = {s1 , s2 , . . . , sm } is the set of signals that impose data dependency and execution order among tasks. We assume all tasks are invoked periodically and scheduled based on static priorities with preemptions allowed. Each task  i has its own activation period T i , worst-case execution time (WCET) c i and priority p i . If two tasks are allocated to the same ECU, signals are transmitted through local memory and we assume the communication delay is negligible. If two dependent tasks are mapped onto di￿erent ECUs, data will be exchanged through messages/frames on the CAN bus. The set of CAN messages is denoted In the task graph, a path is an interleaving sequence of tasks as M = {m1 , m2 , . . . , mq } . and signals denoted as p = [ r 1 , sr 1 ,  r 2 , sr 2 , . . . , srk  1 ,  rk ].  r 1 , the source node of the path, is usually triggered by external events such as sensor inputs. The sink node  rk is often the task that activates actuators. It is possible that multiple paths exist between a source task and a sink task. 3.1 Worst-case End-to-end Path Latency We de￿ne worst case end-to-end latency lp of a path p as the maximum time delay needed for the input changes on the source node to be propagated to the outputs of the sink node. To ensure system safety and performance, a deadline dp may be imposed on lp , i.e. lp  dp . The computation of lp requires the computation of worst-case response time for tasks and messages along the path, as explained in below. Task worst-case response time: In our model, tasks running on the same ECU are scheduled based on static priorities with preemptions (commonly supported by OSEK standard and its derivatives). The execution of a task is subject to the interferences from higher priority tasks on the same ECU. Therefore, the worst-case response time r i of a task  i , which represents the longest time delay needed to complete the task after its activation, can be calculated as follows (similarly as in [9, 28]): r i = c i + X  j 2hp ( i ) & r i T  j ' c  j (1) where hp ( i ) denotes the set of higher priority tasks on the same ECU. The second term represents the interferences from these higher priority tasks within the response time. This formula can be solved with an iterative numerical method. Message worst-case response time: In our model, when two tasks communicating through signals are allocated to di￿erent ECUs, their communication signals are packed into messages and transmitted over the CAN bus. We further assume each signal si is mapped to its own message mi . The transmission delays of these messages contribute signi￿cantly to the path latencies, and can be calculated similarly as tasks. Slightly di￿erent from the preemptive task scheduling policy though, CAN bus employs a ￿xed priority non-preemptive scheduling. Thus, a CAN message may su￿er from additional blocking delay caused by lower priority messages, which can be approximated with the largest possible transmission time among all messages transfered on the same CAN bus. Equation (2) below is the formula for calculating message worst-case response time rm i , where Bma x is the largest blocking time and cm i is the worst-case transmission time of the messages. rm i = cm i + Bma x + Xm j 2hp (m i ) & rm i   cm i Tm j ' cm j (2) Path latency: The worst-case end-to-end path latency lp of path p is the summation of the periods and worst-case response times of all tasks and global signals (i.e., signals that are packed into CAN messages) on the path, as shown below in Equation (3). GS is the set of global signals. Note that in our model, a global signal has the same worst-case response time as its corresponding message, i.e. r s i = rm i . The periods are taken into account because of the asynchronous communication nature. lp = X i 2p (r i + T i ) + Xs i 2p ^s i 2G S (r s i + Ts i ) (3) Schedulability: In this work, a system is schedulable if all the timing constraints shown below in (4) to (6) are met. Constraint (4) ensures that the response time of every task is not greater than its deadline, which equals to its period in our model. Similarly, Constraint (5) ensures that every message is transmitted within its period. Constraint (6) ensures that the end-to-end latency of every path will not exceed its deadline. 8 i 2T , r i  T i 8m j 2M , rm j  Tm j 8pk 2P , lpk  dpk (4) (5) (6) 3.2 Task Level Extensibility We adopt the task level extensibility metric from [27], which measures how much task WCET can be increased without violating design constraints. More speci￿cally, we calculate system extensibility as the weighted sum of each task’s maximum possible increase of its WCET: 1 (7) E = |T | X i 2T w i  c i T i where w i is a predetermined value that indicates how likely a task’s WCET might be increased in future updates.  c i is the maximum possible increase of task WCET c i without violating design constraints (i.e., schedulability constraints (4) to (6) in this work), while all other system con￿gurations remain unchanged. A binary search based algorithm is used to compute the extensibility, as shown in Algorithm 1. In this algorithm, E denotes the system extensibility and is initialized to zero. For every task  i , we use binary search to calculate how much its WCET c i can be increased, as shown from line 2 to line 11. During the binary search, the lower bound lb is initially set to 1, representing the normalized factor with respect to the original WCET; while the upper bound ub is initially set to T i /c i , representing the normalized factor with NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hengyi Liang, Zhilu Wang, Bowen Zheng, and Qi Zhu Algorithm 1: System Extensibility Computation l b = 1; u b = T i /c i ; co r i   i n a l = c i m i d = (l b + u b ) /2; 1: E = 0; 2: for all task  i 2T do 3: 4: while u b   l b >   do 5: 6: 7: 8: 9: 10: 11: 12: 13: return E / | T | ; u b = m i d ; else c i = m i d ⇤ co r i   i n a l ; i s S c h e d = checkTaskSched(); if i s S c h e d == t r u e then l b = m i d ; E += w i ⇤ (m i d   1) ⇤ co r i   i n a l /T  i ; c i = co r i   i n a l respect to the task deadline/period. The iterations end when the upper bound and lower bound meet within   . Inside each iteration, we calculate the middle value mid (line 5) and update WCET c i (line 6). Then, function checkT askSched ( ) updates all the response times of lower priority tasks, and checks whether any schedulability constraint has been violated (line 7). If the system is schedulable, we continue search the upper half (i.e., trying larger value for the execution time), otherwise we search the lower half. The system extensibility is the weighted sum of all tasks. 3.3 Soft Error Tolerance Model We consider two major soft error detection techniques, i.e. embedded error detection (EED) and explicit output comparison (EOC). Usually, EED covers part of the total errors with additional computation overhead (which depends on speci￿c application and implementation method). For instance, state-of-the-art CFC techniques may cover 70% of total errors. EOC can achieve almost 100% error detection at the cost of 100% execution time overhead (temporal redundancy) or 100% resource overhead (spatial redundancy). In this work, we assume EOC detection rate is 100%, similarly as in [26]. System error coverage: During the hyperperiod Th p e r of a task set T (i.e., the least common multiple of the task periods), a total number of K   0 errors may occur. System error coverage is then de￿ned as the probability that all errors are either i) detected and recovered within hyperperiod while all timing constraints are satis￿ed or ii) happened during idle time [26]. Let te oc , te e d , tnon e denote the accumulative time needed by tasks employing EOC, EED and no error detection technique, respectively. ti d l e denotes the total idle time. An exact analysis of system error coverage depends on the speci￿c error occurrence pro￿le and timing pattern, and is hard to capture with a closed form formulation. For simplicity, on a single ECU, we assume that K arbitrary errors of uniform distribution may occur during a hyperperiod. The system error coverage P is then approximated as: P ⇡ KXi =0 iXj =0 K i ! i j ! (   · te e d Th p e r ) j (   · te oc Th p e r ) i   j ( ti d l e Th p e r )K  i (8) where   and   represent the error detection rate of EED and EOC, respectively. Task execution time with error detection and recovery: As we mentioned, EED and EOC come with additional computation overhead. We characterize a task using error detection technique with Cd e c i = C i +  C i , where  C i is , which denotes the time for execution and error detection of task  i . For EOC, Cd e c i = 2C i + i if a temporal redundancy approach is used.  i denotes the time for comparing outputs. If we duplicate the execution of same task on di￿erent cores (i.e. a spatial redundancy approach), Cd e c i = C i +  i . For EED, since we run a task with built-in detection, Cd e c i the increased timing cost for EED. Cr e c i is the error recovery time for task  i if the error is detected. We assume the re-execution of a task is scheduled immediately if error(s) is detected. Cr e c i for EOC, while Cr e c i Worst-case response time analysis with error detection: To analyze the response time of task with EED/EOC technique, we need to integrate error detection time and recovery time into Equation (1). For this, we employ two binary   i and oi to distinguish error detection strategy.   i is 1 if either EED or EOC is employed for task  i and 0 otherwise. oi is 1 if EOC if used, and 0 if EED is used. We rewrite Cd e c i and Cr e c i as following: = C i +  C i for EED. = C i Cd e c i = C i + [o i (C i +   i ) + (1   o i ) C i ]  i Cr e c i = (C i + (1   o i ) C i )   i (9a) (9b) Let r i ,   j denote worst-case response time for task  i when error(s) occurs during the execution of task   j . Task  i will be blocked by task   j ’s recovery time if   j has higher priority. Follow the same idea of Equation (1), we have: & r i ,   j T k ' Cd e c k p i ,   j r i ,   j = Cd e c i + Cr e c  j p i ,   j + X k 2T ^  i ,  j (10) where p i ,   j denotes the relative priority between task  i and   j . Considering a complete task with K errors, the response time of task  i can be formulated as: + K max   j 2T { Xe l 2E a i , e l Cd e c i Cr e c  j p i ,   j h i ,   j , e l } + X k 2T^ i ,  j Xe l 2E & r i ,   j T k ' Cd e c k p i ,   j h i ,   j , e l r i = Xe l 2E (11) where Boolean variable a i , e l is 1 if task  i is assigned to core el and 0 otherwise. h i ,   j , e l is 1 if task  i and   j are on the same core el and 0 otherwise. To ensure each tasks is only mapped to one ECU, the following relations must be enforced: Xe l 2E a i , e l = 1 a i , e l + a  j , e l   1  h i ,   j , e l h i ,   j , e l  a i , e l h i ,   j , e l  a  j , e l (12) (13) (14) (15) 4 OPTIMIZATION AND TRADE-OFFS AMONG OBJECTIVES Based on the models introduced in Section 3, we quantitatively analyze the trade-o￿s among extensibility, fault tolerance and latency (as well as other metrics related to communication cost, such as the number of CAN messages and the bus utilization). In this section, Addressing Extensibility and Fault Tolerance in CAN-based Automotive SystemsNOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Figure 2: An illustrating example showing the trade-o￿ between extensibility and communication cost (i.e., number of messages and path latencies), under di￿erent task mapping choices: (a) task  1 and task  3 mapped to ECU-A while task  2 mapped to ECU-B, and (b) task  1 and task  2 mapped to ECU-A while task  3 mapped to ECU-B. we will ￿rst demonstrate such trade-o￿s with an illustrating example, and then introduce a simulated annealing based algorithm for optimizing these di￿erent design objectives/metrics. 4.1 Illustrating Example Figure 2 shows how the mapping of three tasks onto a CAN-based platform with two ECUs can a￿ect system extensibility and communication cost (measured by the number of CAN messages or path latencies). The task graph, task WCETs and periods are shown on the left side. Task  1 and  2 send their output to task  3 . In mapping (a), task  1 and  3 are mapped to the same ECUA, while  2 is mapped to ECU-B. We assign higher priority to  3 than  1 , based on the Rate Monotonic policy. Although there is still 16.7% utilization left on ECU-A, this mapping makes it impossible to increase the execution time for either  1 or  3 , i.e., their extensibility is zero. On ECU-B, task  2 can increase its WCET by 2 time unit. Thus, the total system extensibility is 2/3/3 = 22.2%. In terms of communication, only message m2 needs to be transmitted over CAN bus and the latency on path  1 to  3 should be relatively short. In mapping (b), task  2 and  3 are swapped. On ECU-A, task  1 and  2 have the same period and WCET, and we assume  1 has the higher priority. The maximum increase of WCET for either task is 1 time unit. The system extensibility is calculate as (1/3 + 1/3 + 1/2)/3 = 38.9%. In terms of communication, messages m1 and m2 need to be transmitted over the CAN bus, and the latency on path  1 to  3 is also higher than mapping (a). In this example, we clearly see the trade-o￿ between extensibility and communication cost (i.e., latency or number of messages). Next, we will introduce how we can optimize these design objectives. 4.2 Objective Function and Constraints We optimize an objective function (16) that includes extensibility, fault tolerance and communication cost, by exploring allocation, priority assignment and error detection technique (EED or EOC or none detection) for each task. The communication cost could be measured by total path latency, number of CAN messages or bus utilization. Cos te x t , Cos t f t , Cos tc om in (16) are costs for extensibility, fault tolerance and communication, respectively. For instance, Cos te x t = 1  E , where E is the system extensibility in (7). Note that the higher the extensibility, the lower the cost Cos te x t is.   , µ and   are weights and can be tuned to trade o￿ these objectives. The optimization is subject to the schedulability constraints in (4) to (6), and possible constraint on each design objective. For instance, there could be upper bounds EXTma x , FTma x and COMma x on each cost, as shown below. min   ⇤ Cos te x t + µ ⇤ Cos t f t +   ⇤ Cos tc om s .t . 0       1, 0   µ   1, 0       1 Cos te x t  EXTma x Cos t f t  FTma x Cos tc om  COMma x (16) (17) (18) (19) (20) 4.3 Simulated Annealing We developed a simulated annealing based algorithm for the above optimization, as shown in Algorithm 2. For the initial con￿guration, tasks are randomly allocated to ECUs and scheduled using the Rate Monotonic policy. T represents current simulation temperature, T ⇤ is the ￿nal temperature and   is the cooling factor. K ⇤ is the maximum number of iterations within each temperature. During each iteration, function r andomChan e modi￿es current solution Ac u r into a candidate solution An ew by randomly performing one of the following operations: i) changing the allocation of a task from one ECU to another, ii) swapping the priorities of two tasks on the same ECU, or iii) changing the error detection technique of a task. Function Comput eOb jec t i e (An ew ) then computes corresponding cost Cn ew as de￿ned in (16), and function checkSched (An ew ) determines the schedulability of this candidate NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Hengyi Liang, Zhilu Wang, Bowen Zheng, and Qi Zhu solution. Note that if the candidate solution is infeasible, we add a penalty   to the cost instead of rejecting the solution directly. Function P (Cc u r , Cn ew , T ) then computes the acceptance possibility of An ew . Ac u r and Cc u r keep track of the latest solution and its cost. Aop t stores the best solution. The simulated annealing procedure stops when temperature reaches the prede￿ned value T ⇤ . Algorithm 2: Simulated Annealing for Task Allocation, Scheduling and Error Detection Technique Selection An e w = randomChange(Ac u r ) Cn e w = ComputeObjective(An e w ) i s S c h e d = checkSched(An e w ) 1: Construct initial con￿guration. 2: while T   T ⇤ do 3: while K < K ⇤ do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: Cn e w = Cn e w +   Ac u r = An e w , Cc u r = Cn e w if Cn e w < Cc u r then if i s S c h e d = t r u e then if i s S c h e d == f a l s e then else if P (Cc u r , Cn e w , T ) > rand() then Aop t = Ac u r Ac u r = An e w , Cc u r = Cn e w K = K + 1 T = T ⇤   15: 16: return Aop t 5 EXPERIMENTAL RESULTS We conducted experiments on an industrial case and a set of synthetic examples. The industrial case is derived from an experimental vehicle subsystem and contains 41 tasks communicating through 81 signals. The subsystem involves distributed functions collecting data from 360  sensors to actuators. All the periods and task WCETs are given in the industrial case. We also use the TGFF tool [6] to generate a set of synthetic examples with random periods and WCETs. We impose end-to-end latency deadlines on selected critical paths. The examples are tested for a number of di￿erent platform con￿gurations. For the industrial case, a minimum of 5 ECUs is needed for ￿nding feasible solutions. 5.1 Extensibility vs. Latency We ￿rst explore the trade-o￿ between extensibility and communication cost, which is measured by the total critical path latency (i.e., the sum of end-to-end latencies for all selected critical paths). We conduct optimizations using our simulated annealing approach (Algorithm 2). More speci￿cally, for extensibility optimization, we set   = 1 and µ =   = 0 in the objective function (16). For latency optimization, we set   = µ = 0 and   = 1. We carry out these optimizations for platform con￿gurations containing 5 to 10 ECUs, and record both extensibility and latency for each optimization. The comparison results for the industrial case are shown in Figure 3, with yellow bars on the right in both sub-￿gures representing extensibility optimization results and blue bars on the left in both sub-￿gures representing latency optimization results. We can clearly see a trade-o￿ between extensibility and latency. Extensibility optimization indeed signi￿cantly improves the extensibility metric over latency optimization, but leads to longer total latency. Intuitively, optimizing extensibility leads to more “balanced"" allocation of tasks on ECUs and thus more CAN messages 6000 4000 2000 ) s m ( y c n e t a l h t a P 0 0.8 0.6 0.4 0.2 0 y t i l i i b s n e t x E 5 ECUs 6 ECUs 7 ECUs 8 ECUs 9 ECUs 10 ECUs Optimize path latency Optimize extensibility 5 ECUs 6 ECUs 7 ECUs 8 ECUs 9 ECUs 10 ECUs Figure 3: Comparison between extensibility optimization and latency optimization for industrial case. (rather than communication through local memory) and longer total latency. In our experiments, extensibility optimization results have over 70 signals mapped to CAN messages while latency optimization results only have 10-20 messages. Furthermore, we can see that the extensibility increases with more ECUs. This is as expected since the tasks get more timing slacks with lower average ECU utilization. 5.2 Error Coverage vs. Latency We then explore the trade-o￿ between error coverage (fault tolerance) and latency. We employ EED and the temporal redundancy model of EOC as described in Section 3.3, and we set   =   = 0 and µ = 1 for error coverage optimization. Figure 4 shows the comparison between error coverage optimization (yellow bars on the right) and latency optimization (blue bars on the left) for the industrial case. The trade-o￿ between the two is also very clear. Intuitively, more balanced allocation of tasks leads to more timing slacks for tasks to add error detection techniques, but results in longer latency. We can also see the error coverage increases with more ECUs. 5.3 Extensibility vs. Error Coverage We study the relation between extensibility and error coverage, by mapping the industrial case onto a platform of 5 ECUs with an average ECU utilization of 58%. Note that extensibility and error coverage are not always mutually exclusive. Both metrics get better when more time slack is available. However, applying the slack to error detection techniques does take away some capability to accommodate future changes. Table 1 shows the trade-o￿ between extensibility and error coverage, when error coverage optimization is performed with minimum extensibility set to 0, 0.1, 0.2, 0.3, 0.4 and 0.5, respectively.     Addressing Extensibility and Fault Tolerance in CAN-based Automotive SystemsNOCS ’17, October 19–20, 2017, Seoul, Republic of Korea 6000 4000 2000 ) s m ( y c n e a t l h a t P 0 1 0.8 0.6 0.4 0.2 0 e g a r e v o c r o r r E 5 ECUs 6 ECUs 7 ECUs 8 ECUs 9 ECUs 10 ECUs Optimize path latency Optimize error coverage 5 ECUs 6 ECUs 7 ECUs 8 ECUs 9 ECUs 10 ECUs Figure 4: Comparison between error coverage (fault tolerance) optimization and latency optimization for industrial case. Table 1: Optimizing error coverage at di￿erent minimum extensibility requirement for industrial case. min. ext. error coverage extensibility 0 0.1 0.2 0.3 0.4 0.5 0.5775 0.5176 0.5122 0.4568 0.4472 0.4089 0.1354 0.1582 0.2162 0.3027 0.4036 0.5605 5.4 Optimization of All Three Objectives   = µ =   = 1 in (16). Cos te x t , Cos t f t and Cos tc om are all normalas Cos tc om = (Lat   Latl b )/ (Latu b   Latl b ) . Lat = Ppk 2P lpk is the We then optimize all three objectives (extensibility, error coverage / fault tolerance, latency / communication cost) by setting ized. In particular, Cos te x t = 1  E . Cos t f t is as de￿ned in Equation (8). Cos tc om represents the cost of total latency, and is de￿ned total latency, Latl b = Ppk 2P P i 2pk (T i + c i ) is a lower bound for the total latency, and Latma x = 2⇤Ppk 2P (P i 2pk T i +Ps j 2pk Ts j ) is an upper bound. Table 2: System total critical path latency, extensibility and error coverage in the solutions from optimizing each individual objective and from optimizing all three objectives (industrial case). Opt. Path Latency Opt. Extensibility Opt. Error Coverage Opt. All Opt. Path Latency Opt. Extensibility Opt. Error Coverage Opt. All 5 ECUs Path Latency 4718.02 5174.52 5488.06 5206.46 8 ECUs Path Latency 4761.50 4969.90 5357.31 5221.79 Extensibility 0.296 0.490 0.323 0.418 Extensibility 0.415 0.681 0.485 0.633 Error Coverage 0.629 0.507 0.684 0.699 Error Coverage 0.856 0.692 0.918 0.806 As shown in Table 2, optimizing all three objectives provides more balanced solutions, when compared with optimizing for each individual objective. Such results are not surprising qualitatively, but the quantitative comparison should facilitate designers to make design choices. 5.5 Impact of ECU Speed and Number of Tasks Finally, we study how extensibility is a￿ected by the ECU computation speed and the number of tasks. We generate a set of synthetic examples with di￿erent number of tasks and map them to a platform with 5 ECUs. We scale all task WCETs by a factor of 1X, 1.5X and 2X to model di￿erent ECU computation speed while task periods remain unchanged. Figure 5 demonstrates the quantitative impact of ECU speed and number of tasks on system extensibility. original 1.5x 2.0x y t i l i i b s n e t x E m e t s y S 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 30 35 40 45 50 Number of Tasks Figure 5: System extensibility under di￿erent ECU speed and number of tasks (synthetic examples). 6 CONCLUSION In this work, we quantitatively analyze the trade-o￿s among extensibility, fault tolerance and latency for CAN-based automotive electronic systems. We introduce metrics for de￿ning these three objectives and present a simulated annealing based algorithm for optimizing them. The clear trade-o￿s among these objectives demonstrate the need to develop design automation methods for facilitating the design space exploration in automotive systems. 7 ACKNOWLEDGEMENTS This work is supported by the National Science Foundation grants CCF-1553757, CCF-1646381 and CNS-1646641, and the O￿ce of Naval Research grant N00014-14-1-0815. "
BiNoCHS - Bimodal Network-on-Chip for CPU-GPU Heterogeneous Systems.,"CPU-GPU heterogeneous systems are emerging as architectures of choice for high-performance energy-efficient computing. Designing on-chip interconnects for such systems is challenging; CPUs typically benefit greatly from optimizations that reduce latency, but rarely saturate bandwidth or queueing resources. In contrast, GPUs generate intense traffic that produces local congestion, harming CPU performance. Congestion-optimized interconnects can mitigate this problem through larger virtual and physical channel resources. However, when there is little traffic, such networks become suboptimal due to higher unloaded packet latencies and critical path delays. We argue for a reconfigurable network that can activate additional channels under high load/congestion and shut them off when the network is unloaded. However, these additional resources consume more power, making it difficult to statically provision a power budget for the network. We introduce BiNoCHS, a reconfigurable voltage-scalable on-chip network for heterogeneous systems. Under CPU-dominated low-traffic conditions, BiNoCHS operates at nominal-voltage and high clock frequency with a topology optimized for low hop count, maximizing CPU performance. Under high-traffic GPU and mixed workloads, it transitions to a near-threshold mode, activating additional routers/channels and non-minimal adaptive routing to resolve congestion. Our evaluation shows that BiNoCHS improves CPU/GPU performance by 57% / 34% over a latency-optimized network under congested conditions, while improving CPU performance by 28% over high-bandwidth design in unloaded conditions.","BiNoCHS: Bimodal Network-on-Chip for CPU-GPU Heterogeneous Systems Amirhossein Mirhosseini Mohammad Sadrosadati* Behnaz Soltani* Hamid Sarbazi-Azad* Thomas F. Wenisch University of Michigan {miramir,twenisch}@umich.edu * Sharif University of Technology {sadrosadati,soltani,azad}@ce.sharif.edu ABSTRACT 1 INTRODUCTION CPU-GPU heterogeneous systems are emerging as architectures of choice for high-performance energy-efficient computing. Designing on-chip interconnects for such systems is challenging; CPUs typically benefit greatly from optimizations that reduce latency, but rarely saturate bandwidth or queueing resources. In contrast, GPUs generate intense traffic that produces local congestion, harming CPU performance. Congestion-optimized interconnects can mitigate this problem through larger virtual and physical channel resources. However, when there is little traffic, such networks become suboptimal due to higher unloaded packet latencies and critical path delays. We argue for a reconfigurable network that can activate additional channels under high load/congestion and shut them off when the network is unloaded. However, these additional resources consume more power, making it difficult to statically provision a power budget for the network. We introduce BiNoCHS, a reconfigurable voltage-scalable on-chip network for heterogeneous systems. Under CPU-dominated low-traffic conditions, BiNoCHS operates at nominal-voltage and high clock frequency with a topology optimized for low hop count, maximizing CPU performance. Under high-traffic GPU and mixed workloads, it transitions to a near-threshold mode, activating additional routers/channels and non-minimal adaptive routing to resolve congestion. Our evaluation shows that BiNoCHS improves CPU/GPU performance by 57% / 34% over a latency-optimized network under congested conditions, while improving CPU performance by 28% over high-bandwidth design in unloaded conditions. CCS CONCEPTS • Hardware → Interconnect; KEYWORDS Bimodal network-on-chip, heterogeneous systems Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130222 GPUs have become the mainstream accelerator for throughput applications by offering massive thread-level parallelism (TLP). Historically, CPUs and GPUs were connected via a PCIe bus and CPUs offloaded data-parallel kernels to the GPU by explicitly copying data to GPU memory. However, such explicit data movement imposes significant performance overheads that, in some cases, outweigh the potential gains of GPU acceleration. To reduce communication costs and improve programmability, recent designs, such as Intel’s Skylake or NVIDIA’s Denver, have integrated GPU/CPU cores on the same die, sharing Last-Level Cache (LLC) banks and memory controllers through a Network-on-Chip (NoC), as in Figure 1(a). Though integration simplifies the programming model, it creates challenges in designing an NoC that is well-suited for the differing memory access patterns of CPU and GPU applications [20]. CPUs typically issue memory accesses individually or in small bursts and benefit greatly from optimizations that reduce latency, but rarely saturate NoC bandwidth or queueing resources [24]. In contrast, high-TLP GPU workloads with working sets that overflow local caches generate intense NoC traffic to access LLC banks. Moreover, their traffic patterns are typically not uniform. Instead, the manyto-few communication pattern between the compute nodes and LLC bank tends to create hot spots that traverse the LLC banks, producing transient local congestion in the NoC [3, 29]. When an NoC is highly congested, head-of-line blocking and queueing delays grow to dominate overall latency. Above some critical load threshold, packet latencies increase exponentially and the latency distribution becomes heavy-tailed. Although GPUs can often hide moderately high latencies through TLP, the high (and high variance) latencies to LLC banks under congested network conditions cannot be effectively hidden, leading to frequent stalls [32]. In application scenarios where CPU and GPU cores operate concurrently, heavy congestion induced by GPU traffic can severely degrade CPU performance [17]. As a result, NoCs designed for heterogeneous systems must be able to accommodate a wide range of load conditions without sacrificing performance. For example, as shown in Figure 2, average network load can vary up to 20× across workloads in the Rodinia heterogeneous benchmark suite [4]. Moreover, different phases of the same application may have disparate NoC load intensity. Figure 3 shows network load of a Computional Fluid Dynamics (CFD) workload wherein network load varies across phases by up to 2.5×. Congestion can be reduced in a straight-forward way by adding network resources, such as additional virtual and physical channels. These resources increase the network’s saturation bandwidth and help to resolve local congestion and head-of-line blocking, rapidly CPU GPU GPU GPU GPU CPU LLC GPU GPU LLC CPU GPU GPU GPU GPU CPU CPU GPU GPU GPU GPU CPU LLC GPU GPU LLC CPU GPU GPU GPU GPU CPU CPU GPU GPU GPU GPU CPU LLC GPU GPU LLC CPU GPU GPU GPU GPU CPU CPU GPU GPU GPU GPU CPU LLC GPU GPU LLC CPU GPU GPU GPU GPU CPU (cid:1)(cid:2)(cid:3)(cid:4)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:3)(cid:10) (cid:11) (cid:12) (cid:13) (cid:14) (cid:9) (cid:2) (cid:3) (cid:8) (cid:15) (cid:7) (cid:13) (cid:17) (cid:6) (cid:16) (cid:12) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:7)(cid:13)(cid:6)(cid:5)(cid:14)(cid:15)(cid:5)(cid:16)(cid:11)(cid:17) (cid:18)(cid:10)(cid:19)(cid:5)(cid:6)(cid:7)(cid:20)(cid:16)(cid:2)(cid:21)(cid:2)(cid:22)(cid:9)(cid:7)(cid:18)(cid:22)(cid:21)(cid:5)(cid:16)(cid:11)(cid:17) (cid:23)(cid:10)(cid:6)(cid:5)(cid:7)(cid:24)(cid:5)(cid:21)(cid:19)(cid:10)(cid:6)(cid:12)(cid:7)(cid:25)(cid:5)(cid:26)(cid:10)(cid:15)(cid:6)(cid:11)(cid:5)(cid:26) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:27)(cid:22)(cid:21)(cid:15)(cid:6)(cid:22)(cid:21)(cid:2)(cid:10)(cid:16)(cid:7)(cid:25)(cid:22)(cid:21)(cid:5) (cid:20)(cid:28)(cid:5)(cid:22)(cid:9)(cid:7)(cid:24)(cid:5)(cid:21)(cid:19)(cid:10)(cid:6)(cid:12) (a) (b) Figure 1: (a) A single-chip CPU-GPU system. Memory controllers are co-located with LLC banks (b) Ideal NoC behavior for heterogeneous systems. reducing queueing delays. However, adding additional channels necessarily adds delay to router critical paths. These delays increase the minimum packet traversal latency when the network is unloaded, sacrificing performance for workloads with low traffic. Moreover, adding additional channels increases the NoC power requirement; to maintain the same power budget, the network voltage and frequency must be scaled down, further increasing the minimum traversal latency [28, 33]. As such, statically optimizing an NoC for a heterogeneous system presents the trade-off illustrated in Figure 1(b). If one optimizes the NoC for latency-sensitive CPU traffic—minimizing the number of virtual and physical channels and maximizing clock frequency—one arrives at the latency-vs-load profile depicted in blue (“Higher Clock Frequency; Lower Initial Latency”). Conversely, if one statically provisions additional channels at the cost of clock frequency, one arrives at the green trade-off curve (“More Network Resources; Higher Saturation Rate”), which is well-suited for GPU workloads but penalizes latency in low-load conditions. In this work, we introduce BiNoCHS—a Bimodal NoC architecture for CPU-GPU Heterogeneous Systems. BiNoCHS is an adaptive NoC design that uses dynamic concentration and link reconfiguration to switch between latency-optimized and congestion-optimized operating modes, enabling a latency-vs-load profile as seen in the red line (“Ideal Network”) in Figure 1(b). BiNoCHS adapts at coarse granularity between modes when the average NoC load crosses the critical threshold illustrated in the figure. BiNoCHS adapts its topology, routing, physical/virtual channel resources, and operating frequency/voltage to optimize for each traffic condition. Under low-traffic conditions, BiNoCHS operates at nominal voltage with a low-diameter concentrated mesh where multiple cores (e.g., 4) share a single router, accessing it through a concentrator. This mode enables the highest clock frequency and lowest hop count, minimizing the NoC traversal latency, which is a critical factor in CPU performance. However, when traffic load is high (e.g., during NoC-intensive GPU kernels), BiNoCHS transitions to a near-threshold operating mode where it activates additional routers and links, deactivating the concentrator and instead connecting each core to a dedicated router. Furthermore, it enables additional virtual channels in each router. The larger topology 0 0.5 1 1.5 2 2.5 3 p k c a b r p o b s f f c d i s s u a g n a t r a e h w a l l k m s n a e l y c o k u e t e m y c o y t e b e e r t o h o p s t t l d u n w i t r a p c l t l i f e r e a p t h i f r e d n d a r s a e r t s m c l t s u r e l a v a M D N o r m a i l h T d z r u p h g u o t ( F / s t i l c y c l e / ) e d o n Figure 2: Normalized network loads across various GPU workloads from Rodinia [4] benchmark suite. 2.7 2.5 2.3 2.1 1.9 1.7 1.5 1.3 1.1 0.9 N o r m a i l d e z T h r u p h g u o t t i l f ( s / c y c l e / e d o n ) Time (us) Figure 3: Normalized network loads within different phases of a CFD GPU workload in a 2 millisecond window. spreads load and provides greater routing flexibility, while the additional virtual channels reduce head-of-line blocking. BiNoCHS uses non-minimal adaptive routing in this mode to exploit the higher path diversity and additional virtual channels to further flatten the traffic and route around hot-spots. Under high load, the reduced queueing delays more than compensate for the higher hop-count and reduced link frequency, improving both latency and throughput. The intuition is similar to the trade-off between M/M/1 and M/M/k queues at high load; having more servers can provide greater average benefit than reducing service time when queuing delays are considerably larger than service times [25]. BiNoCHS employs multiple voltages for different components [11, 13] to aggressively reduce the voltage in its congestion-optimized operating mode. The reduced operating voltage frees power budget to activate the additional hardware resources and channels. Our evaluation demonstrates that BiNoCHS’s congestion-optimized mode improves CPU and GPU performance over a baseline latencyoptimized network under NoC-intensive heterogeneous workloads by 57%, and 34%, respectively, within the baseline NoC power budget. Moreover, the latency-optimized mode, which operates at nominal voltage, improves CPU performance by 28% over several bandwidth-optimized baseline NoCs under low-traffic CPU workloads. 2         P0-P9 R0-R9 P10-P19 R10-R19 (cid:120)(cid:3) P: Priority (cid:120)(cid:3) R: Request (cid:120)(cid:3) G: Grant Cin 10 to 1  Arbiter Cout Cin 10 to 1  Arbiter Cout (cid:120)(cid:3) P: Priority (cid:120)(cid:3) R: Requ est (cid:120)(cid:3) G: Grant Carry-in G0-G9 G10-G19 P0 R0 P1 R1 P2 R2 P3 R3 Carry-out  Nom inal-Voltag e Configuration => Select=0 Near-Threshold-Voltag e Configuration => Select=1 0 1 t c e l e S (a) G0 G1 G2 G3 (cid:120)(cid:3) R0=R3=0,  R1=R2=1 (cid:120)(cid:3) P0=P1=P2=0,  P3=1 (cid:120)(cid:3) G0=G2=G3=0,  G1=1 (b) Figure 4: (a) RLA used in BiNoCHS. (b) A high-level view of programmable priority arbiter functionality. The arbiter searches from the last priority position and wraps around if needed to find the first request, setting the corresponding grant signal. 2 BACKGROUND For NoC-intensive workload phases, to reduce contention, BiNoCHS reduces the NoC voltage and frequency to create head room in the power budget to activate additional channels. A key challenge of near-threshold design is the differing scaling behavior of logic and SRAM structures. When the voltage is decreased from nominal to near-threshold, both logic and storage structures slow significantly. However, the slowdown of storage elements is steeper, and storage becomes relatively slower as compared to logic; this effect gets worse as feature size becomes smaller [13]. As a consequence, aggressive pipeline architectures that are tuned for nominal voltage operation are sub-optimal in the near-threshold regime—their clock frequency is dominated by buffer access latencies. Moreover, storage structures become failure-prone when the voltage is lowered below a critical threshold, typically referred to as Vm i n [11]. Vm i n can be reduced by using more and larger transistors (or more fins in FinFET designs) [13]. However, larger transistors imply significant area and power overheads, especially in systems designed for efficient nominal-voltage operation. As such, recent near-threshold designs [11, 13] choose different nearthreshold operating voltages for logic and storage elements and use dual-rail voltage supplies to optimize overall energy efficiency while ensuring data integrity. We similarly use dual supply voltages with optimized near-threshold operating points for logic and storage structures in our dual-operating-mode routers. 3 BINOCHS ARCHITECTURE As we have noted, there is a trade-off between latency-optimized and bandwidth-optimized on-chip networks. Bandwidth-optimized networks employ additional resources, such as virtual and physical channels, to provide extra bandwidth. However, these extra resources usually come at the cost of lower clock frequencies for two reasons: first, extra channels necessarily prolong router critical paths, and second, to maintain the NoC power budget, its voltage and frequency are usually down-scaled when adding extra network resources. As a result, under high traffic conditions, additional resources play a crucial role in resolving local congestion and mitigating head-of-line blocking—providing higher effective bandwidth. Nonetheless, when the network is unloaded, the additional resources can actually hurt performance by increasing the minimum traversal latency. 3 BiNoCHS is a voltage-scalable reconfigurable concentrated mesh (Cmesh) NoC design (targeting 22nm CMOS technology) for CPUGPU heterogeneous systems. BiNoCHS provides two modes, operating at nominal and near-threshold voltages, which are designed for low-traffic CPU-dominated and GPU-heavy NoC-intensive workloads, respectively. BiNoCHS’s nominal-voltage mode targets lowtraffic scenarios where GPUs are idle or do not issue much NoC traffic (e.g., working sets fit in local caches). This mode seeks to minimize network latency by maximizing clock frequency and reducing hop count. BiNoCHS’s near-threshold mode minimizes NoC voltage and frequency, reapportioning the freed power budget to activate additional routers and channels increasing effective bandwidth and resolving local congestion. To facilitate its bimodal operation, BiNoCHS provisions separate supply voltage lines for router logic and storage elements, allowing optimized logic and storage supply voltage in near-threshold mode. Router logic elements comprise route computation units, allocators, crossbars, and controllers, whereas storage elements comprise the buffers assigned for each virtual channel. In our design, we use a logic supply voltage (Vl oд i c ) of 0.5V. Prior work [13] shows that voltages below 0.5V impose increasingly challenging performance and leakage overheads. In contrast, the near-threshold voltage used for storage elements (Vs t o r aдe ) is the Vm i n of 6T SRAM in 22nm technology—0.6V [13, 15]. BiNoCHS’s near-threshold mode improves performance under high congestion through three techniques: provisioning more virtual channels, reducing network concentration, and using nonminimal adaptive routing. 3.1 More virtual channels Provisioning more virtual channels (VCs) at each router input port alleviates local congestion by reducing head-of-line (HoL) blocking [7]. HoL blocking arises when a packet that is blocked due to link congestion precludes another packet traveling in the same channel (but which requires a different outbound link) from making forward progress. HoL blocking can form a chain of blocked packets spanning several routers, causing congestion on a single link to spread throughout the network. Additional virtual channels allow packets with ready outbound links to route past blocked packets, preventing congestion from spreading. Because BiNoCHS uses two different supply voltages for router buffers and logic, buffers are relatively faster than logic in nearthreshold mode as compared to nominal operation (though both storage and logic are slower in absolute terms). The disparity between storage and logic delay allows us to activate additional buffers on each input port to increase the number of VCs. Our SPICE simulations show that, in near-threshold mode, we can double the number of VCs (from two to four)—the shorter buffer access times compensate for the additional decoding/multiplexing time to route to the additional channels. However, additional VCs increase arbitration delay in the VirtualChannel Allocation (VCA) unit. When multiple head flits contend for a single output port, the VCA unit must arbitrate among them and choose which output VC to allocate to each packet. The VCA’s arbiters must be timed for the worst case, wherein all input channels contend for the same output VC. Under nominal operation (two VC Al locator Switch  Allocator Route  Computation ... Buffers Crossbar Switch Nominal-voltage: Vstorage = Vlogic = 0.9v Near-Threshold-voltage: Vstorage = 0.6v, Vlogic = 0.5v (a) (b) Near-Threshold-Voltage Mode A C B D Reconfigurable Router Bypassable Router Regular Router (a) Nominal-Voltage Mode A A C B D (b) (c) Figure 5: (a) BiNoCHS reconfigurable voltage-scalable router, (b) Reducing C factor, and (c) BiNoCHS reconfigurable topology ..... ...... ..... .. ..... .. .. .. (a) Horizontal Bypassable  Router (b) Vertical Bypassable  Router Figure 6: Bypassable routers. VCs per input), the arbiter must accept at most 10 requests (five input ports, including the local ingress, with two VCs each) in each allocation cycle. In near-threshold mode, when the number of VCs doubles to four, the arbiter must scale to 20 requests per allocation cycle. Since BiNoCHS adapts between nominal and near-threshold modes, its VCA must support both 10-input and 20-input arbitration. However, as the VCA is often on the router critical path, we do not want to penalize nominal operation with a 20-input arbiter. Instead, we use the reconfigurable Reduced Logic Arbiter (RLA) introduced in [28]. RLA builds a single high-radix arbiter from two cascaded arbiters. Figure 4 illustrates BiNoCHS’s arbiter, which comprises two 10-input programmable priority arbiters and a multiplexer that selects the correct feedback line. Under nominal-voltage operation, when only ten inputs are needed, the second arbiter is disabled and the multiplexer is pre-steered to select the output of the first, minimizing impact on the critical path. We use the fast carry-lookahead arbiters proposed in prior work [10]. By using RLA, our approach avoids significant critical-path impact at nominal voltage. However, RLA prolongs the clock period under near-threshold mode by about 46% (relative to a conventional 20-input look-ahead arbiter operating at Vl oд i c ), which translates to significant performance overheads. We address this delay using two insights: First, in near-threshold operation, tiny voltage 4 increases can enable substantial frequency gains. Second, although arbiters have long critical paths, they are small and do not consume much power (relative, e.g., to crossbars). Hence, we can reduce delay impact by up-sizing transistors or over-driving the arbitration circuit with a small voltage increase. We select the latter approach in BiNoCHS and connect the arbitration circuit supply to Vs t o r aдe rather than Vl oд i c . The higher supply voltage allows RLA to operate with a clock frequency only 6% slower than a design with a 20-input arbiter at Vl oд i c . We illustrate the final BiNoCHS reconfigurable voltage-scalable router design in Figure 5(a). Two supply voltages, Vl oд i c and Vs t o r aдe , are available in each router. Vs t o r aдe supplies the buffers and the arbiters used in VC allocation whereas Vl oд i c supplies all other elements. In near-threshold mode, Vs t o r aдe and Vl oд i c are 0.6v and 0.5v, respectively, while both are set to 0.9v in the nominal-voltage mode. 3.2 Reducing network concentration Network concentration (or “C factor”) refers to the number of nodes connected to each router. Reducing network concentration brings down the injection load at each router. Hence, the router can provide higher effective bandwidth to its remaining node(s). BiNoCHS reduces network concentration in its near-threshold mode by activating additional routers and links, reassigning nodes to the new routers as shown in Figure 5(b). The expanded network provides higher bandwidth and greater path diversity. These features mitigate congestion under high traffic from NoC-intensive workloads. Our simulations show that, in near-threshold mode, the power consumption of BiNoCHS’s reconfigurable router is around 25% of a baseline (non-configurable) router’s power consumption if its C factor is also reduced by four. Under high traffic loads, dynamic power dominates the router power breakdown and, hence, by reducing voltage, frequency, and concentration, BiNoCHS can compensate for the power consumption of its extra VCs. To reduce the network concentration by a factor of four in the near-threshold mode, we double the number of routers in each Table 1: Characteristics of different routers in a cluster. Router Reconfigurable? Bypassable? #VC VCA arbiter size Vs t o r aдe (v) Vl oд i c (v) A B C D Yes No No No No 2(NV), 4(NT) RLA{10(NV), 20(NT)} 0.9(NV), 0.6(NT) 0.9(NV), 0.5(NT) Horizontal Vertical No 4 4 4 20 inputs 20 inputs 20 inputs 0.6 0.6 0.6 0.5 0.5 0.5 Table 2: Network simulation parameters. Parameter Number of nodes Topology Routing Flow Control Number of VC per Input Port VC Buffer Length (flits) Flit size (bits) value 64 2D Mesh/Cmesh Dimension-Order(XY)/Adaptive Wormhole 4/2 4 256 Router Microarchitecture 2-cycle Speculative Pipeline mesh dimension and deactivate the concentrator. The additional routers must be bypassed (i.e., behave as wires) in nominal-voltage operation. To this end, we introduce another type of router, which we call a bypassable router, shown in Figure 6. A bypassable router offers two power states: nominal operation and bypass. In nominal operation, it behaves as a conventional router with one (nonconcentrated) ingress port. In bypass operation, all buffering, routing, and arbitration logic is power-gated, and the crossbar is bypassed, instead directly connecting either east-west or north-south links. Because of BiNoCHs topology, a bypassable router need provide only one of horizontal or vertical bypass, depending on its position in the network. We build our reconfigurable network as illustrated in Figure 5(c). The network consists of clusters of four routers with each cluster comprising a reconfigurable voltage-scalable router, two bypassable near-threshold routers, and one regular near-threshold router. We describe the characteristics of each in Table 1 and illustrate the design of a cluster in Figure 5(c) (wherein A-D refer to the four types of routers as described in the table). When the NoC is configured for near-threshold mode, all of the routers are activated and the reconfigurable router (A) is configured in its near-threshold mode. Under nominal-voltage operation, all near-threshold routers (B,C,D) are power-gated (the bypassable ones (B,C) are configured for bypass) and the reconfigurable router (A) is configured for nominal-voltage mode. 3.3 Using non-minimal adaptive routing Adaptive routing algorithms allow packets to take multiple paths to avoid congestion and flatten the NoC traffic distribution. Fully adaptive schemes, which permit non-minimal routes, can further improve NoC efficiency as they allow packets to turn around to avoid becoming blocked in congested regions. In fact, prior work [14] has 5 shown that non-minimal adaptive routing performs best when the traffic has a hot-spot pattern and local congestion is frequent (e.g., as in NoC-intensive heterogeneous workloads). Adaptive routing is synergistic with BiNoCHS additional VCs and greater path diversity in the congestion-optimized near-threshold mode. Although BiNoCHS does not directly increase bisection bandwidth, adaptive routing can utilize the extra routes and VCs more effectively (i.e., utilize the available bandwidth better), avoiding head-of-line blocking and providing a higher saturation throughput. Nonetheless, when the traffic intensity is low and the network is unloaded, non-minimal adaptive routing can lead to higher packet latencies as it can increase the number of hops a packet may take. In BiNoCHS, we exploit this dichotomy and use non-minimal adaptive routing in the near-threshold congestion-optimized operating mode, wherein adaptive routing provides maximal benefit. However, in the nominal-voltage mode, where packets are rarely blocked and can simply follow minimal routes, BiNoCHS disallows non-minimal paths to avoid higher hop counts. Adaptive routing is prone to deadlocks. In BiNoCHS, we use Odd-Even turn model to provide deadlock-freedom [5]. This model guarantees deadlock-freedom by disallowing some paths depending on the location of each router. While our routing scheme is deadlockfree, livelocks are still possible, as there is no guarantee a packet will eventually reach its destination. To avoid livelocks, we impose a limit on the number of non-productive hops (misroutes) a packet may take. Once a packet reaches the misroute threshould, it may still be adaptively routed but may select only productive channels. To select between the different available routes allowed by the turn model, we follow the procedure described in [8]. At each step, the packet must take a productive channel if possible. If no legal productive channel is free, a legal non-productive channel might be taken (only in congestion-optimized mode). We use local buffer occupancy information to select between different available productive/non-productive channels. Prior work [8] have shown that, under uniform traffic patterns, local selection functions may in fact harm network performance as they distribute the load unevenly. However, in CPU-GPU heterogeneous systems, the traffic pattern is already non-uniform and includes many locally congested hot-spots. As such, even random path selections can improve network performance by flattening the traffic [31]. That said, there are selection functions that take into account global network status to further improve performance [14]; we did not use these to avoid additional complexities of such mechanisms. 4 SWITCHING MODES Prior work [21, 30] has shown that GPU applications tend to have coarse-grain phase behavior at the granularity of kernels that last Table 3: Supply voltage and working frequency for different NoC designs. Parameter C4VC2 C4VC4 C1VC4 C1VC4NM BiNoCHS (NT) BiNoCHS (NV) Supply voltage (v) Frequency (GHz) 0.9 2.5 0.9 2.0 0.9 2.0 0.9 2.0 Vs t o r aдe = 0.6, Vl oд i c = 0.5 Vs t o r aдe = Vl oд i c = 0.9 0.9 2.3 Table 4: CPU and GPU microarchitectures. System Unit CPU GPU Cores 16 x86 cores, 3-wide OoO, 64-entry ROB, 2.0 GHz 40 clusters, 1 SM/cluster, 32 SPs/SM, 1.4 GHz Private Caches 32KB L1, 256KB L2, 4-way, 128B Line Size 16KB 4-way, 128B Line Size Shared LLC Main Memory 8MB 16-way, 128B Line Size, 8 banks GDDR5, 8 MCs, FR-FCFS, 800 MHz, 8 DRAM-banks/MC for 10s of microseconds. Even for a 10us kernel, we can optimize for simplicity rather than switching latency and still incur a negligible performance penalty. BiNoCHS switches modes by draining all packets from the network and reconfiguring routers/links while no packets are in flight. Draining the network avoids numerous complexities that arise when attempting to reconfigure routes with packets in transit. Draining the network involves a few corner-tocorner network round-trip latencies. In our experiments, draining never required more than 200ns. We consider another 100ns for voltage/frequency scaling and reconfiguring the network topology, using voltage regulation techniques described in [12, 18], resulting in an overall mode switch latency of 300ns. Even if mode switches occur every 10us, the network would be unavailable only 3% of the time. The performance overhead is typically lower as (1) in most cases, workloads do not change phases at this frequency, and (2) CPU/GPU cores can continue execution while the network is switching, partially hiding the mode switch latency. Whereas we assume phase changes occur at a coarse granularity, we need a mechanism to detect and react to them quickly. Most related schemes that classify GPU kernels based on their memory intensity use the number of stall cycles observed in each streaming multiprocessor to do so. However, we care particularly about network intensity and some workloads, such as the “Back propagation” kernel from Rodinia, have few memory stalls despite imposing a high load on the network, due to high memory-level parallelism. Therefore, we use total GPU L1 Misses-per-Kilo-Cycle (MPKC) [1] as a metric for detecting phase changes. At each LLC bank, we count the number of arriving requests in a 4096-cycle epoch. (We have experimentally determined that shorter detection epochs lead to instability.) At the end of each epoch, we aggregate the MPKC counts through a chain of messages sent among the LLC banks and the last bank triggers a mode switch if the average crosses a pre-selected threshold. We tuned the transition threshold empirically and use the same threshold for all applications. After the mode switch decision has been made, routers do not accept any new ingress packets until the network has fully drained and reconfigured. A global wired-or logic signal that spans all routers is used to signal when the network is fully drained. 5 EVALUATION 5.1 Methodology We use GPGPU-Sim V3.2.2 [2], which employs Booksim 2.0 [16] as a detailed cycle-accurate on-chip network simulator, to analyze the performance of BiNoCHS. We have augmented this infrastructure with a trace-driven x86 CMP simulator to model the joint traffic from both CPU and GPU applications. The micro-architectural details of our GPU and CPU architectures are listed in Table 4. We also implemented an RTL model of our reconfigurable router and synthesized it in 22nm technology using a Synopsys design flow to obtain power estimates. We extract netlists from synthesis and use SPICE simulations to obtain peak frequency estimates at each operating voltage. We model a 64-node CPU-GPU heterogeneous system with a tile-based layout as in Figure 1(a) where each 4×4 tile is composed of 10 GPU cores, 4 CPU cores, and 2 LLC banks. We assume a one-to-one coupling between LLC banks and memory controllers as in prior works [17, 34]. Whereas memory controllers have conventionally been placed at the edges of a mesh, with modern flip-chip packages and 3D stacked chips, this is no longer a requirement; we achieve better performance by placing them in the center of the mesh in our baseline configuration. We have chosen the GPU/CPU core ratio to be 2.5/1 as the area of different 22nm Intel Haswell cores is roughly 2-3× larger than the area of an NVIDIA Maxwell GPU core scaled to 22nm technology. The NoC transitions between a 16×16 mesh and a 4×4 concentrated mesh topology with a C factor of four (each router is attached to four nodes) in its near-threshold/nominal-voltage modes. The key simulation parameters of the NoC are reported in Table 2. We contrast BiNoCHS with the following baselines: (1) C4VC2 and C4VC4: 4 × 4 Cmesh topology (C=4) with 2/4 VCs per input port and minimal adaptive routing. (2) C1VC4: 8 × 8 mesh topology (Cmesh with C=1) with 4 VCs per input port and minimal adaptive routing. (3) C1VC4NM: C1VC4 baseline which also allows non-minimal routes (as in BiNoCHS’s near-threshold mode). We use our RTL model to obtain frequency estimates for each baseline, shown in Table 3. 6 Table 5: Heterogeneous workload classes from Rodinia [4]. Compute-Intesnive Particle-filter, Stream-cluster, Myocyte NoC-Intensive Back-propagation, Heartwall, Kmeans Phase-Changing Guassian-elimination, CFD, SRAD, BFS We study three classes of heterogeneous workloads from the Rodinia [4] suite listed in Table 5: purely compute-intensive applications with moderate network loads, purely NoC-intensive applications with heavy traffic, and applications with phase-changing behavior where BiNoCHS transitions modes across phases. We also consider cases where no GPU kernel is launched and the network accommodates only CPU traffic. On the CPUs, we run multiprogrammed workload mixes of SPEC CPU 2006 applications. y c n e t a L t e k c a P e g a r e v A C P I U P C C P I U P G t i l F r e p y g r e n E t i l F r e p P D E 2.5 2 1.5 1 0.5 0 1.8 1.4 1 0.6 0.2 1.6 1.2 0.8 0.4 5 4 3 2 1 0 3 2 1 0 C4VC2 C4VC4 C1VC4 C1VC4NM BiNoCHS 8.4 8.3 7.8 7.65 (a) (b) (c) (d) (e) No GPU Compute-Intensive NoC-Intensive Phase-Changing Figure 7: Normalized (a) Average packet latency, (b) CPU IPC, (c) GPU IPC, (d) Energy per flit, and (e) EDP per flit under different heterogeneous workload classes. 5.2 Results Figure 7 shows normalized average packet latency, CPU IPC, GPU IPC, network energy per flit, and network Energy-Delay-Product (EDP) under different classes of CPU-GPU workloads. 7 Under low load, clock frequency and hop count have direct effects on packet latency while the extra VCs, routers, and/or links provide no benefit, since there is no congestion or head-of-line blocking. Hence, when the workload is purely compute-intensive or the GPU nodes are idle, we see that all C4VC4, C1VC4, and C1VC4NM hurt network latency by 28%, 91%, and 94% relative to C4VC2, respectively, due to their lower clock frequencies (and higher hop count in C1VC4 and C1VC4NM). These latencies translate to 8.2%, 22%, and 23% reduction in CPU performance; C1VC4NM performs slightly worse than C1VC4 due to higher hop counts taken by nonminimal routes. However, thanks to BiNoCHS’s reconfigurablity mechanisms, it manages to minimize performance overhead, with network latency and CPU performance at most 6.3% and 1.5% worse than of C4VC2, respectively. These results imply that, even with an unlimited NoC power budget, a single non-reconfigurable design with many resources is not well-suited for heterogeneous systems, especially with respect to CPU performance. GPU performance, on the other hand, does not vary significantly across designs, since GPUs are able to hide reasonable latencies through TLP. Energy efficiency tracks network and CPU performance; all C4VC4, C1VC4, and C1VC4NM designs consume more energy than C4VC2 and are less cost effective in terms of EDP. Their energy overhead arises because of the static energy consumed by the additional resources in these designs. In contrast, BiNoCHS power-gates unneeded resources, avoiding energy overhead. With NoC-intensive workloads, C4VC2 and C4VC4 NoC designs become saturated and incur large NoC latencies and CPU stalls. Even GPUs cannot hide such long network latencies. BiNoCHS, on the other hand, activates additional resources to resolve local congestion and uses non-minimal adaptive routing to avoid congested regions, improving performance drastically. As Figure 7 shows, BiNoCHS improves network, CPU, and GPU performance by 76%, 57%, and 34%, respectively, compared to C4VC2 as a latencyoptimized baseline. It also improves network energy efficiency and EDP by 46% and 87%, respectively. C1VC4 and C1VC4NM, on the other hand, outperform BiNoCHS in terms of network and CPU performance when the traffic load is high (GPU performance is almost the same). However, these baselines consume 3.1× and 2.9× more energy-per-flit than BiNoCHS and their EDPs are also 2.2× and 1.8× more, respectively. EDP indicates cost-effectiveness of a design and shows these designs do not offer enough performance for the extra energy they consume. Moreover, assuming the network power budget is not limited, BiNoCHS can simply work under higher voltage and frequency and offer almost the same performance as C1VC4NM when the traffic load is high. Finally, under heterogeneous workloads with phase-changing behavior, BiNoCHS outperforms all other designs, in terms of network and CPU performance, as it can reconfigure between latencyand congestion-optimized modes when the network traffic changes. In particular, BiNoCHS improves CPU/network performance compared to C4VC2 and C1VC4NM, the most latency-optimized and bandwidth-optimized baselines, by 42% / 81% and 16% / 33%, respectively. Energy and EDP results track network performance. Nonetheless, in terms of GPU performance, it is almost the same as C1VC4 and C1VC4NM and only outperforms other baselines. BINoCHS offers highest CPU performance improvements in workloads that change phases frequently.                 5.3 Overheads BiNoCHS incurs several overheads. Requiring dual voltage rails complicates layout. Recent work [13] reports that dual voltage rails impose about a 5% area overhead on the whole design. A second source of overhead is the level converters required in the router when signals transition between voltage domains (e.g., at the input port of the arbiters). Our SPICE simulations indicate that both the area and delay overheads of the level converters are negligible in our design (less than 1%). The biggest overheads of BiNoCHS arise from the added logic for reconfigurability, such as the concentrator MUXs and the bypass logic. Our results indicate that these logic elements impose 11% area and 6% performance overheads, which we include in our reported performance and energy results. 6 RELATED WORK Many recent works have sought to minimize CPU/GPU interference on shared resources in heterogeneous systems. Lee and Kim [19] propose cache partitioning to reduce interference at LLC banks. Several works [20, 34] have explored VC partitioning and traffic prioritization techniques to reduce interference in the NoC. Kayiran et al. [17] deploy throttling techniques to mitigate interference in heterogeneous systems. BiNoCHS, on the other hand, observes that not only is interference minimal when the workload is mostly compute-intensive, but even a bandwidth-optimized network that targets only NoC-intensive workloads can hurt CPU performance when the network load is low, motivating latency- and congestionoptimized operating modes. Mishra et al.[26] propose asymmetric resource allocation to address varying application demands. Das et al.[9] propose several prioritization techniques to address slack times of different applications. Nicoplous et al. [27] propose a reconfigurable NoC design that regulates the number and the organization of VCs at run-time to adapt to the traffic load. Various designs propose dynamic VC power-gating mechanisms that tune the number of VCs based on the traffic load [22, 23]. Yin et al.[33] characterize slack times for GPU packets and propose to route them on non-minimal paths. Choi et al.[6] propose a hybrid wired-wireless NoC design to service different latency/bandwidth demands in heterogeneous systems. To the best of our knowledge, BiNoCHS is the first reconfigurable design that takes a holistic approach to optimize various characteristics of the network based on the traffic in heterogeneous systems. 7 CONCLUSION NoC-intensive heterogeneous workloads impose high loads and cause time-varying local congestion near hot LLC banks. While additional network resources, such as virtual and physical channels, can improve saturation bandwidth and resolve such local congestion, they negatively impact the clock frequency and, thus, unloaded packet latencies. Higher unloaded latency harms performance in CPU-dominated low-traffic scenarios as, unlike GPUs, CPUs are unable to hide latencies longer than a few clock cycles. In this paper, we introduced BiNoCHS as a reconfigurable voltage-scalable NoC design for CPU-GPU heterogeneous systems. In nominal-voltage mode, BiNoCHS offers high clock frequency and low hop counts to maximize CPU performance. In near-threshold mode, it trades off clock frequency for network resources and employs non-minimal 8 adaptive routing to resolve congestion and postpone network saturation. Our experimental results demonstrated that BiNoCHS improves CPU/GPU performance under a congested network by 57% / 34% over a latency-optimized NoC and CPU performance in an unloaded environment by 28% over a bandwidth-optimized NoC. "
Minimally buffered deflection routing with in-order delivery in a torus.,"Bufferless deflection routing is a serious alternative to wormhole flow control and packet switching. It is based on the principle of deflecting a flit to a non-optimal route instead of buffering it, when two flits compete for the same link. The major weakness of deflection is the exploding number of misrouted flits at high network load, which increases the duration of flits within the network and requires to reassemble the flits at the receiver. These deflections can be reduced significantly by adding a small side buffer instead of always deflecting flits.
In the presented approach, the side buffer is complemented by a restricted deflection policy that preserves the flit order: x-y-routing in an unidirectional 2D-torus ensures that collisions are impossible, as long as a flit is transported in the same direction. Only at the transition from x- to y-direction, collisions may happen and are avoided by controlled in-order deflection in x-direction. In-order delivery not only simplifies the arbitration logic, but avoids costly mechanisms for livelock-prevention and reassembly of flits at the receiver.","Minimally bu(cid:128)ered def lection routing with in-order delivery in a torus J ¨org Mische University of Augsburg Germany mische@informatik.uni- augsburg.de Christian Mellwig University of Augsburg Germany christian.mellwig@informatik. uni- augsburg.de Alexander Stegmeier University of Augsburg Germany alexander.stegmeier@informatik. uni- augsburg.de Martin Frieb University of Augsburg Germany martin.frieb@informatik. uni- augsburg.de (cid:140)eo Ungerer University of Augsburg Germany ungerer@informatik.uni- augsburg. de ABSTRACT Bu(cid:130)erless de(cid:131)ection routing is a serious alternative to wormhole (cid:131)ow control and packet switching. It is based on the principle of de(cid:131)ecting a (cid:131)it to a non-optimal route instead of bu(cid:130)ering it, when two (cid:131)its compete for the same link. (cid:140)e major weakness of de(cid:131)ection is the exploding number of misrouted (cid:131)its at high network load, which increases the duration of (cid:131)its within the network and requires to reassemble the (cid:131)its at the receiver. (cid:140)ese de(cid:131)ections can be reduced signi(cid:128)cantly by adding a small side bu(cid:130)er instead of always de(cid:131)ecting (cid:131)its. In the presented approach, the side bu(cid:130)er is complemented by a restricted de(cid:131)ection policy that preserves the (cid:131)it order: x-y-routing in an unidirectional 2D-torus ensures that collisions are impossible, as long as a (cid:131)it is transported in the same direction. Only at the transition from x- to y-direction, collisions may happen and are avoided by controlled in-order de(cid:131)ection in x-direction. In-order delivery not only simpli(cid:128)es the arbitration logic, but avoids costly mechanisms for livelock-prevention and reassembly of (cid:131)its at the receiver. CCS CONCEPTS •Networks → Network on chip; Routers; KEYWORDS NoC, bu(cid:130)erless de(cid:131)ection routing, (cid:131)itwise routing ACM format: J ¨org Mische, Christian Mellwig, Alexander Stegmeier, Martin Frieb, and (cid:140)eo Ungerer. 2017. Minimally bu(cid:130)ered de(cid:131)ection routing with in-order delivery in a torus. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. DOI: 10.1145/3130218.3130227 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro(cid:128)t or commercial advantage and that copies bear this notice and the full citation on the (cid:128)rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi(cid:138)ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci(cid:128)c permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, Seoul, Republic of Korea © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-4984-0/17/10. . . $15.00 DOI: 10.1145/3130218.3130227 1 INTRODUCTION (cid:140)e majority of network-on-chip (NoC) implementations are based on a 2D-mesh topology, packet switching and wormhole (cid:131)ow control [3]. Such conventional NoCs provide short latencies and high throughput, but consume signi(cid:128)cant amounts of chip area and energy. For example, the NoC of the Intel 80-core Tera(cid:131)ops research chip consumes 28% of the power per tile [19]. (cid:140)e NoC power consumption share will even increase, as more cores are bundled on one chip [2]. (cid:140)e main contribution to area and power consumption stems from bu(cid:130)ers, which are ubiquitous in wormhole routers. For example, bu(cid:130)ers occupy 60% of the tile area of the Tilera TILE64 many-core [20]. Consequently, bu(cid:130)erless routing is a promising technique to reduce hardware costs in on-chip networks [18]. (cid:140)e downside of bu(cid:130)erless routing is an increased network tra(cid:129)c, because without bu(cid:130)ers, message collisions cannot be resolved by retarding one of the messages in a bu(cid:130)er. Instead, messages that need to take an already occupied link have to be de(cid:131)ected in another direction [4, 5, 14, 18] or dropped and retransmi(cid:138)ed [6, 7]. Early work on bu(cid:130)erless de(cid:131)ection routing [14] showed that it is an energy-e(cid:129)cient alternative at low and medium tra(cid:129)c. CHIPPER [4] reduced the arbitration costs to an acceptable level and MinBD [5] added small side bu(cid:130)ers that made the performance competitive, preserving the energy-e(cid:129)ciency. (cid:140)ese works showed, that the number of bu(cid:130)ers can considerably be reduced without hurting the performance. Here, we discuss a router architecture [13, 17] where also the arbitration logic is extremely simpli(cid:128)ed, while small bu(cid:130)ers still provide enough performance. It is based on the following principles: • Preserving the (cid:131)it order: Each part of a message ((cid:131)it) can take an individual route, but it is guaranteed that the (cid:131)its arrive in their original order. (cid:140)erefore no serial number is necessary, the message length is unbounded and the (cid:131)its can directly be forwarded to the receiving processing element, without any bu(cid:130)ering or reordering. • Dividing the routes in bu(cid:130)erless sections: Within a section no collisions are possible. Collision resolution by de(cid:131)ection or bu(cid:130)ering is only necessary at the transition points between route sections. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J ¨org Mische et al. • Minimal bu(cid:130)ering and in-order de(cid:131)ection: At the transition points, small bu(cid:130)ers provide a good utilization. If the bu(cid:130)ers are congested, controlled de(cid:131)ection that maintains the (cid:131)it order is used as fall-back. Such a router architecture requires less bu(cid:130)ers than wormhole routers, has a simpler arbitration logic than bu(cid:130)erless routers, does not need additional reassembly bu(cid:130)ers at the receiving node and the length of a message is unlimited. Bu(cid:130)erless in-order routing is of particular interest if very short messages are transmi(cid:138)ed, because the overhead does not increase for short messages, as it is the case for packet switching. Adapteva’s Epiphany architecture [15] is an example for such an architecture where each message has the size of one (cid:131)it. But even in conventional MPSoCs, where the tra(cid:129)c is dominated by short memory requests and long response messages covering a whole cache line [21], semibu(cid:130)erless routing is an option. Due to its low hardware costs, two networks can be established, one for short and one for long messages. In this paper, we analyze two earlier approaches and disclose their weaknesses. While the earlier publications focused on one particular router con(cid:128)guration and only roughly estimated hardware costs and throughput, we investigate all possible router con(cid:128)gurations and use detailed FPGA prototypes and real benchmark traces. We propose improvements to the arbitration and the inorder handling and thoroughly examine the design space to (cid:128)nd an optimal router con(cid:128)guration that maximizes throughput at minimal hardware costs. A(cid:137)er this introduction, related work on de(cid:131)ection routers and other NoCs with ring topology follows. In section 3 the basic algorithm for bu(cid:130)erless de(cid:131)ection routing is presented. In the following section, various modi(cid:128)cations to the basic algorithm are discussed. (cid:140)ese modi(cid:128)cations are evaluated in section 5. Section 6 concludes the paper. 2 BACKGROUND Preserving the order of de(cid:131)ected (cid:131)its is the key idea of this paper, but it is based on the works on disordered bu(cid:130)erless routers. 2.1 BLESS: Basic Bu(cid:130)erless De(cid:131)ection As the link capacity between routers is limited, long messages are split into (cid:131)ow control units ((cid:131)its) of constant size that usually can be transferred within one cycle. Conventional NoC routers use packet switching, where only the (cid:128)rst (cid:131)it carries routing data and all other (cid:131)its of a message just take the same route as the (cid:128)rst (cid:131)it. One of the (cid:128)rst bu(cid:130)erless de(cid:131)ection routers, WORM-BLESS [14] tried to adopt this behavior for bu(cid:130)erless routing. However, it cannot be avoided that a (cid:131)it is de(cid:131)ected to another direction than the previous (cid:131)it. In such a case, the message is split into two packets and the second part takes a di(cid:130)erent route. Although the (cid:131)its of a packet arrive in-order, the packets forming a message might arrive out-of-order, requiring a reassembly bu(cid:130)er at the receiver. (cid:140)e partly ordered arrival has no advantage over a completely scrambled arrival, for which reason the same authors proposed FLIT-BLESS [14], another de(cid:131)ection algorithm, that routes each (cid:131)it of a message independently. Each (cid:131)it is accompanied by its destination and a serial number to reassemble the (cid:131)its at the receiver. Continuous de(cid:131)ection of a (cid:131)it can prevent it from ever reaching its destination. To avoid such a situation called livelock, BLESS uses oldest (cid:128)rst arbitration: each (cid:131)it has a timestamp and when several (cid:131)its compete for the same link, the oldest one wins. (cid:140)is arbitration is e(cid:130)ective, but very costly in terms of hardware [12]: each (cid:131)it has to carry a timestamp and each router needs a full crossbar and multiple comparators. RIDER [16] is a recent de(cid:131)ection router implementation that is very similar to FLIT-BLESS, but replaces the costly crossbar by a ring between the output ports and reduces the de(cid:131)ection rate by adding some small bu(cid:130)ers between the ports. Nevertheless, it su(cid:130)ers from the same problems as BLESS: the age based arbitration and the switch logic is very expensive in terms of hardware. 2.2 Improved Bu(cid:130)erless De(cid:131)ection CHIPPER [4] considerably reduces hardware costs by introducing the golden packet rule for arbitration. Each message has a golden packet ID composed by the source node ID and a transaction number. A synchronized counter in every node iterates over all golden packed IDs, ensuring that only one packet is golden at a time. (cid:140)e time interval during which a packet is golden is long enough to guarantee that a packet of maximum length can travel from any node to any other node. Instead of a costly crossbar that ensures an optimal assignment of input and output ports, CHIPPER uses a much cheaper permutation network that only guarantees that the highest priority (cid:131)it (the golden one) is switched to the desired output port, the other ones are more or less randomly assigned. (cid:140)e drawback of the reduced hardware costs is a worse arbitration and lower network performance. If many (cid:131)its compete for the output ports and no golden (cid:131)it is present, the CHIPPER permutation network may assign the (cid:131)its in a way that none is switched to a productive port. (cid:140)erefore MinBD [5] adds the so-called silver (cid:131)it mechanism to guarantee that at least one (cid:131)it is switched to a productive port. Additionally, MinBD introduces dual ejection and a small side bu(cid:130)er that bu(cid:130)ers some (cid:131)its instead of immediately de(cid:131)ecting them. (cid:140)e combination of these three mechanisms signi(cid:128)cantly reduces the number of de(cid:131)ected (cid:131)its and increases network performance. In an orthogonal topology, there are multiple shortest routes to the destination. Considering all of them in the switch arbitration can also reduce de(cid:131)ections [12]. (cid:140)is technique called multiple dimension routing is one of the enhancements that DeBAR [9] adds to MinBD. Additionally, (cid:131)its that are close to their destination are prioritized and the side bu(cid:130)er can also be used for ejection collisions. But the improvements on network throughput are small and the impact on the critical path of the arbitration logic is unclear. (cid:140)e mentioned approaches identi(cid:128)ed the main challenges for e(cid:129)cient de(cid:131)ection routers: expensive switch arbitration, excessive de(cid:131)ection, receiver reassembly, livelocks and request-respond deadlocks. Bu(cid:130)erless in-order de(cid:131)ection addresses the same challenges, but with completely di(cid:130)erent techniques: the unidirectional torus topology simpli(cid:128)es the switch (only two output ports) and reduces de(cid:131)ections (nonstop transportation in x- and y-rings), while the in-order delivery prevents livelocks and request-respond deadlocks and replaces the reassembly bu(cid:130)er. Minimally bu(cid:128)ered def lection routing with in-order delivery in a torus NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea 2.3 Further Related Work Bu(cid:130)erless transportation in a bidirectional ring was studied in [11]. Comparable to a single horizontal or vertical ring in our approach, (cid:131)its are injected if a free slot appears and continuously transported to the destination. However, there is no mechanism to request a free slot and consequently a node can wait in(cid:128)nitely to inject a message. (cid:140)e Low-Cost Router for On-Chip Networks [10] is similar to our approach, as it prioritizes the routing within one dimension and uses a special bu(cid:130)er when changing from x- to y-direction. But the (cid:131)ow control is credit based with very small bu(cid:130)ers, hence (cid:131)its are stalled, not de(cid:131)ected. Multiple levels of bu(cid:130)erless rings build the hierarchical ring topology of HiRD [1]. Depending on the location of source and destination node, a (cid:131)it has to cross several rings. Whenever there is no free slot in the next ring, the (cid:131)it stays in the current ring for an extra round. To avoid that a (cid:131)it circles in(cid:128)nitely, the bridge routers between two rings monitor the (cid:131)its and when a (cid:131)it has been in the ring for too long, it can change the ring as soon as there is the next free slot. (cid:140)e mechanism to provide fair injection is also interesting: when a node was not able to inject a (cid:131)it for a longer time, it can assert a global signal that stops all other nodes in the ring from injection, until the starved node can inject its (cid:131)it. 3 BUFFERLESS IN-ORDER ROUTING 3.1 Topology An unidirectional ring is the perfect topology for bu(cid:130)erless routing. (cid:140)e arbitration is simple, because apart from the input and output ports to the local processing element, there is only an input from the previous node and a output to the next node. Consequently, there is only one possible collision, if both inputs should be switched to the next output port (switching the local input to the local output makes no sense). By giving the previous input priority over the local input, this collision can be avoided. Of course, the local input port must be bu(cid:130)ered, but some kind of injection bu(cid:130)er between processing element and local input (as well as a ejection bu(cid:130)er between local output and processing element) is required in any NoC architecture and not considered part of the NoC. Given this “in-NoC-(cid:128)rst” arbitration, (cid:131)its are continuously transported to the target node, once they entered the ring. (cid:140)e NoC behavior corresponds to a constantly rotating gear wheel, where a (cid:131)it hooks to a free tooth and leaves the tooth, when the rotation advanced it to the target. A ring topology does not scale when the number of nodes increases. Hence, the one dimensional ring is re(cid:128)ned to a two dimensional torus: the nodes are arranged in a square and every node is connected to two rings: a horizontal one that connects all nodes within a row and a vertical one that connects all nodes within a column. (cid:140)e rings are still unidirectional and so the torus is also unidirectional. 3.2 Basic Algorithm In an unidirectional 2D-torus there are several routes from one node to another, but the shortest one corresponds to x-y-routing: enter the horizontal ring until the target column is reached and then change to the vertical ring until the target node is reached. (cid:140)ereby, each route is divided into two laps, each of them consisting of an unidirectional ring that allows bu(cid:130)erless transportation, because any collisions are avoided. Collisions only appear when entering the two rings. (cid:140)e (cid:128)rst potential collision point is located between the local input port and the horizontal ring. As mentioned above, such an injection bu(cid:130)er is unavoidable and not considered part of the NoC. (cid:140)e second potential collision arises, when a (cid:131)it has to change from the horizontal ring to the vertical ring. (cid:140)ere is no guarantee for an empty slot in the vertical ring, when the (cid:131)it arrives at the intermediate node, where it has to change the direction. (cid:140)erefore, (cid:131)its have to wait in the so-called corner bu(cid:130)er until there is a free slot in the vertical ring. Note that in this basic implementation a (cid:131)it is always injected to the horizontal ring and always ejects from the vertical ring. If source and target node share the same vertical ring, the (cid:131)it has to take a round trip in the horizontal ring before moving to the corner bu(cid:130)er and the vertical ring. Accordingly, a (cid:131)it cannot eject directly from the horizontal ring but has to take a round trip in the vertical ring before ejection. Fig. 1a shows the micro-architecture of the basic router. 3.3 Bu(cid:130)er Over(cid:131)ow Handling In order to prevent the corner bu(cid:130)ers from over(cid:131)owing, de(cid:131)ection is applied: When a (cid:131)it tries to change from the horizontal ring to a corner bu(cid:130)er which is full, the (cid:131)it is forced to remain in the horizontal ring for an extra round. A(cid:137)er a full rotation of the ring, the (cid:131)it can try once again to enter the corner bu(cid:130)er. Unfortunately, this behavior can destroy the order of the (cid:131)its. If the corner bu(cid:130)er becomes available before the (cid:131)it completed its extra round and another (cid:131)it from the same source node takes the free entry in the corner bu(cid:130)er, the second (cid:131)it overtakes the (cid:128)rst (cid:131)it. To preserve the (cid:131)it order, the corner bu(cid:130)er does not accept any (cid:131)its for a complete round, whenever it refused to accept a (cid:131)it due to the lack of free bu(cid:130)er space. Since the other nodes can continue to (cid:128)ll free slots in the ring during the extra round, their (cid:131)it order can be destroyed, too. To preserve their (cid:131)it order, not only the consumer (the corner bu(cid:130)er), but all possible producers (the nodes connected to the ring) have to pause for a round. (cid:140)e other nodes are noti(cid:128)ed by marking the originally refused (cid:131)it. Whenever a node receives a marked (cid:131)it, it immediately stops injecting (cid:131)its to the horizontal ring for one round. (cid:140)ereby, the pause times do not start in the same cycle, but the staggered pauses also preserve the (cid:131)it order. (cid:140)e receive bu(cid:130)er to the local processing element can over(cid:131)ow, too. (cid:140)erefore, the same technique is applied to the vertical rings in combination with the receive bu(cid:130)ers. 3.4 Fair Injection When a node continuously injects (cid:131)its to the horizontal ring, the next node in the ring has no chance to inject any (cid:131)it, because there are no free slots in the ring. (cid:140)e continued non-consideration can lead to starvation and livelocks. To provide a fair injection rate for every node, a mechanism to request a free slot is required. If there is a (cid:131)it in the injection queue but no free slot in the ring, a node can request a free slot from the preceding node by an additional NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J ¨org Mische et al. N W a) basic router S Lo Li E b) unbu(cid:130)ered x-bypass c) bu(cid:130)ered x-bypass d) unbu(cid:130)ered y-bypass e) bu(cid:130)ered y-bypass Figure 1: Router microarchitecture and di(cid:130)erent bypassing variants. signal. When the preceding node receives this signal, it uses only every other free slot for its own injection and passes the remaining free slots to the requesting node. (cid:140)e request might have to be propagated to prevent starvation: if the preceding node also gets no free slot, it requests it from its predecessor and so on. (cid:140)e same mechanism is used when (cid:131)its are waiting in the corner bu(cid:130)er, but there are no free slots in the vertical ring. 3.5 Deadlock Bu(cid:130)erless de(cid:131)ection networks are deadlock free, because each (cid:131)it holds no more than one resource and the number of output ports in a router is greater or equal to the number of input ports [14]. Request-reply deadlocks at the protocol level are impossible, as long as the two nodes are not in the same row or column, because request and reply use physically separate horizontal and vertical rings. If the communicating nodes are within one row or column, the same (horizontal or vertical) ring is used for request and reply. To avoid deadlocks, a simple send bu(cid:130)er that uses eject bubbles is su(cid:129)cient. We illustrate the mechanism by an example: If node A continues to send requests, while node B is processing the (cid:128)rst request from A, the receive bu(cid:130)er and the ring between A and B might get congested with requests. When B completes the request, it cannot send the response, because the ring is congested. By pu(cid:138)ing the response in the send bu(cid:130)er, node B can start to process the next request and removes it from its receive bu(cid:130)er. Consequently, a (cid:131)it from the ring can leave the ring and enter the receive bu(cid:130)er. (cid:140)is creates a free slot in the ring that is immediately used by the send bu(cid:130)er to inject the response. (cid:140)us progress is guaranteed. 3.6 Livelock Bu(cid:130)erless networks are prone to livelocks. To prove livelock freedom, every stage of the transportation must ensure forward progress on its own, i.e. under the assumption that a (cid:131)it can leave the stage (because there is a free slot in the next stage) within (cid:128)nite time, the stage must provide that a (cid:131)it can enter and traverse the stage in (cid:128)nite time, too. Injection to horizontal ring: Progress ensured by the fair injection mechanism. Horizontal ring: Obvious progress in a rotating ring. Horizontal de(cid:131)ection: (cid:140)e (cid:128)rst (cid:131)it that is de(cid:131)ected due to a full corner bu(cid:130)er is the (cid:128)rst (cid:131)it that gets a free slot in the corner injection bu(cid:130)er of node E … A3 B1 de(cid:131)ected (cid:131)it A1 E A2 A D B C Figure 2: Situation, when PaterNoster’s bu(cid:130)er over(cid:131)ow handling destroys the (cid:131)it order. bu(cid:130)er, because the corner bu(cid:130)er injection is blocked during the extra round. Hence, an endless rotation is avoided. Corner bu(cid:130)er: Obvious progress in a (cid:128)rst-in (cid:128)rst-out bu(cid:130)er Injection from corner bu(cid:130)er to vertical ring: Progress ensured by the fair injection mechanism. Vertical ring: Obvious progress in a rotating ring. Vertical de(cid:131)ection: Same as for horizontal de(cid:131)ection. Ejection: Progress ensured by the receiving processing element. 4 VARIATIONS Two recently published implementations of bu(cid:130)erless in-order de(cid:131)ection are discussed in this section. We discover correctness issues and provide (cid:128)xes as well as further modi(cid:128)cations to improve performance. 4.1 PaterNoster (cid:140)e PaterNoster router [13] assumes that a (cid:131)it that reaches the destination can be ejected immediately. (cid:140)us no over(cid:131)ow handling is required in the vertical ring. Although this simpli(cid:128)es the router logic, the receiving processing element needs a mechanism to ensure the rapid ejection, typically a large and costly receive bu(cid:130)er. (cid:140)e second di(cid:130)erence between PaterNoster and basic in-order de(cid:131)ection also a(cid:130)ects over(cid:131)ow handling, but in the horizontal ring. While in the basic implementation a de(cid:131)ected (cid:131)it stalls all nodes along the ring for one round, PaterNoster stalls only speci(cid:128)c nodes. Only if the (cid:128)rst (cid:131)it in the injection queue targets the corner bu(cid:130)er that triggered the de(cid:131)ection, the node is stalled. At (cid:128)rst glance this behavior seems like a good way to reduce the stalling of nodes, but the optimization can destroy the order of (cid:131)its. (cid:140)e problem occurs, Minimally bu(cid:128)ered def lection routing with in-order delivery in a torus NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea if the (cid:128)rst (cid:131)it in the injection queue does not target the de(cid:131)ecting corner bu(cid:130)er, but the second entry does and other (cid:131)its with the same destination are already in the ring. Fig. 2 illustrates the problem ((cid:131)its are named a(cid:137)er their destination). Flit A1 at node E targets A, but was de(cid:131)ected. Since the (cid:128)rst (cid:131)it B1 in the injection queue of E targets B, node E is not stalled and the next two slots in the ring will be (cid:128)lled with the two (cid:131)its from E. Unfortunately, (cid:131)it A2 is already in the ring and by injecting A3 to the free slot at node A, the order of the A-(cid:131)its is destroyed. 4.2 Caerus Like PaterNoster, CAERUS [17] expects immediate ejection at the receiving node. In the horizontal rings it uses the same fair injection mechanism as the basic in-order router, but in the vertical rings, a di(cid:130)erent algorithm is used. As long as the (cid:128)lling of a corner bu(cid:130)er is below a threshold, only every other node uses free slots for injection. A(cid:137)er a (cid:128)xed time interval the other half of nodes is allowed to use free slots for injection. If the corner bu(cid:130)er is (cid:128)lled beyond a threshold, every free slot is used to inject. Hence, at high network load the mechanism will be disabled and at low load it is not necessary. Disabling the arbitration is problematic, because this could cause the starvation of a core (see section 3.4), subsequently forward progress is no longer guaranteed and thus livelocks are possible. 4.3 Bypassing Rings As mentioned in section 3.2, if source and target nodes share one common ring (either vertical or horizontal), the (cid:131)its have to take a round trip in the other ring anyway. (cid:140)is needless de(cid:131)ections can be avoided by additional bypasses in the router. (cid:140)e horizontal bypass connects the local input either directly with the north output (unbu(cid:130)ered horizontal bypass, Fig. 1b) or indirectly with the corner bu(cid:130)er (bu(cid:130)ered horizontal bypass, Fig. 1c). Since the local input typically is already bu(cid:130)ered, a direct injection to the north output seems to be the be(cid:138)er solution, as it reduces the pressure on the corner bu(cid:130)er. Nevertheless, the direct injection complicates the north output arbitration and both PaterNoster and CAERUS use the bu(cid:130)ered bypass. A bypass requires arbitration. Since the input port is bu(cid:130)ered (i.e. does not result in a costly de(cid:131)ection) and the bypass substantially reduces transportation time, giving the bypass the lower priority makes sense. (cid:140)e vertical bypass can either connect the west input with the local output (unbu(cid:130)ered vertical bypass, Fig. 1d) or the corner bu(cid:130)er with the local output (bu(cid:130)ered vertical bypass, Fig. 1e). (cid:140)e la(cid:138)er alternative can reduce de(cid:131)ections by bu(cid:130)ering (cid:131)its in the corner bu(cid:130)er, but also increases the pressure on this bu(cid:130)er, which may lead to other de(cid:131)ections. PaterNoster uses unbu(cid:130)ered vertical bypassing, while CAERUS uses bu(cid:130)ered vertical bypassing and reports a be(cid:138)er performance [17]. Again, preferring the south input to the vertical bypass is reasonable, because a bypass saves several cycles of latency. However, no arbitration is necessary, if two local ejection ports (one from the vertical ring and one from the vertical bypass) are allowed. Such a double ejection already showed a positive in(cid:131)uence on the performance in MinBD [5], another de(cid:131)ection router. B U G G 0 0 x backward request (0, R) y backward request (0, R) x stall (G, S) y stall (G, S) x bypass (N, U, B) y bypass (N, U, B) Figure 3: Router con(cid:128)guration names 4.4 Speci(cid:128)c Stall (cid:140)e handling of de(cid:131)ected (cid:131)its in the basic algorithm is safe, but more nodes than necessary are required to stall for a complete rotation of the ring. (cid:140)e improved algorithm of PaterNoster reduces the stalling of nodes, but as shown in section 4.1, it is not working correctly and may destroy the (cid:131)it order. To reduce the stalling to the minimum, enhanced bookkeeping is necessary. Each node has to record if and when every other node de(cid:131)ected a (cid:131)it. Flits that target a node that recently de(cid:131)ected a (cid:131)it, may not be injected. Each node has a counter for every node in the ring. Whenever a (cid:131)it that was marked due to a de(cid:131)ection arrives at a node, the counter that corresponds to the de(cid:131)ecting node is set to the size of the ring. All counters are decreased in every cycle. A (cid:131)it may only be injected, if the counter corresponding to the target node is equal to zero. 5 EVALUATION Each router con(cid:128)guration is characterized by a name consisting of 6 le(cid:138)ers, see Fig. 3. (cid:140)e bypasses can be bu(cid:130)ered (B), unbu(cid:130)ered (U) or there could be no bypass at all (N). (cid:140)e order of de(cid:131)ected (cid:131)its is guaranteed by general (G) or speci(cid:128)c (S) stalling and the fairness of injections can be increased by using a backward request signal (R) or not (0). 5.1 Hardware Costs All router con(cid:128)gurations were implemented in VHDL and synthezised with Altera (cid:139)artus Prime 16.0 targe(cid:138)ing an Altera Stratix V E FPGA. (cid:140)e hardware costs are measured in terms of Adaptive Logic Modules (ALM), the building block of Altera FPGAs. For realistic results, each router is a(cid:138)ached to a RISC-V processing core and has a cornerbu(cid:130)er with 8 entries of 64 bits each. 4x4 routers are connected to form the NoC. (cid:140)e size of a router is computed by averaging the size of all router instatiations within one NoC. We also investigated the power consumption with Altera Power Play and discoverd that the power consumption is roughly proportional to the logic consumption. Since they give no additional insights and the paper length is limited, we omi(cid:138)ed numbers on the power consumption. Fig. 4 shows the hardware costs of bypasses for a basic router with general stalling and no backward request signals (..GG00). (cid:140)e costs of bu(cid:130)ered and unbu(cid:130)ered bypasses are roughly the same, but single horizontal bypasses are more expensive than single vertical ones. If there are two bypasses, the costs are even higher, with one interesting exception: the con(cid:128)guration with a bu(cid:130)ered vertical and an unbu(cid:130)ered horizontal bypass (BUGG00) is even cheaper than the con(cid:128)guration without the vertical bypass. Anyway, even NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J ¨org Mische et al. ALMs 780 760 740 ALMs 1,000 800 0 0 G G N N 0 0 G G U N 0 0 G G B N 0 0 G G N U 0 0 G G U U 0 0 G G B U 0 0 G G N B 0 0 G G U B 0 0 G G B B Figure 4: Impact of bypasses on router size ....00 ....0R ....R0 ....RR . . G G N N . . G G B B . . S G N N . . S G B B . . G S N N . . G S B B . . S S N N . . S S B B Figure 5: Impact of stall and injection on router size the largest router with two bu(cid:130)ered bypasses (BBGG00) increases the router size only by 53 ALMs or 7%. (cid:140)e backward request signal is even cheaper, as Fig. 5 shows. (cid:140)e con(cid:128)gurations within one bundle of four bars only di(cid:130)er in the number of request signals and the di(cid:130)erences are very low. (cid:140)e highest di(cid:130)erence is between NNSS00 and NNSSRR is only 16 ALMs or 1.5%. Sometimes routers with the additional signals are even smaller than without, therefore the backward request signal costs are negligible. By contrast, the speci(cid:128)c stall mechanism severely increases the router size. Ignoring the small (cid:131)uctuations on the top of the four bars, the bundles in Fig. 5 give the router size of di(cid:130)erent stall con(cid:128)gurations for the smallest (NN….) and the largest (BB….) bypass con(cid:128)guration. If the speci(cid:128)c stall is implemented in one of the rings, the router size is increased by about 175 ALMs (24%) or 350 ALMs (48%) if both rings use the speci(cid:128)c stall method. (cid:140)is holds only for a network size of 4x4 nodes, since a counter is required for every node within a ring, the speci(cid:128)c stall hardware costs further increase with the network size. 5.2 (cid:135)roughput (cid:140)e network throughput was measured with a cycle-accurate inhouse simulator wri(cid:138)en in C. Its accuracy was tested against the VHDL model for up to 4x4 cores with several parallel benchmark programs that run up to 5 million cycles. In every single cycle the state variables (registers) of every single router and link in the VHDL and the C model were compared and all were identical. 0.4 0.2 0 NNGG00 NUGG00 NBGG00 UNGG00 UUGG00 UBGG00 BNGG00 BUGG00 BBGG00 neighbor tornado bitrev shu(cid:132)e transpose uniform Figure 6: Saturation throughput of di(cid:130)erent bypass con(cid:128)gurations 1 0.8 0.6 NNGG00 NUGG00 NBGG00 UNGG00 UUGG00 UBGG00 BNGG00 BUGG00 BBGG00 blackS (cid:131)uidS swapL bodyL dedupM vipsM Figure 7: Normalized netrace execution time for di(cid:130)erent bypass con(cid:128)gurations Multithreaded network traces of the PARSEC benchmarks (provided by netrace [8]) were used to evaluate the network throughput under realistic conditions. For each router con(cid:128)guration we measured the total execution time of the netrace tra(cid:129)c traces and subtracted the execution time within a perfect network (a message arrives at the target node immediately a(cid:137)er sending it), to isolate the network overhead. (cid:140)e remaining network overhead is normalized to a speci(cid:128)c router con(cid:128)guration to enable a comparison between di(cid:130)erent benchmarks. However, netrace traces are only available for 8x8 cores. To examine other network sizes we used synthetic tra(cid:129)c pa(cid:138)erns [3] and measured the saturation throughput in (cid:131)its per cycle and node. (cid:140)e pa(cid:138)erns showed consistent behaviour through all quadratic network sizes from 2x2 to 8x8, but for bitrev, neighbor, transpose and tornado the tra(cid:129)c is so regular and the distance between nodes is so small, that no di(cid:130)erences between the router con(cid:128)gurations occured (Fig. 6). Two bu(cid:130)ered bypasses result in the highest saturation throughput for synthetic tra(cid:129)c pa(cid:138)erns (Fig. 6), while two unbu(cid:130)ered bypasses result in the shortest execution time for real benchmark traces (Fig. 7). Anyway, the di(cid:130)erence between bu(cid:130)ered and unbu(cid:130)ered bypasses is small and both experimental setups show that the absence of bypasses signi(cid:128)cantly hurts performance. Due to the moderate hardware costs, bypasses should be used in both rings, no ma(cid:138)er if they are bu(cid:130)erd or unbu(cid:130)ered. Minimally bu(cid:128)ered def lection routing with in-order delivery in a torus NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea 1.02 1 0.98 UUGG00 UUGS00 UUSG00 UUSS00 blackS (cid:131)uidS swapL bodyL dedupM vipsM Figure 8: Normalized netrace execution time depending on stall mechanism 1 0.95 0.9 0.85 UUGG00 UUGG0R UUGGR0 UUGGRR blackS (cid:131)uidS swapL bodyL dedupM vipsM Figure 9: Normalized netrace execution time depending on the fair injection mechanism By contrast, the speci(cid:128)c stall mechanism does not decrease the execution time. As Fig. 8 shows, if speci(cid:128)c stalls are used in both rings it can even hurt the performance. Additionally, the hardware costs of the speci(cid:128)c stall mechanism are huge and scale with network size, therefore the cheaper general stall mechanism should always be used. (cid:140)e backward request signal for fair injection comes for free, but it reduces network overhead by about 6%, independent of the benchmark (see Fig. 9). (cid:140)erefore any router should implement the backward signal in at least one ring. A backward signal in the other ring is not necessary, as it does not further decrease execution time. Evaluating hardware costs against network thoughput, the best router con(cid:128)guration has two bypasses (no ma(cid:138)er if they are bu(cid:130)ered or unbu(cid:130)ered), the general stall mechanism and at least one ring with backward request signal, for example UUGGRR or BBGGRR. 5.3 Scaling To examine the scalability, we synthesized networks of 2x2, 3x3, 4x4, 5x5, 6x6 and 7x7 routers and varied the number of corner bu(cid:130)er entries from 2 to 32. We discovered that the in(cid:131)uence of the router con(cid:128)guration on the router size is constant and therefore independent of network size and cornerbu(cid:130)er size. Only if speci(cid:128)c stalls are used, the router size grows with the network size. Since speci(cid:128)c ALMs 1,500 1,000 500 0 2x2 3x3 4x4 5x5 6x6 7x7 cb32 cb16 cb8 cb4 cb2 Figure 10: Router size depending on NoC size and corner bu(cid:130)er size for UUGGRR con(cid:128)guration 0.4 0.2 0 15 10 5 0 cb32 cb16 cb8 cb4 cb2 2x2 3x3 4x4 5x5 6x6 7x7 8x8 Figure 11: Saturation throughput per node cb32 cb16 cb8 cb4 cb2 2x2 3x3 4x4 5x5 6x6 7x7 8x8 Figure 12: Total network throughput at saturation stalls are overly expensive anyway, the results of section 5.1 can be generalized to all network and cornerbu(cid:130)er sizes. Consequently, the scaling results of the UUGGRR con(cid:128)guration that are presented here can be generalized to all con(cid:128)gurations. As Fig. 10 shows, the router size is independent of the network size. (cid:140)at is somewhat suprising, because the more nodes are in the network, the more bits are required to address them. Apparently, this logarithmic increase is so small that it vanishes in the inevitable noise of the FPGA synthesis. (cid:140)e (cid:128)gure also shows that the corner bu(cid:130)er has a superlinear in(cid:131)uence on the router size, therefore the number of entries should be kept small. (cid:140)e impact of the corner bu(cid:130)er size on the performance can be seen in Fig. 11, where the saturation throughput of a UUGGRR router under uniform random tra(cid:129)c is presented as a function of the network size and corner bu(cid:130)er size. A small corner bu(cid:130)er with NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea J ¨org Mische et al. 120 100 80 blackL blackM blackS bodyL cannealM dedupM ferretM (cid:131)uidL (cid:131)uidM (cid:131)uidS swapL vipsM x264S MinBD NNGG00 PaterNoster Caerus UUGG00 UUGGRR Figure 13: Normalized netrace execution time normalized to MinBD for di(cid:130)erent in-order de(cid:131)ection routers 2 or 4 entries limits the network throughput, but above 8 entries the performance increase is only small. Fortunately, the distance between the lines in Fig. 11 (i.e. the performance increase due to the corner bu(cid:130)er size) remains constant for larger networks, that means the optimal corner bu(cid:130)er size is independent of the network size. Although the throughput per node decreases for larger networks, the overall network throughput increases, as Fig. 12 illustrates. 5.4 Comparison with MinBD Finally, di(cid:130)erent con(cid:128)gurations of bu(cid:130)erless in-order routers are compared with MinBD [5], a state-of-the-art bu(cid:130)erless router with out-of-order delivery. We implemented MinBD (including side bu(cid:130)ers, silver (cid:131)it and dual ejection) in our cycle-accurate simulator. Fig. 13 shows the network overhead (execution time minus execution time of a perfect router) normalized to MinBD for netrace traces from PARSEC benchmarks. (cid:140)e simplest router con(cid:128)guration without bypasses and backward request signals (NNGG00) has the lowest hardware costs, but it increases the network overhead compared to MinBD. (cid:140)e network overhead of PaterNoster, Caerus and our choice (UUGGRR) is very similar and on average about 15% be(cid:138)er than MinBD. But UUGGRR is always 1% be(cid:138)er than Caerus, which is 1% be(cid:138)er than PaterNoster. For some benchmarks (blackS, cannealM, x264S) omi(cid:138)ing the backward request signal has no e(cid:130)ect, but for most benchmarks there is a penalty of more than 5%. 6 CONCLUSION In this paper we present the (cid:128)rst implementation of bu(cid:130)erless inorder de(cid:131)ection that works correctly without livelocks or deadlocks. A(cid:137)er thorough investigation of the design space of the router con(cid:128)guration, a router with bypasses for x- and y-ring, a general stall mechanism to preserve the (cid:131)it order and a backward request signal in at least one ring was identi(cid:128)ed as optimal in terms of throughput and hardware costs. A corner bu(cid:130)er with 8 entries seems to be su(cid:129)cient for most applications. It reduces the network overhead when executing PARSEC benchmarks by 1% compared to Caerus, 2% compared to PaterNoster and 15% compared to MinBD. (cid:140)e source code of the VHDL model and the simulator is available at h(cid:138)ps://github.com/unia- sik/rcmc. "
XYZ-Randomization using TSVs for Low-Latency Energy Efficient 3D-NoCs.,"In this paper, we propose a method to design low latency and low energy networks for 3D Network-on-Chip (3D-NoC). Recent many-core processors require low-latency interconnection networks since the increasing number of cores limits the network performance. To achieve high performance in such many-core chips, small-world or random networks have been applied in the NoC field. However, the actual diameters and average shortest path lengths (ASPL) of these networks are far from the theoretical lower bound. In this work, we propose an approach based on the graph theory to design ultra low-latency topologies. We introduce a method to design a network that has low values of diameter and ASPL, with configurable upper bound of wire length, called opt ASPL. We also show that irregular topology, such as the topology used in opt ASPL, has a higher average energy consumption than general regular topology like 3D torus. In NoCs, energy budget and link length are limited, and thus such parameters must be carefully considered. Therefore, we introduce a multi-objective optimization for the ASPL and energy consumption called opt A/e which can obtain the Pareto optimal set useful for NoC designers. In a router with 64 nodes per chips and 4 chips stacked with a 3D-NoC, our proposed network optimized for energy consumption has a lower ASPL by 26.8% and a lower energy consumption by 10.9% compared to a 3D torus.","XYZ-Randomization using TSVs for Low-Latency Energy Eﬃcient 3D-NoCs H. Nakahara, Ng.Anh Vu Doan Keio University Yokohama, Japan, 223-8522 doan@am.ics.keio.ac.jp R. Yasudo, H. Amano Keio University Yokohama, Japan, 223-8522 blackbus@am.ics.keio.ac.jp ABSTRACT In this paper, we propose a method to design low latency and low energy networks for 3D Network-on-Chip (3D-NoC). Recent many-core processors require low-latency interconnection networks since the increasing number of cores limits the network performance. To achieve high performance in such many-core chips, small-world or random networks have been applied in the NoC ﬁeld. However, the actual diameters and average shortest path lengths (ASPL) of these networks are far from the theoretical lower bound. In this work, we propose an approach based on the graph theory to design ultra low-latency topologies. We introduce a method to design a network that has low values of diameter and ASPL, with conﬁgurable upper bound of wire length, called opt ASPL. We also show that irregular topology, such as the topology used in opt ASPL, has a higher average energy consumption than general regular topology like 3D torus. In NoCs, energy budget and link length are limited, and thus such parameters must be carefully considered. Therefore, we introduce a multi-ob jective optimization for the ASPL and energy consumption called opt A/e which can obtain the Pareto optimal set useful for NoC designers. In a router with 64 nodes per chips and 4 chips stacked with a 3D-NoC, our proposed network optimized for energy consumption has a lower ASPL by 26.8% and a lower energy consumption by 10.9% compared to a 3D torus. KEYWORDS 3-D NoCs, Multi-Ob jective Optimization, Random Networks ACM format: H. Nakahara, Ng.Anh Vu Doan and R. Yasudo, H. Amano. 2017. XYZ-Randomization using TSVs for Low-Latency Energy Eﬃcient 3D-NoCs. In Proceedings of NOCS’17, Seoul, Rep. of Korea, Oct. 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130232 1 INTRODUCTION 3D chip stacking has become a mature technique for implementing recent high density many core architectures. By distributing logic into small stacked chips, a large amount of the total logic can be implemented at a relatively small cost. Also, the increasing wiring delay problem can be solved by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and /or a fee. Request permissions from permissions@acm.org. NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130232 decreasing the number of long wires thanks to the use of 3D interconnections. 3D stacking systems with Through Silicon Via (TSV) represented by Hybrid Memory Cube [4] and 3D XPoint [11] are now popularly used. For interconnecting stacked chips, a 3D-Network on Chip (3D-NoC) can be used. This technology possesses the advantage that nodes on diﬀerent layers can be connected in the same way as the ones on the same chip. Also, by using links of 3D-NoCs, it is possible to have a more eﬃcient use of TSVs whose cost is higher than that of wires on chip. Currently, the most commonly used topologies for 3D-NoC are simple 3D mesh or torus which naturally ﬁts in stacked systems. However, when the number of nodes in a 3D-NoC becomes large, the increasing latency due the large average shortest path length (ASPL) of such simple topologies has become a serious issue. Random networks which take advantage of the small world phenomenon have received some attention recently as they are cost eﬃcient networks with a small ASPL [8]. Although optimization techniques of this topology have been proposed for 2D-NoC [13], the case of 3D-NoC optimization has not been well studied yet. In this work, we propose an optimization technique for 3D-NoC using random topologies, and considering both the ASPL and energy consumption under limited wire length. First, we apply an optimization technique for the ASPL to 3D-NoCs. Although it reduces the ASPL compared to 3D mesh and classical random networks, it appears that the energy consumption is increased. Generally, reducing the ASPL also reduces the total energy consumption of the NoC by reducing the energy in routers. However, since TSVs between stacked chips consume larger energy than wires, reducing the ASPL sometimes increases the use of TSVs and thus the energy consumption in 3D-NoCs. In such a case, considering an ob jective function so as to include both the ASPL and the energy consumption could be a solution. However, here, we propose to apply a multi-ob jective optimization which can establish a Pareto optimal set which is useful for NoC designers as it can provide deeper information of the problem and trade-oﬀ possibilities. The main contribution of the paper is as follows: • A novel graph called a stacked grid graph that consists of multiple chips is proposed and analyzed. It represents a 3D-NoC connected with TSVs. • An optimization method for the ASPL called opt ASPL is proposed. • A multi-ob jective optimization technique for both for the ASPL and the energy consumption called opt A/e is proposed to obtain a Pareto optimal set useful for NoC designers. The rest of the paper is organized as follows. After introducing some related work in Section 2, we deﬁne the theoretical base for optimization in Section 3. Then, an optimization for the ASPL called opt ASPL is proposed and the results NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea H. Nakahara, Ng.Anh Vu Doan and R. Yasudo, H. Amano are analyzed in Section 5. For addressing the problem of increased energy consumption, in Section 6, we apply MOO and propose opt A/e. Finally, in Section 7, we conclude the paper. 2 RELATED WORK Although the main focus of 3D-NoC has been on 3D-mesh/ torus, researches on optimized topologies have been carried out. Matsutani and his colleagues evaluated the case when a few chips with random networks are inserted into a 3D-NoC with 3D mesh topologies [10]. It appeared that inserting only a chip with random network can improve the ASPL drastically. However, this research only assumed mesh structure for 3D direction. Optimization of additional links to 3D-mesh to minimize the energy delay product (EDP) has been investigated in [5]. This research focused on additional links for the 2D direction. An automatic network generator proposed in [16] synthesizes a network for 3D SoCs considering eﬃcient ﬂoorplanning. This research mainly focused on ﬂoorplanning optimization of chip design rather than on topologies of 3DNoCs. As an approach based on the graph theory, the optimization of irregular topologies has been proposed in [13]. This work treated a method to ﬁnd the minimum diameter and ASPL with a given number of nodes and their degree. They tackled the order/ degree problem [1] instead of the degree/diameter problem [12] which is about ﬁnding the maximum number of nodes with ﬁxed degree and diameter. Although it does not consider an energy consumption, their methodology can still be applied to 3D-NoCs. 3 XYZ-RANDOMIZABLE 3D-NOC Throughout the paper we deal with 3D-NoCs on the basis of the following model. 3.1 Deﬁnition and Notation A stacked grid graph is an undirected graph G = (V , E ) such that V = {(x, y , z ) | 0 ≤ x, y ≤ √N − 1, 0 ≤ z ≤ C − 1} is a set of N C nodes and E is a set of edges connecting a pair of nodes in V . The nodes in V are arranged in a 3-dimensional space so that each node (x, y , z ) is located at the position (x, y , z ) where 0 ≤ x, y ≤ √N − 1 and 0 ≤ z ≤ C − 1. Let ux ,uy , and uz denote the x-, y -, and z -coordinate of u, respectively. A stacked grid graph can be considered as a representation of a 3D-NoC with C chips. When C = 1, a stacked grid graph corresponds to a 2D-NoC. For any two nodes u and v in V , let H (u, v) denote the hop count (the number of edges) of the shortest path between u and v . Let D(G) and A(G) be the diameter and the average shortest path length (ASPL) of a stacked grid graph G, respectively. Formally, D(G) = max{H (u, v)|u, v ∈ V }, A(G) = Xu 6=v H (u, v) N C (N C − 1) . (1) (2) 3.2 Design of Links An edge (u, v) such that uz = vz simply corresponds to a wire on a chip. Since diagonal wiring on a chip is not allowed, the length of a wire ℓ(u, v) is equal to the Manhattan distance between u and v . Formally, (3) ℓ(u, v) = |ux − vx | + |uy − vy |. We evaluate the latency of a link on the basis of the value of ℓ(u, v). Meanwhile, an edge (u, v) such that uz 6= vz must include two wires on two chips and a TSV. In a conventional 3DNoC, such an edge is not considered. That is, there exists edges (u, v) such that uz 6= vz only if ux = vx and uy = vy . In this paper, however, we discuss a more progressive assumption: we can design a hybrid link consisting of both a TSV and wires on two chips. This assumption makes sense in terms of latency, because the latency of a TSV is much lower than that of a wire [2]. A 3D-NoC is said to be XYZrandomizable if it follows this assumption. So, in this case, since the latency of a TSV can be ignored, the length of a link ℓ(u, v) is given by (3). 3.3 Order/Degree Problem for XYZ-randomizable 3D-NoC Using a stacked grid graph, we can easily formulate the order/degree problem for XYZ-randomized 3D-NoCs. However, in practical, we must also consider wire length L(G), deﬁned as follows. L(G) = max (u,v)∈E (ℓ(u, v)). (4) If the wire length between two nodes is larger than a certain limitation, the clock frequency of the NoC is dominated by the wiring delay rather than the delay of the functional logic, arbiter or switch in the router. Although it is depending on the pipeline structure of the router, we can assume a certain maximum wire length between two nodes L as the length with which the wiring delay does not degrade the clock frequency. A stacked graph is, thus, said to be K -regular and Lrestricted if the degree of each node is K or less and L(G) is L or less, respectively. Let G(N , C, K, L) denote a K -regular L-restricted stacked grid graph with C chips each of which consists of N nodes. We then deﬁne the order/degree problem for XYZ- randomizable 3D-NoC as follows: given N , C , K and L, ﬁnd a graph G(N , C, K, L) with the minimum ASPL. 3.4 Energy Consumption of 3D-NoC In a 3D-NoC, the energy consumers include routers, wires on a chip, and TSVs. Let Erouter , Etile , and Etsv denote energy for passing 1 bit of data through a router, a wire on a chip, and a TSV, respectively. The average energy consumption is then given by: H (u, v)Erouter + W (u, v)Etile + T (u, v)Etsv N (N − 1) , Eavg (G) = Xu 6=v (5) where W (u, v) denote the tile length when a ﬂit is sent from u to v , and T (u, v) denote the count of used TSV when a ﬂit is sent from u to v . Formula (5) indicates that the energy consumption of routers decreases as the ASPL decreases, and thus our ﬁrst method will focus on reducing the ASPL. XYZ-Randomization using TSVs for Low-Latency Energy Eﬃcient 3D-NoCs NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea 4 LOWER BOUNDS ON DIAMETER AND ASPL As in [13], we can show tight lower bounds on the diameter and the ASPL of stacked grid graphs. We ﬁrst show lower bounds for K -regular graphs and for L-restricted graphs, separately. Afterward we combine them to obtain tight lower bounds for K -regular L-restricted stacked grid graphs. Finally, we discuss the eﬀects of increasing the number C of chips and the 3D-randomization. 4.1 Respective Lower Bounds for K -regular graphs and for L-restricted Graphs (6) (i = 0) (i > 0). First, we show the lower bounds for K -regular graphs. For any ﬁxed node u, we can partition all the nodes in V into subsets V0 , V1 ,... such that Vi = {v ∈ V | the shortest path length between u and v is i }. Clearly V0 = {u} and |V1 | = K hold. Each node in V1 is connected with K nodes in V0 S V1 S V2 and one of them is u. Hence it is connected with at most K − 1 nodes in V2 , and thus V2 has at most K (K − 1) nodes. Similarly, V3 has at most K (K − 1)2 nodes. In general, Vi (i ≥ 1) has no more than K (K − 1)i−1 nodes. Let m(i) denote the maximum number of nodes reachable along i edges from u. From the above, m(i) = (1 min(1 + Pi j=1 K (K − 1)j−1 , N C ) Using m(i), we can obtain lower bounds on the diameter and the ASPL, denoted as D− K and A− K respectively, as follows: D− K = arg min (m(X ) = N C ), (7) K = Pi≥1 (i(m(i) − m(i − 1))) A− N C − 1 Next, we show the lower bounds for L-restricted graphs. For that purpose, we evaluate the ASPL of a graph in which all the pairs of nodes within distance L are connected. The number of nodes that can be reachable along i edges from a node u = (x, y , z ) is dx,y,z (i) = (cid:26)1 (i = 0) min(|{v ∈ V | ℓ(u, v) ≤ i · L}|, N C ) (i > 0). (9) Using dx,y,z (i), we can obtain the lower bounds on the diameter and the ASPL, denoted as D− L andA− L respectively, as follows: (8) X . D− L = arg min (d0,0,0 (X ) = N C ), A− X L = P(x,y,z)∈V Pi≥1 (i(dx,y,z (i) − dx,y,z (i − 1))) N C (N C − 1) (10) . (11) 4.2 Combined Tight Lower Bounds for K -regular L-restricted Graphs Clearly max(D− K , D− L ) and max(A− K , A− L ) are the lower bounds of a K -regular L-restricted graph. However, we can obtain tighter (i.e. larger) lower bounds by combining m(i) and dx,y,z (i). Let mdx,y,z (i) be min(m(i), dx,y,z (i)); then the number of nodes reachable along i edges from node (x, y , z ) in a K -regular L-restricted graph clearly does not exceed mdx,y,z (i). Thus, tight lower bounds for K -regular L-restricted, denoted as D− K,L and A− K,L respectively, are given by D− K,L = arg min (md0,0,0 (X ) = N C ), (12) A− X K,L = P(x,y,z)∈V Pi≥1 (i(mdx,y,z (i) − mdx,y,z (i − 1))) N C (N C − 1) . (13) 4.3 Eﬀects of XYZ-randomization − (14) Let us discuss eﬀects of XYZ-randomization. To this end, we consider the lower bounds on the ASPL for the three following cases: (1) XYZ-randomizable 3D-NoC (i.e. a stacked grid graph as described above with C > 1) (2) 2D-NoC (i.e. a stacked grid graph with C = 1.) (3) conventional 3D-NoC (i.e. a stacked grid graph with C > 1 such that two nodes u and v with diﬀerent z-coordinate can be connected only if ux = vx and uy = vy ). We assume all the cases have ﬁxed values of N C , K , and L. For convenience, let Ntotal = N C . To simplify the analysis, we use big-Theta notation. First, we consider the case of a XYZ-randomizable 3DNoC. From (6) and (8), K A K ∼ Ntotal and we have K = Θ (cid:18) log Ntotal log K (cid:19) . A− From (9) and (11), we have L = Θ (cid:18) √N L (cid:19) = Θ (cid:18) √Ntotal L√C (cid:19) . A− Next, we consider the case of a 2D-NoC. In this case, since any pair of nodes can be connected if L = ∞, A− K is given by (14). On the other hand, since C = 1, (15) can be rewritten as follows: L (cid:19) . Hence our XYZ-randomizable 3D-NoCs can asymptotically L by 1/√C times compared with 2D-NoC. decrease A− Finally, we consider the case of a conventional 3D-NoC. In this case, A− L can be obtained based on the case of XYZrandomizable 3D-NoC, but it can be larger because two nodes u and v with diﬀerent z-coordinate can be connected only if ux = vx and uy = vy . Considering a conventional 3DNoC with C > 1, for any ﬁxed node u, the number of paths including TSV to the other nodes is at least (Ntotal /C ) · (1 + 2 + ... + ⌈(C − 1)/2⌉) = Θ(Ntotal · C ). Thus, we have + C(cid:19) . A− L√C XYZ-randomization can asymptotically decrease A− L by Θ(C ). Similarly we can obtain A− K . For any ﬁxed node u, the lower bound of the ASPL without accounting TSVs is ). Let us note that it is less than A− K in cases of 2D-NoCs and XYZ-randomizable 3D-NoCs. The number of TSVs, each of which corresponds to one edge, from u to L = Θ (cid:18) √Ntotal L = Θ (cid:18) √Ntotal Θ( log Ntotal−log C log K (15) (16) (17) A− NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea H. Nakahara, Ng.Anh Vu Doan and R. Yasudo, H. Amano the other nodes is at least (Ntotal /C ) · (1 + 2 + ... + ⌈(C − 1)/2⌉) = Θ(Ntotal · C ). Thus, we have A− K = Θ (cid:18) log Ntotal − log C + C(cid:19) . log K Conventional 3D-NoCs asymptotically increase A− K by Θ(C− log C/ log K ) compared with 2D-NoCs and XYZ-randomizable 3D-NoCs. Based on the above analysis, we can obtain balanced parameters K , L, N , and C , by satisfying A− K ≈ A− L . (18) 5 IRREGULAR OPTIMIZED 3D GRAPH FOR ASPL 5.1 The optimization method In this section, we propose a method to optimize 3D-NoC topologies for the ASPL. Our goal is to ﬁnd a graph which is close to the exact solution of the order/degree problem for 3D-NoC. Let us note that ﬁnding the exact solution for this problem is a diﬃcult task as the number of possible graphs can be huge (about O((N C )!) since there are (N C )! possibilities of graph arrangements), so our method will rather focus on exploring near optimal solutions. Here, from the practical viewpoint, we choose to avoid the optimization to reduce the ASPL at the sacriﬁce of the diameter, since the execution time is sometimes dominated by the data exchange with the largest delay. Thus, we include both in the ob jective function. In order to compare the graphs in the exploration process, we deﬁne that a graph G is better than another graph G′ if D(G) < D(G′ ) holds or both D(G) = D(G′ ) and A(G) < A(G′ ) hold. Our algorithm, that we call opt ASPL, will explore solutions to ﬁnd a graph G(N , C, K, L) with near optimal values of D(G) and A(G) with the following steps: (1) Generate an initial graph which satisﬁes given C , K and L. In the ﬁrst step, an initial graph can be generated to connect two adjacent nodes with K edges. This generation is a relatively simple operation since it only requires that two adjacent nodes are connected with K edges, and hence it can be achieved for any L > 0. Note that the initial graph is not related to the ﬁnal optimized graph, since the initial graph is converted to a random graph in the second step. Any graph G(N , C, L, K ) can be used as an initial graph. (2) Repeatedly apply the 2-opt swap operation to generate a random graph. In the second step, a random graph is created by iterating the swap operation. The 2-opt swap operation is deﬁned as follows: 2-opt swap operation selects two distinct edges (u1 , u2 ) and (v1 , v2 ) in the graph randomly, and replaces them with the edges (u1 , v1 ) and (u2 , v2 ), respectively. This swap operation obviously does not change the degree of every router. On the other hand, the length of wires can possibly be longer than L. In other words, l(u1 , v1 ) > L or l(u2 , v2 ) > L can occur, even if both l(u1 , u2 ) ≤ L and l(v1 , v2 ) ≤ L hold. For example, l(u1 , v1 ) can be longer than both l(u1 , u2 ) and l(v1 , v2 ) as shown in Figure 1. If the obtained graph does not meet the maximum wire length L, this swap operation should be canceled in order to maintain the operational clock frequency of the NoC. The swap operation should also be canceled if the obtained graph is not better than the original one. Figure 1: Example of swap operation with two chips stacked (e − e′ < 0) (otherwise) However, only applying swap operations iteratively can lead the optimization process to be stuck in a local minimum. Therefore a simulated annealing technique is used to avoid that issue and eventually ﬁnd better solutions. Classical simulated annealing randomly selects a solution s′ in the neighborhood of a current solution s, and decides whether to transit to s′ or not, according to a probability value depending on the ob jective function and the annealing temperature T . The transition probability P is usually represented with the following equation: P = (exp( e−e′ T ) 1 where e and e′ are the ob jective function of s and s′ , respectively. At the beginning, T is set to a high value (denoted Ts ) in order to increase the possibility of changing the solution, but is gradually lowered (to a temperature denoted Te ) for convergence, given a time budget (denoted TM ). These parameters have to be tuned since they will impact the optimization results. In our case, the swap operation cancellation can be avoided following that probability, even if the graph after operation is not better. The opt ASPL algorithm is iterated until a graph with the required value of D is discovered, and the graph obtained after the swap operation hence becomes an irregular graph. Let us note that although random graphs can have lower diameter and ASPL than regular topologies with the same degree such as mesh and torus [8], their lower bounds for these values are higher. (19) 5.2 Evaluation of the optimized graph for ASPL In this section, we evaluate the obtained graphs with opt ASPL and compare them with the general regular 3D-NoC topology. First, we assess the diameter and ASPL values, and discuss their quality by comparing their lower bounds. Next, we also evaluate the energy consumed in the network and discuss the results. We assume that the number N C of cores in each chip is 16 or 64, and the number of stacked chips is ﬁxed to 4. For comparison with the 3D-mesh and the 3D-torus, the degree of each router is set to six. For the parameters of the simulated annealing algorithm, we empirically ﬁxed them to TM = 200, Ts = 5, Te = 0.1, and repeated the optimization S = 100 times to generates S topologies. XYZ-Randomization using TSVs for Low-Latency Energy Eﬃcient 3D-NoCs NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea Figure 2: Graph with opt ASPL, 3D-mesh and 3Dtorus Figure 3: Hop counts with various L 5.2.1 Diameter and ASPL. Fig. 2 shows the diameter and ASPL of the obtained optimized graph with opt ASPL, 3Dmesh, and 3D-Torus, for N C = 16 (Fig. 2a) and N C = 64 (Fig. 2b). In these ﬁgures, aspl opt(N C, L) shows the values obtained with opt ASPL. In order to have a fair comparison basis with the torus topology, the maximum wire length is set to two, because the maximum wire length of folded tori is constantly two. In any cases, the diameter and ASPL of aspl opt is better than mesh and torus. The average hop is 22.8% smaller than that of 3D-torus, and 38.3% smaller than that of 3D-mesh. With a larger network size (with 256 routers, N C = 64), it is 28.8% smaller than that of 3Dtorus and 45.2% that of 3D-mesh. We observe here that the reduction is larger when the network size is bigger. In Fig. 3, we show the ASPL and the diameter obtained with opt ASPL, compared with random topology, for various maximum wire lengths L. This graph also includes the theoretical lower bound (lower bound(N C ,L)) that will serve as a comparison basis. aspl opt(N C ,L) and random(N C ,L) show the average hop count of respectively opt ASPL and random topology, with N C nodes and a maximum wire length of L. Fig. 3a) illustrates the case with 64 nodes, and we can observe that opt ASPL reaches the theoretical lower bound except for the case L =2, where the diﬀerence is only 0.72%. On the other hand, the random topology has values far from the lower bound. This tendency is even more emphasized in the case with 256 nodes. The hop counts of random topology is larger than the lower bound by 27.9% with L=2 and 8.81% with L=6, whereas the diﬀerences with opt ASPL are 6.60% with L=2 and 4.09% with L=6. This shows that opt ASPL can successfully generate a topology close to the theoretical lower bound. 5.2.2 Energy Consumption. We evaluate the energy consumption by using (5) in Section 3.4. We assume here that the energy for passing data in a TSV (ETSV ) is 0.6 pJ/bit, as deﬁned in the literature [9]. The other values are obtained by using the NoC modeling tool DSENT [15] with Bulk45LVT process. In order to deﬁne the same conditions, the supply voltage Vdd is assumed to be 1.2V. As a result, Erouter = 0.1146pJ/bit and Etile = 0.2098pJ/bit are obtained. Fig. 4 shows the energy consumption of each topology. Although opt ASPL shows better ASPL values than other topologies (as shown in the previous subsection), the energy Figure 4: Energy consumption of graph with opt ASPL compared with 3D-mesh/torus consumption is larger than that of regular topologies. It becomes even worse for large values N C and L. With 64 routers and L = 6, the energy of opt ASPL is larger than that of 3D-mesh by 10.8% and 3D-torus by 30.5%. In the case with 128 routers, it is larger than that of 3D-mesh by 21.1% and 3D-torus by 37.6%. This is caused by a large tile length from u to v (W (u, v)), and the number of TSVs used from u to v (T (u, v)) in networks obtained with opt ASPL. For example, let us assume the example shown in Fig.5. In this case, the route from u to v is u → x → v , that is, W (u, v) = 7 in the network obtained with opt ASPL, even when l(u, v) = 1. Following this reason, we analyze the usage ratio of TSV in each topology, as shown in Fig.6. Compared with other topologies, opt ASPL uses more TSVs for routing. The two rightmost bars show a case where u and v are on the same chip. Although this does not require to use TSVs in mesh and torus topologies, opt ASPL uses them. This tendency is only observed in 3D NoCs using TSVs which consume larger energy than wires. These results suggest that it is needed to improve the optimization algorithm to consider the energy consumption. Therefore, in the next section, we will introduce another approach based on multi-ob jective optimization to take that parameter simultaneously into account. NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea H. Nakahara, Ng.Anh Vu Doan and R. Yasudo, H. Amano In MOSA, the following expression is used instead: P = (exp( −c(x,x′ ) T 1 ) (c(x, x′ ) > 0) (otherwise) (22) where x and x′ are the decision vectors before and after transition, c(x, x′ ) is a cost function where six types can be used, as deﬁned in [14], and T is the temperature of annealing that will vary from high to low for convergence, as in SOSA. Here, we choose to use the following cost function: c(x, x′ ) = 1 m m Xi=1 fi (x) − fi (x′ ) (23) As speciﬁed in [14], the probability function is applied when f (x) ≻ f (x′ ) is satisﬁed. That is, the remaining cases (f (x) ∼ f (x′ ) or f (x′ ) ≻ f (x)) transit without using the probability function. However, this method can cause too many transitions in the case of topology optimization. Therefore, we modify the conditions of transition to transit without the probability function only when f (x′ ) ≻ f (x) is satisﬁed, the other cases depending on the probability function. Another improvement we applied is to output several topologies per iteration. Indeed, SOSA is classically a single-solution based method. Here, when there are two or more solutions in the Pareto front of a given iteration, we consider that it is possible to select any of them, with the same probability, for the transition process. It is then possible to quickly generate small sets of solutions, which will be used to update the Pareto front, and hence it allows to explore a larger number of topologies. 6.2 Topology optimization of 3D-NoC For the topology optimization of 3D-NoC here, we deﬁne the decision vector as x = G, the ob jective function vector as f (G) = {A(G), Eavg (G)}, with the following constraints: g(G) = {L(G) − LA , D(G) − DM }. LA is the maximum allowed wire length, and DM is a target diameter that we want to achieve and that will impact the ASPL. Indeed, if the diameter increases, the ASPL will also increase whereas the energy consumption will decrease. Let us note that, unlike in opt ASPL which used the diameter in the ob jective function, here we use the value obtained with opt ASPL in the constraints for the MOO to achieve better ASPL and energy with the same diameter. DM is chosen following the values found previously when P (G) no longer evolves, denoted DA (N , LA ). So, this model will optimize the energy while achieving a required diameter/ASPL. The optimization algorithm, that we call opt A/e, is organized with the following four steps. We need Step 2 to ﬁrst reduce the diameter to meet the targeted value, then standard MOSA is applied in Step 3. • Step 1: A random topology is generated as an initial state. The same method as in Section 5 is used. The current temperature Tc is set to Ts . • Step 2: Apply the 2-opt swap operation to G. The modiﬁed topology G′ replaces G if the following conditions are all met: – the constraint on LA is satisﬁed; – D(G′ ) < D(G) or D(G′ ) = D(G); Figure 5: Example of increasing the TSV usage Figure 6: Evaluation of T (u, v) 6 MULTI-OBJECTIVE OPTIMIZATION In order to consider the energy consumption, we could change the ob jective function of opt ASPL to the ASPL-energy product. However, such an optimization can obtain a single combination of the ASPL and energy. From the viewpoint of NoC design, it is better to obtain various graphs with various combination of the ASPL and energy comsumption. Thus, we propose to apply the multi-ob jective optimization, here. 6.1 Multi-ob jective simulated annealing Unlike the optimization for ASPL only, we propose here to use a multi-ob jective simulated annealing (MOSA) [14]. This method belongs to the multi-ob jective optimization (MOO) approach [6]. Multi-ob jective problems are formulated as follows: opt {f1 (x), f2 (x), . . . , fk (x)} (20) s.t. x ∈ X (21) where {f1 (x), f2 (x), . . . , fk (x)} are the k ob jectives and x ∈ X represents the contraints of the problems that can be rewritten as a functional form g(x). Most of these methods are based on the Pareto-dominance principle to identify eﬃcient solutions [6]: a solution A dominates a solution B if A is better or equal to B for all the criteria and there exists a criterion where A is strictly better. A method such as MOSA therefore allows designers to explore and select multiple eﬃcient solutions. Classical single-ob jective simulated annealing (SOSA) have a transition probability expressed following Equation (19). XYZ-Randomization using TSVs for Low-Latency Energy Eﬃcient 3D-NoCs NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea – P (G′ ) > P (G), P (G) being the number of router pairs whose distance is equal to the target diameter: (24) P (G) = |{u, v ∈ V |H (u, v) = D(G)}| Here, 2-opt swap operation selects two links randomly and swaps them as in the method described in Section 5. This process helps to reduce the diameter, and is repeated until D(G) = DM . • Step 3: Apply the 2-opt swap operation to G. The modiﬁed topology G′ replaces G if the following conditions are all met: – the constraints on LA and DM are satisﬁed; – G′ dominates G (in the Pareto sense). Update Tc and iterate the process for a duration of TM seconds. • Step 4: Iterate Step 1–3 until S solutions are obtained. From these topologies, only the non-dominated ones (in the Pareto sense) are kept, forming the Pareto optimal set. The NoC designer can select any topology from this set, depending on possible design trade-oﬀs. 6.3 Evaluation of topologies with MOSA For the simulation parameters of MOSA, we use the same parameters as for SOSA, described in Section 5.2. The evolution of the solutions from the simulations is shown in Fig. 7 and 8 for N = 16 and N = 64 respectively. The horizontal axis represents the ASPL whereas the vertical axis represents the energy consumption. The blue and light blue points represent the solutions obtained by diﬀerent simulations for DM = DA (N , LA ) and DM = DA (N , LA ) + 1 respectively. In red and orange are other explored topologies. Let us note that analyzing the ASPL can give an interesting assessment of real applications performance since they are related [3]. For N = 16, we can observe that, compared to the blue solutions, the light blue points (which have a higher diameter) have a lower energy consumption. Also, increasing the value of L allows better values of ASPL but at the cost of slightly higher values of energy consumption. For N = 64, the ﬁrst tendency (larger diameter has lower consumption) is only observed for L = 3, whereas larger values of L still improve the ASPL. These observations show that a multi-ob jective optimization allows to have deeper qualitative and quantitative information about the problem and the trade-oﬀ possibilities. For analysis purpose, we selected the topology from the Pareto optimal set whose D(G) is equal to DM and that has the lowest ASPL value. Here, we denote this topology opt(N , LA ). The evaluation results of ASPL and average energy consumption for the network with 64 and 256 nodes are shown in Fig. 9 and Fig 10, respectively. With 64 nodes, compared to Mesh, opt(16,2) reduces the ASPL by 37.5% and energy consumption by 22.1%. Compared to rnd(16,2), ASPL is reduced by 12.7%, energy consumption by 27.9%. In the evaluation with 256 nodes, opt(16,2) reduces ASPL and energy consumption of 3D-Mesh by 44.2% and 22.7%, respectively. Also, compared with rnd(16,2), ASPL is reduced by 14.2%, and energy consumption by 37.7%. Both random topology and optimized topology have the tendency that with a large LA , the ASPL becomes small yet the energy consumption Figure 7: Evolution of the solutions for N = 16 with diﬀerent values of L Figure 8: Evolution of the solutions for N = 64 with diﬀerent values of L becomes large. In such topologies, considering that a lot of long links are allowed, the results are reasonable. The results show that by using MOSA, topologies with both low ASPL and small energy consumption can be generated. Also, by establishing Pareto optimal sets, NoC designers can have a better understanding of the problem, as well as deeper information about trade-oﬀ possibilities. 7 CONCLUSIONS We have proposed two optimization methods for generating 3D-NoCs: a method based on graph theory called opt ASPL and one with multiple ob jective optimization (MOO) called opt A/e. Although opt ASPL can generate graphs whose ASPL is smaller than 3D Torus by 28.8%, the energy consumption is often larger because of frequent use of TSVs between chips. On the other hand, by using MOO, opt A/e NOCS’17, Oct. 19–20, 2017, Seoul, Rep. of Korea H. Nakahara, Ng.Anh Vu Doan and R. Yasudo, H. Amano ”Research and Development on Uniﬁed Environment of Accelerated Computing and Interconnection for Post-Petascale Era”. This work is in part supported by JSPS KAKENHI S grant number 2522002. "
Hybrid Automotive In-Vehicle Networks.,"The design of automotive in-vehicle networks is influenced by several factors like bandwidth, real-time properties, reliability and cost. This has led to a number of protocols and communication standards like CAN, MOST, FlexRay and more recently the use of Ethernet. In the future, wireless in-vehicle communication might also become a possibility. In all of these cases, often hybrid schemes such as the combination of time-triggered (TT) and event-triggered (ET) paradigms have been considered to be useful. Thus, hybrid protocols like FlexRay and TTEthernet, offering advantages of TT and ET communications, are becoming more popular. However, until now the hybrid nature of the protocols has not been exploited in application design. In this paper, we will discuss design strategies for automotive control applications that exploit the hybrid nature of the underlying communication architecture on which they are mapped. Towards this, we will consider a mix of time- and event-triggered schemes as well as a combination of reliable and unreliable communication. Correspondingly, we will show how appropriate abstractions of these hybrid schemes could be lifted to the application design stage.","Hybrid Automotive In-Vehicle Networks Special Session Paper Debayan Roy Technical University of Munich debayan.roy@tum.de Dip Goswami Eindhoven University of Technology d.goswami@tue.nl Michael Balszun Technical University of Munich michael.balszun@tum.de Samarjit Chakraborty Technical University of Munich samarjit@tum.de ABSTRACT The design of automotive in-vehicle networks is influenced by several factors like bandwidth, real-time properties, reliability and cost. This has led to a number of protocols and communication standards like CAN, MOST, FlexRay and more recently the use of Ethernet. In the future, wireless in-vehicle communication might also become a possibility. In all of these cases, often hybrid schemes such as the combination of time-triggered (TT) and event-triggered (ET) paradigms have been considered to be useful. Thus, hybrid protocols like FlexRay and TTEthernet, offering advantages of TT and ET communications, are becoming more popular. However, until now the hybrid nature of the protocols has not been exploited in application design. In this paper, we will discuss design strategies for automotive control applications that exploit the hybrid nature of the underlying communication architecture on which they are mapped. Towards this, we will consider a mix of time- and event-triggered schemes as well as a combination of reliable and unreliable communication. Correspondingly, we will show how appropriate abstractions of these hybrid schemes could be lifted to the application design stage. CCS CONCEPTS • Computer systems organization → Embedded software; Realtime system architecture ; • Networks → Time synchronization protocols; Sensor networks; ACM format: Debayan Roy, Michael Balszun, Dip Goswami, and Samarjit Chakraborty. 2017. Hybrid Automotive In-Vehicle Networks. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. DOI: 10.1145/3130218.3130235 1 INTRODUCTION Electrical and electronic (E/E) systems of modern vehicles are becoming increasingly more complex and diverse. Such a system This work was supported by (i) Deutsche Forschungsgemeinschaft (DFG) through the TUM International Graduate School of Science and Engineering (IGSSE) and (ii) I-MECH (GA no. 737453) project funded by ECSEL JU. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, Seoul, Republic of Korea © 2017 ACM. 978-1-4503-4984-0/17/10. . . $15.00 DOI: 10.1145/3130218.3130235 comprises number of processing units (PU), sensors and actuators connected over a communication network. Several applications are mapped onto these systems. These applications range from safetycritical control applications and advanced driver assistance systems (ADAS) to comfort-related applications and infotainment systems. In order to derive advanced and complex functionality, such applications are often implemented in a distributed fashion. Thus, they require data transmissions between electronic components over communication networks. The large spectrum of applications has led to diverse requirements from the underlying E/E architecture, in particular, the communication network architecture. The design of automotive in-vehicle networks depends on the nature of data to be transmitted. For example, safety-critical control data must satisfy real-time properties and reliability requirements while infotainment and camera-based driver assistance data require high network bandwidth [1]. As a result, the E/E system is composed of several subnetworks connected via gateways. These subnetworks offer different network bandwidth and implements different communication protocol. Each subnetwork serves several applications belonging to a specific functional domain, e.g., FlexRay for chassis, high speed CAN for powertrain, low speed CAN or LIN for body and MOST or Ethernet for infotainment. Furthermore, in-vehicle wireless communication may become a possibility in the future [2] due to its low cost and less cabling overheads. Naturally, reliability is a very big issue with wireless communication. CAN [3] has been the de facto standard for in-vehicular communications for a long time and offers priority-based ET communication. With increasing number of applications being mapped on a modern E/E system, the amount of data to be transmitted over a CAN bus has also increased manifold. As a result, it is challenging to satisfy the real-time requirements for all data in the worst-case, and correspondingly, safety might be compromised. TT communication protocols, such as TTP/C, are becoming more popular for safety-critical control applications as they offer timing determinism. Typically, in TT communication, resource is periodically reserved for a message, and, in case no data is sent, the reserved resource is not used. However, this is resource inefficient and is not sustainable in cost-sensitive automotive domain. The conflicting requirements on timing determinism and resource efficiency have led to the evolution of hybrid protocols. Examples include (i) FlexRay [4] which offers both time-division multiple access (TDMA) and flexible TDMA (FTDMA) communications and (ii) TTEthernet [5] which allows time-triggered, rateconstrained and best-effort traffic simultaneously. These protocols offer the advantages of both TT and ET communications. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea D. Roy et al. measure acutuate measure acutuate measure acutuate Figure 1: Delayed distrete-time system. ∫ h 0 (i) τ ≈ 0 : In this case, the discrete-time system can be modeled as (2) where ϕ and Γ (for given continuous-time model and sampling period) can be calculated as x [k + 1] = ϕx [k ] + Γu [k ], y [k ] = Cx [k ]. ϕ = e Ah , Γ = (e At d t ) · B . (3) In this case, the control input u [k ] is calculated almost instantaneously based on the value of x [k ] and can be written as u [k ] = −K x [k ], (4) where K is the feedback gain. To stabilize the system, K can be calculated using Linear Quadratic Regulator (LQR) or pole-placement techniques [7]. We may note that a discrete-time closed-loop system is stable when all the closed-loop poles are within a unit circle. (ii) 0 < τ < h : The delayed sampled-data model [7], as shown in Figure 1, becomes x [k + 1] = ϕx [k ] + Γ0u [k ] + Γ1u [k − 1], y [k ] = Cx [k ], (5) where Γ0 and Γ1 (for a given delay τ ) are given by In practice, although FlexRay is employed in modern vehicles, the hybrid nature of the protocol is not exploited while designing the applications. In particular, safety-critical control applications use only TDMA communication in FlexRay. This is resource inefficient in the sense that a control loop is inherently robust and the control law need not be always executed deterministically at high frequency [6]. Therefore, ET communication can be naturally multiplexed with TT communication to improve resource-efficiency. In this paper, we discuss two design strategies for control applications exploiting hybrid communication architecture. First, we consider that multiple control applications share a common hybrid bus offering both TT and ET communications. In this problem setting, we outline a bi-modal control strategy where each mode consists of a different controller and a bus schedule. Here, the mode switch request is triggered when the corresponding controlled plant is disturbed. Second, we describe another bi-modal control strategy which exploits best-effort communication to improve the control performance while satisfying the worst-case performance using TT communication. The switch between a fast and a slow controller is based on the online evaluation of the worst-case performance. Towards discussing control strategies over hybrid automotive in-vehicle networks, the paper is structured as follows. In the next section, we provide background on linear feedback control systems and hybrid communication protocols. Subsequently, in Sec. 3, we explain a control/communication co-design strategy exploiting a mix of TT and ET communication. Then, we describe in Sec. 4 how to efficiently utilize elastic communication resource to enhance control performance. Next, we also outline in Sec. 5 some future works in the direction of control over hybrid networks. Finally, we conclude in Sec. 6. 2 PRELIMINARIES 2.1 Feedback Control Systems In this paper, we consider linear time-invariant (LTI) single-input feedback control systems. The continuous-time dynamics for such a system can be represented mathematically as ∫ h−τ Γ0 = 0 ∫ h h−τ (cid:3) (e At d t ) · B , Γ1 = (e At d t ) · B . (6) (cid:2)x (t ) = Ax (t ) + Bu (t ), y (t ) = Cx (t ). (1) Considering an augmented state vector z [k ] = the system can be modeled as x [k ] u [k − 1]]T , y (t ) ∈ R n×1 , u (t ) ∈ R and For an n-th order system with m outputs, x (t ) ∈ R m×1 represent respectively the system states, the control input and the outputs at time t . For LTI systems, the matrices A, B and C (of appropriate dimensions) are constant. In an embedded implementation, the sensors read the feedback signals and the actuator applies the control input at discrete instants of time. Therefore, the closed-loop system can be naturally represented by a sampled-data model, as shown in Figure 1. Here, the feedback signals are measured at time instants {tk } and are represented by {x [k ]} , where, x [k ] = x (tk ). Traditionally, a controller is implemented according to a constant sampling period h , where, tk +1 − tk = h . In addition, software execution and frame transmission result in non-negligible time delays. Particularly, for highly constrained processing units and contention-based networks, these delays can be sufficiently large. They contribute to sensorto-actuator delay τ (as shown in Figure 1) and must be taken into account in the design. In this paper, we will consider three different cases corresponding to different values of τ . z [k + 1] = ϕz z [k ] + Γz u [k ], y [k ] = Cz z [k ], (7) (cid:4) 0 (cid:5) (cid:4) (cid:5) (cid:6) (cid:3) Γ0 I ϕz = , Γz = , Cz = where ϕz , Γz and Cz are given by ϕ Γ1 0 Now, for u [k ] = −K z [k ], we may use LQR or pole-placement techniques to design the feedback gain K . (iii) τ = h : In this case, we consider the controller model from [8] where the control input for the k -th instant is calculated based on the state at the (k − (cid:5) τ (cid:6) )-th instant. For τ = h , the control input u [k ] can be written as C 0 . (8) h u [k ] = −K x [k − 1]. (9) Based on the above control law in Eq. (9) and the system model in Eq. (2), the control gain K can be calculated using a restricted pole-placement technique described in [8]. Besides stability, the design of a controller often needs to consider performance and physical constraints. Common performance Hybrid Automotive In-Vehicle Networks NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea cycle 63 62 . . . 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 slot Figure 2: Example of FlexRay communication. metrics are settling time, steady-state error, overshoot, and so on. Typical physical constraints include input saturation, actuator bandwidth, etc. To design an optimal controller with pole-placement, we need to formulate an optimization problem with closed-loop poles as decision variables satisfying stability condition and physical constraints. Here, the optimization objective is the closed-loop performance. The resulting problem is non-convex in nature, and, we use Particle Swarm Optimization (PSO) technique [9, 10] to solve the problem due to its polynomial-time complexity. 2.2 Hybrid Communication Protocols As already explained, TT communication offers timing determinism while ET communication is resource-efficient. Hybrid protocols like FlexRay and TTEthernet bring them together as described in the following text. FlexRay: Each FlexRay time cycle is mainly partitioned into static segment(ST) and dynamic segment (DYN). ST exhibits TDMA communication and comprises number of slots of equal length (Ψ). A message assigned to a ST slot is transmitted within the corresponding time window. Thus, the start and end of a message transmission is precisely known. On the other hand, DYN is partitioned into number of minislots of equal length (ψ ), where typically ψ << Ψ. A message assigned to the DYN may consume more than one minislot as shown in Figure 2. A DYN slot is a logical entity with one or more minislots allowing flexible TDMA communication. This hybrid communication is realized using a slot counter, S C , where the counter starts from 1 at the beginning of each cycle. When S C = j , the message mi assigned to the j -th slot is sent over the bus (if ready). In ST, the counter is incremented after every Ψ time units, i.e., at the end of each slot. If no data arrives at the beginning of the slot, entire slot of length Ψ goes unused. An example is shown in Figure 2 for m2 in cycle 5. In DYN, the counter is incremented at the end of the last minislot of transmission for a message mi . Counter is updated at the end of the current minislot when data is not ready for transmission (e.g., m4 and m5 in Figure 2) and only one minislot of transmission time is wasted. This results in time-varying transmission delay of the messages in DYN while the worst-case can still be deterministic. TTEthernet: It is an Ethernet-based solution for automotive systems. It allows for TT, rate-constrained (RC) and best-effort (BE) PU1 PU2 FlexRay Figure 3: Problem setting. traffic. (i) TT communication is realized based on time-synchronized static schedules which are non-overlapping. TT messages have the highest priority, and therefore they are sent exactly when scheduled. (ii) RC messages have a certain upper bound on the inter-arrival rate. They are sent only when there are no TT messages. RC messages can be assigned priorities. Messages of same priority are served according to first-in-first-out (FIFO) scheme. Considering the scheduled TT traffic, the assigned priority and the bounded inter-arrival rate, the worst-case delay of an RC message may be determined [11]. Thus, RC traffic is suited for real-time applications with softer latency and jitter requirements. (iii) BE traffic is realized according to standard Ethernet protocol (CSMA/CD) and has no timing guarantees. 3 HYBRID PROTOCOL AWARE CONTROL/ COMMUNICATION CO-DESIGN In this section, we consider a problem setting where multiple control applications communicate over a shared FlexRay bus. As discussed in Sec. 2.2, FlexRay ST offers TDMA-based TT communication while DYN can be used for ET communication. Given this communication architecture, we will discuss further in this section how to design a bi-modal control strategy exploiting the hybrid characteristic. In particular, the idea is that a control application will use ET transmission when in steady state and requests for TT slots in the event of disturbance until the system settles down. In this context, we first formally describe the problem and then explain the bi-modal control strategy and the corresponding communication scheduling strategy. Here, we summarize the works [12–14]. 3.1 Problem Description Consider a set of control applications C = {C1 , C2 , . . ., C N } sharing the FlexRay bus. The sampling period of the controller Ci is given and is denoted by hi . Each application Ci is partitioned into three processing tasks, i.e., Ts , i (measures x [k ]), Tc , i (computes u [k ]) and Ta, i (applies u [k ] to the plant). We assume that each task is triggered periodically according to a processor schedule. The period of task executions in an application Ci is equal to the sampling period hi . We assume that the tasks Ts , i and Tc , i are mapped on to the same PU which is attached to the corresponding sensors. The task Ta, i is mapped on a different PU connected to the actuator. This setting is common where sensors and actuators are spatially distributed. Ta, i receives u [k ] computed by Tc , i as a message mc , i over the communication bus. Thus, mc , i can be sent as a TT or an ET message. The problem setting is illustrated in Figure 3. We consider that external signal can perturb the behavior of a plant which we refer to as disturbance. It causes the system to go     NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea D. Roy et al. Schedule: TT Controller: :  SteadyState After Transient -State Mode  Change  Request Mode  Change  Protocol Schedule: ET Controller:  Transient -State SteadyState After dwell time Figure 4: Bi-modal control strategy over a hybrid protocol. into a transient state. The performance of real-time control systems is often quantified by settling time – the time taken by a closedloop system after a disturbance to reach and remain with a certain error band of the reference signal. In our setting, each control application must satisfy a performance requirement in terms of settling time, i.e., ξd , i . The design objective is to determine the control and communication strategies for all control applications such that they meet their respective settling time requirements even in the worst-case. Furthermore, we consider that TT slots are limited and a part of them are reserved for system applications. This makes the TT slots expensive and limited in availability. Traditionally, each control application gets one dedicated TT slot for predictable and lowlatency communication. However, it is desirable to use as less TT slots as possible. Thus, we aim to reduce the usage of TT slots without compromising much on the control performance. 3.2 Bi-Modal Control Strategy In the setting described above, for an application Ci two control modes are possible, i.e., MT T , i and M ET , i . In MT T , i , control message mc , i is sent in FlexRay ST and the timings are precisely known. The application tasks can be scheduled accordingly so as to achieve negligible sensor-to-actuator delay, i.e., τ = 0. In M ET , i , mc , i is sent as an ET message in FlexRay DYN. Here, mc , i may have to wait until all the messages mapped on the preceding slot ids (with higher priorities) are transmitted. We assume that the worst-case response time (WCRT) of mc , i leads to the case of one sampling period delay, i.e., τ = h . Corresponding to the modes MT T , i and M ET , i , we can design stable and optimal controllers KT T , i and K ET , i respectively according to the method explained in Sec. 2.1. The control in mode M ET , i is slower due to a higher delay (of one sampling period) compared to the mode MT T , i . Thus, performance requirement may not be met if we use only M ET , i . Performance is much better (with a shorter settling time) than the requirement when each application has a dedicated TT slot and MT T , i is used always. However, this is expensive in terms of communication. Thus, the idea is to use a bi-modal control strategy for each application, as illustrated in Figure 4, which switches between the two modes (MT T , i and M ET , i ). In the process, each application only partially uses TT slots. Thus, it is possible to share a TT slot among multiple applications. This potentially reduces the usage of the TT slots saving the communication cost. Here, two important questions are: (i) When to trigger a mode switch request? (ii) When an application get access to a TT slot? In this section, we will only discuss the first question. We will consider the second question in Sec. 3.3. Based on the reasons explained above, the switching strategy can be devised as shown in Figure 4. In this strategy, a controller stays in M ET , i when the system is in steady state while requests a switch to MT T , i when in transient state. The mode switch request is triggered (or the transient state is detected) when the system energy is greater than a threshold, i.e., x [k ]T x [k ] > Et h, i , where Et h, i is the threshold value. In the steady state, the control input and the plant states do not change appreciably and thus the higher value of delay does not deteriorate the control performance. However, in the transient state, the smaller value of delay enables the plant to reach the reference signal quickly. It is to be noted here that after the mode switch request the controller may have to wait a certain time to switch to MT T , i . This time depends on the scheduling strategy of the TT slots that we will discuss next in Sec. 3.3. Now, the question is when should the controller switch back to M ET , i ? A problem that crops up in such a switching control strategy is to guarantee switching stability. Towards this, we consider a dwell time td w , i for which a controller must be in MT T , i before switching to M ET , i . Let us denote ξT T , i as the settling time of the system when the mode MT T , i is used to stabilize the system after maximum disturbance. td w , i can be pessimistically selected such than td w , i > ξT T , i . Therefore, td w , i allows the system to stay in mode MT T , i for sufficiently long to reach a steady state, i.e., x [k ]T x [k ] < Et h, i [15]. This is to ensure that switching does not bring the system to transient-state again. 3.3 Communication Scheduling Strategy We apply the aforementioned control strategy to every control application in C . Let us denote ξ ET , i as the settling time of the system when the mode M ET , i is used to stabilize the system after maximum disturbance. Now, if ξT T , i < ξd , i < ξ ET , i , then the scheduler must allocate TT slot to the controller Ci before the deadline expires to ensure the worst-case requirement. The conservative method would be to allocate individual TT slot to each application which requires TT service to satisfy the performance requirement. However, this is pessimistic. Can we do better than this? This brings us to an important question: How many minimum TT slots are required to satisfy the performance constraints of all the applications? The computation of number of minimum slots consists of two interlocked steps, i.e., (i) the schedulability analysis of one TT slot shared by multiple control applications, and, (ii) the provisioning of TT slots for applications. 3.3.1 Schedulability analysis. In this step, we consider that a number of control applications are contending for a single TT slot. A fixed priority access of the slot can be considered where the priorities are statically determined according to the settling time requirements {ξd , i } . Shorter the value of ξd , i is, higher is the priority that the slot will be accessed by Ci . Furthermore, we consider a certain minimum inter-arrival time r i of disturbances for an application Ci . We further assume that ξd , i ≤ r i which implies that an application must reject a disturbance before the next one arrives. The worst-case occurs when the disturbances arrive for all the applications at the same time. Here, we discuss two different slot access policies, i.e., preemptive and non-preemptive. Fixed priority non-preemptive slot access: We have already stated that (i) an application may have to wait a certain time tw , i Hybrid Automotive In-Vehicle Networks NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea before getting access to the TT slot, and then, (ii) the application must have access to the TT slot continuously for a certain dwell time td w , i . Now, during tw , i the controller in mode M ET , i rejects a certain part of the disturbance, and correspondingly, we can determine the td w , i as td w , i = ξT T , i − βi tw , i , (10) where βi can be approximated by βi = time can therefore be written as ξT T , i ξ ET , i . The actual settling ξ i = tw , i + td w , i , = ξT T , i + (1 − βi )tw , i . (11) We may note that ξ i increases with increase in tw , i and there must be an upper bound of tw , i to meet the requirement ξd , i . Hence, to test the schedulability of Ci , we need to find the greatest possible tw , i (denoted by ˆtw , i ) which leads to the worst-case settling time of Ci (denoted by ˆξ i ). This occurs when Ci suffers the maximum possible interference due to higher priority applications. For this, we will consider that all higher priority applications C j interfering with Ci require their maximum possible transmission time on the shared slot, i.e., td w , j = ξT T , j . This assumption is pessimistic since td w , j actually decreases with the blocking time suffered by C j . However, this allows us to simplify the analysis and leads to a safe schedulability condition. Now, the worst-case interference on Ci clearly occurs when it needs to have access to the TT slot together with all higher priority C j (sharing the same slot). This again happens when all higher priority C j and Ci undergo disturbances at the same time. In addition, since the scheduling is non-preemptive, Ci may also suffer some blocking time due to lower-priority applications. Computing ˆtw , i and ˆξ i here has some similarities with computing the worst-case response time in a fixed-priority non-preemptive scheduling like the one of CAN. That is, we need to compute the response times of all instances of disturbance rejection within its maximum busy period tma x , i [16]. For ease of exposition, we assume that tma x , i ≤ r i holds for all Ci , i.e., there is only one transmission consecutive messages of Ci within its busy period tma x , i . of This way, we only need to compute the response time ξ i of the sole instance of Ci within tma x , i to obtain its worst-case response time ˆξ i , which can be done solving the following recurrence relation: td w , i h i ξ i (k + 1) = ξT T , i + (1 − βi )bi + (1 − βi ) j =1 ξT T , j , (12) Ns z =i +1 where bi = max (ξT T , z ) denotes the maximum possible blocking time due to lower-priority applications suffered by Ci and Ns is the number of applications mapped onto the slot s . Without loss of generality, we assume in Eq. (12) and in the remainder of this section that applications are sorted in order of decreasing priority (i.e., C j has higher priority than Ci and Ci has higher priority than Ck for 1 ≤ j < i < k ). Eq. (12) can be solved recursively until ξ i becomes greater than ξd , i or converges to a certain value. Clearly, if ξ i exceeds ξd , i , Ci is not schedulable on the shared slot. On the other hand, if there is a convergence value prior to ξd , i , then Ci can meet its deadline and is schedulable. Fixed-priority preemptive slot access: In the aforementioned non-preemptive access scheme, a higher priority application with (cid:8) i −1 (cid:9) ξ i (k ) r j closer settling time deadline may be blocked by a lower priority application with a high dwell time. This may result in deadline miss. However, a higher priority application may meet its deadline if it is allowed to preempt the lower priority application. In this context, we discuss here a scheduling policy with limited preemption. In this scheme, a higher priority application C j is only allowed to preempt a lower-priority application Ci after a configurable blocking time b (cid:8) j . In this scheduling strategy, we consider that the dwell time of an application td w , i is constant and given. Now, based on the assumption that tma x , i ≤ r i , maximum allowable blocking time can be calculated as ˆbi = ξd , i − td w , i − td w , j . (13) (cid:8) i −1 j =1 (cid:9) ξd , i r j Correspondingly, b (cid:8) i must be configured such that b (cid:8) i ≤ ˆbi . Now, this reduction in blocking time for higher priority applications comes at the cost of retransmission of lower priority applications. In case C j preempts Ci , Ci may have to retransmit for b (cid:8) j time. Note that if b (cid:8) j ≥ td w , i holds, Ci has enough time to finish sending its message sequence before b (cid:8) j expires and it will not incur in retransmission. Further, to obtain the maximum retransmission cost tr , i for a Ci , we need to consider the fact that the transmission of Ci can be canceled by any higher-priority C j : b (cid:8) j . tr , i = max (14) 1 ≤ j < i , b (cid:8) j < td w , i Now, for a lower-priority Ci , the retransmission cost of its higherpriority C j needs to be considered as blocking time for Ci . In worst case, all higher priority tasks of Ci may need retransmission. As a result, we can configure any positive blocking time for Ci that is at most equal to i −1 b (cid:8) i = ˆbi − tr , i − tr , j . j =1 (15) Clearly, if no positive b (cid:8) i can be found, Ci cannot be scheduled. Furthermore, the schedulability on one slot can be guaranteed if a positive b (cid:8) i can be found for every Ci allocated to the slot. To this end, we need to compute b (cid:8) i in order of decreasing priorities from the highest to the lowest priority. 3.3.2 Slot Allocation. The problem of finding the minimum number of slots that guarantees the settling time requirements of all Ci is clearly an allocation problem. Often such problems are NP-hard in the strong sense. Here, we will discuss the well-known First Fit (FF) heuristic approach. FF leads to a number of slots that is acceptably close to the optimum and has polynomial complexity. This approach first sorts the applications {Ci } according to increasing priority. Then, it iterates over the sorted set and tries to allocate them in the minimum possible number of slots. The algorithm starts with only one slot and allocates the control applications to it as long as they are schedulable on that slot. A Ci is schedulable on a slot if it can meet its timing requirement ξd , i when assigned to that slot. To test this, the algorithm makes use of the schedulability analysis presented in Sec. 3.3.1. In the current iteration, the algorithm first tries to allocate Ci to a slot in the list of existing slots. If Ci could not be scheduled on any of the exiting slots, it then adds a slot to the list. The algorithm concludes when NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea D. Roy et al. all Ci have been allocated and returns the number of slots that were necessary for accommodating all of them. Actuator Physical Process Sensor 3.4 Experimental Results C i C1 C2 C3 C4 C5 C6 Case 1 r i ξd , i ξT T , i ξ ET , i r i Case 2 ξd , i td w , i 2000 150 50 200 2000 300 2000 500 200 550 2000 400 1500 150 50 200 1500 450 2000 300 200 400 2000 1000 5000 1000 800 2000 5000 3000 600 600 300 700 500 500 Table 1: Case study (All times are in ms ). 100 120 150 300 800 50 Given the 6 applications with details in Table 1–Case 1, we apply the non-preemptive slot access scheme. We calculated that the minimum number of slots required is 4 with the partition: {C1 , C3 } , {C2 , C4 } , {C5 } and {C6 } . Then, we apply the preemptive slot access scheme for data in Table 1–Case 2. Here, we obtained 2 slots with the partition: {C1 , C2 , C3 , C4 , C5 } and {C6 } . Both the schemes result in the allocation of less number of TT slots than there are applications. However, note that the two results are not comparable as they are based on different data sets. 4 EFFICIENT CONTROL OVER ELASTIC COMMUNICATION RESOURCES In the last section, we have discussed a bi-modal control strategy where the closed-loop dynamics are exactly known for both the modes individually. In this section, we will consider a case where closed-loop dynamics are not always known. In particular, we consider that a control loop can be closed elastically over deterministic and non-deterministic communication. Here, we assume that the non-deterministic communication cannot provide any timing guarantees not even the worst-case response time. For such a setting, we study a bi-modal control strategy proposed in [17] which guarantees the worst-case performance requirement using deterministic communication. At the same time, it tries to improve the control performance by exploiting the available non-deterministic communication resource. In this section, we will first introduce the elastic network setting and the bi-modal controller for such a setting. Next, we will formally state the control goals and explain the mode selection algorithm that satisfies the goals. We will also provide some experimental results. 4.1 Network Setting It is often the case that faster sampling improves the control performance. Based on this assumption, control over a hybrid network setting is considered as shown in Figure 5. The network consists of deterministic and non-deterministic communication channel. This can be represented by the combination of TT and BE traffic in a TTEthernet network as described in Sec. 2.2. We consider that the state of the controlled plant is sensed at a faster rate f F and the corresponding instants are denoted by } . The sensed data is then transmitted over the hybrid network. In this network, the deterministic channel is expensive. Thus, we use this channel at a very low rate, f S , corresponding to which the sampling time instances are given by {t S } where {t S } . } ⊂ {t F {t F k k k k Actuator Task Controller Task Switching Scheme Fast Controller Safe Controller Sensor Task Deterministic Communication Non-Deterministic Communication . . . . . . Deterministic Samples Non-Deterministic Samples Figure 5: Control strategy over elastic resource. k k } \ {t S Intuitively, f S depends on the worst-case performance requirement of the control loop. Thus, communication over this channel enable the control scheme to guarantee the worst-case performance. On the other hand, the non-deterministic channel is cheap. Therefore, this channel is used for sampling instances when deterministic channel is not allocated, i.e., {t F } . It may be noted that this channel may have unbounded transmission delay or packet drops. However, in most cases the delay is not too high and packet drops do not occur, which is the case with protocols like CSMA/CD, etc. We further assume that if a packet is not transmitted within a certain time then it is dropped. This can be realized, for example, using a CSMA with persistence within a given timeout or time-stamping. Now, considering this network setting, we may say that the control loop is only closed at instants {t M } where {t S } . Here, control over non-deterministic channel can be exploited to improve the control performance by appropriately coming up with a switching control strategy M . } ⊂ {t M } ⊂ {t F k k k k 4.2 Bi-modal controller Due to nondeterminism in the control loop it is difficult to derive the closed-loop mathematical model of the overall system. Consequently, for the described network setting, we intuitively use a control strategy based on two control modes. We name the two controllers appropriately as slow controller S and fast controller F for which the control gains are denoted as K S and K F respectively. K S can be determined using the optimal control design algorithm explained in Sec. 2.1 according to the sampling period hS = 1 Since hS is large, we expect that this controller makes the system converge to the equilibrium state rather slowly. Moreover, it has been observed from simulations using this controller at higher rate will not improve the rate of convergence appreciably. This is due to the inherent slow action of the controller. f S . Hybrid Automotive In-Vehicle Networks NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea On the other hand, we design the fast controller according to the sampling period h F = 1 f F . This controller is expected to provide a much higher rate of system convergence than the slow controller at higher sampling rate. However, in case of packet drops, performance may degrade from the expected value to a point when requirement is violated. Therefore, in order to achieve the overall system performance close to the performance of the fast controller in case of less constrained non-deterministic channel while limiting the maximum performance degradation within a threshold of the performance of slow controller, we need to devise an efficient control strategy M . 4.3 Control Goals We consider the problem where disturbance arrive sparsely. By sparse, we mean that a new disturbance only arrives after the previous one was successfully rejected. A disturbance means that the augmented system state z is moved from an equilibrium region Z E around the reference point to a disturbed state z0 . This can either be due to an external influence or because the set-point was changed. Now, the goal is to drive the system back to steady state within a time ξd . Thus, the settling time ξ must satisfy ξ ≤ ξd . Here, we need to develop a control strategy M such that for any valid realization {t M } and initial disturbed state z0 , the following inequality holds true: k ξ M (z0 ) ≤ ξ S (z0 ) · Δ (16) where (i) ξ M is the settling time of the mixed control strategy M , (ii) ξ S is the settling time when only slow controller S is used at instants {t S } , and (iii) Δ > 1 represents a design parameter which limits the performance degradation of M in the worst-case. In addition, the objective for the design of M is to minimize the settling time ξ M (z0 ) for the case {t M } (i.e. the channel is available at all non-deterministic time instances as well). } = {t F k k k 4.4 Mode selection algorithm Given the network setting, the two control modes and the control goals, the mixed control strategy M and the corresponding mode selection can be realized follows. The mode selection algorithm is invoked at every time instant. The algorithm first checks if the system states are in the equilibrium region Z E . If the states are inside Z E , the slow control S is picked and the deadline is set to a negative value to indicate that the system is not in a transient state. Note that S only applies control input at time instances {t S } and is also designed accordingly. Thus, the closed-loop system is stable under steady state. Now, in case a new disturbance is detected, then ξ S (z0 ) is calculated. A deadline is set corresponding to the control goal given by Eq. (16). Now, when a transient state is detected, the algorithm tries to determine whether applying fast control F is possible without running the risk to arrive in a state from which Eq. (16) can no longer be satisfied. Here, the algorithm first computes the control input u F corresponding to K F . Then, it predicts the system states zn at the next deterministic sample t S n provided u F is applied at this instant. Correspondingly, it calculates the settling time ξ S (zn ) if S takes over at zn . Now, if the calculated time ξ S (zn ) still satisfies the deadline then u F is applied else S is selected. k 2 . 0 1 . 6 1 . 2 0 . 8 0 . 4 0 . 0  w i t h  - = 1 . 1 - = 1 . 2 - = 1 . 3 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 Figure 6: Settling times for different control strategies k The success of this scheme lies in the fact that at every time instance {t M } the algorithm evaluates whether the fall back scheme S can still satisfy the deadline. Thus, the algorithm satisfies the worst-case performance. Moreover, it is clear that when there are no packet drops, F is used mostly which results in fast control and lower settling time. Settling time computation: The computationally heavy part of this algorithm is the calculation of ξ S (z ). We are not aware of a closed-form solution to compute the settling time for a discretetime system. Instead, one has to simulate the system evolution for a number of steps to determine the settling time. Depending on the dimensionality of the problem and the complexity of the control algorithm, this computation might be infeasible during runtime due to limited computational resources on the embedded platforms. An alternative approach would be to rasterize the state-space, precompute the worst-case settling times for each slice off-line and store them in a lookup table. This approach is computationally cheap but might have a high memory requirement. For simple state-feedback controllers, where the control law only involves a low-dimensional matrix multiplication, we expect however that it is more efficient to compute the settling time during runtime. 4.5 Experimental Results To validate our expectations, we have considered a DC Motor speed control system. As shown in Figure 6, we have implemented different speed control strategies considering the elastic network setting. These strategies are (i) slow controller only at deterministic samples (S ), (ii) slow controller at both deterministic and non-deterministic samples (S with t M k ), (iii) fast controller at both deterministic and non-deterministic samples (F ), and (iv) switching control strategy with fixed values of Δ (M − Δ = X ). We have further simulated random packet drops over non-deterministic channel according to a successful transmission rate p ranging from 0 to 1. We have performed several simulations at each value of p for each strategy. The average normalized settling times (y-axis) for different control strategies at different values of p (x-axis) are illustrated in Figure 6. Important observations (from Figure 6) are stated as follows: (i) S applied at all successful samples {t M } does not improve the performance appreciably as compared to S at {t S } only even at p = 1. (ii) F deteriorates the performance considerably at lower values of p . (iii) At higher values of p , performance of M matches that of F , and therefore, the objective to improve the performance with nondeterministic channel is achieved. (iv) At no point the performance k k NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea D. Roy et al. requirement is violated (we have observed this even for the worstcase). Thus, we may conclude that the studied bi-modal control strategy satisfies the worst-case performance and also improves the performance by exploiting the cheap communication resource. 5 FUTURE OUTLOOK Although hybrid protocols have gained importance and are found in communication architectures of modern cars, applications are still not designed in practice exploiting these protocols. Moreover, there have been only a few preliminary research works in this direction. However, we believe that this is an important research direction towards resource-efficient design of automotive systems. In this context, we propose two possible future extensions to the control strategies that we have studied in this paper. Switching stability: In Sec. 3, we have stated the problem of ensuring stability during mode change. As a solution, we have considered a minimum dwell time to ensure stability when changing the mode from TT to ET. Here, the dwell time may be pessimistic and results in blocking the TT slot unnecessarily. Moreover, we assume that the switching from ET to TT will not jeopardize stability. In this context, we can apply the well-developed notion of switching stability from control theory. The theorem says that any arbitrary switching between two stable closed-loop systems is also stable if there exists a common quadratic Lyapunov function for the systems [18]. Now, we can consider this condition as a constraint for designing the gains for MT T and M ET . This will allow to switch back from MT T to M ET before the system returns to steady state. This can be exploited to devise a more efficient mode switch protocol and can reduce the number of TT slots required. Wireless communication: With increasingly more number of applications mapped onto the E/E architecture of a modern car, the wiring complexity and volume of the underlying communication system has also grown. To address this problem, automotive community foresee the introduction of wireless networks for intravehicular communication [2]. Reliability is a key issue associated with many standard wireless communication protocols like IEEE 802.11x. However, we may note that wireless channel can naturally replace best-effort communication over TTEthernet in the control strategy discussed in Sec. 4. Thus, we propose to apply the bi-modal control strategy to a combination of wired and wireless communication. This reduces the in-vehicle wiring by minimizing the requirement on wired communication. This can also reduce the cost of using expensive communication infrastructure supporting hybrid protocols. Furthermore, wireless protocol IEEE 802.15.4 is becoming popular for its low-cost and low-power solution. It is also possible to tune the parameters of this protocol to meet real-time requirements [19]. In the future, one can study this protocol and customize the general control strategies discussed in this paper accordingly. 6 CONCLUDING REMARKS Hybrid protocols have gained a lot of importance in recent years towards supporting mixed-criticality applications. In this paper, we try to bring into attention the importance of hybrid protocols in the context of safety-critical control applications. In particular, we have pointed out that control applications can be designed in a resourceefficient way exploiting the hybrid characteristic of the underlying communication architecture. We have studied here two control strategies: (i) The first strategy uses ET communication for steadystate control while exploits TT communication only to stabilize the system faster in the event of disturbance. (ii) The second strategy uses deterministic communication only to guarantee the worst-case performance while exploits best-effort communication to improve the performance. Both the strategies reduce the use of expensive TT resources significantly while not compromising the performance. Furthermore, we believe that control over hybrid communication is not fully explored, and therefore, this is a promising research direction for the future. "
Improving the Reliability and Energy-Efficiency of High-Bandwidth Photonic NoC Architectures with Multilevel Signaling.,"Photonic network-on-chip (PNoC) architectures employ photonic waveguides with dense-wavelength-division-multiplexing (DWDM) for signal traversal and microring resonators (MRs) for on-off-keying (OOK) based signal modulation, to enable high bandwidth on-chip transfers. Unfortunately, the use of larger number of DWDM wavelengths to achieve higher bandwidth requires sophisticated and costly laser sources along with extra photonic hardware, which adds extra noise and increases the power and area consumption of PNoCs. This paper presents a novel method (called 4-PAM-P) of generating four-amplitude-level optical signals in PNoCs, which doubles the aggregate bandwidth without increasing utilized wavelengths, photonic hardware, and incurred noise, thereby reducing the bit-error-rate (BER), area, and energy consumption of PNoCs. Our experimental analysis shows that our 4-PAM-P signaling method achieves equal bandwidth with 4.2x better BER, 19.5% lower power, 16.3% lower energy-per-bit, and 5.6% less photonic area compared to the best known 4-amplitude-level optical signaling method from prior work.","Improving the Reliability and Energy-Efficiency of HighBandwidth Photonic NoC Architectures with Multilevel Signaling  Ishan G Thakkar, Sai Vineel Reddy Chittamuru, Sudeep Pasricha  Department of Electrical and Computer Engineering  Colorado State University, Fort Collins, CO, U.S.A.  {ishan.thakkar, sai.chittamuru, sudeep}@colostate.edu  ABSTRACT  Photonic network-on-chip (PNoC) architectures employ photonic  waveguides  with  dense-wavelength-division-multiplexing  (DWDM) for signal traversal and microring resonators (MRs) for  on-off-keying (OOK) based signal modulation, to enable high  bandwidth on-chip transfers. Unfortunately, the use of larger  number of DWDM wavelengths to achieve higher bandwidth  requires sophisticated and costly laser sources along with extra  photonic hardware, which adds extra noise and increases the power  and area consumption of PNoCs. This paper presents a novel  method (called 4-PAM-P) of generating four-amplitude-level  optical signals in PNoCs, which doubles the aggregate bandwidth  without increasing utilized wavelengths, photonic hardware, and  incurred noise, thereby reducing the bit-error-rate (BER), area, and  energy consumption of PNoCs. Our experimental analysis shows  that our 4-PAM-P signaling method achieves equal bandwidth with  4.2× better BER, 19.5% lower power, 16.3% lower energy-per-bit,  and 5.6% less photonic area compared to the best known 4amplitude-level optical signaling method from prior work.  Categories and Subject Descriptors: [Networks] Network on  chip; [Hardware] Integrated Circuits/Interconnect: Photonic and  optical interconnect  Keywords: Photonic network on chip; multilevel optical  signaling; optimization; energy efficiency; reliability  1 INTRODUCTION  In the many-core era, processors with hundreds of cores on a  single chip are gradually becoming a reality. The performance of  these many-core processors is driven by the effectiveness of the  underlying electrical network-on-chip (ENoC) fabrics that are  becoming increasingly crosstalk- and energy-limited [1]. To this  end, due to the recent developments in the area of silicon photonics,  photonic network-on-chip (PNoC) fabrics have been projected to  supersede ENoCs. PNoCs offer several benefits over ENoCs such  as higher bandwidth density, distance-independent bit-rate, and  smaller data-dependent energy.  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.  NOCS '17, October 19–20, 2017, Seoul, Republic of Korea   © 2017 Association for Computing Machinery.  ACM ISBN 978-1-4503-4984-0/17/10…$15.00   https://doi.org/10.1145/3130218.3130226  Typical PNoC architectures (e.g., [1]-[4], [8]) and off-chip  photonic interconnects (e.g., [25], [26]) utilize several photonic  devices such as multi-wavelength lasers, microring resonators  (MRs), waveguides (WGs), and splitters. A broadband laser source  generates light of multiple wavelengths (λs), each wavelength (λ)  of which serves as a data signal carrier. Simultaneous traversal of  multiple λ-signals across a single photonic WG is possible using  dense wavelength division multiplexing (DWDM), which enables  parallel data transfer across the photonic WG. For instance, a  DWDM of 64 λs can transfer 64 data bits in parallel. At the source  node, multiple MRs typically modulate multiple electrical data  signals on the utilized DWDM λs (data-modulation phase). In  almost all PNoCs in literature, modulator MRs utilize on-off keying  (OOK) modulation, where-in the presence and absence of λs in the  WG are used to represent logic ‘1s’ and ‘0s’. Similarly, at the  destination node, multiple MRs with photodetectors at their drop  ports are used to filter and detect light-modulated data signals from  the WG (data-detection phase) and generate proportional electrical  signals. In general, the use of a large number of DWDM λs enables  high bandwidth parallel data transfers in PNoCs.   Unfortunately, a number of challenges related to area [8], cost  [32], reliability [11], and energy-efficiency [13] still need to be  overcome for efficient implementation of PNoCs that utilize a large  number of DWDM λs (typically 64 or more DWDM λs per WG).  Generating a large number of DWDM λs requires a combgenerating laser source, the ineffectiveness, complexity, and cost of  which increase with the number of λs generated [5]. Moreover,  utilizing a larger number of DWDM λs to achieve higher-bandwidth  data-transfers in a PNoC results in larger network flit size and more  electrical and photonic hardware (more number of modulator and  detector MRs and their drivers). A larger network flit size also  results in larger sized buffers in the network gateway interfaces,  which results in significantly higher area and power overheads.  Similarly, larger number of MRs and drivers also incur greater  photonic area and MR heating power overheads. Furthermore, the  use of a larger number of DWDM λs decreases the gap between two  successive λ-channels, which in turn increases the heterodyne  crosstalk noise in PNoCs, harming the reliability of communication  [6], [9]. Thus, the use of larger number of DWDM λs to achieve  higher bandwidth in PNoCs is not a reliable and energy-efficient  option. This motivates the need for a more reliable and energyefficient way of achieving higher bandwidth data transfers in PNoC  architectures.  In [4] and [7], Kao et al. proposed a multilevel optical signaling  format 4-PAM (4-pulse amplitude modulation) to achieve higher  bandwidth and energy-efficient data communication in PNoCs. 4PAM optical signaling format doubles  the bandwidth by  compressing two bits in one symbol carried out by four levels of  amplitude. Kao et al. utilize superposition of two OOK-modulated  optical signals of the same λ with 2:1 power ratio to create a 4-PAM  λ-signal. We found that this signal superposition based 4-PAM  optical signaling method (referred to as 4-PAM-SS henceforth)          incurs significantly high power, photonic area, and reliability  overheads (Section II). The shortcomings of 4-PAM-SS motivate the  need for a more reliable, energy- and area-efficient method of  implementing 4-PAM signaling.  In this paper, we present a novel method (referred to as 4-PAMP henceforth) of generating 4-PAM optical signals, which employs  only one modulator MR per λ to directly modulate the designated  λ-signal in 4-PAM format. We present a search heuristic based link  optimization framework that finds the optimal value of number of  DWDM wavelengths (Nλ) from a constrained space of all its  allowable values to achieve desired performance and/or reliability  goals for the target photonic link. We use our framework to  optimize the designs of three types of photonic links, each of which  uses OOK, 4-PAM-SS, or 4-PAM-P optical signaling method. Our  experimental analysis shows that PNoCs that are comprised of 4PAM-P based optimized links render greater reliability, energyefficiency, and area-efficiency with equal bandwidth compared to  the PNoCs that are comprised of 4-PAM-SS or OOK based  optimized links. We summarize the key contributions in this paper  as follows:  • We propose a novel technique (4-PAM-P) of generating 4-PAM  optical signals in PNoCs, which proves to be more reliable, area-  and energy-efficient than the previously proposed signal  superposition based 4-PAM-SS method [7] and the conventional  OOK method;  • We present a search heuristic based optimization framework that  optimizes the designs of OOK, 4-PAM-SS, and 4-PAM-P  signaling based photonic links to achieve desired performance  and/or reliability goals;  • We evaluate the impact of the optimized designs of OOK, 4PAM-SS and 4-PAM-P based photonic  links on  the  performance, reliability and energy-efficiency of a well-known  PNoC architecture: an 8-ary 3-stage CLOS PNoC [4].  2 BACKGROUND AND MOTIVATION  In this section, we first present an overview of the signal  superposition based 4-PAM optical signaling method (referred to as  4-PAM-SS) from [7]. In 4-PAM-SS, at first, two separate OOKmodulated signals of each of the utilized DWDM λs are generated  in two separate parallel WGs. Then, these two sets of OOKmodulated DWDM signals are superposed using a combiner to  generate one set of 4-PAM modulated DWDM signals. Each  amplitude level of a 4-PAM λ-signal represents one of the four  combinations of two bits (00, 01, 10, or 11). When the entire  DWDM spectrum of 4-PAM signals reach the destination node,  each signal is filtered by its corresponding in-resonance MR and is  converted back into two electrical signals by a photodetector and a  back-end receiver circuit. As discussed in [7], the back-end receiver  circuit consists of three sense-amplifiers and two logic gates that  decode the 4-PAM modulated signal.  Ideally, in the 4-PAM-SS method, when the combiner superposes two OOK-modulated signals, a 4-PAM modulated signal is  generated owing to the constructive interference between the two  OOK signals. However, the constructive interference happens only  if both the OOK signals have identical phases. Unfortunately, in the  presence of non-idealities such as process and on-chip temperature  variations, a significant phase difference exists between the two superposed OOK signals, which leads to destructive interference between them. Owing to the random nature of process and on -chip  temperature variations, this incurred phase difference may fall anywhere in the range from 0 to 2π. This implies that the degree of  destructive interference incurred between the OOK signals due to  the phase difference (and hence the strengths of the symbols of the  resultant 4-PAM signal) may fall anywhere in a very large ranges  of values. This in turn makes it very hard to ensure reliability of  communication with a 4-PAM-SS photonic link.   The worst-case destructive interference in 4-PAM-SS occurs  when the two superposed OOK signals are completely out of phase,  i.e., when the phase difference between them is an odd multiple of  π. The amount of signal loss due to the superposition of two out of  phase OOK signals depends on their individual signal strengths.  Typically, as explained  in [7],  in 4-PAM-SS method,  to  equidistantly space the four amplitude levels of the output 4-PAM  signal in the available range of optical transmission, the strengths  of the individual OOK signals are kept to be two-third and one-third  of the strength of the conventional OOK signal. Hence, for the bestcase constructive interference between the superposed OOK  signals, the strength of the resultant 4-PAM signal becomes  2/3+1/3=1. In contrast, for the worst-case destructive interference,  the strength of the resultant 4-PAM signal becomes 2/3-1/3=1/3,  which makes the worst-case interference-related signal loss to be      -10×log(1/3) = 4.8dB.  In summary, the interference-related signal loss in 4-PAM-SS  [7] reduces signal-to-noise ratio (SNR), BER, and overall communication reliability. Furthermore, as explained in [7], the 4-PAMSS method requires additional photonic hardware, such as one  asymmetric splitter, two modulator MRs per λ (for two OOK signals), and a combiner. This additional photonic hardware reduces  the area benefits of the 4-PAM-SS method compared to the traditional OOK method. In this paper, we present a novel, more reliable,  and energy- and area-efficient 4-PAM signaling method (4-PAMP), which overcomes the shortcomings of the 4-PAM-SS method.  The next section describes our proposed 4-PAM-P method in detail.  3 PROPOSED 4-PAM-P OPTICAL SIGNALING  3.1 Overview  Unlike 4-PAM-SS, our proposed 4-PAM-P method employs  only one modulator MR per λ to directly modulate the designated  λ-signal in 4-PAM format. This type of 4-PAM signal generating  modulator MRs are demonstrated in [19], [21], and [22]. Our  proposed 4-PAM-P method extends the use of such modulator MRs  in DWDM based PNoCs. Before we discuss about how our 4-PAMP method works, it is important to understand how a modulator MR  works in the conventional OOK method.  Fig. 1. Illustration of optical transmission and microring resonator  (MR) spectra for (a) OOK signaling, (b) proposed 4-PAM-P signaling.  Ideally, a modulator MR is designed to operate in resonance with  a signal-λ in its default state. But due to process and on-chip  temperature variations, the MR’s resonance λ often deviates from  the signal-λ (black curve in Fig. 1(a)). In this case, as shown in Fig.  1(a), the MR’s resonance λ (center/peak of the MR’s passband)  needs to be brought in alignment with the signal-λ by either        temperature or electrical tuning of the MR [29]. In this tuned state  (purple curve), the MR remains in resonance with the signal-λ,  which enables the MR to modulate logic 0 on the signal-λ, by  removing the signal-λ from the WG. Thus, in this tuned state of the  MR, a non-zero tuning-bias voltage (VT) but zero signal-bias  voltage (V0 = 0) is applied to the MR. Hence, as shown in Fig. 1(a),  the total bias voltage applied to the MR in the tuned state (purple  curve) is VB = VT + V0 → VB = VT. On the other hand, the MR is  operated in the off-resonance state to modulate logic 1 on the signalλ. For that, a specific non-zero signal-bias voltage V1 is applied to  the MR, which shifts the passband of the MR to be in off-resonance  state (red curve). Thus, the net bias voltage applied in the offresonance state of the MR is VB=VT+V1.  In summary, in conventional OOK modulation, to modulate a  particular sequence of 1s and 0s on the signal-λ, the modulator MR  is switched off and on resonance with the signal-λ by applying the  net bias voltages of VT+V1 and VT+V0 to the MR, respectively. Note  that the value and polarity of VT depends on the amount and  direction of the variation-induced resonance shift, which can be  efficiently assessed by using the dithering signal based method  demonstrated in [18]. Moreover, in the OOK method (Fig. 1(a)),  two levels of optical transmissions (L0 and L1) are achieved that  correspond to net bias voltages of VT+V0 and VT+V1. The difference  between L0 and L1 is defined as modulation depth.  In our 4-PAM-P method, as shown in Fig. 1(b), we introduce two  more intermediate levels of optical transmissions L01 and L10  between L0 and L1 within the modulation depth. For that, we  introduce two more levels of signal-bias voltages V01 and V10  between V0=0 and V1. Thus,  in 4-PAM-P,  signal-bias  voltages/optical transmission levels V0/L0 (or V00/L00), V1/L1 (or  V11/L11), V01/L01, and V10/L10 correspond to bit combinations “00”,  “11”, “01”, and “10” respectively. Note that as demonstrated in  [19], the resonance passbands of the carrier-depletion based  optimized MRs can be shifted with a signal voltage efficiency of  2GHz/V. This allows MRs with bandwidths even as low as ~10GHz  (corresponding to the quality factor of 18000) to have very fine and  efficient control of optical transmission levels in the resultant 4PAM signals [19].  In contrast to the 4-PAM-SS method, 4-PAM-P does not use  signal superposition to create 4-PAM signals, and therefore does  not incur interference-related signal loss. As a result, for given noise  power, 4-PAM-P renders greater SNR with better BER. Moreover,  4-PAM-P requires one less modulator MR per λ, and it does not  require additional splitters and combiners. As a result, 4-PAM-P  consumes less photonic area and dissipates less static power related  to tuning of MRs. Due to all these benefits, 4-PAM-P is more  reliable, energy- and area-efficient than the 4-PAM-SS method.   However, compared to the OOK and 4-PAM-SS methods, the  use of 4-PAM-P requires some minor modifications in how  electrical-to-optical  (E/O)  conversion  of  data  at  the  sender/modulator side is implemented. At the receiver side, opticalto-electrical (O/E) conversion of data in 4-PAM-P is carried out in  the same manner as in the 4-PAM-SS method, as demonstrated in  [7]. Note that from [20] and [7], a 4-PAM signal with the same  baud-rate as of an OOK signal requires 4.8dB more received power  to achieve the same BER as achievable by the OOK signal.  Therefore, for our link- and architecture-level analysis in Section  IV and V, we consider the detector sensitivity threshold for both the  4-PAM-SS and 4-PAM-P methods to be 4.8dB more than the  conventional OOK method, as manifested by the SNRTarget value in  Eq. (3). The next subsection describes how E/O conversion is  implemented in our 4-PAM-P method.  3.2 E/O Conversion in 4-PAM-P Method   In a typical PNoC, at the electrical-optical interface of a sender  node, the input electrical flits are temporarily stored in first-in-firstout (FIFO) buffers before the modulator MRs convert them into the  optical domain. Typically, for PNoCs with equal-sized electrical  and optical flits, the size of each entry of the FIFO buffer is equal  to the size of the optical flit, which is equal to the number of bits  transferred in parallel on DWDM λs [3]. N parallel electrical bitstreams can be produced when multiple N-bit entries of the FIFO  buffer are evicted in sequence, triggered by consecutive clock edges  (or levels). The modulator MRs at the E/O interface convert these  parallel electrical bit-streams into parallel optical bit-streams.  Fig. 2(a) illustrates E/O conversion for the conventional OOK  method, using an example photonic link with 4-bit flit-size. Each of  the four parallel electrical bit-streams available is mapped to a  designated modulator MR that is designed to operate on a particular  λ. Accordingly, each of these bit-streams is applied to a driver  circuit, which produces a corresponding sequence/stream of signalbias voltages V0 and V1. Then, the designated modulator MR is  driven by this sequence of signal-bias voltages, after the voltage  sequence is offset with a constant tuning-bias voltage VT  corresponding to and adjusting for the variation-induced resonance  shift in the modulator MR. As explained in the last subsection,  because of this applied sequence of bias voltages, each modulator  MR (corresponding to an electrical bit-stream) modulates the input  electrical bit-stream onto the corresponding DWDM λ, generating  an OOK-modulated optical signal.  Fig. 2. Schematics of (a) OOK-based, (b) 4-PAM-P based photonic links.  On the other hand, as shown in Fig. 2(b), in the proposed 4-PAMP method as well, the FIFO buffer generates four parallel electrical  bit-streams. Two adjacent electrical bit-streams of these four bitstreams are applied as inputs to a digital-to-analog converter (DAC)  circuit. The DAC coverts each input two-bit combination to a  signal-bias voltage level out of four possible voltage levels V00, V01,  V10, and V11 (see Section III.A). Thus, four parallel electrical bitstreams are converted in two parallel sequences of signal-bias  voltages by two concurrently operated DAC units. These two  parallel sequences of signal-bias voltages, after being offset by        corresponding tuning-bias voltages, are applied to two designated  modulator MRs that are designed to operate on two different λs.  These modulator MRs modulate the applied sequences of four-level  voltages onto their corresponding λs to generate two parallel fouramplitude-level (4-PAM) optical signals.  Now, as given in the inset of Fig. 2(b), two different designs of  a high-speed DAC circuit are possible: electrical DAC [21] and  optical DAC [22]. In [21], a 40Gbps DAC designed in 65nm CMOS  is demonstrated, which utilizes a segmented pulsed-cascode output  stage to achieve 4-PAM modulation on a single MR. This electrical  DAC (demonstrated in [21]) consumes 3.04pJ/bit power to convert  two input electrical bit-streams into a four-level electrical signal (4PAM electrical signal) at 40Gbps. This 4-PAM electrical signal,  after being properly conditioned by signal-bias voltages, is applied  to an MR that generates a proportional optical 4-PAM signal. Thus,  the conversion of two input electrical bit-streams into a single 4PAM optical signal happens in two stages. In the first stage, the  input bit-streams are converted into an electrical 4-PAM signal by  the DAC circuit. Then in the second stage, this 4-PAM electrical  signal is converted into optical domain by the modulator MR. The  caveat of this electrical DAC based conversion method is that it  incurs significant area and energy overhead and imposes nonlinearity onto the E/O transfer functions of the driven MRs [22]. To  overcome these shortcomings, Moazeni et al. in [22] utilized an  optical DAC, which is basically a “spoked” MR of 5μm radius with  ~10000 Q that directly converts two input electrical bit-streams into  a 4-PAM optical signal for only 0.197pJ/bit power consumption at  20Gbps bit-rate. Thus, the use of these “spoked” MRs collapses the  two-stage E/O conversion process into a single stage process, and  thus, eliminates the need for external electrical DAC circuit. In this  case, the DAC circuits shown in Fig. 2(b) are eliminated and the  input electrical bit-streams are directly applied to the corresponding  “spoked” MRs. We utilize these optical DACs (“spoked” MRs) in  our 4-PAM-P method and account for 0.197pJ/bit power  consumption (as part of Tx/Rx power) per DAC in our architecturelevel evaluation presented in Section V.  4 PHOTONIC-LINK DESIGN METHODOLOGY  𝐝𝐁 𝐝𝐁 A naive design of photonic links can result in suboptimal values  of bandwidth, power, and reliability for the associated PNoC [10].  Therefore, irrespective of the utilized optical signaling method, it  becomes imperative to optimize the designs of the constituent pho power-budget (𝐏𝐁𝐮𝐝𝐠𝐞𝐭 tonic links to achieve maximum bandwidth, energy-efficiency, and  reliability at the PNoC architecture level. From [6] and [10], for  photonic-link design optimization, the number of DWDM λs per  parameters Nλ and 𝐏𝐁𝐮𝐝𝐠𝐞𝐭 waveguide (Nλ) is the most important design parameter, and link  ) is the most critical design constraint. For  optimal use of the available power budget and link bandwidth [10],   should meet conditions given in Eq. (1),  where the expressions for the constituent terms of Eq. (1) are given  in Eq. (2)-(4).  Eq. (3) is derived from the equation for the required photodiode  power given in [12]. In Eq. (3), S represents the required detector  sensitivity threshold (i.e., minimum detectable power) in dBm to  achieve the target SNR (SNRTarget). Table 1 gives the definitions and  power (𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶 ) for MR filter Q-factor of 9000 and baud-rate (or bitvalues of various parameters used in Eq. (2)-(4). As evident from  Table 1, SNRTarget is different for OOK and 4-PAM signals, the  reason for which is explained in Section IV-B. We use the power  drop equation given in [24] to reckon the heterodyne crosstalk noise  rate) of 10Gbps, which defines the detector sensitivity threshold S  𝑑𝐵 𝑑𝐵 applicable to OOK and 4-PAM signals for reckoning 𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶 by  in Eq. (3). From [12], the OOK and 4-PAM modulated signals have  identical frequency spectra (represented by the sinc function).  Therefore, the power drop equation given in [24] can be equally  simply replacing the bit-rate in the equation with the signal baudbits, the bit-rate for a 4-PAM signal is twice its baud-rate (or  rate. Note that, as a single symbol in a 4-PAM signal represents two  (𝑃𝐿𝑜𝑠𝑠𝑀𝑀𝑅 and 𝑃𝐿𝑜𝑠𝑠𝐷𝑀𝑅 respectively), all of which depend on Nλ. As can  symbol-rate). We use Lorentzian function based equations given in  be implied from [27] and [24], for a given photonic link, 𝑃𝐿𝑜𝑠𝑠𝑀𝑀𝑅 ,  𝑃𝐿𝑜𝑠𝑠𝐷𝑀𝑅 , and 𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶 increase with increase in Nλ due to corresponding  [6], [28] to reckon through losses of modulator and detector MRs  decrease in channel gap. Hence, 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 , S, and therefore, 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 Nλ can be calculated from values of 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 and 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 the values of 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 and 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 depend on Nλ (Eq. (2)-(4)). Now, as evident from Eq. (2), a value of  parameters 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 and 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 . However,  the parameters 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 and 𝑃𝐵𝑢𝑑𝑔𝑒𝑡  also depend on Nλ (Eq. (2)-(4)).  Therefore, it can be concluded that Nλ has cyclic dependency on  parameters 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 and 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 , i.e., Nλ depends on and also controls  . Due to this cyclic dependency,  the optimal value of Nλ cannot be obtained directly from the  . Therefore, we employ a search  heuristic that finds the optimal value of Nλ that satisfies the  constraint for given sets of input parameters, from a set of its  allowable values.  𝑑𝐵 𝑑𝐵 𝑑𝐵 𝑑𝐵 ≥ 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 + 10 log10(𝑁𝜆 ),  = 𝑃𝑀𝑎𝑥 − 𝑆,  𝑃𝐵𝑢𝑑𝑔𝑒𝑡 𝑑𝐵 𝑃𝐵𝑢𝑑𝑔𝑒𝑡 𝑑𝐵 𝑆 = 0.5×𝑆𝑁𝑅𝑇𝑎𝑟𝑔𝑒𝑡 + 𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶 + 𝑃𝑁𝑜𝑖𝑠𝑒 𝑇ℎ𝑒𝑟𝑚𝑎𝑙 ,  𝑃𝐿𝑜𝑠𝑠𝑑𝐵 = 𝑃𝐿𝑜𝑠𝑠𝑀𝑀𝑅 + 𝑃𝐿𝑜𝑠𝑠𝐷𝑀𝑅 + 𝑃𝐿𝑜𝑠𝑠𝑊𝐺𝑃 + 𝑃𝐿𝑜𝑠𝑠𝑊𝐺𝐵 + 𝑃𝐿𝑜𝑠𝑠𝑆𝑝𝐶 + 𝑃𝐿𝑜𝑠𝑠 𝐼𝑁𝑇𝑅𝐹 ,  (1)  (2)  (3)  (4)  𝑃𝑁𝑜𝑖𝑠𝑒 𝑃𝐿𝑜𝑠𝑠𝑆𝑝𝐶  𝑃𝐿𝑜𝑠𝑠 Table 1: Definitions and values of various link design parameters  Parameter  𝑇ℎ𝑒𝑟𝑚𝑎𝑙 Thermal noise power for detector  Definition  Value  PMax Maximum allowable optical power per WG  20dBm [10]  𝑃𝐿𝑜𝑠𝑠𝑊𝐺𝐵 Waveguide bending loss (dB per 90°)  -22dBm [20]  OOK  21.7dB [12]  SNRTarget Target SNR value  𝑃𝐿𝑜𝑠𝑠𝑊𝐺𝑃 Waveguide propagation loss  4-PAM  31.3dB [12]  0.005 [6]  0.27dB/cm [6]  Splitter + coupling loss  1.2dB [6][7]  Laser efficiency  30% [7]  𝐼𝑁𝑇𝑅𝐹 Signal interference loss   Detector responsivity  1.1 A/W [7]  Detector bandwidth   5GHz  4.8dB  0dB  4-PAM-SS  4-PAM-P and OOK  -  -  -  4.1 Search Heuristic Based Optimization    Our proposed search heuristic performs exhaustive search to  find the optimal constraint-satisfying value of Nλ. To limit the cost  and complexity of the comb-generating laser source [5], and to be  consistent with the prior works on 4-PAM optical signaling [4] and  [7], we limit the maximum allowable value of Nλ to 128. Moreover,  as the flit-size of a PNoC is directly proportional to the value of Nλ,  and as the flit-size is usually a power-of-two value, the allowable  values of Nλ should also be power-of-two values. Because of these  reasons, we give a set Λ = {128, 64, 32, 16, 8, 4, 2, 1}, which is a  set of all allowable values of Nλ, as an input to our search heuristic.  Based on the constraint in Eq. (1), we create an error function  for each  − 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 − 10 log10 (𝑁𝜆 )}. Then,  𝑒𝑓(𝑁𝜆 ) = {𝑃𝐵𝑢𝑑𝑔𝑒𝑡 𝑑𝐵                     4.2 Design for Reliability and Bandwidth  element Nλ of the set Λ, we evaluate an error value ϵ = 𝑒𝑓(𝑁𝜆 ) using  all ϵ values. For that, we evaluate 𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶 , 𝑃𝐿𝑜𝑠𝑠𝑀𝑀𝑅 and 𝑃𝐿𝑜𝑠𝑠𝐷𝑀𝑅values  Eq. (1)-(4) and parameter values from Table 1, and create a set E of  using equations given in [24] and [6] (as mentioned earlier). All Nλ  values corresponding to the positive ϵ values in set E satisfy the  constraint given in Eq. (1). But we choose the Nλ corresponding to  the minimum positive value ϵmin from set E as the optimal value,  because such Nλ is the maximum constraint-satisfying value of the  number of DWDM λs.  such as signal-interference loss 𝑃𝐿𝑜𝑠𝑠 Note that this search heuristic is equally applicable to OOK, 4𝐼𝑁𝑇𝑅𝐹 and SNRTarget differ between  PAM-SS, and 4-PAM-P methods. However, the optimal values of  Nλ would differ for different methods, as the link design parameters  different methods (see Table 1).  We can use the search heuristic given in Section IV-A to find the  constraint-satisfying optimal value of Nλ that can achieve either  maximum bandwidth in terms of aggregate bit transfer rate or  desired reliability in terms of BER for designed photonic links. As  can be implied from [10] and [30], traditionally, the designs of  photonic links are optimized to achieve maximum bandwidth, and  while doing so the reliability of the designed photonic links is  usually disregarded. Therefore, in the link-bandwidth maximizing  optimization frameworks presented in [10] and [30], only the noiselimited detector sensitivity is utilized, and the parameter SNRTarget  is ignored. From Eq. (3), when the parameter SNRTarget is ignored to  achieve maximum bandwidth, the detector sensitivity or the  larger value of Nλ) for a given 𝑃𝐿𝑜𝑠𝑠𝑑𝐵 , but each of these wavelength  minimum detectable signal power (S) is set to be equal to the total  noise power. As a result, the available power budget can  accommodate greater number of λ-signals (corresponding to a  signals yields such a small value of signal power that can be hardly  distinguished from the noise power. This results in poor SNR, BER,  and communication reliability for the bandwidth -maximized  photonic links.  In contrast, we utilize the parameter SNRTarget in Eq. (3) and the  search heuristic to optimize the links for desired reliability.  Introducing the parameter SNRTarget in our optimization framework  sets the minimum detectable signal power (S) to a higher level,  because of which the available power budget can accommodate  only a small number of λ-signals. Nevertheless, doing so ensures  that all the supported λ-signals achieve the target SNR. As a result,  desired BER and communication reliability can be achieved for the  reliability-optimized links. As discussed in [12], BER is a more  appropriate measure of reliability than SNR, and a BER of 10 -9 is a  standard target for reliable on-chip communication. Therefore, we  choose appropriate values of SNRTarget that correspond to BER of  10-9 for different signaling methods. From the equations given in  [12], and as shown in Table 1, to achieve a BER of 10 -9, OOK and  4-PAM signaling require SNR of 21.7dB and 31.3dB, respectively.  For given noise power, 4-PAM signaling requires greater SNR to  achieve a BER of 10-9, as a given amount of noise power impacts a  4-PAM signal more than an OOK signal, due to the decreased gap  between different optical transmission levels of the 4-PAM signal.  As mentioned in Section I, our goal in this paper is to evaluate  the impact of the optimized designs of OOK, 4-PAM-SS, and 4PAM-P based photonic links on the performance, reliability and energy-efficiency of a well-known PNoC architecture: an 8-ary 3stage CLOS PNoC [4]. To achieve this goal, we first optimize  OOK, 4-PAM-SS, and 4-PAM-P based single-waveguide photonic  links for reliability to achieve BER of 10-9, using our search heuristic given in Section IV-A. We optimize 4.5cm long single-waveguide links, as according to our geometric analysis of the CLOS  PNoC, and from [4], the longest link of the CLOS PNoC is 4.5cm  long. Optimizing a 4.5cm long single-waveguide link corresponds  to finding the optimal value of Nλ using the search heuristic when  all other parameters in Eq. (1)-(4) and Table 1 are set based on the  length and geometrical parameters (e.g., number and degree of  bends, number of splitters etc.) of the link. We set the number of  DWDM λs for all the links in the CLOS PNoC to be equal to the  optimal Nλ obtained for the reliability-optimized 4.5cm long link.  Table 2 gives optimal Nλ and worst-case optical loss (optical loss  for the longest 4.5cm link) values for OOK, 4-PAM-SS, and 4PAM-P based variants of the CLOS PNoC. We direct the readers to  Section V for more information on the architecture of the CLOS  PNoC. As our proposed 4-PAM-P method does not require any  additional photonic structures compared to the conventional OOK  method and due to the absence of interference induced signal loss,  the 4-PAM-P based variants have worst-case signal loss values that  are similar to the OOK based variants (Table 2). As evident from  Table 2, the 4-PAM signaling based reliability-optimized variants  of CLOS PNoC (CLOS-4PAM-SS and CLOS-4PAM-P) render  smaller Nλ than the OOK signaling based CLOS-OOK PNoC. This  is because the SNRTarget value of 31.3dB for 4-PAM signaling is  greater than the SNRTarget of 21.7dB for OOK signaling, which  reduces the available power budget for the 4-PAM signaling based  links, rendering smaller Nλ for 4-PAM signaling based variants of  CLOS PNoCs. Nevertheless, as will be clear in Section V, despite  having inferior bandwidth owing to the smaller Nλ, CLOS-4PAM-P  PNoC achieves better energy-efficiency than CLOS-OOK.  Table 2: DWDM degree (optimal Nλ), optical loss and photonic area  for different variants of CLOS PNoC   Configuration Waveguide   Worst-case  Optical loss  DWDM   optical loss  + 10log(Nλ)  (optimal Nλ)  (in dB)  (in dB)  Reliability-optimized PNoCs  64  -1.7   -19.70  4  -6.4  -12.40  16  -1.7  -13.70  Bandwidth-neutral PNoCs  64  -1.7   32  -6.4  32  -1.7  CLOS-OOK   CLOS-4PAM-SS  CLOS-4PAM-P  CLOS-OOK-BN   CLOS-4PAM-SS-BN  CLOS-4PAM-P-BN  -19.70  -21.40  -16.70  Photonic  area  (in mm2)  2.64  2.13  2.22  2.64  2.50  2.36  Based on the standard dimensions and sizes of photonic devices  given in [10], we evaluated the total photonic area consumption of  the reliability-optimized variants of CLOS PNoC. The result of this  evaluation is also given in Table 2. As evident from the optimal Nλ,  worst-case loss, and photonic area results of the reliabilityoptimized CLOS PNoCs given in Table 2, CLOS-OOK achieves  the greatest bandwidth corresponding to the largest Nλ, whereas  CLOS-4PAM-SS and CLOS-4PAM-P achieve the best values of  photonic area and worst-case loss respectively. Thus, a clear winner  from the CLOS-4PAM-SS, CLOS-4PAM-P, and CLOS-OOK  PNoCs cannot be decided by looking at the reliability-optimized  results given in Table 2.   To have a fair comparison and to decide the superior method out  of the three signaling methods, we evaluate bandwidth neutral  designs (with equal aggregate bit transfer rates) of OOK, 4-PAMSS, and 4-PAM-P based variants of the CLOS PNoC, referred to as  CLOS-OOK-BN, CLOS-4PAM-SS-BN, and CLOS-4PAM-P-BN,  respectively. We evaluate the worst-case loss, optimal Nλ, and      photonic area values for all three bandwidth-neutral CLOS PNoCs,  which are listed in Table 2. Note that Nλ=32 for a 4-PAM signal and  Nλ=64 for an OOK signal both achieve equal bandwidth (aggregate  bit transfer rate), as a 4-PAM signal has 2× bit-rate than an OOK  signal (Section II). As evident from Table 2, among all three  bandwidth-neutral variants of CLOS PNoC, CLOS-4PAM-P-BN  achieves the best values of worst-case optical loss (which  determines required laser power) and photonic area. Therefore, it  can be concluded that 4-PAM-P method is more area- and energyefficient than 4-PAM-SS and OOK methods.   For a fairer and more comprehensive comparison of different  signaling methods, it is important to evaluate the bandwidth  (aggregate bit transfer rate), reliability, and energy-efficiency of all  the variants of the CLOS PNoCs listed in Table 2 for real  benchmark applications and in the presence of variations. Such an  evaluation is presented in the next section.  5 EVALUATION  5.1 Evaluation Setup   We considered a PNoC with an 8-ary 3-stage CLOS topology  [4] for a 256-core system, with 8 clusters (C1-C8) and 32 cores in  each cluster. Within each cluster, a group of four cores is connected  to a concentrator. Thus, each cluster has 8 concentrators and these  concentrators are connected electrically through a router for interconcentrator communication. The CLOS PNoC uses point-to-point  photonic links for inter-cluster communication, with a total of 56  waveguides being used to connect all 8 clusters of the CLOS PNoC.  Each point-to-point photonic link uses either forward or backward  propagating λs depending on the physical location of the source and  destination clusters. This PNoC uses two laser sources to enable  forward and backward communication. To power  the 56  waveguides, the PNoC employs a series of 1X2, 1X7, and 1X4  splitters. Based on geometric analysis, we estimated the maximum  length of a WG in the CLOS PNoC to be 4.5cm. This 4.5cm long  WG act as a point-to-point link between cluster C6 and C1.  Modeling of Process Variations of MRs in CLOS PNoC: We  adapt the VARIUS tool [31] to model die-to-die (D2D) as well as  within-die (WID) process variations in MRs for the CLOS PNoC.  We consider a 256-core chip with die size 400mm2 at a 22nm  process node. For the VARIUS tool, we use the parameters and  procedures given in [6] and [14] to generate 100 process variation  (PV) maps, each containing over 1 million points indicating the PVinduced resonance shift of MRs. The total number of points picked  from these maps equal the number of MRs in the CLOS PNoC.  Simulation Setup: We performed benchmark-driven simulationbased analysis to evaluate the impact of various signaling methods  on the performance and efficiency of the CLOS PNoC architecture.  We modeled and simulated all the variants (reliability-optimized as  well as bandwidth-neutral) of the CLOS PNoC given in Table 2  using a cycle-accurate NoC simulator. We evaluated performance  for a 256-core single-chip architecture at a 22nm CMOS node. We  kept the number of WGs and basic floorplan of the architectures  constant across all the variants. We used real-world traffic from  applications in the PARSEC benchmark suite [15]. GEM5 fullsystem simulation [16] of parallelized PARSEC applications was  used to generate traces that were fed into our cycle-accurate NoC  simulator. In GEM5 simulations, we set a “warm-up” period of  100M instructions and then captured traces for the subsequent 1B  instructions. In our benchmark-driven simulations we evaluated  total power, average latency, and energy-per-bit (EPB) values for  different variants of CLOS PNoC. The results of this evaluation are  given in Section V-B and Section V-C.  Moreover, to evaluate dynamic energy consumption values, we  extended the photonic model of the DSENT tool [17] to include all  three signaling methods (OOK, 4-PAM-SS, and 4-PAM-P). Table  3 gives SNRTarget values, dynamic energy values, channel gap  between adjacent λs, and detector sensitivity (S) values (evaluated  using Eq. (3)) for all the variants of the CLOS PNoC considered in  our evaluation. In the table, the detector sensitivity values directly  correspond to SNRTarget values according to Eq. (3). The channel  gap values in Table 3 correspond to a free-spectral range of 50nm  and Nλ values given in Table 2.   Table 3: SNRTarget, detector sensitivity, channel gap (CG) between  adjacent λs, and dynamic energy for different variants of CLOS PNoC  ReliabilityDetector  CG  Dynamic   optimized/Bandwidthsensitivity   (in nm)  energy   neutral PNoCs   S - in  (in fJ/bit)  dBm  -9.2/-22  SNRTarget   (in dB)  2.2  4.4  2.2  2.2  0  2.2  0.5  0.2  0  CLOS-OOK /  CLOS-OOK-BN  21.7/0  0.83  CLOS-4PAM-SS/  CLOS-4PAM-SS-BN  31.3/0  -4.4/-22  CLOS-4PAM-P/  CLOS-4PAM-P-BN  31.3/0  -4.4/-22  17.33/  1.67  3.47/  1.67  Fig. 3. Average total power dissipation comparison for different  reliability-optimized configurations of the CLOS PNoC architecture.  5.2 Results for Reliability-Optimized CLOS PNoCs   Fig. 3 presents a comparison of total power dissipation values  for  the CLOS-OOK, CLOS-4PAM-SS, and CLOS-4PAM-P  PNoCs. The power dissipation values in this figure are averaged  across different PARSEC benchmark applications. The error bars  in the figure represent maximum and minimum total power values  across the 100 PV maps. As evident, compared to CLOS-OOK,  CLOS-4PAM-P and CLOS-4PAM-SS dissipate 45.2% and 77.2%  lower total power respectively. From Table 2, CLOS-OOK has the  largest Nλ, whereas CLOS-4PAM-SS has the smallest Nλ. The  largest value of Nλ results in the largest flit-size and hence the  largest buffer-size, which in turn results in the highest power  dissipation in electrical concentrators. Moreover, the largest Nλ also  results in the highest number of MRs, which translates into the  highest amount of MR heating power. Due to these reasons, CLOSOOK dissipates the highest power compared to the other two  variants. In contrast, the smallest value of Nλ results in the lowest  power dissipation for CLOS-4PAM-SS. Moreover, from Table 2,  as the reliability-optimized CLOS-4PAM-P has greater Nλ than  CLOS-4PAM-SS, CLOS-4PAM-P has higher power dissipation.                           Fig. 4(a), (b) present average packet latency and aggregate  energy-per-bit (EPB) for all three variants of the CLOS PNoC  across 12 multi-threaded PARSEC benchmarks. The error bars in  Fig. 4(b) represent maximum and minimum EPB values across the  100 PV maps.  being equally reliable compared to the OOK and 4-PAM-SS  methods.   5.3 Results for Bandwidth-Neutral CLOS PNoCs  As explained in Section IV-A, the bandwidths of all the  bandwidth-neutral variants (CLOS-OOK-BN, CLOS-4PAM-SSBN, and CLOS-4PAM-P-BN) are equal. Therefore, we do not  present the bandwidth (aggregate bit transfer rate) or average  latency results in this section. Instead, we only present total power  dissipation, EPB, and SNR/BER results.  (a)  (a)  (b)  Fig. 4. (a) Average packet latency, and (b) energy-per-bit comparison  for different reliability-optimized variants of CLOS PNoC across  PARSEC benchmarks. All results are normalized to the baseline  CLOS-OOK PNoC results.  As evident from Fig. 4(a), CLOS-4PAM-P achieves 29.8% lower  average latency than CLOS-4PAM-SS, whereas CLOS-OOK  achieves 50% and 28.8% lower average latency than CLOS-4PAMSS and CLOS-4PAM-P respectively. The larger value of Nλ results  in increased simultaneous bit transfers, which in turn renders lower  average latency for CLOS-4PAM-P compared to CLOS-4PAM-SS.  Similarly, the largest Nλ results in the lowest average latency for  CLOS-OOK. From Fig. 4(b), CLOS-4PAM-P has 12.7% and 11.5%  lower EPB compared to CLOS-OOK and CLOS-4PAM-SS  respectively. The 4-PAM signaling used in CLOS-4PAM-P makes  PAM signaling, but the higher value of  𝑃𝐿𝑜𝑠𝑠 better use of MR heating power and electrical concentrator power  by modulating two bits using only one modulator MR. As a result,  CLOS-4PAM-P renders better energy-efficiency in terms of lower  𝐼𝑁𝑇𝑅𝐹 for 4-PAM-SS  EPB than CLOS-OOK. Interestingly, CLOS-4PAM-SS also uses 4increases the signal loss, which results in very small Nλ, and hence,  worse energy-efficiency (EPB) for CLOS-4PAM-SS.  As evident from Fig. 4(b), CLOS-4PAM-SS yields lower EPB  than CLOS-OOK on average, as CLOS-4PAM-SS dissipates 45.2%  less power, but it yields 2× average latency than CLOS-OOK on  average. But for some applications such as Bodytrack, Facesim,  Swaptions, and X-264, CLOS-4PAM-SS yields greater EPB than  CLOS-OOK. This is because for these applications, CLOS-4PAMSS yields greater than 2× latency compared to CLOS-OOK, the  effect of which translates into greater EPB.  Note that the BER for all the three reliability-optimized variants  of the CLOS PNoC is 10-9. It can be observed from these results  that 4-PAM-P signaling method is more energy-efficient while  (b)  Fig. 5. (a) Worst-case SNR, (b) average total power dissipation  comparison for different bandwidth-neutral configurations of the  CLOS PNoC.   Fig. 5(a) plots the worst-case SNR values for different  bandwidth-neutral variants of the CLOS PNoC. CLOS-OOK-BN,  CLOS-4PAM-SS-BN, and CLOS-4PAM-P-BN yield SNR values of  14dB, 19dB and 20.5dB respectively, which translates into BER  values of 6.2×10-3, 1.7×10-2, and 4.1×10-3 respectively (using the  equations given in [12]). It can be implied from these results of SNR  and BER that CLOS-4PAM-P-BN achieves greater communication  results in the least value of heterodyne crosstalk noise power 𝑃𝑁𝑜𝑖𝑠𝑒𝐻𝑇𝐶  reliability (corresponding to smaller BER) than CLOS-4PAM-SSBN and CLOS-OOK-BN. As shown in Table 2 and Table 3, among  all the bandwidth-neutral PNoCs, CLOS-4PAM-P-BN has the least  signal loss and the largest channel gap. The largest channel gap  [6][24]. Due to the combined effects of these factors, CLOS-4PAMP-BN achieves the best SNR, BER, and communication reliability.  Fig. 5(b) gives average power dissipation for different  bandwidth-neutral variants of the CLOS PNoC. CLOS-4PAM-PBN has 16.9% and 19.5% lower total power dissipation compared  to CLOS-OOK-BN and CLOS-4PAM-SS-BN  respectively.  Decrease in laser power (due to decrease in through losses), ring  heating power (due to decrease in number of MRs) and dynamic  energy (as shown in Table 3) contributes to decrease in total power  dissipation of CLOS-4PAM-P-BN compared to CLOS-4PAM-SSBN and CLOS-OOK-BN.  Lastly, Fig. 6 presents EPB values for all three bandwidth neutral variants of the CLOS PNoC across PARSEC benchmarks.  It can be seen that on an average, the CLOS-4PAM-P-BN has 14.6%  and 16.3% lower EPB compared to CLOS-OOK-BN and CLOS                    4PAM-SS-BN respectively. Decrease in total energy consumption  of CLOS-4PAM-P-BN compared to CLOS-OOK-BN and CLOS4PAM-SS-BN reduces its EPB. Note that, in Fig. 5(a), (b), and Fig.  6, the error bars represent maximum and minimum across the 100  PV maps of the SNR, total power, and EPB values, respectively.  Fig. 6. Energy-per-bit comparison for different bandwidth-neutral  variants of CLOS PNoC across PARSEC benchmarks. All results are  normalized to the baseline CLOS-OOK-BN PNoC results.  In summary, we showed that PNoCs that utilize our proposed 4PAM-P signaling based photonic links have greater reliability,  energy-efficiency, and area-efficiency for the same bandwidth  compared to the PNoCs that are comprised of 4-PAM-SS or OOK  signaling based photonic links. These results corroborate the  excellent capabilities of our proposed 4-PAM-P optical signaling  method in achieving high-bandwidth data transfers in PNoCs with  greater reliability, area-efficiency, and energy-efficiency.  6 CONCLUSIONS  This paper presents a novel method, called 4-PAM-P, for  generating 4-PAM optical signals in PNoCs, which can double the  aggregate bandwidth without increasing utilized wavelengths,  photonic hardware, and incurred noise, thereby improving the bit error-rate (BER), area-efficiency, and energy-efficiency of PNoCs.  Our analysis shows that our 4-PAM-P method achieves equal  bandwidth with 4.2× better BER, 19.5% lower power, 16.3% lower  energy-per-bit, and 5.6% less photonic area compared to the best  known 4-PAM optical signaling method (4-PAM-SS) from prior  work. Moreover, our 4-PAM-P method achieves equal bandwidth  with 1.5× better BER, 16.9% lower power, 14.6% lower EPB, and  10.6% less photonic area compared to the conventional OOK  signaling method. These results corroborate  the excellent  capabilities of our proposed 4-PAM-P method in achieving highbandwidth data transfers in PNoCs with greater reliability, area- and  energy-efficiency.  ACKNOWLEDGMENT  This research is supported by grants from SRC, NSF (CCF1252500, CCF-1302693), and AFOSR (FA9550-13-1-0110).  "
Fairness-Oriented and Location-Aware NUCA for Many-Core SoC.,"Non-uniform cache architecture (NUCA) is often employed to organize the last level cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network latency is becoming the major source of the cache access latency. Second, the communication distance and latency gap between different cores is increasing. Such gap can seriously cause the network latency imbalance problem, aggravate the degree of non-uniform for cache access latencies, and then worsen the system performance.
In this paper, we propose a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA), to alleviate the network latency imbalance problem and achieve more uniform cache access. We strive to equalize network latencies which are measured by three metrics: average latency (AL), latency standard deviation (LSD), and maximum latency (ML). In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to better fit the network topology and traffics, thereby equalizing network latencies from two aspects, i.e., non-contention latencies and contention latencies, respectively. The experimental results show that FL-NUCA can effectively improve the fairness of network latencies. Compared with the traditional static NUCA (S-NUCA), in simulation with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9%, 36.3%, and 35.0%, respectively. In simulation with PARSEC benchmarks, the average improvements for AL, LSD, and ML are 6.3%, 3.6%, and 11.2%, respectively.","Fairness-Oriented and Location-Aware NUCA for Many-Core SoC Zicong Wang National University of Defense Technology Changsha, Hunan, China wangzicong@nudt.edu.cn Chen Li National University of Defense Technology Changsha, Hunan, China lichen@nudt.edu.cn ABSTRACT Non-uniform cache architecture (NUCA) is often employed to organize the last level cache (LLC) by Networks-on-Chip (NoC). However, along with the scaling up for network size of Systems-on-Chip (SoC), two trends gradually begin to emerge. First, the network latency is becoming the major source of the cache access latency. Second, the communication distance and latency gap between different cores is increasing. Such gap can seriously cause the network latency imbalance problem, aggravate the degree of non-uniform for cache access latencies, and then worsen the system performance. In this paper, we propose a novel NUCA-based scheme, named fairness-oriented and location-aware NUCA (FL-NUCA), to alleviate the network latency imbalance problem and achieve more uniform cache access. We strive to equalize network latencies which are measured by three metrics: average latency (AL), latency standard deviation (LSD), and maximum latency (ML). In FL-NUCA, the memory-to-LLC mapping and links are both non-uniform distributed to better fit the network topology and traffics, thereby equalizing network latencies from two aspects, i.e., non-contention latencies and contention latencies, respectively. The experimental results show that FL-NUCA can effectively improve the fairness of network latencies. Compared with the traditional static NUCA (SNUCA), in simulation with synthetic traffics, the average improvements for AL, LSD, and ML are 20.9%, 36.3%, and 35.0%, respectively. In simulation with PARSEC benchmarks, the average improvements for AL, LSD, and ML are 6.3%, 3.6%, and 11.2%, respectively. ∗ Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4984-0/17/10. . . $15.00 https://doi.org/10.1145/3130218.3130225 Xiaowen Chen∗ National University of Defense Technology Changsha, Hunan, China KTH Royal Institute of Technology Kista, Stockholm, Sweden xwchen@nudt.edu.cn Yang Guo National University of Defense Technology Changsha, Hunan, China guoyang@nudt.edu.cn CCS CONCEPTS • Hardware → Network on chip; System on a chip; • Computer systems organization → Processors and memory architectures; KEYWORDS Networks-on-chip, non-uniform cache architecture, memory mapping ACM format: Zicong Wang, Xiaowen Chen, Chen Li, and Yang Guo. 2017. FairnessOriented and Location-Aware NUCA for Many-Core SoC. In Proceedings of NOCS ’17, Seoul, Republic of Korea, October 19–20, 2017, 8 pages. https://doi.org/10.1145/3130218.3130225 1 INTRODUCTION As the number of cores integrated on chip keeps on increasing, Networks-on-Chip (NoC) is becoming the fundamental infrastructure of Systems-on-Chip (SoC). With NoC technology, the SoCs are often designed to non-uniform cache architecture (NUCA) [14]. NUCA provides non-uniform cache access latencies, and thus can distribute the large-capacity shared last-level cache (LLC) on different cores for scalability and efficiency. On the other hand, along with scaling up for network size, two trends begin to emerge. First, the cache access latency is gradually dominated by the network latency. Second, the communication distance and latency gap between different cores begin to increase, and such gap can exacerbate the degree of non-uniform for network latencies. Take the two-dimensional mesh network as an example. Figure 1 shows the average communication distance of each node to other nodes in a 4 × 4 mesh network. The number labeled on each node denotes its average distance to other nodes. We can observe that the cores located in central nodes have the lowest average communication distance (2, compared with 2.5 for nodes located in edge and 3 for nodes located in corner). Hence, they have an advantage over the other cores in terms of communication, and the advantage is growing bigger along with the scaling up for network size. The consequence of more unbalanced network latencies is that the latency gap between packets becomes larger, and there NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Zicong Wang et al. will be more high-latency packets. We call this phenomenon as the network latency imbalance problem. The network latency imbalance problem can cause high variances across cache access latencies, which leads to degradation in overall system performance. A cache access that experiences overhigh latency can block the running progress of the core, and become the bottleneck of system and restrict the performance. From the perspective of improving system performance, it is crucial to provide more uniform cache access latencies although NUCA allows non-uniform cache access latencies. Figure 1: The average communication distance of each node for a 4 × 4 mesh network. To solve the network latency imbalance problem, we strive to equalize network latencies. First, it is necessary to analyze the composition of network latencies. The packet latency can be determined by the following expression [5]: T = H × (tr + tl ) + L b + Tc (1) In Equation 1, H denotes the hop count of the packet traveling from the source node to the destination node. tr and tl indicate the delay through a single router and a single link, respectively, which collectively reflect the non-contention latency in packet traveling. Tc denotes the contention latency, which reflects the time spent waiting for resources. L/b stands for the serialization delay, which is the time for the packet of length L to cross a link with bandwidth b . It is noted that tr , tl , and b are constant under determined network parameters and router architecture. In addition, L is also determined for a specific packet. However, H and Tc can vary considerably for each node. H and Tc , respectively, affect the non-contention latency and contention latency. Therefore, our work concentrates on regulating H and Tc to narrow the gap of network latency for each node. Based on the traditional NUCA, we propose fairness-oriented and location-aware NUCA (FL-NUCA). Our approach uses fairness to denote the degree of network latency equalization, which is measured by three metrics: average latency (AL), latency standard deviation (LSD), and maximum latency (ML). The contribution of our work includes the following two aspects: 1) breaking the traditional uniform memory-to-LLC mapping to nonuniform, which prompts network latencies to be more balanced by affecting network traffics, and 2) based on the non-uniform memory-to-LLC mapping, applying non-uniform link distribution to better fit network traffics. 2 BACKGROUND AND RELATED WORK The NUCA technology is originally proposed by Kim et al. [14] to divide a large LLC into multiple banks with non-uniform access latencies for uniprocessor. Subsequently, Beckmann et al. [3] introduce the concept of NUCA to chip-multiprocessor (CMP). Depending on memory mapping mechanism, two NUCA-based schemes are proposed: static NUCA (S-NUCA) and dynamic NUCA (D-NUCA). The S-NUCA scheme statically maps a cache block to a unique bank based on the block index in physical address. Therefore, every bank corresponds to predetermined sections in physical address space [19]. In D-NUCA scheme, a cache block is associated with a collection of banks (bank set), and the block can migrate between banks in the bank set. Hence, the frequently accessed cache blocks can dynamically migrate towards the requesting cores to reduce the cache access latencies [1]. Because of the flexibility for data migrating, D-NUCA has to depend on the complex smart search mechanism to locate cache blocks. As a result, the hardware costs of area and power are considerably increased compared with S-NUCA. In addition, D-NUCA does not always achieve better performance than S-NUCA [3, 14]. Based on S-NUCA or D-NUCA, many prior research projects explore the design space with architectural and algorithmic techniques. Hub et al. [11] propose the concept of sharing degree, which denotes the number of cores that share a given pool of caches. In addition, a NUCA-based organization that supports the spectrum of degrees of sharing is presented, and an S-NUCA organization with a sharing degree of two or four is evaluated and proved to be the best solution. Dybdahl et al. [7] propose an adaptive shared/private NUCA cache partitioning scheme that can dynamically control the amount of cache space to be shared among the cores. A novel data migration algorithm for parallel applications is proposed by Kandemir et al. [13]. The migration algorithm aims to select the most appropriate locations for each block by taking advantage of the variances across cache access latencies of each node. However, these NUCA-based techniques mentioned above aim to improve performance by exploiting the advantage of NUCA, and rarely notice the disadvantage of NUCA. In contrast, our approach pays attention to the inherent problem of NUCA, and strive to offer more uniform cache access latencies. The contentions and interferences on shared resources, such as crossbar switches, network links, and cache/memory controllers, can cause high variances across request (cache/memory access) latencies [6, 15, 17, 18]. High variances can lead to some requests suffering overhigh latencies, which block the running progress of the cores, become the bottleneck of system, and worsen the system performance. Therefore, some researchers explore how to balance the latencies of requests and improve the fairness. Franco et al. [8] propose Distributed Routing Balancing (DRB), which is targeting the parallel computer interconnection networks and developed to uniformly balance communication traffics. Das et al. [6] propose novel router prioritization policies to improve the system fairness by exploiting interfering packets’ available slack. Sharifi et al. [18] point out that the variances of memory access latencies degrade the overall system performance, so it is crucial to balance latencies of memory accesses and reduce the number of high-latency Fairness-Oriented and Location-Aware NUCA for Many-Core SoC NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea memory accesses. Some researchers find the unbalanced buffer utilization may cause degradation in performance [9, 16]. Gorgues et al. [9] describe a new flow control and routing algorithm in order to achieve balanced buffer utilization. Li et al. [16] introduce a new switch allocation strategy to remit the problem of unbalanced buffer utilization. From the similar perspective of improving system performance, our approach aims to intensify the fairness of network packets, reduce the number of high-latency network packets, and equalize network latencies as well as cache access latencies. 3 FAIRNESS-ORIENTED AND LOCATION-AWARE NUCA The design of FL-NUCA follows a two-step process to equalize network latencies. The first step starts from the non-uniform of average communication distance, and strives to achieve the average communication distance of each node to be more uniform for improving the fairness. The first step is realized by redesigning the traditional uniform memory-to-LLC mapping to be non-uniform. Based on the first step, the second step aims to build non-uniform link distribution for offsetting the additional traffics in central region. 3.1 The baseline S-NUCA Considering the simplicity and extensive use of S-NUCA, we focus attention on S-NUCA in our work. Our baseline architecture as Figure 2 shows is a typical many-core system and widely used in modern SoCs. In such an architecture, the routers are connected by a two-dimensional mesh network (taking the network size 4 × 4 as an example). A processing element (PE) includes a core, a network interface (NI), a private level one (L1) instruction/data cache, and a shared level two (L2) cache bank. A PE is connected to a router by the NI. The distributed L2 cache banks (namely the LLC) are organized as S-NUCA, and adopts block-interleaving addressing. It is very important to note that the block-interleaving for the cache system can destroy the locality of the datum from the memory to the LLC. Besides this, the uniform memory-to-LLC mapping makes the traffics approximate uniformly distributed on each LLC bank. Consequently, the traffic pattern tends to be similar to all-to-all personalized communication, and we can reasonably model the traffic pattern as uniform distribution [20]. 3.2 Step-1: Fairness-Oriented and Non-Uniform Memory Mapping In S-NUCA, every LLC bank is equally mapped with the same number of blocks. The first step aims to balance non-contention network latencies by equalizing the parameter H in Equation 1, i.e., the average communication distance of each core. The basic idea is that if we redesign the memory-to-LLC mapping with emphasis on the central banks (i.e., mapping more sections to the central banks, and less to the peripheral banks), then a part of traffics will be moved from the peripheral banks to the central banks. Because the average communication distance to other nodes of the central nodes is less than the peripheral nodes, the difference of average communication distance between the central nodes and the peripheral nodes will be diminished. Figure 2: The baseline architecture of S-NUCA. The physical memory address includes two parts: byte address and block address. The bank ID field is taken from the LSBs of the block address (assume that the memory address width is 32 bits, the block size is 16 bytes, and the bank ID field occupies 4 bits), hence the cache system applies block-interleaving addressing to uniformly distribute the memory blocks on each LLC bank (i.e., L2 cache bank). In the following discussion, we will calculate the distribution of memory-to-LLC mapping in FL-NUCA. Considering an M × N mesh network with YX deterministic dimension-order routing (YXDOR). Assume that the proportion of mapping blocks for LLC bank located in (i , j ) is pi , j . From the perspective of cache access, pi , j can be regarded as the probability of bank being visited. The probability distribution is represented by matrix P : P = [pi , j ]M ×N (2) The cache access distance of core located in (i , j ) to bank located in (m, n) is determined by the Manhattan distance, and can be calculated by Equation 3: di , j (m, n) = |i − m | + | j − n | (3) The average communication distance of node (i , j ) to other banks is shown in Equation 4: M −1 N −1 M −1 N −1 average communication distance: We use the matrix D to represent the distribution of each node’s [( |i − m | + | j − n | ) × pm, n ] [di , j (m, n) × pm, n ] m=0 n=0 (4) di , j = = m=0 n=0 D = [di , j ]M ×N (5) Then we can get the mean value of D : NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Zicong Wang et al. (6) di , j M N M N i =0 j =0 i =0 j =0 n=0 m=0 = 1 [( |i − m | + | j − n | ) × pm, n ] M −1 N −1 µ (D ) = 1 M −1 N −1 M −1 N −1 To equalize the average communication distance of each node, we expect the difference of elements in D to be as small as possible. Therefore, we set the standard deviation of D as the objective function: M −1 N −1 Note that the sum of the probability of each bank being visited should be one, i.e., satisfy the following constraint: M −1 N −1 In addition, all the visiting probability should be greater than or equal to zero: (cid:118)(cid:117)(cid:117)(cid:116) 1 (di , j − µ (D ))2 σ (D ) = pi , j = 1 M N i =0 j =0 i =0 j =0 (7) (8) pi , j ≥ 0 (0 ≤ i ≤ M − 1, 0 ≤ j ≤ N − 1) (9) With the objective function and constraints summarized above, the optimization can be abstracted as the following nonlinear programming problem: M −1 N −1 minimize σ (D ) sub jec t to pi , j = 1 (10) i =0 j =0 pi , j ≥ 0   We model the nonlinear programming problem in MATLAB, and calculate the probability distribution matrix P with the help of MATLAB’s optimization toolbox. The distribution of pi , j is shown as Equation 11 under M and N equaling to 8: P = (11) × 10−3 12 13 14 15 15 14 13 12 13 15 16 17 17 16 15 13 14 16 18 18 18 18 16 14 15 17 18 19 19 18 17 15 15 17 18 19 19 18 17 15 14 16 18 18 18 18 16 14 13 15 16 17 17 16 15 13 12 13 14 15 15 14 13 12 With the distribution of pi , j , we can calculate the proportion of blocks mapped from memory for each bank. For an 8 × 8 mesh network, the bank ID field in the physical memory address needs 6 (i.e., l oд2 64) bits to directly map the block to the corresponding bank. In other words, memory-to-LLC mapping is interleaved with 64 blocks. In order to map blocks in proportion to the probability distribution as shown in Equation 11, it is necessary to expand the width of bank ID field to rearrange the memory-to-LLC mapping. The longer the bank ID field, the closer we can arrange the practical   mapping to the ideal distribution at the costs of the increasing of the complexity. By trading off between the accuracy and the complexity, we expand the bank ID field to 9 bits, which can interleave the mapping with 512 blocks. We multiply the probability distribution matrix P by the number of blocks in a mapping interval and get the number of blocks distribution for a mapping interval: (12) B = P × 29 ≈ 6 7 7 8 8 7 7 6 7 7 8 9 9 8 7 7 7 8 9 9 9 9 8 7 8 9 9 10 10 9 9 8 8 9 9 10 10 9 9 8 7 8 9 9 9 9 8 7 7 7 8 9 9 8 7 7 6 7 7 8 8 7 7 6 Next, within a mapping interval, we should consider how to map each block to banks. Observing the distribution B , we can find the change is in accordance with our expectations: the peripheral banks are mapped with less blocks, and more blocks are mapped to the central banks. Considering the number of blocks distribution for S-NUCA, 512 blocks should be uniformly distributed on 64 banks by block-interleaving, and each bank is mapped with eight blocks. Compared with the distribution in FL-NUCA, we can remap some blocks from peripheral banks to central banks, and keep the majority of blocks unchanged. This remapping scheme can maintain the minimum extra costs in hardware implementing. The 512 blocks can be divided into eight groups sequentially, and each group includes eight blocks. Because the least number of mapped blocks is six (bank 0/7/56/63), we can keep the first six groups (i.e., 6 × 64 blocks) with the original mapping scheme in S-NUCA, and regulate the mapping in the 7th and 8th groups. Figure 3(a) reveals the memory-to-LLC mapping scheme of FLNUCA for an 8 × 8 mesh network. As Figure 3(a) shows, the bank address is expanded to 9 bits, and includes two parts in FL-NUCA, bank tag and bank index. The bank tag stands for the group number, and the bank index actually indicates the original bank ID in SNUCA. Note that the distribution B can be divided into four sections which have the similar distribution structure, and it is convenient to follow the same mapping scheme in each section. Taking the upper-left section as an example. We remap the two blocks, which originally remain in the 7th and 8th groups (i.e., bank tag equaling to 6 or 7, respectively), from bank 0 to bank 27. In addition, the block which originally remains in the 8th group (i.e., bank tag equaling to 7), is remapped from bank 1 to bank 19 (the similar remapping is implemented from bank 2/8/9/16 to the banks which are guided by the arrowed lines). Figure 3(b) presents the detailed result of the mapping scheme of FL-NUCA for 512 blocks (mainly shows the mapping in upper-left section). In order to simplify the design of mapping scheme and improve the performance, it is worthy to use a linear list to remap the bank from the original position in S-NUCA to the new position in FL-NUCA. For an 8 × 8 mesh network, the linear list is like the remapping result of the 8th group (i.e., bank tag equals to 7) shown in Figure 3(b). With the linear list, the bank ID can be directly obtained using the bank address. Although the above analysis and result are based on an 8 × 8 network, the similar conclusion and method can be also derived for any size network, as long as the the network is mesh topology. Fairness-Oriented and Location-Aware NUCA for Many-Core SoC NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea (a) The memory-to-LLC mapping scheme of FL-NUCA (b) The detailed result of the mapping scheme of FL-NUCA Figure 3: (a) The memory-to-LLC mapping scheme of FL-NUCA for an 8 × 8 mesh network. The nodes can be divided into two categories: unchanged node and remapped node. The unchanged node keeps the mapping unchanged with S-NUCA, and the remapped node is performed with the remapping scheme for some specific blocks (i.e., bank tag equaling to 6 or 7). Note that in the mesh of nodes the number labeled on each node denotes the bank ID, and in the legend the number labeled on the node stands for the number of mapped blocks. (b) The detailed result of the mapping scheme of FL-NUCA for 512 blocks. The horizontal axis denotes the bank index, which indicates the original bank ID in S-NUCA. The vertical axis denotes the bank tag, which stands for the group number in eight groups for 512 blocks. The number labeled on each block represents the actual bank ID of mapping node. For example, the related distribution matrices (i.e., P and B ) for any network size have the similar pattern, which follows the regularity of remapping some blocks from peripheral banks to central banks. 3.3 Step-2: Location-Aware and Non-Uniform Link Distribution Similar to the cores, different hops also reside in different locations, and the traffic pressure on each hop presents non-uniform distribution. In [20], the traffic requirements per hop for uniform distribution traffics are analyzed and presented. However, the traffic pattern is changed since we break the uniform memory-to-LLC mapping, and the practical traffic pattern will not tend to be similar to uniform distribution. Instead, more traffics will be concentrated on the central region of the mesh network, and the contention near the central region will be aggravated. Therefore, Step-2 regulates network contention latencies to balance network latencies by intensifying the communication capacity of each hop. To analyze the traffic pattern of S-NUCA based on Step-1, it is necessary to calculate the traffic pressure on each hop. Since the network size is M × N , the hops for a mesh network can be divided into two groups: horizontal and vertical hops. The horizontal and vertical hops have the size of M × (N − 1) and (M − 1) × N , respectively. We need to calculate the messages through each hop based on the number of blocks distribution (as matrix B shown in Equation 12). Figure 4 shows the location relationship between the source node (x , y ) and the destination node (m, n). According to the different locations for destination node, we should increase the number of messages of hops along the path from (x , y ) to (m, n). Algorithm 1 illustrates the procedure of calculating the distribution of the number of messages for horizontal and vertical hops. For an 8 × 8 mesh network, assume that each core sends 512 packets to access LLC banks, and the number of bank accesses sticks to the distribution as shown in Equation 12. We calculate the distribution of the number of messages for each hop, as Figure 5(a) shows. Figure 4: The location relationship between the source node (x , y ) and the destination node (m, n). Assume that the routing algorithm is YX-DOR. To intensify the communication capacity of each hop, additional physical links for the hops with heavy traffics can relief the traffic contention in central region. The message distribution acts as the traffic contention distribution, and we can add additional physical links based on the traffic pressure on each hop. The minimum number of messages within the hops is msдm i n : msдm i n = min {Hm s д , Vm s д } (13) The number of links for each hop equals to the analytical number of messages divided by the minimum number of messages, and then we can get the horizontal and vertical link distribution: )]M ×(N −1) )](M −1)×N Specifically, based on the message distribution, the link distribution for an 8 × 8 mesh network can be calculated and presented in Figure 5(b). We can observe that there are more additional links in central region. The traffic contention can be balanced by alleviating = [r ound ( hi , j msдm i n = [r ound ( vi , j msдm i n Vl i nk = Hl i nk = Hm s д msдm i n Vm s д msдm i n (14) NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Zicong Wang et al. Algorithm 1 Calculate the distribution of the number of messages for each hop. Input: The size of the mesh network, M × N ; The distribution of number of blocks, B = [bi , j ]M ×N . Output: The distribution of the number of messages for horizontal hops, The distribution of the number of messages for vertical hops, Hm s д = [hi , j ]M ×(N −1) ; Vm s д = [vi , j ](M −1)×N . 1: for (i , j ) ⇐ (0, 0) → (M − 1, N − 2) do 3: end for 4: for (i , j ) ⇐ (0, 0) → (M − 2, N − 1) do hi , j ⇐ 0 vi , j ⇐ 0 6: end for 7: for (x , y ) ⇐ (0, 0) → (M − 1, N − 1) do for (m, n) ⇐ (0, 0) → (M − 1, N − 1) do if x > m then for i ⇐ m → x − 1 do 2: 5: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: vi , n ⇐ vi , n + bm, n end for else if x < m then for i ⇐ x → m − 1 do vi , n ⇐ vi , n + bm, n end for end if if y > n then for j ⇐ n → y − 1 do hx , j ⇐ hx , j + bm, n end for else if y < n then for j ⇐ y → n − 1 do hx , j ⇐ hx , j + bm, n end for end if end for 28: end for 29: return Hm s д , Vm s д (a) Message distribution the traffic pressure in central region, thus avoiding the packets sent from central region suffering overhigh latencies. 4 EVALUATION 4.1 Experimental Setup We evaluate the performance of FL-NUCA using BookSim, which is a popular cycle-accurate interconnection network simulator [12]. The performance simulation is twofold. First, we build the specific traffic distribution corresponding to the probability distribution based on Step-1 under synthetic simulation, and arrange the link distribution based on Step-2. Second, for an 8 × 8 mesh network, we perform evaluation with PARSEC benchmarks [4] based on netrace [10]. We select a representative part of programs within netrace, and the trace-driven simulation configuration can be refereed to [10]. (b) Link distribution Figure 5: The message and link distribution for an 8 × 8 mesh network based on Step-1. The number attached on a hop respectively denotes the number of messages and links through this hop. 4.2 Simulation with Synthetic Traffics Although the simulation with synthetic traffics does not include the memory-to-LLC mapping mechanism, we can build the specific traffic distribution corresponding to the probability distribution based on Step-1 to evaluate the effect for FL-NUCA. In the experiments with synthetic traffics, we use uniform distribution traffics to simulate the performance of S-NUCA. In addition, we use the Fairness-Oriented and Location-Aware NUCA for Many-Core SoC NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea (a) AL (b) LSD (c) ML Figure 6: The network performance comparison between S-NUCA and FL-NUCA for synthetic traffics under 8×8 mesh network. (a) CDF (b) PDF Figure 7: The CDFs and PDFs comparison between S-NUCA and FL-NUCA under 8 × 8 mesh network. that 90% of network latencies are less than 200 cycles. In Figure 7(a), the values on the x-axis denote network packet latency (in cycles), and the values on the y-axis represent the fraction of the total number of packets. Specifically speaking, one point (x , F (x )) on curve F means that the fraction of the total number of packets with latencies less than x is F (x ). We can observe that FL-NUCA has fewer highlatency packets than S-NUCA. As one can observe from Figure 7(b), FL-NUCA prompts the distribution of high latency (Region 1) to transfer to the new distribution of low latency (Region 2). Therefore, FL-NUCA makes network latencies to be more concentrated, and thus achieves the goal of equalizing network latencies. Following the two-step process of FL-NUCA, we calculate the memory-to-LLC mapping distribution and link distribution for different mesh network sizes, and perform the simulation under the saturation point of S-NUCA. Figure 8 reveals the percentage of decrease for three metrics (AL, LSD, and ML) under different mesh network sizes. Compared with S-NUCA, FL-NUCA can achieve considerable improvement in fairness, which is measured by AL, LSD, and ML. Notice that with the network size scaling up, the performance improvement is becoming more pronounced, because the network latency imbalance problem is more serious for large-scale many-core SoCs. 4.3 Simulation with Benchmarks To simulate the mapping optimization in Step-1, we modify the memory-to-LLC mapping of the original trace files as described in Section 3.2 in post-processing of simulation, and arrange the link distribution as Figure 5(b) shown. The experimental result Figure 8: The percentage of decrease in AL, LSD, and ML under different mesh network sizes. traffics corresponding to the bank visiting probability distribution (Step-1) and the link distribution (Step-2) to simulate the performance of FL-NUCA. Figure 6 illustrates the network performance comparison between traditional S-NUCA and FL-NUCA for an 8 × 8 mesh network. Compared with S-NUCA, we can observe that the average latency is decreased and the injection rate is increased to about 0.17. Furthermore, FL-NUCA also outperforms S-NUCA in terms of LSD and ML, and the performance improvement becomes more significant with the increasing of injection rate. To further observe the effect on fairness of network latencies, we collect network latencies in 100,000 cycles. Figure 7(a) and Figure 7(b), respectively, plot the cumulative distribution function (CDF) and the probability density function (PDF) of network latencies under injection rate equaling to the saturation point 0.161. Notice that we only show the main parts of the entire curves for the reason NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea Zicong Wang et al. (a) AL (b) LSD (c) ML Figure 9: The network performance comparison between S-NUCA and FL-NUCA for PARSEC benchmarks. is illustrated as Figure 9. We can observe that FL-NUCA outperforms S-NUCA with each program. Compared with S-NUCA, the maximum improvements for AL, LSD, and ML can be up to 7.8% (bodytrack ), 10.1% (bodytrack ), and 20.6% (vips), respectively. The average improvements for AL, LSD, and ML with all the programs are 6.3%, 3.6%, and 11.2%, respectively. The overall performance improvement is not significant as the experimental results of simulation with synthetic traffics, and the reason comes from two aspects. First, the traffics are not strict uniformly distributed in real programs [2], and FL-NUCA is optimized for S-NUCA which is approximately modeled as all-to-all personalized communication. Hence FL-NUCA does not achieve considerable performance improvement for some programs. Second, the average injection rate is generally lower than the saturation point in real programs, and FL-NUCA can perform better with higher injection rate (as observed in Figure 6). Nevertheless, FL-NUCA can achieve appreciable performance improvement in terms of fairness, and promote more balanced network latencies. 5 CONCLUSION AND FUTURE WORK The network latency imbalance problem can exacerbate the nonuniform of cache accesses, incur some cache accesses suffering from very high latencies, and thus degrading the system performance. In this paper, we propose FL-NUCA, a fairness-oriented and locationaware NUCA-based scheme, to prompt network latencies to be more uniform. In the anticipated future work, we will evaluate and optimize the hardware costs of FL-NUCA, and extend FL-NUCA from mesh to more topologies. ACKNOWLEDGMENTS The authors would like to thank the anonymous referees for their valuable comments and helpful suggestions. The work is supported by the National Natural Science Foundation of China under Grant No.: 61502508 and 61402499, and the Natural Science Foundation of Hunan Province under Grant No.: 2015JJ3017. "
A Novel Approach to Reduce Packet Latency Increase Caused by Power Gating in Network-on-Chip.,"The power gating technique is an effective way to reduce the high static power consumption in a Network-on-Chip (NoC). However, with notable wakeup delay, the power gating technique incurs significant packet latency increase. In this paper, we propose a novel Duty Buffer (DB) structure and an efficient DB-based power gating scheme to overcome this drawback. By keeping minimal number of DB active to replace any sleeping virtual channel in a router, our approach can efficiently reduce the packet latency increase along the whole routing path. Compared with a conventional five-stage pipeline router without power gating, our approach, with only one flit depth of the DB, increases the average packet latency by only 9.67%, which is much less than 57% and 21.75% latency increase in related approaches. With small hardware overhead, our approach can save on average 52.19% of the total power consumption in a NoC, which is comparable with 59.39% and 57.05% power savings in related approaches.","A Novel Approach to Reduce Packet Latency Increase Caused by Power Gating in Network-on-Chip Peng Wang∗† Sobhan Niknam∗ Zhiying Wang† Todor Stefanov∗ Leiden Institute of Advanced Computer Science, Leiden University, The Netherlands∗ State Key Laboratory of High Performance Computing, National University of Defense Technology, China† {p.wang, s.niknam, t.p.stefanov}@liacs.leidenuniv.nl∗ {pengwang, zywang}@nudt.edu.cn† ABSTRACT The power gating technique is an eﬀective way to reduce the high static power consumption in a Network-on-Chip (NoC). However, with notable wakeup delay, the power gating technique incurs signiﬁcant packet latency increase. In this paper, we propose a novel Duty Buﬀer (DB) structure and an eﬃcient DB-based power gating scheme to overcome this drawback. By keeping minimal number of DB active to replace any sleeping virtual channel in a router, our approach can eﬃciently reduce the packet latency increase along the whole routing path. Compared with a conventional ﬁvestage pipeline router without power gating, our approach, with only one ﬂit depth of the DB, increases the average packet latency by only 9.67%, which is much less than 57% and 21.75% latency increase in related approaches. With small hardware overhead, our approach can save on average 52.19% of the total power consumption in a NoC, which is comparable with 59.39% and 57.05% power savings in related approaches. CCS Concepts •Hardware → Network on chip; Keywords Network-on-Chip; Power Gating; Low latency 1. INTRODUCTION A Network-on-Chip (NoC) communication infrastructure with its high scalability and low latency is widely applied in many-core systems [6]. However, the NoC accounts for a large part of the total power consumption in many-core systems. For example, the NoCs in Teraﬂop [10] and Scorpio [9] consume up to 28% and 19% of the total system power consumption, respectively. Due to the low average traﬃc load of real applications [7], most of the NoC power consumption Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and /or a fee. Request permissions from permissions@acm.org. NOCS ’17, October 19–20, 2017, Seoul, Republic of Korea c⃝ 2017 ACM. ISBN 978-1-4503-4984-0/17/10. . . $15.00 DOI: https://doi.org/10.1145/3130218.3130220 is contributed by the static power consumption of idle components. Even, under the minimal resource conﬁguration, a NoC router static power consumption is still about 64% [4] of the total NoC power consumption. As a consequence, the high proportion of the static power consumption in a NoC makes it hard to meet the power budget for future many-core systems [3]. Power gating is a promising technique to reduce the static power consumption by powering oﬀ the idle components. However, there is a notable wakeup delay to power on the powered-oﬀ components during the wakeup process. When power gating is applied on a NoC, this wakeup delay, about 6-12 clock cycles [4], will interrupt the close cooperation between routers and will block the routing path for a while. As a consequence, the packet latency over the whole routing path dramatically increases and the NoC performance is degraded. Several works [17, 13] try to reduce the packet latency increase caused by the power gating. As the drowsy SRAM has only 2 clock cycles wakeup delay, Zhan [17] uses the drowsy SRAM to build virtual channels (VCs) in a NoC router and implements a ﬁne-grained power gating on virtual channels. In this way, the wakeup process becomes faster. By sending a wakeup signal ahead of the packet injection, Matsutani [13] switches on the power of the powered-oﬀ routers earlier, thereby hiding part of the wakeup delay. These approaches [17, 13] can reduce or hide part of the wakeup delay in a single wakeup process. But a packet may experience multiple wakeup processes and accumulate large delay along the routing path. In fact, as the average traﬃc load of real applications is low [7], there is high probability for packets to experience multiple wakeup processes. Furthermore, with more cores integrated on a chip in future many-core systems, this cumulative wakeup delay becomes much higher. In order to further reduce the packet latency increase caused by the power gating, in this paper, we propose a novel and ﬂexible hardware structure, called Duty Buﬀer (DB). Based on the DB structure, we propose a novel ﬁnegrained DB-based power gating scheme to reduce the static power consumption of the VCs in routers. By using our DB to temporarily replace a powered-oﬀ VC and accept packets, an upstream router does not need to block packets while waiting for the VC in a downstream router to completely wake up. Thus, we can eﬃciently reduce the packet latency increase. Furthermore, as all VCs in the same input port of a router share the same DB, we can keep a minimal number of DBs powered on (on duty) and power oﬀ all of the VCs. In this way, we use minimal static power consumption to keep a certain transmission ability, which is helpful to reduce the static power consumption. The speciﬁc novel contributions of this paper are the following: • We propose a novel Duty Buﬀer structure, which can be used to replace any VC in a router. Taking the advantage of our novel DB, we propose a novel DB-based power gating scheme on VCs in a router. This scheme eﬃciently reduces the packet latency increase caused by the power gating. By keeping a minimal number of DBs powered on, our DB-based scheme also achieves a signiﬁcant reduction of the static power consumption in a NoC. • By experiments, we show that our DB-based approach can eﬀectively reduce the packet latency increase caused by the power gating. Taking a conventional router without power gating as the baseline, our DB-based approach, with one ﬂit depth of DB, increases the average packet latency only by 9.67%, which is much less than the 57% latency increase in [13] and 21.75% in [17]. With only 5.76% hardware overhead, our DBbased approach can save on average 52.19% of the total power consumption in a NoC, which is comparable with the 59.39% saving in [13] and 57.05% in [17]. The remainder of the paper is organized as follows: Section 2 gives an overview of the related work. Section 3 gives some background on the packet transmission process in a NoC. Section 4 elaborates on our DB structure, and the DB-based power gating scheme. Section 5 introduces the experimental setup and shows the results. Section 6 concludes this paper. 2. RELATED WORK In this section, we discuss the related works on reducing the packet latency increase caused by power gating in a NoC. As VCs are the main source of the static power consumption in a router, Zhan [17] applies the power gating on the VCs and uses the drowsy SRAM to build the VC because the wakeup delay of the drowsy SRAM is only two clock cycles, thus the wakeup process is much faster. However, this approach cannot completely remove the wakeup delay and packets accumulate large wakeup delay along the whole routing path. In contrast, our approach keeps a certain transmission ability of routers when the VCs are powered oﬀ. The packets are not blocked by the powered-oﬀ VCs in the routers. Thus, even though a packet experiences multiple wakeup processes along the routing path, the packet will not accumulate large wakeup delay. Therefore, our approach can reduce more eﬃciently the packet latency increase. Based on the principle of the look-ahead routing, Matsutani [13] proposes a runtime power gating approach. By sending a wakeup signal into the next router ahead of the packet injection, this approach hides a few clock cycles in the wakeup process and wakes up the powered-oﬀ router earlier. However, the number of hided clock cycles is determined by the number of the router pipeline stages, and is insuﬃcient to cover the entire wakeup delay. By sending the wakeup signal to the rest of the routers along the routing path, Chen [4] improves the approach in [13]. In this way, the powered-oﬀ routers can be waked up much earlier and this approach almost achieves non-blocking power gating in deterministic routing. However, this approach highly depends on the correct prediction of the routing path. If the routing path of a packet is adaptively changed, this approach may not ﬁnd the correct routers to power on. Compared with [13, 4], the power gating mechanism in our approach does not need the extra wakeup or sleeping control signal between routers. By keeping a certain transmission ability of routers when the VCs are powered oﬀ, routers in our DB-based approach still can transfer packets during the wakeup process, which can be eﬃciently used to signiﬁcantly reduce the overall packet latency increase in both deterministic routing and adaptive routing. Kim in [12] proposed a Flexibuﬀer scheme to reduce the static power consumption of the VC buﬀers. Based on the buﬀer occupancy rate, the router predictively powers on the sleeping buﬀers in the VCs in advance. In this way, the negative impact of the power gating on the packet latency increase is reduced. However, the scheme in [12] can reduce the static power consumption, only when the VC size is large enough. This is because each VC in this approach has to keep at least bmin = max(Nwakeup , Ncrt ) (Nwakeup and Ncrt are the number of clock cycles of the wakeup delay and credit round-trip delay, respectively) buﬀers powered on at run-time, which makes the Flexibuﬀer scheme not very eﬃcient in reducing the static power consumption. Furthermore, considering the high wakeup delay, 6-12 clock cycles [4], for NoCs with small size of VCs, such as the NoCs in [9, 17, 4, 16, 8], the scheme in [12] cannot save any power consumption. Compared with the scheme in [12], the DB in our approach is shared with all VCs in the same input port, and the minimal DB size is one ﬂit. In this way, our approach can be widely and eﬃciently used to reduce the static power consumption in NoCs with small and large size of VCs. 3. BACKGROUND In order to understand our approach in Section 4, we provide some background information on the packet transmission process between routers in a NoC. Figure 1(a) shows a 2-D mesh NoC with a virtual-channeled wormhole router. The router consists of input ports, a routing computation (RC) unit, a virtual channel allocation (VA) unit, a switch allocation (SA) unit, a crossbar, and output ports. A packet transmission needs close cooperation between routers. An input port receives packets from an upstream router or the network interface and stores them into the corresponding input VC. The RC unit determines the output port (transfer direction), which a packet should go through. After that, the VA unit selects an available input VC in the downstream router connected to the determined output port, and assigns it to the packet. The VC address will be put in the head ﬂit of the packets. After the SA unit grants a packet to use the crossbar, the packet can be sent to the downstream router. When the head ﬂit of the packet reaches the downstream router, the input channel decodes the VC address from the head ﬂit and stores the head ﬂit and the following ﬂits into the corresponding input VC. In order to avoid buﬀer overﬂow of input VCs, creditbased ﬂow control is widely used. As shown in Figure 1(a), the output port contains a credit counter. Each credit stands for an unoccupied buﬀer space in the downstream router. When a ﬂit of the packet leaves the router, the credit counter Virtual channels RC,  VA, SA Flow Control Credit Power Gating Virtual Channels Input  controller R00 R01 R02 Credit count  Output port 0  Flit Input channel  R10 R11 R12 Input port 0  Crossbar R20 R21 R22 Input port 4  Output port 4  Network  Router Input controller Isolate  input to VC Duty Buffer Isolate  output  from VC Crossbar credit  count Filt Credit  Crossbar To SA, VA VC0 valid VCn valid RC,  VA, SA SA  Output  controller (a) Architecture of a NoC (b) Extended input port (c) Extended output port Figure 1: The NoC structure and extended input port and output port. in the output port of the router decreases the credit to indicate that a buﬀer space in the downstream router is occupied. At the same time, the router will send a credit to its upstream router to indicate that the input VC in the router releases a buﬀer space. The input VCs play a key role in the packet transmission process between routers. A larger number of VCs makes a contribution on eliminating the head of line (HOL) blocking and enhancing the NoC performance. By providing enough buﬀers, a deep VC also can cover the impact of the credit round-trip delay between routers. However, VCs cause a signiﬁcant static power consumption, which accounts for over 80% [5] of the static power consumption of routers. So, reducing the static power consumption caused by VCs can eﬃciently decrease the power consumption of a NoC. 4. OUR DB-BASED APPROACH The key idea of this paper is, by always having a small number of buﬀers powered on (on duty) in VCs, to keep a certain transmission ability of a router in the wakeup process when power gating is used. In this way, we can reduce the packet latency increase caused by the power gating. In order to achieve this goal, we have to overcome the following challenges. • Which VCs should be on duty? Based on the discussion in Section 3, we know that the input VC address of a packet, which indicates where the packet will be stored, is determined in an upstream router. Therefore, a downstream router does not know when packets will arrive or which VCs will be occupied. As a consequence, it is unknown which VC should be on duty in a downstream router. • How to use as few buﬀers on duty as possible? Keeping fewer buﬀers on duty in a VC is helpful to reduce the static power consumption in a NoC. However, the role of the VCs is not the same. Diﬀerent classes of VCs are used to receive diﬀerent packets. For example, the MESI [14] coherence protocol needs at least two diﬀerent data VCs and one control VC to prevent a deadlock. If we want to keep some buﬀers in VCs on duty to reduce the packet latency increase, each VC class should have buﬀers on duty and ready to receive packets. As a consequence, we cannot eﬃciently reduce the static power consumption. 4.1 Input Port with Duty Buffer In order to overcome the challenges, mentioned above, we propose the novel Duty Buﬀer structure shown in Figure 1(b) to extend input ports of a virtual-channeled wormhole router. The following components are added to an input port: Input controller, Duty Buﬀer, and multiplexers. We apply power gating on the VCs as well. The Duty Buﬀer (DB) is a small buﬀer queue. The minimal size of the DB is one ﬂit. It is always powered on and ready to receive packets at run-time. For a downstream router, when a packet reaches an input port, but input VCs are powered-oﬀ or waking up, the input controller controls the input channel and demultiplexer to store the packet into the DB. The DB replaces the corresponding powered-oﬀ VC, which the packets should go into. When the router tries to read the packets from the poweredoﬀ VC, the input controller controls the corresponding multiplexer to select reading the packets from the DB. By replacing the powered-oﬀ VC with DB, the router can transfer the packet as if this VC was powered on. For an upstream router, as its corresponding downstream router can use the DB to replace the powered-oﬀ VCs to store packets, there is no need for the upstream router to block the packet and to wait for the VCs in the downstream router to completely wakeup. Thus, the upstream router still can send a packet to the downstream router, when the VCs in the downstream router are powered-oﬀ or waking up. As a result, the packet latency increase caused by the power gating on VCs is reduced. Since the DB can replace any powered-oﬀ VC, we do not need to determine which VC should be on duty. Furthermore, as all VCs in an input port share the same DB, we do not need to keep powered-on buﬀers for each class of VCs. Thus, we can keep as few buﬀers on duty in the DB as possible, and power oﬀ as many buﬀers in VCs as possible. In this way, we keep a certain transmission ability with minimal static power consumption. 4.2 Power Gating on VCs In our extended input port, as shown in Figure 1(b), the input controller uses one switch to control the power of all VCs in the port. This is because powering on all VCs at the same time is beneﬁcial to guarantee the NoC performance. That is, the traﬃc load in many-core systems is bursty [7], so routers tend to use a larger number of VCs in an input port at the same time. On the other hand, the average traﬃc load is very low [7]. This makes higher percentage of input VCs idle during the applications execution. So, simultaneously powering oﬀ all VCs also can reduce signiﬁcantly the static power consumption. The rest of the components in a router, for instance, the RC unit, the VA unit, the SA unit, the crossbar and the output ports, are always powered on. In this way, if VCs in some input ports are powered oﬀ, the other input ports still can normally work. Packets will not be blocked in the router pipeline. Furthermore, compared with VCs, the power consumption in the rest of the router’s components is much lower [5]. Always keeping them powered on, routers will not waste much static power consumption. p u e k a w T (cid:32) (cid:33) 4.3 Power Gating Scheme In order to correctly use our DB to reduce the latency increase during the waking up process, we have to achieve the following goals: • Keeping the ﬂits order in a packet. The ﬂits of a packet may be separately stored in the DB and a VC when the state of VCs switches from waking up state to charging complete state. In order to keep the packet transmission correct, the transmission order of the ﬂits in the same packet must not be changed. • No deadlock occurs. In general, diﬀerent class packets are allocated with diﬀerent VCs. It is because that a network interface has to ﬁnish processing the reply packet ﬁrst, then to deal with the new request packet. If a request packet and a replay packet are stored in the same buﬀer queue (FIFO), the request packet may be stored in the front of the reply packet and stalls the reply packet to access the network interface. As a consequence, the request packet and the replay packet block each other and the deadlock occurs [6]. Therefore, in the wakeup process, we have to guarantee that only packets with the same VC address enter the input port to prevent deadlock occurrence. This VC address should be determined by the head ﬂit of the packet, which wakes up the powered-oﬀ VCs in the downstream router. In order to achieve the above mentioned goals, the input controller and the output controller are added to a router, as shown in Figure 1(b) and Figure 1(c). In the following subsections, we introduce the working mechanisms of the input controller and the output controller. 4.3.1 Input controller In the extended input port, as shown in Figure 1(b), the input controller monitors the VC control to determine if it can switch oﬀ the power of VCs. The states of the input controller are shown in Figure 2(a). In the active state, VCs in the input port are powered on. The input controller controls the input channel to inject packets into the corresponding VCs and keeps the DB idle. When all VCs and the DB are empty and the packet transmission is completed (the tail ﬂit of the packet has left), the input port controller moves to the ready state and waits for Tidle detect clock cycles to switch oﬀ the power of VCs. We use the equation, Tidle detect = Tcredit delay + Tf lit delay , to compute the Tidle detect , where Tcredit delay and Tf lit delay are the credit and ﬂit transmission delay between the neighbor routers, which are determined by design parameters. waking  up coming packet sleeping p u e k a w T (cid:32) (cid:33) active  all VC idle transmission  finish coming packet holding on  s e n p a d c i n k g e t (cid:33)(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) T i d l e _ d e t wakeup T (cid:33)(cid:32) e c t ready active credit full transmission  finish catching a. input port state (a) Input controller states b. output port state (b) Output controller states Figure 2: Controllers for input and output ports. In the ready state, the VCs are still powered on and can be used to store packets. If there are incoming packets in the Tidle detect period, the packets will be stored to the corresponding VC and the input controller returns back to the active state. The ready state is used to avoid unnecessary power gating activities when the idle time of VCs is short. In the sleeping state, the power of all VCs is switched oﬀ and VCs cannot be used to store packets. Once the head ﬂit of a packet comes into the input port, it will be stored in the DB and the power of VCs will be switched on to charge the circuit. The input port goes into the waking up state. In the waking up state, VCs are not stable and cannot accept packets. The incoming packets still are stored in the DB. After Twakeup clock cycles, the VC charge is completed, and the state changes to active state and the VC can be used to store packets. The input controller stores the packets to the corresponding VC and stops injecting packets into the DB. At the beginning of the active state, if there are any ﬂits left in the DB, the input controller keeps replacing the output of the corresponding VC with the output of the DB until the DB is empty. In this way, the order of the ﬂits in a packet will not be disturbed and the correct transmission is guaranteed. 4.3.2 Output controller As explained in the beginning of Section 4.3, to guarantee deadlock-free packet transmission during the wakeup process, an output port in a router has to send only packets with the same VC address to the downstream router. In order to achieve this, the output controller and some AND gates are added to the output port, as shown in Figure 1(c). In this extended output port, the output controller monitors the credit counter and the grant to use the crossbar from the SA, to identify the state of the input port in the downstream routers and controls the validation signals for the VA and SA, indicating which input VCs in the downstream router are available. The states of the output controller are shown in Figure 2(b). In the active state, the credit counter is not full, or the packet transmission is not completed (the tail ﬂit has not left). This indicates that the corresponding (the VCs) input port in the downstream router is powered on. So, the upstream router can normally send packets and receive credits. When the credit counter is full and packet transmissions   ) s e l c y c ( y c n e t a L t e k c a P e g a r e v A 50 40 30 20 10 0 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ P P P P P P P P P P P P P P P P P P P P P G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G O A O A O A O A O A O A O A D D D D D D D N B B B N B B B N B B B N B B B N B B B N B B B N B B B D D D D D D D D D D D D D D D D D D D D D S P P P L S P P P L S P P P L S P P P L S P P P L S P P P L S P P P L blackscholes bodytrack facesim swaption fluindanimate raytrace average Figure 3: The average packet latency. Table 1: Parameters topology router pipeline network size VC count buﬀer depth packet size Tcredit delay Tf lit delay Tbreakeven Twakeup 2D tours 5 stage 4 × 4 4 VCs per port 4 ﬂits per VC 1 or 8 ﬂits, 8B/ﬂit 1 clock cycles 1 clock cycles 10 clock cycles 10 clock cycles are completed, the output controller moves to the catching state. In the catching state, the output controller considers that the VCs of the corresponding input port in the downstream router are powered-oﬀ, even if, at this time, this input port may be in the ready state. The output port allows the VA to normally allocate VCs and SA to grant the use of the crossbar in the catching state. Once a head ﬂit of a packet is granted by the SA to use the crossbar, the output controller marks the allocated VC address and moves to holding on state. In the following Twakeup clock cycles, only packets allocated to this marked VC address are allowed to use the crossbar and transferred to the downstream router. In the holding on state, the output controller assumes that all the ﬂits sent are stored in the DB of the input port in the corresponding downstream router. The output controller guarantees that the used credits does not exceed the depth of the DB. In this way, the output controller can guarantee that there is no buﬀer overﬂow in the wakeup process. After Twakeup clock cycles, the output controller moves to the active state. The VA and SA can normally allocate router resource for the packets. 5. EXPERIMENTAL RESULTS In order to evaluate our approach in terms of performance and power consumption, we have implemented our approach on the cycle-accurate interconnection network simulator Booksim2.0 [11]. The parameters set in Booksim2.0 are shown in Table 1. Each input port of a router has four 4-ﬂit depth VCs. The total number of buﬀers in a router is consistent with [8]. Based on the prior work in [8], Twakeup is set to 10 clock cycles. To calculate the power consumption cost of power gating when the power of VCs is switched on and oﬀ, we set the Tbreakeven , which is the number of sleep cycles required to compensate the overhead for charging VCs. The Tbreakeven is 10 clock cycles and consistent with the prior work in [17] For comparison purpose, we have implemented the following schemes in Booksim2.0: (1) NO PG is the baseline NoC. It uses a conventional ﬁve-stage pipeline router without power gating; (2) LA PG is a NoC using the lookahead scheme [13] to hide 5 clock cycles of the wakeup delay; (3) DS PG is a NoC where the VCs of a router are implemented by the drowsy SRAM [17]. In the active state, the drowsy SRAM has the same power consumption as the conventional SRAM. When staying in the drowsy state, it only consumes about 10% of the SRAM static power consumption, and has 2 clock cycles wakeup delay; (4) DB PG is a NoC using our DB-based approach, presented in Section 4, with diﬀerent depths of the DB. 5.1 Evaluation on Real Application Workloads In order to evaluate our DB-based approach and compare it with the other approaches mentioned above, on real workloads of applications, we use six applications from the Splash2 benchmark suit: blockscholes, bodytrack, facesim, swaption, ﬂuindanimate, and raytrace. We use Synfull [2] to capture the traﬃc behaviour of these applications. Synfull generates packets and feeds packets to Booksim 2.0 to evaluate the network performance. Based on the collected data from Booksim2.0, we use Dsent [15] to compute the power consumption under the 45nm technology and NoC frequency of 1GH z . 5.1.1 Effect on performance Figure 3 shows the average packet latency for the six different realistic application workloads. The seventh set of bars in Figure 3 gives the average result over these six applications. We use the average packet latency to measure the network performance. The LA PG hides 5 cycles of the wakeup delay by sending the wakeup signal in advance. However, compared with the baseline NO PG, it still incurs an average of 57% (about 16 cycles) packet latency increase throughout these six applications. The latency in the DS PG also increases by 21.75% (about 6 cycles) on average, even though the wakeup delay in the DS PG is only 2 clock cycles. These results indicate that packets experience over three wakeup processes on average and the cumulative wakeup delay is the main reason for the performance degradation. In contrast, the average packet latency in our DB PG only increases by 9.67%, 5.67%, and 2.02% with 1, 2, and 3 ﬂits depth of the Duty Buﬀer, respectively. Compared with the LA PG and the DS PG, our DB PG approach, with only one ﬂit depth of the DB, achieves 47.33% and 12.08% less         ) t t a w ( r e w o P k r o w t e N 0,4 0,35 0,3 0,25 0,2 0,15 0,1 0,05 0 dynamic static G G P P _ _ O A N L 1 2 3 _ _ _ P P _ _ S P P P _ _ _ G G G G G O D N B B B D D D G G P P _ _ S L A D 1 2 _ _ G G P P _ _ B B D D 3 1 _ _ P P P _ _ _ P S P _ L _ G G G G G O A N D B B D D 2 3 _ _ G G P P _ _ B B D D G G P P _ _ O A N L 1 2 3 _ _ _ P P _ _ S P P P _ _ _ G G G G G O D N B B B D D D G G P P _ _ S L A D 1 2 _ _ G G P P _ _ B B D D 3 1 2 3 _ _ _ _ P P P _ _ _ P S P P P _ _ _ _ L G G G G G G G O A N D B B B B D D D D G G P P _ _ O A N L G G G G 1 2 3 _ _ _ P _ S P P P _ _ _ D B B B D D D blackscholes bodytrack facesim swaption fluindanimate raytrace average Figure 4: The breakdown of the NoC power consumption. Table 2: Area overhead input port (×5) crossbar+VA+SA output port (×5) total (um2 ) NO PG 91914.734088 6076.4996 6595.446 104586.679688 DB PG 1 97289.549462 6076.4996 7241.8025 110607.851562 DB PG 2 99265.1354 6076.4996 7241.8025 112583.437500 DB PG 3 102494.10415 6076.4996 7241.8025 115812.406250 packet latency increase, respectively. This is because our DB can replace any sleeping VC in the wakeup process to store packets. The upstream routers do not need to block packets and wait for the VCs to completely waked up. Furthermore, powering on all of the VCs in an input port at the same times can eﬀectively deal with the bursty traﬃc load of real applications. The depth of our DB also has an obvious eﬀect on the network performance, as shown in Figure 3. Since we use a ﬁve-stage pipelined router, there is a notable pipeline delay and credit round-trip delay in the router. In our DB PG with the smallest DB (DB PG 1 in Figure 3), the packet transmission may be intermittently interrupted because of the credit round-trip delay between routers. In addition, as the DB only can replace one VC in a signal wakeup process, packets with diﬀerent VC address may be blocked in the upstream routers, which also causes packet latency increase. 5.1.2 Effect on power consumption Figure 4 shows the breakdown of the NoC power consumption across the six benchmarks and the seventh set of bars shows the average over these six benchmarks. The network power is decomposed into dynamic and static power consumption. Compared with the baseline NO PG, the LA PG, and DS PG can reduce an average of 73.14%, and 68.83% of the static power consumption, respectively. The static power consumption in our DB PG with 1, 2, and 3 ﬂits depth of the DBs is reduced by 64.11%, 58.49% and 53.63% on average, respectively. For the total power consumption, compared with the baseline NO PG, the LA PG, and DS PG reduce the total NoC power consumption by 59.39%, and 57.05%, respectively. Our DB PG with diﬀerent DB depths reduces with 52.19%, 47.55%, and 45.14% the total power consumption, which is comparable to the LA PG and DS PG. It is obvious that we can simpily realize a non-block power gating scheme by combining the LA PG with the DS PG. However, in the sleeping state, the static power consumption of the drowse SRAM increases with the increase of the total number of buﬀers in a router. As a consequence, when a NoC has a large number of buﬀers, the drowse SRAMs still cause signiﬁcant static power consumption in the sleeping state. While, in our approach, the static power consumption in the sleeping state is only determined by the number of DBs, instead of the total number of buﬀers, which presents better scalability and is more suitable for a NoC with a larger number of buﬀers. 5.1.3 Effect on area overhead In order to evaluate the area overhead of our DB PG scheme, we use Synopsys Design Compiler to synthesize the routers used in our experiments under 45nm NanGate Open Cell Library [1]. The area overhead of each component is shown in Table 2. Compared with the baseline router used in the NO PG, the routers used in our DB PG 1, DB PG 2 and DB PG 3 cause about 5.76%, 7.65% and 10.73% area increases, respectively. Most of area overhead increase in our DB PG is contributed by the duty buﬀers and multiplexers in the input ports, while the area overhead of the input controller is very low. This is because the input controller is made up of simple logic circuits and a small number of registers, which does not cause large area overhead. As our DB PG has no inﬂuence on the crossbar, VA and SA, there is no area overhead increase on these components. Compared the area overhead of the output ports in the baseline router, the output ports in our DB PG causes 9.8% area overhead increase. However, these area overhead increase takes neglectable percentage of the total router area, because the structure of the output port controller is simple logic circuits and only contains a small number of registers. 5.2 Evaluation on Synthetic Workloads In order to further explore our DB PG behaviour under a wider range of packet injection rates, in this section, we evaluate the performance of NO PG, LA PG, DS PG and our DB PG under synthetic traﬃc patterns. Booksim2.0 provides abundant synthetic traﬃc patterns. We select four synthetic traﬃc patterns: 1) uniform random: packets’ destinations are randomly selected; 2) transpose: packets from source node (x, y) are sent to destination node (y , x); 3) bitcomp: packets from (x, y) are sent to (N − x, N − y), N is the number of nodes in the X and Y dimensions of a NoC; 4) tornado: packets from (x, y) are sent to (x + N 2 − 1, y).       80 70 60 50 40 30 20 10 0 0 0,2 0,4 0,6 0,8 1 A e g a r e v c a p k e t l a t e n y c ( c y c l ) s e Injection rate (flits/node/cycle)  NO_PG LA_PG DS_PG DB_PG DB_PG ) ) s s e DB_PG DB_PG ) s (a) uniform 80 70 60 50 40 30 20 10 0 0 0,1 0,2 0,3 0,4 0,5 A e g a r e v c a p k e t l a t e n y c ( c y c l Injection rate (flits/node/cycle)  NO_PG LA_PG DS_PG DB_PG DB_PG (b) transpose 80 70 60 50 40 30 20 10 0 0 0,2 0,4 0,6 0,8 A e g a r e v c a p k e t l a t e n y c ( c y c l ) s e Injection rate (flits/node/cycle)  NO_PG LA_PG DS_PG DB_PG DB_PG ) ) s s e (c) bitcomp 80 70 60 50 40 30 20 10 0 0 0,2 0,4 0,6 0,8 A e g a r e v c a p k e t l a t e n y c ( c y c l Injection rate (flits/node/cycle)  NO_PG LA_PG DS_PG DB_PG (d) tornado Figure 5: Packet latency across full range of workloads. Figure 5 shows the average packet latency curves with diﬀerent injection rates under the diﬀerent traﬃc patterns. Since DB PG with diﬀerent depths of the DB has similar trend in terms of the average packet latency curve, in order to clearly show the experimental results, in Figure 5, we only show our DB PG with DB of depth 1. The zero-load latency is an important performance indicator for a NoC. As shown in Figure 5, when the injection rate is almost zero (the injection is about 0.01), where the packet latency is close to the zero-load latency, our DB PG has less packet latency than LA PG and DS PG. This is because, when the injection rate is low, most of the input ports of routers are idle. A packet experiences multiple wakeup processes along the routing path. As a consequence, a packet in LA PG and DS PG accumulates large wakeup delay. In contrast, by keeping a certain transmission ability in the wakeup process, our proposed DB PG can avoid the situation where a packet accumulates too much latency. With the injection rate increasing from 0 to about 0.2, as shown in Figure 5, our DB PG still has less packet latency increase than LA PG and DS PG. However, the packet latency in our DB PG slowly increases, while the packet latency in LA PG and DS PG decreases. This is because, with the injection rate increasing, more and more input ports of routers are always busy and cannot be powered oﬀ. The probability of accumulating a larger wakeup delay becomes lower. On the other hand, multiple packets may compete for a transfer to a downstream router in the same wakeup process, but the DB only can replace one VC in a signal wakeup process in order to avoid deadlock occurrence. As a consequence, in the wakeup process, some of the packets are blocked in our DB PG. When the injection rate further increases and is close to the saturation injection rate where the packet latency sharply increases, as shown in Figure 5(a)(b), the packet latency in the LA PG and DB PG is higher than that in DS PG, because, in this situation, the wakeup delay in a single wakeup process becomes the main reason for the packet latency increase. The LA PG and DB PG have higher wakeup delay than DS PG. However, because of the DB structure, our DB PG outperforms LA PG. Based on the results of the synthetic traﬃc patterns, when the injection is below 0.2, our DB PG can eﬃciently prevents a packet accumulating large wakeup delay along the routing path and achieves better performance than LA PG and DS PG. However, when the injection rate is close to the saturation injection rate, in some traﬃc patterns, our DB PG causes a little bit performance loss in terms of the average packet latency. Considering the injection rate of the real application [7] is much lower than 0.2, our DB PG can be widely used to reduce the packet latency increase caused by the power gating. 6. CONCLUSIONS In this paper, we proposed a novel Duty Buﬀer structure and DB-based power gating scheme to reduce the packet latency increase caused by power gating in a NoC. As the DB can replace any VC in the input port, the input port can power oﬀ all VCs without the need to consider which VC will be used in the future. In this way, we can keep minimal number of buﬀers “on duty” to reduce the packet latency increase caused by the power gating, and power oﬀ most of the VCs to reduce the static power consumption. The experimental results show that our approach outperforms the lookahead and drowsy SRAM approaches. With a small                                         [13] H. Matsutani, M. Koibuchi, D. Wang, and H. Amano. Run-time power gating of on-chip routers using look-ahead routing. In Proceedings of the 2008 Asia and South Paciﬁc Design Automation Conference, pages 55–60. IEEE Computer Society Press, 2008. [14] M. S. Papamarcos and J. H. Patel. A low-overhead coherence solution for multiprocessors with private cache memories. ACM SIGARCH Computer Architecture News, 12(3):348–354, 1984. [15] C. Sun, C.-H. O. Chen, G. Kurian, L. Wei, J. Miller, A. Agarwal, L.-S. Peh, and V. Sto janovic. Dsent-a tool connecting emerging photonics with electronics for opto-electronic networks-on-chip modeling. In Networks on Chip (NoCS), 2012 Sixth IEEE/ACM International Symposium on, pages 201–210. IEEE, 2012. [16] Y. Yao and Z. Lu. Dvfs for nocs in cmps: A thread voting approach. In High Performance Computer Architecture (HPCA), 2016 IEEE International Symposium on, pages 309–320. IEEE, 2016. [17] J. Zhan, J. Ouyang, F. Ge, J. Zhao, and Y. Xie. Dimnoc: A dim silicon approach towards power-eﬃcient on-chip network. In 2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC), pages 1–6. IEEE, 2015. amount of additional hardware overhead, our DB-based approach can eﬃciently reduce the static power consumption, which is comparable with the lookahead and drowsy SRAM approaches. 7. ACKNOWLEDGEMENT This work is partially supported by National Natural Science Foundation of China (No. 61672526, 61572508). 8. "
System-Level Design of Networks-on-Chip for Heterogeneous Systems-on-Chip.,"The network-on-Chip (NoC) is a critical subsystem for many large-scale systems-on-chip (SoC). We present a complete framework for the design and optimization of NoCs at the system-level. By combining a library of pre-designed configurable NoC modules specified in SystemC with high-level synthesis, we can generate a variety of alternative 2D-Mesh NoC architectures for a given SoC. We also support the automatic synthesis of network interfaces to translate between IP-specific messages and NoC flits. We demonstrate our approach with the design-space exploration of two complete SoCs running complex applications on a high-end FPGA board.","System-Level Design of Networks-on-Chip for Heterogeneous Systems-on-Chip Young Jin Yoon∗ Department of Computer Science Columbia University New York, New York youngjin@cs.columbia.edu (Invited Paper) Paolo Mantovani Department of Computer Science Columbia University New York, New York paolo@cs.columbia.edu Luca P. Carloni Department of Computer Science Columbia University New York, New York luca@cs.columbia.edu ABSTRACT The network-on-Chip (NoC) is a critical subsystem for many largescale systems-on-chip (SoC). We present a complete framework for the design and optimization of NoCs at the system-level. By combining a library of pre-designed con(cid:27)gurable NoC modules speci(cid:27)ed in SystemC with high-level synthesis, we can generate a variety of alternative 2D-Mesh NoC architectures for a given SoC. We also support the automatic synthesis of network interfaces to translate between IP-speci(cid:27)c messages and NoC (cid:30)its. We demonstrate our approach with the design-space exploration of two complete SoCs running complex applications on a high-end FPGA board. CCS CONCEPTS • Networks → Network on chip; Network components; • Hardware → Network on chip; KEYWORDS Network-on-Chip, System-Level Design, Synthesizable SystemC ACM Format: Young Jin Yoon, Paolo Mantovani, and Luca P. Carloni. 2017. System-Level Design of Networks-on-Chip for Heterogeneous Systems-on-Chip. In Proceedings of NOCS’17, Seoul, Republic of Korea, October 19–20, 2017, 6 pages. https://doi.org/10.1145/3130218.3130238 1 INTRODUCTION Networks-on-chip (NoC) play a critical role in the integration of components in large-scale systems-on-chip (SoC) at design time, and have a major impact on their performance at run time. Over the last few years, the research community has produced many different frameworks and tools for NoC design and optimization [7, 14, 16, 17]. Most of these approaches provide some degree of parameterization which allows designers to optimize the NoC architecture for the target SoC and the given ASIC or FPGA technology. We leveraged this aggregate research experience for the development of ICON (Interconnect Customizer for the On-chip Network). ∗ Young Jin is now with Intel Corporation, Hillsboro, OR. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro(cid:27)t or commercial advantage and that copies bear this notice and the full citation on the (cid:27)rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner /author(s). NOCS’17, October 19–20, 2017, Seoul, Republic of Korea © 2017 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4984-0/17/10. https://doi.org/10.1145/3130218.3130238 Components Parameters Input/Output units Virtual channel (VC) allocator Switch (SW) allocator Allocator unit Router Physical network Queue size, Number of VCs Input/output-(cid:27)rst, wavefront VC and output-(cid:27)rst, wavefront Independent, speculative RC/SA/VA/ST/VT pipelined Flit width, topology Subcomponents Routing unit, (cid:30)ow-control unit Input/Output arbiters VC arbiters, output arbiters VC/SW allocators Input/Output units allocator unit, crossbar Routers, channels, network interfaces (NI) Table 1: NoC parameters and sub-components in ICON. ICON is a new framework for the design and optimization of NoCs at the system level. Some of its distinguished features include: support for virtual channels for message-class isolation, which is critical for the prevention of protocol deadlock [20], the ability to generate NoC architectures that combine multiple physical networks with multiple virtual channels [23], and the ability to explore the NoC design space by varying the NoC parameters in a non-uniform way (e.g. to have di(cid:29)erent numbers of virtual channels per input port in a router [9]). The generation of NoCs with ICON relies on a rich library of parameterized components that can be combined in a modular way to create complex NoC subsystems and, ultimately, a complete NoC architecture tailored to the target SoC. Table 1 reports a list of the key components that can be used to generate a variety of router micro-architectures. ICON promotes system-level design as it allows the automatic generation of NoC architectures speci(cid:27)ed in SystemC. These generated speci(cid:27)cations can be integrated with full-system simulators, known as virtual platforms, as well as synthesized with highlevel synthesis (HLS) tools to produce corresponding RTL implementations. Make(cid:27)les and scripts for synthesis, simulation, and cosimulation across various levels of abstraction are automatically generated along with the SystemC source code. By bringing the description of the NoC to a higher level, ICON enables the exploration of a broader design space through the combination of system-level parameters with micro-architectural settings for the HLS tool. Also, the compatibility with virtual platforms allows fast full-system simulation, which is crucial to increase the number of design points that can be evaluated. After summarizing the most related NoC research in Section 2, we present the overall architecture of ICON and its unique features in Section 3. Then, in Section 4 we demonstrate some of the capabilities of ICON by generating 36 di(cid:29)erent NoC con(cid:27)gurations that can be seamlessly integrated in two SoCs, which we designed and NOCS’17, October 19–20, 2017, Seoul, Republic of Korea Y. Yoon et al. XML Spec. P a r s e r C o n f i g u r a t i o n Script  Generator NoC Component Generator Testbench Generator Simulation Makefile ESL Synthesis Script RTL Synthesis Script Customized  SystemC NoC SystemC NoC Library RTL NoC Netlist NoC High-Level Synthesis Logic Synthesis SystemC Simulation RTL Co-simulation Netlist Co-simulation Customized Testbench Testbench Comp. Library SystemC ESL Sim Verilog RTL Co-sim Verilog Netlist Co-sim Figure 1: The ICON synthesis and simulation (cid:30)ows. implemented on an FPGA board. We present a comparative analysis of the resources utilization and performance evaluation across these NoC con(cid:27)gurations for the two SoC designs while running real workloads. We also report estimates on area occupation and throughput for a corresponding ASIC implementation tested with synthetic tra(cid:28)c patterns. 2 RELATED WORK How to design low-latency and high-bandwidth architectures by combining (cid:30)exible and con(cid:27)gurable parameterized components has been the focus of many papers in the NoC literature. Mullin et al. proposed low-latency virtual-channel routers with a free virtual channel queue and VA/SA speculation that o(cid:29)er a high degree of design (cid:30)exibility in SystemVerilog [14]. Kumar et al. demonstrated a 4.6Tbits/s 3.6GHz single-cycle NoC router with a novel switch allocator scheme that improves the matching ef(cid:27)ciency by allowing multiple requests per clock cycle and keeping track of previously con(cid:30)icted requests [11]. Becker presented a state-of-art parameterized virtual channel router RTL with a new adaptive backpressure mechanism that improves the utilization of the router input bu(cid:29)ers [3]. Dall’Osso et al. developed ×pipes as a scalable and high-performance NoC architecture, where parameterizable SystemC component speci(cid:27)cations are instantiated and connected to create various NoCs [5]. Stergiou et al. improved this architecture by presenting ×pipes Lite, a synthesizable parameterizable NoC component library that includes OCP 2.0 compatible network interfaces, and by providing a companion synthesis and optimization (cid:30)ow [22]. Fatollahi-Fard et al. developed OpenSoC Fabric [7], a tool that simpli(cid:27)es the generations of NoCs from parameterized speci(cid:27)cation by leveraging the properties (abstract data types, inheritance, etc.) of Chisel hardware description language [1]. A large portion of NoC research focused on FPGAs. Lee et al. analyzed the performance sensitivity to various NoC parameters for FPGA-based NoCs [12]. Kapre et al. presented a detailed analysis of packet-switch vs time-multiplexed FPGA overlay networks [10]. Schelle et al. presented NoCem, an architecture based on composing simple router blocks to build large NoCs on FPGAs [18]. Hilton et al. proposed PNoC, a (cid:30)exible circuit-switched NoC for FPGAbased systems [8]. Shelburne et al. proposed MetaWire to emulate a NoC on FPGAs [19]. Lu et al. presented a cost-e(cid:29)ective lowlatency NoC router for FPGA [13]. Papamichael et al. developed the CON(cid:27)gurable NEtwork Creation Tool (CONNECT) [17] that combines Bluespec SystemVerilog [15] and a web-based front-end to generate a fast FPGA-friendly NoC based on a simple but (cid:30)exible fully-parameterized router architecture. In developing ICON we kept in mind the lessons from many of these works. Given the common emphasis on system-level design, our work has perhaps most commonalities with the CONNECT project. However, we trade o(cid:29) some optimization in favor of more (cid:30)exible framework that targets both ASIC and FPGA technologies. Distinctively, ICON is the (cid:27)rst system-level framework that can generate hybrid NoC architectures which combine virtual channels with multiple physical planes. In addition, ICON pushes the design entry point to the system level in a way that it enables the exploration of a broader design space and the evaluation of a very large number of design points in such space. 3 THE ICON FRAMEWORK The main advantage of using ICON is to generate multiple di(cid:29)erent NoCs, integrate them into existing SoCs, and create new NoC components with minimal e(cid:29)ort. Most of this (cid:30)exibility is achieved by allowing users to mix-and-match several heterogeneous instances of each sub-component listed in Table 1 to build customized NoC components. Following a user-de(cid:27)ned topology and connection scheme, these components are then automatically connected to generate the desired NoC con(cid:27)guration. In addition, ICON generates the necessary simulation environment and testbench for validation, which can be reused across all NoC con(cid:27)gurations generated with both pre-con(cid:27)gured and custom sub-components. Furthermore, users can extend the set of con(cid:27)guration parameters available to ICON. For example, a user can add de(cid:27)nitions of roundrobin or random-based arbiters to create new types of virtual channel (VC) allocators. At a higher level in the NoC hierarchy, these allocators can be selected to build di(cid:29)erent types of routers. Beside the NoC generation, ICON automatically creates network interfaces according to the message types and message classes speci(cid:27)ed for the IP components of the SoC. Hence, users can mix-andmatch di(cid:29)erent NoC con(cid:27)gurations without changing IP component speci(cid:27)cation. Alternatively, the same NoC can be used for multiple SoCs, each with a speci(cid:27)c set of message types and message classes. All customized NoC components can be seamlessly integrated. The communication behavior of the same type of components, i.e. a component group, is pre-de(cid:27)ned in ICON. Testbenches and synthesis scripts can be shared for a component group. This simpli(cid:27)es the validation of user-de(cid:27)ned NoC components and their integration into the target system. ICON consists of six main parts: con(cid:27)guration parser, script generator, NoC component generator, testbench generator, the SystemC NoC library and the testbench component library. Fig. 1 illustrates the high-level relationships between these parts and the (cid:30)ow that ICON follows to generate the NoC design and the corresponding scripts for synthesis and simulation. Starting from the user-provided speci(cid:27)cation of the NoC through an XML template, the parser instantiates the necessary objects to build the NoC architecture with the desired con(cid:27)guration. The objects are then sent to the three generators that produce the actual NoC design, together with the scripts for synthesis and simulation, and the SystemC testbenches to validate the design. With the parameter-speci(cid:27)c or customized SystemC code from the NoC component generator, the user can launch (cid:27)rst HLS and then logic synthesis using the tcl System-Level Design of Networks-on-Chip for Heterogeneous Systems-on-Chip NOCS’17, October 19–20, 2017, Seoul, Republic of Korea u o 4 t u p s t u o 4 t u p s t u o 4 t u p s t u o 4 t u p s t 4 i u p n s t allocator_base (a) allocator_base s c v 2 u o 2 t u p s t u o 2 t u p s t u o 2 t u p s t 2 i u p n s t u o 2 t u p s t s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 vc_allocator_base (b) vc_allocator_base s c v 2 u o 2 t u p s t u o 2 t u p s t u o 2 t u p s t 2 i u p n s t u o 2 t u p s t s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 s c v 2 Input Arbiter (0, 0) Input Arbiter (0, 1) Input Arbiter (1, 0) Input Arbiter (1, 1) Output Arbiter (0, 0) Output Arbiter (0, 1) Output Arbiter (1, 0) Output Arbiter (1, 1) (c) input-(cid:27)rst vc allocator s c v 2 u o 2 t u p s t u o 2 t u p s t u o 2 t u p s t 2 i u p n s t u o 2 t u p s t c v 1 s c v 2 c v 1 s c v 2 c v 1 s c v 2 c v 1 s c v 2 s c v 2 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. 4-to-1  RR Arb. (d) customized vc allocator Figure 2: An example of object-oriented and parameterized module implementation with the virtual channel allocator. FIFO 1 FIFO 0 Input  Status Routing Unit P p e i l i n e c o n t r o l I n p u t F o l w C o n t r o l from/to VC allocator from/to SW allocator from output units (a) router input unit FIFO 1 FIFO 0 Output Status O u t p u t F o l w C o n t r o l from VC allocator from SW allocator to input units P p e i l i n e c o n t r o l V C A r b . (b) router output unit Figure 3: Input and output units of routers with 2 VCs. scripts from the script generator. The synthesized RTL and netlist can then be co-simulated with the same testbenches by using the generated Make(cid:27)les. The SystemC testbench component library is equipped with the set of synthetic tra(cid:28)c models commonly used to evaluate NoCs. These tra(cid:28)c models can be controlled with simulation con(cid:27)gurations speci(cid:27)ed in the XML speci(cid:27)cation. SystemC NoC Component Library. The SystemC NoC library contains a rich set of components and sub-components that are speci(cid:27)ed based on object-oriented programming and that can be combined hierarchically to obtain a variety of NoC architectures. Table 1 gives an example of the many components and sub-components for the router and their hierarchical relationships. The router class is one of the main classes and is de(cid:27)ned as a collection of input units, output units, VC and SW allocators, and crossbars in the NoC component library. All these sub-components are de(cid:27)ned as C++ template parameters in the router class to provide the (cid:30)exibility of combining various sub-component implementations to build a router. A component like the router can have a uniform microarchitecture, where every sub-component is con(cid:27)gured with the same parameter values, or a non-uniform architecture. An example of the latter is a router which supports di(cid:29)erent numbers of virtual channels across di(cid:29)erent inputs. The NoC component generator instantiates a prede(cid:27)ned design from the library for a uniform microarchitecture, while it creates a customized SystemC class at runtime for non-uniform microarchitectures. By sharing the same interface across di(cid:29)erent implementations, NoC components in ICON can be seamlessly combined into a bigger component. Fig. 2 illustrates an example of how these common interfaces are speci(cid:27)ed for the case of virtual channel allocators. All allocators are derived from allocator_base (Fig. 2(a)), and the number of input and output (I/O) virtual channels are speci(cid:27)ed in vc_allocator_base (Fig. 2(b)). When using uniform sub-components to create a large component, ICON leverages SystemC template parameters. For example, the input-(cid:27)rst VC allocator [6] is derived from vc_allocator_base, and contains multiple arbiters in the I/O FIFO Input  Status Routing Unit I n p u t F o l w C o n t r o l from/to VC allocator from/to SW allocator from output units P a c k e t S p l i t t e r P k t S p . P p e i l i n e c o n t r o l (a) source NI input unit FIFO Output Status O u t p u t F o l w C o n t r o l F l i t M e r g e r P p e i l i n e c o n t r o l from VC allocator from SW allocator to input units (b) dest NI output unit Figure 4: Input and output units of network interfaces. stages (Fig. 2(c)). For each I/O stage, the type of arbiter is speci(cid:27)ed as a template parameter for the input-(cid:27)rst VC allocator implementation in the NoC component library. If multiple non-uniform sub-components need to be instantiated in a component, e.g. different number of output VCs per output unit, the front-end SystemC generator dynamically produces SystemC classes by inheriting common interfaces de(cid:27)ned in the SystemC NoC library. For example, to create the allocator of Fig. 2(d) derived from the one of Fig. 2(c), the template parameters for I/O arbiters are speci(cid:27)ed as 4-to-1 round-robin arbiters based on the XML speci(cid:27)cation, and some of unused VCs (gray lines) are bound to constants. Input and Output Units. Fig. 3 illustrates how the I/O units are implemented in the SystemC NoC library. Both the I/O units consist of (cid:30)ow-control, status control, and pipeline control modules with optional FIFOs to store (cid:30)its. In addition, an input unit contains a routing unit to calculate the designated output port based on the destination information in the header (cid:30)it. The routing unit in Fig. 3(a) not only produces the output port of the (cid:30)it, but also provides possible output VCs with the message class of the input VCs. By providing extra information for the output VCs at the routing stage, input units avoid sending unnecessary requests to the VC allocator. Therefore, a generic VC allocator implementation can be used without any modi(cid:27)cation for the message-class isolation. Instead of managing the granted inputs and outputs and their VC information with a centralized status logic, ICON relies on distributed VC and (cid:30)ow management between I/O units. A distributed design makes it easier to instantiate non-uniform I/O ports. It also helps to control the status of non-uniform I/O ports that characterizes a network interface. Network Interfaces. In order to support multiple physical networks [23], message-class isolation [20], and non-uniform packet speci(cid:27)cation, we designed network interfaces in ICON as routers with non-uniform data types for the input or output ports. Thanks to the parameterized and component-based design, the implementation of the I/O unit for both source and destination network interfaces reuses most of the router sub-component implementations in                                                                                                     NOCS’17, October 19–20, 2017, Seoul, Republic of Korea Y. Yoon et al. <network_type name=""example2x2""> <source_network_interfaces num_src=""4""> <source_network_interface index=""0"" type=""sni""/> <source_network_interface index=""1"" type=""sni""/> <source_network_interface index=""2"" type=""sni""/> <source_network_interface index=""3"" type=""sni""/> </source_network_interfaces> <destination_network_interfaces num_dest=""4""> <destination_network_interface index=""0"" type=""dni""/> <destination_network_interface index=""1"" type=""dni""/> <destination_network_interface index=""2"" type=""dni""/> <destination_network_interface index=""3"" type=""dni""/> </destination_network_interfaces> <routers num_routers=""4""> <router index=""0"" type=""r2x2""/> <router index=""1"" type=""r2x2""/> <router index=""2"" type=""r2x2""/> <router index=""3"" type=""r2x2""/> </routers> <channels> <channel type=""ch"" src_ni=""0"" src_port=""0"" dest_router=""0"" dest_port=""4""/> <channel type=""ch"" src_ni=""1"" src_port=""0"" dest_router=""1"" dest_port=""4""/> <channel type=""ch"" src_ni=""2"" src_port=""0"" dest_router=""2"" dest_port=""4""/> <channel type=""ch"" src_ni=""3"" src_port=""0"" dest_router=""3"" dest_port=""4""/> <channel type=""ch"" src_router=""0"" src_port=""4"" dest_ni=""0"" dest_port=""0""/> <channel type=""ch"" src_router=""1"" src_port=""4"" dest_ni=""1"" dest_port=""0""/> <channel type=""ch"" src_router=""2"" src_port=""4"" dest_ni=""2"" dest_port=""0""/> <channel type=""ch"" src_router=""3"" src_port=""4"" dest_ni=""3"" dest_port=""0""/> <channel type=""ch"" src_router=""0"" src_port=""1"" dest_router=""1"" dest_port=""0""/> <channel type=""ch"" src_router=""0"" src_port=""3"" dest_router=""2"" dest_port=""2""/> <channel type=""ch"" src_router=""1"" src_port=""0"" dest_router=""0"" dest_port=""1""/> <channel type=""ch"" src_router=""1"" src_port=""3"" dest_router=""3"" dest_port=""2""/> <channel type=""ch"" src_router=""2"" src_port=""1"" dest_router=""3"" dest_port=""0""/> <channel type=""ch"" src_router=""2"" src_port=""2"" dest_router=""0"" dest_port=""3""/> <channel type=""ch"" src_router=""3"" src_port=""0"" dest_router=""2"" dest_port=""1""/> <channel type=""ch"" src_router=""3"" src_port=""2"" dest_router=""1"" dest_port=""3""/> <channels> <network_type> Figure 5: Example of 2 × 2 NoC XML speci(cid:27)cation for ICON. the NoC component library. Speci(cid:27)cally, a source network interface is implemented as a specialized router where the input unit accepts packets and produces multiple (cid:30)its, while a destination network interface is implemented as a specialized router where the output unit collects multiple (cid:30)its to produce a packet. Fig. 4 illustrates the specialized I/O units to build a network interface. Compared to the router I/O units shown in Fig 3, all components are the same, with the exception of the packet splitter and the (cid:30)it merger. Starting from the user speci(cid:27)cation of the packet format for the source and destination, ICON creates a SystemC module that implements a custom channel. The latter is characterized by a speci(cid:27)c interface implemented with the list of input ports (sc_in) and output ports (sc_out) for the module. This channel is also used as a data type to create status, (cid:30)ow-control, and FIFOs for the I/O units. Packet splitters and (cid:30)it mergers are attached to these components to translate a packet from/to multiple (cid:30)its. Since the (cid:30)it is the base of the control mechanism between I/O units, the packet splitter and (cid:30)it merger must manage the request and grant signals between the input status and the switch allocator. For example, upon receiving a packet from the input queue, the packet splitter creates requests and manages grants for the switch allocator until the entire packet is sent to the output unit as a sequence of multiple (cid:30)its. After sending the last (cid:30)it of a packet, the packet splitter sends a grant signal back to the input status to indicate the complete transmission. Similarly, (cid:30)it mergers keep collecting (cid:30)its from input units to build a packet and send a grant signal to the output status to indicate when a valid packet is ready. Network Generation. Fig. 5 shows the example of an XML tree that de(cid:27)nes a simple 2x2 2D-Mesh NoC. A user can specify routers with router, and network interfaces with source_network_interface and destination_network_interface XML elements. Links are speci(cid:27)ed DDR0 MISC CPU FFT2D FFT2D DB DB LK LK INT1 INT1 INT2 INT2 DDR1 (a) Heterogeneous SoC DDR0 MISC CPU FFT2D FFT2D DDR1 FFT2D FFT2D FFT2D FFT2D FFT2D FFT2D FFT2D FFT2D FFT2D FFT2D (b) Homogeneous SoC Figure 6: High-level (cid:30)oorplan of the two SoC case studies. Symbol Desc. Values Notes F N V Flits Networks VCs 8, 16, 32 1, 2, 5 1, 2, 3, 5 (cid:30)it width for all physical networks number of physical networks number of virtual channels per physical network pipeline con(cid:27)gurations routers in the network queue size of all input units of all routers P Pipelines 2, 4 for all Q Queues 2, 4 Table 2: NoC con(cid:27)guration parameters. N-V Assignments 1N-5V 2N-2/3V 5N-1V N V N V N V Message Class From → To REQ RES REQ MISC RES CPU → MEM MEM → CPU MEM → ACC 0 1 2 3 4 0 1 0 0 0 1 1 2 0 1 2 3 4 0 0 − 1 ACC → MEM Table 3: Message classes and their N-V assignments as channel with the connection information. Based on this speci(cid:27)cation, ICON generates a class with fully customized sc_in and sc_out for the network interfaces, and instantiates and connects all subcomponents (routers, network interfaces, and channels). 4 EXPERIMENTAL RESULTS To demonstrate the capabilities of the ICON framework in exploring the NoC design space for a target SoC, we designed two complete SoCs as instances of Embedded Scalable Platforms [4]. As shown in Fig. 6, each SoC contains a Leon3 CPU running Linux and 2 DDR-3 DRAM controllers together with a set of accelerators: 10 accelerators for 5 distinct application kernels from the Perfect benchmark suite [2] in the heterogeneous SoC and 12 copies of the FFT-2D accelerator in the homogeneous SoC. For each SoC, we used ICON to generate 36 di(cid:29)erent NoC designs by combining the 5 parameters of Table 2. While every combination of parameter values is supported, we limit ourselves to three possible combinations for the number N of physical networks and the number V of virtual channels. Table 3 reports how these three con(cid:27)gurations support the (cid:27)ve distinct message classes that are needed to enable the various independent transactions in the SoC while avoiding protocol deadlock [20]: two for CPU-memory transfers, two for accelerator-memory transfers and one for accelerator con(cid:27)guration and interrupt requests. Note that ICON allows us to use di(cid:29)erent numbers of VCs per physical network, e.g. 2 for the network 0 and 3 for network 1 with 2N-2/3V. All NoC con(cid:27)gurations share a 4 × 4 2D-mesh network topology with XY dimensionorder routing and credit-based (cid:30)ow control. Each of the 36 NoC designs given in SystemC was synthesized into a corresponding Verilog design by using Cadence C-to-Silicon. Then, we used two distinct back-end (cid:30)ows, one for ASIC and another for FPGA, to obtain (cid:27)nal implementations for each NoC. System-Level Design of Networks-on-Chip for Heterogeneous Systems-on-Chip NOCS’17, October 19–20, 2017, Seoul, Republic of Korea Figure 7: Saturation throughput of NoCs (P = 2, Q = 2). Experiments with ASIC Design Flow. We performed logic synthesis targeting a 45nm technology and 500Mhz clock frequency. We simulated the ASIC implementations using the Make(cid:27)les and testbenches generated by ICON for the seven “classic” synthetic tra(cid:28)c patterns: Uniform, Random Permutation, Bit Complement, Bit Reverse, Transpose, Neighbor, and Tornado [6]. Fig. 7 reports the results in terms of saturation throughput for all con(cid:27)gurations with P = 2 and Q = 2. Across all tra(cid:28)c patterns the throughput changes considerably depending on the (cid:30)it width. For the same (cid:30)it width, 5N-1V, which has a bisection bandwidth that is (cid:27)ve times bigger than 1N-5V, provides the highest throughput. The saturation throughput is higher for the simulations with the Random Permutation, Neighbor, and Tornado patterns than in the other cases because on average the destination of the generated tra(cid:28)c is closer to the source. Fig. 8 shows the area-performance trade-o(cid:29) of the NoC con(cid:27)gurations for di(cid:29)erent (cid:30)it-width values. Experiments with FPGA Designs. We combined the generated NoC Verilog designs with those for the two SoCs of Fig. 6 and performed logic synthesis for a Xilinx Virtex-7 XC7V2000T FPGA with two DDR-3 extension boards for a target frequency of 80MHz. For each SoC we run a multi-threaded application that uses Linux to invoke all accelerators (via their device drivers) so that they run simultaneously and, therefore, compete for access to the NoC and DDR-3 controllers. Fig. 9 reports the execution time of the application (normalized with respect to the simplest con(cid:27)guration) and the SoC area occupation for many di(cid:29)erent NoC con(cid:27)gurations. Speci(cid:27)cally, it shows the impact of varying the (cid:30)it width (F) in a NoC with 1 physical network (N=1), 5 virtual channels (V=5), a 4-stage pipeline (P=4) and 2 di(cid:29)erent queue sizes (Q={2,4}). When raising F from 8 to 16, the application for the heterogeneous SoC takes a time that is 86.55% (for Q=2) and 87.57% (for Q=4) of the case for F=8 in exchange for modest area increases (3.1% and 4.3%, respectively). The execution time of the corresponding application on the homogeneous SoC becomes 78.24% and 78.98% of the case with F=8 (with 4.11% and 5.55% of area increase, respectively). While the performance improvement obtained by doubling the (cid:30)it width from 8 bits to 16 bits is considerable, this is not the case when doubling it again from 16 to 32 bits. For both the F=16 and F=32 con(cid:27)gurations, the NoCs are not saturated and the zero-load latency has a bigger impact than the contention latency. The main reason is the long communication delay on the o(cid:29)-chip channels between the DDR-3 controllers and DRAM. The average throughput on this channel is about 2.72 bits per clock cycle for both the F=16 and F=32 con(cid:27)gurations while it decreases to 2.48 for the F=8 con(cid:27)guration when the on-chip links become more congested and the NoC becomes the system bottleneck. Figure 8: ASIC experiments: area/performance trade-o(cid:29)s. Fig 10 reports the normalized execution time and area comparisons for the 3 di(cid:29)erent combinations of numbers of physical networks and virtual channels (N=5 and V=1, N=2 and V=2/3, N=1 and V=5) speci(cid:27)ed in Table 3. Overall, the (cid:27)rst con(cid:27)guration is better from an area viewpoint, while the di(cid:29)erences in performance are minimal. Fig. 11 summarizes the area and performance trade-o(cid:29)s across all the con(cid:27)gurations from the previous two (cid:27)gures as well as the rest of the 36 con(cid:27)gurations that we tested for this SoC case study. For the heterogeneous SoC, the Pareto curve includes 4 NoC con(cid:27)gurations: 8F-5N-1V-2P-2Q, 16F-5N-1V-4P-2Q, 16F-5N-1V-2P-2Q, and 16F-2N-2/3V-4P-2Q. For the homogeneous SoC, the Pareto curve consists of 3 con(cid:27)gurations: 8F-5N-1V-2P-2Q, 16F-1N-5V-4P-2Q, and 16F-2N2/3V-2P-2Q. This set of results shows how ICON can be used to quickly generate and evaluate several network design points. Each design can be seamlessly integrated into a complex heterogeneous SoC without modifying any of the computing IP blocks present in the system. Further, ICON allows us to identify the con(cid:27)guration parameters that have a larger impact on performance for the speci(cid:27)c target SoC. Exploring such a large design space and gathering accurate information from a full-system evaluation would not have been possible without the ICON automation framework. 5 CONCLUSIONS We presented ICON, a complete system-level design framework for the speci(cid:27)cation, synthesis and design-space exploration of NoCs for heterogeneous SoCs. We demonstrated ICON capabilities with a variety of experiments including the complete full-system designs of two SoCs on FPGAs. Future work includes extending ICON to support industry standards (e.g. AMBA-AXI) and open-source protocols (OCP) and to augment its testbench library with statistical NoC models like those proposed by Soteriou et al. [21]. Acknowledgements. This work was supported in part by DARPA (C#: R0011-13-C-0003), the National Science Foundation (A#: 1219001), and C-FAR (C#: 2013-MA-2384), an SRC STARnet center. "
