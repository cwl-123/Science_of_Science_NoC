title,abstract,full_text
Keynote 3 (Banquet Talk) Digital space.,"The multicore trend is universal. Spanning embedded processors, desktop CPUs and DSPs, supercomputers and cloud computing, multicore processors offer a game-changing opportunity for improvements in power efficiency and processing performance. More than anything else, multicores have put on-chip interconnect front and center in terms of design attention, since it has a first order impact on multicore performance, power efficiency, and even ease of programming. This talk will provide the inside scoop on our experiences with onchip interconnect in university research with the 16-core Raw multicore processor, in a commercial environment with Tilera's 64-core Tile processor, and conclude with some startling predictions for future 1000 core processors.","Keynote 3 (Banquet Talk)  Digital Space  Speaker: Anant Agarwal, CSAIL, MIT  Abstract: The multicore trend is universal. Spanning embedded processors, desktop CPUs and  DSPs, supercomputers and cloud computing, multicore processors offer a game-changing  opportunity for improvements in power efficiency and processing performance. More than  anything else, multicores have put on-chip interconnect front and center in terms of design  attention, since it has a first order impact on multicore performance, power efficiency, and  even ease of programming. This talk will provide the inside scoop on our experiences with onchip interconnect in university research with the 16-core Raw multicore processor, in a  commercial environment with Tilera's 64-core Tile processor, and conclude with some startling  predictions for future 1000 core processors.  Bio: Anant Agarwal is a professor of Electrical Engineering and Computer Science at MIT and  an associate director of the CSAIL Laboratory. He enjoys hacking on Websim, an online  circuits laboratory (google MIT websim). He is also a founder and Chief Technology Officer  of Tilera Corporation. Agarwal holds a Ph.D. (1987) in Electrical Engineering from Stanford  University, and a bachelor's from IIT Madras (1982). His teaching and research interests  include computer architecture, software systems, and VLSI. He served as Associate Director of  the MIT Laboratory for Computer Science (LCS) between 1998 and 2003, and was a co-leader  of the Oxygen Project. He led a group that developed Sparcle (1992), an early multithreaded  microprocessor based on the SPARC architecture, and the Alewife machine, a scalable sharedmemory multiprocessor (1993). He led the Raw project at MIT's CSAIL laboratory, which  developed an early tiled multicore microprocessor for distributed instruction level parallelism  (DILP) and streams (2002). Agarwal also led the VirtualWires project at MIT and founded  several start-ups, including Virtual Machine Works, Inc. (now part of Mentor Graphics) and  Tilera Corporation. Agarwal won the Maurice Wilkes prize for computer architecture in 2001,  the Presidential Young Investigator award in 1991, and the Louis D. Smullin Award for  teaching excellence at MIT in 2005. He holds a Guinness World Record for the largest  beamforming acoustic microphone array (LOUD) based on the Raw multicore processor. He is  a Fellow of the ACM, and an author of the textbook Foundations of Analog and Digital  Electronic Circuits.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE          "
Analytical modeling and evaluation of On-Chip Interconnects using Network Calculus.,"Network-on-Chip (NoC) has been proposed as an alternative to bus-based schemes to achieve high performance and scalability in System-on-Chip (SoC) design. Performance evaluation of On-Chip Interconnect (OCI) architectures is widely based on simulation which becomes computationally expensive, especially for large-scale NoCs. In this paper, a performance analysis model using Network Calculus is presented to characterize and evaluate the performance of NoC-based applications. The 2D Mesh on-chip interconnect is analyzed and main performance metrics such as end-to-end delay and buffer size requirements are computed and compared against the results produced by a discrete event simulator. The results shed more light on the potential of this analytical technique as a useful tool for NoC design and performance analysis.","Analytical Modeling and Evaluation of On-Chip Interconnects Using Network Calculus M. Bakhouya UTBM, 90010 Belfort, France bakhouya@gmail.com S. Suboh HPCL, GWU, Washington DC. 20052, USA suboh@gwu.edu J. Gaber UTBM, 90010 Belfort, France gaber@utbm.fr T. El-Ghazawi HPCL, GWU, Washington DC. 20052, USA tarek@gwu.edu Abstract Network-on-Chip (NoC) has been proposed as an alternative to bus-based schemes to achieve high performance and scalability in System-on-Chip (SoC) design. Performance evaluation of On-Chip Interconnect (OCI) architectures is widely based on simulation which becomes computationally expensive, especially for large-scale NoCs. In this paper, a performance analysis model using Network Calculus is presented to characterize and evaluate the performance of NoC-based applications. The 2D Mesh onchip interconnect is analyzed and main performance metrics such as end-to-end delay and buffer size requirements are computed and compared against the results produced by a discrete event simulator. The results shed more light on the potential of this analytical technique as a useful tool for NoC design and performance analysis 1. Introduction Trafﬁc analysis provides crucial insight on performance and power of application speciﬁc system on-chip. With the increasing complexity of SoC and its communications requirement, Network-on-Chip (NoC) has emerged as a solution of non-scalable shared bus schemes currently used in SoC implementation [4, 12, 15, 17, 20]. Understanding and studying trafﬁc generated between components and traverse the network plays an important role in deﬁning the design space. It is useful to perform a trafﬁc analysis in early stages of the design, so the designer can avoid bottleneck and contention by selecting appropriate on-chip interconnect architecture parameters. The objectives are to reduce the latency and increase the bandwidth while lowering power consumption and minimizing the area. Furthermore, the selection of the on-chip interconnect, based on trafﬁc patterns that application speciﬁc SoC generates, allows designers to detect and locate network contentions and bottleneck. Simulation studies are usually carried out to better study the performance of NoC architectures [17, 21, 24]. Generally, simulation is extremely slow for large systems and provides little insight on how different design parameters affect the actual NoC performance [16]. Analytical models, however, allow a fast evaluation of performance metrics of large systems in the early design process. In this paper, we examine the feasibility of incorporating network calculus model to estimate and evaluate performance metrics of on-chip interconnects. Simulations are conducted to study the performance of 2D Mesh on-chip interconnect using a given trafﬁc pattern and the results reported show the effectiveness of using Network Calculus in analytic evaluation. Moreover, the use of a Network Calculus model can give a useful feedback to NoC designers about a coarse- and ﬁne-level granularity such as, buffers’ utilization and latency per node and per data ﬂow. The rest of this paper is organized as follows. In section 2, we summarize existing work on performance analysis methods proposed for evaluating on-chip interconnects. Section 3 gives a succinct introduction on Network Calculus theory. In section 4, we show how Network Calculus can be used to calculate some performance metrics by considering the 2D Mesh on-chip interconnect as a case study. Conclusions and future work are given in section 5. 2. Related Work On-chip interconnect architectures adopted for SoCs are characterized by different trade-offs with regard to latency, throughput, communication load, energy consumption, and silicon area requirements. Recently, there has been a great 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    deal of interest in the development of analytical performance models for NoC design. In [13], an analytical model using queuing theory was introduced to evaluate trafﬁc behavior in Spidergon NoC. Simulation results to verify the model for message latency under different trafﬁc rates and variable message lengths have been reported. A queuingtheory-based model for evaluating the average latency and energy consumption of on-chip interconnects was proposed in [10]. The results from the analytical model are validated with those obtained from a cycle-accurate simulator. A generalized router model, using a probabilistic approach, was proposed in [16] for NoC performance analysis especially the application mapping into an on-chip interconnect. Simulation studies were conducted and results show the accuracy of the proposed model. In [9], a model using a cyclo-static dataﬂow graph was used for buffer dimensioning for NoC applications. The results derived from the model were compared with those extracted from the simulation. A similar work to compute buffers sizes was proposed in [9]. Most analytical models use probabilistic approaches, such as queuing theory, to analyze the performance of NoCs. These approaches consider the incoming and outgoing trafﬁcs as a probability distributions (e.g., Poisson trafﬁc) and allows designers to perform a statistical analysis on the whole system in order to calculate performance network metrics such as average queuing occupancy and average queuing delay. However, NoC applications exhibit trafﬁc patterns [15, 24] which are very different compared to Poisson model used in queuing theory. Network Calculus was originally proposed for Internet quality of service [1, 3, 6], and it was used in many other ﬁelds such as embedded systems like network processors [23, 25], switched Ethernet networks [8], sensor networks [18], optimal routing [2], and service composition [5]. In this paper, we use network calculus to analyze and evaluate performance metrics of on-chip interconnects. In the best of our knowledge, none of past work reported in literature exploits Network Calculus for performance evaluation and analysis of on-chip interconnects. The main contribution of this paper is to show how to drive a Network Calculus based model for NoC applications and how to use this model to evaluate main performance metrics as well. 3. Network Calculus: an Overview Network Calculus [1, 3] is a mathematical modeling framework that allows designers to specify a system as a mathematical model and evaluate main performance bounds such as end-to-end delay. This theory is based on (min, +) algebra and proposed mainly for deterministic network performance analysis, especially for worst-case analysis. Based on shapes of the trafﬁc ﬂows, designers are able to capture some dynamic features of the network. In this section, we brieﬂy introduce Network Calculus especially service and arrival curves, as well as some performance bounds which are used in the rest of this paper. More details about Network Calculus can be found in [1]. We consider that any system can be composed of one or several components that exchange trafﬁcs in order to accomplish a given task. The trafﬁc pattern of the system can be deﬁned by arrival curves of incoming trafﬁc ﬂows to each component of the system. Let’s consider f a data ﬂow characterized by an input function denoted by R(t), which represents the cumulative data units (e.g., packets, bits) of f arriving to the component C within the time interval [0, t]. Let’s consider R∗ (t) the output function (see Figure 1), which represents the cumulative amount of data that leaves the component during the time interval [0, t], R(t) ≥ R∗ (t). Having the input and and output functions, we can derive the two following quantities of interest, the backlog and the virtual delay [1]. The backlog x(t) is the amount of data units that are held inside the system, x(t) = R(t) − R∗ (t). The virtual delay d(t) is the delay that would be experienced by a data unit arriving at time t if all units received before it are served before it, d(t) = inf {τ ≥ 0, R(t) = R∗ (t + τ )}. In order to calculate the delay and the backlog, the input and output functions have to be deﬁned. Their deﬁnition is based on (min, +) convolution and deconvolution principles deﬁned as follows. Given f and g wide-sense increasing functions and f (0) = g(0) = 0, their convolution is deﬁned as (f ⊗ g)(t) = inf0≤s≤t {f (t − s) + g(s)} and their deconvolution is deﬁned as (f ®g)(t) = sups≥0 {f (t+ s) − g(s)}. Each input function can be characterized by an arrival curve as follows. An arrival curve α(t) characterizes a trafﬁc ﬂow R(t), iff it upperbounds the amount of arriving data of this trafﬁc ﬂow during any time interval [0, t]. More formally, given a wide-sense increasing function α(t) deﬁned for t ≥ 0, we say that a ﬂow R(t) is constrained by α iff for all s ≤ t: R(t) − R(s) ≤ α(t − s). It is also said that R has α as an arrival curve, or also that R is α-smooth [1]. Using (min, +) convolution, α is an arrival curve of an input function R iff R ≤ R ⊗ α. An example of the arrival curve is a leaky bucket controller which enforces an arrival curve constraint α(t) = b + rt. It means that no more than b data units can be sent at once and r on the long-term. The output function R∗ (t) can be calculated after the modiﬁcation of the input function R(t) by the component C described by the service curve β (t) of that component. We say that C offers to the ﬂow R a service curve β (nondecreasing function such that β (0) = 0) iff : ∀t ≥ 0, R∗ (t) ≥ inf0≤s≤t{R(s) + β (t − s)}. Using (min, +) convolution of two functions, β is a service curve of ﬂow R iff R∗ ≥ R ⊗ β . An example of the service curve is Figure 1. Arrival and service curves in Network Calculus with delay and backlog bounds rate latency function β (t) = R(t − T )+ , where R denotes the guaranteed service rate and T is the maximum latency caused by the component [19]. The expression (x)+ equals to x when x > 0 and 0 otherwise. Figure 1 shows a component with input/output curves, service curve, delay and backlog. Knowing the service curve β (t) offered by a component C , the output curve α∗ (t) of R∗ (t), can be calculated as follows: α∗ (t) = (α ® β )(t). For example, assuming that a ﬂow is constrained by an arrival curve α(t) = b + rt and C provides a guaranteed service curve β (t) = R(t − T )+ to the ﬂow. The output bound can be calculated as follows: α∗ (t) = α(t) + rt as illustrated in Figure 2. We can see that these curves act like bounds on the input and output trafﬁc ﬂows and can be used to compute the delay bound D and the backlog bound B as follows. The delay D for a data ﬂow R(t) constrained by an arrival curve α(t) that receives the service β (t) to produce a data ﬂow R∗ (t) constrained by the arrival curve α∗ (t) is upper-bounded by: d(t) ≤ sups≥0 (inf {τ ≥ 0 : α(s) ≤ β (s + τ )}). The backlog x(t) can be upper-bounded for all t by: x(t) ≤ sups≤0 {α(s) − β (s)}. An example is depicted in Figure 2 which shows the delay and the backlog bounds of a component receiving a trafﬁc ﬂow characterized by an arrival curve α(t) = b + rt and providing a service curve β (t) = R(t − T )+ , where R ≥ r is the guaranteed bandwidth, and T is the maximum latency of the service. Using these curves, the backlog and delay bounds B and D respectively can be expressed as follows: B = b + rT and D = b/R + T . 4. Modeling OCI-architectures In this section, we present the Network Calculus methodology used to analyze on-chip interconnect architectures and evaluate performance metrics. As described above, in Network Calculus, knowing the arrival and services curves, we can evaluate and calculate main performance metrics such as end-to-end delay and buffer size requirements. Figure 2. Example of backlog (a) and delay (b) bounds 4.1. Network Calculus Methodology In this methodology, we consider that an application can be mapped onto cores or PEs using suitable tools and techniques such as described in [7, 11]. The cores are computational and storage elements like processors, coprocessors, DSPs, and hardware accelerators. Communication between two cores is characterized as ﬂows and are represented by sequences of hops, from a source to a destination as depicted in Figure 4. The system can be represented by acyclic digraph, CG(V , E ), called Core Graph (CG), where each v ∈ V represents a core or PE and e(i, j ) ∈ E is a communication ﬂow edge having one attribute α∗ ij (t), the output arrival curve that represents the maximum data ﬂow sent by a node i to a node j (node can be a PE or switch). For example, the trafﬁc model of a switch node is shown in Figure 3. In this ﬁgure, we consider that, at each time t, a switch si has an input ﬂows αki (t) from m local cores ck , 1 ≤ k ≤ m and input ﬂows α∗ j i (t) from n neighboring switches sj , 1 ≤ j ≤ n. The arrival curve αi (t) characterizing the total input ﬂows of PEs arriving at a switch ci can be computed as follows: αi (t) = m X k=1 αki (t) (1) where αki (t) is the maximum data ﬂow that can be sent by each core ck . For example, in 2D Mesh (Figure 4), there is only one PE linked to each switch, but for the Fat-Tree and Butterﬂy-Fat-Tree structures, switches in the leaf have more than one PEs and the others are connected only to other switches [17, 21]. Hence, the total input ﬂow of a given switch si is the sum of ﬂow of its PEs and the inputs from its neighboring switches and can be calculated as follows: αi (t) = αi (t) + n X j=1 α∗ j i (t) (2) where α∗ j i (t) are input ﬂows coming from n neighboring Figure 3. Input and output trafﬁc of a switch si switches sj . Let’s consider that a switch si has been guaranteed a service curve βi (t), its total output ﬂow can be calculated as follows: α∗ i (t) = αi (t) ® βi (t) = ( n X j=1 α∗ j i (t) + m X k=1 αki (t)) ® βi (t) (3) Having the core graph CG at hand, we can express the input and output arrival curves, αi (t) and α∗ i (t) of each switch. We consider that the maximum data ﬂow sent to a switch si is constrained by the arrival curve αi (t) = ri t+bi , where bi is the maximum burst size of the data ﬂow and ri is its average rate. Using this arrival curve, a node can send bits at once, but without exceeding ri bit/s over the long run. Each switch also provides a guaranteed service constrained by the service curve βi (t) = Ri (t − Ti )+ , where Ri denotes the guaranteed service rate and Ti is the maximum latency caused by the switch. As described above, this service curve is called the rate-latency service curve in which data is delayed by a ﬁxed time Ti and then routed out at a rate Ri . These two curves are widely used in evaluating systems [2, 6, 8, 18] and be considered later in our analysis. Having these curves, the delay bound Di and buffer requirement Bi bound of each switch si can be calculated as follows: Di = bi Ri + Ti Bi = bi + riTi (4) (5) 4.2. Simulation and Evaluation In this section, to show the effectiveness of using Network Calculus for the performance analysis and evaluation of on-chip interconnects, we compare analytical and simulation results using the same trafﬁc pattern,.i.e., a speciﬁed target application. The 4×4 2D Mesh on-chip interconnects Figure 4. On-chip interconnect 2D Mesh and data ﬂows exchanged between nodes depicted in Figure 4 with data ﬂows generated by the application was considered as case study. Using both analytical analysis and simulation we analyze particularly the maximum buffer size needed to store the bulk of data at switches’ input and the delay bound experienced by data ﬂows sent by PEs. As depicted in this ﬁgure, there are three important elements in NoC: cores (or PEs), routers (or switches), and bidirectional links. The number of switches is 16 and the number of cores or PEs is 16. Each core can be either source or sink, in which messages are constructed or consumed. Each ingress port in the switch has a buffer for temporary storage of information. When a ﬂit arrives at a switch, it must go into the buffer. The buffer size is similar to a Drop-tail queue with FIFO queue management mechanism. A simulator developed in [22] using the discrete event driven simulator ns2 [14] is used in this study. The PEs sources and sinks together with data ﬂows (f1 , f2 , f3 , f4 , f5 ) used in the analysis and evaluation study are also used in the simulation study. These data ﬂows are computed using a deterministic routing protocol to direct ﬂits between switches. Unlike dynamic routing protocols that require the implementation of a complex logic at the switch level, a deterministic routing protocol are usually used because they require implementing a simple logic and therefore minimizes the silicon area inside the ship [2]. The cores (c6 , c8 , c11 , c15 ) are randomly selected to be (c1 , c5 , c10 , c11 , c12 ) (c6 , c8 , c11 , c15 ) sources and each ci generates trafﬁc ﬂow which is constrained by an arrival curve αi (t) = ri t. Recall that using this arrival curve, on any time windows of width τ , the number of bits for the ﬂow is limited by rτ . This means that the ﬂow is peak rate limited and is often called a CBR (Constant Bit Rate) ﬂow. The values considered for the injection rate are 25, 50, 75, and 100 Mbps. In the reported results, the ﬂit size value was ﬁxed to 8 bytes generated according to the trafﬁc generator CBR. Cores considered as sinks are selected according to the following communication locality principle in which 75 percent of the trafﬁc takes place between neighboring resources and 25 percent of the trafﬁc is uniformly distributed among the rest. The cores selected according to this principle are (c1 , c5 , c10 , c11 , c12 ) and are also used in the performance analysis. We can see, in this trafﬁc pattern, that c8 is selected two times as a trafﬁc source. As depicted in Figure 4, every PE is directly connected to a switch. Therefore, we can describe the entire network if we know all the data ﬂows that enter and leave every switch. Using a deterministic routing protocol, ﬁve data ﬂows are selected as follows: f1 = (c8 , s8 , s12 , c12 ), f2 = (c8 , s8 , s7 , s6 , s5 , c5 ), f3 = (c6 , s6 , s10 , c10 ), f4 = (c11 , s11 , s7 , s3 , s2 , s1 , c1 ), f5 = (c15 , s15 , s11 , c11 ). 15 (t) 4 rT . α∗ After deﬁning data ﬂows, we can calculate the delay Di of each node participating in transmitting and/or receiving data. For example, according to the trafﬁc ﬂows depicted in Figure 4, the input and output curves α15 (t) and α∗ respectively have to be calculated ﬁrst using (1), (2), and (3). The output bound of the switch s15 is an input to the switch s11 which also has another input from the core c11 . In the second iteration, input and output curves α11 (t) and 11 (t) can now be calculated. In the third iteration, the input and output curves α8 (t) and α∗ 8 (t) respectively have to be calculated. The calculation will be repeated with nodes s12 , s7 , s3 , s2 , s1 , s6 , s10 , and s5 . After arrival curves of these nodes are calculated, the worst case buffer requirements Bi and the data transfer delay Di of each switch si can be calculated using equations (4) and (5). For example, 4 rt, so D6 = ( 5r 4R + 1)T , and B6 = 13 If the injection rate is r = 100M bps, R = 200M bps, and the ﬂit size is k = 8 bytes, then D6 = 0.52µs and B6 = 13 bytes, where T = k/R. When the end-to-end delay bound of each node receiving or/and submitting ﬂow, the total delay, called end-toend delay bound, Dfi of each data ﬂow fi (from PE source to PE sink) can be calculated by summing up the delay of each participating node. Figures 5 and 6 show the average end-to-end and the maximum end-to-end delays calculated using Network Calculus (NC) and network simulator (NS). In these ﬁgures, the delay (maximum or average) increases linearly as injection rate increases. We can see that the delay obtained using Network Calculus analysis is in the same α6 (t) = 2rt + 5 Figure 5. The average e2e delay calculated using simulation (NS) and Network Calculus (NC) Figure 6. The maximum e2e delay calculated using simulation (NS) and Network Calculus (NC) order of magnitude as the end-to-end delay obtained using simulation. We can also see that simulation results conﬁrm analytical results obtained using Network Calculus. As described above, Network Calculus can also be used to compute the buffer size required for each node given a trafﬁc pattern (e.g., application requirements). Figure 7 shows that the buffer size used in simulations is slightly comparable to analytical method. The buffer size in simulations is computed for lossless NoCs. 5. Conclusions and Future Work An analytical study aimed at exploring the use of Network Calculus for performance analysis and evaluation of on-chip interconnects is presented. This work ﬁrst shed more light on how to model NoC applications behavior [8] J.-P. Georges, E. Rondeau, , and T. Divoux. Evaluation of switched ethernet in an industrial context by using the network calculus. 4th IEEE Workshop on Factory Communication Systems, pages 19–26, 2002. [9] A. Hansson, M. Wiggers, A. Moonen, K. Goossens, and M. Bekooij. Applying dataﬂow analysis to dimension buffers for guaranteed performance in networks on chip. NOCS Proc., pages 211–212, 2008. [10] H. J. Kim, D. Park, C. Nicopoulos, V. Narayanan, and C. Das. Design and analysis of an noc architecture from performance, reliability and energy perspective. ACM SANCS Proc., pages 173–182, 2005. [11] S. J. Krolikoski, F. Schirrmeister, B. Salefski, J. Rowson, and G. Martin. Methodology and technology for virtual component driven hardware/software co-design on the system-level. ISCAS 99 Proc., 6:456459, 1999. [12] S. Kumar, A. Jantsch, J.-P. Soininen, M. Forsell, M. Millberg, J. berg, K. Tiensyrj, and A. Hemani. A network on chip architecture and design methodology. Proc. Int’t Symp. VLSI (ISVLSI), pages 117–124, 2002. [13] M. Moadeli, A. Shahrabi, W. Vanderbauwhede, and M. Ould-Khaoua. An analytical performance model for the spidergon noc. 21st AINA Proc., pages 1014–1021, 2007. [14] NS2. Network simulator. http://www.isi.edu/nsnam/ns. [15] U. Y. Ogras, J. Hu, and R. Marculescu. Key research problems in noc design: A holistic perspective. Proc. CODES+ISSS, 2005. [16] U. Y. Ogras and R. Marculescu. Analytical router modeling for networks-on-chip performance analysis. DATE Proc., pages 1–6, 2007. [17] P. P. Pande, C. Grecu, M. Jones, A. Ivanov, and R. Saleh. Performance evaluation and design tradeoffs for networkon-chip interconnect architectures. IEEE Trans. on Computer, 54(8):1025–1040, 2005. [18] J. B. Schmitt, F. A. Zdarsky, and U. Roedig. Sensor network calculus with multiple sinks. IFIP Networking Proc., 2006. [19] D. Stiliadis and A. Varma. Latency-rate servers: a general model for analysis of trafﬁc scheduling algorithms. IEEE/ACM Trans. Networking, 6(5):611624, 1998. [20] S. Suboh, M. Bakhouya, J. Gaber, and T. El-Ghazawi. An interconnection architecture for network-on-chip systems. Telecom. Systems, 37(1-3):137–144, 2008. [21] S. Suboh, M. Bakhouya, S. Lopez-Buedo, and T. ElGhazawi. Simulation-based approach for evaluating on-chip interconnect architectures. SPL Proc., pages 75–80, 2008. [22] Y. R. Sun, S. Kumar, and A. Jantsch. Simulation and evaluation of a network on chip architecture using ns2. In Proceedings of the IEEE NorChip Conference, 2002. [23] L. Thiele, S. Chakraborty, M. Gries, A. Maxiaguine, and J. Greutert. Embedded software in network processors models and algorithms. EMSOFT Proc., LNCS 2211:416–434, 2001. [24] G. Varatkar and R. Marculescu. Trafﬁc analysis for on-chip networks design of multimedia applications. DAC Proc., pages 510–517, 2002. [25] E. Wandeler, L. Thiele, M. Verhoef, and P. Lieverse. System architecture evaluation using modular performance analysis - a case study. STTT Journal, 8(6):649–667, 2006. Figure 7. The buffer size evaluated using Network Calculus (NC) and measured using simulation (NS) using Network Calculus to compute some bounds such as buffer sizes and end-end delay. The 2D regular Mesh was selected and these bounds were computed using a speciﬁc trafﬁc pattern that imitate a speciﬁc class of applications. Simulations have been also carried out to show the effectiveness of using Network Calculus. The comparison of the performance analysis using Network Calculus with a cycle accurate simulator with more case studies is a future work. We will also address the scalability issues of this approach by providing a formal methodology for analyzing and evaluating large NoCs. "
Flow-aware allocation for on-chip networks.,"Current Virtual-Channel routers disregard potentially useful information about on-chip communication flows. This often leads to inefficient resource utilisation in existing Networks-on-Chips for flow-based communication patterns. The flow-based traffic exhibited by forthcoming applications requiring large streaming datasets (sophisticated graphical interfaces, mobile connectivity, scientific applications, etc.) lead us to propose flow additions to a Virtual-Channel network. Our flow-based refinements infer the presence of, and allocate resources to flows rather than individual packets. As a consequence, we are able to demonstrate speedups of close to 40% for synthetic flow-based traffic patterns.","Flow-Aware Allocation for On-Chip Networks Arnab Banerjee and Simon W. Moore Computer Laboratory, University of Cambridge arnab.banerjee@cl.cam.ac.uk Abstract Current Virtual-Channel routers disregard potential ly useful information about on-chip communication ﬂows. This often leads to ineﬃcient resource utilisation in existing Networks-on-Chips for ﬂow-based communication patterns. The ﬂow-based traﬃc exhibited by forthcoming applications requiring large streaming datasets (sophisticated graphical interfaces, mobile connectivity, scientiﬁc applications, etc.) lead us to propose ﬂow additions to a Virtual-Channel network. Our ﬂow-based reﬁnements infer the presence of, and al locate resources to ﬂows rather than individual packets. As a consequence, we are able to demonstrate speedups of close to 40% for synthetic ﬂow-based traﬃc patterns. 1 Introduction Recent work suggests that communication ﬂows – long-lived data streams between ﬁxed sourcedestination pairs – will be a common feature across many future applications [4, 3]. For maximum system performance and power-eﬃciency to be achieved Networks-on-Chips (NoCs) must therefore operate eﬃciently in the presence of such ﬂows. It is further known that maximum eﬃciency cannot be achieved with a static allocation of resources, as with circuit-switched networks. With such allocations, when the service rate demanded by a ﬂow falls below its allocated rate, the concept of exclusive ownership of allocated resources leads to them being idle, but unusable by other requesters. This work focuses on enabling eﬃcient allocation of ﬂows within dynamic allocation based, packet-switched NoCs. We propose a small modiﬁcation to existing Virtual-Channel (VC) routers which has the eﬀect of changing the unit of abstraction within NoC routers from one based solely on allocating packets to VCs, to one allocating ﬂows to VCs. This is achieved with a table-based mechanism which limits the number of packets of the same ﬂow that can exist in a router’s input port at any given time. Not only does this make eﬃcient use of router resources, but moving to a ﬂow based unit of abstraction also enables other aims, such as fair allocation to ﬂows to be achieved. We begin with a review of existing literature in Section 2 showing why ﬂows are expected. Section 3 highlights problems that arise in current dynamic allocation based NoCs in the presence of ﬂows. This leads on to a scalable ﬂow allocation method in Section 4 with an implementation in a NoC router described in Section 5. Section 6 presents the results for a variety of ﬂow and non-ﬂow based traﬃc patterns. Section 7 highlights other approaches that have been demonstrated for solving ﬂow based ineﬃciencies. Section 8 highlights continuing work enabled by the ability to identify ﬂows in NoCs with Section 9 presenting the conclusions. 2 On-chip communication ﬂows Historically, ﬂows have featured strongly within the application domains of graphics and communications processing and scientiﬁc modelling. Within graphics processing, the real-time and changing nature of a displayed environment combines with the naturally pipelined model of the computation to typically result in much more static communication ﬂows. This has been readily exploited across many ﬁelds of research resulting in such varied proposals as stream based programming languages (for example StreamIt [18]) to stream processing architectures (for example Eclipse [17]). Recently, there has been a move towards achieving ubiquitous mobile connectivity driven by continuing user demands [2]. An analysis of such workloads by Wolkotte et al. amongst others has shown that these again exhibit semi-static communication ﬂows [21]. Products operating in this domain (for example those by picoChip [15]) therefore place a strong em978-1-4244-4143-3/09/$25.00 ©2009 IEEE  1   phasis on ﬂow-based traﬃc. The importance of the scientiﬁc applications domain has lead many authors to present analyses of the communication patterns of these applications. Vetter et al. [19] and Kim et al. [11] are among those who show that the ma jority of packets from a single source have only a small number of unique destinations. In this application domain, the commonly-used technique of a division of the modelled environment into smaller regions with similar computation steps for each, results in a repeating and parallel computation model with similarly static communication patterns. It is widely believed that a new era of computing, characterised by huge amounts of data, is currently emerging. Signiﬁcant research, especially that done at Intel, supports the idea of the RMS suite of applications [4]. This suite proposes treating the very large volumes of data in terms of high level models. Recognising models from presented data, Mining for that model in the vast stores of data and Synthesising models to communicate results back to the user will then be a strong focus of applications of the future. The scientiﬁc, media and mobile connectivity classes of applications discussed previously along with large, streaming memory transfers feature strongly within this suite [4, 3]. As a result, the ﬂow-based traﬃc patterns fundamentally associated with them are set to become increasingly common. As an example, Figure 1 shows the number of messages sent by a thread to 31 other co-operating threads for radix from the Splash-2 benchmark suite [22]. The data was obtained by executing the benchmark on the Simics simulator [13] modelling 32 x86 cores. A fully-associative inﬁnitesized cache model was then used to identify communication between threads as every event when a thread ﬁrst reads a cache line previously written to by a different thread. As can be seen, the thread’s behaviour is far from distributing it’s load in a uniform random pattern. It instead has a few favourite destinations, resulting in strong communication ﬂows between this source and those destinations. Similar patterns have been observed for most other Splash and Parsec [3] benchmarks with the Parsec benchmarks showing much higher data-transfer rates between diﬀerent threads over a sustained period of time. These results highlight the importance of ﬂow-based traﬃc in many-core architectures. 3 Resource eﬃciency impact of ﬂows on NoCs In many cases a level of unpredictability exists in ﬂows. As a result, connection-oriented allocation Figure 1. Number of messages sent by a source thread in radix from Splash-2 to 31 other threads, showing non-uniform random distribution. techniques will make ineﬃcient resource utilisation. Dynamic allocation-based NoCs are preferred. Most previous work in dynamic allocation-based NoCs has treated every packet as the same as every other. Given the presence of ﬂows, however, this assumption no longer holds. The order in which packets of the same ﬂow are sent sets an implicit dependency between them. If a packet of a ﬂow blocks, no advantage is gained by allowing packets behind it to bypass it. Since the packets are related, whatever caused the ﬁrst packet to block is also guaranteed to block all subsequent packets. The end result is that these packets acquire and block many network VCs and their buﬀer spaces along their entire path. This subsequently prevents other ﬂows from using these resources to make progress. This is the well known problem of tree saturation [6]. It is caused by ﬂows using much more than the minimum amount of resources they require. A base-case router design is used to further demonstrate this problem. This design operates with single cycle delay, 8 VCs per port with a shared buﬀer architecture of depth 16 ﬂits at each input port and uses wormhole switching. Following the same approach as [12] it does not use a separate VC allocator as a VC is selected by an outgoing head ﬂit from a free VC FIFO at each output port. Figure 2 ﬁrst shows the latency results with an 8×8 node, 2-D mesh network, with all nodes injecting 4-ﬂit packets to uniform random destinations at varying injection rates. The measurements reported are for 2000 measurement packets sent by each node after an initial transmission of 400 warm-up packets. As can be seen, the traﬃc saturates near an injection rate of 0.35 ﬂits/node/cycle. The results in Figure 3 were then obtained with the same setup but with 5% of traﬃc from 2 Figure 4. Translation trafﬁc pattern. Figure 5. Link utilisation under translation trafﬁc for base-case router. ﬂits/cycle the resulting utilisation of all the east links (i.e. how many ﬂits on average are forwarded by that link per cycle) is shown in Figure 5. As can be seen, the link utilisation drastically falls for the more westward links. The problem occurs as multiple ﬂows share the same link bandwidth, with the total bandwidth demand exceeding the link capacity. Flows, therefore, receive less bandwidth than they demand. However, since ﬂow packet dependencies are not enforced, new packets keep being injected into the network. These take up all network VCs and buﬀers, preventing ﬂows further upstream from making progress. 4 Flow-aware allocation 4.1 Deﬁning ﬂows Before any ﬂow-based allocation mechanisms are developed, the term ﬂow must ﬁrst be more clearly deﬁned. The most commonly known network architecture that goes part way to answering this is output queueing [6]. The hypothesis is that, within a router, the only packets that depend on each other are those going to the same output port. Each router therefore provides a queue at each output port. However, from the wider network perspective, this technique only captures the correct packet dependencies in the special case when all ﬂows at a node use the same route from there onwards. In a NoC where packets are not dropped, output queues could still block, as demonstrated in Figure 6. Here a blocking ﬂow blocks an Figure 2. Uniform random trafﬁc latency versus injection rate for base-case router. Figure 3. Hot-spot trafﬁc latency versus injection rate for base-case router. each node addressed to a single hot-spot node at coordinates (3,3). The predictability of these hot-spot packets means that they represent a long term ﬂow. Figure 3 separately shows the average latency of all the packets going to the hot-spot node (labelled hotspot traﬃc) and the average latency of all packets not going to the hot-spot node (labelled non-hot-spot trafﬁc) at varying injection rates. As can be seen, the hot-spot traﬃc saturates at an injection rate of around 0.24 ﬂits/node/cycle. However, since no ﬂow based dependencies are enforced, the hot-spot packets then acquire all the other VCs and buﬀers on their path, thereby blocking the non-hot-spot traﬃc too. This traﬃc therefore also saturates at the same injection rate, much earlier than the case illustrated in Figure 2. Eﬃciency problems can also be caused by limited ﬂow rates at a network node, as opposed to a sink node as demonstrated above. Figure 4 shows a trafﬁc pattern that highlights this problem. Here, each node sends traﬃc at a high injection rate to a destination 4 nodes to its right. For a 1-dimensional, 16 node network with each node injecting traﬃc at 0.3 3 Figure 6. Incorrect dependencies enforced between packets with output queuing when ﬂows do not share a common route. alternate ﬂow which it does not share an entire path with. The eventual implication is that ﬂows should in fact be identiﬁed by their ﬁnal destination node – resulting in end-node queueing [5]. A single reception point exists at the ﬁnal destination and if it blocks all packets destined to it are guaranteed to block. Output queueing can be seen as a limiting case of end-node queueing. Output queueing eﬀectively classiﬁes packets in terms of their next hop address (it could therefore also be called 1-hop queueing), whereas endnode queueing classiﬁes them according to their ﬁnal destination address. A clear continuum exists between these two ends – ﬂows could be identiﬁed by a node address N-hops away from their current position (Figure 7). This is important in the presence of localised traﬃc, when it is known that the ma jority of traﬃc will only go up to N-hops away. This is discussed further in Section 4.2. However, given that this represents a limited-case of end-node queueing, the general-purpose framework demonstrated here identiﬁes ﬂows only by their ﬁnal destinations. From a wider perspective, using physical destination node addresses to identify ﬂows represents a limited interface between the hardware and software entities. Although ﬂows may be clearly identiﬁable from a higher level system view, for instance from a software task-graph, this information is not communicated to the lowest level hardware. Alternative hardwaresoftware interfaces could feasibly be used to communicate ﬂow identiﬁers from higher level software entities to the hardware below. As an example, ﬂow identiﬁers from explicitly stream based programming languages such as streamIt [18] could be used. Although this could increase system performance, this would come at higher hardware power or other such costs. Given the importance of ﬂow-based traﬃc and the need for eﬃcient allocation, such interfaces will need further research. However, such a full-system investigation is Figure 7. Regions of individually identiﬁable nodes from the perspective of a central node resulting in varying queueing strategies in a 49 node mesh network. beyond the scope of this work and ﬂows here are only identiﬁed by their ﬁnal destination addresses. 4.2 Scalable allocation of ﬂows Given a deﬁnition of ﬂows, the utilisation problems of Section 3 above could be solved by allocating diﬀerent ﬂows to diﬀerent VCs to provide full noninterference between ﬂows. This could be achieved by providing as many VCs at each network link as destinations in the network and statically associating each VC with a particular destination [5]. Such an architecture represents a design for the worst-case where every possible ﬂow may be seen in a router at the same time. The tight on-chip power and area constraints mean that the large number of VCs required for increasing network size makes this scheme entirely un-scalable. Such a worst-case design will be unable to support systems with increasing numbers of nodes. This work proposes a design for the typical-case condition, optimising the common-case as argued by Amdahl’s Law. Section 2 has shown that, in practice, the communication graphs seen in most future application classes are of a low dimension. More recently, it has also been shown that using localised communication is the only way to achieve on-chip wiring scalability [7]. Typically, only a low number of ﬂows might be seen in a router at any one time. We propose that a scalable approach is to provide a limited number of VCs (much lower than the total number of destinations in the system) in each router. Changing the unit of abstraction from mapping packets to VCs to mapping ﬂows to VCs will then achieve the required independence between ﬂows. This en4 sures that once a packet of a ﬂow has been allocated a VC, other packets of the same ﬂow are not allocated parallel VCs. The number of VCs is then set by the expected number of ﬂows at each network link. This value being low then provides an acceptable design cost. The scalability of the design can be further investigated with observed traﬃc distributions. Although Greenﬁeld et al. [7] and Wim et al. [9] show that a power-law traﬃc distribution can be expected, this work uses a pessimistic exponential distribution (leading to reduced locality) empirically observed by Greenﬁeld et al. [7]. Equation 1 gives the relationship between hop-count and proportion of traﬃc going to that hop-count away observed in that study. f = Ae−αh (1) where h is the hop count, f is the fraction of traﬃc going that many hops and A and α are constant parameters. Again taken from [7] A and α were set to 0.62 and 0.47 in this study. The more useful parameter of how much traﬃc has a hop count under a certain upper bound may be combined with the topology and routing information to predict how many ﬂows might be seen in a router in a typical case. Figure 8 shows a 2-D mesh topology, with a source node communicating up to 2, 3 and 4 hops away (only one quarter of the destinations are shown, as the analysis is symmetrical in all quarters). The highlighted link represents the highest loaded link with the triangular regions showing the number of destinations reached for a particular hop count limit. For hop count h the number of destinations is given by h2 . For all these ﬂows to be identiﬁable, the number of VCs at that point must be greater than or equal to the number of destinations. This sets the limit on the number of VCs required for that link. With ﬂows identiﬁed by destinations, considering additional source nodes does not change the number of ﬂows seen at a link, as the number of destinations reachable from a link stays the same. A comparison of the VC requirements for this traﬃc distribution for a 2D mesh topology and a 2D mesh with express links spanning two non-express router links (similar to the ﬂattened-butterﬂy topology [10]) in both dimensions is given in Table 1. Although the number of ﬂows at a link grows as h2 in the mesh, as few as 9 VCs are suﬃcient to enable eﬃcient allocation of nearly 80% of all traﬃc. Moreover, with closer to h2 /2 VC requirements of the higher dimension topology 13 VCs are suﬃcient for eﬃcient allocation of more than 90% of all on-chip traﬃc1 . Other strong pressures 1Various approaches are possible to allocate the longer disFigure 8. Number of destinations for up to 2, 3 and 4 hops away. Maximum hop count Cumulative proportion of all traﬃc (%) 1 2 3 4 5 38.8 63.0 78.1 87.6 93.5 VCs required Mesh Express links mesh 1 4 9 16 25 1 2 5 8 13 Table 1. Cumulative proportion of trafﬁc encompassed by regions of varying mesh hop count, with corresponding number of VCs required for mesh and mesh with express links. already exist for the use of higher-order topologies and this work shows another beneﬁt they can provide. 5 Implementation This section proposes a simple modiﬁcation to existing VC router architectures to enable ﬂows to be mapped to VCs. In achieving this aim the analogy of various techniques of mapping data words into caches can be used. Just as a direct-mapped cache speciﬁes a unique cache line to be used for each memory address, in a ﬂow-based router each destination within the area set by the supported hop-count could be mapped to a ﬁxed VC. Alternatively a ﬂow could be allowed to occupy any VC, mimicking the behaviour of a fullyassociative caching strategy. To provide maximum ﬂextance ﬂows, such as providing 1 or 2 additional VCs for them. Alternatively, even if no extra VCs are provided the probability of ineﬃcient ﬂow allocation will still be greatly reduced. 5 ibility, the latter approach is chosen for this work. In caches, the same reasons of limiting the hardware cost while accepting reduced ﬂexibility that lead to setassociative caching strategies may also be applied in mapping ﬂows to VCs. With ﬂows being composed of multiple packets, allocating a ﬂow to a VC can be considered to be the same as the opposing view of restricting the allocation of all packets belonging to that ﬂow to VCs. The aim of making ﬂows stream through routers using only the minimum necessary resources can then be seen to be equivalent to ensuring that no more than one packet from the same ﬂow exists in a router’s input port at any given time. Meeting this condition is suﬃcient for eﬃcient ﬂow allocation and ﬂows do not need to re-use the same VC every time. With the fully associative mapping strategy chosen above, a table based mechanism has been designed to achieve this. A table (referred to from now on as the ﬂow-table ) is provided at each output port with a row for each possible VC in the downstream router’s associated input port. Each row contains an active bit and a ﬂow ID ﬁeld. As ﬂows are identiﬁed by their ﬁnal destinations, the ﬂow ID is equivalent to the X and Y destination address used by the 8 × 8, 2-D mesh topology of this work. When a head ﬂit is allocated to the output link (by the switch allocator) the row identiﬁed by the packet’s allocated VC has its active bit set and the packet’s ﬂow ID written to its ﬂow ID ﬁeld. As part of the allocation mechanism, head ﬂits at the input VCs now search the table at their required output port for their ﬂow ID stored in an active row. If a match occurs, that packet’s switch request is blocked to prevent it from making forward progress. Successful allocation is therefore only achieved by a packet when no active entry is found in the ﬂow-table with a matching ﬂow ID. When a packet is forwarded out of a router, its associated entry in the ﬂow-table in the upstream router must be invalidated to ensure that other packets of the same ﬂow can now make progress. This can be eﬃciently added to the upstream signalling already used by the credit-based buﬀer management mechanism used in this work. With this scheme, every outgoing ﬂit sends a credit identifying its VC to the upstream router. Moreover, with the standard input-queued architecture where all VC queues at an input share a single crossbar port, at most a single ﬂit can leave a router’s input port in one clock cycle. This in turn means that at most a single credit can be sent to the upstream router in a clock cycle. In a ﬂow-based system, only a single bit of upstream signalling needs to be added. When a credit is sent upstream, this adFigure 9. Percentage of packets sent noncontiguously for 8×8 network with uniform random trafﬁc at varying injection rates. ditional ﬂow-freed signal can indicate whether the ﬂit that caused the credit to be sent also signiﬁes that a packet of a ﬂow is leaving the router. If this ﬂow-freed signal were transmitted when the tail of a packet leaves, the round-trip-time (RTT) delay before the arrival of the next packet would negatively impact performance. However, the use of wormhole switching to lower average latencies [12] combined with the static design-time knowledge of the RTT enables an optimisation. Using the same experimental setup as for the uniform random traﬃc test results of Figure 2, Figure 9 shows the number of non-contiguously sent packets observed on the output ports of the router at co-ordinates (3, 3). As can be seen, even by the time the network saturates near an injection rate of 0.35 ﬂits/node/cycle only around 17% of packets are sent non-contiguously. The typical-case design approach followed here therefore justiﬁes the assumption that packets are always sent contiguously. This means that the ﬂow-freed signal can be sent when the ﬂit that is an RT T − 1 number of places before the tail of a packet is transmitted2 . In a typical case, the head ﬂit of a packet will then arrive at an input port in the cycle immediately after the tail ﬂit of the previous packet of that ﬂow left. With this optimisation, it is possible for a second packet of a ﬂow to arrive at a router before the tail ﬂit of the previous packet leaves. However, the network remains functionally correct as this second packet would use an alternative free VC as in a non-ﬂowbased design. Although, various approaches could be used to prevent this second packet from making fur2Note that it is not necessary for packets to carry a count of their total number of ﬂits to enable routers to calculate the correct ﬂow-freed ﬂit position. A single extra bit can be carried and used by the source to mark the position of this ﬂit in a packet. 6 Figure 10. VC allocation logic, including VC ﬂow-table and mechanisms to set and clear it, at one output port. ther progress to improve resource utilisation, none are explicitly implemented in this work. The beneﬁts observed in Section 6 even with this simpliﬁed scheme demonstrates the success of the typical-case design. Figure 10 shows the new VC allocator structure at each output port. Each source node may generate ﬂows to several destinations during the same time period. To provide full non-interference between these ﬂows it is necessary to provide the ﬂow-table mechanism at the output of every computation tile’s network interface. In the current design two VCs are also provided at the egress point of the network. If one VC is busy, the next packet to that destination can utilise the second VC to avoid the RTT penalty that would arise from re-using the same VC. As already discussed, with the ﬂow to VC mapping mechanism described above it is important to realise that packets of a ﬂow do not continuously re-use the same VC. For eﬃciency purposes, this does not matter, in the same way that in a fully-associative caching strategy the cache-line to which an address maps is irrelevant. The order in which the VCs are used is still set by the free VC FIFO (and is hence still LRU). In the absence of ﬂow-based communications, moreover, no ﬂow dependencies exist for the ﬂow-table to enforce and the VC allocation simply reverts back to the existing free VC FIFO-based allocation. 6 Results 6.1 Performance measurements As in Section 3, we begin the evaluation of the proposed design with a hot-spot traﬃc pattern where ﬂows block due to congestion at their destination node. An 8×8 network with 8 VCs per link with uniform-random Figure 11. Hot-spot trafﬁc latency versus injection rate with ﬂow based dependencies enforced. Figure 12. Link utilisation under translation trafﬁc with ﬂow based dependencies enforced traﬃc, but with 5% of each source’s traﬃc going to a single hot-spot destination at co-ordinates (3,3) was again used. Each node sends 400 warm-up and 2000 measurement packets. Figure 11 shows the delay of both the hot-spot and non-hot-spot traﬃc with this setup. As in Figure 3 the hot-spot traﬃc saturates at an injection rate of around 0.24 ﬂits/node/cycle. However, unlike in Figure 3, these ﬂows do not take up all other network resources, so the rest of the traﬃc does not block and saturates near the uniform-random rate of 0.35 ﬂits/node/cycle from Figure 2. For the case of ﬂow problems originating at a network link, Figure 5 had previously used the translation traﬃc pattern (Figure 4) to show very poor link utilisation given the absence of any ﬂow-based scheduling. With the ﬂow additions described above, Figure 12 shows the new link utilisation measured. As can be seen, the progressively worsening utilisation problem is eliminated and all links now operate at a high rate of around 80% of their capacity. 7 Pattern Address function Transpose Shuﬄe Bit rotation Bit reverse Bit complement di = si+b/2 mod b di = si−1 mod b di = si+1 mod b di = sb−i−1 di = s¬i Table 2. Bit permutation trafﬁc patterns. Each bit di of a b-bit destination address is a function of a single bit sj of the source address, with j being a function of i. Completion time (cycles) Pattern Base-case Flow-based Transpose Shuﬄe Bit rot. Bit reverse Bit comp. 28038 19402 22207 28038 25907 28085 18026 18148 28022 16061 Speedup (%) 0.0 7.1 18.3 0.0 38.0 Table 3. Bit permutation trafﬁc results. In the absence of traﬃc from real applications, additional synthetic traﬃc patterns were used to further evaluate the design. The classical bit-permutation trafﬁc patterns described in Table 2 [6] were applied to the 8×8 network already presented. With each of the traﬃc patterns, every source was set to transmit 1000 packets to its corresponding destination. The columns labelled ‘Completion time’ in Table 3 then shows the number of clock-cycles required for every packet to be received for both the base-case and the modiﬁed architectures. The column labelled ‘Speedup’ shows the reduction in clock cycles achieved by the ﬂow-based solution as a percentage of the base-case design. As can be seen, not all traﬃc patterns exhibit ﬂow-based inefﬁciency problems, showing no improvements with the ﬂow-based allocation additions. However, where such problems exist, the better buﬀer and link utilisation provided by the new design directly translates to reduced time to completion, i.e. increased performance. Finally, it is important that the ﬂow based additions do not degrade performance given non-problematic traﬃc conditions. Table 3 has already shown that ﬂow-based patterns not suﬀering any ineﬃciency problems are not slowed down by the new scheme. To test for non-ﬂow-based traﬃc, Figure 13 shows the average latency at varying injection rates obtained by the new design with the uniform-random traﬃc setup from Section 3. As can be seen, the ﬂow-based allocation Figure 13. Uniform random trafﬁc latency versus injection rate with ﬂow based dependencies enforced. scheme does not limit non-ﬂow-based traﬃc in any way and therefore achieves the same performance as in Figure 2. 6.2 Cost analysis The size of the ﬂow-table is an important factor determining the power and delay overheads of the design. The area and power cost of the design will be dominated by any increase in the number of VCs and the size of the ﬂow-table at each output port. As discussed in Section 4.2, the number of VCs is limited by traﬃc locality. Although the number of VCs (such as 8 used in this work) may still seem high, the twin techniques of shared buﬀer organisation and single-cycle routers mitigate this problem, while maintaining high performance. With these techniques, the number of buﬀers nominally associated with each VC can be kept low, keeping the total number of buﬀers low. For example, the design used here only provides 2 ﬂit buﬀer spaces per VC. This requires a total of 16 buﬀers at each input port being the same as the requirements of a conventional conﬁguration of 4 VCs with 4 buﬀers each. Localised traﬃc similarly limits the size of the ﬂowtable. With localised traﬃc, the ﬂow ID ﬁeld widths are limited, maintaining a compact table. With ﬂows identiﬁed by destination addresses in the 8 × 8 network demonstrated here, the ﬂow IDs are 6-bits in length (3-bits each for the X and Y addresses). With the additional 1-bit active ﬁeld, 7-bits are necessary for each VC in the ﬂow-table. With 8 VCs, this results in a ﬂow-table size of 56-bits at each output port. A pessimistic estimate of the area overhead is obtained by comparing this extra state held to the ﬂit buﬀering requirement at each input port. Comparing to a 128bit data-path with 16-ﬂit buﬀers at each input port gives an area overhead of only around 2.7%. 8 The approach here of only adding to the controlpath complexity is further expected to lead to a powereﬃcient design given recent work showing the dominance of data-path over control-path power [1]. Although the technique presented here may increase the critical-path of the router, several optimisations, such as replicating the control logic or parallelising the computation, may be applied to minimise this. 7 Related work A few examples of work trying to solve ﬂow-based NoC ineﬃciency problems can be found in existing literature. Walter et al. identify ﬂow problems originating at hot-spot tiles and propose an end-to-end ﬂow control mechanism to solve it [20]. The approach is of solving an in-network allocation problem with additional control outside of the network. Such an approach can be ineﬃcient. The reported design, for example, requires design-time knowledge of the hot modules and ends up replicating the allocation logic already present in the NoC. Dally et al. therefore attempt to support ﬂows in a dynamic allocation based network by reserving a VC for every destination for every link in the network [5]. As already discussed however, this represents a design for the worst-case where every ﬂow may be using a link at the same time. The, provision of ever more VCs for increasing numbers of nodes required by this design is infeasible and un-scalable. A typical-case design approach is key. Restricting VC allocation has been considered in multi-computer networks [16, 8]. However, these do not place a direct emphasis on on-chip communication ﬂows, thereby being unable to lead on to the important on-chip scope of designing for the typical-case or enabling software labelling of ﬂows. An alternative solution to ﬂow-based congestion might use dynamic routing to route around congested links [14]. In several cases this will only move the problem of ineﬃcient resource utilisation to a new set of links. Dynamic routing does not solve the fundamental problem of packets of a ﬂow being injected at a faster rate than they can be sunk. 8 Future work In addition to the eﬃciency problems highlighted here, the lack of ﬂow-aware allocation creates additional problems in NoCs in the presence of ﬂows. The ﬂow-based extensions of this work, allowing in-network entities to identify ﬂows, is a key enabling mechanism for other strategies to solve these problems. Subsequent work has focused on dividing resources fairly across diﬀerent ﬂows. The packet based scheduling in existing NoC allocators can be shown to lead to unfair allocation to ﬂows and may even lead to certain ﬂows being entirely starved. Modiﬁcations to the switch allocator might therefore be developed to enable it to operate on a per-ﬂow and not per-packet basis thereby providing max-min fairness to ﬂows. 9 Conclusions This work has introduced the idea of allocating ﬂows rather than packets in Networks-on-Chips. It has been shown that the kind of applications now becoming prominent lead to the presence of such stream-like communication ﬂows on-chip. Given the ﬁxed power and wiring constraints now seen in VLSI, maximising system performance requires maximising the allocation efﬁciency in the presence of ﬂows. It has been shown that current dynamic allocation based NoCs can exhibit highly ineﬃcient resource utilisation given ﬂowbased traﬃc patterns. The concept of allocating ﬂows to VCs by restricting the number of VCs that can be occupied by packets of the same ﬂow to one is then proposed. This ensures that ﬂows only use the minimum amount of resources required by them, leaving other resources free for other traﬃc. A table-based mechanism has been developed to achieve this. The design focuses on a typical-case scenario where only a few ﬂows may be using a link at a given time. This limits the number of VCs required and the amount of additional state held ensuring that the design is scalable. The results for a number of synthetic traﬃc benchmarks shows that the design is able to ensure eﬃcient resource utilisation in the presence of ﬂows, which directly corresponds to increased network throughput. Moreover, this does not come at any cost to non-ﬂow-based traﬃc. Finally, the ﬂow identiﬁcation enabled by this design can enable additional ﬂow-based processing in on-chip networks. Acknowledgements The authors would like to thank Christian Fensch at the University of Cambridge for his help in gathering traﬃc data for the Splash-2 and Parsec benchmarks. "
"The design of a latency constrained, power optimized NoC for a 4G SoC.","Network on-Chip (NoC) is being adopted by chip architects as a means to improve design productivity. As the number of modules connected to a bus increase, its physical implementation becomes very complex, and achieving the desired throughput and latency requires time consuming custom modifications. Conversely, NoCs are designed separately from the functional units of the system to handle all foreseen inter-module communication needs. Their inherent scalable architecture facilitates the integration of the system and shortens the time-to-market of complex products. In this work, we discuss and evaluate the design process of a NoC for a state-of-the-art system on-chip (SoC). More specifically, we describe our experience in designing a cost optimized NoC interconnect for a high-performance, power constrained 4G wireless modem. We focus on the power and performance aspects of various module mapping schemes, looking for a tradeoff that is characterized by a minimal power consumption that still meets the timing requirements of all targeted applications. Using a simulated annealing based mapping process, we place the system's modules on a grid, minimizing the dynamic energy consumed by the transmission of packets over the NoC. In many of the studies where network latency was used as a performance goal (either as the optimized cost function or as a constraint), the average delay of all packets over all communicating pairs was considered. However, in a practical SoC, different streams of communication may require different delays and therefore the overall average latency is an inappropriate measure. Consequently, the individual per-flow, point-to-point (source-destination) latencies should be accounted for to get better results. In this paper, we go further to suggest a third, improved approach: knowing the application that is to be used in the SoC, we utilize its functional timing requirements, which are defined by the application end-to-end latency constraints. Each of thos...","The Design of a Latency Constrained, Power Optimized NoC for a 4G SoC  Rudy Beraha1, Isask'har Walter2, Israel Cidon3, Avinoam Kolodny3  1Qualcomm Corp. Research and Development, San Diego, California 92121, USA  2,3Electrical Engineering Department, Technion – Israel Institute of Technology, Haifa 32000, Israel  1rberaha@qualcomm.com,   2zigi@tx.technion.ac.il ,   3{cidon, kolodny}@ee.technion.ac.il        Abstract  Network on-Chip (NoC) is being adopted by chip  architects as a means to improve design productivity.  As the number of modules connected to a bus increase,  its physical implementation becomes very complex,  and achieving the desired throughput and latency  requires  time consuming custom modifications.  Conversely, NoCs are designed separately from the  functional units of the system to handle all foreseen  inter-module communication needs. Their inherent  scalable architecture facilitates the integration of the  system and shortens the time-to-market of complex  products. In this work, we discuss and evaluate the  design process of a NoC for a state-of-the-art system  on-chip (SoC). More specifically, we describe our  experience  in designing a cost optimized NoC  interconnect for a high-performance, power constrained  4G wireless modem. We focus on the power and  performance aspects of various module mapping  schemes, looking for a tradeoff that is characterized by  a minimal power consumption that still meets the  timing requirements of all targeted applications. Using  a simulated annealing based mapping process, we place  the system's modules on a grid, minimizing the  dynamic energy consumed by the transmission of  packets over the NoC.   In many of the studies where network latency was  used as a performance goal (either as the optimized  cost function or as a constraint), the average delay of  all packets over all communicating pairs was  considered. However, in a practical SoC, different  streams of communication may require different delays  and  therefore  the overall average latency is an  inappropriate measure. Consequently, the individual  per-flow, point-to-point (source-destination) latencies  should be accounted for to get better results. In this  paper, we go further to suggest a third, improved  approach: knowing the application that is to be used in  the SoC, we utilize its functional timing requirements,  which are defined by the application end-to-end latency  constraints. Each of those end-to-end traversal delay  requirements  is  composed of  the  cumulative  requirement of a sequence (or a ""chain"") of point-topoint flows. For example, the application may require  that a block of data which is generated by module A is  sent to module B, and then to module C. By observing  that the performance of the application is subject to the  total time it would take the data to get from module A  to module C, we can use this delay as the targeted  performance measure, rather than specifying the two  separate latency constraints (for the flow from module  A to module B and from module B to module C). Since  pair-wise delays may be traded, the timing constraints  are relaxed and the optimization program can use more  freedom in its operation. To the best of our knowledge,  this paper is the first to discuss and quantify the  benefits of  specifying  the end-to-end  traversal  requirements during the design process.  In order  to quantify and evaluate different  alternatives, we report the actual throughput and timing  requirements of the commercial SoC as well as the  synthesis results. We evaluate three mapping schemes:  a power optimized mapping; a power optimized  mapping with point-to-point timing requirements; and a  power optimized mapping with end-to-end timing  requirements. For each of these mappings, we use  simulations to find a uniform assignment of link  capacities so that the run-time latency requirements of  all flows are met. We then further optimize the network  by tuning the capacity of links, reducing the bandwidth  of links that operate faster than necessary.    Synthesis results reveal that considering end-to-end  requirements during the mapping phase of the design  results in an improved implementation, even if only a  limited number of discrete port configurations and link  bandwidths are supported. According to our findings,  the proposed mapping and link tuning techniques offer  up to 40% savings in the total router area and a  reduction of up to 49% in the inter-router wiring area.  As part of this work, we present the bandwidth and  timing requirement of the high-performance, state-ofthe-art 4G application we examine. This information  can be used by the NoC community as a benchmark for  future research.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE            "
NoCs - It is about the memory and the programming model.,"CPUs are multicore (and multi-cache) supported by a coherent, global, shared memory model. FPGAs offer a vast number of distributed programmable function blocks and distributed memory blocks across distributed memory spaces. This presentation will discuss a hybrid computing architecture that unifies the development of applications for a combined CPU-FPGA platform. The proposed programming model is based on message passing (MPI) and distributed memory. NoCs are at the heart of the hybrid platform managing the control and data flows. NoCs are implemented through shared memory buffers on the CPU portion of the hybrid computing platform. On parallel hardware, NoCs are implemented as application-specific point-to-point networks exploiting the abundant routing and switching resources of the FPGA. NoCs enable application-specific memory models while keeping with standard, familiar programming models such as MPI.","Keynote 1  NoCs: It is About the Memory and the Programming Model  Speaker: Ivo Bolsens, Sr. VP and CTO, Xilinx Corp.  Abstract: CPUs are multicore (and multi-cache) supported by a coherent, global, shared  memory model. FPGAs offer a vast number of distributed programmable function blocks and  distributed memory blocks across distributed memory spaces. This presentation will discuss a  hybrid computing architecture that unifies the development of applications for a combined  CPU-FPGA platform. The proposed programming model is based on message passing (MPI)  and distributed memory. NoCs are at the heart of the hybrid platform managing the control and  data flows. NoCs are implemented through shared memory buffers on the CPU portion of the  hybrid computing platform. On parallel hardware, NoCs are implemented as applicationspecific point-to-point networks exploiting the abundant routing and switching resources of the  FPGA. NoCs enable application-specific memory models while keeping with standard,  familiar programming models such as MPI.  Bio: Ivo Bolsens is senior vice president and chief technology officer (CTO), with  responsibility for advanced technology development, Xilinx research laboratories (XRL) and  Xilinx university program (XUP).  Bolsens came to Xilinx in June 2001 from the Belgium-based research center IMEC, where he  was vice president of information and communication systems. His research included the  development of knowledge-based verification for VLSI circuits, design of digital signal  processing applications, and wireless communication terminals. He also headed the research on  design technology for high-level synthesis of DSP hardware, HW/SW co-design and systemon-chip design.  Bolsens holds a PhD in applied science and an MSEE from the Catholic University of Leuven  in Belgium.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE              "
Networks-on-chip in emerging interconnect paradigms - Advantages and challenges.,"Communication plays a crucial role in the design and performance of multi-core systems-on-chip (SoCs). Networks-on-chip (NoCs) have been proposed as a promising solution to simplify and optimize SoC design. However, it is expected that improving traditional communication technologies and interconnect organizations will not be sufficient to satisfy the demand for energy-efficient and high-performance interconnect fabrics, which continues to grow with each new process generation. Multiple options have been envisioned as compelling alternatives to the existing planar metal/dielectric communication structures. In this paper we outline the opportunities and challenges associated with three emerging interconnect paradigms: three-dimensional (3-D) integration, nanophotonic communication, and wireless interconnects.","Networks-on-Chip in Emerging Interconnect Paradigms: Advantages and Challenges Luca P. Carloni Columbia University luca@cs.columbia.edu Partha Pande Washington State University pande@eecs.wsu.edu Yuan Xie Pennsylvania State University yuanxie@cse.psu.edu Abstract Communication plays a crucial role in the design and performance of multi-core systems-on-chip (SoCs). Networks-on-chip (NoCs) have been proposed as a promising solution to simplify and optimize SoC design. However, it is expected that improving traditional communication technologies and interconnect organizations will not be sufﬁcient to satisfy the demand for energy-efﬁcient and highperformance interconnect fabrics, which continues to grow with each new process generation. Multiple options have been envisioned as compelling alternatives to the existing planar metal/dielectric communication structures. In this paper we outline the opportunities and challenges associated with three emerging interconnect paradigms: threedimensional (3-D) integration, nanophotonic communication, and wireless interconnects. (ITRS), material innovation with traditional scaling will no longer satisfy the performance requirements in the long term and radically new interconnect paradigms are needed. The continued progress of interconnect performance will require approaches that introduce materials and structures beyond the conventional metal/dielectric system, and may require information carriers other than charge. Multiple options have been envisioned to provide alternatives to the metal/dielectric system. In particular, three emerging interconnect technologies are three-dimensional (3-D) integration, nanophotonic communication, and RF/wireless interconnects. Meanwhile, networks-on-chip (NoC) have been proposed as a promising solution to structure the design of the on-chip communications infrastructure and enable a high degree of integration in multi-core SoCs [5–7] In the following sections we discuss the opportunities and challenges associated with the possibility of building NoCs with these emerging interconnect technologies. 1 Introduction 2 3D NoC The current trend in SoC design in the ultra deep sub-micron (UDSM) regime and beyond is to integrate a large number of functional and storage cores onto a single die [1]. The possibility of this enormous degree of integration gives rise to new challenges in designing the interconnection infrastructure for these complex SoCs. Extrapolating from the existing CMOS scaling trends, traditional on-chip interconnect systems have been projected to be limited in their ability to meet the performance needs of SoCs at the UDSM technology nodes and beyond. Wire delays that span large fractions of the chip do not scale as well as local wire delays with respect to gate delays [2] and global interconnect has an increasing impact on the performance of the overall SoC [3] as well as on the effectiveness of well-established design methodologies and CAD ﬂows [4]. While copper and low-k dielectrics have been introduced to decrease the global interconnect delay, they only extend the lifetime of conventional interconnect systems by a few technology generations. According to the International Technology Roadmap for Semiconductors Three-dimensional integrated circuits (3D ICs) [8] offer an attractive solution for overcoming the barriers to interconnect scaling, thereby offering an opportunity to continue performance improvements using CMOS technology, with smaller form factor, higher integration density, and the support for the realization of mixed-technology chips. Among several 3D integration technologies [9], TSV (Through-Silicon-Via) approach is the most promising one and therefore is the focus of the majority of 3D integration R&D activities [8]. Even though both 3D integrated circuits and NoCs [10, 11] are proposed as alternatives for the interconnect scaling demands, the challenges of combining both approaches to design three-dimensional NOCs have not been addressed until recently [12–19]. This section gives a brief introduction on the exploration of possible architectural designs for three-dimensional NoC architectures, and discuss the tradeoffs among various design options. Symmetric NoC Router Design. The natural and simplest extension to the baseline NoC router to facilitate a 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  1   Crossbar Type 5×5 Crossbar 6×6 Crossbar 7×7 Crossbar Area 8523 µm2 11579 µm2 17289 µm2 Power (500 Mhz) 4.21 mW 5.06 mW 9.41 mW Table 1. Area and power comparison of the crossbar switches in a 90nm technology. 3D layout is simply adding two additional physical ports to each router; one for Up and one for Down, along with the associated buffers, arbiters (VC arbiters and Switch Arbiters), and crossbar extension. We can extend a traditional NoC fabric to the third dimension by simply adding such routers at each layer (called a symmetric NoC, due to symmetry of routing in all directions). We call this architecture a 3D Symmetric NoC, since both intra- and interlayer movement bear identical characteristics as hop-byhop traversal. For example, moving from the bottom layer of a 4-layer chip to the top layer requires 3 network hops. This architecture, while simple to implement, has two major inherent drawbacks: (1) It wastes the beneﬁcial attribute of a negligible inter-wafer distance in 3D chips (for example, the thickness of a die could be as small as 10s of μm). Since traveling in the vertical dimension is multihop, it takes as much time as moving within each layer. Of course, the average number of hops between a source and a destination does decrease as a result of folding a 2D design into multiple stacked layers, but inter-layer and intra-layer hops are indistinguishable. Furthermore, each ﬂit must undergo buffering and arbitration at every hop, adding to the tion of two extra ports necessitates a larger 7×7 crossbar. overall delay in moving up/down the layers; (2)The addiCrossbars scale upward very inefﬁciently, as illustrated in Table 1. This table includes the area and power budgets of all crossbar types investigated in this section, based on synthesized implementations in a 90nm technology. Clearly, a 7×7 crossbar incurs signiﬁcant area and power overhead over all other architectures. Therefore, the 3D Symmetric NoC implementation is a somewhat naive extension to the baseline 2D network. 3D NoC-Bus Hybrid Router Design. There is an inherent asymmetry in the delays in a 3D architecture between the fast vertical interconnects and the horizontal interconnects that connect neighboring cores due to differences in wire lengths (a few tens of μm in the vertical direction as compared to a few thousands μm in the horizontal direction). Consequently, a symmetric NoC architecture with multi-hop communication in the vertical (inter-layer) dimension is not desirable. Given the very small inter-layer distance, single-hop communication is, in fact, feasible. This technique revolves around the fact that vertical distance is negligible compared to intra-layer distances; the bus can provide single-hop Processing  Processing  Element Element (Cache Bank  (Cache Bank  or CPU Core ) or CPU) NIC NIC Single-Stage  Single-Stage  Router Router Router b bits b bits R R NoC Bus NoC/Bus Interface NoC/Bus Interface Vertical Bus In In p p O O utp utp ut ut B B u u ut ut ff ff e e r r B B u u ff ff e e Figure 1. A hybrid 3D NoC/Bus architecture. The router has one additional input/output ports to connect with the vertical bus. traversal between any two layers. This realization opens the door to a very popular shared-medium interconnect, the bus. The NoC router can be hybridized with a bus link in the vertical dimension to create a 3D NoC-Bus Hybrid structure, as shown in Fig. 1. This hybrid system provides both performance and area beneﬁts. Instead of an unwieldy 7×7 crossbar, it requires a 6×6 crossbar (Fig. 1), since the bus adds a single additional port to the generic 2D 5×5 crossbar. The additional link forms the interface between the NoC domain and the bus (vertical) domain. The bus link has its own dedicated queue, which is controlled by a central arbiter. Flits from different layers wishing to move up/down should arbitrate for access to the shared medium. Despite the beneﬁts over the 3D Symmetric NoC router, the bus approach also suffers from a major drawback: it does not allow concurrent communication in the third dimension. Since the bus is a shared medium, it can only be used by a single ﬂit at any given time. This severely increases contention and blocking probability under high network load. Therefore, while single-hop vertical communication does improve performance in terms of overall latency, inter-layer bandwidth suffers. More details on the 3D NoC-Bus hybrid architecture can be found in [12]. True 3D Router Design. Moving beyond the previous options, we can envision a true 3D crossbar implementation, which enables seamless integration of the vertical links in the overall router operation. Fig. 2 illustrates such a 3D crossbar layout. It should be noted at this point that the traditional deﬁnition of a crossbar - in the context of a 2D physical layout - is a switch in which each input is connected to each output through a single connection point. However, extending this deﬁnition to a physical 3D structure would imply a switch of enormous complexity and size (given the increased numbers of input- and outputport pairs associated with the various layers). Therefore, we chose a simpler structure which can accommodate the interconnection of an input to an output port through more than one connection points. While such a conﬁguration can be viewed as a multi-stage switching network, we still call 2 router, 3D NoC-Bus hybrid router, true 3D router, and 3D dimensionally-decomposed router) are based on the assumption that the processing element (PE) (which could be a processor core or a cache bank) itself is still a 2D design. For a ﬁne-granularity design of 3D design, one can split a PE across multiple layers. For example, 3D cache design [20] and 3D functional units [21] have been proposed before. Consequently, a PE in the NoC architecture is possible to be implemented with such ﬁne-granularity approach. Although such a multi-layer stacking of a PE is considered aggressive in the current technology, it could be possible as 3D technology matures with smaller TSV pitches. With such a multi-layer stacking of processing elements in the NoC architecture, it is necessary to design a multilayer 3D router that is designed to span across multiple layers of a 3D chip. Logically, such NoC architecture with multi-layer PEs and multi-layer routers is identical to the traditional 2D NoC case with the same number of nodes albeit the smaller area of each PE and router and the shorter distance between routers. Consequently, the design of a multi-layer router requires no additional functionality as compared to a 2D router and only requires distribution of the functionality across multiple layers. The router modules can be classiﬁed into two categories - separable and non-separable, based on the ability to systematically split the module into smaller sub-modules across layers with the inter-layer wiring constraints and the need to balance area across layers [14]. Input buffers, crossbar, inter-router links are classiﬁed as separable modules, while arbitration logic and routing logic are classiﬁed as non-separable since they cannot be systematically broken into subsets. The saving in chip area can be used for enhancing the router capability, for example, adding express paths between non-adjacent PEs to reduce the average hop count, and help in boosting the performance and reducing the power. Furthermore, because a large portion of the communication trafﬁc consists of short ﬂits and frequent patterns, it is possible to dynamically shut down some layers of the multi-layer router to reduce the power consumption. 3D NoC Topology Design. All the router designs discussed in previous subsections are based on the mesh-based NoC topology. There exists various NoC topologies, such as concentrated mesh or ﬂattened butterﬂy topology, all of which have advantages and disadvantages. By employing different topologies rather than the mesh topology, the router designs discussed above could also have different variants. For example, in 2D concentrated mesh topology, the router itself has a radix of 8 (i.e. an 8-port router, with four to local PEs and the others to four cardinal directions). With such topology, the 3D NoC-bus hybrid approach would result in a 9-port router design. Such highradix router designs are power-hungry with degraded pert u O t u O t s a E t s e W t u O t u O h t r o N t h u o S t u O E P Figure 2. The true 3D router design. this structure a crossbar for the sake of simplicity. The vertical links are now embedded in the crossbar and extend to all layers. This implies the use of a 5×5 crossbar, since no additional physical channels need to be dedicated for interlayer communication. As shown in Table 1, a 5×5 crossbar is signiﬁcantly smaller and less power-hungry than the 6×6 crossbar of the 3D NoC-Bus Hybrid and the 7×7 crossbar of the 3D Symmetric NoC. Interconnection between the various links in a 3D crossbar would have to be provided by dedicated connection boxes at each layer. These connecting points can facilitate linkage between vertical and horizontal channels, allowing ﬂexible ﬂit traversal within the 3D crossbar. The 2D crossbars of all layers are physically fused into one single three-dimensional crossbar. Multiple internal paths are present, and a traveling ﬂit goes through a number of switching points and links between the input and output ports. Moreover, ﬂits re-entering another layer do not go through an intermediate buffer; instead, they directly connect to the output port of the destination layer. For example, a ﬂit can move from the western input port of layer 2 to the northern output port of layer 4 in a single hop. However, despite this encouraging result, there is an opposite side to the coin which paints a rather bleak picture. Adding a large number of vertical links in a 3D crossbar to increase NoC connectivity results in increased path diversity. This translates into multiple possible paths between source and destination pairs. While this increased diversity may initially look like a positive attribute, it actually leads to a dramatic increase in the complexity of the central arbiter, which coordinates inter-layer communication in the 3D crossbar. The arbiter now needs to decide between a multitude of possible interconnections, and requires an excessive number of control signals to enable all these interconnections. A full crossbar with its overwhelming control and coordination complexity poses a stark contrast to this frugal and highly efﬁcient design methodology. Moreover, the redundancy offered by the full connectivity is rarely utilized by real-world workloads, and is, in fact, design overkill [13]. Multi-layer 3D NoC Router Design. All the 3D router design options discussed earlier (symmetric 3D 3           formance, even though the hop count between PEs is reduced. Consequently, a topology-router co-design method for 3D NoC is desirable, so that the hop count between any two PEs and the radix of the 3D router design is as small as possible. Xu et al. [16] proposed a 3D NoC topology with low hop count (which is deﬁned as low diameter) and low radix router design. The level 2D mesh is replaced with a network of long links connecting nodes that are at least m mesh-hops away, where m is a design parameter. In such a topology, long distance communications can leverage the long physical wire and vertical links to reach destination, achieving low total hop count, while the radix of the router is kept low. For application-speciﬁc NoC architecture, Yan et al. [15] also proposed a 3D-NoC synthesis algorithm that is based on a rip-up and reroute formulation for routing ﬂows and a router merging procedure for network optimization to reduce the hop count. Impact of 3D Technology on NoC Designs. Since TSV vias contend with active device area, they impose constraints on the number of such vias per unit area. Consequently, the NoC design should be performed holistically in conjunction with other system components such as the power supply and clock network that will contend for the same interconnect resources. The 3D integration using TSV (through-silicon-via) can be classiﬁed into one of the two following categories; (1) monolithic approach and the (2) stacking approach. The ﬁrst approach involves a sequential device process, where the frontend processing (to build the device layer) is repeated on a single wafer to build multiple active device layers before the backend processing builds interconnects among devices. The second approach (which could be wafer-to-wafer, die-to-wafer, or die-to-die stacking) processes each active device layer separately using conventional fabrication techniques. These multiple device layers are then assembled to build up 3D ICs using bonding technology. Dies can be bonded face-to-face (F2F) or face-toback (F2B). The microbump in face-to-face wafer bonding does not go through a thick buried Si layer and can be fabricated with a higher pitch density. In stacking bonding, the dimension of the TSVs is not expected to scale at the same rate as feature size because alignment tolerance and thinned die/wafer height during bonding poses limitation on the scaling of the vias. The TSV (or micropad) size, length, and the pitch density, as well as the bonding method (face-to-face or faceto-back bonding, SOI-based 3D or bulk CMOS-based 3D) can have a signiﬁcant impact on the 3D NoC topology design. For example, relatively large size of TSVs can hinder partitioning a design at very ﬁne granularity across multiple device layers, and make the true 3D router design less possible. On the other hand, the monolithic 3D integration provides more ﬂexibility in the vertical 3D connection because the vertical 3D via can potentially scale down with feature size due to the use of local wires for connection. Availability of such technologies makes it possible to partition the design at a very ﬁne granularity. Furthermore, face-to-face bonding or SOI-based 3D integration may have a smaller via pitch size and higher via density than face-toback bonding or bulk CMOS based integration. Such inﬂuence of the 3D technology parameters on the NoC topology design should be thoroughly studied and suitable NoC topologies for different 3D technologies should be identiﬁed with respect to the performance, power, thermal, and reliability optimizations. 3 Nanophotonic Interconnection Networks The combination of increasing requirements on on-chip and off-chip communication bandwidths and strict limitations on the maximum on-chip temperature and power budget imposed by packaging constraints will make the interconnect power consumption become perhaps the most critical problem for multi-core SoC design in the foreseeable future. How electronic NoCs can continue to satisfy future bandwidths and latency requirements within the chip power budget is an important open area of research [22]. Photonic networks-on-chip (NoC) have been proposed as a solution to reduce the impact of intra-chip and offchip communication on the overall power budget [23, 24]. Thanks to the unique properties of optical communication, such as bit-rate transparency and low loss of optical waveguides, photonic NoCs are expected to reach levels of performance-per-watt scaling that cannot be matched by all-electronic interconnects. Recently various research projects have been started in both academia and industry to understand how to realize the potential of photonic NoCs. Shacham et al. have proposed a hybrid approach to photonic NoC [25, 26], which is discussed in more detail below. Kırman at al. have studied the performance improvement obtained by using a CMOS-compatible photonic on-chip bus for future chip multi-processors (CMPs) [27]. Batten et al. have proposed power-constrained processor-memory network architectures for future many-core systems [28]. Vantrease et al. have presented a 3D many-core architecture that uses photonic communication for both inter-core communication and off-stack communication to memory or I/O devices [29]. Beausoleil et al. have made the case for a high-performance many-core computing system divided into multiple silicon compute clusters [30]. Krishnamoorthy et al. have provides a comprehensive overview on the opportunities of using photonic communication into a high-performance computing system at the chassis, chippackage and silicon micro-system levels [31]. Advances in Photonic NoC Components. The photonics opportunity is made possible now by recent breakthroughs in nanoscale silicon photonics and considerably improved photonic integration with commercial CMOS 4 munication layer host the optical components and optoelectronic devices that are combined to realize the photonic NoC and provide the main communication infrastructure to connect the cores among themselves and with off-chip memories and devices. This multi-layer organization will allow to separately optimize logic, memory, and Si photonics planes. Hybrid Approach to Photonic NoC Design. Photonic NoCs can leverage two critical properties of the photonic medium: (cid:129) bit-rate transparency: Differently from electronic routers that must switch with every bit of the transmitted data, leading to a dynamic power dissipation that scales with the bit rate, photonic switches switch on and off once per message, and their energy dissipation is essentially independent from the bit rate. Hence, photonic NoCs can deliver very high bandwidth without incurring the levels of power dissipation that would be necessary for equivalent NoCs based on traditional electronic design. (cid:129) low loss in optical waveguides: At the chip and board scale the power that is dissipated on a photonic link is independent of the transmission distance. Energy dissipation remains essentially the same whether a message travels between two processing cores that are a few millimeters or a few centimeters apart. In fact, photonic NoCs based on low-loss off-chip interconnects would enable the seamless scaling of the optical communication infrastructure beyond the chip boundary to connect multiple multicore SoCs as well as DRAM memories. While photonic technology offers unique advantages for energy-efﬁcient high-bandwidth communication, its limitations in terms of computing and storage capabilities pose some critical challenges to the design of photonic NoCs. In particular, ﬂit buffering and control-ﬂit processing, two important functions of any packet-switched NoCs, are impractical to implement with optical devices. In order to address these challenges and leverage the best advantages provided by the electronic and photonic media respectively, Shacham et al. have proposed a hybrid architecture where a high-bandwidth circuit-switched photonic network is combined with a low-bandwidth packet-switched electronic network [25, 26]. While the electronic network carries small-size control (and data) packets, the photonic network transfers large-size data messages between pairs of cores. The NoC operates as follows: (1) a photonic circuit is reserved through the exchange of a path-setup packet over the electronic network between the source and the destination, followed by a short acknowledgment-pulse over the photonic network (path-setup process); (2) a large data transfer is completed on the high-bandwidth photonic circuit; and (3) at the end of the communication the photonic Figure 3. Possible organization of a SoC architecture combining 3DI and photonic NoC. chip manufacturing [32]. The gains in power efﬁciencies for optical modulators and receivers are driven by the nanoscale device footprints and corresponding capacitances, as well as by the tight proximity of electronic drivers enabled by the monolithic CMOS platform integration. Advances in the fabrication of CMOS-compatible photonic devices have been reported in the literature for all the basic elements necessary to build photonic NoCs: links, modulators, switches, and receivers. In particular, thanks to design and fabrication improvements, subμm dimensional photonic links now achieve propagation losses around 1.7dB/cm, and off-chip coupling losses of 0.5dB /facet. Various NoCs functionalities can be implemented using the micro-ring resonator, a device of impressive versatility that can be used to build modulators, switches, and wavelength multiplexers. Micro-ring silicon modulators that are capable to reach modulation speeds in excess of 10Gbps with a dissipation of 85f J/bit have been fabricated and their energy efﬁciency is expected to continue to improve in the future [33, 34]. Simple micro-ring switches have been demonstrated with throughput bandwidths of 250Gbps and are expected to scale to over 1T bps [35, 36]. A four-port non-blocking photonic switch has been fabricated and characterized: it supports multi-wavelength routing through thermally tuned and stabilized micro-heaters [37]. Receivers based on SiGe or Ge photodetectors combined with high-gain CMOS ampliﬁers have been demonstrated to operate at 15Gbps with limited power dissipation [38]. Combining Photonic NoC and 3D Integration. In order to optimize the fabrication process it is reasonable to expect that the introduction of a photonic NoC in the design of future multi-core SoCs will take advantage of the progress in 3D Integration [39] and, particularly, the Through-Silicon-Via technology [40]. A possible organization of a future 3D SoC with a photonic NoC is illustrated in Fig. 3 and consists of three types of layers [26, 41]: (a) a computational layer hosts multiple, possibly heterogeneous, processing cores together with their local memories and network interfaces; (b) one or more storage layers provide the bulk of on-chip memory; and (c) a com5 circuit is released by the source through the transmission of a tear-down packet (path-teardown process). A possible physical implementation of this photonic NoC and its performance are discussed in [42], which presented also the design of the 4-way ring-resonator photonic switch, whose prototype has been recently fabricated [37]. Alternative physical implementations, including two nonblocking topologies, have been proposed and analyzed by Petracca et al. [43], who presented also one of the ﬁrst assessments of the expected beneﬁts of using a photonic NoC for a real application, i.e. computing a very large Fast Fourier Transform. The simulation-based results conﬁrm that on-chip photonic communication can support energyefﬁcient high-bandwidth data transfers among processing cores. Its potential can be realized especially by building photonic NoC that connect large multi-threaded cores or clusters of cores so that data aggregation can be performed to build and transfer large messages. Challenges in Photonic NoC Design. Besides the need for continued advances in the design and the fabrication of the nanophotonic components, there are several issues that require further investigation. At the system-level more research is necessary to understand how to exploit most effectively the high-bandwidth and low-power connectivity offered by optical links to increase the performance of real applications. In particular, photonic NoCs could allow the seamless extension of this connectivity to off-chip devices such as other SoCs as well DRAM memories, thus potentially removing the off-chip communication bottleneck and enabling the rethinking of the overall system architecture. From a physical-layer viewpoint, photonic NoC designers face important challenges in looking for an optimal implementation of a given network architecture because, for instance, a certain layout could increase the aggregated insertion losses due to waveguide crossing [44]. Other important issues include the effect of scaling the number of wavelengths per link given a particular nonlinear power threshold within the optical waveguide and the effect of temperature variations on photonic components. In fact, from a computer-aided design (CAD) viewpoint there is a need for new design environments and tools that account for the many optical physical-layer aspects that have no electronic analogue. 4 Wireless NoC A radical alternative to the existing metal/dielectric 2-D interconnect infrastructures is to use transmission of signals via RF/wireless interconnects. According to Chang et al. [45] intra-chip low latency communication can be achieved through RF interconnects, where transmission of data is guided through on-chip transmission lines [45]. Using Frequency Division Multiple Access (FDMA) with multiband frequency synthesizers and metal wires available 10μm 3 0 ° m μ 0 8 1 mm Figure 4. The mm-wave antenna conﬁguration. from current CMOS processes as transmission lines a high bandwidth RF interconnect can be created for on-chip data transport. In [46] a hybrid NoC architecture using such RF interconnects has been proposed. An underlying wireline mesh architecture overlaid with a high bandwidth FDMA based RF transmission is envisioned. The RF interconnect acts as an information ”highway” enabling fast data transport across longer distances on the chip, thus reducing overall communication latency. In addition to reduced latency, the NoC power dissipation using overlaid RF shortcuts is demonstrated in [46]to be an order of magnitude less than that in traditional wireline NoCs. Miniaturized onchip inductor based antennas and bank of high frequency precision oscillators and ﬁlters make such a hybrid NoC architecture viable. Unlike 3D and photonic NoCs, NoC with RF interconnects can be built using existing CMOS technology. But it requires laying of long on-chip transmission lines that serve as wave guides. To sustain high throughput, RF-interconnect based NoCs require multiple high frequency oscillators and high precision ﬁlters. In contrast to all these, the on-chip wireless communication network can be developed using existing and well-understood CMOS technology, and as elaborated later, it is capable of reducing the number of existing wired interconnects in the NoC. It also achieves on-chip effective speed of light signal propagation. By replacing multi-hop wireline links in a NoC through high-bandwidth single-hop long-range wireless channels the latency, power consumption and interconnect routing problems of a traditional NoC can be simultaneously addressed. On-Chip Antenna. On-chip wireless interconnects were demonstrated ﬁrst in [47] for distributing clock signals. Recently, design of a wireless NoC based on CMOS Ultra Wideband (UWB) technology was proposed [48]. The particular antennas used in [48] achieve a transmission range of 1 mm while being 2.98 mm in length. Consequently, for a NoC spreading typically over a die of 20 mm x 20 mm, this architecture essentially requires multihop communication through the on-chip wireless channels. Moreover, for 1 mm range of on-chip communication, a wireless link may not be more economical and efﬁcient than metal wires. Having wireless nodes spread all over 6 antennas based on CNTs operating in the THz/optical frequency range [51–54]. Bundles of CNTs are predicted to enhance performance of antenna modules by up to 40dB in radiation efﬁciency and provide excellent directional properties in far-ﬁeld patterns [55]. Moreover these antennas can achieve a bandwidth of around 500 GHz, whereas the antennas operating in the mm-wave range achieve a bandwidth of 10’s of GHz. Thus antennas operating in the THz/optical frequency range can support much higher data rates. CNTs have several characteristics that make them suitable as on-chip antennas for optical frequencies. Given wavelengths of several hundreds of nanometers to several micrometers there is a need for virtually one-dimensional antenna structures for efﬁcient transmission and reception. With diameters of a few nanometers and any length up to a few millimeters possible, CNTs are the perfect candidate. Such thin structures are almost impossible to achieve with traditional microfabrication techniques for metals. In CNTs, ballistic electron transport leads to quantum conductance, resulting in reduced resistive loss, which allows extremely high current densities, namely 4-5 orders of magnitude higher than copper. This enables high transmitted powers from nanotube antennas, crucial for long-range communications. By shining an external laser source on the CNT, radiation characteristics of multi-walled carbon nanotube (MWCNT) antennas are observed to be in excellent quantitative agreement with traditional radio antenna theory [52], although at much higher frequencies of hundreds of THz. The requirements of using external sources to excite the antennas can be eliminated if the electroluminescence phenomenon from a CNT is utilized to design linearly polarized dipole radiation sources [56]. It is demonstrated that, semiconducting CNTs emit infrared photons when an electron-hole pair recombines across the bandgap. Therefore, in addition to being the antenna, the CNT also acts as the signal generator, signiﬁcantly reducing the transmitter circuit complexity. Consequently building an on-chip wireless interconnection network using optical frequencies for inter-core communications becomes feasible with much less overhead than the mm-wave antennas. But unlike the mm-wave antennas, CNTs face signiﬁcant manufacturing challenges. All these investigations regarding miniaturized antennas enable the design of novel wireless communication infrastructures for multi-core SoCs. Network Architecture. The goal in on-chip communication system design is to transmit data with low latencies and high throughput using the least possible power and resources. Currently, the major challenges in wire-based traditional on-chip communication networks are the high latency and power consumption of their multi-hop links. By inserting single-hop long range wireless links in place of multi-hop wireline communication, overall system performance can be signiﬁcantly improved. To do this the entire Figure 5. A hybrid wireless/wired NoC. the die will introduce signiﬁcant overhead due to antennas and associated transceiver circuits. This implementation achieves a peak bandwidth of 10 Gbps on a single channel for a system consisting of 16 embedded cores in the 0.18 um technology node. However the performance of silicon integrated on-chip antennas for intra- and interchip communication with longer range have been already demonstrated by the authors of [49]. They have primarily used metal zig-zag antennas operating in the range of tens of GHz. The propagation mechanisms of radio waves over intra-chip channels with integrated antennas were also investigated [50]. It was shown that zig-zag monopole antennas of axial length 1-2 mm can achieve a communication range of about 10-15 mm. As explained in [49], antenna size can be reduced by using a monopole to utilize the virtual image below the ground plane to make it behave like a dipole with twice the length. A possible conﬁguration of a zig-zag antenna is shown in Fig. 4. Depending on antenna conﬁguration and substrate characteristics, achievable frequency of the wireless channel can be in the range of 50-100 GHz. By varying the axial length, trace width, arm element length and bend angle the antenna bandwidth can be modiﬁed. Propagation of radio signal over intra-chip channels is mainly realized with surface waves guided on the air-wafer interface. A relatively long intra-chip communication range facilitates single-hop communication between widely separated blocks. This is essential to achieve the full beneﬁt of onchip wireless networks for multi-core systems by reducing long distance multi-hop wireline communication. Despite all these advantages, in the mm-wave range the antenna size ( 1-2 mm) is still a limitation. If the transmission frequencies can be increased to THz/optical range then the corresponding antenna sizes decrease, occupying much less chip real estate. One possibility is to use nanoscale 7 Transmitter Side Receiver Side Carrier Frequency   Figure 6. WiNoC with long-range wireless links. network should be divided into multiple small clusters of neighboring cores called subnets. Wireless links will be introduced between the subnets, while intra-subnet communication will still be solely through wires. Each subnet is equipped with a wireless base station (WB), which transmits and receives data packets over the wireless channels. The advantage of this heterogeneous mode of data transport is that, as long as the antennas are placed within their communication range, only a single hop is required for inter-subnet communication, even if subnets are not adjacent. This also reduces the number of multi-hop wired links between distant cores. To reduce the antenna and wireless transceiver circuitry overhead, it is desirable to keep the number of WBs on a single chip as low as possible, without signiﬁcantly compromising the performance beneﬁt. This also helps reduce the load on the shared wireless medium. The optimal interconnect architecture and the number of subnets will vary depending on overall system size, mapping of the application on the entire chip, and the maximum number of WBs. Subnets will consist of relatively fewer cores, giving increased ﬂexibility in designing their architectures. Instead of a single NoC spanning the entire system, as is traditional, there will be subnets with varying architectures for different parts of the chip. Fig. 5 shows a hybrid (wireless/wired) NoC architecture with heterogeneous subnets. Based on the research on small-world graphs [57], we can predict where to insert the wireless links in the network to improve performance. As shown in Fig. 6, we can view the wireless NoC (WiNoC) as a combination of clustered WBs with short-range wired links and a few long-range wireless links that produce shortcuts among the distant subnets. Allocating long range wireless links between distant subnets facilitates better utilization of limited wireless channels, as single-hop communication between distant subnets is the main contributing factor for overall performance gain. The existence of such links between the distant subnets basically helps in adopting the small-world phenomenon in the WiNoC. By incorporating the smallworld network architecture through the wireless links, the Figure 7. Components of mm-wave wireless transceiver. latency and throughput proﬁles of the WiNoC are expected to be signiﬁcantly improved. Wireless Transceiver. The design of the wireless transceiver will depend on the speciﬁc frequency range of the wireless channels. For the WiNoC with mm-wave wireless links the necessary components of the wireless transceiver are shown in Fig. 7. The design of the transceiver will be completely different when CNT antennas are used as the communicating frequency is in THz/optical range. The carrier has to be modulated by optical modulators. It is shown in [58] that high-speed silicon integrated Mach-Zehnder optical modulators are currently commercially available. Twenty four continuous wave laser sources of different frequencies and modulators operating at the rate of 10Gbps can be used. As noted in [59] this data rate is expected to increase many fold with technology scaling in future. Due to the very high frequency range, simple On-Off Keying (OOK) will be adopted. The transceiver module will be much simpliﬁed if the electroluminescence phenomenon from a CNT is used to establish the communication channel. The transmitter will consist of a simple biasing circuit that, whenever the information bit to be transmitted is ”1”, applies a DC voltage across the CNT, which will accordingly generate the light signal, and at the same time act as the transmission antenna. The receiver will include a CNT antenna that generates a DC signal while the infrared signal is illuminating it, connected to a simple signal ampliﬁer. In this case also depending on the bandgap of CNTs, multiple distinct frequency channels can be created. Moreover, it is found that, signals emitted by the nanotubes are polarized, with approximately a cos2 θ dependence [56], where θ is the angle between the nanotube and the polarizer axis. This polarization dependence also exists in the case of absorption. This provides additional means of separating channels by polarization division multiplexing (PDM), using antennas positioned in various directions with respect to each other. The ultimate 8 Design Requirements  3D Integration  Multiple layers with  active devices  Optical Interconnects  Silicon photonic  components  Bandwidth Advantage  Higher connectivity &  less hop count  High speed optical  devices and links  Performance  Gains Lower Power Dissipation Shorter average path  length  Reliability  Vertical Via Failure  Challenges  Heat dissipation due  to higher power  density,  yield  Negligible power  dissipation in optical  data transport  Temperature  sensitivity of photonic  components  Integration of on-chip  photonic components  Wireless Links  On-chip metal or  CNT-based antennas  Direct point-to-point  wireless links  between smaller  subnets Multi-hop paths  replaced by single  hop links  Noisy wireless  channel Low power mm-wave  transceivers &  Control over CNT  growth  Table 2. Comparison of the three emerging interconnect paradigms. effectiveness of this scheme will depend on the achievable communication range and signal power, which are yet to be fully characterized. An important point to note here is that, the mm-wave metal antennas and the infrared CNT antennas are the enabling technologies to establish the on-chip wireless links. The characteristics of the on-chip wireless channels and hence the overall performance of the WiNoCs will depend on the speciﬁcs of these antennas, but the basic design principles of the WiNoC and its performance evaluation methodology remains the same. WiNoCs with mm-wave wireless links are more near-term solutions, which can be achieved with the help of existing CMOS technology. Onchip wireless communication links with CNT antennas will introduce much less overhead compared to the mm-wave ones, but it has to overcome several manufacturing challenges in the future. 5 Concluding Remarks Three-dimensional integration, nanophotonic communication and on-chip wireless links are all promising alternative options to traditional planar metal/dielectric-based interconnects for building the communication infrastructure of future multi-core systems-on-chip. Each of these emerging interconnect paradigms, whose main features are summarized in Table 2, could offer remarkable advantages. However, in order to harvest their potential more research is necessary to address various challenges in multiple areas including system architecture, circuit design, device fabrication and CAD tool development. Acknowledgments The authors gratefully acknowledge the major contributions of their collaborators including B. Belzer, K. Chang, S. Deb, A. Ganguly, D. Heo, K. Bergman, A. Biberman, J. Chan, G. Hendry, J. Kash, B. Lee, M. Lipson, M. Petracca, A. Shacham, Y. Vlasov, C. Das, R. Das, J. Kim, C. Nicopoulos, D. Park, T. Richardson, and N.Vijaykrishnan. This work is partially supported by the National Science Foundation (award numbers: 0702617 and 0811012) and the DARPA MTO ofﬁce under grant ARL W911NF-08-1-0127. 9 "
NoC's at the center of chip architecture - Urgent needs (today) and what they must become (future).,"The collision of large-scale computational capabilities (multi-core) and system-scale integration (system-on-chip) have produced a landscape in which networks-on-chip are a central critical element of system design. However, the traditional approaches and requirements from these two communities are quite different with divergence in requirements of cost, regularity, power, methodology, compatibility, features, etc. This talk will survey the landscape of modern NoC design, and point out challenging new opportunities and directions for the NoC research community. Open challenges include-how to reconcile performance with cost, what functions are within scope for NoC, how to reconcile performance with heterogeneity, how to reconcile performance with long-term compatibility, and so on. The central architectural importance of networks-on-chip are now clear, the challenges are to define the architectural approaches and paths which enable robust, efficient, high-performance, cost-effective, and of course rapidly designed systems.","Keynote 2  NoC’s at the Center of Chip Architecture: Urgent Needs (Today)  and What They Must Become (Future)  Speaker: Andrew Chien, VP and Director of Research, Intel Corp.  Abstract: The collision of large-scale computational capabilities (multi-core) and system-scale  integration (system-on-chip) have produced a landscape in which networks-on-chip are a  central critical element of system design.   However, the traditional approaches and  requirements from these two communities are quite different with divergence in requirements  of cost, regularity, power, methodology, compatibility, features, etc.  This talk will survey the  landscape of modern NoC design, and point out challenging new opportunities and directions  for the NoC research community.  Open challenges include - how to reconcile performance  with cost, what functions are within scope for NoC, how to reconcile performance with  heterogeneity, how to reconcile performance with long-term compatibility, and so on.  The  central architectural importance of Networks-on-Chip are now clear, the challenges are to  define the architectural approaches and paths which enable robust, efficient, high-performance,  cost-effective, and of course rapidly designed systems.  Bio: Andrew Chien is vice president of the Corporate Technology Group and director of  Research for Intel Corporation. Chien previously served as the Science Applications  International Corporation Endowed Chair Professor in the department of computer science and  engineering, and the founding director of the Center for Networked Systems at the University  of California at San Diego. CNS is a university-industry alliance focused on developing  technologies for robust, secure, and open networked systems.  For more than 20 years, Chien has been a global leader in research and development of highperformance computing systems. His expertise includes networking, Grids, high performance  clusters, distributed systems, computer architecture, high speed routing networks, compilers,  and object oriented programming languages. He is a Fellow of the American Association for  Advancement of Science (AAAS), Fellow of the Association for Computing Machinery  (ACM), Fellow of Institute of Electrical and Electronics Engineers (IEEE), and has published  over 130 technical papers. Chien serves on the Board of Directors for the Computing Research  Association (CRA), Advisory Board of the National Science Foundation's Computing and  Information Science and Engineering (CISE) Directorate, and Editorial Board of the  Communications of the Association for Computing Machinery (CACM).  From 1990 to 1998, Chien was a professor at the University of Illinois at Urbana-Champaign.  During that time, he held joint appointments with both the National Center for Supercomputing  Applications (NCSA) and the National Partnership for Advanced Computational Infrastructure  (NPACI), working on large-scale clusters. In 1999 he co-founded Entropia, Inc., an enterprise  desktop Grid company. Chien received his bachelor's in electrical engineering, master's and  Ph.D. in computer science from the Massachusetts Institute of Technology.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE              "
A Communication and configuration controller for NoC based reconfigurable data flow architecture.,"While network-on-chip aspects such as topologies, routing strategies or quality-of-service have been largely studied, the mapping of real applications on distributed NoC-based architecture is still an open issue. In this paper, we address this issue for complex reconfigurable data-flow applications. We introduce the concept of communication and configuration controller (CCC) which interacts both with the usual network interface and the IP core structure. The proposed CCC is a programmable template-based architecture, which provides solutions to manage reconfiguration flows, data synchronizations and global control signaling. An implementation of the CCC is presented, and its performances in a 65 nm technology are discussed through a concrete telecommunication application.","A Communication and Configuration Controller  for NoC based Reconfigurable Data Flow Architecture  Fabien Clermidy, Romain Lemaire, Yvain Thonnart and Pascal Vivet  CEA, LETI, MINATEC, F38054 GRENOBLE Cedex 9, FRANCE  {firstname.lastname}@cea.fr  Abstract  While Network-on-Chip aspects such as topologies,  routing strategies or Quality-of-Service have been  largely studied, the mapping of real applications on  distributed NoC-based architecture is still an open  issue. In this paper, we address this issue for complex  reconfigurable data-flow applications. We introduce  the concept of Communication and Configuration  Controller (CCC) which interacts both with the usual  Network Interface and the IP core structure. The  proposed CCC is a programmable template-based  architecture, which provides solutions to manage  reconfiguration  flows, data synchronizations and  global control signaling. An implementation of the  CCC is presented, and its performances in a 65nm  technology are discussed  through a concrete  telecommunication application.  1. Introduction  The adoption of NoC in industry is strongly linked  to the issue of application mapping. Actually, a lot of  efforts have to be put on the software development, and  the final hardware / software integration often lead to  Time-To-Market (TTM) delay. To be accepted, NoCbased architectures must partly answer this problem.   Up  to now,  it had appeared  that data-flow  applications were not so much suited for NoC  integration, due to their hard real-time constraints and  regular structure leading to pipeline architectures.  Nevertheless, the still demanding computation needs  and complexity of the algorithms will impose a NoC  implementation of these applications. As an example,  fourth generation  telecommunication applications  require a high amount of computation due to multiantennas and multi-mode features [1]. Moreover, new  applications such as Software Defined Radio (SDR)  and Opportunistic Radio (OR) leverage the need for  flexibility and reconfigurability [2]. Pipelined-based  architectures can no longer be used.   In this paper, we propose a NoC-based architecture  addressing complex data-flow applications with hard  real-time constraints. To answer the issues of flexibility  and performance, we introduce a tile-based architecture  with distributed control and synchronization through a  NoC framework. In order to manage the growing  complexity, we develop a common programming  model for all the heterogeneous cores, allowing unified  programming of resources. To fulfill hard real-time  constraints and application reconfiguration, we propose  a distributed reconfiguration scheme with fast and  parallel dynamic reconfiguration of the resources.  The paper is organized as follows. Section 2  introduces  real-time  constraints  and  previous  approaches. Then section 3 presents the programming  model used for data-flow applications, with emphasis  on configuration and synchronization mechanisms, and  proposes  the  corresponding distributed  control  architecture. Section 4 and 5 detail, respectively, the  reconfiguration and the synchronization mechanisms.  Section 6 shows an implementation of the different  mechanisms in the Communication and Configuration  Controller (CCC) and gives some implementation  results in terms of speed, area, and power consumption.  Finally, section 7 presents some applicative results on a  concrete telecommunication application in terms of  system performance and reconfiguration time.   2. Application constraints and related  works  On one hand, telecommunication applications have  hard real-time constraints. On the other hand, data  flows for such applications are highly predictable in  terms of data transfers; worst-case scenarios can be  well-characterized since they are no data-dependant. As  a consequence, the simulation of a single frame is  enough to determine if real-time constraints are met. As  an example, in the 3GPP/LTE protocol (cf. section 6) a  subframe must be computed in less than 1ms. When  introducing flexibility, reconfiguration time overhead  978-1-4244-4143-3/09/$25.00 ©2009 IEEE                    must then be reduced to a minimum, less than 10% of  total processing time, i.e. 100µs for the given example.  Centralized approach for reconfiguration can not be  used in such a scheme, as the reaction time to a single  interruption, hundred of clock cycles in the best cases,  is not compatible with this constraint.  Previous works on architectures  for  flexible  baseband have proposed the Scalable Communication  Core [3] architecture, a 3-ary 2-cubes NoC-based  architecture dedicated to SDR. Based on heterogeneous  units, it allows static configuration management, and  presents a fully centralized control performed by an  ARC core. The FAUST architecture, based on a 2-D  mesh NoC also proposes a static reconfiguration  framework with a distributed synchronization [4].  These two architectures have limited flexibility and do  not allow fast reconfiguration. On the other hand, some  proposals have been made  to use homogeneous  architectures for SDR [5]. These approaches offer a  good flexibility, but do not offer  the required  performance / power consumption trade-off, or fast  reconfiguration.   3. Data Flow Programming Model  3.1. Distributed Application Model  In  the  last  few years,  integrated data-flow  applications have evolved in complexity from the  quasi-linear pipelines found in the older ASICs to  heavily branched data-flow graphs more suited to NoC  implementations.  Furthermore, the need for flexibility  of  the  applications  leads  to  non-specific  implementations which require static or dynamic  reconfiguration of the data-flow graph.  Obviously, the control of such an application on a  data-flow architecture cannot be hardwired. Most  implementations in modern SoCs rely on a centralized  scheduling of  the data-flow processing. Using a  hardware low-latency scheduler, the overhead caused  by this centralized scheduling may be reduced to a  minimum, but at the cost of a lack of flexibility.  Respectively, a general-purpose processor used for the  scheduling offers a maximal flexibility, and may even  compute a new scheduling on the fly. This scheduling,  however,  is done at  the cost of an  important  performance overhead, considering  the  important  number of NoC resources managed by a unique host.  Our proposal is to alleviate the load of the host  processor by distributing the configuration and the  scheduling over the data-flow graph. Actually, most of  the load of the host is spent because of the notifications  of termination of each computation phase sent by the  NoC resources to the host, which triggers an interrupt  in the host, up to several hundred cycles of latency for  the interrupt handler, and a consequent configuration  flow describing the next computation phase.  In order not to suffer from these interrupts at a  consequent rate, part of the scheduling intelligence is  transferred in each NoC resource, and new agents are  introduced to act as configuration servers, which  respond to requests sent by the resources themselves.  In this model, the program memory is distributed in  several memory units scattered on the NoC, and the  system performs dynamic distributed reconfiguration,  insofar as each resource of the NoC will independently  send a request for the configuration needed to execute a  given phase, at the time this configuration is required,  and to the server that contains its configurations, which  can be chosen close to the resource in order to keep the  associated NoC traffic local.  The control is distributed as well, insofar as the host  processor only needs to specify in a row the sequence  of computation steps on each  resource which  corresponds to a small set of micro-programs driving  the input communications, the output communications,  and the core processing, and then let the whole  application run, or at least a long part of an application,  which will be called a task from now on. The  scheduling is then locally done in each resource, and  when a communication should occur between a sender  and the receiver, the coherence of the output flow  description in the sender and the input flow description  in the receiver locally synchronizes the execution  phases of both resources.  3.2. Configuration  Flows  and Communication  In the proposed application model, we define  different kinds of messages, divided in three categories:  configuration, communication and signaling.  Configuration data is split into basic initialization  parameters, sent only once by the host, micro-programs  describing the current task sequencing, also sent by the  host, and finally the configurations referenced in the  micro-programs, which contain all the required data to  perform a single execution phase, and which are  exchanged between the resources and their associated  configuration servers.  As regards communication of applicative data, this  proposal relies on an exchange of “Credits” and “Data”  between the sender and the receiver: a backwards credit  flow emitted by the receiver allows the sender to emit a  given number of data flits to the receiver.  Finally, several other kinds of packets are used  occasionally in the NoC for signaling, short message  passing, or debug purposes… The main signaling        message regards task control: the execution of a given  set of micro-programs over several distributed  resources is triggered by an “Enable task” packet sent  by the host processor, and the completion of these  micro-programs is notified to the host by an “End of  task” interrupt. The host may then dispose of the  resource for a new independent task.  Several events in the execution may also trigger  other  interrupts sent  to  the host processor, for  asynchronous signaling of availability of intermediate  results, functional exception handling or debug of the  application mapping. Besides, a given register value  may be read by the host or any other resource by means  of “Dump” requests and responses. For instance, after  an  initial processing, parameters describing an  evolution of the data-flow structure may be available,  and the host processor will read these parameters to  compute the micro-programs necessary for the next  part of the dataflow.  3.3. Configuration  Protocol  and Communication  Using this set of configuration and communication  flows, a complete framework may be defined in order  to map any deterministic data-flow application on the  NoC; while satisfying the objectives of distributed  configuration and control to alleviate the load on the  host processor. A configuration and communication  protocol has been defined for this purpose, and is  presented by an illustrative example in figure 1.  First, the NoC platform is bootstrapped from outside  the chip, or from the boot sector of the host processor,  to set all the global static parameters needed for  initialization,  that will never change during  the  execution of the application, such as information on the  NoC topology, global identifiers… And then, the  application specific scheduling describing the sequence  of execution phases over the data-flow graph is sent to  all the required resources as a set of micro-programs.  This constitutes the base configuration of the platform  for a given application.  After this base configuration, the platform is ready  to run in standalone mode. The host processor sends an  execution command (“Enable Task”) to all the required  resources, even before the actual configurations of  these resources are present, and lets the platform run  and dynamically reconfigure itself. The host is from  now on relieved from all scheduling or configuration  issues, and may be used as a part of the data-flow  processing, or for higher level system processing.  Upon reception of this “Enable task” packet, the  resources enter a distributed configuration phase, and  will issue configuration requests to several memory  units acting as configuration servers, distributed over  the NoC. For the readability of the figure, a single  configuration server is presented, but in practice, each  resource will issue the request to a configuration server  located close to it, and this phase is actually far more  parallelized than what is presented on figure 1.  Host Host AA BB CC CfgCfg Base configuration Execution command Distr ibuted configuration Execution (phase 1) Dynam ic reconfiguration Execution (phase 2) Execution (phase 3) Microprograms Enable task Configuration requests / responses Credit/Data exchange Configuration request / response Credit/Data exchange End of task interrupt Credit/Data exchange End of task interrupts AA CC BB CfgCfg Host Host Figure 1 : Configuration and communication  protocol example and associated topology  When the corresponding configuration is loaded in a  resource, it begins its own execution phase. End-to-end  connections are established between senders and  receivers, consisting in a “Credit/Data” exchange, and  the corresponding processing of the data is done in the  resources. Here credits have two purposes: they ensure  a flow control mechanism in the NoC, so that no data  can stay blocked inside the network, as commonly  done, but also ensure synchronization of the execution  phases between the sender and the receiver. Credits and  Data will be emitted by the receiver and the sender              only when the corresponding instructions are reached  in each one’s micro-program. It is actually the initial  credit/data exchange between  two resources  that  realizes a synchronization barrier in the associated  micro-programs.  As a phase ends, the resource dynamically requests  its configuration server for the following configuration,  without the intervention of the host, which allows a  distributed reconfiguration of the system.  The phases follow each other, and with the possible  use of configuration caches in the resources, requests to  the configuration servers may be minimized, as visible  in execution phase 3 of figure 1, where  the  configurations of B and C are re-used from phase 1.  Upon completion of the whole micro-programs,  resources emit an “End of task” interrupt, to notify the  host of this completion and their availability for a new  task.  3.4. Configuration  and Communication  Controller Architecture  The proposed programming model and protocol is  handled within each resource using an additional  component to the classical Network Interface (NI) used  in NoCs:  the Configuration and Communication  Controller (CCC).  Enable task End of task Micro-program Config request Config CCCCCC Config Exec/End Config Exec/End mux / mux / demux demux NI Credit Data Credit Credit Data flow flow control control Data Data CoreCore Figure 2 : CCC integration in a NoC resource  The NI  is  in  charge  of  packetization,  depacketization and flow control using credits, handled  by Input/Output Communication Controllers (ICCs and  OCCs). The CCC handles the configuration data  transfers, the storage of micro-programs for the input  communications, output communications and the core  processing, and the local scheduling of the NI and the  core. Figure 2 presents  the  resulting  resource  architecture:   4. Configuration Flows  As stated before, configuration management can be  performed in a distributed manner. Nevertheless, it can  be interesting to directly load configurations, for debug  purposes or during  the development phase. The  classical direct configuration protocol is first presented,  after that the innovative self-configuration protocol is  explained. Finally, we discuss the use of configuration  caches to reduce the number of reconfigurations.   4.1. Direct Configuration Protocol  For this protocol, the configuration flows come from  the host processor which play the role of task scheduler  or from an external source for remote programming.  The main principle is that all configurable elements are  address-mapped relatively to the resource where they  are instantiated. Two levels of addressing are available;  the first one is a resource identifier and is used by the  NoC for routing purpose. The second one is a local  address in the resource, independent of the NoC.   Header : Command MOVE + Path to target destination Destination base address Data Word 1 Data Word 2 Data Word N Figure 3 : “Move” Packet Format  As a result, the static configuration packet (Figure  3) begins with a header flit which contains the MOVE  command and the path to destination. A second flit  indicates the local address of the Data Word 1  contained in the next flit. The CCC loads and  increments a counter for the following flits which are  written in adjacent addresses (separated packets must  be used  for non  adjacent parameters). The  configuration packet size is not fixed. However for  better data flow mixing on the network, configurations  are split in separate packets when transferring a large  amount of data. In such a scheme, an “Initial” or  “Final”  tag  is added  to  the MOVE command,  respectively to the first and last packet, in order to  maintain the unity of the transfer so that the resource is  informed that a configuration is fully received before  starting its execution.  4.2. Self Configuration Protocol  This self-configuration protocol is one of the most  innovative parts of the CCC. Its objective is to allow  fast and distributed  reconfiguration at run-time.  Reconfiguration is used to reorganize communication                    flows, ICC and OCC configurations or to apply  different processing parameters, CORE configuration.   In the CCC, the Configuration Manager (CFM)  catalogs each type of configuration (ICC, OCC or  CORE) thanks to an identification ID number (cf.  section 5 for ID management). The CFM contains  tables of current loaded configurations. For a given  task, a sequence of configurations ID has to be  executed. When  the  configuration  parameters  corresponding to the current ID have not been loaded  yet, the CFM can retrieve a configuration by sending a  “Request Move” packet (Figure 4) to a remote  configuration server. Additionally  to  the header  containing the REQ_MOVE command, the packet is  made up 3 fields: a path for the “Move” packet  response and 2 addresses (Figure 5).  Configuration Server @s Source base  address Server Server Config. Config. Memory Memory Self-reconfiguring Resource MOVE @d Data Word 1 … Data Word N Config. Config. Memory Memory CFMCFM @d Destination base  address REQ_MOVE @s, @d Figure 4 : Self configuration protocol  The configuration server  implements all  the  mechanisms to decode the “Request Move” packets  and generate the “Move” response packets. The  configuration storage  in  the server  is based on  description tables organized for indirect addressing.  For each type of configuration, the resource knows  (reconfigurable registers) the base address @t of the  description table in the configuration server. The  source base address @s to identify the requested  configuration in the remote server is computed by the  CFM by adding  to  the  table base address  the  configuration ID number (Figure 6).  Header : Command REQ_MOVE + Path to target destination Response path to target  Source base address Destination base address Figure 5 : “Request Move” Packet Format  Each entry of the description table gives the actual  address of the configuration @c and its length (number  of data word). The destination base address @d (see  computation details in 4.3) which corresponds to the  targeted address-mapped position of the configuration  will be used to fill in the response “Move” packet.  Compared  to  the direct-configuration protocol,  which was mainly used by  the host for base  configuration of the application, the self-configuration  protocol allows any resource  to fetch  its new  configurations from an associated configuration server.  Server Config. Memory Server Conf ig. Memory @t Source base address @s = @t + ID length Address @c length Data Word 1 Data Word 2 Data Word N Description table  base address for  resource @c Config. ID base address Figure 6 : Configuration server memory  organization  4.3. Optimization with Configuration Caches  The two configuration protocols have been coupled  to a resource storage strategy based on a cache policy  in order to limit the needs of data transfers. The  problem to be solved can be illustrated by the  following example. For a given application, a resource  needs to execute a first initial operation which implies a  set of configuration parameters (CFG 1) for the  processing  core. Then  the  resource  executes  alternatively 2 different computations (requiring CFG 2  and CFG 3). During the set-up phase, the CFG 1 can be  preloaded; after its execution, it will successively be  replaced by CFG2 and CFG 3 thanks to the self  configuration protocol. The drawback  is  that  parameters for CFG 2 and CFG 3 will be transferred  many times through the network. A solution should be  to store the 3 configurations inside the resource but it  has consequences in terms of silicon cost. Eventually, a  better approach is to propose 2 spaces to store the  configuration parameters for core processing and  specify a protocol to define which one has to be  replaced. In this way, the CCC truly works as a cache  memory manager to store a subset of the configurations  available in the remote server.  @0 @base ICC Config. @base OCC Config. @base CORE Config. C F G 1 E m p y t l s o t E m p y t l s o t C F G 1 C F G 3 E m p y t l s o t C F G 2 E m p y t l s o t E m p y t l s o t CORE slot size ICC Config. slots OCC Config. slots CORE Config. slots Figure 7 : Resource address map  For ICC, OCC and CORE configurations, the  resource address map is divided into slots (Figure 7).  The slot is a physical space in the address-map to store                                          a specific type of configuration. A configuration is  identified by a logical number called ID. The cache  policy associating a logical ID to each slot is handled  in the micro-programs presented in section 5. The  number of slots has to be specified for each resource at  design time, function of reconfiguration scenarios  complexity and configuration sizes. In this way, the  cache depth for each type of configuration is directly  linked to the number of available slots. Besides, the  slot size is fixed so the configuration storage address is  obtained by simple arithmetic operations.   Destination base address Flag Flag Cache updating parameters Cache updating parameters Figure 8 : Destination base address format details  Base address Base address In addition, the destination base address of the  “Move” packets are indeed composed of 3 fields  (Figure 8). A specific flag indicates to the CFM  whether the data correspond to a slot update or not.  Cache updating parameters encode the configuration  ID, the slot ID and the configuration type (ICC, OCC  and CORE).  Thanks to these protocols and cache mechanisms,  our proposed CCC is able to support advanced  reconfiguration flows in a full NoC-based approach, we  will see  in  the next section how slot ID and  configuration ID are managed in the overall context of  data synchronization.  5. Data Synchronization  As described in section 3, each data flow is  controlled at network level by a credit mechanism. At  application level, a processing resource has to schedule  data flows both for input and output data and also to  synchronize them with the processing phases of the IP  core. In this section, we present how the CCC plays this  scheduling function in a flexible and efficient way  thanks to a micro-program-based solution.   5.1. Micro-Programmed Control  The CCC schedules the execution of configurations  for the Input and Output Communication Controllers  (ICC/OCC) and the cores. To reduce application  mapping complexity and  to bring flexibility  in  programming, we promote a scheme where  the  sequences of configuration are independent. We want  to avoid constrained mechanisms where to modify a  configuration for communication, configurations for  processing have also to be changed. Therefore, we  choose a solution where the CCC runs in parallel as  many configuration sequences as controlled devices.  Each sequence is described as a basic micro-program.  To determine the instruction set, we had to trade-off  between the complexity of configuration sequences we  want to achieve and the compactness of the CCC  replicated in each resource, which excludes complex  instructions. Loops appear to be mandatory when the  same pattern of configurations is repeated during an  application to limit the micro-program size. On the  contrary, conditional instructions where considered as  useless since all the applications we target have  deterministic scenarios.  The proposed minimal instruction set is detailed in  Table 1. The instruction RC launches the execution of  a configuration by specifying its ID number and the  storage slot ID number (cf. 4.3). Two loop levels (LL  and GL) are available, one for a subset of instructions  repetition, the other for a global restart. The loop  iteration parameter can be either explicitly filled in the  instruction, either indirectly read in a set of dedicated  registers. This  feature enables partial data-flow  reconfiguration when the total loop number can only be  determined on the fly.  Mnemonic,  operand(s)  RC  c s  RCL c s  LL  n  GL  n  LLi r  GLi r  STOP  Table 1 : CCC instruction set  Description  Request configuration c from slot s and execute it  Request configuration c from slot s, execute it  and memorize current instruction position for LL  Go back to stored loop position. Loop n times  Go back to first instruction. Loop n times  Go back to stored loop position. Loop as many  times as value written in register r  Go back to first instruction. Loop as many times  as value written in register r  End of micro-program  5.2. Data Communication and Processing  Sequencing  Thanks to this micro-programmed solution, the CCC  is able to perform advanced sequences for data  communication and processing. The Figure 9 is used as  an illustration of the following features.  •  Input data flow. Each configuration controls the  sending of credit packets to accept data from a specific  resource. Thus, a sequence of configurations on an ICC  enables merging data in any specific order. The order  remains completely reconfigurable and not hardware  defined. For instance, the micro-program instructions  assigned to the ICC 0 of the resource C are:  RC 0 0  RC 1 1  GL 3  STOP                  The first instruction launches the configuration 0  stored in the slot 0. It sends 10 credits to resource A in  order to receive the equivalent amount of data. In a  similar way, the second instruction executes the  configuration 1 from slot 1 sequentially after the  configuration 0 to send 20 credits to B. The two  configurations are executed 3 times due to the loop  instruction.  AA CORE OCC0 Send 30 +30 data OCC0 Send 60 Send 15 BB CORE +75 data ICC0 Recv 10 Recv 20 x3 ICC1 Recv 15 CC CORE x2 45 → 10 15 → 15 OCC0 Send 20 OCC1 Send 15 ICC0 Recv 20 Recv 15 DD CORE -35 data Figure 9 : Data and processing flows example  • Output data flow. On the other side, an OCC  can split a data flow over multiple destinations (e. g.  OCC0 in resource B). The OCC does not mix credits  from different origins so senders and receptors stay  synchronized.  • Data processing. One of the specificities of the  proposed programming model  is  to  transfer  the  scheduling of data processing from the IP core to the  CC controller. As consequence, the same variety of  micro-programmed sequences as for communications is  applied to computation phases. The resource C shows a  first configuration executed two times before a second  one in a total independency regarding data transfers  which are decoupled through FIFO buffers.  6. CCC Implementation  6.1. NoC packet encoding for CCC  The new features brought by the CCC imply a  diversity of messages through the NoC and thus require  a complete control protocol. This information is  handled by packets headers as described table 2.  Headers are atomic communication units composed of  34 bits. The 2 MSB are used to define the Begin of  Packet (BoP), End of Packet (EoP) as explained in [4].  Initial and Final packets of a flow are tagged with BoM  and EoM bits 31 and 30. Generally speaking, headers  can be split in 3 parts, the data management through  data and credit packets, configuration management  with messages implementing the static and dynamic  reconfigurations, and signalling events with  task  enable/disable, dump, interruption and test headers.  Table 2: Packet headers definition  33 18 17  0  num  ICC num  type credit OCC Nb Credits  Length  Src Id  Src Id  1 0 X X 0 Data Packet  1 1 X X 0 sel  Credit Packet  1 0 0 0 1 0 0 Request Dump  1 0 0 0 1 0 1 Dump Response  1 0 0 0 1 1 0 Interruption  1 1 0 0 1 0 1 Task Enable Task  1 0 1 1 1 0 0 req  Src Id  Request Move  1 0 X X 1 1 1 Src Id  Move Packet  Bits 17 down to 0 are used to encode the path of the packet  6.2. Common Interface to CCC  To be efficiently used, the CCC must be able to  interface with various computing cores, as well as  classical NI. As previously stated, the CCC is the  master of the communication, configuration as well as  execution start. In such a scheme, the data flow is  directly connected between the NI and the core. For  data flow applications, simple FIFO-based interface  have proven their efficiency. On the contrary, the  configurations for both the NI and the core are handled  by the CCC. A simple address/data interface is used for  this purpose (Figure 10).  CCC Exec. control  Config./dump  interface interface exec eoc addr cfg wr rd slotid status dump ICC/OCC/CORE Figure 10: CCC control and configuration interface  An execution interface is provided for the core. The  objective of this interface is to launch computations  while assuring a correct load of configurations. The  exec signal can thus be interpreted as a “configuration  ready” statement.  If the core can be only statically  configured, the exec signal is a start of computation. In  that case, other signals of the interface can be ignored.  If a configuration cache is available inside the core, the  slotid signal indicates the configuration to be played.  The answer must be given by the “end of computation”  (eoc) signal, with an optional status field. The  reception of an eoc is used by the CCC to start another  configuration/execution step.                                             6.3. Micro-architecture  The micro-architecture resulting from the protocols  and specifications described in the previous sections is  presented in Figure 11.  The network interface (NI) is decomposed in a  multiplexer/demultiplexer of the NoC flows and several  input  and  output  communication  controllers  (ICC/OCC) for all the FIFO interfaces of the core.  These ICC and OCC, as well as the core, contain a  given number of configuration slots in which specific  configurations may be cached, and scheduled by the  CCC.  NI Moved data Dump request Dump data CCC RWD                 RWD                 Read/Write access to all address map Interrupt ITM                 ITM                 End of task Status from ICCs/OCCs/Core Enable task OCC µPrg µPM                 µPM                 ICC µPrg Core µPrg ConfigID OCC ConfigID ICC ConfigID Core Config request mux / mux / demux demux CFM                 CFM                 Config Exec/End Config Exec/End Core S lots CoreCore Credit Data Credit Data OCC Slots OCCOCC Data Config Exec/End ICC Slots ICCICC Data Figure 11 : CCC micro-architecture and detailed  integration in a NoC resource  The configuration and communication controller  (CCC) is based on four components handling the  different functions presented above:  • The read-write decoder (RWD) handles the  address-based memory transfers from and to the  resource. On one hand, it receives “Move” packets  from the NoC, which are decoded and written in the  according memory-mapped configuration  registers  (which may belong indifferently to another component  of the CCC, the NI or the core, using the interface  described in the previous section. On the other hand, it  may receive “Dump” requests, which are decoded into  read accesses to the address map, which are then  packetized and sent over the network.  • The micro-program manager (µPM) contains  the micro-programs describing  the configuration  sequences for all the ICCs, OCCs and cores present in  this resource. It waits for an “Enable task” from the  host processor, and schedules the execution of the  appropriate configurations. When all micro-programs  have ended, it requests the emission of an “End of task”  interrupt to the host.  • The  configuration manager  (CFM)  has  knowledge of the configurations loaded in the different  ICC/OCC/core slots. Upon a notification by the µPM,  it checks whether the required configuration is already  loaded in a slot. If needed, it sends a configuration  request to the configuration server associated to the  resource, and waits for the configuration to be loaded  in a slot. Finally the CFM requests the execution of the  configuration loaded in the given slot, and waits for a  notification of the end of the configuration execution  by the ICC/OCC/core.  • Finally the interrupt manager (ITM) is in charge  of checking the status of the resource, by means of  several interrupt lines from other components of the  CCC, the NI, and the core. According to an interrupt  mask programmed by the host, any unmasked interrupt  is forwarded to the host through the NoC. As  mentioned above, the “End of task” interrupt is a key  control signal issued by the µPM to notify the host of  the completion of the current task.  6.4. CCC RTL design and results  The proposed CC controller is implemented using a  standard flow, starting from a register-transfer-level  (RTL) description in VHDL, with a modular approach  describing all the components presented in the microarchitecture. Since the targeted data-flow applications  are often heterogeneous, the source files are templatebased, and generated according to the needs. The  designer only needs to provide a small configuration  file, in which the number of inputs and outputs of the  core are parameterized, as well as the micro-program  memory size and the number of configuration slots  available for the ICCs, the OCCs and the core.  Different CCC were generated, and synthesized in  the STMicroelectronics CMOS 65nm LP technology.  Table 3 presents the implementation results of an  instance of the CCC.  Table 3: Implementation results  Parameters  1 ICC (4 slots), 1 OCC (4 slots)  64 micro-program instructions  Area (ST 65nm) 60,000µm²  Equivalent area  23 kGate  Max. frequency  500 MHz  Power @fmax  3.5 mW              7. CCC applicative performance  7.1. Telecom application case-study  To validate and evaluate the performances of the  proposed programming model in realistic conditions,  we have developed a complete NoC system with CC  controllers for each resource and map a concrete  application on it. The application corresponds to the  physical layer of a digital baseband receiver for  wireless telecommunication supporting multi-antenna  (MIMO) and orthogonal frequency division multiple  access (OFDMA) techniques in the frame of current  studies on 3GPP/LTE normalization [7]. The block  diagram of the reception chain is presented Figure 12.  In the following of the section, we intentionally focus  more on configuration and communication aspects than  detail signal processing functions.  Antenna 1 500Mbps OFDM 1 OFDM 1 200Mbps Antenna 2 500Mbps OFDM 2 OFDM 2 270Mbps CFO CFO CORR 1 CORR 1 CHAN CHAN EST 1 EST 1 200Mbps CFO CFO EST EST CFO CFO CORR 2 CORR 2 MIMO MIMO DEC DEC CHAN CHAN EST 2 EST 2 RX BIT RX BIT 200Mbps 100Mbps CHAN DEC CHAN DEC Received data 10.8Mbps Figure 12 : Block diagram of the reception chain  To map this application, a heterogeneous meshbased NoC architecture (Figure 13) composed of 6  resource  types has been defined. The  topology  voluntary  leaves empty places  for  future uses  (transmission chain).  (Reconfiguration Hardware) resources are coarse-grain  reconfigurable cores for complex matrix computations.  The CPU resource plays the role of host processor  according to the programming model.  7.2. Application mapping and results  The mapping of the application on the considered  architecture leads to complex data flows since some  algorithm functions have been divided on several  physical resources. We consider now a configuration  scenario corresponding to the process of a telecom subframe structure called TTI  (Time Transmission  Interval). This scenario is repeated for each TTI  reception. In terms of data, a TTI represents the  demodulation of 14 OFDM symbols of 1024  subcarriers in a constrained time of 1ms. For the TTI  demodulation, the application is split in 3 execution  phases whose data flows are presented Figure 14. In  phase (a), RH1 and RH2 compute the CFO estimation  from pilot subcarriers stored in SME1 and SME2.  Results are sent to the CPU and will be used as  configuration parameters for the next phase. Phase (b)  corresponds to the channel estimation performed by  RH1 and RH2. Finally, in phase (c), data subcarriers  are sent to the MIMO decoder parallelized between  RH3 and RH4 and the end of demodulation is  performed by the specialized resources.  (a) OFDM 1 SME 1 RH 1 CPU OFDM 2 SME 2 RH 2 SME 1 RH 1 RH 2 SME 2 (b) RH 3 RH 4 SME 3 RX BIT CHAN DEC RH 1 CPU SME 1 (c) RH 1 RH 2 SME 1 OFDM 1 SME 2 RH 3 RH 4 CHAN DEC SME 2 e c a f r e t n i O / I C o N OFDM 2 RH 2 SME 3 RX BIT Figure 13 : NoC architecture  The  resources OFDM, CHAN_DEC  (channel  decoding) and RX_BIT (demapping, deinterleaving)  are specific reconfigurable IP cores that directly  support functions of the reception chain. The SMEs  (Smart Memory Engine) are reconfigurable memory  resources with functions to buffer and reorganize data.  Their internal memory spaces are also used to store  configuration parameters (and description tables) and  additional hardware mechanisms are implemented so  that they work as configuration servers. The RH  Figure 14 : Application data flows after mapping  Before each phase, the CPU resource reconfigures  the micro-programs to map the corresponding data  flows. Thanks to the CC controller configuration and  synchronization mechanisms, the resource are fully  autonomous during a phase, this means no host  processor intervention even for complex data flow.  “End of Task” interrupt packets sent by resources  inform the CPU on the phase executions to synchronize  the reconfiguration.  The requirement for a reconfiguration scheme is  demonstrated  in  table 4, where a count of  configurations used for this application is presented.                          Table 4: Application configuration complexity  Maximum number of  configurations used in a  same phase  OCC  CORE  ICC  Total available  configurations for the  application  OCC CORE  ICC  *(I/O/C):  Number of ICC,  OCC and cores  Resources (I/O/C)*  OFDM1 /2 (2/2/1)  RH1 /2 (1/1/1)  SME1 /2 (4/4/-)  RH3 /4 (1/1/1)  RX_BIT (4/3/2)  SME3 (4/4/-)  CHAN_DEC  (1/1/1)  1  2  2  4  4  4  1  1  2  3  1  3  3  1  1  7  n.a.  1  2  n.a.  1  1  6  3  4  4  4  5  1  5  6  1  7  7  5  1  7  n.a.  1  10  n.a.  5  In addition to the 3 processing phases, the reception  chain supports 5 MCS (Modulation and Coding  Scheme) determined by higher layers of the 3GPP/LTE  protocol. It leads to a large potential combination of  configuration for each resource. Depending on the  MCS and current phase, the host processor has just to  send at one time the appropriate micro-programs  whereas the resource will download the configuration  parameters all along the execution time. RH1 needs 7  different CORE configuration during phase (b) and  RH3 switch between 4 sources of data in phase (c). The  Table 5 focuses on  the micro-programming of  resources. Some resources have simple sequences (3  instructions), others needs more complex data flow  such RH1 in phase (a) which computes data from  SME1, then receives results from RH2 to finish the  computation. Loop registers are  largely used  to  partially  reconfigure micro-programmed sequence  according to MCS parameters.  Table 5: Application Micro-program complexity  Micro-program  Used in a same phase  (Number of instructions)  3 (9)  3 (25)  5 (15)  3 (14)  9 (27)  7 (21)  1 (4)  Total available  for application  3  6  9  3  21  11  5  Loop  registers  used  0  3  1  1  2  3  0  Resources  OFDM1 /2  RH1 /2  SME1 /2  RH3 /4  RX_BIT  SME3  CHAN_DEC  Simulations have been performed with a 400MHz  clock frequency for resource modelled in RTL and a  500Mflits/s throughput for a TLM/SystemC [6] NoC  simulation model.  The whole system initialization and phase (a)  configuration take 20µs, then reconfigurations to phase  (b) and (c) take respectively 2 and 4µs. Compared to  the TTI time of 1ms, the reconfiguration duration  performed by the host processor represents no more  than 2,6 % of the total processing time, the rest of  reconfigurations are initiated by resources during the  execution phases (no overhead due to configuration).  This all case-study confirms the programming model  supports a highly-complex dataflow and multi-mode  application  and  stays compatible with  timing  constraints.  8. Conclusion  In this paper, the concept of Configuration and  Communication Controller has been proposed  to  extend  the classical Network Interface of NoC  platforms. In order to alleviate centralized software  scheduling in the host processor, this CCC brings  distributed hardware support for synchronization and  fast reconfiguration for heterogeneous dataflow multicore applications. The CCC has been successfully used  to map a 3GPP/LTE application with hard real-time  constraints. It has been implemented in a 65 nm  technology, showing encouraging results in terms of  performance, area and power consumption. Future  works will address the automation of the application  mapping onto distributed CCCs.  9. "
CTC - An end-to-end flow control protocol for multi-core systems-on-chip.,"We propose Connection then Credits (CTC) as a new end-to-end flow control protocol to handle message-dependent deadlocks in networks-on-chip (NoC) for multicore systems-on-chip. CTC is based on the classic end-to-end credit-based flow control protocol but differs from it because it uses a network interface micro-architecture where a single credit counter and a single input data queue are shared among all possible communications. This architectural simplification reduces the area occupation of the network interfaces and increases their design reuse: for instance, the same network interface can be used to connect a core independently of the number of incoming and outgoing communications. CTC, however, requires a handshake preamble to initialize the credit counter in the sender network interface based on the buffering capacity of the receiver network interface. While this necessarily introduces a latency overhead in the transfer of a message, simulation-based experimental results show that the penalty in performance is limited when large messages need to be transferred, thus making CTC a valid solution for particular classes of applications such as video stream processing.","CTC: an End-To-End Flow Control Protocol for Multi-Core Systems-on-Chip Nicola Concer, Luciano Bononi Dipartimento di Scienze dell’Informazione Universit `a di Bologna {concer,bononi}@cs.unibo.it Michael Souli ´e, Riccardo Locatelli ST Microelectronics {michael.soulie,riccardo.locatelli}@st.com Grenoble, France Luca P. Carloni Computer Science Department Columbia University in the City of New York luca@cs.columbia.edu Abstract We propose Connection then Credits (CTC) as a new end-to-end ﬂow control protocol to handle messagedependent deadlocks in networks-on-chip (NoC) for multicore systems-on-chip. CTC is based on the classic end-toend credit-based ﬂow control protocol but differs from it because it uses a network interface micro-architecture where a single credit counter and a single input data queue are shared among all possible communications. This architectural simpliﬁcation reduces the area occupation of the network interfaces and increases their design reuse: for instance, the same network interface can be used to connect a core independently of the number of incoming and outgoing communications. CTC, however, requires a handshake preamble to initialize the credit counter in the sender network interface based on the buffering capacity of the receiver network interface. While this necessarily introduces a latency overhead in the transfer of a message, simulationbased experimental results show that the penalty in performance is limited when large messages need to be transferred, thus making CTC a valid solution for particular classes of applications such as video stream processing. 1 Introduction Future generations of systems-on-chip (SoCs) will consist of heterogeneous multi-core architectures with a main general-purpose processor, possibly itself consisting of multiple processing cores, and many task-speciﬁc subsystems that are programmable and/or conﬁgurable. These sub-systems, which are also composed of several cores, will provide higher efﬁciency in supporting important classes of applications across multiple use-case scenarios [14, 19].  
 
       #  	           ""  !""    Figure 1. NoC-based System on Chip and the media-accelerator sub-system [6]. Heterogeneity is the combined result of hardware specialization, reuse of Intellectual Property (IP) modules, and the application of derivative design methodologies [7]. Programmability makes it possible to upgrade dedicated software and add the support of new applications and features that were not included at the chip design time. Some current SoCs already offer task-speciﬁc subsystems such as media accelerator subsystems including heterogeneous and specialized cores (e.g. video and audio decoders) that are connected to a shared bus and communicate through the global memory [7]. This approach, however, offers limited programmability and reuse and does not optimize the utilization of the resources (e.g., the buffering queues in the core interfaces are typically sized for the worst-case scenario). Instead, communication among these cores in future SoCs will likely be based on the network978-1-4244-4143-3/09/$25.00 ©2009 IEEE  1 	  	                (a) 
  	  
  	                       (b) Figure 2. NIs connecting cores to the NoC and the possible message dependencies in the (a) shared memory and (b) message passing communication paradigms. on-chip (NoC) paradigm [2, 8, 16]. These NoCs will be also heterogeneous as illustrated in Fig. 1, where a top-level network connects the main components of the chip, while other sub-networks support auxiliary subsystems such as the media accelerator. In some of these task-speciﬁc subsystems the communication will be based on the Message Passing paradigm, which improves the performance of many important applications such as video stream processing and other multimedia applications [15]. In this paper we focus on the design of the sub-network interconnecting the cores of a message-passing sub-system and we propose an end-to-end ﬂow control that optimizes the ﬂexibility and reusability of the cores through the deﬁnition of a uniﬁed network interface (NI) design. Network interfaces are a crucial component for design and reusability in the NoC domain because they decouple the design of the cores from the design of the network. NIs implement the NoC communication protocols and improve performance by providing elasticity between intercore communication tasks and intra-core computation tasks thanks to their storage capabilities. As shown in Fig. 2 input and output queues are used to temporarily store the incoming and outgoing messages. While messages are the units of transfer between the network clients (processors and memories), in the network interface a single message is typically broken down into a sequence of smaller packets for routing purposes; packets may be further segmented in ﬂow control digits (ﬂit) for more efﬁcient allocation of 2 network resources such as link bandwidths and queue capacities [9, 11]. The correct operations of a network requires to efﬁciently handle deadlock situations which may arise due to the circular dependencies on the network resources that are generated by in-ﬂight messages. A variety of methods has been proposed in the literature to either avoid or recover from deadlock [1, 9]. Most of these protocols assume the consumption assumption where the packets of a message traversing the network are always consumed by the destination core once they reach its corresponding network interface [22]. However, as depicted in Fig. 2 and discussed in detail in Section 2, deadlock may be caused also by dependencies that are external to the network, i.e. dependencies that are internal to a core. In fact, in real SoC systems and multiprocessor systems a core typically generates new messages in response to the reception of a previous message. These dependencies between messages can generate a different type of deadlock that is commonly referred as message-dependent (or protocol) deadlock [15, 18, 22]. Message-dependent deadlock occurs at a level of abstraction that is higher than the routing-level deadlock, which can be addressed by deadlock-free routing algorithms such as dimension-order routing [9, 11]. 1 Various solutions for messagedependent deadlock have been proposed in the literature. Dielissen et al. solve this problem by guaranteeing sufﬁcient storage space for each possible pair of communicating elements [10]. Anjan et al., instead, add timers into the router’s output ports to detect deadlock occurrences and move the blocked packets into specialized queues to guarantee progress [1]. Song et al. propose a deadlockrecovery protocol motivated by the observation that in practice message-dependent deadlocks occur very infrequently even when network resources are scarce [22]. These three approaches, however, are meant for parallel computing systems and are not expected to scale well to SoC design. The message-dependent deadlock problem in NoC for shared-memory architectures has been addressed by introducing two physically-separated networks for the two message types (load and store requests) [20] or two logicallyseparated network (virtual networks) [7]. These solutions may be difﬁcult to scale to future multicore SoCs where the increasing number of heterogeneous cores and message types is likely to grow, thus leading to more complex dependencies among packets. The ÆTH ER EA L [13] and FAU S T [12] NoCs use creditbased (CB) end-to-end ﬂow control protocols. Similar to the credit-based ﬂow control mechanisms that operate at the Related Work. 1We focus on addressing message-dependent deadlock while assuming the use of a deadlock-free routing algorithm. Notice that messagedependent deadlock is different from application-level deadlock which is out of the scope of this paper. 	 
 	  	 
 	  	 	 	 	  	 	 
 	  	 
  
  
 
  
  
  
 
  link level between a pair of interconnected routers [9, 11], a CB end-to-end ﬂow control protocol uses credits to inform a sender NI about the current storage capacity of the queue in the receiving NI. As discussed in more detail in Section 3, the sender NI keeps track of this capacity with a credit counter that is initialized with a value equal to the size of the corresponding queue and is dynamically updated to track the number of available packet slots in the queue. Hence, the sender continuously transmits only a subset of the message packets that is guaranteed to eventually arrive inside the NI, thus avoiding a message-dependent deadlock. Notice that for a given SoC a core that may send messages to N different cores needs N credit counters while if it can receive messages from M different cores it needs M different queues. Contributions. We build on the CB approach to develop Connection then Credits (CTC), an end-to-end ﬂow control protocol that allow us to handle the message-dependent deadlock while simplifying the design of the network interface, which is based on the same micro-architecture regardless of the number of communications that its core may require. This micro-architecture uses a single credit counter together with an output queue for sending all the possible outgoing messages and a single pair of data-request queues that is shared across all possible incoming messages. On the other hand, as explained in Section 4, CTC requires the completion of a handshake procedure between any pair of cores that want to communicate before the actual message transfer starts. This procedure is used to initialize the credit counter in the sender NI based on the current available space in the data queue of the receiver NI. While this necessarily adds a latency overhead to the transfer of the message, the penalty in performance is limited when large messages need to be transferred as it is shown by the simulation results that we report in Section 5. 2 Message-Dependent Deadlock There are two main communication paradigms for exchanging data among the processing cores of a system-onchip and they are associated to two corresponding programming models: shared memory and message passing. In a shared-memory paradigm the processing cores communicate via data variables that are deﬁned in the same logical memory space and are physically stored in one or more memory cores. As shown in Fig. 2(a), a processor accesses a memory through either a load or a store request by specifying the memory address and the size of the data block to be transferred. In the case of a load request the addressed memory replies by sending the values of the requested block of data (typically a cache line) to the processor, which saves them in its local cache memory. In the case of a store request the memory receives new values for a block of addresses, which typically correspond to a line in the processor’s local cache, and it replies by generating a short ACK message to conﬁrm their correct delivery. Shared memory is the most used paradigm in current multi-core SoCs. In the message passing paradigm, which is illustrated in Fig. 2(b), the processing cores communicate by sending/receiving data that are pushed directly from a core to another (peer-to-peer communication): the sending and receiving cores are commonly referred as the producer and consumer, respectively. By having dedicated logical addressing space for each processing core and providing direct communication among their physical local memories, message passing avoids the issues of shared-memory coherency and consistency [17], thus potentially reducing the communication latency of each data transfer. This paradigm is particularly suited for data-ﬂow and stream processing applications that consist of chains of processing cores such as the video processing pipeline [15]. The correct implementation of shared memory and message passing paradigms in a system-on-chip requires an underlying NoC with communication protocols that guarantee the correct transfer of each message and, particularly, the absence of deadlocks. As discussed in the Introduction, even if the NoC relies on deadlock-free routing algorithms, message-dependent deadlock may arise due the dependencies among the messages “inside a core”, which are shown in Fig. 2: e.g. the dependence between a load request and response in a memory for the shared memory paradigm and the causality dependency between the consumption and production of data in a core for the message passing paradigm. For both paradigms, the dependencies between pairs of messages may get combined, thus leading to message dependency chains [21]. Indeed, the causality tial order relation ≺ over the set of all possible messages relations among pairs of messages can be modeled as a parthat are transferred in the network. Message dependency chains depend on the chosen communication paradigm and the characteristic of the given application [15]. Example. Fig. 3 shows a simple example of a messagedependent deadlock that may occur due to the dependence between the messages that are received by (sent from) a memory core in a shared memory environment. The network interface N I0 receives packets for a memory load (or store) request message addressed to M emoryA and in reply sends packets with a response message that includes the requested data (or the acknowledgment of a store operation). Since the input and output queues of N I0 have necessarily limited storage capacity, a long sequence of requests may cause a back-pressure effect into the NoC. For instance, the packets of a series of load request messages Loadi from P rocessorA may not be fully stored within N I0 and, instead, may have to wait for several clock cycles in the East queue of Router0 . Then, let’s assume 3       
 
 
 	     	 	               	  
     	 	               	 Figure 3. Message-dependent deadlock in a shared-memory request-response paradigm. that P rocessorB sends a series of load request messages Loadj to M emoryB . Even if M emoryB can immediately serve a ﬁrst subset of these requests, the packets of the corresponding response messages will not be able to reach P rocessorB because they will be blocked as they attempt to access the East Queue of Router0 . On the other hand, when M emoryA will be ﬁnally able to serve the request messages Loadi , the packets of its response messages will not be able to reach P rocessorA because they will be blocked as they attempt to access the West Queue of Router1 , which are occupied by some of the packets of the load request messages Loadj . In summary, even if the NoC uses a deadlock-free routing algorithm, the dependencies across the messages inside the memory cores cause a circular dependency involving N I0 , Router0 , Router1 , and N I1 which leads to a deadlock. Similarly to routing-dependent deadlock, the messagedependent deadlock problem can be addressed with either avoidance or recovery strategies. The relative advantages of the various techniques based on these two approaches depend on how frequently deadlocks occur and how efﬁciently (in terms of resource cost and utilization) messages can be routed while guarding against deadlocks [22]. The introduction of a Virtual Network (VN) for each type of message transfer guarantees the solution of the messagedependent deadlock by satisfying the consumption assumption [7, 22]: the input and output queue of each router and each NI in the network is replicated and assigned to a single speciﬁc message class (e.g. two classes in case of memory request and response messages). This solution “cuts” the causality dependency between messages in the network at the cost of a higher buffer requirement and more complex router and NI design. Stream processing applications implemented with a pipeline of processing cores, where each core produces data for the next consumer core, lead to a dependency chain of (cid:2) 4 message requests request1 ≺ · · · ≺ requestn where n is the number of cores in the pipeline. For example, Fig. 4 shows the task graphs of the Video Object Plane Decoder (VO PD) and Multi-Window Display (MWD) applications where the nodes represent the tasks while the arcs represent the communications between the tasks and the relative bandwidth requirements [3]. The task graphs of these applications, which are examples of stream processing applications, are similar and present a fairly-linear pipeline structure with a sequence of twelve stages, each stage corresponding to a task. Hence, an optimally-concurrent SoC implementation where each task is mapped to a distinct processing core and where each inter-task communication is implemented by end-to-end message passing would lead to as many message types as the number of arcs. Multi-core SoCs for embedded products simultaneously support an increasing number of such streaming applications. This translates into the presence of complex communication patterns among the cores, which simultaneously run multiple threads of computation to implement the multiple tasks of the various applications. The implementation of the communication requirements among these cores with a NoC requires new solutions for the message-dependent protocol. In fact, a solution based on virtual networks does not scale as the number of distinct message types that travel on the network continues to grow. Furthermore, the length of the dependency chains is difﬁcult to predict because it depends on the given application. 3 Credit Based (CB) Protocol A different approach to the solution of the messagedependent deadlock is based on the use of an end-to-end ﬂow control protocol that guarantees that a sender NI does not ever inject more ﬂits in the network than the corresponding receiver NI can eject. The Credit Based (CB) end-to-end ﬂow control protocol is a simple implementation of this idea that has been used in previous works [12, 13]. With a CB protocol, the sender NI maintains a detailed knowledge of the number of queue slots that the receiver NI has still available through the exchange of end-to-end transmission credits. A credit can be associated to either a packet or to a packet ﬂit depending on the desired level of granularity 2 . What is important is the guarantee that no fragment of a message can remain blocked in the network due to the lack of space in the NI input queue, with the potential risk of causing a deadlock situation. Hence, the sender NI can continue to inject ﬂits in the network only if it has still enough credits as proofs that the receiver NI will eject these ﬂits. Dually, the receiver NI must send a credit back to the 2 In this paper we use the granularity of the ﬂit and we refer to them as ﬂit-credits.  70  
	  362  	 	 362   
  64   49  362    
 300    
  27  	  353  500  313  357  16  128  
 96  96  64  96  64  64          96      96   96  
    94  313   16  (a)  
 96  
  
 64  	 (b) Figure 4. The MP (a) Video Object Plane Decoder (VO PD) and (b) Multi-Window Display (MWD) task graphs. sender NI for each ﬂit that its core has consumed, thereby generating an empty slot in its input queue. Generally a single consumer core can be addressed by multiple producers. Also a producer can address multiple consumers and for each of these the producer needs a separated credit counter. Differently from credit-based ﬂow control mechanisms that operate at the link level between a pair of interconnected routers [9, 11], here a pair of communicating cores may be separated by multiple hops in the network. Also all packets generated by the peer cores arrive at the same NI’s input port. Fig. 5(a) shows the simplest way to address this issue. Each NI is provided with multiple and independent input and output queues and credit counters: one input queue for each possible sender NI and one output queue and credit counter for each addressed NI. A generic N Id , upon the reception of a ﬂit from N Is , saves the incoming data into Qs , the input queue associated to the source N Is . When N Id reads a ﬂit from Qs , it generates an end-to-end ﬂit-credit to send back to N Is . In turn, N Is updates the credit counter Cd associated to the destination N Id . Multiple credits can be combined into one single end-to-end credit message for better efﬁciency. The amount of ﬂit-credits K associated to a single credit-message is a ﬁxed parameter of the system. The size Q of each input queue depends on the value of K because the NI must be capable of storing K incoming ﬂits for each ﬂit-credit that Pc = {ni . . . nj } that can possibly communicate with the it has generated. In particular considering the set of peers consumer nc , nc ’s input queues should be sized as: (cid:2) (cid:3) RT T (ni , nc ) | ni ∈ Pc Qnc = K + M ax (1) where RTT is the zero-load round-trip time function that depends on the given NoC implementation: speciﬁcally it depends on the distance in hops between the NIs, the latency 5 of the routers, and, in case of channel pipelining [5], on the number of stages of each pipelined channel. On the input side, the Input Arbiter selects the incoming data to process while on the output side, the Output Arbiter selects the queue that is used to forward the ﬂit or to send a credit. The selection of the input and output queues is made on a packet basis to avoid the delivering/reception of ﬂits of different packets, e.g. according to a round-robin policy. The CB end-to-end ﬂow control protocol differs from the virtual network approach for two main of reasons. First, in VN all the queues, including those in the routers, must be replicated while in the CB protocol only the queues of the NI must be replicated. Second, in VN the number of queues per channel depends on the length of the application message-dependency chain while in the CB protocol this number varies for each given consumer core depending on the number of producer cores that may address it. 4 Connection Then Credits (CTC) Protocol Adding a dedicated input and output queue for each possible source/destination of messages, as required by the CB ﬂow control protocol, forces engineers to design a speciﬁc network interface for each node of the network. This is the case particularly for multi-use-case SoCs where the interaction between cores is driven by the application run by the user [14, 19]. As an alternative to CB, we present the “Connection Then Credits” (CTC) ﬂow control protocol. CTC rationalizes and simpliﬁes the design of NIs while guaranteeing the absence of message-dependent deadlock. CTC regulates the exchange of messages between two peer NIs by introducing a handshake procedure called Connection. A CTC-message is a ﬁxed amount of data to be exchanged between the two NIs. As shown in Fig. 5(b) a 
 	
 
	  	                                                        (a)  
 	
                 
              	 (b) Figure 5. Network Interface implementations: (a) credit based and (b) CTC. CTC NI is composed of only two input queues and one single output queue independently from the number cores that can require a connection with this NI. The data-queue is used for storing incoming data ﬂits while the request-queue instead is used for the incoming transactions requests. When a producer N Is needs to initiate a connection towards a consumer peer N Id , it ﬁrst sends a request packet3 called P R EQ (packet-request) to N Id to signal its request to communicate. The P R EQ packet also indicates the total size of the message to be delivered and some additional information that can be used by the NI (i.e. for priority decisions). Upon the reception of a P R EQ, N Id stores the request in the request-queue together with the other requests previously received and not yet processed. When the core associated to N Id is available for processing a new request (i.e., the data queue has enough free space to accept a new message) it generates an acknowledge packet called P ACK that is forwarded to the source of the given request. The P ACK packet is similar to the end-to-end credit packet in the CB ﬂow control. The difference is that the ﬁrst P ACK sent by N Id actually initializes the output credit counter of N Is so that it can generate and send a speciﬁc amount of data. Upon the reception of credits the producer ﬁrst generates a header ﬂit used to open a path along the routers of the NoC, then it forwards the data ﬂits and decreases the CTCcounter by one unit for each data-ﬂit that has been sent. Fig. 6 shows an example of the CTC protocol operations: at ﬁrst N I0 and N I2 , address N I1 with two P R EQ messages indicating the size of the transaction they want to initiate (respectively 100 and 80 ﬂits). In Fig. 6(b) N I1 selects N I0 to initiate the connections while it stores the other request in the request-queue. Then N I1 generates a chain of consecutive P ACK packets so that N I0 ’s credit counter 3Note that P R EQ and P ACK are actually messages composed by one single packet. When referring to these two messages we use the two words without distinction. is initialized with the maximum amount of ﬂit-credits that N I1 can offer. As for the CB case, a single P ACK conveys K ﬂit-credits per single message but in the former counters are initialized at start up time whereas in CTC the consumer NI initializes the counter of the selected producer peer. Considering the example in Fig.6 if the size of the data input queue were Q=10 and K=3, N I1 would generate 3 consecutive P ACK messages to initialize N I0 ’s credit counter to the maximum storage it can offer. As Q is not a multiple of K one slot is not assigned at the connection start-up time but it will be assigned as soon as N I1 starts receiving ﬂits and forwarding them to the connected core. Finally, Fig. 6 (c) shows the data-packet generated by N I0 reaching the dataqueue of N I1 . Independently from the consumption rate of the core, in both considered end-to-end ﬂow controls each time a consumer frees K slots in the data input-queue, it generates a new P ACK message until the sent credits are sufﬁcient to transfer all the ﬂits of the requested transition (whose size was speciﬁed in the P R EQ message). Considering the example in Fig. 6 with K = 3, N I1 generates a new P ACK for each K ﬂits that are consumed. Hence the 100 ﬂits of message M require a total of (cid:4)M /K (cid:5) = 34 P ACK messages. As the message’s length M is not a multiple of K, the consumer NI assigns to the connection a bigger amount of memory slots than M. Nevertheless at most K-1 slots of Q can be reserved but not actually used. These additional slots are marked as dirty and freed when the connection is terminated. Finally upon the sending/reception of the last ﬂit of M, the producer and the consumer terminate the connection. Note that there is no need of speciﬁc ﬂags or instructions as both producer and consumer nodes know the exact size of the transfer to process. 6              N 1   
	   	 . Q Q a a t D . Q q e R  N 1    
	  
 	 	 	  . Q a a t D . Q q e R          (a) (b) N 1   	   	       . Q a t a D . Q q e R  
   (c) Figure 6. The CTC transaction-deﬁnition procedure: (a) the core interface N I1 receives two P R EQ requests from N I0 and N I2 ; (b) N I1 selects the request from N I0 generating the relative P ACK and stores the one from N I2 in the request-queue; (c) N I1 receives the data-packets from N I0 . To avoid throughput degradation, the data-input queue should be sized accordingly to Equation 1 as function of the number of credits per P ACK and the maximum round trip time between the consumer NI and the producer addressing it. Using a ﬁxed value of K rather than a dynamic one, reduces the number of control packets traveling on the network. Moreover the parameter K inﬂuences both the number of CTC-control messages traveling along the network and the size of the data input queue on the NIs. Hence it has a clear impact on the design and performance of the network. According to Equation 1 a small values of K allows to reduce the size of the input queue saving area and power consumption. On the other hand, it implies a higher number of P ACK packets ﬂowing through the network to update the credits on the producer side. To avoid loss of performance and reduce contentions, a producer node that runs out of credits terminates the injection of the packet by tagging the ﬂit using the last credit as packet tail. In this way the path along the network is made available to other possibly blocked packets. Note that the CTC connection is not terminated until the delivery of all the ﬂits of the message is completed. To guarantee the consumption assumption of the P R EQ messages, the request-queue must be sized accordingly to the maximum number of requests that a NI can receive. By limiting each CTC producer to have one single outstanding P R EQ at time the request queue can be sized according to the number of possible sources of data so that all incoming P R EQ can be stored in the NI and removed from the NoC. CTC deﬁnes three message dependencies that are addressed in the following way: P R EQ→ P ACK: the request-queue is sized accordingly to the number of possible producer-peers addressing the given NI. CTC limits each node to have at most one outstanding P R EQ at time. Hence the consumption of all injected P R EQ is guaranteed. P ACK→data: P ACK packets are always consumed by a network interface that updates the output credit counter and then deletes them. data→ P ACK: the credit mechanism ensures that no more data-ﬂits than those allowed by the output credit counter can ever been injected. Hence all data ﬂits injected in the NoC are eventually consumed by their addressed NI. Thanks to the CTC protocol design, these three dependencies are independent from the applications that is run by the cores and hence the system can be considered protocoldeadlock free. 5 Analysis and Simulation To analyze the characteristics of the CTC protocol and compare the performance of a CTC-based NoC versus a CB-based NoC we developed a C++ system-level simulator that allows us to model various NoC topologies, routing and ﬂow-control protocols as well as the trafﬁc scenarios injected by various types of cores. We used the simulator to compare the two end-to-end protocols on the Spidergon NoC [7] for the case of the VO PD and MWD applications whose task graphs are shown in Fig. 4. We as7                         ) s e l c y c ( y c n e t a L e g a s s e M  240  220  200  180  160  140  120  100  80  60  0.1  CB   CTC   0.2  0.3  0.4  0.5 Offered Load (flit/cycle)  0.6 (a) ) s e l c y c ( y c n e t a L e g a s s e M  CB   CTC    240  220  200  180  160  140  120  100  80  60  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Offered Load (flit/cycle) ) s e l c y c ( y c n e t a L e g a s s e M  500  450  400  350  300  250  200  150  100  0.1 (b)  CB   CTC   0.15  0.2  0.25  0.3  0.35 Offered Load (flit/cycle)  0.4 (c) Figure 7. Message latency as function of the injection rate for (a) VO PD, (b) MWD and (c) Uniform trafﬁc patters.  CB   CTC  ) s e l c y c ( y c n e t a L e g a s s e M  120  110  100  90  80  70  60  50  40  0  5  10  15  20  25 Credits per P_Ack  30  35 Figure 8. Message latency as function of the number of credits associated to a P ACK for the case of the VO PD trafﬁc pattern. signed each task to a different core following the heuristic presented in [4]. We also considered the random uniform trafﬁc pattern where each node may communicate with any other node in the network [9] In the CB-based NoC the NI of each core has a number of input queues equal to the number of incoming streams (see Fig. 4). For both the CTC-based and the CB-based NoC, the size of each data input queue is set uniformly based on Equation 1 and no virtual channels are used. Fig. 7 shows the average end-to-end message latency as function of the average offered load when K is ﬁxed to 32 ﬂit-credits (results are similar for the other credits values) and the messages are 64 ﬂits. As expected, the CTC protocol gives a higher latency due to the handshake procedure. Nevertheless, for the VO PD application the difference between the two protocols remains under 10% up to the saturation point, which is around 0.4. Fig. 8 shows the average peer-to-peer latency as function of the number of credits K per P ACK packet when the offered load is lower than the saturation threshold for the case 8 of the VO PD application. Clearly, by increasing the value of K the performance of the system also improves: PEs can inject more ﬂits per P ACK thus reducing the number of control packets (credits and headers). Conversely, increasing K also requires bigger input queues that must support the additional number of ﬂits received per P ACK sent. Fig. 9 reports the throughput comparison as function of the message size M. As expected, in all these scenarios, the performance of the CTC-based NoC increases with the message size for the same offered load because the rate of connections-per-ﬂits that must be set up is reduced. Therefore, CTC represents a valid proposition for messagepassing applications such as video stream processing that present large inter-core message transfers. Finally, we analyze the amount of storage used by the two alternative ﬂow control protocols. As discussed in Section 3, for both CB-based and CTC-based NoCs the input queues of the network interfaces must be properly sized to avoid throughput degradation. For a CB-based NoC each input queue must be sized accordingly to Equation 1. For a CTC-based NoC, instead, only the data-queue must have this size while the request-queue of a given interface must be as large as the number of distinct producer cores that can send message to its core. Notice that in order to provide a single interface design for each possible core, this number can be over-estimated without a major loss of area because the request-queue has a negligible size compared to the data-queue. Fig. 10 shows the breakdown of the aggregate number of data-queues used in the network interfaces for the two approaches to support the given applications. In VO PD and MWD only the interfaces associated to nodes with incident (outgoing) arrows actually require input (output) queues. Considering VO PD the CTC-based NoC uses a total of 22 data queues, including both input and output, while the CB-based NoC needs 30 data queues. Assuming that the length of each data queue is the same in the two NoCs, CTC allows to save up to 35% of storage space for this particular case study. In the MWD case, since most of                 0.6  0.5  0.4  0.3  0.2  0.1  0  8  16  32  Message Size (flits)  64  T h r u p h g u o t / t i l f ( c y c l e ) CB CTC  (a) 0  0.1  0.2  0.3  0.4  8  16  32  Message Size (flits)  64  T h r u p h g u o t / t i l f ( c y c l e ) CB CTC  (b) 0  0.1  0.2  0.3  0.4  8  16  32  Message Size (flits)  64  T h r u p h g u o t / t i l f ( c y c l e ) CB CTC  (c) Figure 9. NoC Throughput as function of the message size when K = 4ﬂit-credits. 0  20  40  CB  CTC  N u m . o f Q s e u e u Input  Output  (a) 0  20  40  CB  CTC  N u m . o f Q s e u e u Input  Output  (b) 0  100  200  300  CB  CTC  N u m . o f Q s e u e u Input  Output  (c) Figure 10. Breakdown of the aggregate number of input and output queues in the NoC NIs for the VO PD application. the cores communicate only with one other core the two NoCs have many interface with similar structures. Still, even in this case CTC allows to save up to 24% of storage as reported in Fig. 10. Finally, the Uniform trafﬁc pattern represent the special case where each node may communicate with any other node of the NoC. In this case clearly CB is an expensive solution as it requires N − 1 queues where N is the number of nodes in the network. In fact, in absence of a negotiation protocol, replicating the queues is the only way to guarantee that ﬂits of different messages are not mixed in a single queue and that all injected ﬂits are also ejected from CTC can be very high because all N − 1 input and output the NoC. In comparison, the storage reduction achieved by queues of each node are replaced by the single input dataqueue and the output queue. In summary, the reduction of the size of the queues that must be installed in each network interface translates directly in a reduction in the area occupation and is expected to lead also to a reduction in overall NoC power dissipation. 6 Conclusions Message-dependent deadlock is a destructive event that, even if rare [22], must be properly addressed to guarantee the correct behavior of a network. The credit based (CB) end-to-end ﬂow control protocol solves this problem by using multiple dedicated input queues and output registers in the network interfaces. This increases the complexity of the network interface design. Further, since the number of these queues depends on the number of distinct communications that its particular core may have, the same network may present interfaces that have different microarchitectural structures. We proposed the Connection Then Credits (CTC) endto-end ﬂow control protocol as an area-efﬁcient solution to the message-dependent problem that is characterized by a simpler and more modular network interface architecture. CTC-supporting network interfaces use one single input data queue and one output credit counter. Hence, the overall number of queues per network interface remains equal to two, the total amount of storage is reduced and the overall network-interface design becomes independent from the communication requirement of the particular core, thus increasing its reusability. On the other hand, any new communication between a pair of peer nodes requires the preliminary completion of a handshake procedure to initialize the output credit counter on the producer side (after the connection has been established CTC works in a way similar to the original Credit Based ﬂow protocol). This procedure necessarily increases the latency of a message transfer and it also reduces the network throughput for small messages. In summary, the choice between CB versus CTC may be seen as a case of typical “performance versus area” tradeoff. From this perspective, experimental results show that for a video processing application the latency penalty remains 9                               [11] J. Duato, S. Yalamanchili, and L. Ni. Interconnection Networks. An Engineering Approach. Morgan Kaufmann Publishers, San Mateo, CA, 2003. [12] Y. Durand, C. Bernard, and D. Lattard. FAUST : On-chip distributed architecture for a 4G baseband modem SoC, in. In Proceedings of Design and Reuse IP-SOC, pages 51–55, Nov. 2005. [13] O. P. Gangwal, A. R ˘adulescu, K.Goossens, S. G. Pestana, and E. Rijpkema. Building predictable systems on chip: An analysis of guaranteed communication in the Æthereal network on chip. In P. van der Stok, editor, Dynamic and Robust Streaming In And Between Connected ConsumerElectronics Devices, volume 3 of Philips Research Book Series, chapter 1, pages 1–36. Springer, 2005. [14] A. Hansson, M. Coenen, and K. Goossens. Undisrupted quality-of-service during reconﬁguration of multiple applications in networks on chip. Proceedings of the conference on Design, automation and test in Europe, pages 954–959, Apr. 2007. [15] A. Hansson, K. Goossens, and A. R ˘adulescu. Avoiding message-dependent deadlock in network-based systems on chip. VLSI Design, 2007:Article ID 95859, 10 pages, May 2007. [16] A. Hemani et al. Network on chip: An architecture for billion transistor era. In 18th IEEE NorChip Conference, page 117, Nov. 2000. [17] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative Approach. Morgan Kaufmann, 2006. [18] H. D. Kubiatowicz. Integrated Shared-Memory and Message-Passing Communication in the Alewife Multiprocessor. PhD thesis, Massachusetts Institute of Technology. Boston, 1997. [19] S. Murali, M. Coenen, A. Radulescu, K. Goossens, and G. D. Micheli. Mapping and conﬁguration methods for multi-use-case networks on chips. Proceedings of the 2006 conference on Asia South Paciﬁc design automation, pages 146–151, 2006. [20] S. Murali and G. D. Micheli. An application-speciﬁc design methodology for STbus crossbar generation. In DATE ’05: Proceedings of the conference on Design, Automation and Test in Europe, pages 1176–1181, Apr. 2005. [21] T. M. Pinkston and J. Shin. Trends toward on-chip networked microsystems. Intl. J. High Performance Computing and Networking, 3(1):3–18, 2001. [22] Y. H. Song and T. M. Pinkston. A progressive approach to handling message-dependent deadlock in parallel computer systems. IEEE Trans. on Parallel and Distributed Systems, 14(3):259–275, 2003. under 10% while the savings in terms of the overall area occupation of the network interfaces reaches 35%. Therefore CTC is an effective solution of the message-dependent deadlock problem for throughput-driven stream processing applications. Acknowledgments This work is partially supported by the University of Bologna and ST Microelectronics project funds for the project ”Modeling and Analysis of Network Protocols for Spidergon-Based Networks on Chip”, the National Science Foundation (Award #: 0541278) and by the GSRC focus center, one of the ﬁve research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program. "
Connection-centric network for spiking neural networks.,"A reconfigurable network architecture applied to spiking neural networks is presented. For hardware platforms for neural networks that implement some degree of realism of interest to neuroscientists, connectivity between neurons can be a major limitation. Recent data indicates that neurons in the brain form clusters of connections. Through the combination of this data and a routing scheme that uses a hybrid of short-range direct connectivity and an AER (address event representation) network, the presented architecture aims to provide a useful amount of inter-neuron connectivity. A connection-centric design can provide opportunities for NoCs such as optimising power, bandwidth or introducing redundancy. A method of mapping a network to the architecture is discussed, along with results of optimal hardware specifications for a given set of network parameters.","Connection-Centric Network for Spiking Neural Networks Robin Emery, Alex Yakovlev, Graeme Chester Newcastle University, UK {r.a.emery, alex.yakovlev, graeme.chester}@ncl.ac.uk Abstract A reconﬁgurable network architecture applied to spiking neural networks is presented. For hardware platforms for neural networks that implement some degree of realism of interest to neuroscientists, connectivity between neurons can be a major limitation. Recent data indicates that neurons in the brain form clusters of connections. Through the combination of this data and a routing scheme that uses a hybrid of short-range direct connectivity and an AER (Address Event Representation) network, the presented architecture aims to provide a useful amount of inter-neuron connectivity. A connection-centric design can provide opportunities for NoCs such as optimising power, bandwidth or introducing redundancy. A method of mapping a network to the architecture is discussed, along with results of optimal hardware speciﬁcations for a given set of network parameters. 1. Introduction The nature and purpose of connectivity between neurons in the brain is of great interest to neuroscientists as it may reveal how information is routed around the brain and is subsequently processed to produce higher level behaviours such as pattern recognition. Connectivity in neural networks can be investigated using software and hardware models. However, limitations of the implementations of existing models - such as limited parallelism in software and limited neuron count, limited connectivity and limited learning capabilities in hardware can frustrate such investigation[11]. These limitations can be overcome, but the solutions are non-trivial; for example, expensive massively parallel computer setups can be used to add parallelism to software simulations, or tightly constrained application speciﬁc chips can be designed and manufactured with limited versatility. A new architecture for the implementation of a reconﬁgurable spiking neural network model in VLSI is formulated and presented. The architecture focuses on the connectivity between neurons, by implementing it as a hybrid of a relatively dense short-range spike routing resource and a relatively sparse long-range Address Event Representation (AER[17]) packet-based network-on-chip. This architecture aims demonstrate that the problem of limited scaling of interconnect in reconﬁgurable hardware can be alleviated through the use of a network-on-chip. A method for mapping biologically-inspired networks to the architecture is detailed. Spiking neural networks are generated over a range of parameters and mapped to the architecture using a custom tool chain. The mapping aims to be faithful to the characteristics of the original network. Following the background material, the architecture and mapping process are presented, followed by the results of the mapping process and a small demonstration circuit manufactured in 130nm CMOS. 2. Background 2.1. Neural Networks Neurons in the brain communicate by short electrical pulses, known as “action potentials” or spikes[9], emitted when the potential across the cell membrane of the neuron exceeds a certain threshold. These spikes are generally stereotypical, of similar amplitude (about 100mV) and duration (about 1ms, with a refractory period of about 2ms). The information communicated between neurons is encoded in the timing and rate of spikes. The stereotypical nature of spikes has led to much effort in the area of spiking neuron models [8], as such models offer the potential to help to understand brain functions at a higher level than the neuron, possibly bridging the gap between neural networks and expressive behaviour such as sight or emotion. A neuron forms connections with other neurons via chemical synaptic junctions. A spike propagates through the axon of the producer neuron to the synapse. This triggers the generation of a chemical neurotransmitter which moves over the small gap between the producer and consumer halves of the junction. At the consumer side of the synapse on the dendritic tree, the neurotransmitter produces 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    an offset of the membrane voltage of the consumer neuron. The size of this offset is known as the “synaptic efﬁcacy”, otherwise known as the strength of the synapse, and varies between 2mV and 3mV[13], although the effect is reduced by the time it reaches the soma of the neuron. The efﬁcacy of the synapse is plastic, and varies according to a learning rule[19]. This plasticity is widely thought to be a major contributer to learning. To make effective use of spiking models when attempting to understand the brain, the structure of real neural networks must be observed. The primary visual cortex is one of the most extensively studied areas of the brain as it is the earliest and simplest cortical area for visual processing, and shows a strong relationship to the topographic layout of the retina of the eye. Experimental data on the morphology of neural networks in the brain provides varied results due to the difﬁculty of obtaining a reliable tissue sample and subsequent processing of a large amount of data. However, some conclusions can be drawn. Data on the density of neurons in the visual cortex of the rat indicate a range of 32, 000− 87, 000 mm−3 [10] (this paper makes an overestimate of 100, 000 mm−3 to compensate for possible losses during sample preparation), perhaps higher than 200, 000 mm−3 in primates [1]. Several factors affect the total number of connections made by a neuron. [1] reports that neurons in the monkey visual cortex receive an average of 3900 synapses. [2] indicates that the number of synapses of neurons in the visual cortex of the cat ranged between 975 and 9641. Results presented in [10] indicate that a neuron may receive about 30 synaptic inputs directly from similar neuron types and an order of magnitude more indirectly via small locallyprojecting interneurons1 . This paper also shows that while the probability of connection decreases as one moves away from the soma to about a tenth of what it was, the actual number of connections increases. This is due to the high density of neurons observed; however the volume examined in the paper was small, providing data for proximal connectivity only. Work undertaken by Binzegger et al. [3] provides some results for distal connectivity. They ﬁnd that axons of neocortical neurons form connections in multiple, separate clusters (see ﬁgure 1). Neurons formed between one and seven clusters, with almost all forming at least two and most forming about two or three. The cluster with the highest number of connections (primary cluster, average 2036 connections) typically formed near to the parent soma, and the distance to the next cluster was proportional to the size of the parent cluster. The second cluster typically contained four times fewer connections than the primary cluster, and the distance from the soma ranged between 0.5 − 2 mm. 1 The neocortex is made up primarily of excitatory pyramidal neurons (80%) and inhibitory interneurons (20%). Figure 1. Neural connectivity in clusters (for a single neuron). The graph shows the relationship between the cluster rank and the number of connections formed (black: excitatory, grey: inhibitory). Adapted from [3]. The work theorizes that the “spoke-like” arrangement of clusters may be a means of routing information. 2.2. Neural Networks in Hardware Using digital or analogue circuits in silicon to model neural networks can offer several important advantages over software alone, notably massive parallelism and the associated improvement in simulation time. Existing work on hardware for neural networks is mostly focused on application speciﬁc embedded hardware, rather than on accelerating general purpose computing. Artiﬁcial classiﬁer networks tend to be the most common. Of neuromorphic devices, perhaps the Silicon Retina [12] is the most well known, but many other devices exist, generally affected by limited connectivity or neurons that are difﬁcult to conﬁgure. The most successful reconﬁgurable system of recent years is the Silicon Cortex (SCX) [16], a board-level infrastructure for multiple chip neuromorphic systems in which analogue VLSI chips are connected by digital hardware. However, a limited neuron population and a complex infrastructure makes this system unsuitable for large-scale networks. FPGAs would seem to offer an attractive compromise between software and custom hardware for neural network models. They offer relative low cost, versatility of function and a quicker design turnaround. However, preliminary work by the authors that based a reconﬁgurable neural network on a Xilinx FPGA determined that a hardware system must operate neurons asynchronously to be of interest to neuroscientists, and that a large area was required to implement a useful neuron due to the general nature of the FPGA logic. As a result of the large area required and the ample routing resources available, much of the interconnect was unused and was wasted. A recent review of reconﬁgurable hardware for neural networks provided by [11] concludes that FPGAs do not efﬁciently implement biological neuron models (in part due to the size of multipliers), and that the interconnect structures do not scale with network density. Address Event Representation (AER) is a means of asynchronously multiplexing stereotypical “events” (such as neural spikes) over a link shared between groups of processing blocks. It was ﬁrst used in Mahowald’s stereoscopic vision system [12], and a draft standard has subsequently been produced [17]. In AER, an event is represented by a packet consisting of at least a source or target address. If the address of the target is used in the packet, the packet can be routed over a complex network topology. Events are digital - abstract - so any degradation can be restored without affecting the information conveyed, reﬂecting the physiology of real neural networks. Where AER is used for communication all nodes share the same frame of reference, thus time can be seen as being the same throughout the system, with no skew. Information is then conveyed in the time and number of events, providing a robustness to process variation between chips. When used in an adaptive neural system, a small number of lost events will not massively affect the operation of the system, providing an inbuilt robustness to transient faults. The Network-on-chip (NoC)[7] paradigm is particularly attractive for spiking neural networks as it offers scalability, parallelism and ﬂexibility. In a NoC structure, functional blocks are connected to other blocks by high speed links and routers. Information is transmitted as packets, allowing the nodes on the network to implement advanced behaviours such as load balancing and prioritisation. Many network topologies can be used, but the most common is a mesh structure with a router at each crossing point. The blocks themselves are usually asynchronous or mesochronous relative to each other. For example, NoCs have been applied to hardware neural models as part of a large multiprocessor project[14]. In spiking neural networks all nodes are asynchronous and operate in parallel. The majority of links between neurons are relatively short range, but there is still a signiﬁcant number of long range links. The interconnect of a NoC is scalable as the links of the network are shared by many signals, overcoming the problems associated with a large amount of dedicated interconnect such as excessive delay, wasted area and reduced reliability. As all links are active at the same time and the separate blocks are not synchronous, a high level of parallelism can be achieved. For a reconﬁgurable device, the ﬂexibility of routed packets helps to reduce conﬁguration effort, and for an adaptive device (one that supports real time learning) it can help offer an improved robustness to faults. As a spiking neural network Figure 2. System elements is event based, an event based protocol such as AER is appropriate for the network links. Combining a reconﬁgurable architecture with a NoC and AER provides the potential to implement a neural network that makes optimal use of the hardware resources. By using general trafﬁc statistics or by back-propagating real trafﬁc data, inter-tile links of the NoC can be fully utilised, partly utilised to allow for bursts of trafﬁc, or disabled to conserve power. In summary, a reconﬁgurable FPGA-like neural network device would be of interest to neuroscience, and observations of the structure of the brain support a hybrid of local and distal connectivity with a distinct transition from one to the other. A network-on-chip can provide a scalable interconnect structure for a hybrid reconﬁgurable platform. 3. Network Architecture The structural elements of the proposed architecture are depicted in ﬁgure 2. The neuron is a simple leaky integrateand-ﬁre model, which functions as follows: 1. the neuron sums spikes as they arrive, with the magnitude of the increment forming part of the input. 2. The integration is subject to a small but constant decay. 3. If at any time the value of the integral exceeds a static threshold, the sum is set to zero and a spike is “ﬁred”. The focus of this system lies with the connectivity between neurons. While the leaky integrate-and-ﬁre approximation of a neuron is simple, it captures enough of the behaviour of a real neuron to generate interesting network activity [4]. Neurons are connected to other neurons via synaptic junctions. In response to the arrival of a spike at the input, the synapse will emit a value that is a function of the strength of the synapse; this value then propagates to the input of the neuron. The strength of the synapse is plastic according to the rule of Spike Timing Dependant Plasticsouth, west and east directions. The routers interface with the short-range interconnect (effectively the “core” of the tile) via simple bridges which provide conversion between the spike protocol and the AER network protocol. Tiles are addressed using a the simple X,Y scheme and individual targets inside the tile are addressed using an identiﬁcation number that represents the appropriate AER-to-spike bridge. All of the conﬁgurable elements of the network provide serial conﬁguration ports. The system elements are connected together to form a serial conﬁguration chain in a similar manner to an FPGA, through which a bit stream is clocked to set switches for connections and registers for system elements such as neural thresholds. By implementing a global pause signal, this chain can also be used to inspect the current state of the network by clocking out the values of registers, such as the current integral values of neurons. This architecture will be demonstrated and evaluated by generating networks with similar characteristics to topographic structures seen in the brain, such as orientation maps in the visual cortex. These networks will be used to conﬁgure the system elements, then the architecture will be assessed for suitability for purpose by observing trafﬁc loads, especially where bottlenecks could be affecting the quality of simulation, and by comparing outputs to real observed neural behaviours. 4. Mapping to a Field-Programmable Neural Array The reconﬁgurable spiking neural network architecture discussed in the previous section was designed for biologically inspired networks. As an aid for exploration of the mapping and architecture, networks are generated by a software tool. The characteristics of these networks are based on data in [3]. The software tool takes in as parameters the neuron count N and the neuron density d as the number of neurons per mm2 . It then places the neurons uniformly at random on a 2-dimensional plane with square limits determined by d. Each neuron forms between 1 and 7 circular clusters of variable diameter, distance and angle from the soma of the neuron. Surrounding the soma is a circular catchment area of variable diameter (a rough approximation of a dendritic arbour); when a cluster overlaps the catchment area, a connection between the two neurons is formed (ﬁgure 4a). At the time of writing, the network generation tool creates networks by placing neurons and clusters uniformly at random. Work is currently underway to enhance these networks with groupings of clusters, producing uneven connectivity and networks that feed information towards ﬁxed points. The mapping ﬂows along similar lines to placement and routing tools for IC design. These tools start with a genFigure 3. Network structure. a) example of local inter-CNB connectivity; b) example of superimposed network. ity (STDP[19]), with a high output magnitude representing a strong synapse and a low magnitude representing a weak synapse. A positive magnitude represents an excitatory input, and a negative magnitude represents an inhibitory input. A neuron, multiple synapses and a tree of magnitude aggregators are collected to form a Conﬁgurable Neural Block (CNB). These blocks are repeated many times to form a Cartesian grid, with spike links between blocks on a dedicated conﬁgurable routing resource. In case more synapses are required to feed a neuron, the neuron in each CNB can be bypassed and the aggregated post-synaptic magnitudes fed over short single-hop links into other CNBs. In natural neural networks, the distribution of connection lengths favours relatively short connections. For this architecture, the shorter inter-CNB “direct” spike connectivity consists of static wires of different lengths that form hops between conﬁgurable blocks (ﬁgure 3a). Conﬁgurable switch matrices placed between the CNBs enable routing of spike signals over the grid. Once conﬁgured, each route is a dedicated connection carrying spike information between a ﬁxed spike source and a spike sink. While the majority of connections in a neural network are relatively short, there are still many longer connections that must be made. The cost of making direct connections using long interconnects which require strong drivers and intermittent buffering is likely to be too high given the dedicated nature of the link. In order to minimise the cost of these connections, a network is superimposed over the shorter direct connectivity (ﬁgure 3b); the classical NoC method of imposing hard tile boundaries does not accurately reﬂect the structure of a neural network. The grid of CNBs is divided into rectilinear tiles of a suitably populous size, with one router per tile. Each router is connected to other routers via the serial AER inter-tile links in the north, Layer Network (OSI 3) Direct Connection Not required (point-to-point only, no routing) Data-link (OSI 2) Physical (OSI 1) Spikes are buffered at the the receiver; new spikes are dropped if there is contention for the link. Source and destination are implicit, no access control required. No error detection service on physical layer. Single wire per link; RZI; spike is falling edge. Simplex, ﬁxed at conﬁguration time. Pulse has set minimum width, glitches will be ﬁltered. Indirect Connection AER. All nodes have unique address. Full-duplex mesh topology. Unicast Spikes can be ingress and egress queued if link is busy. Error detection service on the physical layer (malformed frames). High-speed asynchronous serial on-chip link; simplex. Simple error detection on frames; acknowledge signal. Table 1. Summarised direct and indirect spike protocols ture is superimposed (ﬁgure 4c). An additional parameter C , the number of CNB units that form a tile, is provided. C is used to calculate the number of tiles that are superimposed by dividing N by C . The majority of tiles will be the same size (and usually square) but can vary a little if required by the dimensions of the grid. As the network is superimposed the connections between CNBs are not simply routed over the AER network if they cross a tile boundary. A simple rule is used: if the connection reaches further than the longest possible route within a tile (corner-to-corner), and crosses at least one tile boundary, it is deemed to be a long connection and is routed over the AER network. This method maintains the hybrid nature of the connectivity. Spike information is conveyed through the system in two forms: abstract point-to-point spikes, and AER packets. To deﬁne the interaction between these forms, a series of protocols have been deﬁned according to the OSI layer model[18] and summarised in table 1. The protocol Figure 4. Mapping of a neural network to the CNB array. a) The original distribution of neurons; triangles: somas, clusters: circles. b) The network mapped to a grid; squares: neurons, lines: connections. c) The grid mapped to the reconﬁgurable array; squares: CNBs, solid lines: direct connections, dotted lines: tile boundaries, dashed lines: inter-tile connections. eral ﬂoor plan, and then place cells according to this ﬂoor plan aiming to minimise area and interconnect cost for wire length, timing and congestion. The ﬂoor plan in this case is the layout of the generated network, however the placement and routing presented here is different in that it aims to optimise the mapping of the spiking neural network over the hybrid of multiple interconnect types of the architecture described in the previous section. The generated network is mapped to a grid (ﬁgure 4b), where each grid point represents a CNB and every neuron is mapped to a single CNB. This is accomplished by dividing the network plane into a grid of equally sized squares with an area calculated by dividing the plane area by N , giving the mean area per neuron. This process provides a starting point, as there will often be several neurons in a grid square when only one is allowed. To correct this, the neuron count per grid point is treated as another dimension and the grid is repeatedly “squashed”, spreading out the excess neurons until no grid points with multiple neurons remain. This method retains the largely proximal nature of the connectivity between neurons by minimising the disruption of the original layout as much as possible. When the grid is complete, the AER network infrastrucstack facilitates the translation between spikes and packets by deﬁning the services that must be offered by the neurons, synapses and bridges. The stack does not make general provision for tolerance of error or saturation, in keeping with the biological inﬂuence. However, a greater level of error detection is required in areas that are more greatly abstracted from the biology. Speciﬁcally, the loss or delay of many AER packets - representing many links - would affect many more elements than the loss of a similar number links between neurons in the brain[15]. 5. Implementation & Results 5.1. Model To facilitate the exploration of the hybrid spiking network architecture and make the analysis more efﬁcient, an abstract model was implemented at the transaction level using VHDL. The following tool chain was developed around this using a mixture of Java, VHDL and scripting languages: 1. Network Generator 2. FPNA Mapper 3. Abstract network simulation (VHDL) 4. Trafﬁc Analysis The network generator and FPNA mapper have already been described. The VHDL simulation model was implemented at the transaction level and consists of the structural elements of the system along with spike trafﬁc generators and trafﬁc monitors. The delays of each element were arbitrarily small, as the purpose of the model was to give an indication of relative trafﬁc levels. When the model is simulated, the trafﬁc monitors embedded in the model output a report to a ﬁle. When the simulation is complete, the trafﬁc analysis tool is used to extract metrics from the data. The tool chain is completely automatable as the output of each stage forms part of the input to the next without any human intervention. Currently, the tool chain can generate and map networks of up to N = 10000 neurons and a density of d = 600mm−2 within an acceptable time (less than an hour). The amount of work the tools must complete increases exponentially with N and d. Work is underway to improve the efﬁciency of each stage to attempt to produce networks of up to N = 100000 neurons and a density of d = 100000mm−2 on a usable time scale (hours - 1 day at the most). The effect of the input parameters N , d, and C upon the result of the mapping process are shown in ﬁgures 5, 6 and 7. A sample of 60 mapped networks was made, with N varying between 10 and 10000, d varying between 200mm−2 and 600mm−2 , and C varying between 0 and Figure 5. CNB output connection lengths by neuron count 1000. These parameter ranges were chosen to expose trends that may apply to all network sizes, provide results for the sort of neuron numbers that would be implemented on a small- to medium-size FPNA device, and produce a result in a reasonable period of time. The average rectilinear distance traversed by the outputs of CNBs is dependant upon N and d, as shown in ﬁgure 5 (mean: 5.7, range: 1-16.7). At larger neuron counts the distance increases with neuron density due to the “squashing” behaviour of the mapping process, as neurons are spread out to accommodate each other. It is expected that this distance continues to increase with density, increasing the demand for routing resources. The average number of inputs to each CNB increases with d as shown in ﬁgure 6 (mean: 30.5, range: 0.5-125.7). Denser networks form more connections, provided there is a large enough number of neurons in the network. It is expected that this trend will continue as d increases. This response is directly related to the number of synapses required either in a single CNB, or a group of CNBs if some neurons are not used. Thus, for a given N , d affects the amount of connectivity available in the architecture. The average number of inputs a tile receives is dependant upon d and C as shown in ﬁgure 7 (mean: 1634, range: 06147). In this ﬁgure, N is ﬁxed at 10000 (CNB grid dimensions: 101x100). This result shows that for a given neuron count and clustering parameters, there is a maximum number of inputs to a tile as the size of a tile, C , is varied. When the tile size is small, the input count is low as there are not many CNBs to connect to. When the tile size is large, larger than the typical range of the clusters of a neuron, fewer connection must be routed over the network. The largest Figure 6. CNB input count by neuron count Figure 8. Proportion of total connections routed over the AER network, by density work connectivity, and to the number of synapses per CNB, these trends can be used to derive an optimal mapping of the neural network. Further, generalised predictions of trafﬁc volume will enhance the quality of the mapping. Modifying the tool chain to accommodate back-propagated trafﬁc data from simulation can provide a signiﬁcantly more accurate prediction of trafﬁc ﬂow and can be used to guide placement of neurons to balance trafﬁc load on links to optimise bandwidth usage for a neural network application. The trafﬁc data can also be combined with layout data and used to analyse the power consumption of the network, for example to adjust the layout to reduce the effect of hot spots, or switch off unused links. The range of parameters used to generate the sample of mapped networks did not reach the levels seen in networks in the brain (see section 2.1), but was chosen to show important trends and produce the sample in a reasonable amount of computing time. Future work will include improving the efﬁciency of the tool chain, adding back-propagation of simulation data, and implementing routing, power and timing cost minimisation. To improve the accuracy of costings, the system elements will be modelled at the gate level and assessed for area and speed. This will also improve the routing stage of the mapping by providing accurate distances to and from network bridges. 5.2. VLSI Neuron A leaky integrate and ﬁre model of a neuron has been designed using standard cells, simulated, and manufactured at 130nm using the Europractice mini@sic service. The Figure 7. Tile input count by density number of inputs is seen between these two limits. An important related result is the proportion of total connections that are routed over the AER network, shown in ﬁgure 8 (mean: 0.25, range: 0-0.75). This proportion is not strongly affected by d but is closely related to C . As the tiles grow larger, fewer connections are routed over the network. These trends indicate that, for ﬁxed clustering characteristics and a given N and d, a suitable trade-off can be reached between the amount of direct and indirect routing, and the number of synapses in the CNB. These results demonstrate trends that are expected to apply to networks with a greater number of neurons and a greater density as well as those contained in the sample. It is clear that by applying timing and area costs to the responses of interconnect requirements for direct connectivity and netis emitted, the counter resets and the neuron starts the cycle again. The width of the pulse is determined by the delay element; the size of this element (3.4ns) was chosen to safely produce the shortest possible pulse while still being visible through the IO. The neuron model is simple, but it will enable a network to show interesting behaviour and demonstrate the interconnect architecture. The neuron occupies a small area such that enough neurons will ﬁt on even a small mini@sic chip, when the anticipated size of the other system elements is accounted for, such that a network built with these should be able to produce an interesting behaviour. The reconﬁgurable architecture achieves a focus on connectivity by using simple models of neurons and synapses. By using simple models, the implementation of the network is made more straightforward, and by grouping them into CNBs, the grid is formed through simple repetition of identical units. There is currently no support for axonal delay which is important to spiking networks a mechanism for neural oscillation and synchrony[6], which may be important to interpretation of sensory input and as a potential solution to the binding problem (how different neuronal interpretations of stimuli are united). A simple mechanism for intentionally delaying spikes will be added to the architecture as part of future work. The representation of post-synaptic magnitudes is intended for short range communication (intra-CNB, or single-hop inter-CNB) only, but makes the size of the CNB very sensitive to the encoding used. Future work will also include examining an optimal representation of magnitude as part of an implementation of synaptic plasticity and a critical analysis of the level of abstraction of the model. 6. Conclusions A connection-centric reconﬁgurable hardware architecture for spiking neural networks was presented. The architecture is a hybrid of classic on-chip interconnect and a Network-on-Chip, alleviating the connectivity scaling problem in spiking neural networks. A method of generating and then mapping a neural network to the architecture was presented and discussed. The results of this mapping indicate trends that can be used to minimise the cost of the mapping, optimising for time and area and also power if simulated trafﬁc data is back-propagated. A simple neuron model in VLSI was also presented. Figure 9. Leaky integrate & ﬁre model Area Gates Density Spike Period Generated clock frequency Max. Spike Rate (threshold=100) 1145.6µm2 (90nm: 700µm2 ) 390 873 p. mm2 (90nm: 1429 p. mm2 ) 4.5ns 160M H z 2.35 million p. second Table 2. 130nm LI&F neuron statistics schematic is shown in ﬁgure 9, and table 2 has some area and timing statistics. The neuron is self-contained and is asynchronous relative to the rest of the system, including other neurons. At the heart of the neuron is a 7-bit up/down counter, driven by a generated clock (as some measure of time is required). A clock divider provides a slower clock that is used for decay. The direction of the counter is controlled by the input pulse which also chooses between the original clock and the divided form, producing a quick increment when an input pulse is present and a slow decay when it is not. The resolution of the counter, and the range of threshold values, is derived from real data on post-synaptic potentials and neural thresholds. For this model, a typical threshold value would be about 100, and each incoming pulse would add between 1 and 3 to the count value. When the counter reaches the conﬁgured threshold value, the counter halts and requests that a spike be emitted by the pulse generator. The asynchronous pulse generator, synthesized using the Petrify tool [5], produces a single stereotypical spike in response to a request. Once the spike [17] http://www.stanford.edu/group/ brainsinsilicon (checked: 27/01/2009). Extended Address Event Representation Draft Standard v0.4. [18] I.-T. R. X.200. Information technology - open systems interconnection - basic reference model: The basic model, 1994. [19] L. I. Zhang, H. W. Tao, C. E. Holt, W. A. Harris, and M.-m. Poo. A critical window for cooperation and competition among developing retinotectal synapses. Nature, 395(6697):37–44, 1998. "
Configurable emulated shared memory architecture for general purpose MP-SOCs and NOC regions.,"Emulated shared memory (ESM) multiprocessor systems on chip (MP-SOC) and network on chip (NOC) regions are efficient general purpose computing engines for future computers and embedded systems running applications unknown at the design phase. While they provide programmer a synchronous, unified, and constant time accessible shared memory, the existing ESM architectures have been shown to be inefficient with workloads having low parallelism. In this paper we outline a configurable emulated shared memory (CESM) architecture that retains the advantages of the ESM architectures for parallel enough code but is also able to execute applications with low parallelism efficiently. This happens by allowing multiple threads to join as a single nonuniform memory access (NUMA) bunch and organizing memory system to support NUMA-like behavior for thread-local data if parallelism is limited. Performance simulations as well as silicon area and power consumption estimations of CESM MP-SOC/ NOC regions are provided.","Configurable Emulated Shared Memory Architecture for general purpose MP-SOCs and NOC regions Martti Forsell1 1VTT Platform Architectures Box 1100, FI-90571 Oulu, Finland Martti.Forsell@VTT.Fi Abstract Emulated shared memory (ESM) multiprocessor systems on chip (MP-SOC) and network on chip (NOC) regions are efficient general purpose computing engines for future computers and embedded systems running applications unknown at the design phase. While they provide programmer a synchronous, unified, and constant time accessible shared memory, the existing ESM architectures have been shown to be inefficient with workloads having low parallelism. In this paper we outline a configurable emulated shared memory (CESM) architecture that retains the advantages of the ESM architectures for parallel enough code but is also able to execute applications with low parallelism efficiently. This happens by allowing multiple threads to join as a single nonuniform memory access (NUMA) bunch and organizing memory system to support NUMA-like behavior for thread-local data if parallelism is limited. Performance simulations as well as silicon area and power consumption estimations of CESM MPSOC/NOC regions are provided. 1. Introduction Networks on chip (NOC) [Jantch03] are on-chip network solutions for computing needs of embedded systems and computers. NOCs typically consist of existing IPs to relieve the burden of designing everything from scratch. Multiprocessor systems on chip (MP-SOC) are integrated multiprocessor computing engines that may use NOC technology, but include replicated computing resources that is another way to simplify the design phase. Most MPSOCs/NOCs are aimed for embedded applications, solving functionality that is foreseen or even fixed already at the design time. Therefore NOC designers often prefer application-specific architectures for their computing needs [Salminen08]. However, in computers and computer-like 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  devices, and programmable embedded systems, e.g. smartphones like Apple iPhone, communicators, PDAs, extended music players, there is also need to process general purpose functionality that is not known beforehand or behaves arbitrarily. The suitability of application-specific architectures is not good for general purpose functionality since if the application falls out of the class of applications for which it is optimized (e.g. a FFT-processor is used to run a c-compiler), performance may drop dramatically. Another cost of utilizing application specific solutions is more difficult programming and more limited application area, because implementing a non-supported operation (e.g. floating point operation with an integer-only architecture) may be complex. According to our on-going investigations, the existing parallel computing paradigms for general purpose functionality, including symmetric multiprocessing (SMP), nonuniform memory access (NUMA), message passing (MP), and vector processing (VP), perform badly compared to the ideal models of parallel computing. On the other hand, it is known that emulated shared memory (ESM) MP-SOCs and NOC regions are efficient general purpose computing engines can provide performance close to the ideal models if the application is parallel enough [Forsell02a] and simple parallel programming methodology [Keller01] making them good suitable for future computers and embedded systems running applications unknown at the design phase. While they provide programmer a synchronous, unified, and constant time accessible shared memory, the existing ESM architectures have been shown to be inefficient with workloads having low parallelism [Forsell02a]. In this paper we outline a configurable emulated shared memory (CESM) architecture that retains the advantages of the ESM architectures for parallel enough code but is also able to execute applications with low parallelism efficiently. This happens by allowing multiple threads to join as a single nonuniform memory access (NUMA) bunch and organizing memory system to support   NUMA-like behavior for thread-local data if parallelism is limited. Performance simulations as well as silicon area and power consumption estimations of CESM MPSOC/NOC regions are provided. The rest of this article is organized so that in section 2 the principles of shared memory emulation are described. In section 3 we outline our configurable emulated shared memory architecture. A brief performance, silicon area and power consumption evaluation of a MP-SOC applying the CESM architecture is given in section 4. Finally, in section 5 we give our conclusions. latency of the network so that as a thread makes a memory reference, the thread is changed and second thread can make its memory request, and so on. If the bandwidth of the network is high enough and hot spots can be avoided in pipelined memory access traffic, the reply of the memory reference of the first thread arrives to the processor core before the thread is put back to execution and no memory delay will occur [Ranade91]. Synchronicity between consecutive instructions can be guaranteed by using an elastic synchronization wave between the steps [Leppänen96]. 2. Shared memory emulation Shared memory parallel computing has many advantages over message passing in its ideal form: There is no need to specify explicit intercommunication between processors, data requiring interaction is just located into the shared memory and accessed by the processors in parallel. The ideal form of shared memory based computing is called the parallel random access machine (PRAM) [Fortune78]. It provides the concept of synchronous execution, all processors execute instructions synchronously, including memory accesses. Programming a machine with PRAM abstraction is simple compared to weaker models having various uncertainties, e.g. non-coherent caches, nondeterministic execution time and order of instructions, used in current parallel computers: Programmer knows exactly what the state of each processor at each moment. Unfortunately the direct implementation of an ideal, PRAM shared memory has proved to be physically infeasible with current silicon technology if the number of processors is say higher than, say 4 [Forsell97]. This is because the wiring area (and the power dissipation) of multiport memory chip the raises quadratically as the number of processors increases with respect to a single ported memory of the same capacity. In this section, we describe the main principles of the shared memory emulation, introduce existing ESM MP-SOC architectures, and describe the processor, memory system and network architecture of the ESM MP-SOC architecture that we will use as a baseline for comparisons. 2.2 Existing ESM MP-SOC architectures Currently there exists only one ESM MP-SOC architecture—the Eclipse architecture [Forsell02b] and its variants, e.g. Eclipse CRCW and Eclipse MCRCW [Forsell06]. The Explicit Multi-Threading architecture (XMT) being developed in University of Maryland is also a MP-SOC architecture realizing a PRAM-like model of computation but without tight lock-step synchronicity [Vishkin08], thus, it can not be considered as a plain ESM architecture. There exists a number of attempts to realize ESM as a larger footprint device, e.g. the NYU Ultracomputers [Schwarz80], Tera MTA supercomputer [Alverson90] and its predecessors MTA2 and XMT provided by Cray, and SB-PRAM project and its 64-processor prototype [Abolhassan93, Keller01]. These works are limited by many sub-optimal solutions, including non-scalable interconnect topologies, inefficient ILP-TLP exploitation techniques, and sub-optimal emulation algorithms. Finally, recent general purpose graphics processors resemble ESM a bit with their shared memory systems and threads, but in practice numerous architectural choices limit their applicability to algorithms making use of locality and almost homogeneous SIMD-like operation. Eclipse is an ESM MP-SOC architecture realizing a synchronous PRAM model with a fixed number of physical threads [Forsell02b, Forsell06]. It consists of P Tpthreaded processors with dedicated instruction memory modules, P fast data memory modules, and a high-bandwidth mesh-like interconnection network (see Figure 1). In the following three subsections, we describe a generic ESM MP-SOC. 2.1 Principles 2.2.1 Processor A scalable architecture to emulate shared memory on a silicon platform consists of a set of processor cores connected to a distributed shared memory via a physically scalable high-bandwidth interconnection network. The main idea is to provide each processor core with a set of threads that are executed efficiently in an interleaved manner and hide the In order to efficiently implement the PRAM model on a top of a distributed memory system, processors need to be multithreaded [Valiant90, Leppänen96]. Figure 2 shows a block diagram of a Tp-threaded ESM processor MTAC [Forsell97]. Multithreading is implemented as a Tp-stage, Ls-way superpipelined, cyclic, in-order (interleaved) interthread pipeline, which provides hazard-free execution for hiding the latency of the memory system, maximizing overlapping of the execution of threads, and minimizing the register access delay. Switching between threads does not slow down operation of the processor, because threads proceed in the pipeline only during the forward time. If a thread tries to refer memory when the network is busy, the pipeline is suspended until the network becomes available again. After issuing a memory read, the thread can wait the reply for at most Mw<Tp clock cycles before the pipeline freezes until the reply arrives. A processor is composed of A ALUs, M memory units, a hash address calculation unit, a compare unit, a sequencer, and Tp sets of R registers like a VLIW processor [Fisher83], but the subinstructions organized as a chain rather than in parallel to be able to exploit virtual instruction-level parallelism (ILP), i.e. lowlevel parallelism between overlapping chained subinstructions, even if the operations are dependent. The scheduling of operations is assumed to be static since dynamic techniques might conflict with the synchronous thread level parallel (TLP) execution. The PRAM model is linked to the architecture so that a full cycle in the pipeline corresponds typically to a single PRAM step. During a step, each thread of each processor of the MP-SOC executes an instruction including at most one shared memory reference subinstruction. Therefore a step lasts for multiple, at least Tp, clock cycles. 2.2.2 Memory modules There are two types of memory modules, data memory modules and instruction memory modules, that are accessed via the data and instruction memory ports of processors, respectively (see Figure 1). All the data is located to physically distributed but logically shared data memory modules emulating the ideal PRAM memory. Instruction memory modules are aimed to keep the program code for each processor. The modules are connected together so that also instruction memory can be accessed Figure 1. High-level block diagram of a 16processor core an ESM CMP architecture. (P=processor core, M=data memory module, I=instruction memory module, I/O=I/O device and S=switch.)memory unit, c=step cache, and t=scratchpad). S S S S I P S S S S I P S S S S I P S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M S S S S I P M I/O I/O I/O I/O I/O I/O I/O M I/O I/O M M I/O I/O I/O I/O I/O Registers R  ...  R (Pre M)     ALUs     (Post M) A   ... A      ...   A    ...  A Sequencer        S MemoryUnits M   ... M OpRegister        O Instruction Fetch Instruction Decode (optional) Operand Select Hash Address Calculation Memory Request Send Memory Request Receive Bypass Compare Operation Result Bypass Sequencer Operation ALU Operation Result Bypass Result Bypass ALU Operation IA-Out I-In D-Out A-Out D-In 1 R-1 0 Q-1 Q A-1 0 M-1 Thread Management (optional) Compare unit           C S1 S1 S2 S2 Ls Mw o Chained organization Figure 2. Organization of the MTAC processor for the ESM architecture. Regions S1 and S2 indicate the registers that will be replaced with register blocks in the CESM architecture. The Thread ID register is not shown, but occupies similar register chain as other registers. via the shared data memory port e.g. by an operating system needing to load executables into the instruction memory. The data and instruction memory modules of size Ssd and Si bytes, respectively, are isolated from each other during normal execution to guarantee parallel high-bandwidth data, and instruction streams to processors. 2.2.3 Interconnection network The communication network connects processors to distributed memory modules so that sufficient throughput and low enough latency can be achieved for random communication patterns with a high probability as outlined in [Ranade91, Leppänen96]. Due to decreasing signal propagation speed on shrinking on-chip wires, a variant of twodimensional mesh, a Mc-way, double acyclic two-dimensional sparse mesh [Forsell05], providing enough bandwidth and fixed degree nodes as well as fixed length of interconnection lines independently on the number of processors, has been selected for the communication network topology. To maximize the throughput for read-intensive portions of code, we there are separate lines for references going from processors to memories and for replies from memories to processors. Memory locations are distributed across the data modules by a randomly chosen polynomial hashing function for avoiding congestion of messages and hot spots [Ranade91, Dietzfelbinger94].  are routed by using a simple greedy algorithm on a randomly selected submesh. Deadlocks are not possible during communication because the network is acyclic. Separation of steps and their synchronization is guaranteed with a synchronization wave technique [Leppänen96]. 3. Configurable ESM The MTAC processor of the ESM MP-SOC architecture described in previous section features an exhaustive combined Tp-level pipeline/register chain based thread storaging. If the number of threads in an application Tb is less than Tp per processor the utilization of a P-processor multithreaded ESM machine drops roughly according to following equation min = U p ( T PT , b PT p ) p . nature of the memory and processor organization optimized for parallel execution. Our idea to eliminate this performance bottleneck is to break the current fixed organization of thread storage/execution machinery. We implement this by making the thread storage configurable/indirect so that multiple thread execution slots can be assigned to execute a single NUMA mode thread bunch by just using the same thread storage address for them all. At the same time we need also modify the pipeline to support efficient sequential execution and memory system to support locality without breaking the parallel execution machinery, since ability to execute parallel programs efficiently is considered more important than ability to to execute sequential ones. It can also happen that some of the threads are being combined to a single thread bunch while rest of the threads continue execution with the PRAM mode in parallel with one or more NUMA bunches. We call the obtained architecture a configurable emulated shared memory (CESM) architecture. 3.1 Reorganizing the thread storaging Indirect thread storaging can be implemented by storing threads (at least partially) into a multiported and multithread register block (like implemented in the SUN Sparc Tx-series) or into a fast thread storage memory block rather than in the pipeline registers. This simplifies the processor pipeline remarkably, but requires multiported, fast and wide register block/memory that can be both read and written within a single clock cycle. Furthermore, this technique allows changing the execution order of threads and to execute workloads with a low number of threads efficiently. We consider here a realization that  packs general purpose registers to a storage S1 and intermediate registers related to the memory request receive phase to another storage S2 (see Figure 2). The architecture of the processor becomes now quite different than in ESM (see Figure 3). 3.2 Implementing configurability In order to make the reorganized thread storage usable by a programmer we need to add a thread address storage pointer for each thread (see rightmost registers of TID dual chain in Figure 3). In order to set a group of threads to execute just one thread storage, i.e. to execute a single thread for all the thread slots, a programmer needs just to set the thread storage pointers to a single value selected out of the values of the thread storage pointers. It is easy to see that the performance of ESM gets low especially as Tb becomes 1. The reason for this is the distributed €    Pre-memory ALUs A0      ...        Aq-1 Post-memory ALUs Aq      ...        Aa-1 Sequencer        S Status SR ThreadID TID Global Memory units M0      ...       Mm-1 Opcode     O LA LS LM Registers R1 ... Rr-1 Intermediate Registers Instruction Address-Out Instruction-In TO/FROM LOCAL INSTRUCTION MEMORY SYSTEM Instruction Fetch Operand Select Local Address-Out Local Data-Out TO/FROM LOCAL DATA MEMORY SYSTEM ALU Operation Local Data-In TO DISTRIBUTED SHARED DATA MEMORY SYSTEM Address Out0 Data Out0 Address Outm-1 Data Outm-1 Step Cache and Scratchpad Unit Data Data In0 Inm-1 FROM DISTRIBUTED SHARED DATA MEMORY SYSTEM Result Bypass ALU Operation Result Bypass Hash Address Calculation Memory Request Send Memory Request Receive Result Bypass ALU Operation Result Bypass ALU Operation Result Bypass Sequencer Operation Figure 3. Organization of the modified MTAC processor for the CESM architecture. 3.3 Modifying the pipeline for sequential processing The chained pipeline can not be exploited for a single thread execution combining multiple thread slots if the slots are not far away form each others, i.e. there are not enough thread slots used for other purposes between them. Actually, this so called bottleneck, that chained ILP model is optimal for parallel processing and parallel ILP model is optimal for sequential processing, has been already shown by us [Forsell02a]. In order to support both at the same time we need to add a few local functional units, namely local memory unit (LM) and local sequencer (LS) in the beginning of the pipeline while existing ALU0 can be reused as a local ALU (LA) for NUMA mode (see Figure 3). The pipeline for local operations consists of four stages—instruction fetch, operand select, execute, and write back. In order to reduce the number of pipeline hazards, the local functional units are connected together via an ordinary forwarding/bypassing network. Since all the local functional units execute in parallel, the only pipeline delays that remain are two delay slots in the case of a control transfer. Actually, all these hazards could be eliminated with more complex hardware and thus increased power consumption [Forsell97]. In this paper, however, we limit ourselves to the basic four stage pipelining.  3.4 Supporting memory locality and NUMA 3.5 Instruction level support The ESM memory system supports only non-local memory accesses, which would add latency effect since for a thread using multiple thread slots we can not any more assume latency hiding. The solution is to add local memories to the local memory unit while keeping the full support for global memories. It is also possible to enhance flexibility of memory partitioning between global and local parts by unifying them to a single two-port memory module and dividing the module internally into multiple banks that can be allocated either for global or local memory with a cost of replicating the modules for one edge per dimension (see Figure 4). If the need for memory for a NUMA bunch exceeds the capacity of the local memory bank, it is possible to add normal cache/virtual memory-based memory hierarchy that will have similar execution time, silicon area, power consumption costs as a single core system with the same configuration. It is possible to extend operation of NUMA bunches to full NUMA operation by allowing them to refer also to non-local memory modules using the NUMA convention. This should, however, used with caution the PRAM mode provides typically much higher performance for non-local memory accesses due to latency hiding if there is enough parallelism available, while placing data to the local memory for efficient local access is often possible if there is not much parallelism. I/O M I/O M I/O M SS S S SS S S SS S S SS S S SS S S SS S S SS S S SS S S SS S S SS S S SS S S SS S S I I I P M L P M L P M L I P I/O L I I I P M L P M L P M L I P I/O L I P I/O L I P I/O L I P I/O L I P I/O L I I I P M L P M L P M L I P I/O L SS S S SS S S SS S S SS S S I/O I/O I/O I/O M M M M Figure 4. High-level block diagram of a 16processor core CESM MP-SOC architecture. (P=processor core, M=shared data memory module, L=local data memory module, I=instruction memory module, I/O=I/O device and S=switch.) Switching between PRAM and NUMA modes is implemented with two special sequencer instructions JOIN and SPLIT that access the scratchpad of the processor core. A scratchpad is an addressable memory with fields for address, data, pending flag, that a thread in execution can use to implement multioperations between threads [Forsell06]. In order to use scratchpad with CESM we need to add a single-bit mode field to the scratchpad. A JOIN Rx instruction joins all the threads to a NUMA bunch specified by Rx. A SPLIT Rx instruction splits all the current NUMA bunches back to PRAM mode synchronized parallel threads starting from address specified by Rx. Figure 5 shows the implementation of JOIN and SPLIT instructions as high-level algorithms. This kind of bunching of threads makes it possible to run both NUMA bunches and PRAM mode threads in a same processor in parallel so that the performance of bunches is proportional to the fraction of threads they allocate. JOIN(Rx): IF Rx = thread identifier THEN ScratchPad[Rx].data:=Rx ScratchPad[Rx].mode:=NUMA ELSE IF thread identifier > ScratchPad[Rx].data THEN ScratchPad[Rx].data:=thread identifier Mode:=NUMA Bunch:=Rx Update PC normally {Note: Only the first thread of the bunch uses this PC value, the others receive their PC value via the forwarding network} SPLIT(Rx): IF ScratchPad[Bunch].mode = PRAMTHEN Mode:=PRAM Bunch:=thread identifier PC:=Rx ELSE IF ScratchPad[Bunch].data = thread identifier THEN ScratchPad[Bunch].mode:=PRAM ELSE Do not alter PC Figure 5.  Implementation of JOIN and SPLIT instructions. 3.6 CESM as a NOC region The CESM architecture can be designed as a uniform MP-SOC computing engine or as a NOC region responsible of executing general purpose functionality. On a NOC, a CESM region can just be a large computing resource aprefix fft max mmul sort spread sum T 64 T 16 64 T T N 1 N log N 1 N 1 N3 1 N log N 1 N 1 N 1 N 1 N N log N 1 N2 N 1 N N3 1 N3 N log N 1 N2 N 1 N N 1 N Determine an arbitrary ordered multiprefix of an array of N integers Perform a 64-point complex Fourier transform Find the maximum of a table of Nwords Compute the product of two 16-element matrixes Sort a table of 64 integers Spread an integer to all N threads Compute the sum of an array of N integers ----------------------------------------------------------------------------------------------------------------------------------------------------  SEQUENTIAL PARALLEL Name N E P W E P=W Explanation ----------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------  Table 1.  Evaluated computational problems and features of their sequential and parallel implementations (E=execution time, M=size of the key string, N=size of the problem, P=number of processors, T=number of threads, W=work). Note that fft, mmul, and sort are fixed size problems, while others depend on T. Milp ILP model PRAM mode -------------------------------------------------------------------------------------------------------------------------------------------------------------------Symbol E4 E16 E64 C4 C16 C64 DLX -------------------------------------------------------------------------------------------------------------------------------------------------------------------Model of computing Mtlp PRAM PRAM PRAM PRAM PRAM PRAM RAM -NUMA -NUMA -NUMA Chained Chained Chained Chained Chained Chained VLIW VLIW VLIW VLIW VLIW VLIW ILP model NUMA mode Milp VLIW VLIW VLIW VLIW VLIW VLIW 5-stage pipeline Processors P 4 16 64 4 16 64 1 Threads per processor Tp 512 512 512 512 512 512 1 Total number of threads T 2048 8192 32768 2048 8192 32768 1 Functional units PRAM mode Fp 10 10 10 10 10 10 Functional units NUMA mode Fn 3 3 3 4 On-chip shared data memory Msd 4 MB 16 MB 64 MB 2 MB 8 MB 32 MB On-chip local data memory Msd 2 MB 8 MB 32 MB On-chip bank access time Ab 1 c 1 c 1 c 1 c 1 c 1 c 1 c On-chip bank cycle time Cb 1 c 1 c 1 c 1 c 1 c 1 c 1 c Length of FIFOs Q 16 16 16 16 16 16 Step cache associativity Ac 4 4 4 4 4 4 -------------------------------------------------------------------------------------------------------------------------------------------------------------------Table 2.  Evaluated configurations (c=processor clock cycles). accessible to the rest of the NOC only via the on-chip network interfaces between the region and the rest of the NOC, or be an integral and transparent part of the NOC so that the traffic generated by the rest of the NOC can be routed through the CESM region if decided by the routing algorithm. In the latter case, the entire NOC network must be designed according to the same guidelines as the CESM network that would constitute just a subnetwork of the NOC with a special boundary operation blocking synchronization waves on a CESM region from spreading to the rest of the NOC. This can be implemented by configuring the inter-region switch elements so that to synchronization wave messages they act like boundary elements, but still let inter-region messages to bypass the region boundary. In order to avoid livelocks, synchronization wave message must be given priority over inter-region messages (formal algorithm skipped here due to space limitations). Interestingly, this technique can be used to implement hierarchical computational models on a NOC consisting of multiple CESM regions—a direction with possible applications as the number of processor cores per chip increases as predicted by ITRS 2007 and number of other white papers, e.g. [Intel06]. 4. Evaluation In order to illustrate the improvements achievable with the proposed CESM architecture on realistic MPSOC/NOC region we mapped parallel and sequential versions of seven parallel computational problems of which three are fixed size and other depend on the number of threads in a processor core (see Table 1) to PRAM thread groups and NUMA bunches, compiled, optimized (-O2 -ilp -fast) and loaded them to 3 MP-SOC/NOC region configurations having 4, 16 and 64  eleven-FU 512-threaded processors (see Table 2), and executed them with the MPSOC simulator modified for the CESM model. In order to see the PRAM mode execution performance, we executed the parallel versions of the programs in the CESM in PRAM mode and in ideal PRAMs having similar configurations. The results as relative execution time with respect to ideal PRAM with the same configuration are shown in Figure 6. We can observe that the PRAM mode execution speed of CESM is very close to that of ideal PRAM, mean overheads being 0.8%, 1.7%, and 1.4% for C4, C16, and C64, respectively. e m i t n o i t u c e x e e v i t a e l R 1,08 0,81 0,54 0,27 0 aprefix fft max mmul sort spread sum ) l s e c y c k c o c l ( e m i t n o i t u c e x E 150000000 1355403 12247 111 1 aprefix fft max mmul sort spread sum E4 C4 E16 C16 E64 C64 Figure 7. The execution time of sequential solutions of the computational problems on a single thread of a singe ESM processor core and on a 512 thread NUMA bunch in a single CESM processor core. The time scale is logarithmic due to big differences in execution time. C4 C16 C64 Figure 6. The relative execution time of CESM MP-SOCs compared to ideal PRAMs with similar configuration (PRAM=1.0, shorter is better). with different number of threads ranging from 1 to 512 threads per bunch in the C4 configuration. The results are shown in Figure 8. We can see linear performance increase as the number of threads per the bunch increases (taking the exponential thread scale into account). The NUMA mode performance was measured by executing the sequential versions of the programs in a single thread of an ESM machine and similarly configured CESMs so that all the threads of a single processor were joined to a single NUMA bunch. The results of these simulations as execution time are illustrated in Figure 7. We see that that CESM in NUMA mode indeed provides radically better performance for sequential programs, but is not able to exploit virtual ILP up to degree possible in the PRAM mode. The mean speedups of using NUMA mode are 13200%, 13196%, and 13995% for C4, C16, and C64, respectively. This does not, however, mean that these NUMA bunches can solve these computational problems faster than the ESM architecture or CESM architecture in PRAM model if parallel solutions are used. Namely, the parallel solutions are 1421%, 3111%, and 6889% faster than the best sequential ones for C4, C16, and C64, respectively The speedup is not linear with respect to the number of processors, since 3 out of 7 benchmarks are fixed size computational problems. To show seamless configurability between NUMA and PRAM modes in the CESM approach, we measured the NUMA mode execution time for sort algorithm for a bunch ) l s e c y c k c o c l ( e m i t n o i t u c e x E 6000000 4500000 3000000 1500000 0 1 2 4 8 16 32 64 128 256 512 sort Figure 8. Execution time of  as a function of number of threads in the bunch for C4 CESM configuration. We compared also the NUMA mode performance of CESM MP-SOCs to a five basic pipelined RISC processor DLX [Hennessy03] by executing all the sequential programs in a single DLX processor with a single step accessible on-chip memory (like the local memories of CESM                 cores) and in a single NUMA bunch composed of a threads of a single processor of CESM. In order to commit fair comparison, we took the variable size of the problems aprefix, max, spread, and sum into account in our measurements so that the amount of actual computation (and the computational problem itself) is the same for both architectures. In addition, the same compiler and even compilation were used to eliminate the effect of the compiler. CESM code was obtained from DLX code just by doing binary translation. The results are shown in Figure 9. Although the code is not optimized with a VLIW compiler for CESM’s NUMA bunch, it provides a bit better performance than DLX, the speedup being 8.8% for all configurations. This is due to more efficient ILP architecture of CESM cores. e m i t n o i t u c e x e e v i t a e l R 1,00 0,75 0,50 0,25 0 aprefix fft max mmul sort spread sum C4 C16 C64 Figure 9. Relative execution time of 512 thread NUMA bunches compared to 5stage pipelined DLX processor with the same memory setup (DLX=1.0, shorter is better). Finally, we estimated silicon area, power consumption, and maximum clock frequency figures for E16, C16, and C16 with configurable memory modules implemented on a high-performance 65 nm silicon process. In order to avoid implementation-specific effects, both architectures ares assumed to be implemented with centralized register blocks instead of pipelined/distributed one shown in Figure 2. The estimations are based on models presented [Pamunuwa03], ITRS 2007, and careful counting of architectural elements broken down to gate counts. The wire delay model gives maximum clock frequency 1.29 GHz for both E16 and C16 assuming minimum width global interconnect wiring with repeaters. The area and power results are shown in Figure 10. These figures except clock frequency are somewhat comparable to those of a X86 class multi-core high-frequency superscalar processor. The area and power overhead of CESM is not large compared to ESM showing less than one percent smaller area and power figures because memories occupy 73 % of the overall die area. If modules sharing flexibly the memory between the global and local parts are used, the silicon area and power consumption overheads jumps to 25.9 % and 33.7 % due to extra memories to allow full configurability. This increase of memory size can be avoided by switching the interconnection network to 2D sparse torus or multi-torus instead of 2D sparse mesh, but then the silicon area occupied by the interconnection network and power consumption of it would increase accordingly. } 2 ^ m m ( a e r a n o c i l i S 300 250 200 150 100 50 0 E16 C16 C16+ ) W ( n o i t p m u s n o c r e w o P 400 350 300 250 200 150 100 50 0 E16 C16 C16+ Com Mem Proc Figure 10. Silicon area and power consumption estimates for E16, C16, and C16 with configurable memory module (C16+) at 1.29 MHz on high-performance 65 nm technology  (Com=communication network, Mem=memory modules,  and Proc=processors). 5. Conclusions We have outlined a CESM architecture that provides the advantages of the best ESM architectures but is still able to execute applications with low thread-level parallelism efficiently breaking the important bottleneck preventing to commit efficient sequential and parallel computing with a single architecture [Forsell02a]. According to our estimations, the silicon area and power consumption are not significantly increased by addition of configurability support hardware. Our simulations with sequential and parallel solutions of simple computational problems show that the performance of an application with low parallelism is indeed better in CESM than in ESM, but falls short in exploiting available virtual ILP (without using parallel computing mode on parallel implementations of the programs). The performance of a NUMA bunch with all the threads of a single processor exceeds that of standard RISC processor even without exhaustive ILP optimization. It is not known how much parallelism real life applications             [Forsell06] M. Forsell, Realizing Multioperations for Step Cached MP-SOCs, In the Proceedings of the International Symposium on System-on-Chip 2006 (SOC’06), November 1416, 2006, Tampere, Finland, 77-82. [Fortune78] S. Fortune and J. Wyllie, Parallelism in Random Access Machines, Proceedings of 10th ACM STOC, Association for Computing Machinery, New York, 1978, 114-118. [Hennessy03] J. Hennessy and D. Patterson, Computer Architecture: A Quantitative Approach, third edition, Morgan Kaufmann Publishers Inc., Palo Alto, 2003. [Intel06] Research at Intel From a Few Cores to Many: A Terascale Computing Research Overview, White Paper, Intel, 2006. [ITRS07]  International  Technology  Roadmap  for Semiconductors, Semiconductor  Industry Assoc., 2007; http://public.itrs.net/. [Jaja92] J. Jaja: Introduction to Parallel Algorithms, AddisonWesley, Reading, 1992. [Jantch03] Networks on Chip edited by A. Jantsch and H. Tenhunen, Kluver Academic Publishers, Boston, 2003, 173-192. [Keller01] J. Keller, C. Keßler, and J. Träff: Practical PRAM Programming, Wiley, New York, 2001. [Leppänen96] V. Leppänen, Studies on the realization of PRAM, Dissertation 3, Turku Centre for Computer Science, University of Turku, Turku, 1996. [Pamunuwa03] D. Pamunuwa, L-R. Zheng and H. Tenhunen, Maximizing Throughput Over Parallel Wire Structures in the Deep Submicrometer Regime, IEEE Transactions on Very Large Scale Integration (VLSI) Systems 11, 2 (April 2003), 224-243. [Ranade91] A. Ranade, How to Emulate Shared Memory, Journal of Computer and System Sciences 42, (1991), 307--326. [Salminen08] E. Salminen, A. Kulmala, and D. Hämäläinen, Survey of Network-on-chip Proposals, White paper OCP-IP, March 2008. [Schwarz80] J. T. Schwarz, Ultracomputers, ACM Transactions on Programming Languages and Systems 2, 4 (1980), 484-521. [Valiant90] L. G. Valiant, A Bridging Model for Parallel Computation, Communications of the ACM 33, 8 (1990), 103111. [Vihkin08] U. Vishkin, G. Caragea, Aand B. Lee, Models for Advancing PRAM and Other Algorithms into Parallel Programs for a PRAM-On-Chip Platform, In Handbook of Parallel Computing—Models, Algorithms and Applications (editors S. Rajasekaran and J. Reif), Chapman & Hall/CRC, Boca Raton, 2008, 5-1—5-60. include. E.g. all the computational problems used in evaluations can easily be solved as parallel programs with clearly superior performance than those of the sequential solutions. Nevertheless, the CESM architecture provides additional flexibility in organizing the computation and simpler migration, since it can also exploit existing single threaded code efficiently. All this helps the CESM architecture to extend the application areas of MP-SOC designs and NOC regions to general purpose computing where computational problems to be solved are not known at the design phase or can generate  arbitrary requests to the system. Our plans for future research in this area include further improving the architecture, the language support and compiling/optimization tools for it. We plan also to build an FPGA prototype of the CESM architecture presented in this paper. Acknowledgements This work was supported by the grants 122462 and 128733 of the Academy of Finland. "
Performance Evaluation of NoC Architectures for Parallel Workloads.,"Network-on-Chip is the state-of-the-art approach to interconnect many processing cores in the next generation of general-purpose processors. In this context, the problem is to choose NoC architectures capable of achieving high performance for parallel programs. Therefore, the main goal of this paper is to evaluate the performance of three NoC architectures using well-known parallel workloads.","Performance Evaluation of NoC Architectures for Parallel Workloads  Henrique C. Freitas†, Marco A. Z. Alves, Lucas M. Schnorr, Philippe O. A. Navaux  Informatics Institute, Universidade Federal do Rio Grande do Sul, RS, Brazil  {hcfreitas, mazalves, lmschnorr, navaux}@inf.ufrgs.br  Abstract  Network-on-Chip is the state-of-the-art approach to  interconnect many processing cores  in  the next  generation of general-purpose processors. In this  context, the problem is to choose NoC architectures  capable of achieving high performance for parallel  programs. Therefore, the main goal of this paper is to  evaluate the performance of three NoC architectures  using well-known parallel workloads.  1. Methodology and Results  Due to the many-core processors, parallel programs  can be an important alternative to explore the high  number of cores. For this reason, this paper describes  an evaluation methodology focusing on parallel  programs and NoC performance results, as follows: i)  Parallel workloads  from NAS  (Numerical  Aerodynamic Simulation) [1] version 3.3 based on  MPI (Message Passing Interface) were executed on a  real eight-core machine. ii) The Triva prototype [2]  was used to provide an alternative way to analyze  parallel applications. The Triva benefits are the  visualization of communication patterns generated by  parallel applications in different time intervals, and the  possibility to analyze these patterns together with the  network topology involved in the execution. iii) An  analyzer tool was designed in Python for reading trace  files and processing performance results. The analyzer  is divided into two main parts: performance analytical  models based on NoC architectures, and transmission  time analysis considering time to send packets related  to transmission start time. Architecture characteristics  to develop the analyzer were obtained from SoCIN –  System-on-Chip Interconnection Network [3] and  MCNoC – Multi-Cluster NoC [4].  The total transmission time shown in Figure 1  considers the influence of all collective communication  This work was partially supported by CNPq (National Council for  Scientific and Technological Development).  † On leave from Pontifícia Universidade Católica de Minas Gerais  (PUC Minas), Belo Horizonte, MG, Brazil, cota@pucminas.br.  patterns. It is important to notice that the main impact  on these results is from one-to-one pattern. This impact  is related to the higher number and larger size of  messages than other patterns. In addition, the number  of hops and the influence of routers add more cost to  transmit messages. Therefore, MCNoC decreases  transmission time relative to 2x4 mesh and torus up to  21.2% and 19% for BT, 11.6% for CG, 49.5% and  45.8% for EP, 23.5% and 21.3% for FT, 26.1% and  25% for IS, 15.8% for LU, 16.2% for MG, and 21%  and 18.8% for SP, respectively.  Figure 1. Total transmission time  To conclude, MCNoC is capable of configuring  topologies through programmable routers and it can  achieve a higher performance than a traditional NoC  based on mesh or torus topologies. MCNoC has a  lower impact of routers since they map communication  patterns decreasing the number of hops.  "
Lookahead-based adaptive voltage scheme for energy-efficient on-chip interconnect links.,"This paper presents a novel adaptive voltage scheme based on a lookahead circuit that checks the transmitter buffer for data transitions. The advanced knowledge of incoming data patterns is used to adjust the link swing voltage, improving delay and energy performance. In the presented example system, a transition detection circuit is used to check the transmitter buffer for rising transitions (dasia0psila in cycle t, dasia1psila in cycle t+1). When a rising transition is detected, a higher supply voltage is applied to the driver for a small portion of the clock cycle to boost the rising edge delay, improving link performance. A lower voltage is used for all other transmissions, improving the delay performance of falling edge transitions and the link energy dissipation. For a 1 GHz link frequency, the proposed approach improves energy dissipation by 45% compared to a traditional two-inverter buffer. An energy savings of up to 15% is achieved compared to a previously proposed dual-voltage scheme.","Lookahead-Based Adaptive Voltage Scheme for Energy-Efficient On-Chip  Interconnect Links  Bo Fu, David Wolpert, Paul Ampadu  Dept. of Electrical and Computer Engineering  University of Rochester, Rochester, NY 14627  {bofu, wolpert, ampadu}@ece.rochester.edu  Abstract  This paper presents a novel adaptive voltage  scheme based on a lookahead circuit that checks the  transmitter buffer for data transitions. The advanced  knowledge of incoming data patterns is used to adjust  the link swing voltage, improving delay and energy  performance. In the presented example system, a  transition detection circuit is used to check the  transmitter buffer for rising transitions (‘0’ in cycle t,  ‘1’ in cycle t+1). When a rising transition is detected,  a higher supply voltage is applied to the driver for a  small portion of the clock cycle to boost the rising edge  delay, improving link performance. A lower voltage is  used for all other transmissions, improving the delay  performance of falling edge transitions and the link  energy dissipation. For a 1 GHz link frequency, the  proposed approach improves energy dissipation by  45% compared to a traditional two-inverter buffer. An  energy savings of up to 15% is achieved compared to a  previously proposed dual-voltage scheme.  1. Introduction  Interconnect links will impose a number of limits to  complexity, reliability, and throughput in nanoscale  system design [1]. Network-on-chip (NoC) topologies  [2] have been proposed  to reduce  interconnect  complexity, managing  the  transfer of data  in  addressable packets between networked nodes rather  than using an unstructured heap of global interconnect.  These network  topologies  reduce  interconnect  complexity and routing problems, but the interconnect  links between networked nodes consume a large  portion of on-chip power and area resources, and can  also introduce new throughput limitations such as  network congestion.   Although global  interconnect dimensions have  scaled much more slowly than transistor dimensions,  the number of interconnect-related issues has increased  with technology scaling because of (i) increased  operating speeds, (ii)  increased energy densities  resulting in larger on-chip thermal gradients, and (iii)  reduced supply voltages increasing the impact of noise  sources. In particular, interconnect power is becoming  a large component of total chip power dissipation [2].  Coupling between data lines results in a huge data  dependency on speed, power, and energy [1][3]. Long  links increase the impact of intra-die variation, which is  generally small at the local level but can result in large  delay and power performance differences across a chip  [4]. In high frequency nanoscale systems, interconnect  performance can become a function of inductance as  well as capacitance and resistance [5], increasing the  complexity of modeling and verification.  Although there are a number of problems that  specifically affect interconnect design, interconnect  links also offer a unique opportunity for improving  speed, power, and/or energy performance. A great deal  of research has focused on finding ways of improving  interconnect performance, such as optimizing link  voltages [6][7], adaptive voltage schemes [8][9],  optimizing data patterns [10][11], repeater insertion  [12] and finding novel ways of passing information  from transmitter to receiver such as current-mode  signaling [13] or pulse transmission [14][15].    This paper proposes an adaptive voltage scheme  using advanced knowledge of data patterns to reduce  power dissipation in on-chip interconnect links. In most  NoC links, the data to be transmitted is known ahead of  time, in some cases multiple cycles in advance. This  advanced knowledge of incoming data patterns offers  an opportunity to look ahead at the data patterns and  precondition the link to improve interconnect delay,  power, and/or energy performance. A simple transition  detection circuit uses incoming data patterns to adjust  978-1-4244-4143-3/09/$25.00 ©2009 IEEE                  the driver voltage.  In  the proposed example  implementation, a reduced link voltage is used in the  general case, and when a rising transition is detected,  the driver supply voltage is raised for a small portion of  the clock cycle. This supply voltage boost allows our  approach to improve the delay performance of a lowswing interconnect system, while maintaining the  energy benefits of the reduced swing voltage.   The remainder of the paper is organized as follows:  in Section 2, we present an overview of using  lookahead information to improve on-chip interconnect  link performance. In Section 3, we propose an example  lookahead approach, using dual supply voltages and a  rising transition detection circuit. Section 4 presents a  characterization of our proposed system and a  comparison with both a traditional single-voltage link  and  a prior dual-voltage  interconnect design.  Conclusions are given in Section 5.  2. Using lookahead information in on-chip  interconnect links  There are two useful pieces of information that can  be determined by looking into the transmitter buffer.   • Transition information: The buffers can be  sampled to find rising and falling transactions  on each wire in the link, which the presented  method will take advantage of.   • Coupling information: The transmitter buffers  can be sampled to detect adjacent bit transitions  which will result in a large amount of crosstalk  coupling, or the lack thereof. This information  has also been used  to change  the clock  frequency to handle crosstalk on interconnect  links [16].  The use of lookahead information can be integrated  with a variety of on-chip signaling methods, such as  pipelined interconnect, pulse transmission or currentmode signaling; however, the system overhead and  energy benefits will differ in each method. For  example, a pipelined interconnect system could easily  make use of the lookahead information in each register.  Further, the lookahead information may be used to  optimize system performance through link voltage,  throughput, clock period, or routing behavior.   A block diagram of a transition-aware system is  shown in Fig. 1. The current wire state (at cycle t) and  the state of the data in the next cycle (cycle t+1) are  inputs to the transition detection unit, which is then  used to adaptively tune the supply or threshold voltage  of the transmitter driver. The transition detection  information can be used to adjust the voltages on either  rising or falling transitions.   Fig. 1. Block diagram of lookahead-based  adaptive system.  Fig. 2. Impact of reduced link swing voltage  on 3 mm link delay of a traditional singlevoltage scheme.  In this paper, an example method is proposed using  the lookahead information to select between two driver  supply voltages, VLow and VHigh. The proposed approach  reduces link energy by only applying VHigh for a small  portion of the clock period when a rising transition is  detected. VLow is used in all other conditions. During a  low-to-high transition, the transition detection circuit is  used to trigger a high-voltage pulse that lasts for a  small portion of the clock cycle. This high-voltage  pulse is enough to improve the rising transition of the  circuit, but VHigh is not applied long enough to charge  the entire link capacitance. Limiting the amount of  capacitance charged to VHigh results in up to 45%  energy savings over a fixed VHigh design. While  traditional low swing voltage links result in reduced  operating frequencies because of the increase in the  rising transition delay (shown in Fig. 2), the proposed  system  is able  to achieve comparable delay  performance to a nominal voltage design by using  lookahead information to dynamically adjust the buffer  supply voltage to VHigh.  The use of lookahead information in additional  signaling methods and  the use of parameter  optimizations other  than supply voltage will be  examined in future work.          adjusted based on some external information such as  noise or process, voltage, or temperature (PVT)  variation. Link voltages are increased in the presence  of high noise conditions to reduce the probability of a  noise signal causing an undesired transition on a line.  Compared  to previous dual-voltage schemes, our  approach provides a fine-grained control of the voltage  on each wire rather than the entire bus.  3.1. Link modeling   The link model used in all of our simulations is a  global wire broken into RC segments, shown in Fig. 3,  with each segment representing a link length of 1 mm.  The global wire parameters and parasitic capacitance  values are shown in Table 1 with a cross section shown  in Fig. 4. The corresponding resistance and parasitic  capacitance values (R, Cg, and CC) were calculated [21]  using a 45 nm global link interconnect model [22], with  parameters shown in Table 1. No inductances are  included in the parasitic model because of the large  signal transition time, tr (the line lengths in question are  )LC much shorter than  t r 2/  [5]).  ( 3.2. Lookahead transmitter design   The transition detection and voltage control circuits  are shown in Fig. 5. As mentioned in Section 2, the  transition detection circuit is used to detect a rising  transition (a ‘0’ at node Q, the output of the register,  and a ‘1’ at node In, the input to the register). In the  transition detection circuit,  the PMOS device  is  enabled when Q = 0, passing In to the transmission  gate. In all other cases, the NMOS device is enabled  and the transition detection output is pulled down to  ground; thus, the only case when the transition  detection output is ‘1’ is when Q = 0 and In = 1.   The duration of VHigh can be adjusted depending on  performance requirements. In this example, VHigh is  applied for half of the clock cycle. The transmission  gate latches the output of the transition detector when  Clk = 0. This is needed to prevent a change in the value  of In from affecting the voltage boost during the rising  transition. The latched output of the transition detection  circuit is then passed into a low-skewed inverter with a  clocked NMOS device to prevent the inverter from  triggering until the beginning of the clock cycle (the  output of the transition detection circuit reaches the  input of the skewed inverter at the falling clock edge of  the previous cycle). The inverter is skewed to improve  the speed at which node X is pulled down.    The clock enable in the skewed buffer causes the  proposed system rise time performance to be superior  Fig. 3. Link schematic with substrate and  coupling capacitances.  Fig. 4. Cross-section of interconnect with  wire parameters labeled.  Table 1. Global wire parameters.  Parameter  Width, WL (µm)  Minimum Spacing, S (µm)  Thickness, t (µm)  Height, h (µm)  Dielectric Constant  R (Ω / mm)  Substrate Capacitance, Cg (fF/mm)  Coupling Capacitance, CC (fF/mm)  Value  0.31  0.31  0.83  0.14  2.1  85.5  77.4  70.3  3. Dual-VDD on-chip  design  interconnect  link   Although  the proposed design uses multiple  voltages in a novel way, we are certainly not the first to  propose the use of multiple voltages on an interconnect  link [17]. Multiple voltage links have been used  extensively to improve energy [8][18] or reliability  [19][20]. To improve energy, multiple operation modes  may be defined, with high voltages used when high  frequency performance is needed, and low voltages  used to save energy during reduced performance or idle  modes. To improve reliability, the link voltage is                      (a)  (b)  Fig. 5. (a) Lookahead transmitter schematic  and  (b)  traditional  two-inverter buffer  schematic.  to that of a system using a single voltage VHigh, as will  be shown in Section 4.   3.3. Timing diagram   All of the presented circuit simulations were  generated in Cadence Spectre using a 45 nm Predictive  Technology Model (PTM) [23]. The waveforms in  Fig. 6 show the behavior of the lookahead transmitter  in more detail. At t = 0.9 ns, the input switches from  low to high, activating the transition detection circuit.  On the next rising edge of the clock (t = 1 ns), the  transition status is latched by the transmission gate.  Node X is driven low, and VHigh is applied to the final  buffer stage until the falling edge of the clock (t =  3 ns). During this time, the buffer output Out is charged  to VHigh, with the link output decaying to VLow once the  precharge node X returns to logic high.    The transition between VHigh and VLow results in the  pattern shown in the Out waveform in Fig. 6. VHigh is  used to improve the rising delay of the output;  however, the link capacitance is not necessarily fully  charged to VHigh. Applying the high voltage for only a  portion of the cycle results in a significant energy  savings compared to the traditional approach using a  single high voltage with a two-inverter buffer (shown in  Fig. 5(b)). The proposed system is able to achieve  delays that are far superior to those possible with VLow  Fig. 6. Lookahead  waveforms.  transmitter  timing  alone, while still taking advantage of the energy benefit  of operating at a reduced supply voltage.    Drawbacks of the approach include slight increases  in area and complexity. In addition, the use of VLow  reduces link reliability by making the link more  susceptible to external noise sources. Link reliability  has been researched in great detail, and a number of  promising  techniques have been proposed  for  improving resilience to noise and single-event upsets  [24][25].  3.4. Voltage selection   One of the most important design decisions in a  multi-voltage system is the selection of the operating  voltages. The choice of voltage impacts nearly every                    important design metric,  including speed, power,  energy, area (i.e. sizing requirements to meet a given  frequency target), and reliability.    The proposed approach is intended to improve the  power and energy performance of an NoC link, and the  energy dissipations of the approach are shown in Fig. 7  for a variety of operating voltages. The x-axis in Fig. 7  represents different values of VHigh, while the y-axis  indicates a range of values of VLow for each value of  VHigh. For example, the point where x = 0.9 and y = 0.8  refers to the system where VHigh = 0.9 V and VLow =  0.8*9V = 720 mV. Values of VLow < 0.6*VHigh are not  shown because the receiver flip-flop is assumed to be  operating at VHigh, and input voltages lower than  0.6*VHigh may be insufficient to switch the flip-flop  state.   One obvious conclusion to draw from Fig. 7 is that  lower voltages provide lower energy dissipation, but  there is another very important point to draw from the  data. For a given VHigh, reducing the value of VLow from  0.9*VHigh all the way to 0.6*VHigh results in a smaller  energy savings than reducing VHigh by 100 mV. For  example, the energy consumed by the system with VHigh  = 1 V and VLow = 0.6 V is larger than the energy  consumed by the system with VHigh = 0.9 V and VLow =  0.9*VHigh = 810 mV. In that case, reducing VLow by over  200 mV has less of an impact on energy dissipation  than reduced VHigh by 100 mV.    Reducing VLow has very little effect on the rising  edge delay because VHigh is used to boost the link  voltage. Reducing VLow can also improve the falling  edge delay because of the lower voltage to discharge.  This information combined with the data from Fig. 7  can be used to provide an easy guideline for optimizing  the voltages in the proposed multi-voltage system.  When using the proposed approach to optimize energy  dissipation, one should first find the smallest VHigh that  satisfies the given frequency requirement, and then  reduce VLow to the lowest point at which the receiver  flip-flop can be controlled (with an appropriate design  margin). For example, the minimum VHigh meeting the  500 MHz performance requirement in the system in  Fig. 5(b)  is 700 mV. To minimize  the energy  dissipation at 500 MHz, we choose VHigh = 700 mV and  VLow  = 0.6*VHigh = 420 mV.  4. Results  In this section we examine the potential energy  savings of the lookahead transmitter by comparing it  with a traditional two-inverter buffer and a prior dualvoltage switching method.  (a)  (b)  Fig. 7. The impact of voltage selection on the  energy dissipation of the proposed approach  at (a) 100 MHz and (b) 500 MHz.  4.1. Characterization   A more detailed explanation of how the proposed  system  is able  to achieve a  low-to-high delay  improvement over the two-inverter buffer is shown in  Fig. 8. The internal node voltage in Fig. 8 refers to  node X in the proposed transmitter circuit, and refers to  node Y in the traditional two-inverter buffer (both  nodes are labeled in Fig. 5). As shown, the proposed  internal node voltage drops almost immediately after  the rising clock edge because the latched transition  detection output bypasses the clkQ latency of the  flip-flop. In the traditional system, the internal node  latency is the sum of the clkQ delay of the flip-flop  and the delay of the first inverter in the two-inverter  buffer. This delay advantage carries over into the  buffer output, with the proposed system’s rise time                    Fig. 8. Waveform comparison on  rising  transition between proposed scheme and  single voltage scheme using VHigh.  Fig. 9. Receiver input voltage, driver power and  link power comparison between proposed  scheme and single voltage scheme using VHigh.  reaching 0.5 V nearly 100 ps earlier than the traditional  system.    Note that because the proposed system does not  fully charge the link to VHigh like the traditional system,  the rising delays of the two systems are nearly equal at  the far end of the link (at the input to the receiver)  shown in the top waveform in Fig. 9. All of the power  results from Fig. 9 were measured by sizing the  proposed system for the same delay (at the receiver  end) as the traditional approach.   The energy savings of the proposed approach over a  traditional two-inverter buffer are separated into driver  and link transient power consumption in the lower two  waveforms in Fig. 9. The driver power shown is the  amount of power dissipated by four drivers driving a  5 mm, four-bit bus segment with the worst case  coupling input (a ‘1010’ pattern in cycle t followed by  a ‘0101’ pattern in cycle t + 1). The link power shown  is the amount of power dissipated by a 5 mm, four-bit  bus segment using the parameters from Table 1. The  maximum driver power in the proposed approach is  34% smaller than in the traditional single voltage  system.    The link power savings of the proposed approach  are even more significant. Because VHigh is only applied  to the link for a short time, the peak link power  dissipation is 61% smaller than in the traditional  system. The total energy savings of the proposed  approach compared  to a  traditional approach  is  dependent on the clock period and activity factor, as  shown in Fig. 10.  In the proposed approach, the clock period controls  the amount of time that VHigh is applied to the link; thus,  as the clock period decreases, the total link swing will  be reduced (despite the reduced swing, the link is still  functional at a clock period of 1 ns). This results in  energy savings of up to 35% in Fig. 10(a). The other  energy saving feature of the proposed approach is the  switch to VLow on the falling edge of the clock. The  total energy dissipation decreases as the link is pulled  closer to VLow while the link state is logic ‘1’.    Two parameters affect the time that the link remains  at logic ‘1’: the clock period, and the activity factor.  Thus, with α = 1 and the clock period set to 5 ns, the  total energy savings of the proposed approach is  24.7%. If α is reduced to 0.5 for the same clock period,  the energy savings becomes 29.3%. The trade-off in  energy savings between the reduced swing at small  clock periods and the state duration at high clock  periods results in the parabolic shape in Fig. 10. The          (a)  (b)  Fig. 10. Link, driver, and total energy savings  of proposed scheme with  respect  to  traditional scheme with changing activity  factors for a link length of 5 mm. (a) αααα = 1,   (b) αααα = 0.5.  peak corresponds to the point where the link is charged  closest to VHigh and the ‘1’ state does not last long  enough to take advantage of VLow. The maximum total  energy savings of the proposed approach occurs with α  = 0.5 and a clock period of 1 ns, resulting in a 45.4%  improvement. The above energy analysis includes both  dynamic and leakage power; the average leakage power  for both input states in the proposed system is 15%  smaller than the fixed voltage system because of the  lower swing voltage.   All of the previously reported simulations use a link  length of 5 mm. Much of the energy savings of the  proposed approach are achieved by only applying VHigh  to the link for a short time period; thus, our system’s  performance improvements are directly tied to the link  length. The energy dissipation of our system and a  traditional two-inverter buffer system are plotted in  Fig. 11 for link lengths of 2 mm, 3 mm, and 5 mm. In  Fig. 11(a), the energy measurements are taken at a  (a)  (b)  Fig. 11. Comparison of impact of link length  on power dissipation between proposed  scheme  and  traditional  single-voltage  scheme. (a) 1 Ghz, (b) 100 MHz.  frequency of 1 GHz, while in Fig. 11(b) the frequency  is reduced to 100 MHz.    At each frequency, the proposed approach achieves  an increasing energy performance improvement over  the single voltage design as the link length increases,  up to a 45% improvement at 1 GHz. Again, this  performance improvement results from the shorter  amount of time which VHigh is applied to the link (VHigh  provides a fast rising edge but is shut off on the falling  edge of the clock, resulting in a reduced swing  voltage). At the lower frequency, shown in Fig. 11(b),  the energy benefits of the proposed approach are  reduced because of the longer period in which VHigh is  applied; 30.7% energy savings are achieved in the  100 MHz case with a 5 mm link length.   The proposed system achieves significant energy  benefits over a fixed VHigh design without sacrificing  delay; however, the proposed approach requires a                Fig. 12. Schematic of prior dual-voltage  switching method. [18]  much larger driving buffer because the dual-voltage  scheme inserts an additional device into the final pull  up and pull down networks. When sized for equal delay  as the fixed VHigh design, the proposed approach has an  area overhead of 33%.   4.2. Comparison with prior dual-voltage  switching method  An alternative method of using two voltages to aid  in rising link transitions is shown in Fig. 12 [18].  Rather than the lookahead information used in the  proposed scheme, the method shown in Fig. 12 uses the  value at node Q to apply VHigh to the output. The  system operates as follows: assume that Q is set low,  Out is set low, and In is set high. The small inverter in  the feedback path uses the supply voltage VLow, and is  unable to fully shut off the PMOS device P1 (the gatesource voltage of P1 is VLow - VHigh). Node Z is thus  charged to VHigh, which fully shuts off the PMOS  device P2. When the clock is triggered and Q is set  high, the NMOS device N1 is turned on; as node Z is  pulled low, P2 is turned on. The output node will be  pulled up through both P2 and P3, with P2 shutting off  when Out reaches a high enough voltage to toggle the  feedback inverter.   The above method results in the dashed waveform  shown in Fig. 13 (the dashed waveform is the same in  Fig. 13(a) and Fig. 13(b)). Fig. 13(a) compares the  waveforms of the proposed lookahead-based approach  and the previous dual-voltage approach with both  schemes using VHigh = 1 V and VLow = 0.6 V. The  lookahead rising delay is smaller than the previous  design on the rising edge because the proposed design  bypasses the clkQ delay of the flip-flop, and applies  VHigh for a longer time period. The falling edge delay of  Fig. 13 is shown to be slower than the previous design.  If the output node in the proposed design does not have  (a)  (b)  Fig. 13. Waveform comparison with prior  dual-voltage switching method. (a) Proposed  VHigh = 1 V, (b) Proposed VHigh = 0.7 V.  Fig.  14. Power  comparison between  proposed approach and prior dual-voltage  switching method.   enough time to pull down to VLow, it will affect the  output falling delay.  VHigh can be reduced to match the delay of the  proposed scheme to that of the prior scheme. Reducing  VHigh from 1 V to 700 mV in the proposed approach  results in an equal rising edge delay with the previous                        approach, as shown in Fig. 13(b). In addition, the  falling delays are matched because the pull-down  network in the proposed approach has enough time to  pull down to VLow before the falling transition.   The power dissipations of the proposed approach  and the prior approach are shown in Fig. 14. The  proposed approach is shown to have superior power  performance across the entire range of clock periods  measured, with an improvement of 5% for a 10 ns  clock period and an improvement of 15% for a 3 ns  clock period.  5. Conclusions and future work   The proposed system is able to use the lookahead  information with a dual-voltage system to provide a net  energy savings of up to 45% compared to a traditional  two-inverter buffer, and up to 15% compared to a prior  dual-voltage switching method. The reported energy  savings are achieved with the same rising and falling  latency as the compared systems, at the expense of area  and complexity. One drawback of  the proposed  approach compared to the traditional two-inverter  approach is a reduction in reliability, resulting from the  lower link voltage (this reduced reliability is also a  concern in the other dual-voltage method).    The reliability problem may be somewhat alleviated  by the use of error control coding, which has been  widely researched for link-to-link communication in  networks on chip. The potential energy-reliability  trade-off of combining coding with the proposed  approach will be examined in future work. In addition,  a hybrid approach will be examined in a link with  repeater insertion using a dual-voltage boost circuit to  further improve the already significant energy savings.  6. "
Power reduction through physical placement of asynchronous routers.,"This paper presents a way to reduce power consumption by minimizing wirelength and hop-count of an asynchronous NoC using simulated annealing and force-directed algorithms. Asynchronous NoCs (aNoCs) can provide important benefits over clocked NoCs. However, there is little published research on generating a custom, optimized aNoC for a fixed- function, power-constrained system-on-chip (SoC). Such tools must consider physical SoC properties and especially NoC link delay and power. This research is motivated by this need, and the mantra that ""transistors are fast, wires are slow and power-hungry,"" due to process scaling differences between transistors and global wires.","Power Reduction Through Physical Placement of Asynchronous Routers Daniel Gebhardt and Kenneth Stevens University of Utah gebhardt@cs.utah.edu kstevens@ece.utah.edu Our work reduces power consumption by minimizing wirelength and hop-count of an asynchronous NoC using simulated annealing and force-directed algorithms. Asynchronous NoCs (aNoCs) can provide important beneﬁts over clocked NoCs. However, there is little published research on generating a custom, optimized aNoC for a ﬁxedfunction, power-constrained system-on-chip (SoC). Such tools must consider physical SoC properties and especially NoC link delay and power. Our research is motivated by this need, and the mantra that “transistors are fast, wires are slow and power-hungry,” due to process scaling differences between transistors and global wires. We consider an aNoC composed of simple, three bidirectional port routers, connected into a tree-based topology. These optimization techniques are for a single use-case SoC using soft-IP cores. The key insight is that by reducing router complexity and size as much as possible, the routers can be placed nearly anywhere on the ﬂoorplan, including inside the core outline. This ﬂexibility has the potential to reduce wirelength and better utilize the asynchronous channel property that physically closer routers or wire pipeline buffers will have a reduced cycle-time. Our methodology takes as input communication properties of a SoC and approximate IP core dimensions. Communication between cores is given by two values: the average bandwidth and the minimum required bandwidth, similar to a core graph. We use the Parquet ﬂoorplanner to determine core locations, minimizing a combination of area and wirelength based upon the core graph information. Our tool then ﬁxes the router positions, and a ﬂoorplacement using Capo or a similar tool can be done on the full netlist. We use simulated annealing (SA) to explore tree-based topologies, using a neighbor-state selection function that maintains a tree topology. The ﬁtness of a solution is based on two metrics: weighted wirelength and hop count, which factors in the trafﬁc quantity of each path, as the amount of data carried by a link or router also affects its power. This ﬁtness value is minimized to represent a desire for low power and routing congestion. Router locations must be known to ﬁnd wirelength, so we developed a force-directed method to place routers on the ﬂoorplan, which is integrated 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  with the SA process. The general idea is that “force” is applied to each router causing movement in the direction that will shorten the source-to-destination path through the network. The magnitude of this force is proportional to the amount of trafﬁc a path carries and its wirelength. We evaluated this work by noting the ﬁtness improvement from the initial solution to the ﬁnal post-SA solution. The initial solution is a balanced tree, constructed with highly communicating cores topologically near each other, and routers placed by our force-directed method. Three SoCs were used: a 12-core MPEG4 decoder, a 33-core, and a 50 core SoC, where the latter two had “synthetically generated” trafﬁc patterns estimating possible designs. We varied the inﬂuence that wirelength (WL) has compared to router hops on solution ﬁtness to account for a range of process technologies or implementation details, such as wire repeater sizing and spacing. The percent of improvement ranged from 17% to 47%, with the MPEG4 design showing the most beneﬁt, as it had the least uniform communication requirements. The improvement differences by varying the WL:Hop inﬂuence ratio were small, but showed this methodology is increasingly valuable as wires become more power-dominant. Figure 1 shows the 50-core SoC ﬂoorplan after network optimization. We observed a clustering of routers in certain areas, which may hint at a further use of these methods, such as determining where to “merge” small routers into a single higher-radix router. 44:sb44 29:sb29 9:sb9 18:sb18 3:sb3 5:sb5 23 4:sb4 40:sb40 6:sb6 25 18 1:sb1 2 36:sb36 11 41 11:sb11 37:sb37 14 16 45:sb45 10:sb10 49:sb49 47 6 39:sb39 17:sb17 19:sb19 21:sb21 48:sb48 46:sb46 15 1 7 3 31:sb31 24 4 10 33:sb33 13 15:sb15 8:sb8 23:sb23 25:sb25 21 28 41:sb41 24:sb24 34:sb34 27 22:sb22 35:sb35 45 12 37 29 44 5 35 3236 19 34 0 40 20 42 22 30 46 31 26 13:sb13 14:sb14 17 9 39 38 7:sb7 8 27:sb27 38:sb38 43 20:sb20 28:sb28 16:sb16 26:sb26 33 12:sb12 47:sb47 2:sb2 42:sb42 0:sb0 30:sb30 32:sb32 43:sb43 Fig. 1. NoC topology and router placement.   "
Energy efficient application mapping to NoC processing elements operating at multiple voltage levels.,"An efficient technique for mapping application tasks to heterogeneous processing elements (PEs) on a network-on-chip (NoC) platform, operating at multiple voltage levels, is presented in this paper. The goal of the mapping is to minimize energy consumption subject to the performance constraints. Such a mapping involves solving several subproblems. Most of the research effort in this area often address these subproblems in a sequential fashion or a subset of them. We take a unified approach to the problem without compromising the solution time and provide techniques for optimal and heuristic solutions. We prove that the voltage assignment component of the problem itself is NP-hard and is in approximable within any constant factor. Our optimal solution utilizes a mixed integer linear program (MILP) formulation of the problem. The heuristic utilizes MILP relaxation and randomized rounding. Experimental results based on E3S benchmark applications and a few real applications show that our heuristic produces near-optimal solution in a fraction of time needed to find the optimal.","Energy Efﬁcient Application Mapping to NoC Processing Elements Operating at Multiple Voltage Levels Pavel Ghosh and Arunabha Sen Department of Computer Science and Engineering Arizona State University, Tempe, AZ Email: {pavel.ghosh, asen}@asu.edu Alexander Hall Department of EECS UC Berkeley, CA, USA Email: alex.hall@gmail.com Abstract An efﬁcient technique for mapping application tasks to heterogeneous processing elements (PEs) on a Network-onChip (NoC) platform, operating at multiple voltage levels, is presented in this paper. The goal of the mapping is to minimize energy consumption subject to the performance constraints. Such a mapping involves solving several subproblems. Most of the research effort in this area often address these subproblems in a sequential fashion or a subset of them. We take a uniﬁed approach to the problem without compromising the solution time and provide techniques for optimal and heuristic solutions. We prove that the voltage assignment component of the problem itself is NP-hard and is inapproximable within any constant factor. Our optimal solution utilizes a Mixed Integer Linear Program (MILP) formulation of the problem. The heuristic utilizes MILP relaxation and randomized rounding. Experimental results based on E3S benchmark applications and a few real applications show that our heuristic produces near-optimal solution in a fraction of time needed to ﬁnd the optimal. 1 Introduction In recent years System-on-Chip (SoC) design has become extremely challenging due to the increasing complexities in processor and semiconductor technologies. Multicore SoC based embedded systems may contain either all homogeneous generic processing cores, or a varying number of heterogeneous PEs. These heterogeneous PEs may represent programmable general purpose cores, task speciﬁc co-processors or hardware accelerators, etc. Networkon-Chip (NoC) architectures provide an alternative to the bus-based communication mechanism that can meet the challenging requirements of performance, scalability and ﬂexibility [1, 2]. As the number of PEs on a SoC and the data trafﬁc between them continues to grow, energy minimization subject to performance constraint has become one of the most important objectives. The problem of minimizing energy consumption during application execution while satisfying the performance constraints can be divided into four main subproblems: (i) mapping of the application tasks to the PEs, (ii) mapping of the PEs to the routers of the NoC architecture, (iii) assigning operating voltages to the PEs (in case they can operate at multiple voltages) and (iv) routing of data paths, i.e., trafﬁc movement on the NoC architecture. As consideration of all four subproblems simultaneously increases the complexity of the problem, most of the research effort in this domain [4, 6, 8, 9] either solve problems (i), (ii), (iii) and (iv) in a sequential fashion, or solve only a subset of them. To ﬁnd an energy efﬁcient application mapping, all four problems (i)-(iv) have to be solved. There are two options available - solve them sequentially or solve them in a uniﬁed way. The sequential approach has manifold disadvantages. Firstly, decision taken at an early phase may turn out to be expensive later. Secondly, because of some earlier decisions may lead to violation of constraints at some later phase, and thus resulting in re-execution of all the steps multiple times involving an enormous amount of computation. We show with a motivating example here and later with extensive experimental results that the sequential approach may lead to sub-optimal solution. To the best of our knowledge, our proposed technique is the ﬁrst that uniﬁes all the four subproblems under a single problem formulation and develops optimal and heuristic solutions for the problem. Although scaling down voltage levels of PEs is favorable for reduction of energy consumption, excessive number of voltage islands may be detrimental from the perspective of physical design as it creates voltage island fragmentation of the chip and increases the complexity of the power delivery network. Therefore, the number of voltage islands on the chip should follow an upper bound. In literature [7, 8], the constraint on the maximum number of voltage either has not been captured properly or involves a huge computation 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  1   (a) Application Task Graph (b) Mapping using 2-step approach (c) Mapping using 1-step approach Figure 1. Task Graph and Mappings time for the solution. We incorporate this constraint by an efﬁcient formulation of the upper bound on the number of voltage islands created. The motivation behind our uniﬁed approach comes from the following example. Fig. 1(a) shows an example application task graph consisting of four tasks T 0, T 1, T 2 and T 3. The edges represent the task dependencies and the labels on the edges represent the inter-task communication volume in number of bytes. Tables 1 and 2 show the execution time and power consumption of these four tasks on four available PEs P 0, P 1, P 2 and P 3, respectively. Following a sequential approach, the resultant mapping is as shown in Fig. 1(b), with computation energy consumption 3.8124µJ and communication energy consumption 0.952µJ . Thus, the overall energy consumption for the application is (3.8124 + 0.952) = 4.7644µJ . With such a mapping, the execution ﬁnish-time of the application is 86.104µs, well within the speciﬁed deadline of 122µs. For our proposed uniﬁed mapping, as shown in Fig. 1(c), the total energy consumption is 3.8734µJ = 3.8575µJ + 0.0159µJ , leading to 18.7% save of energy as compared with the sequential approach. With this mapping, the execution ﬁnish-time of the application is 84.852µs, still within deadline of 122µs. Energy consumption can be further reduced if we take advantage of operating the PEs of the NoC at multiple voltage levels. When the PEs are allowed to operate at different voltages, a certain amount of energy will be consumed by Table 1. Execution time (in seconds) for taskprocessor pair Tasks T0 T1 T2 T3 P0 7.7e-06 4.1e-06 7.2e-06 5.47e-05 P1 7e-06 5.2e-06 4.2e-06 4.15e-05 P2 6.9e-06 4.9e-06 5.95e-06 5.95e-05 P3 5.8e-06 4e-06 4.4e-06 7.2e-05 Table 2. Task Power (in Watts) processor pair for taskTasks T0 T1 T2 T3 P0 0.16 0.07 0.102 0.048 P1 0.14 0.045 0.25 0.084 P2 0.12 0.065 0.131 0.041 P3 0.15 0.077 0.21 0.028 the generation of additional clock signals, voltage level converters and mixed clock-mixed voltage FIFOs used by the level shifters connecting adjacent PEs in NoC architecture. From the example above, it is clear that a uniﬁed approach to application mapping to NoC PEs, operating at multiple voltages leads to better energy utilization than a sequential approach. The energy consumption model considered in this paper has three components - (i) energy consumption due to computation, (ii) energy consumption due to communication and (iii) voltage transition energy between adjacent PEs operating at different voltages. In this paper, we have assumed a regular mesh architecture as the communication infrastructure, where each router has 5 ports. One of the ports is used for connecting it to a PE and the other four are for connection to the neighboring routers. The algorithms proposed in this paper can be used with any other NoC topologies as well. Communication power consumption parameter values used for the evaluation of our techniques are taken from [9]. 2 Problem Formulation In this section, we provide formal deﬁnition of the application mapping problem. The input instance to this problem is explained in Table 3. The output of the problem is as shown in Table 4. The objective is to minimize the overall energy consumption, such that: 1. All the application tasks ﬁnish their execution before deadline D . 2. Dependencies among the tasks are maintained. 3. Bandwidth constraint on each router link is satisﬁed. 4. The total number of voltage islands created does not exceed κ. 5. Total energy consumption (computation + communication + voltage transition) is minimized. 3 Computational Complexity In this section we deﬁne the voltage assignment problem, subproblem (iii) of the application mapping problem and prove it to be NP-complete and inapproximable within any constant factor. If M1 (tj ) = pi , then for each pi we can ﬁnd out an allowable set of voltage levels Li = Symbols G(VT , ET ) wij D κ Pt ⊆ P P Vp τ (t, p, vp )  (t, p, vp ) GR (VR , ER ) β λ Lij ψi ψo ψl αvk vl Table 3. Input of the Problem n Explanation Set of tasks VT = {t1 , t2 , . . . , tm }; directed edge eij = (ti , tj ) ∈ ET representing the dependency of task tj on task ti For edge eij ∈ ET , the data communication volume (in Mbps) between tasks ti and tj Application deadline, by which all the tasks need to be completed Maximum number of voltage islands created A set of processing elements {p1 , p2 , . . . , pk } For each task t ∈ VT , a subset of PEs, potential to execute the task t For each PE p ∈ P , an allowable set of voltage levels in which this PE can operate, where np denotes number of distinct voltage levels it can operate Execution time of task t ∈ VT on PE p ∈ Pt , when p is operating at voltage vp ∈ Vp Computation energy consumption of task t ∈ VT on PE p ∈ Pt , when p is operating at voltage vp ∈ Vp An undirected n × n mesh architecture graph, where each node r ∈ VR denotes a router in the NoC architecture, whereas each edge eij = (ri , rj ) represents a router link in the NoC architecture between those two router nodes Capacity of each link eij ∈ ER Data communication latency on each link eij ∈ ER Link length of each link eij ∈ ER Communication energy consumption rate at the router input por t = 328nW/M bps[9] Communication energy consumption rate at the router output por t = 65.5nW/M bps[9] Communication energy consumption rate at the router links = 79.6nW/M bps/mm[9] Power consumption by the level shifter when two adjacent PEs are operating at voltages vk and vl , respectively v1 , v2 , . . . , vnp o Table 4. Output of the Problem Output Task to PE Mapping function M1 : PE to Router Mapping Function M2 : PE to Voltage Assignment Function M3 : Task Edge to Path in Mesh Mapping Function M4 : Explanation M1 (ti ) = pj , which means that task ti has been mapped to PE pj ; ti ∈ VT , and pj ∈ Pti ⊆ P M2 (pi ) = rj , which means that PE pi has been mapped to NoC router rj ; pi ∈ P and rj ∈ VR M3 (pi ) = vj , which means that PE pi has been assigned to voltage vj ; pi ∈ P and vj ∈ Vpi rπ(1) → rπ(2) → . . . → rπ(l) → rz , where ˘r1 , rπ(1) ¯ , ˘rπ(l) , rz If eix ∈ ET , M1 (ti ) = pj , M2 (pj ) = rk and M1 (tx ) = py , M2 (py ) = rz , then M4 (ti , tx ) = rk → [1, l − 1] and π(q) (cid:54)= k, z , ∀q ∈ [1, l − 1] {vi1 , vi2 , . . . , vini }, such that the task tj can meet its deadline dj (obtained using deadline D and communication latencies) whenever pi is operating at the voltage levels in Li . We consider two components for energy consumption: (1) Energy consumption by PE pi when operating at voltage v ∈ Li . We denote this component by ηv (2) Energy consumption by the level shifter connecting two adjacent PEs pi and pj , where pi is operating at voltage x and pj is operating at voltage y , denoted by αxy . Voltage Assignment Problem: Given an n × n = N grid, pi . where each node represents a router in the NoC architecture with a PE attached to it and a list Li of allowable operating voltages associated with each PE pi , the problem is to assign a voltage vj to PE pi , where vj ∈ Li for all the PEs, such that the following energy consumption expression is minimized:(cid:88) (cid:88) αxy ηv pi + i=1 to N ,v=M3 (pi ) x=M3 (pi ),y=M3 (pj ),eij ∈ER Minimum Weight Grid Coloring Problem: Given a grid The voltage assignment problem has one-to-one correspondence with the minimum weight grid coloring problem, deﬁned as follows: graph G = (V , E ) of dimension a × b = M . Each node u ∈ V has an associated set of colors Cu = {1, . . . , Ku} each with a certain color-cost c (k) ∈ (cid:60)+ , k ∈ Cu . Let u, w ∈ V be neighbors in the grid-graph, i.e., {u, w} ∈ E . We are also given combination-costs c (k , l) ∈ (cid:60)+ for each ¯ , ˘rπ(q) , rπ(q+1) ¯ ∈ ER , ∀q ∈ color combination k ∈ Cu and l ∈ Cw . The goal of the minimum weight grid coloring problem is to ﬁnd a coloring, i.e., for each node u ∈ V a color k ∈ Cu , which minimizes c (ku ) + (cid:88) the following objective function: We now prove the decision version of this problem, called MINCOL, to be NP-hard by reducing an instance of the known NP-hard problem GRAPH 3-COLORABILITY (cid:88) c (ku , kw ) {u,w}∈E v∈V with no vertex degree exceeding 4 [5]-(page 85) to an instance of MINCOL. Theorem 1. Min weight grid coloring is NP-hard and even inapproximable within any factor. Proof. Let G(cid:48) = (V (cid:48) , E (cid:48) ) denote the given planar graph. From this graph we construct an instance of the min weight grid coloring problem as follows. We embed G(cid:48) in a grid graph G = (V , E ) of dimension O (n(cid:48) ) × O (n(cid:48) ), where n(cid:48) is the number of nodes in G(cid:48) , using a polynomial algorithm for computing an orthogonal representation. Let W ⊆ V denote the subset of nodes in G which corresponds to the embedded node set V (cid:48) of corresponding to the embedding of e(cid:48) . P = (cid:83) the given planar graph. For each edge e(cid:48) = {u(cid:48) , w (cid:48)} ∈ denotes the set of all such paths. Fig. 2 gives an example of how such an embedding could look like for a simple graph E (cid:48) we denote by pe(cid:48) = u1 , e1 , u2 , . . . , ej−1 , uj the path e(cid:48)∈E (cid:48) pe(cid:48) (a) Example planar graph (b) Embedding into a grid graph of dimension 6 × 6 Figure 2. Planar graph and its embedding with four nodes. With each node u ∈ V of G we associate 3 colors Cu = {1, 2, 3}. We set all color-costs c (k) = 0 for k ∈ Cu and u ∈ V . Similarly, all combination-costs of edges {u, w} ∈ E contained in none of the paths, i.e., {u, w} /∈ P , are set to zero as well: c (k , l) = 0 for k ∈ Cu and l ∈ Cw . For each path p ∈ P we set the combinationcosts as follows: i) For e1 = {u1 , u2 } we set c (k , l) = 0 for k ∈ Cu1 , l ∈ Cu2 and k (cid:54)= l. For remaining combinations with k = l we set c (k , l) = 0. In other words, if u1 and u2 are assigned different colors, we have cost 0, otherwise 1. ii) For ei with i ∈ {2, . . . , j − 1} we set c (k , l) = 1 for k ∈ Cui , l ∈ Cui+1 and k (cid:54)= l. For the remaining combinations with k = l we set c (k , l) = 0. In other words, if ui and ui+1 are assigned the same color, we have cost 0, otherwise cost 1. If we aim for a total cost of 0, the path p will propagate the color chosen for uj all the way to u2 . For the cost to remain at zero, u1 and u2 (and therefore uj ) must be colored with different colors. Therefore, the given planar graph G(cid:48) is 3-colorable, if and only if there is a solution to the constructed min weight grid coloring problem of total cost zero. This proves the problem to be NP-hard. The inapproximability within any factor follows, since any approximation algorithm with multiplicative approximation ratio ρ and additive factor ρ(cid:48) could be used to decide whether G(cid:48) is 3ρ(cid:48) + 1. If G(cid:48) is 3-colorable, the optimal solution has cost 0 colorable as well: simply multiply all combination costs by and therefore the approximation algorithm must ﬁnd a solution with cost ≤ ρ · 0 + ρ(cid:48) = ρ(cid:48) . Otherwise, if G(cid:48) is not 3-colorable, any solution has cost ≥ ρ(cid:48) + 1. Hence, the approximation algorithm distinguish between two cases. 4 Optimal Solution for Application Mapping In this section we use the mathematical programming techniques to solve the application mapping problem. We formulate the problem as a Mixed Integer Linear Program (MILP). In Table 5 we deﬁne a few of the variables used in the MILP formulation and also declare their types. The parameters used for calculation of energy consumption and execution time are as follows: τtpv = execution time of task t ∈ VT on PE p ∈ Pt at voltage level v ∈ Vp (in sec) tpv = computation energy consumption for task t ∈ VT on Table 5. Varibales used in MILP Formulation Variable δv tp ζrv f ij xy Type Binary Binary Binary Deﬁnition 1, 1, if t ∈ VT , p ∈ Pt ⊆ P, v ∈ Vp , and M1 (t) = p and M3 (p) = v ; 0, otherwise M2 (p) = r and M3 (p) = v for some p ∈ P ; 0, 1, if eij ∈ ET , exy ∈ ER , and exy ∈ M4 (eij ); if r ∈ VR operating at voltage v , otherwise i.e., 0, otherwise PE p ∈ Pt at voltage level v ∈ Vp (in Joules) αv1 v2 = voltage transition energy consumption parameter for two adjacent nodes operating at voltage levels v1 and v2 , respectively (in Joules) The objective of the application mapping problem is to minimize the energy consumption of the system subject to the application deadline constraint, mesh interconnection link bandwidth constraint and maximum allowed number of voltage islands constraint, i.e., Obj: minimize E = Ec + Er + El + Evt (cid:88) where Ec is the computation energy consumption, Er and El are the communication energy consumed at the router ports and links, respectively and Evt is the voltage transition Ec = (cid:88) energy consumption, and calculated as: Er = (cid:88) El = (cid:88) (cid:1) (ψi + ψo ) (cid:0)f ij (cid:0)(cid:0)f ij (cid:1) Lxy (cid:1) ψl (cid:0)δv ∀exy ∈ER ∀eij ∈ET tp tpv xy wij v∈Vp t∈VT (cid:1) xy wij p∈Pt (cid:88) (cid:88) (cid:88) (cid:88) Evt = (cid:88) (cid:88) ∀exy ∈ER ∀eij ∈ET v1∈V v2∈V ∀exy ∈ER (ζxv1 ζyv2 ) αv1 v2 θv1 v2 v2∈V v1∈V ∀exy ∈ER (cid:88) (cid:88) θv1 v2 xy αv1 v2 In order to eliminate the quadratic term in the expression of Evt , we deﬁne the following decision variable: Hence, Evt = (cid:88) xy = 1, if ζxv1 = 1 and ζyv2 = 1, otherwise 0 Constraints: Due to the limitations in space, here we omit the mathematical details of the formulations of the constraints. (1) Each task executes on exactly one PE, at exactly one voltage, connected to exactly one router node. (2) Each router operates at exactly one voltage level and is attached with at most one processor. (3) Each task needs to ﬁnish within the speciﬁed application deadline, and need to maintain task dependencies. (4) Bandwidth constraint on each router link: the total ﬂow passing through a link does not exceed its bandwidth. (5) The number of voltage islands is less than or equal to the speciﬁed maximum allowed value κ. 5 Heuristic for Application Mapping In this section we describe our proposed MILP relaxation and randomized rounding based heuristic. The algorithm takes as input the parameters speciﬁed in the Section 2 and the corresponding MILP formulation. Output of the algorithm consists of the mappings M1 , M2 , M3 and M4 , deﬁned in Table 4. The heuristic is explained below. Algorithm 1 Randomized Rounding based Heuristic 4: 1: Relax all the integer constraints in the MILP (/* relaxation */) 2: Solve the relaxed LP using CPLEX 3: while (Solution NOT Integral) do Set the integral variables in LP solution as constants and leave them unaltered during further iterations (/* variable ﬁxing */) for (each task t ∈ VT ) (/* rounding */) do Round δv tp to 1 with probability as its value tp = 1 for some p and v ) repeat until (δv repeat Round γtr to 1 with probability as its value until (γtr = 1 for some r) Round ζrv variables following co-relation constraints Round σrp variables following co-relation constraints Solve modiﬁed LP if (NO Constraint Violation) then if (Integer Solution) then Solution Found - break out of outer while loop end for end if else Eliminate constraint violations by modifying the rounding of δv tp and γtr variables for the corresponding t ∈ T 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end if 23: end while 24: return Solution in terms of functions M1 , M2 , M3 and M4 6 Experimental Results In this section we present the experimental results to evaluate our proposed approach. All our experiments can be classiﬁed into the following three categories considering the evaluation goal of the experiment: i) The optimal solution for variable voltage setup is compared with the optimal solution for ﬁxed voltage setup. ii) The optimal solution of our proposed uniﬁed approach is compared with that of the sequential approach. iii) The quality of the heuristic solution is evaluated by comparing it with the optimal solution. The experiments are performed using applications (autoindustry, consumer, networking and ofﬁce-automation) from the E3S benchmark suite [3] and three real applications MPEG4, MWD (Multi-Window Display) and OPD (Object Plane Decoder). The following six voltage setups were used, the ﬁrst one as variable voltage (VV) setup and the other ﬁve as ﬁxed voltage (FV) setups: a) Case I : Voltage level varies in the range from V0 to V4 , b) Case II : Voltage level is ﬁxed at V0 = 1.9V , c) Case III : Voltage level is ﬁxed at V1 = 2.3V , d) Case IV : Voltage level is ﬁxed at V2 = 2.5V , e) Case V : Voltage level is ﬁxed at V3 = 3.3V , f) Case VI : Voltage level is ﬁxed at V4 = 3.6V . (a) Auto-Industry (b) Consumer (c) Networking (d) Ofﬁce-Automation Figure 3. Optimal with variable voltage levels vs. Optimal with ﬁxed voltage Energy Consumption Comparison for E3S Benchmark For low voltage setups, no feasible solution was found. On the other hand, for high voltage setups feasible solutions were found with higher energy consumption values. The heuristic is implemented in C++. The MILP for achieving the optimal solution and the corresponding relaxed LP as part of the heuristic were executed on the same machine using ILOG CPLEX 10.0 Concert technology. Experiments with E3S Benchmark Applications: Mesh dimension 4 × 4 was used for the auto-industry application and 3 × 3 for all the other three applications. The value for the maximum number of voltage islands κ was set to 4 and 3, respectively, for these two different sizes of mesh topologies. Fig. 3 compares the optimal solution while using the variable voltage (VV) levels with the optimal solution while setting a ﬁxed voltage (FV) level, i.e., optimal at Case I with the optimals for all other cases. On average for these four applications, the variable voltage setup can save 18% energy consumption over the ﬁxed voltage setup.We compare the solution quality of our proposed uniﬁed approach with the sequential approach in Fig. 4. For these four applications, on average, we are able to save 16% energy consumption. Fig. 5 shows the quality of our proposed heuristic as compared with the optimal solution. i.e., the optimal and heuristic solution for Case I. In all the cases, the heuristic is able to ﬁnd near-optimal solutions. Fig. 6 shows that the time taken to obtain the heuristic solution is negligible compared to that required to solve the MILP for the optimal solution. Experiments with Real Applications: Mesh dimension 3× 3 for MPEG4 and MWD applications and 4× 4 was used for the OPD application. The value of maximum number of voltage islands κ was set to 3 and 4, respectively, for these two different sizes of mesh. Fig. 7 compares the optimal solution while using the variable voltage (VV) levels with the optimal solution while setting a ﬁxed voltage (FV) level, i.e., optimal at Case I with the optimals for all other cases. (a) Auto-Industry (b) Networking (c) Ofﬁce-Automation (a) MPEG4 (b) MWD (c) OPD Figure 4. Comparison of the Energy Consumption values using Our Proposed Uniﬁed Approach and the Sequential Approach Figure 8. Variable Voltage Optimal vs. Heuristic Energy Comparison for Real Applications shows that the optimal solution with ﬂexible voltage levels is more energy-efﬁcient than the optimal at some ﬁxed voltage level. Moreover, it shows that optimal solutions can be achieved using the uniﬁed approach as opposed to the sub-optimal solution obtained by the sequential approach. (a) Auto-Industry (b) Consumer 7 Conclusion (c) Networking (d) Ofﬁce-Automation Figure 5. Variable Voltage Optimal vs. Heuristic Energy Consumption Comparison for E3S Benchmark Applications Figure 6. Exec. Time Comparison (log scale) (a) MPEG4 (b) MWD (c) OPD Figure 7. Optimal with variable voltage levels vs. Optimal with ﬁxed voltage Energy Consumption Comparison for Real Applications On average for these three applications, the variable voltage setup can save 11% energy consumption over the ﬁxed voltage setup. Fig. 8 shows the quality of our proposed heuristic as compared with the optimal solution. i.e., the optimal and heuristic solution for Case I. In all the cases, the heuristic is able to ﬁnd near-optimal solutions within seconds. Thus, the experimental results support our claim of achieving near-optimal solutions from the heuristic. It also In this paper, we have proposed a uniﬁed approach to solve the application mapping problem on a heterogeneous NoC platform for energy minimization. The voltage assignment problem is proven to be NP-hard. Our solution techniques are evaluated using benchmark suite E3S [3] and three real applications. Experimental results demonstrate effectiveness of our heuristic, superiority of the uniﬁed approach over sequential approach and advantage of operating the PEs at multiple voltage levels for energy minimization. "
Increasing NoC power estimation accuracy through a rate-based model.,"Summary form only given: This research work presents and compares two NoC power estimation models, one based on the volume of information transmitted in the network, and another based on the transmission rates of each router. NoC power/energy volume-base estimation model is based in the volume of information transmitted through the network, and have been widely used as an option to commercial power estimation tools. These models are faster compared to commercial tools, since results are based in the application of mathematical equations. The volume-based models calculate how much will cost, in terms of power/energy, to transmit a packet from the origin router to the destination router, passing through n hops. This method gives a first order estimation, not considering effects due to the traffic propagating in the network fabric such as traffic congestion and burst transmissions. For energy consumption estimation model, this work uses the energy estimation model embedded in the CAFES framework, which estimates energy based on the Hu and Marcules- cu energy consumption model. The rate-based power estimation model is an option to the volume-based models, and it is more precise than the volume-based models, since traffic effects change the transmission rates in each router. The power consumption of a router may be divided into three main components: buffers, control logic and crossbar. Buffers are responsible for the most significant part of power consumption in the router. The rate-based power model, introduced in, comprises two steps: calibration and application. The calibration step defines the parameters used in the model. This step starts synthesizing the router in the target technology. The synthesis generates a mapped RTL, and this new RTL description replaces the original router description. The new RTL NoC description is then simulated, applying different traffic scenarios. At the end of each simulation, a value change dump (VCD) file of the synthesized router ...","Increasing NoC Power Estimation Accuracy through a Rate-Based Model  Guilherme Guindani, Cezar Reinbrecht, Thiago R. da Rosa, Fernando Moraes  PUCRS, Av. Ipiranga 6681, P. 32, Porto Alegre, Brazil  {guilherme.guindani, fernando.moraes}@pucrs.br  This research work presents and compares two NoC  power estimation models, one based on the volume of  information transmitted in the network, and another  based on the transmission rates of each router.  NoC power/energy volume-base estimation model  is based in the volume of information transmitted  through the network, and have been widely used as an  option to commercial power estimation tools. These  models are faster compared to commercial tools, since  results are based in the application of mathematical  equations. The volume-based models calculate how  much will cost, in terms of power/energy, to transmit a  packet from the origin router to the destination router,  passing through n hops. This method gives a first order  estimation, not considering effects due to the traffic  propagating in the network fabric such as traffic congestion and burst transmissions. For energy consumption estimation model, this work uses the energy estimation model embedded in the CAFES framework [1],  which estimates energy based on the Hu and Marculescu energy consumption model [2].  The rate-based power estimation model is an option to the volume-based models, and it is more precise  than the volume-based models, since traffic effects  change the transmission rates in each router. The power  consumption of a router may be divided into three main  components: buffers, control logic and crossbar. Buffers are responsible for the most significant part of  power consumption in the router. The rate-based power  model, introduced in [1], comprises two steps: calibration and application. The calibration step defines the  parameters used in the model. This step starts synthesizing the router in the target technology. The synthesis  generates a mapped RTL, and this new RTL description replaces the original router description. The new  RTL NoC description is then simulated, applying different traffic scenarios. At the end of each simulation, a  value change dump (VCD) file of the synthesized router is generated. Then, the power consumption of a  complete router is annotated using a commercial power  estimation tool. At the end of the calibration phase, a  table with the power consumption for each injection  rate is generated, for each element of the router. An  equation is obtained for each table, applying a linear  adjustment technique. This equation gives the power  consumption as a function of the injection rate. This is  a generic procedure, and it can be applied to networks  with different features. In the second step, the application of the model, the NoC is simulated to obtain the  reception rate of each buffer. The buffer reception rate  is measured with a monitor inserted in each buffer of  every router. This monitor counts the amount of flits  received in a parameterizable sample window. For each  buffer reception rate the associated power consumption  is annotated, applying the power consumption equations generated in the calibration phase.  Experiments show that for traffic with up to a  couple of flows, the volume-based power estimation  error is kept below 10%. However, for traffics with  three or more flows, the error increases, due to the  packet collision effect. The error in the rate-based  power estimation is kept below 10%, even with more  packet flows injected in the network. The insertion of  more packet flows modifies the buffers transmission  rates in each router, this fact increase the power estimation precision given by the rate-based model. A second  experiment, with all routers injecting traffic into the  network, even with small rates (15% of the available  rate), the rate-based power estimation model keeps an  error below 10%, compared to the PrimePower estimation tool, while the volume-based power estimation  model error surpasses the 50% in the same comparison.  The volume-based power estimation models can  produce very fast results, but the NoC designer must  have in mind that the network task mapping, and the  increased NoC data flow, can produce large variations  in the power estimative due to the packet collision/contention. For that purpose, the rate-based power  estimation model allow a more precise power estimation, while performing the NoC power dissipation in a  acceptable time.  [1] Marcon, C.; et. al. “Exploring NoC Mapping Strategies: An  Energy and Timing Aware Technique”. In: DATE, 2005.  [2] Hu, J.; Marculescu, R. “Energy-aware mapping for tile-based  NoC architectures under performance constraints”. In: ASPDAC, 2003.  [3] Guindani, G.;  et al. ""NoC Power Estimation at the RTL Abstraction Level"". In: ISVLSI, 2008.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE          "
Analysis of photonic networks for a chip multiprocessor using scientific applications.,"As multiprocessors scale to unprecedented numbers of cores in order to sustain performance growth, it is vital that these gains are not nullified by high energy consumption from inter-core communication. With recent advances in 3D Integration CMOS technology, the possibility for realizing hybrid photonic-electronic networks-on-chip warrants investigating real application traces on functionally comparable photonic and electronic network designs. We present a comparative analysis using both synthetic benchmarks as well as real applications, run through detailed cycle accurate models implemented under the OMNeT++ discrete event simulation environment. Results show that when utilizing standard process-to-processor mapping methods, this hybrid network can achieve 75times improvement in energy efficiency for synthetic benchmarks and up to 37times improvement for real scientific applications, defined as network performance per energy spent, over an electronic mesh for large messages across a variety of communication patterns.","Analysis of Photonic Networks for a Chip Multiprocessor Using Scientiﬁc Applications Gilbert Hendry† , Shoaib Kamil‡(cid:63) , Aleksandr Biberman† , Johnnie Chan† , Benjamin G. Lee† , Marghoob Mohiyuddin‡(cid:63) , Ankit Jain‡ , Keren Bergman† , Luca P. Carloniγ , John Kubiatowicz‡ , Leonid Oliker(cid:63) , John Shalf(cid:63) † Lightwave Research Laboratory, Columbia University, New York, NY 10027 γ Computer Science Department, Columbia University, New York, NY 10027 (cid:63) CRD/NERSC, Lawrence Berkeley National Laboratory, Berkeley, CA 94720 ‡ Computer Science Department, University of California, Berkeley, CA 94720 Abstract As multiprocessors scale to unprecedented numbers of cores in order to sustain performance growth, it is vital that these gains are not nulliﬁed by high energy consumption from inter-core communication. With recent advances in 3D Integration CMOS technology, the possibility for realizing hybrid photonic-electronic networks-on-chip warrants investigating real application traces on functionally comparable photonic and electronic network designs. We present a comparative analysis using both synthetic benchmarks as well as real applications, run through detailed cycle accurate models implemented under the OMNeT++ discrete event simulation environment. Results show that when utilizing standard process-to-processor mapping methods, this hybrid network can achieve 75× improvement in energy efﬁciency for synthetic benchmarks and up to 37× improvement for real scientiﬁc applications, deﬁned as network performance per energy spent, over an electronic mesh for large messages across a variety of communication patterns. 1 Introduction The microprocessor industry is set to double the number of cores per chip every 18 months – leading to chips containing hundreds of processor cores in the next few years. This path has been set by a number of conspiring forces, including complexity of logic design and veriﬁcation, limits to instruction level parallelism and – most importantly – constraints on power dissipation. In this brave new world of ubiquitous chip multiprocessing (CMP), the on-chip interconnect will be a critical component to achieving good parallel performance. Unfortunately, a poorly designed network could easily consume signiﬁcant power, thereby nullifying the advantages of chip multiprocessing. Consequently, we must ﬁnd communication architectures that can somehow maintain performance growth under a ﬁxed power budget. Current processor-manufacturing roadmaps point to simple mesh or torus networks-on-chip (NoC) via electrical routers as the medium-term solution; however, previous work [1] has shown that such architectures may not be best-suited for balancing performance and energy usage. In this paper, we investigate a promising alternative to electrical NoCs, namely architectures that exploit optics for some or all inter-processor communications. According to the International Technology Roadmap for Semiconductors [10], three-dimensional chip stacking for three-dimensional integration (3DI) is a key focus area for improving latency and power dissipation, as well as for providing functionally diverse chip assemblies. Recent advances in 3DI CMOS technology [3] have paved the way for the integration of silicon-based nanophotonic devices with conventional CMOS electronics, with the premise of realizing hybrid photonic/electronic NoCs [17]. High density through-silicon-vias (TSVs), the critical enabling technology for 3DI, electrically connect wafer layers. One of the fundamental assumptions of this work is that 3D integrated chips will play an important role as the interconnect plane for future chip multiprocessors, whether the NoC is electrical or photonic, and that the TSVs have a minimal impact on the power dissipation for these chip implementations. To evaluate the tradeoffs between the electrical and photonic network designs, we conduct extensive cycle-accurate simulations using custom software within the OMNeT++ framework [19]. This work differs from previous efforts through the use of a comprehensive event-driven simulation allowing us to model the low-level electronic and photonic details of the evaluated interconnect conﬁgurations. The modeling detail enables us to analyze the energy, latency, and physical performance of the devices. In addition to standard synthetic trafﬁc models, our study utilizes traces of real parallel scientiﬁc applications to determine the potential beneﬁts of the hybrid network for Single Program 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  1   Multiple Data (SPMD) style algorithms. The simulation environment is used to analyze interconnection networks of various types and conﬁgurations for performance and energy consumption. Reported metrics include the execution time of the benchmark/application, the total energy consumed therein, and the energy efﬁciency, a metric which emphasizes the network performance gained with each unit of energy spent. We simulate the performance of electronic mesh and torus topologies along with the photonic NoC studied in [15], known as a blocking torus (which we refer to as a photonic torus). In this photonic NoC, a photonic network and an electronic control network coordinate to provide the system with high bandwidth communications. The simulations show that the photonic interconnects studied here offer excellent power-efﬁciency for large messages, but are less advantageous for carrying small messages. We present a detailed set of results that show how different application characteristics can affect the overall performance of the network in ways that are not readily apparent in higher level analysis. 2 Related Work Prior related works have made signiﬁcant gains in the area of on-chip optical interconnects. Petracca et al. investigated Cooley-Tukey FFT trafﬁc patterns on different photonic topologies in [15]. The photonic NoC is described as an electronic control network augmented with a photonic network made up of silicon waveguides and photonic switching elements (PSEs). Each PSE, shown in Figure 1, is composed of silicon micro-ring resonators that deﬂect light when polarized. These building blocks are extended to create a broadband circuit-switched 2D torus topology for onchip communication. Novel wavelength-routed architectures have also been proposed both for inter-core communications [18] and for off-chip communications [2]. These networks take advantage of wavelength-division multiplexing (WDM) to dedicate wavelengths to destinations in the network. Lower level modeling was performed in [5, 14], which is a good step towards achieving a comprehensive analysis of an architecture, but it has yet to be seen how these networks compare to other competing systems under real workloads. For electronic CMPs, Dally et al. [1] compared several possible NoC topologies using detailed timing, area, and energy models for the network components. Of the explored networks, the best in terms of energy and communication time was a Concentrated Mesh, a type of mesh topology that uses larger-radix routers to cluster four processors at each mesh node and contains express channels around the perimeter of the network. Other work proposing a hybrid interconnection network for multiple processor systems [11] characterized the inter(a) Off state (b) On state (c) 4 × 4 switch Figure 1. Photonic Switching Element. (a) Message propagate straight through. (b) Light is coupled into the perpendicular path. (c) A combination of eight ring resonators allows the construction of a 4×4 nonblocking optical switch. chip communication requirements for full scientiﬁc applications using similar measurement tools. The study found that fully connected network topologies are overprovisioned for most applications and their size grows exponentially with system concurrency. However, mapping application communication topologies onto simpler interconnect topologies such as meshes or tori leads to difﬁcult topology mapping and resource scheduling problems. A hybrid approach that employs optical circuit switches to reconﬁgure the interconnect topology to match application requirements can retain the advantages of a fully connected network using far fewer components. No timing models were used in this study whose focus was on the mapping of the inter-chip communication topologies rather than performance. 3 Studied Network Architectures This section describes the NoC architectures we examine which includes various networks for both conventional electronic networks and hybrid photonic-electronic networks. 3DI utilizing Thru-Silicon-Vias (TSVs) showcases inherently short interconnect paths with reduced resistance and capacitance, as well as lower power consumption. These characteristics enable the TSV’s to enable the switching plane to be integrated on a separate plane of stacked silicon with very low power dissipation for the vias that connect between the planes. For the 32 nm technology node, the TSV is expected to scale to a 1.4 µm contact pitch, 0.7 µm diameter, almost 5 × 107 cm−2 maximum density, and 15 µm maximum layer thickness [10]. By stacking memory and interconnect resources on dedicated CMOS layers above the processors, it is possible to integrate larger memories and faster interconnects with future CMPs [17]. Silicon nanophotonic technology may alleviate the limitations of conventional electronic networks by using optics to deliver much higher bandwidth within the same power budget, however it has several inherent limitations, such as the inability to perform buffering and processing in the optical 2 (a) Mesh (b) Concentrated Mesh (c) Concentrated Torus Figure 2. Mesh, concentrated mesh, and concentrated torus topology. The concentrated topologies require a larger-radix switch, but reduce the average hop count. domain, which need to be circumvented in order to take the full advantage of this new technology. Electrical NoC Architecture. We assume a CMP with 64 processors arranged in a 2D planar fashion. Although we do not simulate the processors themselves, we assume simple in-order cores with local store memories. The individual core size is 1.5mm × 2.0mm; the cores are located on the lowest layer of the 3DI CMOS die. Above the bottom layer are multiple layers devoted to the local store, allowing our cores sufﬁcient capacity to feed computational units. Lastly, the top layer is where the global NoC is found. This consists of the electronic routers, and for the systems that include a photonic NoC, silicon nanophotonic components. For our electrical network, we model the topologies shown in Figure 2. The mesh topology is the baseline for our comparisons against all of the other studied networks. In comparison to more exotic electronic networks, the mesh is simple to implement due to its use of relatively low radix switches in a regular 2D planar layout. We also incorporate the concept of concentrating processing cores at a network node, originally explored in [1]. For example, a full mesh would include an access point for each node, creating an 8 × 8 mesh. By concentrating a set of four nodes together, the size of the mesh can be reduced to 4 × 4 thereby reducing the average hop count each message must incur but increasing the radix of each router to accommodate the four node connections. We explore the use of a concentrated mesh and concentrated torus, shown in Figure 2 (b) and (c). Note that unlike the concentrated networks in [1], the topologies we explore do not contain express channels between non-adjacent switches. Photonic NoC Architectures. The photonic NoC is composed of two layers on the top plane of the 3DI structure, a photonic layer and an electronic control layer. The photonic layer provides a high bandwidth network for transmitting data and is constructed using silicon nanophotonic ring resonator structures that can be switched to control the propFigure 3. The photonic torus topology, studied in [15]. Switch blocks are abbreviated: X - 4 × 4 nonblocking, I - injection, E - ejection, G - gateway. Zoomed in view of the dotted box is shown in Figure 4. agation of optical signals (Figure 1). The electronic control layer is a secondary network used to transmit and act on control packets for the purpose of setting up and breaking down photonic links on the photonic layer. The control layer can also be provisioned as a low bandwidth network for transmitting small amounts of data. Switching functionality on the photonic layer is derived from the use of ring resonator structures that act as PSEs, as in [15]. In Figure 1(a), the PSE is shown in the offresonance state where messages propagate straight through the switch. Figure 1(b) shows the on-resonance state of the PSE, which bends the optical pathway implementing a turn. A control system is fabricated along with the switch to enable active switching of the device. The PSE models are implemented with the on-resonance state dormant, where no electrical current is applied, while the off-resonance state draws current to change the behavior of the device. By combining several PSEs together, functional network components such as the 4 × 4 nonblocking switch shown in Figure 1(c) can be created. As described in [15], the main network structure of the topology is a folded torus shown as black lines in Figure 3. Included on the same topology is an additional set of waveguides and switches, shown as red lines, that are used to inject and eject optical messages into and from the network. Typically, this network provides a single access point for each processing node; however, we also include variations of this network with concentrated nodes, as previously described. The transmission of data on the photonic network is enabled through the use of circuit switching, which requires the provisioning of an optical path before any data can be injected. The path-setup phase begins by sending a electronic setup control packet in the control layer, which travels through the network, establishing an optical path by 3 data along the network that is most appropriate. A preliminary study using random trafﬁc indicates a cross-over point of 256 bytes where transmitting smaller packets over the electronic control layer results in better performance and energy efﬁciency than using the photonic network alone. 4 Studied Benchmarks Our work extends related work by utilizing two sets of benchmarks: both standard synthetic trafﬁc patterns and scientiﬁc application traces. Whereas the synthetic benchmarks help to identify the kinds of trafﬁc best suited for each architecture, the application-based communication traces put real scientiﬁc workloads on the networks and test different mapping parameters. Figure 5 shows the spy plots of the eight benchmarks in this study. These plots illustrate the communication volume between each set of processors: a white square at the coordinate (pi , pj ) in the plot represents no communication, while darker shades of gray represent increasing volumes of communication between two given processors. Details of the different benchmarks are given in Table 1. Synthetic Benchmarks. We compare our NoC testbeds using four standard synthetic benchmarks from the literature [9], shown in the top of Figure 5. For each synthetic messaging pattern, two instances of the test are run: one with small messages and another with larger messages. Because of the restrictions of the hybrid interconnect studied, message transmissions are modeled as follows: each processor sends its messages as fast as possible, but blocks until receiving an acknowledgment from the destination processor before sending the next message. In the Random test, each processor sends several messages to destinations chosen uniformly at random, independently from the previous destinations. Neighbor is a stanTable 1. Benchmark Statistics Num Num Phases Messages Total Size (B) 1 1 1 1 1 1 1 1 2 2 195 34 6400 6400 6400 6400 6400 6400 6400 6400 285 63 15414 126059 614400 819200000 614400 819200000 614400 819200000 614400 819200000 7296000 8177148 86516544 5457332 Avg Msg Size (B) 96 128000 96 128000 96 128000 96 128000 25600 129796 5613 43.3 Benchmark Random-Small Random-Large Neighbor-Small Neighbor-Large Bitreverse-Small Bitreverse-Large Tornado-Small Tornado-Large Cactus GTC MADbench PARATEC 4 Figure 4. View of a single node in the photonic torus. The node(s) are connected to the gateway (GW) and the boxed areas represent switches used to control optical paths through the network. conﬁguring the appropriate PSEs. Once the setup packet reaches the destination node, the complete optical path has been allocated and an electronic acknowledgment is returned — allowing the source to begin data transmission upon receipt. The breakdown phase occurs upon complete transmission of data, where a breakdown control packet is sent along the network to release the optical path. Figure 4 shows a detail view of the required photonic components needed to transmit and receive messages on the photonic NoC. The processing node (or nodes, for the concentrated conﬁguration) injects messages electrically to the gateway, marked GW. Upon receiving an acknowledgement packet for a setup request, the gateway begins transmitting the message optically. The message ﬁrst propagates through a gateway switch, which handles the routing of messages going to and from the gateway. Next, the message is directed towards the injection switch where it is switched into the torus network. The message then propagates through the torus (using dimension-ordered routing) until it reaches the correct turning point where it turns at a 4×4 nonblocking switch. Once at the destination, the message exits the network via the ejection switch, and is directed to the gateway by the gateway switch where it is converted to an electronic signal and forwarded to the proper node. Selective Transmission. Networks that transmit data exclusively on a photonic network ideally should favor large message sizes so that the path-setup overhead is sufﬁciently amortized over the transmission time of the entire message. Applications that send many small messages are subject to the full penalty of the path-setup overhead and will see substantially lower performance. In this study, we also include a selective transmission conﬁguration of the photonic NoC that leverages the use of the electronic network as a low bandwidth data transmission medium. This conﬁguration ﬁlters the packets using a size threshold, and transmits the (a) Random (b) Neighbor (c) Bitreverse (d) Tornado (e) Cactus (f) GTC (g) MADbench (h) PARATEC Figure 5. Spyplots for the synthetic traces (top) and studied applications (bottom). dard test where each processor sends messages to its neighboring processors in the physical two-dimensional topology of the NoC. The last two synthetic messaging patterns are designed to stress two-dimensional NoC topologies: the communication of the Bitreverse pattern requires each processor to send a message to its corresponding bitreversed address, involving traversals to far regions of the network. Lastly, Tornado is a pattern designed to stress 2D meshes by having each processor communicate to its neighbor’s neighbors; the idea is to “shift” the communication of the Neighbor pattern in an adversarial way. Each of the synthetic benchmark traces are generated from their descriptions in the literature using Python scripts. Application-Based Benchmarks. A novel contribution of this research is the use of actual application communication information for the simulation of network performance. We developed a custom-designed proﬁling interface, used along with Linux’s library preloading feature to overload the communication functions, thus keeping track of all function calls in an efﬁcient, ﬁxed-size array. At the end of application execution, we output our trace data to a separate ﬁle for each process, and the ﬁles are later combined. In order to accurately approximate communication behavior without including computation time, the trace tools order the communication into “phases” that are composed of sets of communications that must complete before further communication; essentially, we use the point-to-point synchronizations inherent in message passing to build an ordering of the communication. We proﬁle and study four different SPMD-style scientiﬁc applications, with traces obtained using a custom framework to measure interprocessor communication. The parallelization style of these applications is an ideal starting point for our study, because of their easily understandable synchronous communication model and their wide use in the scientiﬁc programming community. The ﬁrst evaluated application is Cactus [6], an astrophysics computational toolkit designed to solve coupled nonlinear hyperbolic and elliptic equations that arise from Einstein’s Theory of General Relativity. Consisting of thousands of terms when fully expanded, these partial differential equations (PDEs) are solved using ﬁnite differences on a block domain-decomposed regular grid distributed over the processors. The Cactus communication characteristics reﬂect the requirements of a broad variety of PDE solvers on non-adaptive block-structured grids. The Gyrokinetic Toroidal Code (GTC) is a 3D particlein-cell (PIC) application developed to study turbulent transport in magnetic conﬁnement fusion [13]. GTC solves the non-linear gyrophase-averaged Vlasov-Poisson equations in a geometry characteristic of toroidal fusion devices. By using the particle-in-cell method, the non-linear PDE describing particle motion becomes a simple set of ordinary differential equations (ODEs) that can be solved in the Lagrangian coordinates. GTC’s Poisson solver is localized to individual processors, allowing the communication traces to only reﬂect the needs of the PIC core. The PARAllel Total Energy Code [7] (PARATEC) is a materials science application that is widely used to study properties such as strength, cohesion, growth, and transport for materials like nanostructures, complex surfaces, and doped semiconductors using the Density Functional Theory (DFT) method. In solving the Kohn-Sham equations using a plane wave basis, part of the calculation is carried out in real space and the remainder in Fourier space using specialized parallel 3D FFTs. The all-to-all communication used to implement the 3D data transpose for the FFT is the most 5 demanding portion of PARATEC’s communication characteristics. Finally, we examine MADbench [4], a benchmark based on the MADspec cosmology code. MADspec calculates the maximum likelihood angular power spectrum of the cosmic microwave background (CMB). MADbench tests the overall performance of the subsystems of real parallel architectures by retaining the communication and computational complexity of MADspec and integrating a dataset generator that ensures realistic input data. Much of the computational load of this application is due to its use of dense linear algebra, which is reﬂective of the requirements of a broader array of dense linear algebra codes in scientiﬁc workloads. Together, these four applications represent a broad subset of scientiﬁc codes with particular communication requirements both in terms of communication topology and volume of communication. For example, the nearestneighbor Cactus communication represents components from a number of applications characterized by stencil-type behavior. Thus, the results of our study are applicable to a broad range of numerical computations. 5 Simulation Methodology We have developed a comprehensive simulation framework capable of capturing key low-level physical details of both optical and electronic components, while maintaining cycle-accurate functional modeling using event-driven execution to achieve low-overhead simulation. The core framework is implemented in the OMNeT++ environment [19], which consists of around 25k lines of code, many of which are dedicated to specifying the detailed layout of photonic devices. Though OMNeT++ enables a modular construction and hierarchical instantiation of components, subtle differences in spatial positioning and orientation require some manual conﬁguration of each network. The electronic NoC, which is studied as a network for comparison, is functionally modeled cycle-accurately at 5 GHz. Electronic components, which pertain to both the electronic NoC and the electronic control plane of the photonic networks, are discussed below, followed by the photonic devices. Processing Cores. Trace ﬁles captured from evaluated benchmarks (Section 4) are read into a processing core model that injects messages into the network. Messages are injected as quickly as possible for each messaging phase, once the core is ﬁnished with previous communication. This simulates the bulk-synchronous style of communication employed by the studied applications. Likewise, the destination processors take ﬂits out of the network as soon as they arrive, under the assumption that the processor is not busy performing other computation or communication. This methodology is used to stress the network, illustrating the effects of having many messages in-ﬂight. The trace ﬁles keep track of individual messaging phases in the application. Explicit small synchronization messages are sent to and from a master core, which enforces barriers between application phases. In addition, communication elements are randomly assigned to cores in the network for the application data, to decrease the likelihood of a trace producing especially poor results by exploiting a single aspect of the network — a common artifact in real scientiﬁc computing. Each simulation is run ﬁfty times with different mappings for each trace and topology, and the min, max, and average are subsequently collected. This randomization is not performed for the synthetic traces because they are intended to stress speciﬁc aspects of the physical NoC layout. Routers. The router model implements XY dimension ordered routing with bubble ﬂow control [16] for deadlock prevention and to avoid overrunning downstream buffers. Additionally, the routers are fully pipelined with four virtual channels and can issue two grant requests in a single cycle. For power dissipation modeling, the ORION electronic router model [21] is integrated into the simulator, which provides detailed technology-speciﬁc modeling of router components such as buffers, crossbars, and arbiters. The technology point is speciﬁed as 32 nm. Buffer sizes, shown in Table 2, are determined through preliminary experiments that identify optimal power-performance tradeoffs for each implementation to enable a fair comparison between electronic and photonic networks. In general, purely electronic networks have larger buffers and channel widths to increase their performance. This involves an important tradeoff with power consumption, making it necessary to gauge efﬁciency and not merely performance or power, which will be discussed further in the analysis of the results obtained. The concentrated networks also have larger buffers, presuming that this is appropriate given the smaller network size. Finally, the photonic networks using the Selective message ﬁlter have larger buffers to accommodate the electronic trafﬁc that is allowed to travel on the interconnect. Wires. Our detailed wire model is based on data collected for various wire lengths with different numbers of repeaters, running at 5 GHz with double pumping. This allows us to optimally buffer wires for power dissipation (around 50 fJ/bit/mm), which dictates the wire latency. Individual wire lengths are calculated using core size, router area (calculated by ORION), number of routers, and topology. Photonic Devices. Modeling of optical components is built on a detailed physical layer library that we and others 6 Table 2. Electronic Router Parameters Topology Channel Buffer Width Size (b) Electronic Mesh Electronic Concentrated Mesh Electronic Concentrated Torus Photonic Torus Selective Photonic Torus Photonic Concentrated Torus Selective Photonic Concentrated Torus 128 128 128 32 64 32 64 1024 2048 2048 512 1024 1024 2048 have validated through the physical measurement of fabricated devices. The modeled components are primarily fabricated in silicon at the nano-scale, and include modulators, photodetectors, waveguides (straight, bending, crossing), ﬁlters, and PSEs consisting of ring resonators. These devices are characterized by attributes such as insertion loss, extinction ratio, delay, and power dissipation. Table 3 shows the optical parameters used [12, 20], excluding insertion loss and extinction ratio for brevity. Devices are sized appropriately and laid out into a network topology, which is controlled by the underlying electronic network. A key parameter for the photonic devices, which greatly affects network performance, is the number of allowable wavelengths. This number is ultimately constrained by network size, since larger networks will exhibit a greater network level insertion loss [8]. The upper limit on available source power is the non-linear threshold of the ring resonators, while the lower limit in received power is dictated by the sensitivity of the photodetectors. An important advantage of our detailed simulator is the ability to perform this physical layer analysis, as shown in Figure 6, which determines the number of wavelengths available at different power budgets for a 64-core photonic torus. We found that 65 wavelengths can be used for the normal 8×8, and 150 Figure 6. Insertion loss analysis of Photonic Torus topology. 7 Table 3. Optical Device Parameters Sim Parameter Value Data rate (per wavelength) PSE dynamic energy PSE static (OFF) energy Modulation switching energy Modulation static energy (ON) Detector energy Wavelengths (8×8 network) Wavelengths (4×4 conc. network) 10 Gb/sec 375 fJ∗ 400 uJ/sec† 25 fJ/bit‡ 30 µW§ 50 fJ/bit¶ 65 128 for the 4×4 concentrated network for an optical power budget of 35 dB. We limit the max number of wavelengths to 128, considering space limitations on laser delivery to the modulators. 6 Results We now evaluate the performance characteristics of the selected NoC implementations using the synthetic and application traces. The synthetic benchmarks provide a highlevel picture of the interconnect’s responsiveness to different commonly-observed communication patterns, while the application traces give insight to performance under realistic scientiﬁc loads. The reported metrics are as follows: (1) performance is analyzed via the execution time of the benchmark or application, (2) energy cost by the total energy spent in execution, and (3) energy efﬁciency by the performance gained from each unit energy. Note that while typical network comparisons use message latency as a performance metric, such analysis would underscore the true performance of the system by only examining the transmission speed of single streams of data. Because the execution times and energies of the benchmarks varies broadly, we normalize the results to the electronic mesh performance. We choose an electronic mesh as the baseline because it represents the most straightforward engineering approach to interconnecting cores for emerging manycore processor designs. Recall that the scientiﬁc application experiments are conducted using ﬁfty random process placements to develop a statistical view of the networks responsiveness to varying communication mappings (see Section 5). Appli∗Dynamic energy dissipation calculation based on carrier density, assuming 50-µm micro-ring diameter, 320-nm × 250-nm micro-ring waveguide cross-section, 75% waveguide volume exposure, 1-V forward bias. †Based on switching energy, including photon lifetime for re-injection. ‡ Same as ∗ , for a 3µm ring modulator. §Based on experimental measurements in [22]. Calculated for half a 10GHz clock cycle, with 50% probability of a 1-bit. ¶Conservative approximation assuming femto-farad class receiverless SiGe detector with C < 1f F . (a) (b) Figure 7. Network speedup relative to the electronic mesh. (c) (a) (b) (c) Figure 8. Energy savings relative to electronic mesh. MADbench and PARATEC shown in inset for clarity in (c). cation results are therefore shown using the average performance, with error bars indicating min and max behavior. Network Speedup. Figure 7 presents the application execution time speedup achieved by the examined NoC architectures relative to the execution time of the baseline electronic mesh. Values start at one, which indicates even performance with the baseline. For the synthetic tests with small messages, which are shown in Figure 7 (a), the photonic networks without selective transmission do not show improved performance, because the setup messages result in increased latency that is not sufﬁciently amortized by the high bandwidth end-to-end transmission of the photonic network. We see that selective transmission shows improvement, but does not gain in speedup over the electronic mesh due to the increased number of routers in the hybrid network used for injection and ejection (see Figure 3). The synthetic tests with large messages, which are displayed in Figure 7 (b), show a signiﬁcant improvement for the hybrid photonic networks, compared to what is observed for the experiments conducted on small messages. This illustrates the beneﬁt of amortizing the setup overhead for purely circuit-switched photonic networks. Additionally, it is interesting to note the improvement for the Bitreverse benchmark, which exhibits signiﬁcantly longer communication patterns, in that circuit-switching directly improves the performance by mitigating contention on a one-time basis. Recall that the effective bandwidth of the photonic network only matches that of the electronic ones when the photonic network is concentrated (128λ×10Gbps vs. 128 channel width × 5GHz double pumped), which is why they perform signiﬁcantly better than their full-network counterparts. Figure 7 (c) shows the relative speedup of the real application traces. The concentrated photonic networks clearly outperform the other interconnect conﬁgurations for both Cactus and GTC, similar to the synthetic large-message traces. The photonic networks do not perform as well for the MADBench and PARATEC applications primarily because those benchmarks exhibit all-to-one and broadcast communication patterns, which are expected to behave poorly in circuit-switched networks. For these types of applications, wavelength-routed inter-core networks would likely be more appropriate, and future work investigating the use of both circuit-switched and wavelength-routed photonics is under way. In addition, these two benchmarks use signiﬁcantly smaller message sizes (see Table 1). The se8 (a) (b) (c) Figure 9. Energy efﬁciency (network performance per unit energy) relative to the electronic mesh. MADbench and PARATEC shown in inset for clarity in (c). lective networks narrow the performance difference somewhat, but still do not achieve the nominal performance of the electronic mesh network, similar to the synthetic traces using small messages. Energy Consumption. Figure 8 presents the results of the metric of total energy consumption; the plot shows the inverse of consumption (i.e. the energy savings), again relative to the electronic mesh baseline. The photonic networks are clear winners for most experiments — particularly the large-message synthetics as well as Cactus and GTC applications — showing over 10× improvement due to the decoupling of distance, bandwidth, and power during optical transmission. Since the circuit-switched photonic network does not consume power per-hop, the energy usage is much lower than the packet-switched electrical networks, which require energy consumption in order to make routing decisions at each hop. This point is particularly illustrated again in the Bitreverse benchmark. Because photonics is completely decoupled from distance travelled with repect to energy spent during transmission, it will provide higher beneﬁts when communication pairs are further apart. Performance for Energy Spent. Figure 9 shows the ﬁnal metric: performance gained for every unit of energy spent, which is effectively a measure of a network’s efﬁciency. This metric is calculated by multiplying the network execution time by the energy spent (plotted as the inverse so that values greater than 1 indicate a better performance per energy). The numbers are shown relative to the electronic mesh. The benchmarks with small messages perform poorly on photonic networks, as seen in Figure 9 (a). Although network speedup is reasonable for some photonic networks in Figure 7, and energy gains are achieved for some photonic networks in Figure 8, the overall network performance is not improved over the electronic mesh when message sizes are small. However, as shown in Figures 9 (b) and (c), the photonic networks’ energy efﬁciency improvement over the electronic mesh for traces with large message sizes is ampliﬁed by the gains in both speedup and energy, resulting in improvements of over 20×. This beneﬁt is realized over a variety of communication patterns, including two of the real applications, which demonstrates the possible appeal of on-chip photonics for many classes of applications. 7 Conclusions and Future Work This work compares the performance and energy characteristics of electronic and photonic NoCs using a suite of synthetic communication benchmarks as well as traces from SPMD-style scientiﬁc applications on a detailed simulation framework. We show that a hybrid NoC has the potential to outperform electrical NoCs in terms of performance, while mitigating the power/energy issues that plague electronic NoCs when the communications are sufﬁciently large to amortize the increased message latency. For messaging patterns with small messages and high connectivity, the current photonic network design does not perform as well as an electronic mesh, although parameter searches may mitigate this by sizing queues and message size cutoffs to enable better performance in the selective approach. The comprehensive and detailed level of simulation as well as the range of applications and topologies investigated achieves interesting results that are not possible using a higher-level analysis. These observations will be important in guiding future CMP engineers who seek to design an interconnect architecture that does not become the bottleneck for performance or energy. As future architectures scale to even higher concurrencies, the power requirements and performance beneﬁts of photonic interconnects will become increasingly attractive. Although these results have addressed some questions about how different applications would behave on different NoCs, it also raises a number of concerns that will lead to 9 important future studies. This work focuses completely on the interconnection network and does not account for data transfer onto the chip from DRAM, nor does it account for computing performance. Furthermore, it is not clear how the performance and energy consumption of the networks ﬁt into overall system performance and energy, and how communication can be overlapped with computation more efﬁciently. These experiments are currently being pursued for future work. Alternative topologies for both electronic and photonic networks must also be explored. Photonic network architectures that exhibit less blocking under heavy loads have been proposed in related work, and will be examined in detailed future studies. Many methods of improving electronic interconnect performance are also emerging that may substantially change the comparison between photonic and electronic NoCs. A key contribution of our work was the focus on SPMD style applications found in the scientiﬁc community. Although many elements of these algorithms are ﬁnding their way into consumer applications such as realistic physics for games, and image processing kernels, future studies will also explore applications with more asynchronous communication models. We plan to make a deeper examination of the differences between message passing and shared memory applications and how they interact with both photonic and electronic networks characteristics. All of these reﬁnements will be subjects for future work, using the foundation presented in this paper. 8 Acknowledgements This research is partially supported by DARPA MTO ofﬁce under grant ARL W911NF-08-1-0127 and the National Science Foundation (Award #: 0811012). "
Diagnosis of interconnect shorts in mesh NoCs.,We propose a method to diagnose interconnect short-circuit faults in mesh NoCs. The fault model comprises all shorts between any two wires of a defined NoC neighborhood. Test sequences are applied in NoC functional mode. Experimental results show that 93% of the interconnect shorts can be diagnosed.,"Diagnosis of Interconnect Shorts in Mesh ҭoCs Marcos Hervé       Érika Cota      Fernanda Lima Kastensmidt      Marcelo Lubaszewski Universidade Federal do Rio Grande do Sul PPGC – PGMICRO - Instituto de Informática / Escola de Engenharia  P.O. Box 15064, ZIP 91501-970, Porto Alegre, RS, Brazil marcos.barcellos@ufrgs.br, {erika,  fglima}@inf.ufrgs.br, luba@ece.ufrgs.br Abstract We propose a method to diagnose interconnect short-circuit faults in mesh ̱oCs. The fault model comprises all shorts between any two wires of a defined ̱oC neighborhood. Test sequences are applied in ̱oC functional mode. Experimental results show that 93% of the interconnect shorts can be diagnosed. 1. Introduction Due to the advance of process technology, it is possible to design and fabricate complex system-onchip (SoC) composed of a large number of cores connected through a network on-chip (NoC) [1]. Fabricating such systems with no defects in the logic and interconnect structures is a major challenge. Faults in the network interconnects are of special concern because of the wire density and the central role of this structure in the system function. Ensuring the integrity of the interconnects is important to guarantee the communication between cores, not only during in-field operation, but also during test since the NoC is very often used as a  Test Access Mechanism (TAM) for the cores [1-4]. As a result, efficiently checking the communication infrastructure becomes essential to guarantee the final system test quality and reliability. A great advantage of a large SoC is its inherent redundancy (many NoC channels and many cores). In addition, adaptive scheduling of tasks in NoC-based multiprocessor SoCs has been used to improve the system performance. Thus, even if few faults are detected in the interconnects, the system can still work properly if there are enough remaining cores and network channels that are not affected by the faults. To deal with faulty network channels, it is necessary to adapt the routing to avoid the faulty channels. Smart routing schemes have been proposed to deal with network’s congestion and those solutions can be used for NoCs with permanent interconnect faults. Smart routing is based on techniques that combine the advantages of both deterministic and adaptive routing schemes [5]. In this case, it is possible to switch between deterministic and adaptive routing based on not only the network’s congestion conditions, but also on the testing diagnosis. If a channel has a permanent fault, the traffic in the network must continue avoiding that channel. In many cases, the core connected to a faulty channel becomes inaccessible, which requires a new distribution of the tasks as well. In a lower granularity, if the faulty wires are identified and spare wires are included in the design, the faulty channel can still be used (no tasks rescheduling required) and a simpler routing adaptation can repair the system. In any case, interconnect fault diagnosis is a prerequisite to increase the yield and the product lifetime. Recent works have been addressing the detection of faults in the NoC infrastructure, including routers (or switches) [6-8] and interconnect channels (or links) [911]. However, detection is not enough if one wants to mitigate interconnect faults to increase the yield and system availability. The diagnosis of faults must be pursued. Diagnosis of links has been addressed in the frame of on-line testing by Grecu et al. in [12]. Also, Raik et al. extend in [13] the use of test configurations proposed in [10] for locating faulty links inside the NoC switches. Nevertheless, these two approaches can neither guarantee the detection of inter-link shorts, nor locate inter or intra-channel faulty wires. In this paper, we propose a method to diagnose pairwise short faults in a defined neighborhood of a mesh NoC. Fault detection and diagnosis, as addressed here, do apply for both, post-production testing and infield off-line periodic checking. First, the faulty channels are identified and later on the wires. The extended fault model proposed in [11] is assumed and a functional test method is used. Section 2 details the used fault model and reviews the functional method proposed in [11] for the test of data wires. The two main contributions of this work are: i) a detailed  analysis of the diagnosability of interconnect short faults for mesh NoCs is presented in Section 3, considering functional test methods and a basic 2x2 NoC; ii) a novel integrated test and diagnosis method is proposed in Section 4, which achieves 100% fault detection and over 90% of complete diagnosis for a particular NoC. Furthermore,  in Section 5  the infrastructure necessary to implement the method is 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    discussed and the implementation costs are evaluated for a NoC-based SoC. Finally, experimental results for an academic NoC are presented in Section 6 and Section 7 concludes the paper. 2. Related prior work We assume the test of the NoC interconnects is one part of the whole test of a NoC-based SoC, which consists of the test of the NoC and the test of the IP cores. The test of the NoC can be divided into the test of the routers and the test of the interconnects. This paper focuses on the test and diagnosis of interconnects only. Short-circuits are considered here, because the test sequences detecting these faults also achieve a high coverage of stuck-at and open-circuit faults. In next sections, we briefly review the fault model and the method proposed in [11] for the test of data wires. Both are analyzed and improved in this work so as to propose a new test method with enhanced diagnosis figures. 2.1. ҭoC case study We base our analysis on the SoCIN packetswitching network model introduced in [14]. As most existing NoCs, this network implements a 2-D mesh topology. Each router in the SoCIN network is composed of five input and five output ports. One pair of input/output ports is dedicated to the connection between the router and the core, while the remaining four pairs connect the router with the four adjacent routers. The network uses credit-based flow-control and XY routing, where a packet is first routed on the X direction and then on the Y direction before reaching its destination. Switching is based on the wormhole approach, where a packet is broken up into multiple flits (flow control units): a header flit and a set of payload flits followed by the tail flit. Flit is the smallest unit over which the flow control is performed and its size equals the channel width. For the case-study NoC topology, the communication channels are defined to be 8-bit wide. The interconnect wires under test consist on the 8-bit data wires, disregarding the four available control wires (bop, eop, ack, val). 2.2. Fault space definition A short fault is a state in which a net is connected to another net, which can be placed at the same metal layer or at the top or bottom metal layers. There are three types of short faults: OR-short, AND-short and strong driver. Short faults in the NoC can reach nets1 of 1 The words net, wire, and line mean a single wire interconnecting two entities (routers and cores).  The words channel and  link mean a group of wires connecting two entities. many different interconnect channels, including routerto-router  interconnect  links  and  router-to-core interconnect links in both directions. When considering short faults in the NoC, it is important to define the region where the faults may occur, i.e., which lines are more likely to be shortcircuited.  In  this work we assume a  faulty neighborhood composed of 4 routers (in a 2x2 grid configuration) and all data wires connected to these routers, as illustrated in Figure 1. This configuration was defined taking into account average connections lengths and the probability of shorts into different metal layers. In the worst case scenario, shorts can occur between any two data wires of this entire 2x2 NoC architecture. Let us call this configuration our “basic NoC”, which is considered here as the minimum search space for the detection and diagnosis of short faults. As proposed in [11], the test of larger NoCs can be defined based on the test of this basic NoC. Scan chain Scan chain (from ATE) (from ATE) TDG TDG TED TED Scan chain Scan chain (to ATE) (to ATE) core 0 core 0 core 0 core 0 core 1 core 1 core 1 core 1 NI 0 NI 0 N N router router router router 10 10 10 10 E E L L NI 1 NI 1 router router router router 00 00 00 00 router router router router 10 10 10 10 router router router router 01 01 01 01 router router router router 11 11 11 11 L L W W router router router router 01 01 01 01 S S NI 2 NI 2 core 2 core 2 core 2 core 2 NI 3 NI 3 core 3 core 3 core 3 core 3 8 data bits 8 data bits bop bop eop eop va l va l ack ack Figure 1. Test strategy for the basic NoC topology. For the basic NoC,  there are 16 communication channels to be tested, each one with 8 data lines. In total, the basic 2x2 NoC has 128 data wires that can be potentially shorted with each other. This means that 8,128 pairwise shorts are possible and equally likely. 2.3. Functional test method The test method proposed in [11] consists in detecting all pairwise shorts within a 2x2 NoC (as defined in the fault model) by sending packets through this basic network. Assuming the routers in functional mode, the communication protocol (packets organized                                                  as header, payload, and tail) must be used to apply test vectors to the NoC interconnects. The test sequence is placed on the packet payload. For the detection of short faults between any pair among the interconnect wires under test, one can reason as if all channels of the basic 2x2 NoC formed a large bus, and reuse the sequences developed in the past for the test of conventional wiring networks, e.g. [15, 16]. Consequently, the interconnect wires of all NoC channels must be completely filled with the test vectors in order to detect each fault. In other words, the packets must be sent in such a way that all channels are filled up at the same time. Aiming at  the diagnosis at wire-level, our interconnect test and diagnosis approach uses the Walking Sequence proposed in [16]. The Walking Sequence was proven in [17] to be the shortest onestep test sequence providing for the maximal diagnosis of stuck-at, opens and short faults in wiring networks. Without loss of generality, in this work we exemplify our method using a Walking-One sequence  and evaluate its coverage to the AND-short fault model. The number of test vectors of the Walking-One Sequence is equal to the entire number of nets under test. Test Data Generators (TDGs) and Test Error Detectors (TEDs) are assumed to be connected to the network interfaces (NIs in Figure 1) to send and receive the test packets. Since all packets are sent with headers and tails, short faults may affect those flits, thus modifying the packet routing. When this occurs, the packet can be routed to any other node of the NoC and the target node may not receive the test packet. Thus, shorts that affect the header bits that carry the routing information are classified as timeout faults. If the short does not cause packet loss, the fault is of payload type. Figure 1 illustrates the application of the test sequence of [11] in the basic NoC topology. The proposed approach exercises all the data wires using the routers functional mode. Considering the XY routing strategy, this means that 4 packets can be sent across the 2x2 mesh network. The packet paths are shown in Figure 1 by the four different lines, one solid, one dashed, one with lines and points, and one with triple lines. For instance, for Packet 00 (solid lines) the routing path is given by one step east and one step south, from NI 0 to router 00, to router 01, to router 11, to NI 3. Each path has four 8bit channels which can be seen as a large 32-bit wide bus. Thus, using the Walking-One sequence for the basic NoC, we have 32 test vectors for each path. These 32 test vectors then must be organized into packets to be sent through the NoC by the 4 NIs simultaneously, as detailed in Figure 2. Since all NIs use the same test sequence (32 vectors), the payload containing the test sequence should be shifted in time and channels should be filled in with strings of zeros to ensure the detection capabilities of the Walking-One sequence by having only one bit at ‘1’ at a time in the 128 wires (16*8-bit channels in total). This results in a total of 182 flits, as detailed in [11]. The proposed pairwise testing method is capable of detecting all faults in the data wires but, despite the use of the Walking Sequence, presents poor diagnosability, as shown next. 3. Analyzing the diagnosis capability For the sake of simplicity, let us assume first the diagnosis capability of the test method as the ability of identifying in which channel the fault occurs. As it will be discussed later, the identification of the erroneous flits in the TEDs will make it possible to locate the faulty wires. N N I I 0 0 h h e e a a d d e e r r N N I I 1 1 h h e e a a d d e e r r N N I I 2 2 h h e e a a d d e e r r N N I I 3 3 h h e e a a d d e e r r 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 strings of zeros strings of zeros l l p p a a y y o o a a d d strings of zeros strings of zeros t t a a i i l l strings of zeros strings of zeros l l p p a a y y o o a a d d strings of zeros strings of zeros t t a a i i l l strings of zeros strings of zeros l l p p a a y y o o a a d d strings strings of of zeros zeros t t a a i i l l strings of zeros strings of zeros l l p p a a y y o o a a d d t t a a i i l l time time Figure 2. Test sequence application for the detection of shorts in the data wires. Considering the same fault model described in Section 2, there are 136 cases of possible faulty channels (combination of the 16 channels two by two plus 16 channels with internal shorts). To identify the faulty channel, it is necessary to identify which test error detector (TED) signals the error. For the basic 2x2 NoC, the possible error signals are shown in Table 1. Note that because of the defined fault model (single shorts), at most two TEDs can signal an error, which gives 10 possible error combinations.                 Using the information shown in Table 1, it is possible to relate the error detection with the faulty channels. This can be done by making a list of the channels located in the path that causes the error detection.  For example, if only TED11 (Test Error Detector connected to the router 11) signals the error, the fault must be in one of the following channels only: TDG00-to-Router00, Router00-to-Router01, Router01to-Router11, Router11-to-TED11. These channels represent the suspect channels u and v and there is a short between two wires (i and j) within one of these channels (u = v), or between two wires from two different suspect channels (u ≠ v). For the example mentioned above, there are 10 possible cases of faulty channels, as shown in Table 2. Table 1. Error detection combinations for the test method described in section 2. TED (Test Error Detector) Location 00 01 10 11 OK OK OK OK OK OK Error Error Error OK OK OK OK OK Error Error Error OK OK OK Error OK OK Error Error OK OK OK Error OK Error OK Error Error OK OK Error OK Error Error 1 2 3 4 5 6 7 8 9 10 Table 2. List of possible faulty channels (shorts between wire i from channel u and wire j from channel v). Channel u (containing wire i) TDG00-to-Router00 TDG00-to-Router00 TDG00-to-Router00 TDG00-to-Router00 Router00-to-Router01 Router00-to-Router01 Router00-to-Router01 Router01-to-Router11 Router01-to-Router11 Router11-to-TED11 1 2 3 4 5 6 7 8 9 10 Channel v (containing wire j) TDG00-to-Router00 Router00-to-Router01 Router01-to-Router11 Router11-to-TED11 Router00-to-Router01 Router01-to-Router11 Router11-to-TED11 Router01-to-Router11 Router11-to-TED11 Router11-to-TED11 Extending this analysis to all the payload faults in the basic NoC we obtain a diagnosis classification for the method in Section 2. This classification is shown in Table 3. According to the table, for 65% of the possible short faults, the number of suspect channel pairs is at least 10. We note that the same analysis can be done for timeout  faults, but  the payload  faults already demonstrate the poor diagnosability of the test method. If the granularity of the diagnosis is smaller (wire diagnosis), the results are even less effective. We must then consider alternatives for the functional test in order to keep the high detection coverage while improving the diagnosis capability. Table 3. Diagnosis classification for all the faults detected using the functional test method described in section 2. Payload Errors 10 suspect channel pairs 1,043 16 suspect channel pairs 4,285 Timeout Errors Total 2,800 8,128 4. ҭew integrated test and diagnosis method The poor diagnosis capability of  the original functional method is a consequence of the path taken by the test packet in the network. To reduce the test application time,  the method described in Section 2 uses the smallest number of packets that guarantee 100% fault detection. However, a packet sent from one TDG is analyzed after two hops2, which precludes the reduction in the number of suspects during diagnosis. To improve the diagnosis capability, we propose a different path for the test packets so that less hops are required. The new test scheme uses the same test vectors shown in Figure 2 but comprises two test cycles, as depicted in Figure 3. In the first cycle, the packets are sent clockwise from the TDG to the adjacent TED, i.e., TDG00 sends to TED01, TDG01 sends to TED11, TDG11 sends to TED10, and TDG10 sends to TED00, as shown in Figure 3(a). In this first cycle, the channels Router00-to-Router10, Router10to-Router11, Router11-to-Router01 and Router01-toRouter00 remain unused. We take advantage of a characteristic of the SoCIN router (RASoC), which keeps unused channels at logic level “0”.  This feature is based on the reset/preset of the input/output buffers in the router and it can be easily modified if needed. In the second cycle, the packets are sent in the opposite direction, i.e.: TDG00 sends to TED10, TDG10 sends to TED11, TDG11 sends to TED01, and TDG01 sends to TED00, as shown in Figure 3(b). In this second cycle, the channels that were used in the first cycle remain unused. This way, after the two cycles, all channels have been exercised. Using the same diagnosis methodology, we applied the modified test scheme in a NoC 2x2, using the two cycles, and analyzed the error detection by the TEDs. Again, for the sake of simplicity, let us present the analysis only for the payload faults, since they are the 2 hop is the number of times the packet passes from one router to another router                                                  ones that represent most of the cases. The same study can be carried out for timeout faults as well. The diagnosis analysis is made by combining the error detection in the first cycle with the error detection in the second cycle. For instance, if an error is detected by TED01 in the first cycle, there are three suspect channels: TDG00-to-Router00, Router00-to-Router01 and Router01-to-TED01. If in cycle 2 TED11 signals an error, three other channels are suspects: TDG10-toRouter10, Router10-to-Router11 and Router11-toTED11. Since the channels connected to the TDGs and TEDs do not present faulty behavior in at least one of the cycles, they can be discarded from the suspects list, leaving only the two router-to-router channels as suspects. So, the diagnostic for this example is that there is a short between a wire in the channel Router00-to-Router01 and a wire in the channel Router10-to-Router11. In the presence of a pairwise short, the application of the Walking-One sequence to these two channels will make that one payload flit will not pass the checking of TED01 in cycle 1, and another payload flit will not pass the checking of TED11 in cycle 2. Assuming a wired-AND fault model, once the erroneous flits are known, those logic 1 bits that became logic 0 due to the fault uniquely identify the two wires involved in the short-circuit. However, not all cases can be diagnosed directly. If, for example, TED01 signals an error in cycle 1, the suspect channels are TDG00-to-Router00, Router00-toRouter01, and Router01-to-TED01. If TED10 indicates an error in cycle 2, the list of suspect channels is TDG00-to-Router00,  Router00-to-Router10  and Router10-to-TED10. In this case, the error signal is associated to four possible pairs of faulty channels as presented in Table 4. Table 4. Possible faulty channels associated with a particular error. Suspect channels Router00-to-Router01 Router00-to-Router10 Router00-to-Router10 TDG00-to-Router00 Router00-to-Router01 TDG00-to-Router00 TDG00-to-Router00 TDG00-to-Router00 From the diagnosability analysis performed for all payload faults, one can observe three possibilities regarding the relationship between the error detection and the fault which caused it, as shown in Table 5. The first group corresponds to the diagnosed cases (D), in which the error signal points to a single pair of suspect channels, therefore uniquely identifying the faulty channel pairs. The second group corresponds to unresolved cases (U), where the error detection relates to at least two suspect channel pairs. The third group corresponds to the combinations of error detection that are impossible to occur (N) for the assumed fault model. From the 136 possible faulty channel pairs, the diagnosed group (76 detection combinations in Table 5) corresponds to 76 cases and the unresolved group (18 detection combinations) corresponds to 60 faulty channel pairs. The remaining 26 cases of error signals are not possible to occur (N) in cycles 1 and 2, and thus are not related to any possible fault. For the unresolved cases  (U), we propose a dedicated technique presented later in this paper. core 0 core 0 core 0 core 0 core 0 core 0 NI 0 NI 0 NI 0 NI 2 NI 2 NI 2 core 2 core 2 core 2 core 2 core 2 core 2 router router router router router router 00 00 00 00 00 00 router router router router router router 10 10 10 10 10 10 router router router router router router 01 01 01 01 01 01 router router router router router router 11 11 11 11 11 11 Unused Unused Unused core 1 core 1 core 1 core 1 core 1 core 1 NI 1 NI 1 NI 1 NI 3 NI 3 NI 3 core 3 core 3 core 3 core 3 core 3 core 3 core 0 core 0 core 0 core 0 NI 0 NI 0 NI 2 NI 2 core 2 core 2 core 2 core 2 router router router router 00 00 00 00 router router router router 10 10 10 10 router router router router 01 01 01 01 router router router router 11 11 11 11 core 1 core 1 core 1 core 1 NI 1 NI 1 NI 3 NI 3 core 3 core 3 core 3 core 3 Unused Unused (a) test cycle 1  (a) test cycle 1  (b) test cycle 2 (a) test cycle 2  (a) test cycle 2  Figure 3. New testing scheme.                           Table 5. Possible combinations of error signals in cycles 1 and 2 of the new test scheme  (D=diagnosed,  N=not possible to diagnose and U=unresolved). Cycle 1 (TED #) Error 10 / 11 00 D 01 D 10 D 11 D 00 / 01 D 00 / 10 D 00 / 11 01 / 10 01 / 11 D 10 / 11 D Not detected D N N 01 / 11 D D D D D D N N D D D 01 / 10 N U U N N N N U N N D 00 / 11 U N N U N N U N N N D 00 / 10 D D D D D D N N D D D 00 / 01 D D D D D D N N D D D 11 U D D U D D U N D D D 10 D U U D D D N U D D D 01 D U U D D D N U D D D 00 U D D U D D U N D D D C c y l e 2 ( D E T # ) Not detected D D D D D D D D D D N Once the faulty channel is identified, one can easily reduce the granularity of the method and diagnose the actual faulty wire within  the channel using  the information of  the applied vector sequence, as explained. If spare wires are used in the NoC channels, such a strategy can be interesting to keep the network working without  path  and  task  scheduling reconfiguration. The faulty wire can be identified by counting the flit of the test sequence that triggers the error signal in the TED. Since the faulty wire identification is related to the used test sequence, and is easily diagnosed once  the  faulty channels are identified, we will focus mainly on the identification of the faulty channels. Experimental results, however, were generated considering the wire diagnosis level. Within  the unresolved cases, we  identified 3 different groups considering the number of possible faults associated with the error signals. The first group is associated with 4 possible faulty channel pairs (like the example above, with the diagnosis result shown in Table 5), the second group is associated with 3 possible faulty channel pairs, and the third group is associated with 2 possible faulty channel pairs. Extra test cycles are required to identify the faulty channels when an unresolved diagnostic is given. The extra cycles for the example in Table 5 are shown in Figures 4(a), 4(b), and 4(c), respectively. With the new cycles one can diagnose all cases that remained unresolved after the first two cycles of the test sequence. The test vectors used in the extra cycles consist in a number of all-zero flits (enough to fill the path of the message with zeros), and one all-one flit in the middle of the packet. This sequence is able to detect short circuit faults in the suspect channels and is considerably smaller than the sequence used in the first two cycles. Each extra cycle excludes one suspect from the list. Thus, for the subgroup with 4 suspect faulty channel pairs, three extra cycles are necessary for diagnosis. For the subgroup with 3 suspects, two extra cycles are necessary, and for the subgroup with 2 possible faulty channel pairs only one extra cycle is needed for diagnosis. Notice that, in the extra cycles, all wires inside the suspect channels that are connected to the TDGs or TEDs, but that do not belong to the message path, must be set to a pre-defined logic level (either “0” or “1”), as shown in Figures 4(a) and (b). If these channels are involved in the fault, then the imposed 0-logic will dominate the short and will ensure that the TED in use indicates an error. If a 1-logic is imposed, then that channel will not provoke an error, and the other suspects may be checked. Setting the channels to a pre-defined value is very simple to do for the channels connected to the TDGs, since the TDG is already responsible for driving the channel. The channels connected  to  the TEDs, however, must be driven by the routers connected to them. This can be done by applying a test vector in the channel before the test begins until after the test finishes or by using a router that makes it possible to choose  the  logic  level of  the unused channels connected to it. Considering the example of the unresolved case that resulted in the suspect’s list shown in Table 4, the next extra cycle applied for diagnosis would be cycle 3. In this cycle and for the suspect channels listed, the packet is routed from TDG01 passing through router01, router00, router10 and reaching TED10. The channel connecting TDG00 and router00 must be set to logic level “1”. Notice that this channel is part of the suspect       channel’s list. The sequence sent by TDG01 fills the message’s path with zeros, sending all-zero flits. In the middle of the packet the TDG sends an all-one flit. Once this flit reaches the channel that connects router00 to router01 it will only be affected if the fault is between the channel Router00-to-Router01 and the channel Router00-to-Router10 (considering a wiredAND fault). Therefore, if the TED10 detects an error in cycle 3 the fault is diagnosed, otherwise one suspect from the suspect’s list is removed and another extra cycle is applied. NI 0 NI 0 1 1 Wires in this Wires in this channel are set to  channel are set to  logic level “1”. logic level “1”. NI 0 NI 0 0 0 Wires in this Wires in this channel are set to  channel are set to  logic level “0”. logic level “0”. NI  1 NI  1 Message Message Send Send Message Message 00 00 router router 00 00 10 10 router router 10 10 01 01 router router 01 01 11 11 router router 11 11 NI 2 NI 2 NI 3 NI 3 (a) Test Cycle 3 (extra) (a) Test Cycle 3 (extra) NI  1 NI  1 Message Message Send Send Message Message 00 00 router router 00 00 10 10 router router 10 10 01 01 router router 01 01 11 11 router router 11 11 NI 2 NI 2 NI 3 NI 3 (b) Test Cycle 4 (extra) (b) Test Cycle 4 (extra) Send Send Message Message Message Message NI 0 NI 0 NI  1 NI  1 00 00 router router 00 00 10 10 router router 10 10 01 01 router router 01 01 11 11 router router 11 11 NI 2 NI 2 NI 3 NI 3 (c) Test Cycle 5 (extra) (c) Test Cycle 5 (extra) Figure 4. Additional test cycles to diagnose unresolved detection combinations. Cycle 4 is very similar to cycle 3, with the same message and message’s path, but the logic level set in the channel TDG00 is “0” instead of “1”. In this cycle, when the all-one flit reaches channel Router00-toRouter10, it will only be affected if the fault is between the channel Router00-to-Router10 and TDG00-toRouter00. In this case, if TED10 detects an error, the fault is diagnosed, otherwise one additional suspect is removed from the suspect’s list and extra cycle 5 is applied. In cycle 5, the same message is sent, but the message’s path is different from the path used in the previous extra cycles. The packet is routed from TDG00 to TED10 passing through  router00 and router10. When the all-one flit is sent, it will only be affected if the fault is between wires from two different channels, eliminating the possibility of a fault between two wires within the same channel. In this case, the TED10 error detection makes it possible to identify the faulty channels from the two remaining suspects. The message’s path and the channels involved in the extra cycles depend on the suspect’s list. For the timeout faults, the same kind of diagnosis classification as in Table 5 was done. The time-out classification resulted in 72 cases associated to 30 suspect channel pairs. This is possible since more than one error signal is associated with only one fault. It was also observed a total of 24 unresolved cases associated to 28 possible faulty channel pairs. 5. Implementation of the proposed method To implement the proposed test and diagnosis strategy, Test Data Generators (TDGs) and Test Error Detectors (TEDs) must be included in the logic that connects an IP core to the network, i. e., the network interface (NI). A programmable core can also perform those tasks. However, not all cores can be programmed and the network interface is designed according to the network protocol. Thus, the inclusion of the test structures in the NI is a more generic solution. The test data generator (TDG) and the test error detector (TED) have a quite simple logic. Basically, the TDG must generate one test sequence for cycles 1 and 2, and another test sequence for cycles 3, 4 and 5. Additionally, cycles 1, 2 and 5 implement test paths the differ from those of cycles 3 and 4. For cycles 1 and 2, the TDG generates a number of zero flits followed by the payload flits and then another series of zero flits, as shown in Figure 2. The payload flits are such that they guarantee only one bit in “1” in the whole network at the same time. For cycles 3, 4 and 5, the TDG also generates a number of zero flits followed by the payload flit and then another series of zero flits. The difference is that the payload here is a single all-one flit. To cope with these two different test sequences and the different message paths,  the  test sequence identification, the numbers of strings of zeros and the number of cycles a flit takes to traverse the network are inputs of the TDG. The header data is also received from the external test equipment (ATE) to make it possible programming different destinations for the test packet. The tail flit is a constant. The TDG outputs the flit that is sent to the router connected directly to it. The Test Error Detector has a similar structure. TED waits for the router control signal that indicates the presence of incoming flits from the network. Once the first flit arrives, the TED reproduces the data generated by the TDG and compares each flit with the incoming flit. If the generated data differs from the incoming flit, an error is signaled. If the router control signal does not arrive within a certain time limit, a timeout signal is triggered, resulting in a timeout error indication. Similarly to the TDG, the test sequence identification, the numbers of strings of zeros and the number of cycles a flit takes to traverse the network are also TED inputs. TDG and TED are configured through the same scan chain. The TED has an additional scan chain to provide the ATE with information on the test result. The configuration bits build one chain, while a 2-bit error flag, two counters of erroneous flits and a 1-bit time-out flag build the other TED scan chain. To make it possible the diagnosis at wire level, two counters are available in each TED. They indicate the number of the flit being processed. Once the first mismatch between the received and the expected flit occurs, the first counter is stopped. The second counter will be stopped in case a second erroneous flit is received. At the end of the execution of each test cycle, the ATE can get access to those counters through a scan chain. Having the number of the erroneous flits, the information on the expected contents for the erroneous flits can be found. The wires involved in the fault will be those corresponding to the bits that were expected to carry a logic 1 (for wired-AND shorts). The test modules are included in each NI (connected to the routers) and activated in test mode. We assume in this work that the test structures are tested together with the network interface using a scan-based method, as mentioned. The test structures present an area of approximately 50% of the router area, when only the routers are considered, as shown in Table 6 (using Encounter RTL Compiler as the synthesis tool). However, the routers alone have a rather small contribution to the whole system area, whereas the cores and the connections are responsible for most of the chip occupation. Therefore, the area overhead of the test structures does not have a great impact at system level. The configuration of TDGs and TEDs is done through a scan chain so that the programmable data can be loaded from the ATE. Test structures are configured before each cycle defined in the test sequence. Test results are extracted from TEDs using a separate scan chain. The first two cycles use the complete testing sequence defined in Section 2. The last three cycles require less test vectors, as described in Section 4. Furthermore, cycles 3 to 5 will be executed only if necessary. Table 6. Comparison between the area of the test structures and the SoCIN router. Structure TDG Size  (# of gates) 341 TED (embedded test only) 401 TED (embedded diagnosis) 494 RASoC 1688 The time necessary to configure the test structures depends on the number of routers in the network and the number of cycles necessary for the test and diagnosis scheme.  Equations 1 and 2 show the number of cycles required to configure the test infrastructure for the execution of a single cycle.         TDG :       s + z1 + z2 + p + h1 (1)         TED :        s + z1 + z2 + p (2) In Equations (1) and (2), s is 1-bit wide and represents the test sequence to apply (walking-one for cycles 1 and 2; single all-one flit for cycles 3, 4 and 5). z1 and z2 represent the number of configurable zero flits sequence that consists in 7 bits each for an 8-bit wide NoC channel. p represents the number of cycles a flit takes to traverse the network that consists in 2 bits (three for cycles 1, 2 and 5; four for cycles 3 and 4). h1 represents the header that contains 10 bits. Since we are considering a 2x2 basic NoC topology, there are 4 TEDs and 4 TDGs that need to be configured before the test. Considering that the same chain feeds the TDG and TED configuration, then the configuration time is given by the sum of equations 1 and 2, multiplied by the number of routers (4), and corresponds to 176 clock cycles (((1+7+7+2+10)+(1+7+7+2))*4). Additionally,  Equation 3 shows the number of cycles required to extract the test results from an individual TED.         TED :            c1 + c2 + e1 + t1 (3) In Equation (3), c1 and c2 represent the erroneous flits counters that consist in 8 bits each for an 8-bit wide NoC channel. e1 represents the 2-bit error flag and  t1  the 1-bit  time-out flag. Again, we are considering a 2x2 basic NoC topology, then 4 TEDs are to provide test results. Considering a separate scan chain for the TED results, then the results extraction time is given by four times equation 3 and corresponds to 76 clock cycles ((8+8+2+1)*4) for the example. Performing the same time analysis as in [11] for the different test sequences and different test paths used in this work, we conclude that test cycles 1 and 2 will take 147 clock cycles each, test cycles 3 and 4 will last 27 clock cycles each, and test cycle 5 will take 23 clock cycles. Then, the total test time for the 2x2 basic NoC (t)  will be t=(176+147+76)*2=798 clock cycles if only cycles  1  and  2  are  needed,  it will  be t=798+(176+27+76)=1,077 clock cycles if the extra cycle  3  is  needed,  it  will  be t=1077+(176+27+76)=1,356 clock cycles if the extra cycle  4  is  required,  and  it will  be t=1,356+(176+23+76)=1,631 clock cycles if the extra cycle 5 is applied. For larger NoCs, the diagnosis method can be implemented by testing groups of 2x2 basic NoCs using test configurations devised according to the procedure presented in [11]. For example, Figure 5 shows the 16 test configurations necessary to cover all possible basic NoC topologies in a 5x5 network. In [11], in order to reduce the test application time, these test configurations are grouped in four test rounds ((a), (b), (c) and (d) in Figure 5), and every test round runs a number of test configurations in parallel. In this work, since we are dealing with fault diagnosis, we are naturally less concerned about the test application time. Therefore, to make it possible the detection and diagnosis of pairwise  shorts beyond  the 2x2 neighborhood, we now apply the test configurations individually, one after another. This means that, in total, the method in section 4 will be applied 16 times to cover all the test configurations of the example NoC. Then, reminding that unused channels are kept at logic level “0” in the SoCIN, for a short affecting any two wires of a larger than 2x2 NoC, three error detection/diagnosis situations are possible: - all failed test configurations indicate the same faulty channel: this denotes a short circuit inside that channel; - one failed configuration points  to one faulty channel, possibly another configuration points to another different faulty channel, and certainly one test configuration points to both previous channels as faulty: this denotes a short circuit involving those two channels (that do belong to the same 2x2 neighborhood); - some failed configurations  indicate one faulty channel, other test configurations indicate another different faulty channel: this denotes a short circuit involving those two channels (that do not belong to the same 2x2 neighborhood). As for the 2x2 basic topology, for larger NoCs it is also possible to precisely diagnose the faulty wires by combining the information on the faulty payload flits of the failed test configurations. In general, for a mxm NoC, m≥3, the number of test configurations (tc) will be given by: Tc = m2 – 2 * m + 1 (4) Then, considering that the ATE has direct access to each individual TDG-TED scan chain of every basic NoC, the total test time for the whole NoC will be: T =  tc * t (5) For the example NoC, the total test time will range from T=16*798=12,768 clock cycles in the best case, to T=16*1,631=26,096 clock cycles in the worst case. (a) (b) (c) (d) Figure  5. Test configurations for a 5x5 NoC. Finally, for wider channels (channels containing more than 8 bits) it is necessary to adapt the walking test sequence adding more zero flits so that there is always only one bit with logic level “1” being broadcasted through the NoC. 6. Experimental results In order to evaluate the proposed technique, a 2x2 SoCIN NoC and the proposed TDG-TED modules were implemented in VHDL, together with a fault injection mechanism capable of inserting AND-shorts between all pairs of data wires in the network. Normally,  the diagnosis algorithm should be executed by an external Automatic Test Equipment. In our experimental setup a testbench simulates the ATE. Diagnosis is done by analyzing the responses of the TEDs, firstly fitting them into the diagnosis table (Table 5) and then comparing the erroneous flits (number is given by TED counters) against the expected contents. As mentioned previously, we use the Walking-One sequence to define the test vectors and diagnosis cases at the wire level. A similar approach can be used to analyze any other sequence by adapting the TDGs and TEDs accordingly. The 8,128 possible shorts were injected in all 128 data wires of the 2x2 NoC topology. The simulation was performed using ModelSim tool, where internal signals can be manipulated using the command signal force placed at the testbench file. The entire simulation takes only a few minutes. Table 7 presents the results of the diagnosis for all the 8,128 possible pairwise shorts, since the test method is capable of detecting all injected faults. The table is divided into two groups: “diagnosed faults” and “non-diagnosed faults”. The “diagnosed faults” group is further divided into the cases diagnosed in the first two test cycles, and the cases that were diagnosed during the extra cycles. The faults are also divided into payload and timeout faults. From Table 7, one can observe that the diagnosis capability of the proposed test method is very effective (93%). For the unresolved cases, that represent 7.32% of the faults, some cases are related to two and other cases are related to three faulty channel pairs. 7. Conclusions and future works We proposed a new test scheme for the detection and diagnosis of interconnect short faults in NoC communication channels.  Starting from a previously defined test method we developed a detailed analysis on  the diagnosis capabilities of  functional  test sequences. Based on this analysis a new test application sequence was proposed and analyzed for a basic 2x2 mesh NoC. Experimental results have demonstrated the effectiveness of the new approach, which achieved a diagnosis ratio of about 93%. We have also shown that a set of configurations based on 2x2 NoCs can be devised to test and diagnose shorts in larger NoCs.  Current work includes the optimization of the way TDGs and TEDs are configured by the external ATE for reducing the total test application time. The evaluation of  the fault coverage for other  test sequences, and additional fault models, such as crosstalk faults, is also being considered. 8.  Acknowledgements This research is supported by the CNPq Brazilian Research Agency under contract number 478200/2008-0. 9.  "
HiRA - A methodology for deadlock free routing in hierarchical networks on chip.,"Complexity of designing large and complex NoCs can be reduced/managed by using the concept of hierarchical networks. In this paper, we propose a methodology for design of deadlock free routing algorithms for hierarchical networks, by combining routing algorithms of component subnets. Specifically, our methodology ensures reachability and deadlock freedom for the complete network if routing algorithms for subnets are deadlock free. We evaluate and compare the performance of hierarchical routing algorithms designed using our methodology with routing algorithms for corresponding flat networks. We show that hierarchical routing, combining best routing algorithm for each subnet, has a potential for providing better performance than using any single routing algorithm. This is observed for both synthetic as well as traffic from real applications. We also demonstrate, by measuring jitter in throughput, that hierarchical routing algorithms leads to smoother flow of network traffic. A router architecture that supports scalable table-based routing is briefly outlined.","HiRA: A Methodology for Deadlock Free   Routing in Hierarchical Networks on Chip  Rickard Holsmark, Shashi Kumar  Dept. of Electronics and   Computer Engineering  Jönköping University  Sweden  Maurizio Palesi   Dept. of Computer Science and  Telecommunications Engineering   University of Catania    Italy  Andres Mejia  Dept. of Computing Engineering  Technical University of Valencia   Spain   Abstract— Complexity of designing large and complex NoCs can  be reduced/managed by using the concept of hierarchical  networks. In this paper, we propose a methodology for design of  deadlock free routing algorithms for hierarchical networks, by  combining routing algorithms of component subnets. Specifically,  our methodology ensures reachability and deadlock freedom for  the complete network if routing algorithms for subnets are  deadlock free. We evaluate and compare the performance of  hierarchical routing algorithms designed using our methodology  with routing algorithms for corresponding flat networks. We  show that hierarchical routing, combining best routing algorithm  for each subnet, has a potential for providing better performance  than using any single routing algorithm. This is observed for both  synthetic as well as traffic from real applications. We also  demonstrate, by measuring jitter in throughput, that hierarchical  routing algorithms leads to smoother flow of network traffic. A  router architecture that supports scalable table-based routing is  briefly outlined.   Keywords: Networks on Chip, Hierarchical Networks, Deadlock  Free Routing, Connectivity   INTRODUCTION  I.  Scalability is considered to be an important, sometimes  essential, property for most data communication networks. This  is because the applications and usage of most networks evolve  with time. The scalability property helps to reduce the time to  extend the network for adding new resources and applications.  Scalability property of a network is generally reflected in many  aspects including topology, communication protocols, routing  algorithm etc. Internet is a famous example of a scalable  network.   Scalable architectures are preferred even for physically very  small networks called networks-on-chip (NoC). This is because  the number of cores which can be integrated on a chip is  growing exponentially. For example, Intel has recently  announced design and implementation of an 80-core NoC  design using scalable mesh topology. Embedded system  designers face the high cost of developing a complex chip.  Design effort can be significantly reduced by chip architectures  that can be used by many diverse applications requiring  different types and number of resources. Like any large and  complex system, concept of hierarchy will be useful for design,  analysis, testing, usage and management of NoCs with a large  number of cores.   978-1-4244-4143-3/09/$25.00 ©2009 IEEE  One can apply the concept of hierarchy in two different  ways in the NoC context. In the first way, it is a design tool for  partitioning complex systems. Resources in a large flat network  could be grouped in physical or logical regions or sub-networks  (subnets). At higher level of abstraction the network could be  treated as an interconnection of nodes, where each node is a  subnet.   The concept of hierarchy can also be applied for building a  large network by reusing and interconnecting many already  designed NoCs. This is illustrated in Figure 1, where a  hierarchical network N is formed by interconnecting four  individual networks. A very large network could even be  organized in more than two levels, as exemplified by network  C that already contains a sub-network which becomes a subsub-network in network N. This view raises the abstraction  level of reuse from resources (components) to subnets.  In this  case, networks at higher level consist of subnets as resources.  Figure 1.  Hierarchical network.  Such hierarchical NoCs are likely to be heterogeneous in  many different ways. Different subnets may have diverse  topologies and exhibit variability in type of resources. Different  subnets may also use different communication protocols and/or  routing algorithms. The operating clock speeds within these  subnets may also be different. Due to these dissimilarities,  design and analysis of hierarchical NoCs will raise many new  issues and pose many new challenges as compared to flat  networks. In this paper, we deal with one important issue for  large hierarchical networks, namely design of deadlock free            routing algorithms. We call router nodes through which subnets  are connected to each other as boundary nodes. A deadlock free  routing methodology is described by defining rules that  prevents packet deadlocks in a network of subnets.     A. Scalability and Routing Algorithms  The scalability property of a communication network is a  combination of several aspects.  For example, a network may  have a scalable topology like mesh, but can have a non-scalable  routing algorithm using a table based implementation [1] [2].  Current way to design deadlock free high performance  networks, is to view all nodes and links as part of one single  flat network. This view has the clear advantage that it provides  the designer with the possibility to organize the network such  that a certain objective function is optimized. Designing an  effective routing function for such a large heterogeneous  network consisting of hundreds or thousands of nodes, which  guarantees deadlock freedom and which makes it possible to  optimize several other objectives (e.g., routing adaptivity, link  load balancing, etc.) is a computationally challenging task. As  all commercial  systems are constrained by business  considerations; cost, quality and time to market are essential for  networks in these products. In many cases, reuse of subsystems has been applied to accomplish these goals. We feel  that reuse of networks will also become increasingly important.   The interesting issue of conflicting requirements for  different subnets in network design can lead to a situation  where a globally optimal solution is worse than a solution  which is a combination of local optimal solutions for each  subnet. Consider for example a network system consisting of  two identifiable parts, P1 and P2 such that it is found that the  optimal routing algorithm for the whole network is Odd-Even  [3]. However, the best performance for P2 individually, is  achieved by using West-First [4], but the requirements of  deadlock free routing restricts the number of routing algorithms  to just one. If it was possible to safely mix routing algorithms  in a system, higher performance could be achieved. This is the  guiding motivation for our work.  B. Hierarchically Designed Routing Algorithms  To  tackle  the problem of poorly scalable routing  algorithms, one idea could be to use a hierarchical approach as  follows. Locally, each sub-network is analyzed separately from  each other and the most suitable routing algorithm which  allows communication between nodes of each sub-network is  designed. Globally, a routing algorithm is designed for  communication between  the  subnets. However,  some  consideration must be taken into account when designing the  global routing algorithm. This is to avoid the situation that  when subnets are put together, the resulting global routing  function may not be deadlock free. The essence of designing a  deadlock free routing algorithm for a network of interconnected  subnets can be obtained by studying the example in Figure 2.  Figure 2 shows an interconnection of two simple subnets,  where each subnet provides deadlock free internal routing by  the use of bidirectional routing restrictions. But when interconnected, they are prone to deadlocks. The reason is that a  cycle is formed in the channel dependency graph (CDG) of the  combined network. If such cycles exist, freedom from  deadlocks cannot be guaranteed [5] [6]. The cycle is caused by  implicitly avoiding a routing restriction in the internal subnet  using links in another subnet. It is not possible to avoid the  cycle by setting restrictions on the grey “boundary nodes” that  are used for connecting the subnets to each other. If such a  restriction is set, the combined network is not fully connected.  For example, assume a restriction that prevents the cycle by  prohibiting the output of Subnet B to connect to the input of  Subnet A. Indeed, such restriction will remove all CDG cycles;  but it will also remove all communication possibilities from  Subnet B to Subnet A. The only way to achieve deadlock  freedom and maintaining connectivity is to modify the internal  routing algorithms such that the routing restrictions are moved.  Figure 2.  A  deadlock-prone network with deadlock free subnets.  This leads to our problem formulation of hierarchical  design of deadlock free routing algorithms: Given that a large  network is built by connecting many subnets where each subnet  provides deadlock free routing among its internal nodes. The  problem is to design a deadlock free global routing algorithm  that does not affect the internal subnet routing algorithms and  provides full connectivity. In this paper we propose general  solutions to the deadlock issues that arise when building a  network by connecting several individual deadlock free  networks (subnets). We call this methodology hierarchical  routing algorithms (HiRA).   II. RELATED WORK  The idea of hierarchy has been used to improve scalability  of both on-chip and off-chip buses. Proteo NoC architecture [7]  is one of the earliest hierarchical NoC proposals. The vision of  Proteo was that large networks will be heterogeneous and  constructed by connecting smaller subnets (referred as  clusters). Clusters should be connected using bi-directional ring  networks. The larger systems could be built using multiple  connected rings.    Two dimensional mesh and its numerous variations have  been the most popular NoC topologies. The importance of a  region concept that allows irregularities in a mesh topology is  discussed in [8] [9]. The stated purpose is to support varying  sizes of resources, design partitioning and reuse of previously  designed systems, including SoCs and NoCs. Bourduas et al.  [10] have proposed a hybrid ring/mesh interconnect topology to  remove limitations of large diameter of a large mesh topology    network. In their proposal, a large mesh is partitioned into  many small sub-meshes connected using hierarchical rings.  They have shown that the hierarchical network topology  reduces the average hop count as compared to a flat mesh  network.   In [11], a hybrid mesh-ring NoC topology is proposed that  is suitable for future 3D ICs. Jain et al. [12] have compared a  hybrid photonic network having hierarchical mesh topology  with a flat mesh network, for performance and energy  consumption. A hierarchical on-chip approach is also taken in  HiNoC [13], which offers both packet- and stream-based  communication services. In HiNoC, the network has two levels  of hierarchy; the asynchronously communicating mesh at the  top level and an optional synchronously operating fat tree  structure attached to a mesh router network node.  Hierarchy is  used in a NoC architecture based on CDMA techniques and  star topology presented in [14]. Other approaches which  emphasize the use of network hierarchies in the context of NoC  based design are GigaNoC [15]  and THIN [16].   Usually, hierarchical partitioning of networks refers to  managing/optimization of complex network structures by  splitting them into smaller sub-structures. According to our  search, there is no research work reporting explicit hierarchical  partitioning of a large network into non-homogeneous subnets,  such  that different subnets could use different routing  algorithms and the whole network is still deadlock free.   Although not allowing multiple routing algorithms in a  network, Mejia et al. [2] use hierarchical partitioning in their  SR (segment based routing) methodology.  III. TERMINOLOGY, DEFINITIONS AND BASIC THEOREM  The primary objective of HiRA is to provide deadlock free  communication and total connectivity among network nodes of  different subnets without modifying internal topologies and/or  routing algorithms. Deadlock  freedom  is ensured by  guaranteeing a cycle free channel dependency graph (CDG) for  the combined network. To achieve this, some rules regarding  the interconnect of subnets must be followed.   We propose the concept of a “Safe Boundary Node” for  this purpose. A node in a subnet with at least one port, used (or  intended to be used) for connecting the subnet to another  subnet is called a boundary node. A boundary node is safe if a  deadlock cannot occur and connectivity can be guaranteed  without modifying the internal routing algorithm, when it is  connected to other subnet(s). We will define the properties of  safe boundary nodes more formally later in the section. This  section will prove that a network of interconnected deadlock  free subnets will be deadlock free and connected, if the network  for inter-subnet communication is deadlock free and subnets  are connected to each other using only safe boundary nodes.   A. General Notation and Definitions related to Deadlock  Free Routing  Variants of the definitions in this section are extensively  used in literature on deadlock free routing and graph theory.  Definition 1  A network is modeled by a graph T=(N,L)  where each vertex n∈N is a network (router-) node and each  directed arc lij∈L, represent a unidirectional link connecting  node ni with node nj. Let out(n) / in(n) represent the set of  output links / input links of a node n respectively, and let src(l)  / dst(l) represent the source / destination nodes of a link l  respectively.  Definition 2  A path is a connected graph P=(V,U), where  the set of distinct vertices V={v1,v2,…,vk} are connected by a set  of distinct edges U={u12,u23,…,u(k-1)k}, where an edge uij  connects vertices vi and vj. Let p(v1,vk) indicate a path  connecting and including vertex v1 and vertex vk. A cycle is a  closed path C=p(v1,vk)+ uk1.  Definition 3  A routing algorithm R defines all allowed  message routes in a network. Each message route p(ns,nd)∈R is  a network path from a source node ns to a destination node nd.   Definition 4 [5] [6] A channel dependency graph is a graph  CDG=(L,D) where a vertex l∈L, is a link in T and each  directed arc dij∈D is a link dependency dij=(li,lj) such that lj is  pair (lin∈in(n),lout∈out(n)) is in some message route p(ns,nd)∈R,  allowed immediately after li by a routing algorithm R.   The routing algorithm R is implemented by switching  messages (packets) in network nodes. If a node n and a link  a packet is allowed to be switched from lin to lout. A routing  restriction between lin and lout indicates that a packet is not  allowed to be switched in node n from lin to lout, and  consequently there is no dependency d(in)(out)=(lin,lout) in the  CDG.   Note that the term routing algorithm is used in Definition 4.  This is to indicate that the CDG, as used by our methodology is  not limited to a specific routing function.   B. Notation and Definitions regarding Hierarchical Network  structure  This section defines topological elements of a hierarchical  network.  Figure 3.  Hierarchical network structure.  Ѕ={S1=(N1,L1), S2=(N2,L2),…,Sk=(Nk,Lk)}, and a set of external  links E={l∈T:l∉L(⋃Ѕ)}.  Definition 5  TH=(N,L), partitioned  A hierarchical network  into a set of disjoint subnets  is a network     Definition 6  The boundary nodes  in a hierarchical  network interconnect subnets via external links. Formally, the  set of boundary nodes are B={n∈N:∃ l∈E:src(l)∨dst(l)=n}.  As can be seen in Figure 3, all network nodes are in some  subnet. Some links, however are not in any subnet. These links  are the external links belonging to E that connects boundary  nodes of different subnet to each other. Note that all internal  subnet nodes, including boundary nodes, and links of a subnet  are exclusive to that subnet, since they are disjoint, i.e. ∀Si,Sj∈  Ѕ, Si∩Sj=∅ for i≠j.  C. Notation and Definitions on Hierarchical Deadlock Free  Routing  This section defines the components of a hierarchical  routing algorithm.   Definition 7 An internal routing algorithm Ri, defines the  allowed internal message routes in a subnet Si. A message  route is internal, p(ns,nd)∈Ri if both the source node ns and the  destination node nd are in the same subnet.   Any sub-path p(nx,ny) of an external message route p(ns,nd)  Definition 8 The external routing algorithm RG defines the  allowed external message routes in a hierarchical network. A  message route is external, p(ns,nd)∈RG if the source node ns and  the  destination  nd  are  in  different  subnets.   where all links are internal links of subnet Si is given by Ri.  Definition 9 The hierarchical routing algorithm RH defines all  allowed message routes in a hierarchical network. Any  message route p(ns,nd)∈RH is given, either by a subnet internal  routing algorithm Ri or the external routing algorithm RG.   A boundary node b∈B in a subnet Si is safe if  iff ∄p(lout,lin)⊆CDGi:  Definition 10  there does not exist any path p(lout, lin) from an internal output  link lout to an internal input link lin in the subnet internal CDG.  In  other words,  b  is  safe  lout∈out(b)∧lin∈in(b)  The purpose of defining a hierarchical routing algorithm RH  by two separate routing algorithms Ri and RG is to formalize the  main idea of this paper. Namely, that subnets with arbitrary  internal deadlock free routing algorithms can be interconnected  while maintaining deadlock freedom, without modifying the  internal subnet routing algorithm. Due to this, the internal  routing are independent of whether a subnet is used as a subnet  or a “normal” network. A key aspect of external routing  algorithm RG is that it has no power to affect internal message  routes. If such routes are necessary for reaching a destination  node, they are given by the internal routing algorithm.   The consequence of the definitions of Ri and RG is that RG  only can implement routing restrictions on boundary nodes.  That is, given a dependency dij=(li,lj) removed by a routing  restriction in RG, either li or lj or both must be an external link.  These external restrictions can result in that some external  routes from/to internal nodes are not reachable from all  boundary nodes.  The implementation of a hierarchical routing algorithm is  yet not defined. In the context of this paper it is assumed that  RH is the only implemented routing algorithm. Ri and RG  routing algorithms are then components in the design phase of  RH. Once the routes are given by each Ri and RG they are no  longer used, and the hierarchical network is treated as a  “normal” network. However, hierarchical routing approach  provides a natural way of using two-level addressing, by  implementation of Ri and RG instead of RH. This is briefly  discussed in section 6.   An external message route example is shown in Figure 4.  The first part of the route p(ns,nd)∈RG is the internal route  p(ns,bs)∈Ri from the source node ns to boundary node bs. Since  it is a sub-path of p(ns,nd)∈RG, it is defined by Ri. In boundary  node bs, RG routes to the boundary node bd. The internal route  p(bd,nd)∈Rj again, is a sub-path of the external route and is  routed by the internal routing algorithm Rj of subnet Sj.   Figure 4.  Internal and external routing algorithms.   A safe boundary node example is shown in Figure 5, which  in upper-left corner illustrates a subnet routed by the NegativeFirst routing algorithm [4].   Figure 5.  Safe boundary nodes.   Two grey shaded safe boundary nodes are connected to  external links. To the right is the CDG of the subnet and below  this is an illustration of safe node requirements of one boundary  node. If it is safe, there should be no internal path formed in the  CDG from any of its internal outputs to any of its internal  inputs. This is shown as abstract dependencies connecting the  internal links of the node. Checking all nodes for such paths  reveals that there are four possible safe boundary nodes in this  subnet. The other two nodes are not safe as boundary nodes.      D. Deadlock Freedom and Connectivity  Now we show that a hierarchical routing algorithm is  deadlock free and connected, i.e. ensures that each network  Theorem A hierarchical routing algorithm RH is deadlock free  node can reach any other node, for any hierarchical network  configuration. Note that 1-cycles are not allowed in CDGs.   if   i.  each subnet routing algorithm Ri is deadlock free, and  ii.  the external routing algorithm RG is deadlock free, and  iii.  all boundary nodes are safe  Proof:   ⇐ Consider an arbitrary subnet Si in a hierarchical network  TH. Internal links of this subnet Si may exist in Ri and/or RG. Let  CDGi be acyclic by Ri, and let CDGG be acyclic by RG using  routing restrictions on safe boundary nodes.   Since RG may define message routes that enter and leave  subnets, there can exist paths in CDGG from external input  links to external output links of boundary nodes. Therefore, in  addition to restrictions on external links, RG may define  restrictions for entering or exiting Si on a boundary node to  avoid cycles in CDGG. Note, as boundary nodes are safe, there  cannot be a CDGi path from any internal output to any internal  input of the same boundary node.  Assume a restriction for traversing a boundary node bs  using an external input lext_in and an internal output lint_out. If RG  is connected and bs is safe, there must be a CDGG path starting  in an output link of the internal neighbour node of bs,  dst(lint_out), through another boundary node in Si to the external  input lext_in of bs. If the restriction disconnects any node with  lext_in, RG is not connected. Similarly, if there is a restriction on  an internal input, lint_in and an external output lext_out of a  boundary node bs, there must be a path starting in lext_out  through CDGG and another boundary node in Si to an input link  of the internal neighbour src(lint_in) in Si. If there is a restriction  on an external input lext_in and external output lext_out there must  be a path from lext_out to the input of the neighbour src(lext_in) in  CDGG, if RG is connected.  Let RH be the routing algorithm that includes each subnet  routing algorithm Ri and the external routing algorithm RG.  Given that each internal CDGi is cycle free by Ri, an ordering  can be established among the links of CDGi such that the order  of a link lj, in a link dependency dij=(li,lj) is lower than the link  li. If there are no lower order links,  lj is with minimum index.  If CDGG of RG is cycle free, the ordering of external links and  outputs/inputs of a boundary node will be constantly  decreasing, where higher order links and minimal links are not  necessarily  in  the same subnet. As  there can be no  dependencies other than in CDGG and in each subnet CDGi,  CDGH is cycle free.   If RG and each Ri are connected, the destination node of any  packet on a minimal, lj must always be dst(l j). Then, dst(lj) will  sink (remove) any packet on lj within finite time. Let W ={ w1 ,  w2 , ..., wk } be any succession of k packets in TH, where, for  any two packets (wi, wi+1), i=1,(cid:2)2, …, k-1,  wi+1 is waiting on  wi . Since all packets in W are routed on links in a decreasing  order, wh in head of W is either proceeding unblocked (or is on  a sink) it will always advance. Then wh will eventually be sunk,  in any W will be delivered. Thus, RH is connected and deadlock  leaving wh+1, as new head of W. For each removed head wh,  each new head wh+i  ,i=1,(cid:2)2, …, k-h will be sunk. Then all k  packets in W are eventually ejected from the network. As W is  an arbitrary succession of waiting packets, any waiting packet  free.  (cid:0)  IV. HIERARCHICAL ROUTING: DESIGN METHODOLOGIES  This section presents basic guidelines that translate the  theory in Section 3 into practical design techniques. Two  scenarios are considered. In the first scenario a hierarchical  network is designed using subnets that were designed without  being aware that these could be part of a larger network. We  call these subnets as “unaware” subnets. The other scenario is  that a network is built using subnets, designed with the  awareness that they will be used to build hierarchical networks.  We call these subnets as “aware” subnets. In both these  scenarios it is important that the subnet should have some safe  boundary nodes.        A. Safe Boundary Nodes  A hierarchical deadlock free routing algorithm assumes that  nodes used for interconnecting subnets are safe. The basic  requirement on a safe node is that there does not exist a path of  link dependencies from an output link to an input link of such  node. Recall Figure 2 in Section 1 with subnets interconnected  by un-safe boundary nodes. Both subnets are individually  deadlock free but when interconnected, this property is lost for  the full network. In this case, link dependencies combine in  such a way that cycles are formed. The boundary nodes allows  a path of link dependencies from its internal output links to its  internal input links, which is not possible to remove by  restrictions on the boundary node links, while maintaining full  connectivity.    Figure 6 shows the two networks interconnected by safe  boundary nodes. Both networks are deadlock free and there are  no cycles in the channel dependency graph. Therefore, it is not  necessary to modify the internal routing algorithms to avoid  cycles that are caused only from the interconnection of the two  networks. This is the main intention of hierarchical routing;  interconnection of networks should not require modification of  the internal routing algorithms. Restrictions on boundary node  links should only be applied for communication among  different subnets. In Figure 6, such restriction is not even  necessary, as it should not be, since there are no possible CDG  cycles formed by the external communication links.  Note that use of safe boundary nodes is a sufficient  condition for hierarchical deadlock free routing. For example, it  is easy to see in Figure 6 that the CDG of the interconnected  subnets is cycle free even if only one of the boundary nodes is  safe. Consequently, unsafe boundary nodes can be used for  subnet interconnection. Still, as two unsafe nodes in many    cases are enough to produce cyclic CDGs, the possibilities for  use of unsafe nodes are limited and each configuration require  careful analysis.   Figure 6.  Deadlock free interconnection of subnets.  1) Testing “safeness” of  Boundary Nodes   The safeness of a boundary node can be easily tested by  using termination as shown in Figure 7. Both the upper and  lower version of the particular four-node network is deadlock  free as guaranteed by an acyclic channel dependency graph, but  the position of a routing restriction is different. Termination of  the grey shaded boundary node creates a dependency between  the external output and input links. If, considering the  termination, the subnet CDG is still cycle free, the boundary  node is safe. Otherwise, if cycles are found, it is not safe. As  shown in Figure 7, the lower version CDG of the network is  not cycle free and the boundary node is not safe.  bidirectional routing restrictions, like for example NegativeFirst [4], Up/Down [17] and SR [2]. The possible safe  boundary nodes in a network with an adaptive routing  algorithm are fewer than the nodes in the network as a result of  more available paths between nodes. We briefly describe an  approach to quickly detect unsafe nodes in such networks.   The purpose of using safe nodes is to prevent implicitly  violating an existing restriction and enabling a CDG cycle.  Therefore a strategy to find safe nodes is to first find the  routing restrictions. Consider a node n with a bidirectional  restriction in a network. The purpose of the restriction is to  prevent one or several CDG cycles to be formed by a particular  set of links. But, it is possible that a path dependency from  other input to output links of n, not affected by the restriction,  still remains even if the cycle is broken. Normally, this is not a  problem since a cycle including this path will be detected and  restricted at some other node. However, a safe boundary node  is not allowed to reach such dependency, as it can result in a  CDG cycle with another subnet. Therefore, to quickly find  unsafe nodes, follow all allowed routes from the restricted  input links at the restricted node n. The nodes visited in this  path are not safe.   B. A Simple Example of Combining Subnets  The following simple example illustrates the possibilities  with hierarchical routing algorithms. Consider Figure 8 and an  intention to interconnect subnets with routing algorithms from  left to right, Y-X, X-Y, Y-X, such that each subnet routing  algorithm is kept unchanged.   Figure 7.  Termination of boundary node.  2) Finding Unsafe Boundary Nodes  In a large network consisting of hundreds of nodes, finding  safe boundary nodes can be both complex and time consuming.  In the following some guidelines to simplify this task is  proposed.   Consider networks with deterministic routing algorithms,  like for example, X-Y. Such networks allow only a single path  between any source-destination pair. This automatically  prevents the possibility to create a CDG path from an output  link of any node to an input link of the same node. Thus, all  nodes in such networks are safe boundary nodes. Next consider  networks with partially adaptive routing algorithms with  Figure 8.  Interconnection of X-Y and Y-X  networks.   Generally, creating a network routing algorithm by  combining Y-X and X-Y routing is to beg for deadlocks. If the  networks were just connected horizontally by some extra links  there would be a cyclic CDG. For example, as indicated by the  shaded cycle, a packet turning  X-Y (west-north), waits for  another taking a Y-X (north-east), which waits for a packet  turning X-Y (east-south), that is blocked by a Y-X (south-west)  turning packet that waits for the first packet turning  X-Y  (west-north). A hierarchical approach could solve this by first  considering the networks as subnets. Fortunately, all possible  nodes are safe boundary nodes. In addition, since it is        deterministic routing, we know that there is only one path  between the boundary nodes in each subnet. Then we decide  that X-Y, which in this case is the same routing algorithm as  the middle subnet, is defining the external routing algorithm.  This means that whenever a packet comes in and out of a  subnet it is routed by X-Y. Note that even though internal  subnet links are used by the external routing algorithm X-Y, the  internal routing of the Y-X subnets are still Y-X.  V. PERFORMANCE EVALUATIONS AND RESULTS   In this section we compare a set of general purpose routing  algorithms (X-Y; Negative-First, North-Last, West-First [4]  and Odd-Even [3]) and some application specific routing  algorithms (computed using APSRA [1]) that have been  configured to see the network as a “flat network” against a  hierarchical  routing algorithm computed using HiRA  methodology. Note that all configurations are hierarchical in  the sense of global interconnection, as other routing algorithms  are not usable in the described configurations. A hierarchical  routing algorithm governs the entire network and the routing  algorithms within the subnets, have been configured to exploit  the intrinsic hierarchy of the subnets (SN). Evaluations were  performed using a publically available cycle accurate NoC  simulator, Noxim [18]. We consider wormhole switching with  a packet size randomly distributed between 2 and 8 flits, and  routers with input buffer sizes of 4 flits. We use the source  packet injection rate (pir) as load parameter.  We consider four different groups of communication flows.  SN->SN refers to communication flows with source and  destination located within the same subnet. SN->EN are  communication flows with source node located within a SN  and destination node located outside the subnet, or what is the  same, in the external network (EN). The group EN->SN is  communication flows with source node located outside the  subnet (in EN) and destination node located within the SN in  question. Finally the pass-through traffic (EN->SN->EN) are  communications that have both source and destination nodes  located in EN but there exists at least one routing path from  source to destination that passes through the SN in question.  A. Analysis of the Impact of Global Traffic to Local Traffic  We define as global traffic, those communication flows that  involve at least two SNs (subnets). First we evaluate how  global traffic affects the local traffic within a subnet. To do  this, consider the network topology shown in Figure 9, which is  partitioned into four subnets (A, B, C, D) interconnected  through boundary nodes. We focus the analysis on the subnet  labeled with A, located at the top-left corner of the topology.   This subnet has two boundary nodes located in nodes 11  and 25. We assume that local traffic (SN->SN) within A has a  Transpose pattern behavior with variable packet injection rate  (pir). The global traffic, on the other hand, is set to a pir equal  to 0.16 and consists of three kinds of communications.   • Three SN->EN communication flows in which source  node is randomly selected from SN A and destination  node is randomly selected from the set {14, 49, 54}.  • Three EN->SN communication flows where source  node is randomly selected from the set {14, 49, 54}  and destination node is randomly selected from SN A.  • Two EN->SN->EN communication flows from node  14 to node 49 and from node 49 to node 14.  Figure 9.  Network topology.  Figure 10 shows the average delay of local communications  measured in A for different pir. Two different routing  algorithms (X-Y and Odd-Even) have been used in A.  Measurements are taken with and without global traffic.  Figure 10.  Average delay for local communications under transpose traffic for  X-Y and Odd-Even with and without global traffic.  As can be observed, without global traffic, the differences  in performance between X-Y and Odd-Even are very  pronounced. However, in the presence of global traffic, the  difference between the two algorithms reduces. This effect can  be explained by observing that as soon as global traffic load  increases, the local traffic deviates from pure Transpose to a  mix of transpose with random traffic. As X-Y is known to  perform better than Odd-Even for random traffic, this shift in  traffic type has a higher impact on Odd-Even.   B.   Effectiveness of Hierarchical Routing  In this experiment we consider the topology and local  traffic combinations shown in Figure 11. It consists of four    subnets (A, B, C and D) with two boundary nodes each  (represented as gray tiles). Different traffic patterns are  assigned to each different subnet; A uses Uniform, B presents  Shuffle, C observes Transpose 1, and finally D sees a Butterfly  pattern. The local pir used in each of them is such that the SN is  below the saturation range according to the routing algorithm  considered. We compare the following routing algorithms: XY, Odd-Even, West-First, and Hierarchical. The latter is a  heterogeneous routing algorithm which uses  X-Y routing in  subnet A, Odd-Even in subnet B, West-First in subnet D, and  Odd-Even in subnet C. Figure 12 shows the average delay  under different global traffic loads when different routing  algorithms are used.  Figure 11.  Network topology and traffic patterns considered in each subnet.  As a metric for the global traffic load we use the ratio  between the global pir and the local pir. Measurements were  taken by fixing the local traffic pir to a level below the  saturation point, and varying the global traffic pir. As expected,  as soon as the global to local ratio increases, overall  performance of the network (measured in terms of average  communication delay) decreases.  0% 2% 4% 9% 13% 17% 22% 0 10 20 30 40 50 60 70 80 90 100 XY Odd -Even We s t-Firs t H i e ra rch ica l Glo ba l to L oca l ra tio A e v r e g a e d l y a ( s e c y c l ) Figure 12.  Average delay under different global to local ratios for different  routing algorithms.  It  is  interesting  to observe  that such decrease  in  performance is much more pronounced in X-Y, Odd-Even, and  West-First as compared to Hierarchical. This trend is analyzed  in Figure 13 which shows the increase in percentage of the  delay when the global to local ratio increases. As can be  observed,  variation.  the hierarchical routing exhibits  the smallest  0% 2% 4% 9% 13% 17% 22% 0% 20% 40% 60% 80% 100% 120% 140% 160% 180% XY Od d -Eve n We s t-Firs t H i e ra rch ica l Gl ob a l to L oca l ra tio D e l y a i c n r e s a e m n e t Figure 13.  Increase in delay for different routing algorithms for different  global to local ratios.  Finally, Figure 14 shows the percentage of performance  improvement  (i.e., percent average delay decrease) of  hierarchical routing as compared to X-Y, Odd-Even, and WestFirst. On average, the hierarchical solution is over 50%, 20%,  and 38% more efficient (in terms of average delay) than the  network configured with X-Y, Odd-Even and West-First  respectively.  0% 2% 4% 9% 13% 17% 22% A v erage 0% 10% 20% 30% 40% 50% 60% 70% vs . XY vs . Odd -Even vs . We s t-Fi rs t Globa l to Lo ca l ra ti o P e f r o r m e c n a i m p r e v o m n e t o f H i e r a r h c i a c l Figure 14.  Percent performance improvement of hierarchical routing vs. X-Y,  Odd-Even and West-First.  C. A Real Case Study  We also used a real case study to evaluate the benefits of  hierarchical routing. Four applications were mapped on a 64  node NoC.  1. MMS: A generic MultiMedia System which includes  an H.263 video encoder, an H.263 video decoder, an  mp3 audio encoder and an mp3 audio decoder [19].   2. MIMO-OFDM: A MIMO-OFDM receiver in which,  to support the maximum data rate of WWiSE (WorldWide Spectrum Efficiency) proposal for the nextgeneration wireless LAN systems, some of IPs have  been parallelized to multiple IPs [20].   3. PIP and MWD: A Picture-In-Picture application and a  Multi-Window Display application [21] [22].                       Network topology along with the mapping of applications  in shown in Figure 15. We refer to [19] [20] [21] [22] for  details about the communication topology and the bandwidth  requirements of the different applications. We evaluate the  HiRA solution against both general purpose and application  specific routing algorithms implemented for a flat network  topology. For the hierarchical case, APSRA is used in the  subnets where MMS-Enc and MWD are mapped, X-Y is used  in subnets where MIMO-OFDM and PIP are mapped, and  Negative-First is used in MMS-Dec subnet. In the other cases,  the same routing algorithm is used over the entire network.  bandwidth constraints. Once again, the proposed hierarchical  routing approach outperforms the other routing algorithms. On  average, using hierarchical  routing, only ~10% of  communications violate  their bandwidth constraints as  compared to over ~20% when X-Y, Negative-First, North-Last  and West-First are used and over ~18% when Odd-Even and  APSRA are used.  s t i n a r t s n o c W B e a o t l i v h c h i w . s m m o c f o % 60% 50% 40% 30% 20% 10% 0% MMS-Enc MMS-Dec PIP MWD MIMO-OF DM NegativeFirst WestFirst APSRA XY NorthLast OddEven Hierarchical Figure 15.  Network topology and mapping of applications on the NoC.  Maximum link bandwidth for the NoC is assumed to be 800  MB/s. As performance metric we use the throughput jitter  which is defined as the variance in throughput during the  simulation period. This parameter gives indication about how  smooth the flow of traffic is in the network. Figure 16 shows  the normalized throughput jitter for each subnet when a certain  routing algorithm is used.   1.80 1.60 1.40 1.20 1.00 0.80 0.60 0.40 0.20 0.00 r e t t i J t u p h g u o r h T d e z i l a m r o N MM S -Enc MM S -Dec P IP MWD M IMO-OFDM NegativeFirst WestFi rst APSRA XY NorthLast OddEven Hierarchical Figure 16.  Normalized throughput jitter for each subnet and routing  algorithm.  The values are normalized with respect to the lowest  throughput jitter in that subnet. As it can be observed,  hierarchical routing outperforms the other routing algorithms in  each subnet. Finally, we analyze effectiveness of different  routing algorithms in meeting bandwidth constraints of the  applications mapped on the NoC. A communication is said to  violate the link bandwidth constraints if a link used by it  exceeds the maximum link bandwidth during a simulation.  Figure 17 shows for each routing algorithm and for each subnet  the percentage of communications that do not satisfy the link  Figure 17.  Percentage of communications which do not meet bandwidth  constraints.  VI. SCALABLE ROUTER ARCHITECTURE   Routing  in  irregular  topologies, for example using  Up/Down [17], is usually table-based, where sizes of the tables  are dependent on the size of the network. Compression  techniques [1] can reduce this problem to some extent, but in  general table-based routing is not scalable.   The hierarchical deadlock free routing methodology does  not imply a new router design with special features. It is  possible to re-map the router address space to a flat network  and use standard router design, since it is only the routes that  are the important output of hierarchical deadlock-free routing.  We outline logical extensions to a router design that would  make routing in hierarchical networks more scalable. These  ideas may also be useful for increasing scalability of other  table-based routing algorithms.   Two of the most important aspects of a hierarchical  network organization are the issues of addressing network  nodes and implementation of routers. In this context a  hierarchical approach to node addressing and routing will be  logical and scalable. For example, in a two level hierarchical  network the node address could be specified using two fields.  The first field of the address identifies the subnet to which the  node belongs and the second field specifies the position of the  node within the subnet. Since a hierarchical network must be  scalable, future expansion of the network must be kept in mind  while deciding the size of the first field. Due to the  heterogeneity of connected subnets, the size of the second field  is either chosen such that it can handle the largest possible  subnet, or a variable size is used dependent on the size and  topology of subnet specified in the first field.   Routers for hierarchical networks pose interesting design  challenges. A natural way to implement the routing function in  a two-level hierarchical network is by using two tables. The  first table helps in finding admissible boundary ports available  for reaching the destination subnet. The second table is used for                        routing within the subnet. In both cases, if multiple routing  choices are available, then the global or local network traffic  information may be used for making a good path selection. It is  easy to see that the first table can be by-passed for local  communication. It is obvious that the hierarchical table based  implementation will add some area as well as power overheads.  It will also slightly slow down the computation of a routing  decision. We are currently working on the design of a router for  hierarchical  networks  that will  complement HiRA  methodology, with the aim to quantitatively evaluate the  impact of hierarchical table based implementation on both  delay, area, and power of the router.  VII. CONCLUSIONS  Like in many other contexts, concept of hierarchy has a  potential to provide many advantages but also challenges while  designing, analyzing and using large and complex NoCs.  Among the advantages are the possibilities for reuse of earlier  designed smaller NoCs to build a large heterogeneous NoC and  the possibility of virtualization of a large network as a set of  many small networks for the purpose of running concurrent  applications. In this paper we looked at one important issue,  namely, design of deadlock free routing algorithms for  hierarchical NoCs.   We described a method to design a deadlock free routing  algorithm for a large network which is an interconnection of  many subnets, such that each subnet has an individual deadlock  free routing algorithm for  internal communication among its  nodes. We have modeled and evaluated hierarchical NoCs and  showed that hierarchical organization has potential of providing  better latency and throughput performance as compared to a  large flat network for both synthetic traffic and traffic  generated by a set of concurrently running real applications.  We also showed that the traffic flow in a hierarchical network  is smoother and has lower latency and throughput variance as  compared to large flat networks. We also propose changes in a  table based router design to handle hierarchical routing.  "
Silicon-photonic clos networks for global on-chip communication.,"Future many core processors will require energy-efficient, high-throughput on-chip networks. Silicon-photonics is a promising new interconnect technology which offers lower power, higher bandwidth density, and shorter latencies than electrical interconnects. In this paper we explore using photonics to implement low-diameter non-blocking crossbar and Clos networks. We use analytical modeling to show that a 64-tile photonic Clos network consumes significantly less optical power, thermal tuning power, and area compared to global photonic crossbars over a range of photonic device parameters. Compared to various electrical on-chip networks, our simulation results indicate that a photonic Clos network can provide more uniform latency and throughput across a range of traffic patterns while consuming less power. These properties will help simplify parallel programming by allowing the programmer to ignore network topology during optimization.","Silicon-Photonic Clos Networks for Global On-Chip Communication Ajay Joshi*, Christopher Batten*, Yong-Jin Kwon†, Scott Beamer†, Imran Shamim* Krste Asanovi ´c†, Vladimir Stojanovi ´c* * Department of EECS, Massachusetts Institute of Technology, Cambridge, MA † Department of EECS, University of California, Berkeley, CA Abstract Future manycore processors will require energyefﬁcient, high-throughput on-chip networks. Siliconphotonics is a promising new interconnect technology which offers lower power, higher bandwidth density, and shorter latencies than electrical interconnects. In this paper we explore using photonics to implement lowdiameter non-blocking crossbar and Clos networks. We use analytical modeling to show that a 64-tile photonic Clos network consumes signiﬁcantly less optical power, thermal tuning power, and area compared to global photonic crossbars over a range of photonic device parameters. Compared to various electrical on-chip networks, our simulation results indicate that a photonic Clos network can provide more uniform latency and throughput across a range of trafﬁc patterns while consuming less power. These properties will help simplify parallel programming by allowing the programmer to ignore network topology during optimization. 1. Introduction Today’s graphics, network, embedded and server processors already contain many processor cores on one chip and this number is expected to increase with future scaling. The on-chip communication network is becoming a critical component, affecting not only performance and power consumption, but also programmer productivity. From a software perspective, an ideal network would have uniformly low latency and uniformly high bandwidth. The electrical on-chip networks used in today’s multicore systems (e.g., crossbars [8], meshes [3], and rings [11]) will either be difﬁcult to scale to higher core counts with reasonable power and area overheads or introduce significant bandwidth and latency non-uniformities. In this paper we explore the use of silicon-photonic technology to build on-chip networks that scale well, and provide uniformly low latency and uniformly high bandwidth. Various photonic materials and integration approaches have been proposed to enable efﬁcient global on-chip communication, and several network architectures (e.g., crossbars [7, 15] and meshes [13]) have been developed bottom-up using ﬁxed device technology parameters as 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  drivers. In this paper, we take a top-down approach by driving the photonic device requirements based on the projected network and system needs. This allows quick design-space exploration at the network level, and provides insight into which network topologies can best harness the advantages of photonics at different stages of the technology roadmap. This paper begins by identifying our target system and brieﬂy reviewing the electrical on-chip networks which will serve as a baseline for our photonic network proposals. We then use analytical models to investigate the tradeoffs between various implementations of global photonic crossbars found in the literature and our own implementations of photonic Clos networks. We also use simulations to compare the photonic Clos network to electrical mesh and Clos networks. Our results show that photonic Clos networks consume signiﬁcantly less optical laser power, thermal tuning power, and area as compared to photonic crossbar networks, and offer better energyefﬁciency than electrical networks while providing more uniform performance across various trafﬁc patterns. 2. Target System Silicon-photonic technology for on-chip communication is still in its formative stages, but with recent technology advances we project that photonics might be viable in the late 2010’s. This makes the 22 nm node a reasonable target process technology for our work. By then it will be possible to integrate hundreds of cores onto a single die. To simplify design and veriﬁcation complexity, these cores and/or memory will most likely be clustered into tiles which are then replicated across the chip and interconnected with a well-structured on-chip network. The exact nature of the tiles and the inter-tile communication paradigm are still active areas of research. The tiles might be homogeneous with each tile including both some number of cores and a slice of the on-chip memory, or the tiles might be heterogeneous with a mix of compute and memory tiles. The global on-chip network might be used to implement shared memory, message passing, or both. Regardless of their exact conﬁguration, however, all future systems will require some form of on-chip network which provides low-latency and high-throughput commu  (a) Crossbar (b) Mesh (c) CMesh (d) Clos Figure 1: Logical View of 64 Tile Network Topologies – (a) 64x64 distributed tristate global crossbar, (b) 2D 8x8 mesh, (c) concentrated mesh (cmesh) with 4x concentration, (d) 8-ary, 3-stage Clos network with eight middle routers. In all four ﬁgures: squares = tiles, dots = routers, triangles = tristate buffers. In (b) and (c) inter-dot lines = two opposite direction channels. In (a) and (d) inter-dot lines = uni-directional channels. Figure 2: Clos Layout – Router group is three routers. Only a subset of the channels are shown. nication at low energy and small area. For this paper we assume a target system with 64 square tiles operating at 5 GHz on a 400 mm2 chip. Figure 1 illustrates some of the topologies available for implementing on-chip networks. They range from highradix, low-diameter crossbar networks to low-radix, highdiameter mesh networks. We examine networks sized for low (LTBw), medium (MTBw), and high (HTBw) bandwidth which correspond to ideal throughputs of 64, 128, and 256 b/cycle per tile under uniform random trafﬁc. Although we primarily focus on a single on-chip network, our exploration approach is also applicable to future systems with multiple physical networks. 3. Electrical On-Chip Networks In this section, we explore the qualitative trade-offs between various network architectures that use traditional electrical interconnect. This will provide an electrical baseline for comparison, and also yield insight into the best way to leverage silicon photonics. 3.1. Electrical Technology The performance and cost of on-chip networks depend heavily on various technology parameters. For this work we use the 22 nm predictive technology models [16] and interconnect projections from [6] and the ITRS. All of our inter-router channels are implemented in semi-global metal layers with standard repeated wires. For medium length wires (2–3 mm or approximately the width of a tile) the repeater sizing and spacing are chosen so as to minimize the energy for the target cycle-time. Longer wires are energy optimized as well as pipelined to maintain throughput. The average energy to transmit a bit transition over a distance of 2.5 mm in 200 ps is roughly 160 fJ, while the ﬁxed link cost due to leakage and clocking is ≈20 fJ per cycle. The wire pitch is only 500 nm, which means that ten thousand wires can be supported across the bisection of our target chip even with extra space for power distribution and vias. Given the abundance of on-chip wiring resources, interconnect power dissipation will likely be a more serious constraint than bisection bandwidth for most network topologies. We assume a relatively simple router microarchitecture which includes input queues, round-robin arbitration, a distributed tristate crossbar, and output buffers. The routers in our multihop networks have similar radices, so we ﬁx the router latency to be two cycles. For a 5×5 router with 128 b ﬂits of uniformly random data, we estimate the energy to be 16 pJ/ﬂit. Notice that sending a 128 b ﬂit across a 2.5 mm channel consumes roughly 13 pJ, which is comparable to the energy required to move this ﬂit through a simple router. Future on-chip network designs must therefore carefully consider both channel and router energy, and to a lesser extent area. 3.2. Electrical On-chip Networks Figure 1 illustrates four topologies that we will be discussing in this section and throughout the paper: global crossbars, two-dimensional meshes, concentrated meshes, and Clos networks. Table 1 shows some key parameters for these topologies assuming a MTBw system. For systems with few tiles, a simple global crossbar is one of the most efﬁcient network topologies and presents a simple performance model to software [8]. Such crossbars are strictly non-blocking; as long as an output is not oversubscribed every input can send messages to its desired output without contention. Small crossbars can have very low-latency and high-throughput but are difﬁcult to scale to tens or hundreds of tiles. Figure 1a illustrates a 64×64 crossbar network implemented with distributed tristate buses. Although such a network provides strictly non-blocking connectivity, it also requires a large number of global buses across the length of the chip. These buses are challenging to layout and must be pipelined for good throughput. Global arbitration can add signiﬁcant latency and also needs to be pipelined. These global control and data wires result in signiﬁcant power consumption even for communication Topology Crossbar Mesh CMesh Clos NC ∗64 224 48 128 NBC NBC · bC NR Channels bC ∗ 128 256 512 128 ∗64 16 8 64 8,192 4,096 4,096 8,192 Routers radix 64x64 5x5 8x8 8x8 1 64 16 24 H 1 2-15 1-7 3 TR 10 2 2 2 TT C Latency TC n/a 1 2 2-10 0 0 0 0-1 TS 4 2 1 4 T0 14 7-46 3-25 14-32 Table 1: Example MTBw Network Conﬁgurations – Networks sized to support 128 b/cycle per tile under uniform random trafﬁc. Nc = number of channels, bC = bits/channel, NBC = number of bisection channels, NR = number of routers, H = number of routers along data paths, TR = router latency, TC = channel latency, TT C = latency from tile to ﬁrst router, TS = serialization latency, T0 = zero load latency. ∗Crossbar “channels” are the shared crossbar buses. between neighboring tiles. Thus global electrical crossbars are unlikely choices for future manycore on-chip networks, despite the fact that they might be the easiest to program. Two-dimensional mesh networks (Figure 1b) are popular in systems with more tiles due to their simplicity in terms of design, wire routing, and decentralized ﬂowcontrol [3, 14]. Unfortunately, high hop counts result in long latencies and signiﬁcant energy consumption in both routers and channels. Because network latency and throughput are critically dependent on application mapping, low-dimensional mesh networks also impact programmer productivity by requiring careful optimization of task and data placement. Moving from low-dimensional to high-dimensional mesh networks (e.g., 4-ary 3-cubes) reduces the network diameter, but requires long channels when mapped to a planar substrate. Also, higher-radix routers are required, resulting in more area and higher router energy. Instead of adding network dimensions, researchers have proposed using concentration to help reduce hop count [1]. Figure 1c illustrates a two-dimensional mesh with a concentration factor of four (cmesh). One of the disadvantages of cmesh topologies is that, for the same theoretical throughput, channels are wider than an equivalent mesh topology as shown in Table 1. One option to improve channel utilization for shorter messages is to divide resources among multiple parallel cmesh networks with narrower channels. The cmesh topology should achieve similar throughput as a standard mesh with half the latency at the cost of longer channels and higher-radix routers. CMesh topologies still require careful application mappings for good performance. Clos networks offer an interesting intermediate point between the high-radix, low-diameter crossbar topology and the low-radix, high-diameter mesh topology [4]. Figure 1d illustrates an 8-ary 3-stage Clos topology which reduces the hop count but requires longer point-to-point channels. Figure 2 shows one possible layout of this topology. Clos networks use many small routers and extensive path diversity. Although the speciﬁc Clos network shown here is reconﬁgurably non-blocking instead of strictly non-blocking, we can still minimize congestion with an appropriate routing algorithm (assuming the outputs are not oversubscribed). Unfortunately, Clos networks still require global point-to-point channels and, as with a crossbar, these global channels can be difﬁcult to layout and have signiﬁcant energy cost. 4. Photonic On-Chip Networks Silicon photonics is a promising new technology which offers lower power, higher bandwidth density, and shorter latencies than electrical interconnects. Photonics is particularly effective for global interconnects and thus has the potential to enable scalable low-diameter on-chip networks, which should ease manycore parallel programming. In this section, we ﬁrst introduce the underlying photonic technology before discussing the cost of implementing some of the global photonic crossbars found in the literature. We then introduce our own approach to implementing a photonic Clos network, and compare its cost to photonic crossbars. 4.1. Photonic Technology Figure 3 illustrates the various components in a typical wavelength-division multiplexed (WDM) photonic link used for on-chip communication. Light from an off-chip two-wavelength (λ1 , λ2 ) laser source is carried by an optical ﬁber and then coupled into an on-chip waveguide. The waveguide carries the light past a series of transmitters, each using a resonant ring modulator to imprint the data on the corresponding wavelength. Modulated light continues through the waveguide to the other side of the chip where each of the two receivers use a tuned resonant ring ﬁlter to “drop” the corresponding wavelength from the waveguide into a local photodetector. The photodetector turns absorbed light into current, which is sensed by the electrical receiver. Both 3D and monolithic integration approaches have been proposed in the past few years to implement silicon-photonic on-chip networks. With 3D integration, a separate specialized die or layer is used for photonic devices. Devices can be implemented in monocrystalline silicon-on-insulator (SoI) dies with Design Aggressive Conservative Modulator and Driver Circuits DDE FE TTE 20 fJ/bt 5 fJ/bt 16 fJ/bt/heater 80 fJ/bt 10 fJ/bt 32 fJ/bt/heater Receiver Circuits FE TTE 5 fJ/bt 16 fJ/bt/heater 20 fJ/bt 32 fJ/bt/heater DDE 20 fJ/bt 40 fJ/bt ELP 3.3 W 33 W Table 2: Aggressive and Conservative Energy and Power Projections for Photonic Devices – fJ/bt = average energy per bittime, DDE = Data-trafﬁc dependent energy, FE = Fixed energy (clock, leakage), TTE = Thermal tuning energy (20K temperature range), ELP = Electrical laser power budget (30% laser efﬁciency). Photonic device Optical Loss (dB) Optical Fiber (per cm) 0.5e-5 Coupler 1 Splitter 0.2 Non-linearity (at 30 mW) 1 Modulator Insertion 0 – 1 Waveguide (per cm) 0 – 5 Waveguide crossing 0.05 Filter through 1e-4 – 1e-2 Filter drop 1.5 Photodetector 0.1 Table 3: Optical Loss Ranges per Component have to be thermally tuned to maintain their resonance under on-die temperature variations. Monolithic integration gives the most optimistic ring heating efﬁciency of all approaches (due to in-plane heaters and air-undercut), estimated at 1 µW per ring per K. Based on our analysis of various photonic technologies and integration approaches, we make the following assumptions. With double-ring ﬁlters and a 4 THz free-spectral range, up to 128 wavelengths modulated at 10 Gb/s can be placed on each waveguide (64 in each direction, interleaved to alleviate ﬁlter roll-off requirements and crosstalk). A non-linearity limit of 30 mW at 1 dB loss is assumed for the waveguides. The waveguides are single mode and a pitch of 4 µm minimizes the crosstalk between neighboring waveguides. The ring diameters are ≈10 µm. The latency of a global photonic link is assumed to be 3 cycles (1 cycle in ﬂight and 1 cycle each for E/O and O/E conversion). For monolithic integration we assume a 5 µm separation between the photonic and electrical devices to maintain signal integrity, while for 3D integration the photonic devices are designed on a separate specialized layer. Table 2 shows our assumptions for the photonic link energy and electrical laser power. 4.2. Photonic Global Crossbar Networks A global crossbar provides non-blocking all-to-all communication between its inputs and outputs in a single stage. Figure 4 shows two approaches for implementing a 4×4 photonic crossbar. Both schemes have multiple single-wavelength photonic channels carried on Figure 3: Photonic Components – Two point-to-point photonic links implemented with WDM. thick layer of buried oxide (BOX) [5], or in a separate layer of silicon nitride (SiN) deposited on top of the metal stack [2]. In this separate die or layer, customized processing steps can be used to optimize device performance. However, this customized processing approach increases the number of processing steps and hence manufacturing cost. In addition, the circuits required to interface the two chips can consume signiﬁcant area and power. With monolithic integration, photonic devices are designed using the existing process layers of a standard logic process. The photonic devices can be implemented in polysilicon on top of the shallow-trench isolation in a standard bulk CMOS process [9] or in monocrystalline silicon with advanced thin BOX SoI. Although monolithic integration may require some post-processing, its manufacturing cost can be lower than 3D integration. Monolithic integration decreases the area and energy required to interface electrical and photonic devices, but it requires active area for waveguides and other photonic devices. Irrespective of the chosen integration methodology, WDM optical links have many similar optical loss components (see Table 3). Optical loss affects system design, as it sets the required optical laser power and correspondingly the electrical laser power (at a roughly 30% conversion efﬁciency). Along the optical critical path, some losses such as coupler loss, non-linearity, photodetector loss, and ﬁlter drop loss are relatively independent of the network layout, size, and topology. For the scope of this study, we will focus on the loss components which significantly impact the overall power budget as a function of the type, radix, and throughput of the network. In addition to optical loss, ring ﬁlters and modulators (a) DMXbar (b) CMXbar Figure 4: Photonic 4x4 Crossbars – Both crossbars have four inputs (I1−4 ), four outputs (O1−4 ), and four channels which are wavelength division multiplexed onto the U-shaped waveguide. Number next to each ring indicates resonant wavelength. (a) distributed mux crossbar (DMXbar) with one channel per output, (b) centralized mux crossbar (CMXbar) with one channel per input. a single waveguide using WDM. Crossbars with higher radix and/or greater channel bandwidths will require more wavelengths and more waveguides. Both examples require global arbitration to determine which input can send to which output. Various arbitration schemes are possible including electrical and photonic versions of centralized and distributed arbitration. Figure 4a illustrates a distributed mux crossbar (DMXbar) where there is one channel per output and every input can modulate every output channel. As an example, if I1 wants to send a message to O3 it ﬁrst arbitrates and then modulates wavelength λ3 . This light will experience four modulator insertion losses, 13 through losses, and one drop loss. Notice that although a DMXbar only needs one ring ﬁlter per output, it requires O(nr2 ) modulators where r is the crossbar radix and n is the number of wavelengths per port. For larger radix crossbars with wider channel bitwidths the number of modulators can signiﬁcantly impact optical power, thermal tuning power, and area. For large distributed-mux crossbars this requires very aggressive photonic modulator device design. Vantrease et al. have proposed a global 64 × 64 photonic crossbar which is similar in spirit to the DMXbar scheme and requires about a million rings [15]. Their work uses a photonic token passing network to implement the required global arbitration. Figure 4b illustrates an alternative approach called a centralized mux crossbar (CMXbar) where there is one channel per input and every output can listen to every input channel. As an example, if I3 wants to send a message to O1 it ﬁrst arbitrates and then modulates wavelength λ3 . By default all ring ﬁlters at the receivers are slightly off-resonance so output O1 receives the message by tuning in the ring ﬁlter for λ3 . This light will expeFigure 5: Serpentine Layout for 64x64 CMXbar – Electrical circuitry shown in red. 64 waveguides (8 sets of 8) are either routed between columns of tiles (monolithic integration) or over tiles (3D integration). One 128 b/cycle channel is mapped to each waveguide, with 64 λ going from left to right and 64 λ going from right to left. Each tile modulates a unique channel and every tile can receive from any channel. rience one modulator insertion loss, 13 through losses, three detuned receiver through losses, and one drop loss. If all ring ﬁlters were always tuned in, then wavelength λ3 would have to be split among all the outputs even though only one output is ever going to actually receive the data. Although useful for broadcast, this would drastically increase the optical power. A CMXbar only needs one modulator per input (and so is less sensitive to modulator insertion loss), but it requires O(nr2 ) drop ﬁlters. As with the DMXbar, this can impact optical power, thermal tuning power, and area, and it necessitates aggressive reduction in the ring through loss. Additionally, tuning of the appropriate drop ﬁlter rings when receiving a message is done using charge injection, and this incurs a ﬁxed overhead cost of 50 µW per tuned ring. Kırman et al. investigated a global bus-based architecture which is similar to the CMXbar scheme [7]. Nodes optically broadcast a request signal to all other nodes, and then a distributed arbitration scheme allows all nodes to agree on which receiver rings to tune in. Psota et al. have also proposed a CMXbar-like scheme which focuses on supporting global broadcast where all receivers are always tuned in [12]. Although Figure 4 shows two of the more common approaches proposed in the literature, there are other schemes which use a signiﬁcantly different implementation. Zhou et al. describe an approach which replaces the U-shaped waveguide with a matrix of passive ring ﬁlters [17]. This approach still requires either multiple modulators per input or multiple ring ﬁlters per output, but results in shorter waveguide lengths since all wavelengths do not need to pass by all tiles. Unfortunately, the matrix also increases the number of rings and waveguide crossings. Petracca et al. describe a crossbar implementation which leverages photonic switching elements that switch many wavelengths with a single ring resonator [10]. Their scheme requires an electrical control network to conﬁgure these photonic switching elements, and thus is best suited for transmitting very long messages which amortize conﬁguration overhead. In this paper, we focus on the schemes illustrated in Figure 4 and leave a detailed comparison to more complicated crossbars for future work. The DMXbar and CMXbar schemes can be extended to much larger systems in a variety of ways. A naive extension of the CMXbar scheme in Figure 4b is to layout a global loop around the chip with light always traveling in one direction. Unfortunately this layout has an optical critical path which would traverse the loop twice. Figure 5 shows a more efﬁcient serpentine layout of the CMXbar scheme for our target system of 64 tiles. This crossbar has 128 b/cycle input ports which makes it suitable for a MTBw system (i.e., 128 b/cycle per tile under uniform random trafﬁc). At a 5 GHz clock rate, each channel uses 64 λ (10 Gb/s/λ ), and we need a total of 64 waveguides (1 waveguide/channel). An input can send light in either direction on the waveguides, which shortens the optical critical path but requires additional modulators per input. The total power dissipated in the on-chip photonic network can be divided into two components. The ﬁrst component consists of power dissipated in the photonic components, i.e., power at the laser source and the power dissipated in thermal tuning. The second part consists of electrical power dissipated in the modulator driver, receiver, and arbitration circuits. Here we quantify the ﬁrst power component and then in Section 5 we provide a detailed analysis of the second power component. The optical losses experienced in the various optical components and the desired network capacity determine the total optical power needed at the laser source. In the serpentine layout of a CMXbar, the waveguide and ring through loss are the dominant loss components, due to the long waveguides (9.5 cm) and large number of rings (128 modulator rings and 63 × 64 = 4032 ﬁlter rings) along each waveguide. Figure 6 shows two contour plots of the optical power required at the laser source for the LTBw and HTBw systems with a photonic CMXbar network. For a given value of waveguide loss and through loss per ring, the number of wavelengths per waveguide is the same for the two systems. However, the higher bandwidth system requires wider global buses which increases the optical power required at the laser source. As a result, the LTBw system can tolerate higher losses per component compared to the HTBw system for the same optical (a) LTBw (b) HTBw Figure 6: Laser Optical Power (W) (top row) and Percent Area (bottom row) for 64×64 CMXbar – Systems implemented with serpentine layout on 20×20 mm die. Global Crossbar Clos System Rings Power Rings Power LTBw 266 k 5.3 W 14 k 0.28 W HTBw 1,000 k 21.3 W 57 k 1.14 W Table 4: Thermal Power – Power required to thermally tune the rings in the network over a temperature range of 20K. power budget. Figure 6 shows contour plots of the percent area required for the optical devices for the LTBw and HTBw systems. The non-linearity limit affects the number of wavelengths that can be routed on each waveguide and hence the number of required waveguides, making photonic device area dependent on optical loss. As expected, the HTBw system requires increased photonic area for each loss combination. There is a lower limit on the area overhead which occurs when all of the wavelengths per waveguide are utilized. The minimum area for the LTBw and HTBw systems is 6% and 23%, respectively. To calculate the required power for thermal tuning, we assume that under typical conditions the rings in the system would experience a temperature range of 20 K. Table 4 shows the power required for thermal tuning in the crossbar. Although each modulator and ring ﬁlter uses two cascaded rings, we assume that these two rings can share the same heater. The large number of rings in the crossbar signiﬁcantly increases both thermal tuning and area overheads. We can use a similar serpentine layout as the one shown in Figure 5 to implement a DMXbar. There would be one output tile per waveguide and there would be no need to tune or detune the drop ﬁlters. We would, however, require a large number of modulators per waveguide (a) Clos with Photonic Point-to-Point Channels (b) Clos with Photonic Middle Routers Figure 7: Photonic 2-ary 3-stage Clos Networks – Both networks have four inputs (I1−4 ), four outputs (O1−4 ), and six 2×2 routers (R0−2,0−1 ). (a) four point-to-point photonic channels use WDM on each U-shaped waveguide. (b) the two middle routers (R1,0−1 ) are implemented with photonic 2 × 2 CMXbars on a single U-shaped waveguide. Number next to each ring indicates resonant wavelength. (63 × 64 = 4032) and modulator insertion loss would most likely dominate the optical power loss. For this topology to be feasible, novel modulators with close to 0 dB insertion loss need to be designed. The area for photonic devices and power dissipated in thermally tuning the rings would be similar to that in the CMXbar implementation. The large number of rings required for photonic crossbar implementations make monolithic integration impractical from an area perspective, and 3D integration is expensive due to the power cost of thermal tuning (even in the case when all the circuits of the inactive transmitters/receivers can be fully powered down). The actual cost of these crossbar networks will be even higher than indicated in this section since we have not accounted for arbitration overhead. These observations motivate our interest in photonic Clos networks which preserve much of the simplicity of the crossbar programming model, while signiﬁcantly reducing area and power. 4.3. Photonic Clos Networks As described in Section 3.2, a Clos network uses multiple stages of small routers to create a larger non-blocking all-to-all network. Figure 7 shows two approaches for implementing a 2-ary 3-stage Clos network. In Figure 7a, all of the Clos routers are implemented electrically and the inter-router channels are implemented with photonics. As an example, if input I2 wants to communicate with outFigure 8: U-Shaped Layout for 8-ary 3-stage Clos – Electrical circuitry shown in red. 56 waveguides (8 sets of 7) are either routed between columns of tiles (monolithic integration) or over tiles (3D integration). Each of the 8 clusters (8 tiles per cluster) has electrical channels to its router group which contains one router per Clos stage. In the inset, the ﬁrst set of 7 waveguides are used for channels (each 64 λ = 128 b/cycle) connecting to and from every other cluster. The second set of 7 waveguides are used for the second half of the Clos network. The remaining 42 waveguides are used for point-to-point channels between other clusters. put O4 then it can use either middle router. If the routing algorithm chooses R1,1 , then the network will use wavelength λ2 on the ﬁrst waveguide to send the message to R1,1 and wavelength λ4 on the second waveguide to send the message to O4 . Figure 7b is logically the same topology, but each middle router is implemented with photonic CMXbar. The channels for both crossbars are multiplexed onto the same waveguide using WDM. Note that we still use electrical buffering and arbitration for these photonic middle routers. Using photonic instead of electrical middle routers removes one stage of EOE conversion and can potentially lower the dynamic power of the middle router crossbars, but at the cost of higher optical and thermal tuning power. Depending on photonic device losses, this tradeoff may be beneﬁcial since for our target system the radix of the Clos routers (8×8) is relatively low. In this paper, we focus on the Clos with photonic point-to-point channels since it should have the lowest optical power, thermal tuning power, and area overhead. As in the crossbar case, there are multiple ways to extend this smaller Clos network to larger systems. For a fair comparison, we keep the same packaging constraints (i.e., location of vertical couplers) and also try to use 5. Simulation Results In this section, we use a detailed cycle-accurate microarchitectural simulator to study the performance and power of various electrical and photonic networks for a 64-tile system with 512 b messages. Our model includes pipeline latencies, router contention, ﬂow control, and serialization overheads. Warm-up, measure, and drain phases of several thousand cycles and inﬁnite source queues were used to accurately determine the latency at a given injection rate. Various events (e.g., channel utilization, queue accesses, arbitration) were counted during simulation and then multiplied by energy values derived from ﬁrst-order gate-level models. Our baseline includes three electrical networks: a 2D mesh (emesh), a mesh with a concentration factor of four (ecmeshx2), and an 8-ary 3-stage Clos (eclos). Because a single concentrated mesh would have channel bitwidths larger than our message size for some conﬁgurations, we implement two parallel cmeshes with narrow channels and randomly interleave messages between them. We also study a photonic implementation of the Clos network (pclos) with aggressive (pclos-a) and conservative (pclos-c) photonic devices (see Table 2). We show results for LTBw and HTBw systems which correspond to ideal throughputs of 64 b/cycle and 256 b/cycle per tile for uniform random trafﬁc. Our mesh networks use dimension-ordered routing, while our Clos networks use a randomized oblivious routing algorithm (i.e., randomly choosing the middle router). All networks use wormhole ﬂow control. We use synthetic trafﬁc patterns based on a partitioned application model. Each trafﬁc pattern has some number of logical partitions, and tiles randomly communicate only with other tiles that are in the same partition. These logical partitions are then mapped to physical tiles in either a co-located fashion (tiles within a partition are physically grouped together) or in a distributed fashion (tiles in a partition are distributed across the chip). We believe these partitioned trafﬁc patterns capture the varying locality present in manycore programs. Although we studied various partition sizes and mappings, we focus on the following four representative patterns in this paper. A single global partition is identical to the standard uniform random trafﬁc pattern (UR). The P8C pattern has eight partitions each with eight tiles optimally co-located together. The P8D pattern stripes these partitions across the chip. The P2D pattern has 32 partitions each with two tiles, and these two tiles are mapped to diagonally opposite quadrants of the chip. Figure 10 shows the latency versus offered bandwidth for the LTBw and HTBw systems with different trafﬁc patterns. In both emesh and ecmeshx2, the P8C trafﬁc pattern requires only local communication and thus has higher performance. The P2D trafﬁc pattern requires global communication which results in lower per(a) LTBw (b) HTBw Figure 9: Laser Optical Power (W) (top row) and Percent Area (bottom row) for 8-ary 3-stage Clos – Systems implemented with U-shaped layout on 20×20 mm die. the light from the laser most efﬁciently. Figure 8 shows the U-shaped layout of the photonic Clos network in a MTBw system, which corresponds to 64 λ per channel. Each point-to-point photonic channel uses either forward or backward propagating wavelengths depending on the physical location of the source and destination clusters. In a Clos network, the waveguide and ring through losses contribute signiﬁcantly to the total optical loss but to a lesser extent than in a crossbar network, due to shorter waveguides and less rings along each waveguide. All the waveguides in the Clos network are roughly 2× shorter and with 20× less rings along each waveguide compared to a crossbar network. Figure 9 shows the optical power contours for the Clos network. Although the number of optical channels in the Clos network is higher than in the crossbar network, the total number of rings (for same bandwidth) is signiﬁcantly smaller since optical channels are point-to-point, resulting in signiﬁcantly smaller tuning (Table 4) and area costs. The area overhead shown in Figure 9 is much smaller than for a crossbar due to shorter waveguides and smaller number of rings and is well suited for monolithic integration with a wider range of device losses. The lower limit on the area overhead is 2% and 8% for LTBw and HTBw, respectively. Based on this design-space exploration we propose using the photonic Clos network for on-chip communication. Clos networks have lower area and thermal tuning costs and higher tolerance of photonic device losses as compared to global photonic crossbars. In the next section we compare this photonic Clos network with electrical implementations of mesh, cmesh, and Clos networks in terms of throughput, latency, and power. (a) emesh LTBw (b) emesh HTBw (a) P8C trafﬁc pattern (b) P8D trafﬁc pattern (c) ecmeshx2 LTBw (d) ecmeshx2 HTBw (e) pclos LTBw (f) pclos HTBw Figure 10: Latency vs. Offered Bandwidth – LTBw systems have a theoretical throughput of 64 b/cycle per tile for UR; corresponding for HTBw is 256 b/cycle. formance. On average, ecmeshx2 saturates at higher bandwidths than emesh due to the path diversity provided by the two cmesh networks, and has lower latency due to lower average hop count. Although not shown in Figure 10, the eclos network has similar saturation throughput to pclos but with higher average latency. Because pclos always distributes trafﬁc randomly across its middle routers, it has uniform latency and throughput across all trafﬁc patterns. Note, however, that pclos performs better than emesh and emeshx2 on global trafﬁc patterns (e.g., P2D) and worse on local trafﬁc patterns (e.g., P8C). If the pclos power consumption is low enough for the LTBw system then we should be able to increase the size to a MTBw or HTBw system. A larger pclos network will hopefully have similar performance and energy-efﬁciency for local trafﬁc patterns as compared to emesh and ecmeshx2 and much better performance and energy-efﬁciency for global trafﬁc patterns. Figure 11 shows the power dissipation versus offered bandwidth for various network topologies with the P8C Figure 11: Power Dissipation vs. Offered Bandwidth – 3.3 W laser power not included for the pclos-a topology. and P8D trafﬁc patterns. In order to match the performance of ecmeshx2 LTBw system we need to use the pclos-a MTBw system which has slightly higher power for the P8C trafﬁc pattern (local communication) and much lower power for the P8D trafﬁc pattern (global communication) assuming we are at medium to high load. Laser power is not included in Figure 11 which may be appropriate for systems primarily limited by the power density of the processor chip, but may not be appropriate for energy-constrained systems or for systems limited by the total power consumption of the motherboard. Figure 12 shows the power breakdowns for various topologies and trafﬁc patterns, for both LTBw and HTBw design points that can support the desired offered bandwidth with lowest power. Compared to emesh and ecmeshx2, the pclos-a network provides comparable performance and low power dissipation for global trafﬁc patterns, and comparable performance and power dissipation for local trafﬁc patterns. The pclos-a network energy-efﬁciency increases when sized for higher throughputs (higher utilization) due to static laser power component. More importantly, the pclos-a network offers a global low-dimensional network with uniform performance which should simplify manycore parallel programming. The energy efﬁciency of pclos network might be further improved by investigating alternative implementations which use photonic middle switch router as shown in Figure 7b. It is important to note that with conservative optical technology projections, even in relatively simple optical network like pclos, the required electrical laser power is much larger than other components, and the photonic network will usually consume higher power than the electrical networks. This strong coupling between overall network performance, topology and underlying photonic components underlines the need for a fully integrated vertical design approach illustrated in this paper. Acknowledgments This work was supported in part by Intel Corp. and DARPA awards W911NF-08-1-0134 and W911NF-08-10139. "
A Modeling and exploration framework for interconnect network design in the nanometer era.,"As we approach serious scaling roadblocks in the next few process nodes, it is imperative to identify new emerging technologies that can complement or supplant CMOS in the future. We present an integrated cyclic approach to explore new interconnect technologies in the nanometer era for many core systems, where on-chip interconnects are jointly optimized at all the levels in the design hierarchy to develop a complete interconnect solution - from interconnect technology to network topology.","A Modeling and Exploration Framework for Interconnect Network Design in the Nanometer Era Ajay Joshi, Fred Chen and Vladimir Stojanovi ´c Department of EECS, Massachusetts Institute of Technology, Cambridge, MA 1. Introduction To provide a well-balanced design in a power and areaconstrained manycore system, and enable performance scaling with increase in number of cores, high-throughput energy-efﬁcient on-chip networks need to be developed. We propose a design-space exploration framework to bridge technology, circuit, and architecture levels that will not only enable the evaluation of a new technology, but also the speciﬁcation of the technology roadmap necessary to support the desired system roadmap. 2. Integrated approach to interconnect network design Figure 1: Integrated cyclic approach to explore CNT interconnect technology. As we move into the nanometer regime, to explore new interconnect technologies, a generic and integrated approach, where interconnect designs are simultaneously optimized at all levels in the hierarchy is necessary. Figure 1 shows such an integrated approach that can be adopted for investigating the design space of carbon nanotube (CNT) interconnect technology. As a ﬁrst step, circuit-level models are developed using the available CNT device models. Given today’s dimensions of on-chip interconnects, and bandwidth density and area requirements, their inductance can be ignored and they can be modeled as RC interconnects. We explore the use of CNTs as both interconnects and vias. Various area, power and performance tradeoffs are explored at the circuit-level to map the RC design space using CNTs. These tradeoffs are then utilized to perform an architecture-level analysis using analytical models and detailed cycle-accurate simulations. The results obtained from the architectural analysis drive both circuit-level and device-level improvements needed to meet the overall system speciﬁcations. A similar approach can be easily adopted to study other interconnect technologies that can be mapped to the RC design space. 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  Figure 2: Ideal throughput (in Gbps) contour of a repeaterinserted cmesh network designed for a 64-node system, operating at 2.5 GHz, with 400 sq mm area and 10 W on-chip communication (wires and routers) power budget in 22 nm process for uniform random trafﬁc. W = interconnect width, H = dielectric thickness, k = CNT non-ideality factor, Rw = resistance per unit length and Cw = capacitance per unit length. 3. Design example As a design example, we consider a power-constrained concentrated mesh (cmesh) network for a 64-node system. Figure 2 shows the ideal network throughput contour plotted for the cmesh network for various combinations of Rw and Cw values, which have been normalized to the ITRSpredicted copper (Cu) interconnect for 22 nm technology. These Rw, Cw values correspond to various interconnect technologies (Cu and CNT) and dimensions. The RwCw design space is divided into three distinct regions that show the different effects of change in Rw and/or Cw on the ideal throughput. Using this throughput contour, design decisions could be made at all levels in the design hierarchy. For example, if an ideal throughput of 14 Gbps is desired for this 64-node system, then this cmesh network, with repeaterinserted interconnects designed using either of the four conﬁgurations in Figure 2 could be used as the underlying technology. However, if an ideal throughput of 18 Gbps is desired then only the CNT(0.5,1,1) conﬁguration can be used. 4. Conclusion As we approach serious scaling roadblocks in the next few process nodes, it is imperative to identify new emerging technologies that can complement or supplant CMOS in the future. We present an integrated cyclic approach to explore new interconnect technologies in the nanometer era for manycore systems, where on-chip interconnects are jointly optimized at all the levels in the design hierarchy to develop a complete interconnect solution – from interconnect technology to network topology.   "
Dynamic packet fragmentation for increased virtual channel utilization in on-chip routers.,"Conventional packet-switched on-chip routers provide good resource sharing while minimizing latencies through various techniques. A virtual channel (VC) is allocated on a per-packet basis and held until the entire packet exits the VC buffer. This sometimes leads to inefficient use of VCs at high network loads. A blocked packet can affect adjacent routers, resulting in a congestion propagation effect. In such a scenario, VC buffers may be empty although they are regarded as fully occupied by a blocked packet. This paper proposes a dynamic packet fragmentation technique which releases empty VC buffers by fragmenting packets and allowing other packets to use the freed VC buffers. Thus, fragmentation increases VC utilization. Simulation experiments show performance improvement in terms of latency and throughput up to 20% and 7.5%, respectively.","Dynamic Packet Fragmentation for Increased Virtual Channel Utilization in  On-Chip Routers  Young Hoon Kang, Taek-Jun Kwon, and Jeff Draper  University of Southern California / Information Sciences Institute  {youngkan, tjkwon, draper}@ISI.EDU  Abstract  Conventional packet-switched on-chip routers provide  good resource sharing while minimizing latencies through  various techniques. A virtual channel (VC) is allocated on a  per-packet basis and held until the entire packet exits the VC  buffer. This sometimes leads to inefficient use of VCs at high  network loads. A blocked packet can affect adjacent routers,  resulting in a congestion propagation effect. In such a  scenario, VC buffers may be empty although they are  regarded as fully occupied by a blocked packet. This paper  proposes a dynamic packet fragmentation technique which  releases empty VC buffers by fragmenting packets and  allowing other packets to use the freed VC buffers. Thus,  fragmentation  increases VC  utilization.  Simulation  experiments show performance improvement in terms of  latency and throughput up to 20% and 7.5%, respectively.  1. Introduction  Networks-on-Chip (NoCs) have been suggested as a  scalable communication solution for many-core architectures.  As the number of System-on-Chip (SoC) cores increases,  power and latency limitations make buses increasingly  unsuitable. In contrast, NoCs offer fundamental benefits of  high bandwidth, low latency, low power and scalability.  Prior  literature  [1][4][8][11]  proposed  highly  performance-driven  router architectures with dynamic  channel sharing. A packet-switched virtual channel (VC)  router exploits multiple VCs, achieving high throughput with  dynamic allocation of resources. A high utilization of the  physical link is accomplished through multiple VCs, but little  effort has focused on VC utilization. Especially given the  limited number of VCs in on-chip routers, better VC  utilization techniques must be considered.   As adopted in most VC routers, a VC is allocated on a  per-packet basis and de-allocated at the time that the entire  packet exits the VC buffer. This characteristic sometimes  leads to inefficient use of VCs at high traffic loads. With a  trend of shallow flit buffers per VC, a blocked packet easily  propagates congestion to neighboring routers and can cause  congestion to spread to the overall network. A blocked  packet can cause VC buffers to be regarded as fully occupied  even when they are empty. Several studies [5][12][14]  characterized buffer utilization with various traffic models  and proposed power optimization by placing idle buffers in a  sleep mode. This paper, however, proposes a technique to  exploit empty VC buffers to increase throughput and reduce  latency via increased VC utilization.  978-1-4244-4143-3/09/$25.00 ©2009 IEEE  We propose dynamic packet fragmentation to prevent  some VC blocking scenarios from propagating congestion to  adjacent routers. Once a packet is fragmented, the blocking is  localized by releasing the hold of empty VC buffers, thereby  allowing other packets to use the freed VC. Thus, congested  upstream routers do not force inputs of downstream routers  to throttle to a reduced rate. The fragmentation router, rather,  provides more flexible flow control and VC utilization at  high network loads.  The  implemented fragmentation router  is evaluated  through various simulation experiments with synthetic  workloads. Performance benefits are demonstrated compared  to a baseline router, and accurate power and area  measurements are analyzed from a layout. The result  demonstrates that the fragmentation router outperforms the  baseline while it consumes less energy. This result of packet  latency reduction and increased throughput justifies the  slightly more complex flow control required, making the  fragmentation router suitable for future NoC design.  This paper is organized as follows. Section 2 describes a  baseline router used for comparison in the evaluation  experiments. Section 3 elaborates on the dynamic packet  fragmentation technique, and the scheme is applied to the  proposed model in Section 4. Section 5 discusses the  simulation results, and Section 6 concludes the paper.  2. Baseline router  To evaluate the concept of dynamic fragmentation, we  must first define a baseline router. This section discusses key  attributes of a baseline router used in our evaluation  experiments. A single cycle baseline router has been  implemented based on the presented parameter selections,  and the same baseline router is used for incorporating the  dynamic fragmentation scheme.  Figure 1(a) shows the pipeline stages and steps of a  baseline router. The basic steps are comprised of buffer write  (BW), pre-routing computation (preRC), virtual-channel  allocation (VA), switch allocation (SA), switch traversal (ST),  and link traversal (LT) [3]. The following sections describe  each of the basic steps adopted in the baseline router.  BW & preRC stages: When a flit is written into an input  buffer, routing pre-computation (look-ahead routing [6]) is  concurrently performed to reduce the number of pipeline  stages. This pre-computation eliminates the RC stage from  the critical path by overlapping it with the BW stage.                      (a) Baseline router                    (b) Proposed router  Figure 1. Pipeline stages of the routers  SAVA stage: With the information of the routing  computation, the VA for a free VC and SA for the crossbar  time slot are the next steps. Peh and Dally [1] suggested  speculation techniques for improving latency. Mullins et al.  [8][11] proposed low-latency routers by predetermining the  arbitration decisions one cycle earlier than they are requested.  In this baseline router, we followed the scheme suggested in  [4]. VA and SA are done in the same cycle, but VA is  performed sequentially after the SA for a combined SAVA  stage. A SA winner is first selected through two separate  stages of arbitration (local arbitration and global arbitration).  VA is then accomplished simply by finding a free output VC  from the requested output port of the SA winner.   ST stage: Bypassing the input buffer [7] is another  technique to optimize performance. In this case, upon  successful arbitration, a flit goes directly to the crossbar  without delaying one cycle for input buffering. Figure 2  shows how bypassing is implemented. Since the depth of the  FIFO buffers in this design is assumed to be small (i.e. 5entry buffer), the input buffers are implemented with  synthesizable flip-flops. Each entry of the flip-flop buffers  can act as an input pipeline eliminating the separate input  pipeline overhead. The read operation is done in parallel with  local arbitration of the SA. The VC winner from the local  arbitration selects a request among the VCs and makes the  selected flit ready at the crossbar. While the local VC winner  requests global arbitration, the flit from the input port can  cross the crossbar half-way to the output port, awaiting the  final arbitration result.   Figure 2. Bypassing in Flip-Flop based buffers  If the SA request to the global arbitration is granted and  the VA succeeds by finding a free output VC, the header flit  traversal is complete. At the LT stage, the header flit is  finally transferred to the next router through the link. The rest  of the body and tail flits use the same VC allocated to the  header flit. Only the SA is performed while they traverse the  crossbar as shown in Figure 1(a).   3. Dynamic packet fragmentation  Now that a baseline router definition has been established,  this section introduces the concept of dynamic packet  fragmentation to increase VC utilization. Given the trend of a  small number of VCs in NoCs to minimize router overhead,  VC utilization is a key factor for performance improvement.  We first analyze blocking scenarios to show where poor VC  utilization occurs, motivating  the need  for dynamic  fragmentation.   The baseline router described in the previous section is  adequate in the absence of blocking situations. Flits proceed  in an orderly fashion through router pipelines when there is  little network contention. However, at high traffic loads  router pipelines often  stall, propagating congestion  throughout the network. Router pipeline stalls are generally  classified into two types: packet stalls and flit stalls. The  packet stalls are related to the packet processing functions of  the pipeline whereas the flit stalls occur when a flit cannot  complete the switch allocation [3]. Flit stalls can further be  subdivided into credit stalls, buffer empty stalls, and stalls  due to a failing SA for the switch time slot.   These flit stalls cause a packet to span multiple routers  holding the VCs, as shown in Figure 3. In Figure 3(a), the  blocked packet in router 3 generates a credit stall to router 2,  and the credit stall propagates to upstream routers. The  packet is stuck in the VCs until the credit is returned from  router 3 to router 1. The situation is even worse with longer  packets. Longer packets span more routers, so the chance that  the flow is congested by the holding of multiple network  resources is increased.   Figure 3(b) shows that mid-packet blocking in a delayed  packet leads to a buffer empty stall. While packet 1 is being  forwarded to router 2, router 1 may become congested due to  other conditions. The flits of packet 1 that are already  forwarded to router 2 before congestion occurs are routed to  router 3 without any delay because router 2 is not congested.  The delayed packet in router 1 causes the VCs in the  downstream routers to be throttled to a reduced rate. Router 2  and 3 may experience empty buffer stalls, preventing the VC,  although empty, from being assigned to packet 2. These stalls  lead to a bandwidth loss and increased network congestion.   In fact, the VC router design mitigates the effect of packet  blocking by sharing a physical channel among several VCs.  Although the VC router design increases physical link  utilization with dynamic sharing, it does not address VC  utilization. Efficient utilization of the limited number of VCs  can be a key factor to further performance improvement.  Since the number of VCs cannot be increased without limit  due to area, power and latency constraints, a better utilization  technique of VCs should be considered.  Dynamic packet fragmentation provides a means for VCs  to be utilized more efficiently by preventing blocking from  propagating in some situations. Since the router dynamically  fragments packets in these blocking situations, the frequency  of fragmentation is dynamically adjusted to network traffic.  Indeed, this router design avoids fragmentation overhead at  low traffic rates where there is little contention. The  following sections describe how fragmentation is applied to  the blocking situations we address.               (a) Credit Stall  (b) Buffer empty stall  Figure 3. Examples of flit stalls  3.1 Fragmentation at credit stall  Since a credit stall is induced by the blocking of a  downstream router, the stall is released when a flit of the  downstream router leaves the input buffer and a credit is  returned. The returned credit experiences latencies due to  transmission and updating of the credit count. The buffer can  then be assigned to a new flit. At the time of a credit stall,  even though there may be idle buffers in other comparable  VCs in the downstream router, the current packet cannot use  the empty buffers because of VC allocation restrictions.  However, if the packet is fragmented and considered as a  different packet, the stalled packet of the current node can be  assigned to a new VC and be routed without credit stalls.  Figure 4 shows  the process of dynamic packet  fragmentation in the case of a credit stall. The VC controller  in the router senses the lack of credits when the VC has 1  credit left and no more credits are returned (Figure 4(a)).  When the router uses the last available credit by forwarding a  body flit, the VC controller fragments the packet to avoid  credit stalls. The fragmentation starts by changing the type  field of the sending body flit to a virtual-tail flit. The VC  controller then regards the packet forwarding to be complete  and releases the hold of the output VC (in this example,  output VC 0 is released, Figure 4(b)).   Now, the remaining body flits in the input buffer are  considered as a new packet and attempt to acquire a new VC.  A copy of the header flit from the original packet must be  maintained for fragmentation.  This header flit copy serves as  a virtual header flit for all fragments of the original packet  and is treated as a normal header flit by VC controllers. With  the buffered header information, the VC controller can treat  the remaining body and tail flits as a new complete packet.  The router performs VA and SA steps again as if a new  packet is delivered. If the VA and SA succeed, the VC  controller sends a virtual-header  flit  first using  the  information of the buffered header (in this example, output  VC 1 is acquired, Figure 4(c)). After that, the body and tail  flits follow the same route setup by the virtual-header at the  success of SA. Since a new VC is allocated to the fragmented  packet, the credit count is also updated to the amount of  available buffers at the new VC.  (a)                         (b)                          (c)  Figure 4. Packet fragmentation at credit stall  By regarding credit stalls as a congestion metric of  downstream routers, a fragmented packet could be routed to a  different path in adaptive routing schemes. This enables  dynamic, flexible finer-grained flow control. We leave more  details about coupling fragmentation with adaptive routing as  a future work.  3.2 Fragmentation at buffer empty stall  Buffer empty stalls are caused by upstream routers, and  such a stall is released when a new flit is delivered to the  input buffer. A buffer empty stall prevents empty storage  from being used, as shown in Figure 3(b). To give other  packets a chance  to use  the empty buffer, packet  fragmentation can be performed.  Figure 5 shows the process for how a packet is  fragmented in the case of a buffer empty stall. From Figure  5(a), a router detects that it may encounter a buffer empty  stall when it has a single flit in the input buffer and no flit  coming into this buffer. When the last available flit in the  input buffer is forwarded to the next router, the VC controller  fragments the packet by changing the type field of the  sending flit  to a virtual-tail (Figure 5(b)). With  the  fragmentation, the VC controller releases the output VC (in  this example, output VC 0 is released), regarding the packet  forwarding as complete.                    (a)                           (b)                        (c)  Figure 5. Packet fragmentation at buffer empty  stall  When the delayed upstream router is released, and a new  flit arrives in the input buffer, the VC controller treats the flit  as a new packet. As described in the previous section, since  the newly arrived body flit does not have any of the routing  information, the VC controller retains a copy of the header  flit and uses it for VA and SA steps for packet fragments.   For the routing pre-computation step, the pre-computed  routing information is already available in the header buffer;  thus, the routing computation process can be skipped for  trailing packet fragments. If the VC controller succeeds in  VA and SA, the buffered head flit is forwarded first as a  virtual-header (in this example, output VC 1 is acquired,  Figure 5(c)) similar  to  that shown  for credit stall  fragmentation. The rest of the newly arrived body flits follow                        the same route of the virtual-header flit until the VC  controller detects another buffer empty stall or tail flit.  Fragmentation prevents the mid-packet blocking from  propagating to multiple routers. Figure 6 shows how  fragmentation mitigates the example of Figure 3(b). Once the  empty buffer in router 3 is released by fragmentation from  the empty buffer of router 2, the buffer can be utilized by  packet 2 as soon as the head part of the fragmented packet 1  leaves the buffer. Thus, fragmentation prevents upstream  blocking from propagating to downstream routers. Congested  upstream routers do not force inputs of downstream routers  to throttle to a reduced rate; instead the input of a  downstream router can be utilized by another packet.  A fragmented packet can be fragmented again if the same  situation occurs in another router. So the virtual-head flit  overhead is increased according to the number of times  fragmentation occurs. Note that the virtual-head and virtualtail flits are created during routing at  the point of  fragmentation; intermediate routers treat them identically as  normal head and tail flits, respectively.  Figure 6. After the fragmentation of Figure 3(b)  4. Proposed router  This section describes how dynamic packet fragmentation  is applied to the baseline router described in Section 3.  Although fragmentation is introduced for better resource  utilization, unnecessary fragmentation should be avoided due  to the overhead involved. The proposed router uses the  following techniques to address this trade-off.   In the case of credit stall fragmentation, fragmentation  just because of the credit loop latency needs to be avoided.  The flit buffers are recycled when a credit is returned. If the  number of flit buffers is below the number of cycles of the  credit loop, the VC will incur a credit stall without any  blocking. In this paper, a single-cycle router latency and  single-cycle link latency are assumed. So, a 5-entry buffer  per VC is sufficient to cover the credit loop of 5 cycles.  Therefore, the router does not experience a credit stall just  because of the credit loop latency.  Another fragmentation which could occur in the case of  buffer empty stalls can also be avoided with the priority  scheme of arbitration. With a fair arbitration scheme, the  VCs interleave their flits on a single physical channel,  potentially causing downstream router input buffers to be  throttled even when there is no blocking. If the next  downstream router is not congested, the forwarded flit may  be transferred to the next router immediately. The first  downstream router would then fragment the packet since it  experiences a lack of flits in the input buffer.   On the other hand, a winner-take-all arbitration [3][4] for  the SA solves the problem. Since this scheme allocates all the  bandwidth to a single packet until it sees a tail flit or the  packet is blocked, the input buffer of the downstream router  does not see any empty slots between consecutive flits  forwarded. Once the stream is disconnected because of the  stall, the packet is fragmented and the next packet gets the  priority to be forwarded until the end of the stream.  From the techniques mentioned above, the input buffers  of the VC always receive a complete packet stream from  header (virtual-header) to tail (virtual-tail). Even though the  packet is fragmented, the fragmented packet is treated as a  complete packet. Forwarding a complete packet makes it  possible to emulate a wormhole router in the proposed model.  In fact, body and tail flits do not require the SA step in a  wormhole router. They just follow the same path set up by a  header flit. The proposed router performs similarly to a  wormhole router, but with the use of fragmentation it does  not have the problem of mid-packet blocking due to how  channels are held on a per-packet basis.  The pipeline stages of the proposed model can be seen in  Figure 1(b). Unlike the baseline router, the proposed model  does not require a SA stage for body and tail flits. The  crossbar passage is set up by a header or virtual-header flit  and is valid until the tail or virtual-tail flit traverses the  crossbar. The guaranteed single hop latency for body and tail  flits is achieved with the circuit switching quality of the data  streaming. The scheme is beneficial in terms of power  consumption since the number of activities for the SA stage  is reduced to a per-packet basis.  Although the presented router can be applied to various  routing algorithms, we assume dimension-order (XY) routing  with a two-dimensional mesh network for the simplicity of  design. Since  intermediate routers forward  the packet  fragments based on the arrival sequence at the input port, the  fragmented packets are delivered in-order with deterministic  routing. The packet reassembling process at the destination is  straightforward and can be left to a network interface  controller.  5. Simulation results  This section describes performance and power analyses of  the implemented designs. A baseline and the proposed  dynamic packet fragmentation routers were developed in  synthesizable VHDL code. The codes were synthesized using  Synopsys Design Compiler, and layouts were generated with  Cadence SOC Encounter targeting the Artisan standard cell  library for IBM 90 nm technology. Table I summarizes the  common router features and network parameters for  synthesis and circuit simulation.  Table 1. Design evaluation parameters  Topology Mesh 4x4  # of ports  5  # of VCs  4  Flit size  128 bits  Routing  Buffer per  port  Packet length  Dimension-order (XY)  24 (6-entry depth per  VC)  8, 16 flits                Figure 8.  Performance and fragmentation rate graphs for 8-flit and 16-flit packets                                                    5.1. Performance  The performance of the proposed router is evaluated in  cycle-accurate  simulations using  synthetic workloads  generated with uniform random traffic. Figure 8 shows a set  of performance graphs and fragmentation rates based on  various injected loads. Since the proposed router, which  performs fragmentation, has an extra header buffer in  addition to the 5-entry flit buffer, baseline routers are  assumed to have 6-entry flit buffers for fairness of the  comparison.  Figure 8 shows the simulation results for packet sizes of 8  flits and 16 flits, respectively. Base indicates the same  baseline router mentioned in the section 3, and Fragment  indicates the proposed router. Both routers are assumed to  have a winner-take-all arbitration scheme.  As can be seen in Figure 8(a), in the 8-flit packet  simulation, Base shows close performance to the proposed  router. Since the 8-flit packet can just be buffered in two  adjacent  routers assuming 6-entry  input buffers,  the  interference of the blocked packet to other packets was  minimal. On the other hand, in the 16-flit packet simulation,  the fragmentation router is superior to Base for latency and  throughput. The proposed router shows 20% less latency at  the base saturation point of a 52% injection rate and 7.5%  more throughput between the respective saturation points. As  expected, this result indicates that fragmentation is more  beneficial  to  longer packets. Although  fragmentation  increases the overhead in terms of added virtual header flits,  the increased resource utilization and dynamic reaction  capability more than compensate for the added overhead.  Figure 8(b) shows the fragmentation rate of the proposed  router versus the injection rate. The fragmentation rate is  measured by counting the number of virtual header flits  among the total number of packets received at the destination  node. So, 100% and 200% fragmentation indicate that every  packet is fragmented in 8-flit and 16-flit packet simulations,  respectively. Note that the size of the fragmented packet is  determined by the number of buffer entries. Therefore, with  6-entry input buffers the 8-flit packet is fragmented only  once and the 16-flit packet is fragmented up to twice. From  Figure 8(b), the fragmentation is gradually increased as more  traffic is applied, as expected. At low traffic loads, most  packets are delivered  to destination nodes without  fragmentation because blocking is less likely. At the  saturation point, however, most packets are fragmented due  to congestion.  To assess the impact of the dynamic nature of the  proposed fragmentation scheme, the performance of dynamic  fragmentation is compared to the performance of a static  fragmentation scheme where packets are fragmented at the  point of injection. Figure 8(c) shows the performance graphs  of 8-flit and 16-flit packets. 100% and 200% Fragment  indicates statically fragmented packets whereas Fragment  denotes the proposed dynamically fragmented scheme. From  both graphs, dynamic fragmentation shows more than a 10%  latency reduction at the saturation points of the statically  fragmented packet cases. This indicates that the dynamic and  reactive aspect of our proposed router is an important aspect  of the design.   5.2. Place and Route  The Fragment and Base routers were synthesized with  clock gating to minimize dynamic power. Table 2 shows the  layout picture of the fragmentation router and a breakdown  of the router area. This section provides accurate timing, area,  power, and energy analyses based on the generated layouts.  Table 2. Fragmentation router layout & area  Base  Fragment  Cell area  400673µm²  413773µm²  Flit buffer  72%  60%  Header  0%  10%  Crossbar  5.4%  5.5%  VC ctrl  12%  14.1%  [820x820] µm² with 76.73% density  5.2.1. Area. Although the Fragment router is slightly bigger  than the Base due to the slightly more complex control logic  for  fragmentation,  the area difference  is negligible  (approximately 3%). Instead, data-path elements (flit buffers,  header buffer, & crossbar) dominate over the control  elements by taking over 75% of the entire area for both  designs. Since the area is roughly proportional to the  dynamic power, this measure is also indicative of the power  consumption.  5.2.2. Critical path. The critical path is measured as 3.0 ns  including wire delays for both designs. Fragmentation is not  on the critical path. Note that the fragmentation router has a  characteristic of wormhole router, unlike highly dynamic VC              allocation. So, the pre-allocation technique of resources can  be easily applied to it. Although several techniques for lowlatency routers were proposed in [8][11], they are quite  complex and were not applied to maintain the simplicity of  our design.  5.2.3. Power. To see the impact of traffic to the overall  network  power  consumption,  the  network  power  consumption is measured with the same workload used in the  performance evaluation. Every annotated switching activity  is captured and reflected in the power calculation using  Synopsys Power Compiler with typical operating conditions  (1.0V, 25℃). The result of Figure 10(a) shows the power  requirement according to the various traffic rates. Given that  the main overhead of the Fragment router is due to the more  complex control logic, the power overhead can be regarded  as more overall switching activities due to the larger number  of standard cells and utilizing empty buffers.  5.2.4. Energy. Power alone can be a misleading metric. The  correlation of power and performance must be considered to  evaluate the true merit of a design. The energy cost graphs in  Figure 10(b), therefore, are a better reflection of the design  efficiency. The dynamic energy per packet for the applied  synthetic workloads is indicated by multiplying the latency  and the network power consumption [13]. As can be seen in  Figure 10(b), the Fragment router consumes almost the same  energy as Base at low injection rates. However, at high  injection rates near the saturation point, the Fragment router  consumes 15% less energy than Base.  (a) Network dynamic power        (b) Network energy  Figure 10. Network power and energy consumption  6. Conclusions  A dynamic packet fragmentation router has been  presented and evaluated in terms of performance, power, and  energy. The fragmentation router increases performance in  terms of latency and throughput up to 20% and 7.5%,  respectively. Moreover, simulation results indicate an energy  savings as well. Since dynamic fragmentation reacts to traffic  conditions,  it  increases VC utilization and  relieves  congestion. While introducing the packet fragmentation  technique in on-chip routers for the first time, we believe that  the presented idea opens up another research area for flow  control mechanisms in NoC router design. For future work,  we are exploring how the decision to fragment can be made  more intelligently. Such work is necessary to scale the  concept for larger packets so that the fragmentation overhead  does not negate the benefit.  7. "
On-Chip photonic interconnects for scalable multi-core architectures.,"In this paper, we propose PROPEL, a photonic network-on-chip (NoC) that improves performance and power with energy-efficient opto-electronic components for future chip multiprocessors (CMPs). Our analytical and simulation results indicate that PROPEL improves throughput and reduces power over optical and electrical networks for various traffic traces while requiring fewer photonic components and devices.","On-Chip Photonic Interconnects for Scalable Multi-core Architectures  Avinash Karanth Kodi1, Randy Morris1, Ahmed Louri2 and Xiang Zhang2  1School of Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701  2Electrical and Computer Engineering, University of Arizona, Tucson, AZ 85721.  E-mail: {kodi,rm700603}@ohio.edu, louri@ece.arizona.edu, zxkidd@gmail.com  Abstract       In this paper, we propose PROPEL, a photonic  network-on-chip (NoC) that improves performance and  power  with  energy-efficient  opto-electronic  components for future chip multiprocessors (CMPs).  Our analytical and simulation results indicate that  PROPEL improves throughput and reduces power over  optical and electrical networks for various traffic  traces while requiring fewer photonic components and  devices.   1. Introduction  In this paper, we propose PROPEL - a power and  area-efficient, high-performance NoC architecture  targeting future 22nm technology node with 64 cores.  The proposed architecture uses optical interconnects  for long distance inter-router communication and  electrical switching at the routers. This reduces the  power dissipation on long inter-router links while  electrical switching provides flow control to prevent  buffer overflow. PROPEL uses dimension order  routing (DOR) by traversing in both X and Y  directions with an intermediate electrical conversion.   2. Architecture and Results       Figure 1 shows the proposed architecture. The  proposed off-chip broadband light source will generate  few wavelengths. By transmitting a continuous optical  signal in the x- and y- direction simultaneously, the  optical signal can be modulated at  the optical  transmitters. The figure shows 4 tiles combined  together to form a super-tile. A tile is comprised of a  processing core and its L2 cache. This grouping  reduces the cost of the interconnect as every core does  not require  lasers attached and facilitates  local  communication through cheaper electronic switching.  The number of wavelengths required is equal to the  maximum number of super tiles in the x- or y-  direction. Each super-tile consists of dual-set (x and y)  photonic transceivers and an electronic switch. It takes  a maximum of 2 hops to traverse any-to-any super tile,  one hop in the x-dimension and one hop in the ydimension. For inter super-tile communication, the  static routing and wavelength allocation (RWA)  involves selective merging of different wavelengths  from various super-tiles into separate channels thereby  maximizing  the bandwidth  (wavelength division  978-1-4244-4143-3/09/$25.00 ©2009 IEEE  multiplexing) and re-using the same wavelengths on  different waveguides (space division multiplexing).  λ 0, λ 1, λ 2, λ 3 Off -Chip Light Source Tile (Core  + L2 Cache) Photonic Transce iver Super -Tile  S(0,0) Tile 0 Tile 2 Tile 1 Tile 3 Figure 1: Proposed PROPEL architecture.  3 2.5 2 1.5 1 0.5 0 ) h s e M o t d e z i l a m r o N ( t u p h g u o r h T Uniform B it Reversal Butterfly Complement Matrix  Transpose Perfect  Shuffle Neighbor Mesh Flattened-Butterfly Corona PROPEL Figure 2: Throughput comparison.       Figure 2 shows the performance of PROPEL when  compared to other competing electrical and optical  NoC topologies including Mesh, Flattened Butterfly,  CORONA  and PROPEL. Although PROPEL  significantly reduces the optical hardware complexity  (components), its performance is comparable and even  better than other competing technologies. For a more  detailed report on PROPEL and its performance please  see [2].  3. "
Fault-tolerant architecture and deflection routing for degradable NoC switches.,"Networks-on-Chips (NoCs) provide inherent structural redundancy of on-chip communication pathways. This redundancy can be exploited to maintain connectivity even if some components of an NoC exhibit faults which will appear at an increasing rate in future chip generations. Based on a fine-grained functional fault model, error-detecting circuitry, and distributed online fault diagnosis, we determine the fault status of NoC switches, including their adjacent links. The remaining functionality of partly defective switches is utilized by a modified deflection routing algorithm to achieve graceful degradation of packet throughput.","Fault-Tolerant Architecture and Deflection Routing   for Degradable NoC Switches  Adán Kohler, Martin Radetzki  Institut für Technische Informatik, Universität Stuttgart, Stuttgart, Germany  {kohleran, radetzki}@informatik.uni-stuttgart.de  Abstract  Networks-on-Chips (NoCs) provide inherent structural redundancy of on-chip communication pathways.  This redundancy can be exploited to maintain connectivity even if some components of an NoC exhibit faults  which will appear at an increasing rate in future chip  generations. Based on a fine-grained functional fault  model, error-detecting circuitry, and distributed online fault diagnosis, we determine the fault status of  NoC switches, including their adjacent links. The remaining functionality of partly defective switches is  utilized by a modified deflection routing algorithm to  achieve graceful degradation of packet throughput.  1. Introduction  A network-on-chip (NoC) is an on-chip communication infrastructure that implements multi-hop and  predominantly  packet-switched  communication.  Through pipelined packet transmission, NoCs permit a  more efficient utilization of communication resources  than traditional on-chip buses. Regular NoC structures  reduce VLSI layout complexity compared to custom  routed wires [3].  The inherent redundancy of NoCs with multiple alternative routes between packet sources and sinks  bears the potential to compensate faults. In future chip  generations, faults will appear with increasing probability due to the susceptibility of shrinking feature sizes to process variability, age-related degradation,  crosstalk, and single-event upsets. To sustain chip production yield and reliable operation, very large numbers of faults will have to be tolerated [5].  In order to take countermeasures against NoC  faults, we must first detect and diagnose them. To this  end, we propose to equip data packets with CRC  checksums and to augment the switch circuit architecture with error detecting units. Based on the information from these units and additional directed test patterns, a distributed diagnosis of switches and their immediate neighborhood is performed. Diagnosis results  are stored in on-chip structures that represent information of a detailed functional fault model. This information requires special protection so as not to become  corrupted by faults, which would jeopardize switch  operation. It is used by a new fault-adaptive deflection  routing algorithm to avoid defective parts of the switch  but still utilize its remaining functionality. Thereby, a  more graceful degradation of network operation, even  in presence of large numbers of faults, is achieved.  The paper is organized as follows: Section 2  presents the related work. In Section 3, the foundations  upon which our contribution is built are explained.  Section 4 presents fault models and fault diagnosis of  switches and links. Protection mechanisms for critical  structures are presented in Section 5. Section 6 deals  with routing adaptation to diagnosed faults. Experimental results and conclusions are provided in Sections 7 and 8.  2. Related work  Fault-tolerant routing can be classified as probabilistic vs. deterministic. Probabilistic algorithms provide  fault tolerance through data redundancy by replicating  packets and sending them over different routes [2].  The approach of probabilistic gossip flooding allows a  network switch to forward a packet to any of its neighbors with some pre-determined probability [4]. Directed flooding refines this principle by preferring  hops that bring the packet closer to its addressed sink  [15]. Random walk [15] limits the number of packet  copies by allowing replications at the source only. In  the rest of the network, these copies probabilistically  take different routes without further replication.  Fault-tolerant deterministic routing avoids bandwidth-consuming data redundancy by utilizing the  structural redundancy of NoCs. Faulty network  switches are bypassed by locally or globally adapting  978-1-4244-4143-3/09/$25.00 ©2009 IEEE                      the affected routes. Adaptation must avoid creating  deadlocks or livelocks. To this end, previous research  has suggested identifying rectangular or convex regions that enclose faulty network components [18].  Routing detours around these regions can be selected  so that deadlock-prone cyclic dependencies are impossible, e.g. by using the odd-even turn model [7] or  cycle-free contours [19]. Routing adaptation with  global computation of routing paths and source routing  has been in investigated in [17]. Another approach [11]  is based on Q routing, which selects routing paths so  that packet delivery time is minimized. Fault adaptation is achieved by considering the delivery time of  faulty switches as infinite.  Any routing adaptation to avoid faults requires  knowledge of the network’s fault status to be obtained  by means of fault diagnosis, consisting of fault localization (pinpointing the faulty component) and fault  characterization  (discriminating between different  kinds of faults, e.g. transient and permanent ones).  Diagnosis can be performed offline, in a mode different from normal system operation (e.g. during production test or built-in self test), or online, concurrent with  regular system operation.  The probe send algorithm from [13] performs diagnosis by flooding the network with test packets to explore available routing paths. Permanent faults in NoC  crossbar switches can systematically be located by  injecting directed test patterns at the boundaries of a  2D mesh network [16]. However, regular network operation is disabled for an amount of time that grows  linearly with mesh edge length. The approach in [6]  uses a parity bit to detect faults affecting an odd number of bits of regular network traffic. It locates switch  faults and link faults online, but does not discriminate  transient from permanent faults nor narrow down the  location of faults inside switches. Further methods and  associated fault models have been suggested for the  diagnosis of individual link wires [20] and packet misrouting due to control errors (stuck at port faults) [1].  Previous approaches to fault-tolerant routing assume NoC switches to be either fully functional or out  of service. This contribution suggests a fine grained  fault model, a distributed online diagnosis method, and  a fault-adaptive routing algorithm to reduce the performance penalty of NoC switch defects.  3. Basic principles  Our work follows the basic concepts of the Nostrum  NoC [14] in order to facilitate comparison with an established architecture. Network topology (Fig. 1) is a  2D mesh of configurable size Nx·Ny. Packets are transmitted via wide links without being split into smaller  flits. Deflection routing is employed, which enables an  efficient chip implementation without buffers and flow  control. In order to achieve high performance, each  incoming packet is processed in one cycle, requiring  all internal logic to be combinational.   We will use the principle of deflection as the basis  for implementing routing adaptation to faults. Another  feature useful for fault-tolerance is the deactivation of  unused switch ports (e.g. at the mesh boundaries) with  the help of a bit from Nostrum’s load (congestion)  detection. This bit, referred to as LD in the following,  will be employed to deactivate faulty links.  Figure 1. Network topology  The Nostrum packet format (Fig. 2) includes a flag,  V, to mark valid packets. If that flag is set to zero, all  other bits are ignored, whereby the absence or invalidity of a packet is represented. The HC field holds the  current hop count of the packet. A relative addressing  scheme is employed that encodes the ∆x and ∆y distances to the target with a sign-magnitude representation instead of using absolute (x, y) coordinates. This  relieves switches from having to know their position  and allows routing decisions to be made based on the  sign bits only, but requires implementation of address  updating (±1). Address bit width is chosen in accordance with a reference design for sake of comparison,  but could be reduced to the benefit of the payload in  case of moderately sized NoCs.  original 128-bit packet V HC ∆x ∆y Payload (96 b its) 1 V 11 10 10 88 8     b its HC ∆x ∆y Payload CRC modified packet with CRC field Figure 2. Packet format modification            As shown in Fig. 2, we modify the packet format by  using eight payload bits to store a CRC checksum for  error detection. The CRC polynomial used  is  g(x) = x8 + 1. It detects all packets with an odd number  of erroneous bits or a single burst of up to 8 erroneous  bits. The latter property ensures that spatially correlated errors, e.g. from crosstalk between adjacent  wires, are covered. The choice of the polynomial is  primarily driven by the need to compute the CRC in a  combinational way, without using LSFRs. The parity  trees employed for that purpose grow with the complexity of the polynomial. The area cost of using polynomials with additional properties [8] such as detection  of all independent double errors is prohibitive, as  shown in Table 1. While we cannot guarantee the detection of such errors, the probability of missing arbitrary bit faults (regardless of their number and position) is less than 0.004.  Table 1. Chip area of CRC parity trees  CRC polynomial  x8+1  x8+x6+x3+x2+1  x8+x5+x3+x2+x+1  Hamming  distance  2  3  4  Area [μm2]  Nangate 45nm typ  191.52  848.54  1015.06  In the following, beyond error detection, we use the  CRC field to fully diagnose NoC links and switch datapaths.  4. Fault diagnosis  The applied fault model differentiates between errors that occur during the transmission of packets between adjacent switches and errors that are introduced  locally at a switch in the forwarding process. For local  faults, all elements of the switch datapath are accounted for. However, errors provoked by router malfunction are currently not taken care of because in case  of a defective router the switch as a whole unit needs  to be avoided or shut down. The respective diagnosis  is not in scope of this work; it can be performed on  higher network layers or by gate-level structural diagnosis of the router logic. For transmission errors, both  the link wires and the input registers are taken into  account, as a fault in any of these two components  leads to corrupted data at the output of the input register.  In order to determine the source of detected errors,  each input and output except those connected to the  local resource features a combinational CRC unit  (Fig. 3). On reception of a packet as well as before  being sent out to the next switch, its checksum is verified by the CRC units assigned to the input and output,  respectively. Errors induced internally by the crossbar  are detected by a matching checksum at an input and a  mismatch at the output, whereas errors caused by  switch to switch transmission are characterized by a  CRC mismatch at the input. The connections to and  from the local resource L have no CRC checks as there  is no redundancy that could be used to bypass diagnosed faults.  As the packet header is changed during the routing  process, the checksum also needs updating. The recalculation is performed by a CRC instance right after the  router that employs an own CRC tree for taking the  new header bits into account. For the payload bits the  intermediate results from the CRC unit after the input  register are used. This not only reduces area consumption, but also ensures that packets altered between the  input register and the crossbar input are not assigned a  valid checksum by mistake.  The CRC units at the in- and output get further information from the diagnostic parts (namely the fault  matrix and the link error counter, cf. Sections 4.1 and  4.2), so that in test mode they check for valid test patterns instead of correct CRC checksums.  input  registers output drivers N E S W C R C C R C C R C C R C C R C C R C C R C C R C N E S W Router input  checks input  register output checks L L Figure 3. Sw itch w ith CRC checks  4.1. Switch diagnosis  Crossbar faults are managed by means of a fault  matrix. The matrix stores the status of all input-output  combinations with three bits each, except for those  involving the local resource (cf. Fig. 4). Each matrix  element fij can be seen as a state machine that advances  on each packet switched from the input i to output j.  While all elements initially carry the no fault state and  all input-output combinations may be used, a detected  CRC mismatch involving input i and output j will ad              vance fij as shown in Fig 4: The first error is treated as  transient, i.e. the connection remains active and fij is  reset if the next packet switched from i to j does not  cause an error. This way, transient errors with the duration of just one cycle are tolerated without causing  overhead through diagnosis. After two consecutive  errors, the state advances to intermittent, causing this  crossbar connection to be considered faulty and being  avoided by the router. The ports at input i and output j  are shut down in order to send test patterns over the  connection.  output N E S W N E S W Fault matrix 000 no fault crc OK all  pat terns 011-110 testing send test  patterns crc·V crc ·V 001 transient crc ·V 010 intermittent crc 111 permanent Fault state machine Figure 4: Sw itch fault model for diagnosis  In total, four patterns are applied: {0}128 and {1}128  test for the absence of stuck-at faults inside the data  path of the crossbar multiplexers [12]. Afterwards,  {01}64 and {10}64 are used to check for crosstalk. On  pass of all tests, the state is reset to no fault, the connection is reactivated and the input and output ports  are enabled again. If at least one test fails, the state of  the connection is set to permanent fault, which it will  never leave again. The input and output previously  shut down for testing are reactivated if the fault matrix  still indicates fault-free routing options for them. Let  IEi (input enable) denote the state for the input from  direction i, OEj (output enable) the state of the output  to direction j, where a logic 1 means “enabled” and a  logic 0 means “shut down”. The generation of these  signals is done by boolean operations on the entries of  the fault matrix:  (1)    IE = f testing  (2)    testing  OE = f transient f The or-expression in Eq. 1 is responsible for enabling inputs whose incoming packets can be switched to  at least one working output, while the and-expression  ensures that the input is shut down as soon as some  input-output combination involving this input is being  tested. The formula for outputs (Eq. 2) works likewise.  A switch can signalize its neighbor that it cannot  send data packets over an output. This is achieved by  the “load detection” (LD) signals. Each unidirectional  link has exactly one LD line associated, a value of zero  ∨ ∨ transient ∧ ∧ ( ( ( ( ) ) ∉ f ij ∉ ∧ ∧ ≤ ij ≤ ij j i ) ) i j j i ij meaning that no data will be sent over this link. A  switch receiving an LD signal of zero for one of its  inputs knows that no packets will arrive over this input  and must in turn not send any packets over the associated output. This guarantees that no switch receives  more packets per cycle than it can send out – otherwise  a packet would have to be dropped due to bufferless  flow control.  A switch that has diagnosed e.g. a defective crossbar output in direction j sets its output LDj out to zero,  which effectively shuts down the link in both directions. This mechanism also works in the opposite direction: If the switch is unable to accept data over its  input j, it sets LDj out to zero and so prevents its neighbor from sending in packets. In other words, LDj out is  set to one if both the input and the output for direction  j are enabled:  (3)   LD IE OE Whereas Eq. 3 describes the LD signals generated by a  switch itself, the signals received from its neighbors  are denoted LDi in subsequently.  out j ∧ = j j 4.2. Link diagnosis  Unlike crossbar faults, link faults cannot be detected by a single switch, but involve communication  between the sender, where the packet was still intact,  and the receiver, where the data corruption caused by  the faulty link is detected. The mechanism for handling  link faults works as follows: Each port i has two counters li and si that count the number of received corrupted packets, li, and the number of sent test patterns,  si. These can again be seen as state machines (cf.  Fig. 5).  On reception of two erroneous packets directly following each other, the link is shut down and test patterns are sent. Like in the case of crossbar faults, single  cycle transient errors are tolerated and do not cause  unnecessary shutdown of communication resources.  The patterns used for testing links with continuing  errors have to be created on the sending side of the  link, whereas the fault is diagnosed on the receiving  side, thus the receiver must signal the fault back to the  sender. This cannot be done by the LD signals, as it  would be ambiguous with crossbar faults. Hence, a  second signal called “error detection” (ED) is introduced. When set to one, it signalizes the sender that the  last two packets sent over a link have been received  corrupted. The sender advances si as shown in Fig. 5 b)  and sends the two test patterns {01}64 and {10}64 to  check for the absence of stuck-at faults. If both tests  are successful, the receiver resets the ED signal to zero, reactivating the link. Otherwise, the link fault is            considered to be permanent and the link stays shut  down forever. Aside from link test patterns, no switch  may send data over a link where at least one direction  signals ED = ‘1’.  The ED signal for the output i is generated directly  by the most significant bit of the counter li (cf.  Fig. 5 a). The availability of output j for packet data  transmission (Eq. 4) is determined by both the incoming and the outgoing LD and ED lines for this direction:  (4)   avl = ED ED ∧ ∧ in j ∧ out j LD in j LD out j j   a) link error counter li    b) sent pattern counter si  Figure 5. Counters for link diagnosis  5. Protection against critical errors  The error detection and diagnosis approaches introduce additional features such as the fault matrix and  the error status signaling between adjacent switches.  Any error in the information stored or transmitted by  these structures would have severe impact on network  functionality. Assume a soft error that causes a bit to  flip to 1 in the fault matrix. This could erroneously  deactivate a fully functional crossbar connection. Or  consider the LD signal to be stuck-at-zero. This would  let an intact link idle only because our additional logic  has a failure. In order to protect the switch against such  critical errors, we utilize appropriate error-correcting  and error-detecting codes.  A switch has eight 2-bit counters li and sj. Their bits  are identified as li 0, li 1, sj 0, and sj 1. We group the four  bits l  0 and protect them with a single-error correcting  (SEC) Hamming code, requiring three additional parity  bits. The same is done for l1, s0, and s1. The parity information is used to correct a single-bit error in each of  l  0, l1, s0, and s1. Since the code is orthogonal to the  registers, a double-fault of the two bits in a counter can  be corrected as long as the other (spatially separated)  bits are OK. Fig. 6 shows an example of maximal correction capacity: one error in each row can be corrected, but not a second one (s3 0).  A separate (7,4)-Hamming code is employed on  each directional (N, E, S, W) switch output to protect  the signals LDj and EDj. Of the four Hamming information bits, two are left unused so that only five wires  have to be implemented. The adjacent switch implements single error correction. Thereby, temporary as  well as permanent faults on a single wire are tolerated.  input i or output j parity bits 1      2      3      4      5      6      7 x x x x x bit # 0 1 0 1   li sj Figure 6. SEC code for link error counters  Error detection alone is not an option for interswitch signaling as there is no way to deal with erroneous information. However, the parity fault matrix  internal to the switch can effectively be protected by a  scheme that resets erroneous information. We employ  a two-dimensional parity array (Fig. 7) that follows the  structure of the fault matrix. Each row and each column is protected against odd-numbered bit faults with  a single parity bit. The 2d scheme moreover guarantees  detection of all double bit faults.  columns j 1              2               3              4 bit #   2    1    0    2    1    0     2    1    0    2    1    0 x x i s w o r 1 2 3 4 px (cid:51) (cid:51) (cid:51) (cid:51) py (cid:51)(cid:51)(cid:51)x(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)x(cid:51) Figure 7. Parity protection of fault matrix F  We do not use the 2d scheme to localize bit errors  as this would require a single-fault assumption. Instead, if the parity of a row or column indicates an error, the complete row or column is reset to zero, i.e.  the state no fault. Should a fault exist, diagnosis will  restore the correct fault information.  The example in Fig. 7 assumes two bit errors  marked with ‘x’ in row i = 1. While the row parity  does not detect these errors, the column parities do. As  a result, all three bits in each cell of the affected columns (j = 1, 3) are reset in order to obtain the fault-free  state 0002. We cannot combine all three bits in a single  column parity, though, because this would disable detection of double bit errors located in a single cell fij.  The above error handling provides protection  against soft errors of the registers that implement the  fault matrix. However, the matrix could also be affected by permanent faults from production or due to                    chip aging. A stuck-at-one fault cannot be corrected by  reset. Hence, it may lead to a permanent error entry  which in effect leaves a crossbar connection unused. A  stuck-at-zero fault, on the other hand, may make it  impossible to represent the error status of a connection  correctly. In this case, packets would be sent over this  connection and be discarded by the output CRC check  if they have suffered an error on their path. The effect  of both behaviors lies in performance degradation due  to idle resources or packet loss, which can be tolerated.  Soft errors cannot be tolerated in the same way as  they would accumulate in the fault matrix, which is  avoided by the reset scheme explained above.  The overhead introduced by these mechanisms may  seem disproportionate for current technology; however  it is justified as significantly higher fault probabilities  are assumed for smaller feature sizes.  6. Routing adaptation  Our work is based on the basic deflection routing  variant of delta XY with weighted priority, following  the classification of [10]. It is implemented by the following operations which are repeatedly performed by  each switch:  1. Priorities are assigned to incoming packets.  Priority increases with accumulated latency of packets,  measured with the help of the HC header field (cf. Section 3). Inputs are served (e.g. W in Fig. 8) so that high  priority packets are routed before lower priority packets.  2. The preferred route of a packet is chosen so that  it gets closer to its destination either horizontally or  vertically (delta XY routing, e.g. (1) or (2) in Fig. 8).  3. If the preferred outputs are not available, an alternative route is determined by the following deflection policy: If possible, a route orthogonal to the incoming direction is chosen, cf. (3) in Fig. 8. If that  output is not available, the packet is reflected into the  direction from which it was received, cf. (4) in Fig. 8.  Figure 8. Deflection routing of packet from W  Upon reaching the switch at its destination coordinates, a packet is immediately forwarded to the local  resource. Since up to four packets may arrive at the  same time, the link to the local resource must provide  four times as much bandwidth than the other links.  Packets from the local resource (L in Fig. 8) are injected into the network only if an output is still unused  after routing all other packets. The fact that the number  of available outputs (N, E, S, W) is never less than the  number of received packets ensures absence of deadlocks: as packets in the network never have to wait, no  cyclic waiting dependencies emerge. Livelocks that  might occur due to infinite packet deflection are  avoided by prioritizing older packets and thereby giving them a higher probability of being routed in the  preferred direction.  While this assures absence of livelocks in the faultfree case, the problem persists if links or switches have  to be shut down because of faults. Since individual  switches have only a limited view of the network, such  cases cannot be tackled on the data link layer and must  be taken care of on higher protocol layers. The same  applies to the retransmission of dropped packets.  6.1. Cost-based deflection routing  As we will show with the following example, the  strict priority based scheme must be modified in order  to find feasible routes for all packets in the presence of  faults. Consider the situation shown in Table 2: Four  packets with different priorities come in from the four  directional inputs of the switch. The table further lists  the preferred route and deflection options assumed.  Note that a packet that has already reached either its x  or its y destination coordinate has only one direction  left that brings it closer to its target. Hence, the preferred route in this case consists of only one direction,  while two directions (besides reflection) lead the  packet farther away from the target (e.g. P3 and P4 in  Table 2).  Table 2. Sample packets to be routed  packet  P1  P2  priority  max (4) high (3)  from  N  E  preferred route S, W  N, W  veer away  E  S  reflection  N  E  P3  P4  low (2) min (1)  S  W  N  E  W,E  N,S  S  W  Fig. 9 shows the concept of a routing matrix with  rows and columns corresponding to switch inputs and  outputs, and elements representing routing decisions.  A valid routing matrix must have only one packet per  row and per column as inputs and outputs are nonsharable resources. In other words, a given routing can  be represented by a permutation of the four directions  N, E, S, and W.                    Fig. 9 a) shows the routing of packets in our example, assuming absence of faults. Due to their high  priorities, P1 and P2 each get one of their preferred  outputs. P3 cannot get the N output which is already  occupied by P2. Therefore, it is deflected towards W.  Finally, P4 gets the E output. In case of multiple possibilities (e.g. the preferred output of P1 or the deflection  direction of P3), the choices are made randomly.  In Fig. 9 b), we assume that the crossbar switch  cannot make connections from E to S and from W to  E. In this case, as P4 is served last due to its minimal  priority, no valid routing choice is left other than using  the faulty connection from W to E. However, if we  would choose to route P3 to the E output, as shown in  Fig. 9 c), the fault could be avoided since the W option  is left for P4.  output N E S W P2 N E S W P1 P3 P4 output N E S W P2 N E S W P1 P3 P4 t u p n i a) no faults  b) no choice left  c) feasible Figure 9. Sample routing choices  The example shows that making premature routing  decisions may constrain the routes left to other packets  so that their routing cannot avoid faults. In other  words, a greedy algorithm for constructing a routing  permutation may not find the optimal solution. Therefore, we devise a cost-based deflection routing that  investigates all possible routing permutations, of which  4! = 24 exist for a 4x4 crossbar switch. The cost definition (Eq. 5) penalizes the choice of a faulty connection with a conceptually infinite value that can be  represented by a sufficiently large number in an implementation. It rates the cases of no deflection or deflection towards the target, deflection away from the  target, and reflection with cost values of 0, 1, and 2,  respectively. Note that after a deflection, the preferred  route may be identical with reflection, which must be  avoided in order to prevent packet oscillations between  adjacent switches. This is achieved by prioritizing the  reflection cost over preferred route cost in the following definition:  (5)  c ij ⎧∞ ⎪ ⎪= ⎨ ⎪ ⎪⎩ 2 0 1 if elsif elsif else f i i > ij j = → j , ∨ avl 2001 , j reduces P .addr i , ,  ) ij c i ij , j ] = [ π i = ], i [ r ini ∈ ∈ ∈ in π out ( )∑ cw ir The total cost c(π) of a routing permutation π is  given by Equations 6 and 7:  (6)   routing matrix   (7)   routing cost      ( where wi is the priority weight of the packet received  via input i, and rij are the entries of a routing matrix  with weighted cost. The weight depends on hop count  in our work, but can in general include other criteria  such as network congestion. We assign weights in the  range of 1 to 4 to the input packets in order of increasing HC value. If no (valid) packet is present at an input, the corresponding weight is set to 0.  Fig. 10 a) shows the entries of the routing matrix  for the example specified in Table 2. Different routing  permutations π can be represented and evaluated as  shown in Fig. 10 b) and c).  output N E S W N E S W 8 0 0 1 4 0 6 ∞ 2 4 ∞ 1 0 0 2 2 output N E S W N E S W 8 0 0 1 4 0 6 ∞ 2 4 ∞ 1 0 0 2 2 output N E S W N E S W 8 0 0 1 4 0 6 ∞ 2 4 ∞ 1 0 0 2 2 a) matrix [rij]  b) c(πgood) = 3  c) c(πbad) = 9  Figure 10. Routing evaluation  6.2. Cost optimization  In order to find the cost-optimal permutation πopt,  we have to solve the optimization problem  (8)   minimize c(π) subject to π is a permutation.  While this problem in general can be tackled by sequential algorithms such as the Hungarian method [9]  and integer linear programming, we require an on-chip  implementation that finds a suitable π in a combinational way. To this end, we devise a datapath that  computes cost values in parallel for all π. The datapath  follows the structure of a permutation tree (Fig. 11).  input output N E S W N           E           S           W    E   S   W   N   S   W   N   E   W   N   E   S S W E W E S S W N W N S E W N W N E E S N S N E W S W E S E W S W N S N W E W N E N S E S N E N = … = Figure 11. Routing permutation tree                            The upper layer of the tree has branches for the outputs to which the packet from N input can be routed.  Further layers add branches for the remaining options  to route packets from inputs E, S, and W.  To construct the datapath from this tree, we replace  each output node j of the tree by the routing cost rij,  where input i is defined by the layer of the tree. Following the tree bottom-up, we sum up the cost values.  At the nodes in the higher layers, where multiple paths  converge, it is sufficient to keep the minimum of the  lower tree branches, as it always leads to a smaller  overall sum. Fig. 12 shows the datapath computations  for the example of the routing cost matrix introduced  in Fig. 10 a). Here, ‘x’ stands for a sufficiently large  cost of routing a packet via a faulty connection. The  computation of the cost of πgood is highlighted in the  right half of the tree; the computation of c(πbad) can be  found in the left half. Note that c(πbad) would not actually be computed since the first minimum operation  on the path to the root pursues computation only with  the lower value of 0+1.  input N E S W output min 8           4           0           0    +           +           +           +     min         min         min         min    6   x   0   0   x   0   0   6   0   0   6   x +   +   +   +   +   +   +   +   +   +   +   + min min min min min min min min min min min min + + + + + + + + + + + + + + + + + + + + + + + + 4 2 2 2 2 4 4 2 0 2 0 4 2 2 0 2 0 2 2 4 0 4 0 2 2 1 2 x 1 x 2 1 2 1 1 1 2 x 2 1 x 1 1 x 1 1 x 1 Figure 12. Datapath for min-cost permutation  In contrast to cost computation, the cost-optimal  routing permutation πopt is determined top-down, by  following the minimum decisions starting at the root of  the tree. In our example, we get πopt = πgood; another  optimal permutation would be {(N,W), (E,N), (S,E),  (W,S)}. In the case of too many faults (each path from  root to leaf contains at least one ‘x’), no fault-free  routing of all four packets exists, and at least one  packet has to be dropped. This is achieved without  further provisions: πopt will include at least one of the  large ‘x’ values, and any packet routed via the corresponding faulty connection will be dropped at the output CRC check.  The structure of the permutation tree allows further  datapath optimization. At the bottom of the tree, each  mapping from the S and W inputs to outputs occurs  twice, as indicated in Fig. 11. By performing the respective addition only once, the number of + and min  operations is reduced to half in the lowest layer. The  resulting datapath has 28 additions and 11 min operations that can be implemented with 4 to 6 bit accuracy.  7. Experimental results  The fault-tolerant switch has been designed at register transfer level in VHDL on the basis of a plain Nostrum switch. It has been synthesized to the 45nm  Nangate library. Synthesis results are shown in Table 3. The fault-tolerant version is just 24% larger,  despite the overhead of implementing the fault matrix,  CRC checks, and test pattern generation (the latter  included in the toplevel logic).   The moderate increase is achieved through savings  in the router logic, as the original Nostrum deflection  policy is implemented in a combinational way and  requires computation and sorting of all possible packet  routes and deflections in parallel. The cost-based  routing leads to a significant decrease in router size,  which is of particular benefit to fault tolerance as a  smaller router is less often hit by faults, and as router  faults are likely to disable the complete switch. Link  management is simpler in our design as we do not collect congestion statistics.  Table 3. Synthesis results  CHIP AREA [μm²]  plain  fault-tol.  Nangate 45nm  Nostrum  version  complete switch  toplevel  crossbar  router  link management  input adjust  header update  fault matrix  CRC units  13570.0  4315.8  2773.1  4218.5  529.1  671.9  1061.6  0.0  0.0  16790.2 5497.3 2753.9 2968.8 243.7 629.4 548.8 1147.5 3000.8 The additional logic causes higher router latency,  which reduces the maximum operating frequency from  106.84 MHz for the plain Nostrum design by 28% to  77.4 MHz.  For measuring the performance of our approach, we  have simulated a model with 8 x 8 of the fault-tolerant  switches. In addition, we have simulated two further  models that employ only partial error detection and no  diagnosis for reference. The end-to-end model checks  the integrity of a packet only before delivering it to the  destination resource. A packet being corrupted during  transmission nevertheless traverses the complete path  from its source to its destination. The drop-detected            model verifies packet integrity at the in- and outputs of  each switch and drops corrupted packets immediately.  Therefore, a packet that suffers an error does not use  up further network resources. In case of persistent  faults however, following packets taking the same  route will be affected by the same fault as no further  actions are taken.  We have simulated each model with varying numbers of structural (i.e. permanent) faults. A set of uniformly distributed defects is used as a fixed fault pattern, starting from zero defects up to the injection of all  defects from the set with both uniform random and  transposed traffic.   0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  0  10  20  30  40  50 Number of permanent link faults  60  70  80 r a i t o o f o c r r c e l t y r d e v e c e i e k c a p t s e p r i e k c a p d e c e n j t t uniform: with diagnosis transposed: with diagnosis uniform: drop detected uniform: end-to-end transposed: end-to-end transposed: drop detected Figure 13. Permanent link faults  Fig. 13 shows the effect of permanent link faults on  the ratio of correctly received packets to injected packets: While the ratios for both the end-to-end and the  drop-detected model drop below 70 percent (uniform  random traffic) and 60 percent (transposed traffic) for  50 link failures, more than 95 percent of the injected  packets still arrive correctly at their destination in the  fault-tolerant design for both traffic patterns.   0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  0  20  40  60  80  100 Number of permanent crossbar faults  120  140 r a i t o o f o c r r c e l t y r e k c a p d e v e c e i t e p s r i c e n j t e k c a p d e t uniform: with diagnosis transposed: with diagnosis uniform: end-to-end uniform: drop detected transposed: end-to-end transposed: drop detected Figure 14. Permanent crossbar faults  Similar results can be observed for defects inside  the crossbar (cf. Fig. 14): While the fault-tolerant design still delivers more than 90 percent of the injected  packets to their destinations in case of the maximum  simulated 150 crossbar faults, performance drops to  around 70 percent for uniform traffic and around 55  percent for transposed traffic for the models without  diagnosis.  Finally, the effect of transient errors on the proposed design has been analyzed. For this case, faults  have been injected during runtime for a range of extremely high probabilities so as to stress the NoC significantly.   0.9  0.92  0.94  0.96  0.98  1  0  0.002  0.004  0.006 Transient error probability  0.008  0.01 r d e v e c e i o c r r c e t e k c a p t s / i c e n j t d e s e k c a p t r a i t o uniform: end-to-end uniform: with diagnosis uniform: drop detected transposed: end-to-end transposed: drop detected transposed: with diagnosis Figure 15. Transient errors  As can be seen in Fig. 15, the diagnosing switch  model performs similar as the drop-detected model, as  both drop corrupted packets immediately. The end-toend model’s seemingly better performance is due to  the fact a packet corrupted by a transient error can be  hit by more than one transient error during transmission and nevertheless be counted as a single damaged  packet at the destination, whereas in the fully diagnosing and the drop-detected models these transient errors  would damage more than one packet.  It is in the nature of fault-adaptive routing to be ineffective against transient faults that disappear before a  reaction is possible. The value of our diagnosis approach is in correctly identifying such faults so that no  inappropriate actions are taken and their performance  penalty is avoided. Furthermore, as Fig. 15 shows, our  protection schemes of critical infrastructure are effective: In spite of having introduced additional logic susceptible to transient faults, performance does not deteriorate even in presence of these faults.                                                          8. Conclusions  We have presented a fault-adaptive deflection  routing mechanism that takes the detailed fault status  of NoC crossbar connections into account. The fault  status is obtained through distributed online diagnosis  that distinguishes between permanent and transient  faults. The combination of routing and diagnosis results in a highly reliable transmission of injected packets to their destinations even under the effect of a large  number of faults and thus minimizes costly retransmissions, while keeping the increase in area and latency in  an acceptable range.  9. Acknowledgments  The authors would like to thank the German Research Foundation (DFG) for financial support of the  project within the Cluster of Excellence in Simulation  Technology (EXC 310/1) at the University of Stuttgart.  We also thank the Nostrum research group at KTH  Kista for providing VHDL sources of a switch, to  which we have added the fault-tolerance features.  10. "
Contention-free on-chip routing of optical packets.,"We propose a new architecture for on-chip routing of optical packets. The proposed infrastructure, referred to as CONoC, facilitates the development of an all-optical on-chip network and alleviates the role of electrical NoCs. As the first step for designing an all-optical NoC, CONoC resolves packet congestions optically and does not use electrical methods. Utilizing wavelength routing method, wavelength division multiplexing, and path reconfiguration capability in CONoC leads to a contention-free architecture. This architectural advantage along with simple and small photonic router architecture results in simple electrical transactions, reduced setup latency, and high transmission capacity. Moreover, we discuss the proper topology for on-chip optical interconnects. Performing a series of simulation-based experiments, we study the efficiency of CONoC along with its power and energy consumption and data transmission delay.","Contention-Free on-Chip Routing of Optical Packets  Somayyeh Koohi, Shaahin Hessabi  Sharif University of Technology, Tehran, Iran  koohi@ce.sharif.edu, hessabi@sharif.edu  Abstract  We propose a new architecture for on-chip routing  of optical packets. The proposed  infrastructure,  referred to as CONoC, facilitates the development of  an all-optical on-chip network and alleviates the role  of electrical NoCs. As the first step for designing an  all-optical NoC, CONoC resolves packet congestions  optically and does not use electrical methods. Utilizing  wavelength routing method, wavelength division  multiplexing, and path reconfiguration capability in  CONoC leads to a contention-free architecture.  This  architectural advantage along with simple and small  photonic router architecture results in simple electrical  transactions,  reduced  setup  latency, and high  transmission capacity. Moreover, we discuss  the  proper topology for on-chip optical interconnects.  Performing a series of simulation-based experiments,  we study the efficiency of CONoC along with its power  and energy consumption and data transmission delay.   1. Introduction  Various limitations of electrical interconnect have  been predicted for about two decades [1]. While NoC,  as a new architectural trend, can improve bandwidth of  electrical interconnections, it is unclear how electronic  NoCs will satisfy future bandwidths and latency  requirements within the package power budget [2].   Optics is a very different physical approach that can  address most of the problems associated with electrical  interconnects, such as bandwidth, latency, precise clock  distribution, and crosstalk [3]. Additionally, bit rate  transparency [4] of optical switching elements and low  propagation loss of optical waveguides [5] lead to low  power dissipation of silicon photonics.    Importance of power dissipation in NoCs along with  power  reduction capability of on-chip optical  interconnects offers optical network-on-chip (ONoC) as  a novel technology solution which can introduce onchip interconnection architecture with high transmission  capacity, low power consumption and low latency.  While electrical NoCs enforces unaffordable power  dissipation in high performance MPSoCs, the unique  978-1-4244-4143-3/09/$25.00 ©2009 IEEE  advantages of ONoC offer considerable power  efficiency and also performance-per-watt scaling as the  most critical design metric.   To analyze on-chip optical interconnect at the  system-level, Briere et al. [6] have developed a virtual  contention-free ONoC. In the proposed ONoC, the  address of the target is not contained in the data packet  but rather in the wavelength of the optical signal.  Routing optical signals according to their wavelengths  is called wavelength routing method. The proposed  contention-free structure is obtained at the cost of large  arrays of fixed-wavelength light sources and fast  switches for wavelength selection. Hence, the speed of  wavelength selection and number of fixed sources at  each IP block limits the scalability, and also severely  increases power consumption and area issues.   Shacham et al.  [2] have  introduced hybrid  architecture for ONoC that combines a photonic circuitswitched network with an electronic packet-switched  control network. Every transmitted optical message is  preceded by a control packet (path-setup packet) which  is routed in the electrical network to reserve the path.  Despite the optical network proposed by Briere et al.  [6], hybrid network introduced by Shacham et al. [2] is  not contention-free. In other words, path-setup packets  may be dropped at  intermediate routers due  to  congestion. In this case, the source may attempt to  transmit the packet again, which increases path-setup  latency. Although the modulated data streams are  grouped using passive WDM multiplexers,  the  proposed ONoC by Shacham et al. [2] cannot route  optical signals according to their wavelengths and  suffers from contention problems.   In this paper, we propose an optical NoC as a global  communication medium for a high performance chip  multiprocessor (CMP). A scalable all-optical NoC can  overcome  the  limitations of previously proposed  ONoCs by Briere et al. [6] and Shacham et al. [2]. In an  all-optical NoC, packet congestions are resolved  optically and do not use electrical methods. As the first  step for designing such an optical switching network,  we improve the hybrid architecture proposed by  Shacham et al. [2]. Our proposed ONoC utilizes the                    advantages of wavelength routing method to resolve  optical packet congestions. In addition to its capability  for optically resolving packet congestions, the proposed  ONoC differs from the one developed by Shacham et  al. [2] in several aspects. It is based on different simpler  and smaller photonic routing architectures. Moreover,  utilizing wavelength routing method along with path  reconfiguration capability leads to a contention-free  architecture. Hence, multiple optical flows can be  multiplexed on  the same waveguide, and  then  demultiplexed according to their wavelengths. These  architectural advantages lead to much simpler electrical  routers, reduced setup latency, and higher transmission  capacity.   Rest of the paper is organized as follows: Section 2  discusses the proper network topology for on-chip  optical interconnects and proposes a novel architecture  of a nonblocking optical router which guarantees  contention-free operation of the network. Moreover, we  describe data transmission through the proposed optical  on-chip  infrastructure. Section 3 describes  the  developed simulation environment for evaluating the  proposed architecture. In Section 4, we present our  simulation-based experiments and discuss the results.  Finally, Section 5 concludes the paper and present  future work.    2. Proposed ONoC architecture  In this section, we propose a new architecture for onchip routing of optical packets which is referred to as  CONoC  (Contention-free Optical NoC). Before  architectural exploration of CONoC, we briefly discuss  the proper interconnection topology which can exploit  the advantages of photonic switching elements and does  not confront with their limitations.  2.1. Topology  The selected topology of on-chip interconnection  network plays an important role in the performance of  NoC architecture. One of the most recent architectures  and topologies proposed is the novel Spidergon NoC  architecture [7] which is a 2-D regular topology. The  Spidergon architecture with N (even) nodes is similar to  a ring enriched by across links between opposite nodes  (Fig. 1). Results show that the Spidergon topology  compared to Ring and Mesh topologies is a good tradeoff between performance, scalability, and energy and  area requirements for SoCs [8].   When designing an optical network-on-chip, we  should concern for physical properties of  light  transmission, and examine the advantages and threats of  routing data streams  through photonic switching  elements. One of these considerations is the waveguide  intersection crosstalk. When two waveguides intersect  Figure 1. 16-Node Spidergon topology  at a right angle, the waves continue propagating in their  original direction and the crosstalk is negligible.  Moreover, while implementing high-degree electronic  crossbar is simple, it is quite difficult to construct  crossbars larger than 4×4 using existing 2×2 photonic  switching elements. Implementing Torus topology,  Shacham et al. [2] have encountered the same problem  and surmounted it by introducing extra injection and  ejection switches which increases power and area  overheads. Fortunately, in a Spidergon topology, 4×4  photonic crossbar of a node can simply interconnect  neighbor nodes and local IP with each other. Moreover,  right angle waveguide intersections introduce negligible  crosstalk. These advantages along with efficiency of the  Spidergon topology motivate us to implement our  ONoC on top of this topology.  2.1.1. Routing algorithm. The Spidergon NoC adopts  the Across-first routing as its deterministic routing  algorithm [8]. According to this routing scheme, if the  target node for a packet is at distance D > N/4 (N is the  number of nodes) on the external ring (that is, in the  opposite half of the Spidergon external ring) then the  across link is traversed first, to reach the opposite node.  Then, clockwise or counterclockwise direction is taken  and maintained, depending on the target’s position. In  the case that  D ≤ N/4, packets only traverse along the  external ring. Based on its simpilicity, we use Acrossfirst routing on the Spidergon network for routing  optical packets.  2.2. Optical router architecture  In this section, we propose a scalable architecture  for a Nonblocking Contention-Free Optical Router  (NCFOR) which benefits from both wavelength routing  and electrical techniques for path reservation.  2.2.1 Basic photonic switching block. According to  Spidergon topology, two-dimensional 4×4 crossbars  are needed to interconnect local IPs and neighbor  nodes with each other. Based on silicon microring  resonator structure, Shacham et al. [9] have proposed a  spatially nonblocking optical 4×4 crossbar. While the  proposed optical crossbar consists of eight microring  resonators, Xu et al. [10] have proposed a 4×4 crossbar  photonic switch node, which comprises an array of  four identical silicon microring resonator switches.  These switches are side-coupled with low-loss and  low-crosstalk multimode-interference  (MMI)-based  waveguide crossing array [11] in a silicon-on-insulator    (SOI) substrate. The experimental results show that the  proposed photonic crossbar design offers low-power  dissipation, scalability, and small device foot-print.  Enlarged view in Fig. 2 schematically shows this twodimensional 4×4 switch node. Each microring  resonator acts as an electro-optic switch grid, enabled  by carrier dispersion effect.   Smaller number of microring resonators offers  smaller area and power consumption requirements.  Hence, we use the photonic switch nodes proposed by  Xu et al. [10] as the required 4×4 crossbar in the  Spidergon topology. Although the proposed photonic  switch in [10] does not suffer from internal blocking  problem in most of data transmission scenarios, it  cannot guarantee contention-free operation of the  optical network. Therefore, based on this photonic  crossbar, we will propose a novel optical router  architecture for a contention-free Spidergon topology.  In the remaining sections of this paper, we will refer to  the photonic 4×4 crossbar proposed by Xu et al. [10] as  Basic Photonic Switching Block (BPSB) and build our  NCFOR upon it.  2.2.2.  Nonblocking  contention-free  router  architecture.    Fig. 2 illustrates an enlarged view of  microring resonator switches are labeled as Si (cid:2778) (cid:3409) (cid:2191) (cid:3407) BPSB and an example of 16-node Spidergon topology  4 . Spidergon topology has vertex symmetry but for  built upon it. In this figure, four identical silicon  depicted in this figure, for Ni  (cid:2777) (cid:3409) (cid:2191) (cid:3407) (cid:1840)/2 , local and  convenience, we arrange the nodes as shown in Fig. 2,  where Ni stands for ith node of the network. As  for Ni (cid:2170)/(cid:2779) (cid:3409) (cid:2191) (cid:3407) (cid:1840) these  across links are connected to North and South ports,  respectively, while  connections are interchanged.   As mentioned before, BPSB cannot guarantee  contention-free operation of the network. For example,  a message routed from West to East will block message  requests from North to East, since the second optical  to packet injection blockage for Ni 0 (cid:3409) (cid:1861) (cid:3407) (cid:1840)/2  and  prohibits across link traversing to reach Ni (cid:1840)/2 (cid:3409) (cid:1861) (cid:3407) (cid:1840)  flow needs S2 to be switched on while the first one  requires its off state transmission. This limitation leads  due to clockwise data transmission on the external ring.  Generally, a blocking scenario occurs when two input  messages need the same microring resonator to be  switched on or they impose inconsistent switching  states on it. As mentioned before, local link position  depends on the node index which leads to different  blocking scenarios for opposite nodes on the external  ring. All these scenarios are listed in Table 1. For each  scenario,  there exist  two different optical data  transmission paths (referred to as Cont. paths) which  contend for electrically controlling the same silicon  microring resonator switch(es) (named as cont. Sw).  Figure 2. 16-node Spidergon topology built on BPSB  In this table, T1(R1), T2(R2), T3(R3), and T4(R4)  represent West, North, East, and South input(output)  ports, respectively. Each of the two contending paths is  determined by a pair of input and output ports of the  photonic router as shown by input-port(cid:198)output-port.  Path Des. column in Table 1 describes type of the  contending paths, such that: +(-)Ejection represents  packet reception by IP block from West(East) port,                   +(-)Injection represents packet transmission by IP  block to East(West) port, +(-)ExternalRing represents  packet transmission from West(East) port to East(West)  port in the router, and finally +(-)AcrossLink represents  packet transmission on the across link to reach the  East(West) port of the router. A simple solution to  overcome a blocking scenario is dropping one of the  message requests and then attempting to transmit it  again. This solution was chosen by Shacham et al. [2],  which leads to performance degradation of the network.  Despite its simplicity, this strategy prohibits multiple  data streams multiplexing on  the same optical  Table 1 Blocking scenario in the BPSB  Path Des.  (cid:2777) (cid:3409) (cid:2191) (cid:3407) (cid:1840)/2  + AcrossLink  + Ejection  + Ejection  + Injection  - Injection  - AcrossLink  - Ejection  - Ejection  - Ejection  - Ejection  - Injection  (cid:2170)/(cid:2779) (cid:3409) (cid:2191) (cid:3407) (cid:1840)  + Injection  + AcrossLink  + Ejection  + Ejection  - AcrossLink  - Ejection  - Ejection  - Injection  + Injection  - AcrossLink  - AcrossLink  Cont. Sw  S1 S1 S1 S2 S3 S4 S4 S4 S3, S4 S1, S4 S2, S4 S1 S2 S2 S2 S3 S3 S3 S4 S1, S2 S2, S3 S2, S4  Cont.  Path1  T4(cid:198)R3 T1(cid:198)R2 T1(cid:198)R2 T2(cid:198)R3 T2(cid:198)R1 T4(cid:198)R1 T3(cid:198)R2 T3(cid:198)R2 T3(cid:198)R2 T3(cid:198)R2 T2(cid:198)R1 T4(cid:198)R3 T2(cid:198)R3 T1(cid:198)R4 T1(cid:198)R4 T2(cid:198)R1 T3(cid:198)R4 T3(cid:198)R4 T4(cid:198)R1 T4(cid:198)R3 T2(cid:198)R1 T2(cid:198)R1  Cont.  Path2  T1(cid:198)R3  T1(cid:198)R3  T4(cid:198)R3  T1(cid:198)R3  T3(cid:198)R1  T3(cid:198)R1  T4(cid:198)R1  T3(cid:198)R1  T2(cid:198)R1  T4(cid:198)R3  T4(cid:198)R3  T1(cid:198)R3  T1(cid:198)R3  T1(cid:198)R3  T2(cid:198)R3  T3(cid:198)R1  T2(cid:198)R1  T3(cid:198)R1  T3(cid:198)R1  T1(cid:198)R4  T1(cid:198)R4  T4(cid:198)R3  Path Des.  + ExternalRing  + ExternalRing  + AcrossLink  + ExternalRing  - ExternalRing  - ExternalRing  - AcrossLink  - ExternalRing  - Injection  + AcrossLink  + AcrossLink  + ExternalRing  + ExternalRing  + ExternalRing  + AcrossLink  - ExternalRing  - AcrossLink  - ExternalRing  - ExternalRing  + Ejection  + Ejection + Injection        Figure 3. Optical Add/Drop Element  waveguide. To design a contention-free ONoC  infrastructure, NCFOR utilizes wavelength routing  method along with Optical Add/Drop (OAD) elements  to overcome path blocking scenarios listed in Table 1.   As mentioned before, wavelength routing method  introduces a totally new dimension by improving the  functionality of the routing devices since it allows  wavelength  selective  filtering. Based on  this  functionality, it is possible to devise fully contentionfree structures. As a means of wavelength selective  filtering, SOI-based microring resonator structures have  been explored as passive optical add/drop filters [12].  These elements insert (add) or extract (drop) optical  channels  (wavelengths)  to or  from  the optical  transmission stream without any electronic processing.  As an example, Fig. 3 depicts an OAD element which  adds optical wavelength λ1 and drops optical  wavelength λ2 to and from the optical data stream,  respectively.  In CONoC, contention-free operation of the network  is accomplished by having one (or a set of) dedicated  wavelength(s) for each node. Careful wavelength  selection, to be discussed in more details later, reduces  the number of distinct wavelengths and retains the  scalability of the network. For data injection to the  network, optical data streams targeted to a specific node  are modulated on its dedicated wavelength(s), and are  ejected from the network to the destination node  according  to  their wavelength(s). Utilizing OAD  elements, we augment  the BPSB with Ejecting  Microring Resonators (EMRs) to extract optical data  streams targeted to this IP block from those passing  through  the router. Taking advantage of EMRs  overcomes the contention problem when ejection of an  optical data stream meets a blocking scenario (specified  as +(-)Ejection in Table 1). In addition to EMRs,  Injecting Microring Resonators (IMRs) are added to the  BPSB to multiplex optical data streams transmitted  from this IP block with those passing through the  router. Utilizing IMRs avoids the contention problem  when optical data injection meets a blocking scenario  (specified as +(-)Injection). Despite these blocking  scenarios, a blocking scenario may occur while  transmitting a packet on the across link to reach the  East/West port of the opposite node (specified as +(-)  AcrossLink). Across Microring Resonators (AMRs) are  added on across links to overcome contention problems  in these cases. The proposed photonic router composed  of BPSB and extra microring resonators including  EMRs, IMRs, and AMRs is shown in Fig. 4.  NCFOR  allows  for multiple data  streams  multiplexing on the same optical waveguide and then  demultiplexing according to their wavelengths. This  additional flexibility, besides ultra-wide bandwidth of  the optical interconnection medium, leads to high  performance ONoC infrastructure. In the proposed  router architecture, IMRs and AMRs are responsible for  multiplexing an optical flow with those transmitting on  the external ring, and EMRs, as the wavelength  selective filters, are responsible for demultiplexing an  optical flow (targeted to the corresponding node) from  those traversing the external ring. These extra microring  resonators are switched on in case of path contention,  otherwise they allow light to pass through without  being affected. In other words, in the absence of  contending paths,  routing of optical packets  is  accomplished by four primary microring resonators in  BPSB. Efficiency of the proposed router architecture is  illustrated in Fig. 5. This figure shows a blocking  scenario in which three different optical paths (shown  by P1, P2, and P3) contend for controlling the same  microring resonator switch, S4. It is worth noting that  the optical flow represented by P1 may be composed of  multiple data streams targeted to different nodes in the  network (other than the indicated node in the figure).  As depicted in Fig. 5.a, P2 and P3 need S4 to be  switched on while P1 requires its off state transmission.  Fig. 5.b  shows  the proposed photonic  router  architecture augmented with extra microring resonators.  As illustrated in this figure, properly switching AMRs  and EMRs leads to contention-free operation of the  network and allows data flow multiplexing on the  East/West optical waveguide.       Figure 4.  Proposed architecture for a) 0 (cid:3409) (cid:1861) (cid:3407) (cid:1840)/2  b) (cid:1840)/2 (cid:3409) (cid:1861) (cid:3407) (cid:1840)                   Figure 5. a) Blocking scenarios b) Contention-free operation of NCFOR        While implementing optical flow multiplexing on  the waveguides, CONoC architecture necessitates  microring resonators to be capable of filtering and  switching both WDM optical signals and multiplewavelength optical packets. Hence, NCFOR utilizes  compact microring switches capable of operating with  high performance over a large spectral bandwidth. As  discussed by Small et al. [13], free-spectral range (FSR)  of the microring resonator can be reduced by increasing  its size. Based on this property, comb-switching  technique has been proposed and simultaneous alloptical switching of 20 continuous-wave wavelength  channels has been achieved in a microring resonatorbased silicon broadband comb switch by Lee et al. [14].  Based on the proposed router architecture, FSRs of the  primary microring resonators, IMRs, and AMRs are  small since they must switch WDM optical flows over a  large spectral bandwidth. On the other hand, EMRs  only switch when the wavelengths of the optical signal  overlap the dedicated wavelengths of the corresponding  node. Therefore, their FSR is larger than that of IMRs  and AMRs.  Since CONoC benefits from wavelength routing  method and optical flows multiplexing, number of  distinct wavelengths required in the network depends  on the maximum number of multiplexed optical flows  on a waveguide (referred to as maximum degree of  multiplexing). Since we have assumed that only one  data stream can be targeted to a specific node at each  time, it is straightforward to show that maximum degree  of multiplexing equals to the network diameter. In an  of nodes, equals to (cid:1729)(cid:1840)/4(cid:1730) . Therefore, (cid:1729)(cid:1840)/4(cid:1730) distinct  N-node Spidergon ONoC implementing Across-first  routing algorithm, network diameter, which is defined  as the maximum shortest path length between any pair  in the case of  (cid:1861) (cid:1568) (cid:1862) (cid:4666)(cid:1865)(cid:1867)(cid:1856) (cid:1729)(cid:1840)/4(cid:1730) (cid:4667). Concerning multiplewavelength optical packets transmission, (cid:1729)(cid:1840)/4(cid:1730) distinct  wavelengths guarantee contention-free operation of the  CONoC if the same wavelength is devoted to Ni and Nj  sets of wavelength are required.   Contention-free operation of the ONoC proposed by  Briere et al. [6] requires one wavelength to be  associated to each distinct physical path where the  number of distinct paths is quadratically proportional  with the node count. CONoC however, limits the  number of distinct wavelengths to the quarter of the  node counts in the optical network while retaining the  contention-free functionality of the ONoC.  2.3. Data transmission  While electronic NoCs can simply provide header  processing and data buffering functions, no buffer or  all-optical processing unit can be implemented in a  chip-scale area [9]. Based on  these  limitations,  Shacham et al. [2] have employed two layers of on-chip  interconnects including optical and electrical networks.  In this network, every transmitted photonic message is  preceded by an electronic path-setup control packet,  which is routed in the electronic network, reserving  optical resources for the message transmission. Since  the proposed ONoC is not contention-free, path-setup  packets may be dropped at intermediate routers due to  congestion. In this case, a path-blocked packet is  transmitted by the dropping router to the source,  canceling the reservation carried out by the path-setup  packet.   Unlike the ONoC architecture proposed by Shacham  et al. [2], CONoC prevents blocking scenarios at  intermediate routers, and path-setup packets are not  dropped during path reservation phase. Consequently,  there is no need to transmit path-blocked packets in the  case of contention. But due to receiver's inability to  simultaneously receive multiple data streams from  different transmitters, optical data transmission may be  delayed while the corresponding destination node is  receiving another optical message. For resolving this  problem, we have proposed a suitable  request  retransmission scheme. In addition to proposing a  contention-free  reservation phase,  the proposed  architecture benefits from optical path reconfiguration,  as discussed in more details later, by utilizing extra  microring resonators (i.e. AMRs, EMRs, and IMRs) of  the proposed router architecture. All these privileges  lead to much simpler electrical routers, reduced setup  latency, and higher transmission capacity compared to  the ONoC proposed by Shacham et al. [2]. As follows,  we will describe data transmission phases in CONoC in  more details.  2.3.1. Path reservation. Path reservation phase for a  given source-destination pair  in  the network  is  preceded by inspecting whether the corresponding  destination node is receiving another optical data  stream. For this purpose, a destination-checking packet  is routed by the electrical network to the corresponding  destination node, without reserving optical resources at  the intermediate nodes. In the case that another optical  message  is  received by  the destination node,  destination-checking packet is sent back to the source  and informs it to attempt request transmission again  (retransmission scheme is described in more details  later). Otherwise,  path-reservation  packet  is  transmitted from the destination node to the source,  and on its path reserves proper optical resources. In the  case that two path-reservation packets arrive at the  same node simultaneously, one of them is randomly  chosen to reserve the path. This reservation strategy  eliminates the need for resource reservation canceling  in the case of busy destination.   Since switching of OAD elements increases optical  loss, extra microring resonators of NCFOR are only  switched on in the case of path contention, while in  other cases, they allow light to pass through without  being affected. This means that while reserving optical  resources, optical routing through primary microring  resonators of the BPSB is preferred. Based on this  approach, selected microring  resonator(s)  to be  switched on in each of the blocking scenarios (from  Table 1) are listed in Table 2. In each scenario, Switch1  and Switch2 are switched on to remove the path  blocking and route optical data streams through Path1  and Path2, respectively. While Table 2 depicts  microring reservations in the case of congestion, path  reservation is obvious in the absence of path contention.  In some situations, there exists more than one solution  Ni    (cid:1840)/2 (cid:3409) (cid:1861) (cid:3407) (cid:1840) can be accomplished by switching on  for simultaneous routing of contending paths. For  example, concurrent routing of T4(cid:198)R3 and T1(cid:198)R4 for  either pair of S7-S2 or S1-S9 microring resonators.  Regarding preference of optical routing through  primary resonators, an optical path may be reconfigured  during data transmission to retain the contention-free  functionality of the network. This reconfiguration is  inevitable to prevent path blocking while another  optical transmission request tries to reserve the path. As  an example, consider the case that an optical path from  South to East is guided in a photonic router, optical data  packets are transmitted through the path, and so, S1 is  switched on. Now assume that a new path-reservation  packet intends to reserve an optical path from West to  East in the same router which needs S1 to be switched  off. The proposed reservation strategy allows the new  reservation request to reconfigure the existing path(s)  such that all optical data streams can be routed through  the network without any contention. In this case,  previously guided path (South(cid:198)East) is reconfigured  by  the new path-reservation packet  (reserving  West(cid:198)East). Consequently, S1 is switched off and  optical packets on South port are routed through S7 to  East port in the photonic router.  In CONoC, a proper request retransmission scheme  is utilized to postpone optical data transmission while  the corresponding destination node is receiving another  optical message. In this case, transmitter waits for a  period of waiting interval and then reattempts to reserve  the path. The duration of waiting interval significantly  influences  the path-setup  latency and also  total  performance of the CONoC. Too small waiting interval  increases number of request retransmissions, while too  large interval cannot utilize the recently released  destination nodes. Consequently, optimizing waiting  interval is a prominent task for reducing path-setup  latency. In the proposed ONoC architecture, waiting  interval is estimated in each router using the number of  Ni  (cid:2170)(cid:2779) (cid:3409) (cid:2191) (cid:3407) (cid:1840)  (cid:2777) (cid:3409) (cid:2191) (cid:3407) (cid:2170)(cid:2779)  Table 2 Switch reservations to overcome blocking scenarios Path 1 Switch1  Path 2  Switch2 S7 T1(cid:198)R3  S9  T1(cid:198)R3  S9(S1)  T4(cid:198)R3  S1(S7) S6  T1(cid:198)R3  S5  T3(cid:198)R1  S8  T3(cid:198)R1  S4(S10)  T4(cid:198)R1  S8 S10  T3(cid:198)R1  S4(S10)  T2(cid:198)R1  S5(S3) S4(S10)  T4(cid:198)R3  S7(S1) S5(S3)  T4(cid:198)R3  S1(S7) T1(cid:198)R3  T1(cid:198)R3  T1(cid:198)R3  T2(cid:198)R3  S6 T3(cid:198)R1  T2(cid:198)R1  S5(S3) T3(cid:198)R1  T3(cid:198)R1  T1(cid:198)R4  S2(S9) T1(cid:198)R4  S2(S9) T4(cid:198)R3  S1(S7) T4(cid:198)R3 T1(cid:198)R2 T1(cid:198)R2 T2(cid:198)R3 T2(cid:198)R1 T4(cid:198)R1 T3(cid:198)R2 T3(cid:198)R2 T3(cid:198)R2 T3(cid:198)R2 T2(cid:198)R1 T4(cid:198)R3 T2(cid:198)R3 T1(cid:198)R4 T1(cid:198)R4 T2(cid:198)R1 T3(cid:198)R4 T3(cid:198)R4 T4(cid:198)R1 T4(cid:198)R3 T2(cid:198)R1 T2(cid:198)R1 S7  S6  S9  S1(S9)  S5  S3(S10)  S10  S8  S7(S1)  S5(S3)  S5(S3)  unsuccessful path-reservation requests for already  received data from IP block and also average and  maximum  numbers  of  unsuccessful  request  transmissions for different optical data streams in this  router till now. Details of the implemented function are  eliminated for brevity.   2.3.2. Optical data transmission. Once a pathreservation packet completes  its  journey  from  destination  to source router, a chain of silicon  microring resonator switches is ready to route the  optical data stream from source to destination. Hence,  the optical message can be transmitted through the  optical waveguides and switches without buffering.   2.3.3. Path teardown. Complete data reception at the  destination router can be checked by examining the  message size or an end tag encapsulated in the  transmitted optical data. After  the optical data  transmission is completed, a path-teardown packet is  sent by the data receiver to the transmitter router to  free up  the path. As discussed before, path  reconfiguration strategy allows the new reservation  request to reconfigure the existing path(s). Hence,  while  tearing  down  the  path,  carried  out  reconfigurations should be considered.  3. Simulation environment  For functional validation and design exploration of  the CONoC, we have developed a behavioral simulator  and evaluated the proposed architecture. As follows, we  describe general specifications of the implemented  simulator.     We developed a parameterized Spidergon CONoC  simulator based on OMNeT++ simulation framework  [15]. As a case study, we analyze a 36-node topology  which is arranged in a 6×6 planar layout and will be  built in a future 22nm CMOS process technology.  Assuming chip size of 1.8cm×1.8cm [16], optical and  electrical  links between adjacent nodes have an  approximate length of 2mm. We assume a propagation  velocity of 15.4ps/mm in a silicon waveguide for the  optical signals [17] and 131ps/mm in an optimally  repeated wire at 22nm for the electronic signals [18].  Consequently, optical and electrical delays between  adjacent routers are 30.8ps and 262ps, respectively. We  also assume length of 1mm for local links, which lead  to delay value of 15.4ps and 131ps on the optical and  electrical links between each router and its associated  IP block, respectively. For an error-free transmission  through an on-chip network, a bit error rate (BER) of  10-15 is assumed on both optical and electrical links  [19]. Assuming 5GHz clock [2], electrical processing in  each router (for path reservation and teardown) is  considered to be one clock cycle, i.e. 200ps. Moreover,  8-bit electrical  links are utilized for transmitting  electrical signals through the network.   Traffic pattern in an on-chip network is defined by  packet size distribution, packet injection process, and  distribution of the packet destination. In our developed  CONoC, packet sizes are determined uniformly in a  predefined range. Shacham et al. [2] have argued that  for small block sizes in a photonic NoC, the overall  latency is dominated by the path-setup overhead which  is greater than the serialization latency, while for large  blocks,  the  increased serialization and contention  latencies overshadow the gain in bandwidth. Therefore,  the optimum packet sizes in our case study are  determined uniformly in the range of [1KB, 4KB]. We  model inter-message gap as a Poisson random variable  with the parameter of µ. Large values of this parameter  impose light traffic to the network while reducing its  value leads to waiting time increment and performance  degradation. Finally, in our case study, each node sends  its messages to any other node in the network with  equal probability;  i.e. destination distribution  is  assumed to be uniform which can be exchanged with  any other distribution function by a slight modification.    As follows, we briefly discuss wavelength selection  approach in CONoC. Assuming 2 Tbps peak bandwidth  per port, with maximum degree of multiplexing equal to  nine (i.e. N/4), each optical data stream should be  modulated on six distinct wavelengths at the rate of 40  Gbps. Xu et al. [20] have shown that optical interchannel crosstalk is negligible with a channel spacing  of 1.3 nm [20]. Consequently, 54 distinct wavelengths  with channel spacing of 1.3 nm are required to achieve  maximum bandwidth of two Tbps.   4. Experimental results and analysis   In this section, we report a series of simulation-based  experimental results and investigate the feasibility and  competence of CONoC. Moreover, we intend to  compare  the experimental  results obtained  from  CONoC with those of traditional NoCs at future 22nm  technology node. Based on the predictions made by  Shacham et al. [2], future electrical NoCs (ENoCs) will  route packets on 168-bit parallel links under 5-GHz  clock frequency. Router processing delay is assumed to  be 600ps, or three cycle times of a 5GHz clock [2].  Moreover, we suppose propagation velocity of  131ps/mm in an optimally repeated wire at 22nm  technology [18] for Spidergon ENoC.   4.1. Architecture efficiency   Simulation results presented in this section confirm  the effectiveness of six extra microring switches. Fig.  6.a depicts total number of multiplexed optical flows  transmitting through four primary microring resonators  of BPSB and all resonator switches of NCFOR for  varying values of µ. This figure represents maximum  values over the simulation time for all topology nodes.  As shown in this figure, while extra switches increase  utilization percentage (percentage of the total use of  bandwidth for a waveguide based on a simulation  period) of the whole router, they are switched off most  of the times.   Activating microring resonator switches imposes  drop-port insertion loss (IL), which is calculated as the  difference between the reference waveguide output  power and  the peak drop-port power for each  wavelength channel. Hence, keeping small number of  activated microring  resonators while  increasing  maximum degree of multiplexing should be addressed.  Fig. 6.b shows average number of microring switches in  the ON state for all nodes of the network for varying  values of µ. As depicted in this figure, although we  have added some extra microring resonators, the  proposed photonic router preserves small number of  activated switches which leads to inconsiderable dropport power losses.  4.2. Delay analysis   In the case of optical interconnects, there are four  contributions to the data transmission latency arising  from modulator, propagation delay in the waveguide,  photo-detector, and Transimpedance Amplifier (TIA).  Moreover, reservation and teardown packets should be  routed through electrical network in CONoC for each  transmitted optical message. Hence, some additional  latency arises from electrical packet processing at the  routers and propagation delay in the electrical wires.   Table 3 lists main contributions to the data transmission  19 14 9 4 Primary MRs All MRs  0.7 0.5 0.3 0.1 0 10 20 30 40 50 0 10 20 30 40 50 Poisson Parameter (us) Poisson Parameter (us)       Figure 6. a) Max no. of flows on MRs      b) Avg. no. of activated MRs   latency in the CONoC. Assuming 2mm links between  adjacent routers, optical and electrical delays per hop  and processing delay in each router are reported from  previous section. Remaining parameters have been  predicted by Chen et al. [21] for 1cm optical path at the  future 22nm CMOS process technology. For a 36-node  Spidergon  topology,  average network distance  approximately equals to 5.5 (calculated from the  equation reported by Bononi et al. [8]), which is  defined as the average path length of all different paths  in the network. Assuming 2mm optical links, this leads  to about 1cm optical path from source to the destination  node. Consequently, predictions made by Chen et al.  [21] can be applied to our case study.   Based on delay parameters from Table 3, simulation  results for optical and total transmission latencies of the  data streams for varying values of µ are depicted in Fig.  7. Total data transmission latency consists of path-setup  latency, photonic data transmission delay, and pathteardown latency. As shown in this figure, for low  traffic conditions, total latency is dominated by data  transmission  through optical waveguides. This  domination is a result of packet size optimization  (discussed in the previous section). While for high  traffic offered to the network, setup-latency dominates  total data transmission latency and degrades total  performance of the CONoC.   Fig. 8.a shows average value of waiting intervals as  the percentage of total data transmission latency for  varying values of µ. Fig. 8.b depicts  request  retransmission probability. As shown by these figures,  for low traffics, request retransmission scheme leads to  inconsiderable performance degradation, while this  impact remains  tolerable for high  traffic  loads.  Consequently,  the experimental results verify  the  effectiveness of the proposed method for calculating  waiting interval in the case of busy destination node.  Table 3 Delay contributions to transmission latency in CONoC  Parameter  Value (ps) Optical  Electrical  Modulator driver  Modulator  Photo-detector  TIA  Waveguide delay  Router processing  Wire delay  9.5 14.3 0.2 4 30.8 200 262 Finally, Fig. 9 compares average data transmission  delay through CONoC with that of Spidergon ENoC for  varying values of µ. As depicted in this figure,  traditional NoC compared to CONoC, leads to about  five times bigger average latency for low and moderate  traffics and approximately equal latency for high  traffics which emphasizes the inefficiency of the  electrical interconnects for future CMPs.  4.3. Power analysis   Total loss in any optical link is the sum of losses (in  dB) of all optical components [19]:    (cid:1842)(cid:3013)(cid:3036)(cid:3041)(cid:3038) (cid:3404) (cid:1842)(cid:3004)(cid:3023) (cid:3397) (cid:1842)(cid:3024) (cid:3397) (cid:1842)(cid:3003) (cid:3397) (cid:1842)(cid:3026) (cid:3397) (cid:1842)(cid:3004)(cid:3019)     (1)  where, PCV is the coupling coefficient between the  photonic source and optical waveguide, PW is the  waveguide propagation loss per unit distance, PB is the  bending loss, PY is the Y-coupler loss (not present in  the proposed ONoC), and PCR is the coupling loss from  the waveguide to the optical receiver. Since we have  supposed an off-chip  light source, PCV  is zero.  Propagation loss of the optical waveguides is set to  0.51dB/cm [5]. The authors in [22] have shown a 90◦  bend with submicrometer dimensions and losses of less  than 1% per bend which leads to negligible bending  loss. Finally, in this case study, the coupling efficiency  is assumed to be 0.6 dB [23]. In addition to optical  power losses in the waveguides, switching elements of  the CONoC significantly impact total power dissipated  in the network. The power consumed in a microring  resonator switch in the ON state, when the multiwavelength message  is forced  to  turn [24],  is  approximately 10mW, while there is no dissipation in  the OFF state. Moreover, an activated microring  resonator  leads  to  average drop-port  IL of  approximately 1.4 dB [14], while its through-port IL is  negligible. Switching power of about 32 nW has been  ENoC_Total Delay ONoC_Total Delay 6. E+6 6. E+5 6. E+4 0 10 20 30 40 50 0 10 20 30 40 50 6. E+6 6. E+5 6. E+4 ONoC_Optical Delay ONoC_Total Delay 10 8 6 4 2 0 0 10 20 30 40 50 0.25 0.2 0.15 0.1 0.05 0 0 10 20 30 40 50 Poisson Parameter (us) Poisson Parameter (us) Poisson Parameter (us) Poisson Parameter (us)                   Figure 7.  Delay in ONoC  (ps)          Figure 8. a) Waiting Time %    b) Retransmission probability        Figure 9. Delay of ONoC and ENoC (ps)                 Table 4 Main contributions to electrical power of CONoC  Parameter  Value  Unit  1  mW/mm 0.6  mW/bit  1.8  mW/bit  1.75  mW/bit  PLink  PBuffer  PCrossbar  PStatic  40 35 30 25 ONoC_Electrical Power ONoC_Total Power ENoC_Total Power 5 0.5 0.05 ONoC_Optical Energy ONoC_Total Energy ENoC_Total Energy reported for microring resonator arrays in BPSB [10]  which can be discarded. Besides optical losses in  waveguides and microring switches, electrical power is  consumed in electro-optical converters. Chen et al. [21]  have predicted that power consumed by the transmitter  and receiver circuits for 1cm optical path at the future  22nm technology are 5 mW and 0.3 mW, respectively.   In addition to power dissipation in optical devices,  routing and processing path-reservation and pathteardown packets in the network impose additional  electrical power  losses. The  estimated power  consumption per unit  length for delay-optimized  electrical interconnects with optimal repeaters is of the  order of 1 mW/mm [25]. Shacham et al. [2] have  reported values of the energy spent in flit processing  operations (neglecting arbiter energy). Based on 5-GHz  clock frequency, main contributions to the electrical  power consumption for data transmission through  CONoC are listed in Table 4.   Based on the above parameters, we calculate power  dissipated for transmitting an optical message through  CONoC as follows:   (cid:1842)(cid:3016)(cid:3043)(cid:3047)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3404) (cid:4666)(cid:1842)(cid:3014)(cid:3019),(cid:3016)(cid:3015) (cid:3397) (cid:1842)(cid:3014)(cid:3019) ,(cid:3031)(cid:3045)(cid:3042)(cid:3043)(cid:2879)(cid:3043)(cid:3042)(cid:3045)(cid:3047) (cid:3010)(cid:3013) (cid:4667) (cid:3400) (cid:1840)(cid:3016)(cid:3015) (cid:3397) (cid:1842)(cid:3019)(cid:3032)(cid:3030)(cid:3032)(cid:3036)(cid:3049)(cid:3032)(cid:3045)         (cid:3397)(cid:1842)(cid:3024) (cid:3400) (cid:1838) (cid:3400) (cid:1834)(cid:1867)(cid:1868)(cid:1829)(cid:1867)(cid:1873)(cid:1866)(cid:1872) (cid:3397) (cid:1842)(cid:3004)(cid:3019) (cid:3397) (cid:1842)(cid:3021)(cid:3045)(cid:3028)(cid:3041)(cid:3046)(cid:3040)(cid:3036)(cid:3047)(cid:3047)(cid:3032)(cid:3045)     (2)  where, NON is the number of microring resonators in the  ON state passed by the optical message, L is the link  length between adjacent nodes which is approximately  2mm, and HopCount is the number of hops passed by  the optical message. For a BER of 10-15, the smallest  power required by the receiver is -22.3dBm [19].  Hence, minimal power required for optical data  transmission equals to (cid:1842)(cid:3017)(cid:3028)(cid:3030)(cid:3038)(cid:3032)(cid:3047) (cid:3404) (cid:3435)(cid:3398)22(cid:1856)(cid:1828)(cid:1865) (cid:3397) (cid:1842)(cid:3016)(cid:3043)(cid:3047)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3439) .        (cid:3400) (cid:3435)  (cid:1842)(cid:3013)(cid:3036)(cid:3041)(cid:3038) (cid:3400) (cid:1838) (cid:3397) (cid:1842)(cid:3003)(cid:3048)(cid:3033)(cid:3033)(cid:3032)(cid:3045) (cid:3397) (cid:1842)(cid:3004)(cid:3045)(cid:3042)(cid:3046)(cid:3046)(cid:3029)(cid:3028)(cid:3045) (cid:3397) (cid:1842)(cid:3020)(cid:3047)(cid:3028)(cid:3047)(cid:3036)(cid:3030) (cid:3439)    (3)  While computing optical power, appropriate power unit  conversion should be considered. Electrical power  consumed by path-reservation and tear-down packets is  computed as follows:   (cid:1842)(cid:3006)(cid:3039)(cid:3032)(cid:3030)(cid:3047)(cid:3045)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3404) 2 (cid:3400) (cid:1828)(cid:1873)(cid:1871)(cid:1849)(cid:1861)(cid:1856)(cid:1872)(cid:1860) (cid:3400) (cid:1834)(cid:1867)(cid:1868)(cid:1829)(cid:1867)(cid:1873)(cid:1866)(cid:1872)    where, BusWidth equals to 8. For simplicity, we have  assumed that power consumed by path-reservation  packet is equal to that of the path-teardown packet.  Taking into account both of optical data transmission  losses and electrical power consumed by path  reservation and  teardown packets,  total power  consumption for transmitting a data message through  CONoC is computed as follows:   (cid:1842)(cid:3021)(cid:3042)(cid:3047)(cid:3028)(cid:3039) (cid:3404) (cid:1842)(cid:3017)(cid:3028)(cid:3030)(cid:3038)(cid:3032)(cid:3047) (cid:3397) (cid:1842)(cid:3006)(cid:3039)(cid:3032)(cid:3030)(cid:3047)(cid:3045)(cid:3036)(cid:3030)(cid:3028)(cid:3039)       (4)  0 10 20 Poisson Parameter (us) Poisson Parameter (us)      Figure 10. Power consumption (dBm)   Figure 11. Energy spent (uJ)  30 40 50 0 10 20 30 40 50 Based on previously discussed power parameters, in  CONoC simulator, values of PPacket, PElectrical, and PTotal  are calculated for each optical message received at the  destination node. Fig. 10 depicts average values of  PElectrical and PTotal (in dBm) for varying values of µ. As  shown in this figure, reservation and teardown phases  dissipate nearly half of the total power consumed in  CONoC (in mW). Hence, by eliminating electrical  transactions in an all-optical on-chip network, total  power dissipation may be reduced by a factor of two.  Fig. 10 also compares average power consumption for  data  transmission  through CONoC with  that of  Spidergon ENoC for varying values of µ. As depicted  in this figure, traditional NoC as compared to the  proposed photonic NoC, leads to approximately eight  times larger average power consumption (in mW).  Considering aggregated power consumption of all  packets in the network, Spidergon ENoC cannot meet  the package power constraints predicted by ITRS [16].   4.4. Energy analysis   For transmitting optical messages through CONoC,  electrical energy is consumed by path reservation and  teardown packets while optical energy is dissipated by  optical data routing. These energies are calculated for  each packet at the destination node, and we have:    (cid:1831)(cid:3021)(cid:3042)(cid:3047)(cid:3028)(cid:3039) (cid:3404) (cid:1831)(cid:3016)(cid:3043)(cid:3047)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3397) (cid:1831)(cid:3006)(cid:3039)(cid:3032)(cid:3030)(cid:3047)(cid:3045)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3404) (cid:1830)(cid:1857)(cid:1864)(cid:1853)(cid:1877)(cid:3016)(cid:3043)(cid:3047)(cid:3036)(cid:3030)(cid:3028)(cid:3039) (cid:3400) (cid:1842)(cid:3021)(cid:3042)(cid:3047)(cid:3028)(cid:3039)                 (cid:3397) (cid:3435)(cid:1830)(cid:1857)(cid:1864)(cid:1853)(cid:1877)(cid:3006)(cid:3039)(cid:3032)(cid:3030)(cid:3047)(cid:3045)(cid:3036)(cid:3030)(cid:3028)(cid:3039) – (cid:1849)(cid:1853)(cid:1861)(cid:1872)(cid:1861)(cid:1866)(cid:1859)(cid:1846)(cid:1861)(cid:1865)(cid:1857)(cid:3439) (cid:3400) (cid:1842)(cid:3006)(cid:3039)(cid:3032)(cid:3030)(cid:3047)(cid:3045)(cid:3036)(cid:3030)(cid:3028)(cid:3039)   (5)  where, WaitingTime is the sum of waiting intervals  computed by request retransmission scheme for each  packet. It is worth noting that during these intervals no  more power is consumed. Fig. 11 compares average  values of EOptical and ETotal (in µJ) with that of Spidergon  ENoC for varying values of µ. As shown in this figure,  energy spent in optical devices for data transmitting  dominates the total energy dissipated in CONoC. On  the other hand, average energy spent by data  transmission through traditional NoC is approximately  38 times larger than that of the proposed photonic NoC.   4.5. Area analysis   One of the significant advantages of the microring  crossbar switch proposed by Xu et al. [10] is the  compact device structure enabling an overall ~100μm ×  ~100μm foot-print for BPSB. Based on the microring               radius of 5 µm reported by Xu et al. [26], and small  bending radius of a few micrometers [27], total area  consumed by NCFOR is estimated to be about 180μm ×  180μm. Comparing the proposed photonic router with  the one introduced by Shacham et al. [9], total area is  nearly reduced by a factor of eight.   5. Conclusions and future work  The proposed on-chip optical infrastructure, which is  referred to as CONoC, enables low power dissipation,  high bandwidth and contention free routing of data  streams using WDM, wavelength routing techniques,  and path reconfiguration. The proposed architecture  benefits from simple electrical transactions, reduced  setup latency, and high transmission capacity. Optical  NoC proposed in this paper facilitates the development  of an all-optical on-chip network and alleviates the role  of electrical NoC.  According  to  the discussed propriety of  the  Spidergon topology for optical NoCs, we designed our  ONoC on top of this topology. Moreover, we developed  an event-driven behavioral simulator for CONoC.  Performing a series of simulation-based experiments,  we studied the efficiency of CONoC along with its  power and energy consumption and data transmission  delay, and finally, we analyzed its area requirements.   The only limitation of the proposed ONoC is the  receiver's  inability for simultaneous optical data  reception from different transmitters. For resolving this  problem, we proposed a proper request retransmission  scheme. We are going to enhance the presented  architecture in future such that simultaneous packets  can be sent from the same transmitter, and multiple  packets can be received from different sources at the  same destination.   6. "
Packet-level static timing analysis for NoCs.,"Networks-on-chip (NoCs) are used in a growing number of SoCs and multi-core processors, increasing the need for accurate and efficient modeling to aid the design of these highly-integrated systems. Towards this modeling goal, we present a methodology for packet-level static timing analysis in NoCs. Our methodology enables quick and accurate gauging of the performance parameters of a virtual-channel wormhole NoC without using simulation techniques and supports any topology, link capacities, and buffer depths. It provides per-flow analysis that is orders-of-magnitude faster than simulation while being both significantly more accurate and more complete than prior static modeling techniques. Our methodology is inspired by models of industrial flow-lines. Using a carefully derived and reduced Markov chain, the model can statically represent the dynamic network state and closely estimate the average latency of each flow. Use of the model in a placement optimization problem is shown as an example application of the method.","Packet-Level Static Timing Analysis for NoCs Evgeni Krimer1,2 , Mattan Erez1 , Isaac Keslassy2 , Avinoam Kolodny2 , Isask’har Walter2 1Department of Electrical and Computer Engineering, University of Texas at Austin 2Department of Electrical Engineering, Technion — Israel Institute of Technology {krimer, mattan.erez}@mail.utexas.edu, {isaac@ee, kolodny@ee, zigi@tx}.technion.ac.il Networks-on-chip (NoCs) are used in a growing number of SoCs and multi-core processors, increasing the need for accurate and efﬁcient modeling to aid the design of these highly-integrated systems. Towards this modeling goal, we present a methodology for packet-level static timing analysis in NoCs. Our methodology enables quick and accurate gauging of the performance parameters of a virtual-channel wormhole NoC without using simulation techniques and supports any topology, link capacities, and buffer depths. It provides per-ﬂow analysis that is orders-of-magnitude faster than simulation while being both signiﬁcantly more accurate and more complete than prior static modeling techniques. Our methodology is inspired by models of industrial ﬂow-lines. Using a carefully derived and reduced Markov chain, the model can statically represent the dynamic network state and closely estimate the average latency of each ﬂow. Use of the model in a placement optimization problem is shown as an example application of the method. Our goal is to derive a rigorous delay model for packet-level per-ﬂow static timing analysis (STA) for NoC-based SoCs. The motivation for a per-ﬂow STA technique is to enable a range of design optimizations that can rely on accurate and fast network analysis. Methods such as module placement and resource allocation require a large number of iterations, and thus the evaluation of network performance within each iteration must be very efﬁcient. Until now, an accurate and complete modeling of advanced NoCs has only been possible with detailed and time-consuming simulations. Our breakthrough is inpired by the work in manufacturing operational research [1], and we draw a parallel between ﬂowlines with malfunctioning machines and congestion experienced by a NoC ﬂow. Much of the prior work on analytical delay modeling in wormhole-enabled networks approximates the mean delay of packets in the entire system rather than estimating the delay of each source-destination ﬂow separately. Such gross approximations are often inadequate, and in such cases cannot be used in the NoC design process to efﬁciently optimize the allocation of resources. In addition, while state-of-the-art NoC architectures multiplex multiple packets on the network links using virtual channels, most existing analytical models do not support virtual channels. To the best of our knowledge, the only published model handling VCs and calculating per ﬂow estimations is a heuristic delay model (HDM) introduced in [2]. We show how our new 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  model is more accurate than HDM and demonstrate the importance of higher-ﬁdelity with a placement optimization. Our model supports an arbitrary NoC topology with wormhole routing and virtual channels. The capacity of each link in the network can be conﬁgured separately, as well as the capacities of the buffers in each virtual channel. In this initial work, we assume that all packets have a ﬁxed length, and that the packet arrival times at the injection port of each node can be modeled by a Poisson random process. We further assume that there is no blocking in the network due to a lack of virtual channels, and that the destination node can always eject packets from the network. Finally, we place no restriction on the routing algorithm except that it be deterministic. Our technique follows three main steps: We focus on the NoC service for a particular ﬂow of interest, which we generically call ﬂow X , and model it using a Markov chain. The Markov chain represents the network state of the routers and buffers on the path of ﬂow X , as well as the impact of interfering ﬂows, i.e., those ﬂows that share at least one link with ﬂow X . Next, we derive the ﬂit propagation characteristics by computing the stationary distribution of the Markov chain. Finally, use the derived properties and standard analysis of M/G/1 queues to calculate the expected packet delay and the throughput of ﬂow X . A possible use of the analytical delay model is to estimate network and ﬂow properties within the inner-loop of a module placement optimization algorithm. Our model can quickly compute delay with high accuracy and we demonstrate that, contrary to HDM, it also reﬂects the change in delay as a result of varying module placement. Hence, our model can be used to predict, and correctly and efﬁciently choose between multiple placement options. Our new model allows for a per-ﬂow STA that is orders-ofmagnitude faster than simulation. Ultimately, the objective is for this packet-level STA model to be used in the inner-loop of NoC optimization tools — and become the packet-level equivalent of gate-level critical path analysis utilized in CAD tools. "
Exploring concentration and channel slicing in on-chip network router.,"Sharing on-chip network resources efficiently is critical in the design of a cost-efficient network on-chip (NoC). Concentration has been proposed for on-chip networks but the trade-off in concentration implementation and performance has not been well understood. In this paper, we describe cost-efficient implementations of concentration and show how external concentration provides a significant reduction in complexity (47% and 36% reduction in area and energy, respectively) compared to previous assumed integrated (high-radix) concentration while degrading overall performance by only 10%. Hybrid implementations of concentration is also presented which provide additional tradeoff between complexity and performance. To further reduce the cost of NoC, we describe how channel slicing can be used together with concentration. We propose virtual concentration which further reduces the complexity - saving area and energy by 69% and 32% compared to baseline mesh and 88% and 35% over baseline concentrated mesh.","Exploring Concentration and Channel Slicing in On-chip Network Router Prabhat Kumar, Yan Pan, John Kim† , Gokhan Memik, Alok Choudhary Northwestern University †KAIST 2145 Sheridan Road, Evanston, IL Daejeon, Korea {prabhat-kumar, panyan,memik,alok}@northwestern.edu jjk12@cs.kaist.ac.kr Abstract explored. Sharing on-chip network resources efﬁciently is critical in the design of a cost-efﬁcient network on-chip (NoC). Concentration has been proposed for on-chip networks but the trade-off in concentration implementation and performance has not been well understood. In this paper, we describe cost-efﬁcient implementations of concentration and show how external concentration provides a signiﬁcant reduction in complexity (47% and 36% reduction in area and energy, respectively) compared to previous assumed integrated (high-radix) concentration while degrading overall performance by only 10%. Hybrid implementations of concentration is also presented which provide additional tradeoff between complexity and performance. To further reduce the cost of NoC, we describe how channel slicing can be used together with concentration. We propose virtual concentration which further reduces the complexity – saving area and energy by 69% and 32% compared to baseline mesh and 88% and 35% over baseline concentrated mesh. 1 Introduction Increasing number of transistors in modern VLSI technology has increased the number of cores on a chip and is making chip multiprocessors (CMP) widely available. As a result, on-chip communication is becoming critical and requires efﬁcient Network-on-chip (NoC) designs where data are routed in packets on shared channels instead of dedicated buses [16, 19]. Designing cost-efﬁcient on-chip networks will require efﬁcient sharing of on-chip network resources such as buffers and wire bandwidth. In this paper, we explore two techniques, concentration and channel slicing [8], in on-chip networks to create a cost-efﬁcient on-chip network architecture. Concentration and slicing are well-known techniques in interconnection networks that have been adopted in different off-chip networks such as the Cray X1 [1] and the Cray BlackWidow network [24]. However, their use in on-chip networks have not been well Concentration in on-chip networks was ﬁrst proposed by Balfour and Dally [4]. Recently proposed on-chip network topologies such as the hybrid topology [10], ﬂattened butterﬂy [14], hierarchical ﬁreﬂy [22], and the multidrop express channels topology [12] have also used concentration. However, the trade-off in implementing concentration is not well understood and little work has been done to evaluate the optimal degree of resource sharing. Most prior work in on-chip network concentration have also assumed an increase in router radix to support concentration which increases the complexity and the cost of the on-chip network routers. In this work, we present an alternative external concentration which signiﬁcantly reduces concentration complexity and evaluate performance impact of alternative concentration. However, concentration can lead to performance degradation because of the channel sharing between multiple terminal nodes. As a result, we also explore the use of channel slicing to increase utilization of the wiring resources and increase performance. We propose how virtual concentration – where concentration and channel slicing are combined to further reduce the cost of the network – can be implemented. We hold the on-chip resources (buffers and bisection bandwidth) constant in our evaluation to provide a better understanding of the optimal resource utilization scheme. Our results show that using channel slicing factor of 4 can provide a concentrated mesh which matches the throughput of a conventional mesh while providing 69% and 32% reduction in area and energy cost, respectively. In this work, we focus on evaluating concentration and channel slicing on a 2D mesh topology since it is a commonly used topology for NoC [23, 26, 25] and maps well to a 2D VLSI planar layout. However, the techniques presented can be easily extended to other topologies which use concentration [12, 10, 14, 22]. In addition, other techniques to increase the performance of on-chip networks such as express virtual-channel ﬂow control [20], token ﬂow control [6], on-chip data compression [11], micro-architectural techniques [15, 9], and load-balanced routing algorithm [5], 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R (a) (b) (c) (d) Figure 1. 64-node on-chip network using (a) conventional 2D mesh, (b) concentrated mesh with C = 2 (Cx = 2, Cy = 1), (c) C = 4 (Cx = Cy = 2) and (d) C = 8 (Cx = 4, Cy = 2) can all be used in conjunction with the proposed techniques of concentration and channel slicing to reduce network cost. In summary, the contribution of the work includes: • Exploration of the performance implication of varied degrees of concentration. • Exploration of the performance and efﬁciency implication of integrated and external concentration. • Virtual concentration techniques, combining concentration with channel slicing, for efﬁcient resource sharing. The rest of the paper is organized as follows. In Section 2, we provide an overview of using concentration in on-chip networks and describe alternative concentration implementation. In Section 3, we describe how channel slicing can be implemented on a concentrated mesh topology. We describe the simulation setup in Section 4 and present the results and discussion in Section 5. Section 6 presents related work and we conclude in Section 7. 2 Concentration 2.1 Overview Conventional 2D mesh network does not implement any concentration as each terminal node is connected to a single router (Figure 1(a)). However, the routers and the channels can be shared to create a concentrated mesh (CMESH). The degree of concentration in a 2D mesh network can be characterized with the following parameters: • Cx : concentration in the x-dimension • Cy : concentration in the y -dimension • C : topology concentration (= Cx × Cy ) The total degree of concentration in the topology is the product of the concentration in the two dimensions. With these parameters, a conventional 2D mesh topology can be described as C = Cx = Cy = 1. As C increases and approaches √N where N is the network size, the resulting topology is a fully-connected or a crossbar network. Different values of C are shown in Figure 1. For example, with a concentration factor of 4 with Cx = 2 and Cy = 2 (Figure 1(c)), 2 terminal nodes in the x-dimension and 2 in the y-dimension share a single router resulting in C = 4 – four terminals sharing a single router and the inter-router channels connected to the router. The beneﬁts of using concentration include: • Sharing of resource (routers and channels): By combining two rows of channels into a single channel (e.g. Cy = 2), the x-dimension channels of CMESH can have 2× bandwidth compared to the channels of a mesh. • Reduction of network diameter: Concentration reduces the number of intermediate routers and thus, reduces the hop count and zero-load latency. • Reduction in the cost of local communication: All communication in a mesh network requires traversing at least two routers but communication with (C − 1) terminal nodes require accessing only a single router in a CMESH. However, concentration presents some disadvantages which include: • Router Complexity: Additional router ports increase router complexity such as switch and virtual channel allocators and leads to higher per-hop router latency. • Area: Crossbar area is quadratically proportional to data path width and wider channels adversely impact the crossbar area. • Performance: Increase in the number of ports creates higher probability of contention which can reduce performance. In this work, we explore alternative concentration implementations to create an efﬁcient on-chip network architecture that provides the beneﬁts of concentration yet achieves while minimizing the cost of implementing concentration. south north west east in south north west east out south north west east in 3 in 2 in 1 in 0 south north west east out3 out2 out1 out0 south north west east in south nor th west east out in3 in2 in1 in0 out3 out2 out1 out0 concentrator distributor in3 in2 in1 in0 south nor th west east south nor th west east out3 out2 out1 out0 (a) (b) (c ) (d) Figure 2. Router block diagram of (a) conventional 2D mesh topology and C = 4 CMESH router micro-architecture using (b) integrated concentration, (c) external concentration and (d) hybrid implementation using M 2D2. 2.2 Concentration Implementation Prior work on concentration in on-chip networks have implemented concentration by increasing the router radix [4, 12, 14]. We refer to this approach as integrated concentration as the concentrator is integrated into the router by increasing the router radix. Integrated concentration is shown in Figure 2(b) for C = 4 and results in a radix-8 router, compared to a radix-5 router for a conventional 2D mesh router (Figure 2(a)). In this work, we evaluate an alternative concentration implementation using external concentration (Figure 2(c)). The radix of the router is identical to MESH as terminal nodes connected to the router share a single input to the router with a concentrator (or a multiplexer). Similarly, there is only a single output of the switch and a distributor is added to the router output to route the packets to the appropriate terminal node. Thus, the distributor is the inverse of a concentrator. The main difference between integrated and external concentration is the amount of switch bandwidth – resulting in a trade-off between router micro-architecture complexity and performance. Integrated concentrator requires a high-radix implementation of on-chip network routers and increases router micro-architecture complexity such as the switch and virtual channel allocators and the crossbar. External concentration reduces the router complexity but can create additional contention with the external concentrator. 5x5 al locator req(south) req(north) req(west) req(in) req(east) grant(south) grant(nor th) grant(west) grant(in ) grant(east) req(in3) req(in2) req(in1) req(in0) Concentrator arbiter 5x5 al locator req(in ) req(east) req(west) req(nor th) req(south) grant( in) grant(east) grant(west) grant(nor th) grant(south) req(in0) req(in1) req(in2) req(in3) grant(conc ) 8x8 al locator req(south) req(nor th) req(west) req( in3) req(east) req( in2) req( in1) req( in0) grant(south) grant(nor th) grant(west) grant(in 3) grant(east) grant(in 2) grant(in 1) grant(in 0) (a) (b) ( c) Concentrator arbiter Figure 3. Allocator diagram of router with an (a) integrated concentrator and (b) external concentrator using serial allocator and (c) parallel allocator In addition, it requires an external input arbitration and if not done properly, can degrade the performance of an onchip network router. In addition to these two implementations, we also evaluate hybrid concentration implementations by varying the number of concentrators and distributors used. For example, in Figure 2(d), two concentrators and two distributors are used to create a radix-6 router with two terminal nodes sharing a single concentrator instead of 4 terminals as in Figure 2(c). To describe the hybrid implementation, we use M xDy notation where x describes the number of multiplexers (mux) or additional input ports added and y describes the number of demultiplexers (demux) or output ports added. The number of mux inputs and the demux outputs will be C/x and C/y respectively. When x or y equal C , the number of mux inputs or demux outputs will be one – thus, no additional mux or demux logic is needed and only the number of ports on the router is increased. Using this notation, integrated concentration shown in Figure 2(b) is M 4D4 while the external concentration is M 1D1. The x and y parameters do not have to be equal. For example, M 1D4 micro-architecture results in a single concentrator but provides output speedup while a M 4D1 microarchitecture provides input speedup. R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R (a) (b) Figure 4. Concentrated mesh topology with channel slicing (a) S = 2 and (b) S = 4. Each router is composed of S parallel router. 2.3 Parallel Arbitration A critical aspect of external concentration is the additional arbitration required at the input. Integrated concentration requires a single allocator for all the inputs (Figure 3(a)). However, two-stage allocation is needed for external concentration – an input arbitration among the inputs to the concentrator mux and the switch allocation. If these two arbitrations are done sequentially (Figure 3(b)), this not only adds router latency but also create unnecessary headof-line (HoL) blocking since the state of the router switch information is not used in the input MUX arbitration. For example, if in0 win the concentrator mux arbitration, it can cause HoL blocking as it waits for the output resource to become available. In order to avoid these problems, we implement a parallel arbitration scheme that parallelizes the input (concentrator) arbitration with the router switch arbitration (Figure 3(c)). If there is one or more packet among the injection port, a request is sent to the router switch arbitration while the input arbitration is done in parallel. Prior to asserting this request, the output resource is checked to determine the availability of the output virtual channel. This technique cannot be applied to the serial allocation in Figure 3(b) since it requires multiple pipeline stages. To ensure that there is no starvation, the priority pointer in the concentrator arbitration is not updated unless the output of the mux is also granted from the switch allocation similar to how pointers are updated in islip allocation algorithm [19]. 3 Channel Slicing Concentration results in inter-router channels being shared and results in inter-router channels that are wider than the conventional 2D mesh topology. However, wider channels result in poor channel utilization since some packets in on-chip network are small [10]. For example, while cache lines can exceed 512 – 1024 bits in width, request packets, control packets, or coherency messages are much narrower. In addition, recent work has shown that frequent data patterns exist in on-chip network trafﬁc which can be compressed into a narrower data-path [11]. terminal  nodes concentrated  router terminal  nodes concentrated  router R0 R1 R2 R3 (a) h s e m d e t a r t n e c n o c k r o w t e n R0 R1 R2 R3 (b) h s e m d e t a r t n e c n o c k r o w t e n Figure 5. Channel-sliced network using (a) full connection and (b) virtual concentration. To provide an efﬁcient usage of wire resource, we evaluate channel slicing [8] in on-chip networks. Channel slicing divides the wide data-path channel into multiple, narrower channels. Examples of a channel-sliced concentrated mesh is shown in Figure 4 for S = 2 and S = 4, where S is the slicing factor. The sliced network consists of S parallel concentrated mesh with each network consisting of channels that are reduced in width by 1/S compared to an unsliced (S = 1) network. Thus, each router in Figure 4 is composed of S parallel routers. Channel slicing results in higher serialization latency with narrower channels and results in increased zero-load latency for long packets. However, for short packets, there is no impact on zero-load latency and providing multiple channels between neighboring routers increases the utilization of the wiring resources. In implementing channel slicing on top of concentration, the C terminal nodes can be connected to S parallel routers as shown in Figure 5(a) with C = 4 and S = 4. Using external concentration described in Section 2 the router complexity can be reduced but wiring complexity becomes problematic with the wiring to the S routers from the C terminal nodes. This approach also requires careful load-balancing in selecting the slice as if not done properly, one or more of the slices can become the bottleneck. To overcome these problems, we introduce virtual concentration (Figure 5(b)). Each physical router in the network (R0 to R3) is only connected to a single terminal node to minimize wiring. Thus, each terminal node in the sliced network has access to its own dedicated router without sharing the input bandwidth. Although the individual routers no longer implement concentration, the collection of routers create virtual concentration. The outputs of the routers still need to be routed to all C terminal nodes to ensure that regardless of which slice the packet is injected into, the packet can be routed to its destination. This requires either an integrated distributor or an external distributor as described in Section 2. The different channel-sliced architecture can be described as S aRb, where a is the number of slices and b is the number of injecting nodes connected to each router. For example, the         Table 1. Architectural conﬁguration Table 2. Network Trafﬁc Pattern Evaluated Code Name # Slices MESH (C=1) CMESH (C=2) CMESH (C=8) S1R4MxDy S2R4MxDy S2R2MxDy S4R4MxDy S4R2MxDy S4R1MxDy 1 1 1 1 2 2 4 4 4 Inj  Node 1 2 8 4 4 2 4 2 1 Channel  Width 0.5w 0.5w 1w 1w 0.5w 0.5w 0.25w 0.25w 0.25w 2 2 Router Latency for  MxDy M1D1 M1D2 M1D4 Buffer  Depth 0.6x 1.0x 1.0x 0.75x M2D1 M2D2 M2D4 0.75x 1.0x 0.75x M4D1 M4D2 M4D4 1x 1.2x 3 3 3 2 3 2 3 architecture shown in Figure 5(a) is S 4R4 and virtual concentration in Figure 5(b) is described as S 4R1. 4 Evaluation Setup 4.1 Simulation Environment A cycle accurate network simulator is developed based on the booksim simulator [8, 4] and modiﬁed to represent the alternative architectures evaluated. Channel traversal time is modeled as 1 cycle between neighboring routers for baseline mesh topology. For CMESH topologies, the channel traversal time is modeled as Cx and Cy cycles in the respective dimensions. 1 For example, with C = 8 (Cx = 4, Cy = 2), the xdimension link latency is 4 cycles, while the y-dimension link latency is 2 cycles. Table 1 summarizes the architectural conﬁguration parameters. The alternative architectures are describe as S aRbM xDy with S aRb describing the channel slicing (Section 3) and M xDy describing the concentration (Section 2.2). With this notation, x ≤ b since the number of additional injections ports x can not exceed b. To provide a fair comparison, we hold the on-chip resources (wires and buffers) constant in comparing alternative architectures. We assume constant bisection bandwidth across the different architectures and hence, the channel width and ﬂit size vary based on number of bisection channels. We use w as the width of the ﬂit and channel for C = 8 architecture and the other architectures’ channel width are described in terms of w . However, to keep the bandwidth of all the channels constant for a given architecture, the bisection of C = 2 and C = 8 are different. This would results in an asymmetric router design. We also keep the total amount of storage constant. Thus, buffer depth varies depending on the number of ports/VCs needed as shown in Table 1. The router latency of high-radix switches is increased to account for additional complexity. We compare the alternative architectures using both latency/throughput and synthetic workload. For the latency/throughput comparisons, we plot the injection rate in 1 The performance of C = 8 will be worse than presented here since we do not consider the additional wire delay from the terminal nodes to the router which can not be physically adjacent to all 8 terminal nodes. MineBench Splash Traces apriori, hop, kmeans, scalparc_flex barnes, cholesky, lu, radix, water_spatial Synthetic Load Details Closed Loop Batch Job 10K requests/node, request and reply dependence Traffic Name Bitcomp Uniform NonUR Synthetic Traffic Patterns Details dest = bitwise-not (src) Uniform Random Traffic 75% 1-Hop Neighbor Traffic, 25% Uniform Random terms of w bits/cycle to provide a fair comparison of the alternative architectures. The synthetic workload models the memory coherence trafﬁc of a shared memory with each processor generating 10K remote memory requests. Once the requests are received, responses are generated and the total execution time is measured. We allow 8 outstanding requests per router to model the effect of MSHRs – thus, when 8 outstanding requests are injected into the network, new requests are blocked from entering the network until response packets are received. Traces from SPLASH2 [27] and MineBench [21] applications are also used in the evaluation. Traces are generated using GEMS [17] simulator with the Garnet [3] on-chip network model using a mesh topology, 4-stage pipeline router, and single cycle channel latency. Limited by space, selected representative results are presented in the following section. 5 Results and Discussion 5.1 Impact of Concentration Latency and saturation throughput are compared as concentration is increased from C = 1 (MESH) to C = 8 in Figure 6 for different trafﬁc patterns. With concentration, the per hop router latency increases but the overall zeroload latency decreases due to the reduction of average hop count. The decrease in the hop count is proportional to concentration and can be described with the following equation Havg = kx/Cx + ky /Cy 3 where kx , ky represent the dimension of the unconcentrated mesh in the x and the y dimensions. For uniform random trafﬁc, increasing concentration reduces zeroload latency by up to 10% when C = 4 and by up to 23% when C = 8. There is no change in the zero-load latency as C increases from 2 to 4 because the decrease in hop count is offset by the increase in per-hop latency. The saturation throughput of C = 4 matches that of C = 1 and exceeds the throughput of C = 2 and C = 8 by 79% (Figure 6). The throughput for C = 2 and C = 8 is degraded by the asymmetry of the network as described earlier in Section 4. For permutation trafﬁc such as bitcomp, C = 1 exceeds the throughput of C = 4 by 9.5% but for nonUR trafﬁc, C = 4 exceeds C = 1 by 30% as concentration exploits trafﬁc locality to increase the network throughput. ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϬϭϬϮϬϯϬϰϬϱϬϲϬ Ϭ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϬϱϭϬϭϱϮϬϮϱϯϬ Ϭ сϭ;D^,Ϳ сϮ ;ďͿŝƚĐŽŵƉdƌĂĨĨŝĐ Ϭ͘Ϭϱ /ŶũĞĐƚ ŝŽŶ ZĂƚĞ сϰ Ϭ͘ϭ сϴ Ϭ͘ϭϱ Figure 6. Performance comparison of different concentration ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϬϱϭϬϭϱϮϬϮϱϯϬϯϱϰϬ Ϭ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱϯϯ͘ϱϰϰ͘ϱϱ сϭ;D^,Ϳ сϮ сϰ Ϭ͘Ϭϱ ;ĂͿhŶ ŝĨŽƌŵZĂŶĚŽŵdƌĂĨĨŝĐ /Ŷ ũĞĐƚ ŝŽŶZĂƚĞ Ϭ͘ϭ Ϭ͘ϭϱ сϭ;D^,Ϳ ƵŶ ŝĨŽƌŵ Ϭ͘Ϯϱ сϴ сϮ сϴ Ϭ͘Ϯ сϰ ŶŽŶƵƌ ď ŝƚĐŽŵƉ Figure 7. Synthetic workload completion time comparison for different concentrations normalized to that of MESH with UR trafﬁc. For synthetic workload, MESH with no concentration leads to higher performance, exceeding the performance of C = 4 on UR trafﬁc by 22% and by 66% compared to C = 8 (Figure 7). Because of the bottleneck in ydimension, C = 2 and C = 8 results in poor performance. The performance of C = 1 also exceed C = 4 by 13% and 41% on nonUR and bitcomp trafﬁc, respectively. With the reduced number of channels as C increases, more contention occurs in the network and results in lower performance. Although C = 1 (MESH) provides high performance, as we will show in Section 5, the area/power cost of a MESH is much higher and results in poor efﬁciency. For the rest of this paper, we focus on C = 4 architecture and alternative implementations to improve performance and efﬁciency. 2 5.2 Hybrid Implementation of Concentration The results presented in the previous section assumed an integrated concentration implementation. In this section, we compare alternative concentration implementations including external concentration (M 1D1), integrated concentration (M 4D4), and hybrid implementations (M xDy ). Figure 8 shows the latency vs. load curves for the different implementations and for clarity, only selected curves 2 For 3D, stacked architectures, higher values of C might be suitable but we focus on conventional, 2D architecture. сϭ;D^,Ϳ сϮ Ϭ͘ϭ ;ĐͿEŽŶhZdƌĂĨĨŝĐ /ŶũĞĐƚ ŝŽŶ ZĂƚĞ Ϭ͘Ϯ Ϭ͘ϯ сϰ Ϭ͘ϰ сϴ Ϭ͘ϱ Ϭ͘ϲ are shown. As expected, the reduced switch bandwidth of M 1D1 implementation increases contention in the network and results in approximately 11% reduction in throughput as compared to the integrated implementation (M 4D4) for UR trafﬁc and approximately 58% with nonUR trafﬁc. As for hybrid implementations, any architecture with a single distributor (M xD1) achieve throughput similar to M 1D1 because of the reduced bandwidth at the ejection port. However, any hybrid implementation with M xDy where y ≥ 2 achieves saturation throughput similar to M 4D4 with signiﬁcant reduction in complexity. For example, M 1D2 achieve identical saturation throughput to M 4D4 but results in 21% reduction in zero-load latency. The saturation throughput for bitcomp trafﬁc does not change with alternative concentration implementation while the zero-load latency is dependent on the router latency (Figure 8(b)). For permutation trafﬁc such as bitcomp, the routers are not the bottleneck and results in the different implementation achieving identical throughput. However, M 1D1 results in a reduction of zero-load latency by an average of 21% compared to M 4D4 on the three trafﬁc patterns because of its reduced router complexity. For nonUR trafﬁc pattern, conﬁgurations with a single concentrator or distributor, i.e., M 1Dy or M xD1, signiﬁcantly reduces the saturation throughput, by up to 58% compared to M 4D4. The performance of M 1Dy and M xD1 achieve very similar throughput and for clarity, only M 1D1 and M 4D1 are shown in Figure 8(c). On trafﬁc such as nonUR with signiﬁcant locality, the throughput of M xDy hybrid implementation is proportional to min{x, y} as the concentrated router implementation determines the overall throughput of the network. Thus, the throughput increases by 79% as the distributor is increased from M 4D1 to M 4D2 and by 33% as the distributor is increased further from M 4D2 to M 4D4. Synthetic workload execution time is compared in Figure 9. Results show that output speedup (i.e. increase y ) can increase performance while providing only input speedup increase x) can actually reduce performance. For a (i.e. ﬁxed number of concentrators (x), as the number of distributors (y ) increases, the performance improves or at least remains the same for all the three trafﬁc patterns. However, (a) Uniform Random Traf f ic (b) Bitcomp Traf f ic (c) NonUR Traf f ic Figure 8. Performance comparison of concentration implementations DϭϭDϰϰ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϬϱϭϬϭϱϮϬ Ϭ Ϭ͘Ϯ /ŶũĞĐƚ ŝŽŶ ZĂƚĞ Ϭ͘ϰ DϭϭDϮϮDϮϰDϰϭDϰϮDϰϰ Ϭ͘ϲ D^, ^ϭZϰDϭϭ ^ϮZϮDϮϰ ^ϭZϰDϭϰ ^ϭZϰDϰϰ ^ϮZϰDϰϰ ^ϰZϰDϰϰ ^ϰZϭDϭϰ ^ϰZϮDϮϰ /Ŷ ũĞĐƚ ŝŽŶZĂƚĞ Ϭ͘ϭ Ϭ͘ϭϱ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱϯϯ͘ϱ Ϭ͘Ϭϱ D^,DϭϭDϭϮDϭϰDϮϭDϮϮDϮϰDϰϭDϰϮDϰϰ DϭϭDϭϮDϭϰDϰϭ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϭϬϭϱϮϬϮϱϯϬϯϱϰϬ Ϭ Ϭ͘Ϭϱ /Ŷ ũĞĐƚ ŝŽŶ ZĂƚĞ Ϭ͘ϭ Ϭ͘ϭϱ Ϭ͘Ϯ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ ϭϬϭϱϮϬϮϱ Ϭ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱϯϯ͘ϱ Figure 9. Synthetic workload comparison for alternative M xDy concentration implementations normalized to that of MESH on uniform trafﬁc. hŶŝĨŽƌŵ ŝƚĐŽŵƉ EŽŶƵƌ with ﬁxed y , as x increases, the performance is reduced because of additional input trafﬁc is injected into the network and causes more contention. The performance is reduced by as much as 20% for UR and 53% for bitcomp compared to MESH. Similar to the latency vs. load curve comparison, nonUR trafﬁc follows a different trend because of the trafﬁc locality and increasing x and y improves the performance as the switch bandwidth is increased to handle local trafﬁc. 5.3 Network Slicing Comparison of alternative conﬁgurations using concentration and channel slicing is shown in Figure 11. As we increase the slicing factor (S ) to 2 and 4, the zero-load latency is increased by 6% (16%) for S = 2 (S = 4) under UR. However, sliced conﬁguration (S 4R4M xDy ) achieves similar throughput as S 1R4M xDy without any slicing on UR while providing 99% higher throughput on nonUR trafﬁc. As described earlier in Section 4, the use of slicing in addition to concentration creates wiring complexity but virtual concentration (S 4R1) reduces the complexity. On UR trafﬁc and bitcomp trafﬁc, S 4R1 exceeds the throughput of S 4R4 by 6% and 18%, respectively. However, for nonUR, the throughput of S 4R1 is signiﬁcantly lower (by 46%) and achieves similar throughput as S = 1 conﬁguration since the performance of nonUR is dominated by the Figure 10. Synthetic workload comparison for alternative channel sliced architectures normalized to MESH under UR. hŶŝĨŽƌŵ ŝƚĐŽŵƉ EŽŶhZ router switch bandwidth. 3 For synthetic workloads (Figure 10), S 4R1M 1D4 is within 20% and 15% of MESH on UR and bitcomp trafﬁc but suffers 79% increase in execution time on nonUR trafﬁc. 5.4 Impact of Router Micro-architecture Since the router micro-architecture can impact the overall performance, we evaluate the impact of virtual channels [7] (VCs) and router speedup on the alternative concentration implementation. As shown in Figure 12, C = 1 has very little beneﬁt from increasing the number of VCs. The external concentration (M 1D1) beneﬁts from increasing VCs by reducing execution time by up to 13% but high-radix router implementation of concentration (M 4D4) achieve higher beneﬁt as execution time is reduced by up to 24.5%. With larger port count, head-of-line (HoL) blocking [13] limits the router throughput and VCs helps to reduce the impact of HoL. We also vary the router speedup from 1.0× (no speedup) to 1.7× and for UR, beneﬁts are similar to increasing VCs while for bitcomp, there is no beneﬁt from providing router speedup. However, for nonUR trafﬁc (Figure 13), providing router speedup provides signiﬁcant reduction of execution time – for M 1D1, 3S 4R1M 4D4 conﬁguration is not possible since with virtual concentration, each router is only connected to a single router. Figure 11. Performance comparison of channel slicing Ϭ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϭϬϮϬϯϬϰϬϱϬϲϬϳϬϴϬ ϬϬ͘ϮϬ͘ϰϬ͘ϲϬ͘ϴϭϭ͘Ϯϭ͘ϰϭ͘ϲ ^ϭZϰDϰϰ ^ϮZϮDϮϰ ^ϰZϭDϭϰ ^ϰZϰDϰϰ D^, Ϭ͘Ϯϱ Ϭ͘Ϯ сϰ;DϰϰͿ ^ϮZϰDϮϰ ^ϰZϭDϭϰ ^ϭZϰDϰϰ Ϭ͘Ϭϱ Ϭ͘ϭ ^ϰZϰDϰϰ D^, Ϭ͘ϭϱ ;ďͿŝƚĐŽŵƉdƌĂĨĨŝĐ /Ŷ ũĞĐƚ ŝŽŶZĂƚĞ ηs Ɛсϰ ηs Ɛсϴ ηs Ɛсϭϲ сϭ;D^,Ϳ ;ďͿŝƚĐŽŵƉ dƌĂĨĨŝĐ сϰ;DϭϭͿ сϰ;DϰϰͿ Ϭ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϱϭϬϭϱϮϬϮϱϯϬϯϱϰϬ ϬϬ͘ϰϬ͘ϴϭ͘Ϯϭ͘ϲϮϮ͘ϰ ^ϰZϭDϭϰ ^ϮZϮDϮϰ ^ϭZϰDϰϰ ^ϰZϰDϰϰ D^, Ϭ͘ϱ Ϭ͘ϰ сϰ;DϰϰͿ Ϭ͘ϭ ;ĐͿEŽŶhZdƌĂĨĨŝĐ /ŶũĞĐƚ ŝŽŶ ZĂƚĞ Ϭ͘Ϯ Ϭ͘ϯ ηsƐсϰ ηs Ɛсϴ ηs Ɛсϭϲ сϭ;D^,Ϳ ;ĐͿEŽŶhZdƌĂĨĨŝĐ сϰ;DϭϭͿ Figure 12. Synthetic workload comparison for the impact of virtual channels Ϭ͘Ϭϱ ;ĂͿhŶ ŝĨŽƌŵZĂŶĚŽŵdƌĂĨĨŝĐ /Ŷ ũĞĐƚ ŝŽŶZĂƚĞ Ϭ͘ϭ Ϭ͘ϭϱ ηs Ɛсϰ ηs Ɛсϴ ηsƐсϭϲ сϭ;D^,Ϳ ;ĂͿhŶŝĨŽƌŵZĂŶĚŽŵdƌĂĨĨŝĐ сϰ;DϭϭͿ ^ƉĞĞĚƵƉ сϭǆ ^ƉĞĞĚƵƉ сϭ͘Ϯǆ ^ƉĞĞĚƵƉ сϭ͘ϳǆ ^ƉĞĞĚƵƉ сϭ͘ϱǆ сϭ;D^,Ϳ Ϭ ǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ;ηǇĐůĞƐͿ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϭϬϮϬϯϬϰϬϱϬϲϬϳϬϴϬ ϬϬ͘ϮϬ͘ϰϬ͘ϲϬ͘ϴϭϭ͘Ϯϭ͘ϰϭ͘ϲ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱϯE Ě Ğ Ă Ğ   Ž ǌ ƌ ŝ ů Ă ŵ ƌ Figure 13. Impact of Router Speedup on nonUR trafﬁc. сϰ;DϭϭͿ сϰ;DϰϰͿ Figure 14. Area comparison of alternative architectures. router speedup of 1.7× results in 41% reduction of execution time and achieving performance that is within 14% of MESH. 5.5 Area / Energy Analysis The on-chip network area and energy consumption of the different architectures discussed in Section 2 and 3 are compared in this section. We estimate the router area usFigure 15. Average packet energy consumption during synthetic workload normalized to MESH under UR trafﬁc. ing the model in [4] for the router components. As shown in Figure 14, external concentration (S 1R4M 1D1) results in reduction of 47% area compared to integrated concentration (S 1R4M 4D4). The area analysis includes the additional area for the muxes needed in S 1R4M 1D1. Channel slicing further reduces the area with the reduction of crossbar complexity and the virtual concentration implementation (S 4R1M 1D1) achieves 69% reduction in area compared to the baseline MESH topology. We compare the average packet energy consumption in Figure 15. The total energy consumed by a packet consists of crossbar energy, buffer energy and link traversal energy [4, 12]. Under UR trafﬁc, the internal concentration scheme (S 1R4M 4D4) consumes 6.6% more energy per packet than the baseline MESH, while the external (S 1R4M 1D1) reduces energy by 22%. Further energy reduction can be achieved by adopting channel slicing, where EŽƌŵĂůŝǌĞĚǀĞƌĂŐĞWŬƚ>ĂƚĞŶĐǇ ϬϬ͘ϮϬ͘ϰϬ͘ϲϬ͘ϴϭϭ͘Ϯϭ͘ϰ ^ϭZϰDϭϭ ĂƉƌŝŽƌŝ ^ϭZϰDϭϰ ďĂƌŶĞƐ ^ϭZϰDϰϭ ĐŚŽ ůĞƐŬǇ ^ϭZϰDϰϰ ŚŽƉ the proposed virtual concentration routers further reduces energy by 32% under both UR and bitcomp trafﬁc. 5.6 Trace Based Evaluation Since the total execution time for a trace is dependent on the timing information in the trace and does not capture any inter-dependencies between packets, we focus on the average latency of the packets from the trace. From Figure 16, because the network is lightly loaded for these application, the average packet latency matches closely with the zero-load latency comparison of the different architectures as virtual concentration (S 1R4M 1D1) reduces average packet latency by up to 17% compared to the baseline MESH. 6 Related Work As discussed earlier, concentrated mesh with express channels was proposed by Balfour and Dally [4]. Our CMESH architecture differs as we do not provide any express channels but still provide the same bisection bandwidth. They also proposed CMESH2x with two parallel CMESH networks. However, their CMESH and CMESH2x comparison is not the same as our use of channel slicing as their CMESH2x doubled the bisection bandwidth. The external concentration approach in on-chip networks was described by Kim et al. [14] but they did not provide a quantitative comparison of alternative concentration implementations. In the hybrid topology proposed by Das et al. [10], an 8-way concentration is implemented through the use of a shared bus. Although a bus can provide a high bandwidth medium, the wire delay becomes problematic and proper bus arbitration becomes critical to fully utilize the network bandwidth. The MIT RAW [25] and Tilera Tile64 [2] processors use a similar approach to channel slicing in their on-chip networks as they provide 5 parallel 2D mesh networks. However, their networks use dedicated trafﬁc class for each network and they do not incorporate concentration to reduce network cost. The MECS topology [12] describes their topology using S = 2 sliced topology as they evaluate MECSx2. Similar to our study, they hold the constant bisection bandwidth constant in their comparison. However, Figure 16. Average packet latency from 64-core CMP SPLASH2 and Minebench traces. ^ϮZϮDϭϭ ŬŵĞĂŶƐ ^ϮZϮDϮϰ ůƵ ^ϰZϭDϭϭ ^ϰZϭDϭϰ ƌĂĚ ŝǆ ^ϰZϰDϰϰ D^, ƐĐĂůƉĂƌĐ ǁĂƚĞƌͺƐƉĂƚ ŝĂů they do not explore slicing beyond S = 2 and do not exploit the ability to implement virtual concentration. The XShare router micro-architecture [10] provides a microarchitecture that provides the ability to share the wide data path with multiple short packets. Our proposed channel sliced techniques provides similar beneﬁts by using a sliced network without adding any complexity to the router microarchitecture. Concentrated or bristled networks and channel slicing have been used in off-chip networks and large-scale systems. For example the Cray X1 [1] network uses a dualbristled torus topology by connecting two nodes to a single router. The Cray BlackWidow network [24] uses a foldedClos topology and implements a channel slicing factor of 4 – creating four parallel folded-Clos network. Martinez et al. [18] showed the beneﬁts of using virtual channels in bristled hypercube topology. Our work shows similar trend for on-chip network concentrated mesh topology. However, the constraints of on-chip networks and off-chip networks are very different and this work describes the beneﬁts and trade-off in implementing concentration and channel slicing in on-chip networks. 7 Conclusion In this paper, we explored cost-efﬁcient techniques for reducing complexity of on-chip network through the use of concentration and channel slicing. Instead of the commonly assumed, high-radix (integrated) implementation of concentration, we show how external implementation of concentration signiﬁcantly reduces complexity with small loss in performance. Hybrid implementations of concentration is also presented which approaches the performance of an integrated concentration implementation while still reducing complexity. We further demonstrated that adopting both concentration and channel slicing can yield a highly efﬁcient virtual concentration architecture, which saves 69% in area and 32% in energy as compared to a conventional 2D mesh topology, while matching its throughput. Acknowledgements This work is supported by NSF grants CNS-0551639, IIS-0536994, CCF-0747201, and CCF-0541337. We would like to thank all the anonymous referees for their helpful suggestions and comments. "
BiNoC - A bidirectional NoC architecture with dynamic self-reconfigurable channel.,"A Bidirectional channel Network-on-Chip (BiNoC) architecture is proposed to enhance the performance of on-chip communication. The BiNoC allows each communication channel to be dynamically self-configured to transmit flits in either direction in order to better utilize on-chip hardware resources. This added flexibility promises better bandwidth utilization, lower packet delivery latency, and higher packet consumption rate at each on-chip router. In this paper, a novel on-chip router architecture supporting the self-configuring bidirectional channel mechanism is presented. It is shown that the associated hardware overhead is negligible. Cycle-accurate simulation runs on this BiNoC network under synthetic and real-world traffic patterns demonstrate consistent and significant performance advantage over conventional mesh grid NoC architecture equipped with hard-wired unidirectional channels.","BiNoC: A Bidirectional NoC Architecture with                                        Dynamic Self-Reconfigurable Channel  Ying-Cherng Lan1, Shih-Hsin Lo1, Yueh-Chi Lin1, Yu-Hen Hu2, Sao-Jie Chen1, 3  1Graduate Institute of Electronics Engineering, National Taiwan University,  Taipei, Taiwan, ROC, {f94943068, r96943130, r96943123 }@ntu.edu.tw, csj@cc.ee.ntu.edu.tw  2Department of Electrical and Computer Engineering, University of Wisconsin, Madison  Madison, WI53706, USA, hu@engr.wisc.edu  3Graduate Institute of Electrical Engineering, National Taiwan University, Taipei Taiwan, ROC  Abstract  A Bidirectional channel Network-on-Chip (BiNoC)  architecture is proposed to enhance the performance  of on-chip communication. The BiNoC allows each  communication channel  to be dynamically selfconfigured to transmit flits in either direction in order  to better utilize on-chip hardware resources. This  added flexibility promises better bandwidth utilization,  lower packet delivery latency, and higher packet  consumption rate at each on-chip router. In this paper,  a novel on-chip router architecture supporting the selfconfiguring bidirectional channel mechanism  is  presented. It is shown that the associated hardware  overhead is negligible. Cycle-accurate simulation runs  on this BiNoC network under synthetic and real-world  traffic patterns demonstrate consistent and significant  performance advantage over conventional mesh grid  NoC  architecture  equipped with  hard-wired  unidirectional channels.  Keywords: NoC, bidirectional channel  1. Introduction  Network-on-chip (NoC) is a general purpose onchip interconnection network that offers great promises  to mitigate  the ever  increasing communication  complexity of modern many-core system-on-chip (SoC)  designs. An NoC advocates a communication-centric  design style: A general purpose communication  backbone will first be deployed. Then application  specific client logic (processors, memory subsystems,  peripheral device controllers, etc.) can be mapped onto  pre-allocated empty slots to form a complete system [1,  2]. This is analogous to modern land development  process where road and communication infrastructures  are laid first before specific buildings are designed and  built.  Along this direction, city-block style tiled NoC  architecture proposed in [1,3,4] has gained high  popularity due to its simplicity and flexibility.  A typical NoC consists of computational processing  elements (PEs), network interfaces (NIs), and routers.   978-1-4244-4143-3/09/$25.00 ©2009 IEEE  The  latter  two  comprise  the  communication  architecture.  An NI is used to packetize data before  using the router backbone to traverse the NoC.  Each  PE is attached to an NI which connects the PE to a  local router.  When a packet was sent from a source PE  to a destination PE as shown in Figure 1, the packet is  forwarded hop by hop on the network via the decision  made by each router.  For each router, the packet is  first received and stored at an input buffer. Then the  control logics in the router are responsible to make  routing decision and channel arbitration. Finally, the  granted packet will traverse through a crossbar to the  next router, and the process repeats until the packet  arrives at its destination.   MUX C. L. l o c r i t g n o o L C M U X M U X C L o o n g t r i c o l Figure 1. Typical NoC architecture in mesh topology  In a city-block tiled NoC architecture, neighboring  routers are connected via a pair of communication  channels. One channel will support out-going traffic  and the other channel will support in-coming traffic.   Under various traffic conditions, it is often observed  that the out-going channel may be flooded with outgoing traffic while the incoming channel remains idle.  In many metropolitan areas, during rush hours, the  travel directions of some highway lanes are reversed  temporarily to relieve congestions of the opposing  traffic direction.   In  this paper, we explore similar  idea as a  mechanism to relieve intermittent traffic congestion in  the NoC communication backbone, and hence enhance  overall performance. Specifically, we propose a key  innovation  to replace  the pair of unidirectional  channels between routers by a pair of bidirectional  channels that can be dynamically self-reconfigured to                facilitate flits traffic in either out-going or incoming  direction. This added flexibility promises better  bandwidth utilization, lower packet delivery latency,  and higher packet consumption rate at each on-chip  router.  To facilitate this bidirectional traffic, we devise a  novel inter-router traffic control algorithm to allow  neighboring  routers  to coordinate  the  specific  directions of the pair of channels between them for  each data packet. A novel router architecture that  supports bidirectional channels with dynamic selfreconfiguration capability is also proposed. It is  observed that the hardware overhead for bidirectional  channels  is negligible. Finally, a cycle-accurate  behavioral simulator for NoC is developed to validate  the potential performance gain of this proposed new  approach. This simulator is capable of simulating  cycle-true traffic behaviors of a moderate size NoC  over different  traffic patterns. Very encouraging  simulation results have been observed.   In summary, major contributions of this work are:  • New bidirectional channel NoC architecture   • New traffic control algorithm to support dynamic  self-configuration of bidirectional channels  • New router design to support bidirectional channel  and dynamic self-configuration.   The remaining of this paper is organized as follows:  In Section 2, state-of-the-art techniques used in NoC  are reviewed. In Section 3, the inter-router traffic  control algorithm  is developed.   A new router  architecture that supports bidirectional NoC (BiNoC) is  described and presented in Section 4.  Finally, in  Section 5,  experiment  results  comparing  the  performance of  the proposed biNoC architecture  against the conventional NoC architecture are provided.    2. Related Work  In order to make every application running on an  NoC follow the same basic operation principle defined  in the backbone, several issues should be encapsulated  into its design methodology.  First of all, backbone  architecture design should be scalable to the growing  amount of traffic in a graceful manner.   Also,  flexibility is important to deal with the increasing  complexity of an application.  Various approaches  have been explored for improving NoC performance:  such as packet routing technique, application mapping  and scheduling, topology synthesis, and flow control,  etc. [5,6,7,8,9].    However, considering the physical level of a  backbone, the interconnect wire between routers is also  an important factor in determining the performance of  a system.  To the best of our knowledge, interconnect  wires connecting routers and used in previous NoCs  are all unidirectional.  This is the first paper to analyze  and promote the use of bidirectional wires for NoC  interconnection.   Bidirectional wires are  in fact  commonly used  in  the design of high-speed  commercial microprocessors [10].   Since signal  interconnects are inherently bidirectional, they can be  used between the tri-state driver and receiver to realize  the bidirectional operation.   Recently,  several  researches in the circuit design and EDA fields have  also mentioned similar designs [11, 12].  To enable a competent NoC backbone design, we  develop a novel NoC architecture, named BiNoC, with  bidirectional transmission channels, which is selfreconfigurable,  to enable  the most bandwidth  utilization between routers.  This new design concept  of a self reconfigurable bidirectional channel that  enables reconfiguring the transmission direction of a  data channel can dynamically adjust the bandwidth  based on  the real-time  traffic requirement. The  implementation detail and experimental  results  described in the following section can prove that this  design can get significant performance improvement  with reasonable hardware design overhead, making this  concept realistic and suitable for NoCs.  3. Self-Reconfigurable Routing Scheme  3.1. NoC Architecture  We use an array of n × n nodes arranged in a  homogenous 2-D mesh to be our NoC architecture.   Besides direct connection to the corresponding PE,  each router is also connected to four neighboring  routers (North, East, South, and West) such that  propagating a flit through one hop requires one clock  cycle.  Each router is also equipped with input buffers  to each of its five connections.  Packets are broken into  head, body, and tail flits and sent via wormhole routing  with smaller buffer sizes to save area.  The routing  algorithms that we use for reference and comparison  are minimal routing algorithms, thereby ensuring  freedom from livelock.  We also implement the  dimension-ordered XY routing restrictions to guarantee  that no deadlock occurs.   3.2. Problem Formulation  In a conventional NoC architecture, each pair of  neighboring routers uses two unidirectional channels in  opposite direction to propagate data on the network as  shown in Figure 2.(a).  To enable the most bandwidth  utilization, data channels between each pair of routers  should be able to transmit data in any direction at each  run cycle.  That is, four kinds of channel direction  combinations should be allowed for data transmission  as shown  in Figure 2(b).   However, current  unidirectional NoC  architectures, when  facing  applications that have different traffic patterns, cannot  achieve the high bandwidth utilization objective.             (a)                                             (b)  Figure 2. Channel directions in: (a) a conventional NoC  and (b) our  proposed BiNoC  As shown in Figure 3(a), an application task graph  is typically described as a set of concurrent tasks that  have been already assigned and scheduled onto a list of  selected PEs.  Each vertex represents a task with a  value tj of its computational execution time and each  edge represents the communication dependence with a  value of communication volume which is divided by  the bandwidth of a data channel.  For the most  optimized mapping in a 2×2 2-D mesh NoC as shown  in Figure 3(b), the conventional NoC architecture in  this case can only use three channels during the whole  simulation and result in a total execution time of 80  cycles.  However, if we can dynamically change the  direction of each channel between each pair of routers  like the architecture illustrated in Figure 3(c), the  bandwidth utilization will be improved and the total  execution time can be reduced to 55 cycles.  Figure 4  shows the detailed execution schedules, where the  required communication time between nodes in BiNoC  is extensively reduced.  10=At 10=Bt 15=Ct 10=Dt Figure 3. Example of (a) task graph mapping to (b) a  conventional NoC architecture and (c) to our proposed  BiNoC  To discuss our motivation more specifically, Figure  5 shows how the bandwidth utilization of a typical  conventional NoC under uniform,  regional and  transpose traffics (please refer to Section 5 for detailed  description of the simulation setup and characteristics  of traffic patterns).  Here we define the bandwidth  utilization as:  ∑ (cid:3032)(cid:3051)(cid:3032)(cid:3030)(cid:3048)(cid:3047)(cid:3036)(cid:3042)(cid:3041) _(cid:3047)(cid:3036)(cid:3040)(cid:3032)(cid:3400)(cid:3047)(cid:3042)(cid:3047)(cid:3028)(cid:3039) _(cid:3030)(cid:3035)(cid:3028)(cid:3041)(cid:3041)(cid:3032)(cid:3039)(cid:3046) ,    (cid:3047)(cid:3045)(cid:3028)(cid:3041)(cid:3046)_(cid:3030)(cid:3035)(cid:3028)(cid:3041)(cid:3041)(cid:3032)(cid:3039)(cid:3046)   (cid:3280)(cid:3299)(cid:3280)(cid:3278)(cid:3296)(cid:3295)(cid:3284)(cid:3290)(cid:3289)_(cid:3295)(cid:3284)(cid:3288)(cid:3280)   (cid:3295)(cid:3284)(cid:3288)(cid:3280)(cid:3128)(cid:3116) Bandwidth_utilization =  where execution_time represents the total time needed  to execute the whole pattern, and trans_channels is the  number of data channel being used for each time cycle  while total_channels is the total channel numbers in an  NoC architecture.  We observe that the bandwidth  utilization in a conventional NoC architecture running  with these three synthetic traffic patterns are all very  low.  Therefore, we can speculate that many channels  are idle during each processing cycle because of the  fixed direction of channel restriction.  n o i t a z i l i t u h t d i w d n a b uniform regional transpose 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 0.05 0.1 0.1 5 0.2 0.25 0.3 0.3 5 0.4 0.45 0.5 injection rate ( flits/nodes/cycles) Figure 5. Bandwidth utilization for the conventional NoC   As observed above, we propose a novel architecture  with self-reconfigurable capability  to dynamically  change the channel direction between neighboring  routers based on the real-time need of bandwidth.  In  the following section, we will describe in detail the  design methodology for the bidirectional channel and  its desirable property to achieve better bandwidth  utilization.  3.3. Inter-Router Transmission Scheme  To achieve the objective of self-reconfiguring the  direction of a bidirectional channel dynamically, each  bidirectional channel needs a pair of control schemes  communicating each other to arbitrate the channel  authority.    Figure 4. Detailed execution schedules of (a) conventional  NoC architecture and (b) BiNoC  Figure 6. Inter-Router data transmission scheme  Figure 6 shows the detail of the control blocks and  signals for one channel between the two adjacent              routers.  Routing module detects the packet header and  computes the output direction.  The channel control  module on the target channel decodes this request and  is responsible to communicate with the other channel  control module  in  the  neighboring  router.   Configuration of the bidirectional channel is managed  by a finite state machine designed in the channel  control blocks. As the current state of the requested  channel is available for outputting, the packet which  gains the output grant from the arbiter will go through  the crossbar and be delivered to the neighboring router.  3.4. Bidirectional Channel Control Logic  Different from  the unidirectional  transmission  channel  used  in  conventional  routers,  each  bidirectional channel needs to be managed carefully to  ensure that each transmission is legitimate in our  BiNoC platform.  An inter-router transmission channel  control scheme is implemented in a finite state  machine for each channel as shown in Figure 7(a) to  make sure that only one direction is valid on each  bidirectional channel at one time.  More specifically,  the finite state machine in the channel control block  dynamically reconfigures the direction of each channel.   As illustrated in Figure 7(b), the direction setup  scheme for a bidirectional channel is formed by a  handshake protocol implemented in the two individual  finite state machines located at the two neighboring  routers in this channel.  Due to the inter-router  communication delay, the control signal for direction  setup should take two additional cycles to traverse  from one router to the other.  For example, the  input_req signal in Router 2 uses the same output_req  signal from Router 1 but having a two cycles of delay.  The state machine in our design is quite simple and  area efficient.  It contains three main states, which are  idle, free and waiting as shown in Figure 7(a).  At the  initialization time of the whole system, the two  bidirectional channels for each neighboring router pair  will be set as each having a different direction at first.   For each bidirectional channel, the initial states of the  pair of finite state machines in this channel will be  respectively set to idle and free to make sure that only  one direction is valid for data transmission at this  moment.  For example, if the direction of bidirectional  channel in Figure 7(b) is set from Router 1 to Router 2,  then the finite state machines will stay at free and idle  states respectively.    Each state machine has three input signals and one  output signal.  The input signal channel_req will be at  logic 1 if any packet in the buffer at the local router is  requesting this bidirectional channel to transmit.  The  output signal output_req sends an output request to the  neighboring router which will become the input request,  input_req, for the neighboring router in two cycles  later.   Therefore,  the  input signal  input_req  is  generated by the neighboring router in the same  manner as output_req.  These two signals are used as  inter-router control signals, thus they suffer a 2-cycle  register delay.  The input signal count is designed for  calculating the communication delay of the two routers.    Figure 7. (a) Finite state machine for inter-router control  and (b) control scheme of a bidirectional channel setup  between two routers.  The major function of the state machine is to  control the in-out port configuration at the two  terminals of a bidirectional channel.  Being in free state  means that this channel is now available to be used as  an output port in the local router.  Packet that requests  this channel can be moved at free state while getting  grant from arbitration.  In order to use the bidirectional  channel legally, our channel control protocol makes  sure that the states of the two routers on this channel  will not both stay at free state simultaneously.  If there  is no need to change the channel direction, it will keep  in free state.  On the other hand, a free state will switch  to an idle state if there is no channel request  (channel_req = 0) and the channel is receiving an input  request (input_req = 1) from the neighbor router  meanwhile.  This transition indicates that the output  authority of this channel is handed over from the local  router to the neighboring one, and the channel direction  is reversed.  The finite state machine will stay in idle  until any packet in this router requests this channel  again (channel_req = 1).  At this time, output_req  signal is pulled up to notify the neighbor and the  channel switches to waiting state.  The packet can only  use this channel to transmit while there is no interrupt  (input_req = 0) caused by the neighboring router  during  the waiting  state  (inter-router  signal  transmitting delay time).  The signal count will begin  to accumulate from the start of waiting state.  Finally,  the state will switch to free while the count reaches a  threshold number.  The threshold number depends on  the inter-router register delay which is set to be 3 in our  design.  On the other hand, if the router receives the  input request (input_req = 1) during the waiting state,  the channel state will turn back to idle since the  neighbor router which was already in free state has a  higher priority to use this shard bidirectional channel  with no additional delay.  3.5. Example of Bidirectional Transmission  To describe  the functionality of bidirectional  transmission more clearly, Figure 8 shows a practical  transmission timing diagram of our design.  In this  example, Router 1 and Router 2 are two neighboring  routers as illustrated in Figure 6, and have some data to  transmit.  We assume one of the bidirectional channels  between these two routers is being occupied by other  traffic, so the packets f, g, h, and i, which are in length  of 8, must use the same bidirectional channel to deliver  in this demonstration.  At the beginning, both routers  receive a new packet, f and g at their input buffer on  clock cycle 1 simultaneously.  Among the signal  waveforms, input_data signal stands for arrivals time  of each flit at the input buffer.  Signal input_data_d  represent the time that the flit is processed by the  control unit (such as routing module, channel control  module, and arbiter, etc.) in the local router.  We want  to illustrate how a bidirectional channel switches its  configuration dynamically.  Based on the routing results of the two packet  headers (f1, g1), same bidirectional channel from both  ends is being requested. In other words, the output_req  signals in the two routers will both be set as logic 1.  If  the finite state machines in Router 1 and Router 2 are  initially set as free and idle respectively, Router 1 will  obtain the authority to use this bidirectional channel  first since the initial direction for this channel is from  Router 1 to Router 2.  Then the 8 flits of packet f pass  through the bidirectional channel in order and arrive at  Router 2.    In this case, the input_req in Router 1 (which is  being in the process of outputting flits on the  bidirectional channel) will not switch its free state.   Under this circumstance, flits in packet g could not get  the requested channel directly.  At this time, the  channel state of Router 2 will turn back to idle state  and the channel request for this bidirectional channel  will be canceled by the channel control module.    At clock cycle 11, when Router 1 completes the  transmission of packet f, it pulls down the output_req  signal.  After sensing this transition, Router 2 sends the  output_req and switches the channel state to waiting  again on clock cycle 13.  Since there is no packet to  transmit  from Router 1  through  this  shared  bidirectional channel during the 3 cycles of waiting  state in Router 2 (input_req = 0), the channel state in  Router 2 can be switched to free state.  And the state  machine in Router 1 will be switched to idle.  This  state transition in both adjacent routers indicates that  the output authority of the shared bidirectional channel  is handed over from Router 1 to Router 2.  And Router  2 begins to transmit packet g on clock cycle 16.  The second part of this example starts at clock cycle  26 while the previous two packet transmissions have  finished and a new packet h arrives at the input buffer  in Router 1.  The current channel state in Router 1 that  was id le has to be changed to free for output ting.   Router 1 will pull up output_req signal and switch its  channel state to waiting.  On the other side, Router 2  receives packet i at its input buffer at clock cycle 28  which also requests the same bidirectional channel.   According to our channel control protocol, the state  machine in Router 2 will be switched to idle by the  signal because Router 2 has not already been in an  Figure 8. Example of bidirectional data transmission  in Router 1, packet h gains the output authority through  this channel and begins to transmit.  As h finishes its  transmission, packet i follows the same procedure to  obtain the right to use this bidirectional channel and  completes the delivery.   This inter-router channel control protocol makes  sure that every data transmission is rightful and fair.   Multiple signals driving at both ends of bidirectional  data channel  is avoided and  the  inter-router  communication delay is considered in our cycle  accurate simulation.  4. Router Architecture  4.1. Conventional Router Architecture  A typical conventional router architecture in a  mesh-type NoC contains ten hardwired unidirectional  data communication channels in each router node.  As  illustrated in Figure 9, a single router has one data  input port and one data output port in each direction.   All the input/output ports in a router are registered to  provide clean signals from router to router, as an NoC  hardware implementation may require long wires.   Therefore all the inter-router signals have to pass  through at least two stages of flip-flop to traverse from  one router to the neighboring one.  Figure 9. Conventional router architecture  A typical router uses an input buffer to hold the  blocked flits.  First, each routing module will make a  routing decision for the packet in each buffer and  generate a channel request signal to the arbiter  individually.  If the requested channel is available, it  means that the corresponding downstream buffer at the  neighboring router has enough buffer space.  Then the  arbiter can begin to allocate these channels to let the  granted packets  traverse  the crossbar  to  the  neighboring router simultaneously.  The crossbar is a  big switch fabric which is used to connect each input  channel and output channel.  The size of the crossbar in  this typical router is 5×5.  Since flits arriving at an  output port will never stop, only one output register is  needed in each data output port.  In this architecture, there are two unidirectional data  channels connected in opposite direction.  One channel  is used as input and the other is used as output.  In  other words,  there are  totally  ten unidirectional  channels used for data transmission.  However, the  channel bandwidth utilization is not flexible in this  hardwired 5-input 5-output NoC router.    For example, while a packet PA in the west input is  requesting for the north output channel which is being  used by another packet PB from the east input.  PA has  to wait for the north output channel until PB has  completed its transmission.  In this situation, PA has no  chance to use the other channel at the north direction  because it is hardwired for input channel, which can  only be used by the neighboring router as an output  channel.  In the next section, it is shown if all the ten  unidirectional channels are replaced by bidirectional  channels, the channel utilization will be more efficient.   Back to the previous example, provided that the  neighbor router is not using the other channel in north  direction, the router can dynamically self-reconfigure  this channel as a second output channel and the  contention at the north direction can be relieved.  4.2. Bidirectional Channel Router Architecture  To  realize a dynamically  self-reconfigurable  bidirectional channel NoC architecture, we modify the  input/out port configuration and router control unit  designs based on the conventional router.  As shown in  Figure 10, in order to dynamically adjust the direction  of each bidirectional channel at run time, we add a  channel control module to arbitrate the authority of the  channel direction.  Each bidirectional channel which is  composed of an in-out port inside is the main  difference from the typical conventional router design  which unidirectional channel employs a hardwired  input port or output port.  However the total number of  data channels is not changed.  In our design, each channel can be used as either an  input or an output channel.  As a result, the width of a  channel request signal, channel_req, generated from  the routing modules is doubled.  Two bidirectional  channels can be requested in each output direction.  In  other words, this router is able to transmit at most two  packets to the same direction simultaneously which  decreases the probability of contentions.    The channel control block has two major functions.   One is to dynamically configure the channel direction    between neighboring routers.  Since the bidirectional  channel is shared by a pair of neighboring routers,  every transition of the output authority is achieved by a  channel control protocol between these two routers.   The control protocol, implemented as a finite state  machine was described in the previous section.  The  input_req and output_req signals are used  to  communicate with the two neighboring routers during  their configuration.  input_req Channel Control Module output_req North Channel A in-out select arbiter North Channel B in-out select input buffer routing  module routing  module input buffer East Channel A in-out select East Channel B in-out select input buffer rout ing  module routing  module input buffer South Channel A in-out select South Channel B in-out select input buffer routing  module routing  module input buffer West Channel A in-out select West Channel B in-out select input buffer rout ing  module routing  module input buffer PE Channel A in-out select PE Channel B in-out select input buffer rout ing  module routing  module input buffer Figure 10. Dynamically self-reconfigurable bidirectional  channel NoC architecture.  The other responsibility is that whether the channel  request (channel_req) for the corresponding channel is  blocked or not will depend on the current status of  channel direction.  If the channel is able to be used, the  arb_req will be sent to the arbiter to process the  channel allocation.  The most important point of this architecture is that  we can replace all the unidirectional channels in a  conventional NoC with our bidirectional channels.   That will increase the channel utilization flexibility  without requiring additional transmission bandwidth  compared to the conventional NoC platform.  5. Experimental Results  5.1. Experiments Setup  A cycle-accurate NoC simulation environment was  implemented along with different inter-router channel  designs in HDL.  Each design is comprised of multiple  functional blocks involving routing, channel control,  arbitration and switch fabric.  Design modules were  broken down into respective components to allow rapid  prototyping and customization.  This environment  simulates wormhole switching with separation of  packets into header, body, and tail flits.  Basic routing  algorithms used as a basis for comparison is XY routing,  which routes packets in X direction before Y direction.   Also, basic FCFS principle is used to resolve the  arbitration conflicts.    Table 1. NoC Architectures used in our experiments  Table 1 shows the four router architectures used for  comparison purpose.   Typical-NoC represents a  conventional NoC using fixed uni-direction channels  between routers.  For each direction, one input channel  and one output channel are used to connect between  two neighboring routers.  Also, we implement TypicalNoC-double which use a 64-flit buffer in each input  channel for comparison.  In our proposed BiNoC  architecture, two bidirectional channels are used to  connect the two neighboring routers in each direction.   In order to inspect the performance trend among  different NoC architectures, we designed a ReducedBiNoC which uses only one bidirectional channel for  each direction (that is, it uses five bidirectional  channels in total).  For fair comparison in buffer  resources, Typical-NoC, BiNoC and Reduced-BiNoC  listed on Table 1 were all implemented with total  buffer size of 160 flits.  Only Typical-NoC-double has  doubled its total buffer size to 320 flits.  Since the  number of input buffer is doubled in our BiNoC, the  buffer size buffer in BiNoC is reduced to 16 flits only.  5.2. Synthetic Traffic Analysis  In synthetic traffic analysis, the physical layer of  our simulation environment comprises of 8 × 8 nodes  connected as a mesh array.  Each packet has a constant  16 flits length.  The tests performed send packets at a  fixed injection rate for 25000 cycles.  Tests were run at  varying injection rates with varying patterns.    Three types of traffic patterns were run: uniform,  transpose, and regional.  In a uniform traffic, a node  receives a packet from any other node with equal  probability based on the injection rate.  In a transpose  traffic, a node at (i,j) always sends packets to a single  node at (j,i).  In a regional traffic, 90% of the packets  are sent to the destination within 3 hops away.  That is  more realistic since each node does not often send  packets to other nodes in a longer distance.  Figures 11(a), 11(b), and 11(c) show that our  BiNoC owns a lower latency than the Typical-NoC in  these three traffic patterns even if compared with      Typical-NoC-double.  In Figure 11(a), we can find that  Reduced-BiNoC performs the worst in the uniform  traffic case.  This is because the uniform pattern sends  packets randomly, which will cause the bidirectional  channel switch frequently.  In the transpose traffic case,  such regular traffic patterns with XY routing applied  will make at least one of the two hardwired directional  channels connecting neighboring routers unable to be  used.  Therefore, the latency results of a Typical-NoC  is the same as a Reduced-BiNoC which has only one  bidirectional channel between two neighboring routers  as illustrated in Figure 11(b).  In the regional pattern  case, BiNoC  still outperform  the other  three  architectures even at low injection rate, as show in  Figure 11(c).  That is because more packets can be  transmitted via the two bidirectional channels, which  breaks  the bottleneck caused by  the hardwireddirection channel.  0 50 100 150 200 250 300 0 0 .05 0.1 0 .15 0.2 0 .25 0 .3 0.35 0 .4 l a t n e y c ( e c y c l ) injection rate (flit/node/cycle) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 50 100 150 200 250 300 0 0.05 0 .1 0.15 0 .2 0.25 0.3 l a t n e y c ( e c y c l ) injection rate (flit/node/cycle ) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 50 100 150 200 250 300 0 0.1 0.2 0.3 0.4 0.5 0.6 l a t n e y c ( c y c l e ) injection rate (flit/node/cycle ) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC Figure 11. Latency versus injection rate results obtained by  running (a) uniform, (b) transpose, and (c) regional traffics.  Figures 12(a), 12(b) and 12(c) represent  the  performance results of consumption rate versus  injection rate obtained by running three synthetic  patterns.  At low injection rates, the injection rates  match with the consumption rates of an NoC.  But after  reaching a definite injection rate, the flits are no longer  being consumed at the same rate as they are being  input to the NoC.  This phenomenon of NoC injection  rate increasing faster than their consumption rate is  analogous  to fundamental  traffic model reaching  critical density point [13].  Note that BiNoC can  exhibit higher critical densities than others.    0 0.05 0 .1 0.15 0 .2 0.25 0 .3 0.35 0 .4 0 0.05 0.1 0 .15 0.2 0.25 0.3 0.35 0.4 0.4 5 0.5 c n o s u m p t i n o r a t e i l f ( / t c y c / e d o n l e ) injection rate (flit/node/cycle ) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 0.05 0 .1 0.15 0 .2 0.25 0 .3 0 0.05 0.1 0 .15 0.2 0.25 0.3 0 .35 0.4 c n o s u m p t i n o r a t e i l f ( / t c y c / e d o n l e ) injection rate (flit/node/cycle ) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 0.1 0 .2 0.3 0.4 0 .5 0.6 0.7 0.8 c n o s u m p t i n o r a t e i l f ( / t c y c / e d o n l e ) injection rate ( flit/node/cycle) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC Figure 12. Consumption rate versus injection rate results  obtained by running (a) uniform, (b) transpose, and (c)  regional traffics.  By observation above, the diversity of traffic also  affects the performance of a bidirectional channel  because of the times needed to configure the channel  direction.  Figures 13(a) and 13(b) show the total  number of configurations of BiNoC and ReducedBiNoC respectively obtained by running three kinds of  synthetic patterns.  The uniform traffic sends packets  randomly and causes the bidirectional channel to  switch direction frequently.  In other words, a great  deal of  time  is wasted  to operate  the channel  reconfiguration (by switching its direction).  Therefore,  NoC with bidirectional channel can perform better in  regional and transpose patterns because of their lower  traffic diversity.  0 20000 40000 60000 80000 100000 120000 0 0 .05 0 .1 0 .15 0 .2 0 .25 0 .3 0 .35 0 .4 0 .45 0 .5 s w g n h c t i i i t m s e injection rate (flit/node/cycle ) BiNoC-uni BiNoC-trans BiNoC-regional 0 20000 40000 60000 80000 100000 120000 0 0 .1 0 .2 0 .3 0 .4 0 .5 0 .6 s w g n h c t i i i t m s e injection rate (flit/node/cyc le) Reduced-BiNoC-uni Reduced-BiNoC-trans Reduced-BiNoC-regional Figure 13. Total number of configuration times on (a) BiNoC  and (b) reduced-BiNoC running synthetic patterns  5.3. Buffer Size Analysis  To understand the effects caused by buffer size, we  use regional patterns to run simulation since it is more  realistic compared to other two. Figure 14 show the  result of simulation on Typical-NoC and BiNoC under  different buffer sizes and at a flit injection rate of 0.05                                      for comparison.  As buffer size decreases, the latencies  of BiNoC are not increased dramatically as the  Typical-NoC did.    0 200 400 600 800 1000 1200 1400 0 20 40 60 80 100 120 140 l a t n e y c ( c y c l e ) buffer size (flit) Typical-NoC BiNoC Figure 14. Latency versus buffer sizes analysis over  regional traffic patterns  5.4. Experiments with Real Applications  According to the experimental results above, the  channel configuring times have important impact on  transmission latency.  However, in real cases, data  transmission is much regular than synthetic patterns.   Therefore, we use E3S benchmarks  from  the  Embedded Microprocessor Benchmark Consortium  (EEMBC) [14] to demonstrate the improvements of  our BiNoC architecture compared to the typical NoC in  this section.  The three tests used are auto-indust,  consumer, and telecom which were run on a 5×5, 4×4,  and 6×6 NoC architectures, respectively.  First we input task graphs given in the E3S  benchmarks and performed task mapping to map each  task in the graph to a tile on NoC.  Task mapping will  put tasks with heavy communication closer to optimize  communication cost.  Then the resulting process graph  was converted to a packetized traffic flow which was  fed into our NoC simulator.  Here we adopt the  common simulated annealing (SA) algorithm as our  task mapping algorithm.  The cost function of SA for  mapping is the data flow between tasks:  task (cid:2919) and task (cid:2920) , respectively.  The term wij represents the total communication  volume between taski and taskj while (xi, yi) and (xj, yj)  represent the positions of PEs which are assigned with  As shown in Figures 15(a), 15(b), and 15(c), BiNoC  consistently provides  the  lowest average packet  latencies among the three benchmark applications.   This is because its self-reconfigurable bidirectional  channel has more flexibility for data transmission.  In  real traffic, we observe more clearly that the ReducedBiNoC has almost the same latencies with respect to  the Typical-NoC  because  of  the  regularity  characteristic in real traffics. In other words, many  channels in the Typical-NoC cannot be used for  transmission due to its fixed unidirection.  Also, the  ( ) ∀ ∈ w task graph _ ij ij i j i j w x x y y × − + − ∑ simulation results also show that the buffer size  increase from a Typical-NoC to a Typical-NoC-double  cannot help much on the performance under regular  pattern while using the same task mapping algorithm.   Another important result for BiNoC running E3S  benchmarks is that BiNoC owns better latencies even  at low injection rate.  0 50 1 00 1 50 2 00 2 50 3 00 0.1 0.12 0.14 0.16 0.18 0.2 0.22 l a t n e y c ( e c y c l ) injection rate ( flit/node/cycle) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 5 0 1 00 1 50 2 00 2 50 3 00 0 0 .02 0 .04 0 .06 0 .08 0 .1 0.1 2 0.1 4 0.1 6 0.1 8 0.2 l a t n e y c ( c y c l e ) injection rate (f lit/node/cycle) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC 0 50 1 00 1 50 2 00 2 50 3 00 0.1 0.1 2 0 .14 0.16 0.1 8 0.2 0 .22 0.24 l a t n e y c ( e c y c l ) injection rate (f lit/node/cycle) Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC Figure 15. Latency versus  injection rate running E3S  benchmarks of (a) auto-indust, (b) consumer, and (c) telecom  5.5. Bandwidth Utilization Analysis  Figures 17(a), 17(b), and 17(c) show the bandwidth  utilization experiments versus injection rate under  synthetic patterns.  We can find that BiNoC always has  the best saturation performance due to its flexibility.   And Typical-NoC always saturates at  the worst  bandwidth utilization when running real case traffic  patterns as illustrated in Figures 18(a), 18(b), and 18(c).   As injection rate increases, either BiNoC or ReducedBiNoC will saturate at the best bandwidth utilization,  because of the regularity characteristic in real patterns.   Also, Reduced-BiNoC always has better bandwidth  utilization at low injection rate in both Figure 17 and  Figure 18.  Note that the bandwidth utilization results  of Typical-NoC-double are almost  the same as  Typical-NoC since they have the same architecture.  5.6. Implementation Overhead  Four NoC router architectures were implemented in  TSMC 0.13 µm technology and synthesized using  Synopsys Design Compiler under typical operating  conditions.  As shown in Figure 16, the area numbers  of these four router architectures are compared with the                    Typical-NoC normalized as one.  Since the total buffer  size dominates major part of area number, Table 1  shows their total buffer size for reference.   We can  find that our BiNoC has owns about 40% area  overhead over the Typical-NoC due to its double  crossbar design and control logic.  However, the  simulation results above showed that BiNoC has even  better performance  than  the Typical-NoC-double  (which has about 90% area overhead over a TypicalNoC due to its use of double-size buffers).  If we cut  the bandwidth of BiNoC in half as a Reduced-BiNoC,  then the crossbar and control logic occupy almost the  same area as a Typical-NoC.  The simulation results  showed that the Reduced-BiNoC has almost the same  latency but owns much higher bandwidth utilization  compared to the Typical-NoC.  0 0 .2 0 .4 0 .6 0 .8 1 1 .2 1 .4 1 .6 1 .8 2 1 Typical-NoC Typical-NoC-double BiNoC Reduced-BiNoC Figure 16. Area Comparison of NoC Architectures  6. Conclusion  In this paper, we proposed a novel NoC backbone  architecture BiNoC which features dynamically selfreconfigurable bidirectional channels. An algorithm  that supports real-time traffic-direction arbitration in  the bidirectional channels is presented.  Experimental  results using both synthetic traffic patterns and E3S  benchmarks verified  that  the proposed BiNoC  architecture can significantly  reduce  the packet  delivery latency at all levels of packet injection rates.   Compared to the conventional NoC, the bandwidth  utilization of our BiNoC also exhibits higher efficiency.    7. Acknowledgements  This work was partially supported by the National  Science Council, under Grants 97-2221-E002-241MY3, and 97-2200-E002-019.  "
Comparing tightly and loosely coupled mesochronous synchronizers in a NoC switch architecture.,"With the advent of networks-on-chip (NoCs), the interest for mesochronous synchronizers is again on the rise due to the intricacies of skew-controlled chip-wide clock tree distribution. Recently proposed schemes agree on a source synchronous design style with some form of ping-pong buffering to counter timing and metastability concerns. However, the integration issues of such synchronizers in a NoC setting are still largely uncovered. Most schemes are in fact placed between communicating switches, thus neglecting the abrupt increase of buffering resources needed at switch input stages. This paper goes a step forward and aims at deep integration of the synchronizer in the switch architecture, thus merging key tasks such as synchronization, buffering and flow control into a unique architecture block. This paper compares the integrated and the loosely coupled solutions from a performance and area viewpoint, while devoting special attention to their robustness with respect to physical design parameters.","Comparing Tightly and Loosely Coupled Mesochronous Synchronizers in a NoC Switch Architecture Daniele Ludovici§ , Alessandro Strano† , Davide Bertozzi† , Luca Benini† † , Georgi N. Gaydadjiev§ † ENDIF, University of Ferrara, 44100 Ferrara, Italy. † † DEIS, University of Bologna, 40136 Bologna, Italy. § Computer Engineering Lab., Delft University of Technology, The Netherlands. email: d.ludovici@tudelft.nl, alessandro.strano@student.unife.it dbertozzi@ing.unife.it, luca.benini@unibo.it, g.n.gaydadjiev@tudelft.nl Abstract With the advent of Networks-on-Chip (NoCs), the interest for mesochronous synchronizers is again on the rise due to the intricacies of skew-controlled chip-wide clock tree distribution. Recently proposed schemes agree on a source synchronous design style with some form of ping-pong buffering to counter timing and metastability concerns. However, the integration issues of such synchronizers in a NoC setting are still largely uncovered. Most schemes are in fact placed between communicating switches, thus neglecting the abrupt increase of buffering resources needed at switch input stages. This paper goes a step forward and aims at deep integration of the synchronizer in the switch architecture, thus merging key tasks such as synchronization, buffering and ﬂow control into a unique architecture block. This paper compares the integrated and the loosely coupled solutions from a performance and area viewpoint, while devoting special attention to their robustness with respect to physical design parameters. I. Introduction The problem of distributing the global clock in a chip with minimal clock skew is getting difﬁcult to solve due to the reverse scaling of wire delay in nanoscale integrated circuits. At the same time, the design paradigm shift towards on-chip multi-processor architectures is raising the need for easily extensible clock trees and for self-assembly clock tree synthesis strategies like the one used in the Polaris chip [1]. A fully asynchronous approach to global intra-chip communication would eliminate the clock distribution concern and would make designs more modular since timing assumptions are explicit in the hand-shaking protocols. Unfortunately, current design tools and IP libraries heavily rely on the synchronous paradigm instead, thus making intermediate solutions more attractive and affordable in the short run. An incremental approach with respect to the current design practice consists of mesochronous synchronization. A single clock signal is distributed to the various macrocells in the design with an arbitrary amount of space-dependent time-invariant phase offset (i.e., the skew). Mesochronous synchronization can be viewed not just as an enabler for architecture scalability, but also as a means of eliminating (or relieving) the skew constraints in the clock tree synthesis process, thus resulting in frequency speed-ups, power consumption reductions and fast back-end turnarounds [20]. Unfortunately, mesochronous synchronizers come with their own set of problems. A traditional approach to their design consists of delaying either data or the clock signal so to sample data only when it is stable. This solution requires components often not available in a standard-cell design ﬂow (e.g., conﬁgurable digital delay lines) or explicitly targeting full-custom design (e.g., Muller C-elements). Another issue concerns the synchronization latency, which impacts not just communication performance but has architecture-level implications as well, thus resulting in a more costly system overall. As an example, a latencyinsensitive receiver architecture has to be designed. Flexible synchronizers have been proposed that can support even arbitrary clock frequencies in the communicating domains (e.g., dual-clock FIFOs [3]), however they incur a large area and power overhead. Even custom-tailoring the synchronizer for mesochronous systems does not easily avoid such an overhead, like the scheme in [2], which implies 8 FFs and a mux for each synchronized bitline. In the direction of providing a low-area and low-latency realization of a mesochronous synchronizer, some recent works suggest a source-synchronous design style combined with some form of ping-pong buffering to counter timing and metastability concerns [6], [7]. While [6] avoids a phase detector but requires the link delay to be less than one clock period, the scheme in [7] can handle slow and long links but requires a phase detector. Both schemes feature substantial pros (ease of implementation in traditional design ﬂows, minimal complexity), which motivates the further work that has been done in order to more thoroughly investigate the implications of their utilization in the context of on-chip networks (NoCs) [5], [8]. Knowledge of the target application domain may in fact pave the way for large optimizations of the basic synchronizer architecture and circuits and for tuning the low-level details of the design. Unfortunately, such customization works for synchronizers are not as frequent in the open literature as the proposal of new synchronization concept schemes. As an example, the work in [8] assesses timing margins in a real NoC test case, thus coming up with optimized circuit solutions. Both [5] and [8] address the support for bidirectional communication and for backwards ﬂow control needed for mesochronous signaling in a NoC. In this paper, a further optimized variant of the source synchronous scheme presented in [8] is used as the baseline synchronization scheme for the sake of comparison. In particular, the phase detector is avoided through a careful design of the synchronizer buffer stage and of its behavior at reset. However, while sharing the same synchronizer design 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    philosophy of both [5] and [8], this paper moves a signiﬁcant step forward in the direction of a synchronizer customization for NoCs. In fact, all previous solutions envision a loosely coupled synchronizer with the switch architecture. This approach has the clear drawback of implying a larger amount of buffering resources in the upstream and/or downstream switch of the mesochronous link, which depends on the introduced synchronization latency and on the speciﬁc ﬂow control strategy employed by the network. The idea behind this work consists of merging the synchronizer architecture with that of the switch input buffer, thus coming up with a compact architecture block taking care of ﬂow control, synchronization and buffering and tightly integrated in the switch architecture itself. Beyond making mesochronous NoC design more modular, this approach results in large area savings for the overall switch due to the multipurpose buffer design strategy and in a reduction of the communication latency as well. Layout synthesis of the synchronization-enriched switch architecture by means of a commercial ﬂow proves its feasibility in a 65nm technology under different physical design parameters. II. Related Work Research on mesochronous synchronization started a long time ago. A well-established solution illustrated in [9] consists of delay-line synchronizers, using a variable delay on the data lines. This delay is computed in such a way to avoid switching in the metastability window of the receiving registers. Variable delay lines make this solution expensive and not always available in standard cell libraries. This is the same problem of the works in [12], [13], which use voltage comparators. Several periodic synchronizers are illustrated in [16], which avoid metastability by delaying either the data or the clock signal to sample data when the clock is stable. Conﬁgurable digital delay lines are again needed and experimented frequency is very low. The works in [12], [11], [10] achieve mesochronous data synchronization by using Muller C-elements and digital delay lines that are typically designed with a full-custom approach. [10] presents a self-tested self-synchronization method for mesochronous communication. The scheme uses two clocks with a phase shift of 180◦ and a failure detector is used to select which one to use. In [11] a phase detector in place of a metastability detector is used in the same scheme. Architectures based on FIFO synchronizers are proposed in [9], [14]. FIFO size in [14] depends on the skew, hence is link-dependent or given in the worst-case. Implementation is also very expensive, as showed in [15]. More recently, an optimized bisynchronous FIFO has been proposed in [3] featuring low-latency and small footprint. It can be adapted to the mesochronous needs while proving able to tolerate skew only up to 50% of the clock period. Summing up, mesochronous synchronizers presented so far incur few out of several disadvantages: high implementation overhead, use of non-trivial or full-custom components or low skew tolerance. Moreover, very few works are able to assess timing margins with layout awareness. More recently, the unrelenting pace of technology scaling and the design paradigm shift to NoC-based on-chip multiprocessor systems are bringing physical layer issues to the forefront, especially clock distribution. Solutions able to mitigate them are more urgent, and this has justiﬁed a renewed interest in mesochronous synchronization. However, the perspective is not on the concept synchronization scheme any more, but rather on its suitability for the integration into the on-chip network architecture [18]. This involves assessing timing margins in the target domain and dealing Fig. 1. Baseline synchronizer of [8]. with domain-speciﬁc concerns such as the need to properly handle ﬂow control. An early mesochronous scheme for the SoCBus NoC was proposed in [17], aiming at compact realization while still lacking of a validation on an NoC test case. A signiﬁcant step forward comes from the OCN system [19], which uses a source synchronous scheme. A matcheddelay architecture is used to compensate the strobe skew and enable high-speed mesochronous communication. A FIFO synchronizer is used at the receiver side. Two recent papers [7], [5] both suggest to implement the boundary interface with a source-synchronous design style and propose some form of ping-pong buffering to counter timing and metastability concerns. The SKIL link [5] can support arbitrarily skewed clock signals by relying on a twostage buffer structure and can go through a standard design ﬂow. The same work has been taken a step forward in [4] by considering ﬂow control and support to virtual channels in the synchronizer architecture. Similarly, the approach in [7] aims at assuring that the receiver clock reads data when they are stable. However, the focus of this work is still on long generic links, nothing closely related to NoCs. The study of this synchronizer inside of a NoC layout for short-range mesochronous links is reported in [8], where NoC-speciﬁc timing margins are assessed and ﬂow control implications thoroughly investigated. A similar scheme is implemented in the mesochronous interfaces of the Polaris chip [1]. This paper takes the same approach to synchronization of [5], [8] (source synchronous data transmission, safe storage of data at the receiver side, sampling in the receiver domain only when data is stable) but improves baseline architectures and circuits by providing a more compact and equally robust solution. However, this is used just as the baseline architecture for the sake of comparison with the novel synchronization structure that we propose in this paper. Our guiding principle consists of tightly integrating the synchronizer module into the switch architecture, so to design a multi-purpose switch input stage taking care of synchronization, buffering and ﬂow control. We view this as a way of keeping architecture overhead when moving from fully synchronous to mesochronous clocking close to the minimum. III. Baseline Synchronization Architecture This work moves from the synchronizer architecture presented in [8] and illustrated in Fig.1. The circuit receives as its inputs a bundle of NoC wires representing a regular NoC link, carrying data and/or ﬂow control commands, and a copy of the clock signal of the sender. Since the latter wire experiences the same propagation delay as the data and ﬂow control wires, it can be used as a strobe signal for them. The circuit is composed by a front-end and a back-end. The front-end is driven by the incoming clock signal, and should be synchronized in the receive clock domain. In fact, if a reset removal to a ﬂip ﬂop occurs close to the active edge of its clock, ﬂip ﬂops can enter a metastable state. We use a brute-force synchronizer (available in some new technology libraries as a standard cell) for reset synchronization with the receiver clock. Now the problem arises about how to reset the front-end. Typically, a reset can be sent by the upstream switch. In our architecture, we prevent metastability in the front-end by delaying the strobe generation in the upstream switch by one clock cycle after reset deassertion. This way, on the ﬁrst strobe edge, the receiver synchronizer is already reset. This strobe generation delay is compliant with network packet injection delay after reset. The transmitter clock signal is used as the strobe signal in our architecture. Differently than [8], a larger timing margin is enforced for safe input data sampling. In fact, the transmitter clock signal has to be processed at the receiver in order to drive the latch enable signals. In actual layouts, this processing time adds up to the routing skew between data and strobe and to the delay for driving the latch enable high-fanout nets. As a result, the latch enable signal might be activated too late, and the input data signal might have already changed. In order to make the synchronizer more robust to these events, we ensure that input data sampling occurs in the middle of the clock period. In fact, a switching latch enable signal opens the sampling window of the next latch during the rising edge, and closes the same during the falling one. As a result, the latch enable activation has a margin of half clock cycle to occur. Our post-layout simulations prove that this margin is largely met in practice. Finally, in agreement with [8], we computed the minimum size of the input buffer in the downstream switch to be 4 slots (ﬂits). They are required by the stall/go ﬂow control protocol in order to cover the round trip latency and not to drop ﬂits in ﬂight when a stall signal has to be propagated backwards. The original input buffer size is 2 slots, reﬂecting the requirements of stall/go without synchronization. Please refer to [8] for further details on this. Fig. 3. Proposed tightly coupled synchronizer. IV. Tightly Integrated Synchronizer Architecture The previous synchronizer, similarly to SKIL or to the Polaris one, is a module of the NoC architecture, thus loosely coupled with the downstream switch. The loose coupling stems from the fact that the ﬂip ﬂop in the synchronizer back-end belongs directly to the switch input Fig. 2. Baseline loosely coupled synchronizer of this paper. strobes the incoming data and ﬂow control wires onto a set of parallel latches in a rotating fashion, based on a counter. The back-end of the circuit leverages the local clock, and samples data from one of the latches in the front-end thanks to multiplexing logic which is also based on a counter. The rationale is to temporarily store incoming information in one of the front-end latches, using the incoming clock wire to avoid any timing problem related to the clock phase offset. Once the information stored in the latch is stable, it can be read by the target clock domain and sampled by a regular ﬂip-ﬂop. Counters in the front-end and back-end are initialized upon reset, after observing the actual clock skew among the sender and receiver with a phase detector, so as to establish a proper offset. The phase detector only operates upon the system reset, but given the mesochronous nature of the link, its ﬁndings hold equally well during normal operation. Since few ﬂow control wires are traveling backwards, another similar but much smaller synchronizer needs to be instantiated at the sender to handle them. A. Optimizations of the baseline synchronizer We agree with [8] that it is always possible to choose a counter setup so that the sampling clock edge in the back-end captures the output of the latches in a stable condition, even accounting for timing margin to neutralize jitter. Therefore, no more than 2 latches in parallel are needed in the front-end for short-range (i.e., single cycle) mesochronous communication with this scheme. Other considerations however suggest that a different choice may be desirable at this point. In particular, we feel that by increasing the number of input latches by one more stage it becomes possible to avoid the phase detector (see the new architecture in Fig.2). This would be desirable due to the timing uncertainty or the high area footprint or the non-compliance to a standard cell ﬂow that affects many phase detector implementations. A third latch bank allows to keep latched data stable for a longer time window and to even ﬁnd a unique and safe bootstrap conﬁguration (i.e., counters initialization) that turns out to be robust in any phase skew scenario. Post-synthesis simulations conﬁrmed that the circuit works properly when sweeping the clock skew from -360◦ to +360◦ . Post-layout robustness will be assessed in the experimental section. At regime, the output multiplexer always selects the output of the latch bank preceding the bank which is being enabled by the frontend counter. Rotating operation of both front- and back-end counters preserves this order. In contrast to [8], the reset architecture is designed, as Fig.2 shows. In most SoCs, the reset signal coming into the chip is an asynchronous input. Therefore, reset de-assertion buffer. The mux output is therefore sampled like any other input in the fully synchronous scenario. However, our early exploration indicates that the area overhead induced in this input buffer as an effect of the added synchronization latency is much larger than the synchronizer area itself. This indicates that a tighter integration of the synchronizer into the switch input buffer is desirable. In particular, the latch enable signals of the synchronizer front-end could be conditioned with backward-propagating ﬂow control signals, so to exploit input latches as useful buffer stages and not just as an overhead for synchronization. Should this be the case, input data would be at ﬁrst stored in the latches and then synchronized. This would allow to completely remove the switch input buffer and to replace it with the synchronizer itself. The synchronizer output would then be directly fed to the switch arbitration logic and to the crossbar. The ultimate consequence is that the mesochronous synchronizer becomes the actual switch input stage, with its latching stages acting as both buffering and synchronization stages (see Fig.3). A side beneﬁt is that the latency of the synchronization stage in front of the switch is removed, since now the synchronizer and the switch input buffer coincide. The main change required to make the new architecture come true is to bring ﬂow control signals to the front-end and back-end counters of the synchronizer. This solution would still require 4 slot buffers, i.e., 4 latching banks. However, a further optimization is feasible. The backward-propagating ﬂow control signal (the stall/go signal) could be directly synchronized with the strobe signal in the synchronizer front-end before being propagated to the upstream switch. This would save also the synchronizer at the transmitter side. In fact, the backward-propagating signal would be already in synch with the strobe, which in turn is in synch with the transmitter clock. The ultimate result is the architecture illustrated in Fig.3. We are aware that this latter choice shrinks timing margins for the backward ﬂow control signal, in that it leaves the downstream switch with some generation delay across its synchronizer and also experiences the link propagation delay. This margin will be assessed post-layout in the experimental section, proving the applicability of the scheme and providing simple variants when long links need to be crossed. For this architecture solution, only 3 latching banks are needed in the synchronizer. In practice, only 1 slot buffer more than the fully synchronous input buffer. The tightly coupled synchronizer makes the mesochronous NoC design fully modular like the synchronous one, since no external blocks to the switches have to be instantiated for switch-to-switch communication. Please notice that the reset architecture remains unchanged with respect to Fig.2. A. Operating principle In case a go signal comes from the switch arbiter, at each clock cycle data are latched in the input buffers of the synchronizer, synchronized with the local clock and propagated to the switch arbiter and crossbar. When a stall occurs, the output mux keeps driving the same output until communication can be resumed. While the stall signal gets synchronized with the strobe and reaches the front-end, the front-end latches keep sampling input ﬂits in a rotating way. When the stall signal ﬁnally leaves the synchronizer, it will stop the transmission of the upstream switch and the front-end counter operation at the same time. At this point, the situation is frozen. When then a go arrives, the output mux becomes operational again. Later, input latches and upstream switch resume their operation again at the same time. Please observe that this mechanism does not waste bandwidth on ﬂow resumption, since the synchronizer backend can immediately start sweeping the output of frontFig. 4. Waveforms example of the tightly coupled synchronizer. end latches upon receipt of a go. Interestingly, ﬂow control logic in the synchronizer is simpliﬁed with respect to that of the original switch input buffer. Before, a ﬁnite state machine used to generate a stall by monitoring the number of elements in the buffer. When it was equal to one and a stall came from the switch internal logic, than a stall was also generated for the upstream switch. In the new architecture, the synchronizer just synchronizes the stall signal from the switch logic with the transmitter clock and propagates it upstream. This way, large logic is saved. The latency of both the tightly and the loosely coupled synchronizers varies depending on the skew, and ranges from one to two clock cycles. Fig.4 reports the waveforms showing operation of the tightly coupled synchronizer. A delay from the strobe signal is assumed for the latch enable signals to account for their high-fanout. V. Experimental Results The mesochronous synchronizers illustrated so far have been implemented by means of the xpipesLite NoC library [21]. All the analyzes discussed in this work have been carried out by means of a backend synthesis ﬂow leveraging industrial tools. The technology library is a low-power lowVth 65nm STMicroelectronics library available through the CMP project [22]. A. Comparative latency Analysis Since the tightly coupled synchronizer not only changes the synchronizer implementation but also affects the entire network architecture, we performed basic tests to capture the macroscopic performance differences implied by the different synchronization architectures. We focus on synchronization latency, since the stall/go mechanism implemented Fig. 6. Test-case platform under analysis. (a) Execution cycles (b) Input buffer + synch. area breakdown (c) Power consumption Fig. 5. Performance, area breakdown and power results. in our synchronizer ensures that a stall-to-go transition of the ﬂow control signal can be immediately propagated to the next stage. Hence, there are no wasted cycles at ﬂow resumption, differently than [8]. Since what matters here is not a network-wide performance analysis, but just to investigate the latency of each scheme, this feature can be more conveniently stimulated and analyzed in a simple ad-hoc experimental test case for ﬁne-grain performance analysis. We opted for a simple processor–NoC–memory topology (see Fig.6). The investigated NoC is comprised of a couple of 2x2 switches respectively connected to the processor and the memory. Furthermore, each network switch is connected to its own RX-, TX-mesochronous part meant for synchronizing received data and ﬂow control signals. For the sake of comparison, the SKIL synchronizer is considered as well. This is another loosely coupled module with the switch architecture. The implementation was entirely derived from [5]. The trafﬁc pattern consists of full-bandwidth read and write transactions, i.e., the target memory never stops the access ﬂow. Of course, the only performance differentiation is seen for read transactions, since they are blocking for the processor core, hence they rely on the network ability to keep latency to a minimum. Performance results could be easily interpreted by means of a simple analytical model (Formula 1). It relates performance results to the intrinsic design characteristics of each synchronizer. In fact in the best case, SKIL exposes two cycles synchronization overhead plus a further execution cycle for traversing the network switch; whereas our loosely coupled solution only requires one cycle latency in the mesochronous plus one cycle in the network switch. Even better, the tightly coupled mesochronous synchronizer requires the same computational resources of the vanilla switch (i.e., 1 execution cycle). The reason is that the tightly coupled solution seamlessly replaces the input buffer of the network switch thus providing a fast, reliable and robust mechanism for data synchronization. Summarizing, whenever the system with the tightly coupled mesochronous synchronizer performs a computational task in n cycles, the alternative schemes, i.e., SKIL and the loosely coupled synchronizer respectively require a number of cycles equal to Formula 1, where latency is the number of clock cycles of the deployed mesochronous architecture whereas #transaction is the number of read operations performed by the processor unit. As depicted in Fig. 5(a) there is a direct impact of the adopted synchronization solution on the overall system performance. While the tightly coupled solution keeps the same performance as the vanilla network switch, a performance drop up to 6% incurs when using the SKIL scheme in spite of the very simple test case. cycles = n + latency × 2 × #transactions (1) B. Post-layout analysis We went through a commercial backend synthesis ﬂow and reﬁned RTL description of the mesochronous switches (tightly and loosely coupled) up to the physical layout. The following analysis considers a stall/go ﬂow control scheme. SKIL loosely coupled tightly coupled Critical Path RX TX 0.89 ns 0.28 ns 0.68 ns 0.29 ns 0.49 ns Area (µm2 ) 6400 4380 1200 Area overhead 5.33x 3.65x 1x TABLE I. Post-layout report. Table I reports post-layout critical path as well as area footprint (also Fig. 5(b)) of the investigated mesochronous solutions taken in isolation. The area breakdown in Fig. 5(b) is that of the switch input buffer plus that of the transmitter and receiver synchronizers. Size of the switch input buffer is 4 due to the fact that 2 clock cycles of latency are added in the round-trip from the tx and rx synchronizers (1 cycle each). Therefore, a properly oversized input buffer is needed to handle a stall without incurring any data loss. For the same reason, as deploying a SKIL synchronizer in the link requires 3 clock cycles to traverse the channel, the amount of required buffering resources in the switch input buffer is 6 as shown in Fig. 5(b). For the tightly coupled solution, total area just refers to the multi-purpose switch input buffer (which is also the synchronizer). Concerning performance, post-layout critical path of the tightly coupled solution is not directly comparable with the SKIL and the loosely coupled one as it is fully integrated in the network switch architecture. Nonetheless, as the mesochronous design per se does not represent the performance bottleneck of the whole NoC architecture (which can run at 1 GHz in all cases), the low area footprint overhead of the tightly coupled mesochrounous solution turns out to be an appealing peculiarity when compared with more area-greedy alternatives as SKIL and the loosely coupled synchronizers. C. Skew tolerance This section studies the skew tolerance of the tightly vs. loosely coupled mesochronous synchronizers. In order to estimate the window size where the incoming data can be sampled and held as valid, we performed a complete set of post-layout simulations with 0% → 100% different skews for the investigated synchronizers. Fig.7 reports the hold time for the sampling element fed by the synchronizer’s multiplexer, since the setup time was found to meet even to the loosely coupled solution. In fact in the latter, the mesochronous synchronizer requires an additional power greedy input buffer in the switch in order to properly work. VI. Conclusions In this paper we address the customization of mesochronous communication concept schemes for the target network-on-chip domain. We move from the consideration that a loosely coupled synchronizer with the switch architecture implies a signiﬁcant buffering overhead in the switch input buffer, larger than the mesochronous synchronizer itself. As a consequence, we advocate for tight integration of the synchronizer in the switch. In the proposed architecture, the synchronizer latches act as both synchronization, buffering and ﬂow control resources at the same time, completely replacing the pre-existing input buffer. The resulting architecture proves robust to physical design parameters (skew, link length) and features almost no area overhead with respect to the baseline switch used in fully synchronous networks. Acknowledgements This work has been partially supported by the GALAXY European Project (FP7-ICT-214364), by the Hipeac Network of Excellence (Interconnect Cluster) and by the European Commission in the context of the Scalable computer ARChitectures (SARC) integrated project (FP6 #27648). "
Best of both worlds - A bus enhanced NoC (BENoC).,"While NoCs are efficient in delivering high throughput point-to-point traffic, their multi-hop operation is too slow for latency sensitive signals. In addition, NoCS are inefficient for multicast operations. Consequently, although NoCs outperform busses in terms of scalability, they may not facilitate all the needs of future SoCs. In this paper, the benefit of adding a global, low latency, low power shared bus as an integral part of the NoC architecture is explored. The Bus-enhanced NoC (BENoC) is equipped with a specialized bus that has low and predictable latency and performs broadcast and multicast. We introduce and analyze MetaBus, a custom bus optimized for such low-latency low power and multicast operations. We demonstrate its potential benefits using an analytical comparison of latency and energy consumption of a BENoC based on MetaBus versus a standard NoC. Then, simulation is used to evaluate BENoC in a dynamic non-uniform cache access (DNUCA) multiprocessor system.","Best of Both Worlds: A Bus Enhanced NoC (BENoC)  Ran Manevich1, Isask'har Walter1, Israel Cidon2, and Avinoam Kolodny2  Electrical Engineering Department, Technion – Israel Institute of Technology, Israel  1{ranman,zigi}@tx.technion.ac.il  2{cidon,kolodny}@ee.technion.ac.il  Abstract  While NoCs are efficient  in delivering high  throughput point-to-point  traffic,  their multi-hop  operation is too slow for latency sensitive signals. In  addition, NoCS are inefficient for multicast operations.  Consequently, although NoCs outperform busses in  terms of scalability, they may not facilitate all the  needs of future SoCs. In this paper, the benefit of  adding a global, low latency, low power shared bus as  an integral part of the NoC architecture is explored.  The Bus-enhanced NoC (BENoC) is equipped with a  specialized bus that has low and predictable latency  and performs broadcast and multicast.  We introduce  and analyze MetaBus, a custom bus optimized for such  low-latency low power and multicast operations. We  demonstrate its potential benefits using an analytical  comparison of latency and energy consumption of a  BENoC based on MetaBus versus a standard NoC.  Then, simulation is used to evaluate BENoC in a  dynamic non-uniform  cache access  (DNUCA)  multiprocessor system.†    1. Introduction  Novel VLSI literature promotes the use of a multistage Network-on-Chip (NoC) as the main on-chip  communication infrastructure (e.g., [2], [4], [6]). NoCs  are conceived to be more cost effective than buses in  terms of traffic scalability, area and power in large  scale systems [3]. Thus, NoCs are considered to be the  practical choice  for  future CMP  (Chip MultiProcessor) and SoC  (System on Chip) system  communication.   The majority of  the  traffic delivered by  the  interconnect in SoC and CMP systems involves latency  insensitive, point-to-point, large data transfers such as  DMA memory replication. However, other kinds of  communication   should   also   be   facilitated   by   the  ______________________________________________________________________________ †A preliminary concise version of this work was published in IEEE  Computer Architectures Letters, Volume 7, Issue 1, 2008.  interconnect.   Examples  include L2 cache read  requests, invalidation commands for cache coherency,  interrupt signals, cache line search operations, global  timing and control signals and broadcast or multicast  valid recourses query. Although the volume of traffic  of these operations is relatively small, the manner in  which the interconnect supports them heavily affects  the performance of the system due to their latency  sensitive nature. While  interconnect architectures  which solely rely on a network are cost effective in  delivering large blocks of data, they have significant  drawbacks when other services are required. Multi-hop  networks impose inherent multi-cycle packet delivery  latency on the time-sensitive communication between  modules. Moreover, advanced communication services  like broadcast and multicast incur prolonged latency  and  involve additional hardware mechanisms or  massive duplication of unicast messages.   Current NoC  implementations  are  largely  distributed  (borrowing  concepts  from off-chip  networks). We argue that the on-chip environment  provides  the architect with a new and unique  opportunity to use ""the best of both worlds"" from the  on-chip and  the off-chip worlds. In particular,  communication schemes that are not feasible in large  scale networks become practical, since on-chip  modules are placed in close proximity to each other.  Consequently, we propose a new architecture termed  BENoC (Bus-Enhanced Network on-Chip) composed  of two tightly-integrated parts: a low latency, low  bandwidth specialized bus, optimized for system-wide  distribution of control signals, and a high performance  distributed network that handles high-throughput data  communication between module pairs (e.g., XPipes  [2], QNoC  [4], AEthereal [6]). We also propose an  implementation of a BENoC bus termed MetaBus,  optimized for low latency and multicast, that is used  throughout the paper. As the bus is inherently a single  hop, broadcast medium, BENoC is shown to be more  cost-effective than pure network-based interconnect.  Fig. 1 demonstrates BENoC for a cache-in-the-middle  CMP. In this example, a grid-shaped NoC serves pointto-point transactions, while global, time critical control    978-1-4244-4143-3/09/$25.00 ©2009 IEEE                      2. BENoC Service  BENoC is composed of two tightly related parts: a  packet switched network (e.g., Xpipes[2], QNoC [4],  AEthereal  [6])  for point-to-point massive data  transfers, and MetaBus that functions as a low latency  broadcast/multicast/unicast media. MetaBus is used for  NoC subsystem control, propagation of critical signals  and special custom services. In this section we describe  some of the benefits of using BENoC.  2.1 BENoC for latency sensitive signaling  In typical NoC-based systems, packets that traverse  a path of multiple hops suffer from high latency, due  the routers switching and routing delays accumulated  along its way. This latency is often unacceptable for  short but urgent signaling messages required for the  timely operation of the system. This is stated many  times as one of the main obstacles for an early  adoption of a NoC-based architecture. MetaBus,  designed for low bandwidth and short latency, offers a  valuable alternative: urgent messages may be sent over  the bus, traversing only a single arbitration stage. This  enables quick delivery of time critical signals between  modules.  2.2 BENoC multicast services  BENoC enables efficient  implementation of  advanced communication services. For example, a SoC  may include multiple specialized resources distributed  across  the chip  (e.g., DSP processors, ALUs,  multipliers, memory banks). A processor may wish to  send a task to one (or more) of these resources. Hence,  the processor needs to know which of them are idle. As  an alternative to extensive probing, BENoC can  integrate an anycast service where a task is delivered  obliviously to one of the idle resources. For instance,  the processor may initiate a bus multicast destined at  ""any idle multiplier"". In response, idling multipliers  may arbitrate for the bus and send back their ID, while  the data itself is delivered point-to-point over network.  Even more sophisticated buses may  include a  convergecast mechanism that facilitates the efficient  collection of acknowledgements or negative responses  back to the initiator. Finally, the most basic service  provided by  the bus  is a multicast (broadcast)  operation: In order to deliver a message from one  source to a group of (all) destinations using a basic  NoC, the sender needs to generate multiple unicast  messages [5]. While NoCs may include a built-in  multicast mechanism (e.g., [7]), it will fall behind the  simplicity and low latency of the proposed bus.  Figure 1. A BENoC-based CMP system,  composed of 16 processors and L2 caches.  messages are sent using the bus.  BENoC's bus (e.g. MetaBus) is a synergetic media  operating in parallel with the network, improving  existing functionality and offering new services. In  previous NoC proposals that include a bus (e.g., [10],  [11]) the bus typically serves as a local mechanism in  the interconnect geographical hierarchy. In [11], each  cluster of modules uses a shared bus for intra-cluster  traffic while inter-cluster traffic uses the network. This  way, the delivery of data to short distances does not  involve the multi-hop network. [10] suggests a busNoC hybrid within a uniprocessor system. By  replacing groups of adjacent links and routers with fast  bus segments performance is improved. However, the  data which is delivered over the network is also  delivered over the bus. In contrast, BENoC's bus is  used to send messages that are different than those  delivered by the network, such as control and multicast  messages.  This paper also introduces the benefit of BENoC for  facilitation cache access, coherency and search in CMP  systems. In particular the searching for migrating cache  lines in CMP DNUCA (Dynamic Non-Uniform Cache  Architecture). More services of the built-in bus include  NoC subsystem control, multicast, anycast and  convergecast services.   The paper is organized as follows: in Section 2, we  discuss possible usages of the BENoC architecture. In  section 3 we introduce MetaBus – a BENoC custom  bus  implementation. MetaBus employs a novel  combination of a tree topology and a power saving  masking mechanism that disables signal propagation  toward unused tree leaves. In section 4 we analyze  MeatBus based BENoC  latency  and  energy  consumption  and  show  it's  advantage  over  conventional NoC. Our latency model is confirmed  using a  layout  implementation described  in  this  section. In section 5 we analyze the power saving  benefits of the masking mechanism. In section 6 we  evaluate the proposed architecture using network  modeler simulation. Section 7 concludes the paper.                 2.3 BENoC for CMP cache  A broadcast operation is valuable in shared memory  CMP systems. Typically, each of these processors is  equipped with a local (L1) cache and they all share a  distributed L2 cache (Figure 1). In order to facilitate  cache coherency,  the system should provide a  mechanism that prevents applications from reading  stale data. More specifically, when a processor issues a  read exclusive (i.e., read for ownership) command to  one of the L2 caches, all other processors holding a  copy of that cache line should invalidate their local  copy, as it no longer reflects the most updated data.  Such invalidation signal is best propagated using a  broadcast/multicast service. As wire latency becomes a  dominant factor, the L1 miss penalty is heavily  affected by the distance between the processor and the  L2 cache bank holding  the fetched  line. This  observation gave rise to the DNUCA (Dynamic NonUniform Cache Architecture) approach: instead of  having a few statically allocated possible L2 locations,  cache lines are moved towards processors that access  them [8], [9]. One of the major difficulties in  implementing DNUCA is the need to lookup cache  lines: whenever a processor needs to conduct a line fill  transaction (fetch a line into its L1 cache), it needs to  determine the identity of the L2 cache bank/processor  storing its updated copy. As described above, in a  network-based interconnect, the line can be looked for  using multiple unicast probes. BENoC offers a more  efficient alternative: MetaBus can be used to broadcast  the query to all cache banks. The particular cache  storing the line can acknowledge the request over the  bus and simultaneously send the line's content over the  NoC. As queries include small meta-data (initiating  processor's ID and line's address), they do not create  substantial load on MetaBus.  2.4 BENoC for system management  MetaBus can also facilitate the configuration and  management of the NoC itself. For example, when  changing the system's operation mode (""usecases"" in  [12]), the network may need to be configured. Such a  configuration may include updating routing tables,  adjusting link speeds and remapping the system  modules address space. It may also be desirable to shut  off parts of the NoC when they are not used for a long  time in order to save power. Interestingly, although  these operations are not performed during the run-time  of the system, they should be handled with extreme  care, since the configuration of network elements may  interfere. For example, if a configuration packet turns  off a certain link (or a router), other configuration  messages may not be able to reach their destination due  to ""broken paths."" Similarly, trying to update routing  table while the network is being used to deliver other  configuration messages is problematic. Alternatively,  configuration can be done via MetaBus making the  configuration and management process much simpler.   3. MetaBus implementation  In this section we present MetaBus – a custom bus  for BENoC. The design principles and guidelines are  presented first, followed by bus architecture, control  mechanisms and communication protocol.  3.1 MetaBus Architecture   MetaBus serves as a low-bandwidth complementary  bus aside a high-bandwidth network. Conventional  system busses (e.g. [20], [21], [22]) are too expensive  in terms of area, power and system complexity for the  limited bus tasks in the BENoC architecture. Thus,  MetaBus does not utilize segmentation, spatial reuse,  pipelining,,  split  transactions and other costly  throughput boosting mechanisms. MetaBus should  pose a  low, predictable  latency communication  medium that outperforms the network in terms of  power and latency for short unicast, broadcast and  multicast transactions.   MetaBus is constructed as a tree (not necessarily  binary or symmetric) with the communicating modules  on its leaves (Fig 2). The bus is comprised of a root  and ""bus stations"" on the internal vertices of the tree.  Bus access is regulated by the well known Bus Request  (BR) – Bus Grant (BG) interface. For example, if,  module 2 wishes to transmit a metadata packet to  module 9, it issues a bus request that propagates  through the bus stations up to the root. The root  responds with a bus grant that propagates all the way  back. At this stage of the transaction a combinatorial  path between the transmitting module and receiving  modules is built up. When the bus grant is received,  module 2 transmits an address flit that is followed by  data flits. These flits first go up to the root and then are  spread throughout the tree (broadcast) or through  selected branches (unicast or multicast).   A masking mechanism that is mastered by the root  and configured according to the address flit, prevents  the data from reaching unnecessary tree branches and  thus saves power in unicast and multicast transactions.  After the reception of all valid data flits, the receiver  (module 9) releases the bus by an acknowledgement to  the root and a new transaction may take place.                     receivers. The delay of this path defines the minimum  clock period for the bus.  3.1.3 Masking. In conventional busses, data reach all  the receivers connected to the bus (or to a bus segment)  even  in a unicast  transactions. The masking  mechanism's role is to save power by preventing the  data to reach unnecessary modules.  Figure 4. Masking Concept.  The mask control logic is located in the root and  controls data propagation from the root down using a  dedicated line to every bus station. In Figure 4 module  3 transmits data to modules 1 and 5. Due to masking,  the data do not reach 6 of the 7 non-recipient modules,  saving redundant switching of wires and gates.  3.1.4 Addressing. Unlike conventional system busses,  MetaBus does not include a separate address bus for  simplicity and area reduction. We dedicate the first  word of every transaction for the destination address,  which is consistent with the NoC addressing scheme.  Transactions length are either constant or defined by  the sender using an End Of Transaction signal. We  allocate part of the address space to define commonly  used pre-defined multicast sets. For instance if the data  bus width is 8 (K=8) and we have a 64 modules  system, we have 192 possible addresses for pre-defined  multicast sets (one of them is broadcast). This scheme  is similar to the multicast address convention in  Ethernet and IP.  3.1.5 Acknowledge signals. The bus supports two  feedback signals – Ack and Nack. Each module has  active high Ack and Nack outputs that are joined with  AND gates in the bus stations and form a global Ack  and a global Nack that are generated in the root. Since  the bus is acting as a fast, low-bandwidth metadata  transmission medium aside a high bandwidth network,  many of the signals delivered through the bus may  trigger a response through the NoC. The Ack and Nack  may be used to distinguish between the case when the  Figure 2. An example of a 9 modules MetaBus.  MetaBus data and  its control  signals are  synchronized with a bus clock that is connected only to  the root and to the modules in the leaves of the tree.  Bus stations are clocked by a Bus Grant signal from  the level above them.  3.1.1 Arbitration. The proposed bus utilizes a  distributed arbitration mechanism. The BR/BG  interface is used between all bus units including  modules, bus stations and the root. A requesting bus  station, if granted, will pass a BG to one of its  descendant vertices. The arbitration block in the bus  stations (Fig. 3) uses the BG input as a clock for its  edge sensitive arbitration state machine. The proposed  mechanism enables arbitration priorities adjustments  by placing higher priority modules in higher tree levels  or manipulating specific bus stations arbitration logic.   Figure 3. Arbitration and data switch block in a  binary bus station and their connectivity.  3.1.2 Data Path. MetaBus data path is combined of  two parts – from the transmitter to the root and from  the root to the receivers. A combinatorial route  between the data sending module and the root is  established during bus grant and propagates down to  the transmitter by data switches in the bus stations  (Fig. 3). From the root down, the data propagates to the  designated recipients and is blocked from arriving to  irrelevant modules by a masking mechanism described  in the next sub-section. Once established, there is a  combinatorial data path between the transmitter and the                           transmitter is supposed to receive data through the  NoC (Ack) and the case when all the recipients  intercepted the current message and free the bus  (Nack).   3.2 State machine and communication protocol  In this sub-section we present a communication  protocol for a variable length transaction bus with a  data valid line. The proposed bus is clocked at the root  and the modules in the leaves. The bus state machine is  implemented in the arbitration unit in the root and is  presented in figure 5. The events sequence starts from  the ""Wait for BR"" state. As soon as one of the modules  issues a bus request by setting its BR high, one of  root's BR inputs goes high. Root arbitration unit issues  a BG to one of the children that requested the bus  according, for example, to a round robin principle.  Once BG is issued, the root waits for the first word  from the transmitter. This word includes the address of  the receiver or a multicast set and is marked with a data  valid signal. Then, according to address decoding,  mask lines are activated. Following the masking, data  is transmitted until sender sets data valid bit to ""0"".  Next, the root de-asserts the mask by masking all the  bus again, waits for Ack or Nack, de-asserts BG timed  according to the received acknowledgement type and  passes again to BR wait state.    We dedicate half of the bus clock cycle for  interactions between the modules and the root (bus  grant  propagation,  address word  decoding,  acknowledgement, masking setting, etc.) and a full  clock  cycle  for  data  transmission  between  communicating modules. Under these assumptions  each transaction requires 3 clock cycles for arbitration,  addressing, masking and acknowledgement processes.  For each transaction we define the  transaction latency  as the number of bus clock cycles required from bus  request of the transmitter until the last data word  clocked  by the receiver. In the proposed protocol, a K  data words transaction latency is K+2.5 clock cycles.    Figure 5. Bus state machine. * - BG goes low  according to acknowledge type.  4. Latency and power analysis  For simplicity, the NoC (including the NoC part of  BENoC) is assumed to have a mesh topology, where  system modules are square  tiles. The following  notation is used: n  is  The number of modules in the  system, ΔV is the logic voltage swing, C0 and R0 are  the capacitance and resistance of wires per millimeter  of length and P is tile size [mm]. The number of global  wire segments between the modules and the root is  denoted by BD, and LW is the data link width in bits.  The minimal clock period for MetaBus is the sum  of all segment latencies along the data path. The NoC  latency is the number of NoC clock cycles required to  complete a given wormhole transaction. Our power  dissipation analysis takes into account the switching of  global wires and their drivers, for both the NoC and  MetaBus. Local logic power dissipation is neglected.   We define   as the technology time  constant, where Rinv and Cinv are the input capacitance  and the effective output resistance of an inverter.  Assuming that the load capacitance Cdriver at the end of  each wire segment on the bus is a driver of another  identical segment, we get an upper bound for the bus  segment delay  [13]:  R C τ(cid:17) inv inv T = 0.7 + C Driver ) ( τ⎛ C ⎜ ⎝ Wire C Driver + R C Wire Driver + 0.4 R C Wire Wire (1)  ⎞ ⎟ ⎠ If the capacitance driven by the wire is a small gate  such as in a NoC link, the load is negligible, and the  delay becomes:   T = 0.7 C τ Driver C Wire + 0.4 R C Wire Wire                 (2)  = T N ,net uni Nclk flits + T CiR Nclk n N T For the latency and energy of transactions in a NoCbased system, we assume that a broadcast is composed  of multiple unicast messages. Since an average unicast  message has to travel  n modules away from the  source, the minimal time to complete the unicast is:                    (3)   Where TNclk stands for NoC clock period, NCiR for the  router latency in clock cycles and Nflits for the number  of flits per transaction. In Broadcast, we assume that  the first messages are sent to the most distant modules,  and the last ones to the nearest neighbors. We assume  that the source transmits a flit every NoC clock cycle:                              (4)    Note that the NoC latency approximations used do  not account for network congestion, and thus, may  underestimate the realistic values.    Assuming a NoC link is P millimeters long, its  resistance and capacitance are   and  . We use (2) to find the appropriate NoC  driver strength CND. Therefore, the NoC link delay is  = ⋅ P C = ⋅ P R ≈ ⋅ n T ,net broad NLC NLR Nclk flits N T 0 0                                                                equal to one half of NoC clock cycle under the  assumption that between two routers, data is sent and  received on opposite network clock edges.  The input  capacitance of the routers is neglected since this is a  capacitance of a driver that drives an internal logic, and  therefore is small. We get:   0.7 τ C − 0.4 0.5 T NL ND Nclk NL NL C R C =                   (5)  In order to calculate the total energy required for  NoC broadcast, the number of packet transmissions is  determined. In a regular mesh, a source node may have  at most 8 modules at a distance of one, 16 modules two  hops away, 24 modules three hops away and so on. In  the energy-wise best case, the broadcasting module is  located exactly in the middle of the mesh. It therefore  has to send 8 messages that would each travel a single  link each, 16 messages that travel two links, and in  general, 8j messages to a distance of j hops, until  transmitting a total of n-1 messages. It can be easily  shown that if n is an integral, odd number, then the  Manhattan distance between the module in the middle  of the mesh and the ones in its perimeter is exactly  . Since a message transmitted to a  destination j hops away has to traverse j router-torouter links, the minimal number of transmissions  required to complete the broadcast is  ( ) max 1 / 2 D n = − max 2 max max 0 = ⋅ + 8 1 16 2 24 3 ... 8 ⋅ + ⋅ + + 8 D j K D D j = ⋅ = ∑      (6)  In the second data path part data spread from the root  across the whole tree in broadcast and, using masking,  Consequently, the lower bound of the total energy  consumed by a single broadcast operation would be:               propagates to a much smaller set of modules that              (7)  includes  the recipient of a unicast (from space  considerations, multicast  is not presented). We  An average network unicast transaction would  distinguish between data driver strength of the first part  comprise of   hops, therefore:   and the second part since the driver of the second part,  potentially drives a larger capacitance by a factor BR,  where BR is the bus tree rank. We define γ as the  ""wires overlap factor"". This factor is 1 if data wires  from the root down are physically separated, and it  asymptotically reaches 1/BR if data wires are separated  only very near to units that share the same upper level  unit. The role of this factor is illustrated in figure 7. In  broadcast, the power consumption for the way down is:  ∑ (10)  Where CBD,down is the same as CBD,up but for the second  part of the data path (from the root down).  In a unicast, due to masking, only BR links are  loaded in every tree level, thus:  ( ) 2 net flits NL ND E = Δ V N K C C + ( ) max 1 / 2 D n = − ( ) ( ) 2 net uni , n ⎞− 1 2 flits W NL ND E = Δ V N L C C ⎛ ⎜ ⎜ ⎝ ⎟ ⎟ ⎠ +        (8)  Note that (7) and (8) underestimate the NoC energy  consumption since in-router power is neglected.  Similarly, we estimate the latency, and energy  consumption for MetaBus. The data path is comprised  of two parts – from the transmitter up to the root, and  from the root down to the receivers. The die size is  defined as PD=√nP. We assume that the average hop  distance between the modules and the root is PD/2 and  that the bus stations are uniformly spread along this  distance. An average data link segment length between  two bus units would be PD/2BD.   The actual structure of the first part of the MetaBus  data path (from the modules up to the root) is  illustrated in figure 6. The second part of the data path  (from the root down) differs with the fan-out of each  bus station. Moreover, the data switch is replaced with  a masking gate that drives the same driver, but this  difference does not change the delay according the  model we use (1). The model of a driver that drives a  wire with an identical driver in it's end, implies an  upper delay bound for the second part of the data path  since the capacitances of the large drivers are loaded  simultaneously by the input gates of the bus stations.    The first data path part is identical for both  broadcast and unicast transactions, data goes through  BD global wire segments. The energy required for this  part is given by:  ( ) 2 bus up , BD up , flits D BL E = Δ V N B C C +          (9)  Where CBL stands  for MetaBus data segment  capacitance and equals to C0PD/(2BD) and CBD,up is the  capacitance of drivers that drive data segments in the  first part of the data path.     Figure 6. MetaBus modules-to-root bus  segment. BG's are Bus Grant signals to the next  bus stations. These signals also control the data  switch.  − 1 2 bus broad down , , BD down , = 1 = 1 D D B B n R n R flits BL n n E = Δ V N γ C B C B ⎛ ⎜ ⎜ ⎝ ⎞ ⎟ ⎟ ⎠ + ∑ ( ) ( ) 2 bus uni down , , BD down , 1 flits R D BL D E = Δ V N B B C γ B C + −   (11)                                                                       Figure 7. The parameter gamma describes the  amount of wire sharing in the MetaBus datapath  from the root to the leaves (gamma=1 means no  wire sharing, gamma = 1/2 for maximal wire  sharing in a tree of rank 2).  We deduce bus data path delay by summing the  delays of global wires segments. Using (1) we get:   ( ) BUS UP , link UP , BD up , BL BD up , BD up , 0.7 0.4 D BL D BL BL T B T ⋅ τ C C B R C R C C = = ⎛ ⎜ ⎜ ⎝ ⎞ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎝ = ⎞ ⎟ ⎟ ⎠ + + +     (12)  ( ) BUS DOWN , link DOWN , BD down , BL BD down , BD down , 0.7 0.4 D R BL D BL BL T B T ⋅ τ γ B C C C R C B R C γ = ⎛ ⎜ ⎜ ⎝ ⎞ ⎟ ⎟ ⎠ ⎛ ⎜ ⎜ ⎝ ⎞ ⎟ ⎟ ⎠ + + +  (13)  Where RBL stands for bus data segment resistance  and equals to R0PD/(2BD). We find the optimal drivers  sizing CBD,up and CBD,down by finding the minima of the  functions in (12) and (13) and get:   BD up opt , − BL BL τ C R C = 2 BD down opt , − R BL BL τ γ B R C C =    (14)  Consequently, using (12) and (13) with the optimal  driver sizes from (14) we can get the estimation of the  MetaBus minimum data path delay:  ( ) ( ) BUS UP , BD up opt , − BUS DOWN , BD down opt , − MetaBus T T C T C = +     (15)  A bus transaction requires 1.5 clock cycles for  arbitration and bus access management before data  transmission and a single clock cycle afterwards for  acknowledgement (section 3.3). For latency calculation  we consider only the first 1.5 cycles, thus, Nflits  transaction latency would be:  ( )1.5 flits MetaBus Bus Latency N T = +                 (16)  Using (9), (10), (11), and (14) we can deduce the  total bus energy consumption for Nflits transaction.   The derived expressions are valuated using typical  values for 0.18um  technology (C0=243e-15F/mm;  R0=22Ω/mm; τ=7.5ps ; ΔV=1.5V, [15] [16]). We also  assume a die size PD=10mm, γ=0.3 (a reasonable  number for a topology where architecturally close bus  stations and modules are close topologically as well),  NoC clock frequency 1/TNclk=1Ghz, data link width  LW=16, and the number of flits Nflits=3 including the  address flit, for both MetaBus and NoC cases.  Analytical results of power dissipation vs. number of  modules for MetaBus and NoC broadcast and unicast  transactions are presented in figure 8. MetaBus is  assumed to be a homogeneous tree with a rank of 4.  and its drivers were speed optimized according to (14).  The broadcast transactions latencies of the NoC and  MetaBus are presented in figure 9. Note that the  MetaBus  latency decreases with  the number of  modules which seems counter intuitive. This happens  since the bus stations are acting as repeaters along the  long resistive data interconnect. Calculations for a 64  modules system are presented in table 1 for future use.  0 0.5 1 1.5 2 2.5 3 3.5 0 5 10 15 20 25 30 35 40 Number of Modules e n E r e p y g r r t c a s n a i t n o [ J n ] MetaBus Broadcast Network Broadcast MetaBus Unicast Network Unicast Figure 8. Bus and NoC unicast and broadcast  energy per transaction.   0 20 40 60 80 100 120 0 5 10 15 20 25 30 35 40 Number of modules a L t y c n e [ s n ] MeatBus Network Broadcast Network Unicast Figure 9. Bus and NoC broadcast latencies.  Note that for a reasonable system size, MetaBus  significantly outperforms the NoC in terms of latency  for both unicast and broadcast transactions. Therefore,  BENoC latency should also outperform a NoC with a  built in broadcast replication mechanism. Our analysis  predicts MetaBus to also outperform NoC in energy  consumption for broadcast transactions.  Table 1.  MetaBus  Type Speed Opt. NoC Energy [nJ] Broad Uni 4.13 1.26 5.69 0.13 Latency [ns] Broad Uni 1.98 1.98 192 27                                                                                             Our latency model was verified by simulations  using extracted layout parameters of a 0.18um process.  First we constructed a floor-plan of a balanced 64  modules binary tree on a 10mmX10mm die (Figure  10). Then automated layout and routing with timing  optimization was performed. Our tools do not perform  repeater insertion. In addition, although the MetaBus  infrastructure occupied less than 0.1% of the die area,  the layout tool did not perform wire sizing and rarely  used high metal levels. Despite these limitations, by  deducing average driver strength, wire capacitance and  resistance per unit of length (C0=130e-15F/mm, R0~1K  Ω/mm, CBD,UP~3.7e-15F, CBD,DOWN~5.8e-15F) and  using τ=7.5ps, we estimated  the data-path delay  using our model and compared the results to the postlayout  timing simulation results. The bus delay  according our model was ~8.5ns while the minimum  post layout simulation data-path delay was ~10ns –  within 15% from the estimation.  Figure 10. A 64 modules binary MetaBus floorplan. The bus stations are bounded with  rectangles that were placed by hand. The blue  thick lines stand for airlines of data links and  arbitration interfaces between bus-stations and  form a tree structure. The thin lines are the  masking wires that go separately from the root to  each bus-station.  5.  Masking mechanism benefits  In figure 4, the inputs of modules and bus stations  that are driven with unmasked bus station are counted  as ""active gates"". The power calculations consider the  active gates and the wires power. We define ""power  ratio"" as the ratio between the power consumption of  data transfer from the root to the modules with and  without the use of the  masking mechanism . The  power consumption of the first data path part is much  smaller than that of the second data path part, even  with masking (by a factor of the tree rank for unicast  and by a higher ratio for multicast and broadcast).  Therefore, we disregard the first part in our power ratio  calculations. In terms of active gates this ratio would  be:  PR = ( AG TG ( Active Gates ) Total Gates )                           (17)  We also disregard the contribution of the masking  mechanism to the power consumption since it only  drives, once in a transaction, a single global line per  active bus station.                       (a)                                    (b)  Figure 11. 3 levels trees with a rank of 4. The 4  highlighted leaves are the recipients. Blue bus  stations are not masked (active).  For the sake of simplicity, we assume a balanced  tree. In multicast transactions, the masking mechanism  power saving efficiency depends on  recipient's  distribution across the tree leaves. Dispersed multicast  sets utilize non-adjacent bus-stations across the tree, a  fact that increases the number of active gates (Fig. 11).  We deduce an analytical lower bound of the number  of active gates for a given multicast set size (MN). It is  obvious that the lowest active gate count is achieved if  modules are arranged from left to right (or vise versa)  as in figure 11a. In this example, AG starts from 12  and jumps by 4 (R=Tree Rank) every 4 modules, by 8  (2R) every 16 modules, by 12 (3R) every 64 modules  etc. Generally, in tree of depth D:  AG MN ( ) DATA = R D ( − + 1) ⎛ ⎜ ⎝ ⎞− 1              (18)  ∞ ∑ K = 1 MN R ⎢ ⎢ ⎣ K ⎥ ⎥ ⎦ ⎟ ⎠ A simulation of power ratio vs. multicast set size for  256 modules trees with ranks of 2,4 and 16 was  performed. For each rank and multicast set size we  have deduced the average PR of 10,000 different  random sets. In figure 12 we present the results  together with the minimum bound analytical results  from Eq. 18. As expected, the masking mechanism is  highly effective for unicast and small multicast sets.  Moreover, if modules are topologically arranged with  masking and multicast set awareness such as that  commonly used sets are concentrated topologically in  the tree, the masking mechanism stays effective even  for large multicast sets.                                                                          o i t a r r e w o P 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 R=2, Average R=4, Average R=16, Average R=2, Min R=16, Min 0 0.2 0.4 0.6 0.8 1 Re lativ e multicast se t size Figure 12. Average and analytical minimum  power ratios.  6. Experimental results  In this section, we evaluate the performance of a  large scale chip multi-processor (CMP) using dynamic  non-uniform cache access (DNUCA) for the on-chip  shared cache memory. The cache is comprised of  multiple banks, as illustrated in Figure 1. We compare  a classic NoC and a MetaBus based BENoC  infrastructure. We use  the 0.18um  technology  parameters  from  section 3. Two  time-critical  operations are addressed. The first is a basic line-fill  (""read"") transaction, performed by a processor that  reads a line into its L1 cache. If an L2 cache has a valid  copy of the line, it must provide its content to the  reading processor. If the most updated copy resides in  a L1 cache of another processor, it is asked to  ""writeback"" the line. Else, the line is fetched from a  lower memory hierarchy level (L3 cache or memory).  The second transaction being addressed is the read-forownership  (""read-exclusive"")  transaction. While  similar to the basic line-fill operation, it also implies  that the reading processor wishes to own the single  valid copy of that line for updating its content. In order  to complete the transaction, all other L1 copies of the  line (held by an owning processor or by sharers) must  be invalidated.  In a classic DNUCA implementation, the processor  has to lookup the line prior to the read/read exclusive  operation. When a regular NoC is used, the line is  sought using multiple unicast messages, while in  BENoC the search is conducted over MetaBus. In this  work, a distributed directory model is assumed: each  L2 cache line includes some extra (directory) bits to  keep track of the current sharers/owner of the line  [1].  As explained in Section 2, a static directory would  render the DNUCA policy useless; hence these bits  migrate together with the line. The simulated system  consists of 16 processors and 64 L2 cache tiles (80  modules in total). The modules are arranged as  depicted in Fig 1, while the 4x4 cache array is replaced  by an 8x8 array of banks. The network link is set at  16Gbits/s (reflecting 1Ghz network clock and 16 bits  wide links). MetaBus is 16 bits wide with transaction  latency of 4ns (twice the optimal number from table 1).  In order  to evaluate  the proposed  technique, a  combination of two simulators is used. The BENoC  architecture is simulated using OPNET [15]. The  model accounts for all network layer components,  including wormhole flow control, virtual channels,  routing, buffers and link capacities. In addition, it  simulates the bus arbitration and propagation latencies.  The DNUCA system is modeled using the Simics [17]  simulator  running  SPLASH-2  and  PARSEC  benchmarks [18] [19].    APACHE BARNES CANNEAL FACESIM OCEAN SPECJBB WATER Average NoC (a)  BENoC 700 600 500 400 300 200 100 0 160 140 ] s n [ y c n e a L t n o i t c a s n a r T c e s / 120 s n o i t c a s n a r t a g e M 100 80 60 40 20 0 NoC (b)  BENoC       Figure 13. Performance improvement in  BENoC compared to a NoC-based CMP for 7  different applications. Upper (a): average read  transaction latency [ns]. Lower (b): application  speed [trans./sec].  Fig. 13 presents the improvement achieved by the  BENoC architecture compared to a classic NoC in  terms of average transaction latency and application  speedup. Average transaction latency is 3-4 times  lower  in BENoC (Fig. 13a). Fig. 13b depicts  application execution speed  in  terms of  read  transactions/sec. On average, BENoC introduced an  execution speedup around 300%. The FACESIM  benchmark was accelerated only by 3% since it  produces relatively little coherency traffic.                               7. Summary  The BENoC architecture, described in this paper,  combines a customized bus with a NoC for getting the  best of two worlds: the low latency and broadcast  capability inherent in the bus, together with the spatial  reuse and high throughput of the network. The  customized bus can circumvent most weaknesses of the  NoC, since critical signals which require low latency  are typically comprised of just a few bits. Similarly, the  complexity and cost of broadcast operations in the  NoC can be avoided by using the bus, because only  short metadata messages are usually transmitted in  broadcast mode.. Operations  requiring  global  knowledge or central control can be  readily  implemented on the bus, and typically do not involve  massive data transfer. The bus can also support  specialized operations and services, such as broadcast,  anycast and convergecast, which are important for  common operations such as cache line search and  cache invalidation. BENoC is superior to a classical  NoC in terms of delay and power. In addition to the  BENoC concept, a customized and power/latency  optimized bus termed MetaBus was presented. Our  analysis shows that a BENoC using a MetaBus is more  advantageous than a traditional NoC even for a  relatively small system size of 10-20 modules, and the  advantage becomes very significant as system size  grows. Consequently, BENoC  type architectures  introduce a lower risk and a faster migration path for  future CMP and SoC designs.  8. "
Using adaptive routing to compensate for performance heterogeneity.,"Scalable and power efficient multi-core architectures must be performance heterogeneous to accommodate semi-conductor parametric variations and non-uniform access to shared resources. Due to its rate matching, a NoC on a Voltage-Frequency Island architecture can connect cores without forcing each one to give up its own operating point for the chip-wide common worst case. With run-time adaptive routing and task-to-core mapping, a NoC can run at the average not the worst case network saturation bandwidth. These run-time processes compensate for variations because they match application resource requirements with heterogeneous cores and routers. We focus on adaptive routing that simultaneously combats communication load imbalance from on-die variations and application topology. We show that even with static, fixed task-to-core mapping on multi-core architectures affected by stochastic variations, our MATC router increases the expected saturation bandwidth by 7-25% vs Dimension Order router. With systematic variations, the improvements are 5-50%. These gains compensate for saturation bandwidth degradation due to manufacturing variations and help to reduce design guard-bands.","Using Adaptive Routing to Compensate for Performance Heterogeneity Yury Markovsky, Yatish Patel and John Wawrzynek {yurym, yatish, johnw}@cs.berkeley.edu University of California, Berkeley Abstract Latency Homogeneous NoC Oblivious Adaptive Heterogeneous NoC Oblivious Adaptive Scalable and power efﬁcient multi-core architectures must be performance heterogeneous to accommodate semiconductor parametric variations and non-uniform access to shared resources. Due to its rate matching, a NoC on a Voltage-Frequency Island architecture can connect cores without forcing each one to give up its own operating point for the chip-wide common worst case. With run-time adaptive routing and task-to-core mapping, a NoC can run at the average not the worst case network saturation bandwidth. These run-time processes compensate for variations because they match application resource requirements with heterogeneous cores and routers. We focus on adaptive routing that simultaneously combats communication load imbalance from on-die variations and application topology. We show that even with static, ﬁxed task-to-core mapping on multi-core architectures affected by stochastic variations, our MATC router increases the expected saturation bandwidth by 7–25% vs Dimension Order router. With systematic variations, the improvements are 5–50%. These gains compensate for saturation bandwidth degradation due to manufacturing variations and help to reduce design guard-bands. 1. Introduction Advances in semiconductor technology scaling are allowing designers to create larger and more powerful circuits, fueling the growth of multi-core chip architectures. Sadly, these advances in device technologies are more susceptible to sources of parametric variations such as those caused by process characteristics and voltage/temperature differentials. These variations are making it increasingly difﬁcult to design reliable systems, even with the use of complex Design-for-Manufacturing (DFM) layout and circuit mitigation techniques. It is possible for these types of techniques to mask certain levels of variation in the underlying technology in order to create circuits that meet speciﬁcation. Unfortunately they quickly become costly as they 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  Offered B/W Figure 1. The impact of heterogeneous architectures and adaptive routing on network performance. “over-engineer” devices and result in larger, more complex and power hungry circuits. In the domain of multi-core architectures, variation is traditionally addressed by constructing homogeneous systems designed with guard-bands to ensure each core operates within the same speciﬁcation parameters as all other cores. This method essentially limits system performance to that of the slowest core. As transistor device sizes decrease, the Process-Voltage-Temperature (PVT) variations on chip become more pronounced and difﬁcult to mask. An alternative to hiding the variations is to create architectures that accommodate and even expose them. Such systems can improve yield and reduce the system area and power consumption as each core operates at its own power efﬁcient performance point. In this type of architecture, the parametric variations present on chip are essentially translated into differences in processor core performances. We investigate the use of system-level adaptive techniques within these types of heterogeneous multi-core architectures. This work focuses on routing algorithms for the Network-on-Chip (NoC) and the impact of parametric device variations on communication performance. NoC runtime adaptive routing can provide signiﬁcant beneﬁts in the face of parametric variations as illustrated in Figure 1. Homogeneous NoC implies that all cores and routers operate at the slowest rate (i.e. performance or operating frequency),   the chip-wide worst case regime. In a homogeneous NoC, adaptive routing can compensate for inherent communication load imbalance in an application but not more. The heterogeneous multi-core NoC architecture potentially offers a performance boost as it allows each core/router tile to operate at its best rate rather than the worst. This assumes a globally asynchronous communication fabric for which NoC is a natural ﬁt. Adaptive routing can improve the network performance further (increase the saturation bandwidth) by intelligently balancing the communication trafﬁc across heterogeneous NoC resources (routers and channels). We evaluated these ideas on a heterogeneous NoC mesh with communication traces from real multi-tasking High Performance Computing applications, allowing us to study diverse spatial communication patterns. With respect to network routing, the heterogeneous architecture abstracts the underlying semiconductor parametric variations as trafﬁc congestion. This allows an adaptive router to simultaneously compensate for communication load imbalance induced by both the PVT variations and the application. The presented routing algorithm shows a 5–50% improvement in network saturation bandwidth over a simple Dimension Order router for stochastic and systematic on-chip performance proﬁles. This paper is organized as follows. Section 2 discusses the sources and types of PVT variations, and Section 3 motivates heterogeneous multi-core architecture that exposes variations as performance. Sections 4 and 6 describe our adaptive routing algorithm, the simulation framework and the evaluation benchmarks. Sections 5 and 6 discuss the way PVT variations affect network performance and the simulation results. Sections 7 and 8 highlight the related works and our conclusions. 2. PVT Variations and Over-engineering The impact of semiconductor device parameter variations on system performance is a reﬂection of the physical world, an environment where no two components can be manufactured with exactly the same precision at the molecular level. Semiconductor manufacturing processes have always encountered problems with variations, but with smaller geometries they became increasingly difﬁcult and costly to manage. In addition to process variations, onchip voltage and temperature gradients also induce performance variations [9]. These Process, Voltage, Temperature (PVT) variations determine transistor, interconnect and system performance. They can be addressed cost-efﬁciently with a combination of layout, circuit and system-level techniques depending on the nature of the variation: • Systematic Includes variations that can be traced to predictable sources. If characterized, their effects can be compensated, e.g. sub-wavelength proximity corrected optical lithography or lens aberrations. • Random Variations that are inherently unpredictable, e.g. ion dopant density ﬂuctuations or impurities. • Dynamic Variations that stem from circuit operation, e.g. temperature/voltage hotspots and ﬂuctuations. Process variations are normally addressed with Design for Manufacturing (DFM) layout rules that take a safe approach, growing design margins as transistors and interconnect shrink. DFM results in larger device sizes, spacings, metal ﬁlls, and other layout modiﬁcations that the original circuit designer may not be aware of. Circuit techniques, e.g. static CMOS for noise immunity or Adaptive Body Biasing, are also used to compensate for parametric variations. These techniques improve the parametric yield, but their individual contributions are hard to quantify as their impacts overlap. There is a disconnect between the problem and the solutions. The variations arise at the molecular level, but DFM and circuit techniques attempt to minimize them with large margins at layout and circuit levels. It is similar to using a sledge hammer instead of a scalpel, but a molecular scalpel does not exist. While parametric yield can be improved with these techniques, they are accompanied by a reduction in system performance. The system must run at the worst-case operating point that accommodates all outliers, resulting in wasted power, area and performance. Due to trade secrets, few published efforts have quantiﬁed the losses from this DFM “over-engineering,” although anecdotal evidence suggests that they are are signiﬁcant. In [7], the authors demonstrated a 12% area reduction in 90nm and 65nm standard cell designs when the performance guard-bands are reduced by 40%. If we had dynamic techniques that could embrace the heterogeneity and manage system components with wider performance variations, we could further reduce the area beyond these 12% with equivalent or better die yield. 3. Heterogeneous Multi-core Architectures need Run-time Resource Management Multi-core architectures naturally lend themselves to heterogeneity as the chip is already partitioned into independently operating regions. Many researchers have proposed and hailed multi-core architectures with Voltage Frequency Islands (VFIs) for their power efﬁciency and fault tolerance [2, 11, 13]. We believe that multi-core VFIs are also the key to on-die PVT variation tolerance. An independent clock and voltage domain for each core/router tile allows each tile to operate at its own performance point dictated by the local PVT variations. Tile performance thus abstracts complex semiconductor device parametric variations with simple computation and communication throughput metrics. This enables automatic and introspective run-time adaptation and compensation without detailed low-level circuit characterization. Run-time resource management is required to maximize resource utilization and deliver average, rather than the worst-case system performance. Dynamic task mapping and routing must match the application resource requirements with heterogeneous cores and routers. Without runtime management, intelligent task mapping and routing, a heterogeneous system may under-perform its homogeneous counterpart if critical path tasks or communication ﬂows are mapped onto slower on-chip resources. As the number of cores on chip increases, the run-time system must embrace the heterogeneity as it will be exceedingly difﬁcult and costly to build systems with uniform, homogeneous performance across the whole die. 4. Adaptive Routing Algorithms To scale with device generations and changing PVT variations, routing algorithms should not rely on detailed onchip performance characterization. Instead, they should operate introspectively and adapt to the number of possibly unknown and hard to quantify performance variables in the system. If an architecture exposes the variations as performance, an adaptive router can compensate for the variations using nothing more than inter-router ﬂow control. Static and dynamic variations are exposed in the same manner as application communication load imbalance — as trafﬁc congestion. A congestion aware router can simultaneously compensate for multiple sources of imbalance. We considered several minimal adaptive dead-lock free routing algorithms on a 2D mesh including variants of Minimal Adaptive with two virtual channels and Minimal West First algorithms with a single channel. For the Minimal Adaptive algorithm, we investigated two output channel selection heuristics. The ﬁrst one, “Total Congestion,” is a simple method that uses only adjacent router buffer occupancy as the congestion metric. The second heuristic estimates the time to drain a local packet backlog into an adjacent router and selects the output channel that offers the best throughput based on past performance. While they vary in implementation complexity, neither heuristic consistently delivered superior performance. Instead, the routing algorithm with the most adaptive choices consistently performed the best. For example, our Minimal West First (MWF) algorithm typically showed 10% lower saturation bandwidth than Minimal Adaptive due to its limited adaptivity for the west-bound trafﬁc. Due to space constraints we omit the details on these routing algorithms and heuristics, and focus our discussion on the best overall performing algorithm: Minimal Adaptive Total Congestion (MATC). The router operates on the following data types in a 2D mesh with arity k : VC0 VC1 VC1 VC0 (a) Dimension Order (b) Minimal Adaptive Figure 2. Allowed routing turns to avoid cyclical resource dependency. • Address a uniquely identiﬁable mesh position, a tuple A = (a0 , a1 ), where a0 , a1 ∈ [0, k). • Channel represents a router input or output as a tuple (m, d, v), where m ∈ [0, 1] is the dimension and d ∈ {+, −} is the direction, v ∈ I+ is the virtual channel. Channel may equal local to refer to the connection to/from the adjacent core. The MATC router performs two steps: (1) compute a set of valid output channels, and (2) select a single output channel from the set. The algorithm is livelock free because it produces output channels that enable a packet to move toward its destination. The algorithm is also deadlock free. Figure 2(b) shows two prohibited turns that eliminate cyclical resource dependency in a 2D mesh. In the minimal routing algorithm, all trafﬁc travels strictly in the direction from its source to destination, and never takes a mis-routing, nonminimal hop. Eastbound trafﬁc will switch the virtual channel when turning. Thus, there cannot be a cyclical resource dependency between eastbound and westbound trafﬁc in the same virtual channel. Given a function OU T V C (I nC h : Channel; dest dim: [0, 1] ) that assigns output virtual channel as in Figure 2(b), the routing algorithm is: 1: MATC ROUTE (I nC h : Channel; M yAddr , DestAddr : Address) : Channel 2: P ⇐ ∅ 3: # iterate through all dimensions to ﬁnd all differences 4: for all i ∈ 0..1 do 5: ∆ ⇐ DestAddr[i] − M yAddr [i] 6: 7: 8: 9: if ∆ > 0 then P ⇐ P ∪ {(i, +, OU T V C (I nC h, i))} else if ∆ < 0 then P ⇐ P ∪ {(i, −, OU T V C (I nC h, i))} 10: end if 11: end for 12: return (P = ∅) ? local : SELEC T T C (P ) SELEC T T C selects the minimally congested output channel, measured as the total ﬂit buffer occupancy of an adjacent router. The routers can exchange these metrics with dedicated connections or by overloading the ﬂow control credits. [6] suggests that either implementation should result in less than 7% area overhead. 5. Variations and Routing Compared to deterministic routing, an increase in network saturation bandwidth from an adaptive router is often accompanied by degraded average packet latency when the network utilization is low. In a lightly utilized network, the congestion metrics are noisy and do not accurately reﬂect communication load imbalance. An adaptive router may make bad local routing decisions that adversely affect packet latency. [6, 12] proposed hybrid approaches to deliver the low packet latency of deterministic routing and high saturation bandwidth of adaptive algorithms. However, as we focus only on compensation of PVT variations, the unloaded latency is not our metric of interest. In contrast, the network saturation bandwidth is our key metric that deﬁnes the performance region where an application operates productively. Exceeding the network saturation bandwidth is akin to page thrashing in virtual memory systems, i.e. the application cannot realize its parallel performance potential. The improvement in the saturation bandwidth from adaptive routing can compensate for PVT induced performance variations. Although the variations create a very complex on-die performance proﬁle, for NoC they can be modeled as router throughput and latency. These metrics represent the outside black-box view of the router, which is consistent with our intro-spective approach to compensate for variations without detailed performance characterization. 5.1. Terms and Nomenclature • Network saturation injection rate is the saturation bandwidth normalized to the total network capacity. The rate enables us to discuss network performance without referring to a particular implementation and its parameters. • Nominal channel throughput C0 is a static variable that represents the router throughput set by the designer. C0 represents an ideal NoC unimpacted by variations. Our experiments are not tied to a particular implementation, and thus we use C0 = 1 ﬂit per time unit. • Nominal saturation injection rate RR 0 is a static variable that represents the saturation injection rate on the nominal system with routing algorithm R. For example, RM AT C and RDO are saturation rates for MATC and Dimension Order (DO) routing algorithms on the nominal NoC. • Realistic channel throughput C is a variable that represents a distribution of NoC router throughput induced by PVT variation. If the distribution is stochastic, Cσ is a random variable describable with the expected value µ and the standard deviation σ . In our discussion, the expected value is µ = C0 = 1. If the distribution is 0 0 not stochastic, C is a function that maps on-die location (or generally, operating conditions) onto a throughput value. • Realistic saturation injection rate RR σ . This random variable represents a distribution of network saturation injection rates on a NoC with router throughput Cσ and routing algorithm R. For example, RM AT C refers to the distribution of the saturation injection rate on a NoC with router throughput C0.14 — normally distributed as N (µ = 1, σ = 0.14) — and running a MATC routing algorithm. The distribution of saturation injection rate may itself be characterized by its expected value 0.14 ) and its standard deviation σ(RR µ(RR 0.14 ). 0.14 5.2. PVT variations impact PVT variations impact the network saturation injection rate by turning nominal router bandwidth C0 into a random variable C . A routing algorithm may push a packet ﬂow through a set P of valid network paths P between a packet source and destination. The minimal capacity channel along a path determines the path bandwidth. Considering router performance variations and assuming for presentation simplicity that P contains non-intersecting paths, the upper bound of a path set bandwidth w(P ) is: w(P ) = XP ∈P (cid:20)min e∈P (C [e])(cid:21) (1) where P is a path with average length Davg , and e is an edge on that path. Each application has its own Davg , an average number of router hops traveled by a packet. The expression above optimistically assumes that the router will select the path with maximum throughput, which is not necessarily true. A typical path bandwidth is the minimum of a set of Davg random numbers, and it tends to approach the lower bound of the distribution of C as the path length Davg increases. This reduction in an individual path bandwidth leads to degradation in network saturation bandwidth. The advantage of the adaptive over the oblivious router comes from exploiting a greater path diversity between every packet source and destination. The adaptive router: (1) increases total cross-section bandwidth between the source and the destination, and therefore the network saturation bound; (2) aggregates the path bandwidths between a packet source and destination, and this summation averages the net bandwidth closer to the nominal value. Table 1 presents the bounds RDO and RM AT C of the nominal saturation injection rate for Dimension Order (DO) and Minimal Adaptive Total Congestion (MATC) algorithms for our benchmark applications. These bounds were computed by formulating application communication bandwidth requirements as a Multi-Commodity Max Flow problems on a 2D mesh. The data shows two trends: (1) In 0 0 Avg. Comm Size Upper Bounds Name gtc3-64 cactus-64 fvcam 2d lbmhd-64 pmemd-64 gtc2 mdh2d cactus-256 madbench1 slu-256 lbmhd madbench2 paratec-256 Dist Davg 1.09 1.73 2.62 3.37 5.24 1.12 2.02 2.12 3.31 5.72 5.81 5.83 10.6 N 64 64 64 64 64 256 256 256 256 256 256 256 256 RDO 0 0.98 0.91 0.67 0.50 0.41 0.92 0.67 0.71 0.40 0.25 0.29 0.36 0.19 RM AT C 0 Problem and Method 0.98 Vlasov-Poisson Equation via Particle in Cell 0.91 Einstein’s Theory of GR via Finite Differencing 0.74 Atmospheric Circulation via Finite Volume 0.84 Magneto-Hydrodynamics via Lattice Boltzmann 0.46 Molecular Dynamics via Particle Mesh Ewald 0.99 Vlasov-Poisson Equation via Particle in Cell 1.00 Grid 0.84 Einstein’s Theory of GR via Finite Differencing 0.54 Cosmic µWave Background via Newton-Raphson 0.25 Sparse Solve via LU Decomposition 0.48 Magneto-Hydrodynamics via Lattice Boltzmann 0.49 Cosmic µWave Background via Newton-Raphson 0.19 Density Functional Theory via FFT Table 1. Summary of benchmarks and their routing bounds. 0 0 0 hints at − RDO RM AT C 0 < RM AT C general, RDO demonstrates the performance advantage of adaptive routing that utilizes greater network path diversity. The magnitude (cid:12)(cid:12)(cid:12) the ideal adaptive routing performance gain that can compensate for PVT variations. (2) The saturation injection rates are inversely correlated with average communication distance Davg (hops), the measure of communication locality. This is consistent with our intuition that applications with greater average communication distance utilize more network resources, consume more bandwidth and saturate the network earlier. (cid:12)(cid:12)(cid:12) 6. Experimental Results We developed our own discrete event simulator framework to model the effects of different PVT variations on a range of router implementations. Our framework allows each simulated router component (e.g. input buffer, crossbar, multiplexer) to be parametrized independently with its PVT variation proﬁle. The framework performs Monte Carlo simulations to evaluate the performance impact of stochastic on-die variations. We used 100 iterations per experiment, which is a pragmatic compromise between the software NoC simulation time and the quality of results. The simulator supports a variety of network topologies, our results are for 2D mesh due to its popularity. The simulated NoC router contains two virtual channels, 8 ﬂit input buffers per VC, Virtual Output Queues and credit based ﬂow control. Further implementation details are omitted for brevity. 6.1. Benchmarks Simulated network trafﬁc was generated from Message Passing Interface (MPI) traces of applications shown in Table 1. The traces do not contain task inter-dependencies that are required to reconstruct the exact timing. However, they capture the spatial communication patterns accurately, which is sufﬁcient to learn a great deal about the application network behavior. Each application trace represents a task communication graph, which requires a task-to-core mapping to orchestrate its particular network communication trafﬁc pattern. Mapping quality has a critical impact on network performance. The mappings were computed with a simulated annealing algorithm with the bounding box cost function that minimizes average communication distance between the tasks, i.e. preserves communication locality [1]. Table 1 contains the benchmarks sorted by size N and average communication distance on a 2D mesh (number of hops, Davg ). 6.2. Stochastic Variations Here, assume the router throughput is normally distributed, Cσ = N (µ = 1, σ) as visualized by the density plot in Figure 3(a). Figure 4(a) illustrates the impact of stochastic variations on the PMEMD-64 application. The graph has two curves for the nominal system with no performance variations, and two “clouds” that represent the results of Monte Carlo experiments over a space of router performance C0.07 = N (1, 0.07): MATC N(1, 0.07) and DO N(1, 0.07). MATC has an approximately 15% “intrinsic” saturation bandwidth advantage vs DO on the nominal NoC 0 = 1.15). Each “cloud” illustrates the distribution of the “latency vs bandwidth” curves on a system with variations. A cloud contains two crossed lines. The vertical line marks the location of the statistical expected value for the saturation injection rate µ(RR 0.07 ). The horizontal line spans the 6σ(RR 0.07 ) range of the saturation injection rate. Notice that the 6σ (six standard deviations) span reaches (RM AT C /RDO 0  8  7  6  5  4  3  2  1  0  8  7  6  5  4  3  2  1  0  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8 (a) Stochastic N (µ, σ) (b) Systematic SP Figure 3. Stochastic vs systematic variations. 0.07 ) < RR outside of the curve cloud. The reason is that the distribution of saturation injection rate is not Normal or symmetric. The distribution involves (1) the selection of the lowest bandwidth channel from a routing path (Equation 1 is skewed toward the minimum of Cσ ), (2) the summation of different path capacities, and (3) averaging across all communication ﬂows in the application. We use the standard deviation only as a measure of the distribution “width” for comparison between algorithms. With stochastic variations, the network saturation bandwidth delivered by both algorithms degrades relative to the nominal, µ(RR 0 . For PMEMD-64 and C0.07 router performance variations, the expected network saturation bandwidth degrades by 1%. The degradation increases sharply as the standard deviation of router throughput increases to C0.21 in Figure 4(b). Figure 5 illustrates the impact of performance variations on the expected value µ(RR σ ) and range of the saturation bandwidth distribution. Two trends are clear: (1) As router throughput variations Cσ widen, the network saturation injection rate RR σ decreases. As expected, the saturation bandwidth degrades faster for the Dimension Order router because it has a single ﬁxed path between each source and destination node and does not take advantage of network path diversity. The path throughput is only as large as the slowest router in the path, and DO is likely to encounter an outlier — an unduly slow router — that it cannot bypass. MATC has an advantage in that it selects a path from a larger set of legal, minimal paths. (2) MATC has smaller standard deviation of the saturation bandwidth distribution than DO. The relationship between the magnitude of on-chip performance variations Cσ , and the expected value µ(RR σ ) and the standard deviation σ(RR σ ) of the saturation bandwidth distribution, can be described with a simple linear model with parameters RR 0 , AR and BR : µ(RR σ ) = RR 0 − AR × σ σ(RR σ ) = BR × σ (2) (3) Although the actual relationship between these quantities is complex, in fact there is no closed form, the equations above highlight the key trend. These parameters, which can be extracted with a curve ﬁt as shown on Figure 6, describe µ(RDO 0.07 ) 6 × σ(RDO 0.07 ) RDO 0.07 RM AT C 0.07 RDO 0 RM AT C 0 (a) N (µ = 1, σ = 0.07) RDO 0.21 RM AT C 0.21 RDO 0 RM AT C 0 (b) N (µ = 1, σ = 0.21) Figure 4. MATC improves saturation BW for PMEMD-64 and compensates for variations. • RR a routing algorithm’s sensitivity to PVT variations for a particular communication trafﬁc. 0 is the nominal saturation injection rate. Improvements in this quantity come from the router’s ability to compensate for the application’s intrinsic communication load imbalance. • AR is the rate at which the expected saturation bandwidth degrades relative to the standard deviation of the router performance, σ . This key metric characterizes the router’s ability to minimize the impact of variations. The smaller the AR value, the more capable the algorithm. The ideal AR = 0 implies that the algorithm completely compensates for variations. A negative value AR < 0 implies that the router takes advantage of the performance heterogeneity and achieves a higher than nominal network saturation injection rate. • BR is the growth rate of the standard deviation of the saturation bandwidth σ(RR σ ) relative to the standard deviation of the router performance distribution, σ . A smaller BR value is better, and it describes the router’s ability to minimize the impact of variations. RM AT C 0 µ ( R σ R ) Cσ RDO 0 Figure 5. PMEMD-64 Sat. BW degradation.  0.28  0.29  0.3  0.31  0.32  0.33  0.34  0.35  0.36  0 0.05 0.1 0.15 0.2 0.25 E c e p x t S d e a t I n . j R a t e MATC DO  0  0.002  0.004  0.006  0.008  0.01  0.012  0.014  0.016  0 0.05 0.1 0.15 0.2 0.25 d S t D o v e f S a t I n . j R a t e MATC DO Std Dev of Router Performance Distribution (Cσ ) BM AT C BDO −AM AT C −ADO RDO 0 µ ( Rσ R ) σ ( Rσ R ) RM AT C 0 Figure 6. PMEMD-64 Extract params A, B and RR 0 from curves µ(RR σ ) vs σ , and σ(RR σ ) vs σ . Figure 7 summarizes µ(RM AT C ) and σ(RM AT C ) for the MATC routing algorithm for our benchmarks. Results for the DO algorithm are qualitatively similar. They are omitted for lack of space because we will use parameters 0 , AR and BR to compare MATC vs DO. The applications are partitioned into two groups: 64 task gtc3-64 → σ σ RR pmemd-64, and 256 task gtc-2 → paratec-256, and each group is sorted by application’s average communication distance (Table 1). Consider the expected network saturation rate graph in Figure 7(a): (1) µ(RM AT C ) decreases as application communication distance increases. (2) For each application, as the router performance variations Cσ grow, the network saturation rate degrades. Figure 7(b) presents a more complex picture. (1) There is no consistent trend between the application’s communication locality and the standard deviation of the saturation injection rate distribution, σ(RM AT C ). The standard deviation depends on local routing decisions and the degree to which communication load is balanced. More experiσ σ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 g t c 3 4 6 c a c t u s 4 6 f v a c m d 2 l b m d h 4 6 p m e m d 4 6 g t c 2 m d 2 h d c a c t u s 6 5 2 m a b d e n c 1 h s l u 6 5 2 l b m d h m a b d e n c 2 h p a r a t c e 6 5 2 S a t u r a i t n o I n j c e i t n o R a t e MATC vs DO on the Nominal System DO MATC Figure 8. Sat. BW on nominal NoC. mentation is required to properly understand its relationship with the communication locality. (2) In general, as expected ) increases with the router performance variations Cσ because the network saturation bandwidth is the sum of the stochastic channel capacities across the topology bisection. σ(RM AT C σ Figures 8 and 9 compare the ability of DO and MATC routers to minimize the impact of PVT variations. Figure 8 matches up the network saturation injection rate for both algorithms on the nominal system, which highlights the “intrinsic” adaptive algorithm advantage. With the exception of gtc3-64, gtc2, and cactus-64 with local, essentially nearest-neighbor communication patterns, MATC has a higher saturation bandwidth than DO. The primary reason for the improvement is that the applications have a communication load imbalance speciﬁc to their topology and task-to-core mapping, and the adaptive algorithm routes the trafﬁc better than DO and improves channel utilization. Local communication patterns, e.g. gtc2, do not beneﬁt from adaptive routing because of low or no network path diversity. Figure 9 presents parameters AR and BR to compare the ability of each router to compensate for variations. With a few exceptions (cactus-64/256 and gtc3-64), MATC minimizes the network saturation bandwidth degradation better than DO and thus has a lower value of AR . The same trend applies to BR , with a few exceptions (madbench2 and gtc2). For some applications, such as slu-256, the adaptive router reduces BR to 0, which means that the standard deviation of saturation injection rate does not grow at all as performance variations widen.                                         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 g t c 3 4 6 c a c t u s 4 6 f v a c m d 2 l b m d h 4 6 p m e m d 4 6 g t c 2 m d 2 h d c a c t u s 6 5 2 m a b d e n c 1 h s l u 6 5 2 l b m d h m a b d e n c 2 h p a r a t c e 6 5 2 No Variations σ = 0.07 σ = 0.14 σ = 0.21 (a) Expected saturation injection rate µ(RM AT C σ ) 0.000 0.004 0.008 0.012 0.016 0.020 g t c 3 4 6 c a c t u s 4 6 f v a c m d 2 l b m d h 4 6 p m e m d 4 6 g t c 2 m d 2 h d c a c t u s 6 5 2 m a b d e n c 1 h s l u 6 5 2 l b m d h m a b d e n c 2 h p a r a t c e 6 5 2 σ = 0.07 σ = 0.14 σ = 0.21 (b) Std Dev of saturation injection rate σ(RM AT C σ ) Figure 7. Results summary for Minimal Adaptive Total Congestion (MATC) router. 0.00 0.10 0.20 0.30 0.40 0.00 0.50 g t c 3 4 6 c a c t u s 4 6 f v a c m d 2 l b m d h 4 6 p m e m d 4 6 g t c 2 m d 2 h d c a c t u s 6 5 2 m a b d e n c 1 h s l u 6 5 2 l b m d h m a b d e n c 2 h p a r a t c e 6 5 2 A R 0.02 0.04 0.06 0.08 0.10 B R DO MATC Figure 9. Variations sensitivity parameters AR and BR . 6.3 Results Summary MATC vs DO advantage stems from two sources: 1. MATC reduces application speciﬁc load imbalance on the nominal system (Figure 8). RM AT C . 0 > RDO 0 2. MATC minimizes the impact of variations further by slowing the PVT-induced saturation bandwidth degradation (Figure 9). AM AT C < ADO and BM AT C < BDO . Figure 10 summarizes the improvements in saturation injection rates from MATC and DO routers as the ratio σ ). A higher ratio represents a greater saturation bandwidth and a wider efﬁcient NoC operating region. Two trends are clear. First, as the average communication distance increases, the applications show greater saturation bandwidth improvement. Applications gtc3-64, cactus-64 and gtc2, whose average communication distance does not exceed 2 router hops, simply do not offer enough path diversity to MATC, and yield no appreciable gains. In contrast, applications with more global communication patterns show 7–25% improvement in network saturation bandwidth with MATC. Only madbench2 and paratec-256 deviate from this trend, although their spatial communication pattern is global. Our careful analysis revealed that these applications have a communication pattern that closely resembles the uniform random, i.e. their communication load is essentially balanced across the network. For these applications, MATC and DO deliver approximately equivalent saturation bandwidth on the nominal NoC because MATC cannot improve on an already balanced communication load. µ(RM AT C σ )/µ(RDO The second trend can be identiﬁed by comparing the MATC/DO ratios for each application. In general, as router throughput variations grow from σ = 0.07 to 0.21, so does the ratio µ(RM AT C σ ). Applications fvcam 2d, σ )/µ(RDO lbmhd-64, pmemd-64, madbench1 and lbmhd show 0.9 1.0 1.1 1.2 1.3 1.4 g t c 3 4 6 c a c t u s 4 6 f v a c m d 2 l b m d h 4 6 p m e m d 4 6 g t c 2 m d 2 h d c a c t u s 6 5 2 m a b d e n c 1 h s l u 6 5 2 l b m d h m a b d e n c 2 h p a r a t c e 6 5 2 Stochastic Variations: ratio µ(RM AT C σ )/µ(RDO σ ) No Variations σ = 0.07 σ = 0.14 σ = 0.21 Figure 10. MATC vs DO saturation rate improvement with stochastic variations. clear improvements that indicate that the saturation bandwidth degrades slower with MATC vs DO router. Application paratec-256 highlights MATC’s ability to tolerate variations. paratec-256 is inherently load balanced, and the performance variations create an imbalance in the NoC resource utilization. MATC rebalances the load, which is an improvement over the oblivious DO router. To evaluate the impact of systematic variations, we have experimented with a simple linear on-chip performance gradient (Figure 3(b)). The simulation results show similar trends to those with a stochastic performance proﬁle. Applications with non-local communication patterns show the most improvement from adaptive routing. Due to its symmetry, this particular systematic proﬁle is relatively simple for an adaptive algorithm to manage, which resulted in net saturation bandwidth gains of 5–50% over DO. The results highlight the key feature of our approach. Adaptive NoC routing compensates for a range of PVT variations independent of their nature and sources, as long as the NoC router architecture exposes the variations as network congestion. The approach ﬂexibly accommodates a range of static variation effects, such as Process variations and faults; dynamic effects, induced by on-die temperature and voltage ﬂuctuations, in addition to application speciﬁc load imbalance and device aging. 6.4. Reducing Design Guard-bands Figure 11 illustrates the trade off between the gains from adaptive routing and smaller design guard-bands. Without adaptive routing, a NoC would be designed with router µ(RDO σ [Cσ ]) MATC MATC DO µ(RM AT C σ [Cσ ]) Network Saturation Injection Rate Cσ Cσ Reduce Guard-bands spec µ(RDO σ [Cσ ]) Figure 11. From improvements in saturation bandwidth to reduced design margins. performance Cσ , which would correspond to network saturation bandwidth target RDO [Cσ ]. The increased network saturation bandwidth from MATC allows a designer to choose a lower performance target Cσ < Cσ for router/channel throughput. The reduction in guard-bands or performance target can be proportional to the gains from adaptive over oblivious routing, 7–25% as shown on Figure 10. The actual savings depend on the implementation and underlying manufacturing technology. With MATC, implementation Cσ corresponds to lower area and power to deliver the same network performance. Each application graph depending on its trafﬁc pattern has its own MATC vs DO saturation bandwidth advantage. The application domain determines the best way to realize the gains from adaptive routing. If the domain can be characterized, and it comprises applications with similar communication properties in terms of locality and load balance, a designer can select the target router performance Cσ to reduce the guard-bands and still meet the system performance speciﬁcation. If the application domain is not sufﬁciently narrow and encompasses members with divergent communication properties, a combination of design and run time techniques is required. A designer can reduce the guardbands enough to accommodate the majority of the domain. At run-time, voltage and frequency scaling can extend the router operating range to also accommodate the outliers. σ 7. Related Works In certain domains, circuit techniques can help to tolerate variations and reduce power consumption, e.g. by trading off power for error rate in NoC transmission [15]. In general, Adaptive Body Biasing [14], Voltage and Frequency scaling [4] help to improve die yield. These techniques are orthogonal to the circuit functionality. They realign circuit performance to reduce the impact of variations. Adaptive routing is not a new concept, but we apply it to compensate for PVT variations to deliver average rather than the worst case network performance. DyAD [6] and CQR [12] routing algorithms use buffer utilization as the congestion metric and inﬂuenced our work. Nostrum NoC combines deﬂection routing with Proximity Congestion Awareness: a switch indicates its current load by exchanging stress values with its neighbors [8, 10]. [5] discusses a power management DVS/DFS scheme that strives to operate the network just below its bandwidth saturation point. In contrast our work, all network resources operate in the same voltage and frequency. [11] presents a partitioning scheme for a heterogeneous VFI architecture to optimize energy consumption. The authors report 35% energy reduction on a 10 task application. The improvements will be more pronounced in larger applications. 8. Conclusion and Future Directions Multi-core NoC architectures with Voltage-Frequency Islands can abstract complex semiconductor device parametric variations as cores and routers with varying performance. To a router, the variations are exposed as network congestion. Process-Voltage-Temperature (PVT) variations create communication load imbalance that degrades the network saturation bandwidth. Our Minimal Adaptive Total Congestion (MATC) router increases the saturation bandwidth on the nominal, no variations NoC by “ﬁxing” the load imbalance in an application and its task-to-core mapping. MATC further reduces saturation bandwidth degradation caused by PVT variations. With static, locality preserving task-to-core mapping, our High Performance Computing benchmarks show 7–25% saturation bandwidth improvement with MATC over Dimension Order router with stochastic on-chip variations. The improvements are 5 – 50% with systematic variations. These gains can be turned into smaller design guard-bands to reduce system implementation area and power. In the future, we will combine dynamic routing with mapping and core-level DVS/DFS to further improve communication and computation performance of heterogeneous systems. 9. Acknowledgments The authors acknowledge the support of the Gigascale Systems Research Center, one of ﬁve research centers funded under the Focus Center Research Program, a Semiconductor Research Corporation program, and BWRC. "
Performance and power efficient on-chip communication using adaptive virtual point-to-point connections.,"In this paper, we propose a packet-switched network-on-chip (NoC) architecture which can provide a number of low-power, low-latency virtual point-to-point connections for communication flows. The work aims to improve the power and performance metrics of packet-switched NoC architectures and benefits from the power and resource utilization advantages of NoCs and superior communication performance of point-to-point dedicated links. The virtual point-to-point connections are set up by bypassing the entire router pipeline stages of the intermediate nodes. This work addresses constructing the virtual point-to-point connections at run-time using a light-weight setup network. It involves monitoring the NoC traffic in order to detect heavy communication flows and setting up a virtual point-to-point connection for them using a run-time circuit construction mechanism. The evaluation results show a significant reduction in power and latency over a traditional packet-switched NoC.","Performance and Power Efficient On-Chip Communication Using Adaptive  Virtual Point-to-Point Connections  Mehdi Modarressi, Hamid Sarbazi-Azad, Arash Tavakkol  Sharif University of Technology, Tehran, Iran   Institute for Research in Fundamental Sciences (IPM), Tehran, Iran  modarressi@ce.sharif.edu, azad@sharif.edu, arasht@ipm.ir  Abstract  In  this paper, we propose a packet-switched  network-on-chip (NoC) architecture which can provide  a number of low-power, low-latency virtual point-topoint connections for communication flows. The work  aims to improve the power and performance metrics of  packet-switched NoC architectures and benefits from  the power and resource utilization advantages of NoCs  and superior communication performance of point-topoint dedicated  links. The virtual point-to-point  connections are set up by bypassing the entire router  pipeline stages of the intermediate nodes. This work  addresses constructing  the virtual point-to-point  connections at run-time using a light-weight setup  network. It involves monitoring the NoC traffic in  order to detect heavy communication flows and setting  up a virtual point-to-point connection for them using a  run-time  circuit  construction mechanism. The  evaluation results show a significant reduction in  power and latency over a traditional packet-switched  NoC.  1. Introduction  Current multi-core system-on-chips (SoC) have grown  in size and complexity in recent years and future SoCs  will consist of complex  integrated components  communicating with each other at very high-speed  rates. Moreover, the microprocessor industry is moving  from single-core to multi-core and eventually to manycore architectures, containing tens to hundreds of  identical cores arranged as chip multiprocessors (CMP)  [1]. The lack of scalability in bus-based systems and  large area overhead of point-to-point dedicated links  have motivated the researchers to propose packetswitched Network-on-Chip (NoC) architectures to  overcome complex on-chip communication problems  [2]. However, although NoC solves some problems,  e.g. scalability, the need for complex and multistage  pipelined routers results in a high router-to-link  energy/delay ratio and increases the delay and energy  of communication.  In the systems with point-to-point connections,  packets travel on dedicated pipelined wires which  directly connect their source and destination nodes.  Thus, they can yield the ideal performance and power  results [3]. A network-on-chip increases these ideal  values by the delay and power related to the router  pipeline stages (buffer read/write, route selection,  arbitration and crossbar traversal). An analytical and  experimental  comparison  of  the  power  and  performance metrics of dedicated point-to-point links  and NoCs can be found in [4] and [3].   Poor scalability and considerable area overhead are  the  important drawbacks of dedicated  links. In  addition, predicting the delays of these links before the  late stage in the design cycle (when the actual layout  and routing are performed) is difficult.   In this work, we develop a packet-switched NoC  architecture which can provide low-power and lowlatency dedicated VIrtual Point-to-point (or VIP, for  short) paths between any two nodes by bypassing the  pipeline of the intermediate routers. In this design, one  virtual channel (VC) of each physical channel is  designated to bypass the router pipeline. In this virtual  channel, the buffer is replaced by a latch (1-flit buffer)  which holds the flits arriving on the VC. Moreover, the  arbiter is modified in order to store the selected output  port for each incoming VIP connection and prioritize it  over packet-switched flits on that output port.   The data traveling on VIP connections do not find  any channel busy along their path by prioritizing them  over the packet-switched data and also not allowing the  VIP connections to share the same links (i.e., each  router port can be used by at most one VIP  connection). Consequently, being constructed by  chaining  the VIP  latches along  the path, VIP  connections can act as a dedicated pipelined-link. In  other words, the flits only travel over the crossbars and  links which cover the actual physical distance between  their source and destination nodes and skip through  buffer read, buffer write and allocation operations.   978-1-4244-4143-3/09/$25.00 ©2009 IEEE                  However, unlike dedicated point-to-point links,  which are physically established between  the  communicating nodes of a multi-core chip ( based on  the communication pattern of a single application) and  are fixed during  the system  life-time,  the VIP  connections are dynamically reconfigurable and can be  established based on the traffic pattern exhibited by the  currently running application. It is an important feature  as several different applications are integrated onto  today’s multi-core  SoCs  (and CMPs)  and  communication characteristics can be very different  across the applications. Besides, being established over  traditional network-on-chips, VIPs benefit from the  predictability and  reusability of network-on-chip  architectures.  The cores connected by a VIP are also able to  bypass the network adaptor (NA) and communicate  through  the protocol  implemented  in  their core  interface. This leads to further power and performance  improvements by removing the power and delay  related to packetization and depacketization of data.  This paper exploits the VIPs to customize the  network for a target application. More precisely, we try  to minimize the average message latency and power  consumption of on-chip networks by constructing as  many VIPs as possible for communication flows of the  application. Nonetheless, the VIPs can be used for  some other purposes, for example guaranteeing a given  quality of service required by an application.  The fact that VIPs are not allowed to share the same  link  restricts  the number of VIPs  that can  simultaneously exist in the network. However, in most  of the applications implemented as a multi-core SoC  [5][6][7] each core communicates with a few (less than  3, on average) other cores.  Similarly, CMPs, as a  replacement  for  the  traditional multi-chip  multiprocessors, can benefit from both temporal and  spatial communication  locality. Temporal  locality  represents the effect of temporal aggregation of the  inter-core communications [8]. High temporal locality  suggests that during any given time period, inter-core  communication occurs across a certain number of  connections, which can be called a communication  working set. Spatial locality is determined by the  distribution of the connections in the application and  determines the size of the working set. It has been  shown that each node tends to have a small number of  favored destinations for the messages it sends. For  example, the NAS parallel benchmark suite exhibits  very high spatial locality and therefore contains small  working sets [9]. Similar behavior has been reported  for SPLASH-2 benchmark suit [9] and SPECweb99  and SPECjbb2000 commercial workloads [10].  As a result, we expect the VIP connections to be  able to handle most of the on-chip traffic and deliver  them to the destination node with near-ideal power  consumption and delay. Nonetheless, if the VIP  connections cannot cover the entire inter-core traffic,  the VIPs are set up for higher volume communication  flows to obtain more power and performance gains and  the other flows will use the packet-switched network.  These packets should be directed through the links not  used by VIPs or used by VIPs with lighter traffic.   In our previous work [11], we have proposed a  static mechanism for VIP connection construction,  where the VIPs are constructed for an application  based on its task-graph at mapping and route selection  phases of the NoC design process. In multi-core SoCs,  mapping involves physically placing the cores in the  network, while  in CMPs with several  identical  processing nodes, mapping  is accomplished by  assigning each task to a network node. Upon starting  execution of each application, the corresponding VIP  connections are set up in the network.   For example, Figure 1 displays the task-graph and  VIP connections constructed in a 4(cid:175)4 mesh for VOPD  (Video Object Plane Decoder) [6] application using the  static VIP construction approach. As the figure shows,  the entire on-chip traffic generated by this application  is directed through VIPs.   V1 V7 V2 V6 V13 V12 V3 V5 V9 V4 V16 V8 Figure 1. The VOPD task-graph and its mapping on  a 4×4 mesh  V14 V15 V11 V10  In this paper, we propose a dynamic circuit  construction mechanism which uses a light-weight  setup network in order to monitor the network traffic  pattern and dynamically set up a circuit for the traffic  flows with a communication volume higher than a  predefined threshold (half of the maximum weight, in  this work).    The  setup network  is designed based on  reconfigurable mesh structure [12]. This network is  composed of simple switches that can only make one  or more connections between their ports. In this work,  a shortest path algorithm  is developed on  the  reconfigurable mesh in order to find a VIP of minimum  conflicts with already established VIP connections for  a high volume communication flow.  The rest of the paper is organized as follows.  Section 2 surveys some related work. Section 3  introduces  the proposed router architecture. The  dynamic scheme for constructing VIP connections is          presented in Section 4.  Section 5 presents the  evaluation results. Finally, Section 6 concludes the  paper.  2. Related Work  Improving the power and performance metrics of  packet-switched NoCs by  integrating a second  switching mechanism has been addressed in several  previous work, such as hybrid circuit-packet switched  networks [13], express virtual channels [3], and long  range links [14].  Circuit-switching  is a well-known  switching  mechanism proposed for traditional interconnection  networks [15]. This switching scheme has been used in  a number of NoC architectures to reduce on-chip  communication latency, when compared to packetswitched networks, since packets need not go through  routing and arbitration once circuits are set up.  However, this switching method often suffers from  performance degradation due to circuit setup delay and  poor resource utilization. Some methods try to benefit  from the interesting properties of both switching  methods by integrating them into a single NoC fabric    [13][10]. The main task of the packet-switched part of  these architectures, however, is to carry out the circuit  setup process rather than transferring the application  data.  The structure of VIP connections in this work  differs from the circuit structure in methods supporting  both traditional circuit-switching and packet-switching  [13], in terms of flow control mechanism and resolving  conflict among circuits. In addition, the long setup time  and complex slot allocation process of the circuitswitching is removed.   Express virtual channels [3] allow packets to  virtually bypass intermediate routers along their path.  The bypass paths are made by a set of network virtual  channels (VC). Based on the design, a packet starts  traveling on an express VC either on some specific  nodes or on every node and bypasses some router  pipeline stages as long as it is traveling on express  VCs. Express VCs are restricted to connect nodes only  along a single dimension and cannot turn from one  dimension to another. VIPs are different from express  virtual channels since the VIP connections virtually  provide the communicating nodes with dedicated  point-to-point links and completely connect the source  and destination of a communication flow, rather than  improving  the performance of a packet-switched  network.  Setup networks were used in some research before,  but our proposed setup network differs from them. A  setup network has been proposed in [10] which handles  the construction and teardown of circuits and stores the  switch configuration for active circuits. The main data  network supports both circuit-switched and packetswitched traffic. The setup network supports packetswitched traffic and has three-stage pipeline routers.   Constructing a new circuit may destroy some old  circuits. The flits of destroyed circuits are tagged as  packet-switched flits and will route through packetswitched network. The switches in our setup network,  however, are very simple and run a fast algorithm to  find the best path for a new circuit which minimizes  the number of destroyed old circuits. AdNoC  architecture  [16] presents a NoC adapting  to  communication  requirements  imposed  by  an  application aiming at increasing resource utilization.  For a requesting transaction, this method calculates a  weight for each output port based on the available  bandwidth and the distance between the current and the  destination nodes. It can also distribute virtual channel  buffers on-demand among the routes by assigning  them  to proper output ports. Although AdNoC  effectively increases the resource utilization, it requires  some multipliers, adders, lookup tables, and other  logics for each output port. Our proposed setup  network aims at reducing the average power and  communication  latency and uses much simpler  structure for adapting the NoC to the communication  behavior of the running application.  3. The NoC Architecture  Figure 2 displays the microarchitecture of the proposed  router. The figure shows the implementation details of  one input port and one output port of the router. As  shown in the figure, in one virtual channel (virtual  channel 0) in each physical channel, the buffer is  replaced by a latch (1-flit buffer).  These virtual  channels are devoted for establishing VIP connections  between nodes. This router uses a mux-tree-based  crossbar fabric.  Since a packet coming through an  input port does not loop back, each multiplexer is  connected to three input ports as well as the local PE.   This crossbar fabric is slightly modified in order to  prioritize the VIP data over packet-switched flits. Each  VIP latch has a signal called full which is set to 1 when  the latch has incoming flits to service. In the input  ports, this signal is used to select the proper select  signal of the multiplexer connecting one of the virtual  channels to the crossbar. If the latch is empty (full= 0),  a virtual channel is selected based on the outcome of  the routing function, virtual-channel allocation, and  switch allocation units, just like in traditional packetswitched networks. Otherwise, select signal is set to 0  and the flit in the VIP latch (virtual channel 0) is  directed to the crossbar.          and write, route calculation, and arbitration are  removed.   As mentioned earlier, the cores are able to bypass  the network  interface and directly communicate  through their interface protocol (for example OCP). In  this case, all required services (such as end-to-end error  control) are provided by the core protocol, just like in a  dedicated physical link.   In the current work, we have designed a simple endto-end flow control scheme between the cores based on  the traditional on/off flow control mechanism [15].  An  off signal is sent by the downstream node of a VIP  when the free buffers at its interface falls below the  threshold Toff. If the number of free buffer rises above a  threshold Ton, an on signal is sent. When the off signal  is received at the VIP upstream node, it stops  transmission of data over the VIP until an on signal is  received.    These signals are sent along the path from the  source to destination of a VIP. This path is set up  during the VIP construction process. The signals  propagate to the sender along this path one hop per  cycle.  When an off signal is transmitted to the upstream  node, there must still be sufficient buffering at the  receiver to store all of the data items in transit, as well  as all of the data items that will be injected during the  time it takes for the signal to propagate back to the  sender. As a result Toff must be at least 2×d, where d is  the VIP length (in terms of hop count).   Prioritizing the data traveling on VIP links over  packet-switched flits may produce starvation at the  intermediate routers along the VIP, if a VIP link  always has incoming flits to forward.  We have modified the proposed flow control  mechanism to avoid starvation at the intermediate  routers. To this end, the on and off signals can also be  asserted by the routers along a VIP. Each intermediate  router counts the number of consecutive cycles on  which the data of a VIP connection are serviced. If this  number of cycles exceeds the threshold Tvip, while there  are packet-switched flits waiting to use the output port,  it asserts the off signal to service some packet-switched  flits, and sends an on signal after Tps cycles to allow the  VIP source to resume sending data. Tvip and Tps are two  design parameters that can be set for each VIP  separately. If there is not a demand for very lowlatency communication on a VIP, the bandwidth can be  fairly shared between the packet-switched flits and VIP  data by selecting the two thresholds close to each  other. In this case, the VIP data still benefit from the  low-power  communication  provided  by VIP  connections. By increasing Tvip over Tps, more portion  of bandwidth is allocated to a VIP.  Figure 2. The proposed router architecture  The full signal is also used to control the crossbar  operation. Each output port contains a VIP allocator  which for each output port belonging to a VIP,  determines (using a 2-bit register) the input port  connected to this output port along that VIP. This logic  takes the full signal of the input ports and the output of  the switch allocator unit as input and allocates the  output port to the selected input port (along the VIP)  when it has an incoming flit in its VIP latch to forward.  Otherwise, the allocation is done by switch-allocator  unit, just like in traditional packet-switched NoCs.  Supporting VIPs in a traditional packet-switched  router involves adding a 1-flit buffer at each input port,  a simple VIP allocator at each output port, and some 1bit wires  to propagate  the  full signals. These  modifications impose a completely negligible area  overhead, as will be showed in Section 5.   A VIP for a communication flow is established by  appropriately setting the VIP allocators in each router  in such a way that the latches in each router are  connected to proper output ports and then to the latches  in the next router along the VIP.  The source node of a communication flow selected  for traveling on a VIP sends corresponding packets  using VC number 0 of the output port through which  the flits should be transferred. Since the VIP is already  set up, the VIP allocator of the designated output port  selects the injection port and connects it to the output.  When the flits reach the next node, they are demultiplexed based on their VC identifier and the VIP  flits (identified by VC identifier 0) are stored in the  VIP latch of the input port. Similarly, this latch and the  other latches in the intermediate routers along the path  are connected to the proper output port and construct a  VIP connection towards the destination.  Since the VIP flits bypass the router pipeline, the  power consumption and delay related to buffer read        To remove redundant off signal transmission, if  multiple nodes are getting starved at the same time, a  node which has already asserted the off signal blocks  the off signals received from downstream nodes.  Moreover, an off signal resets the counter in all  upstream routers along the VIP connection. A similar  policy is applied for on signals.  4. Dynamic VIP Connections   This section proposes an scheme for VIP construction  which dynamically changes the VIP configurations in  response to communication requirements imposed by  an application. This is achieved by monitoring the  traffic generated by each node and using a simple setup  network which selects  the best route for VIP  construction. Constructing a VIP may take a few clock  cycles. During these cycles, the packets of the traffic  flow keep on  traveling on  the packet-switched  network. Therefore, unlike the traditional circuitswitched networks, this setup latency does not degrade  the network performance.  4.1. Setup Network Structure  The setup network is designed based on the  reconfigurable mesh presented in [12], where a large  number of algorithms are implemented using this  interesting structure. Figure 3 shows a reconfigurable  mesh where each node has four ports N, S, E, and W,  through which it connects to nodes (if any) at North,  South, East, and West.   Each node is simply composed of some switches  that can establish internal connections among its ports.  In addition, each node has some simple controller  logic. Figure 4 depicts the internal structure of the local  controller consisting of a controller and a datapath. The  datapath implements the VIP setup algorithm and will  be described later in this section.   The controller can read from and write to the ports  through the E, W, S, and N lines in Figure 4 and  configure the internal switches through the cl1 line.  Two possible node configurations for the internal  switches are displayed in Figure 3 (at the right side).  The connection among the controller and the ports are  not shown in Figure 3 for simplicity. The controller is  also connected to the data network in order to receive  the VIP construction requests and new VIP weights. In  addition, it can configure the VIP allocator units in the  data network in order to set up a VIP.  Figure 3. The internal structure of reconfigurable  mesh nodes and two sample node configurations  Figure 4. The local controller structure and its data  (thick lines) and control (thin line) lines.  Having a small bit-width and a simple internal  structure, the area of the setup network is negligible,  compared to the main data-network. The size of the  setup network is the same as the size of the main data  network and each node  in  the setup network  corresponds to a node in the data network with the  same address. The address of node v is composed of  two parts, namely Xv and Yv, which determine its row  and column number, respectively.   The setup network keeps track of the path and  resources occupied by data network VIPs.   In this architecture, the communication volume  (number of transmitted packets) of each traffic flow is  stored in a table entry at the source node of the flow  indexed by destination address.   If a VIP has been already set up for the flow, the  VIP has a weight based on its traffic flow volume. The  weight is stored in an n-bit register. Although any  weight assignment and updating policy can be applied,  we use the following scheme: at specific intervals, the  n most-significant bits of the register holding the traffic  volume are considered as the current weight of the  VIP. The new weight, then, is computed as the average  of the current and the old weights of the VIP. This  weight is used to decide on establishing and tearing  down VIPs. Using a 4-bit wide setup network (n=4),  for example, we can provide 16 different weights.  4.2. Dynamic VIP Construction Algorithm  The controller logic in the setup network nodes  implements a fast algorithm for constructing and                          tearing down a VIP between two nodes. In this section,  we describe algorithms for constructing a VIP for a  traffic flow, updating the weight of each VIP, and  tearing down old VIPs in order to make room to  construct new ones. More specifically, the problem can  be defined as follows:  1. Periodically update the weight of each traffic  flow at specified times. If a traffic flow has already a  VIP, update its weight in the setup network. If the  weight of a traffic flow in packet-switched network  reaches a threshold, it will request for a VIP.  2. Select the requesting traffic flows in some order,  based on a priority scheme. Construct a VIP for a  selected traffic flow f with source node S and  destination node D along one of the shortest paths  between S and D. If there are not sufficient free  resources (as a VIP connection cannot share a common  port with other VIPs) the new VIP can tear down old  VIPs. The algorithm should minimize the cumulative  weight of the torn down VIPs. A new VIP cannot be  established if the cumulative weight of the torn down  VIPs is greater than the weight of the requesting traffic  flow.  3. Notify the source of a torn down VIP that it has  to send data through packet-switched network. The  traffic flows corresponding to the torn-down VIPs can  request for constructing a new VIP at the next round,  provided that its traffic flow weight is still above the  given threshold.   This new VIP setup and old VIP destroying process  starts at specific times and continues until all requests  are serviced. Then, the setup network will be idle until  the next round.  As the problem definition states, the setup network  implements algorithms for updating VIP weights,  constructing new VIPs, and destroying old VIPs, as  follows. We omit some implementation details for the  sake of brevity.  Updating the VIP weights. Each port of a setup  network keeps the favored output port of an incoming  VIP connection (if any). It also keeps the weight of an  outgoing VIP connection. The weights are updated at  specific times at the VIP source nodes. Upon updating  VIP weights at VIP source nodes, the new weights  should be sent to all setup network nodes along the  VIP. To do this, at each node belonging to a VIP, the  node controller connects the incoming and outgoing  ports of the VIP by turning on the switch between them  via the control line cl1 in Figure 4. Afterwards, the VIP  source nodes propagate the new weights along the VIP.  This step takes 2×n cycles (the maximum length of a  VIP) for an n×n network.  A problem may arise when two VIPs share a  common link in different directions. This is not a  problem in the data network, where the links are bidirectional. However, the setup network links are  unidirectional and, in this case, a conflict may occur  between the weights propagated by the two sources.  This situation is depicted in Figure 5.a for circuit 1 and  circuit 2. To solve this problem, we perform the  updating process for the VIPs whose links are span  along W and N, W and S, E and N, and E and S  directions in four distinct (and consecutive) steps.  Since the VIPs are constructed along a shortest path  between two nodes, they can span at most along two  directions. Obviously, the circuits belonging to each  group (for example, circuit 1 and circuit 3 in Figure  5.a) cannot have a common link (otherwise, the  corresponding VIPs will conflict in data network);  thus, the overlapping problem cannot occur. The type  (in terms of the connection direction) of a circuit is  easily determined at the setup phase based on the  source and destination addresses and stored in all  nodes belonging to the VIP.   a)           b) Figure 5. (a) Conflict among setup network circuits  during weight updating process. (b) The process of  finding the minimum weight. The shaded area is the  shortest-path area between S and D.  Constructing new VIPs. This process starts after  starting the previous step, when all VIP weights are  updated. In this phase, the packet-switched network  flows weighting higher than a given threshold request  for a VIP. The VIPs are constructed along one of the  shortest paths between the source and destination  nodes of the traffic flow. The nodes start this process in  order of their priority. Without loss of generality, we  assign a static priority to each node based on its  address in which node (0,0) has the highest and node  (n-1,n-1) has the lowest priority in an n×n network.  Each node tries to build a VIP for its traffic flows  weighting higher than the threshold and then, sends a  grant signal to the node with the next priority level.  This process is completed in 3 steps as follows. Let  shortest-path area between nodes S and D denote the  nodes along the shortest paths between S and D (the  shaded area in Figure 5.a).  Step1. Each node, upon receiving the grant signal,  should broadcast the destination address of its traffic  flow to the nodes of the corresponding shortest-path  area. Then, each node within this area can determine  the nodes from which it should receive data and the        nodes to which it should send its data during the VIP  construction process. To do this, the source node  broadcasts the y-coordinate of the destination address  of its traffic flow, Yd, to the nodes at its row, and the xcoordinate of the destination address to the nodes at its  column. Once all the nodes in the row (column) have  acquired the data, they initiate a broadcast in their  respective columns (row). The data is propagated in the  column (row) until reaching the node with the same ycoordinate (x-coordinate). To restrict the address  propagation to the nodes within the shortest path area  between the source and destination, the addresses at the  first step of this process are sent in a single direction  along the row (column) and towards the node with the  same x-coordinate (y-coordinate) as the destination. in  addition, Xd (Yd) is sent over the row (column) one  cycle ahead of Yd (Xd). The node with the same xcoordinate (y-coordinate) detects the address and  blocks the propagation of Yd (Xd) which will be  received at the next cycle.  This step takes d cycle, where d is the distance (in  terms of hop count) between the source and destination  nodes.   Step2. This step computes the cost of setting up a  new VIP. The cost is the cumulative weight of the  VIPs that will be torn down by this new VIP. If a port  is not used by any of the currently established VIPs,  the cost of using it for constructing the new VIP is 0.  In this step, the source node (node S in Figure 5.b)  sends the weight of its ports along the shortest paths (N  and E ports in Figure 5.b) as the cost to its neighboring  nodes.   The intermediate nodes may receive two costs from  the neighboring nodes. It is evident from Figure 5.b  (and can be easily proved) that the two costs are  received at the same cycle. The nodes select the  smaller cost, as the minimum cost of VIP construction  from the source node to the current node. These nodes  and the other intermediate nodes, in turn, add this  minimum cost to the weight of their ports along the  shortest path, and send it to the next nodes towards the  destination, until this cost is received to the destination  node. Each node keeps the track of the path with  minimum cost by storing the direction from which the  smaller cost is received.  The source node starts propagating  the cost  immediately  (one cycle) after broadcasting  the  destination addresses. In Figure 6, for example, if the  process starts by node S at time t, the node K receives  the y-coordinate and x-coordinate of the destination  address at cycle t+2, from S and E ports, respectively.  The node controller immediately processes the data  and determines that it should receive two costs from E  and S ports and send them along W and N ports. It  then, appropriately configures the input and output of  the dtatpath via the control lines cl2 and cl3 in Figure  4. At cycle t+3, when the node K sends the ycoordinate and x-coordinate of the destination address  to W and N ports, the costs received from E and S  ports are directed to the data-path in Figure 4, where  the minimum cost is added to the cost of the proper  ports. The new costs are then, sent to N and W port at  cycle t+4.   Finally the minimum cost is received at the  destination node while its corresponding path is kept  by the intermediate nodes.   Step3. At this step, the destination node sends back  the final minimum cost towards the source through the  nodes along the path with the minimum cost. The  nodes which have not received this cost at a specific  cycle (determined by their distance to the destination  node) do not belong to the path of the minimum cost  and will reset all their connections. The source receives  the cost and if the cost is less than its weight, it sends a  signal on the path to acknowledge setting up a new  VIP. Afterwards, a flag in all used ports is set and used  to tear down the conflicting VIPs at the next step. If the  minimum cost is greater than the traffic flow weight,  no VIP is constructed.  Destroying old VIPs. In this step, all setup network  nodes check their ports and if a node detects that a port  belonging to an old VIP is tagged to be used for  constructing the new VIP, it sends a signal towards the  source node of the VIP. Upon receiving the signal and  after all intransient data is delivered to the destination  node, the source node sends a signal to all nodes to  destroy the old VIP by resetting the registers in which  its direction and weight is stored. The VIP source node  should reset VIP flag associated to the traffic flow and  direct the traffic through the packet-switched network  hereafter.   The new VIP is then established by setting the  registers of the setup network nodes along the path to  store the weight and direction of the VIP. An identifier  is also assigned to a VIP. At the same time, the VIP is  set up in the data network by configuring the VIP  allocators in the nodes along the path and setting a flag  at the source node indicating that the traffic of the  corresponding flow should be directed through the  established VIP. Since each data network node is  configured by its corresponding node in the setup  network, setting up a VIP in the data network can be  done in a single cycle.  5. Evaluation Results  In this section, we first describe the simulation  environment. Then, the results of evaluating the  proposed method using some synthetic traffic patterns      which model the real CMP workloads are presented.  Afterwards, the proposed method is evaluated using an  existing multi-core SoC design which is widely used in  the literature. Finally, the effect of the proposed  method on the NoC area is evaluated.  5.1. Simulation Infrastructure  To evaluate the proposed on-chip communication  scheme, we have implemented the NoC architecture  using Xmulator, a fully parameterized simulator for  interconnection networks [17]. The simulator  is  augmented with the Orion power library [18] to  calculate the power consumption of the networks.  Simulations experiments are performed for a 128-bit  wide system. Moreover, the process feature size and  working frequency of the NoC is set to 100nm and 250  MHz, respectively, in the Orion library. In our NoC,  we used the conventional 5-stage pipelined wormhole  routers [15].   In this section, we compare our approach with a  conventional packet-switched (wormhole) NoC [15].  This NoC applies an adaptive shortest-path routing  scheme, and uses two virtual channels per physical  channel. Each VC has an 8-flit deep buffer. To ensure  freedom from deadlocks,  the escape route [15]  mechanism is used. When the static VIP construction  algorithm is used [11], the route for each traffic flow in  the packet-switched network is set during the mapping  phase; so, the deadlock-freedom of the NoC is  guaranteed at this phase. As a result, we can replace  the 1-flit VIP buffers with one of the virtual channels  of each physical port (1 VC + 1 VIP latch per port).  However, in the scheme proposed in this paper, where  the traffic pattern of the running application is not  known at the design time, the packet-switched data are  routed using an adaptive routing scheme, just like the  mentioned conventional packet-switched NoC. In the  packet-switched part of these NoCs, we keep the two  virtual channels per physical channel  to handle  deadlocks using escape routes. Therefore, the 1-flit  VIP buffers are added to the current set of VCs (2 VCs  + 1 VIP latch per port).  5.2. CMP Workloads  First, we use three synthetic traffic patterns to evaluate  the effects of the VIP connections on the CMP  workloads. In the first pattern, 1-hot traffic pattern,  each node sends 80% of the generated packets to  exactly one other node and the remaining 20% to other  randomly chosen nodes. The other two traffic patterns,  2-hot and 3-hot traffic patterns, send 20% of the  generated packets to randomly chosen nodes. In 2-hot  traffic pattern, the remaining 80% of the traffic of each  source node is sent to exactly two other nodes (40% to  each destination node), while in 3-hot traffic pattern,  this 80% traffic generated by each node is divided  among exactly three destination nodes. As mentioned  before, most of the CMP workloads exhibit a high  degree of temporal and spatial communication locality,  so the synthetic traffic we have used has similar  characteristics to commercial and scientific CMP  workloads. The favored destination nodes of each  source node are selected randomly and are changed  every 50,000 cycles to evaluate the dynamic VIP  construction algorithm. The setup network initiates the  VIP construction process 5000 cycles after changing  the traffic pattern.   600 500 400 300 200 100 ) s e l c y C ( y c n e t a L e g a s s e M e g a r e v A 0.02 0.04 0.06 0.08 0.1 0.12 Traffic (Message/node/cycle) 0 0 110 100 90 80 70 60 50 40 30 20 ) l e c y c J n / ( r e w o P 1-hot Conv . 1-hot V IP 2-hot c onv . 2-hot V IP 3-hot Conv . 3-hot V IP 1-hot Conv. 1-hot VIP 2-hot conv. 2-hot VIP 3-hot Conv. 3-hot VIP 0 0.02 0.04 0.06 0.08 0.1 0 .12 0.14 Tra ffic (Messa ge /nod/cycle ) Figure 6. (a) Average message latency (cycles for 8flit packets) and (b) Power consumption (nJ/cycle)  in a conventional and the proposed NoC with  dynamic VIPs for different traffic patterns  For future work, we intend to evaluate the dynamic  VIP construction process using the traffic patterns  extracted from real CMP workloads, and then we can  explore the effect of varying frequency of the VIP  construction process on the performance and power  consumption of the NoC.    Figure 6.a displays the average message latency of a  conventional NoC architecture and the proposed NoC  architecture under  the described workloads  for  different traffic injection rates in a 4×4 network. Figure  6.b shows the power consumption of the NoCs. The  results are obtained by applying the dynamic VIP  construction algorithm. The setup network has a  negligible contribution in the NoC power consumption  due to its small bit-width and infrequent operation.                        Nonetheless, we have  considered  the power  consumption of VIP construction process and NoC  traffic monitoring in the reported results.   As shown by the figures, the proposed method can  significantly improve the power and performance  metrics of the NoC. Note that increasing the number of  favored destinations of a source node results in  decreasing the effect of the proposed approach. The  reason is that the number of high-volume connections  is increased while the network resources that can be  used by VIP links are fixed.  Thus, the NoC cannot provide a VIP connection for  some traffic flows and larger portion of the traffic is  directed  through  the packet-switched network.  However, even in 3-hot traffic pattern, the VIP  connections can handle a significant portion of on-chip  traffic and improve the power and performance of  NoCs.  5.3. Multi-Core SoC Workloads  To evaluate the proposed dynamic VIP construction  mechanism on multi-core SoCs, we used  three  benchmarks, namely H.263 decoder+MP3 decoder,  H.263 decoder+MP3 encoder, and MP3 decoder+MP3  encoder. The task-graphs of these benchmarks are  presented in [5]. These benchmarks use the same cores  and we run them on a 4×4 mesh-based NoC one at a  time in three consecutive periods. In the simulation,  packets are generated with exponential distribution and  the communication rates between any two nodes are  set to be proportional to the communication volume  between them in the task-graph. This task-graph-based  traffic generation approach is introduced and used in  [19].  Table 1 shows the results of the proposed dynamic VIP  construction method using a 4-bit setup network when  running the above-mentioned benchmarks one at a time  (the running application is changed every 50,000  cycles). The  setup network  initiates  the VIP  construction process 5000 cycles after changing the  application.   Table 1. Average message latency (cycles for 8-flit  packets) and Power consumption (nJ/cycle) in a  conventional and the proposed NoC with dynamic  VIPs  Conventional  NoC  5.52  47.09  Proposed  NoC  2.98  29.65  Proposed/  Conventional  0.54  0.62  Power  Latency  In the proposed NoC, the cores are mapped by  NMAP, a simple and fast heuristic power-aware core  mapping method presented in [6]. The packet-switched  network uses an adaptive routing algorithm. In the  conventional architecture, mapping and routing is  performed by the NMAP algorithm on a mesh with 2  VCs per physical channel. The results show 46%  reduction in power and 38% reduction in average  message latency compared to a conventional NoC.   5.4. Area Evaluation Results  As mentioned before, the area of the setup network  (with 4-bit wide nodes and 4-bit unidirectional links)  can be ignored compared to 128-bit wide routers and  128-bit bi-directional  links of  the data-network.  However, we have evaluated  the area overhead  imposed by setup network using the NoC area model  presented in [20] and the Orion area estimation  function. The results show that the setup network  imposes less than 3% area overhead to the NoC.   In addition to setup network, we should evaluate the  impact of  the proposed router microarchitecture  modifications on the router area. The area overhead  due  to  router microarchitecture modification  is  completely negligible since the modifications require  adding a few 1-bit local wires and 2-bit registers to the  router. Adding the 1-flit VIP buffers to the current set  of VCs as an extra VC, as done for CMP workloads (2  VCs and 1 latch per port), imposes negligible overhead  to the router area. However, in static VIP construction  scheme, where one virtual channel and corresponding  8-flit deep buffers at each port are replaced with a 1-flit  VIP buffer, our approach not only have no area  overhead but also decreases the router area by 35%.  6. Conclusion  In this paper, we presented a packet-switched router  architecture that can provide low-power and lowlatency dedicated VIP connections between the source  and destination nodes of heavy communication flows.  This is achieved by means of a subset of virtual  channels which bypass the pipeline of the intermediate  routers along the path. Afterwards, amongst different  potential applications of the VIP connections, we  focused on power and latency reduction and developed  a run-time VIP construction scheme using a lightweight setup network. Since in multi-core SoCs and  CMPs, each node tends to have a small number of  favored destinations for the messages it sends, most of  the on-chip  traffic  is directed  through  the VIP  connections and considerable power and performance  improvement can be achieved. Simulating the proposed                            [11] M. Modarressi, H. Sarbazi-Azad, A. Tavakkol,  ""Virtual Point-to-Point Links in Packet-Switched  NoCs"", in IEEE Computer Society Symposium on  VLSI (ISVLSI'08), France, 2008.  [12] R. Vaidyanathan,  J.  Trahan, Dynamic  Reconfiguration: Architectures and Algorithms,  Springer, 2004.  [13] K. Goossens, J. Dielissen, and A.Radulescu, “The  Æthereal Network  on Chip: Concepts,  Architectures, and Implementations”, in IEEE  Design and Test of Computers, Vol. 22, No. 5,  Sept-Oct 2005, pp. 414-421.  [14] U. Ogras, et al., “Application-Specific Networkon- Chip Architecture Customization via LongRange Link Insertion”, in Proc. of DAC, 2005.  [15] W. J. Dally and B. Towles, Principles and  Practices of Interconnection Networks, Morgan  Kaufmann Publishers, 2004.  [16] M. Faruque, et al., “Run-time Adaptive On-Chip  Communication Scheme”, in Proc Intl. Conf. on  Computer Aided Design, 2007.  [17] Xmulator NoC Simulator: www.xmulator.org,  2008.  [18] H. Wang, et al., “Orion: A Power-Performance  Simulator for Interconnection Networks”, in Proc.  35th. MICRO, Turkey, 2002.  [19] J. Hu, and R. Marculescu, “Application Specific  Buffer Space Allocation for Networks on Chip  Router Design”, in Proc. Intl. Conf. on Computer  Aided Design, 2004.  [20] J. Balfour, W.J. Dally, “Design Tradeoffs for Tiled  CMP On-Chip Networks”, in Proc. of ICS, 2006.  NoC architecture using a synthetic  traffic flow  confirms these improvements. Besides, simulation  results using some multi-core SoC benchmarks showed  that, compared to a conventional NoC, the proposed  NoC architecture reduces the power consumption by  46% and average communication latency by 38%, on  average.  "
A modular synchronizing FIFO for NoCs.,"Systems-on-chip designs often use functional blocks operating at different clock frequencies. This motivates the use of an asynchronous network-on-chip (NoC) with synchronizing FIFOs interfacing between the NoC and the functional blocks. To minimize design time, these FIFOs should be constructed from cells available in a standard cell library and configurable to work in a wide range of applications. We present a modular synchronizing FIFO design that can be implemented using logic gates from a typical standard-cell library. The FIFO has interchangeable input and output interfaces for edge-triggered synchronous communication and for two asynchronous handshake protocols: asP* and LEDR. The FIFO capacity, synchronizer latency and interface protocols are independent parameters, allowing the FIFO to be easily configured for different NoC requirements. We evaluate performance using post-layout simulation results and analyze the metastability induced failure rate for synchronization latencies from half a clock cycle up to three clock cycles.","A Modular Synchronizing FIFO for NoCs Tarik Ono Sun Microsystems tarik.ono@sun.com Mark Greenstreet University of British Columbia mrg@cs.ubc.ca Abstract Systems-on-chip designs often use functional blocks operating at different clock frequencies. This motivates the use of an asynchronous network-on-chip (NoC) with synchronizing FIFOs interfacing between the NoC and the functional blocks. To minimize design time, these FIFOs should be constructed from cells available in a standard cell library and conﬁgurable to work in a wide range of applications. We present a modular synchronizing FIFO design that can be implemented using logic gates from a typical standard-cell library. The FIFO has interchangeable input and output interfaces for edge-triggered synchronous communication and for two asynchronous handshake protocols: asP* and LEDR. The FIFO capacity, synchronizer latency and interface protocols are independent parameters, allowing the FIFO to be easily conﬁgured for different NoC requirements. We evaluate performance using postlayout simulation results and analyze the metastability induced failure rate for synchronization latencies from half a clock cycle up to three clock cycles. 1 Introduction Large system-on-chip designs typically consist of multiple timing domains motivating the use of a network-on-chip as depicted in Figure 1. Transferring data between these domains requires synchronization as indicated by the sync blocks in the ﬁgure. Designers often use FI FOs because they provide a simple interface that decouples the sender and receiver actions and maximizes throughput. For networkon-chip applications, designers need synchronization FI FOs that can be constructed from components in a standard cell library and that can be conﬁgured to match the requirements of the particular design. We present a modular FI FO design that allows each of the following design choices to be made independently: • clocked or clockless interfaces; • synchronization latency for resolving metastability; 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  Figure 1. A Network-on-Chip for a design with multiple timing domains • FI FO capacity. The FI FO provides high-throughput; simulation results from extracted layout using the T SMC 90nm process show that it operates at 2-2.5Gtransfers/sec. The conﬁgurable synchronization latency supports latencies as small as half a clock period for designs operating at relatively low frequencies with more synchronization cycles for higher frequency operation. Our designs can be implemented using only cells that are available in a typical, standard cell library. We believe that no previous design offers the ﬂexibility, performance and support for design within an A S IC ﬂow that our FI FOs provide. The rest of the paper is structured as follows. We give an overview of related work in the next section and present our FI FO structure in Section 3. Our clockless FI FO with asP* handshake interfaces is described in Section 4. Section 4.3 introduces protocol converters between the asP* and L EDR protocols, enabling the delay-insensitive LEDR protocol to be used for global communication. Section 5 presents the clocked version. Finally, in Section 7 we give latency, throughput and power numbers from our FI FO implementations. 2 Related Work Our design was inspired by the modular FI FO from [3]. Like their design, our FI FO is composed of a ring of stages,   where each stage is composed of a data storage cell, a put interface and a get interface. Likewise, our put and get interfaces come in synchronous and asynchronous versions supporting a mix-and-match design approach. Our design differs from that in [3] in that ours only uses cells that are available from a typical standard cell library, whereas their design requires custom, precharged cells in the control blocks. Nevertheless, our design achieves performance that is comparable or even slightly higher than that reported in [3] when scaling for the different fabrication technologies. Furthermore, our design separates the FI FO control logic from the synchronizers, allowing the synchronizer latency to be chosen according to the clock frequency and reliability requirements. The design in [3] has the assumption of a two ﬂip-ﬂop (two-FF) synchronizer hardwired into the control logic. If more synchronizing FFs are required, then the control logic needs to be modiﬁed accordingly. There have been several proposals for interfacing different timing domains that pause or stretch the receiver’s clock (see e.g. [2],[15]). Such an approach requires changes to the receiver and also leads to more clock jitter, which might make such an approach infeasible for some applications. [7] looks at locally generated clocks in GAL S and shows that clock distribution delays can lead to synchronization failures. The authors present several solutions for metastability-free synchronizers, including the locally delayed latching (LDL) scheme. [6] expands on the LDL scheme and analyzes its performance and reliability. [14] introduced a pipelined synchronizer that allows high-throughput while maintaining reliable synchronization. The drawback is long data latencies through the pipeline. Our design achieves lower latency by taking the synchronization overhead out of the control path. [4] proposed a synchronizing FI FO design based on read and and write pointer comparison using Gray codes. The result of the comparison is synchronized to the sender’s and receiver’s clock. Their design uses two binary counters for read and write addresses and a dual-port RAM for data storage which results in a larger and more complicated design than ours for the modest FI FO capacities that are typically needed for NoC designs. Furthermore, the design in [4] only provides FI FO capacities that are powers of two, and it does not provide the interchangeable interfaces for synchronous and various asynchronous protocols that we offer. [17] presents a ripple-FI FO with clocked write and read interfaces, and a modiﬁed 4-phase handshake protocol to transfer data between the asynchronous stages. The design uses a custom latch circuity for the datapath to reduce FI FO area, and the paper focuses on the design-for-test (DfT) features of the FI FO. While noting the use of a custom latch cell, the paper does not provide the information to compare their design with ours in terms of throughput, latency, robustness or modularity. In particular, [17] does not describe Figure 2. FIFO structure the implementation of the interfaces between the self-timed FI FO stages and clocked stages, nor does it present any latency or throughput results. The design in [12] uses the position of the read and write pointers to determine fullness and emptiness. It has one synchronizer to detect fullness, and per-stage synchronizers to detect emptiness. To prevent errors due to metastability when sampling the pointers, they use two tokens each as the read pointer and write pointer. This makes their empty detector a little more complicated than ours, as it requires comparing two synchronized write pointer bits with two read pointer bits for each stage. In addition, to differentiate between a full and an empty FI FO, they only ﬁll the FI FO up-to the second-to-last position, which means that an n stage FI FO can only hold n − 1 items. 3 FIFO Structure Figure 2 shows our synchronizing FI FO design. The sender communicates with the FI FO through the put interface and the receiver through the get interface. The FI FO consists of a ring of stages, where each stage has its own put interface, get interface, full-empty control and data storage cells. As described in Sections 4 and 5, the put and get interfaces can support either clocked or clockless communication. The put interface cells implement a ring counter where a token is passed from one cell to the next each time Figure 3. An asP* FIFO a data value is written into the FI FO. Likewise, the get interface cells implement a ring counter whose token marks the cell for the next data read operation. 4 The asP* FIFO We chose the asP* handshaking protocol [11] for our design because it offers good performance using cells that are available in a standard cell library. In particular, asP* does not require C-elements or other circuits that are peculiar to asynchronous design. In this section, we ﬁrst describe asP* and then use it to implement our FI FO. Subsequent sections show variations of this design that support clocked operation and other asynchronous handshaking protocols. 4.1 asP* Figure 3 shows a simple asP* FI FO. Each stage consists of a SR latch that keeps track of the empty/full status of the stage and a D latch that holds data. AND-gates coordinate data transfers between stages. In particular, SR latch EFi is set to indicate that D latch Li holds valid data. When stage i − 1 is full and stage i is empty, the output of the AND-gate between them goes high, loading data into latch Li , setting SR latch EFi to indicate that latch Li now holds valid data, and clearing SR latch EFi−1 to indicate that latch Li−1 is now empty. The asP* protocol is not delay insensitive. In particular, the resetting of one SR latch and the setting of its successor occur concurrently, and either one resets the output of the AND-gate between them. This means that the minimum pulse width required for R (resp. S) must be less than the minimum propagation delay from S to Q (resp. R to Q) plus the delay of the AND-gate. In practice this requirement is easily satisﬁed for structures such as a FI FO where all of the components are in relatively close physical proximity. For longer connections, such as those with an asynchronous NoC, a delay insensitive protocol such as LEDR [5], L ET S [9] or a one-out-of-four code [1] can be used. We describe a protocol adapter to LEDR in Section 4.3 and note that an interface to a one-out-of-four, L ET S or other delay-insensitive codes would be similar. Figure 4. Three stage FIFO with clockless sender and receiver 4.2 asP* FIFO Description Figure 4 shows a three-stage clockless synchronizing FI FO. Requests by the sender to write to the FI FO arrive on the req_put input port and the FI FO raises ack_put_top, if the write succeeded. When the FI FO has data available, it notiﬁes the receiver by raising data_valid_top. The receiver acknowledges receipt of data on the input port got_data. Figure 5 shows a single stage of the asP* FI FO. The put_token_in and put_token_out signals transfer the put token between put interface cells and likewise for the get token and get interface cells. Data is stored in the latches on the left, which are written when the cell is empty. Figure 6(a) presents the asP* put interface for our FI FO. The reset signal is used during initialization and sets the put_token_out output in all but one cell to low. Requests to put a new data item arrive on the req_put port. If this FI FO stage holds the put token and empty is high, the ack_put/write signal is asserted, see Figure 7. If the stage is empty, the put request also causes incoming data to be latched by the data holding elements. As speciﬁed by the asP* protocol, the sender drops req_put in response to the rising edge of ack_put. This triggers the FFs in all of the put interfaces thereby passing the token to the stage to the right. Furthermore, the lowering of req_put will cause the put interface to lower ack_put. In fact, the other two inputs to the AND-gate that generates ack_put/write are also self-resetting: the put_token_in signal drops when the token moves, and the full/empty controller lowers the empty signal in response to the assertion of write. This insures that the put interface correctly follows the asP* protocol. Note that the req_put signal from the sender is broadcast to all of the put interface cells; conversely, the ack_put signals from each of the put interface cells are combined with an OR-gate to produce the ack_put_top large as the minimum clock pulse width for the FFs in the put interfaces. • The minimum high time for req_put must be at least as large as the minimum pulse width for the set signal of the SR latch in the empty/full controller. • The minimum high time for got_data must be at least as large as the minimum pulse width for the set signal of the SR latch. All three of these are minimum delay times should be easily satisﬁed by real designs – any circuit that implements the asP* protocol will provide sufﬁcient delays to satisfy these requirements. 4.3 Asynchronous Protocol Converters We implemented our basic FI FO using the asP* protocol because it can be implemented using cells from a typical library and is relatively efﬁcient. While the delay constraints for asP* are easily satisﬁed for the FI FO where all cells are placed relatively close to one another, asP* is less appropriate for long-wire interconnect as routing tools for standard A S IC design ﬂows provide minimal support for delay matching of interconnect paths. This section presents an interface between asP* and the LEDR protocol [5] which is (a) put interface (b) get interface Figure 6. asP* interface cells Figure 5. FIFO stage with asP* put and get interfaces signal from the FI FO back to the sender. The asP* get interface cell is depicted in Figure 6(b). If a stage has the get token and is full, then it raises the signal data_valid. These data_valid signals from each of the get interfaces are combined with an OR-gate to generate the data_valid_top signal for the complete FI FO. This functions as a request to the receiver to take the data. When the receiver has consumed this data, it raises the got_data signal which causes read to go high and then full to go low, so that data_valid will drop as well. The resulting drop of data_valid_top is inverted to create a rising edge on move_data thereby triggering the FF in all of the get interface cells and passing the get token to the next stage. As shown in Figure 8, the data full-empty controller keeps track of whether the FI FO stage is empty or full. The SR latch is set by a write operation from the put interface and reset by a read operation from the get interface. The AND gates ensure that the set and reset inputs of the latch are not both high at the same time. This simpliﬁes timing analysis for our design by avoiding races when the put interface writes to a stage that the receiver is waiting to read or vice-versa. If a SR latch is not available in the standard cell library, a FF can be used instead, where the FF output toggles when either a write or a read is asserted. Correct operation of the FI FO requires the sender and receiver to ensure the following timing requirements: • The minimum low time for req_put must be at least as Figure 7. Timing of asP* put interface Figure 8. Data full-empty control block better suited for long wire interconnect [13]. We choose the two-phase, L EDR protocol as our example because it can achieve higher bandwidth in the presence of long wire delays than a four-phase protocol such as the one-in-four code used in CHA IN [1]. If a converter to a four-phase protocol is required, [10] describes standard-cell based converters that convert between L EDR and four-phase protocols. L EDR [5] uses two wires per bit: a data wire and a parity wire. For each symbol, the data wire is set to the value of the bit and the parity wire is set to that symbols alternate between codes where the two wires have different values (odd parity) and codes where they have the same value (even parity). There is exactly one transition per symbol, and the arrival of successive symbols can be detected with an XOR-gate. 4.3.1 LEDR-to-asP* protocol converters Figure 9(a) shows our LEDR-to-asP* protocol converter. The XOR-gates for each data input form the completion detector. These bit-wise completion signals are combined by the wide AND- and OR-gates and the SR latch SR1. Latch SR1 is set when all input bits have odd parity, and reset when they all have even parity. When all bits have changed to a new code symbol, put_ack_LEDR/req_mp changes value which provides an acknowledgment to the LEDR sender and causes latches D1 to go opaque and store the data word from the LEDR sender. The protocol converter is pipelined to support highthroughput applications. The ﬁrst stage converts from LEDR to the micropipeline protocol [16], and remainder of the circuit is a micropipeline to asP* converter. This second half (a) LEDR-to-asP* conver ter (b) asP*-to-LEDR conver ter Figure 9. Protocol converters could be used by itself to provide a micropipeline interface to our FI FO, in which case signals req_mp, and d_mp would be the request and data from the micropipeline and ack_mp_bar would be the inverted acknowledge to to micropipeline. The micropipeline to asP* conversion is effected by toggle element T which keeps track of the parity of the number of put_ack pulses. When this parity matches the parity of the micropipeline, latch D2 is empty and enabled to receive new data. When the parities differ, latch D2 goes opaque to hold the last data word received. Figure 9(b) shows our asP*-to-LEDR converter. Like the L EDR-to-asP* converter, it is pipelined: the ﬁrst stage converts from asP* to micropipeline; and the second stage from micropipeline to asP*. The ﬁrst stage is empty if t1 = t2. If data_valid is asserted when the ﬁrst stage is empty, a pulse is generated on c1 loading the data into the latch D1, toggling the value of t1 and thereby marking the ﬁrst stage as full. Likewise, the second stage is empty if t2 = LEDR_ack. When the ﬁrst stage is full and the second empty, a pulse is generated on c2 outputing a new data word. The XOR of the data with the parity of the number of data words that have been output ensures that the ledr_data_out and ledr_parity_out signals alternate between the even- and odd-parity L EDR codes. These protocol converters have internal timing constraints typical for asP* or micropipeline designs. We do not enumerate them here due to space constraints. Of course, the LEDR interfaces are delay insensitive. 5 Clocked FIFOs If our only goal were to produce an asP* FI FO, the design from Figure 3 would be quite adequate and considerably simpler than the ring-structure shown in Figure 2. The advantage of the latter design is that we can replace its put and get interfaces with clocked versions to produce asynchronous-to-synchronous, synchronous-toasynchronous and synchronous-to-synchronous converters, with the last one supporting arbitrary relationships between the sender’s and receiver’s clocks. For brevity, we describe the synchronous-to-synchronous version here. The other two conﬁgurations are obtained by combining an asynchronous put interface with a synchronous get interface or vice-versa. We ﬁrst present the FI FO design, and then we examine the design of the synchronizer circuits for the full and empty signals in more detail. 5.1 Clocked put and get interfaces Figure 10 shows a three-stage clocked FI FO. The space_avail signal indicates that data can be put into the FI FO, and the data_valid signal indicates that data is available for the receiver. The sender may request to put data into the FI FO by asserting the data on datain and driving req_put high prior to the rising edge of clk_put. If there is space in the FI FO, then the data will be written into the FI FO and space_avail will be asserted following the rising edge of the clock. As long as FI FO does not become full, space_avail will remain asserted and the sender may insert a new data value each cycle. Likewise, the receiver can assert its readiness to receive data by asserting req_get prior to the rising edge of clk_get, and the FI FO will assert data_valid following the rising edge to assert that the data on dataout satisﬁes the request. Thus, the receiver can remove a value every clock cycle provided that the FI FO does not become empty. To ensure fully synchronous behavior by the interfaces, the FI FO has data FFs at the input and at the output and also holds signals space_avail and data_valid in FFs. Figure 11 shows a FI FO stage consisting of clocked put and get interface cells, datapath latches and a full-empty controller. The full-empty controller was shown in Figure 8. The clocked put interface cell is depicted in Figure 12. It consists of two FFs and one put control block. The put control block handles synchronization of the full signal that can fall asynchronously with respect to put_clk and is described in Section 5.2. When the sender asserts req_put, the stage for which put_token_in is high will latch the incoming data into its data storage latch by generating a pulse on the write signal. A rising clock edge of put_clk with en_put asserted also causes the token to be passed to the Figure 10. Three stage FIFO with clocked sender and receiver Figure 11. FIFO stage with clocked sender and receiver The synchronizer can consist of any number of halfcycle and full-cycle synchronizing stages; the number needs to be carefully chosen given the required M TB F and the desire to minimize latency through the FI FO. To meet a given M TB F, the required synchronization time is inversely proportional to the clock speed [8]. One possible half-cycle stage is a transparent latch as depicted as the ﬁrst stage in Figure 15. Full-cycle synchronizing stages can be FFs. When using a half-cycle synchronizer, events that occur during the latch transparent period are still synchronized, as shown in the timing diagram of Figure 16. Events that arrive within the time period Twindow have to wait until the clock edge at the end of the synchronization period Tsync half before they are sampled by the next latching stage. Note that introducing any additional logic into the path between the synchronized signal and its next latching stage would reduce the synchronization time. Figure 12. Clocked put interface next cell. Figure 13 shows the timing diagram. The get interface cell works in an analogous fashion. Figure 13. Timing of clocked put interface Figure 15. 3.5 cycle synchronizer with asynchronous set Figure 14. FIFO put control block 5.2 Synchronization Figure 16. Timing for half-cycle synchronizer Figure 14 shows the put control block of a clocked put interface. It consists of a synchronizer and an AND gate, and raises write_enable if the synchronized full signal is low. The write_enable outputs from all FI FO stages are combined together into the space_avail signal for the sender (see Figure 10). Similarly, the clocked get interface has a get control block that raises the read_enable signal if the synchronized empty signal is low. The read_enable signals from all FI FO stages are combined into the data_valid signal for the receiver. Table 1 shows example τ (metastability resolving constant) values and M TB F numbers for different synchronizers, depending on clock speed. We obtained these numbers using the method described in [18]. The simple latch is more robust than the single FF, because the FF simulated here has NAND and NOR gates (rather than inverters) driving its master and slave stages, which increases its τ . When a FI FO stage changes from empty to full, this status change is caused by a write operation, which is synchronous to the put clock; so there is no need to have this status Table 1. MTBF for different synchronizers and clock speeds, 90nm technology 1 2GHz synchronizer MTBF cycles τ (ps) 100MHz 500MHz 12.74 (cid:29) 109 yrs (cid:29) 109 yrs 2 ( latch) 23 yrs < 1 ms 1 ( 1 FF) 44.56 (cid:29) 109 yrs 105 yrs 17 mins < 1 µs 44.49 (cid:29) 109 yrs (cid:29) 109 yrs 106 yrs 2 ( 2 FFs) 65 days 44.14 (cid:29) 109 yrs (cid:29) 109 yrs (cid:29) 109 yrs 3 ( 3 FFs) 100 yrs 1GHz change go through the synchronizer in the put controller. Similarly, a change from full to empty happens because of events that are already synchronous to the get clock, and there is no need to synchronize the status change to the get clock. To avoid such unnecessary synchronizations, we use FFs with asynchronous set inputs, as shown in Figure 15. In the put controller, the synchronizer FFs use the write signal as the asynchronous set, causing the synchronizer output to go high as soon as a put request has been granted. This immediately lowers the write_enable signal, preventing any subsequent writes to this FI FO stage, until full is lowered by a read operation. In the get controller the read signal is used as the asynchronous set to the synchronizer. The immediate synchronizer output change prevents FI FO overﬂows and underﬂows. A similar scheme is used in [4]. This asynchronous set is not necessary if the FI FO has enough stages to hide the synchronization latency. 5.3 Throughput of the Clocked FIFO To maximize throughput, the FI FO needs to allow the slower of the put and get interfaces to read from or write to the FI FO every cycle. There is a minimum FI FO length that achieves maximum throughput. This minimum length depends on the synchronization latencies of the put and get interface, on the ratio of put and get clock speeds, on the clock speeds and on the phase relationship between the clocks. As an example, Figure 17 shows a simulation plot of a three stage FI FO with a 2-cycle synchronizer. The put and get clocks have the same frequency and zero phase offset. At around 12ns the ﬁrst put request is issued and data is written to the ﬁrst FI FO stage on the next rising clock edge. Three clock cycles later data_valid rises, allowing the receiver to consume the data. By this time, three more FI FO writes have taken place and the space_avail signal drops to notify the sender that the FI FO is full. Another three clock cycles later, at around 24.4ns, signal space_avail rises and the sender can write to the ﬁrst FI FO stage again. The put request at 18ns is not serviced until this time. It is obvious that this FI FO doesn’t allow the sender to write every clock cycle nor the receiver to read every clock cycle. A 6-stage FI FO however would achieve a throughput of one data item every clock cycle. In essence, one FI FO stage is Figure 17. Simulation of clocked interfaces needed for every clock cycle of roundtrip latency through the FI FO, with the roundtrip latency deﬁned as the minimum time from a put operation to FI FO stage i until stage i is empty again. In this example, we need two FI FO stages for the two synchronizer latencies plus two additional stages to account for the delay in the synchronizers sampling the full/empty signals. More generally, a FI FO with equal put and get frequencies and an n-cycle synchronizer at both interfaces needs at least 2 ∗ (n+ 1) stages to support maximum throughput. 6 FIFO Testing The datapath through the F IFOs can be tested on-chip by having scanable input and output registers. In the clocked FI FO, the input and output FFs can perform this function. The majority of control path faults can be detected by ﬁlling the FI FO, observing that the FI FO correctly refuses to write further data into the FI FO, then reading out the data and observing that the empty FI FO correctly prevents further reads. Unfortunately detecting faulty behavior doesn’t necessarily indicate what went wrong. For example, if the FI FO doesn’t allow reads despite not being empty, it could be that there is no get token, that the ﬁrst stage’s full-empty controller is not correctly set to full, or that the read_enable/ data_valid signal is stuck low. For further observability the state holding elements in the control path could be made scanable at the expense of additional area. To test the maximum frequency of the clocked FI FO, the clock frequency can be gradually increased. Since the critical path involves the en_put signal (see Figure 12), if the frequency is too high, then there will either be more than one token or zero tokens in the FI FO. Inspection of the output data will show if there are multiple write or read tokens. If there is no token, then the FI FO won’t accept any more read or write requests. This is easily detected. Table 2. Summary of results clockless 3 stage clocked 3 stage 360 + trecv up to 1.95 a 257 + tsync 0.5 ∗ 2.56 390 clocked 6 stage 257 + tsync 2.33 430 latency (ps) throughput (GHz) critical path (ps) power max(513, 300 + tsender ) 2.7pJ/request 4.20mW 8.69mW aNote that tsync can be multiple clock periods 7 Implementation Results We built three FI FOs using a TSMC 90nm standard cell library developed in-house for other projects. The fan-outof-four (FO4) delay for this technology is 34ps. One FI FO has clockless interfaces, while the other two have clocked interfaces with two-FF synchronizers. One of the clocked FI FOs and the clockless FI FO have three stages, while the third FI FO has six stages. The datapath is 8 bits wide. In our layout, the FI FO stages were placed side-by-side and the datapath width has no effect on the critical path. In larger FI FOs the stages might have to be arranged in an array, and the datapath width would affect the wire delays in the critical path. We simulated the clockless and clocked FI FOs by extracting a netlist from the layout that included parasitic capacitances. The results from our simulations are summarized in Table 2. The following sections discuss these results in more detail. 7.1 Minimum Latency of Clockless FIFO In the clockless FI FO stage, the latency is measured from when the sender issues a put request by raising req_put until the stage is empty again and empty rises. In our simulations we measured a delay of 220ps from from req_put rising to data_valid rising. After got_data rises, it takes 140ps for the cell status to change to empty. So the minimum latency is 360ps (10.5 FO4 delays) excluding the delay in the receiver to raise got_data after being notiﬁed that data is available. Adding an interface converter, like the LEDR converters presented in Section 4.3.1 will increase this latency. 7.2 Throughput of Clockless FIFO The throughput of the clockless FI FO is limited by the slower of the put and get interfaces. The critical path in the put interface is from when one put request arrives, until the next FI FO stage is ready for the next put request, assuming that it is empty. From a rising req_put to the next stage’s put_token_out rising, we measured a delay of 300ps (about 9 FO4), plus the delay in the sender (t sender) to lower req_put after ack_put_top rises. On the get interface side the critical path is from a rising got_data signal to the next data_valid signal. Assuming that the FI FO had data available, we measured a delay of 513ps, or 15 FO4 delays. Hence, if t sender ≤ 213ps, then the throughput of the clockless FI FO is limited by the get interface and is equal to 1.95G words/second. Otherwise the put interface is the bottleneck and the throughput is lower than 1.95G words/second. 7.3 Minimum Latency of Clocked FIFO We measure the latency through the clocked FI FO as the time from the rising edge of clk_put which latches the data in the FI FO, until the rising edge of clk_get, which tells the receiver that there is valid data at the FI FO’s output. Our synchronizer FFs have a setup time of 20ps, so to get the ideal minimum latency we had to shift our receiver clock such that the rising edge of clk_get arrives exactly 20ps after a stage’s full signal rises. This means that the synchronizer in the get interface samples the stage’s full signal at the earliest possible moment. In our simulations we measured an ideal minimum latency of 257ps, excluding the synchronizer delay in the get interface. This is equivalent to about 7.5 FO4 delays. This latency is independent of the number of FI FO stages. 7.4 Throughput of Clocked FIFO Our simulation results showed a critical path delay of 390ps (11.4 FO4) for the 3-stage FI FO, limiting the clock speed to around 2.56GHz. As discussed in Section 5.3, a three-stage FI FO with a two-cycle synchronizer can’t support one data insertion or removal every clock cycle. It will only allow one every two clock cycles on average, resulting in a maximum throughput of 1.28GHz. To maximize the throughput of the clocked design with two-FF synchronizers we needed a 6-stage FI FO. This 6-stage FI FO had a maximum put and get clock speed of 2.33GHz at full throughput. 7.5 Power We determined the power usage of our FI FO control circuits by simulating the FI FOs with quiescent data paths. The clockless design consumed about 2.7pJ per request. At a throughput of 1.95GHz, this results in an average power usage of 5.27mW. At 2.33GHz throughput the power consumption of the clocked 6-stage synchronizer FI FO was 8.69mW. We ran the 3-stage FI FO with 2.56GHz clocks and one FI FO write and read every two clock cycles. We measured a power use of 4.20mW. For an equivalent throughput, the 6-stage FI FO consumes 4.91mW, with clocks running at 1.28GHz. 8 Conclusion We have presented a modular synchronization FI FO suitable for use in NoC designs. The design allows independent choices of the sender’s protocol (synchronous or asynchronous), the receiver’s protocol, the FI FO capacity and the time allocated for metastability resolution. Our design only uses cells that are available in a typical standardcell library making it well-suited for use in an A S IC design ﬂow. Even without the use of full-custom logic, our FI FOs achieve excellent throughputs of up to 1.95 Gtransfers/sec for an async-to-async implementation and 2.56 GHz operation for the synchronous-to-synchronous version when implemented in the TSMC 90nm process. When used with a synchronous client, the control signals must be synchronized with the client’s clock. The synchronization and thus FI FO latency can be as little as 1 2 clock cycle for low frequency designs and longer synchronization chains can be used to achieve an acceptable M TB F with higher frequency operation. Because the synchronizer latency is a separate design concern from the other aspects of the FI FO, a design could dynamically change the synchronization latency to achieve optimum performance and high M TB F while in tandem with dynamic frequency scaling. We expect that the two most common applications of this design will be a simple, high-throughput interface between synchronous blocks operating with independent clocks and as an interface between synchronous blocks on a systemon-chip and an asynchronous NoC. To support the latter, we presented protocol converters between LEDR and the asP* protocol of the FI FO. These protocol converters are implemented using cells from a standard cell library, maintaining the practicality of the design for use in an A S IC design ﬂow and increasing the ﬂexibility for conﬁguring the FI FO for use in a wide range of applications. "
Analysis of worst-case delay bounds for best-effort communication in wormhole networks on chip.,"In packet-switched network-on-chip, computing worst-case delay bounds is crucial for designing predictable and cost-effective communication systems but yet an intractable problem due to complicated resource sharing scenarios. For wormhole networks with credit-based flow control, the existence of cyclic dependency between flit delivery and credit generation further complicates the problem. Based on network calculus, we propose a technique for analyzing communication delay bounds for individual flows in wormhole networks. We first propose router service analysis models for flow control, link and buffer sharing. Based on these analysis models, we obtain a buffering-sharing analysis network, which is open-ended and captures both flow control and link sharing. Furthermore, we compute equivalent service curves for individual flows using the network contention tree model in the buffer-sharing analysis network, and then derive their delay bounds. Our experimental results verify that the theoretical bounds are correct and tight.","Analysis of Worst-case Delay Bounds for Best-effort Communication in Wormhole Networks on Chip ‡ † Yue Qian , Zhonghai Lu and Wenhua Dou School of Computer Science, National University of Defense Technology, China Dept. of Electronic, Computer and Software Systems, Royal Institute of Technology (KTH), Sweden †{yueqian, douwh}@nudt.edu.cn; ‡ E-mail: zhonghai@kth.se † ‡ † Abstract In packet-switched network-on-chip, computing worst-case delay bounds is crucial for designing predictable and costeffective communication systems but yet an intractable problem due to complicated resource sharing scenarios. For wormhole networks with credit-based ﬂow control, the existence of cyclic dependency between ﬂit delivery and credit generation further complicates the problem. Based on network calculus, we propose a technique for analyzing communication delay bounds for individual ﬂows in wormhole networks. We ﬁrst propose router service analysis models for ﬂow control, link and buffer sharing. Based on these analysis models, we obtain a bufferingsharing analysis network, which is open-ended and captures both ﬂow control and link sharing. Furthermore, we compute equivalent service curves for individual ﬂows using the network contention tree model in the buffer-sharing analysis network, and then derive their delay bounds. Our experimental results verify that the theoretical bounds are correct and tight. 1. Introduction The provision of Quality-of-Service (QoS) has been a major concern for Network-on-Chip (NoC) since its birth around year 2000 [2]. The reason is due to the fact that routing packets in resource-sharing networks creates contention and thus brings about unpredictable performance. This non-determinism does not meet the requirements of building predictable communication systems in which delay bounds must be guaranteed even under worst-case conditions. Many applications such as multimedia, HDTV, set-top and gaming boxes have stringent requirements on communication delay bounds [9]. For example, processing 25 high deﬁnition video frames (1680 x 1024, 1920 x 1280, etc.) must be completed within 20 ms. Generally speaking, a packet-switched network may provide best-effort (BE) and guaranteed services to satisfy the requirements of different QoS provisions. To offer strict promises, a guaranteed service typically reserves resources for exclusive use. This essentially isolates interferences. For example, timedivision-multiplexing virtual circuits (VCs) are proposed for the Æthereal [4] and Nostrum [7] NoCs. However, there are two main drawbacks. First, while such VCs provide guarantees once they are established, the setup procedure itself is nonpredictable if it is done dynamically. Second, resources may often be over-reserved, leading to lower resource utilization. To make a good utilization of the shared network resources, BE 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  networks are preferred. Nevertheless, BE networks can achieve good average performance, but the worst case performance is extremely hard to predict. The reasons are that (1) network contention for shared resources (buffers and links) includes not only direct contention but also indirect contention. They are dynamic and difﬁcult to capture in their entirety; (2) identifying the worst case is nontrivial. The worst case is in general unknown or uncertain. In wormhole networks with credit-based ﬂow control, the existence of cyclic dependency between ﬂit delivery and credit generation further complicates the problem. This hard situation leaves designers with simulation as almost the only way to ﬁnd the maximal delay by simulating various trafﬁc scenarios. While the simulation based approach can offer high accuracy, it can be very time-consuming. Each simulation run may take considerable time and evaluates only a single network conﬁguration, trafﬁc pattern, and load point. It is difﬁcult, if not impossible, to cover all the system states. Figure 1. A network delivering trafﬁc ﬂows. In this paper, based on network calculus [3][5][6][11], we aim for deriving the worst-case delay bounds for individual ﬂows in on-chip networks, particularly in wormhole networks, since we also take into account of ﬂow control besides link and buffer sharing. To illustrate the problem, we draw a network delivering multiple ﬂows in Figure 1. Our objective is to derive the worst-case delay bound for each ﬂow. Our assumption is that the application-speciﬁc nature of on-chip communication enables us to characterize trafﬁc with sufﬁcient accuracy. This trafﬁc characterization follows the abstraction of the arrival curve. The router service model follows the abstraction of the service curve. We also assume a deterministic routing, which is cheap to implement on silicon and gives more determinism. To derive the delay bound per ﬂow, the main problem is to derive the equivalent service curve the tandem of routers provides to the ﬂow. To this end, we ﬁrst analyze the resource   sharings in routers, and then build analysis models for different resource sharing components. Based on these models, we can derive the equivalent service curve a router provides to an individual ﬂow. To consider the contention a ﬂow may experience along its routing path, we classify and analyze ﬂow interference patterns. Such interferences can be captured in a contention tree model. Based on this model, we can derive the equivalent service curve the tandem of routers provides to an individual ﬂow. With a ﬂow’s arrival curve known and its equivalent service curve obtained, we can compute the ﬂow’s delay bound. The remainder of the paper is organized as follows. Section 2 reviews related work and summarizes our contributions. In Section 3, we describe the wormhole network and discuss different resource sharings in the network. In Section 4, we ﬁrst analyze the resource sharings and then derive equivalent service curves for individual ﬂows at each router. Based on the analysis models in Section 4, we present our delay bound analysis technique in Section 5, followed by an illustrative example in Section 6. Experimental results are reported in Section 7. Finally we draw conclusions in Section 8. 2. Related Work In general queuing networks, network calculus provides the means to deterministically reason about timing properties of trafﬁc ﬂows. Based on the powerful abstraction of arrival curve for trafﬁc ﬂows and service curve for network elements (routers, servers), it allows computing the worst-case delay and backlog bounds. Network calculus has been extremely successful for ensuring performance bounds when applied to ATM, Internet with differentiated and integrated services, and other types of networks. Systematic accounts of network calculus can be found in books [3][5]. Our work applies the network calculus theory [3][5][6][11] for on-chip networks. Our major contribution is the analysis technique to compute communication delay bounds for ﬂows in BE wormhole networks (Section 5). As part of this technique, we decompose resource sharing into three components, namely, control sharing, link sharing and buffer sharing (Section 3), and propose analytical models for the three sharing components (Section 4). The control sharing model allows us to break the feedback loop in the initial analysis model and, together with the link sharing model, we can transform the initial model into a buffer-sharing analysis model (Section 4 and 5). Furthermore, we identify three basic ﬂow contention patterns in the buffer-sharing analysis network (Section 5). Any complex contention scenario can be described by a composition of these basic patterns. In our previous work [10], we have combined the network calculus with the contention-tree model [8] to compute delay bounds for individual ﬂows. However, (1) we have assumed conceptually inﬁnite (big enough) buffers. In this paper, we consider bounded buffers. The inﬂuence of bounded buffers is signiﬁcant because of the back-pressure introduced by the ﬂow control; (2) In [10], we focused on buffer sharing, and have not distinguished link sharing from buffer sharing. This separation is necessary due to the variety of link sharing algorithms and their impact on performance and cost; (3) We have not considered the combined effect of buffer sharing together with ﬂow control and link sharing in [10]; (4) This work performs analysis on wormhole networks. 3. Resource Sharing in Wormhole Networks We introduce the wormhole network and assumptions. Then we discuss resource sharing in the network. 3.1. The Wormhole Network t i d e t i r l C F t i d e r C t i l F Credit Router Flit Router Flit Credit NI IP Core NI IP Core West Port Credit Flit Flit Credit Local Port Flit Flit NI Local IP North Port Remote Buffer Space (Space) Switch East Port Credit Flit Flit Credit Input Buffer South Port Credits to Report (Credit) t i d e r t i l F t t i i l d F e r (a) Two Connected Routers  (b) An Input-buffering Wormhole Router C C Figure 2. Portion of a wormhole network. Figure 2(a) shows part of a wormhole network with two nodes. A node contains a core and a router, which are connected via a network interface (NI). At the link level, the routers perform credit-based ﬂow control. The two routers are connected with each other via two bi-directional channels. Each direction contains one channel for transmitting ﬂits (data ﬂow) and the other for sending credits (control ﬂow). The internal structure of the wormhole router is illustrated in Figure 2(b). The router contains one crossbar switch, one buffer per inport, and a credit counter for ﬂow control. Each buffer is associated with control status (routing computation, switch allocation and switch traversal) for ﬂit delivery and with credit information sent back to the upstream router to indicate the number of its free buffers. A ﬂit cannot be forwarded until there is free buffer space (credit) available in the input buffer of the downstream router. This provides back-pressure for trafﬁc ﬂows, avoiding buffer overﬂow and ﬂit drop. There exists a one-to-one correspondence between ﬂits and credits, meaning that delivering one ﬂit requires one credit, and forwarding one ﬂit generates one credit. Initially the number of credits equals the buffer size. The wormhole network delivers trafﬁc ﬂows. A ﬂow f is an inﬁnite stream of unicast trafﬁc (packets) sent from a source node to a destination node. Flow i is denoted as fi , and f{i,j} represents an aggregate ﬂow which is a composition of ﬂows fi and fj . In addition to credit-based ﬂow control, we assume that the network performs deterministic routing, which does not adapt trafﬁc path according to the network congestion state but is cheap to implement in hardware. This means that the path of a ﬂow is statically determined. Further, we assume that, while serving multiple ﬂows, the routers employ weighted round-robin scheduling to share the link bandwidth. The switches use the FIFO discipline to serve packets in buffers. 3.2. Resource Sharing In such a network, ﬂows subject to ﬂow control are delivered via shared links and buffers. At each router, we identify three types of resource sharing, namely, control sharing, link sharing and buffer sharing. Control sharing means that routers share and use the status of buffers in the downstream routers North Port North Port West Port East Port West Port East Port Switch Switch Local Port Flit Flit South Port Local Port Flit Flit South Port (a) Link sharing (b) Buffer sharing Figure 3. Link and Buffer Sharing to determine when packets are allowed to be forwarded. We call this sharing of control information by packets ﬂow control sharing or simply control sharing. While packets advance in the network, they share link bandwidth and buffers. Accordingly, we identify link sharing and buffer sharing. Link sharing means that multiple ﬂows from different buffers share the same outport and thus the output link bandwidth. As shown in Figure 3(a), ﬂows, f1 , f2 and f3 , which are from the West, North and South inport, respectively, share the East output link. The switch arbitrates and sends the three ﬂows to the East outport. Buffer sharing means that an aggregate ﬂow, which are to be split, share a buffer. For instance, as illustrated in Figure 3(b), ﬂow f{1,2,3} shares the buffer at the West inport, which is an aggregate of three ﬂows, f1 , f2 and f3 , and to be split by the switch to the East, North and South outport, respectively. As can be observed, the control sharing is for control ﬂow sharing while the other two are for data ﬂow sharing. In the next section, we ﬁrst build network calculus analysis models for the three sharing components. Based on the three analytical models, we build router service analysis models in an example considering the combined effect of the three sharing components. Throughout the paper, we assume that ﬂow fi can be characterized as an arrival curve αi and router Rj provides a service i as fi ’s output arrival curve, αRj curve βRj . We also denote α as arrival curve of fi at router Rj and ˆβRj as the equivalent service curve guaranteed by router Rj to fi . While building network calculus analysis models, we follow the notation conventions in the min-plus algebra [5]. ⊗ represents the min-plus convolution of two functions f , g ∈ F , the set of wide-sense increasing functions deﬁned on [0, ∞), (f ⊗ g )(t) = inf s) + g (s)}; (cid:5) represents the min-plus deconvolution of two functions f , g ∈ F , (f (cid:5) g )(t) = sup for f ∈ F represents the sub-additive closure of f , which is ; ∧ represents the minimum operation, f ∧ g = min(f , g ); a+ = a, if a ≥ 0; a+ = 0, otherwise. Following [5], we deﬁne the afﬁne arrival curve γb,r (t) = rt + b for t > 0 and 0 otherwise, where r is the trafﬁc rate and b burstiness. The burst delay function δT (t) = +∞ for t > T , and 0 otherwise. f = δ0 ∧ f ∧ (f ⊗ f ) ∧ (f ⊗ f ⊗ f ) ∧ ... = inf {f (t + s) − g (s)}; f , f (n) n≥0 (cid:2) (cid:3) {f (t− 0≤s≤t s≥0 ∗ i i ing. Afterwards, we investigate router service analysis models and derive equivalent service curves for individual ﬂows considering the three sharing components together. 4.1. Analysis of Credit-based Flow Control Due to the existence of cyclic dependency between ﬂit delivery and credit generation, we can not directly apply network calculus analysis techniques because they are generally applicable to forward networks (networks without feedback control). To resolve the problem, we virtualize the functionality of ﬂow control as a network element, ﬂow controller, which provides service to trafﬁc ﬂows. This enables us to derive its service curve and transform the closed-loop network into an open-loop one. Buffer Network element (a) (b) Figure 4. The ﬂow control analytical model for ﬂow f traversing adjacent routers. We consider a trafﬁc ﬂow f passing through adjacent routers and construct an analytical model with the network elements depicted in Figure 4(a). In the model, we abstract the function of credit-based router-to-router ﬂow control as a network server denoted by τ1 , the switch in router 1 as σ1 , and router 2 as R2 . When f traverses the two routers, it can be regarded as being served by the concatenation of three servers, σ1 , τ1 and R2 . As shown in Figure 4(a), there are three cumulative processes representing the total trafﬁc volume from time 0 up to t, namely, arrival process A(t), admitted process I (t) and departure process D(t). The arrival process A(t) produced by switch 1 is fed to the ﬂow controller 1; The departure process I (t) is fed to router 2; The output D(t) is the departure process to the downstream router. Since the credits are sent to router 1 and have the identical volume as output ﬂits from buffer 2 (one ﬂit indicates one credit), we feedback the departure process D(t) to the ﬂow controller 1 directly, serving as credits governing the ﬂow controller. Next we give a theorem to derive the service curve for the ﬂow controller 1 and router 1. Theorem 4.1. Consider a trafﬁc ﬂow passing through two adjacent routers. Assume the switch 1 of router 1 guarantees service curve β σ1 and router 2 guarantees the service curve βR2 , the size of input buffer i (i = 1, 2) along the traversing path equals to Bi . Let β τ1 be the service curve of ﬂow controller 1 and βR1 be the service curve of router 1. Then and β τ1 = βR2 + B2 , βR1 = β σ1 ⊗ βR2 + B2 . (4.1) (4.2) 4. Analysis of Resource Sharing We build analysis models for the resource sharing components, starting from control sharing, link sharing to buffer sharProof. The credit counter in router 1 tracks the number of free ﬂit buffers in the input buffer of router 2. This counter is initialized with the input buffer size B2 . Till time t, the total number of credits received by ﬂow controller 1 is D(t) + B2 . It’s obvious that the admitted input I (t) is less than or equal to D(t) + B2 . In addition, I (t) is less than or equal to A(t). Thus I (t) can be deﬁned by the following equation I (t) = min{A(t), D(t) + B2} = A(t) ∧ (cid:4) D(t) + B2 (cid:5) . By Deﬁnition 1.3.1 of service curve in [5], we have D ≥ I ⊗ βR2 = [A ∧ (D + B2 )] ⊗ βR2 = (A ⊗ βR2 ) ∧ (D ⊗ (βR2 + B2 )). (4.3) (4.4) By Theorem 2.1.6 (iii) in [3]: if f (0) > 0 then g ≥ h ∧ (g ⊗ f ) implies g ≥ h ⊗ ¯f for f , g , h ∈ F , and with f (0) = (βR2 + B2 )(0) > 0, we get D ≥ (A ⊗ βR2 ) ⊗ βR2 + B2 . (4.5) the combination of a switch σ plus a ﬂow controller τ depicted in Figure 5(a) and guarantees the service curve βR = β σ ⊗ β τ . Since the router performs the weighted round-robin scheduling, the ﬂows are served according to their conﬁgured weight, φi for ﬂow fi . We assume that the weight assignment ensures that packets are delivered entirely in each round such that ﬂits of different packets do not interleave in transmission. In each round, for a non-empty buffer encountered, the router will try to serve up to φi ﬂits before moving to the next buffer. Assume the service rate of the router is C ﬂits/cycle per link, the maximum length of a round is consequently equal to cycles and the time for ﬂits of ﬂow fi to be forwarded within a round is bounded by φi C cycles. The service offered to one ﬂow completely depends on the weight of the ﬂow. In weighted round-robin scheduling, the worst-case appears when a ﬂow just misses its slot in the current round. Consequently it will have to wait for its slot at the next round. In the worst case, ﬂits will have to wait up to cycles to be served. Hence the equivalent service curve for f1 can be derived as j (cid:2)=i φj (cid:6) (cid:6) i φi C C Bringing Eq. (4.5) into Eq. (4.3), we have I ≥ A ∧ (cid:4) = A ∧ (cid:4) (A ⊗ βR2 ) ⊗ βR2 + B2 + B2 A ⊗ (βR2 + B2 ) ⊗ βR2 + B2 = A ⊗ βR2 + B2 . (cid:5) (cid:5) ˆβR 1 = φ1 φ1 + φ2 βR ⊗ δ φ2 C = φ1 φ1 + φ2 β σ ⊗ β τ ⊗ δ φ2 C . (4.9) (4.6) Analogously, the equivalent service curve for f2 equals to Hence, by Deﬁnition of service curve [5], the service curve of ﬂow controller 1 equals to β τ1 = βR2 + B2 . (4.7) Consequently by Theorem 1.4.6 (Concatenation of Nodes) in [5], we have ˆβR 2 = φ2 φ1 + φ2 βR ⊗ δ φ1 C = φ2 φ1 + φ2 β σ ⊗ β τ ⊗ δ φ1 C . (4.10) The equivalent service curves both ﬂows fi (i = 1, 2) receive are illustrated in Figure 5(b). 4.3. Analysis of Buffer Sharing βR1 = β σ1 ⊗ β τ1 = β σ1 ⊗ βR2 + B2 . (4.8) FIFO After obtaining the service curves of ﬂow controller 1 and router 1, we can transform the closed-loop model to the forward one depicted in Figure 4(b), where the cyclic dependency caused by the feedback control is resolved (“eliminated”). Next, we consider link sharing and buffer sharing under ﬂow control. Our aim is to derive the equivalent service curve each ﬂow receives from the router. 4.2. Analysis of Link Sharing Flits Flits (a) (b) Figure 5. (a) Two ﬂows f1 and f2 share one output link; (b) The equivalent service curve ˆβR for fi (i = 1, 2) guaranteed by the router. i Without losing generality we consider two ﬂows f1 and f2 share one output link. The router they traverse is abstracted as (a) (b) Figure 6. (a) An aggregate ﬂow f{1,2} shares the input buffer and is served by the router in the FIFO order; (b) The equivalent service curve ˆβR for ﬂow fi guaranteed by the router. i As drawn in Figure 6(a), an aggregate ﬂow f{1,2} sharing the same input buffer is to be split to different outports. Flits of f{1,2} are stored in the input buffer and served by the switch of the router in the FIFO order. Since f1 and f2 aim for different destinations, their ﬂit delivery is under the ﬂow control over their corresponding downstream buffer. Speciﬁcally, for ﬂow fi (i = 1, 2), there exists a ﬂow controller τi with the service curve β τi , and τ1 and τ2 can be regarded as parallel trafﬁc regulators. According to Theorem 2.3.5 (Filter bank summation) in [3], we get the service curve of the router for f{1,2} as βR = β σ ⊗ (β τ1 ∧ β τ2 ). (4.11) The equivalent service curve for an individual ﬂow fi depends also on the arrival curve of its contention ﬂows at the ingress of the buffer. For f1 , the equivalent service curve can be ∗ 1 = α1 (cid:5) (β , α2 ). In 2 = α2 (cid:5) (β , α1 ). derived as ˆβR 1 = (βR , α2 ), where (., .) is a function to compute the equivalent service curve. Thus, according to [5], we obtain the output arrival curve of f1 as α the same way, we can obtain f2 ’s equivalent service curve ˆβR (βR , α1 ) and output arrival curve as α Hence, we can view that both ﬂows are served with their equivalent service curve, as illustrated in Figure 6(b). We give an example to compute (., .). If the router provides a latency-rate service curve [11], i.e., βR (t) = γ0,C ⊗ δT (t) = C (t − T )+ , and f2 is constrained by an afﬁne arrival curve, i.e., α2 (t) = γb2 ,r2 (t) = r2 t + b2 , then applying Corollary 4.5 in [6], the equivalent service curve for f1 can be computed as: 2 = ∗ East Port South Port Credit Credit East Port South Port (a) The initial analysis model (b) Eliminating flow control East Port South Port East Port South Port ˆβR 1 = (βR , α2 ) = γC ·s,C−r2 ⊗ δT + b2 C +s , (s ≥ 0), (4.12) (c) Eliminating link sharing (d) Creating buffer-sharing analysis model Figure 8. Router service analysis models. where s is an intermediate argument for computing the least upper delay bound [6]. 4.4. Analysis of Combined Resource Sharings Combining the three resource-sharing models for ﬂow control, link sharing and buffer sharing, we can create precise router service analysis models and obtain equivalent service curves for individual ﬂows at a router. We give an example as follows. North Port West Port East Port Credit Switch Local Port Flit Flit South Port Router C r e d i t Figure 7. Flows share resources. As shown in Figure 7, two ﬂows pass a router R. One is f1 and the other an aggregate ﬂow f{2,3} . f1 and f2 aim for the East outport and f3 for the South outport. As an aggregate ﬂow f{2,3} , f2 and f3 experience buffer sharing. f2 is split from f{2,3} and merged with f1 . Hence f2 and f1 experience link sharing. Both output ﬂows f{1,2} and f3 are under ﬂow control by the feedback credits. The initial router service analysis model is depicted in Figure 8(a). According to the credit-based ﬂow control model, we get the service curves of ﬂow controllers, τ1 and τ2 , as β τ1 = βR1 + B1 ; β τ2 = βR2 + B2 . (4.13) Thus we can “eliminate” the ﬂow control and transform the initial analysis model to an open-loop model depicted in Figure 8(b), where the router provides the service curves to the output aggregate ﬂow f{1,2} and ﬂow f3 as βR{1,2} = β σ ⊗ β τ1 ; 3 = β σ ⊗ β τ2 . βR (4.14) f1 and f2 aim to the same output link. Following the link sharing model, we “eliminate” the link sharing and further transform the open-loop analysis model to the model shown in Figure 8(c). The equivalent service curves for f1 and f2 are ˆβR 1 = φ1 φ1 + φ2 ˜βR 2 = φ2 φ1 + φ2 βR{1,2} ⊗ δ φ2 βR{1,2} ⊗ δ φ1 C C ; . (4.15) Note that ˜βR 2 is not the ﬁnal equivalent service curve for f2 since it is affected by f3 under buffer sharing. 3 = β σ ⊗ β τ2 without conf3 receives the service curve βR sidering the blocking caused by f2 due to buffer sharing. Based on the buffer sharing model, we get the service curve provided by the router to the aggregate ﬂow f{2,3} as βR{2,3} = ˜βR 2 ∧ βR = φ2 β σ ⊗ (β τ1 ∧ β τ2 ) ⊗ δ φ1 φ1 + φ2 3 C (4.16) . Therefore we obtain a ﬁnal simpliﬁed analysis model as illustrated in Figure 8(d). Since only buffer sharing remains to be resolved (“visible”) in this model, we call it buffer-sharing analysis model. Based on this model, we can compute the equivalent service curve for f2 and f3 as ˆβR 2 = (βR{2,3} , α3 ); ˆβR 3 = (βR{2,3} , α2 ). (4.17) Finally, we list the equivalent service curve for each individual ﬂow as follows: (cid:7) (cid:7) ˆβR 1 = φ1 φ1 + φ2 ˆβR 2 =  φ2 φ1 + φ2 φ2 φ1 + φ2 ˆβR 3 =  C ; β σ ⊗ βR1 + B1 ⊗ δ φ2 β σ ⊗ (βR1 + B1 ∧ βR2 + B2 ) ⊗ δ φ1 β σ ⊗ (βR1 + B1 ∧ βR2 + B2 ) ⊗ δ φ1 C C (cid:8) (cid:8) , α3 , α2 (4.18) ; . 5. The Delay Bound Analysis Technique We present the delay bound analysis technique. We ﬁrst describe the buffer-sharing analysis model. Then we identify three basic interference patterns for a tagged ﬂow (see deﬁnition in Section 5.2). Finally we detail the delay-bound analysis steps. 5.1. The Buffer-Sharing Analysis Model We have analyzed the three sharing components. For the ﬂow control, we have introduced a ﬂow controller as a network element to model its function. This enables us to remove the credit feedback loop. Consequently, we can transform a network analysis model with feedback into one without feedback, and then apply network calculus theory for its analysis. For the link sharing, we have assumed the round-robin scheduling, which offers good isolation between ﬂows providing a guaranteed minimum service curve to each ﬂow. For the buffer sharing, the equivalent service curve for each individual ﬂow depends also on the arrival curve of its contention ﬂows, and cannot be separated in general, since ﬂits from different ﬂows are stored in the same buffer and served in order. Of course, a router serves ﬂows performing the three sharings concurrently. Combining the three models, we can obtain a simpliﬁed analysis model, which “eliminates” the feedback and link contention and keeps only the buffer sharing. This model is called buffer-sharing analysis model/network. This simpliﬁcation procedure can be viewed as a transformation procedure. In Section 4.4, we have given an example in Figure 7 to show how to derive a buffer-sharing analysis model (Figure 8(d)) from its initial analysis model (Figure 8(a)). The transformation steps can be generalized as four steps: (1) Build an initial analysis model taking into account of ﬂow control, link sharing and buffer sharing; (2) Based on the model in step (1), “eliminate” (resolve) ﬂow control; (3) Based on the model in step (2), “eliminate” link sharing; (4) Based on the model in step (3), derive a buffer-sharing analysis model. After analyzing per-router resource sharing, we can view the network as a buffer-sharing analysis network. In this buffersharing analysis network, we continue to investigate multipleﬂow contention scenarios along ﬂows’ routes. 5.2. Interference Patterns and Analytical Models In a buffer-sharing analysis network, ﬂow contention scenarios are diverse and complicated. We call the ﬂow for which we shall derive its delay bound tagged ﬂow, other ﬂows sharing resources with it contention or interfering ﬂows. A tagged ﬂow directly contends with interfering ﬂows. Also, interfering ﬂows may contend with each other and then contend with the tagged ﬂow again. To decompose a complex contention scenario, we identify three basic contention or interference patterns [10]. from source to destination, and is multiplexed with contention ﬂows. The contention scenarios the tagged ﬂow may experience can be classiﬁed into three basic patterns, namely, Nested, Parallel and Crossed, as shown in Figure 9. In the following, we analyze the three scenarios and derive their analytical models with focus on the derivation of the equivalent service curve the tandem provides to the tagged ﬂow. Scenario I: Nested contention ﬂows. As illustrated in Figure 9(a), contention ﬂow f3 is nested in f2 . The subtandem {R3 , R4} serves the aggregate ﬂow f{1,2,3} with service curve βRi . The equivalent service curve for f{1,2} 4(cid:9) i=3 R{3,4} {1,2} = ( is ˆβ βi , α3 ). The service curve of sub-tandem {R2 → R5} for the aggregate ﬂow f{1,2} is computed as {1,2} ⊗ βR5 . Thus we can derive the equivalent service curve ˆβ for f1 as (βR2 ⊗ ˆβ {1,2} ⊗ βR5 , α2 ). Therefore the tandem {R1 → R6} serves f1 with the service curve βR2 ⊗ ˆβ R{2→5} R{3,4} R{3,4} 1 R{1→6} ˆβ 1 = βR1 ⊗ ˆβ 1 R{2→5} ⊗ βR6 . i=3 4(cid:9) ( i=2 = R{1→6} 1 5(cid:9) i=4 3(cid:9) = ( ⊗ ˆβ R{2,3} 1 R{4,5} 1 R{4,5} 1 R{2,3} 1 ⊗ βR6 . = βR1 ⊗ ˆβ βi , α2 ) and ˆβ Scenario II: Parallel contention ﬂows. As shown in Figure 9(b), contention ﬂows f2 and f3 are independent. Similarly to Scenario I, we can get the equivalent service curve of sub-tandem {R2 , R3} and {R4 , R5} for f1 as ˆβ βi , α3 ), respectively. Hence the service curve of tandem {R1 → R6} for f1 can be calculated by ˆβ Scenario III: Crossed contention ﬂows. As shown in Figure 9(c), contention ﬂow f2 is crossed with f3 . There are two cross points, one between R2 and R3 and the other between R4 and R5 . If we cut f2 arbitrarily at the ﬁrst cross point, i.e., at the ingress of R3 , f2 will be split into two ﬂows, f2a and f2b , as shown in Figure 10. Then the problem is strictly transformed to the combination of scenario I and II such that f2a is separate and f2b is nested in f3 . Apparently the arrival curve α2a of f2a equals to α2 and the arrival curve α2b of f2b equals to α 2a . To compute α 2a , we need to get the arrival curve of f1 at router R2 as α1 (cid:5) βR1 . Then the equivalent service which equals to αR2 curve of R2 for f2a can be derived as ˆβR2 1 ). Thus we obtain the output arrival curve of f2a as α With α2a , α2b and α3 obtained, we can calculate the tandem service curve for f1 as ˆβ = (βR2 , αR2 = α2 (cid:5) ˆβR2 ⊗ βR6 . = βR1 ⊗ ˆβR2 1 ⊗ ˆβ R{3→5} R{1→6} 2a . ∗ 2a 2a 1 1 ∗ ∗ 1 (a) Nested (b) Parallel (c) Crossed Figure 9. The three basic contention patterns for a tagged ﬂow. Suppose that a tagged ﬂow f1 traverses a tandem of 6 routers Figure 10. Transform the crossed ﬂows to noncrossed ﬂows. In the above, we have derived three basic analytical models for analyzing the delay bound for a tagged ﬂow multiplexed with two contention ﬂows in three different scenarios. For complex scenarios with more than two contention ﬂows, the problem can be decomposed using the basic models and expressed with a contention tree model [8] (See example in Section 6). Then we can derive the tagged ﬂow’s equivalent service curve provided by a tandem of nodes. With the ﬂow’s arrival curve and equivalent service curve known, we can derive its delay bound. 5.3. The General Analysis Procedure We detail a general delay-bound analysis procedure as follows: Step 1: Construct a buffer-sharing analysis network that resolves the feedback control and link sharing contentions using the transformation steps in Section 5.1; Step 2: Given a tagged ﬂow, construct its contention tree to model the buffer sharing contentions produced by interfering ﬂows in the buffer-sharing analysis network; Step 2.1: Let the tandem traversed by the tagged ﬂow be the trunk; Step 2.2: Have the tandems traversed by the interfering ﬂows before reaching a trunk node as branches; A branch may also have its own sub-branches; Step 3: Scan the contention tree and compute all the output arrival curves of ﬂows traversing the branches using the basic interference analytical models iteratively; Step 4: Compute the equivalent service curve for the tagged ﬂow and derive its delay bound. We have designed three algorithms, one for constructing the buffer-sharing analysis network, the second for building contention tree and the third for computing the equivalent service curves provided by branches and trunk. Due to space limitation, we do not present the algorithms. Instead, we present a case study to exemplify the analysis procedure in Section 6. 6. A Delay-Bound Analysis Example We exemplify the general delay-bound analysis procedure step-by-step. We also derive closed-form formulas for calculating delay bounds under the assumptions of afﬁne arrival curve and latency-rate service curve. 6.1. An Example Input Data NI DSP1 NI Input Data NI Output Data NI MIPS2 NI R2 R3 Measure Point 2a R4 DSP2 NI MIPS4 NI R6 R7 R8 RAM5 NI TTY NI MIPS1 NI Measure Point 1a R1 MIPS3 NI Measure Point 3a R5 RAM4 NI RAM2 NI R9 R10 R11 R12 Output Data NI RAM1 NI Measure Point 1b R15 RAM3 NI Measure Point 3b R16 Measure Point 2b R13 R14 delay bound for f1 . Thus f1 is the tagged ﬂow and f2 and f3 are contention ﬂows. In the following, we detail the analysis steps. Step 1: Build a buffer-sharing analysis network. The initial closed-loop analysis network is shown in Figure 12(a). This network can be simpliﬁed into a forward buffersharing analysis network, as depicted in Figure 12(b). We also list the service curve formulas for all network elements in Figure 12(b) in Table 1. For brevity, we omit the transformation and service curve derivation procedures. Step 2: Construct a contention tree. Figure 13. Contention tree for tagged ﬂow f1 . Figure 12(b) also shows how ﬂows pass routers, and how dem of routers {R1 , R2 , R3 , R7 , R11 , R15 }, loosely denoted they contend for shared buffers. Flow f1 traverses a tanas {R1 → R15 }; f2 traverses {R4 , R3 , R7 , R6 , R10 , R9 , R13 } ({R4 → R13}); f3 traverses {R5 , R6 , R10 , R11 , R15} ({R5 → R15}). At router R7 , f1 and f2 share buffer B7 ; At router R15 , f1 shares buffer B15 with f3 . At router R10 , two contention ﬂows f2 and f3 share buffer B10 . Using this contention relation, we can build a contention tree for f1 , as drawn in Figure 13. In a contention tree, we call the ﬂow over the trunk trunk ﬂow, which is the tagged ﬂow, and ﬂows over branches branch ﬂows. Step 3: Compute output arrival curves of branch ﬂows. To derive the equivalent service curve for trunk ﬂow f1 , we scan the contention tree in Figure 13 using the Depth-FirstSearch to ﬁrst compute the output arrival curve of f1 traversing sub-sub-branch {R1 → R3 } for sub-branch {R4 → R6} as 1 = α1 (cid:5) (βR1 ⊗ βR2 ⊗ ˆβR3 1 ). αR7 (6.1) Then, using the nested interference model, the output arrival curve of f2 traversing sub-branch {R4 → R6} for branch {R5 → R11} can be computed as βR4 ⊗ ˆβR3 2 ⊗ (βR7 , αR7 1 ) ⊗ ˆβR6 2 . (6.2) 2 = α2 (cid:5) (cid:4) αR10 Figure 11. A 4 × 4 mesh NoC. Figure 11 shows a network with 16 nodes. There are 3 ﬂows, f1 , f2 and f3 . f1 is from MIPS1 to RAM1, f2 from MIPS2 to RAM2 and f3 from MIPS3 to RAM3. We shall derive the Further, using the nested model again, we can derive the output arrival curve of f3 traversing branch {R5 → R11} for trunk {R1 → R15} as βR5 ⊗ ˆβR6 3 ⊗ (βR10 , αR10 2 ) ⊗ ˆβR11 3 . (6.3) 3 = α3 (cid:5) (cid:4) αR15 (cid:5) (cid:5) (a) (b) Figure 12. (a) An initial closed-loop analysis network; (b) A buffer-sharing analysis network. βR16 = βσ16 Service Curves βR15 = βσ15 ⊗ βR16 + B16 ˆβ φ1+φ3 βσ11 ⊗ βR15 + B15 ⊗ δ φ1 3 = φ3 R11 C11 ˆβ 1 = φ1 R11 φ1+φ3 β σ11 ⊗ βR15 + B15 ⊗ δ φ3 C11 βR13 = βσ13 βR10 = βσ10 ⊗ (βR9 + B9 ∧ ˆβ R11 3 + B11W ) ˆβ φ2+φ3 βσ6 ⊗ βR10 + B10 ⊗ δ φ2 3 = φ3 R6 C6 βR5 = βσ5 ⊗ ˆβ 3 + B6 R6 ˆβ φ1 +φ2 βσ3 ⊗ βR7 + B7 ⊗ δ φ1 2 = φ2 R3 C3 βR1 = βσ1 ⊗ βR2 + B2 βR7 = βσ7 ⊗ ( ˆβ 2 + B6E ∧ ˆβ R6 1 + B11E ) R11 βR2 = βσ2 ⊗ ˆβ R3 1 + B3W βR4 = βσ4 ⊗ ˆβ R3 2 + B3E ˆβ βR9 = βσ9 ⊗ βR13 + B13 φ2+φ3 βσ6 ⊗ βR10 + B10 ⊗ δ φ3 2 = φ2 R6 C6 1 = φ1 R3 φ1+φ2 βσ3 ⊗ βR7 + B7 ⊗ δ φ2 C3 ˆβ Table 1. Service curve formulas of the network elements in the buffer-sharing analysis network. {R4 → R3} for trunk {R1 → R15} is derived as Analogously, the output arrival curve of f2 traversing branch 2 = α2 (cid:5) (βR4 ⊗ ˆβR3 2 ). αR7 (6.4) Step 4: Compute the delay bound. After all arrival curves of injected ﬂows to the trunk are obtained, we can compute the trunk service curve for f1 as R{1→15} 1 β =βR1 ⊗ βR2 ⊗ ˆβR3 1 ⊗ (βR7 , αR7 ⊗ ˆβR11 1 ⊗ (βR15 , αR15 3 ), 2 ) and thus the delay bound for f1 can be derived as ¯D1 = H (α1 , β R{1→15} 1 ), (6.6) where H (., .) is the function to compute the maximum horizontal distance between the arrival curve and the service curve. 6.2. Closed-Form Formulas Assuming the afﬁne arrival curve for ﬂows and latency-rate service curve for routers, we can obtain closed-form formulas for the delay bound calculation. Suppose the arrival curve of fi (i = 1, 2, 3) is αi = γbi ,ri , the switch service curve β σi = γ0,C ⊗ δT (i = 1, 2, ..., 16); the buffer size of each router equals to B , and each ﬂow has an equal weight for link sharing, i.e., φ1 = φ2 = φ3 = φ. Due to space limitation, we give the formulas for the two cases, omitting the derivation details. Case 1: When φ ≤ C T ≤ B : The equivalent service curve β provided by the tandem {R1 → R15 } to ﬂow f1 can be calculated as follows: R{1→15} 1 (6.5) R{1→15} 1 β = βR1 ⊗ βR2 ⊗ ˆβR3 1 ⊗ ˆβR7 1 ⊗ ˆβR11 1 ⊗ ˆβR15 1 (cid:11) (cid:7) (cid:10) ⊗ ⎡ = = 8 C ⊗ (cid:7) ⊗ δ4T +5 φ γ0, C γC ·s2 ,C−r3 ⊗ δ 4 ·s1 , C 4 −r2 γC ·s2 ,C−r3 ∧ γ0, C ⎤ ∧ ⎦ ⊗ δ 8 ⎣γ C T + (cid:8) ⊗ δ T +4 b7 2 C +s1 γ C 4 ·s1 , C 4 −r2 (cid:8) b15 3 C +s2 6T +5 φ C +4 b7 2 C + b15 3 C +s1+s2 , (6.7) where s1 and s2 are intermediate arguments for computing the least upper delay bound [6]. Then we can compute the least upper delay bound ¯D1 for ﬂow f1 : ¯D1 = h(α1 , β R{1→15} 1 ) 6T + 5 φ = inf s1≥0,s2≥0 (cid:16) C + 4 b7 2 C + b15 C 3 + s1 + s2 + (cid:17) b1− C 4 ·s1 4 −r2 C ∨ b1−C ·s2 C−r3 ∨ b1 C 8 (cid:18) (cid:19) . (6.8) We know that R1 , R2 and R3 serve f1 with a minimum rate of 8 ; R3 and R4 serve f2 with a minimum rate of C serve f3 with a minimum rate of C not be greater than the minimum service rate, we have r1 ≤ C 4 . Since the trafﬁc rate must 8 , 8 and r3 ≤ C 4 . Thus Eq. (6.8) can be deduced further. When s1 = s2 = 0, C 8 ; R5 and R6 r2 ≤ C ¯D1 = 6T + 5 φ C + 4 b7 2 C + b15 C 3 + 8 b1 C . (6.9) Case 2: Analogously, we can compute the delay bound ¯D1 for ﬂow f1 when φ ≤ B < C T : ¯D1 = h(α1 , β R{1→15} 1 ) 6T + 5 φT = inf s1≥0,s2≥0 (cid:16) B + 4 b7 2T B + b15 3 T B + s1 + s2 + (cid:17) b1− B 4T ·s1 4T −r2 B ∨ b1− B T ·s2 T −r3 B ∨ b1 B 8T (cid:18) (cid:19) . (6.10) In this case, R1 , R2 and R3 serve f1 with a minimum rate of 8T ; R3 and R4 serve f2 with a minimum rate of B serve f3 with a minimum rate of B 4T . Since the trafﬁc rate must not be greater than the service rate, we have r1 ≤ B and r3 ≤ B 4T . When s1 = s2 = 0, Eq. (6.10) can be deduced as B 8T ; R5 and R6 8T , r2 ≤ B 8T ¯D1 = 6T + 5 φT B + 4 b7 2T B + b15 3 T B + 8 b1T B . (6.11) 7. Experimental Results We designed experiments to verify our analytical approach, and show the correctness and tightness of calculated bounds via comparing them with simulated results. We apply our analysis and simulation for both real on-chip traces and synthetic trafﬁc. 7.1. Simulation Setup We use a simulation platform in an open source simulation environment SoCLib [1] to collect application traces and to simulate their delays in on-chip networks. As shown in Figure 11, the platform contains four MIPS R3000 processors, onchip memories, a display component (TTY), and other components such as DSPs and input/output interfaces. These components are interconnected with each other via a 4 × 4 mesh network. The network performs wormhole switching with creditbased ﬂow control, and uses deterministic routing. Application code and data are stored in RAMs. The Network Interfaces (NIs) encapsulate transactions into ﬂits and de-encapsulate ﬂits into transactions. We run three embedded multimedia programs simultaneously on the platform, speciﬁcally, an MP3 audio decoder on MIPS1, a JPEG decoder on MIPS2 and an MPEG2 video decoder on MIPS3, generating three ﬂows, f1 , f2 and f3 , respectively. The MP3 decoder processes a 4KB audio stream, JPEG decoder a 256 × 256 image, MPEG2 decoder a 176 × 176 video frame. We set up three pairs of measurement points (1a, 1b), (2a, 2b) and (3a, 3b) to observe the transactions between MIPS1 and RAM1 (f1 ), between MIPS2 and RAM2 (f2 ), and between MIPS3 and RAM3 (f3 ) in the platform, respectively, as indicated in Figure 11. While the application code running on a processor, for example, MP3 on MIPS1, at Point 1a we record the sequence number and timing of ﬂits generated by MIPS1 in a trace ﬁle, and at Point 1b we observe the end-toend delay experienced by each ﬂit after traversing six routers, {R1 , R2 , R3 , R7 , R11 , R15 }. We analyzed all the three application traces and derived their afﬁne arrival curves as follows: MP3 ﬂow f1 : α1 (t) = γ0.2,0.075 (t) = 0.075t + 0.2, JPEG ﬂow f2 : α2 (t) = γ0.1,0.05 (t) = 0.05t + 0.1, MPEG2 ﬂow f3 : α3 (t) = γ0.3,0.12 (t) = 0.12t + 0.3. For example, α3 of MPEG2 trafﬁc means that MIPS3 generates 0.12 ﬂit per cycle on average with burstiness of 0.3 ﬂit. Routers are uniform with a per-link service rate C of 1 ﬂit/cycle, delaying 5 cycles to process head ﬂits (T = 5) and switching ﬂits in one cycle. Thus the switch service curve of routers can be abstracted as: (7.1) β σi (t) = γ0,1 ⊗ δ5 (t) = 1 · (t − 5)+ , (1 ≤ i ≤ 16). The routers use a fair weight for each ﬂow, i.e., φi = 1 ﬂit (i = 1, 2, 3) for the round-robin link scheduling. The buffer size varies from 3 to 6 ﬂits. We also synthesize three trafﬁc ﬂows according to the afﬁne arrival curves in Eq. (7.1) and run them in the same experimental platform. We shall compare the simulated results of the real traces and their corresponding synthetic trafﬁc ﬂows. 7.2. Analysis and Simulation Results We consider f1 , f2 and f3 as the tagged ﬂow each time and derive their delay bound using the proposed analytical approach. The delay bound for f1 can be computed according to Eq. (6.9) or Eq. (6.11) directly. 0 0 100 200 300 400 500 600 700 800 900 1000 5 10 15 20 25 30 35 40 45 43.58 cycles 39.25 cycles Flit (1 to 1E+3) D y a e l , n u t i e c y c = l (a) A Record Segment of Delay for MPEG2 Flow 0 0 100 200 300 400 500 600 700 800 900 1000 5 10 15 20 25 30 35 40 45 43.58 cycles 41.38 cycles Flit (1 to 1E+3) D y a e l , n u t i e c y c = l (b) A Record Segment of Delay for Synthetic Flow 3 Figure 14. Record segments of delays for MPEG2 and its corresponding synthetic ﬂow. We report both analysis and simulation results on delay under different ﬂow control buffer sizes for all the applications in Table 2. We denote calculated delay bound as ¯D and maximum simulated delay for real and synthetic trafﬁc as Dmax -r and Dmax -s , respectively. For all results, the unit for delay is cycle. Using the MPEG2 as an example, we also plot a sequence of 1E+3 observed delays containing the maximum delay for the real ﬂow and its corresponding synthetic ﬂow in Figure 14 when B = 4 ﬂits. In this ﬁgure, a circle indicates a ﬂit delay and the straight line represents the calculated delay bound.             Application Delay Flow Control Buffer Size 3 4 5 6 MP3 Syn. Flow 1 Dmax -r 1 Dmax -s 1 ¯¯D1 JPEG Syn. Flow 2 Dmax -r 2 Dmax -s 2 ¯D2 ¯D3 MPEG2 Syn. Flow 3 Dmax -r 3 Dmax -s 3 54.19 47.15 43.27 43.27 49.35 41.96 37.13 37.13 51.04 43.82 38.95 38.95 66.17 57.50 52.58 52.58 60.89 52.11 46.42 46.42 63.45 53.78 48.31 48.31 49.64 43.58 40.34 40.34 45.37 39.25 34.80 34.80 47.52 41.38 36.27 36.27 Table 2. Calculated delay bounds and simulated maximum delays for MP3, JPEG and MPEG2, and their corresponding synthetic ﬂows under different buffer sizes. We can observe from Table 2 with support from Figure 14: (1) In all cases, ¯D > Dmax -s > Dmax -r . This has two meanings. One is that the calculated delay bounds envelop the maximum simulated delays. The other is that a simulated maximum delay for synthetic trafﬁc is greater than the delay for its corresponding real trafﬁc. This is due to the fact that the arrival curves of real multimedia trafﬁc are derived from the trace ﬁles and the synthetic trafﬁc ﬂows are generated according to the arrival curves. Thus the synthetic trafﬁc is closer to the arrival curve than the real trafﬁc. As a consequence, their delays are closer to calculated delay bounds. (2) The calculated delay bounds are fairly tight. To quantify how tight a delay bound ¯D is, we deﬁne delay-bound tightness as λ = Dmax / ¯D , where Dmax is the observed maximum delay in simulations. With the real trafﬁc, λr = Dmax -r / ¯D ∈ [0.86,0.92]. On average, λr,avg = 88.7%. For the synthetic trafﬁc, λs = Dmax -s / ¯D ∈ [0.9,0.96]. On average, λr,avg = 92.6%. (3) As the ﬂow control buffer size increases, the delay bounds and corresponding maximum observed delays decrease until an optimal buffer size is reached. “Optimal” refers to the minimal buffer size that achieves the minimum delay. This is as we expected, because increasing the buffer size reduces the effect of ﬂow control, and other factors such as link and buffer contentions are becoming more inﬂuential in performance. Continuing this trend, the buffer size will eventually become logically inﬁnite, thus ﬂow control has no effect for such ideal buffers. As we can see from the table, the values of ¯D , Dmax -s and Dmax -r have no change when B equals to 5 and 6. Thus “B=5” is optimal in this example. 8. Conclusions Application exerts stringent requirements on on-chip networks to ensure performance bounds even under worst cases. In this work, we present a network-calculus based analysis method to compute the worst-case delay bounds for individual ﬂows in best-effort wormhole networks with the credit-based ﬂow control. We ﬁrst build router service analysis models for ﬂow control, link sharing and buffer sharing. Then we investigate and analyze ﬂow interference patterns. Based on these analysis models, we are able to compute per-ﬂow equivalent service curve provided by the tandem of routers each ﬂow passes. Using a ﬂow’s equivalent service curve and arrival curve, we can compute the ﬂow’s delay bound. Under the assumptions of afﬁne arrival and latency-rate service curves, we can give closed-form formulas. Our simulation results with both real on-chip multimedia traces and synthetic trafﬁc validate the correctness and tightness of analysis results. We conclude that our technique can be used to efﬁciently compute per-ﬂow worstcase delay bound, which is extremely difﬁcult to cover even by exhaustive simulations. Our method is topology independent, and thus can be applied to various networks with a regular or irregular topology. We have considered wormhole networks where a router contains only one virtual channel per port. We shall extend our analysis to general wormhole networks where a router has multiple virtual channels per port. The analysis technique remains the same. However, we need to take into account the allocation of virtual channels in our analysis due to the existence of multiple virtual channels. We will also extend our framework to consider other link sharing algorithms. Furthermore, we will automate the analysis procedure. "
Static virtual channel allocation in oblivious routing.,"Most virtual channel routers have multiple virtual channels to mitigate the effects of head-of-line blocking. When there are more flows than virtual channels at a link, packets or flows must compete for channels, either in a dynamic way at each link or by static assignment computed before transmission starts. In this paper, we present methods that statically allocate channels to flows at each link when oblivious routing is used, and ensure deadlock freedom for arbitrary minimal routes when two or more virtual channels are available. We then experimentally explore the performance trade-offs of static and dynamic virtual channel allocation for various oblivious routing methods, including DOR, ROMM, Valiant and a novel bandwidth-sensitive oblivious routing scheme (BSORM). Through judicious separation of flows, static allocation schemes often exceed the performance of dynamic allocation schemes.","Static Virtual Channel Allocation in Oblivious Routing Keun Sup Shim Myong Hyon Cho Michel Kinsy Tina Wen Mieszko Lis G. Edward Suh∗ Srinivas Devadas Computer Science and Ar tiﬁcial Intelligence Laboratory Massachusetts Institute of Technology ABSTRACT Most virtual channel routers have multiple virtual channels to mitigate the effects of head-of-line blocking. When there are more ﬂows than virtual channels at a link, packets or ﬂows must compete for channels, either in a dynamic way at each link or by static assignment computed before transmission starts. In this paper, we present methods that statically allocate channels to ﬂows at each link when oblivious routing is used, and ensure deadlock freedom for arbitrary minimal routes when two or more virtual channels are available. We then experimentally explore the performance tradeoffs of static and dynamic virtual channel allocation for various oblivious routing methods, including DOR, ROMM, Valiant and a novel bandwidth-sensitive oblivious routing scheme (BSORM). Through judicious separation of ﬂows, static allocation schemes often exceed the performance of dynamic allocation schemes. 1. INTRODUCTION Routers may mitigate head-of-line blocking by organizing the buffer storage associated with each network channel into several small queues rather than a single deep queue [7]. Such “virtual” channels (VCs) increase hardware complexity but offer a mechanism to achieve Quality-of-Service (QoS) guarantees and performance isolation — important considerations for on-chip interconnection networks (OCINs) [14]). Most routers in OCINs have a small number of VCs, though network routers can have large numbers of queues and channels (e.g., Avici TSR [4]). While overhead considerations tend to limit routers used in multicore or multiprocessor systems to 16 or fewer VCs, applications may have hundreds if not thousands of ﬂows, which must compete for channels, buffer space, and bandwidth at each network link. Conventional virtual channel (VC) routers dynamically allocate VCs to packets or head/control ﬂits based on channel availability and/or packet/ﬂit waiting time. Typically, any ﬂit can compete for any VC at a link [6], and the associated arbitration is often the highest latency step [16]. Statically allocating VCs to ﬂows can simplify the VC allocation step. Judicious separation of ﬂows during static allocation may reduce or eliminate head-of-line blocking and so enhance throughput, but may result in worse utilization of available VCs because dynamic behavior is not considered. There has been little or no exploration of such performance tradeoffs outside of evaluating QoS guarantees; therefore, in this paper, we answer the question of whether static allocation outperforms dynamic allocation in the context of oblivious routing through performance simulation. Exploring this tradeoff requires methods that statically allocate ∗Cornell University ﬂows to VCs at each link and a router architecture that supports these methods. Section 2 describes modiﬁcations to a standard router architecture for table-based application-aware routing and static VC allocation; section 3 describes how algorithms such as Dimension Order Routing (DOR), ROMM [13] and Valiant [19] can assign VCs via a table-based routing architecture. The performance of these routing algorithms varies under different static allocations, and we show how this allocation can be judiciously determined. In Section 4, we describe a bandwidth-sensitive oblivious routing scheme, BSORM, which produces a set of minimal routes that attempt to minimize maximum channel load; VCs are statically allocated to optimize performance. We show how an analysis of the classical turn model [10] can be used to derive a static VC allocation scheme that assures deadlock freedom for an arbitrary set of minimal routes with ≥ 2 available VCs. Related work is summarized in Section 5. We compare static and dynamic VC allocation for DOR, ROMM, Valiant, and BSORM in Section 6, and Section 7 concludes the paper. 2. ROUTER ARCHITECTURE 2.1 Typical Virtual Channel Router We assume a typical virtual channel (VC) router on a 2-D mesh network as a baseline [6, 12, 16], but our methods can be used independent of network topology and ﬂow control mechanisms. Router operation takes four steps: routing (RC), virtual channel allocation (VA), switch allocation (SA), and switch traversal (ST), often done in one to four stages in modern routers. When a head ﬂit (the ﬁrst ﬂit of a packet) arrives at an input channel, the router stores the ﬂit in the channel’s buffer and determines the next hop for the packet (RC). The router then allocates a VC in the next hop (VA). Finally, if the next hop can accept the ﬂit, it competes for a switch (SA) and moves to the output port (ST). 2.2 Table-Based Routing The only architectural change required for static VC allocation and application-aware oblivious routing (see Section 4) is in the routing module. While the baseline architecture implements simple oblivious routing algorithms such as DOR via ﬁxed logic and dynamically allocates VCs to packets, our routing module needs table-based routing so that routes can be conﬁgured for each application. This single change is sufﬁcient as long as routing algorithms preclude cyclic channel dependence through route selection or VC allocation (cf. Section 4.3). As illustrated in Figure 1, table-based routing can be realized in two different ways: source routing and node-table routing. In the source routing approach, each node has a routing table with a route from itself to each destination node in the network. The routes 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    C  B  RC  N … A  packet  … N N E  … N N  RC  RC  C  out  index  0 N  3  1 E  5  2 W  1  … … …  0 packet  1  A  B  out  0 N  1 E  index  3  2  2 W  5  … … …  2  out  0 E  1 S  2 N  index  1  6  0  … … …  (a) Source routing  (b) Node-table routing  Figure 1: The table-based routing architecture (a) Source routing (b) Node-table routing are pre-computed by a routing algorithm and written into the tables before application execution. When sending a packet, the node prepends this routing information to the packet, and routers along the path determine output ports directly from the routing ﬂits. Figure 1(a) illustrates source routing for a packet routed through nodes A, B, and C. The route corresponds to East, North, and North, which is reﬂected in the routing ﬂits. Source routing eliminates the routing step and can potentially reduce the number of pipeline stages, but results in longer packets (with extra routing ﬂits) compared to the case where the route is computed at each hop. To avoid this, the nodes along the path can be programmed with next-hop routing information for relevant ﬂows. In this node-table routing approach, illustrated in Figure 1(b), the routing module contains a table with the output port for each ﬂow routed through the node. The head packet carries an index into this table, which, once looked up, is replaced with the index for the next hop stored in the table entry. To set up the route, our routing algorithm computes a route for each ﬂow and conﬁgures the routing tables accordingly. If we conservatively assume that each routing table has 256 entries (256 ﬂows), the table only takes a few KB: an entry needs 2 bits to represent the output port in a 2-D mesh and 8 bits for the next table index. Therefore, the table can be accessed in one cycle without impacting clock frequency. Both routing methods are widely known and have been implemented in multiple routers (e.g., [4, 8]). 2.3 Static Virtual Channel Allocation Statically allocating a VC to each ﬂow simpliﬁes the VC allocation step of the baseline router. Rather than being dynamically allocated using arbiters, VCs at each link are allocated per ﬂow by the routing algorithm. The router then assigns the next-hop VC in the same way as it obtains the route: with source routing, each packet carries its VC number for each hop along with its route, while in node-table routing an entry in the routing table is augmented with the VC number for the ﬂow. Since the router can thus obtain both the output port and the next VC number in the routing (RC) step, the primary complexity in the VA step lies in the arbitration among packets: two or more packets may be assigned the same VC siwill be sent ﬁrst. This requires a P · V to 1 arbitration for each VC multaneously, and arbitration is needed to determine which packet where packets from P physical channels with V VCs each vie for the same VC, and is simpler than the P · V to V arbitration required by dynamic routing. A previous study [16] indicates that P · V to 1 arbitration is about 20% faster than P · V to V arbitration (11.0 FO4 vs. 13.3 FO4 with 8 VCs). Static VC allocation requires additional bits in the routing table to specify the VC for each ﬂow. For example, for 8 VCs, 3 extra bits are required for each entry; if each routing table has 256 entries, this results in an increase of 96 bytes, still keeping the routing table accessible in a single cycle. Since static allocation does not consider dynamic behavior, it can potentially result in worse utilization of available VCs; for example, statically allocating VC0 to ﬂow A and VC1 to ﬂow B may be inefﬁcient when ﬂow A is idle, because ﬂow B might be able to use both VCs. On the other hand, static allocation can enhance throughput by separating or isolating ﬂows (cf. Figure 2). 3. STATIC VC ALLOCATION IN OBLIVIOUS ROUTING We assume the router design described in Section 2 with support for static VC allocation as described in Section 2.3. Since each link has multiple VCs, the assignment of channels to ﬂows is done on a per link basis. 3.1 Dimension-Ordered Routing (DOR) On a mesh, dimension-ordered routing corresponds to either XY or YX routing. Figure 2 exhibits the advantages of static allocation: four uncorrelated ﬂows with the same demands are shown, using XY routing with four VCs. Flows B, C, and D share link 2, which becomes congested when injection rates are high; this limits the throughput of ﬂow B to approximately one-third of the link bandwidth. If dynamic allocation is used, ﬂow A also suffers because of head-of-line blocking when ﬂow A is held up by ﬂow B. If we statically allocate VCs, however, we can assign ﬂows A and B to separate channels and utilize the full bandwidth of link 1. Dynamic 0.3234 0.3146 0.3365 0.3488 Flow A Flow B Flow C Flow D Throughput (ﬂits/cycle) Static 0.6681 0.3319 0.3332 0.3349 Figure 2: Motivation for Static Allocation A pair of ﬂows is said to be entangled if the ﬂows share at least one VC across all the links used by both ﬂows. Prior to channel assignment, no pairs of ﬂows are entangled, and, if the number of ﬂows for a given link is smaller then the number of VCs, we can avoid entanglement by assigning one channel per ﬂow. Otherwise, in order to mitigate the effects of head-of-line blocking, we allocate VCs so as to reduce the number of distinct entangled ﬂow pairs. Flows are assigned to VCs separately at each link. Given a link and a ﬂow F using it, the allocation algorithm proceeds as follows: 1. Check if there is a VC containing only ﬂows that are already entangled with F. Once two ﬂows share a VC somewhere, there is no advantage to assigning them to different VCs afterwards, and, if such a channel exists, it’s allocated to F. 2. Look for empty VCs on the link; if one exists, assign it to F. 3. If some VC contains a ﬂow entangled with F, assign it to F. 4. If none of the criteria above apply, assign F to the VC with the fewest ﬂows. 5. Update ﬂow entanglement relationships to reﬂect the new assignment.       The process above is repeated for each ﬂow at the given link, and the algorithm moves on to the next link. with any number of VCs. The third turn model, Negative-First, does not serve our purposes and so is not shown.1 3.2 ROMM and Valiant The ROMM [13] and Valiant [19] routing algorithms attempt to balance network load by choosing random intermediate nodes in the network and using XY/YX routing to route ﬁrst from the source to the intermediate node and then from there to the destination. The basic algorithm for static allocation is same as for DOR. The only difference arises from the requirement that the sourceto-intermediate and intermediate-to-destination subroutes not share the same VCs, in order to avoid deadlock. This reduces our allocation choices, since ﬂows must be assigned VCs only within the particular set. While ROMM and Valiant thus require a minimum of 2 VCs, having more than 2 is desirable as it affords some freedom in allocating VCs. 4. STATIC VC ALLOCATION BANDWIDTH-SENSITIVE ROUTING IN We now show how to select routes to minimize maximum channel load given rough estimates of ﬂow bandwidths, and how deadlock freedom can be assured through static VC allocation subsequent to route selection. (We again assume the router design from Section 2 with the static VC allocation support of Section 2.3). 4.1 Flow graph and Turn Model D E FIN I T ION 1. Let G(V, E ) be a ﬂow graph where each edge (u, v) ∈ E has a capacity c(u, v) representing the available bandwidth on the edge. Let K = {K1 , K2 , . . . , Kk } be a set of k data transfers (or ﬂows) where Ki = (si , ti , di ) and si represents the source for connection i, ti the sink (with si (cid:5)= ti ), and di the demand; multiple ﬂows with the same source and destination are permitted. The ﬂow i along an edge (u, v) is f i (u, v). A route for ﬂow i is a path pi from si to ti ; edges along this path will have f i (u, v) > 0, while other edges will have f i (u, v) =0. If f i (u, v) > 0, then route pi will use both bandwidth and buffer space on edge (u, v); the magnitude of f i (u, v) indicates how much of the edge’s bandwidth is used by ﬂow i. Although we assume ﬂit-buffer ﬂow control in this paper, our techniques also apply to other ﬂow control schemes. With a single VC per link or dynamic VC allocation, packets routes that conform to an acyclic channel dependence graph avoid network deadlock [5]. This is also a necessary condition unless false resource dependencies exist [17]. (a) (b) Figure 3: Turns allowed (solid) and disallowed (dotted) under (a) the West-First turn model and (b) the North-Last turn model. Turn models [10] are a systematic way of generating deadlockfree routes, and have been used for adaptive routing. Figure 3 shows two turn models that can be used in a 2-D mesh: each model disallows two out of the eight possible turns. If a set of routes conforms to one of the turn models, then deadlock freedom is assured 4.2 Bandwidth-Sensitive Oblivious Routing with Minimal Routes (BSORM) We now describe a routing method that targets improved network throughput given rough estimates of ﬂow bandwidths. We show how any set of minimal routes produced using any routing method can be made deadlock-free through appropriate static VC allocation (cf. Section 4.3); our argument for deadlock freedom invokes the turn models of Figure 3. Given rough estimates of bandwidths of data transfers or ﬂows, bandwidth-sensitive oblivious routing selects routes to minimize the maximum channel load, i.e., the maximum bandwidth demand on any link in the network. The method works on a ﬂow graph G(V, E ) corresponding to the network; for each ﬂow, we select a minimal route that heuristically minimizes the maximum channel load using Dijkstra’s weighted shortest-path algorithm. We start with a weighted version of G, deriving the weights from the residual capacities of each link. Consider a link e in G with a capacity c(e). We create a variable for ˜c(e) representing the current residual capacity of e; initially, ˜c(e) equals the capacity c(e), and is set to be a constant C. If the residual capacity ˜c(e) exceeds the demand di of a ﬂow i, then ﬂow i can be routed via link e and di is subtracted from ˜c(e). Since ﬂows are not routed through links with insufﬁcient ˜c(e), no residual capacity is ever negative. For the weighting function, we use the reciprocal of the link residual capacity, which is similar to the CSPF metric described by , except if ˜c(e) ≤ di then w(e) = ∞ Walkowiak [20]: w(e) = 1 and the algorithm never chooses the link. The constant C is set to the smallest number that provides routes for all ﬂows without using ∞-weight links. The maximum channel load (MCL) from XY or YX routing gives us an upper bound for C, but in most cases, there are solutions for lower values of C; in effect, a smaller C places more weight on avoiding congested links. We run Dijkstra’s algorithm on the weighted G to ﬁnd a minimum-weight path si ; ti for a chosen ﬂow i. The algorithm we use also keeps track of the number of hops, and ﬁnds the minimumweight path with minimum hop count. (While our weight function allows the smallest weight path to be non-minimal, the algorithm will not generate such a path). After the path is found, we check to see whether it can be replaced by one of the XY/YX routes of Figure 4(b) while keeping the same minimum weight; if so, this replacement is made, which minimizes the number of turns in the selected routes and allows greater freedom for the static VC allocation step (cf. Theorem 1). Finally, the weights are updated, and the algorithm continues on to the next ﬂow, until all ﬂows are routed. ˜c(e)−di 4.3 Deadlock-Free Static VC Allocation Since the routes selected by the Dijkstra-based algorithm may not conform to a particular acyclic CDG or turn model, they may not be deadlock-free. If the number of available VCs exceeds 2, however, we can ensure deadlock freedom via static VC assignment by partitioning the ﬂows across available VCs. TH EOR EM 1. Given a router with ≥ 2 VCs, and an arbitrary set of minimal routes over an n × n mesh, it is possible to statically allocate VCs to each ﬂow to ensure deadlock freedom. 1We have ignored the Negative-First turn model because it does not induce a ﬂow partition (and yield a channel allocation strategy) in combination with either of the other two turn models (cf. Theorem 1). This is true even when rotations are used.      	   (a) (b) Figure 4: (a) The eight different two-turn minimal routes on a 2-D mesh. (b) The four (out of a possible eight) different oneturn routes on a 2-D mesh that conform to both the West-First and North-Last turn model. Proof: Consider, without loss of generality, the case of 2 VCs. Figure 4(a) shows the eight possible minimal routes with two different turns each. (Minimal routes that have a single turn or no turns can be ignored as special cases of two-turn routes for the subsequent analysis). Looking at Figure 4(a), it is easy to see that minimal routes 3, 4, 5, and 8 conform to the West-First turn model (but violate the North-Last model as shown by the boxes over the violating turns), while minimal routes 1, 2, 6, and 7 conform to the NorthLast turn model (but violate the West-First turn model as indicated by the circles over the illegal turns). Therefore, we can partition an arbitrary set of routes into two sets: the ﬁrst conforming to the West-First turn model, and the second to the North-Last model. Note that the four one-turn minimal routes shown in Figure 4(b), and routes with no turns, can be placed in either set; the four other one-turn routes (not shown) will be forced to one of the sets. If we assign VC 1 to the ﬁrst set and VC 0 to the second, no deadlock can occur. The proof of Theorem 1 suggests a static VC allocation strategy. After deriving minimal routes using the BSOR algorithm of Section 4.2, we create three sets of ﬂows: 1. ﬂows with two-turn and single-turn routes that conform to the West-First turn model, 2 2. ﬂows with two-turn and single-turn routes that conform to the North-Last turn model, and 3. ﬂows with single-turn or zero-turn routes that conform to both. Before moving on to static VC allocation, we assign the ﬂows in the third set to either of the ﬁrst two sets, appropriately balancing the bandwidths and number of ﬂows. Each ﬂow in the third set is assigned to the set that has fewer ﬂows that share links with the ﬂow, or, if the number of shared ﬂows is the same for both sets, to the set with fewer ﬂows. After only two sets remain, we have local ﬂexibility in determining the ratio of VCs across the two sets. The number of ﬂows for the ﬁrst set and that for the second set can be different for each link, so we must assign VCs to the two sets on a per-link basis. We follow a simple principle: at each link, split available VCs evenly into two groups associated with the two ﬂow sets and, if unused VCs remain in exactly one group, shift the unused VCs to the other group. For example, if the number of ﬂows in the ﬁrst set is 2 and that for the second set is 6, the VCs are divided into two groups of size (1,1), (2,2), and (2,6) for #VC=2, #VC=4, and #VC=8, respectively. (Notice that for the #VC=8 case, we do not allocate four channels to the ﬁrst set since it only has two ﬂows). This localized division reduces wasted VCs, and the route is now deadlock-free since the two sets of ﬂows are assigned to disjoint groups of channels. Finally, at each link, we assign a given ﬂow to either set, with the VC allocation within the set the same as in DOR. 5. RELATED WORK 5.1 Routing Techniques A basic deterministic routing method is dimension ordered routing (DOR) [5] which becomes XY routing in a 2-D mesh. Necessary and sufﬁcient conditions for deadlock-free deterministic routing were given in [5] assuming no false resource dependences. ROMM [13] and Valiant [19] are classic oblivious routing algorithms, which are randomized in order to achieve better load distribution. In o1turn [18], Seo et al show that simply balancing trafﬁc between XY and YX routing can guarantee provable worst-case throughput. A weighted ordered toggle (WOT) algorithm that assumes 2 or more virtual channels (VCs) assigns XY and YX routes to source-destination pairs in a way that reduces the maximum network load for a given trafﬁc pattern [9]. Classic adaptive routing schemes include the turn routing methods [10] and odd even routing [1]. 5.2 Bandwidth-Aware Routing Palesi et al [15] provide a framework and algorithms for application-speciﬁc bandwidth-aware deadlock-free adaptive routing. Given a set of source-destination pairs, cycles are broken in the CDG to minimize the impact on the average degree of adaptiveness. Bandwidth requirements are taken into account to spread trafﬁc uniformly through the network. Our focus here is on oblivious routing. Bandwidth-aware routing for diastolic arrays is described in [2]; deadlock is avoided by assuming that each ﬂow has its own private channel. An application-aware oblivious routing (BSOR) framework for conventional routers with dynamic VC allocation and one or more VCs is presented in [11]; this framework selects possibly non-minimal routes that conform to an acyclic CDG typically derived from a turn model. In this paper, our focus is static VC allocation schemes for traditional oblivious routing methods (e.g., DOR, ROMM, Valiant) as well as for bandwidth-sensitive oblivious routing. 6. RESULTS AND COMPARISONS This section compares the performance of static and dynamic VC allocation using synthetic trafﬁc through simulation. We also compare our routing scheme (BSORM) with other oblivious routing algorithms like DOR, ROMM [13], and Valiant [19]. 6.1 Benchmarks We use a set of standard synthetic trafﬁc patterns: transpose, bit-complement, and shufﬂe, as well as an application benchmark H.264. The synthetic patterns are widely used to evaluate routing algorithms and provide basic comparisons between our routing scheme and other oblivious algorithms; in the synthetic benchmarks, all ﬂows have the same average bandwidth demands. H.264 is a set of ﬂows reﬂecting the trafﬁc pattern of an H.264 decoder, with ﬂow bandwidths derived through proﬁling. 6.2 Simulator Details A cycle-accurate network simulator was used to estimate the routing. We use an 8 × 8 2-D mesh network with 1, 2, 4 or 8 VCs throughput of each ﬂow in the application for various oblivious per port, and we simulate a ﬁxed packet length of 8 ﬂits. The simulator is conﬁgured to have a per-hop latency of 1 cycle and the ﬂit buffer size per VC of 16 ﬂits. For each simulation, the network was warmed up for 20,000 cycles and then simulated for 100,000 cycles to collect statistics, which was enough for convergence. 0.9 0 2 4 6 8 10 12 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Transpose XY and YX VC = 2 XY dynamic XY static YX dynamic YX static (a) 1 1 1.5 2 2.5 3 3.5 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Bitcomp XY and YX VC = 2 XY dynamic XY static YX dynamic YX static (b) 1 0 2 4 6 8 10 12 14 1.5 2 2.5 3 3.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Shuffle XY and YX VC = 2 XY dynamic XY static YX dynamic YX static (c) 1.3 0 10 20 30 40 50 1.4 1.5 1.6 1.7 1.8 1.9 2 2.1 2.2 2.3 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) H264 XY and YX VC = 2 XY dynamic XY static YX dynamic YX static (d) Figure 5: Throughput for dimension-ordered routing under static and dynamic allocation with 2 VCs. 0.9 0 2 4 6 8 10 12 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Transpose ROMM and Valiant VC = 4 ROMM dynamic ROMM static Valiant dynamic Valiant static (a) 1 1.5 2 2.5 ROMM dynamic ROMM static Valiant dynamic Valiant static 3 3.5 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Bitcomp ROMM and Valiant VC = 4 (b) 0.5 0 2 4 6 8 10 12 14 1 1.5 2 2.5 3 3.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Shuffle ROMM and Valiant VC = 4 ROMM dynamic ROMM static Valiant dynamic Valiant static (c) 0 10 20 30 40 50 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) H264 ROMM and Valiant VC = 4 ROMM dynamic ROMM static Valiant dynamic Valiant static (d) 0.9 0 2 4 6 8 10 12 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Transpose ROMM and Valiant VC = 8 ROMM dynamic ROMM static Valiant dynamic Valiant static (e) 1 1.5 2 2.5 ROMM dynamic ROMM static Valiant dynamic Valiant static 3 3.5 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Bitcomp ROMM and Valiant VC = 8 (f) 0.5 0 2 4 6 8 10 12 14 1 1.5 2 2.5 3 3.5 4 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Shuffle ROMM and Valiant VC = 8 ROMM dynamic ROMM static Valiant dynamic Valiant static (g) 0.5 0 10 20 30 40 50 1 1.5 2 2.5 3 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) H264 ROMM and Valiant VC = 8 ROMM dynamic ROMM static Valiant dynamic Valiant static (h) Figure 6: Throughput for ROMM and Valiant under static and dynamic allocation with 4 and 8 VCs. 6.3 DOR, ROMM and Valiant Figure 5 shows the performance of XY and YX routing with 2 VCs for static and dynamic VC allocation for various benchmarks. Figure 6 shows the performance of ROMM and Valiant under static and dynamic allocation for 4 and 8 VCs. ROMM and Valiant routes require 2 VCs to avoid deadlock; these routes are broken into two segments, with a VC allocated to each segment. Hence static and dynamic allocation schemes differ when there are multiple VCs that can be allocated to each route segment. For all these algorithms, static allocation performs as good or better than dynamic allocation for high injection rates by more effectively reducing head-of-line blocking effects as exempliﬁed in Figure 2. 6.4 BSORM Figure 7 shows the performance of the BSORM algorithm for four VCs and compares it to XY (static and dynamic) for various benchmarks. We use BSORM to obtain the routes and break these routes into two sets to avoid deadlock, as described in Section 4.3. We perform static allocation or assume dynamic allocation within each set. Figure 8 compares BSORM under static and dynamic allocation for 8 VCs. As each benchmark uses a single routing derived using BSORM, the performance differences are due only to static versus dynamic VC allocation. BSORM performs better than DOR on the benchmarks because the bandwidth-aware routing reduces MCL; BSORM with static allocation outperforms dynamic allocation for the same reasons as in DOR. 7. CONCLUSIONS Our results indicate that static VC allocation often outperforms dynamic VC allocation for existing oblivious routing schemes. This is because static allocation can better reduce the effects of head-of-line blocking. When given rough estimates of bandwidths, the BSORM algorithm provides better performance than existing oblivious routing schemes, and here too, static allocation produces as good or better results. If head-of-line blocking effects are small, maximum channel load serves as a dominant factor in determining the performance of a given route. This justiﬁes the BSORM algorithm’s minimization of the maximum channel load. Two other advantages of static allocation are that routers with static allocation can be slightly simpler than those with dynamic allocation, and static allocation assures in-order packet delivery, since two VCs in a link will never be assigned to ﬂits/packets from the same ﬂow. Bandwidth-adaptive networks contain adaptive bidirectional links and can improve the performance of conventional oblivious routing methods [3]. Ongoing work includes evaluating BSORM on a bandwidth-adaptive network.                                                 0.5 0 2 4 6 8 10 12 1 1.5 2 2.5 3 3.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Transpose BSORM and XY VC = 4 BSORM dynamic BSORM static XY dynamic XY static (a) 1 1 1.5 2 2.5 3 3.5 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Bitcomp BSORM and XY VC = 4 BSORM dynamic BSORM static XY dynamic XY static (b) 1 0 2 4 6 8 10 12 14 1.5 2 2.5 3 3.5 4 4.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Shuffle BSORM and XY VC = 4 BSORM dynamic BSORM static XY dynamic XY static (c) 1.6 0 10 20 30 40 50 1.8 2 2.2 2.4 2.6 2.8 3 3.2 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) H264 BSORM and XY VC = 4 BSORM dynamic BSORM static XY dynamic XY static (d) Figure 7: Throughput for BSORM and XY under static and dynamic allocation with 4 VCs. 0.5 0 2 4 6 8 10 12 1 1.5 2 2.5 3 3.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Transpose BSORM and XY VC = 8 BSORM dynamic BSORM static XY dynamic XY static (a) 1 1 1.5 2 2.5 3 3.5 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Bitcomp BSORM and XY VC = 8 BSORM dynamic BSORM static XY dynamic XY static (b) 1 0 2 4 6 8 10 12 14 1.5 2 2.5 3 3.5 4 4.5 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) Shuffle BSORM and XY VC = 8 BSORM dynamic BSORM static XY dynamic XY static (c) 1.6 0 10 20 30 40 50 1.8 2 2.2 2.4 2.6 2.8 3 3.2 Offered Injection Rate (packets/cycle) o T t a l h T r u p h g u o t ( e k c a p t s / e c y c l ) H264 BSORM and XY VC = 8 BSORM dynamic BSORM static XY dynamic XY static (d) Figure 8: Throughput for BSORM and XY under static and dynamic allocation with 8 VCs. 8. "
Adaptive stochastic routing in fault-tolerant on-chip networks.,"Due to shrinking transistor geometries, on-chip circuits are becoming vulnerable to errors, but at the same time on-chip networks are required to provide reliable services over unreliable physical interconnects. A connection oriented stochastic routing (COSR) algorithm has been used on one NoC platform that provides excellent fault-tolerance and dynamic reconfiguration capability. A probability model has been built to analyze the COSR algorithm. According to the model, the performance may be improved by implementing a self learning mechanism in each router. Thus a new adaptive stochastic routing (ASR) algorithm is proposed whereby each router learns the network status from acknowledgement flits and stores the outcomes in a routing table. Simulation of both algorithms reveals that the ASR algorithm shows a higher path reservation success rate and a larger maximal accepted traffic than the COSR algorithm. The simulations also show that the learning procedures are accurate and that both algorithms are fault-tolerant to intermittent/permanent errors.","Adaptive Stochastic Routing in Fault-tolerant On-chip Networks∗ Wei Song1 , Doug Edwards1 , Jos ´e Luis Nu ˜nez-Ya ˜nez2 , and Sohini Dasgupta1 1School of Computer Science, University of Manchester, Manchester, M13 9PL UK 2Department of Electrical and Electronic Engineering, Bristol University, Bristol, BS8 1UB UK {songw,doug,shinid}@cs.man.ac.uk, j.l.nunez-yanez@bristol.ac.uk Abstract Due to shrinking transistor geometries, on-chip circuits are becoming vulnerable to errors, but at the same time onchip networks are required to provide reliable services over unreliable physical interconnects. A connection oriented stochastic routing (COSR) algorithm has been used on one NoC platform that provides excellent fault-tolerance and dynamic reconﬁguration capability. A probability model has been built to analyze the COSR algorithm. According to the model, the performance may be improved by implementing a self learning mechanism in each router. Thus a new adaptive stochastic routing (ASR) algorithm is proposed whereby each router learns the network status from acknowledgement ﬂits and stores the outcomes in a routing table. Simulation of both algorithms reveals that the ASR algorithm shows a higher path reservation success rate and a larger maximal accepted trafﬁc than the COSR algorithm. The simulations also show that the learning procedures are accurate and that both algorithms are fault-tolerant to intermittent/permanent errors. 1 Introduction As transistor dimensions continue to shrink, on-chip circuits are becoming vulnerable to transient, intermittent and permanent errors [4], especially on long interconnects [9]. The paradigm shift in current multiprocessor system-onchip (SoC) designs, that replaces the SoC buses with a NoC communication fabric, introduces the challenge of providing reliable communication on unreliable physical interconnects. Normally there are two ways for a network-on-chip (NoC) to recover from transient errors: retransmission and error correction. Considering the low error rate for current circuit design technology and the large overhead of error detection/correction circuits, retransmission is more power efﬁcient than error correction [2]. Therefore, short-term transient errors could be handled in the transport layer (end-toend level) and routers only need to provide a reliable path ∗ This work is supported by EPSRC grant EP/E06065X/1 and EP/E062164/1. in the presence of permanent/longer term intermittent errors. Adaptive routing algorithms have long been utilized in NoCs with on-line defective interconnects. However, the turn-model is constrained to avoid deadlocks. In some extreme cases, an existed path is prohibited by the routing algorithm. In constrast to adaptive routing algorithms, the stochastic routing algorithms are always able to ﬁnd an available path in the presence of errors. The stochastic routing algorithm was used in a probabilistic ﬂooding scheme which demonstrated a high message arrival rate with the cost of high energy consumption and long transmission latency [5, 3]. A tradoff to reduce the energy consumption could be made by constraining the extra messages ﬂooded, such as the redundant random walk and directed ﬂooding algorithms [8]. We have proposed a fault-tolerant and dynamically reconﬁgurable network-onchip (NoRC) platform [7] based on a further constrained random walk routing algorithm, namely connection oriented stochastic routing (COSR) algorithm. Combining the reconﬁguration technology and the COSR routing algorithm, this platform can a) map tasks onto network nodes at run-time, including moving a task from one node to another, b) provide delay guaranteed services, c) maintain data integrity through the frame level error detection and retransmission scheme and d) cope with intermittent/permanent errors. However, the COSR algorithm tends to reserve long paths, constraining trafﬁc, increasing the probability of rejection and consuming unnecessary channel resources. A new adaptive stochastic routing (ASR) algorithm is proposed. By adding a routing table in each router, the router can learn from random walks and then gradually direct random walks to shorter paths. This claim is derived from a probability model and is supported by transaction level simulations. Simulation results based on faulty networks also show that both algorithms are fault-tolerant to intermittent/permanent errors. The remainder of this paper is organized as follows: section 2 redeﬁnes the fault-tolerant dynamically reconﬁgurable network-on-chip platform and the COSR algorithm described in [7], section 3 analyzes the COSR algorithm using a probability model and the new ASR algorithm is proposed, section 4 compares the performance of these two al978-1-4244-4143-3/09/$25.00 ©2009 IEEE    gorithms on a fault-free network, section 5 demonstrates the performance when intermittent and permanent errors occur, and the paper is concluded in section 6. 2 The NoRC Platform Figure 1 shows a conﬁgured 4x4 mesh NoRC with nine tasks running on the chip. Every network node comprises a processor element, a network interface and a router. Applications are divided into several parallel tasks. One processor element can run one or multiple tasks according to the run-time conﬁguration. For simplicity, we only conﬁgure one task per processor element in this paper. The network interface provides its local processor element a bidirectional channel to the on-chip network. The router is responsible for forwarding messages to its adjacent routers or the local network interface and reserve/release the channels according to the COSR algorithm. Shown in Figure 1, one task may run on only one node, e.g. tasks {1,2,4,5,8,9}, or run on multiple nodes, e.g. tasks {3,6,7}. Multiple processor elements may be conﬁgured with the same task if heavy computation loads are required or the task is important and a backup is necessary. Nodes identiﬁed as task 0 are idle nodes which are waiting for a conﬁguration. Figure 1. A 4x4 NoRC The communications on this platform are connection oriented and function oriented. The target of a message is addressed by a function identiﬁer (FID) denoting the task that can consume this message. When a master processor element sends out a message, namely a frame, the network interface divides the frame into a sequence of ﬂits. It ﬁrstly encapsulates the FID into a request ﬂit and sends it into the network. Routers in the network stochastically forward the request ﬂit and reserve the path until a node matches the FID or no channel is available in a certain router. An ok-ack ﬂit is sent back by the network interface of the target slave or a false-ack ﬂit is bounced back to inform the failure and release the path. According to the ﬂit received, the master node sends out all data ﬂits to the reserved path (success), or re-sends the request ﬂit again after a retry interval (failed). After all data ﬂits have been received by the slave node, the communication procedure is terminated by a false-ack ﬂit bounced back by the slave to release the path and inform the master. Suppose the bandwidth of one channel is enough for all applications, the router in NoRC can be structured as Figure 2. Except for the routers on the boundaries, each router is connected with four adjacent nodes (south, west, north and east) and its local network interface by an input channel and an output channel. Every input channel is connected with a ﬂit size input buffer. All input channels and output channels are fully connected through the crossbar dynamically conﬁgured by the arbiter, which is responsible for analyzing the ﬂits arrived. When the crossbar is conﬁgured, the input buffer is able to transmit all received ﬂits directly to the output channel without interrupting the arbiter. The paths conﬁgured in the crossbar are released automatically when a false-ack is transmitted. Figure 2. A simpliﬁed router for NoRC The conﬁguration procedure of the crossbar has two stages: the forward setup and the backward setup. When a request ﬂit arrives at the input buffer, the arbiter tries to connect the forward path to an idle output channel. Because of congestion, the arbiter may fail to ﬁnd an available output channel and a false-ack ﬂit is bounced back. When a false-ack/ok-ack ﬂit arrives from the backward channel, the arbiter then conﬁgures the crossbar to connect the backward channel. The network interface also has a timeout mechanism. When no further ﬂit is received by a certain time interval, both the master and the slave node can send a false-ack ﬂit to release the path and to avoid deadlocks. Therefore, when one permanent or intermittent error occurs on a reserved path, the path is released due to the timeout mechanism. This transport layer protocol cannot handle the case when two or more errors happen nearly simultaneously on a path. An error detection scheme running in the routers is required but this is not within the scope of this paper. 3 The Adaptive Stochastic Routing Algorithm 3.1 Analyzing the random walks We intend to build a simple model to analyze the probability with which a master reserves a path to its slave in a fault-free and idle mesh in a limited number of hops. We assume a mesh network with inﬁnite dimensions to eliminate the impact of boundaries, channels are exclusively allocated and only one active master is assumed to be using the network. A task may run on multiple nodes but this signiﬁcantly complicates the calculation. Therefore, we only consider the single slave case. According to the COSR algorithm, a request ﬂit is forwarded to all available ports with an equal probability. For the ith router on a path, the port ˆpi is selected. For a possible path that traverses L routers (hop count L), the event S Table 1. The success rates when the slave is less than 4 hops away (∆x , ∆y ) P = P {L ≤ 16} Pp = P {L ≤ 16 | ˆp ∈ P } P ¯p = P {L ≤ 16 | ˆp /∈ P } (1, 0) (1, 1) (2, 0) (2, 1) (3, 0) (2, 2) (3, 1) (4, 0) 42.07% 32.32% 24.93% 21.22% 14.98% 16.43% 14.17% 9.60% 100.00% 48.39% 49.00% 31.10% 28.53% 24.15% 20.34% 17.94% 22.76% 16.25% 16.91% 11.35% 10.46% 8.71% 8.00% 6.81% denotes that the path is selected by the random walk. Shown in (1), S occurs when all the routers on the path select the corresponding port ˆpi . P {S } = P { L in one byte. C = 31 denotes the strongest conﬁdence to ˆp ∈ P while 0 indicates that the router has no knowledge about this FID. Therefore, all of the conﬁdence ﬁelds in RT are set to 0 after the initialization stage. Table 2. A routing table after initialization ˆp (0..4) C (0..31) 0 0 0 0 0 0 0 0 FID1 FID2 – FIDn The ASR algorithm is an improved version of the COSR algorithm. When a request ﬂit arrives, the router sends the ﬂit to the estimated ˆp with probability C/32 or to a random port with probability (1 − C/32). Therefore, the router is likely to use the estimated port with a strong conﬁdence while also exploring the network when not so conﬁdent. The estimated ˆp and conﬁdence C are updated by acknowledgement ﬂits. C increases when an ok-ack ﬂit is received from port ˆp and decreases when a false-ack ﬂit is received from ˆp. When the conﬁdence to ˆp drops to 0, ˆp is set to the new incoming port of the next ok-ack ﬂit. The new ASR algorithm is still a stochastic routing algorithm. Since all routing tables are set to 0 after initialization, the routers use the COSR algorithm at ﬁrst and then update routing tables according to acknowledgement ﬂits. Due to this learning procedure, the ASR algorithm is adaptable to status changes and run-time errors. 4 Performance Comparisons on Error Free NoCs Both the COSR and the ASR algorithms have been implemented using timed SystemC transaction level models (TLM). We intend to implement the routers by fully asynchronous circuits to reduce the dynamic power consumption and the channel latency. Routers and network interfaces are connected by Chain style [1] asynchronous channels. The channel latency is set to 6.24 ns/byte to simulate a 3800 µm Chain channel in a 0.18 µm technology [6]. The router arbitration and internal crossbar latency is set to 5 ns, as it is set in [7]. For each test case, the simulation runs for 4 ms with a warm up time of 500 µs. A uniform random trafﬁc pattern is used in all simulation cases. The conﬁgured processor elements periodically and randomly generate frames to all FIDs except the local FID and the idle FID 0 (see section 2). Each frame has a 64 byte data ﬁeld which is divided into 16 data ﬂits. 4.1 Performance comparisons of COSR and ASR Figure 3 shows the comparison results of the COSR and the ASR algorithms on a 4x4 NoC. A total number of nine (a) The average frame latency (b) The average success rate Figure 3. Comparison results on a 4x4 NoC tasks running on this NoC are mapped as shown in Figure 1. The average frame latency in Figure 3(a) denotes the time interval since a frame being generated until the endof-frame false-ack ﬂit of this frame being received by the master node. The average success rate in Figure 3(b) is calculated by Nok−ack Nok−ack + Nf alse−ack where Nf alse−ack dose not include end-of-frame false-ack ﬂits. Shown in Figure 3(b). The ASR algorithm demonstrates a 15.59% higher success rate when the network load is heavy. Moreover, this 15.59% higher success rate alleviates the network contention and decreases the path length, which together push the maximal accepted trafﬁc from 7.5 MByte/Node/Sec to 10.5 MByte/Node/Sec, providing 40.0% improvement. 4.2 Results of the learning procedure To demonstrate the accuracy of the learning procedure, a 4x4 NoC mapped as shown in Figure 1 has been simulated for 4 ms and all estimations in routing tables are extracted in the end. Since nodes and channels may be reserved for a long time and these reservations could affect the network status, an extremely light trafﬁc 1 MByte/Node/Sec is injected into the network to avoid this effect and simplify the explanations. By analyzing the extracted data, Figure 4 shows the estimated results in a graphic way. The estimated ports are denoted by corresponding arrows (south: ↓, west: ←, north: ↑, east: → and local: ◦). Ports with comparatively strong conﬁdences (C ≥ 16) are drawn by solid and bold arrows while those with lower conﬁdences (C < 16) are drawn with dash and slim arrows. (a) task 1 (b) task 2 (c) task 3 (d) task 4 (e) task 5 (f) task 6 (g) task 7 (h) task 8 (i) task 9 Figure 4. The estimated directions For all tasks in Figure 4, nearly all slaves for certain functions, denoted by ◦, are surround by strongly estimated ports pointed to them except the top left node (0, 0) in Figure 4(f). However, the estimations around this node are not wrong and actually explain how the network status can affect the estimated results. The node (0, 0) with FID 6 should be reserved at the time, since all strong estimations are pointing to the alternative node (1, 3) and the estimations around node (0, 0) are weak and are directing frames to the alternative. The routing tables indeed adjust themselves to the network status. The majority of port estimations in routing tables are correct. For this test case, there are a total number of 144 estimated ports and conﬁdence ﬁelds. Of these, 29 ports do not point to the nearest slave, which leads to an error rate 20.14%. Note that some erroneous estimations are affected by the changes of the network status and in fact point to the right directions. The actual error rate is lower than 20.14%. Furthermore, only the strong estimations significantly change the random walks of the COSR algorithm. The error rate for strong estimations is just 6.58%. Therefore, the port learning procedure of the ASR algorithm successfully learns the network status from the acknowledgement ﬂits. 5 Performance on Faulty NoCs As mentioned, the errors in network could be classiﬁed into three categories: transient errors, intermittent errors and permanent errors. On the NoRC platform, transient errors are handled by the error detection and retransmission mechanism in the transport level. Therefore, we only demonstrate the network performance in presence of permanent/intermittent errors. For asynchronous channels like Chain [1], any intermittent/permanent error causes a line to be stuck to a certain voltage, which then stops the whole channel. Therefore, unlike synchronous buses where intermittent/permanent errors give erroneous data on a bus, asynchronous channels are stopped when these errors occur. In the following simulations, routers detect the intermittent/permanent errors and drop all ﬂits heading to the defective channels, and the performance of the the ASR algorithm is evaluated. Figure 5 shows an example of a NoC with intermittent errors. During the simulation time 1 ms to 3 ms, the west channel of router (1, 2) is defective. During the simulation time 2 ms to 4 ms, the whole router (1, 0) is defective. Figure 6 demonstrates the simulation results of the variations on the accepted trafﬁc when intermittent errors occur. The network is loaded with a heavy load of 8 MByte/Node/Sec. The accepted trafﬁc of the ASR algorithm does not have a signiﬁcant variance during the occurrence of intermittent errors. The accepted trafﬁc of the COSR algorithm has a drop during 1 ms to 3 ms. The single error on the the west channel of router (1, 2) has a more severe impact than the fully defective router (1, 0). Note that the network has two nodes running FID 7 but only one for FID 1, the single error of router (1, 2) reduces the reachability of the rare resource FID 1. The results show that the ASR algorithm has a larger accepted trafﬁc than the COSR algorithm during errors and the errors on rare resources have a more severe impact than redundant resources. Duplicating the resources can alleviate the impact of errors. Figure 5. NoC with intermittent errors Figure 6. Accepted trafﬁc with intermittent errors Regarding the permanent errors, the NoC in Figure 7 suffers two permanent errors. After 1 ms simulation, the router (0, 0) is permanently defective and isolates the node (0, 0) from the network. Later at 2 ms, the router (0, 3) is also permanently defective and isolates the only node for FID 9. Without node (0, 3), FID 9 is unavailable for the network, which would deadlock the network since all nodes are stuck when requiring FID 9. According to the NoRC platform, the system level reconﬁguration mechanism can detect the failure and search an idle node to reconﬁgure it to FID 9. In this example, the selected idle node is node (2, 0), which is reconﬁgured to FID 9 after 2.5 ms. Figure 7. NoC with permanent errors Figure 8 demonstrates the simulation results of the variations on the accepted trafﬁc when permanent errors occur. The network is also loaded with a heavy load 8 MByte/Node/Sec. Both the COSR and the ASR algorithms show a slight drop when node (0, 0) is isolated from the network and drop to zero when the router (0, 3) is defective. After the system level reconﬁguration mechanism reconﬁgures the node (2, 0) to FID 9, they all return back to normal after a period of instability. The ASR algorithm shows a signiﬁcant “bounce” after the reconﬁguration and it saturates the network at a larger injected trafﬁc than the COSR algorithm, therefore the ASR algorithm can send out the frame stuck by failures in burst while the COSR algorithm is always running at the maximal accepted trafﬁc. Figure 8. Accepted trafﬁc with permanent errors In this section, the simulations demonstrate the faulttolerant capability of the COSR and the ASR algorithms. Especially in the second simulation case, both algorithms recover when an alternative node is conﬁgured, which is a feature that is not easily supported by non-stochastic routing algorithms. When intermittent/permanent errors occur, the ASR algorithm shows better accepted trafﬁc performance than COSR. 6 Conclusion This paper has proposed a new adaptive stochastic routing (ASR) algorithm running on the NoRC platform [7]. The ASR algorithm supports function oriented routing, provides strong fault-tolerance to intermittent/permanent errors and demonstrates the better path reservation success rate, average frame latency, maximal acceptable trafﬁc performance than the COSR algorithm. We have designed a simple probability model to analyze the success rate of the previous COSR algorithm. According to the analyses, we claim that sending a frame in the direction which has successfully reserved a path can improve the success rate. Both the COSR and the ASR algorithms have been modeled and simulated using TLMs. The simulation results on error free networks show that: the ASR algorithm has a 15.59% higher success rate and increases the maximal acceptable trafﬁc by 40.0%, compared with the COSR algorithm on a 4x4 NoC; the routing tables successfully adapt themselves to the network status through the ASR algorithm. On faulty networks, both the COSR and the ASR algorithms demonstrate fault-tolerance to intermittent/permanent errors. "
Estimating reliability and throughput of source-synchronous wave-pipelined interconnect.,"Wave pipelining has gained attention for NoC interconnect by its promise of high bandwidth using simple circuits. Reliability issues must be addressed before wave pipelining can be used in practice; so, we develop a statistical model of dynamic timing uncertainty. We show that it is important to distinguish between static and dynamic sources of timing uncertainty, because source-synchronous wave pipelining is much more sensitive to the latter. We use HSPICE simulations to develop a model for a wave pipelined link in a 65 nm CMOS process and apply a statistical approach to determine the achievable throughput at acceptable bit-error rates. Reliability estimates show that a modest amount of dynamic noise can cut achievable throughput in half for a ten-stage wave-pipelined link, and will further degrade longer links. After accounting for noise, traditional globally synchronous design is shown to offer higher throughput than the wave-pipelined design.","Estimating Reliability and Throughput of Source-synchronous Wave-pipelined Interconnect Paul Teehan, Guy G.F. Lemieux, and Mark R. Greenstreet University of British Columbia, Vancouver, BC, Canada Abstract Wave pipelining has gained attention for NoC interconnect by its promise of high bandwidth using simple circuits. Reliability issues must be addressed before wave pipelining can be used in practice; so, we develop a statistical model of dynamic timing uncertainty. We show that it is important to distinguish between static and dynamic sources of timing uncertainty, because source-synchronous wave pipelining is much more sensitive to the latter. We use HSPICE simulations to develop a model for a wave pipelined link in a 65nm CMOS process and apply a statistical approach to determine the achievable throughput at acceptable bit-error rates. Reliability estimates show that a modest amount of dynamic noise can cut achievable throughput in half for a ten-stage wave-pipelined link, and will further degrade longer links. After accounting for noise, traditional globally synchronous design is shown to offer higher throughput than the wave-pipelined design. 1 Introduction Due to the long latencies of global wires, global interconnect is often pipelined. The traditional globally synchronous, latch pipelined (GSLP) model shown in Figure 1(a) is well-understood and can be made highly reliable. Here, a link consists of several stages. Several recent papers have proposed that wave pipelining may offer further speed, area, and power advantages over traditional latch pipelining [17, 3, 5]. It has also been proposed for global interconnect in FPGAs [10]. Unfortunately, it is difﬁcult to achieve reliable communications with wave pipelining. Figure 1(b) shows wave pipelining with a global clock (GSWP). Without latches to keep edges separated, multiple data bits are simultaneously in ﬂight in the link and have only their nominal time separation to distinguish them. Because the data transport latency can vary, a phase alignment circuit is needed to correct for clock/data skew at the destination. This circuit needs to be continuously adjusted for latency drift and variation 978-1-4244-4143-3/09/$25.00 ©2009 IEEE  resulting in a complex and/or unreliable design. In contrast, Figure 1(c) shows source-synchronous wave pipelining (SSWP), where both clock and data experience similar latency, making phase alignment largely unnecessary. To further reduce skew, it may be necessary to occasionally latch the data, shown as SSWPL in Figure 1(d). Data pulsed latch Source Destination Low−skew global clock edge to pulse (a) GSLP: Globally synchronous, latch pipelined Data Source Low−skew global clock (b) GSWP: Globally synchronous, wave pipelined Data Source Clk gen Forwarded clock (generated at source) (c) SSWP: Source synchronous, wave pipelined Data Source pulsed latch edge to pulse Destination Phase Align Destination Destination Clk gen Forwarded clock (generated at source) (d) SSWPL: Source synchronous, wave pipelined, periodic latches Figure 1. Types of pipelined interconnect: GSLP, GSWP, SSWP, and SSWPL   Even with SSWP or SSWPL, there is inevitably still some timing uncertainty due to noise and variation throughout the link. Hence, sufﬁcient timing margin must be added between bits to ensure reliable operation. Many techniques for calculating these margins exist, but most rely upon worst-case bounds and are unrealistically pessimistic. Random statistical models for skew were developed in [7] for synchronous pipelines. Shyur [13] proposed using statistical methods to measure timing uncertainty in wavepipelined logic circuits, but only considered static timing. Similar yet more sophisticated approaches were taken in [5], but that paper also considered only static uncertainty. Timing constraints for source-synchronous wave pipelining were developed in [2], but that work did not include statistical modeling. There is precedent for using statistical models for random timing uncertainty in off-chip communication, for example in [8] and [12]. Zhang et al. [16] analyse latch and ﬂip-ﬂop based interconnect pipelining in a statistical model that includes both static and dynamic uncertainty. In [17], the same authors proposed a SSWP onchip interconnect but did not extend the statistical analysis of their previous work to this wave-pipelined design. This paper uses statistical methods to model the timing behavior of source-synchronous wave-pipelined links. This allows us to develop a methodology to estimate the probability of error as a function of circuit structure, throughput, and timing uncertainty due to noise and variation. In our analysis of SSWP and SSWPL, we show that: • dynamic timing uncertainty causes jitter to accumulate; • dynamic timing uncertainty plus systematic static variation cause skew to accumulate; • skew can be attenuated with latches, making jitter the sole factor to limit throughput; • jitter is a more dominant effect than skew, which differs from GSLP where skew is the dominant effect; • crosstalk can be mitigated through shielding; • high-frequency voltage supply noise remains the dominant form of dynamic timing uncertainty; and • after accounting for dynamic timing uncertainty, traditional GSLP offers higher throughput than wave pipelining when reasonable error-rates are required. The paper is organized as follows. Section 2 brieﬂy describes a programmable, source-synchronous pipelined interconnect design which is used as an example throughout the paper. Section 3 describes traditional worst-case timing constraints, illustrates the difference between static and dynamic timing, and makes an argument for statistical models. Section 4 shows how to measure dynamic timing uncertainty and provides simulation results which do so. Section 5 uses those results to estimate reliability in terms of probability of error as a function of throughput and dynamic uncertainty. Section 6 concludes the paper. Data in Clk in 0.5mm wire Data out 2x 7x 25x 0.5mm wire Clk out 2x 7x 25x 16:1 muxes Figure 2. Wave-pipelined interconnect circuit 2 NoC Circuit Design To evaluate the reliability and throughput of wave pipelining, we start with a particular NoC circuit design which will be used as an example throughput the paper. The four NoC interconnect pipelining strategies are shown in Figure 1 and have already been introduced. We assume the NoC must have low area and support both bursty and continuous data transmission – this limits our ability to employ circuitry like DLLs or PLLs, which can be large and take a long time to lock. Also, we presume that NoC interconnect is based upon a switched fabric, not ﬁxed point-topoint links. Hence, we include multiplexers in the interconnect paths to allow for packet or circuit switching. To keep power low and ensure clocking rates do not limit data transfer rates, we use pulsed latches that are sensitive to both clock edges (DDR clocking). Figures 1(c) and (d) use source-synchronous clocking. Its main advantage is that data and clock experience similar static timing uncertainty, which means static timing does not limit the link speed. This is elaborated upon in the next section. Although only one data wire is shown in these circuits, similar results would apply for bundled-data wires provided they are kept close enough to encounter the same uncertainty conditions. Figure 2 shows a detailed circuit of one stage in the interconnect pipeline. The 16:1 multiplexer puts two minimumsize CMOS transmission gates in the signal path to implement a conﬁgurable fabric. Three tapered inverters, the largest of which is 25 times minimum size, drive a 0.5mm wire. A link is composed of any number of such stages cascaded together. The circuit was designed to optimize the area-power-delay-throughput product; the last term, throughput, is unique to wave-pipelined circuits and requires a relatively large driver [15]. 3 Timing uncertainty in wave pipelining Many factors affect timing. Some are high frequency disturbances such as crosstalk and power supply noise; some drift at lower rates such temperature or electromigration; and some, such as most process effects, are effectively ﬁxed [11]. Lumping all of these effects into one category of global clock received data destination depth source dynamic static Figure 3. Timing uncertainty in globally synchronous wave pipelining (GSWP) “timing uncertainty” is not always appropriate. For example, when considering time separation between edges representing successive bits in SSWP, only those noise sources which operate in a time range comparable to the time separation between bits need to be considered. This paper classiﬁes noise sources as contributing to either dynamic or static timing uncertainty. Sources contributing to dynamic timing uncertainty must inﬂuence timing on a cycle-to-cycle basis. For this paper, this means a time scale of roughly 1ns or smaller. Crosstalk and fast supply noise are the two biggest sources of dynamic uncertainty [11] and both are considered in this paper. The other noise sources described above vary slowly enough to be considered static, even if they include a time varying component such as temperature. received clock received data destination depth source data forwarded clock data forwarded clock (a) Fast static timing (b) Slow static timing Figure 4. Timing uncertainty in sourcesynchronous wave pipelining (SSWP) received data (prob. density) received data (peak times) destination depth source Figure 5. Statistical timing uncertainty proper modeling the link reliability is sensitive to these circuit design details. For these reasons, we do not consider GSWP any further in this paper. 3.1 Globally synchronous wave pipelining 3.2 Source-synchronous wave pipelining Figure 3 illustrates timing uncertainty in GSWP, wavepipelined systems that use a global clock (similar to a ﬁgure in [17]). Timing constraints can be derived by bounding arrival times at the destination. Both forms of timing uncertainty, static and dynamic, must be considered. To sample the data reliably, setup and hold constraints must be met on each global clock edge. Due to data transport latency, there is no guarantee that the data arrives exactly aligned with the global clock edges, so phase alignment circuitry must compensate for any skew between the global clock and received data. Dynamic timing variation causes edges to arrive outside of their expected times, resulting in a window of uncertainty that narrows the data eye opening. The clock rate must be chosen slow enough to keep the eye aligned with the compensated clock edge. In addition, consecutive data edges must be spaced far enough apart to not interfere with each other through inter-symbol interference (ISI). Keeping the receivers sampling clock phase aligned with the incoming data represents an additional challenge for circuit design and reliability. The details of how phase alignment is done are crucial to the reliability of the circuit. Also, Figure 4 shows timing uncertainty in SSWP, a sourcesynchronous wave-pipelined link (similar to a ﬁgure in [2]). There is no global clock; instead, the receiver samples the data using a forwarded clock that is sent alongside the data. Timing constraints in this case are not derived from the global clock, but instead are derived from the relative timing between the ﬁnal received clock and data signals. The ﬁgure highlights the differences between static and dynamic timing. The static delay through a link will vary from chip to chip, but for a given link it is ﬁxed. There will still be timing uncertainty due to dynamic effects, such as crosstalk and power supply noise, but the mean relative timing between edges (either between data and clock edges, or between consecutive edges on the same wire) does not change from cycle to cycle. Two scenarios are illustrated; the ﬁrst shows a fast chip, while the second shows a slow chip. Despite the wide range in mean arrival times, the relative timing between consecutive edges and between data and clock edges is the same regardless of the static timing. Because clock and data paths travel on very similar paths, they experience very similar delays. However, the Random aggressor Source Input shaping stage Data Clk Data out Clk out Load stage 1.1 ) V ( e g a t l o V 1 0.9 stdev=15mV stdev=30mV stdev=45mV Random aggressor 0.8 1 1.5 2 2.5 3 3.5 Time (ns) 4 4.5 5 5.5 6 Figure 6. Crosstalk simulation circuit (a) VDD noise waveforms Delay variation due to crosstalk Delay variation due to crosstalk y t i s n e d y t i l i b a b o r P 0.3 0.2 σ=12ps 0.1 0 100 150 Stage latency (ps) 200 y t i s n e d y t i l i b a b o r P 0.3 µ=162ps 0.2 σ=1.74ps 0.1 0 100 150 Stage latency (ps) 200 (a) No shielding (b) Full shielding Figure 7. Delay variation due to crosstalk paths will not, in general, be identical due to OPC differences, difference in neighboring layout structures, datapattern dependent delays, etc., resulting in one path being slightly faster or slower than the other. Furthermore, these differences may include systematic or correlated effects that apply at every stage, resulting in static skew that accumulates linearly with link length. Unlike GSWP, reliability is independent of static timing variation in SSWP. However, dynamic timing variations will still affect the timing spread of each individual bit just as for SSWP, inﬂuencing both reliability and throughput. 3.3 Statistical timing model The timing diagrams in the previous section are a convenient way to visualize timing uncertainty. At ﬁrst glance, they imply a worst-case, bounded timing model. For example, with SSWP we simply need to bound the arrival times to guarantee: (1) clock and data arrival times do not overlap, and (2) consecutive data and clock edges are spaced far enough apart such that they don’t interfere. Bounded models are by their nature conservative, especially for long wave pipelined links in which timing uncertainty can accumulate. The bounded model can only say if a circuit is safe given a worst-case noise estimate. We believe a probabilistic noise model, shown in the top line of Figure 5, is more useful in this context, since it can be used to derive the probability of failure for a given amount of injected timing uncertainty. This allows us to compare the robustness of different pipelining techniques when subjected to the same input noise conditions. To do so, we model the skew as a random variable S which is normally distributed with zero mean and unknown Vdd1 ~ N (µ, σ )2 Vdd2 ~ N (µ, σ )2 Vdd3 ~ N (µ, σ )2 Data source din clkin Wave pipelined stage d1 clk1 Wave pipelined stage d2 clk2 Wave pipelined stage dout clkout (b) Test circuit used to measure delay Figure 8. Experimental setup to measure delay impact of VDD noise standard deviation σs . The jitter, which affects consecutive edge separation, is modeled as a random variable E with zero mean and unknown standard deviation σe . In Section 4, HSPICE measurements are used to estimate these values. Section 5 then uses these random variables in a statistical timing model to show how the bit error rate of a link may be estimated as a function of the data period. 4 Quantifying dynamic timing uncertainty The goal of this section is to determine the delay impact of the two biggest sources of dynamic timing uncertainty, crosstalk and supply noise, in SSWP. The delay impact of each source is measured from HSPICE simulations in a CMOS 65nm process and approximated as a normal distribution to estimate the standard deviations of jitter and skew, σe and σs , respectively. In addition, we demonstrate that timing uncertainty accumulates with link length. 4.1 Crosstalk Crosstalk is often modeled with changes in the coupling capacitance between two neighboring wires, which is a function of the voltage on each wire. This model is useful for producing worst-case bounds on crosstalk-induced delay variation. Section 3 discussed why probabilistic bounds are more useful; this is true when discussing crosstalk as well if there are many possible sources of crosstalk and transition times are not known ahead of time, which is the case if the link can be circuit switched. 4.1.1 Simulation setup Probability distributions for crosstalk-induced delay were estimated by applying random data onto neighboring ag          gressor wires as shown in Figure 6. The latency through one wave-pipelined stage from the clock input to clock output was measured about 10,000 times using different random aggressor data. All signal wires including aggressors are twice minimum width. In the unshielded case, all wires are spaced apart by twice the minimum spacing, while in the shielded case, minimum-width shields are inserted between each pair of signal wires at the minimum spacing. All wires are assumed to be in one of the middle metal layers. Coupling capacitances are determined from an HSPICE ﬁeld solver using process data; second-order coupling capacitances (i.e. from a signal wire through a shield to the next signal wire) are included, and account for about 3% of total capacitance. The data wire carries a 16-bit pattern while the clock wire has an edge corresponding to each bit. 4.1.2 Results The resulting delay histograms are shown in Figure 7. The curves are not normally distributed because of deterministic coupling between wires. Also, a slight mismatch between rising and falling edges leads to double-peaked behavior. When the behavior is ﬁt to normal curves, the standard deviation of the delay is σe = 12ps for the unshielded case, while in the shielded case σe = 1.74ps. The reduction due to shielding is sufﬁcient to suggest that shielding should always be employed for wave-pipelined interconnect. 4.2 Supply noise There are many ways to model supply noise. Some models include slow sinusoids in the 100–500MHz range [4] to model resonance in the LC circuit formed by the power grid and the lead inductance. One study of ASICs measured power supply noise and found a mixture of deterministic noise caused by the clock signal and its harmonics, random cyclostationary noise caused by switching logic, and random high frequency white noise [1]. A recent study suggests that decoupling capacitors can remove this high frequency noise, so the supply should be considered a constant DC voltage [6]; we believe this to be unrealistic and include high frequency noise in our simulations. In the context of high-speed wave-pipelined links, slowly varying or constant changes in supply noise will not impact the dynamic (i.e., cycle-to-cycle) timing; instead they will affect the static timing. To assess the static and dynamic effects independently, supply noise will be modeled as the sum of a nominally ﬁxed DC component and a fast transient component. The transient noise is assumed to be a memoryless random process which is normally distributed and changes value every 100ps; this rate was chosen because it has a strong impact on cycle-to-cycle delay at the bit rates in this paper. The mean or DC level, µ, is nominally 1.0V; noting that low supply voltages limit performance more than ) y c n e t a l f o % ( σ r e t t i J 10 8 6 4 2 0 0 ) y c n e t a l f o % ( σ r e t t i J 10 8 6 4 2 0 0 0.05 0.1 0.15 Mean Vdd drop (V) 0.2 20 40 Supply σ (mV) 60 (a) DC noise response (σ = 15mV, µ =variable) (b) Transient noise response µ = 0.95V , σ =variable) Figure 9. Delay variation due to supply noise high supply voltages, our analysis focuses on DC voltage levels below the nominal. The standard deviation σ is left as a parameter. Figure 8(a) shows example power supply waveforms at µ = 0.95V DC and σ = 15mV, 30mV, and 45mV; supply voltages like these are applied in circuit simulations. Note that σ = 45mV leads to 3σ variations of ±0.135V , or ±13.5%, which is more pessimistic than typical bounds of ±10%. 4.2.1 Simulation setup Simulations were conducted to measure the impact of DC and transient supply noise on delay. A multi-stage link is constructed and the stage latency is measured over several thousand trials. Figure 8(b) shows the measurement circuit. Each stage has an independently varying supply voltage, but all supply voltages have the same distribution parameters, with mean or DC value µ and standard deviation σ . To measure the impact of transient noise, the DC value is ﬁxed at 0.95V, and transient noise ranging from σ = 0mV to σ = 60mV is applied. To measure the impact of DC noise, a small ﬁxed amount of transient noise (σ = 15mV) is applied, and DC voltage is varied from 1.00V to 0.80V. 4.2.2 Results At each supply voltage tested, the delay measurements are plotted as histograms and ﬁt to normal curves (not shown due to space constraints; the normal distributions ﬁt quite well, which is unsurprising because the input supply noise was also normally distributed.) Figure 9 shows a summary of the histogram data; the standard deviation, σe , of the jitter, is plotted against the amount of added noise. The curves are plotted as a percentage of the stage latency, which is 165ps at VDD = 0.95V . In this case, a σe = 5% jitter corresponds to σe = 8.3ps per stage. The trend lines clearly show that jitter increases steadily with applied transient noise but is relatively insensitive to changes in DC value. Slow changes in the DC voltage level will thus have relatively little impact on cycle-to-cycle jitter.                 4.3 Jitter and skew propagation If a normally distributed timing uncertainty with standard deviation σe is applied at each stage, then the uncertainty at stage k should be σe√k ; we are pessimistically assuming the timing uncertainty is independent at each stage. The previous simulations measured the uncertainty at one stage; simulations in this section measure it for eight-stage links and extrapolate out to ﬁfty-stage links in order to conﬁrm this assumption. We limited our simulations to eight stages because of the large simulation times for performing thousands of trials. Instead of measuring the latency through a stage as was done when preparing Figure 9, we will now directly measure the separation of consecutive edges and the skew between the strobe and the data. Because these measurements are the difference between two varying edges, the measured standard deviation will be p(2) times larger than what was computed in Figure 9. 4.3.1 Simulation setup The simulation setup is similar to the setup in Figure 8, except a longer link with 9 stages is simulated. The goal of this simulation is to measure the skew and jitter at the output of each of the ﬁrst 8 stages to determine the relationship with respect to link length. The ninth stage provides a load for the output of stage 8. We ran one hundred trials with each trial containing sixteen measurements. Ideally more trials would be run, but this would require longer simulation times than we could complete. 4.3.2 Results The jitter and skew measurements produce histograms with a normal distribution at a certain mean and standard deviation (not shown). Mean skew and jitter is constant, but the standard deviation varies both with the amount of noise applied and with the length of the link. Figure 10 shows the standard deviation of jitter and skew. Simulation data is marked on the graph with a thick line,. At very small levels of noise, the curves are jagged because disparities between rising and falling edges are the dominant source of skew and jitter. The simulation data is also ﬁt to curves of the form y = A√x + B , where x is the number of stages. The curves show a good ﬁt to the simulation data and we extrapolate them out to 50 stages as shown with dashed lines. Table 1 gives the standard deviations (A in the equation; the B terms are small). For comparison, the table includes the jitter measurements from Figure 9, which have been scaled by √2 because they measured uncertainty for one edge, not the difference between two edges. 200 150 100 50 ) s p ( σ n o i t a r a p e s e g d e e v i t u c e s n o C 0 0 200 150 100 50 ) s p ( σ n o i t a r a p e s a t a d − k c o l C 0 0 Wave pipelined extrapolated jitter Vdd σ=0 Vdd σ=15mV Vdd σ=30mV Vdd σ=45mV Vdd σ=60mV 10 20 30 Number of stages 40 50 (a) Wave-pipelining jitter Wave pipelined extrapolated skew Vdd σ=0 Vdd σ=15mV Vdd σ=30mV Vdd σ=45mV Vdd σ=60mV 10 20 30 Number of stages 40 50 (b) Wave-pipelining skew Figure 10. Jitter and skew propagation 4.4 Summary This section decried techniques for quantifying dynamic timing uncertainty due to crosstalk and fast supply voltage noise in terms of their standard deviations, assuming they are normally distributed. If the timing uncertainty applied to each stage has a standard deviation of σ , the uncertainty at stage i is shown to have a standard deviation of σ√i. From Table 1, we also notice that σe ≈ 1.8σs . Hence, jitter is larger in magnitude than skew. Table 1. Standard deviations of timing uncertainty Supply σ (mV) 15 30 45 60 √2) Predicted σe (Fig 9× 5.5 9.2 13.3 18.0 √2) Units in ps Extrap. σe (Fig 10a× 5.7 10.7 14.8 21.5 √2) Extrap. σs (Fig 10b× 2.7 5.8 9.3 11.0                       5 Estimating reliability Knowledge of the standard deviation of the dynamic uncertainty allows us to estimate link reliability. Table 2 lists the parameters and variables used in this analysis. There are two timing constraints that must be met as illustrated in Figure 11. First, the separation between two consecutive edges (nominally equal to the period, T ) must be greater than some minimum edge separation; otherwise, clock or data events could be lost. The minimum edge separation, tsep was measured from simulation using the approach in [15] and was found to be about 160ps in the worst case. Second, the time separation between the data edge and the clock edge, which is nominally equal to half the period, must be larger than the setup time at all latches in the link. Using a simple logical effort [14] model, the latch setup time was estimated to be about 20ps. We deﬁne PE ,I to be the probability of an error due to intersymbol interference resulting from a violation of the minimum edge separation, and PE ,L to be the probability of an error due to incorrect sampling at any latch. A successful transmission requires that neither error condition occur. We can thus deﬁne PE , the overall probability of error, to be probability of an error due to ISI or due to sampling failure, so that PE = PE ,I + PE ,L − PE ,I Symbol Name Value Source Table 2. List of parameters and variables tsep tstage tsetup tlatch tskew T n q PE ,L PE ,I PE σ σe σS σS2 S E E2 Min. consecutive edge separation Stage latency Latch setup time Latch latency Global clock skew Data period Number of stages between latches Number of stages in link Probability of error due to failed sampling Probability of error due to intersymbol interference Probability of error, overall Standard deviation of supply noise Standard deviation of dynamic jitter Standard deviation of dynamic skew Standard deviation of static skew Dynamic skew (normal random variable) Dynamic jitter (normal random variable) Static skew due to variation (normal random variable) 160ps 160ps 20ps 50ps 10ps 160ps to 1ns+ 1 to 50 1 to 50 10−25 to 1 10−25 to 1 10−25 to 1 0 to 60 mV 0 to 160ps 65nm HSPICE simulation 65nm HSPICE simulation Estimate/logical effort Estimate/logical effort Estimate [9] Calculation 50 stages = 25mm cross-chip link 50 stages = 25mm cross-chip link Calculation Calculation PE ,I + PE ,L − PE ,I · PE ,L Model assumption HSPICE simulation (Fig. 10a) HSPICE simulation (Fig. 10b) 2% link latency Model assumption Model assumption Model assumption Model assumption ≈ σe /1.8 N (0, σS ) N (0, σe ) N (0, σS2 ) ation. We can use another random variable, S2 , to represent this skew, which we assume has zero mean and is normally distributed. In that case, we have PE ,L = P (T /2 > S + S2 + tsetup ). The dynamic uncertainty S at stage k has zero mean and a standard deviation of σS √k . If we assume at worst a 2% change in latency per stage, then the static skew S2 at stage k has a standard deviation of σS2 = 0.02 · tstage · k . Because the two variables are independent, the overall skew at stage k (i.e. the sum of S + S2 ) has zero mean and a standard deviation of pσ2 S k + (0.02 · tstage · k)2 ). The probability of error, PE ,L , can then be calculated using the formula above. We must also consider PE ,I , the probability of an error due to intersymbol interference. Nominally, the edge separation is T , the data period; we require simply that T > tsep . If we add dynamic timing uncertainty E , then the probability of error is PE ,I = P (E < (T − tsep )). At stage k , E has zero mean and standard deviation σe√k . The overall probability of error, PE , is PE ,I +PE ,L −PE ,I ·PE ,L as discussed above. For a small number of stages, say less than 30, we note that PE ,I dominates the overall probability of error. However, for longer links, the linear skew term causes PE ,L to dominate. In SSWPL, latches are periodically inserted to remove skew. To keep our model simple, we reset the skew term to 0 after a latch is encountered, so that PE ,L is similar to the GSLP case; however, jitter accumulates throughout the whole link, as in the SSWP case. In such cases, the jitterinduced failure captured in PE ,I dominates. 5.2 Results This section presents the reliability and throughput of wave pipelining and latch pipelining as a function of q , the total link length, n, the number of stages between latches, and σe , the dynamic jitter. Dynamic skew, σs , is set to σe/1.8 to follow the results in Table 1. Figure 13 shows the error probability as a function of throughput at a ﬁxed link of length q = 10 stages. With no noise (σ = 0), wave pipelining achieves 6.2Gbps and latch pipelining reaches 4.8Gbps. As noise is added, the achieved throughput for a given error rate very quickly drops in SSWP. However, GSLP is much more resilient. Also, there is little improvement from the additional latches in SSWPL showing that the wave pipelined designs are limited primarily by loss of edge separation rather than by skew. Figure 14 shows the error probability as a function of the link length q at a ﬁxed noise level of σe = 10ps. In this case, SSWPL outperforms SSWP when the number of stages is large, because the linear skew term is reset in SSWPL, but grows to dominate in SSWP. The GSLP graph demonstrates the expected result that throughput drops quickly as latches are placed farther apart. Figure 12(a) presents the same throughput data at a ﬁxed probability of error of P (E ) = 10−25 with no dynamic noise. In this case, wave pipelining has much higher throughput than latch pipelining. This result agrees with most previous work which claims that wave pipelining offers superior bandwidth. However, we note that linear skew term (a static effect) eventually causes SSWP performance to drop when the link gets too long. With noise, however, the situation changes dramati0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r σ e =0 σ e =5ps σ e =10ps σ e =15ps σ e =20ps (a) SSWP 0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r σ e =0 σ e =5ps σ e =10ps σ e =15ps σ e =20ps (b) SSWPL, latch every 5 stages 0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −30 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r σ e =0 σ e =5ps σ e =10ps σ e =15ps σ e =20ps (c) GSLP, latch every stage Figure 13. Probability of error estimates, ﬁxed link of 10 stages (σe varied, σs = σe/1.8). 0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −30 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r 1 stage 2 stages 5 stages 10 stages 25 stages 50 stages (a) SSWP 0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −30 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r 1 stage 2 stages 5 stages 10 stages 25 stages 50 stages (b) SSWPL, latch every 5 stages 0 1 2 3 4 Throughput (Gbps) 5 6 7 10 −30 10 −25 10 −20 10 −15 10 −10 10 −5 10 0 E s i t m a t d e p r b a b o t i l i y o f e r r o r 1 stage 2 stages 5 stages 10 stages 25 stages 50 stages (c) GSLP, only one latch at the end of link Figure 14. Probability of error estimates, ﬁxed noise (σe = 10ps, σs = 5.5ps per stage) cally. Figure 12(b) presents the same throughput data with σ = 10ps of noise. Throughput degrades for both wave pipelining and latch pipelining. However, latch pipelining throughput remains independent of the link length, while wave pipelining throughput degrades rapidly. This result indicates latch pipelining throughput is superior to wave pipelining in the presence of noise. It is an open question as to how much dynamic timing uncertainty is realistic. These results are probably quite pessimistic due to the very large amounts of supply noise added. Nevertheless, these results show that designing without taking dynamic noise into account may lead to expectations for high throughput and robustness that are unachievable on a real chip. 6 Conclusions Wave pipelining has attracted attention of NoC researchers due to its promise of high bandwidth using simple circuits and relatively low power. However, wave pipelining is much more susceptible to timing uncertainty than traditional interconnect pipelining techniques based on latches or ﬂip-ﬂops. This is because timing uncertainty accumulates across the entire link for a wave-pipelined design; whereas for a synchronous link, the uncertainty is reset or partially reset at each ﬂip-ﬂop or latch. Statistical models of timing uncertainty are required to obtain a realistic assessment of the impact of timing uncertainty on wave-pipelined interconnect because using simple simple, worst-case assumptions at each stage produces a highly pessimistic model. We used HSPICE simulations to develop such a model for a switched interconnect in a 65nm CMOS process and then used this model to analyze the throughput achievable with wave-pipelined and latchpipelined interconnect. When dynamic timing uncertainty was ignored, the wave-pipelined interconnect offers about 50% greater bandwidth than traditional synchronous designs. When jitter from noise is taken into account, wavepipelining rapidly loses its advantage and the traditional synchronous design then offers higher bandwidth. The limiter for the wave-pipelined design was loss of the strobe pulses due to jitter; thus, inserting latches clocked by the forwarded strobe to the wave-pipelined design did not improve its performance. Finally, our analysis shows the importance of quantifying interconnect performance at realistic bit-error-rates (BERs). For BERs of 10−3 , about the limit of what is practical to observe with circuit simulators such as HSPICE, wave pipelining offers much higher throughput than what can be achieved at more useful BERs of 10−20 or 10−25 .                                                             ) s p b G ( t u p h g u o r h T ) s p b G ( t u p h g u o r h T 7 6 5 4 3 2 1 0 0 7 6 5 4 3 2 1 0 0 GSLP (L every stage) SSWPL (L every 5 stages) SSWP GSLP (L every 5 stages) 10 20 30 Link length (# of stages) 40 50 (a) No dynamic noise GSLP (L every stage) SSWPL (L every 5 stages) SSWP GSLP (L every 5 stages) 10 20 30 Link length (# of stages) 40 50 (b) Noise σ=10ps Figure 12. Throughput versus link length at a ﬁxed probability of error, P (E ) = 10−25 For synchronous pipelining, the loss of performance with decreasing BER is much less severe than for wave pipelining. It may be possible to mitigate these limitations of wavepipelined interconnect by using error-correcting codes, but this would increase the complexity, area, power and latency of the link. Alternatively, our work motivates looking for design techniques that mitigate the accumulation of dynamic timing uncertainty for wave-pipelined interconnect. We see this as an important area for further research. 7 Acknowledgements We would like to thank Suwen Yang, Andy Ye, Terrence Mak, Alastair Smith, the Actel architecture team including Jonathan Greene and Sinan Kaptanoglu, and the anonymous reviewers for valuable feedback. This work was funded by NSERC. "
A GALS many-core heterogeneous DSP platform with source-synchronous on-chip interconnection network.,"This paper presents a many-core heterogeneous computational platform that employs a GALS compatible circuit-switched on-chip network. The platform targets streaming DSP and embedded applications that have a high degree of task-level parallelism among computational kernels. The test chip was fabricated in 65nm CMOS consisting of 164 simple small programmable cores, three dedicated-purpose accelerators and three shared memory modules. All processors are clocked by their own local oscillators and communication is achieved through a simple yet effective source-synchronous communication technique that allows each interconnection link between any two processors to sustain a peak throughput of one data word per cycle. A complete 802.11a WLAN baseband receiver was implemented on this platform. It has a real-time throughput of 54 Mbps with all processors running at 594 MHz and 0.95 V, and consumes an average 174.76 mW with 12.18 mW (or 7.0%) dissipated by its interconnection links. We can fully utilize the benefit of the GALS architecture and by adjusting each processor's oscillator to run at a workload-based optimal clock frequency with the chip's dual supply voltages set at 0.95 V and 0.75 V, the receiver consumes only 123.18 mW, a 29.5% in power reduction. Measured results of its power consumption on the real chip come within the difference of only 2-5% compared with the estimated results showing our design to be highly reliable and efficient.","A GALS Many-Core Heterogeneous DSP Platform with Source-Synchronous On-Chip Interconnection Network Anh T. Tran, Dean N. Truong, and Bevan M. Baas Department of Electrical and Computer Engineering University of California - Davis, USA fanhtr, hottruong, bbaasg@ucdavis.edu Abstract This paper presents a many-core heterogeneous computational platform that employs a GALS compatible circuit-switched on-chip network. The platform targets streaming DSP and embedded applications that have a high degree of task-level parallelism among computational kernels. The test chip was fabricated in 65nm CMOS consisting of 164 simple small programmable cores, three dedicated-purpose accelerators and three shared memory modules. All processors are clocked by their own local oscillators and communication is achieved through a simple yet effective source-synchronous communication technique that allows each interconnection link between any two processors to sustain a peak throughput of one data word per cycle. A complete 802.11a WLAN baseband receiver was implemented on this platform. It has a real-time throughput of 54 Mbps with all processors running at 594 MHz and 0.95 V, and consumes an average 174.76 mW with 12.18 mW (or 7.0%) dissipated by its interconnection links. We can fully utilize the bene(cid:2)t of the GALS architecture and by adjusting each processor’s oscillator to run at a workload-based optimal clock frequency with the chip’s dual supply voltages set at 0.95 V and 0.75 V, the receiver consumes only 123.18 mW, a 29.5% in power reduction. Measured results of its power consumption on the real chip come within the difference of only 2-5% compared with the estimated results showing our design to be highly reliable and ef(cid:2)cient. 1. Introduction Fabrication costs for state-of-the-art chips can now easily exceed several million dollars; and design costs associated with ever-changing standards and end user requirements are also extremely expensive. In this context, programmable and/or recon(cid:2)gurable platforms that are not (cid:2)xed to a single application or a small class of applications become increasingly attractive. The power wall limits the performance improvement of conventional designs exploiting instruction-level parallelism that rely mainly on increasing clock rate with deeper pipelines. Many new techniques and architectures have been proposed in the liter978-1-4244-4143-3/09/$25.00 ©2009 IEEE  ature; and multiple-core designs are the most promising solution among them [1]. Recently, a large number of multi-core designs were found in both industry and academia [2(cid:150)5]. Also, recon(cid:2)gurable and programmable many-core designs for DSP and embedded applications are becoming popular research topics [6(cid:150)8]. Transistor density and integration continue to scale with Moore’s Law, and for practical digital designs, clock distribution becomes a critical part of the design process for any high performance chip [9]. Designing a global clock tree for a large chip becomes very complicated and it can consume a signi(cid:2)cant portion of the power budget, which can be up to 40% of the whole chip’s power [10]. One particular method to address this issue is through the use of globally-asynchronous locally-synchronous (GALS) architectures where the chip is partitioned into multiple independent frequency domains. Each domain is clocked synchronously while inter-domain communication is achieved asynchronously [11]. GALS, therefore, becomes a top candidate for multi- and many-core chips that wish to do away with complex global clock distribution networks. In addition, GALS allows the possibility of (cid:2)ne-grained power reduction through frequency and voltage scaling [12]. The method of inter-domain communication is a crucial design point for GALS architectures. One technique is asynchronous clockless handshaking, which uses multiple phases of signal (i.e. request/send/valid/ack) exchange to transfer data. Due to the round-trip signal exchange, the transferring latency between two consecutive data words is high. Besides that, the asynchronous clockless circuits are dif(cid:2)cult to verify in traditional CAD (cid:3)ows, and they also demand a comparatively large area and energy requirement [13, 14]. An alternative is sourcesynchronous clocking, commonly used in off-chip communication, whose design only requires a sender’s clock signal to be sent with the sender’s data to the receiver. For synchronization, a dual-clock FIFO at the receiver is used to buffer the data between two clock domains with the FIFO’s write side clocked by the sender while its read side is clocked by the receiver. This method achieves high ef(cid:2)ciency by obtaining a peak throughput of one data word per cycle with low area and power costs [15, 16]. In this paper, we present the design of a GALS many-core computational platform utilizing a source-synchronous communication architecture. In order to evaluate the ef(cid:2)ciency of this platform and its interconnection network, we mapped a com  plete 802.11a WLAN baseband receiver on this platform. Actual chip measurement results are reported, analyzed, and compared against simulation. The outline of the paper is organized as follows. Section 2 explains our motivation for designing a GALS many-core heterogeneous DSP platform. The design of our computational platform is described in Section 3. Section 4 presents the architecture of our recon(cid:2)gurable high-throughput GALScompatible circuit-switched inter-processor communication network. The implementation and measurement results of the test chip are shown in Section 5. Mapping, analyzing and measuring the performance and power consumption of an 802.11a baseband receiver on this platform is discussed in Section 6. Finally, Section 7 concludes this paper. 2. Motivation Our design is highly scalable and consists of a large array of small (cid:2)ne-grained cores plus dedicated-purpose accelerators, forming a GALS many-core heterogeneous platform that targets DSP, multimedia and embedded workloads motivated by the following key observations. 2.1. High Performance with Many›Core Design Pollack’s Rule states that performance increase of an architecture is roughly proportional to the square root of its complexity [12]. This rule implies that if we try to apply many sophisticated techniques to a single processor and make its logic area double, we only speedup its performance by around 40%. On the other hand, with the same area increase, a dual-core design using two identical cores could achieve a 2x improvement assuming that applications are 100% parallelizable. With the same argument, a design with many small cores should have more performance than one with few large cores for the same die area. However, performance increase is heavily hindered by Amdahl’s Law, which implies that this speedup is strictly dependent on the application’s inherent parallelism: Speedup (cid:25) 1 (1 (cid:0) Parallel%) + 1 N (cid:1) Parallel% (1) where N is number of cores. Fortunately, for most applications in the DSP and embedded domain, a high degree of task-level parallelism can be easily exposed [6]. By partitioning the natural task-graph description of an embedded application, where each task can easily (cid:2)t into one or a few small processors, the complete application will run much more ef(cid:2)ciently. This is due to the elimination of unnecessary memory fetching and complex pipeline overheads. In addition, the tasks themselves run in tandem like coarse pipeline stages. 2.2. Power Savings through GALS Clocking Style Since each core is in its own frequency domain, we are able to reduce the power dissipation and increase energy ef(cid:2)ciency on a (cid:2)ne-grained level as illustrated in Fig. 1 in many ways: Accelerator 1 Shared Memory Accelerator 2 Figure 1. Illustration of a GALS many-core heterogeneous system consisting of many small identical processors, dedicated-purpose accelerators and share memories. (cid:15) GALS clocking design allows to utilize simple local ring oscillator for each core, and hence eliminates the need of complex and power hungry global clock trees [10]. (cid:15) Unused cores can be effectively disconnected by power gating, and thus reducing leakage. (cid:15) When workloads distributed for cores are not identical, we can allocate different clock frequencies and supply voltages for these cores either statically or dynamically. This allows the total system to consume a lower power than if all active cores had been operating at a single frequency and supply voltage [17]. (cid:15) We can reduce more power by architecture-driven methods such as parallelizing or pipelining a serial algorithm over multiple cores [18]. (cid:15) We can also spread computationally intensive workloads around the chip to eliminate hot spots and balance temperature. From the advantages on both performance and power consumption above, clearly, a many-core GALS design style is highly desirable for programmable/recon(cid:2)gurable DSP platforms. 2.3. High Ef(cid:2)ciency from Heterogeneous Architecture For many tasks that have computationally intensive requirements such as error control coding/decoding, security encryption/decryption, FFT/IFFT, video motion estimation, etc., which do not map well to a set of (cid:2)ne-grained cores, one compromise is to build dedicated-purpose accelerators for tasks that are commonly found in many embedded, multimedia, and DSP applications. These accelerators are then integrated into the rest of the GALS array of identical processors to form a heterogeneous many-core platform. This approach is found in many designs such as high-speed SDR platforms [19, 20], and modern multicore GPUs. Supply  Voltages Controller Osc. CORE Datapath Comm. Circuit Figure 2. Common block diagram of processors or accelerators in our heterogeneous system. The main differing component among these processors is their computational cores. serial configuration bit-stream test out Configuration and Test Logic input  mux select input data, valid and  clock input request Motion Estimation to analog pads Viterbi Decoder FFT 16 KB Shared Memories output mux select output data,  valid and  clock output request DVFS VFC Osc Core Comm Figure 3. Block diagram of the 167-processor heterogeneous computational platform 3. Design of Our Programmable Heterogeneous GALS Many-core Platform We implemented the many-core platform using a standard-cell design (cid:3)ow. Because of the array nature of the platform, the local oscillator, voltage switching, con(cid:2)guration and communication circuits are reused throughout the platform. These common components are designed as a generic (cid:147)wrapper(cid:148) which could then be reused to make any computational core compatible with the GALS array, and thus allowing easy design enhancements. The difference between the programmable processors and the accelerators is mainly in their computational datapaths as illustrated in Fig. 2. The top level block diagram of our 167-processor computational platform is shown in Fig. 3. The platform consists of 164 small programmable processors, three accelerators (FFT, Viterbi decoder and Motion Estimation), and three big shared memory modules. All processors, accelerators and memory modules operate at their own clock frequency and share multiple global VddHigh VddLow VddOsc VddAlwaysOn Volt. &  Freq. Controller control_high control_low control_freq VddCore config and status from core Osc Comm. Circuit GndOsc GndCom Figure 4. The Voltage and Frequency Controller (VFC) architecture power supply voltages. Their clock frequency and supply voltage can be set statically or dynamically by their local voltage and frequency controllers (VFC). In this section, we brie(cid:3)y present the design of the processors, accelerators, shared memory modules of the system, and the local VFC architecture. The communication network for these processors and accelerators will be discussed in Section 4. 3.1. Programmable Processors, Recon(cid:2)gurable Acceler› ators and Shared Memories Processors utilize an in-order single-issue six-stage pipeline with a 16-bit (cid:2)xed-point ALU and a 40-bit MAC. Each processor has a local 128x35-bit instruction memory and a local 128x16-bit data memory. It supports more than 60 basic instructions. The Viterbi and FFT accelerators, which are computationally intensive in high-speed communication systems, and thus are included in the platform. The Viterbi can decode convolution codes up to a constraint length 10 and the FFT is capable of performing 16- to 4096-point FFT/IFFT transformations. The platform also has a motion estimation processor typically used for video processing applications. Furthermore, the platform contains three 16-KB shared memory modules used for applications that require a shared set of data memory among different kernels and/or a local stored cache. The accelerators and memories are highly con(cid:2)gurable depending on user requirements. 3.2. Per›Processor Clock Frequency and Supply Voltage Con(cid:2)guration Each processor has its own ring-oscillator that can be con(cid:2)gured to operate over a wide range of frequencies from 5 MHz to 1.7 GHz. At runtime, if the processor is idling the clock oscillator fully halts after 6 cycles; and it restarts immediately once work becomes available. In order to achieve energy-ef(cid:2)ciency, the processor should be supplied at the appropriate voltage level corresponding to its current operating frequency. This means we need to supply different frequencies and voltages to different processors depending on their workloads. Because on-chip DC-DC converters have high design cost (complexity and area) and also large voltage switching delay [21], we use multiple global external supply voltages with hierarchical power grids that are simple and ef(cid:2)cient [22, 23]. The core of each processor is con(cid:2)gured to connect to one of two supply voltages VddH igh or VddLow or fully disconnected if unused. The bene(cid:2)ts of having more than two supply voltages are small when compared to the increased area and complexity of the controller needed to effectively handle voltage switching [24]. The frequency and supply voltage of each processor are controlled dynamically or statically by its VFC as shown in Fig. 4. Dynamic control is based on the workload of processor that is derived from its input FIFO utilization status. Static con(cid:2)guration is intentionally set by the programmer for a speci(cid:2)c application to optimize its performance and power consumption. This con(cid:2)guration is useful for many DSP applications that have static load behavior at runtime. To avoid the effect of noise caused by voltage switching, the oscillator is powered by its own voltage and ground. The VFC and communication circuit also have their own voltage that is shared among all processors in the platform to guarantee the same voltage level for all interconnection links, thus only requiring level conversion to and from the processor core. 4. GALS Compatible Source-Synchronous InterProcessor Communication Network The communication circuit of each processor is also a part of the generic (cid:147)wrapper(cid:148) as mentioned previously (Fig. 2). This section describes the design of the communication network using source-synchronous interconnection technique across clock domains. 4.1. Recon(cid:2)gurable High›Speed Circuit›Switched Inter› connection Network Figure 5 depicts the interface between any two neighboring processors in the platform. Each processor communicates with other processors through its two switches. Each switch has (cid:2)ve ports: the Core port which is connected to its local core, and the North, South, West, and East ports which are connected to its four nearest processors’ switches. As shown in the (cid:2)gure, an input from the West port of one switch can be con(cid:2)gured to go out to any port among the Core, North, South, East ports and vice versa. For simplicity, Fig. 5 only shows the full connections to and from the West port of one switch; all its other ports are connected in a similar fashion. Connections of these switches form two separate networks such that one processor can send data to any of the eight directions and can receive data from any two directions through its two input FIFOs. The multiplexers of each switch are con(cid:2)gured pre-runtime which (cid:2)xes the communication link between any two processors. Thus, the circuit-switched link is guaranteed to be independent and never shared. So long as the destination processor’s FIFO Osc Osc Core FIFO Core FIFO Processor Core t s e W Switch North South Switch Processor t s a E clock data valid request Figure 5. Interconnection network architecture with sourcesynchronous communication technique. clock mux data mux configurable delay circuit dual-clock FIFO Osc Osc Osc from Core Proc. A Proc. B Proc. C to Core Figure 6. Example of a long-distance source-synchronous communication through one intermediate processor is not full, a one data word per cycle throughput can be sustained. This compares favorably to a packet-switched network whose runtime network congestion can signi(cid:2)cantly degrade its communication performance [25, 26]. Our interconnection network’s architecture is well suited for DSP applications that have high-speed interconnect requirements (cid:2)xed at runtime. 4.2. Communication Reliability Figure 6 shows an example of a communication link that is con(cid:2)gured to connect two long-distance processors. This link passes through one intermediate processor, Proc. B, which is in between the source and destination processors, Proc. A and Proc. C. The (cid:2)gure also shows both clock and data being multiplexed by the circuit-switched architecture. The destination processor (Proc. C) uses a dual-clock FIFO to buffer the received data before processing. Its FIFO’s write port is clocked by the source clock of Proc. A, while its read port is clocked by its own oscillator, and thus supports GALS communication [27]. Data and clock are sent by the source processor to the destination processor through a sequence of multiplexers of intermediate clock’s mux + wire delay configurable delay dest. clock source clock source data dest. data FIFO clock data valid clock data valid (a) (b) data’s mux + wire delay Figure 7. A simpli(cid:2)ed communication model derived from Fig. 6 at output of  source processor clock delay at write side of the  destination FIFO data delay clock delay data delay +  configured delay potential timing violation (a) good timing (b) Figure 8. Timing illustration. a) Without using con(cid:2)gurable delay circuit. b) With an appropriately con(cid:2)gured delay switches, and each data word is valid for one cycle. Fig. 7 is a simpli(cid:2)ed version of Fig. 6 focusing only on the impact of delay on source-synchronous timing. The dotted lines represents the boundary between two nearest processors. The total delay of wire and multiplexer in each processor’s switch is depicted as a delay block. As shown, the clock and data signals are sent to the destination in the same way; thus, with a careful layout, their total delay can be nearly equal. Because the delay of the clock and data is generally close, a timing violation can occur as illustrated by the waveform in Fig. 8(a). Also, the data bus can have mismatches due to variations and crosstalk, and the clock can have jitter causing unreliable communication in actual chip implementations. Instead of leaving reliable communication up to chance, we purposefully add a delay circuit before the input FIFOs of each processor. This delay circuit is con(cid:2)gurable in order for the clock rising edge to trigger within the safe timing window where the data is stable, as depicted in Fig. 8(b). This requires the delay value to be adjusted to satisfy the following timing constraint: thold < Dcon f + Ddata (cid:0) Dclk + tclk(cid:0)q < T (cid:0) t setu p (2) where, Dcon f , Ddata and Dclk are the con(cid:2)gured, data and clock delays, respectively; T is the source clock cycle time. The value of t setu p , thold and tclk(cid:0)q are mainly dependent on the standard cell design, technology process and fabrication variation; thus the value of Dcon f can be different even for two connections with the same length and the same source clock frequency. The best value of Dcon f for each link can be found through chip testing. The testing results con(cid:2)rm that all processors correctly communicate their data at 1.2 GHz (the maximum operating frequency of processors) when delay values are appropriately con(cid:2)gured; that gives a peak throughput of 19.2 Gbps per 16-bit connection link. Figure 9. Source-synchronous communication methods. a) Source clock is alway active along the connection path. b) Only sending clock when having data. Two extra active cycles after the last valid data word to increase the communication robustness. 4.3. Low›Power Communication Method Figure 9 illustrates two strategies of source-synchronous data communication from one processor to another. In the case depicted by Fig. 9(a), the clock is always active even without any accompanying data. The clock travels along the connection path from the source to destination and consumes some power for doing nothing while there is no data sent. Measurement results show that when sending clock alone without data, intermediate switches and the destination FIFO can dissipate 45% of the total power had it been sent with data (including the power dissipated by interconnect wires). Our proposed method is shown in Fig. 9(b), where the source clock is only sent when there is data to transfer. However, if we aggressively send only one cycle of clock for each data word, the data can be lost if there is a large delay mismatch between clock and data links. Thus, we add two more cycles of clock after the last valid word sent. This method compromises between the aggressive and always active methods in order to maintain the high robustness at lower power. Most importantly, the high energy-ef(cid:2)ciency of our circuitswitched communication network is achieved due to its simple switch architecture, which does not buffer at the switch’s input or output ports, and has no arbitration circuit, therefore no power is wasted for resolving runtime traf(cid:2)c congestion which is a signi(cid:2)cant portion of the power budget in dynamic packet-switched networks [28]. 5. Test Chip Implementation and Measurement 5.1. Chip Implementation The platform was fabricated in ST Microelectronics 65nm low-leakage CMOS process and its die micrograph is shown in Fig. 10. It has a total of 55 million transistors with an area of 39.4 mm2 . Each programmable processor occupies 0.17 mm2 , with its communication circuit occupying 7% including the two switches, wires and buffers. The area of the FFT, motion estimation and Viterbi decoder accelerators is six times, four times and one time, respectively that of one processor; the memory module is two times the size of one processor. 5.2. Measurement At 1.3 V, the programmable processors can operate up to 1.2 GHz. The con(cid:2)gurable FFT, Viterbi, motion estimation pro(cid:149) 65 nm STMicroelectronics  410 (cid:541)m 102 ) 4 1 0 (cid:541) m @ m m 9  3 .9 5 1.19 GHz, 1.3 V @ 66 MHz, 0.675 V 1.095 GHz, 1.3V Mot. Est. Mem Mem Mem Vit FFT 5.516 mm Figure 10. Die micrograph of the test chip. 1200 1000 800 600 400 200 ) z H M ( y c n e u q e r F x a M 0 0.6 0.7 0.8 0.9 1.0 1.1 Supply Voltage (V) 1.2 1.3 W m ( r e w o P e g a k a e L 101 100 0.6 0.7 0.8 0.9 1 1.1 Supply Voltage(V) 1.2 1.3 Figure 12. Leakage power of one programmable processor over various supply voltages Table 1. Average power consumption measured at 0.95 V and 594 MHz. Operation of Processor FFT Viterbi FIFO Write Switch 100% Active (mW) 17.6 12.7 6.2 1.9 1.1 Stall (mW) 8.7 7.3 4.1 0.7 0.5 Standby (mW) 0.031 0.329 0.153 (cid:24)0 (cid:24)0 while the clock is active) the processors and communication circuits (including wires) also consume signi(cid:2)cant portions, approximately 35-55%, of their normal operating power. Leakage power are very small while processors are in the standby mode with clock halting. ) W m ( r e w o P 60 50 40 30 20 10 0 Figure 11. Maximum frequency and 100% active power dissipation of one programmable processor over various supply voltages 6. Application Mapping: a Case Study cessors, and memory modules can run up to 866 MHz, 894 MHz, 938 MHz and 1.3 GHz, respectively. The maximum frequency and power consumption of the programmable processors versus supply voltage is shown in Fig. 11. As shown in the (cid:2)gure, they have a nearly linear and quadratic dependence on the supply voltage, respectively. These characteristics are used to reduce power consumption of an application by appropriately choosing the clock frequency and supply voltage for each processor as detailed in Section 6. Figure 12 shows the measured leakage power of processors over various supply voltages. As shown, this leakage power is exponentially dependent on supply voltage and is negligible which can be ignored when compared with the dynamic power in a real application. Table 1 shows the average power dissipation of processor, accelerators and communication circuit at 0.95 V and 594 MHz. This supply voltage and clock frequency is used to evaluate and test the 802.11a baseband receiver application described in the next section. The FFT is con(cid:2)gured to perform 64-point transformations, and the Viterbi is con(cid:2)gured to decode 1/2-rate convolution codes. Also shown in the table, during stalls (i.e. non-operation In order to relatively evaluate the performance and energyef(cid:2)ciency of the platform, we mapped and tested a real 802.11a baseband receiver. Some steps to reduce its power consumption while keeping the real-time throughput requirement are also presented. 6.1. Mapping a Complete 802.11a Baseband Receiver The receiver is complete including all necessary features of a practical one such as frame detection and timing synchronization, carrier frequency offset (CFO) estimation and compensation, and channel estimation and equalization. It consists of 23 processors plus the FFT and Viterbi accelerators as shown in Fig. 13. In this implementation, the CFO compensation uses a lookup table to compute the complex unit vector of the accumulated offset angle, and then uses a complex multiplication for sample rotation instead of using CORDIC algorithm as reported in our previous published paper [29] (all other processors are unchanged). Processors are programmed using our simple C language version combined with assembly code for con(cid:2)guration of interconnect links and also for optimization. The compiled code of the whole receiver is simulated on the Verilog RTL model of our platform using Cadence NCVerilog and its results are compared with               from ADC Data  Distribution Auto Correlation Accumulated Offset Vector  Computation CFO Compensation Guard  Removal Energy Computation Frame  Detection CORDIC – Angle Channel  Equalization Channel  Estimation Pre - Channel  Estimation Subcarrier  Reordering Timing  Synchronization CFO Estimation Demodulation Bit Rate &  Data Length Computing Descrambling Pad Removal to MAC layer Data  Distribution  Control Post - Timing  Synchronization Deinterleaving  1 Deinterleaving  2 Depuncturing :     Connections on the Critical Data Path :     Other Connections (for Control, Detection, Estimation) Viterbi Decoding (Accelerator) FFT (Accelerator) Figure 13. Mapping of a complete 802.11a baseband receiver on the many-core computational platform a Matlab model to guarantee its accuracy. By using the activity pro(cid:2)le of the processors reported by the simulator, we evaluate its throughput and power consumption before testing it on the real chip. This implementation methodology reduces debugging time and allows us to easily (cid:2)nd the optimal operation point of each task. 6.2. Receiver Critical Data Path The dark solid lines in Fig. 13 show the connections between processors that are on the critical data path of the receiver. The operation and execution time of these processors determine the throughput of the receiver. Other processors in the receiver are only brie(cid:3)y active for detection, synchronization (of frame) or estimation (of the carrier frequency offset and channel); then they are forced to stop as soon as they (cid:2)nish their job1 . Consequently, these non-critical processors only add latency to the system and do not affect the overall data throughput2 [29]. 6.3. Performance Evaluation Figure. 14 shows the overall activity of the critical path processors. In this (cid:2)gure, the Viterbi accelerator is shown to be the system bottleneck. It is always executing and forces other processors on the critical path to stall while waiting either on its output to send data or on its input to receive data3 . Therefore, the total execution time and waiting time of each processor equals to the total execution time of the Viterbi accelerator (2376 cycles) during the processing of a 4-(cid:22)s OFDM symbol. In essence, all OFDM symbols are processed by a sequence of processors on the critical path in a way that is similar to a pipeline (with 4 (cid:22)s per stage per 2376 cycles). Therefore, the receiver can obtain a real-time 54 Mbps throughput when all processors operate at the same clock frequency of 594 MHz. According to measured data, 1 Processors stop working after six cycles if their input FIFOs are empty. 2 These non-critical processors will be woken up to detect and synchronize new frame after the current frame is completely processed. The control information is provided by the Pad Removal processor. 3 This assumes that the input is always available from the ADC and the MAC layer is always ready to accept outputs. Execution Input Waiting Output Waiting 2376 ) l s e c y c ( e m i T Data Distribution Acc. Offset Vector Co m p. Post - Timing Syn. C F O Co m pensation G uard Re m oval Subcarrier Reordering 64-point FFT Channel Equalization De-m odulation De-interleavering 1 De-interleavering 2 De-puncturing Viterbi Decoding De-scra m bling Pad Re m oval Figure 14. The overall activity of processors for processing a 4 (cid:22)secOFDM symbol in the 54 Mbps mode in order for all processors operate correctly they must be supplied at the lowest voltage level of 0.95 V. 6.4. Power Consumption Estimation The overall activity of processors allows us to reasonably estimate the average power consumption of the receiver. Based on the analysis results done with simulation and estimation steps, we con(cid:2)gure the processors accordingly when running on the test chip. 6.4.1. Power Consumption on the Critical Path Power consumption of the receiver is primarily by processors on the critical path because all non-critical processors have stopped when the receiver is processing data OFDM symbols. In this time, the leakage power dissipated by these ten non-critical processors is 0.31 mW (10 (cid:2) 0:031). The total power dissipated by the critical path processors is estimated by: PT otal = X PE xe:i + X PS tall:i + X PS tandby:i + X PComm:i (3)   Table 2. Operation of processors for processing one OFDM symbol in the 54 Mbps mode, and their power consumptions Processor Data Distribution Post-Timing Sync. Acc. Off. Vec. Comp. CFO Compensation Guard Removal 64-point FFT Subcarrier Reorder. Channel Equal. De-modulation De-interleav. 1 De-interleav. 2 De-puncturing Viterbi Decoding De-scrambling Pad Removal Ten non-critical Proc.s Total Execution Time (cycles) 320 240 2320 2160 176 205 1018 1488 2352 864 1130 576 2376 2160 648 Stall with Active Clock (cycles) 960 960 56 216 768 768 576 576 24 1512 1246 1800 0 216 1296 Standby with Halted Clock (cycles) 1096 1176 0 0 1432 1403 782 312 0 0 0 0 0 0 432 Output Time (cycles) 80 (cid:2) 2 80 (cid:2) 2 80 (cid:2) 2 80 (cid:2) 2 64 (cid:2) 2 64 (cid:2) 2 48 (cid:2) 2 48 (cid:2) 2 288 288 288 432 216 216 216 Comm. Distance (# switches) 6 5 2 2 6 3 4 2 2 2 2 2 3 2 2 Execution Power (mW) 2.37 1.78 17.19 16.00 1.30 1.10 7.62 11.02 17.42 6.40 8.37 4.27 6.20 16.00 4.80 Stall Power (mW) 3.56 3.56 0.21 0.80 2.84 2.36 2.13 2.13 0.09 5.60 4.62 6.67 0 0.80 4.80 121.84 40.17 Standby Power (mW) 0.01 0.01 0 0 0.01 0.20 0.01 0.01 0 0 0 0 0 0 0.01 0.31 0.57 Comm. Power (mW) 1.14 1.00 0.53 0.53 0.92 0.55 0.51 0.31 0.96 0.96 0.96 1.44 0.93 0.72 0.72 Total Power (mW) 7.08 6.34 17.93 17.33 5.07 4.21 10.27 13.47 18.47 12.96 13.95 12.38 7.13 17.52 10.33 0.31 12.18 174.76 (cid:2) 2: 2 words (real and imaginary) of each sample or subcarrier where PE xe:i , PS tall:i , PS tandby:i and PComm:i are the power consumed by computational execution, stalling, standby and communication activities of the ith processor, respectively, and are estimated as follows: PE xe:i = (cid:11)i (cid:1) PE xeAvg PS tall:i = (cid:12)i (cid:1) PS tallAvg PS tandby:i = (1 (cid:0) (cid:11)i (cid:0) (cid:12)i )(cid:1) PS tandbyAvg (4) here PE xeAvg , PS tallAvg and PS tandbyAvg are average power of processors while 100% execution, stalling or in standby (leakage only); (cid:11)i , (cid:12)i and (1 (cid:0) (cid:11)i (cid:0) (cid:12)i ) are percentages of execution, stall and standby activities of processor i, respectively. For the worst case communication power, a processor will send its output words discretely, thus each data word is sent along with three cycles of clock as described in Section 4.3. Therefore, the communication power of processor i is estimated by PComm:i = (cid:13)i (cid:1) [(PS witchActive + 2PS witchS tall )(cid:1) Li + (PF I FOW riteActive + 2PF I FOW riteS tall )] (5) where Li is communication length of its output link counted by the number of switches that it passes through; (cid:13) i is its communication activity percentage. PS witchActive , PS witchS tall and PF I FOW riteActive , PF I FOW riteS tall are the average power consumed by one switch and one FIFO write, respectively with and without data sent while the clock is active. While measuring the chip with all processors running at 0.95 V and 594 MHz the values of PE xeAvg , PS tallAvg , PS tandbyAvg , PS witchActive , PS witchS tall , PF I FOW riteActive and PF I FOW riteS tall are shown in Table 1. For the ith processor, its (cid:11)i , (cid:12)i and (1 (cid:0) (cid:11)i (cid:0) (cid:12)i ) values are derived from Column 2, 3 and 4 of Table 2; and (cid:13) i , Li are derived from its Column 5, 6 with a note that each processor computes one data OFDM symbol in 2376 cycles. The power consumed by execution, stalling, standby and communication activities of each processor are listed in Column 7, 8, 9 and 10; and their total is shown in Column 11. In total, the receiver consumes 174.76 mW with a negligible standby power due to leakage (only 0.57 mW including the ten non-critical processors). The power dissipated by communication of all processors is 12.18 mW, which is only 7% of the total power. 6.4.2. Power Reduction The power dissipated by the stalling activity is 40.17 mW, which is 23% of the total power. This wasted power is caused by the fact that the clocks of processors are almost active while waiting for input or output as shown in Column 3 of Table 2. Clearly, we expect to reduce this stall time by making the processors busy executing as much as possible. To do this, we need to reduce the clock frequency of processors which have low workloads. Recall that in order to keep the 54 Mbps throughput requirement, each processor has to (cid:2)nish its computation for one OFDM symbol in 4 (cid:22)s, and therefore, the optimal frequency of each processor is computed as follows: fO pt:i = NE xe:i 4 (cycle s) ((cid:22) s) (MH z) (6) where, NE xe:i is number of execution cycles of processor i for processing one OFDM symbol, which is listed in Column 2 of Table 2. From this, the optimal frequencies of processors are shown in Column 2 of Table 3. By running at these optimal frequencies, the power wasted by stalling and standby activities of the critical processors is eliminated while their execution and communication activity percentages increase proportionally to the decrease of their frequencies. Therefore, total power is now 134.32 mW as listed in Column 3 of Table 3, a reduction of 23% when compared with the previous case4 . Now that processors run at different frequencies, they can be supplied at different voltages as shown in Fig. 11. Since power consumption at a (cid:2)xed frequency is quadratically dependent on 4 Ten non-critical processors still dissipate the same leakage power of 31 mW. Table 3. Power consumption while processors running at optimal frequencies when: a) Both VddLow and VddH igh are set at 0.95 V; b) VddLow is set at 0.75 V and VddH igh is set at 0.95 V Processor Data Distribution Post-Timing Sync. Acc. Off. Vec. Comp. CFO Compensation Guard Removal 64-point FFT Subcarrier Reorder. Channel Equal. De-modulation De-interleav. 1 De-interleav. 2 De-puncturing Viterbi Decoding De-scrambling Pad Removal Ten non-critical Proc.s Total (mW) Optimal Frequency (MHz) 80 60 580 540 44 51 257 372 588 216 283 144 594 540 162 Power Consump. (mW) 3.52 2.78 17.72 16.53 2.23 1.64 8.12 11.34 18.38 7.36 9.34 5.70 7.13 16.72 5.52 0.31 134.32 Optimal Voltage (V) 0.75 0.75 0.95 0.95 0.75 0.75 0.75 0.95 0.95 0.75 0.95 0.75 0.95 0.95 0.75 0.95 Power Consump. (mW) 2.63 2.11 17.72 16.53 1.73 1.23 5.22 11.34 18.38 4.95 9.34 4.10 7.13 16.72 3.71 0.31 123.18 supply voltage, more power can be reduced due to voltage scaling. Because our platform supports two global supply voltage grids, VddH igh and VddLow , we can choose one of these voltages to power each processor depending on its frequency5 . Since the slowest processor (Viterbi) is always running at 594 MHz to meet the real-time 54 Mbps throughput, VddH igh must be set at 0.95 V. If VddLow is set to equal to VddH igh , the power consumption does not change. If VddLow is lowered to where its supported maximum frequency is smaller than the optimal frequencies of all processors, then in order to correctly operate, all processors must be set to VddH igh . In this case, power consumption is also not improved. To (cid:2)nd the optimal VddLow we changed its value from 0.95 V (i.e VddH igh ) down to 0.6 V where its maximum frequency begins to be smaller than the lowest optimal frequency among processors. The total power consumption corresponding to these VddLow values (while processors are set appropriately) is shown in Fig. 15. As shown in the (cid:2)gure, the optimal VddLow value is 0.75 V with total power of 123.18 mW as detailed in Column 5 of Table 3. Notice that the power reduction comes from the effect of voltage scaling on the processor’s execution activity. The communication circuits use their own supply voltage which is always set at 0.95 V, so they still consume the same 12.19 mW, which now is approximately 10% of the total power. ) W m ( n o i t p m u s n o C r e w o P l a t o T 140 135 130 125 120 115 110 105 100 134.32 mW 123.18 mW 0.6 0.65 0.7 0.75 0.8 VddLow (V) 0.85 0.9 0.95 Figure 15. The total power consumption over various values of VddLow (with VddH igh is (cid:2)xed at 0.95 V) while processors running at their optimal frequencies. Each processor is set at one of these two voltages depending on its frequency. Table 4. Estimation and measurement results of the receiver at different con(cid:2)guration modes Con(cid:2)guration Mode At 594 MHz and 0.95 V At optimal frequencies only At both optimal freq. & volt. Estimated Power (mW) 174.76 134.32 123.18 Measured Power (mW) 177.96 139.64 129.82 Difference 1.8% 3.9% 5.1% putational results as with simulation. The power measurement results are shown in Table 4. When all processors run at 0.95 V and 594 MHz, they consume a total of 177.96 mW that is a 1.8% difference from the estimated result. When all processors run at their optimal frequencies with the same 0.95 V supply voltage, they consume 139.64 mW; and when they are appropriately set at 0.75 V or 0.95 V as listed in Column 4 of Table 3, they consumes 129.82 mW. In these con(cid:2)gurations, the differences between the measured and estimated results are only 3.9% and 5.1%, respectively. These differences are small that show that our design methodology is highly robust. Our simulation platform allows programmers to map, simulate and debug applications correctly before running on the real chip reducing a large portion of application development time. For instance, we mapped and tested this complex 802.11a receiver in just two months plus one week for (cid:2)nding the optimal con(cid:2)guration compared to tens of months if implemented on ASIC which includes fabrication, test and measurement. 6.5. Measurement Result 7. Conclusion We tested and measured this receiver on the real chip with the same con(cid:2)guration modes of clock frequency and supply voltage as used in the previous estimation steps. In all con(cid:2)guration modes, the receiver operates correctly and shows the same com5Non-critical processors are always set to run at VddH igh and 594 MHz for minimizing the detection and synchronization time. A high-performance and energy-ef(cid:2)cient programmable DSP platform consisting of many simple cores and dedicated-purpose accelerators has been presented. Its inter-processor communication network utilizes a novel source-synchronous interconnection technique allowing ef(cid:2)cient communication among processors which are in different clock domains.       The on-chip network is circuit-switched and is con(cid:2)gured before runtime such that interconnection links can achieve their ideal throughput at a very low power and area cost. For a real 802.11a baseband receiver with 54 Mbps data throughput mapped on this platform, its interconnection links only dissipate around 10% of the total power. We simulated this receiver with NCVerilog and also tested it on the real chip; the small difference between estimation and measurement results con(cid:2)rms the robustness of our design. Acknowledgments The authors thank Zhiyi Yu who inspired the original idea on source-synchronous clocking technique for many-core design. This work was supported by ST Microelectronics, IntellaSys, a VEF Fellowship, SRC GRC Grant 1598 and CSR Grant 1659, UC Micro, NSF Grant 0430090 and CAREER Award 0546907, Intel, and S Machines. "
Recursive partitioning multicast - A bandwidth-efficient routing for Networks-on-Chip.,"Chip Multi-processor (CMP) architectures have become mainstream for designing processors. With a large number of cores, Networks-on-Chip (NOCs) provide a scalable communication method for CMP architectures. NOCs must be carefully designed to meet constraints of power consumption and area, and provide ultra low latencies. Existing NOCs mostly use Dimension Order Routing (DOR) to determine the route taken by a packet in unicast traffic. However, with the development of diverse applications in CMPs, one-to-many (multicast) and one-to-all (broadcast) traffic are becoming more common. Current unicast routing cannot support multicast and broadcast traffic efficiently. In this paper, we propose Recursive Partitioning Multicast (RPM) routing and a detailed multicast wormhole router design for NOCs. RPM allows routers to select intermediate replication nodes based on the global distribution of destination nodes. This provides more path diversities, thus achieves more bandwidth-efficiency and finally improves the performance of the whole network. Our simulation results using a detailed cycle-accurate simulator show that compared with the most recent multicast scheme, RPM saves 25% of crossbar and link power, and 33% of link utilization with 50% network performance improvement. Also RPM is more scalable to large networks than the recently proposed VCTM.","Recursive Partitioning Multicast: A Bandwidth-Efﬁcient Routing for Networks-On-Chip Lei Wang, Yuho Jin, Hyungjun Kim and Eun Jung Kim Department of Computer Science and Engineering Texas A&M University College Station, TX, 77843 USA {wanglei, yuho, hkim, ejkim}@cse.tamu.edu Abstract Chip Multi-processor (CMP) architectures have become mainstream for designing processors. With a large number of cores, Networks-on-Chip (NOCs) provide a scalable communication method for CMP architectures. NOCs must be carefully designed to meet constraints of power consumption and area, and provide ultra low latencies. Existing NOCs mostly use Dimension Order Routing (DOR) to determine the route taken by a packet in unicast trafﬁc. However, with the development of diverse applications in CMPs, one-to-many (multicast) and one-to-all (broadcast) trafﬁc are becoming more common. Current unicast routing cannot support multicast and broadcast trafﬁc efﬁciently. In this paper, we propose Recursive Partitioning Multicast (RPM) routing and a detailed multicast wormhole router design for NOCs. RPM allows routers to select intermediate replication nodes based on the global distribution of destination nodes. This provides more path diversities, thus achieves more bandwidth-efﬁciency and ﬁnally improves the performance of the whole network. Our simulation results using a detailed cycle-accurate simulator show that compared with the most recent multicast scheme, RPM saves 25% of crossbar and link power, and 33% of link utilization with 50% network performance improvement. Also RPM is more scalable to large networks than the recently proposed VCTM. 1. Introduction As the clock speed race turns into the core count race in the current microprocessor trend, providing efﬁcient communication in a single die is becoming a critical factor for high performance CMPs [15]. Traditional shared buses that can connect only a handful number of components do not satisfy the need for a chip architecture containing tens to hundreds of processors. Moreover, the shrinking technology exacerbating the imbalance between transistors and wires in terms of delay and power has embarked on a fervent search for efﬁcient communication designs [9]. In this regime, Networks-On-Chip (NOCs) are a promising architecture that orchestrates chip-wide communications towards future many-core processors. NOCs are implemented as a switched network connecting cores in a ﬂexible and scalable manner, which achieves higher performance, higher throughput, and lower power consumption than a bus-based interconnect. Recent innovative tile-based chip multiprocessors such as Intel Teraﬂop 80-core [10] and Tilera 64-core [20] gain high interconnect bandwidth through 2D mesh topologies. Mesh networks match well a planar silicon geometry and provides better scalability and higher bandwidth than 1Dbased bus or ring networks. However, the implementation cost of NOCs is constrained within tight chip power and area envelopes. In fact, NOCs power consumption is signiﬁcant enough to occupy 28% of the tile power in Teraﬂop [10] and 36% of the total chip power in 16-tile RAW chip [18]. In the (5×5) mesh operand network of TRIPS, the router takes up to 10% of the tile area mostly due to FIFO buffers [8]. Therefore, any existing high-cost feature or new functionality needs to be carefully examined if it unduly increases the design cost. Looking to the future, supporting one-to-many communication such as broadcast and multicast in NOCs will provide many potentials in diverse application domains and programming models. In cache-coherent shared memory systems with a large number of cores, partitioned cache banks, and multiple memory controllers, hardware-based multicast is critical in maximizing performance. In fact, cache coherence protocols heavily rely on multicast or broadcast communication characteristics to maintain ordering amongst requests [14] or to invalidate shared data spread on different caches using directory. Motivated by the importance of multicast and broadcast support, recent work pro978-1-4244-4143-3/09/$25.00 ©2009 IEEE    posed these functions in the design of the routers [11, 17]. The key problem is to decide when and where to replicate multicast packets. Poor replication decisions can signiﬁcantly degrade network performance and increase the power consumption of links because multicast or broadcast communications easily exhaust the network bandwidth. Figure 1 shows two different routing examples in a (4×4) mesh network for the same trafﬁc pattern where the source is 9 and its four destinations are 0, 1, 2, and 3. In Example 1, packet replication occurs in routers 9 and 10, while in Example 2, packet replication occurs in routers 1 and 2. Note that the total number of replication operations is the same (three) in both examples. However, Example 2 performs packet delivery with only 5 links while Example 1 does with 11 links. As a result, Example 1 consumes 2.2 times link bandwidth of the network than Example 2. This increased bandwidth usage may cause contention in links and router ports, hence, increasing the latency. Furthermore, Example 1 dissipates more power due to more operations (buffer read/write, crossbar traversal, and link traversal) than Example 2. Examples in Figure 1 clearly show the need for intelligent routing algorithms for multicasting. Motivated by this problem, we propose a novel Source Destination excludes destinations in different delivery directions. This is required to prevent redundant packet delivery. Because each intermediate router uses RPM to make a routing decision, the whole packet traversal path is optimized. In this way, RPM can reduce the whole network link utilization. As a result RPM improves network bandwidthefﬁciency and decreases power consumption. Our main contributions are summarized as follows: • We propose a new routing algorithm, Recursive Partitioning Multicast (RPM), to support multicast trafﬁc in NOCs. • We explore the details of the multicast wormhole router architecture, especially the virtual channel arbiter and switch arbiter designs. • We evaluate different multicast schemes by varying the trafﬁc pattern in unicast trafﬁc, multicast trafﬁc portion, and the average number of destinations. Additionally, we show a good scalability of our scheme as the network size increases. • Detailed simulation results show that RPM saves 25% of crossbar and link power and 33% of link utilization with 50% latency improvement compared with the recently proposed VCTM. 0 4 8 1 5 9 2 6 3 7 10 11 0 4 8 1 5 9 2 6 3 7 10 11 The rest of this paper is organized as follows. We brieﬂy analyze the recent multicast work in Section 2. We propose the multicast router design in Section 3. RPM routing is discussed in Section 4. In Section 5, we describe evaluation methodology and summarize the simulation results. Finally, we draw conclusions in Section 6. 12 13 14 15 12 13 14 15 2. Related Work (a) Example 1 (b) Example 2 Figure 1. Different Bandwidth Usage in Multicasting for Four Destinations: Example 1 requires 11 link traversals, 12 buffer writes, 15 buffer reads, and 15 crossbar traversals, while Example 2 requires 5 link traversals, 6 buffer writes, 10 buffer reads, and 10 crossbar traversals. routing algorithm called Recursive Partitioning Multicast (RPM). The basic idea is that a routing path is computed based on all the destination positions in a network, and the network is recursively partitioned according to the position of the current router. The current node computes the output ports using a new partition and its destination list of the packet, and makes one packet replica for each output port. The replicated packet has an updated destination list, which Multicast (one to many) and broadcast (one to all) refer to the trafﬁc pattern in which the same message is sent from one source node to a set of destination nodes. A growing number of parallel applications show the necessity to provide multicast services. The main problem of multicast is to determine which path should be used to deliver a message from one source node to multiple destination nodes. This path selection process is called multicast routing. There are several multicast routing schemes. Multiple unicast is the simplest one. In multiple unicast, routers do not need to add any extra component and just treat multicast trafﬁc as unicast trafﬁc. Tree-based multicast routing [13] is to deliver the message along a common path as far as possible, then replicate the message and forward the copy on a different channel bound for a unique set of destination nodes. The path followed by each copy will further branch in the same manner until the message reaches every destination node. Multicast communication has been studied in distributed systems [7], local-area networks [1] and multicomputer networks [12]. However, supporting multicast in NOCs has different requirements, because current NOCs have power and area constraints with high performance requirement. Most recent work on multicast routing in NOCs is Virtual Circuit Tree Multicasting [11] and bLBDR [17]. The work in [11] proposes an efﬁcient multicast and broadcast mechanism. However, the main disadvantages of VCTM are threefold. First, VCTM needs extra storage to maintain the tree information for multicast, which needs more chip area. Second, before sending multicast packets, VCTM needs to send a setup packet to build a tree, introducing multicasting latency. Third, even with the same set of nodes, if the multicast source node changes, VCTM should build another tree. This makes VCTM not scalable to large networks. bLBDR [17] enables the concept of virtualization at the NOC level and isolates the trafﬁc into different domains. However, multicasting in bLBDR is based on broadcasting in a small domain. The problem of this scheme is that it is hard to provide multicasting if the destination nodes are spread in different parts of the network, because it is hard to deﬁne a domain to include all the destination nodes. 3. Multicast Router Design Our Recursive Partitioning Multicast is built on the stateof-the-art wormhole-switched router. In this section, we brieﬂy present a general router architecture and propose our RPM router architecture. 3.1. General Router Architecture Figure 2 shows a virtual channel (VC) router architecture used in NOCs [6]. The main building blocks are input buffer, route computation logic, VC allocator, switch allocator, and crossbar. To achieve high performance, the router processes packets with four pipeline stages, which are routing computation (RC), VC allocation (VA), switch allocation (SA), and switch traversal (ST). First, the RC stage directs a packet to a proper output port of the router by looking up a destination address. Next, the VA stage allocates one available VC of the downstream router determined by RC. The SA stage arbitrates input and output ports of the crossbar, and then successfully granted ﬂits traverse the crossbar in the ST stage. Due to the stringent area budget of a chip, routers use ﬂit level buffering for wormhole-switching as opposed to packet level buffering. Additionally, buffer is managed with credit-based ﬂow control, where downstream routers provide back-pressure to upstream routers to prevent buffer overﬂow. Because the router latency affects the packet delivery latency signiﬁcantly, recent router designs use techniques such as lookahead routing and speculative switch allocaRoute Computation VC Allocator Switch Allocator Input 0 . . . Input 4 VC 1 VC 1 VC 2 VC 2 VC n VC n Input buffers Input buffers VC 1 VC 1 VC 2 VC 2 VC n VC n Output 0 . . . Output 4 Input buffers Input buffers Crossbar switch Figure 2. Baseline Router Architecture tion [16], to reduce the number of pipeline stages. Lookahead routing removes the RC stage from the pipeline by making a routing decision one hop ahead of the current router. Speculative switch allocation enables the VA stage to be performed with the SA stage simultaneously. A separate switch allocator ﬁnds available input and output ports of the crossbar after the normal switch allocator reserves them. In this work, our basic wormhole router supports both techniques, hence having only two stages. 3.2. RPM Router Architecture In multicast trafﬁc, a packet that has multiple destinations needs to be replicated to several copies in intermediate routers. To support a multicast function, routers need a replication component with modiﬁcation of existing VC and switch allocators used in unicast routers. In wormhole switching, a router sends out a ﬂit to the next router before receiving the rest of the ﬂits in the same packet. To avoid the storage overhead for replica management, replications take place at the ST stage, and the basic unit is a ﬂit rather than a packet. When the switch allocation for one ﬂit at the head of selected VC succeeds, the ﬂit is replicated and sent out to the downstream router. In this way, current router does not need to hold replicated ﬂits. Actually, the replication component in our multicast wormhole router is only a control logic, thus, it will not consume much area. 3.2.1. Replication Scheme Replication schemes are instrumental to improve the performance of multicast router. Two replication schemes are proposed in literature: synchronous and asynchronous replication [4]. Synchronous replication requires multicast packets to proceed in a lock-step. At a fork, a ﬂit in those packets can proceed only when all of its target output ports are available. Any branch replica which is blocked can block other branch replicas of the same multicast packet. In terms of this nature, synchronous replication is susceptible to deadlock. In other words, if two multicast packets holding an output port request output ports reserved by each other, neither of them can advance. They will block each other forever. Thus, synchronous replication needs additional feedback architecture [4] to conﬁrm that ﬂits are processed in a lock-step. However, in asynchronous replication, branch replicas will not block each other, since each of them proceeds independently. Asynchronous replication is preferred for a practical implementation due to the following reasons. First, it does not need additional feedback architecture. Second, each replica is independent of others which reduces individual packet latency. Third, there is no potential deadlock inside the router. For the above reasons, we select asynchronous replication scheme in this paper. 3.2.2. Virtual Channel and Switch Allocator Design The basic function of VC and switch allocator is to do arbitration among different requesters, which request the same resource, and map those requests to free resources. In VC allocation, the unit of operation is a packet. We need to maintain the status of each VC to keep track of how far the packet has proceeded in a router. Right after the tail ﬂit is sent to the SA stage, the VC status information is to be ﬂushed and this VC is available for other packets. Unlike unicast, maintaining the status information for a multicast packet is more complicated. One multicast packet needs to reserve multiple output ports. Some ports may be free while others may not. As we are using asynchronous scheme, if one replica gets a free VC from its selected output port, that replica can go to the SA stage while the failed replicas toward other branches keep requesting in the following cycles until they pass that stage. Finally, the router will ﬂush the VC status when the last replica’s tail ﬂit departs the input buffer. For switch allocation, we choose two-stage arbiter. The ﬁrst stage is doing arbitration among different VCs which belong to the same physical input port. This stage is the same as unicast router. The second stage is doing arbitration among different winning VCs from the ﬁrst stage which go for the same output port. Differences exist between a unicast router and a multicast router. In unicast, one input VC has only one candidate output port, so its request goes for only one second-stage arbiter. However in multicast, two or more output ports can be requested by the same winning VC, and its requests should go to several different secondstage arbiter. Even more, some requests may succeed and some may fail. In this scenario, we continuously follow the asynchronous scheme. The successful request can make a ﬂit copy and transmit it to the downstream router. The failed ones are waiting for the next arbitration chance. In our router design, we use the round-robin arbiter in both VC and switch allocation. We do not assign any priority between multicast packets and unicast packets. We believe that if we use more intelligent priority arbitration in this part, the performance of the whole network can be improved further. This work is left for the future. 3.2.3. Destination List Management The header of a packet carries the addresses of its destination nodes. Several approaches have been made to ﬁnd out an efﬁcient way to put multiple destination addresses into the header such as all-destination encoding, bit string encoding, and multiple-region broadcast encoding [3]. The all-destination scheme puts each destination node identiﬁer into the header. This approach is good when the number of destinations is small but it introduces a signiﬁcant overhead when the number becomes large. Bit string encoding uses a bit-vector where each bit represents one destination node. This scheme performs well when the number of destinations is large. In the multiple-region broadcast encoding scheme, the header carries ranges of destination addresses: the beginning and ending addresses of regions. The message is, then, sent to all addresses in those ranges. The main disadvantage of this scheme is that it is expensive to decode addresses. Further efforts have been made to minimize the header length and the header processing time. Bit string encoding is used in our work. Figure 3 depicts an example of a multicast and its corresponding packet header. The packet header contains a bit string which is of length of the number of nodes in the network. A bit in this string corresponds to a node and setting a bit means that the node is one of the destinations. Since the destination information is carried in the packet header, there is no need to maintain tables in the intermediate routers for routing computation, thus we can save chip area and time to setup those tables. As the network size grows, the number of bits increases only linearly. Using the bit string encoding, the intermediate router has to generate output bit string (s) to avoid redundant replication. When a packet reaches a destination, the bit corresponding to that destination node must be reset. Otherwise, the next router which receives this packet will send it back to the previous one. At a fork, the destination list must be exclusively divided into subsets such that packets forwarded to different directions should not meet at the same node in the future. This destination list management is done when packets are replicated at the ST stage. Detailed discussion with examples can be found in the following section. 2 3 4 1 7 Source node 1 5 0 7 6 5 6 W Eight Parts Three Parts (5, 6, 7) 3 N S E 0 7 4 5 2 1 3 Figure 3. Packet Header Example Three Parts (0, 1, 7) Three Parts (3, 4, 5) Three Parts (1, 2, 3) 4. Recursive Partitioning Multicast (RPM) In this section, we propose the RPM routing algorithm, achieving deadlock freedom as well as bandwidth efﬁciency. 4.1. RPM Routing Our RPM routing algorithm is built on the multicast router hardware explained in Section 3. Unlike VCTM, RPM do not maintain any lookup table for multicast in each router. RPM directly sends out multicast packets without sending a unicast+setup packet to each destination ﬁrst. In other words, RPM does not need to build a tree structure in each intermediate router before sending the real multicast data packet. However, in each multicast packet header, we need one ﬁeld to indicate the destination nodes’ positions. In RPM, the routing decision is made based on the current network partitioning. A source node divides the whole network into at most eight parts according to its position. Then destination nodes in a multicast packet belong to one of these parts. In general case, if the source node is in the center of the network, all the eight parts have at least one node. However, if the source node is located in the corner of the network, some parts may be empty. Taking a (4×4) mesh network as an example, partitioning is deﬁned as Figure 4. Since the source node, which is in the center of the network, always has at least one node in each part, we normally choose the center node as our multicast source node in the following discussion. Normally, at the RC stage, the input of routing computation component is a destination list from the packet header, and the output is an output port identiﬁer. However, when the packet is a multicast packet, the output port identiﬁer may not be unique. One packet can go for different output ports simultaneously. Some destination nodes can be reached through more than one directions. For example, one destination node is in the northeast direction of the source node. Then north and east directions are two routing deciFigure 4. Network Partitioning Based on Source Node Positions sion candidates for that node. Only one direction we can choose for that destination node, since we want to achieve bandwidth efﬁciency in our multicast routing. The key point is how we can minimize packet replication times, in other words, maximizing the reusability of each replica. To avoid redundant replication, we deﬁne replication priority rules for each direction. Basic priority rules (Figure 5(a)) • North direction has a higher priority than East to reach destination nodes in Part 0 (Northeast Part). • West direction has a higher priority than North to reach destination nodes in Part 2 (Northwest Part). • South direction has a higher priority than West to reach destination nodes in Part 4 (Southwest Part). • East direction has a higher priority than South to reach destination nodes in Part 6 (Southeast Part). However, only with the above basic rules, in some cases, we still cannot maximize the reusability of some replicas. To make the replication point selection more bandwidthefﬁcient, besides the above basic rules, we propose additional rules. Optimized priority rules • If there are destination nodes in both Part 0 (Northeast Part) and 2 (Northwest Part), North direction replication will be used for both of them. The same rule is applied to South direction, as in Figure 5(b). • Part 1 and 2 have destination nodes, but Part 3 does not have any. In this case North direction replication, not West, will be used for destination nodes in Part 2. The same rule is applied to South direction, as in Figures 5(c) and (d). Source Destination 2 1 3 4 N E W S 5 (a) 2 1 3 4 5 (c) 0 7 6 0 7 6 2 1 3 4 2 3 4 5 (b) 1 5 (d) 0 7 6 0 7 6 bit-encoded destinations partitioning  logic Part 0 Part 1 Part 2 Part 3 Part 4 Part 5 Part 6 Part 7 a0 a1 a2 . . . an-1 a0 ~ an-1: node address Part 0 ~ Part 7: partition N E S W Figure 5. Basic Routing Priority Figure 6. Hardware Implementation of Routing Logic Besides the above replication priority for each direction, when replicated packets are sent out at the ST stage, current router will modify the original packet header, deleting useless destinations from the old destination list. After this the packet replica for each direction only has a subset of destination nodes in its packet header, which facilitates that this replica only sent to the subset destination nodes. We can build the original destination list from all the subset destination lists, and there are no redundant nodes in each subset list. The pseudo-code for the operation of the routing computation is in Table 1 and its hardware implementation is shown in Figure 6. The RPM logic consists of two steps. At the ﬁrst step, the partitioning logic ﬁnds out which parts the packet has to be sent to. It takes the destination information (bit-encoded destinations) from the header as input and produces the output (Part 0, Part 1, ... , and Part 7) according to the current location. The second step is the core of the routing logic. With the help of the intermediate result from the partitioning logic, the decision on output ports (N, E, S, or W) is made in this step. To make it more clear, we use the example in Figure 7 to walk through the process of sending a multicast packet to all destinations. When the network interface in Node 9 initiates one multicast packet, the routing computation component decodes the destination list in the packet header (current destination nodes are 0, 2, 3, 13, and 15). At step 1, according to Node 9’s position, the destination nodes are in four parts (0, 2, 5, and 6). Based on the routing priority rules, Node 9 only needs to make one copy of the packet, and sends original packet to North and the new copy to South. In North direction, the destination list only contains Nodes 0, 2, and 3. In South direction, the destination list only has if IN(dest,7) OR (IN(dest,6) AND !IN(dest,5) AND !IN(dest,4) then ADD(EAST) if IN(dest,1) OR (IN(dest,0) AND (!IN(dest,7) OR !IN(dest,4) AND IN(dest,6))) OR (IN(dest,0) AND IN(dest, 2)) then ADD(NORTH) if IN(dest,3) OR (IN(dest,2) AND !IN(dest,1) AND !IN(dest,0)) then ADD(WEST) if IN(dest,5) OR (IN(dest,4) AND (!IN(dest,3) OR !IN(dest,0) AND IN(dest,4))) OR (IN(dest,4) AND IN(dest,6)) then Add(SOUTH) IN(dest, n): at least one destination node is in part n. ADD(direction): add direction as candidate. Table 1. Pseudo-Code for Routing Computation Nodes 13 and 15. At step 2, one replica arrives at Node 5. Now the new partitioning is based on Node 5’s position. Node 5 can only see three destination nodes (0, 2, and 3) and they are in Part 0 and 2. Node 5 does not need to do any replication and just forward the same packet to Node 1 in its North direction. Another replica arrives at Node 13. Unlike Node 5, Node 13 should make a copy. One goes to an ejection port since Node 13 itself is a destination node and the other is forwarded to Node 14 with only Node 15 in its destination list. At step 3, Node 1 ﬁnds that Node 0 lies in Part 3 and that Nodes 2 and 3 lie in Part 7. So Node 1 makes one new copy. The original packet goes to West direction and the new copy goes to East. This recursive partitioning continues at step 4 and 5. Finally, all the destination nodes receive the packet. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Step 1 Step 2 Step 3 Current node Destination node Ejection node s 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Step 5 Step 4 Figure 7. Multicast Packet Traveling Example 4.2. Deadlock Avoidance Deadlock freedom is an important feature for every routing algorithm. The routing algorithm explained so far is likely to produce deadlock since no turn restriction is deﬁned. Without any turn restriction, ﬂows in the same network may cause a cyclic dependency, which freezes movement of all packets in a cycle. To guarantee deadlock freedom, a turn restriction must be given. Packets in different paths do not generate any cycle in the network. Most of deadlock avoidance techniques are designed to remove cyclic dependency as in [5]. Virtual network (VN) is introduced to make writing a deadlock-free algorithm easy by separating a physical network into multiple virtual networks; each of those networks does not produce cycles by itself. It has been extensively explored in previous interconnection research in [2]. Taking a 2D mesh topology as an example, two VNs (VN0 and VN1) lie in the same physical network and are used as a pair. VN0 does not allow packets to turn from a current location to North whereas VN1 does not allow packets to turn to South. Using deadlock-free VNs makes no cycle in a network either clockwise or counterclockwise since some turns to form a cycle are prohibited. Once the whole network is divided into two VNs, packets also must be distinguished by which VN they should follow. One bit in the packet header is used to indicate VN identiﬁer as shown in Figure 3. A source router where the multicast is initiated deﬁnes this bit at the RC stage. If destinations lie in the upside of the source router, the packet goes through VN0, whereas if destinations are in the downside, the packet takes VN1. Note that if destinations lie in both sides, the source node makes two copies, one for up and the other for down. Intermediate routers never change the VN bit and only forward the packet to the same VN in which the packet has traveled. 5. Experimental Evaluation We evaluated RPM with different synthetic multicast workloads, comparing it with VCTM. We also examined RPM’s sensitivity to a variety of network parameters. 5.1. Methodology We use a cycle-accurate network simulator that models all router pipeline delays and wire latencies. We use Orion [19] to estimate dynamic and static power consumption for buffer, crossbar, arbiter, and link with 50% switching activity and 1V supply voltage in 65nm technology. We assume a clock frequency of 4GHz for the router and link. For a (8×8) mesh network, the area of one tile is 2mm x 2mm, resulting in a chip size as 256mm 2 . We model a link as 128 parallel wires, which takes advantage of abundant metal resources provided by future multi-layer interconnect. We consider network sizes of 36, 64, 100, and 256 terminals. We use 2-stage router and synthetic workloads for performance evaluation. On top of Uniform Random (UR), Bit Complement (BC) and Transpose (TP) unicast packets, our synthetic workloads have multicast packets. For multicast packets, the destination numbers and positions are uniformly distributed, while unicast packet’s destination positions are determined by three patterns (UR, BC, and TP). We also control the percentage of multicast packets in whole packets. Table 2 summarizes the simulated conﬁgurations, along with the variations used in the sensitivity study. Characteristic Baseline Variations Topology Routing Virtual Channels/Port Virtual Channel Depth Packet Length(ﬂits) Unicast Trafﬁc Pattern Multicast Packet Portion Multicast Destination Number Simulation Warmup Cycles Total Simulation Cycles 8×8 Mesh RPM and VCTM 4 4 4 Uniform Random 10% 0-16 (uniformly distributed) 10,000 20,000 6×6 Mesh, 10×10 Mesh, 16×16 Mesh – – – – Bit Complement, Transpose 5%, 20%, 40%, 80% 0-(4, 8, 32) (uniformly distributed) – – Table 2. Network Conﬁguration and Variations 5.2. Performance Figure 8 summarizes the simulation results of an (8×8) network with the three synthetic trafﬁc patterns. Mul unicast means sending multicast packet as multiple unicast packet, one by one. VCTM (80%) means 80% of entries in the virtual circuit table can be reused by different multicast packets. RPM has the lowest average packet latency, 50% of that of VCTM at low loads and almost 25% at high loads. When we look at the network saturation points, RPM saturates at higher loads, 20% higher than VCTM, which means RPM can provide high throughput. The results are consistent with our expectations. The performance improvement comes from two main reasons: First, since a multicast packet in RPM carries a destination list in the packet header, we do not need setup packets to each destination node to construct a tree. This reduces the actual number of injected packets in the network and also gets rid of the setup delay. Second, the routing paths generated by RPM do not strictly follow a dimension order. According to our description in Section 4, RPM selects the routing path based on global distribution of destination nodes. This feature can provide more diverse paths than VCTM, which means links in the whole network can be fairly used with less contention. Also we observe that when the rate of virtual circuit table reusability decreases (more than 30% degradation), the performance of VCTM decreases much. This indicates the performance of VCTM is sensitive to the reusability of virtual circuit table. To keep high reusability in a large network, VCTM needs a big table which consumes larg chip area. 5.3. Power Figure 9 shows the power consumption of the two schemes. We assume 80% entries of virtual circuit table reusable for VCTM. Although we ignore the power consumption of the virtual circuit table for VCTM, we observe that RPM is more power efﬁcient than VCTM. Before network is saturated, for example load 0.1, RPM saves almost 60% power consumption compared with VCTM. Because we use the same network conﬁgurations in RPM and VCTM, the static power consumption is the same but dynamic power consumption is different. Figure 10 summarizes the dynamic power consumption of each component in a router. Crossbar power and link power are dominant. Because RPM uses less crossbar and link resources than VCTM, it saves 25% crossbar and link power. The same conclusion can be drawn from Figure 11, which shows the average link utilization of RPM and VCTM. RPM saves almost 33% of link utilization compared with VCTM (80%). 0 5 10 15 20 P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M P R M T C V M 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Injec tion Rate(f lits/cyc le/core) o T t a l P o w e ( r W ) Dynamic Power Static Power a Figure 9. Power Consumption (Dynamic and Static) in an (8×8) Mesh Network. (10% multicast trafﬁc, average 8 destinations) 2 0 6 4 10 8 12 P R M C V T M P T R C V M M P R M C V T M P R M C V T M P T R C V M M P R M C V T M P R M C V T M P T R C V M M P R M C V T M P R M T P C R V M M C V T M P R M C V T M P R M T P C R V M M C V T M P R M C V T M P T R C V M M P R M C V T M P R M C V T M 0.010.020.030.040.050.060.070.080.09 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Injec tion Rate(f lits/cyc le/core) D a n y m i c P o w e ( r W ) Buf fer VC Arb iter SW Arb iter Xbar Link Figure 10. Dynamic Power Consumption in an (8×8) Mesh Network. (10% multicast trafﬁc, average 8 destinations)     VCTM(40%) 120 100 80 60 40 20 0 0.01 0.03 0.05 0.07 0.09 0.15 Injection rate(fli ts/cycle/core) y c n e a L t ( e c y c l ) RPM Mul unicast VCTM(20%) VCTM(80%) 120 100 80 60 40 20 0 0.01 0.03 0.05 0.07 0.09 0.15 Injection rate(fli ts/cycle/core) y c n e a L t ( e c y c l ) RPM Mul unicast VCTM (20%) VCTM (40%) VCTM (80%) 120 100 80 60 40 20 0 0.01 0.03 0.05 0.07 0.09 0.15 Injection rate(fli ts/cycle/core) y c n e a L t ( e c y c l ) RPM Mul unicast VCTM (20%) VCTM (40%) VCTM (80%) (a) Uniform Random (b) Bit Complement (c) Transpose Figure 8. Packet Latency with Three Synthetic Trafﬁc Patterns in an (8×8) Mesh Network. multicast trafﬁc, average 8 destinations) (10% 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0 . 0 1 0 . 0 2 0 . 0 3 0 . 0 4 0 . 0 5 0 . 0 6 0 . 0 7 0 . 0 8 0 . 0 9 0 . 1 0 . 1 5 0 . 2 0 . 2 5 0 . 3 0 . 3 5 0 . 4 0 . 4 5 0 . 5 Injection Rate(flits/cycle/core) k n i L ) e l c y c / p o ( n o i t a z i l i t U RPM VCTM(20%) VCTM(40%) VCTM(80%) Figure 11. Link Utilization in an (8×8) Mesh Network. (10% multicast trafﬁc, average 8 destinations) 5.4. Scalability Previous work [11] shows that different applications have different portions of multicast trafﬁc, such as 5% with directory protocol, 5.5% with token coherence and more than 10% with operand network. Figure 12 summarizes the performance’s change with the increasing portion of multicast trafﬁc. When the multicast portion is beyond 20%, VCTM’s performance decreases very sharply. However, from 5% to 40% portion, RPM’s performance stays almost stable with small degradation. We also observe that when the portion of multicast trafﬁc is less than 10%, VCTM shows better performance than RPM. That is because when the number of multicast packets is small, VCTM does not need to rebuild many trees and the tree setup overhead is marginal. Furthermore, as RPM uses two VNs to avoid deadlock, buffer resource (the number of VCs) is half of that in VCTM. On the contrary, we observe that when the portion of multicast trafﬁc is above 10%, the performance of RPM is better than VCTM. Because tree setup overhead becomes dominant, the disadvantage of VCTM appears obviously. Figure 13 shows the performance of RPM and VCTM 0 50 100 5% 10% 20% 40% 80% Portion of multicast traffic a L t y c n e ( s e c y c l ) RPM VCTM Figure 12. Scalability to Multicast Portion in an (8×8) Mesh Network. (average 8 destinations) with different network sizes. The same trend is observed. As network size becomes bigger, the performance of VCTM degrades much worse than RPM. Building more trees for VCTM results in more setup packets. RPM gets rid of maintaining different tree structure in the router. We observe that from (6×6) mesh to (16×16) mesh, the performance of RPM is more stable than VCTM. The average packet latency of RPM is almost 50% of VCTM. 0 50 100 6×6 8×8 10×10 16×16 Network s ize a L t y c n e ( s e c y c l ) RPM VCTM a Figure 13. Scalability to Network Size. (10% multicast trafﬁc, average 8 destinations) Another simulation is done based on multicast destination numbers. The trend is similar to previous work. From   the above scalability study, we can see that VCTM shows good performance in low multicast trafﬁc portion, small network size, and number of destinations. However, when these three metrics become bigger, the tree maintenance overhead becomes dominant. Compared with VCTM, our RPM scheme is more scalable. RPM VCTM ) l s e c y c ( y c n e t a L 100 50 0 4 8 16 32 Max. number of destinations Figure 14. Scalability to Number of Destinations in an (8×8) Mesh Network. (10% multicast trafﬁc) 6. Conclusions The prevalent use of NOCs in current multi-core systems indicates that it is important for NOCs to support multicast trafﬁc. The key problem in supporting multicast is when and where to replicate multicast packets. In this paper, we propose Recursive Partitioning Multicast (RPM), which intelligently selects proper replication points for multicast packets based on the global distribution of destination nodes. We explore the details of the multicast wormhole router architecture, especially the virtual channel arbiter and switch arbiter designs. We study the scalability of different multicast schemes, based on three characteristics of multicast trafﬁc workload. Detailed simulation results show that compared with previous multicast schemes, RPM saves 25% of crossbar and link power and 33% of link utilization with, and improves 50% of latency. Due to the current simulation environment, we evaluate different multicast schemes only using synthetic trafﬁc patterns. We plan to integrate our design into a full-system simulator to evaluate the performance of the overall system. "
Scalability of network-on-chip communication architecture for 3-D meshes.,Design constraints imposed by global interconnect delays as well as limitations in integration of disparate technologies make 3D chip stacks an enticing technology solution for massively integrated electronic systems. The scarcity of vertical interconnects however imposes special constraints on the design of the communication architecture. This article examines the performance and scalability of different communication topologies for 3D network-on-chips (NoC) using through-silicon-vias (TSV) for inter-die connectivity. Cycle accurate RTL-level simulations are conducted for two communication schemes based on a 7-port switch and a centrally arbitrated vertical bus using different traffic patterns. The scalability of the 3D NoC is examined under both communication architectures and compared to 2D NoC structures in terms of throughput and latency in order to quantify the variation of network performance with the number of nodes and derive key design guidelines.,"Scalability of Network-on-Chip Communication Architecture for 3-D Meshes Awet Yemane Weldezion∗ , Matt Grange+ , Dinesh Pamunuwa+ , Zhonghai Lu∗ , Axel Jantsch∗ , Roshan Weerasekera+ , and Hannu Tenhunen∗ ∗School of Information and Communication Technologies, Department of Electronics, Computer, and Software Systems, KTH Royal Institute of Technology, Electrum 229, Kista SE 16440, Sweden {aywe,zhonghai, axel, hannu}@kth.se +Centre for Microsystems Engineering, Department of Engineering {m.grange, d.pamunuwa, r.weerasekera}@lancaster.ac.uk Lancaster University, Lancaster LA1 4YW, UK Abstract Design Constraints imposed by global interconnect delays as well as limitations in integration of disparate technologies make 3-D chip stacks an enticing technology solution for massively integrated electronic systems. The scarcity of vertical interconnects however imposes special constraints on the design of the communication architecture. This article examines the performance and scalability of different communication topologies for 3-D Network-onChips (NoC) using Through-Silicon-Vias (TSV) for inter-die connectivity. Cycle accurate RTL-level simulations are conducted for two communication schemes based on a 7-port switch and a centrally arbitrated vertical bus using different trafﬁc patterns. The scalability of the 3-D NoC is examined under both communication architectures and compared to 2-D NoC structures in terms of throughput and latency in order to quantify the variation of network performance with the number of nodes and derive key design guidelines. 1. Introduction The performance bottleneck imposed by on-chip interconnects in the deep submicron regime of process technology has been widely documented [15], as have the beneﬁts of standardization of on-chip communication [3]. Implementing a standardized communication architecture such as a packet-switched NoC for massively integrated multiprocessor systems provides an abstraction of the global interconnection link and can greatly reduce design effort, potentially at the cost of some area and possibly power and performance penalties. The suitability of the NoC as a communication architecture depends on the overall system; i.e. the number of autonomous functioning blocks, the degree of parallelism, and area and performance requirements dictate its usefulness. The precise quantiﬁcation of the performance and overhead of the network is of paramount importance in making this decision. The potential of the 2-D NoC for interconnecting multi-processor systems has been widely researched, and most recently an 80 tile, 100M transistor system with 1.28 TFLOP peak performance has been demonstrated in a 65nm, 1V technology [17]. However a major new paradigm for continued Moore’s law integration is 3-D chip stacks based on a variety of vertical interconnection techniques [1], [16]. 3-D integration provides opportunities for cost reduction and yield improvement in integration of different technologies such as CMOS, DRAM and MEMS circuits through the ability to implement them over multiple die layers on the same chip. It can also reduce form factor in applications where size is critical, while effective heat dissipation and temperature control can be a challenge. To get the most beneﬁt out of 3-D chip stacks in multiprocessor systems, the communication architecture has to support efﬁcient and high throughput vertical communication. In this article we examine the scalability of the NoC for such systems. We build on previous work published in the literature and investigate the scalability of the three NoC architectures of 2-D mesh, 3-D mesh (with switch connectivity between layers) and 3-D bus (with bus connectivity between layers) with respect to performance. This is accomplished by quantifying latency and throughput of the different architectures for various network sizes, under two representative trafﬁc 978-1-4244-4143-3/09/$25.00 ©2009 IEEE    pending on whether a die housed in a tile has one layer or several layers, and whether the NoC itself is 2-D or 3-D. The average unloaded latency per bit has also been derived, showing the potential improvements in latency and power consumption obtainable through a 3-D architecture, but the performance of the network for different trafﬁc patterns under a packet-switched protocol is necessary to evaluate its scalability. An investigation into different router structures was conducted in [6] which proposes a 3-D crossbar-style NoC and performs cycle accurate simulations to extract energy and latency metrics. However the study ﬁxes on a 64 node network. In [13] a multi-layer NoC router architecture is explored for a number of trafﬁc patterns. In this paper, the number of nodes is ﬁxed at 36 and the number of layers in the 3-D stack is kept constant at four. A well known paper that describes the performance of communication networks of varying dimension for wormhole routing is [2], which generalizes the interconnection network as being a k-ary ncube torus, with n being the dimension of the cube, and k being the radix, or number of switches in a given dimension. In it Dally points out that VLSI circuits are wire-limited, and any growth in dimension has to be accompanied by a related reduction in the parallelism of each link and therefore the appropriate analysis of performance of interconnection networks for VLSI circuits has to be under the constraint of constant bisection bandwidth. It is shown that under various wire delay models, including transmission-line like and RC like behavior, for relatively large networks with 1M nodes, the best performance is delivered by switches of relatively low dimension, around 5 or 6. Due to layout restrictions, the most common implementation of the NoC for both 2D and 3-D is a mesh, which is not directly comparable to the tori considered in [2]. In a 2-D mesh a switch has links to four other switches and its resource, while the straighte d o N r e p s k n L i f o r e b m u N e g a r e v A 5.5 5 4.5 4 3.5 3 2.5 0 Average Number of Links per Node trend 3D Network 2D Network 3D Bus 200 400 600 800 No. of nodes in network 1000 1200 1400 Figure 2. Number of links per node for a 2-D mesh, a 3-D mesh and a 3-D bus NoC Figure 1. Each switch, S, is connected to a resource, R. a) a 2x2, 2-D mesh b) a 2x2x2, 3D mesh structures, c) a 1x2x2, 3-D centrally arbitrated vertical bus for layer-to-layer network communication patterns, uniform random and local. The Nostrum network communication protocol used in this study is based on a bufferless hot-potato routing algorithm [12], while the bus protocol utilizes a centrally arbitrated least-served-ﬁrst priority scheme. Cycle accurate RTL simulations are carried out to capture latency and throughput in terms of hops between nodes and hops per node per cycle respectively, for symmetric networks up to 1000 nodes. Various injection rates are used to study saturation points for the two trafﬁc patterns. The main contribution of this paper is in providing a novel and extensive analysis into the scalability of 3-D NoC architectures as the number of nodes and layers grows. This study quantiﬁes performance to aid the design of the communication architecture of future massively integrated 3-D systems.The rest of the paper is organized in the following manner. Section 2 discusses related work while the following section discusses network implementation issues including the protocols, network topology and architecture of the different switches and bus arbiter. Sections 4 and 5 describe the exploration space, the simulation and calculation methodology and the results. We end with a discussion of the results and our conclusions. 2. Related Work A comprehensive overview of processing and technological issues in 3-D integration can be found in [16] while a discussion of the physical level cost and performance tradeoffs associated with the different techniques is provided in [18]. The different NoC-based system architectures for 3D systems have been exhaustively enumerated in [14], de          forward extension for 3-D is to consider a switch with two additional inter-layer links (with the seventh port being connected to the resource). Therefore it is of interest to investigate the scalability (i.e. variation of key metrics related to latency and throughput with size of network) of the NoC for this topology. Further, two key assumptions in [2] are likely to have an effect; ﬁrstly the study is carried out under the assumption of uniform links, but as explained below the vertical links have a signiﬁcantly different delay model than the horizontal links. Secondly, the assumption of constant bisection bandwidth needs to be revisited as the footprint of the vertical interconnects is different in general to the horizontal links. The electrical behavior of the relatively short and wide TSV is much better than that of long on-chip interconnects, primarily due to the low resistance, and can support much higher signaling speeds, but a relatively high area penalty due to via blockage may impose constraints on the number of TSVs that can be used for communication. This has led the authors of [7] to propose a dynamic time-divisionmultiple-access (dTDMA) bus with a centralized arbiter for the vertical communication link, which allows single hop latency for packets between any number of vertical layers. Most recently [5] describes a detailed study using different trafﬁc patterns under a realistic protocol for both architectures to derive ﬁgures of merit related to throughput, latency, energy and area overhead to characterize various topologies. However the question of the general scalability of the different architectures with network size has not been addressed. Finally [11] describes a working 3-layer 27 node prototype that provides proof of concept of the 3-D NoC, but identify the need for a 3-D network simulator and system-level explorations of the kind discussed here. 3. Link and Network layer Protocols The NoC protocol employed in this study is a hot-potato implementation with switch architecture as described in [12]. The routing strategy is based on non-minimal and load dependent deﬂection type packet switching, with adaptive per hop routing. A relative addressing scheme is implemented which simpliﬁes the duplication of identical switches when network structures of varying sizes are designed. The switches employed in all of the network conﬁgurations in this study are bufferless, and directly connected with their associated resource with a 1:1 resource to switch ratio. A packet cannot be stored in a switch, and thus in each cycle the packets must be moved from switch to switch or deﬂected back to a resource. To reduce the complexity of the switches, deterministic routing is favored over adaptive routing in buffered networks [10],[4],[1]-[3]. In bufferless networks, deﬂection routing is advocated [16] because it is Average normalized latency growth 2.8 2.6 2.4 2.2 2 1.8 1.6 1.4 1.2 y c n e t a l d e z i l a m r o n e g a r e v A 1 0 500 1000 1500 2000 2500 3000 3500 Time in clock cycles Figure 3. Variation of average normalised latency for a 5×5×5 mesh with 0.5 injection rate under the local trafﬁc model, illustrating the warming up phase possible to design fast and small switches with simpliﬁed control circuitry. The implemented switch uses 128-bit input and output channels for each switch-to-switch Physical Channel (PC) and resource to switch connection. For 2-D structures a 5-port switch is used as in Figure (1a), while for 3-D structures a 7-port switch, as shown in Figure (1b), is used where the inter layer connectivity is implemented with two simplex channels having the same switch-to-switch bitwidth as the horizontal channels. A TDMA vertical bus to provide connectivity between network layers is also designed as seen in Figure (1c). It is a centrally arbitrated bus employing a least-served-ﬁrst priority scheme for packet arbitration. The matrix arbiter circuit in [4] has been extended to deal with up to 10 layers. The bus has a 10 packet deep FIFO buffer at each input to prevent packet loss for the maximum network size. The bus runs on a clock 16 times faster than the network clock to provide serialization and de-serialization time, potentially circumventing area limitations for vertical TSV connectivity between layers. Hence the bus can accommodate a reduction in the bit width of the vertical channel up to a ratio of 1/16 and still receive and deliver a packet to a destination, on any layer, within one cycle of the network clock. It is connected to 6-port switches through a 128-bit Input/Output PC. In this study, every switch on a given layer is connected to a separate bus, and a single bus connects all switches that are vertically in-line. Thus, a 5x5x5 network will have 25 buses in total. The packets are generated by each resource and injected into the network with an injection rate of up to 0.9 packets per node per cycle. For this study, a packet is considered to be one ﬂit long. Each resource has a FIFO buffer to temporarily store packets if they cannot enter the network due       to congestion. These packets are queued and re-injected into the network with a higher priority than newly generated packets. The implemented network does not drop packets. The packet headers generated by the resource contain ﬁnal destination addresses and the switches make routing decisions on the ﬂy based on this information. Any NoC structure is comprised of switches located in general at the corners, edges, surface and center of the physical structure. For example a 2-D switch has 4 links to other switches in 4 directions, namely, North, South, East and West. A 3-D switch has two additional, Up and Down links giving a total of 6 bi-directional links, whereas a bus architecture adds only one extra link to the switch, to give a total of 5 bi-directional links. There is also an additional bidirectional port from switch to resource in each case. For a given NoC structure, not all of the links can be used for routing packets. For example, only half of the switch-toswitch links in the corner of the network are connected, halving the available switch bandwidth due to the unconnected edge ports. The total number of connected links, in any NoC structure depends on the topology, dimension and size of the network. Figure 2 depicts the number of links as a function of network size in a mesh topology to highlight the differences between a 2-D 5-port switch, a 3-D 7-port switch, and a 3-D bus architecture as deﬁned in equations (1), (2) and (3) respectively. For a 2-D n×n network: L2D = 4n(n − 1) For a 3-D n×n×n network: (1) L3DN = 6n2 (n − 1) (2) (3) For a 3-D Bus n×n×n network: L3DB = 4n2 (n − 5) + 32 All three curves for the different architectures show a sharp rise as network size increases and then begin to level off as the number of corner and edge nodes (i.e. nodes with some ports unconnected) become outweighed by the fully connected nodes. This trend of growth of links per node allows for higher throughput as the network size increases due to the increased bandwidth available to route a packet in the network. Additionally, it is clear from this ﬁgure that the 7-port switch has an advantage over the other topologies due to the increased number of switch-to-switch links per node available to route the trafﬁc in the network for a given network size. The 3-D bus architecture has the least number of links due to the fact that all switches in a given vertical line share a common uni-directional link. The 7port design beneﬁts from separate PCs between every layer to handle multiple packets simultaneously within one cycle on any given vertical line from the bottom to the top layer. l e n n a h C r e p d a o L 6 5 4 3 2 1 0 0 l e n n a h C r e p d a o L l e n n a h C r e p d a o L 2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 0 2.5 2 1.5 1 0.5 0 0 2D Network with 5−port Switches − Load per Channel Injection Rate = 0.1 Injection Rate = 0.3 Injection Rate = 0.5 Injection Rate = 0.7 Injection Rate = 0.9 200 400 600 800 No. of nodes in network 1000 1200 1400 (a) 3D Network with 7−port Switches − Load per Channel Injection Rate = 0.1 Injection Rate = 0.3 Injection Rate = 0.5 Injection Rate = 0.7 Injection Rate = 0.9 200 400 600 800 No. of nodes in network 1000 1200 1400 (b) 3D Network with Bus 6−port Switches − Load per Channel Injection Rate = 0.1 Injection Rate = 0.3 Injection Rate = 0.5 Injection Rate = 0.7 Injection Rate = 0.9 200 400 600 800 No. of nodes in network 1000 1200 1400 (c) Figure 4. Load per channel for 2-D, 3-D mesh and 3-D Bus architectures The bus in contrast can only deal with a single incoming or outgoing packet within a cycle of the network. This trend is evident in the simulation results, as the bus’ available band            width is necessarily limited by its design. 4. Trafﬁc Patterns and Simulation Setup The simulations were performed for a 2-D network with size n×n, for n = 3, 5, 8, 12, 15, 19, 23, 27 and 32 nodes, with packet injection rates, r , varying from 0.1 to 0.9 packets per node per cycle in step increments of 0.1. For the 3-D 7-port switch and 3-D bus architectures, the network size was n×n×n for n = 2, 3, 4, 5, 6, 7, 8, 9 and 10 with the same injection rate as in the 2-D case. The number of simulated nodes in the 2-D mesh follows the 3-D setup to provide a fair comparison. The data sample used is extracted following the warm-up phase of the network and preceding the cool-down phase to ensure reliable results. For example, Figure 3 shows the average latency growth for a 5×5×5 3D mesh network with an injection rate of 0.5. The warm-up phase occupies the ﬁrst 200 clock cycles. After 500 clock cycles the network is fully stable until the simulation ends. In our experiments samples were obtained after the network reached stability but for certain combinations of network size and injection rate, the network never becomes stable. These cases are identiﬁed in the discussion. The metrics used to quantify the performance of the network in each case are raw latency, normalized latency, and throughput, deﬁned in (4), (5) and (7) respectively. TRaw = (CF inal − CI nit ) H C (4) The raw latency, TRaw , is the distance traveled by a packet from the source to the destination address in terms of hop counts, denoted by H C . CF inal and CI nit represent the ﬁnal and initial clock cycles respectively. When the network is at zero-load, the raw latency is equivalent to the minimum distance. The normalized latency, TN orm is the ratio of the raw latency (i.e. the actual distance traveled by a packet) to the minimum distance with zero-load, TZ ero load , deﬁned as: TN orm = TRaw TZ ero load (5) The average normalized latency is the mean of the normalized latencies for the collected samples deﬁned as: TN orm Avg = mean(TN orm ) (6) In each cycle, packets arrive at all nodes at a rate based on the injection rate and the congestion level of the network. The throughput per node per cycle, λ, is deﬁned in (7), where PT otal is the total number of packets received over the simulated range, N is the number of nodes in the network and C is the number of cycles in the sampling region. λ = PT otal N × C (7) The generated trafﬁc patterns attempt to load the network in a realistic manner as would be encountered on a multi-processor SoC. A hybrid of two trafﬁc patterns are employed for all cases, uniform random trafﬁc (URT) and local trafﬁc models. The URT model stipulates that each node in the network is equally likely to become the destination address of any packet emitted from a resource. From a design perspective, it is customary to place frequently communicating resources close to each other to maximize efﬁciency. This increases the performance of the communication in terms of power, timing and resource management. The local trafﬁc pattern generated for each resource replicates this localized communication behavior. The destination address is assigned randomly and then a localized probability formula [8] is applied to the address, as shown in Equation (8) and (9). This formula increases the probability that the ﬁnal destination of the packet will be local to the sending resource rather than farther away. This follows the principle that the resources will be arranged within the network such that their nearest neighbors are devices with which they communicate with most frequently. The localized probability for local trafﬁc patterns is deﬁned in (8), where D is the maximum distance in the network and (9) is a normalizing factor guaranteeing that the sum of all probabilities is 1 [8]. P (d) = 1 (A(D)2d ) A(D) = (cid:2) 1 (2d ) (8) (9) In order to compare trafﬁc patterns, a fully URT pattern without localization was also implemented. This pattern means that any address within a given network is equally likely to be assigned to each packet being generated. To maintain stability in the network, the injection rate is lowered as the number of nodes is increased. The injection rate as a function of size is determined by Equation (15) and shown in Table 1. The load, γ , that each node puts on the network is shown in (10), where r is injection rate from 0.1 to 0.9 and H C is the average hop count in the network [9]. γN twrk = r × H C (10) We assume that a packet loads the network with each hop by 1, because it occupies 1 link per hop. Equation (10) shows that the load depends on the injection rate of each node and on the average distance of each packet. H C grows with the size of the network. In k-dimensional meshes when 1 0 200 400 600 800 No. of nodes in network (a) 2−D Network with 5−port switches and Local Traffic 1000 1200 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 A e v r e g a N o r m a i l y c n e a L d e s t 2−D Network with 5−port switches and Local Traffic Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 2.5 0 200 400 600 800 No. of nodes in network (b) 2−D Network with 5−port switches and Local Traffic 1000 1200 3 3.5 4 4.5 5 R a w a L t y c n e 2−D Network with 5−port switches and Local Traffic Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 0 0 200 400 600 800 No. of nodes in network (c) 1000 1200 0.1 0.2 0.3 0.4 0.5 h T r u p h g u o t e p r N e d o e p r C e c y l Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.5 0 0 0.2 0.4 0.6 Injection Rate per Cycle (d) 0.8 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 h T r u p h g u o t e p r e d o n e p r e c y c l Network Size=9 Network Size=25 Network Size=64 Network Size=144 Network Size=225 Network Size=361 Network Size=529 Network Size=729 Network Size=1024 Figure 5. 2-D Mesh Performance n is even, we have H C = k × n 3 In 3-D meshes H C is coincidentally H C = 3×n 3 = n, whereas for a 2-D mesh H C = 2×n 3 . Hence the total load in the network is (11) γN twrk,2D = γN twrk,3D = r × n4 2r × n3 3 (12) (13) The network capacity can be expressed as the number of links for 2-D and 3-D meshes as given in Equation (1) and (2) respectively. As the network grows, the network capacity grows and the network load grows. However, the load under uniform random trafﬁc grows faster than the network capacity. Consequently, the injection rate has to decrease as the network grows. The load per channel, γC hnl , is deﬁned as: γC hnl,3D = γN twrk,3D L3d (14) n N r 2 3 4 5 6 7 8 9 10 8 27 64 125 216 343 512 729 1024 0.75 0.67 0.56 0.48 0.42 0.37 0.33 0.30 0.27 Table 1. Injection rate for a channel load of 0.5 in a 3-D n×n×n mesh The load per channel for 2-D-mesh and 3-D mesh and 3-D bus architectures for varying injection rates is shown in Figures 4(a), (b) and (c) respectively. Substituting equations                       1 0 200 400 600 No. of nodes in network (a) 3−D Network with 7−port switches and Local Traffic 800 1000 1.2 1.4 1.6 1.8 2 A e v r e g a N o r m a i l y c n e a L d e s t 3−D Network with 7−port switches and Local Traffic Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.5 2.5 0 200 400 600 No. of nodes in network (b) 3−D Network with 7−port switches and Local Traffic 800 1000 3 3.5 4 4.5 5 5.5 R a w a L t y c n e 3−D Network with 7−port switches and Local Traffic Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.5 0 0 200 400 600 No. of nodes in network (c) 800 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 h T r u p h g u o t e p r N e d o e p r C e c y l Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.5 0 0 0.2 0.4 0.6 Injection Rate per Cycle (d) 0.8 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 h T r u p h g u o t e p r e d o n e p r e c y c l Network Size=8 Network Size=27 Network Size=64 Network Size=125 Network Size=216 Network Size=343 Network Size=512 Network Size=729 Network Size=1000 Figure 6. 3-D Mesh Performance (2), (13), into (14) leads to r = 6 × γC hnl,3D × (n − 1) n2 (15) Now in order to maintain stability, the load per channel, γC hnl,3D , is set constant. For example if γC hnl is set to 0.5, i.e. a constant load of 0.5 packets per channel, the injection rates, r , corresponding to various network sizes are given in Table 1. 5. Results The simulation results produced in this study highlight the effect of increasing the number of nodes and injection rates for different NoC topologies on network performance. The results quantify the normalised and raw latency, and throughput versus injection rate for a 2-D mesh, 3-D 7-port mesh, and a 3-D 6-port mesh with vertical bus connectivity for localized and uniform trafﬁc. Figures 5(a), (b) and (c) are plots of normalized latency, throughput and raw latency, respectively, versus network size for injection rates ranging from 0.1 to 0.9 in a 2-D mesh. Figure 5(d) is a plot of throughput versus injection rate for different network sizes. These localized 2-D simulations show that as network size increases, the normalized latency quickly reaches a saturation point whereas the throughput matches the injection rate for stable input conditions. For injection rates of 0.1 to 0.4, once the size has increased beyond 225 nodes, scaling the network size up to 1024 nodes incurs no signiﬁcant performance penalty in terms of latency or throughput due to the localized trafﬁc pattern. Figure 5(d) shows that the 2-D network investigated in this study fails to match injection rate with throughput above 0.5 packets per node per cycle; i.e. the network becomes congested and unstable. The instability of injection rates above 0.5 was conﬁrmed by an examination of the transmitter output buffers from each resource, where the network becomes unstable when the buffer ﬁlls to capacity. Figure 5(d) clearly shows the effect of an overly congested network on the throughput as the number of nodes grows. These 2-D simulations show                       3−D Network with Bus 6−port switches and Local Traffic 3.5 3−D Network with Bus 6−port switches and Local Traffic 10 Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 1 0 200 400 600 No. of nodes in network (a) 3−D Network with Bus 6−port switches and Local Traffic 0.45 800 1000 200 400 600 No. of nodes in network (b) 3−D Network with Bus 6−port switches and Local Traffic 0.7 800 1000 Injection Rate=0.1 Injection Rate=0.2 Injection Rate=0.3 Injection Rate=0.4 Injection Rate=0.5 Network Size=8 Network Size=27 Network Size=64 Network Size=125 Network Size=216 Network Size=343 Network Size=512 Network Size=729 Network Size=1000 y c n e t a L d e s i l a m r o N e g a r e v A 3 2.5 2 1.5 l e c y C r e p e d o N r e p t u p h g u o r h T 0.4 0.35 0.3 0.25 0.2 0.15 0.1 y c n e t a L w a R 9 8 7 6 5 4 3 2 0 l e c y c r e p e d o n r e p t u p h g u o r h T 0.6 0.5 0.4 0.3 0.2 0.1 0.05 0 200 400 600 No. of nodes in network (c) 800 1000 0 0 0.2 0.4 0.6 Injection Rate per Cycle (d) 0.8 1 Figure 7. 3-D Bus Performance that under a localized trafﬁc model, a bufferless 2-D mesh network can maintain relative stability, and provide acceptable performance for up to 1000 nodes with injection rates below 0.5. Figures 6(a), (b) and (c) show the normalized latency, throughput and raw latency against network size for varying injection rates in a 7-port 3-D NoC with local trafﬁc. The 3-D 7-port mesh copes with a higher level of trafﬁc better than the 2-D mesh. Where the 2-D network becomes congested at an injection rate of 0.5 as the number of nodes grows, the 3-D 7-port maintains throughput and a constant latency for all network sizes. The increased number of links per node in the 7-port 3-D design allows the network to remain stable for a wider range of network sizes and injection rates. This topology manages to match throughput to injection rate up to 0.6 packets per node per cycle. This result is interesting as it demonstrates the capability of a 3-D 7-port mesh to handle higher injection rates than a 2-D mesh for a given number of nodes. The plot in 6(a) however shows that the average normalized latency increases sharply with injection rates beyond 0.6 as the packets spend more time in output buffers at their origin resource due to congestion. Figure 6(d) is interesting because it correlates the injection rate to the throughput per node per cycle directly, so the saturation point for each network size can be determined. This understanding is paramount to designers as it determines the boundary conditions for maximum performance for different sized networks. Figures 7(a), (b) and (c) plot the average latency, throughput, and raw latency against network size, respectively for the 3-D mesh network with the vertical bus. The 3-D bus has a markedly worse performance in comparison with the 7-port 3-D topology and the 2-D mesh. Figure 7(a), shows that the average normalized latency values are lower than the corresponding values for both the 2-D and 3-D cases when the network has a lower injection rate. This is largely due to the bus only requiring 1 hop for a packet to reach its destination on any vertical layer. The bus performs well below 200 nodes for low injection rates in terms of latency; however, the throughput plotted in Figure                       URT Traffic Comparison 3−D 7−port mesh 2−D 5−port mesh 3−D 6−port bus mesh 200 400 600 800 No. of nodes in network 1000 1200 (a) URT Traffic Comparison 3−D 7−port mesh 2−D 5−port mesh 3−D 6−port bus mesh 200 400 600 800 No. of nodes in network 1000 1200 (b) URT Traffic Comparison 3−D 7−Port mesh 2−D 5−Port mesh 3−D 6−Port bus Mesh 100 80 60 40 20 y c n e t a L d e s i l a m r o N e g a r e v A 0 0 800 700 600 500 400 300 200 100 0 0 y c n e t a L w a R 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 l e c y C r e p e d o N r e p t u p h g u o r h T d e s i l a m r o N 0 0 200 400 600 800 No. of nodes in network 1000 1200 (c) Figure 8. URT Performance 7(b) begins to drop for injection rates of 0.2 and above for any network size greater than 3x3x3. This is due to packets being deﬂected horizontally around the network, as contention for the vertical bus link increases with the number of layers. This effect increases as the network size grows as the number of users requesting access to the bus grows in proportion to the number of layers. Finally Figure 7(d) clearly shows that the bus struggles to match throughput to injection rates of over 0.3. The local trafﬁc model is not friendly to the bus topology either, as the advantage the bus provides in sending a packet in one vertical hop to any layer is not fully utilized. Although the bus appears to be the worst of the three architectures under localized conditions, it may have advantages in other trafﬁc conditions, such as when a stack of memory lies above a processor, where the bus will provide equal access time to any layer of memory. The results in Figure 8(a), (b) and (c) relate to the uniform random trafﬁc model showing plots of the average normalized latency, raw latency and normalized throughput versus increasing network size respectively based on the injection rate given in Table 1 for each network size . Figure 8(a) shows that the bus has the lowest latency and that a conventional 2-D network has the highest, with the 3-D 7-port lying in-between. Figure 8(c) plots the ratio of the network throughput per node per cycle over the injection rates given in Table 1 versus the number of nodes. This shows that the 3-D 7-port switch behaves reasonably similar to the localized pattern in that it reaches a saturation point quickly for throughput and further increases in network size add little performance penalty. However the 2-D mesh and 3-D bus architectures are the worst for this trafﬁc pattern. They both have decreasing throughput with increasing network size. In the 2-D case, the packet must travel over a long distance, and the network easily becomes congested. The bus again suffers from the reduced link bandwidth per node when it has to deal with trafﬁc patterns with injection rates greater than 0.1. However, different from the local trafﬁc simulations, the bus has outperformed the 2-D mesh in terms of throughput and latency. The uniform random trafﬁc allows the bus to utilize its ability to transport a packet any distance in just one hop, and this shows through in these results. These ﬁgures appear to highlight that under uniform random patterns, the 7-port switch is the best option for large sized networks in terms of packet throughput, but trade offs such as area overhead and power may render the bus a viable alternative, especially when the vertical links are limited, and a scarce resource. 6. Discussion and Conclusions This paper has explored the scalability of two different bufferless 3-D NoC topologies in an effort to develop design guidelines for future development of massively integrated SoC devices. The cycle accurate simulation results presented clearly show the saturation points and performance in terms of latency and throughput of the different topologies as the number of nodes and layers in a 2-D and 3-D mesh network is increased. This is paramount for designers in determining the best balance between the number of features on a chip and the required communication performance. The results indicated that the 3-D 7-port switch is the best performer in terms of throughput and normalized latency as the number of nodes in a network is increased. It has the highest link per node ratio and thus the most bandwidth between the three designs. However, we have shown that a 2-D NoC with localized architecture can be scaled to a                 very large dimension with no signiﬁcant effect on throughput or latency. The TDMA vertical bus design was shown to perform the worst out of the three communication architectures in terms of scalability under local trafﬁc, as it is physically limited by its raw bandwidth due to a smaller links per node ratio and contention issues as the number of layers increase. Although it can transfer packets through many layers in one hop, it struggles to handle the increased requests and congestion as the network grows. Although shown to be weak in this paper, the bus may be appropriate for hot spot trafﬁc injection where many packets may need to be sent through several layers to a hot spot frequently. This may be akin to a processor on one layer, and a memory stack directly above it. The URT trafﬁc pattern again highlights the 7-port switch’s superior bandwidth capability. Of the three architectures, the 3-D 7-port switch is the only one that manages to gain throughput as network size increases. We aim to build on these preliminary results and carry out further investigations into different trafﬁc patterns, switch architectures, and communication protocols to quantify the performance differences in the various network topologies under more trafﬁc patterns as well as the physical constraints imposed by the horizontal and vertical interconnections. 7. Acknowledgments Funding support from the European Commission under grant F P7 − I C T − 215030 (ELITE) of the 7th framework program is gratefully acknowledged. "
