title,abstract,full_textTOPAZ - An Open-Source Interconnection Network Simulator for Chip Multiprocessors and Supercomputers.,"As in other computer architecture areas, interconnection networks research relies most of the times on simulation tools. This paper announces the release of an open-source tool suitable to be used for accurate modeling from small CMP to large supercomputer interconnection networks. The cycle-accurate modeling of TOPAZ can be used standalone through synthetic traffic patterns and application-traces or within full-system evaluation systems such as GEMS or GEM5 effortlessly. In fact, we provide an advanced interface that enables the replacement of the original lightweight but optimistic GEMS and GEM5 network simulator with limited performance impact on the simulation time. Our tests indicate that in this context, underestimating network modeling could induce up to 50% error in the performance estimation of the simulated system. To minimize the impact of detailed network modeling on simulation time, we incorporate mechanisms able to attenuate the higher computational effort, reducing in this way the slowdown of the full system simulation with accurate performance estimations. Additionally, in order to evaluate large-scale networks, we parallelize the simulator to be able to optimize memory resources with the growing number of cores available per chip in the simulation farms. This allows us to simulate node networks exceeding one million of routers with up to 70% efficiency in a multithreaded simulation running on twelve cores.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip TOPAZ: An Open-Source Interconnection Network Simulator for Chip  Multiprocessors and Supercomputers  Pablo Abad, Pablo Prieto, Lucía G. Menezo, Adrián Colaso, Valentin Puente, José-Ángel Gregorio  Departamento de Electrónica y Computadores  University of Cantabria  Santander, Spain  {abadp, prietop, gregoriol,  colasoa, vpuente, monaster} @unican.es  Abstract—As  in other computer architecture areas,  interconnection networks research relies most of the times on  simulation tools. This paper announces the release of an opensource tool suitable to be used for accurate modeling from  small CMP to large supercomputer interconnection networks.  The cycle-accurate modeling of TOPAZ can be used  standalone through synthetic traffic patterns and applicationtraces or within full-system evaluation systems such as GEMS  or GEM5 effortlessly. In fact, we provide an advanced  interface that enables the replacement of the original  lightweight but optimistic GEMS and GEM5 network  simulator with limited performance impact on the simulation  time. Our tests indicate that in this context, underestimating  network modeling could induce up to 50% error in the  performance estimation of the simulated system. To minimize  the impact of detailed network modeling on simulation time,  we incorporate mechanisms able to attenuate the higher  computational effort, reducing in this way the slowdown of the  full system simulation with accurate performance estimations .  Additionally, in order to evaluate large-scale networks, we  parallelize the simulator to be able to optimize memory  resources with the growing number of cores available per chip  in the simulation farms. This allows us to simulate node  networks exceeding one million of routers with up to 70%  efficiency in a multithreaded simulation running on twelve  cores.  Keywords;  simulator,  interconnection networks,  multiprocessor, supercomputer  chip  I.INTRODUCTION  Retrospectively,  the  research  in  the  field of  interconnection networks has mostly been restricted to  supercomputing. However, nowadays the outlook is very  different, and the advent of on-chip interconnection networks  [1] has significantly increased the relevance of the area.  Today, the interconnection network is a key element  everywhere  from  large supercomputers  [2]  to highperformance general purpose chip-multiprocessor [3][4],  through systems-on-a-chip[5]. Different environments with  particular technological constraints and system requirements  have diversified this research area. Although the foundations  of each kind of network are similar, in practice some of the  design parameters are feasible in some fields but not in  others. For example, in the on-chip context, wire availability  is profuse, making  it  feasible  to use ultra wide  communication links. On the contrary, in the off-chip  context, router implementation cost or network energy  consumptions are not first-order design parameters, allowing  the utilization of very large on-network storage. Many other  characteristics such as link latency, network interfaces, node  count, topology, etc. can also have a different impact  depending on the environment, making network proposals  very specific to the system constraints.  This diversification turns into an important handicap  when trying to design “wide-spectrum” simulation tools,  increasing the complexity and computational effort required  to cover multiple environments. As a consequence of this,  many simulation tools used in interconnection networks are  limited to a single field. For chip-multiprocessor-oriented  simulators, such as GARNET [6], which supports the  integration with GEMS [7], it seems adequate to restrict  simulator capabilities  targeting only network on chip  requeriments. On the other hand, off-chip system-oriented  tools such as INSEE [8] are focused on simulating tens of  thousands of nodes, which makes more sense  in a  supercomputing system. Finally, system-on-chip-oriented  tools, such as NOXIM [9], allow the evaluation of  heterogeneous networks, usually found in this kind of  environment. Like in many computer architecture expertise  areas, more accurate simulation  tools  imply higher  computational effort. If this tradeoff is not carefully adjusted,  simulations could present an error margin rendering  meaningless the results or the computational effort could turn  simulation time prohibitive. Systems such as SimOS [10] or  GEM5 [11] are able to dynamically adjust this tradeoff when  moving from the initial exploration of the design space to the  final product. A different solution, employed by GEMS [7],  relies on the simulation of different parts of the system with  different levels of detail. Most of network simulation tools  are not able to adapt their computational effort to different  scenarios, relying only on one of these premises; speed or  accuracy.  This work presents TOPAZ, a simulation tool with a  broad spectrum of utilization scenarios and different  tradeoffs between accuracy, simulation speed and field of  application for interconnection network research. TOPAZ  has been conceived to be embedded in other simulation tools  with limited effort. In fact, the GEMS/GEM5-TOPAZ  interface is also presented in this document. TOPAZ is  designed to be easily extendable and deeply configurable on  runtime. TOPAZ  includes multithread  capabilities,  minimizing the impact of more accurate simulations on  execution  time. As well as a description of  these  characteristics, this paper provides a brief overview of  TOPAZ construction, philosophy, capabilities, and examples  of use along with pointers  to sources of additional  information. In order to facilitate access to the tool by other  978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.19 99     researchers and simplify  the adoption of  their own  modifications, a public source code repository and project  management tools have been made available [12].  The rest of the paper is structured as follows: Section 2  describes the simulator, Section 3 explains the integration of  the tool in a full-system simulation environment, Section 4  indicates how the simulator can be used as an effective tool  in very large networks, and finally Section 5 draws the main  conclusions of this paper.  II. SIMULATOR DESCRIPTION  TOPAZ is a general-purpose interconnection network  simulator that enables the modeling of a wide variety of  message routers, with different tradeoffs between speed and  precision. TOPAZ originates from the SICOSYS [13]  simulator, which was originally conceived to obtain results  very close to those obtained by using HDL description of  network components by hardware simulators but at lower  computational cost. In order to make the tool easily  comprehensible, extensible and reusable, the design of  TOPAZ is object-oriented and has been implemented in  C++. For the models provided, approximately 100 classes,  distributed in about 50,000 lines of code have been  necessary. TOPAZ provides  router descriptions with  different levels of detail. Simplified routers are described as  a single component, which allows fast simulation times. On  the other hand, detailed routers require the description of  each router component separately, much more similar to  RTL router descriptions. Detailed routers are much more  precise, at the cost of simulation speed.  Finally, the  simulator has also support for parallel execution using  standard POSIX threads. The portability is very high and it  can be executed on any UNIX platform with a C++ standard  compiler.  Simulations in TOPAZ can be divided into three different  phases; building, running and printing. In the first phase, the  network simulated is constructed at run-time according to a  set of parameters selected by the user. TOPAZ network is  constructed hierarchically. The simulator builds the network  and the links (topology), then the network builds all the  routers and finally the routers build its intrinsic components  and interconnect them. Each simulated structure is associated  with  two C++ classes: components and  flows. The  components are descriptive, characterizing each structure  and its relationship with the remaining components of the  system. The flows establish how  the stream of  the  information will move inside the component. As an example,  for a buffer structure, the component will determine its size,  number of ports or delay, while the flow will determine its  behavior according to the flow control selected. During the  running phase, all system components are iteratively visited  and all dependent flows simulated for every cycle simulated.  Simulation length can be configured both in terms of  simulation cycles or messages injected. The flows of each  router component are run in first place each cycle, and links  interconnecting each component (both inter and intra router  links) are run subsequently. While TOPAZ is a time-driven  simulation tool, some flows can internally be constructed as  FSMs, making these components event-driven. If chosen,  results can be periodically printed during running phase.  Finally, printing phase returns all the parameters derived  from simulation process. As well as the main figures of merit  of interconnection network (latency and throughput), this  phase can be configured to produce additional details, such  as execution time, per-VC results, per-router injection rate,  per-router consumption rate, link utilization, event count  (useful for power estimations), etc.  A. Structure  The implementation of the simulator has been oriented  towards high portability and an easy learning curve. Files  present into code directories (/inc and /src), can be divided  into the following parts:   SGML constructors (*builder.cpp). Employed in the  building phase. Responsible for interpreting the specification  files and generating the objects required for the simulation.  There is a constructor for each one of the three existing  specification files described in next section.   Components & Flows. They represent the hardware that  will be simulated. The functional components of the router,  their characteristics and associated delays are modeled. In  this way, a direct relationship between a specific hardware  structure and the model that our simulator generates is  established. The constructors simply generate a hierarchy of  components related to each other.   Traffic patterns (TPZTrafficPattern.cpp). Module in  charge of the injection of packets into the network. When the  simulator is used in a stand-alone way, the main traffic  patterns (random, matrix transpose, reversal bit, perfect  shuffle, hot-spot, etc.) can be used. Additionally, trace-based  simulations are also supported. For injecting traffic generated  by the execution of real applications, an ""empty"" pattern has  been defined with the aim of working in an integrated way  with other tools.   Simulator: This module carries out the simulation  process. It progressively performs the actions belonging to  each simulation phase. This module is in charge of  interpreting all the parameters provided through commandline, making feasible the utilization of scripting based  simulation.  B. Specification files  The tool provides a solution to the generic problem of  interconnection network simulation. The same compiled  environment can be used to experiment with very different  architectural proposals. The specification of the type of  experiment is carried out through three description files  written in SGML. These files are interpreted by the simulator  at runtime, making code recompilation unnecessary if the  user wants to use any of the modules included in the  simulation. Static configuration only has been used to switch  from sequential to multithread execution. Each file specifies  the following aspects:   Simulation parameters (Simulation.sgm): Definition of  general simulation parameters such as traffic pattern and  distribution, applied load, message length, etc. In this file,  the network to be used for the simulation is also referenced.  100   Interconnection network (Network.sgm): Specifications  of the type of network through parameters such as topology  (mesh, torus, etc.), dimensions and interconnection delays.  The router employed at each network cross point is defined  as an additional parameter of the network.  Router microarchitecture (Router.sgm): Description of  the elements (memories, switches, multiplexers, etc.) which,  connected to each other, form the router. The main  parameters of each router component (delay, size, inputs,  outputs) are also described in this file.  The simulation tool employs an additional file, named  “TPZSimul.ini”. This way, it is possible to easily select one  among  the multiple  router, network and simulation  description files. Through command line, we only need to  specify the name of the simulation we want to run. In the file  ""Simulation.sgm"" we find a simulation with the same name  passed through command line, which will specify the  dynamic conditions of the simulation, i.e. traffic pattern,  packet or message length, etc. The network configuration we  want to simulate is an additional parameter in this file. In  ""Topology.sgm"" file we find a description of the selected  network. Different  topologies can be used, from bidimensional or tri-dimensional meshes, tori, to irregular  networks. Finally, router is also selected at this point, and the  file ""Routers.sgm"" contains a SGML description of all  architectural characteristics. When components have been  implemented, it is possible to instantiate them. In the  example, fifo-buffers, routing units and a crossbar are used  (detailed description). After instantiating the components,  connections have to be made within the router and between  the routers. In the instantiation, different parameters of the  components can be set, overriding global router parameters,  for example buffer capacity, which allows different  components of the router to be customized. For example,  escape channels may require different buffer size to adaptive  channels in a fully adaptive router [2].  C. Out-of-the Box Models Available  Table I enumerates network models supplied in the initial  release of the simulator. For most router models, both fast  and accurate models are supplied. Some of the components  target the CMP environment, such as low degree topologies,  whereas others are more suitable for off-chip networks, such  as 3D network interconnects. Although some combinations  are fixed, for example the Adaptive Bubble Router requires  Bubble Flow Control, it could be possible to combine  different network  topologies, router architectures and  additional features. The models provided are selected in  order to offer CMP researchers an out-of-the-box setup to  work with, but are also intended to provide a framework to  develop new components. The design style, once the  programmer is familiar with it, simplifies the implementation  of new components. This enables to implement new  components in a short period of time. As was mentioned  earlier, we provide an open platform to collaborate in the  future development of the simulator, which could facilitate  further advancement and new components. We hope that  users of the tool will share their developments with other  users.  TABLE I. COMPONENTS SUPPLIED WITH TOPAZ   Network Topologies  Ring  Mesh   (2D & 3D)  Torus   (2D & 3D)  Midimew  (2D)  Square Midimew  (2D)  Flow controls  Virtual Cut Through Bubble Flow Control  Wormhole  Virtual Channel  Flow control  Multicast Support  Source-decomposed  Path-based  DOR-Tree based Adaptive (PathTree)  Traffic Patterns  Random  Tornado  Bit-Reversal  Perfect-Shuffle Transpose Matrix  Hot Spot  Local  Trace-Based  Message Size / Packet Size  1 to unlimited / 1 to unlimited  Power consumption (Event count)  Buffer Write  Buffer Read  VC Allocation  SW Allocation  SW Traversal  Link Traversal  Routers  ID  [14]  [15]  [16][17]  Adaptive Bubble  Deterministic Bubble  Deterministic with VC  (Dally)  VCTM (Dally + MC  support)  Year  2001  1998  2001  Level of detail  complex &simple  complex & simple  complex & simple  [18]  2008  complex & simple  Rotary Router  [19][20]  Bufferless Router  Bidirectional Router   Buffered Crossbar  Pipeline Optimized  [21]  [22]  [23]  [24]  2009  2010  2009  1987  2008  complex & simple  simple  simple  complex  complex & simple  Configuration Parameters  Buffer Size  Packet Size  Number of Virtual  Channels  Number of  message types  Router Pipeline  Buffer Delay  Link Delay  Number of  physical networks  D. Multithread Implementation  In general, parallelization of computer architecture  simulators is a non trivial task, and usually the performance  benefit does not compensate the effort. The scalability of this  development  is greatly  limited by  the  fine-grained  parallelism: in a conservative implementation it is necessary  to synchronize each thread every cycle. Although speculative  approaches  are possible,  they  introduce  additional  complexity in a usually very complex piece of software. If  we consider the fact that the number of simulations required  is very large, using throughput computing is more appealing.  Nevertheless, the simulated system size can make this  approach a costly solution. In particular, today’s server farms  devoted to running these simulations use CMPs with a  101       growing number of cores per chip. Under  these  circumstances, throughput computing demands massive  quantities of DRAM per server, which increases acquisition  and costs of ownership. From the TOPAZ perspective, this  can happen if massive supercomputer networks have to be  simulated, i.e. networks with hundreds of thousands of  computing elements. In its design, TOPAZ has been  conceived to run these necessarily large networks in parallel.  For the sake of portability, Topaz parallelization is based on  POSIX threads.   As the simulation engine is time-based, parallelization of  the tool is greatly simplified. It is possible to split the  network and assign their simulation to different slave  execution threads. The boundary of the group of components  assigned to each slave thread has to be synchronized  periodically with neighboring threads. The synchronization  between slaves is done each cycle using a barrier. The master  thread is responsible for orchestrating the initialization and  finalization of the simulation and facilitating the integration  with full-system simulators. This approach abstracts the  details of simulation implementation completely to the  external full-system simulator.   III. CHIP MULTIPROCESSORS: INTEGRATION WITH FULL  SYSTEM SIMULATION TOOL  A. Topaz-GEMS Integration  TOPAZ is fully modular and can be used in an abstracted  way by full-system simulation tools. It provides the API  necessary to connect external simulators without extensively  modifying code. The developer should create an interface,  according to external simulator peculiarities and system  necessities. In order to simplify TOPAZ adoption, we  provide a detailed implementation with GEMS [7] and as  GEM5 [11]. This could be easily extended to other derived  simulation tools such as FeS2 [25]. As Figure 1 shows,  TOPAZ is connected to the memory subsystem of Ruby. As  it is a common simulator with GEM5, FeS2 and GEMS,  TOPAZ will work with all the tools seamlessly. We provide  a patch for version 2.1 of GEMS that performs the  integration. This patch could be merged with further  development using standards tools such as quilt or equivalent  ones. We also provide a clone of GEM5 development  repository.  In contrast to GARNET, TOPAZ integration is fully  isolated. In the process of compilation, the simple network  simulator of Ruby is connected to the external simulator. A  new event is introduced in the event-queue of Ruby in such a  way that if TOPAZ has to be run, the engine is activated for  the number of cycles that the network-memory clock ratio  indicates. If multithreading is activated, the main thread does  not wait for TOPAZ until the beginning of the next cycle. At  this time Ruby takes the packets delivered by TOPAZ and  injects the new ones.   Both, the modified GEMS and GEM5 are supplied as a  Mercurial repository at the public TOPAZ page [12].  Undeniably, increasing accuracy decreases simulation  performance. In full-system simulation, where thousands of  millions of CPU cycles have to be simulated in order to  obtain meaningful results, slightly decreasing simulation  speed could have a significant cost. In order to minimize  performance effects, TOPAZ provides  three different  mechanisms. First, its multithread implementation enables  network simulations to be run in a separate thread from the  main simulation. Although the usually reduced size of onchip networks makes it unnecessary to subdivide them into  separate threads, it makes sense to use master thread  abstraction to isolate network simulation from the main  thread. Second, we have developed an adaptive interface  between both simulators. When network load is low,  contention in the network is negligible and the original Ruby  network is enough to model the network. Nevertheless, when  contention is high, the optimistic latency provided by Ruby  could be tens or even hundreds of cycles below the real  value. Our adaptive interface works as follows: when the  number of packets in the network is below a predefined  threshold, TOPAZ is disabled and only Ruby is simulated. If  this threshold is surpassed, TOPAZ is activated and will  remain activated until a second threshold number of packets  is surpassed. Finally, the third technique is based on the  modification of the router complexity. For almost all routers  provided, two different accuracy models can be used, making  it possible to use simplified models in initial design phases  and more detailed models in later stages.  No modification has been performed in GEMS’s nor  GEM5’s original code besides the creation of the interface to  connect Ruby to TOPAZ.   Simics  Simics  Thread1  Opal  PTLSim  M5  Ruby  Topaz  Thread2  Figure 1 Topaz-Ruby Connection.  B. Effects of Network Simulation Accuracy  The original Ruby network simulator is quite simple: it  models contention only at link level. Although this speeds up  simulation, it introduces a non negligible error in network  latency estimation when the load applied is high. Recent  versions of GEMS/GEM5 have introduced the possibility of  using GARNET [6] as a replacement for the original  simulator. Although much more detailed than the original  simulator, GARNET has limited flexibility with fixed router  architecture. It only contemplates changing a few parameters  in the network such as number of virtual channels, router  pipeline stages, buffer size and flit size. TOPAZ is much  more flexible, providing a full set of very different router  architectures, routing algorithms, network topologies, flowcontrols, etc. The TOPAZ-Ruby  interface has been  developed  to support any CMP or SMP network  configuration, using both file-defined topologies and regular  102       ones. TOPAZ does not replace the original Ruby network  simulator but is added to it, in order to be able to  dynamically activate or deactivate it to speed up the  simulation during phases of low traffic. During those  moments contention should be negligible and therefore  results obtained by precise router modeling are usually close  to those provided by a simpler router model.  In order to demonstrate how relevant accurate network  simulation can be, we have carried out the simulation of very  diverse applications and coherence protocols. We will  restrict  this study  to GEMS. Twenty workloads are  considered in this study, including both multi-programmed  and multi-threaded applications (numerical and server)  running on top of the OpenSolaris 10 OS. The numerical  applications are the whole NAS Parallel Benchmarks suite  (OpenMP  implementation version 3.2 [26]) and four  benchmarks of  the PARSEC suite [27]. The server  benchmarks correspond to the whole Wisconsin Commercial  Workload suite [28], released by the authors of GEMS in  version 2.1. The remaining class corresponds to multiprogrammed workloads using part of the SPEC CPU2006  suite running in rate mode (where one core is reserved to run  OS services) [29]. Each application is simulated multiple  times with random perturbations in memory access time in  order to reach 95% confidence intervals. The number of  applications enables the sweeping of a broad spectrum of  application types, with diverse network demands in order to  know the margin of error caused by different network  modeling.   TABLE II. EVALUATED WORKLOADS (PROBLEM SIZES).  Multithreade d-Workloads  Wisconsin  Commercial  Workload  NAS Parallel  Bench.  Apache (1000 Surge  dynamic)  Zeus (1000 Surge Static)  Jbb (4000 SpecJbb)  OLTP (500 TPC-C alike)  BT (Class A)  FT (Class W)  LU (Class A)  SP (Class A)  CG (Class A)  IS (Class A)  MG (Class W)  UA (Class A)  PARSEC  blackscholes (native)  canneal (native)  fluidanimate (native)  streamcluster (native)  Multiprogram medWorkloads  Spec 2006 (Rate  Mode)  astar (reference)  hmmer (reference)  lbm (reference)  ommetpp (reference)  We have used a 16-processor CMP, with out-of-order  Nehalem [30] like cores, using a 4×4 mesh network which is  provided in the version 2.1 of GEMS. We analyzed how the  system behaves with the two coherence protocols provided:  MOESI_CMP_token  and MOESI_CMP_directory. No  change has been made in either of the two coherence  protocols. All the configuration parameters chosen in the  comparison are provided in the GEMS Mercurial repository  provided in [12].  We adjust all simulators  to have similar router  configurations. In particular we chose the fixed 5-cycle  wormhole pipeline router provided by GARNET [6]. We  will assume 1 cycle wires between routers. In the simple  Ruby network simulator, the latency is adjusted to match  GARNET routers. We use four virtual networks to avoid  end-to-end deadlock and 10 flits of 16 bytes of buffer per  virtual channel, having only one virtual channel per traffic  class. TOPAZ is configured using the same router in its  simple and complex implementation and matching all  configuration parameters. The two routers are provided in  out-of-the-box components.  C. MOESI_CMP_directory Coherence Protocol  This coherence protocol is characterized by a limited  network load, therefore it can be considered as baseline  where careful network simulation has lower impact on final  system performance. Figure 2 shows  the normalized  execution time for the workloads considered. On the one  hand, it is clear that contention modeling of the original  simulator is too optimistic, inducing a substantial error in the  estimation of the execution time of each application.  Although in some cases the effect is lower, such as for  blacksholes and fluidanimate, in others it is surprisingly  high. If we compare TOPAZ’s complex and simple  implementations the differences are small but appreciable  with respect to other simulators. Taking into account that the  tool from which TOPAZ derives [13] is capable of achieving  less than 3% error  with respect to hardware simulators, it  seems reasonable to use TOPAZ as a reference point.   On the other hand, GARNET network modeling seems to  be too pessimistic. We used the publicly available tool,  compiled and run using all the benchmarks considered. We  prefer not to modify it in order to allow other researchers to  repeat the evaluation. Like in the case of the original network  simulator, with low load applications the error is small but  with high contention applications the effect is substantially  increased to almost three times the application execution  time differences. This behavior can be easily reproduced  with the configuration used, which is available in [12].  On average, the original network simulator introduces an  optimistic error of 25% in execution time and GARNET a  pessimistic one of 25%. Therefore, even in a not very  demanding scenario, inaccurate modeling of the contention  in the interconnection network could induce substantial  errors  in evaluation results, which could render  the  hypothetical comparison of two or more architectural  solutions for the CMP unreliable. Note that original Ruby  only models contention at link level, which is optimistic  given the contention suffered at the crossbar by conventional  routers, and GARNET seems to have some kind of error in  pipeline implementation.  Figure 3 shows the performance of each simulator in  terms of Ruby network normalized simulated cycles per  second of CPU. We have used the same hardware platform  to perform this measurement with a large number of samples  in order to have reliable averages. As can be appreciated, the  complex TOPAZ router has a non negligible impact on  performance, increasing the simulation time by 20 % on  average. In some applications, such as CG, performance  could be degraded by up to 50%. As can be seen, using  simple models could attenuate this slowdown on average,  103   e m i T n o i t u c e x E d e z i l a m r o N Y B U R 3  2.5  2  1.5  1  0.5  0  Figure 2. Directory Coherence Protocol: Execution time differences through different network simulators.  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  0  d n o c c e s / d e t l a u m i S s l e c y C d e z i l a m r o N RUBY  GARNET  TOPAZ_SIMPLE  TOPAZ_COMPLEX  RUBY  GARNET  TOPAZ_SIMPLE  TOPAZ_COMPLEX  Figure 3 Directory Coherence Protocol: Simulator Performance differences through different network simulators  obtaining reasonable accuracy with  less  than 5% of  performance degradation. Surprisingly, GARNET is the best  performer, improving on the original gems simulator by an  average of 10%, which is motivated by its pessimistic  contention modeling. It increases the average latency  perceived by the processors in such a way that the activity in  the rest of the system falls, increasing the number of cycles  simulated per clock cycle. In Topaz simple, this also happens  in workloads with low parallelism at instruction level, in  which processor activity is lower than in Ruby.   In contrast, Topaz complex compensates the lower  computational cost of modeling the processors with the  higher network activity, which makes the number of cycles  simulated per second almost constant.  D. MOESI_CMP_token Coherence Protocol  In contrast to directory, this coherence protocol is  characterized by intense network requirements due to  multicast traffic. This example could be considered as a  reference point where careful network simulation has a large  impact on final system performance and simulation speed.  Given the fact that GARNET has no native support for  multicast traffic and this protocol uses it heavily, we decided  to exclude GARNET in the comparison. Our simulator does  have support for this kind of traffic. Note that broadcastbased coherence protocols heavily depend on network  support for multicast [18].  Figure 4 shows how the CMPs perform for each  application and as the coherence protocol is characterized by  a larger bandwidth requirements [31], the contention in the  network will be higher and consequently the optimistic  modeling of GEMS induces a larger error in the execution  time of the workloads. In some cases, such as Apache and IS  the observed error is above 100%, which is much higher than  that observed in the case of directory.  As mentioned before, more network load implies a higher  computational workload for the network simulator and a  slowdown in simulation time, as Figure 5 indicates. Now the  performance reduction with the most detailed router falls on  average by 40%. Simple implementation attenuates this fall  by 5-10%. For some applications such as CG,  the  performance falls 80% i.e., simulation is five times slower.  Under  these conditions,  in  this protocol we explore  additional performance optimizations such as an adaptive  interface (denoted AI in the results). Even with such a small  network and simple router, the unbalance between network  simulation and the remaining components of the system keep  multithreaded simulations (denoted P  in  the results)  interesting, especially if it is required to run a particular  application faster or maximum precision is required for  network model. If a large batch of runs is required, in most  cases to run sequential simulations will be more efficient  because it requires two cores and in most cases speedup is  lower than two. In contrast, adaptive interface is able to  improve simulation performance significantly, reducing the  gap with the original Ruby simulator by more than 10%,  with an error below 2% for both simple and complex routers.  The data provided have been obtained using a threshold of  25 packets in the network before turning on the Topaz  simulator. Only with applications such as streamcluster is  the error relevant. Even when using 25 in-flight messages as  a threshold, the traffic pattern could create significant  contention, for example, if most traffic is highly localized  around some specific parts of the system.  104                       Figure 5 Token Broadcast Coherence Protocol: Performance differences through different TOPAZ optimizations.  IV. SUPERCOMPUTERS INTERCONNECTION NETWORK  EVALUATION  This class of system has the singular characteristic of  having a massive number of routers. The biggest challenge  for a simulation tool for this kind of system is when the  number of nodes grows significantly and accurate router  modeling is paramount to discover potential instabilities in  the system [32]. When thousands of nodes have to be  simulated,  the memory required could be significant.  TOPAZ could use an advanced memory allocation approach  which speeds up execution and limits further potential  problems during the simulation. Additionally, multithread  implementation enables  taking advantage of current  multicore server predominance.   To show the scalability limit of the tool, like in the case  of IBM Blue Gene systems [2], we use a 3D torus with the  simple model of the Bubble Router [14]. We evaluate the  performance obtained with up to one million of routers in the  network in order to determine the scalability of the  parallelization. Figure 6 presents the results obtained with a  server with 12 cores and with 54GBytes of main memory  based on Intel Xeon E5645. As we can observe, the  scalability with such large systems is adequate. With 32K  routers the simulation uses approximately 1.5GB of memory,  5.5GB for 128K nodes, 12GB for 256K nodes, 24GB for  512K routers and for 1 million it uses 49GB. As the number  of nodes grows the speedup decreases slightly. When the  number of routers is higher, the data synchronization  between threads is more demanding for the memory  hierarchy of the server. With 12 threads there is performance  degradation due to threads unbalance due to non divisible  number of nodes.   TOPAZ has also demonstrated to be a suitable tool for  dealing with such a large system, because the speedup  achieved maximizes core utilization on the server. Running  the simulation for such large systems using sequential  simulations would be prohibitive due to the massive amount  of memory needed to provide 12 or more cores with the  Figure 4 Token Broadcast Coherence Protocol: Execution time differences through different network simulators Figure 6 (a) Simulation Time for 50K Cycles (b) Speedup Observed.  0  0.5  1  1.5  2  2.5  Y B U R N o r m a i l z c e x E d e u i t T n o i m e RUBY  TOPAZ_SIMPLE  TOPAZ_COMPLEX  (AI)TOPAZ_SIMPLE  (AI)TOPAZ_COMPLEX  0  0.2  0.4  0.6  0.8  1  1.2  1.4  N o r m a i l z e c y C d e l S s i m a u t l d e s / c c e d n o RUBY  TOPAZ_SIMPLE  TOPAZ_COMPLEX  (AI)TOPAZ_SIMPLE  (AI)TOPAZ_COMPLEX  (P)TOPAZ_COMPLEX  (P)TOPAZ_SIMPLE  0.E+00  1.E+05  2.E+05  3.E+05  4.E+05  5.E+05  6.E+05  1  3  5  7  9  11  S i m a u l t i T n o i m e ( c e S d n o s ) Number of Cores  0  1  2  3  4  5  6  7  8  1  3  5  7  9  11  p u d e e p S Number of Cores  32K Rotuers  Routers  128K Routers  256K Routers  512K Routers  1M Routers  105                                 memory requirements. According  to  the system size,  scalability and the memory available in the server, the user  could select the optimal number of threads per simulation.  For example, in our case, if the number of routers is 256K,  the optimal number of simulations to run in our server is  three devoting for each one 4 cores. We would have 36GB  memory utilization and almost a perfect speedup.  V. CONCLUSIONS  We have presented TOPAZ, which is a comprehensive  and extensive tool conceived to facilitate interconnection  network research. Its integration with one of the most  common evaluation platforms in CMPs and its flexibility to  simulate large-scale interconnection networks could make  TOPAZ an attractive tool for a wide range of users. Given  the current usage of GEMS, we hope the tool will attract  many active users. In the long term, we will provide  continued support for the tool, as it is one of our main  resources for performing our research. The open source  approach will also simplify how third party users can make  their own contributions to the tool.  ACKNOWLEDGMENTS  The authors would like to thank José Ángel Herrero for  his valuable assistance with computing environment, and the  anonymous reviewers for many useful suggestions. This  work has been supported by the Spanish Ministry of Science  and Innovation, under contract TIN2010-18159, and by the  HiPEAC European Network of Excellence.   "Déjà Vu Switching for Multiplane NoCs.,"In chip-multiprocessors (CMPs) the network-on-chip (NoC) carries cache coherence and data messages. These messages may be classified into critical and non-critical messages. Hence, instead of having one interconnect plane to serve all traffic, power can be saved if the NoC is split into two planes: a fast plane dedicated to the critical messages and a slower, more power-efficient plane dedicated only to the non-critical messages. This split, however, can be beneficial to save energy only if system performance is not significantly degraded by the slower plane. In this work we first motivate the need for a timely delivery of the ""non-critical"" messages. Second, we propose Déjà Vu switching, a simple algorithm that enables reducing the voltage and frequency of one plane while reducing communication latency through circuit switching and support of advance, possibly conflicting, circuit reservations. Finally, we study the constraints that govern how slow the power-efficient plane can operate without negatively impacting system performance. We evaluate our design through simulations of 16 and 64 core CMPs. The results show that we can achieve an average NoC energy savings of 43% and 53%, respectively.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip D ´ej `a Vu Switching for Multiplane NoCs Ahmed K. Abousamra, Rami G. Melhem University of Pittsburgh Computer Science Department {abousamra, melhem}@cs.pitt.edu Alex K. Jones University of Pittsburgh Electrical and Computer Engineering Department akjones@ece.pitt.edu Abstract—In chip-multiprocessors (CMPs) the network-onchip (NoC) carries cache coherence and data messages. These messages may be classiﬁed into critical and non-critical messages. Hence, instead of having one interconnect plane to serve all trafﬁc, power can be saved if the NoC is split into two planes: a fast plane dedicated to the critical messages and a slower, more power-efﬁcient plane dedicated only to the non-critical messages. This split, however, can be beneﬁcial to save energy only if system performance is not signiﬁcantly degraded by the slower plane. In this work we ﬁrst motivate the need for a timely delivery of the “non-critical” messages. Second, we propose D ´ej `a Vu switching, a simple algorithm that enables reducing the voltage and frequency of one plane while reducing communication latency through circuit switching and support of advance, possibly conﬂicting, circuit reservations. Finally, we study the constraints that govern how slow the power-efﬁcient plane can operate without negatively impacting system performance. We evaluate our design through simulations of 16 and 64 core CMPs. The results show that we can achieve an average NoC energy savings of 43% and 53%, respectively. I . INTRODUCT ION Power efﬁciency has become a primary concern in the design of chip-multiprocessors (CMPs). A signiﬁcant percentage of a CMP’s power budget is spent in the network-on-chip (NoC). The on-chip interconnect of Intel’s TeraFLOPS processor architecture consumes more than 28% of the chip’s power budget [1]. Owens et al. [2] estimate that the interconnect consumes 20% of the CMP power. Reducing the NoC power is critical for scaling up the number of cores in future CMP systems. A CMP has processing cores with one or more levels of private caches and often one or more levels of shared cache. Examples include Intel’s Xeon processor [3] and AMD’s Opteron processor [4]. One of the primary beneﬁts of CMPs is having fast on-chip communication, which makes them very attractive for running parallel workloads. The threads of parallel workloads often share data resulting in multiple copies of the same data simultaneously existing in the private caches of different cores. The NoC allows the communication necessary for exchanging data and ensuring data coherence; thus, fast communication is critical to system performance. Although fast communication is critical, not all messages need to be urgently delivered. In particular, consider the interconnect trafﬁc comprised of cache coherence and data messages. When an instruction needs to access a data word but misses in the local private cache(s), a request for the cache This work is supported, in part, by NSF award CCF-1064976. line containing the required word is sent to the line’s home node in the next level(s) of shared cache. Depending on the cache coherence protocol, different coherence messages may be exchanged such as invalidations to the current sharers of the line, acknowledgments to the invalidation requests, and sending a copy of the cache line to the requesting core. The instruction remains stalled until it is able to access the required data word. The request message along with the other coherence messages and ﬁnally the required data word are all on the critical execution path of the instruction. However, the rest of the words in the requested cache line are not critical to the execution of the stalled instruction. This observation intuitively suggests that instead of having one interconnect plane serving all the cache trafﬁc, the NoC may be physically split into two planes: A fast control plane for serving the critical trafﬁc, and a power-efﬁcient data plane that operates at a lower voltage and frequency and serves the non-critical trafﬁc. However, how slow the powerefﬁcient plane can operate is contingent upon not degrading performance, since any of the slowly traveling non-critical words of a requested cache line may actually become critical for a subsequently executing instruction. Interestingly, the relation between performance and energy - energy is power integrated over time - is not a simple tradeoff; if performance drops, execution time increases, possibly causing more energy consumption, which is counterproductive. In this work we address this challenge in two steps: First, we propose D ´ej `a Vu switching, a simple algorithm that compensates for the speed reduction of the power-efﬁcient plane by: 1) Simplifying the slow plane’s design to be circuit switched while using the fast plane to do all the routing decisions, and 2) Speeding up circuits’ conﬁguration through a novel resource reservation scheme that allows reserving conﬂicting circuits while guaranteeing correct routing. Second, we analyze how slow the power-efﬁcient plane can operate by studying the constraints that relate the plane’s speed to system performance. The paper is organized as follows. Section II presents an overview of the split-plane NoC design and motivates the need for the timely delivery of the “non-critical” trafﬁc. Section III describes D ´ej `a Vu switching and the associated resource reservation scheme. In Section IV we study the constraints that govern the speed of the slow plane. Evaluation and simulation results are presented in Section V. Related work is described in Section VI. Finally, Section VII concludes the paper. 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.9 11 Ϭй ϱй ϭϬй ϭϱй ϮϬй Ϯϱй ϯϬй ϯϱй ϰϬй ϰϱй ϱϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ ƌ Ǉ Ă ƌ ƚ Ğ Đ Ă Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ   ŵ Ŷ Ă Ğ й  Ž Ĩ  > ϭ  ŵ ŝ Ɛ Ɛ Ğ Ɛ   ƚ Ă Ś ƚ Ă ƌ Ě Ğ Ǉ Ă Ğ Ě Ğ  ů   Ś ŝ Ɛ ƚ Fig. 1: Percentage of delayed hits out of all L1 misses using the baseline NoC. I I . S PL IT- PLANE INTERCONNECT DES IGN FOR SAV ING POWER AND ENERGY A split-plane NoC design can be beneﬁcial for power saving. The baseline link bandwidth can be physically partitioned into two planes: A control plane that is as fast as the baseline NoC and dedicated to the critical messages, and a power-efﬁcient data plane dedicated to the non-critical data messages. For example, 16-byte links may be split into 6-byte links for the control plane and 10-byte links for the data plane. The segregation of the network alone allows more efﬁcient use of resources resulting in power savings. Data messages (e.g. cache lines) are large, while control messages (e.g. data requests, invalidations, acknowledgments, etc.) are much smaller. Thus, data messages beneﬁt from wider links; the wider the links the fewer ﬂits that are transmitted, leading to less trafﬁc contention and serialization delay. In contrast, control messages need links that are just wide enough to ﬁt any message in a one-ﬂit packet. In a single plane conﬁguration, control messages waste power in buffers and links due to the underutilized link width. Hence, sending control and data messages on two different planes saves power due to the more efﬁcient use of the buffers and bandwidth resources, even when both planes are as fast as the baseline (see simulation results in Section V). In addition, reducing the data plane’s voltage and frequency allows further power savings. However, slowing the data plane raises the following question: How important to performance are the latencies of the messages that travel on the data plane? To answer this question, we simulate different parallel benchmarks on a 16-core tiled CMP. We assume a CMP architecture having a private L1 cache for each core, and a distributed shared L2 cache, with a single plane packetswitched 2D mesh interconnect of 16-byte links (simulation details can be found in Section V). The requests and coherence messages are all one-ﬂit long while data messages are ﬁve-ﬂits long. We apply the critical word ﬁrst technique to the data messages, i.e., the ﬁrst word received in a data message is the required data word by the instruction that suffered the local cache miss. The other words of the cache line are ordered in the data message in ascending order of physical address. Once the critical word is received, the pending instruction is granted access to the word to complete execution. A subsequent instruction may miss in the same cache line before the line is completely received. When such a miss occurs, the pending instruction is allowed access to the required word once received instead of waiting until the entire line is received. We refer to this miss as a delayed cache hit. Speciﬁcally, a delayed cache hit is a miss for an already requested cache line. The latency to service such a miss is longer than a cache hit but shorter than a regular miss. Figure 1 shows the percentage of L1 misses that are delayed cache hits. Although the percentage varies for different benchmarks, we can see that in general delayed hits represent a signiﬁcant percentage of the misses. Accordingly, we should be careful how much we slow down the data plane; ideally, the last ﬂit should arrive without delay. In Section IV we study the constraints that limit how slow the data plane can operate without degrading performance, but ﬁrst we describe the routing of the data plane trafﬁc. I I I . D ´E J `A VU SW ITCH ING FOR MULT I -PLANE INTERCONNECTS D ´ej `a Vu switching is proposed for routing trafﬁc on the slow data plane, while regular packet switching is used for routing trafﬁc on the control plane. Figure 2 shows the control and data planes’ router models. Assuming a mesh topology, each router has 5 input/output ports: north, east, south, west, and local and uses credit-based ﬂow control. The control plane’s router stages (buffer write (BW), route computation (RC), virtual channel allocation (VA), switch arbitration (SA), switch traversal (ST) and link traversal (LT)) are pipelined [5]. Optimizations such as lookahead routing and aggressive speculation allow some of the stages to be performed in parallel [5, 6]. We assume that the router pipeline is composed of 3 stages: BW/RC, VA/SA, ST+LT. Although we use X-Y routing and design the control plane such that all types of control packets are one-ﬂit long, virtual channels (VCs) are still used for different cache protocol message types to avoid protocol deadlocks. Conversely, the data plane carries only data messages, which are consumed at their destinations. Depending on the routing algorithm, the data plane may not require VCs, as is the case, for example, in mesh X-Y routing. To reduce communication latency on the data plane, we design it as a reconﬁgurable express switching plane such that data packets travel on circuits without suffering the delays of making routing decisions at every router. A circuit is composed of a set of consecutive network links and the crossbar connections that join these links. A crossbar connection connects an input port of a router to an output port. A circuit starts with the connection joining the source node’s local input port to the path’s ﬁrst network link, and similarly ends with the connection joining the path’s last network link to the destination node’s local output port. Circuits are established through the help of the control plane. Before sending a data packet the source node sends a reservation packet (r-packet) on the control plane to the data packet’s destination node. The r-packet establishes the circuit on the data plane by reserving the crossbar connections along the path. When a crossbar connection is realized in a data plane 12 ŽŶƚƌŽů WůĂŶĞ ZŽƵƚĞƌ /ŶƉƵƚϬ ͘  ͘  ͘  ͘  ͘  ͘ /ŶƉƵƚϰ ĂƚĂ WůĂŶĞ ZŽƵƚĞƌ /ŶƉƵƚϬ ͘  ͘  ͘  ͘ /ŶƉƵƚϰ ƌŽƐƐďĂƌ^ǁŝƚĐŚ s ϭ s Ŷ ͙ ͘ ͘ /ŶƉƵƚƵĨĨĞƌƐ s ϭ s Ŷ ͙ ͘ ͘ /ŶƉƵƚƵĨĨĞƌƐ ZŽƵƚĞ ŽŵƉƵƚĂƚŝŽŶ sůůŽĐĂƚŽƌ ^ǁŝƚĐŚůůŽĐĂƚŽƌ /ŶƉƵƚWŽƌƚƐ /ŶƉƵƚϬ KƵƚƉƵƚϬ KƵƚƉƵƚWŽƌƚƐ ZĞƐĞƌǀĂƚŝŽŶ YƵĞƵĞƐ ͘  ͘  ͘  ͘  ͘  ͘  ZĞƐĞƌǀĂƚŝŽŶ YƵĞƵĞƐ /ŶƉƵƚϰ KƵƚƉƵƚϰ DĂƚĐŚŝŶŐ,ĞĂĚƐŽĨYƵĞƵĞƐ /ŶƉƵƚƵĨĨĞƌ KƵƚƉƵƚϬ ͘  ͘  ͘  ͘  ͘  ͘ KƵƚƉƵƚϰ KƵƚƉƵƚϬ ͘  ͘  ͘  ͘  ͘  KƵƚƉƵƚϰ /ŶƉƵƚƵĨĨĞƌ ƌŽƐƐďĂƌ^ǁŝƚĐŚ Fig. 2: Diagrams of the control and data plane’s routers (not to scale). router, it remains intact until the tail ﬂit of the data packet that crosses the connection leaves the output port; at which time the crossbar connection is removed, making the input and output ports of the connection available again. Note that routing an r-packet slightly differs from routing other types of messages on the control plane; in addition to competing for the output port of the control plane router, an r-packet needs to successfully reserve the required crossbar connection on the corresponding data plane router. If the required connection cannot be reserved, the r-packet waits at the router until it successfully reserves the connection. Since data packets carry cache lines that are mostly supplied by the last level shared cache1 , the r-packet can be sent as soon as a cache hit is detected. Using separate tag and data arrays in the cache enables early detection of cache hits since the tag match operation requires fewer cycles than reading the contents of a cache line. However, the beneﬁt of sending the r-packets early is reduced if a packet has to wait to reserve a crossbar connection because one or both ports of the connection on the data plane are in use by an active crossbar connection. D ´ej `a Vu switching is designed to overcome this problem by supporting advance sequential reservations of conﬂicting circuits, thus allowing r-packets to continue making progress towards their destinations, while making provisions for a crossbar connection on the data plane to be established when both ports become available. Essentially, at any point in time, a port of a data plane router can be part of multiple reserved connections that are to be sequentially realized. These reserved connections route different data packets, which 1 The ﬁrst level(s) private caches can also send data packets (write-back messages) containing the modiﬁed version of an evicted cache line. traverse the port some time after their r-packets traverse the corresponding port on the control plane. Thus, the data plane always experiences d ´ej `a vu; data messages replay the history of the reservation packets by traversing router ports in the same order in which the r-packets traversed the corresponding ports on the control plane. D ´ej `a Vu switching can be applied, in general, to interconnects in which reservations are done on a plane separate from the data plane. All packets that travel on plane Pd , which uses D ´ej `a Vu switching, travel on circuits that are established by reservation packets, which travel on a separate plane, Pc . The packets on Pd mimic the paths traveled by their corresponding r-packets - thus placing no restrictions on the routing algorithm of Pc . The advantage of D ´ej `a Vu switching is that it simpliﬁes the design of Pd ’s routers and does not stall or drop a circuit reservation due to a conﬂicting earlier one. Rather, it allows reservations to proceed, hence speeding up the reservation process and improving the communication latency on Pd (see Section V), while guaranteeing correct routing as described below. A. Connection Reservation and Realization with Head of Queues Duo Matching (HQDM) Intuitively, in each data plane router each input port should track the reserved connections it is part of. In particular, an input port needs to keep track of the reserved output ports to which the input port should be connected in the future. However, this is not enough to guarantee correct routing of data packets. For example, consider two reservation packets, ra and rb and their corresponding data packets da and db . Assume ra arrives at the west input port of the control plane router Ri , and rb arrives at the east input port of Ri , and that each of ra and rb make a future reservation for the north output port of Ri . When the north output port becomes available, the question arises: which connection should be realized next, is it the west-north or the east-north? The answer depends on which of ra and rb reserved the output port ﬁrst, because the r-packet that reserves the port ﬁrst will also traverse it if ra did, then ra arrives at ﬁrst. Hence, the south input port of the neighbor router, Rj , before rb . Consequently, Rj routes ra before rb , i.e., the south input port of Rj records the connection reservation of ra before that of rb . Therefore, correct routing requires that da traverses the north output port of Ri on the data plane before db . In general, to guarantee correct routing of data packets, a number of conditions must be satisﬁed: (1) Since a connection is reserved by simultaneously reserving an input and an output ports, each input and output port needs to independently track its reserved connections. (2) If two r-packets, ra and rb , share part or all of their paths, the order in which they traverse the shared links must be the same for all their shared links; this guarantees that each data packet mimics the movements of the correct r-packet. (3) Finally, since data packets follow the footsteps of their r-packets, every node must inject data packets onto the data plane in the same order their corresponding reservation packets are injected onto the control plane. 13 TABLE I: Pseudo-code summarizing the routing actions performed on the control and data planes as part of D ´ej `a Vu switching. Routing r-packets from input port pi to output port po on the control plane Wait until Qout (pi ) and Qin (po ) are not full. Compete for port po on the control plane router When po is granted: - Add pi to the end of Qin (po ). - Add po to the end of Qout (pi). Routing on the data plane If input port pi is Free then Let po be at the head of Qout (pi). Wait until po is Free and pi is at the head of Qin (po ) then - Realize the crossbar connection pi − po - Change status of pi and po to Busy - Dequeue the head of the reservation queues of pi and po If input port pi is connected to an output port, p(cid:2) When tail ﬂit is seen change status of the input port pi and the output port p(cid:2) o to Free. o To satisfy condition (1) each input port, pi , of a data plane router maintains an ordered queue, Qout (pi ), of the reserved future output ports to which pi should connect. Similarly each output port, po , maintains an ordered queue, Qin(po ), of the future input ports to which po should connect. Reserving the input-output port connection pi - po is accomplished by adding po to the end of Qout (pi ), and adding pi to the end of Qin(po ). If either queue is full, the reservation cannot be completed at this time. Note that the length of all reservation queues maintained by all ports is the same, and is equal to the number of allowed future reservations. Satisfying condition (2) can be achieved by allowing rpackets to travel only on one virtual channel (VC). Note that a VC may be dedicated for r-packets to avoid blocking other types of messages. Finally, condition (3) can be easily satisﬁed by using a queue to keep track of the order of sent r-packets whose data packets are not yet sent. Realizing a crossbar connection: The input-output port connection pi - po is realized in the crossbar of the data plane router only when both: (a) pi and po are free (not part of any current connections) and (b) the output port at the head of Qout(pi ) is po and the input port at the head of Qin (po) is pi , i.e., a matching of the input and output ports takes place. Once a connection is realized, its reservation is removed from Qin(po ) and Qout (pi ). The connection remains active until the tail ﬂit of the data packet that traverses this connection exits through po . We refer to this reservation scheme as Head of Queues Duo Matching (HQDM). Table I presents pseudocode that summarizes the actions taken on the control and data planes as part of the D ´ej `a Vu routing algorithm. IV. ANALY S I S O F ACCEPTABLE DATA PLANE SLOWDOWN We study the constraints that limit how slow the data plane can operate without negatively impacting performance. First, since a data packet cannot move ahead of its reservation packet, it is inefﬁcient to have data packets catch-up with their r-packets; rather, the data plane should be further slowed down to save power. Second, the transmission time, tc , of critical words on the two-plane NoC should be no longer than tc on the baseline NoC. Developing this constraint depends on which of the two planes critical words are sent. For simplicity we choose to keep the critical word as part of the data packet such that it is the ﬁrst word in the packet - note that there is no critical word for a write-back message. Finally, since delayed cache hits represent a signiﬁcant percentage of cache misses (see Fig. 1), the transmission time, tl , of a cache line on the data plane should be no longer than tl on the baseline NoC. These constraints help us compute the factor S by which the data plane can be slowed relative to the baseline NoC. 1) R-packet arrives at a router before the data packet: Assume that an r-packet (one ﬂit) is sent on the control plane k cycles in advance of the corresponding data packet (Note that k depends on the cache design). In the following inequality we compare the time the r-packet takes to traverse h routers relative to when the data packet is injected (right-hand side), with the time it takes the data packet’s head ﬂit to traverse h routers (left-hand side): hcS + hβcS > (h − )xc ⇒ S > (1) k x (xh − k) h(1 + β) k Where c is the cycle time on the control plane, S is the slowdown factor which we want to compute, such that cS is the cycle time on the data plane and is enough to traverse one network link; β is the average delay cycles incurred per router due to contention with existing reservations on the data plane, and x is the number of cycles incurred per hop (routing + link traversal) on the control plane. Speciﬁcally, in the lefthand side, hcS is the time needed for traversing h routers and links by the data packet’s head ﬂit, in the absence of contention delays and assuming that the required crossbar connection at each router is realized before the head ﬂit needs to cross it. hβcS is the total contention delay suffered by the head ﬂit while traversing h routers. In the right-hand side, x is the number of routers traversed by the r-packet during k cycles, and xc is the time needed by an r-packet to traverse one hop on the control plane. Notice that r-packets should experience minimal contention delays since they either travel on a dedicated plane or share a plane with only the cache coherence request and control messages, and are allowed to make future reservations and continue advancing to their destinations. In the rest of the analysis we assume that this constraint is already met, i.e., we assume that an r-packet is always ahead of the corresponding data packet. 2) Critical words are not delayed: Assuming the head ﬂit carries the critical word, we are interested in the transmission time, th , of the data packet’s head ﬂit on both the baseline NoC and the data plane; th on the data plane should not be longer than on the baseline NoC, that is: hcS + hβcS ≤ hxc + hβc ⇒ S ≤ x + β 1 + β (2) In the ﬁrst inequality the left-hand side computes th across h routers on the data plane and the right-hand side computes th on the baseline interconnect also across h routers. 3) Delayed cache hits are not overly delayed: A delayed cache hit needs access to a word which is part of an already 14 VC usage For coherence request and control messages For r-packets For data packets Baseline NoC (16-byte links) 3 VCs, each 2 ﬂits (2 packets) wide N/A 1 VC, 10 ﬂits (2 packets) wide Proposed NoC - Control Plane (6-byte links) 3 VCs, each 2 ﬂits (2 packets) wide Proposed NoC - Data Plane (10-byte links) N/A 1 VC, 2 ﬂits (2 r-packets) wide N/A N/A 1 VC , 14 ﬂits (2 packets) wide TABLE II: Speciﬁcations of virtual channels requested cache line. In developing this constraint we assume the worst case; that the last word in a data message is critical for a delayed cache hit. Thus, consider the transmission time of the data packet’s tail ﬂit, tt . In the following inequality, the left-hand side computes tt across h routers on the data plane, while the right-hand side computes tt on the baseline NoC: hcS + hβcS + (f (cid:2) − 1)cS < hxc + hβc + (f − 1)c Where f and f (cid:2) are the number of ﬂits of the data packet on the baseline NoC and data plane, respectively, such that (f − 1)c and (f (cid:2) − 1)cS are the serialization delays of the body and tail ﬂits on the baseline NoC and data plane, respectively. Solving for S , we get: S ≤ hx + hβ + f − 1 h + hβ + f (cid:2) − 1 (3) Given that f < f (cid:2) , it is clear that constraint (3) subsumes constraint (2). Each of the three constraints implies a range of S , however, a situation may arise where for a set of design parameters we are unable to ﬁnd a range of S that satisﬁes all three. Let us consider what happens when S violates any of the constraints. If (1) is violated, then data packets move faster than necessary that they often catch-up with their rpackets, thus wasting power that could be saved by further slowing the data plane. If (2) is violated, then critical words may be overly delayed causing system performance to suffer resulting in longer execution time and possibly more system energy consumption. Similarly, violating (3) may negatively impact performance if the service times of delayed cache hits are signiﬁcantly impacted. However, the impact depends on the data access patterns, which may not always require the last words in data messages to satisfy delayed cache hits. This analysis suggests that if we cannot ﬁnd a value of S that satisﬁes all three constraints, maintaining system performance requires that we choose S that satisﬁes constraint (3). 4) Computing the slow-down factor: We use the above constraints to compute S for a 4x4 and a 8x8 CMPs. First, however, we need to determine the value of β , the average contention delay cycles incurred per router. Contention delay depends on the volume of trafﬁc injected onto the NoC. Hence, we generate and simulate synthetic traces of random trafﬁc on a 4x4 and a 8x8 CMP, using the baseline NoC to empirically measure β with different trafﬁc injection rates (explanation of trace generation, injection rates, and simulation parameters are in Section V). We found 0.39 ≤ β ≤ 0.64, and 0.27 ≤ β ≤ 0.54, for the 4x4 and 8x8 CMPs, respectively. Thus, in calculating S , we use the average value of β for each CMP, i.e., β = 0.5 for the 4x4 CMP and β = 0.4 for the 8x8 CMP. For the remaining parameters, we use: x = 3 and k = 5. For 64-byte cache lines, a data packet on the baseline NoC consists of ﬁve 16-byte ﬂits (i.e., f = 5) or seven 10-byte ﬂits on the data plane (i.e., f (cid:2) = 7). For h, we use the average path length which is 3.33 for the 4x4 CMP and 6 for the 8x8 CMP. Plugging these numbers we ﬁnd that 1 ≤ S ≤ 1.42 and 1.55 ≤ S ≤ 1.69 for the 4x4 and 8x8 CMPs, respectively. These ranges of S guide our choice of the clock frequencies we use in the evaluation of Section V. V. EVALUAT ION We use Simics [7], a functional simulator, to evaluate our proposed two-plane NoC design with D ´ej `a Vu switching for 16- and 64-core CMPs. For workloads, we use synthetically generated traces which allow varying the trafﬁc load injected into the NoC, and use execution driven simulation of scientiﬁc and commercial benchmarks from the Splash2 [8], Parsec [9], and Specjbb [10] suites. Execution driven simulation inherently captures the effects of the spatial locality of data accesses, thus exposing the misses due to delayed hits. The simulated cores are UltraSPARC III, in-order, clocked at 4GHz, with an instruction issue width of 2. Each core has private 16 KB L1 data and instruction caches (access time: 1 cycle). The L2 cache is distributed shared with a 1 MB bank at each core (access time: 10 cycles - access time is estimated using Cacti [11]). Cache coherency is maintained through a directory-based MESI protocol. The baseline NoC is a single plane 2D mesh with one router per core and 16-byte links. Control messages are one ﬂit long while data messages, which carry 64-byte cache lines, are ﬁve ﬂits long. Table II shows the VCs and their sizes for the baseline and the proposed NoC. The proposed NoC is composed of a control and data planes. The control plane is clocked like the baseline at 4GHz and has 6-byte links, where each control message is one ﬂit long. The data plane has 10-byte links and carries data messages composed of seven ﬂits. The data packets on both the baseline and the proposed NoC carry the critical word (eight bytes) as the ﬁrst word in the packet. A stalled instruction that is waiting for a critical word is allowed to proceed as soon as the word is received. Similarly, when the word required for a delayed cache hit arrives, the stalled instruction is allowed to proceed without waiting to receive all the words in the data packet. TABLE III: Voltage and frequency of the evaluated data planes. Slow-down Factor (S ) Frequency (GHz) Voltage (V) 1 4 1.0 1.33 3 0.8 1.5 2.66 0.733 2 2 0.6 A. Evaluation with synthetic traces First, we use synthetic traces to study the communication latency, performance, and energy consumption with varying trafﬁc loads. We generate synthetic traces such that each node sends 20K data request messages to random destinations. When a data request is received, a reply data packet is sent by the receiving node to the requesting node. The data reply 15 Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ ϯϬ ϯϱ ϰϬ ϰϱ Ϭ͘Ϭϭ Ϭ͘Ϭϯ Ϭ͘Ϭϱ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ Đ Ǉ  ů Ğ Ɛ dƌĂĨĨŝĐ/ŶũĞĐƚŝŽŶZĂƚĞ;ƌĞƋƵĞƐƚͬĐǇĐůĞͬŶŽĚĞͿ (a) Average latency of the data packet’s head ﬂit. Ϭ͘Ϭ ϭ͘Ϭ Ϯ͘Ϭ ϯ͘Ϭ ϰ͘Ϭ ϱ͘Ϭ ϲ͘Ϭ ϳ͘Ϭ ϴ͘Ϭ ϵ͘Ϭ ϭϬ͘Ϭ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ Ϭ͘Ϭϭ Ϭ͘Ϭϯ Ϭ͘Ϭϱ dƌĂĨĨŝĐ/ŶũĞĐƚŝŽŶZĂƚĞ;ƌĞƋƵĞƐƚͬĐǇĐůĞͬŶŽĚĞͿ Ğ Đ Ǉ  ů Ɛ (b) Average cycles saved along paths of data packets with 1 future reservation. Fig. 3: Synthetic trafﬁc - Communication latency on a 64-core CMP. Ϭй ϮϬй ϰϬй ϲϬй ϴϬй ϭϬϬй ϭϮϬй Ϭ͘Ϭϭ Ϭ͘Ϭϯ Ϭ͘Ϭϱ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ E Ž ƌ ŵ Ă ŝ ů ǌ Ě Ğ  Ğ Ŷ ƌ Ŷ Ž  Ǉ Ő  Ɛ Ƶ ŵ Ɖ ƚ ŝ Ŷ Ž dƌĂĨĨŝĐ/ŶũĞĐƚŝŽŶZĂƚĞ;ƌĞƋƵĞƐƚͬĐǇĐůĞͬŶŽĚĞͿ (a) Normalized NoC energy consumption. ϴϱй ϵϬй ϵϱй ϭϬϬй ϭϬϱй ϭϭϬй ϭϭϱй ϭϮϬй Ϭ͘Ϭϭ Ϭ͘Ϭϯ Ϭ͘Ϭϱ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ dƌĂĨĨŝĐ/ŶũĞĐƚŝŽŶZĂƚĞ;ƌĞƋƵĞƐƚͬĐǇĐůĞͬŶŽĚĞͿ E Ž ƌ ŵ Ă ŝ ů ǌ Ž  Ě Ğ  ŵ Ğ Ɖ ů ƚ ŝ d Ŷ Ž  ŝ ŵ Ğ (b) Normalized execution completion time. (Y-axis starts at 85%.) Fig. 4: Synthetic trafﬁc - Normalized execution completion time and NoC energy consumption on a 64-core CMP. is sent 10 cycles (time to access the L2 cache) after the data request is received, while the r-packet is sent 5 cycles (time for a tag match) after the request is received. The pending request is satisﬁed once the critical word is received in the data packet. We generate traces with varying request injection rates: 0.01, 0.03, and 0.05 requests per cycle per node. We evaluate different data plane speeds as listed in table III (the voltage/frequency range is similar to [12] except that we use 2GHz instead of 1.9 GHz). We use Orion-2 [13] to estimate the static and dynamic power of routers’ components and wires (assuming 1.5mm hops) in 45 nm technology. a) Effect of future reservations: Figure 3(a)2 shows the average latency of the head ﬂit of the data packets on the baseline and proposed NoCs on a 64-core CMP (simulations of a 16-core CMP exhibit similar trends), with one future reservation, while Fig. 3(b) shows the average saved cycles along the path of a data packet with one future reservation compared to zero future reservations (cycles shown are 0.25 ns corresponding to the 4GHz frequency). With one future reservation, the head ﬂit’s communication latency improves by 8% to 22% for the evaluated conﬁgurations (for a 16-core CMP, we observe improvements in the range 7% to 21%). We also study the effect of using more future reservations (not shown in the ﬁgures) and ﬁnd that one future reservation is sufﬁcient to keep the r-packets ahead of the data packets. b) Execution time and energy consumption: For synthetic traces, execution completion time is the time required to inject all the request messages into the NoC and to receive 2 In Figs. 3-7, the notation x/y GHz indicates the frequencies of the control and data planes of a split-plane NoC. For example, 4/3 GHz indicates the control and data planes are clocked at 4GHz and 3GHz, respectively. Also, in Figs. 3-6, 4GHz indicates the frequency of the baseline single plane NoC. all the corresponding reply data messages. Figure 4 shows the NoC energy consumption and the execution completion time using the baseline and proposed NoC normalized to the system with the baseline NoC. With a slower data plane less energy is consumed in the NoC, but the execution time may increase, for example, when the data plane is clocked at 2 GHz in Fig. 4(b). This may increase the overall energy consumed by the CMP due to more energy being consumed by the cores. Also, notice that keeping the same performance with a high trafﬁc load may require a faster data plane. Interestingly, although the average latency of the data packet’s head ﬂit may be longer on the proposed NoC than on the baseline, the completion time with the proposed NoC can be better, such as the 64-core CMP with the data plane clocked at 3 GHz in Fig. 3(a) and Fig. 4(b). The reason is that the two-plane design allows a control and a data ﬂit to simultaneously cross the link between two neighboring cores, instead of serializing the link access as on the baseline NoC. B. Evaluation with benchmarks Second, we evaluate our proposed design with executiondriven simulation, which - unlike synthetic traces - results in exchanging all kinds of cache coherence messages such as invalidations, acknowledgments, write-backs, etc. and exposes the misses due to delayed cache hits. Further, communication is not always evenly distributed throughout a program’s execution; often programs exhibit alternating compute intensive and communication intensive periods. For evaluation on a 16-core CMP, we simulate the entire parallel section of each benchmark, except for Specjbb, for which we stop simulation after 3200 transactions have been executed. For a 64-core CMP it takes a very long time to run 16  Ϭй ϮϬй ϰϬй ϲϬй ϴϬй ϭϬϬй ϭϮϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ ƌ Ǉ Ă ƌ ƚ Ğ Đ Ă Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ  ŵ Ŷ Ă Ğ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ E E Ž Ž ƌ ƌ ŵ ŵ Ă Ă ŝ ŝ ů ů ǌ ǌ Ě Ě Ğ Ğ     Ğ Ğ ǆ ǆ Đ Đ Ƶ Ƶ ƚ ƚ ŝ ŝ Ŷ Ŷ Ž Ž     d d ŝ ŝ ŵ ŵ Ğ Ğ (a) Normalized execution time Ϭй ϭϬй ϮϬй ϯϬй ϰϬй ϱϬй ϲϬй ϳϬй ϴϬй ϵϬй ϭϬϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ ƌ Ǉ Ă ƌ ƚ Ğ Đ Ă Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ  ŵ Ŷ Ă Ğ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ E Ž ƌ ŵ Ă ŝ ů ǌ Ě Ğ   Ğ Ŷ ƌ Ŷ Ž  Ǉ Ő Ɛ Ƶ ŵ Ɖ ƚ ŝ Ŷ Ž (b) Normalized NoC energy consumption Fig. 5: 16 core CMP - Normalized execution time and NoC energy consumption. Ϭй ϮϬй ϰϬй ϲϬй ϴϬй ϭϬϬй ϭϮϬй ϭϰϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ  ŵ Ŷ Ă Ğ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ E Ž ƌ ŵ Ă ŝ ů ǌ Ě Ğ   Ğ ǆ Đ Ƶ ƚ ŝ Ŷ Ž   d ŝ ŵ Ğ (a) Normalized execution time Ϭй ϭϬй ϮϬй ϯϬй ϰϬй ϱϬй ϲϬй ϳϬй ϴϬй ϵϬй ϭϬϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ  ŵ Ŷ Ă Ğ ϰ',ǌ ϰͬϰ',ǌ ϰͬϯ',ǌ ϰͬϮ͘ϲϲ',ǌ ϰͬϮ',ǌ E Ž ƌ ŵ Ă ŝ ů ǌ Ě Ğ   Ğ Ŷ ƌ Ŷ Ž  Ǉ Ő Ɛ Ƶ ŵ Ɖ ƚ ŝ Ŷ Ž (b) Normalized NoC energy consumption Fig. 6: 64 core CMP - Normalized execution time and NoC energy consumption. the entire parallel section, thus after cache warm-up, we stop simulation when core 0 completes executing 10M benchmark instructions3 (not counting the system instructions). Figures 5 and 6 show the normalized execution time and NoC energy consumption relative to the baseline CMP for 16and 64-core CMPs, respectively. Similar trends of execution time and energy consumption are observed for the two CMPs. We notice that slowing down the data plane to half the frequency of the control plane (i.e., 2GHz) prolongs execution time for most benchmarks, but when clocked at 2.66 GHz (2/3 the speed of the control plane), the execution time shows no increase4 , while reducing the NoC energy by an average of 43% and 53% on the 16-core and 64-core CMPs, respectively. Split-plane NoC comparison: To isolate the effect of D ´ej `a Vu switching from just splitting the baseline NoC into a control and data planes, we consider three split-plane packet switched NoCs with their D ´ej `a Vu counterparts for a 16core CMP. The results are shown in Fig. 7 normalized to the baseline packet switched NoC without split planes operating at 4 GHz (the highlighted grid line at 100%). Splitting the planes (PS 4/4) provides negligible change over the baseline; however, when using D ´ej `a Vu switching (DV 4/4), we see a performance improvement. Additionally, our stated goal was to reduce network energy without impacting performance. When reducing the speed of the data plane to 2.66 GHz in a split packet switch (PS 4/2.66) the performance reduces considerably. We also tried to send the critical word on the faster control plane (PS+CW 4/2.66) [14] which provides a slight 3 Raytrace was too small to give meaningful results on the 64-core CMP 4 Specjbb’s execution time increases by only 1% beneﬁt but does not approach the speed of the baseline. Finally, our proposed D ´ej `a Vu switched network (DV 4/2.66) restores the performance of the baseline and is comparable with PS 4/4, while providing the energy reductions of reducing the data plane speed as enumerated in Fig. 5(b). This demonstrates that D ´ej `a Vu switching is a critical component of a split-plane NoC approach for reducing energy without penalizing performance. ϵϬй ϵϱй ϭϬϬй ϭϬϱй ϭϭϬй Ă ď ƌ Ğ Ŷ Ɛ Đ Ɛ Ŭ Đ Ă ď ů Ğ Ž Ś ů Ɛ Ǉ Ě Ž ď ƌ ƚ Ŭ Đ Ă ů Ĩ Ŷ Ă Ě Ƶ ŝ ŝ ŵ Ă ƚ Ğ ů Ƶ  Đ Ŷ Ž ƚ ŝ Ő ů Ŷ Ž Ŷ Ƶ  Đ Ŷ Ž ƚ ŝ Ő Ž Ŷ Ă Ğ Đ  Đ Ŷ Ž ƚ ŝ Ő ƌ Ž Ě Ă ŝ Ɛ ŝ ƚ Ǉ ƌ Ě Ă ŝ ǆ ƌ Ǉ Ă ƌ ƚ Ğ Đ Ă Ɛ Đ Ğ Ɖ ũ ď ď ǁ Ă ƚ Ğ ƌ  Ŷ Ɛ Ă Ƶ Ƌ ƌ Ě Ğ ǁ Ă ƚ Ğ ƌ  Ɛ Ă Ɖ ƚ ŝ Ă ů ' Ž Ğ ŵ Ğ ƌ ƚ ŝ Đ  ŵ Ŷ Ă Ğ W^ϰͬϰ sϰͬϰ W^ϰͬϮ͘ϲϲ W^нtϰͬϮ͘ϲϲ sϰͬϮ͘ϲϲ Ğ W Ĩ ƌ Ž ƌ ŵ Ŷ Ă Đ Ă Ğ Z Ğ  ů ƚ ŝ Ğ ǀ  ƚ Ő Ŷ ^ Ž  ŝ ů Ğ Ŷ Ă W Ğ  ů  Ă  Ɛ Ğ ŝ ů Ğ Ŷ Fig. 7: Comparing performance on a 16-core CMP with split-plane NoCs, with and without D ´ej `a Vu switching (Y-axis starts at 90%.) V I . R ELATED WORK Duato et al. [15] propose a router architecture with multiple switches for concurrently supporting wormhole and circuit switching in the interconnections of multicomputers and distributed shared memory multiprocessors. Network latency and throughput are improved with enough reuse of circuits. Peh and Dally propose ﬂit-reservation ﬂow control [16] to reserve buffers and channel bandwidth during the exact time slots that the data ﬂits will use the buffers and channels. Cheng et al. [17] propose a heterogeneous interconnect for 17      carrying the cache trafﬁc of CMPs. Three sets of wires are used with varying power and latency characteristics to replace a baseline two-level tree NoC. With 75-byte baseline links, the authors report a reduction in both execution time and energy consumption, however, they report signiﬁcant performance losses with narrower links. Flores et al. [14] also propose a heterogeneous interconnect for a 2D mesh topology in which the baseline NoC is replaced with one having two sets of wires; one 2x faster and the other 2x slower than the baseline. The authors report results with similar trends to the results in [17]. Our work differs from these proposals in that we do not use faster wires than the baseline NoC, and compensate for the data plane’s reduced frequency by using D ´ej `a Vu switching to reduce the per hop latency. Shang et al. [18] use dynamic voltage scaling (DVS) for links to reduce power consumption using a history-based DVS policy. Soteriou and Peh [19] propose self-regulating poweraware interconnections that turn their links on/off in response to bursts and dips in trafﬁc in a distributed fashion. Lee and Bagherzadeh use dynamic frequency scaling links [20] to save power in the NoC. We use a simpler method of constant voltage and frequency reductions at the plane-level rather than the individual link-level. Kumar et al. [21] propose express virtual channels to improve communication latency in 2D mesh NoCs. Abousamra et al. [22] propose exploiting the temporal locality in the trafﬁc through periodic conﬁguration of the most important circuits. Ahn and Kim [23] and Park et al. [24] suggest router designs that exploit temporal locality in the trafﬁc to allow ﬂits to bypass the router pipeline. Kumar et al. [25] suggest a design in which routers announce the availability of buffers and VCs to other routers such that packets may quickly traverse routers on less congested routes. However, these schemes still need to support packet switching; in contrast our work simpliﬁes the design of the data plane routers and makes it completely circuit switched. Li et al. [26] perform time-slot reservations of the buffers and channels for data packets. One interconnect plane carries all trafﬁc including the reservation ﬂits. Our proposal is different in that we send data messages on a separate plane and support multiple future circuit reservations, thus allowing the reduction of the data plane’s voltage and frequency. V I I . CONCLU S ION We propose D ´ej `a Vu switching for saving power in multiplane NoCs. Starting with a baseline single plane NoC and splitting it into two planes: (1) a control plane dedicated for the coherence and control messages, and (2) a data plane dedicated for the data messages. D ´ej `a Vu switching simpliﬁes the design of the data plane’s routers and enables reducing the data plane’s voltage and frequency to save power. We present an analysis of the constraints that govern how slow the data plane can operate without degrading performance, and use the results of this study to guide the evaluation of our design. The viability of our design is conﬁrmed by simulations of both synthetically generated message traces and execution-driven simulations. In our simulations, running the data plane at 2/3 the speed of the control plane maintained system performance while allowing an average savings of 43% and 53% of the NoC energy in 16-core and 64-core CMPs, respectively. "An Optimal Control Approach to Power Management for Multi-Voltage and Frequency Islands Multiprocessor Platforms under Highly Variable Workloads.,"Reducing energy consumption in multi-processor systems-on-chip (MPSoCs) where communication happens via the network-on-chip (NoC) approach calls for multiple voltage/frequency island (VFI)-based designs. In turn, such multi-VFI architectures need efficient, robust, and accurate run-time control mechanisms that can exploit the workload characteristics in order to save power. Despite being tractable, the linear control models for power management cannot capture some important workload characteristics (e.g., fractality, non-stationarity) observed in heterogeneous NoCs, if ignored, such characteristics lead to inefficient communication and resources allocation, as well as high power dissipation in MPSoCs. To mitigate such limitations, we propose a new paradigm shift from power optimization based on linear models to control approaches based on fractal-state equations. As such, our approach is the first to propose a controller for fractal workloads with precise constraints on state and control variables and specific time bounds. Our results show that significant power savings (about 70%) can be achieved at run-time while running a variety of benchmark applications.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip An Optimal Control Approach to Power Management for Multi-Voltage and  Frequency Islands Multiprocessor Platforms under Highly Variable Workloads  Paul Bogdan, Radu Marculescu  Department of ECE  Carnegie Mellon University  Pittsburgh, USA 15213  {pbogdan,radum}@ece.cmu.edu  Siddharth Jain  Department of EE  Indian Institute of Technology  Kanpur, India 208016  sidjain@iitk.ac.in  Rafael Tornero Gavila  Departament d'Informàtica  Universitat de València  46100 Burjassot, Valencia, Spain  rafael.tornero@uv.es  Abstract—Reducing energy consumption in multi-processor  systems-on-chip (MPSoCs) where communication happens via  the network-on-chip (NoC) approach calls for multiple  voltage/frequency  island (VFI)-based designs. In turn, such  multi-VFI architectures need efficient, robust, and accurate  run-time control mechanisms that can exploit the workload  characteristics in order to save power. Despite being tractable,  the linear control models for power management cannot  capture some  important workload characteristics  (e.g.,  fractality, non-stationarity) observed in heterogeneous NoCs; if  ignored, such characteristics lead to inefficient communication  and resources allocation, as well as high power dissipation in  MPSoCs. To mitigate such limitations, we propose a new  paradigm shift from power optimization based on linear  models to control approaches based on fractal-state equations.  As such, our approach is the first to propose a controller for  fractal workloads with precise constraints on state and control  variables and specific time bounds. Our results show that  significant power savings (about 70%) can be achieved at runtime while running a variety of benchmark applications.  Keywords - Networks-on-Chip; Power Management; Fractal  Workloads; Finite Horizon Optimal Control  INTRODUCTION  I. Networks-on-Chip (NoCs) have been recently proposed  as a promising communication paradigm able to overcome  various communication issues (e.g., increased interconnect  delays, high power consumption) in highly integrated multi-  core systems. In contrast to traditional bus-based designs, the  NoC paradigm enables various processing elements (PEs)  communicate by  routing packets  instead of wires  [4][5][17][22]. As such, NoCs not only offer a high degree of  scalability and reusability, but also represent the main driver  for achieving tera-scale computing [15].   Integrating many cores operating at high frequencies in  order to accommodate complex applications [29][33] leads  to heterogeneous workloads, higher power consumption, and  temperature fluctuations within die [6][8][19][26]. Such  problems cannot be predicted or corrected at the design and  manufacturing stages so, in order to sustain the increasing  computational demands, it is essential to enhance the multicore platforms with smart power management policies,  which can enable per core/tile control of power consumption  while satisfying various performance levels [16][19].  Towards this end, in this paper, we formulate the  problem of optimal power management for multi-domain  platforms where communication happens via a globally  asynchronous  locally  synchronous  (GALS) NoC  architecture. The goal of our on-line control algorithm is to  determine the optimal operating frequencies for both PEs  and routers that belong to separate voltage and frequency  islands (VFIs) such  that  the performance constraints,  typically measured by queues utilization (or occupancy)  values, are met despite the high variability exhibited by real  computational workloads. Queues utilization is used as a  performance measure because it is directly related to the  waiting times of packets in the queues and so it is  proportional with packet latency1.   Generally speaking, several types of control techniques  have been used in many engineering applications; they  would then naturally appear as good candidates to control the  MPSoC behavior as well [12][25][34][40]. For instance, the  feedback-based control approaches compute a set of control  actions meant to bring the system into a desired state with no  constraints on the magnitude of the control signal. In many  situations, however,  such control  signals can  take  exceedingly high values which make them (physically)  unfeasible. In addition, the feedback-based control strategies  have the drawback that only a limited number of design  parameters can be found from the closed-loop pole locations.   An alternative approach is to consider the problem of  finite  horizon optimal control with a predefined reference which  finds the best sequence of control actions over a fixed time  interval (horizon); this set of control actions can bring  the  system (characterized by differential equations) to the  desired reference at the end of the control interval.   The new optimal control approach proposed in this paper  is also finite horizon in nature. As opposed to existing  approaches,  it allows  to directly optimize a certain  performance metric subject  to  fractal (i.e., fractional  derivatives) state equations (i.e., for queue utilization) and  bounded control signals (i.e., operating frequencies). In other  words, the proposed fractal controller is able to provide the  optimal control signals (i.e., operating frequencies), if they  exist, for a given performance level.    1 We note that our mathematical formulation for power management can be  extended for other performance metrics as well.  978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.32 35                                                                     Application Application Application (task graphs) (task graphs) (task graphs) Architecture Architecture Architecture (topology, routing) (topology, routing) (topology, routing) NoC Workload NoC Workload NoC Workload (network queue utilization, arrival process,  (network queue utilization, arrival process,  (network queue utilization, arrival process,  departure process) departure process) departure process) Off-line Off-line On-line On-line Fractal  Fractal  Modeling Modeling Parameter identification (Section 5) Parameter identification (Section 5) Parameter identification (Section 5) (fractal order derivative, utilization contribution  (fractal order derivative, utilization contribution  (fractal order derivative, utilization contribution  parameter, arrival and departure coefficients) parameter, arrival and departure coefficients) parameter, arrival and departure coefficients) System System Identification Identification Optimal controller design (Section 4) Optimal controller design (Section 4) Optimal controller design (Section 4) (Given some performance constraints finds the  (Given some performance constraints finds the  (Given some performance constraints finds the  operating frequencies for each VFI) operating frequencies for each VFI) operating frequencies for each VFI) System  System  Control Control e e e c c c n n n a a a m m m t t t s s s o o o C C C r r r t t t r r r t t t f f f i i i r r r s s s o o o n n n a a a e e e p p p s s s d d d n n n n n n o o o a a a c c c s s s Figure 1. Overview of our methodology. We first input the information  about application (e.g., task graphs), architecture (e.g., topology, routing)  and the set of costs and performance constraints that have to be met by  the NoC platform. Building on NoC workload measurements (i.e., queue  utilization, packet arrival and departure times), we build a fractal model  of the queue dynamics by estimating the fractional order of the time  derivative,  the utilization parameter and  the arrival/departure  coefficients. Next, we use this identified model to design an optimal  controller which determines the VFI optimal operating frequencies such  that the performance constraints are satisfied.  We note that existing control-based approaches for power  management [2][12][25][34][40] rely on the assumption that  real workloads can be modeled via linear state-space models  without explicitly  including  the  timing constraints  in  problem formulation, i.e., they work under the assumption of  having an infinite time horizon to control the system. As  shown later in the paper, such approaches can provide  inefficient solutions that actually may consume more power  in real situations.   In summary, our novel contribution to state-of-the-art  power management of MPSoCs is threefold:  • First, we propose a fractal-based state description of the  dynamics of queues interfacing neighboring VFIs (see the  top part of Figure 1). For completeness, we also describe  a strategy for estimating the parameters of the fractal  model as shown in the mid-section of Figure 1.  • Second, we formulate the power management problem as  a constrained finite horizon fractional control problem,  which brings the utilization of queues in the multicore  platforms as closely as possible to some predefined  reference values, while minimizing  the  individual  operating frequency of both PEs and routers (see Figure  1). Of note, the controller we synthesize does account for  the high variability observed in computational workloads  and ensures that the operating frequencies of processors  and routers remain within a predefined interval.  • Third, using Lagrange optimization and calculus of  variations, we derive the optimality conditions that need  to be satisfied by all operating frequencies across the  VFIs in order to reach the desired performance level  across the entire multicore platform.   • Finally, we evaluate the practical impact of all these  solutions using realistic benchmarks and applications.   36 The remainder of the paper is organized as follows.  Section II reviews several power and thermal management  techniques proposed to date for multicore platforms and  highlights our novel contribution. In Section III, we discuss  the main concepts specific to fractional calculus and explore  how these ideas can be related to the observed characteristics  of real world processes. Section IV presents the fractional  optimal control problem and summarizes the optimality  conditions for the run time power management algorithm we  propose. Section V describes how the parameters of the  proposed fractal model can be identified (at run time) from  real traces and presents the experimental results we obtained  while evaluating  this approach. Finally, Section VI  concludes the paper by summarizing our main contribution.  II. RELATED WORK AND NOVEL CONTRIBUTION  The power and/or thermal management methodologies  proposed to date for multicore systems focus on either  balancing the power/thermal profile via task or thread  manipulation  (i.e., allocation, migration,  scheduling)  [10][11][13][28][37], optimizing power consumption via  clock gating [20], or dynamic voltage and frequency scaling  (DVFS) [3][18][24][30]. For instance, while building on a  DVFS-based approach, Sharifi et al. [30] propose a joint  technique for thermal and energy management which  optimizes for energy efficiency. Arjomand et al. [3] propose  a thermal-aware heuristic for regular 3D NoCs which, for a  predefined mapped application, seeks to find the voltage and  frequency of all cores such that the power consumption is  reduced. Along the same lines, Coskun et al. [10] propose an  autoregressive moving average  (ARMA) model  for  predicting the core temperature and allocating the application  tasks to the cores while trying to balance the thermal profile.  Coskun et al. [11] also describe a thermal-aware job  scheduling that reduces the frequency of hot spots and spatial  thermal gradients. In contrast, a task migration strategy for  thermal management is presented in [13]. Along  the same  lines, a dynamic thermal management for 2D MPSoCs is  proposed in [24] which scales the frequency for a hot core  such that its temperature is kept below a given threshold.   Several  research  studies propose control-theoretic  approaches to optimize for power and/or thermal profiles  [2][12][23][25][34][40]. For example, while  trying  to  minimize the power consumption in multiple clock domain  multicores, the pioneering work by Wu et al. [39][40]  models cores as queuing systems and proposes a  Proportional-Integrator-Derivative (PID) controller which  scales the operating frequency (for each clock domain) such  that the queue utilization is kept close to a reference value.  Ogras et al. [25] propose a feedback-based control  mechanism to control the speed of each VFI while describing  the NoC traffic through a linear state-space equation [35].  Along the same lines, Garg et al. [12] investigate the  performance of a small world distributed control approach  and propose a custom feedback control strategy which seeks  to minimize  the  implementation  cost of global  communication, while  sacrificing  some performance.  Adopting a similar linear system representation, Alimonda et  al. [2] propose a feedback control DVFS of data-flow                      a) a) b) b) c) c) Figure 2. a) Packet inter-arrival times at queue between tiles (2,0) and (1,0) a 4×4 mesh NoC running a 16-node multithreaded online transaction  processing (oracle) application. b) Comparison between the probability of inter-arrival times to exceed a given threshold and the maximum likelihood  exponential fitting for the inter-arrival times at queue between tiles (2,0) and (1,0) obtained from a 4×4 mesh NoC running a 16-node multithreaded  online transaction processing (oracle) application. c) Comparison between the probability of inter-arrival times to exceed a given threshold and the  maximum likelihood exponential fitting for the inter-arrival times at queue between tiles (2,1) and (1,1) obtained from a 4×4 mesh NoC running a 16node multi-threaded scientific (ocean) application. This significant departure from exponential assumption shows that the network control and power  management cannot be done properly using the classical linear system theory. Such experimental characteristics invalidate the use of classical control  theory for regulating bursty workloads, and motivate our fractional calculus approach to power management.  applications mapped on MPSoCs. Zanini et al. propose a  predictive model for balancing the thermal profile of the  multicore platform by controlling the operation frequencies.    Before getting into the details of our new approach, we  note that in all this previous work, the authors assume that  either the workloads are stationary (and therefore use  techniques like task migration/remapping which are based on  average values of the variables that need to be optimized), or  the NoC traffic can be characterized by exponentially  distributed inter-arrival times between successive packet  arrivals at various queues in the network. Unfortunately, the  real applications can rarely be characterized by exponentialtype distributions [9]. For instance, Figure 2.a shows the time  series of inter-arrival times between successive packets  received at a queue located between tiles (2,0) and (1,0) in a  4×4 mesh NoC under XY wormhole routing. In this  experiment, the packets consist of 15 flits, queues have 50  slots, and inter-arrival times are recorded while running a  multi-threaded transaction processing application.  By looking at data in Figure 2.a, we can easily note that  the inter-arrival times exhibit not only a high variation, but  also a non-stationary type of behavior which cannot be  properly captured by the classical linear system theory  models [9]. This observation can be directly proved by  checking the validity of the exponential assumption as  shown in Figure 2.b and Figure 2.c. More precisely, we  observe that the empirical probability distribution of interarrival times between successive packets does not follow the  exponential assumption and it can be better fit by a heavy tail  distribution instead.  The existence of heavy tails in the dynamics of queue  utilization suggests that the average value of any variable  characterizing a metric of interest in the system (e.g., queue  utilization) at time t does not depend only on its previous  value at time t-1, but instead, due to the long term memory of  the traffic, it depends on the weighted sum of several values  at prior moments in time t-1, t-2, etc. This is precisely the  point where correlations observed in the inter-arrival times of  packet communication can and should be exploited via  fractal-based models to control and optimize the overall  multi-core platform operation [7]. From a mathematical  perspective, the fractal model is based on fractional (i.e.,  non-integer) derivatives which are described next.   III. BASICS ON FRACTIONAL CALCULUS  Since its inception, fractional calculus has found many  applications in physics (e.g., dielectric polarization, heat  transfer phenomena, etc.) and engineering (e.g., control, bioengineering, etc.) [27]. More recently, the calculus of  variations has been extended to systems characterized by  fractional dynamical equations [1].  Simply speaking, the fractional (or fractal) calculus is  based on techniques for differentiation and integration of  arbitrary orders [21][27]. Unlike classical (i.e., integer order)  calculus,  fractional derivatives allow us  to directly  incorporate the dynamical characteristics (fractal behavior)  of any target process x(t) (e.g., queue utilization in a  network) through a weighted sum denoting the contribution  of the previous events x(Ĳ), for Ĳ ࣅ [0,t]:  ݀ఈ ݔሺݐሻ ݀ݐ ఈ ൌ ͳ ʒሺ݊ െ ߙሻ ݀௡ ݀ݐ ௡ න ௧ ଴ ݔሺ߬ሻ ሺݐ െ ߬ሻఈି௡ାଵ ݀߬ሺͳሻ where Į is the fractional order of the derivative and Ƚ(n-Į) is  the Gamma function [21], n is an integer and n-1<Į<n. This  continuous time definition of a fractional derivative can also  be written in a discretized form as follows:   ቂ௧ି௔ ௛ ቃ ͳ ݄ఈ ෍ ሺെͳሻ௝ ܥ௝ఈ ௝ୀ଴ ݔሺݐ െ ݆݄ሻሺʹሻ  ݀ఈ ݔሺݐሻ ݀ݐ ఈ ൌ  ௛՜଴ 37           PE PE i-th i-th VFI VFI yi(t) yi(t) Router Router PE PE m-th m-th VFI VFI ym(t) ym(t) Router Router j-th VFI j-th VFI xj(t) xj(t) l-th VFI l-th VFI xl(t) xl(t) Figure 3. Representation of j-th and l-th neighboring VFIs where each PE  is set to run, if necessary, at its own frequency. The xj(t) variable  represents the utilization of the interface queue between the  j-th and l-th  VFIs. The yi(t) represent the utilization values of the interface queue  between the  i-th PE and  j-th VFI. Note that various colors of the tiles in  the above NoC imply that each island can run at a certain frequency.  In (3), wi, zi, rj and qk are some positive weighting  coefficients. In (4), Įi is the fractional order which depends  on the fractal dimension characterizing the queue utilization  process ݕ௜ ሺݐሻ, ai(t) represents the weighting coefficient of the  utilization ݕ௜ ሺݐሻ, bi  and ci(t) reflect the contributions of the  writing frequency (fi) and the reading frequency (fj), ݕ௜௠௜௡   and ݕ௜௠௔௫ are the admissible lower and upper bounds on the  queue utilization ݕ௜ ሺݐሻ. Of note, the optimal controller allows  the designer to set individual weighting coefficients (i.e., wi,  zi, rj and qk) in (3) for each of the NoC components such that  the major power consumption elements can have a higher  impact on the overall cost function.  The next set of constraints is meant to characterize the  utilization of queues between neighboring VFIs:  ݀ఈೖ ݔ௞ ሺݐሻ ݀ݐ ఈೖ ൌ ܽ௞ ሺݐሻݔ௞ ሺݐሻ ൅ ܾ௞ ሺݐሻ݂௞ െ ܿ௞ ሺݐሻ݂௟ ǡሺͷሻ  Ͳ ൏ ݔ௞௠௜௡ ൑ ݔ௞ ሺݐሻ ൑ ݔ௞௠௔௫ ൏ ͳǡ ݇ ൌ ͳǡ Ǥ Ǥ ǡ ܰ௝ ௤ Ǥ  POWER MANAGEMENT IN FRACTAL WORKLOADS  where h is the time increment, [(t-a)/h] represents the integer  part of the ratio between (t-a) and h. Equations (1)            (continuous) and (2) (discrete) capture directly the role of the  power law distributions observed in packet inter-arrival  utilization process ݔ௞ ሺݐሻ , ܽ௞ ሺݐሻ represents the contribution  where Įk is the fractional order characterizing the queue  times (i.e., the term (t-Ĳ)Į-n+1); they allow not only for a more  of utilization ݔ௞ ሺݐሻ to  accurate description of the dynamics of queue utilization x(t),  dynamics,ܾ௞ ሺݐሻ and ܿ௞ ሺݐሻ are the coefficients of the writing  the entire queues utilization  but also for a better optimization; this issue is discussed next.   frequency  respectively,ݔ௞௠௜௡ and ݔ௞௠௔௫ are the lower and upper bounds  (fk)  and  the  reading  frequency  (fl)  IV. on the utilizationݔ௞ ሺݐሻ.   A. Problem Formulation  Note that the cost function in (3) maintains all NoC  Next, we formulate the power management as an optimal  differences between  ݕ௜ ሺݐሻ and ݕ௜ queues at specific utilization references (see the squared  or ݔ௞ ሺݐሻ and ݔ௞ control problem which takes into account the fractal  characteristics of the NoC workload. As shown in Figure 3,  ),  while the control inputs (i.e., operating frequencies) are  we consider a VFI-based MPSoC architecture consisting of  NPE PEs, Nr routers, and ܰ௝ queues interfacing the router in  prevented from taking exceedingly large values which would  induce a too high power consumption. The role of the  the j- th VFI with other routers in the neighboring VFIs.   optimal controller is to select the minimum operating  The goal of our nonlinear control problem is to find, for a  frequencies for which the performance constraints are  given starting time (ti) and a final time (tf), the optimal  assignment of operating frequencies for the PEs, routers, and  satisfied. Moreover, in order to prevent the nonlinear  controller from selecting an unacceptable range of operating  queues, which minimize the quadratic costs of queues  utilization with respect to a predefined reference, as well as  frequencies, we also impose the following constraints:   the operating frequency of each VFI (this would implicitly  minimize also the power consumption):  ௥௘௙ ௥௘௙ max j ,...,1    , ௤ N ≤ = ≤ f f f f , j  ݉݅݊ ׬ ൜σ ቂ௪೔ଶ ൫ݕ௜ ሺݐሻ െ ݕ௜ ൅ σ ଶ ݂௝ ଶ ሺݐሻ ൅ σ ቂ௤ೖଶ ൫ݔ௞ ሺݐሻ െ ݔ ௥௘௙ ൯ଶ ቃ ௥௘௙ ൯ଶ ൅ ௭೔ଶ ݂௜ ଶ ሺݐሻቃ ൅ ௧೑௧೔ ቂ௥ೕ ேೕ ௞ୀଵ ேೝ௝ୀଵ ேುಶ௜ୀଵ ೜ ൨ൠ ݀ݐሺ͵ሻ  subject to the constraints given in Eq. (4) through Eq. (6) :  ݀ఈ೔ ݕ௜ ሺݐሻ ݀ݐ ఈ೔ ൌ ܽ௜ ሺݐሻݕ௜ ሺݐሻ ൅ ܾ௜ ሺݐሻ݂௜ െ ܿ௜ ሺݐሻ݂௝ ǡሺͶሻ  Ͳ ൏ ݕ௜௠௜௡ ൑ ݕ௜ ሺݐሻ ൑ ݕ௜௠௔௫ ൏ ͳǡ ݅ ൌ ͳǡ Ǥ Ǥ ǡ ܰ௉ா  where ݕ௜ ሺݐሻ and ݕ௜ , for i = 1,…,NPE, are the actual  the i-th PE and its corresponding router,  ݔ௞ ሺݐሻ and ݔ௞ utilization and the utilization reference of the queue between  k = 1,.., ܰ௝  for    are the actual and the reference utilization of the  k-th queue located between the routers in j-th and l-th VFIs.    ௥௘௙ ௥௘௙ ௤ 38 i r , f i f j ≤ ≤ = N PE ,...,1 min j min i max i          (6)  where ݂௜௠௜௡ and ݂௜௠௔௫  are the lower and upper frequency  bounds at which each PE can run, ݂௝௠௜௡ and ݂௝௠௔௫  are the  lower and upper frequency bounds for each router.   Note that we introduce two indices i- for the PEs and jfor the routers and implicitly two variables (i.e., fi and fj)  such that the operating frequencies of the PEs are decoupled  from the router frequencies; this way, we avoid setting the  PE to a small frequency which may affect the computational  performance requirements, or setting the router to a too high  frequency when it may not be necessary. Since considering a  single VFI for each router would  introduce further  complexity (due to a larger number of mixed-clock queues),  we limit ourselves at considering that the Nr consists of just a  few VFIs and include more constraints to reflect the fact that  neighboring routers are operating at the same frequency.                       B. Significance of Fractal Power Management Problem  To put our power management approach into a broader  perspective, we look at the optimal control problem defined  in equations (3), (4), (5), and (6) from three different  perspectives (see Figure 4), namely: i) the degree of fractal  behavior exhibited by the system (state-space) model (Z-axis  in Figure 4), ii) the degree of constraining imposed on the  state and control variables (Y-axis), and iii) the scale (infinite  vs. finite) of the timing constrains imposed on the control  algorithm (X-axis).   The classical optimal control refers to infinite time  horizon approaches that rely on first order derivative statespace models with no constraints imposed on state and  control variables (see the striped red dot in Figure 4). We  note that a linear quadratic regulator (LQR) approach as in  [12][25] is a particular case of our problem formulation by  considering Įi = Įk = 1, for i = 1,…,NPE, k = 1,…,Nj q, tf =   and imposing no constraints on the state variables in (4) and  (5), and control signals in (6).  By the same token, a proportional-integral-derivative  (PID) type of controller as in [39][40] can be also obtained  as a particular case of our approach by considering  zj = rj =0  and Įi = Įk = 1, for i =1,…,NPE  and k =1,…,Nq, infinite time  horizon tf =  and imposing no constraints on the state  variables in (4) and (5), and control signals in (6). So, it  becomes clear now that the newly proposed problem  formulation (shown as the chessboard green dot in Figure 4)  is more general than either classical LQR- or PID-type of  approaches. Given its generality, our new formalism allows  to handle the high variability that occurs in real workloads.   Next, we derive the necessary and sufficient conditions  for optimal control using convex optimization concepts; we  also show how the constrained finite horizon fractal control  problem can be solved efficiently via linear programming.   C. New Algorithm for Optimal Controller Synthesis   To solve the power management problem, we use  concepts from the optimization theory and first construct the  Lagrangian functional, L(yi, fi, Ȝi, xk, fj, Ȗk,j), as follows:  ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) dt ftc ftb txta tx dt d t x txq t fr 2 t ftc t ftb tyta tyd dt t fz 2 t y tyw f x fyL , l k k k k k k α jk , ref k k k N j N k 2 j j j i i i i i i α i t t N i 2 i i ref i i i jk , j k i i i k k r q j i i f i PE º ½ · ¸ » ° ¸ » ¾ ° ¹ » ¼ ¿ § ¨ ¨ © + − − + − 2 ª « « ¬ + + · º ¸ » ¸ » ¹ ¼ § ¨ ¨ © + − − + ­ ° ® ° ¯ ª « « « ¬ + − 2 = ¦¦ ³ ¦ = 1 = 1 = 1 α α γ λ γ λ , 2 2 , , ,    (7)  where ݕ௜ ሺݐሻ and ݔ௞ ሺݐሻ denote the queue utilization variables,  fi is the frequency associated with the i-th PE, fj is the  frequency associated with the j-th router, Ȝi is the Lagrange  multiplier associated with the constraint imposed for the  queue between the PE and the router, and Ȗk,j are the  Lagrange multipliers associated with the constraints on the  queue between neighboring routers in different VFIs.  For completeness, we also have to add some boundary  constraints on the utilization of mixed clock queues:   ݕ௜ ሺݐ ൌ ݐ௜ ሻ ൌ  ݕ௜଴ ݕ௜ ൫ݐ ൌ ݐ௙ ൯ ൌ  ݕ௜ ௥௘௙ ݅ ൌ ͳǡ ǥ ǡ ܰ௉ா ሺͺሻ          ݔ௞ ሺݐ ൌ ݐ௜ ሻ ൌ  ݔ௞଴ ݔ௞ ൫ݐ ൌ ݐ௙ ൯ ൌ  ݔ௞ ௥௘௙ ݆ ൌ ͳǡ Ǥ Ǥ ǡ ܰ௥ ǡ ݇ ൌ ͳǡ ǥ ǡ ܰ௝ ௤ These conditions are required in order to satisfy a certain  performance level from the computation standpoint.  By expanding the Lagrangian in (7) via the Taylor  formula and considering that it attains its minimum in the  vicinity of Ĳ = 0, i.e.,డ௅ డఛ ൌ ͲǢ we obtain the next relations:   ߲ܮ ߲ܮ ߲ݔ௞ ௜ ߲ܮ ൅ ܦ௧೑ ߲ ܦ௧ ߲ܮ ൅ ܦ௧೑ ߲ ܦ௧ ఈ೔ ఈ೔ ݕ௜ ఈೖ ݔ௞ ௧೔ ൌ Ͳ ߲ܮ ߲݂௜ ൌ Ͳ݅ ൌ ͳǡ ǥ ǡ ܰ௉ா ሺͻሻ  ൌ Ͳǡ ߲ܮ ߲݂௝ ൌ Ͳ݆ ൌ ͳǡ ǥ ǡ ܰ௥ ǡ ݇ ൌ ͳǡ ǥ ǡ ܰ௝ ௧ ఈೖ ௧೔ ௧ ௤ where ܦ௧೑ and ܦ௧  represent the fractional derivatives  operating backward and forward in time, respectively.  In order to solve the equations in (9), we discretize the  interval [ti, tf] into N intervals of size (tf - ti)/N and use the  formula in (2) to construct a linear system which can be  solved using LU decomposition. In short, the algorithm of  the optimal controller synthesis works as follows:   a) Step 1: From the system identification module, read  the coefficients Įi,  ai, bi, ci for i=1,…,NPE characterizing the  ck for j = 1,…,Nr, k = 1,…,ܰ௝ dynamics of queues between PEs and routers, and Įk, ak, bk,  describing the dynamics of  queues between neighboring routers in different VFIs;  b) Step 2: For a fixed number of discrete steps N,  compute the coefficients obtained after discretizing the  ఈ೔ ௧ ఈೖ ௧೔ ௤ Fractal (State-Space)  Fractal (State-Space)  System Model System Model Finite  Finite  Time  Time  Horizon Horizon Unconstrained State  Unconstrained State  & Control Variables & Control Variables Non-fractal (StateNon-fractal (StateSpace) System Model Space) System Model Infinite  Infinite  Time  Time  Horizon Horizon Degree of Fractality  Degree of Fractality  (Workload Memory) (Workload Memory) Degree of Constraining  Degree of Constraining  State and Control variables State and Control variables Figure 4. Power management problem seen from three different  perspectives: Degree of fractality of state-space model (Z-axis), degree of  constraining the state and control variables (Y-axis), and scale (finite vs.  infinite) of time constraints (X-axis). The red (striped) dot corresponds to  an infinite time horizon optimal control problem with no constraints on  state and control variables relying on a non-fractal state-space model.  Particular cases of this problem are linear quadratic regulator (LQR) and  proportional-integral-derivative (PID) controller. Our approach (the green  chess dot) refers to a finite-horizon optimal control problem which  operates with constraints on both state and control variables and relies on  a fractal state-space model.  LQR, P, PI, PID LQR, P, PI, PID Proposed Approach (Constrained  Proposed Approach (Constrained  State & Control Variables Finite  State & Control Variables Finite  Horizon Fractal Optimal Control Horizon Fractal Optimal Control Degree of  Degree of  Constraining  Constraining  the Control  the Control  Algorithm in  Algorithm in  Time Time Constrained  Constrained  State &  State &  Control  Control  Variables Variables 39     (cid:1)       EXPERIMENTAL SETUP AND RESULTS  fractional derivatives in (4), (5) and (9) using the formula in  (2) and construct a linear system, where the unknown  variables are represented by the operating frequencies (i.e.,  fi, fj) and Lagrange multipliers (i.e., Ȝi  and Ȗk,j);  c) Step 3: Solve the linear system in (9) and find the  operating frequencies for each VFI in the NoC architecture.  V. To evaluate our fractal optimal control algorithm, we  consider a combination of trace-driven and cycle-accurate  simulation of a VFI-based NoC architecture. From an  application perspective, we consider four 16-node multithreaded commercial workloads (i.e., Apache HTTP server  v2.0 from SPECweb99 benchmark [33], on-line transaction  processing consisting of TPC-C v3.0 workload on IBM DB2  v8 ESE and Oracle 10gExterprise Database Server, blocked  sparse Cholesky  factorization and ocean simulation)  obtained  from a FLEXUS-based shared-memory 16processor environment consisting of cycle accurate models  of out-of-order processors and cache hierarchy [36][38].  From an architectural perspective, we consider a 4×4  mesh NoC employing XY wormhole routing with mixed  clock queues of 10 flit size and packets consisting of 15 flits.  In this setup, we consider that the execution of a set of  applications is divided into control intervals of 20ȝs length.  Of course, the proposed control algorithm can also work  with larger time intervals but the model parameters need to  be estimated for the new time scale.  A. New Power Management Methodology  For each control interval, the parameter identification  module (PIM) estimates the fractional exponent Įk in (5) in  two stages: First, it computes the workload variation  coefficients for log2(m) resolution scales based on queue  utilizations (m is the size of the control interval). Second, it  performs a linear regression between the resolution scales  and the variation coefficients. Because this approach relies  only on variation coefficients at various time scales (as  opposed to the entire time series which would be needed for  any linear regression method [14]), the PIM module not only  reduces the computational complexity from a O(N3) order to  a linear order O(N), but it also enables an on-line estimation  procedure with minimum memory overhead (i.e., it does not  require to store all queue utilizations and can be done  iteratively whenever there is a change in queue occupancy).   Next, the PIM module estimates parameters ak,  bk, and ck  from arrival (Ak), departure (Dk) and queue utilization (Xk)  processes by solving a 10×3 linear system with three  unknowns ak,  bk, and  ck.  After the identification step is completed, and in parallel  with the application computations, the fractal optimal  controller implemented in the Power Manager (PM) module  reads these parameters and solves the linear system defined  by the equations (9) to determine the optimal operating  frequencies for the PEs and routers (for the next interval of  20ȝs) that can ensure a predefined performance level  specified in terms of queue utilization references.  B. Hardware Complexity of the Power Manager  To illustrate the hardware requirements of the PM  module, we consider that the optimal controller solves the  optimality conditions in (9) for 30 discretization steps.  Implementing the controller in Verilog and synthesizing it  using the Xilinx’s SXT/Isim on a Virtex4 FPGA (Device:  XC5VLX30, Package: FF324), for controlling a large  multiVFI platform with 80 queues we would need about 19440  registers, 19120 LUTs, 80 RAM/FIFOs and 480 DSP48Es in  a basic, un-optimized, FPGA design. This looks very  reasonable for all practical purposes; with some additional  optimization, these requirements can be reduced even  further. Alternatively, the linear program solver in (9) can be  implemented in software.   C. Power Management under Real Workloads  Next, we apply the optimal controller for N = 30 discrete  steps and a 4×4 mesh NoC running an Apache HTTP  webserver application; the objective is to determine the  operating frequencies such that the queues utilization  remains below 0.1. The reason for bounding the maximum  utilization of the queues at a reference value of 0.1 is twofold: First, a small utilization of the queues implies smaller  packet waiting times in buffers and, implicitly, smaller  source-to-destination  latencies. Second, a small queue  utilization, also minimizes the chances of thermal hotspot  buildup; this can improve the chip reliability significantly.  Note also that the number of discrete steps (N) chosen for  discretizing equations (4), (5) and (9) influences the  precision of computing the operating frequencies. Thus,  when less precision is needed in terms of operating  frequency, we can use a smaller N value (e.g., 5 to 10).   For illustration purposes, we consider the case of N = 30  discrete steps and solve the linear system describing the  dynamics of the same large multi-VFI platform with 80  queues in less than 1ȝs. This clearly illustrates that our  approach is suitable for online power management of future  multicore platforms.   Figure 5.a shows the utilization of a few queues at tiles  (0,0), (0,2), (1,1), (1,2), (2,2), (2,1), and (3,3). We can  observe that the optimal controller is able to bring the  utilization of these tiles below the reference value of 0.1.  Figure 5.b) shows the operating  frequencies of the tiles at  (0,0), (0,2), (1,1), (1,2), (2,2), (2,1), and (3,3) needed to  attain the imposed reference values. Moreover, Figure 5.c)  shows the utilization of the queues without and with fractal  optimal control. The optimal controller is able to keep the  utilization of all queues below 0.1 by adjusting the operating  frequencies of all PEs and routers.   For completeness, we have also applied the optimal  controller  to a 4×4 mesh NoC  running Cholesky  factorization with a maximum reference value of 0.4  allowed for utilization of all queues. Note that, by imposing  a value of 0.4 for queue utilization in this experiment, the  impact on average packet latency is less than 4%, while the  power savings are about 40%.   40 a) a) b) b) c) c) Figure 5. a) Utilization of the queues at tiles (0,0), (0,2), (1,1), (1,2), (2,2), (2,1), and (3,3) for a 4×4 mesh NoC running Apache HTTP webserver  application. b) The variation of the operating frequencies for the routers at (0,0), (0,2), (1,1), (1,2), (2,2), (2,1), and (3,3) necessary to reach the reference  values imposed on the utilization of all queues. c) Comparison between the utilization of all queues in the uncontrolled case and the queue utilization for the  case of the fractal optimal controller described in Section IV.C.   D. Comparison Between our Approach and Classical  Approaches  for Power Management  To illustrate the difference our fractal controller (FC) can  make in terms of power savings, we consider next a 4×4  NoC running one of the burstiest applications among all the  benchmarks we  tested  (namely,  the Apache HTTP  webserver) and compare the power consumption observed  under three different PM approaches, i.e., PM with PID, PM  with LQR, and PM with FC, against a baseline platform  consisting of a homogeneous NoC running at 3GHz. More  precisely, tin order to better understand the mathematical  underpinnings of this comparison, for the PM based on PID  control (as in [40]) and for the PM based on LQR (as in  [12]), the model behind the controller is based on integer  order differential equations that characterize the queues  utilization at run time. In contrast, for the FC approach, the  queue utilization is modeled using fractional calculus as  discussed in Section IV.  As we can see from data in Table 1, the PM based on  PID turns out to oscillate because it sometimes selects  exceedingly high frequencies. Consequently, an approach  based on PID control cannot deal with the kind of burstiness  a real workload like Apache HTTP exhibits. The PM based  on LQR control does a better job and reduces the overall  power consumption of the system to approximately 70% of  the power consumption of the baseline platform. In contrast,  the PM based on FC makes the platform consume only about  30% of the dynamic power consumption of the baseline.  This shows that the PM based on fractional control performs  significantly better than classical control approaches for  power management like PID or LQR which can get trapped  in local minima for such bursty workloads.   Of note, the power savings for PM with LQR can further  decrease, if no bounds on maximum frequency are set.  However, the fractal optimal controller allows to find the  optimal solution; this enables the highest amount of power  savings to be achieved (as shown, almost 2X power savings  compared to LQR).   TABLE I.  NORMALIZED POWER CONSUMPTION VALUES  TO THE HOMOGENEOUS BASELINE PLATFORM AND THREE  PM APPROACHES: PID, LQR, AND FC WHILE RUNNING A  BURSTY APPLICATION (APACHE HTTP).  No PM (baseline)  Normalized power  consumption 1  PM with  PID PM with  LQR PM with  FC cannot  stabilize  0.70  0.30  Similarly, by performing other experiments with the  Cholesky factorization benchmark, we observe that our  approach leads to 40% power savings compared to the  homogeneous baseline where PEs and routers run at 3GHz.  Finally, by applying the proposed algorithm and comparing  the savings with NoC architectures running at 3GHz leads to  50% and 20% power savings for ocean simulation and online transaction processing application, respectively.   VI. CONCLUSION  In this paper, we have addressed the problem of power  management in multicore architectures where computational  workloads are highly complex and exhibit  fractal  characteristics. Towards this end, we have proposed a new  modeling approach based on the dynamics of queue  utilization and fractional differential equations. This fractal  model is used to formulate an optimal control problem for  dynamic power management which determines the necessary  operating frequencies such that the NoC queues reach and  remain at their target reference values for bursty workloads.   ACKNOWLEDGMENTS  The authors would like to thank to anonymous reviewers  for their constructive comments and helpful suggestions.  The material presented herein was supported by the NSF  under CCF-0916752 grant and GRC under 2011-HJ-2168  and 2012-HJ-2232 grants. R. T. Gavila has been supported  41       by the Spanish MICINN, European Commission FEDER  funds, and University of Valencia under grants ConsoliderIngenio 2010, CSD2006-00046, TIN2009-14475-C04-04,  and V SEGLES program.  "Hierarchical Network-on-Chip and Traffic Compression for Spiking Neural Network Implementations.,"The complexity of inter-neuron connectivity is prohibiting scalable hardware implementations of spiking neural networks (SNNs). Traditional neuron interconnect using a shared bus topology is not scalable due to non-linear growth of neuron connections with the neural network size. This paper presents a novel hierarchical NoC (H-NoC) architecture for SNN hardware which addresses the scalability issue by creating a 3-dimensional array of clusters of neurons with a hierarchical structure of low and high-level routers. The H-NoC architecture also incorporates a spike traffic compression technique to exploit SNN traffic patterns, thus reducing traffic overhead and improving throughput on the network. In addition, adaptive routing capabilities between clusters balance local and global traffic loads to sustain throughput under bursting activity. Simulation results show a high throughput per cluster (3.33×10 9 spikes/second), and synthesis results using 65-nm CMOS technology demonstrate low cost area (0.587mm 2 ) and power consumption (13.16mW @100MHz) for a single cluster of 400 neurons, which outperforms existing SNN hardware strategies.","Hierarchical Network-on-Chip and Traffic Compression for Spiking Neural Network ImplementationsS. Carrillo, J. Harkin, L. J. McDaid Intelligent Systems Research Centre University of Ulster, Magee CampusDerry, Northern Ireland (UK)e-mail:  S. Pande, S. Cawley, B. McGinley, F. Morgan School of Electrical and Electronic Engineering National University of Ireland, Galway CampusGalway, Republic of Irelande-mail: Abstract—The complexity of inter-neuron connectivity is prohibiting scalable hardware implementations of spiking neural networks (SNNs). Traditional neuron interconnect using a shared bus topology is not scalable due to non-linear growth of neuron connections with the neural network size. This paper presents a novel hierarchical NoC (H-NoC) architecture for SNN hardware which addresses the scalability issue by creating a 3-dimensional array of clusters of neurons with a hierarchical structure of low and high-level routers. The H-NoC architecture also incorporates a spike traffic compression technique to exploit SNN traffic patterns, thus reducing traffic overhead and improving throughput on the network. In addition, adaptive routing capabilities between clusters balance local and global traffic loads to sustain throughput under bursting activity. Simulation results show a high throughput per cluster (3.33x109 spikes/second), and synthesis results using 65-nm CMOS technology demonstrate low cost area  (0.587mm2)  and  power  consumption (13.16mW @100MHz) for a single cluster of 400 neurons, which outperforms existing SNN hardware strategies.Keywords-adaptive routing; hierarchical architecture; network-on-chip; network topology; spiking neural networks.INTRODUCTIONSNNs attempt to emulate (to a degree) information processing in the mammalian brain based on massively parallel arrays of neurons [1]. SNNs communicate by transmitting short transient pulses or spikes between neurons, via weighted synaptic connections. SNNs differ from traditional ANNs in the sense that SNNs uses the interval between spikes (i.e. time), as a way to encode information. An SNN neuron will emit a spike when its potential exceeds a neuron-specific membrane potential threshold value. The computational power of SNNs is realised by the parallel synaptic connections between neurons, the weights on these connections, the timing of the spikes and the internal membrane potential threshold of each neuron. Moreover SNNs exhibit the ability to quickly adapt providing fault- tolerant capabilities. These attributes and the ability of SNNs to provide a good solution in partially observable environments, make SNNs suitable for implementing embedded applications such as neuromorphic circuits [2] and control applications [3], with high parallelism and low power consumption compared to traditional von Neumann computer paradigms [4]. One key research goal is to harness this efficiency and build artificial neural systems that can emulate information processing principles of the brain. However, a central challenge to this goal is to provide a scalable inter-neuron architecture, as existing approaches cannot provide the dense interconnect for the millions of neurons and synapses that are required. For large SNNs, hardware implementations offer superior speed of execution compared to sequential software approaches due to the inherent parallelism of hardware. However, traditional direct neuron-to-neuron interconnection based on a shared bus topology is not scalable.This paper proposes a H-NoC architecture for SNN hardware implementations, which in comparison to bus- based approaches, offers high-levels of scalability for inter- neuron connectivity. The proposed hierarchical approach provides an efficient method of exploiting multicast communication within a population of neurons, and also removes the one-to-one correspondence between neurons and routers imposed by earlier NoC SNN implementations [5], [6], [7], [8]. In addition, this paper also introduces the concept of a spike traffic compression technique, which exploits the low-firing rate shown by biological neurons [4], thus reducing traffic overhead and improving throughput on the network. The contributions proposed in this paper can be further extended and applied to different NoC domain applications, especially CMP systems [9] and SoC platforms[10] where communication locality is predominant and low power consumption is a constraint.The proposed H-NoC architecture is validated based on an RTL-level implementation, whilst area/power analysis is performed using 65-nm CMOS technology. Results show significant scalable characteristics such as high throughput and low power/area footprint that make it suitable for large scale SNN embedded implementations. The remainder of this paper is organised as follows: Section II discusses previous works and Section III introduces the proposed scalable H-NoC architecture and traffic compression for SNNs. Section IV presents results from the evaluation of the hierarchical approach and traffic compression in terms of scalability, throughput, area/power footprint. Finally, Section V provides a summary and conclusion.RELATED WORK AND MOTIVATIONThe NoC interconnect paradigm was introduced in [11], [12], [13], as a promising solution to solve the on-chip			communication problems experienced in SoC and CMP platform implementations. In general, NoC architectures are composed of a set of shared cores or processing elements, routers and links, which are arranged in a specific topology depending on the application. In the context of SNNs, the organisation of the NoC infrastructure can be viewed as follows: the processing elements refers to the spiking neuron models attached to NoC routers, the NoC channels are analogous to the synapses of neurons and the NoC topology in this case, refers to the way those neurons are interconnected across the network. Although previous work [5], [6], [7], [8] has provided the foundations for using NoC as an interconnect fabric for SNNs, their interconnection strategies or flat topologies, i.e. one-to-one correspondence between neurons and routers, makes it difficult to achieve high scalability with low power consumption. The latter is of vital important because a high percentage of the power consumed by NoC-based SNN hardware implementations is produced by the interconnection fabric, where the neuron model has a power consumption approximately six orders of magnitude smaller than that of the interconnect fabric [14].Previous work [15], [16] has shown that SNNs can exhibit short communication paths, i.e. high communication locality between neurons, allowing a group of neurons PROPOSED HIERACHICAL NOC ARCHITECTURE FORSNN IMPLEMENTATIONSThis section presents an overview of the H-NoC architecture and its design. The proposed H-NoC architecture for scalable SNN implementations is based on a hierarchical array of NoC routers. Spike events generated by neurons are packetized and then issued in the NoC and received by targeted neurons for processing. The proposed architecture does not consider the use of multiprocessors to emulate the neurons but instead uses an analogue, low power synapse/neuron neuron cell on hardware to model spiking neurons [19]. The latter approach reduces the power consumption cost compared to emulating spiking neurons using embedded processors. More importantly, the proposed H-NoC architecture for SNNs exploits data locality [15], [16], by creating clusters of n neurons located at the bottom of the hierarchy. The proposed H-NoC architecture bases its core on three structures namely, (1) neuron facility, (2) tile facility and (3) cluster facility. These structures are arranged in three on-chip communication levels using an array of specialised, low area/power footprint NoC routers, as shown in Fig. 1.allocated within the same region to share the same incoming spike events. With this motivation, in this paper we explore Level 3: Cluster FacilityNorth WThird Level To/From other clustersNhierarchical NoCs (H-NoCs) as a way to exploit the conceptof region-based routing, where virtual regions or facilities are used to allocate resources that process either local or West ClusterRouterSouth East (Top)S ETo/From 2nd levelglobal traffic. Recently, H-NoCs have seen success in addressing scalability and offering low power footprints in Level 2: Tile FacilityNeuron NeuronZ	Neuron Facility Facility Neuron To/From 3rd levelother large scale applications such as CMP systems [9], SoC Facility Neuron Tile Facility Neuron Second Levelplatforms  [10]  and  video  image  processing  [17].  In  thecontext of SNNs, these virtual regions or facilities can be used to create clusters of neurons by isolating neurons with local and global connections. Hence, this multi-level NoC structure allows the variation in local and global connections between neurons to be accommodated for scalable implementations. A recent theoretical evaluation of a hierarchical 2D-mesh topology for neural network hardware Cluster Facility	XTile Facility Neuron FacilityY Facility Router Facility(Intermediate)First Level (Bottom) To/From 1st levelTo/From 2nd levelwhich uses an embedded processor to emulate neuron behaviour [18], shows the theoretical benefits of grouping neurons based on a hierarchical organisation. Nevertheless, the evaluation is constrained only to mesh topology and does not consider combined topologies for H-NoC architectures.This paper presents an advanced H-NoC architecture for SNN implementation by combining star-mesh topologies to support local and global connections between neurons. The on-chip communication for the proposed H-NoC is based on an array of NoC routers that offers the one-to-many multicast communication between neurons in the networks. The main contributions of this paper are: (1) a hardware implementation of a novel H-NoC architecture for SNN implementations; (2) a specialised array of NoC router architectures combined with a traffic congestion mechanism;(3) analysis of SNN-traffic and validation of throughput, scalability and area/power footprint for the H-NoC architecture. Figure 1. Porposed H-NoC architecture for implementing cluster of SNNs.The aforementioned array of NoC routers per cluster is distributed as follows: forty node routers at the bottom of the hierarchy, i.e. one node router per neuron facility. Four tile routers in the mid-level of the hierarchy, i.e. one tile router per tile facility, and one cluster router at the top of the hierarchy. Furthermore, each node router provides a fine- grained neuron interconnectivity so that each neuron can be either configured independently or several neurons can be configured as a group of neurons to create several neural layers. Hence, a maximum of forty neural layers of 10 neurons each or, a minimum of a single neural layer of up to 400 neurons per cluster can be achieved. Nevertheless, if larger neuron numbers are required, the additional neurons are mapped to another cluster forming a grid of clusters using a mesh topology on top of the H-NoC architecture.The first level of the proposed H-NoC architecture is the neuron facility. The neuron facility, which is found at the bottom of the hierarchy, allocates ten spiking neuron cells using a node router that converts the incoming data from the neural cells into NoC data packets. The ten neurons per cluster is just an initial density value to show the functionality of the proposed approach, and it was chosen as the sizes of neural networks are often defined in quantities to the power of ten. Therefore, a total of forty neuron facilities are provided within this hierarchical organisation.The tile facility is the second level of the hierarchy and uses a tile router in a star topology to group ten neuron facilities. The proposed hierarchical architecture comprises a total of four tiles, where each tile contains one hundred neurons. Similar to the node router, each tile router supports unicast, multicast and broadcast communication protocols; this combined with the star topology used to interconnect the ten neuron facilities, provides an efficient way to manage the traffic demand of the neurons allocated in this level of the H-NoC architecture.Finally, the cluster facility is located at the top level of the hierarchy and it groups four tile facilities using an adaptive router referred to as the cluster router. This router allows the exchange of spike events between any of the four hundred neurons inside the cluster facility and neighbouring clusters. The cluster router incorporates an adaptive routing scheme which facilitates router adaptation according to the spike traffic loads, improving router throughput according to the traffic behaviour presented across the network [20]. It also supports broadcast and multicast communication with any cluster, for example, in terms of its four neighbours a neuron fan-out of up to 1,600 neuron connections can be achieved.The internal architecture of the cluster and tile routers are quite similar and their functionality and performance have already been tested on FPGA hardware and discussed in [20], [21]. Therefore, the rest of this section is focused on the describing the node router which comprises a custom micro- architecture and routing engine which seamlessly integrates the spiking neurons into the hierarchy and processes spike events in order to support all of the NoC communication mechanisms (i.e. unicast, broadcast and multicast) within the proposed H-NoC architecture.Node RouterMicro-architecture. A detailed block description of the node router is shown in Fig. 2. The node router provides the communication infrastructure and incorporates the necessary specialised hardware modules to transfer/receive spike events to/from any of its ten neural cells. This router also works as a gateway to reach other neurons located either in a different tile facility inside the same cluster or in others neighbouring clusters. The main characteristics and functionalities of these components are summarised as follows: The node router provides a set of ten internal input ports to interface with the neural cells. The internal input ports are 1-bit each, and each incoming spike event from a neural cell is encoded as an on/off state. The practical implementation of the input ports includes a 2-deep FIFO queue per port and the packets are handled based on the store-and-forward (S&F) switching technique. The S&F was chosen over the cut-through or wormhole switching techniques as the routing decision needs to be made on the basis of the entire packet, and also because the buffer size is acceptable since the packets for the spike events are small. Besides the internal ports, the node router is also provided with an Input/Output Scheduler module implemented as a hybrid architecture that combines the fairness of the round- robin approach and the prioritised behaviour of the first- come first-serve arbiter [21], so that it can fairly allocate multiple incoming requests based on the traffic demanded by the input ports.Another important component of the node router are the crossbars. Although there is only one crossbar shown in Fig. 2, the practical implementation contains two crossbars, facilitating the switching task from input to output and vice versa for the downstream and upstream data paths. The up/downstream data path comprises a 10 x 1 crossbar switch, which offers the physical interconnection infrastructure to route a data packet from any of the 10 input ports that are interfacing the neural cell to the output port that communicates with the tile facility. This two-level hierarchical crossbar switch allows speedup of the transfer of data packets and also reduces the probability of data contention, as incoming and outgoing packets are handled independently. Each node router also provides a set of ten internal output ports to send information to any of the ten neural cells allocated at the bottom of the hierarchy. Each internal output port is 7-bits wide to match the synapse index value coming from the synapse allocator module. The node router also offers external input/output ports to communicate with the tile facility at the mid-level of the hierarchy (48-bit paths). These external ports provide network visibility for the neuron facility to exchange data packets with the rest of the components within the H-NoC architecture.Configuration Registers. All the information regarding the configuration of the proposed H-NoC architecture is stored on the configuration bank registers. These registers can be re-configured at any time, depending of the neural connectivity specification that is required for the application to be executed. The packet layout of the aforementioned registers is depicted in Fig. 3a, and the main functionality of the registers include: (1) the spike event timer register (14-bit width), which holds the pre-scalar value that is used to divide the main NoC router clock (i.e. 100MHz). This produces a lower frequency signal (up to 10KHz) which is used in the input scheduler to setup a time window for the spike event traffic compression technique; (2) the neuron facility address register (4-bit width) that stores the address of the router node and has a range from 0 to 9 corresponding to the number of neuron facilities allocated within a tile (this value is used for routing purposes, as it is needed for the synapse allocator module to determine the exact position of the synapse that is going to receive a spike event); (3) the communication protocol register selects values that can be setup as internal communication (IT_CM), neighbour multicast (NB_MC), neighbour broadcast (NB_BC) or debug data (DB). 	request from 	 tile facility 	  grant totile facility Node Routerconfiguration data configuration/debug datarequest grantConfiguration bank register request to  	 tile facility 	grant from 	 tile facility 	       spike event        	          from neural cell nconfig / debug / spike event from tile facility (9)      Spike   wrapper datadataQueuing input portsCrossbar switch data data   Queuing output ports spike event    	             to neural cell ndebug / spike event to tile facilityFigure 2.    Node router block diagram.This register is used to setup the communication Layout for the configuration bank registerprotocols that the generated data packets need to follow in order to reach a specific neuron within the cluster. IT_CM is the default on-chip communication protocol and it is used to identify the packets that remain within the cluster. Upon Neuron facility address register	Spike event timer register Communication protocol register	Spike event reception registerselection of NB_MC, data packets are sent to two or three neighbour clusters in the upper level at the same time, whilst for NB_BC data packets are sent to all four neighbour clusters. For the DB option, a special packet is sent out from the cluster to be analysed by a monitor system for debug purposes. (4) In addition, the spike event reception register is used to store the reception status of the neurons, where each bit of the register represents one of the positions of the ten neurons allocated within the neuron facility. Thus, an asserted register bit value of '1' indicates that the neuron which corresponds to this specific position is able to receive spike events from other neurons, otherwise the neuron is permanently disabled. (5) Finally, the target neuron population register is used to store the address of the target population. This value and the one obtained from the neuron facility address register are then used by the spike wrapper module to create a valid data packet, which is then sent across the network to target a specific population of neurons.On-Chip Communication Protocols. The routing algorithm for the node router follows a distributed-base implementation, i.e. each router computes the next routing address based on the target address contained on the data packet. In addition, the routing algorithm avoids the use of look-up tables and instead it uses the combination of a compact combinational logic and the information from the bank configuration registers to route all the data packets (following a uni/multi/broad-cast fashion, according to the configuration registers) within the proposed H-NoC architecture. The router engine is the core of the node router and is implemented as a synchronous FSM that can work in three different modes namely, configuration, execution and debug. Fig. 3b, shows the packet layout for the aforementioned three modes and their header options. During the configuration/debug mode, packets follow a top/down data flow, i.e. the router located at the top of the hierarchy receives the data packets that are coming from the programming/monitor system. Target neuron population registerNB_MC: neighbour multicast DB: debugNB_BC: neighbour broadcast IT_CM: internal communication(a)Packet layoutPacket layout for the configuration modePacket layout for the debug modePacket layout for the execution modeTarget address fieldSource address fieldFigure 3. (a) Layout for the configuration bank register and (b) packet layout for the three routing modes: configuration, execution and debug. The cluster address field is not required during configuration mode, thus the modified target address for this mode is as follows: target tile addr [43:40] & target node addr [39:30].During the execution mode, the routing engine coordinates the data transfer that is generated either by the neural cells or from others tile/cluster facilities. However, contrary to the configuration/debug mode, all the packets generated by the neural cells (i.e. spike events) follow a bottom-up approach. Thus, once the routing engine detects a spike event coming from any of the neural cells, it selects the necessary information from the configuration register and configures the spike wrapper module to create a valid data packet representing the spike event within the network, which is routed following a distributed-based approach.Once the node router receives an spike event from a neural cell, a packetisation process is initiated by the spike wrapper module. The spike wrapper module is located after the input ports so that single or multiple spike events (represented as 1-bit) can be converted into a valid NoC data packet. The NoC data to represent spike events is similar to the one depicted in Fig. 3b (i.e. execution mode) and it consists of 48-bits, organised in three fields: header, source address and target address. The spike wrapper uses the information contained in the configuration bank registers to append the correct values for the three aforementioned fields. Once an data packet representing a spike event has arrived to its final destination, a spike allocation process takes place and is done by the synapse allocator module that is situated at the output of the crossbar. The spike allocation process consists of calculating the index value for the synapse position that is going to receive the spike event within the neural tile. In order to do this, the source address field contained in the 48-bit data packet is used as an input argument within the synapse allocator to compute the synapse index, As, within the neural cell, as given by (1). technique uses a bitmask approach to annotate spike events generated by neuron cells within a predefined time window, thus exploiting the low firing rate shows by spiking neurons (i.e. 1 spike every 1ms per neural cell). This significantly reduces the traffic overhead compared to a scenario where the compression approach is not applied. The time window range used in this technique expands from 10ns to 100us; large time windows are sufficient to match the integration time of spiking neuron models [4], i.e. time for the model to internally compute the pre-synaptic events which is approximately over five times faster (i.e. ~200us) than that of the firing time. A small time window can also be used when the proposed H-NoC needs to work on an acceleration regime, where neuron models can be accelerated to generate spike events every 100ns.Fig. 4 shows two example scenarios at the node router with and without the compression technique. For both scenarios it is assumed that the node router frequency is 100 MHz and the neural cells are firing as shown. In the first scenario, the spike event timer register is configured with 0x001h, i.e. that the spike traffic compression is not enabled. Hence, every spike event generated from the neurons is transferred as an individual packet to the tile facility. In the second scenario, the spike event timer register is configured with 0x1Eh, so that the input scheduler only grants request from input ports every 300ns. Therefore, all spike events generated within that period of time (e.g. from 370ns to 670ns) are compressed into the same data packet. This reduces the amount of traffic issued to the H-NoC fabric. For example, the  number  of  packets  issued  is  reduced   from 10 to 1.As   Aneuron  Afacility   (1)Where the Aneuron refers to the address of the neuron that is generating the spike event within the neuron node and its position value is determined according to the port number that the neural cell is using to interface with the node router. Similarly, the Afacility refers to the address of the node within the neuron facility, where the neuron that is generating the spike is located. This value is given by the neuron facilityaddress register. Finally, the constant value   refers to thetotal number of neurons allocated in the neuron facility.Spike Compression Technique. Biological SNNs are highly irregular and not all neural cells in the complete network are expected to generate spikes at the same time [1], however significant numbers of them can. These irregular traffic patterns can have a major impact on the packet latency and can ultimately lead to traffic congestion. Consequently, depending of the number of incoming requests, the input scheduler ignores idle ports and focuses its attention on those that are active and handling spike events. The latter provides the opportunity for spike event compression, where spike events generated by different neurons within the same neuron facility are compressed on a single data packet, decreasing spike traffic volume without affecting the network performance. The proposed traffic compression Figure 4. Node router performance with/without spike compression.PERFORMANCE EVALUATIONExperiemental SetupA 3x3 proof-of-concept array of clusters of the proposed H-NoC architecture (see Fig. 5) has been described in VHDL and evaluated based on simulation. In addition, the 3x3 array of cluster has also been implemented and tested on an FPGA to verify its performance on a real hardware platform [21]. Moreover, in order to analyse the power consumption andthe area utilisation, the VHDL description of the cluster was synthesised using Synopsys Design Compiler based on the TSMC 65nm CMOS technology. A VHDL spike event generator/counter [21] was used in the analysis to inject different traffic loads into the 3x3 array, i.e. to emulate incoming spike events to the clusters and to facilitate the measurement of packet throughput. All simulations were evaluated using Modelsim and the following criteria was used in the setup: a data packet width of 48-bits, 100 MHz system frequency, four hundred spike generators (SGs)/spike counters (SCs) attached to each cluster, for generating and receiving spike events, and a Poisson neuron firing rate [4], where the number of clock cycles for generating a new spike event are increased/decreased in order to emulate different traffic patterns. The second stage of the traffic load analysis used three clusters, i.e. C[1,2], C[2,2] and C[3,2]. The C[1,2] was configured to send spike events to C[3,2], forcing all the spike events to travel through C[2,2]. The aim of this experiment was to measure the time it takes for the spike events to travel from one cluster to another. The result of this experiment together with the result obtained from the first stage were used to derive an analytical equation to model thepacket propagation delay (  ) between  two  clusters of theH-NoC architecture. The delay path is composed of three elements namely, upstream delay (node-to-cluster), downstream delay (cluster-to-node) and delay between cluster hops. The upstream delay ( Tup ) represents the packet delay from when packets are generated by the neural cells until all they leave the cluster, and can be expressed by (2):Traffic Load AnalysisThe traffic load analysis of the 3x3 array was performed Tup ıu  tu npg 1 (2)in four stages, with and without the spike event compression Where ıu is a constant that represents the time for thetechnique in order to compare its performance advantage. The first stage used two clusters, i.e. C[1,2] and C[2,2] connected on the east and west direction respectively, as first data packet to appear outside the cluster (24 clock cycles). Additionally, tu represents the time interval betweenpackets, i.e. equivalent to pipeline latency (12 clock cycles).shown in Fig. 5. Cluster C[1,2] was configured to send Finally, npg represents the number of packets generateddifferent spike patterns that were absorbed by the spike counters allocated on C[2,2]. This simple experiment allowed the measurement of the average synaptic connection delay and also the average firing rate under zero-load traffic. The average connection delay is defined as the time taken from the moment a spike event is generated until it reaches the allocated synapse associated with the target neuron. within the cluster. Similarly, the downstream delay ( Tdown) represents the delay of data packets from when they go inside  the  cluster  until  they  are  absorbed  by  any  of theneurons located at the bottom of the hierarchy. The downstream delay is expressed by (3) where,  ıd representsthe time for the first data packet to be absorbed by any of the neurons within the cluster (30 clock cycles), td represents the time interval between packets, i.e. equivalent to pipeline latency (1 clock cycle), and npa represents the total  numberof packets absorbed within the cluster.Tdown  ı d   td  npa  1 (3Using (2) and (3) the total packet propagation delay (  ), can be re-expressed as in (4); where the delay between hopsTh  represents the time taken by a packet to travel from oneFigure 5. Evaluation platform of H-NOC using a 3x3 array of clusters. cluster to another. This time is a constant and expressed in terms of clock cycles, Th 12 . Additionally, Nc represents the total number of clusters that a packet needs to go through in order to reach its target clusters, and can vary depending on the source to target path.Analysis of Packet DelayThe average connection delay gives a measure of how   Tup   Tdown  Nc  Th Large Scale Analysis (4)fast neurons can exchange information with each other. The synaptic connection delay is expected to increase at the same rate as neuron activity increases within the clusters. However, due to the spike compression technique each node router is able to accommodate up to ten spike events in a single packet under high spike activity periods. Hence, the resulting synaptic connection delay using the compression technique can be reduced by up to ~90% compared to a full- load scenario when 400 neurons generate a spike within the same time window. During the third stage of the traffic load analysis, the packet propagation delay derived in (4) was used to analyse the average synaptic connection delay and therefore, allowed average firing rate analysis for large scale scenarios. Fig. 6 shows results for the average synaptic delay across varied sizes of cluster arrays. For this experiment the spike traffic was varied from 10% to 100% heavy-load (100% spiking activity), and the size of the array was varied as well, so that the number of hops between a source and the target cluster increased with the array size.Figure 6. Synaptic connection delay under different traffic scenarios.Fig. 6 also depicts an improvement of ~41% on the synaptic connection delay for 10x10 array size scenario under low traffic injection rate (~10% neuron activity). Moreover, with the same scenario but with an increased injection rate of 100% the synaptic delay can be improved to~90% compared to a scenario where the compression technique is not used. Results also show that reduction of synaptic connection delay is preserved whilst the size of the network is increased. Indeed, improvements between 40% and 80% on the synaptic connection delay can be achieved for a 50x50 array of clusters, when injecting 10% and 100% traffic load, respectively. Fig. 7 shows the relationship between average firing rate and the size of the network, where a maximum firing rate of ~5.0x106 Hz for a 10 hop scenario is highlighted using the compression approach. This offers a ~3.3x improvement compared to the same scenario without the compression technique. In the 50 hop scenario, although the firing rate can decrease to 1.72x105 Hz when the compression technique is not used, this firing rate is still higher, i.e. ~172x improvement compared to typical spiking neurons firing rate (~1KHz) [1]. The latter result surpasses biological real-time specifications and provides for hardware acceleration.Throughput AnalysisFinally, the fourth stage of the traffic load analysis, evaluates the throughput per cluster under different neuronal traffic activity. In this stage, C[1,2], C[2,3], C[3,2] and C[2,1] were configured to send spike events to a neighbour located on the E, S, W and N, respectively.Figure 7. Average firing rate under different traffic scenarios. This configuration forced each cluster to send the spike event through the central cluster C[2,2], as is  shown  in Fig. 5. This enabled the throughput capability of the C[2,2] to be evaluated under different traffic loads, whilst ports were busy. Fig. 8 shows the result on throughput capability, where a maximum throughput of 3.33 x 109 spikes per second was achieved, when the spike compression technique was used. Using the spike event compression technique gives an order of magnitude improvement compared to a throughput of 0.33 x 109 spikes per second, where no compression is used. In addition, due to the adaptive routing and arbitration combined with the spike compression technique, the proposed H-NoC approach achieves a higher throughput performance, showing 16.5x to 1.65x improvement compared to existing NoC-based SNNs approaches [5], [7], [8], [22].Figure 8. Throughput for a single cluster for different traffic scenarios.Area Utilisation and Power ConsumptionArea utilisation for the proposed scalable hierarchical NoC architecture was performed using Synopsys Design and Power Compiler based on the TSMC 65nm CMOS technology, and results are shown in Table 1.TABLE I.	AREA AND POWER METRICS PER COMPONENTSTable 1 highlights the small total area utilisation of 0.587mm2 and low power consumption of 13.16mW achieved for the H-NoC Cluster. The latter is the result of the compact implementation for each of the NoC routers and due to the free-look-up table approach used to implement the on- chip communication protocols, where the use of registers and the distributed-base routing algorithm implemented at the bottom of the hierarchy overcome the necessity of using large look-up tables (LUTs). To quantify the improvement of the free-look-up table approach compared to traditional distributed-based routing algorithms using LUTs, Table II highlights the reduction in memory requirement (bits) per single router. Note that the cost in term of memory requirement for the implementation is attributed only to theregisters that are involved in the on-chip communication process: the router's address registers (address), the communication register (comm), the spike event reception "A Novel Flit Serialization Strategy to Utilize Partially Faulty Links in Networks-on-Chip.,"Aggressive MOS transistor size scaling substantially increase the probability of faults in NoC links due to manufacturing defects, process variations, and chip wire-out effects. Strategies have been proposed to tolerate faulty wires by replacing them with spare ones or by partially using the defective links. However, these strategies either suffer from high area and power overheads, or significantly increase the average network latency. In this paper, we propose a novel flit serialization method, which divides the links and flits into several sections, and serializes flit sections of adjacent flits to transmit them on all available fault-free link sections to avoid the complete waste of defective links bandwidth. Experimental results indicate that our method reduces the latency overhead significantly and enables graceful performance degradation, when compared with related partially faulty link usage proposals, and saves area and power overheads by up to 29% and 43.1%, respectively, when compared with spare wire replacement methods.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip A Novel Flit Serialization Strategy to Utilize Partially Faulty Links in Networks-on-Chip ∗ † ∗ ∗ Changlin Chen , Ye Lu , Sorin D. Cotofana Computer Science and Engineering, Software and Computer Technology Delft University of Technology, Delft, the Netherlands Email: {c.chen-2, S.D.Cotofana}@tudelft.nl The institute of Electronic, Communication, and Information Technology (ECIT) Queen’s University of Belfast, Northen Ireland, UK Email: ylu10@qub.ac.uk † Abstract—Aggressive MOS transistor size scaling substantially increase the probability of faults in NoC links due to manufacturing defects, process variations, and chip wire-out effects. Strategies have been proposed to tolerate faulty wires by replacing them with spare ones or by partially using the defective links. However, these strategies either suffer from high area and power overheads, or signiﬁcantly increase the average network latency. In this paper, we propose a novel ﬂit serialization method, which divides the links and ﬂits into several sections, and serializes ﬂit sections of adjacent ﬂits to transmit them on all available fault-free link sections to avoid the complete waste of defective links bandwidth. Experimental results indicate that our method reduces the latency overhead signiﬁcantly and enables graceful performance degradation, when compared with related partially faulty link usage proposals, and saves area and power overheads by up to 29% and 43.1%, respectively, when compared with spare wire replacement methods. Keywords-Networks-on-Chip; partially faulty link usage; permanent error; ﬂit serialization; I . IN TRODUC T ION While the aggressive transistor size shrinking improves the chip capability, it also makes the manufacturing yield and chip dependability increasingly serious concerns. Links in Networks-on-Chip (NoC) are becoming more prone to various kinds of failures caused by manufacturing defects [1], chip wear-out effects [2], or Process Parameter Variations (PPV) [3] [4]. As a single permanent fault in a link may degrade the system’s performance dramatically or even render the chip useless, defective links need to be tolerated. The most intuitive way to deal with defective links is making use of fault-tolerant adaptive routing protocols [5] [6]. Defective links are discarded and alternative fault-free routes are chosen by the routing function to forward packets. This ineffective usage of link bandwidth increases packet delivery time if the route is not on the minimal path, and decreases network throughput due to the congestion around the faulty links. For a given permanent wire fault rate, the fault probability in a defective link is typically low, thus rather than completely discarding a defective link, a more effective approach is to isolate the faulty wires in the defective links and to keep on use the fault-free ones to transmit packets. While wires with small frequency deviation due to PPV can be dealt with by the methods described in [7] or [8], this paper proposes a novel ﬂit serialization method to tolerate permanent faulty wires and to utilize partially faulty links. In order to achieve the maximum utilization of the link bandwidth, links and ﬂits are divided into several sections, and novel fault-tolerant transmitters and receivers, which are placed inside the output and input ports of NoC routers, respectively, are proposed to efﬁciently utilize the fault-free sections. Adjacent ﬂits are serialized at the transmitter side to ﬁt the narrowed link and are deserialized at the receiver side. The proposed transmitter and receiver are transparent to the router such that their utilization is not constrained by the router architecture and implementation, nor by the network topology. The proposed link fault-tolerant architecture is compared with equivalent state of the art solutions described in [9], [10], and [11] in the context of a baseline NoC system. The experimental results indicate that the proposed method has following advantages: 1) The proposed method utilize the remaining link bandwidth more efﬁciently than other partially faulty link usage strategies and enables graceful performance degradation of the NoC system. When compared with the spare wire replacement method, which has a similar fault-tolerant capability, our approach reduce the area and the dynamic power overheads by up to 29% and 43.1%, respectively. The rest of the paper is organized as follows. A brief survey of related work is presented in Section II. Architecture and detailed implementation of the proposed transmitter and receiver are described in Section III. Two options to combine our proposal with Error Control Coding (ECC) logic are illustrated in Section IV. Section V presents the simulation result and Section VI concludes the presentation. 2) 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.22 124 I I . R E LAT ED WORK Grecu et al. [1] proposed a simple link fault-tolerant method that uses pre-fabricated spare wires to replace faulty wires due to manufacturing, which can effectively enhance the NoC interconnect yield. However, this approach has the drawback of high silicon cost. Since wires do not scale with transistors, and as the feature size of transistor further decreases, the overhead brought by using spare wires is expected to increase [12]. Lehtonen et al. [13] proposed a cost effective method that divides the link into four sections and provides each section with one spare wire. However, this approach cannot tolerate the case in which more than one faulty wire exist in the same section. To address this drawback, an improved method was proposed in [9], where the 4 spare wires are shared rather than exclusively owned by each section. As the number of cross-points used for sharing spare wires increases with the link width and the count of spare wires, this approach is constrained by the link width and wire error probability. To maintain a fault-free link, more spare wires and cross-points are needed resulting in a wider link and a higher permanent error probability. Palesi et al. [11] and Lehtonen et al. [9] proposed the method of using ﬂit splitting to tolerate faulty wires. As opposed to the spare wires replacement method discussed above, extra transmission latency is introduced by using the ﬂit splitting method. In this approach, a link is divided into four sections, and fault-free sections can be used to transmit ﬂits. However, the available bandwidth is reduced to half even if only one section is defective, and if faulty wires exist in every section, the link has to be discarded. In the remainder of this paper, we name this method as Simple Flit Quad Splitting (SFQS). A packet rebuilding/restoring algorithm is proposed by Yu et al. [14] to utilize the links with reduced bandwidth. To be transmitted on the defective link, every ﬂit is split into two parts (a big part and a small part). The ﬁrst part (the big part) of each ﬂit is transmitted ﬁrst in the narrowed link, followed by a reassembled ﬂit at the end of the packet containing the rest (the small part) of the ﬂits. The saved fault free wires are used to replace the broken wires. This method can efﬁciently utilize the remaining link bandwidth. However, because an integral ﬂit can only be rebuilt after the reassembled ﬂit has been received, this method cannot be used in low latency routers with wormhole or VirtualCut-Through (VCT) switching technology. By noticing that the faulty wires are typically evenly distributed, Vitkovskiy et al. [10] introduced a link recovery mechanism named PFLRM, which is mainly comprised of a ﬂit shifter, a de-shifter, and a ﬂit re-assembler. Using this approach, the ﬂits are ﬁrst rotated at the transmitter side according to the fault vector of the link and then transmitted. At the receiver side, the ﬂits are de-rotated and re-assembled. The number of iterations of shift operation and transmission to transmit a ﬂit correctly is decided by the maximum width of the faulty wire cluster in the link. At least two cycles (one iteration) are needed to transmit a ﬂit successfully even if only a single faulty wire exists in the link. If 𝑚 broken wires are clustered together, 𝑚 iterations are required. Although this approach can theoretically be tolerant to a defective link with an arbitrary number of faulty wires, the latency overhead introduced by this approach can be signiﬁcantly high. In summary, spare wires can preserve the performance but introduce a high silicon overhead. By contrast, ﬂit splitting and PFLRM have low area overhead but introduce high extra latency. The method proposed in this paper use a novel serialization strategy to utilize the partially faulty links. It signiﬁcantly reduce the latency, when compared with ﬂit splitting and PFLRM, while maintaining a more reasonable silicon cost, when compared with faulty wire replacement method. I I I . TH E PRO PO S ED ARCH I T EC TUR E A high-level block diagram of the proposed partially faulty link utilization strategy is depicted in Fig. 1. Similar with the work proposed in [11], we divide the data link into several sections, for example 4 or 8 link sections. Accordingly, ﬂits are divided into the same number of ﬂit sections. A Test Data Generator (TDG) and a Test Error Detector (TED) are used to diagnose the link and generate a fault vector to indicate the faulty link sections. Two transmitter (TX) and receiver (RX) pairs with unidirectional links are used to connect adjacent routers. TX and RX are made aware about the health status of the link via the fault vector. When the link is fault-free, ﬂits can be transmitted according to the normal protocol applicable to a healthy link, bypassing the proposed ﬂit deserialization unit at the receiver side. When faulty wires exist in some link sections, ﬂit sections of adjacent ﬂits are ﬁrst serialized by the ﬂit serialization unit, and then transmitted via the fault-free link sections. At RX side, the ﬂit deserialization unit deserializes all ﬂit sections to reconstruct the original ﬂits. We note in here that since the number of control lines is much smaller than the number of data lines, they can be protected by Triple Modular Redundancy (TMR) method with a marginal overhead. Moreover, ECC logic (not shown in Fig. 1) can be added before (after) the ﬂit serialization (deserialization) unit to protect data from transient errors. A. Link Diagnosis TDG and TED diagnose the link and provide the fault vector to the ﬂit serialization and deserialization modules. Unlike spare wire replacement or PFLRM, which require the precise status of each wire, our method just need link fault vectors at the section level. For example, if the third section of a link (with four sections in total) contains faulty wires, the fault vector of the link is marked as ‘1101’. Thus 125 Router RX TX TX RX Router fault_vector data_acceptable flit_type flit_serialize_ctrl sel optional output buffer . . . Router flit  serialization TDG data_lines section 1 . . . section N ctrl line bypass flit  deserialization TED Router input buffer . . . Figure 1. Proposed link fault-tolerant architecture. the effort to do link diagnosis is reduced, when compared with the complicated bit level link diagnosis methods in [13] and [15]. Link testing is performed periodically and can be triggered by an uncorrectable error detected by the ECC logic. Each section is tested independently while data transmission is still going on other sections. Test vectors are generated by TDG at TX side and are transmitted on the section under test. Received test vectors are analyzed by TED at RX side. This process is repeated on each section until the link fault vector is obtained. Because intermittent errors may have the same syndrome as permanent errors when they happen, sections which are marked as faulty in the previous test are also tested, to prevent situations when vanished intermittent errors are still disabling sections. At the end of the diagnosis process the achieved fault vector is sent to the transmitter and receiver control units. B. Flit Serialization The ﬂit serialization unit is presented in Fig. 2. For the sake of simplicity, we assume that both the ﬂit and link are divided into four sections, and the link width is 32-bit. We note that the proposed principle is more general and can be applied to wider links with more sections. To serialize the ﬂits, a link register (link reg TX ) with a width of two ﬂits is used. The link register is divided into eight sections and each section can be read and write independently. The register is designed in this way such that a new ﬂit can be registered in the Least Signiﬁcant Half (LSH) of the register if there are ﬂit sections in the Most Signiﬁcant Half (MSH) still waiting to be transmitted, and vice versa. If the link is fault-free, only the MSH of the register is used, acting as a conventional link register. Otherwise, ﬂits are serialized in link reg TX under the control of the ﬂit serialize ctrl unit. The serialization process is presented in more detail in Section III-D. Multiplexers are used to select the ﬂit sections to be transmitted. The link sections containing faulty wires that are indicated by the fault vector are not used for data transmission. The signal ﬂit type indicates the presence of a new ﬂit from the crossbar (output buffers are not used). With a narrowed link, the ﬂit is transmitted at a lower rate on the link than on the crossbar within the router, thus a data acceptable signal is added to disable the transmission update_0 update_1 flit[31: 24] flit[23: 16] flit[15: 8] flit[7: 0] 12 do[31: 24] do[23: 16] do[15: 8] do[7: 0] 3 mux mux mux mux link_reg_TX Figure 2. Flit serialization unit - TX. flit_deserialize_ctrl 16 2 fault_vector flit_type flit[31: 24] flit[23: 16] flit[15: 8] flit[7: 0] di[31: 24] di[23: 16] di[15: 8] di[7: 0] link_reg_RX Figure 3. Flit deserialization unit - RX. requests when the buffer space in the serialization unit is full and cannot accept a new ﬂit. For networks that employ STALL/GO and ACK/NACK ﬂow-control mechanisms, data acceptable can be directly connected to the ﬂowcontrol signals. For networks that use credit-based ﬂowcontrol mechanism, a Virtual Channel (VC) can only request for ﬂit transmission when the credit in its downstream VC is larger than zero, so the cancellation of requests does not affect the implementation of the ﬂow-control mechanism. C. Flit Deserialization The ﬂit deserialization unit is depicted in Fig. 3. Similar with the ﬂit serialization unit, a link register (link reg RX ) is also employed. Multiplexers are used to select the valid sections from the link under the control of ﬂit deserialize ctrl unit. Relative sections of the link register are updated when new ﬂit sections are received. When the MSH or LSH of the link register is full, one ﬂit is recovered and can be read out by the router. A ﬂit type signal indicates the availability of a new valid ﬂit. 126 D. Flit Transmission Process Fig. 4 graphically depicts the timing diagrams capturing the ﬂit transmission process speciﬁc to our method. When the link is fault-free, a ﬂit from the crossbar is loaded into the MSH of link reg TX and then transmitted to the input buffers of the downstream router directly (see Fig. 4(a)). At the RX side, the ﬂit deserialization unit is bypassed. Fig. 4(b) presents the situation when one of the sections is affected by faults. At TX side, ﬂit 𝑎 ﬂoats at the output port of the crossbar at the rising edge of 𝑐𝑙𝑘1 and is written into the MSH of link reg TX at the rising edge of 𝑐𝑙𝑘2. During 𝑐𝑙𝑘2 cycle, the ﬁrst three sections of the ﬂit 𝑎 (𝑎3 , 𝑎2 , 𝑎1 ) are transmitted via the three fault-free sections of the link. At the rising edge of 𝑐𝑙𝑘3, ﬂit 𝑏 is written into the LSH of link reg TX. Flit sections 𝑎0 , 𝑏3 , and 𝑏2 are transmitted in the same cycle. The signal data acceptable is set to ’0’ in 𝑐𝑙𝑘3 such that no new ﬂit may appear at the output port of crossbar in 𝑐𝑙𝑘4. A wait cycle is inserted in this way to wait for the last three sections of ﬂit 𝑐 to be sent in 𝑐𝑙𝑘5. The signals high reg state and low reg state are used to indicate the status of link reg TX MSH and LSH, respectively. Each signal is asserted once a ﬂit is written into the corresponding register part, and de-asserted in the clock cycle when the data are read out. At the receiver side (see Fig. 4(c)), ﬂit sections are deserialized and reassembled into integral ﬂits in link reg RX. Valid ﬂit sections are selected by input side multiplexers and be written into the correct positions in link reg RX. Once the MSH or LSH of the register is full, an integral ﬂit is recovered. The signals ﬂit 1 recovered and ﬂit 2 recovered indicate the availability of recovered ﬂits and control the output side multiplexers to select the corresponding register sections. In principle, we can use ﬁner section granularity to achieve better performance. However this implies that more and larger multiplexers are required, which may have a negative impact on area and power consumption overheads. The number of sections can be determined via a trade-off process, which takes into consideration the targeted faulttolerant capability and the available silicon real estate. E. Link Latency The link latency when the proposed method is used to continuously transmit different number of ﬂits (ﬂit number) can be expressed as: 𝑙𝑎𝑡𝑒𝑛𝑐𝑦 = ⌈ 𝑠𝑒𝑐𝑡𝑖𝑜𝑛 𝑛𝑢𝑚𝑏𝑒𝑟 × 𝑓 𝑙𝑖𝑡 𝑛𝑢𝑚𝑏𝑒𝑟 𝑓 𝑎𝑢𝑙𝑡 𝑓 𝑟𝑒𝑒 𝑠𝑒𝑐𝑡𝑖𝑜𝑛 𝑛𝑢𝑚𝑏𝑒𝑟 ⌉ . (1) Where section number is the number of sections in the link. For example, if 10 ﬂits are waiting to be transmitted via a partially faulty link, which has one defective link section, the link latency is 14 cycles when the link is divided into 3 2 1 0 a a a a 3 2 1 0 b b b b 3 2 1 0 c c c c 3 2 1 0 a a a a 3 2 1 0 b b b b 3 2 1 0 c c c c …. . . … … CLK data_from_crossbar data_on_link 3 2 1 0 d d d d …. . . 3 2 1 0 d d d d clk1 clk2 clk3 clk4 clk5 …. . . … 3 2 1 0 a a a a xxxx 3 2 1 0 b b b b xxxx 3 2 1 0 c c c c xxxx cyclic_reg_TX 3 2 1 0 d d d d xxxx 3 2 1 a a a 3 2 1 0 a a a a 3 2 1 0 b b b b 3 2 1 0 c c c c …. . . … … CLK data_from_crossbar data_on_link 0 3 2 a b b 2 1 0 c c c wait 3 2 1 0 d d d d …. . . … 3 2 1 0 a a a a xxxx 3 2 1 0 3 2 1 0 a a a a b b b b 3 2 1 0 3 2 1 0 c c c c b b b b 1 0 3 b b c cyclic_reg_TX clk1 clk2 clk3 clk4 clk5 high_reg_state low_reg_state data_acceptable 3 2 1 a a a 3 2 1 0 a a a a 3 2 1 0 b b b b 3 2 1 0 c c c c …. . . … … CLK data_to_input_buffer flit_1_recovered data_ on_link 0 3 2 a b b 2 1 0 c c c …. . . … 3 2 1 a a a xxxxx 3 2 1 0 3 2 a a a a b b xx 3 2 1 0 3 2 1 0 c c c c b b b b 1 0 3 b b c cyclic_reg_RX clk2 clk3 clk4 clk5 clk6 3 2 1 d d d 3 2 1 0 3 2 1 0 c a a a b b b b …. . . flit_2_recovered (a) (b) (c) Figure 4. Timing diagram of proposed mechanism (a) Timing diagram for a fault-free link; (b) Transmitter side when one section contains faulty wires; (c) Receiver side when one section contains faulty wires. four sections, and 12 cycles when the link is divided into eight sections. By comparison, the link latencies of PFLRM and SFQS are expressed as in (2) and (3), respectively. 𝑙𝑎𝑡𝑒𝑛𝑐𝑦𝑝𝑓 𝑙𝑟𝑚 = (𝑐𝑙𝑢𝑠𝑡𝑒𝑟 𝑠𝑖𝑧𝑒 + 1) × 𝑓 𝑙𝑖𝑡 𝑛𝑢𝑚𝑏𝑒𝑟(2) 𝑠𝑒𝑐𝑡𝑖𝑜𝑛 𝑛𝑢𝑚𝑏𝑒𝑟 × 𝑓 𝑙𝑖𝑡 𝑛𝑢𝑚𝑏𝑒𝑟 𝑙𝑎𝑡𝑒𝑛𝑐𝑦𝑠𝑓 𝑞𝑠 = 𝑎𝑣𝑎𝑖𝑙𝑎𝑏𝑙𝑒 𝑠𝑒𝑐𝑡𝑖𝑜𝑛 𝑛𝑢𝑚𝑏𝑒𝑟 (3) Where cluster size in (2) is the fault cluster size in the link, and available section number in (3) can have value 1, 2, or 4 according to the number of fault-free sections in the link. A comparison of link latency overheads when the proposed method, PFLRM, and SFQS are used to continuously transmit ﬂits via a defective link is presented in Table I. The link latency overhead is 0% if the link is fault free. Note that the fault number in the table indicates the fault cluster size in the PFLRM worst scenario, and the number of faulty sections for the proposed method and SFQS. In the best PFLRM scenario, the cluster size is always one. From the Table we can observe that only when more than half of the link sections are broken, the link latency overhead of the proposed method reaches 100%. By comparison, the overheads are at least 100% for both PFLRM and SFQS methods. If we assume that each wire has the same probability (𝑝𝑒 ) to be permanently fault, the probability that the number of faulty wires (𝑁𝑒 ) equals 𝑘 in an 𝑛-bit wide link can be calculated using (4). 127 L INK LAT ENCY OV ERH EAD S WH EN FL I T S AR E TRAN SM I T T ED CON T INUOU S LY Table I Fault number 0 1 2 3 4 5 6 7 Proposed 4 sections 0% 33.3% 100% 300% – – – – Proposed 8 sections 0% 14.3% 33.3% 60.0% 100% 167% 300% 700% PFLRM PFLRM Best situation Worst situation 0% 0% 100% 100% 100% 200% 100% 300% 100% 400% 100% 500% 100% 600% 100% 700% SFQS 0% 100% 100% 300% – – – – 𝑒 (1 − 𝑝𝑒 )𝑛−𝑘 𝑝 𝑘 𝑛 𝑘 ( ) 𝑃𝑁𝑒=𝑘 = (4) In a 32-bit wide link, if 𝑝𝑒 = 10−5 [9], the probability of 𝑁𝑒 = 2 is 𝑃𝑁𝑒=2 = 4.96 × 10−8 , while the probability of 𝑁𝑒 = 4 is 𝑃𝑁𝑒=2 = 3.60×10−16 . This means that while it is possible to have links with high fault degrees in an NoC, the probability for this to happen in practical implementations is rather low. Thus, in practice, we can expect that most of the links are affected by a low number of faults in the same time, in which case the proposed method is quite effective. In extreme cases corresponding to large physical defects multiple adjacent wires may get faulty. In such a case PFLRM requires as many iterations as faulty wires to successfully transmit a ﬂit, which results in a large latency overhead. Alternatively, to be able to face such defects, a spare wire replacement method has to make use of multiple spare wires, which results in a large area overhead. In the case of the proposed method such a large defect will most likely affect one or two link sections, thus it can better face such cases at the expense of less area overhead. IV. ECC IN T EGRAT ION ECC and retransmission are widely used effective methods to tolerate transient errors in communication. In a router which embeds the proposed link fault-tolerant architecture, ECC and retransmission logic can be used either outside or inside of the proposed architecture. If ECC is used outside of the link fault-tolerant architecture, i.e., the error coding logic is placed before the ﬂit serialization unit and the error decoding logic is placed after the ﬂit deserialization unit (see Fig. 5(a)), a powerful ECC is needed to detect and correct transient errors in, not only the data link, but also in the transmitter and the receiver. Because our proposed architecture is transparent to the rest of the router, ECC logic can be implemented in a conventional way. The detection of transient errors in a ﬂit that cannot be corrected by ECC will trigger a request for retransmission and the link diagnosis process. 128 ECC coder TX Router TX ECC  coder . . . ECC  coder Router link_section_1 . . . link_section_N (a) link_section_1 . . . link_section_N (b) RX ECC decoder Router RX ECC  decoder . . . ECC  decoder Router Figure 5. Proposed architecture collaborate with ECC (a) ECC logic before TX and after RX; (b) ECC logic after TX and before RX. If ECC is used inside of the proposed architecture, i.e., the error coding logic is placed after ﬂit serialization unit and the error decoding logic is placed before the ﬂit deserialization unit (see Fig. 5(b)), only transient errors that might appear on the link are covered. Error coding and decoding logics are added to each link section. If the error in a ﬂit section cannot be corrected by ECC, the ﬂit section is marked as invalid. As a consequence the ﬂit it belongs to is invalidated and the retransmission process is invoked. The drawback of this approach is that the transmitter and receiver cannot be protected by ECC. V. P ER FORMANC E EVA LUAT ION To put the implications of our link fault-tolerant architecture in a better practical prospective, we evaluate and compare it with other three tightly related proposals presented in [9], [10], and [11], namely spare wire replacement, PFLRM, and SFQS, respectively. To this end, we implemented all these four link fault-tolerant methods at RTL level by using Verilog HDL, and applied them in the context of the NoC platform developed by Lu et al. [16]. The baseline router has 2 pipeline stages: look-ahead Routing Computation (RC) and combined VC/Switch Allocation POW ER AND AR EA OV ERH EAD O F D I FF ER EN T L INK FAU LT- TO L ERAN T M E THOD S Table II Link fault-tolerant methods Basic router 4 sections 8 sections 4 8 Proposed Spare wires PFLRM SFQS Dynamic Power (𝑚𝑊 ) 18.20 / 0% 20.81 / 14.3% 21.18 / 16.4% 26.71 / 46.8% 29.03 / 59.5% 19.30 / 6.0% 20.27 / 11.4% Leakage Power (𝑚𝑊 ) 0.5269 / 0% 0.7994 / 51.7% 0.9294 / 76.4% 0.9959 / 89% 1.0442 / 98.2% 0.5789 / 9.9% 0.6807 / 29.1% Area (𝜇𝑚2 ) 69560 / 0% 89567 / 28.8% 96538 / 38.8% 102383 / 47.2% 116789 / 67.9% 83326 / 19.8% 81288 / 16.9% (VA/SA) in the ﬁrst stage, and Switch Traversal (ST) in the second stage. Link Traversal (LT), where the proposed fault-tolerant architecture is applied, is implemented as a separated pipeline stage. Each baseline router has 5 Physical Channels (PC), and each PC is facilitated with 5 Virtual Channels (VC), and a 4-ﬂit deep, 32-bit wide buffer is applied in each VC. Both the width of ﬂit and link are 32, which is most widely used in NoC proposals [17]. The router and the link fault-tolerant modules are synthesized using the Synopsys Design Compiler with TSMC 65-nm standard cell as target technology. The target frequency is 500MHz. A. Area and Power Overhead The power consumptions and area overheads of all these four different link fault-tolerant methods are presented in Table. II. The proposed method is evaluated with two versions containing 4 and 8 link sections, and the spare wire is evaluated with two versions containing 4, and 8 spare wires. From the Table we can observe that, the area and power overheads of the proposed architecture are lower than the ones of spare wire replacement, but higher than the ones of PFLRM and SFQS. For example, with 8 link sections, the area overhead of our method is 38.8%, which is 29% less than the one of the spare wire replacement method (8 wires), but 19% higher than PFLRM and 21.9% higher than SFQS. The dynamic power overhead of the proposed architecture with 8-section version is 16.4%, which is 43.1% lower than of using 8 spare wires (59.5%), but 5% higher than SFQS (11.4%) and 10.4% higher than PFLRM (6.0%). The leakage power consumption of proposed method also falls between spare wire replacement and other two partially faulty link usage methods. For all the fault-tolerant methods we have evaluated, the main parts of the power and area overheads are caused by multiplexers and registers. When compared with SFQS, the proposed method with four link sections use twice more multiplexers and registers in the transmitter and receiver, and with eight sections the ratio increases to four. Therefore, the area and power cost of our proposed method is higher than SFQS. When the spare wire replacement method is used, an 𝑁 + 1 : 1 multiplexer is used at each end of a link wire (𝑁 is the number of faulty wires can be tolerated) , leading to higher area and power consumptions than all the other methods. B. System performance To evaluate the performance of our proposed architecture, we applied different fault patterns of the links with a range of permanent wire fault rates (0.001, 0.01, 0.05, and 0.1) to an 8 × 8 2D mesh NoC system. We assume that each wire has the same fault rate 𝑝𝑒 and faults are uniformly distributed across the links. For comparison and illustration purpose, the fault rate 𝑝𝑒 is set higher than its typical value [9]. The three partially faulty link usage strategies, i.e., the proposed method with four (proposed s4) and eight (proposed s8) link sections, PFLRM, and SFQS, are applied to the NoC system and simulated with each fault pattern. Synthetic uniform random trafﬁc pattern and XY routing protocol are used in the simulation. Fig. 6(a), (c), (e), and (g) present the fault distribution in each fault pattern and Fig. 6(b), (d), (f), and (h) plot the corresponding performance curves of the NoC system in terms of average latency when different strategies are utilized. In the fault distribution ﬁgures, the height of each column represents the proportion of defective links in the NoC. In each column, different colors represent the proportions of defective links with different fault degrees. For example, the red parts of the columns denote the proportions of links with two faulty wires (fault num), or links with two unusable link sections in proposed s4, proposed s8, and SFQS, or links with a fault cluster size of two in PFLRM. As we can observe in Fig. 6(a), when the permanent wire fault rate is low (𝑝𝑒 = 0.001), the percentage of defective links in the NoC is 2.7%, out of each 91% contain one faulty wire while the rest of them have two. For the proposed method, this means that 94% of the defective links contain one unusable section and 6% contain two. As it is indicated in Table I, the link latency overheads on this defective links are very low in proposed s8. Therefore, proposed s8 achieves the best performance as its average latency is very close to the fault-free case. The performance of proposed s4 is lower than proposed s8 but still better than PFLRM and SFQS. This can be explained by the fact that in links with one defective section, both PFLRM and SFQS will double 129 0,0% 0,5% 1,0% 1,5% 2,0% 2,5% 3,0% fault_num proposed_s4 proposed_s8 cluster_size SFQS e p n e c r t o e g a f e d f c e Ɵ e v i l s k n Pe = 0.001 2 1 (a) fault pattern when 𝑝𝑒 = 0.001; 0 0.05 0.1 0.15 0.2 0.25 injection rate  (flits/cycle/node) 0.3 0.35 0 20 40 60 80 100 120 140 160 180 200 packet_length = 4, Pe = 0.001 e v a r e g a l a t y c n e ( ) s e c y c l fault_free proposed_s8 proposed_s4 PFLRM SFQS (b) performance when 𝑝𝑒 = 0.001; 0,0% 5,0% 10,0% 15,0% 20,0% 25,0% 30,0% fault_num proposed_s4 proposed_s8 cluster_size SFQS e p n e c r t o e g a f e d f c e Ɵ e v i l s k n Pe = 0.01 3 2 1 (c) fault pattern when 𝑝𝑒 = 0.01 0 0.05 0.1 0.15 0.2 0.25 injection rate (flits/cycle/node) 0.3 0.35 0 20 40 60 80 100 120 140 160 180 200 packet_length = 4, Pe = 0.01 e v a r e g a l a t y c n e ( ) s e c y c l fault_free proposed_s8 proposed_s4 PFLRM SFQS (d) performance when 𝑝𝑒 = 0.01; 90,0% 80,0% 70,0% 60,0% 50,0% 40,0% 30,0% 20,0% 10,0% 0,0% fault_num proposed_s4 proposed_s8 cluster_size SFQS e p n e c r t o e g a f e d f c e Ɵ e v i l s k n Pe = 0.05 7 6 5 4 3 2 1 (e) fault pattern when 𝑝𝑒 = 0.05 0 0.05 0.1 0.15 0.2 0.25 injection rate (flits/cycle/node) 0.3 0.35 0 20 40 60 80 100 120 140 160 180 200 packet_length = 4, Pe = 0.05 e v a r e g a l a t y c n e ( ) s e c y c l fault_free proposed_s8 proposed_s4 PFLRM SFQS (f) performance when 𝑝𝑒 = 0.05; 100,0% 90,0% 80,0% 70,0% 60,0% 50,0% 40,0% 30,0% 20,0% 10,0% 0,0% fault_num proposed_s4proposed_s8 cluster_size SFQS e p n e c r t o e g a f e d f c e Ɵ e v i l s k n Pe = 0.1 >8 7 6 5 4 3 2 1 (g) fault pattern when 𝑝 𝑒 = 0 .1 0 0.05 0.1 0.15 0.2 0.25 injection rate (flits/cycle/node) 0.3 0.35 0 20 40 60 80 100 120 140 160 180 200 packet_length = 4, Pe = 0.1 e v a r e g a l a t y c n e ( ) s e c y c l fault_free proposed_s8 PFLRM (h) performance when 𝑝𝑒 = 0. 1. Figure 6. Fault Patterns and Corresponding Performance of the NoC. the link latency at least, while proposed s4 can keep the latency overheads as low as 33.3%. With the increase of 𝑝𝑒 , more links contain faults and the average faulty wire number becomes larger, leading to more unusable sections and bigger fault cluster size in a link (see Fig. 6(c), (e), and (g)). The average latencies increase for all partially faulty link usage strategies. But the proposed method still outperforms PFLRM and SFQS (see Fig. 6(d)) when the fault rate is not very high (𝑝𝑒 = 0.01). When 𝑝𝑒 further increases to 0.05 (see Fig. 6(e) and Fig. 6(f)), proposed s8 still has the best performance because the link latency overhead on more than 99% of the defective links is less than 100%. As the number of links which have 200% plus link latency in proposed s4 130                                                           and SFQS (more than two sections are broken) surpasses PFLRM (cluster size larger than one), PFLRM achieves a better performance than proposed s4 and SFQS but still worse than proposed s8. We note that at such 𝑝𝑒 value, totally broken links exist in proposed s4 and SFQS (all link sections are broken). Because fault-tolerant routing is not considered in this paper, the totally broken links are still given one available section to maintain the simulation integrity. This cannot fundamentally affect the results because only 1 or 2 such links may exist in the NoC at this fault rate. When the permanent wire fault rate is as high as 0.1, 97% of the links are defective and the average fault degree is very high, as dipected in Fig. 6(g). Under this extreme condition, proposed s8 exhibits equivalent performance as PFLRM (see Fig. 6(h)). Proposed s4 and SFQS have so many totally broken links that they are not considered. If the fault rate keeps on increasing, totally broken links appear in proposed s8 and as a consequence its performance gets worse than the PFLRM one. The number of faulty wires in a link equals the number of spare wires required in spare wire replacement method. From Fig. 6(g) we can observe that when the permanent wire fault rate is 0.1, spare wire replacement method can maintain 98% of links available with 8 spare wires, while proposed s8 can achieve the same fault-tolerant capability with reduced performance. As the fault degrees of links increase to the fault pattern presented in Fig. 6(g), the performance of proposed s8 degrades gracefully. V I . CONC LU S ION S A novel structure which can utilize partially faulty links with graceful performance degradation has been presented. The ﬂits and links are divided into several sections according to the required fault-tolerant capability and silicon budget. When faulty wires exist in some of the link sections, ﬂit sections of adjacent ﬂits are serialized at the transmitter side, transmitted via the remaining fault-free link sections, and recovered at the receiver side before they are written into the input buffers. The proposed transmitter and receiver are transparent to other parts of the routers thus other function units, e.g., the ECC logic, can be implemented conventionally. Experimental results demonstrate that the latency overhead is signiﬁcantly reduced by our method when compared with related partially faulty link usage methods, i.e., PFLRM and SFQS, while the area and power overheads can be diminished by up to 29% and 43.1%, respectively, when compared with the spare wire replacement method with similar fault-tolerant capability. Our experiments also suggest that for links which are too much affected by faults, it might be a better choice to deem them as totally broken and to route packets on alternative low latency links. The combination of fault-tolerant routing and our proposed partially faulty link usage method to achieve an intelligent path selection constitute a future work subject. "In-network Monitoring and Control Policy for DVFS of CMP Networks-on-Chip and Last Level Caches.,"In chip design today and for a foreseeable future, on-chip communication is not only a performance bottleneck but also a substantial power consumer. This work focuses on employing dynamic voltage and frequency scaling (DVFS) policies for networks-on-chip (NoC) and shared, distributed last-level caches (LLC). In particular, we consider a practical system architecture where the distributed LLC and the NoC share a voltage/frequency domain which is separate from the core domain. This architecture enables controlling the relative speed between the cores and memory hierarchy without introducing synchronization delays within the NoC. DVFS for this architecture is more difficult than individual link/core-based DVFS since it involves spatially distributed monitoring and control. We propose an average memory access time (AMAT)-based monitoring technique and integrate it with DVFS based on PID control theory. Simulations on PARSEC benchmarks yield a 33% dynamic energy savings with a negligible impact on system performance.","In-network Monitoring and Control Policy for DVFS of CMP Networks-on-Chip and Last Level CachesXi Chen∗	Zheng Xu∗	Hyungjun Kim∗	Paul Gratz∗	Jiang Hu∗	Michael Kishinevsky†	Umit Ogras†∗Department of Electrical and Computer Engineering, Texas A&M University†Strategic CAD Labs, Intel  Corporation∗{xchen19, xuzheng0309, hyungjun, pgratz, jianghu}@tamu.edu, †{michael.kishinevsky, umit.y.ogras}@intel.comAbstract—In chip design today and for a foreseeable future, on-chip communication is not only a performance  bottleneck but also a substantial power consumer. This work focuses on employing dynamic voltage and frequency scaling (DVFS) policies for networks-on-chip (NoC) and shared, distributed last-level caches (LLC). In particular, we consider a practical system architecture where  the  distributed  LLC  and  the  NoC  share  a voltage/frequency domain which is separate from the core domain. This architecture enables controlling the relative speed between the cores and memory hierarchy without introducing synchronization delays within the NoC. DVFS for this archi- tecture is more difﬁcult than individual link/core-based DVFS since it involves spatially distributed monitoring  and  control. We propose an average memory access time (AMAT)-based monitoring technique and integrate it with DVFS based on PID control theory. Simulations on PARSEC benchmarks yield  a  33% dynamic energy savings with a negligible impact on system performance.Index Terms—Multicore, NoC, dynamic power, memory systemIntroductionThe progress of chip design technology faces two related challenges: power and on-chip communication [13]. A recent study by Google [1] shows that, as power-efﬁciency improves for server processors, the interconnection network is becoming a major power consumer in the datacenter. Likewise, on- chip communication now forms a power bottleneck in chip multiprocessors (CMPs) given the considerable progress on processor core power-efﬁciency. In his speech at Interna- tional Conference on Computer-Aided Design 2011, Chris Malachowsky, co-founder of Nvidia, pointed out that the energy expended delivering data on  chip  has far exceeded the energy in computation operations. Dynamic voltage and frequency scaling (DVFS) is an effective and popular low- power technique. This paper presents techniques that facilitate efﬁcient DVFS for NoC (Networks-on-Chip), which is widely recognized as a scalable approach to on-chip communication. In this work, we will focus on DVFS for NoCs for CMPs. Previous work in DVFS for NoCs and CMPs have focused on per-core or per-router DVFS policies, as shown in Figure 1a. Unlike much prior work, we consider a realistic scenario wherein the entire NoC and shared last-level cache (LLC) forms a single voltage/frequency domain, separate from the domains of the cores (see Figure 1b). Latency is a critical characteristic in CMP NoCs [6, 5]. Synchronizing across clock domains is expensive in cycles per hop; placing many clock domain crossings in the interconnect makes the designunscalable by imposing a high cost in latency per hop [18].Furthermore, we argue placing the shared LLC in one clock domain across the chip is logical because it is one large, partitioned structure. Allowing some portions of the address space to see a penalty in performance due to a given CMP with V/F domains by tile.Separate V/F domain for the uncore and cores.Fig. 1: Logical CMP diagrams highlighting Voltage/Frequency domain partitions.LLC bank being clocked slower relative to other portions would impact performance determinism and could make the performance of active threads hostage to the DVFS state of idle threads. Unlike the individual cores which are running different threads/programs, the LLC banks have a mostly homogeneous load due to the interleaving of cache lines in the system; in this case, the voltage/frequency domain partitioning like Figure 1a can be inefﬁcient. For example, if one core is			active and makes many LLC requests while the other cores are idle, then, according to the partition in Figure 1a, only the V/F domain for the active core is in high V/F mode. However, this is not sufﬁcient as its data travels in other domains which are in low V/F modes. Therefore, Figure 1b is a more reasonable V/F domain partitioning for a CMP with shared LLC.DVFS of an entire NoC system is more difﬁcult to manage than the DVFS in previous works, such  as  that  for  each link [20] or each core [25]. In these cases, voltage/frequency levels are determined by a single, local observation, (e.g. link congestion [20] or core workload [25]). In contrast, sharing V/F level over the entire network, requires information on the load/activity of many different entities. Therefore, the challenge is how to tune the single V/F level to satisfy the need of the entire network, without impacting performance of the network by ﬂooding it with status information messages. Two crucial components of a DVFS system are the perfor- mance monitor, and the controller which computes the output V/F level according to observed performance. Compared to previous works [20, 25], our challenge is developing online monitoring techniques. That is, how to efﬁciently and reliably monitor the overall network performance, which is obviously more complex than a single core or link. If many places are monitored, then how to collect the monitoring results to thecentral power controller with low overhead?In this paper we address these questions. The key contribu- tions of this work are as follows:We introduce several new uncore status metrics to predict the impact of DVFS policy on system performance.We propose a novel, extremely low overhead, uncore status monitoring technique. This technique is composed of the following: 1) Per-node metric sampling, 2) Passive in-network status encoding, no extra packets needed, 3) Metric extrapolation to properly scale value weights.We introduce an uncore DVFS policy based upon PID (Proportional-Integral-Derivative) control.Results for our technique on the PARSEC suite show that we achieve a 33% dynamic energy reduction while limiting the system’s average memory access time (AMAT) increase to 5%, and overall system performance penalty to ∼ 2.5%.Background and Related WorkThis section introduces the basics of shared, distributed, last-level caches and their NoC interconnect in CMPs (col- lectively the “uncore”). We then introduce the basic concepts in NoC performance monitoring and discuss the power and performance constraints on the uncore. Finally we discuss power management using DVFS in the uncore.CMP Uncore BasicsTypical CMPs are composed of a set of cores, consisting of the processor and private lower level caches (level-1 and sometimes level-2), along with an “uncore”. The uncore por- tion of the die refers to all the integrated subsystems on the chip except the cores. More precisely, the LLC, the routers and links of NoC, integrated memory controller, integrated I/O controller etc. constitute the uncore. In other words, the uncore enables communication between the processing cores, and with the LLC, off-chip memory, I/O devices, graphics core and accelerators, if any. Therefore, any miss in the local caches of a core will result in an uncore request. Finding the location of the requested cache line, transferring the cache line to the core or the memory controller as well as controlling the global state of the cache line are all managed by the uncore. In modern LLCs, the banks of the LLC are partitioned and distributed such that a portion of the LLC is co-located with each core. This LLC  arrangement  has  the  advantage of improving performance over the prior, monolithic cache designs.In our baseline design, we assume coherence between the private caches in the cores is maintained via a distributed directory cache in the uncore. The NoC interconnecting the uncore and the cores primarily carries memory system trafﬁc. Particularly, the NoC carries lower-level cache spills and ﬁlls, lower-level cache coherence messages, and LLC cache spills and ﬁlls. We assume that LLC cache set indices are spread about the partitions of the LLC in a round robin fashion,       to ensure that each partition receives approximately the same amount of trafﬁc and no single partition becomes a hotspot.Uncore Power and Performance ImplicationsThe uncore consumes a signiﬁcant fraction of the whole chip power due to the relatively large proportion of the chip area it consumes. Dynamic power dissipation for CMOS circuits is given byP = α · C · V 2 · f	(1)Although the activity factor (α) for the uncore is not neces- sarily high, its total area and capacitance (C) can be large. Similarly the leakage power, a growing problem in future VLSI process technologies, is also proportional to the area of uncore. Equation (1) also includes two interrelated components– the voltage squared (V 2) and frequency (f ). For a givendesign, increasing the voltage makes transistors to switch faster, allowing the chip to operate at a higher frequency. Conversely, lowering the voltage forces a decrease of the clock frequency to meet timing constraints. Dynamic voltage and frequency scaling (DVFS), is a well-known technique which leverages this relationship to lower dynamic power consumption. Lowering the voltage has a quadratic effect on the dynamic power of the circuit being lowered, though this comes at the cost of some performance due to the required decrease in frequency. By lowering the voltage and frequency to match but not exceed the demands of the application, a good DVFS policy can achieve substantial power savings.Achieving power savings through DVFS in the cores is a comparatively simple problem, considering that the informa- tion needed to select an appropriate voltage and frequency  are available in a localized place, the CPU core itself (e.g. from performance counters within that core). Determining an appropriate DVFS state for the uncore is a signiﬁcantly more difﬁcult problem. Because the uncore consists of the LLC and network, the relative criticality of the uncore’s performance to the performance of the system is highly dependent on the application’s demand for LLC data and inter-thread commu- nication. Applications which are mostly L1 cache resident place little performance pressure on the uncore and the uncore can safely run at a relatively low frequency, while those with frequent L2 cache misses place high demands on the uncore and require the uncore to run at a high frequency.For purposes of studying uncore DVFS polices, we decom- pose the problem into three major components. First, because the uncore consists of two very different components, the LLC and NoC, it is unclear what performance metrics are appropriate as an input to the DVFS policy. Second, because the uncore is distributed across the chip, a mechanism must be developed to monitor the status of the uncore performance metrics and inform the DVFS policy. Finally third, once the inputs have been deﬁned and arrive at the DVFS controller,an appropriate policy must be developed. We explore these components in the remainder of this section.Uncore Performance MetricsNetwork communication trafﬁc load and performance can be described via several potential networks metrics. In this section we explore some potentially suitable indicators that may predict network load.Queue Occupancy and Crossbar Demand: Routers typ- ically have queues, or  input  FIFOs  for  temporary  storage of data. The occupancy level of a queue naturally indicates local congestion. Hence, queue occupancy has been widely used in adaptive routing [4, 10] and DVFS for individual links [20, 15, 17]. The overall congestion status of a network can be estimated by monitoring queue occupancy of all routers.A different and slightly overlapping metric for congestion is crossbar demand, which is the number of active requests to an output of a router [4]. Many requests to an output imply a convergent trafﬁc patten, which is likely to become a bottleneck. While queue  occupancy  measures the degree of downstream congestion, crossbar demand indicates local congestion. In either case, estimating the performance of the network as a whole requires monitoring many if not all routers. Average Per-Hop Latency (APHL): While queue occu- pancy and crossbar demand reﬂect network congestion, packet latency is a direct measure of network performance. Packet latency is deﬁned by the time span from a packet request being made to the arrival to its destination. Although low packet latency is obviously preferred, it is not directly obvious what should be the absolute goal in packet latency. Different types of workload can be expected to produce different average hop-counts and average packet lengths and hence different nominal packet latencies. For the convenience of a normalized comparison, we suggest average per-hop latency (APHL), which is the average latency a ﬂit incurs as it traverses each router along its path through the NoC. For any speciﬁc system, there is an unique minimum latency for one hop which is determined by the link delay plus the router pipeline latency. By removing distance traveled and serialization latency, APHL gives a trafﬁc pattern independent metric of network load. As such, APHL’s deviance from a given NoC’s inherent minimum per-hop latency serves as a clear target for the DVFS tuning. Average Memory Access Time (AMAT): While the previ- ous metrics merely provide the information of current network congestion, we also propose average memory access time (AMAT) as a metric which provides not only the current network status but also the demand for its performance. When a cache miss occurs on the private caches, the NoC is used to fetch the missing cache line from LLC. Thus, NoC perfor- mance translates to memory operation latency. Experimentally we determined that for small AMAT increases, IPC (instruc- tions per cycle) of the cores decreases approximately linearly with AMAT with a slope of .5. Consequently, AMAT provides a good index of required uncore performance. Equation (2)shows a simpliﬁed uncore AMAT formula: Fig. 2: AMAT with respect to uncore clock period.clock period (1/f ) of the uncore, such that Figure 2 shows a representation of AMAT.Figure 2 shows two extreme cases. f0 depicts the AMAT with respect to the clock period when HitRate(private) = 0, while f1 shows the AMAT when HitRate(private) = 1. f0 represents the case the most of the memory access results    in the private cache miss such that the missing block must    be transferred over the network. In this case, the AMAT is highly dependent on the uncore performance, hence decreasing the uncore frequency via DVFS has a strong negative impact on system performance and should be avoided. On the other hand, f1 represents the case where all memory accesses are served by the private caches. In such a case, decreasing uncore frequency has no impact on system behavior, hence should be done to achieve power savings with little impact on performance. Thus, it is desirable for the DVFS controller to account for AMAT in deciding whether increasing the frequency is worth the increased power or not.Uncore Status MonitoringIn the prior work, network status information has been pre- dominately used locally, particularly for adaptive routing [10, 24, 22, 21], and for localized DVFS policies [20, 15, 17]. Gratz et al. [4] and Ma et al. [12] propose small, light-weight status information networks to provide deeper visibility into the NoC and hence enable better adaptive routing decisions. In these cases, the performance metric obtained from the local router is assumed to provide enough information to enable local de- cisions, such as which output port to send a packet to, or what DVFS setting for a given router. These techniques, however, provide a limited view of the status of the network as a whole, either decreasing exponentially with distance [4] or limited to one bit of data per node in only the orthogonal directions [12]. There are some other monitoring techniques to inform DVFS policy in NoCs. Yin et al. [26] propose dedicated links and virtual channels for collecting and monitoring system status information; this approach is expensive in terms of design time to create a dedicated interface, power for these links and area for the logic associated. Rahimi et al. [17] propose to monitor network performance based on link utilization; this approach is reasonable for ﬁne-grained, local link DVFS control but does not scale to a full-chip shared resource.AMAT = HitRate(private) × Latency(private)+ (1 − HitRate(private)) × Latency(uncore) (2) Ideally for global decisions, one would like continuous monitoring of uncore status, whether per-packet statistics towhere  HitRate(private)  is  the  aggregate  hit  rate  on theprivate caches (L1 and L2) and Latency(private) is the average access time on those caches. Latency(uncore) is the average access time to the shared LLC (ie. access time to the L3 slice on a remote tile), plus the latency of the memory controller if the required cache block  is  missing  in  LLC. For simplicity we assume Latency(uncore) linear with the calculate APHL, or per-core memory statistics to calculate average AMAT. Such complete, active monitoring entails a calculation overhead at every tile, which includes counting starting/arrival time of packets, calculating the average, etc. Moreover, the status obtained at each tile must be regularly sent to the central power controller to set DVFS policy, consequently causing increased trafﬁc and congestion.To avoid increasing trafﬁc and congestion within the pri- mary NoC we propose to leverage unused space in  the header ﬂits of existing packets to “piggyback” network status information. This approach has the advantage of being scalable to larger networks because no extra networks or links must be designed. Furthermore, it is passive, ie. it does not perturb the network with extra status packets. A potential disadvantage to this approach is the non-determinism of status message delivery. As we will show, however, this does not signiﬁcantly degrade the accuracy of this approach.Prior Work in NoC and CMP DVFSSeveral groups have explored DVFS in NoCs and/or CMPs. Shang et al. wrote a pioneering work in the use of DVFS     for NoCs [20]. They perform DVFS for individual links in NoC. DVFS has also been studied for individual routers [14]. Son et al. explored DVFS for speciﬁc application NoCs [23]. In other previous works [15, 7, 17], the voltage/frequency domains within the NoC are assumed to be associated with individual cores processor cores. In each of these works, the DVFS policy is determined by local information. DVFS is also widely studied for processor cores or CMPs. A simple ap- proach is rule-based DVFS [8] that changes voltage/frequency level when monitored performance crosses certain threshold. Rule-based method is improved by including hysteresis [19]. Control theoretic techniques are proposed in [25, 15]. Jung and Pedram proposed a learning-based approach [9].In this paper, we address DVFS for the uncore as a whole. We will demonstrate that there is a large opportunity for uncore power saving with an accurate but low cost monitoring technique to fetch network information, meanwhile using a simple but effective control algorithm to adjust uncore voltage and frequency based on the monitoring result.AMAT-Based DVFS Policy DescriptionWe propose to estimate current AMAT to provide feedback to the DVFS controller. Each tile keeps track of its status information and a monitor collects the information for AMAT computation. To minimize hardware overhead, we use the existing NoC infrastructure to convey the information, to avoid increasing trafﬁc and congestion, we leverage unused space in the header ﬂits and employ passive monitoring rather than any active system. We propose a single monitor for our 4 4 network; experimental results show this is sufﬁcient for this size network. We assume the uncore contains a Power Control Unit (PCU), which is a dedicated small processor for chip power management as in Intel’s Nehalem architecture [11].Data Collection increasing trafﬁc and congestion, each tile “piggybacks” its status information into every packet injected into the network. Under the assumption of 128-bit wide links, and 64-byte cache blocks, we ﬁnd there are 64 unused  bits  in  the header ﬂit or single ﬂit packets. We leverage these unused bits to encode the status information as shown in Figure 3. Here, Time Stamp denotes the time the packet is generated. HitRate(L1) and HitRate(L2) show the hit rate of L1 and L2 caches during the time window. No.of UncoreRequest and Sum(RL) correspond to the number of L2 requests sent into the Uncore and the sum request latency during the time window, respectively. The Sum(RL) is the accumulated time span required for L2 requests into the uncore, and includes the access time of the L3, cache coherence resolution time and main memory access time on L3 misses. The additional hardware cost to maintain this status information is discussed in Section III-D.Fig. 4: NoC layout; the monitor resides at tile 6   Figure 4 shows an NoC layout illustrating our monitoringtechnique. Instead of adding monitors to each tile, we employ a single monitor that collects the data from the whole network. We chose tile 6 as the location of the monitor intuitively because a central location provides the best vantage point to passively collect the desired statistics (an assumption experi- mentally veriﬁed). In addition, the monitor tile should be near the PCU so that the overhead of interconnect between the monitor tile and the PCU is negligible. The monitor grabs status information from all passing packets (e.g. a packet traveling from tile 4 to tile 7) or packets bound to tile 6 (e.g.  a packet from tile 13 to tile 6). Data collection is passive,      a potential downside of this approach is non-determinism of message delivery, as we will discuss in the next section.Overall AMAT ComputationOnce a packet arrives at the monitor, the source node’s status information, encoded in the packet’s header, is handed over to the PCU (Power Control Unit). The PCU is in charge of8bits      8bits	32bits 18bits 16bits 7bits 7bits 12bits 20bits computing AMAT on the packet’s arrival. The AMAT for each tile is computed as per Equation 3 based on the information gathered.2bits 1bit (Flow control bits: Flit Type, Virtual Channel) AMAT  = HitRate(L1) × AccessT ime(L1)+ (1 − HitRate(L1)) × Latency(L2) Latency(L2) = HitRate(L2) × AccessT ime(L2)+ (1 − HitRate(L2)) × Latency(uncore) Latency(uncore)= (Sum(RL))/(No. of Uncore Request) (3)Fig. 3: Header ﬂit bit ﬁelds.In our proposed technique, each tile maintains statistics on its private, lower-level cache’s behavior necessary to compute AMAT. These statistics must be sent to the a central, mon- itoring node for overall AMAT estimation and DVFS policy generation. Rather than injecting more packets into the system, Note that Eq. 3 is  a  more  detailed  version  of  Eq.  2, where HitRate(private) and Latency(private) are decom- posed into their constituent L1 and L2 components. The AccessT ime values are constants for a given L1 and L2 cache design and hence need not be  sent  in  the  header.  Also note, the Sum(RL) the sum of round trip time for Uncore requests, including NoC latency, L3 access time, cachecoherency latency, as well as main memory latency when L3 misses occur.Our DVFS policy requires the overall, chip-wide AMAT as an input. This overall AMAT is computed for every time window of 50000 cycles. To calculate overall AMAT, we use a weighted average of the per-node AMAT, with respect to the number of memory instructions issued by that tile during the window. Upon packet arrival or traversal of the monitor tile, the PCU calculates the AMAT for the source tile of the packet and stores it in a table with 16 entries; one for each tile. Memory instruction count is also stored in the table. At the end of each time window, the PCU, using the memory instruction counts as weights, computes the weighted average of AMAT across all tiles as the overall AMAT. At the end of every time window, all entries are reset to 0; entries not updated are excluded from overall AMAT computation. Figure 5 depicts this process. The graph shows the AMATs of T ilei and T ilej, in a two tile system. The cross marks on the time line denote packet arrivals, and the circles are the AMAT for the corresponding tile to be stored in the table. The numbers above the circles denote the memory instruction count. The triangles are the ﬁnal AMAT values used in overall AMAT calculation. In “Window 1”, T ilei sends two packets, but the data from the ﬁrst packet is discarded as the second one overrides it. At the end of this window, the PCU ﬁrst calculates the per- tile  AMAT  for  T ilei  and  T ilej  of  80  and  65  respectively from the status information in the ﬁnal packet from each. It then determines there are 120 memory instructions in T ilei and  60  in  T ilej,  and  these  values  are  used  in  the  weighted average overall AMAT thus (80 120 + 65 60)/(120 + 60). In “Window 2”, the PCU receives no packet from T ilei thus the overall AMAT is computed only from AMATj and it is120. The system clock is also reset at the end of time window, thus if a packet’s time stamp is later than the current system time, the packet is discarded as it is from the previous window. Thus, we introduce a method of linear extrapolation to correct this bias. We assume that the number of memory instructions linearly increases in a time window. With the fact that the memory instruction count is 0 at the beginning of a time window,  we can estimate the count in the end of it using       a sampled count at any location of the window. We use the “Time Stamp” in the packet to deﬁne the relative location within the time window, and that needs to be stored in the table. By this “extrapolation”, in “Window 3”, the effective memory instruction count of T ilei becomes 400 while that of T ilej  becomes  133.  Finally,  the  overall  AMAT  becomes  45 while in “Na¨ıve” it is 40.Figure 6 compares the performances of the “Na¨ıve” and “Extrapolation” methods. The ﬁgure shows the correlation between actual and computed overall AMATs for monitor tiles, tile0-tile15 (tiles numbered as shown in Figure 4). Generally, higher correlations across all tiles indicates a given method provides a better estimate of overall AMAT. For Canneal we see that “extrapolation” generally results in a more accurate overall AMAT than “Na¨ıve.” For Vips, much higher correlation is achieved by “extrapolation” method. Vips’s trafﬁc pattern is more highly skewed and hence requires “extrapolation” to produce reasonable results. Note that in both cases tile6 shows the highest correlation among the tiles, thus we select tile6   as our monitor tile which also matches our assumption that central location provides better visibility.10.90.80.70.60.50.40.30.£0.10Canneal10.90.80.70.60.50.40.30.£0.10	Fig. 5: Overall AMAT ComputationWe refer the above method as “Na¨ıve” as it calculates AMAT without accounting for packet arrival time. Unfortu- nately, this passive monitoring method does not guarantee that each tile’s statistics are current. As the ﬁnal value is evaluated at the end of each time window, it is better to have a packet from a tile near the end of the window. For example, at “Window 3” in Figure 5, AMATi and AMATj have the same weight as the numbers of memory instructions carried over are the same. However, in the end of the time window, it is very likely that T ilei eventually has more memory instructions than T ilej  since the count had been determined much earlier. VipsFig. 6: Overall AMAT from each tile’s perspectivePID-Based DVFS Policy and Stability AnalysisWe  choose to implement a DVFS control scheme based   on PID (Proportional-Integral-Derivative) control. Compared to rule-based approaches [8, 19], PID control can easily  adapt to various application scenarios. Its computation cost   is signiﬁcantly less than learning-based methods [9]. It has been applied for DVFS in processor cores [25]. Moreover, it has theoretic grounds for stability analysis.The block diagram of PID control system is shown inAMAT_ ref   frequency TABLE I: Simulation setupFig. 7: PID system diagram.Figure 7. The controller takes two inputs: the reference AMAT and monitored AMAT. The reference AMAT is the control target and can be obtained from empirical data. The control output is updated once per control interval (same as time window for AMAT monitoring). The difference between the two  inputs  is  the  error  function  ej  = AMATref     AMATj where AMATj is the AMAT observed at control interval j.The controller calculates the control output u according touj = uj−1  + KI · ej + KP · (ej − ej−1)	(4) where KI and KP are constant coefﬁcients. Note we only implement the Proportional (P) and Integral (I) terms in ourcontroller, as this is simpler and often more robust than includ- ing the Derivative (D) term [25]. Control output u is converted to a V/F setting for the uncore system. In general, AMAT is η)/(1+KP η +KI η) is within the unit circle. To satisfy this condition, we simply need to ﬁnd PID coefﬁcient KP , KI > 0.When ρL1,miss ρL2,miss = 0 (where  ρ means  rate),  AMAT is not affected by f and the above closed-loop based analysis does not hold. However,  the chance of ρL1,miss ρL2,miss = 0  in practice is very small. Even when it happens, the monitored AMAT is very low and the controlled frequency gradually decreases to its minimum. Therefore, the system is still stable.Implementation OverheadThe DVFS controller needs a modest amount of hardware support. At each tile, one counter is needed to frame thea nonlinear function with respect to uncore frequency f . Weperform a transformation of u = 1/f such that AMAT is approximately a linear function of u.302520f5 f0 500.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	fUncore frequencyFig. 8: AMAT versus uncore frequency.In order to analyze the stability of this control system, we need to obtain an analytical form of the system function. Figure 8 shows simulation results of AMAT versus uncore frequency (f ). Then, by curve ﬁtting, we can obtain an approximated expressionηAMAT =	+ β	(5)fwhere η and β are two ﬁtting coefﬁcients. Although the values of η and β are speciﬁc for each application, the subsequent analysis is general, as long as the AMAT (f ) relation conforms to Equation (5). We performed many other simulations and all results follow similar trend as Figure 8.By performing z-transform, we can get system transfer function as control interval, in our case 50000 cycles, so 16 bits is sufﬁcient. To track each tile’s memory operation status the following additional registers are required by each tile: One 20-bit register for L1 hit count, one 12-bit register for L2     hit count, one 12-bit register is required to count the number of L2 misses and one 20-bit register to sum up all of the L2 miss latencies. These registers are updated at the completion of memory instructions, and reset as the time windows end. These registers are used to compute HitRate(L1) and HitRate(L2) prior to encoding in the header ﬂit, this latency is hidden      by the packet generation delay. Both AMAT computation and the PID algorithm are composed of a few simple arithmetic calculations, which can be easily handled by the PCU. The PCU has sufﬁcient storage to accommodate the data collected. We assume the PCU is co-located with the monitor tile, if this is not the case the monitor will need additional registers for the temporary storage of collected data prior to sending it to the PCU. Therefore, the overall hardware overhead is 80 bits per tile plus a possible 64 bits at the monitor tile.EvaluationIn this section we ﬁrst discuss our methodology and then compare the performance of our proposed technique and several variation versus baseline.Experiment SetupThe testbed architecture in our experiment is a  16-tile CMP as shown  in  Figure  1(b).  Each  tile  is  composed  of a processing core with 2-levels of private cache, a network interface (NI) and a partition of the shared L3 cache (LLC). Ta- ble I summarizes our experimental conﬁgurations and param- eters. We use M5 full system simulator to generate PARSECAMAT (z) = 	(KP + KI ) · η · z − KP · η	(1 + KP · η + KI · η) · z − (1 + KP · η)The characteristic equation for this system is (6) shared-memory multi-processor benchmark memory system traces [2]. Each trace contains up to 250 million memory oper- ations. These traces are run through a memory hierarchy (L1-(1 + KP · η + KI · η) · z − (1 + KP · η) = 0	(7)According to control theory [3], the system is stable if and only if the root of the above equation is inside the unit circle of the z-plane. In our case, this requires that z = (1 + KP · L2-LLC+directory) and network simulator based upon  Ocin-tsim [16]. Although trace-driven, open-loop, NoC simulation can introduce error, we expect that for the small changes in AMAT experienced, these errors are minimal. Uncore DVFS is emulated by varying packet injection rate (e.g. slowing  downuncore frequency by a half emulates doubling the injection rate). Please note that uncore DVFS does not affect off-chip memory access time. We assume that memory access time is constant with respect to core frequency. We explored several options for the DVFS control interval between 10000 clock cycles to 100000 clock cycles. In this work present results assuming an interval of 50000 clocks cycles as it provides more than sufﬁcient time to collect trafﬁc information as well as being short enough to capture ﬁne-grain program phase behavior.In this work we focus on the dynamic energy reduction due to DVFS based off impact on the dynamic power equation (Eq. 1), because dynamic power currently dominates in mod- ern process technologies. DVFS should also provide beneﬁt for static energy consumption though the absolute amount is highly dependent on particular process technology, we plan to explore this in future work.Power and Performance ComparisonsFigure 9 compares the dynamic energy savings and per- formance degradation of our proposed approach (labeled Est. AMAT+PID) versus baseline without DVFS, along with four variations which we discuss in the following  subsections. The ﬁgure shows our PID technique, informed by estimated AMAT provides an average savings of 33% while reducing the performance of the uncore (measured by impact on absolute AMAT) by 5.3%. We empirically determined that AMAT increases of 5% tend to decrease processor performance (IPC) by < 2.5%. Blackscholes shows the best beneﬁt,  with  a  73% reduction in energy, while Canneal shows the least reduction in energy at 1%. Both benchmarks show negligible performance impact. In Blackscholes the uncore is not critical to performance, because the L1 and L2 hit rates are high, so voltage and frequency (V/F) can be dropped without impacting performance. Canneal, however, has a relatively lower L1 and L2 hit rate and thus the uncore’s performance is more critical to application performance. In both cases our algorithm preserves our performance goal of <5% AMAT loss.PID-Based vs. Rule-Based DVFS: Figure 9 also shows the results for a simple, na¨ıve, rule based approach for DVFS policy (labeled Rule-Based). In this approach, V/Fs are associ- ated with speciﬁc ranges of AMAT (e.g. if monitored AMAT is a then uncore frequency level is set to b). The advantage  to this technique would be that it eschews the overheads of the PID controller, however, its static nature ignores time- varying dynamics of the system. While rule-based DVFS obtains similar energy reduction as PID-based DVFS, it is unable to adapt as well and performance decreases by 10% compared to the 5% by PID-based DVFS.Monitoring AMAT vs. APHL: APHL (Average Per-Hop Latency) is a simple, direct measure of network performance discussed in Section II-C. Figure 9 also shows results for PID informed by the APHL metric (labeled APHL+PID).   In some cases, like Blackscholes, the power savings from APHL-based DVFS is much less than that of AMAT-based.  In such applications, the packet injection  rate  is  not  high but constantly above zero-load. Thus, the network is often moderately busy and APHL-based DVFS will not lower V/F level. On the other hand, the L1 and L2 miss rates in these cases are low and therefore moderate increase of APHL does not have signiﬁcant impact to system performance. AMAT- based DVFS is able to capture such opportunities for energy savings. In other cases, such as Canneal, APHL-based DVFS is overly aggressive, causing > 50% performance degradation. This phenomenon is due to the static APHL target of three uncore cycles. When trafﬁc load is not high, lowering uncore frequency may still maintain an APHL close to 3 uncore cycles, however, the network latency in core cycles increases due to the frequency ratio change. This dramatically affects performance when L1 or L2 miss rates are high. Monitoring APHL in core cycles, however, is impractical because of the difﬁculty in ﬁnding appropriate PID reference levels without knowing the dynamically changing network latency relative to system performance. Overall, APHL-based DVFS achieves about the same power savings of about 30% as AMAT-  based DVFS, but degradation performance signiﬁcantly more at about 16%.Sampled AMAT vs. Perfect Knowledge of AMAT: The AMAT  monitoring technique we propose uses a single tile to collect information from its  own  packets  and  passing- by trafﬁc. Compared to monitoring all tiles for complete knowledge of AMAT, our technique has much lower overhead, however, this comes at the potential cost of some inaccuracy in AMAT estimation due to incomplete knowledge. Figure 9 also shows results for PID DVFS based upon perfect knowledge of the system AMAT each time window (labeled Perfect AMAT+PID). The ﬁgure shows estimating AMAT produces results quite close to perfect knowledge. Overall, the difference in energy savings and performance is < 1%.Power-Performance Tradeoff: The tradeoff between power savings and AMAT degradation can be easily adjusted by tuning the AMATref to the PID system (see Figure 7). In Est. AMAT+PID we set AMATref to 2, empirically determined to reach our goal of < 5% performance degradation. Figure 9 shows results for increasing AMATref to 4.2 to achieve a more aggressive power savings (labeled Ag. Est. AMAT+PID). As expected, the aggressive PID yields increased energy savings of 44% as well as a greater AMAT degradation of 13%. This comparison conﬁrms that the power-performance tradeoff can be easily managed by our approach. In future research, we will develop techniques that make AMATref adaptable to different applications at runtime.AnalysisHere we provide some simulation details to aid developing an intuition on the behavior of our approach. In Figure 10,  we show a snapshot of the uncore frequency over time of our Est. AMAT+PID system versus an “Ideal” DVFS policy. The Ideal policy is an unrealistic case where every benchmark is simulated once for each V/F setting and the lowest V/F are chosen for each time window which meet the performance goal of < 5% performance loss. Generally Est. AMAT+PID follows the Ideal policy very closely. Where variance occurs, the estimated method is generally more conservative. Initially, both frequencies are high due to high cache miss rate during initialization. After two millions of clock cycles, the lower level caches are ﬁlled and the frequency is lowered, reﬂecting the lower performance criticality of the uncore. There are some frequency spikes arising from occasional trafﬁc spikes due to application phase changes. Surprisingly, the Est. AMAT+PID policy is able to track the performance needs of the uncore quite well even during these spikes.Conclusions and Future WorkIn this work, we propose a DVFS policy for a CMP’s un- core. This policy leverages an uncore performance monitoring based upon AMAT estimation. This technique is integrated with a PID-based DVFS control system. In simulation onEst. AMAT+PID   Rule−Based    APHL+PID    Perfect AMAT+PID    Ag. Est. AMAT+PID f0.90.80.70.60.50.40.30.20.f 0 Est. AMAT+PID      Rule−Based      APHL+PID      Perfect AMAT+PID       Ag. Est. AMAT+PID 60%50%40%30%20%f0% 0%Normalized dynamic energy consumption Performance degradationFig. 9: Energy and performance impact for PARSEC benchmarks. Our proposed method is “Est. AMAT + PID”.1.110.90.80.70.60.50.40.30.2 0	2	4	6	8	10	12clock cycles	6 L. Guang, E. Nigussie, L. Koskinen, and H. Tenhunen, “Autonomous DVFS on supply islands for energy-constrained NoC communication,” Lecture Notes in Computer Science: Architecture of Computing Systems, vol. 5455/2009, pp. 183–194, 2009.A. Iyer and D. Marculescu, “Power efﬁciency of voltage scaling in multiple clock multiple voltage cores,” in ICCAD, 2002, pp. 379–386.H.-S. Jung and M. Pedram, “Supervised learning based power manage- ment for multicore processors,” TCAD, vol. 29, no. 9, pp. 1395–1408, Sep. 2010.J. Kim, D. Park, T. Theocharides, N. Vijaykrishnan, and C. R. Das, “A Low Latency Router Supporting Adaptivity for On-Chip Interconnects,” in DAC, 2005.R. Kumar and G. Hinton, “A family of 45nm IA processors,” in ISSCC, 2009, pp. 58–59.S. Ma, N. Enright Jerger, and Z. Wang, “Dbar: an efﬁcient routing algorithm to support multiple concurrent applications in networks-on- chip,” in ISCA, 2011, pp. 413–424.x 10Fig. 10: Simulation snapshot of Blackscholes showing DVFS frequency over time, Est. AMAT+PID vs. Ideal.PARSEC benchmarks, the proposed approach achieves 33% NoC and LLC dynamic power reduction, with only 5% degra- dation on AMAT, correlated to a 2.5% system performance drop. We show that the technique has low computation and communication overheads and is practical to implement.In future research, we will validate this approach on multi- application benchmarks and architectures with signiﬁcantly more cores. While we believe that future VLSI technology will allow rapid DVFS changes, such as analyzed here, in   the future we will explore how latency in Voltage/Frequency changes could impact our results.AcknowledgmentsWe would like to thank Pritha Ghoshal, Mark Browning and David Kadjo for their helpful discussions. This research is supported by a gift from Intel Corp."HARAQ - Congestion-Aware Learning Model for Highly Adaptive Routing Algorithm in On-Chip Networks.,"The occurrence of congestion in on-chip networks can severely degrade the performance due to increased message latency. In mesh topology, minimal methods can propagate messages over two directions at each switch. When shortest paths are congested, sending more messages through them can deteriorate the congestion condition considerably. In this paper, we present an adaptive routing algorithm for on-chip networks that provide a wide range of alternative paths between each pair of source and destination switches. Initially, the algorithm determines all permitted turns in the network including 180-degree turns on a single channel without creating cycles. The implementation of the algorithm provides the best usage of all allowable turns to route messages more adaptively in the network. On top of that, for selecting a less congested path, an optimized and scalable learning method is utilized. The learning method is based on local and global congestion information and can estimate the latency from each output channel to the destination region.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip HARAQ: Congestion-Aware Learning Model for Highly  Adaptive Routing Algorithm in On-Chip Networks  Masoumeh Ebrahimi1, Masoud Daneshtalab1, Fahimeh Farahnakian1,   Juha Plosila1, Pasi Liljeberg1, Maurizio Palesi2, Hannu Tenhunen1   1University of Turku, Finland, 2University of Kore, Italy  Abstract— the occurrence of congestion in on-chip networks can  severely degrade the performance due to increased message  latency. In mesh topology, minimal methods can propagate  messages over two directions at each switch. When shortest paths  are congested, sending more messages through them can  deteriorate the congestion condition considerably. In this paper,  we present an adaptive routing algorithm for on-chip networks  that provide a wide range of alternative paths between each pair  of source and destination switches. Initially, the algorithm  determines all permitted turns in the network including 180degree turns on a single channel without creating cycles. The  implementation of the algorithm provides the best usage of all  allowable turns to route messages more adaptively in the  network. On top of that, for selecting a less congested path, an  optimized and scalable learning method is utilized. The learning  method is based on local and global congestion information and  can estimate the latency from each output channel to the  destination region.  INTRODUCTION  I.  Networks-on-Chip (NoC) has emerged as a solution to  address the communication demands of future multicore  architectures due to its reusability, scalability, and parallelism  in communication infrastructure [1]. The performance and  efficiency of NoCs largely depend on the underlying routing  model which establishes a connection between input and output  channels in a switch.   In minimal adaptive routing algorithms, shortest paths are  used for transmitting messages between switches. In low traffic  loads, minimal methods can achieve optimized performance,  while they are very inefficient in avoiding hotspots when the  network load increases. The reason is that they can deliver  messages through at most two minimal directions and thereby  they cannot reroute messages around congested areas. The  routing policy (output selection) of minimal methods can be  based on local, non-local, or mix of local and non-local  congestion of  the network. However, minimal routing  algorithms suffer from a low degree of adaptiveness, which are  inefficient in distributing the traffic over the network even if  they have accurate knowledge of the network condition.   In wormhole routings, messages are divided into small flits  traveling through the network in a pipelined fashion. This  approach eliminates the need to allocate large buffers in  intermediate switches along the path. However, a message  waiting to be allocated to an outgoing channel may prohibit  other messages from using the channels and buffers and  thereby wasting channel bandwidth and increasing latency.  Adding virtual channels can alleviate this problem, but it is an  expensive solution. Non-minimal methods can partially  overcome this blocking problem and reduce the waiting time of  messages by delivering them via alternative paths. In contrast,  performance can severely deteriorate in non-minimal methods  due to the uncertainty in finding an optimal path as they may  choose longer paths and meanwhile delivering messages  through congested regions. Moreover, non-minimal methods  can suggest minimal and non-minimal paths between a source  and destination but this flexibility is at the cost of a more  complex switch structure or additional virtual channels. On the  other hand, an output selection should choose a single channel  from a set of predetermined channels to forward a message to  the next hop. This becomes one of the main challenges  involved in designing an efficient non-minimal method to  select a less congested path from a set of alternative paths. The  decision for an output channel should not be based on local  information as it may route messages through paths which are  not only longer but also highly congested. On the other hand,  even if a global knowledge of the network is provided, due to a  large number of alternative paths, finding a less congested path  is questionable which demands an intelligent method to cope  with.   Reinforcement learning has attracted considerable attention  in industry and academia because it provides an effective  model for problems where optimal solutions are analytically  unavailable or difficult to obtain. The learning methodology is  based on the common-sense idea that if an action is followed  by a satisfactory state, or by an improvement, then the  tendency to produce that action is strengthened, i.e., reinforced.  On the other hand, if the state becomes unsatisfactory, then that  particular action should be suitably punished [2][3]. QLearning [4] is one of the algorithms in the reinforcement  learning family of machine learning. In the Q-Learning  approach, the learning agent first learns a model of the  environment on-line and then utilizes this knowledge to find an  effective control policy for the given task. Q-Routing [5] is a  network routing method based on Q-Learning models which  learn a routing policy to minimize the delivery time of  messages to reach their destinations [6]. Q-Routing methods  are implemented by having each switch maintains a table of Qvalues, where each value is an estimate of how long it takes for  a message to be delivered to a destination, if sends via a  neighboring switch [4]. The method requires a switch to update  its routing table whenever a message is delivered to the next  switch and the congestion information is returned. Q-Routing  methods allow a network to be continuously adopted to load  changes. Although these schemes make the most optimal  routing decisions, due to employing large tables, they are not  cost-efficient approaches for Networks-on-Chip.   In this paper, we proposed a novel routing algorithm,  named Highly Adaptive Routing Algorithm using Q-Learning  (HARAQ) where the main contributions of the paper are  summarized as follow:   1. A low-restrictive non-minimal algorithm to provide several  alternative paths between each pair of source and destination  switches.   The algorithm uses only an extra virtual channel in the Y  dimension and provides a large number of paths for routing  messages. Different turns are defined on each virtual channel,  978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.10 19 such that the prohibited turns in one virtual channel are  permitted in the other one. Another subtle point of the  presented method is its ability to enable 180-degree turns on a  single channel (i.e. a message can arrive through a channel that  is previously used to deliver it) without creating cycles.   2. An efficient output selection strategy for finding a lowlatency path from a source to a destination.   The presented routing scheme utilizes an optimized QLearning model for the output selection function. In this way,  the output selection can efficiently estimate the latency of a  message to reach its destination through each of the possible  output channels. This information is extracted from a Q-Table  available at each switch. Unlike typical Q-Routing methods,  our proposed model is scalable and the size of Q-Tables is  relatively small.   The rest of this paper is organized as follows: Section II  reviews the related work. The minimal mad-y method and the  proposed highly adaptive routing algorithm along with the  presented optimal Q-Routing model are explained in Section  III. The results are given in Section IV while we summarize  and conclude in the last section.  II. RELATED WORK  Most common  implementations of minimal  routing  algorithms, e.g. FRA [7], NoP [8], RCA [9], and DBAR [10],  have focused on collecting local or non-local congestion  information to get an estimation of the congested areas in the  network. However, due to a low degree of adaptiveness (i.e. at  most two directions per switch in fully adaptive minimal  methods), minimal routing algorithms cannot distribute the  traffic over the network efficiently even if they have accurate  knowledge of the network condition. Virtual channels can be  used to avoid deadlock and increase adaptiveness. DyXY [11]  and mad-y [12] are the methods using only two virtual  channels along one of the two physical channels. Although  they are fully adaptive in minimal paths, messages are limited  in terms of routing options and thus the traffic load cannot be  efficiently balanced over the network. Generally, non-minimal  routing schemes have been proposed for tolerating faults rather  than avoiding congestion [13][14][15]. Most of these models  are proposed to support special cases of faults, such as onefaulty switches, convex or concave regions, which require a  large number of virtual channels to avoid deadlock. In fact,  adding virtual channels is expensive because of additional  buffers and complex control logics. There are other faulttolerant approaches [16][17][18] which do not require any  virtual channels. However, these algorithms are partially  adaptive and very limited in supporting faults. A highly  resilient routing algorithm is proposed in [19]  to tolerate a large  number of faults without exploiting any virtual channel, but  only one path can be selected between each pair of source and  destination nodes. In general, each method defines a new  tradeoff between the number of virtual channels, the ability to  handle different fault models, and the degree of adaptiveness.   Fault-tolerant methods cannot be efficiently employed as  non-minimal models to alleviate congestion in NoCs for  several reasons. If congestion occurs in multiple disjoint  concave/convex regions at the same time, the fault-tolerant  models with a small number of virtual channels are either  deterministic or unable to handle different hotspot (fault)  models while using a large number of virtual channels is not  cost efficient for on-chip networks. In addition fault-tolerant  algorithms are relatively complex due to considering different  fault models in order to find a path between a source and  destination. The complexity added to the algorithms is  unnecessary for congestion-aware methods because congestion  does not disconnect a path and in some cases highly congested  paths might be selected. Most of the proposed fault-tolerant  algorithms support only static faults and a few of those take  dynamic faults  into consideration. If hotspots changes  dynamically, the algorithms should be reconfigured for new  hotspots and handle a transition phase whenever a location of  hotspot changes. To overcome  this, some fault-tolerant  algorithms allow dropping messages in the transition phase and  other approaches use relatively complex models which are not  appropriate solutions for on-chip networks. In sum, the goal of  our approach is to present a low-restrictive method using both  minimal and non-minimal paths. The method utilizes only one  virtual channel along the Y dimension while does not have the  above mentioned restrictions of fault-tolerant models.  Q-Routing based models have been studied in several  literatures such as [20] and [21], but they have rarely been  investigated in the context of on-chip networks. The algorithm  in [6] is proposed to handle communication among modules  which are dynamically placed on a reconfigurable NoCs. This  algorithm is inspired by the method in [21] for a general case  of message switching networks. FTDR-H [22] utilizes QRouting methods to tolerate faults and find a path between each  pair of switches as long as a path exists. Moreover, the size of  Q-Tables is reduced by taking advantages of the clustering  model. This clustering model is also extensively discussed in  the C-Routing method [23].  III. HIGHLY ADAPTIVE ROUTING ALGORITHM USING QLEARNING MODEL  The proposed mesh-based routing scheme is based on the  mad-y method which has been introduced by Glass and Ni in  [12]. The mad-y and the presented routing methods utilize a  double-Y network where the X and Y dimensions have one and  two virtual channels, respectively (Fig. 1(a)). Each switch in  the double-Y network has seven pairs of channels, i.e. East(E),  West(W), North-vc1(N1), North-vc2(N2), South-vc1(S1),  South-vc2(S2), and Local(L). In 2D mesh-based network, three  types of turns can be taken: 0-degree, 90-degree, and 180degree turns (U turns) [24]. By taking a 0-degree turn, a  message transmits in a same direction with a possibility of  switching between virtual channels. The turn is called 0degree-ch if in a turn neither the direction nor the virtual  channel changes (Fig. 1(b)) while it is a 0-degree-vc turn if the  virtual channel changes (Fig. 1(c)). By taking a 90-degree turn,  a message transmits between the switches in perpendicular  directions (Fig. 1(d)). By taking a 180-degree turn, a message  is transferred to a channel in the opposite direction [24]. If the  virtual channel is changed, the turn is called 180-degree-vc  (Fig. 1(e)); otherwise, it is represented as 180-degree-ch (Fig.  1(f)). Note that, in all figures, the vc1 and vc2 are differentiated  by – and = respectively.    Fig. 1. (a) A switch in a double-Y network (b) 0-degree-vc (c) 0-degree-ch (d)  90-degree (e) 180-degree-vc (f) 180-degree-ch  20   A. The mad-y Method  In order to avoid deadlock, the mad-y method [12],  prohibits several turns in the double-Y network. In mad-y, the  0-degree-vc turns from vc1 to vc2 are permitted, also all 0degree-ch turns are allowable, however 0-degree-vc turns from  vc2 to vc1 may cause deadlock in the network and are  prohibited (Fig. 2(c) and Fig. 2(d)). As illustrated in Fig. 2(a)  and Fig. 2(b), out of sixteen 90-degree turns that can be  potentially taken in a network, four of them cannot be taken in  mad-y. Finally, 180-degree turns are not allowed in mad-y. To  prove the deadlock freeness in mad-y, a two-digit number (a, b)  is assigned to each output channel of a switch in n×m mesh  network. According to the numbering mechanism, a turn  connecting the input channel (aic, bic) to the output channel (aoc,  boc) is called an ascending turn when (aoc>aic) or ((aoc=aic) and  (boc>bic)). Fig. 3 shows how the channels of a switch at the  position (x,y) are numbered. Since this numbering mechanism  causes the messages to take the permitted turns in strictly  increasing order, so that mad-y is deadlock-free.   Fig. 2. 90-degree turns in (a) vc1 (b) vc2 (c) 0-degree-ch (d) 0-degree-vc  the mad-y and HARA methods combine two virtual channels  with different prohibited turns, they diminish the drawbacks of  turn models prohibiting certain turns at all locations. In  minimal routings, (e.g. mad-y), 180-degree turns are prohibited  but can be incorporated in non-minimal routings. One way to  incorporate 180-degree turns is to examine the turns one by one  to see whether the turn causes any cycle. After determining all  allowable turns, in order to prove deadlock freeness, the  numbering mechanism is utilized. At first we use the  numbering mechanism of the mad-y method to learn all 180degree turns that can be taken in ascending order, and then  modify the numbering mechanism to meet our requirements.  According to the numbering mechanism in Fig. 3, among 180-  degree-vc turns, those from vc1 to vc2 are taken in ascending  order (Fig. 4(a), Fig. 4(b)), so that it is safe to employ them in  the network. As all 180-degree-vc turns from vc2 to vc1 take  place in descending order, so they cannot be used in the  network (Fig. 4(c), Fig. 4(d)). Now, let us examine a 180degree-ch turn when a message uses it in vc1 of the north  direction (Fig. 4(e)). As shown in Fig. 3, the label on the output  vc1 of the north direction is (m-1-x,1+y) and the label on the  input vc1 of the north direction is (m-1-x,n-1-y). The turn takes  place in ascending order if and only if n-1-y is greater than  1+y. Therefore, this turn can be safely added to a set of  allowable turns if the y coordinate of a switch is less than (n2)/2. Similarly, in Fig. 4(f), 180-degree-ch turn on the vc2 of  the north direction is permitted if the y value of a switch is less  than (n-2)/2. 180-degree-ch turns on the vc1 and vc2 of the  south direction are permitted (Fig. 4(g) and Fig. 4(h)) if and  only if the y coordinate of a switch is greater than n/2. Finally,  180-degree-ch turn on the west direction is always permitted  (Fig. 4(i)) while 180-degree-ch turn on the east direction is  prohibited in the network (Fig. 4(j)).   Fig. 3. Channel numbering in the mad-y method  B. Highly Adaptive Routing Algorithm  As mad-y is a minimal adaptive routing method, it cannot  fully utilize the eligible turns to route messages through lesscongested areas. The aim of the proposed routing algorithm,  named Highly Adaptive Routing Algorithm (HARA), is to  enhance the capability of the existing virtual channels in mad-y  to reroute messages around congested areas and hotspots. Since Fig. 4. Allowable 180-degree turns in the HARA method  As shown in Fig. 4, there are four conditional 180-degree  turns. Two of those are allowable only in the northern part of  the network and two others in the southern part of the network.  This not only increases the complexity of the routing function  but also imposes heterogeneous routing function for switches.  To overcome this issue, we modify the numbering mechanism  such that two turns are permitted in the whole network (Fig.  4(g) and Fig. 4(h)) and two other are prohibited (Fig. 4(e) and  Fig. 4(f)). The numbering mechanism of HARA along with all  permitted turns in the network is shown in Fig. 5. As can be  observed from this figure, all allowable turns are taken in  ascending order.   Fig. 5. The numbering mechanism of HARA along with all eligible turns in the network  21       Table 1. Potential output channels according to the input channel (InCh) and relative position of source and destination switches (Pos)   Pos.  InCh  L  N1  N2  S1  S2  E  W  N  N1, N2, S1, W  N2, S1, W  -  N1, N2, S1, W  N2  N1, N2, S1, W  N2  S  N1, S1, S2, W  S1, S2, W  S2  N1, S1, S2, W  -  N1, S1, S2, W  S2  E  N1, N2, S1, S2, E, W  N2, S1, S2, E, W  S2, E  N1, N2, S1, S2, E, W  N2, E  N1, N2, S1, S2, E, W  N2, S2, E  W  N1, S1, W  S1, W  -  N1, S1, W  -  N1, S1, W  -  NE  N1, N2, S1, S2, E, W  N2, S1, S2, E, W  S2, E  N1, N2, S1, S2, E, W  N2, E  N1, N2, S1, S2, E, W  N2, S2, E  NW  N1, S1, W  S1, W  -  N1, S1, W  -  N1, S1, W  -  SE  N1, N2, S1, S2, E, W  N2, S1, S2, E, W  S2, E  N1, N2, S1, S2, E, W  N2, E  N1, N2, S1, S2, E, W  N2, S2, E  SW  N1, S1, W  S1, W  -  N1, S1, W  -  N1,S1, W  -  In the non-minimal routing, employing only eligible turns  at each switch is necessary but not sufficient to avoid blocking  in the network. The reason is that using the allowable turns  (Fig. 5), a message may not be able to find a path to the  destination from the next hop and is blocked. On the other  hand, one of the aims of HARA is to fully utilize all eligible  turns to present a low-restrictive adaptive method in the  double-Y network. To achieve the maximal adaptiveness along  with the blocking avoidance, for each combination of the input  channel and destination switch position, we examined all  eligible 0-degree, 90-degree, and 180-degree turns, separately.  The output channels are selected in a way that not only the turn  is allowable but also it is guaranteed that there is a path from  the next switch to the destination. When a message arrives  through one of the input channels, the routing unit determines  one or several potential output channels to deliver the message.  The routing decision is based on the relative position of the  current and destination switches (i.e. within one of the  following eight cases: North(N), South(S), East(E), West(W),  Northeast(NE),  Northwest(NW),  Southeast(SE),  and  Southwest(SW)). All permissible output channels of HARA,  for each pair of the input channel (Inch) and destination  position (Pos) are shown in Table 1. As can be obtained from  the table, HARA offers a large degree of adaptiveness to route  messages. One of the drawbacks of non-minimal methods is  the complexity of determining eligible output channels.  However, as shown in Fig. 6 (i.e. that is extracted from Table  1), the implementation of HARA is relatively simple.   InCh: Input Channel; OutCh: Output Channel  Pos: Destination Position  ----------------------------------------  IF Pos={L} THEN     OutCh(L)<=’1’;   IF Pos={E or NE or SE} THEN    OutCh(E)<=’1’;  IF InCh={L or N1 or S1 or E} THEN  OutCh(W)<=’1’;  IF InCh={L or S1 or E} THEN    OutCh(N1)<=’1’;  IF InCh={L or N1 or E or S1} THEN  OutCh(S1)<=’1’;   IF (InCh/= {N2}) AND      (Pos={N or E or NE or SE}) THEN OutCh(N2)<=’1’;  IF (InCh/={S2}) AND      (Pos={S or E or NE or SE}) THEN OutCh(S2)<=’1’;   Fig. 6. Determining all eligible output channels by HARA  Theorem 1:  HARA is deadlock-free.   Proof: If numbering mechanism ensures that all eligible turns  are ordered in ascending order (descending order), no cyclic  dependency can occur between channels. As can be observed  from Fig. 5, all connections between input channels and output  channels to form eligible turns in HARA take place in  ascending order and thus HARA is deadlock free.  Theorem 2:  HARA is livelock-free  Proof: In HARA, whenever a message transmits in the east  direction, it can never be routed back in the west direction.  Therefore, in the worst case, the message may reach to the  leftmost column and then starts moving in the east direction  toward the destination column. Therefore, after a limited  number of hops, the message reaches the destination, and  Theorem 2 is proved.  Fig. 7 shows an example of the HARA method in a 5×5  mesh network in which the source switch at (2,1) sends a  message to the destination switch at (4,2). According to Table  1, the message arriving from the local channel and delivering  toward the destination in the northeast position has six  alternative choices (i.e. N1, N2, S1, S2, E, and W); among  them, the output channels N1, N2, and E introduce the minimal  paths and S1, S2, and W indicate the non-minimal paths. Since  the neighboring switches in the minimal paths are in the  congested area, the message is sent in a non-minimal direction  that is not congested. Again, at the switch (2,0), all the minimal  paths are congested, so the message is sent to switch (1,0)  which is not congested. The same strategy is used until the  message reaches the destination switch. This example shows  the ability of the HARA method to reroute messages around  congested areas.  0,4 1,4 2,4 3,4 4,4 0,3 1,3 2,3 3,3 4,3 0,2 1,2 2,2 3,2 4,2 D 0,1 1,1 2,1 3,1 4,1 S 0,0 1,0 2,0 3,0 4,0 Fig. 7. An example of HARA  C. Q-Learning-based Output Selection Function  As mentioned earlier, we utilize an optimized Q-Routing  model for the selection function of HARA to estimate the  latency of sending a message from each output channel to the  destination switch. Since the output selection function of  HARA is inspired by the Q-Routing model, the proposed  routing method is named HARAQ (HARA using Q-Routing).  Let us explain the idea of HARAQ using the example of Fig. 8  where a message is generated at the source switch S for the  destination D. According to HARA, when a message arrives  from the local input channel and destined for a destination  switch in the northeast position, six output channels can be  selected to forward the message (i.e. N1, N2, S1, S2, E, and  W). At Fig. 8(a), suppose that the colored entry of the Q-Table  indicates the estimated latencies of sending a message from  each possible output channel to the northeast region. Since  output channel N1 has the lowest estimated latency, the  message is delivered through this channel. At the switch X, the  message is received by the input channel S1 (Fig. 8(b)). Using 22         the information in Table 1 or Fig. 6, multiple output channels  can be used to forward the message (i.e. N1, N2, S1, S2, E, and  W). Among eligible output channels, the output channel E has  the lowest latency, and thus it is selected for sending the  message to the switch Y. At this time, the local and global  congestion values should be returned to the switch S. The time  the message waited in the input buffer of the switch X before  transmission to the switch Y is counted as the local information  (i.e. BX=1). The minimum estimated latency of routing  messages from the switch X to the destination region via the  neighboring switch Y is considered as the global latency and is  extracted  from  the Q-Table of  the  switch X  (i.e.  minQX(D,Y)=4). The summed value of the local and global  information provides a new latency estimation of the path from  the switch S to the destination D. Finally, the corresponding  entry of the Q-Table at switch S (i.e. row: NE; column: N1)  should be updated with the new value. This is done taking the  average of the old and new latency estimation (Fig. 8(a)). At  switch Y, the message is received via the west input channel  (Fig. 8(c)). The output channel with the lowest latency is  selected among the three possible output channels (i.e. N2, S2,  and E). Upon connecting the input channel to the output  channel of the switch Y, local and global information are  returned to the switch X. The local congestion shows the  waiting time of the message at the input buffer of switch Y (i.e.  BY=3) while the global congestion indicates the estimated  latency from the switch Y to the destination switch D via the  neighboring switch Z (i.e. minQY(D,Z)=5). The sum of the  local and global values is new latency estimation from the  switch X to the destination switch. As shown in Fig. 8(b), the  corresponding entry of the Q-Table at the switch X is updated  taking an average of  the new estimated value  (i.e.  BY+minQY(D,Z)) and an existing estimation (QX(D,Y)).  Finally, the message arrives at the switch Z from the input  channel S2 (Fig. 8(d)). This message can reach the destination  by delivering it through N2 or E output channels. The output  channel E has the lowest value and selected for routing the  message. The local latency, the waiting period at the input buffer of the switch Z, is 3 while the global latency to the  destination is equal to 0 as the message reaches the destination  in the next hop. Similarly, the latency values are returned to  switch Y and update the Q-table (Fig. 8(c)). Hence, as  messages are propagated  inside  the network, Q-Tables  gradually incorporate more global information [20].  Q-Table format: Q-Routing models  learn  the network  condition at run time and based on the obtained information  sends a message through the path that has the lowest estimated  latency to the destination switch [6]. Generally, each switch  maintains a Q-Table to store the estimated latency of routing  messages to a destination switch via each output channel. Two  typical types of Q-Table, named Q-Routing and C-Routing  tables, are investigated in [23]. The size of a Q-Routing table is  n×m×k where n is the number of switches in the network, m is  the number of output channels per switch, and k is the size of  each entry in the Q-Table. The required area of Q-Routing  tables not only is very large but also increases as the network  size enlarges. C-Routing tables can decrease the size of tables  by taking advantages of the clustering approach. The size of CRouting tables is (l+c)×m×k consisting of two parts: the  cluster part having a size of c×m×k where c is the number of  clusters , and the local part having a size of l×m×k where l is  the number of switches within each cluster. The size of QTables can be reduced by using C-Routing tables, but this  model still suffers from the scalability issue since the size of CRouting tables can become rather high as the network scales  up. There are some other issues regarding the clustering model  such as determining the size of each cluster for different  network sizes or partitioning the network when the network  size is not a multiple of the cluster size. The Q-Table in our  model is named Region-based Routing (R-Routing); each row  of this table corresponds to one of the eight different positions  of the destination switch (i.e. N, S, E, W, NE, NW, SE, and  SW) and each column indicates output channels (i.e. N1, N2,  Fig. 8. The process of updating the Q-Tables  23   S1, S2, E, and W). Regardless of the network size, the size of  R-Routing tables is 8×6×k that is considerably smaller than CRouting and Q-Routing tables. The required size for each  approach is given in Table 2 where l=4 and k=4. Note that the  reported areas for Q-Routing and C-Routing tables are based  on using no virtual channel in the network while R-Routing  tables are based on utilizing an extra virtual channel. While our  approach reduces the size of Q-Tables such that they can be  applicable in NoCs, someone might think that the accuracy of  the estimated latency toward each destination diminishes by  our model. In fact, under real traffic conditions, each entry of  the R-Routing table is inherently influenced by the switches  which are in more communication with them at that period.  Therefore, it is not necessary to allocate a row for each specific  switch in the network. Moreover, R-Routing tables are updated  more occasionally than C-Routing and Q-Routing tables since  messages designated for the same regions can be used to  update R-Routing tables while in two other models each entry  is updated only by the messages for the same destination.  Table 2. the area overhead  Size\method Q-Routing C-Routing R-Routing  8×8  16×16  32×32  128 bytes  512 bytes  2048 bytes  40 bytes  64 bytes  160 bytes  24 bytes  24 bytes  24 bytes  Transferring Local and Global Information: Q-Routing  models are explored in on-chip networks but it is not obvious  how the congestion is defined for local and global information  and how many bits are considered for them. The congestion  statuses are delivered over the channel whenever a message is  transferred between two neighboring switches, which reduce  the power consumption. In this work, a 4-bit congestion wire is  used between each two neighboring switches to propagate local  and global congestion information. The local congestion  information is a 2-bit value indicating the waiting time of a  message from when the header flit is accommodated in an  input buffer until an output channel is dedicated to it. The  pseudo code of Fig. 9 is used to encode the waiting time in 2bit value. For example, if the waiting time of a message is less  than the average size of three messages (i.e. waited for the  service time of three messages), the local congestion can be  encoded in “00”. The global congestion information is a 4-bit  value giving a global view of the latency from the output  channel of the current switch to the destination switch region.  This global information is extracted from the corresponding  entry of R-Routing table. The Q-Values are updated whenever  a message is propagated between two neighboring switches.  Suppose that a message is sent from the switch X toward the  destination switch D through the neighboring switch Y and  then switch Z with the lowest estimated latencies. At the switch  Y, upon connecting the input channel to the output channel, 2bit local and 4-bit global values are aggregated into a 4-bit  value (with the maximum value of “1111”) and then transfer to  the switch X. This value is a new estimation of the latency  from the selected output channel of the switch X to the  destination D. The corresponding entry of the Q-Table at the  switch X is updated taking an average of the new estimated  value (i.e. BY+minQY(D,Z)) and an existing estimation  (QX(D,Y)). Commonly, in Q-Routing models, the following  formula is used:   ܳ௑ ሺܦ ǡ ܻሻ ൌ ሺͳെןሻܳ௑ ሺܦ ǡ ܻሻ൅ן ሺܤ௒ ൅ ݉݅݊ܳ௒ ሺܦ ǡ ܼሻሻ  In this formula, Į represents the learning rate at which  newer information overwrites the older one. A factor of 0 will  make no learning while a factor of 1 would consider only the  most recent information [22]. In our simulation, a 50-50 weight  is assigned to old and new information so that Į=0.5.   AMS: Average Message Size   LCV: Local Congestion Value  WT: Waiting Time  ---------------------------  IF (WT  3×AMS) Then     LCV<=”00”; END IF;  ELSIF (WT  9×AMS) Then  ELSIF (WT  27×AMS) Then     LCV<=”01”; END IF;     LCV<=”10”; END IF;  ELSE LCV<=”11”; END IF;  END IF; Fig. 9. Encoding the waiting time into 2-bit  Table Initialization: Q-Routing models have an initial learning  period during which it performs worse than minimal schemes.  The reason is that there is a possibility of choosing nonminimal paths even if the network is not congested. To cope  with this problem in the initialization phase, all entries of QTables are initialized such that minimal output channels are set  to “0000” and non-minimal output channels are set to “1000”  and never can be less than it. Accordingly, in a low traffic  condition, only minimal paths are selected while non-minimal  paths are used to distribute traffic when the network gets  congested.   RESULTS AND DISCUSSION  IV.  To evaluate the efficiency of the proposed routing scheme,  DBAR and C-Routing schemes are also implemented. The  former is an adaptive routing using local and non-local  congestion information, while the latter is an adaptive and  cluster-based routing using the Q-Learning technique. For  fairness, DBAR and C-Routing utilize a fully adaptive routing  function based on mad-y [12]. A wormhole-based NoC  simulator is developed with VHDL to model all major  components of the on-chip network and simulations are carried  out to determine the latency characteristic of each network  [25]. The message length is uniformly distributed between 1  and 5 flits. For all switches, the data width is set to 64 bits (the  maximum bandwidth at each link is 1 flit per cycle) and each  input channel has the buffer (FIFO) size of 8 flits. The request  rate is defined as the ratio of the successful message injections  into the network interface over the total number of injection  attempts. The simulator is warmed up for 12,000 cycles and  then the average performance is measured over another  200,000 cycles. Two synthetic traffic profiles  including  uniform random and hotspot, along with SPLASH-2 [26]  application traces are used.   A. Performance Evaluation under Uniform Traffic Profile  In the uniform traffic profile, a switch sends a message to  other switches with a uniform distribution. In Fig. 10, the  average communication delay as a function of the average  message injection rate is plotted for both mesh sizes. As  observed from the results, in low loads, the Q-Routing schemes  (HARAQ and C-Routing) behave as efficiently as DBAR. As  load increases, DBAR is unable to tolerate the high load  condition, while the Q-Routing schemes learn an efficient  routing policy. HARAQ leads to the lowest latency due to the  24   fact that it can distribute traffic more efficiently than the other  two schemes. In fact, in DBAR and C-Routing, messages use  minimal paths so that under this traffic they are routed through  the very center of the network which creates large permanent  hotspots in the network. Correspondingly, messages traveling  through the center of the network will be delayed much more  than they would use any non-minimal paths. Due to the fact  that the HARAQ method can reroute messages, it alleviates the  congested areas and performs considerably better than other  schemes. Using minimal and non-minimal routes along with  the intelligent selection policy reduces the average network  latency of HARAQ in the 8×8 network (near the saturation  point, 0.3) about 18% and 37%, compared with C-Routing and  DBAR, respectively.   B. Performance Evaluation under Hotspot Traffic Profile   Under the hotspot traffic profile, one or more switches are  chosen as hotspots receiving an extra portion of the traffic in  addition to the regular uniform traffic. In simulations, given a  hotspot percentage of H, a newly generated message is directed  to each hotspot switch with an additional H percent probability.  We simulate the hotspot traffic with a single hotspot switch at  (4, 4) and (7, 7) in the 8×8 and 14×14 2D-meshes, respectively.  The performance of each network with H = 10% is illustrated  in Fig. 11. As observed from the figure, the proposed routing  scheme achieves better performance compared with the other  schemes. In the 8×8 network, the performance gain near the  saturation point (0.18) is about 31% and 42%, compared with  C-Routing and DBAR, respectively. In addition, the impact of  using non-minimal paths on the link utilization for the hotspot  traffic profile is summarized in Table 3. We observe a  reduction as large as 25% in the 8×8 network compared with  DBAR. The results reveal that using the non-minimal scheme  along with the Q-Learning policy can distribute the traffic  efficiently.  Table 3. The impact of using non-minimal paths on the link utilization  8×8  14×14 0.365  0.263  0.352  0.256  0.274  0.202  25%  23%  22%  21%  DBAR  C-Routing  HARAQ  Reduction over  DBAR  Reduction over CR  Maximum  link  utilization  (flits/cycle)  C. Application Traffic Profile  Application traces are obtained from the GEMS simulator  [27] using some application benchmark suites selected from  SPLASH-2. We use a 64-switch network configuration: 20  processors and 44 L2-cache memory modules. For the CPU,  we assume a core similar to Sun Niagara and use SPARC ISA.  Each L2 cache core is 512KB, and thus, the total shared L2  cache is 22MB. The memory hierarchy implemented is  governed by a two-level directory cache coherence protocol.  Each processor has a private write-back L1 cache (split L1 I  and D cache, 64 KB, 2-way, 3-cycle access). The L2 cache is  shared among all processors and split into banks (44 banks, 512  KB each for a total of 22 MB, 6-cycle bank access), connected  via on-chip switches. The L1/L2 block size is 64B. Our  coherence model  includes a MESI-based protocol with  distributed directories, with each L2 bank maintaining its own  local directory. The simulated memory hierarchy mimics  SNUCA [28] while the off-chip memory is a 4 GB DRAM  with a 220-cycle access time. Fig. 12 shows the average  25 message latency across four benchmark traces, normalized to  DBAR. HARAQ provides lower latency than other schemes  and it shows the greatest performance gain in Radix with 27%  reduction in latency (vs. C-Routing). The average performance  gain of HARAQ across all benchmarks is up to 22% vs. CRouting and 33% vs. DBAR.   D. Hardware Analysis  To assess the area overhead and power consumption of the  proposed scheme, the whole platform of each scheme is  synthesized by Synopsys Design Compiler. Each scheme  includes switches, communication channels, and congestion  wires. For synthesis, we use the UMC 90nm technology at the  operating frequency of 1GHz and supply voltage of 1V. We  perform place-and-route, using Cadence Encounter, to have  precise power and area estimations. The power dissipation of  each scheme is calculated under the hotspot traffic profile near  the saturation point (0.18) using Synopsys PrimePower in a  8×8 2D mesh. The layout area and power consumption of each  platform are shown in Table 4. Comparing the area cost of the  platform using HARAQ with the platforms using C-Routing  and DBAR indicates that the C-Routing platform consumes  more power and has a higher area overhead while the overhead  of HARAQ platform compared with DBAR is less than 1% but  with a significant performance gain. The HARAQ platform  consumes more average power because of rerouting messages  around the congestion areas which increases the hop counts. To  illustrate how the presented approach reduces the network  hotspots, the maximum power value of each platform is also  reported in the table. The results indicate that the maximum  power of the presented approach is 8% and 12% less than that  of the DBAR and C-Routing platforms, respectively. This is  achieved by smoothly distributing the power consumption over  the network using the highly adaptive routing scheme which  reduces the number of the hotspots. In fact, the maximum  power values, reported in the table, belong to the switch  designated as the hotspot one, (4, 4).  Network  platforms  DBAR  C-Routing  HARAQ  Table 4. Hardware implementation details  Area  Avg. Power (W)  Max. Power (W)  (mm2)  dynamic & static  dynamic & static  6.791  2.41  3.33  6.954  2.52  3.46  6.822  2.81  3.06  V. SUMMARY AND CONCLUSION  In this paper, we proposed a highly adaptive routing  algorithm based on minimal and non-minimal paths for on-chip  networks. The presented algorithm provides a large number of  paths for routing messages using only an extra virtual channel  in the Y dimension. In the proposed method, messages can  temporarily move away from the destination and be routed  around congested regions. Moreover, the use of some 180degree turns on a single channel is also permitted in the  algorithm. To choose a less congested path, we have utilized an  optimized and scalable learning model to estimate the latency  from each output channel to the destination switch. In the  learning model, switches maintain distributed tables to store the  global congestion information from different regions of the  network. This congestion information is collected via a fully  distributed approach requiring a small number of bits per link.  Finally, a less congested output channel is chosen by the  selection function.     0.1 Injection Rate (flits/switch/cycles) (a) 0.3 0.4 0.2 0.05 0.1 Injection Rate (flits/switch/cycles) (b) 0.15 0.25 0.2 0.3 Fig. 10. Performance under different loads in (a) 8×8 2D-mesh and (b) 14×14 2D-mesh under the uniform traffic model  HARAQ CR DBAR HARAQ CR DBAR y c n e l t ) a e L c e y g c a ( r e v A 350 300 250 200 150 100 50 0 0.5 0 y c n e l t ) a e L c e y g c a ( r e v A 350 300 250 200 150 100 50 0 0.25 0 HARAQ CR DBAR HARAQ CR DBAR y c n e l t ) a e L c e y g c a ( r e v A 350 300 250 200 150 100 50 0 0 y c n e l t ) a e L c e y g c a ( r e v A 350 300 250 200 150 100 50 0 0 0.05 Injection Rate (flits/switch/cycles) (a) 0.15 0.2 0.1 0.05 Injection Rate (flits/switch/cycles) (b) 0.1 0.15 Fig. 11. Performance under different loads in (a) 8×8 2D-mesh and (b) 14×14 2D-mesh under hotspot traffic model with H=10%  e g a r e v a d e z i l a m y c n e t a l r o N DBAR C-Routing HARAQ 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Barnes cholesky FFT LU Ocean Radix Raytrace Water-Nsq "MinBD - Minimally-Buffered Deflection Routing for Energy-Efficient Interconnect.,"A conventional Network-on-Chip (NoC) router uses input buffers to store in-flight packets. These buffers improve performance, but consume significant power. It is possible to bypass these buffers when they are empty, reducing dynamic power, but static buffer power, and dynamic power when buffers are utilized, remain. To improve energy efficiency, buffer less deflection routing removes input buffers, and instead uses deflection (misrouting) to resolve contention. However, at high network load, deflections cause unnecessary network hops, wasting power and reducing performance. In this work, we propose a new NoC router design called the minimally-buffered deflection (MinBD) router. This router combines deflection routing with a small ""side buffer,"" which is much smaller than conventional input buffers. A MinBD router places some network traffic that would have otherwise been deflected in this side buffer, reducing deflections significantly. The router buffers only a fraction of traffic, thus making more efficient use of buffer space than a router that holds every flit in its input buffers. We evaluate MinBD against input-buffered routers of various sizes that implement buffer bypassing, a buffer less router, and a hybrid design, and show that MinBD is more energy efficient than all prior designs, and has performance that approaches the conventional input-buffered router with area and power close to the buffer less router.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip MinBD: Minimally-Buffered Deﬂection Routing for Energy-Efﬁcient Interconnect Chris Fallin, Greg Nazario, Xiangyao Yu† , Kevin Chang, Rachata Ausavarungnirun, Onur Mutlu {cfallin,gnazario,kevincha,rachata,onur}@cmu.edu Carnegie Mellon University †Tsinghua University & Carnegie Mellon University yxythu@gmail.com A conventional Network-on-Chip (NoC) router uses input buffers to store in-ﬂight packets. These buffers improve performance, but consume signiﬁcant power. It is possible to bypass these buffers when they are empty, reducing dynamic power, but static buffer power, and dynamic power when buffers are utilized, remain. To improve energy efﬁciency, bufferless deﬂection routing removes input buffers, and instead uses deﬂection (misrouting) to resolve contention. However, at high network load, deﬂections cause unnecessary network hops, wasting power and reducing performance. In this work, we propose a new NoC router design called the minimally-buffered deﬂection (MinBD) router. This router combines deﬂection routing with a small “side buffer,” which is much smaller than conventional input buffers. A MinBD router places some network trafﬁc that would have otherwise been deﬂected in this side buffer, reducing deﬂections signiﬁcantly. The router buffers only a fraction of trafﬁc, thus making more efﬁcient use of buffer space than a router that holds every ﬂit in its input buffers. We evaluate MinBD against input-buffered routers of various sizes that implement buffer bypassing, a bufferless router, and a hybrid design, and show that MinBD is more energy-efﬁcient than all prior designs, and has performance that approaches the conventional input-buffered router with area and power close to the bufferless router. I . IN TRODUC T ION A network-on-chip is a ﬁrst-order component of current and future multicore and manycore CMPs (Chip Multiprocessors) [10], and its design can be critical for system performance. As core counts continue to rise, NoCs with designs such as 2Dmesh (e.g., Tilera [40] and Intel Terascale [19]) are expected to become more common to provide adequate performance scaling. Unfortunately, packet-switched NoCs are projected to consume signiﬁcant power. In the Intel Terascale 80-core chip, 28% of chip power is consumed by the NoC [19]; for MIT RAW, 36% [35]; for the Intel 48-core SCC, 10% [3]. NoC energy efﬁciency is thus an important design goal [4], [5]. Mechanisms have been proposed to make conventional inputbuffered NoC routers more energy-efﬁcient. For example, bypassing empty input buffers [39], [27] reduces some dynamic buffer power, but static power remains.1 Such bypassing is also less effective when buffers are not frequently empty. Bufferless deﬂection routers [12], [28] remove router input buffers completely (hence eliminating their static and dynamic power) to reduce router power. When two ﬂits2 contend for a single router output, one must be deﬂected to another output. Thus, a ﬂit never requires a buffer in a router. By controlling 1One recent estimate indicates that static power (of buffers and links) could constitute 80–90% of interconnect power in future systems [6]. 2 In a conventional bufferless deﬂection network, ﬂits (several of which make up one packet) are independently routed, unlike most buffered networks, where a packet is the smallest independently-routed unit of trafﬁc. which ﬂits are deﬂected, a bufferless deﬂection router can ensure that all trafﬁc is eventually delivered. Removing buffers yields simpler and more energy-efﬁcient NoC designs: e.g., CHIPPER [12] reduces average network power by 54.9% in a 64-node system compared to a conventional buffered router. Unfortunately, at high network load, deﬂection routing reduces performance and efﬁciency. This is because deﬂections occur more frequently when many ﬂits contend in the network. Each deﬂection sends a ﬂit further from its destination, causing unnecessary link and router traversals. Relative to a buffered network, a bufferless network with a high deﬂection rate wastes energy, and suffers worse congestion, because of these unproductive network hops. In contrast, a buffered router is able to hold ﬂits (or packets) in its input buffers until the required output port is available, incurring no unnecessary hops. Thus, a buffered network can sustain higher performance at peak load. Our goal is to obtain the energy efﬁciency of the bufferless approach with the high performance of the buffered approach. One prior work, AFC (Adaptive Flow Control), proposes a hybrid design that switches each router between a conventional input-buffered mode and a bufferless deﬂection mode [21]. However, switching to a conventional buffered design at high load incurs the energy penalty for buffering every ﬂit: in other words, the efﬁciency gain over the baseline input-buffered router disappears once load rises past a threshold. AFC also requires the control logic for both forms of routing to be present at each network node, and requires power gating to turn off the input buffers and associated logic at low load. Another prior design, the Chaos router [23], also combines buffering with deﬂection, but still uses its input buffers for every ﬂit that arrives. Ideally, a router would contain only a small amount of buffering, and would use this buffer space only for those ﬂits that actually require it, rather than all ﬂits that arrive. We propose minimally-buffered deﬂection routing (MinBD) as a new NoC router design that combines both bufferless and buffered paradigms in a more ﬁne-grained and efﬁcient way. MinBD uses deﬂection routing, but also incorporates a small buffer. The router does not switch between modes, but instead, always operates in a minimally-buffered deﬂection mode, and can buffer or deﬂect any given ﬂit. When a ﬂit ﬁrst arrives, it does not enter a buffer, but travels straight to the routing logic. If two ﬂits contend for the same output, the routing logic chooses one to deﬂect, as in a bufferless router. However, the router can choose to buffer up to one deﬂected ﬂit per cycle rather than deﬂecting it. This ﬁne-grained buffering-deﬂection hybrid approach signiﬁcantly reduces deﬂection rate (by 54% in our evaluations), and improves performance, as we show. It also incurs only a fraction of the energy cost of a conventional 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.8 1 e c n a m r o f r e P e v i t a l e R  1.2  1  0.8  0 Bufferless Performance Relative to Buffered with Bypassing  0.2  0.4 Injection Rate (flits/node/cycle)  0.6 y c n e i c i f f E y g r e n E e v i t a l e R  2  1  0  0 Bufferless Energy Efficiency Relative to Buffered with Bypassing  0.2  0.4 Injection Rate (flits/node/cycle)  0.6 Figure 1. System performance and energy efﬁciency (performance per watt) of bufferless deﬂection routing, relative to conventional input-buffered routing (4 VCs, 4 ﬂits/VC) that employs buffer bypassing, in a 4x4 2D mesh. Injection rate (X axis) for each workload is measured in the baseline buffered network. buffered router, because only a relatively small fraction of ﬂits are buffered (20% of all ﬂits in our evaluations). As we show in our evaluations, MinBD provides higher energy efﬁciency while also providing high performance, compared to a comprehensive set of baseline router designs. Our contributions are: • A new NoC router, MinBD (minimally-buffered deﬂection), that combines deﬂection routing with minimal buffering. The router performs deﬂection routing, but can choose to buffer up to one ﬂit per cycle in a small side buffer, which signiﬁcantly reduces deﬂection rate and enhances performance compared to a pure bufferless design while requiring smaller buffer space than a conventional input-buffered design. • An evaluation of MinBD against aggressive NoC router baselines: a two-cycle virtual channel buffered router [9] with empty buffer bypassing [39], [27] at three buffer capacities (with a sensitivity analysis over many more conﬁgurations), a state-of-the-art bufferless deﬂection router, CHIPPER [12], and a hybrid bufferless-buffered design, AFC [21], with SPEC CPU2006 [34] multiprogrammed workloads on 16- and 64-node CMP systems. From our results, we conclude that MinBD has the best energy efﬁciency over all of these prior design points, while achieving competitive performance and critical path with the input-buffered router (the best-performing baseline) and competitive area and power with the pure-bufferless router (the smallest and most power-efﬁcient baseline). I I . BACKGROUND In this section, we give background on NoC-based cachecoherent CMPs, and on bufferless deﬂection routing, which we build upon. We assume the reader is familiar with the basic operation of conventional input-buffered routers; Dally and Towles [9] provide a good reference on these routers. NoCs in cache-coherent CMPs: On-chip networks form the backbone of memory systems in many recently-proposed and prototyped large-scale CMPs (chip multiprocessors) [35], [19], [40]. Most such systems are cache-coherent shared memory multiprocessors. Packet-switched interconnect has served as the substrate for large cache-coherent systems for some time (e.g., for large multiprocessor systems such as SGI Origin [25]), and the principles are the same in a chip multiprocessor: each core, slice of a shared cache, or memory controller is part of one “node” in the network, and network nodes exchange packets that request and respond with data in order to fulﬁll memory accesses. For example, on a miss, a core’s private cache might send a request packet to a shared L2 cache slice, and the shared cache might respond with a larger packet containing the requested cache block on an L2 hit, or might send another packet to a memory controller on an L2 miss. CMP NoCs are typically used to implement such a protocol between the cores, caches and memory controllers. Bufferless Deﬂection Routers: Bufferless deﬂection routing was ﬁrst proposed by Baran [2]. It has found renewed interest in NoC design because on-chip wires (hence, network links) are relatively cheap, in contrast to buffers, which consume signiﬁcant die area and leakage power [28], [21], [6], [4], [5]. The fundamental unit of routing in a bufferless network is the ﬂit, a packet fragment transferred by one link in one cycle. Flits are routed independently; thus, they must be reassembled after they are received. The basic operation of a bufferless deﬂection router is simple. In each cycle, ﬂits arriving from neighbor routers enter the router pipeline. Because the router contains no buffers, ﬂits are stored only in pipeline registers, and must leave the router at the end of the pipeline. Thus, the router must assign every input ﬂit to some output port. When two ﬂits request the same output port according to their ordinary routing function, the router deﬂects one of them to another port (this is always possible, as long as the router has as many outputs as inputs). This output will likely take the deﬂected ﬂit further from its destination, and the ﬂit will later have to work its way back. Thus, in such a design, care must be taken to avoid livelock, where a ﬂit never arrives. Prior work uses priority schemes to ensure that every ﬂit is eventually delivered. An early implementation of bufferless deﬂection routing for a NoC was BLESS [28], and CHIPPER [12] later provided a more efﬁcient hardware implementation of the deﬂection routing and packet reassembly. CHIPPER is a baseline in our evaluations. I I I . MOT IVAT ION Previous NoC designs based on bufferless deﬂection routing [12], [28] were motivated largely by the observation that many NoCs in CMPs are overprovisioned for the common-case network load. In this case, a bufferless network can attain nearly the same application performance while consuming less power, which yields higher energy efﬁciency. We now examine the relative application performance (weighted speedup: see §V), buffered-bufferless comparison in more detail. Fig. 1 shows (i) and (ii) relative energy efﬁciency (performance per watt), when using a bufferless network, compared to a conventional buffered network. Both plots show these effects as a function of network load (average injection rate). Here we show a virtual channel buffered network (4 VCs, 4 ﬂits/VC) (with buffer bypassing) mesh CMP (details on methodology in §V). and the CHIPPER bufferless deﬂection network [12] in a 4x4For low-to-medium network load, a bufferless network has performance close to a conventional buffered network, because the deﬂection rate is low: thus, most ﬂits take productive network hops on every cycle, just as in the buffered network. In addition, the bufferless router has signiﬁcantly reduced power (hence improved energy efﬁciency), because the buffers in a conventional router consume signiﬁcant power. However, as network load increases, the deﬂection rate in a bufferless deﬂection network also rises, because ﬂits contend with 2       Ruleset 1 MinBD Prioritization Rules (based on Golden Packet [12] with new rule 3) Given: two ﬂits, each Golden, Silver, or Ordinary. (Only one can be Silver.) 1. Golden Tie: Ties between two Golden ﬂits are resolved by sequence number (ﬁrst in Golden Packet wins). 2. Golden Dominance: If one ﬂit is Golden, it wins over any Silver or Ordinary ﬂits. 3. Silver Dominance: Silver ﬂits win over Ordinary ﬂits. 4. Common Case: Ties between Ordinary ﬂits are resolved randomly. Figure 2. MinBD router pipeline. each other more frequently. With a higher deﬂection rate, the dynamic power of a bufferless deﬂection network rises more quickly with load than dynamic power in an equivalent buffered network, because each deﬂection incurs some extra work. Hence, bufferless deﬂection networks lose their energyefﬁciency advantage at high load. Just as important, the high deﬂection rate causes each ﬂit to take a longer path to its destination, and this increased latency reduces the network throughput and system performance. Overall, neither design obtains both good performance and good energy efﬁciency at all loads. If the system usually experiences low-to-medium network load, then the bufferless design provides adequate performance with low power (hence high energy efﬁciency). But, if we use a conventional buffered design to obtain high performance, then energy efﬁciency is poor in the low-load case, and even buffer bypassing does not remove this overhead because buffers consume static power regardless of use. Finally, simply switching between these two extremes at a per-router granularity, as previously proposed [21], does not address the fundamental inefﬁciencies in the bufferless routing mode, but rather, uses input buffers for all incoming ﬂits at a router when load is too high for the bufferless mode (hence retains the energy-inefﬁciency of buffered operation at high load). We now introduce our minimally-buffered deﬂection router which combines bufferless and buffered routing in a new way to reduce this overhead. IV.M INBD : M IN IMA L LY-BU FF ER ED D E FL EC T ION ROU T ER The MinBD (minimally-buffered deﬂection) router is a new router design that combines bufferless deﬂection routing with a small buffer, which we call the “side buffer.” We start by outlining the key principles we follow to reduce deﬂectioncaused inefﬁciency by using buffering: 1) When a ﬂit would be deﬂected by a router, it is often better to buffer the ﬂit and arbitrate again in a later cycle. Some buffering can avoid many deﬂections. 2) However, buffering every ﬂit leads to unnecessary power overhead and buffer requirements, because many ﬂits will be routed productively on the ﬁrst try. The router should buffer a ﬂit only if necessary. 3) Finally, when a ﬂit arrives at its destination, it should be removed from the network (ejected) quickly, so that it does not continue to contend with other ﬂits. Basic High-Level Operation: The MinBD router does not use input buffers, unlike conventional buffered routers. Instead, a ﬂit that arrives at the router proceeds directly to the routing and arbitration logic. This logic performs deﬂection routing, so that when two ﬂits contend for an output port, one of the ﬂits is sent to another output instead. However, unlike a bufferless deﬂection router, the MinBD router can also buffer up to one ﬂit per cycle in a single FIFO-queue side buffer. The router examines all ﬂits at the output of the deﬂection routing logic, and if any are deﬂected, one of the deﬂected ﬂits is removed from the router pipeline and buffered (as long as the buffer is not full). From the side buffer, ﬂits are reinjected into the network by the router, in the same way that new trafﬁc is injected. Thus, some ﬂits that would have been deﬂected in a bufferless deﬂection router are removed from the network temporarily into this side buffer, and given a second chance to arbitrate for a productive router output when re-injected. This reduces the network’s deﬂection rate (hence improves performance and energy efﬁciency) while buffering only a fraction of trafﬁc. First, §IV-A describes the deﬂection routing logic that computes We will describe the operation of the MinBD router in stages. an initial routing decision for the ﬂits that arrive in every cycle. Then, §IV-B describes how the router chooses to buffer some (but not all) ﬂits in the side buffer. §IV-C describes how buffered ﬂits and newly-generated ﬂits are injected into the network, and how a ﬂit that arrives at its destination is ejected. Finally, §IV-D discusses correctness issues, and describes how MinBD ensures that all ﬂits are eventually delivered. A. Deﬂection Routing The MinBD router pipeline is shown in Fig. 2. Flits travel through the pipeline from the inputs (on the left) to outputs (on the right). We ﬁrst discuss the deﬂection routing logic, located in the Permute stage on the right. This logic implements deﬂection routing: it sends each input ﬂit to its preferred output when possible, deﬂecting to another output otherwise. MinBD uses the deﬂection logic organization ﬁrst proposed in CHIPPER [12]. The permutation network in the Permute stage consists of two-input blocks arranged into two stages of two blocks each. This arrangement can send a ﬂit on any input to any output. (Note that it cannot perform all possible permutations of inputs to outputs, but as we will see, it is sufﬁcient for correct operation that at least one ﬂit obtains its preferred output.) In each two-input block, arbitration logic determines which ﬂit has a higher priority, and sends that ﬂit in the direction of its preferred output. The other ﬂit at the two-input block, if any, must take the block’s other output. By combining two stages of this arbitration and routing, deﬂection arises as a distributed decision: a ﬂit might be deﬂected in the ﬁrst stage, or the second stage. Restricting the arbitration and routing to two-ﬂit subproblems reduces complexity and allows for a shorter critical path, as demonstrated in [12]. In order to ensure correct operation, the router must arbitrate 3 between ﬂits so that every ﬂit is eventually delivered, despite deﬂections. We adapt a modiﬁed version of the Golden Packet priority scheme [12], which solves this livelock-freedom problem. This priority scheme is summarized in Ruleset 1. The basic idea of the Golden Packet priority scheme is that at any given time, at most one packet in the system is golden. The ﬂits of this golden packet, called “golden ﬂits,” are prioritized above all other ﬂits in the system (and contention between golden ﬂits is resolved by the ﬂit sequence number). While prioritized, golden ﬂits are never deﬂected by non-golden ﬂits. The packet is prioritized for a period long enough to guarantee its delivery. Finally, this “golden” status is assigned to one globally-unique packet ID (e.g., source node address concatenated with a request ID), and this assignment rotates through all possible packet IDs such that any packet that is “stuck” will eventually become golden. In this way, all packets will eventually be delivered, and the network is livelock-free. (See [12] for the precise way in which the Golden Packet is determined; we use the same rotation schedule.) However, although Golden Packet arbitration provides correctness, a performance issue occurs with this priority scheme. Consider that most ﬂits are not golden: the elevated priority status provides worst-case correctness, but does not impact common-case performance (prior work reported over 99% of ﬂits are delivered without becoming golden [12]). However, when no ﬂits are golden and ties are broken randomly, the arbitration decisions in the two permutation network stages are not coordinated. Hence, a ﬂit might win arbitration in the ﬁrst stage, and cause another ﬂit to be deﬂected, but then lose arbitration in the second stage, and also be deﬂected. Thus, unnecessary deﬂections occur when the two permutation network stages are uncoordinated. In order to resolve this performance issue, we observe that it is enough to ensure that in every router, at least one ﬂit is prioritized above the others in every cycle. In this way, at least one ﬂit will certainly not be deﬂected. To ensure this when no golden ﬂits are present, we add a “silver” priority level, which wins arbitration over common-case ﬂits but loses to the golden ﬂits. One silver ﬂit is designated randomly among the set of ﬂits that enter a router at every cycle (this designation is local to the router, and not propagated to other routers). This modiﬁcation helps to reduce deﬂection rate. Prioritizing a silver ﬂit at every router does not impact correctness, because it does not deﬂect a golden ﬂit if one is present, but it ensures that at least one ﬂit will consistently win arbitration at both stages. Hence, deﬂection rate is reduced, improving performance. B. Using a Small Buffer to Reduce Deﬂections The key problem addressed by MinBD is deﬂection inefﬁciency at high load: in other words, when the network is highly utilized, contention between ﬂits occurs often, and many ﬂits will be deﬂected. We observe that adding a small buffer to a deﬂection router can reduce deﬂection rate, because the router can choose to buffer rather than deﬂect a ﬂit when its output port is taken by another ﬂit. Then, at a later time when output ports may be available, the buffered ﬂit can re-try arbitration. Thus, to reduce deﬂection rate, MinBD adds a “side buffer” that buffers only some ﬂits that otherwise would be deﬂected. This buffer is shown in Fig. 2 above the permutation network. In order to make use of this buffer, a “buffer eject” block is placed in the pipeline after the permutation network. At this point, the arbitration and routing logic has determined which ﬂits to deﬂect. The buffer eject block recognizes ﬂits that have been deﬂected, and picks up to one such deﬂected ﬂit per cycle. It removes a deﬂected ﬂit from the router pipeline, and places this ﬂit in the side buffer, as long as the side buffer is not full. (If the side buffer is full, no ﬂits are removed from the pipeline into the buffer until space is created.) This ﬂit is chosen randomly see §IV-D). In this way, some deﬂections are avoided. The ﬂits among deﬂected ﬂits (except that a golden ﬂit is never chosen: placed in the buffer will later be re-injected into the pipeline, and will re-try arbitration at that time. This re-injection occurs in the same way that new trafﬁc is injected into the network, which we discuss below. C. Injection and Ejection So far, we have considered the ﬂow of ﬂits from router input ports (i.e., arriving from neighbor routers) to router output ports (i.e., to other neighbor routers). A ﬂit must enter and leave the network at some point. To allow trafﬁc to enter (inject) and leave (eject), the MinBD router contains inject and eject blocks in its ﬁrst pipeline stage (see Fig. 2). When a set of ﬂits arrive on router inputs, these ﬂits ﬁrst pass through the ejection logic. This logic examines the destination of each ﬂit, and if a ﬂit is addressed to the local router, it is removed from the router pipeline and sent to the local network node.3 If more than one locally-addressed ﬂit is present, the ejector picks one, according to the same priority scheme used by routing arbitration. However, ejecting a single ﬂit per cycle can produce a bottleneck and cause unnecessary deﬂections for ﬂits that could not be ejected. In the workloads we evaluate, at least one ﬂit is eligible to eject 42.8% of the time. Of those cycles, 20.4% of the time, at least two ﬂits are eligible to eject. Hence, in ∼8.5% of all cycles, a locally-addressed ﬂit would be deﬂected rather than ejected if only one ﬂit could be ejected per cycle. To avoid this signiﬁcant deﬂection-rate penalty, we double the ejection bandwidth. To implement this, a MinBD router contains two ejector blocks. Each of these blocks is identical, and can eject up to one ﬂit per cycle. Duplicating the ejection logic allows two ﬂits to leave the network per cycle at every node.4 After locally-addressed ﬂits are removed from the pipeline, new ﬂits are allowed to enter. There are two injector blocks in the router pipeline shown in Fig. 2: (i) re-injection of ﬂits from the side buffer, and (ii) injection of new ﬂits from the local node. (The “Redirection” block prior to the injector blocks will be discussed in the next section.) Each block operates in the same way. A ﬂit can be injected into the router pipeline whenever one of the four inputs does not have a ﬂit present in a given cycle, i.e., whenever there is an “empty slot” in the network. Each injection block pulls up to one ﬂit per cycle from an injection queue (the side buffer, or the local node’s injection queue), and places a new ﬂit in the pipeline when a slot is available. Flits from the side buffer are re-injected before new trafﬁc is injected into the network. However, note that there is no guarantee that a free slot will be available for an injection in any given cycle. We now address this starvation problem for side buffer re-injection. 3Note that ﬂits are reassembled into packets after ejection. To implement this reassembly, we use the Retransmit-Once scheme, as used by CHIPPER, which uses MSHRs (Miss-Status Handling Registers [24], or existing buffers in the cache system) to reassemble packets in place. See [12] for details. 4 For fairness, because dual ejection widens the datapath from the router to the local node (core or cache), we also implement dual ejection in the baseline bufferless deﬂection network and input-buffered network to which we compare. 4 CPU cores L1 caches L2 (shared) cache Shared cache mapping Cache coherence scheme Data packet sizes Network Links Baseline bufferless router Baseline buffered router AFC (Adaptive Flow Control) MinBD Out-of-order, 3-wide issue and retire (1 memory op/cycle), 16 MSHRs [24] 64 KB, 4-way associative, 32-byte blocks Distributed across nodes; perfect (always hits) to penalize our design conservatively & isolate network performance from memory effects Consecutive cache blocks striped across L2 cache slices Directory-based, perfect directory (SGI Origin protocol [25]) 1-ﬂit request packets, 4-ﬂit data packets 1-cycle latency (separate pipeline stage), 2.5mm, 128 bits wide CHIPPER [12], 2-cycle router latency; 64-cycle Golden Epoch; Retransmit-Once [12] (m VCs, n ﬂits/VC): (8, 8), (4, 4), (4, 1). 2-cycle latency, buffer bypassing [39], [27]. Additional conﬁgurations evaluated in Fig. 4. As described in [21]: 4 VCs/channel, 4 ﬂits/VC. 2-cycle latency (buffered & bufferless modes). Implements buffer bypassing as well. 2-cycle latency (§IV); 4-ﬂit side buffer (single FIFO); Cthreshold = 2; 64-cycle Golden Epoch; Retransmit-Once [12] Table I. Simulated baseline system parameters. D. Ensuring Side Buffered Flits Make Progress When a ﬂit enters the side buffer, it leaves the router pipeline, and must later be re-injected. As we described above, ﬂit reinjection must wait for an empty slot on an input link. It is possible that such a slot will not appear for a long time. In this case, the ﬂits in the side buffer are delayed unfairly while other ﬂits make forward progress. To avoid this situation, we implement buffer redirection. The key idea of buffer redirection is that when this side buffer starvation problem is detected, one ﬂit from a randomly-chosen router input is forced to enter the side buffer. Simultaneously, the ﬂit at the head of the side buffer is allowed to inject into the slot created by the forced ﬂit buffering. In other words, one router input is “redirected” into the FIFO buffer for one cycle, in order to allow the buffer to make forward progress. This redirection is enabled for one cycle whenever the side buffer injection is starved (i.e., has a ﬂit to inject, but no free slot allows the injection) for more than some threshold Cthreshold cycles (in our evaluations, Cthreshold = 2). Finally, note that if a golden ﬂit is present, it is never redirected to the buffer, because this would break the delivery guarantee. E. Livelock and Deadlock-free Operation MinBD provides livelock-free delivery of ﬂits using Golden Packet and buffer redirection. If no ﬂit is ever buffered, then Golden Packet [12] ensures livelock freedom (the “silver ﬂit” priority never deﬂects any golden ﬂit, hence does not break the guarantee). Now, we argue that adding side buffers does not cause livelock. First, the buffering logic never places a golden ﬂit in the side buffer. However, a ﬂit could enter a buffer and then become golden while waiting. Redirection ensures correctness in this case: it provides an upper bound on residence time in a buffer (because the ﬂit at the head of the buffer will leave after a certain threshold time in the worst case). If a ﬂit in a buffer becomes golden, it only needs to remain golden long enough to leave the buffer in the worst case, then progress to its destination. We choose the threshold parameter (Cthreshold ) and golden epoch length so that this is always possible. More details can be found in our extended technical report [13]. MinBD achieves deadlock-free operation by using Retransmit-Once [12], which ensures that every node always consumes ﬂits delivered to it by dropping ﬂits when no reassembly/request buffer is available. This avoids packetreassembly deadlock (as described in [12]), as well as protocol level deadlock, because message-class dependencies [16] no longer exist. V. EVALUAT ION M E THODO LOGY To obtain application-level performance as well as network performance results, we use an in-house CMP simulator. This simulator consumes instruction traces of x86 applications, and faithfully models the CPU cores and the cache hierarchy, with a directory-based cache coherence protocol (based on the SGI Origin protocol [8]) running on the modeled NoC. The CPU cores model stalls, and interact with the caches and network in a closed-loop way. The modeled network routers are cycle-accurate, and are assumed to run in a common clock domain. The instruction traces are recorded with a Pintool [26], sampling representative portions of each benchmark as determined by PinPoints [30]. We ﬁnd that simulating 25M cycles gives stable results with these traces. Detailed system parameters are shown in Table I. Note that we make use of a perfect shared cache to stress the network, as was done in the CHIPPER [12] and BLESS [28] bufferless router evaluations. In this model, every request generated by an L1 cache miss goes to a shared cache slice, and the request is always assumed to hit and return data. This potentially increases network load relative to a real system, where off-chip memory bandwidth can also be a bottleneck. However, note that this methodology is conservative: because MinBD degrades performance relative to the buffered baseline, the performance degradation that we report is an upper bound on what would occur when other bottlenecks are considered. We choose to perform our evaluations this way in order to study the true capacity of the evaluated networks (if network load is always low because system bottlenecks such as memory latency are modeled, then the results do not give many insights about router design). Note that the cache hierarchy details (L1 and L2 access latencies, and MSHRs) are still realistic. We remove only the off-chip memory latency/bandwidth bottleneck. Baseline Routers: We compare MinBD to a conventional input-buffered virtual channel router [9] with buffer bypassing [39], [27], a bufferless deﬂection router (CHIPPER [12]), and a hybrid bufferless-buffered router (AFC [21]). In particular, we sweep buffer size for input-buffered routers. We describe a router with m virtual channels (VCs) per input and n ﬂits of buffer capacity per VC as an (m, n)-buffered router. We compare to a (8, 8), (4, 4), and (4, 1)-buffered routers in our main results. The (8, 8) point represents a very large (overprovisioned) baseline, while (4, 4) is a more reasonable general-purpose conﬁguration. The (4, 1) point represents the minimum buffer size for deadlock-free operation (two message classes [16], times two to avoid routing deadlock [20]). Furthermore, though 1-ﬂit VC buffers would reduce throughput because they do not cover the credit round-trip latency, we optimistically assume zero-latency credit traversal in our simulations (which beneﬁts the baseline design, hence is conservative for our claims). Finally, we simulate smaller power-performance tradeoff evaluation in §VI-B (Fig. 4), solely (non-deadlock-free) designs with 1 and 2 VCs per input for our for completeness (we were able to avoid deadlock at moderate loads and with ﬁnite simulation length for these runs). Application Workloads: We use SPEC CPU2006 [34] benchmarks (26 applications5 ) to construct 75 multiprogrammed 5 410.bwaves, 416.gamess and 434.zeusmp were excluded because we were not able to collect representative traces from these applications. 5  16  14  12  10  8  6  4  2  0 W ) static buffer W static link static other e i h g t p u d e e p S d e 4x4 0.00 - 0.15 0.15 - 0.30 0.30 - 0.40 0.40 - 0.50 > 0.50 AVG Buffered (8,8) Buffered (4,4) Buffered (4,1) Pure Bufferless (CHIPPER) AFC (4,4) MinBD-4  5  4  3  2  1  0 D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i o P w e r ( dynamic buffer dynamic link dynamic other  0  5  10  15  20 0.0 0 - 0.1 5 0.1 5 - 0.3 0 0.3 0 - 0.4 0 0.4 0 - 0.5 0 > 0.5 0 A V G E e n ( r e y P g E i f / f f r c i ) y c n e W Network Intensity  0  20  16  12  8  4  0  16  32  48  64 W e i h g t p u d e e p S d e 8x8 0.00 - 0.05 0.05 - 0.15 0.15 - 0.25 > 0.25 AVG D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i D B C H B B B C A u u u ( ( ( I f f f R 4 4 E 8 4 1 P 4 8 4 4 P , , , ) ) ) n F ( ) , M i o P w e r ( W ) static buffer static link static other dynamic buffer dynamic link dynamic other 0.0 - 0.0 5 0.0 5 - 0.1 5 0.1 5 - 0.2 5 > 0.2 5 A V G  25  20  15  10  5  0 E e n ( r e y P g E i f / f f r c i ) y c n e W Network Intensity Figure 3. Performance (weighted speedup), network power, and energy efﬁciency (performance per watt) in 16 (4x4) and 64 (8x8)-node CMP systems. workloads (which consist of many single-threaded benchmark instances that run independently). Each workload consists of 16 or 64 randomly-selected applications which are randomly mapped onto the mesh. Workloads are categorized by average network injection rate in the baseline (4, 4)-buffered system (measured in ﬂits/cycle/node). For 4x4 workloads, these injection rate categories are (0, 0.15), (0.15, 0.3), (0.3, 0.4), (0.4, 0.5), and > 0.5; for 8x8 workloads, (0, 0.05), (0.05, 0.15), (0.15, 0.25), and > 0.25 (an 8x8 mesh saturates at a lower load than a 4x4 mesh, due to limited bisection bandwidth). Each category contains 15 workloads. Synthetic-Trafﬁc Workloads: To show robustness under various trafﬁc patterns, we evaluate 4x4 and 8x8 networks with uniform-random, bit-complement, and transpose synthetic trafﬁc [9] in addition to application workloads. For each pattern, network injection rate is swept from zero to network saturation. Performance Metrics: To measure system performance, we use the well-known Weighted Speedup metric [33]: W S = values are measured on the baseline bufferless network. Weighted speedup correlates to system throughput [11] and is thus a good general metric for multiprogrammed workloads. Power Modeling: We use a modiﬁed and improved version of ORION 2.0 [38] (conﬁgured for 65nm), as developed by Grot et al. [15], as well as Verilog models synthesized with a commercial 65nm design library. We use Verilog models of router control logic, and add datapath area and power using ORION models for crossbars, buffers, links, and pipeline registers. CHIPPER and MinBD do not use a conventional crossbar, instead decoupling the crossbar into a permutation network, which we model using muxes. We rely on ORION’s modeling for the baseline input-buffered router’s control logic (e.g., arbitration). This hybrid synthesis / ORION approach models each portion of the router in a way that captures its key limitations. The control logic is logic- rather than wiringdominated, and its arbitration paths determine the critical path; hence, standard-cell synthesis will model area, power and timing of MinBD and CHIPPER control logic with reasonable accuracy. The router datapath is wiring-dominated and relies (cid:2)N i=1 I P C shared i I P C alone i . All I P C alone i on heavily-optimized components with custom layouts such as large crossbars and wide muxes, pipeline registers and memories. ORION explicitly models such router components. We report energy efﬁciency as performance-per-watt, computed as weighted speedup divided by average network power. V I . EVALUAT ION In this section, we evaluate MinBD against a bufferless deﬂection router [12] and an input-buffered router with buffer bypassing [39], [27], as well as a hybrid of these two, AFC [21], and demonstrate that by using a combination of deﬂection routing and buffering, MinBD achieves performance competitive with the conventional input-buffered router (and higher than the bufferless deﬂection router), with a smaller buffering requirement, and better energy efﬁciency than all prior designs. A. Application Performance Fig. 3 (top pane) shows application performance as weighted speedup for 4x4 (16-node) and 8x8 (64-node) CMP systems. The plots show average results for each workload category, as described in §V, as well as overall average results. Each bar group shows the performance of three input-buffered routers: 8 VCs with 8 ﬂits/VC, 4 VCs with 4 ﬂits/VC, and 4 VCs with 1 ﬂit/VC. Next is CHIPPER, the bufferless deﬂection router, followed by AFC [21], a coarse-grained hybrid router that switches between a bufferless and a buffered mode. MinBD is shown last in each bar group. We make several observations: 1. MinBD improves performance relative to the bufferless deﬂection router by 2.7% (4.9%) in the 4x4 (8x8) network over all workloads, and 8.1% (15.2%) in the highest-intensity category. Its performance is within 2.7% (3.9%) of the (4, 4) inputbuffered router, which is a reasonably-provisioned baseline, and within 3.1% (4.2%) of the (8, 8) input-buffered router, which has large, power-hungry buffers. Hence, adding a side buffer allows a deﬂection router to obtain signiﬁcant performance improvement, and the router becomes more competitive with a conventional buffered design. 2. Relative to the 4-VC, 1-ﬂit/VC input-buffered router (third bar), which is the smallest deadlock-free (i.e., correct) design, 6             MinBD performs nearly the same despite having less buffer space (4 ﬂits in MinBD vs. 16 ﬂits in (4, 1)-buffered). Hence, buffering only a portion of trafﬁc (i.e., ﬂits that would have been deﬂected) makes more efﬁcient use of buffer space. 3. AFC, the hybrid bufferless/buffered router which switches modes at the router granularity, performs essentially the same as the 4-VC, 4-ﬂit/VC input-buffered router, because it is able to use its input buffers when load increases. However, as we will see, this performance comes at an efﬁciency cost relative to our hybrid design. B. Network Power and Energy Efﬁciency Network Power: Fig. 3 (middle pane) shows average total network power, split by component and type (static/dynamic), for 4x4 and 8x8 networks across the same workloads. Note that static power is shown in the bottom portion of each bar, and dynamic power in the top portion. Each is split into buffer power, link power, and other power (which is dominated by datapath components, e.g., the crossbar and pipeline registers). We make several observations: 1. Buffer power is a large part of total network power in the input-buffered routers that have reasonable buffer sizes, i.e., (4, 4) and (8, 8) (VCs, ﬂits/VC), even with empty-buffer bypassing, largely because static buffer power (bottom bar segment) is signiﬁcant. Removing large input buffers reduces static power in MinBD as well as the purely-bufferless baseline, CHIPPER.6 Because of this reduction, MinBD’s total network power never exceeds that of the input-buffered baselines, except in the highest-load category in an 8x8-mesh (by 4.7%). 2. Dynamic power is larger in the baseline deﬂection-based router, CHIPPER, than in input-buffered routers: CHIPPER has 31.8% (41.1%) higher dynamic power than the (4, 4)-buffered router in the 4x4 (8x8) networks in the highest-load category. This is because bufferless deﬂection-based routing requires more network hops, especially at high load. However, in a 4x4 network, MinBD consumes less dynamic power (by 8.0%) than the (4, 4)-buffered baseline in the highest-load category because reduced deﬂection rate (by 58%) makes this problem relatively less signiﬁcant, and allows savings in buffer dynamic energy and a simpliﬁed datapath to come out. In an 8x8 network, MinBD’s dynamic power is only 3.2% higher. 3. MinBD and CHIPPER, which use a permutation networkbased datapath rather than a full 5x5 crossbar, reduce datapath static power (which dominates the “static other” category) by 31.0%: the decoupled permutation network arrangement has less area, in exchange for partial permutability (which causes some deﬂections). Input-buffered routers and AFC require a full crossbar because they cannot deﬂect ﬂits when performing buffered routing (partial permutability in a non-deﬂecting router would signiﬁcantly complicate switch arbitration, because each output arbiter’s choice would be limited by which other ﬂits are traversing the router). 4. AFC, the coarse-grained hybrid, has nearly the same network power as the (4, 4) buffered router at high load: 0.6% (5.7%) less in 4x4 (8x8). This is because its buffers are enabled most of the time. At low load, when it can power-gate its buffers frequently, its network power reduces. However, AFC’s 6Note that network power in the buffered designs takes buffer bypassing into account, which reduces these baselines’ dynamic buffer power. The (4, 4)buffered router bypasses 73.7% (83.4%) of ﬂits in 4x4 (8x8) networks. Without buffer bypassing, this router has 7.1% (6.8%) higher network power, and 6.6% (6.4%) worse energy-efﬁciency. network power is still higher than the pure bufferless router (CHIPPER) or MinBD because (i) it still spends some time in its buffered mode, and (ii) its datapath power is higher, as described above. On average, AFC still consumes 36.8% (18.1%) more network power than CHIPPER, and 33.5% (33.0%) more than MinBD, in the lowest-load category. Energy efﬁciency: Fig. 3 (bottom pane) shows energy efﬁciency. We make two key observations: 1. MinBD has the highest energy efﬁciency of any evaluated design: on average in 4x4 (8x8) networks, 42.6% (33.8%) better than the reasonably-provisioned (4, 4) input-buffered design. MinBD has 15.9% (8.7%) better energy-efﬁciency than the most energy-efﬁcient prior design, the (4, 1)-buffered router. 2. At the highest network load, MinBD becomes less energyefﬁcient compared to at lower load, and its efﬁciency degrades at a higher rate than the input-buffered routers with large buffers (because of deﬂections). However, its per-category energyefﬁciency is still better than all baseline designs, with two exceptions. In the highest-load category (near saturation) in an 8x8-mesh, MinBD has nearly the same efﬁciency as the (4, 1)buffered router (but, note that MinBD is much more efﬁcient than this baseline router at lower loads). In the lowest-load category in a 4x4 mesh, the purely-bufferless router CHIPPER is slightly more energy-efﬁcient (but, note that CHIPPER’s performance and efﬁciency degrade quickly at high loads). p u d e e p S d e t h g i e W  16  15  14  13  12  11  10  9  8 B ette r E fficie n c y MinBD o rs e E fficie n c y 1,1 W 4,1 2,1 1,4 1,8 2,2 1,2 CHIPPER  1  1.2  1.4  1.6 2,8 2,4 4,2 4,4 4,8 8,2 8,4 8,8 AFC Buffered  1.8 Power (W)  2  2.2  2.4  2.6 Figure 4. Power (X) vs. application performance (Y) in 4x4 networks. The line represents all points with equivalent performance-per-watt to MinBD. We conclude that, by achieving competitive performance with the buffered baseline, and making more efﬁcient use of a much smaller buffer capacity (hence reducing buffer power and total network power), MinBD provides better energy efﬁciency than prior designs. To summarize this result, we show a 2D plot of power and application performance in Fig. 4 for 4x4 networks, and a wider range of buffered router designs, as well as MinBD and CHIPPER. (Recall from §V that several of the baseline input-buffered designs are not deadlock free (too few VCs) or have a buffer depth that does not cover credit round-trip latency, but we evaluate them anyway for completeness.) In this plot, with power on the X axis and application performance on the Y axis, a line through the origin represents a ﬁxed performance-per-watt ratio (the slope of the line). This equal-efﬁciency line is shown for MinBD. Points above the line have better efﬁciency than MinBD, and points below have worse efﬁciency. As shown, MinBD presents the best energy efﬁciency among all evaluated routers. The trend in an 8x8 network (not shown for space) is similar (see technical report [13]). C. Performance Breakdown To understand the observed performance gain in more detail, we now break down performance by each component of MinBD. Fig. 5 shows performance (for 4x4 networks) averaged across all workloads for eight deﬂection systems, which constitute all possible combinations of MinBD’s mechanisms 7   y c n e t a L  25  20  15  10  5  0 Uniform Random Bit Complement Transpose Uniform Random Bit Complement Transpose 4x4 8x8  25  20  15  10  5  0  0.2  0.4  0.6 Injection Rate  25  20  15  10  5  0.2  0.4  0 Injection Rate CHIPPER  0.2  0.4  0.6  0 Injection Rate MinBD  50  40  30  20  10  0  50  40  30  20  10  0  0.1  0.2 Injection Rate Buffered (8,8)  0.1  0.2 Injection Rate  0.2  0.4 Injection Rate Buffered (4,1)  50  40  30  20 y c n e t a L  10  0.3 Figure 6. Synthetic trafﬁc evaluations for MinBD, CHIPPER and input-buffered routers (with small and large input buffers), in 4x4 and 8x8 meshes. (For the transpose pattern, the curves for both input-buffered routers overlap.) added to the baseline (CHIPPER) router: dual-width ejection (D), silver-ﬂit prioritization (S), and the side buffer (B), shown with the same three input-buffered conﬁgurations as before. The eighth bar (D+S+B), which represents all three mechanisms added to the baseline deﬂection network, represents MinBD. Table II shows deﬂection rate for the same set of systems. Figure 5. Breakdown of performance gains for each mechanism in MinBD. Table II. Average deﬂection rates for deﬂection-based routers. Baseline (CHIPPER) 0.28 D S B D+S D+B S+B 0.22 0.27 0.17 0.22 0.11 0.16 D+S+B (MinBD) 0.10 We draw three main conclusions: 1. All mechanisms individually contribute to performance and reduce deﬂection rate. Dual ejection (D) increases performance by 3.7% over baseline CHIPPER.7 Adding silver-ﬂit prioritization (D+S) increases performance by 0.7% over dualejection alone. Adding a side buffer to the other two mechanisms (D+S+B) increases performance by 2.0% on average. 2. Adding a side buffer by itself (B) is not sufﬁcient to attain the performance that MinBD achieves. In fact, when only the side buffer is added, performance drops by 4.3% relative to baseline CHIPPER. The degradation occurs primarily because the increased in-network throughput (enabled by a reduced deﬂection rate) in the network with side buffers places more tleneck that we observed in §IV-A. This performance reduction ejection pressure on nodes, which exacerbates the ejection botcomes despite a reduced deﬂection rate: even though ﬂits are not deﬂected as frequently, they must still be removed from the network efﬁciently for performance to increase. 3. Adding dual ejection to the side buffered system (D+B) to address the ejection bottleneck increases performance to 5.8% above baseline. Silver-ﬂit prioritization in addition to this conﬁguration point yields the MinBD router (eighth bar), which attains 6.6% performance above baseline (2.7% above dualejection alone) on average for all workload intensities. Overall, deﬂection rate reduces by 64% from baseline CHIPPER to MinBD (and 54% from CHIPPER with dual-ejection (D) to MinBD, as shown in our primary results). 7 The main results presented in Fig. 3 use this data point (with dual ejection) in order to make a fair (same external router interface) comparison. 8 D. Synthetic Trafﬁc Performance We study the network-level performance of MinBD and baseline designs by applying synthetic trafﬁc patterns: uniform random, bit-complement, and transpose [9]. Fig. 6 shows latency curves with injection rate swept from zero to saturation for the bufferless deﬂection router, MinBD, and the (4, 1) and (8, 8) input-buffered router (other input-buffered routers are omitted for clarity; these are the smallest and largest evaluated input-buffered routers, to show lower and upper bounds). Under uniform random trafﬁc (which most closely resembles our multiprogrammed application workloads with striped data mapping), MinBD performs better than the bufferless baseline, with a higher saturation point. MinBD has a lower network saturation point than the input-buffered network with large input buffers, but very similar saturation point to the small (4, 1) input-buffered router, as our earlier results indicated. We conclude that with only 4 ﬂits of buffering per router, MinBD closes nearly half of the gap in network saturation throughput between the bufferless router (CHIPPER) and the largest input-buffered router (with 256 ﬂits of total buffer space), and performs similarly to a smaller input-buffered router with 16 ﬂits of total buffer space. In addition, non-uniform trafﬁc patterns demonstrate the robustness and adaptivity of deﬂection routing: in particular, the transpose trafﬁc pattern demonstrates a lower saturation point in the input-buffered router than either deﬂection-based router (MinBD or CHIPPER). This advantage occurs because deﬂection routing is adaptive (a ﬂit’s path can change based on network conditions). Deﬂections spread trafﬁc away from hotspots and balance load in unbalanced trafﬁc patterns. While adaptive routing is also possible in input-buffered routers, it is more complex because it requires the router to track network load or congestion (locally or globally) and make decisions accordingly. In contrast, deﬂection routing provides adaptivity by virtue of its ordinary operation. E. Sensitivity to Side Buffer Size As side buffer size is varied from 1 to 64 ﬂits, mean weighted speedup (application performance) changes only 0.2% on average across all workloads (0.9% in the highest-intensity category) in 4x4 networks. We conclude that the presence of the buffer (to buffer at least one deﬂected ﬂit) is more important than its size, because the average utilization of the buffer is low. In a 4x4 MinBD network with 64-ﬂit side buffers at saturation (61% injection rate, uniform random), the side buffer is empty 48% of the time on average; 73% of the time, it contains 4 or fewer ﬂits; 93% of the time, 16 or fewer. These measurements suggest that a very small side buffer captures most of the beneﬁt. Furthermore, total network power increases by 19% (average across all 4x4 workloads) when a 1-ﬂit buffer per router is increased to a 64-ﬂit buffer per router. Hence, a larger buffer wastes power without signiﬁcant performance beneﬁt. Router Design CHIPPER MinBD Buffered (8, 8) Buffered (4, 4) Buffered (4, 1) Normalized Area 1.00 1.03 2.06 1.69 1.60 Normalized Critical Path Length 1.00 1.07 0.99 0.99 0.99 Table III. Normalized router area and critical path for bufferless and buffered baselines, compared to MinBD. We avoid a 1-ﬂit side buffer because of the way the router is pipelined: such a single-ﬂit buffer would either require for a ﬂit to be able to enter, then leave, the buffer in the same cycle (thus eliminating the independence of the two router pipeline stages), or else could sustain a throughput of one ﬂit only every two cycles. (For this sensitivity study, we optimistically assumed the former option for the 1-ﬂit case.) The 4-ﬂit buffer we use avoids this pipelining issue, while increasing network power by only 4% on average over the 1-ﬂit buffer. F. Hardware Cost: Router Area and Critical Path We present normalized router area and critical path length in Table III. Both metrics are normalized to the bufferless deﬂection router, CHIPPER, because it has the smallest area of all routers. MinBD adds only 3% area overhead with its small buffer. In both CHIPPER and MinBD, the datapath dominates the area. In contrast, the large-buffered baseline has 2.06x area, and the reasonably-provisioned buffered baseline has 1.69x area. Even the smallest deadlock-free input-buffered baseline has 60% greater area than the bufferless design (55% greater than MinBD). In addition to reduced buffering, the reduction seen in CHIPPER and MinBD is partly due to the simpliﬁed datapath in place of the 5x5 crossbar (as also discussed in §VI-B). Overall, MinBD reduces area relative to a conventional input-buffered router both by signiﬁcantly reducing the required buffer size, and by using a more area-efﬁcient datapath. Table III also shows the normalized critical path length of each router design, which could potentially determine the network operating frequency. MinBD increases critical path by 7% over the bufferless deﬂection router, which in turn has a critical path 1% longer than an input-buffered router. In all cases, the critical path is through the ﬂit arbitration logic (the permutation network in MinBD and CHIPPER, or the arbiters in the input-buffered router). MinBD increases critical path relative to CHIPPER by adding logic in the deﬂection-routing stage to pick a ﬂit to buffer, if any. The buffer re-injection and redirection logic in the ﬁrst pipeline stage (eject/inject) does not impact the critical path because the permutation network pipeline stage has a longer critical path. V I I . R E LAT ED WORK To our knowledge, MinBD is the ﬁrst NoC router design that combines deﬂection routing with a small side buffer that reduces deﬂection rate. Other routers combine deﬂection routing with buffers, but do not achieve the efﬁciency of MinBD because they either continue to use input buffers for all ﬂits (Chaos router) or switch all buffers on and off at a coarse granularity with a per-router mode switch (AFC), in contrast to MinBD’s ﬁne-grained decision to buffer or deﬂect each ﬂit. Buffered NoCs that also use deﬂection: Several routers that primarily operate using buffers and ﬂow control also use deﬂection routing as a secondary mechanism under high load. The Chaos Router [23] deﬂects packets when a packet queue becomes full to probabilistically avoid livelock. However, all packets that pass through the router are buffered; in contrast, MinBD performs deﬂection routing ﬁrst, and only buffers some ﬂits that would have been deﬂected. This key aspect of our design reduces buffering requirements and buffer power. The Rotary Router [1] allows ﬂits to leave the router’s inner ring on 9 a non-productive output port after circulating the ring enough times, in order to ensure forward progress. In this case, again, deﬂection is used as an escape mechanism to ensure probabilistic correctness, rather than as the primary routing algorithm, and all packets must pass through the router’s buffers. Other bufferless designs: Several prior works propose bufferless router designs [12], [28], [37], [14], [17]. We have already extensively compared to CHIPPER [12], from which we borrow the deﬂection routing logic design. BLESS [28], another bufferless deﬂection network, uses a more complex deﬂection routing algorithm. Later works showed BLESS to be difﬁcult to implement in hardware [12], [17], [27], thus we do not compare to it in this work. Other bufferless networks drop rather than deﬂect ﬂits upon contention [14], [17]. Some earlier large multiprocessor interconnects, such as those in HEP [31] and Connection Machine [18], also used deﬂection routing. The HEP router combined some buffer space with deﬂection routing [32]. However, these routers’ details are not wellknown, and their operating conditions (large off-chip networks) are signiﬁcantly different than those of modern NoCs. Improving high-load performance in bufferless networks: Some work has proposed congestion control to improve performance at high network load in bufferless deﬂection networks [29], [7]. Both works used source throttling: when network-intensive applications cause high network load which degrades performance for other applications, these intensive applications are prevented from injecting some of the time. By reducing network load, source throttling reduces deﬂection rate and improves overall performance and fairness. These congestion control techniques and others (e.g., [36]) are orthogonal to MinBD, and could improve MinBD’s performance further. Hybrid buffered-bufferless NoCs: AFC [21] combines a bufferless deﬂection router based on BLESS [28] with input buffers, and switches between bufferless deﬂection routing and conventional input-buffered routing based on network load at each router. While AFC has the performance of buffered routing in the highest-load case, with better energy efﬁciency in the low-load case (by power-gating buffers when not needed), it misses opportunity to improve efﬁciency because it switches buffers on at a coarse granularity. When an AFC router experiences high load, it performs a mode switch which takes several cycles in order to turn on its buffers. Then, it pays the buffering energy penalty for every ﬂit, whether or not it would have been deﬂected. It also requires buffers as large as the baseline input-buffered router design in order to achieve equivalent highload performance. As a result, its network power is nearly as high as a conventional input-buffered router at high load, and it requires ﬁne-grained power gating to achieve lower power at reduced network load. In addition, an AFC router has a larger area than a conventional buffered router, because it must include both buffers and buffered-routing control logic as well as deﬂection-routing control logic. In contrast, MinBD does not need to include large buffers and the associated bufferedmode control logic, instead using only a smaller side buffer. MinBD also removes the dependence on efﬁcient buffer powergating that AFC requires for energy-efﬁcient operation at low loads. We quantitatively compared MinBD to AFC in §IV and demonstrated better energy efﬁciency at all network loads. Reducing cost of buffered routers: Empty buffer bypassing [39], [27] reduces buffered router power by allowing ﬂits to bypass input buffers when empty. However, as our evaluations (which faithfully model the power reductions due to buffer bypassing) show, this scheme reduces power less than our new router design: bypassing is only effective when buffers are empty, which happens more rarely as load increases. Furthermore, buffers continue to consume static power, even when unused. Though both MinBD and empty-buffer-bypassed buffered routers avoid buffering signiﬁcant trafﬁc, MinBD further reduces router power by using much smaller buffers. Kim [22] proposed a low-cost buffered router design in which a packet uses a buffer only when turning, not when traveling straight along one dimension. Unlike our design, this prior work does not make use of deﬂection, but uses deterministic X-Y routing. Hence, it is not adaptive to different trafﬁc patterns. Furthermore, its performance depends signiﬁcantly on the size of the turn-buffers. By using deﬂection, MinBD is less argued in §VI-E. In addition, [22] implements a token-based dependent on buffer size to attain high performance, as we injection starvation avoidance scheme which requires additional communication between routers, whereas MinBD requires only per-router control to ensure side buffer injection. V I I I . CONC LU S ION In this paper, we present MinBD, a new, minimally-buffered deﬂection router design. MinBD combines deﬂection routing with a small buffer, such that some network trafﬁc that would have been deﬂected is placed in the buffer instead. By using the buffer for only a fraction of network trafﬁc, MinBD makes more efﬁcient use of a given buffer size than a conventional input-buffered router. Its average network power is also greatly reduced: relative to an input-buffered router, buffer power is much lower, because buffers are smaller. Relative to a bufferless deﬂection router, dynamic power is lower, because deﬂection rate is reduced with the small buffer. Our evaluations show that MinBD performs competitively with the input-buffered router design, and has the best energy efﬁciency of all evaluated networks. We conclude that a router design which augments bufferless deﬂection routing with a small buffer to reduce deﬂection rate is a compelling design point for energy-efﬁcient, high-performance on-chip interconnect. ACKNOW L EDGM EN T S We thank the anonymous reviewers for their feedback. We gratefully acknowledge members of the SAFARI group and Michael Papamichael at CMU for feedback. Chris Fallin is currently supported by an NSF Graduate Research Fellowship. Greg Nazario and Xiangyao Yu were undergraduate research interns while this work was done. We acknowledge the generous support of AMD, Intel, Oracle, and Samsung. This research was partially supported by an NSF CAREER Award CCF-0953246. "Fine-Grained Bandwidth Adaptivity in Networks-on-Chip Using Bidirectional Channels.,"Networks-on-Chip (NoC) serve as efficient and scalable communication substrates for many-core architectures. Currently, the bandwidth provided in NoCs is over provisioned for their typical usage case. In real-world multi-core applications, less than 5% of channels are utilized on average. Large bandwidth resources serve to keep network latency low during periods of peak communication demands. Increasing the average channel utilization through narrower channels could improve the efficiency of NoCs in terms of area and power, however, in current NoC architectures this degrades overall system performance. Based on thorough analysis of the dynamic behaviour of real workloads, we design a novel NoC architecture that adapts to changing application demands. Our architecture uses fine-grained bandwidth-adaptive bidirectional channels to improve channel utilization without negatively affecting network latency. Running PARSEC benchmarks on a cycle-accurate full-system simulator, we show that fine-grained bandwidth adaptivity can save up to 75% of channel resources while achieving 92% of overall system performance compared to the baseline network, no performance is sacrificed in our network design configured with 50% of the channel resources used in the baseline.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Fine-Grained Bandwidth Adaptivity in Networks-on-Chip Using Bidirectional Channels Robert Hesse, Jeff Nicholls and Natalie Enright Jerger Department of Electrical and Computer Engineering, University of Toronto Toronto, Ontario, Canada Email: {hesserob, nichol79, enright}@eecg.toronto.edu Abstract—Networks-on-Chip (NoC) serve as efﬁcient and scalable communication substrates for many-core architectures. Currently, the bandwidth provided in NoCs is overprovisioned for their typical usage case. In real-world multi-core applications, less than 5% of channels are utilized on average. Large bandwidth resources serve to keep network latency low during periods of peak communication demands. Increasing the average channel utilization through narrower channels could improve the efﬁciency of NoCs in terms of area and power; however, in current NoC architectures this degrades overall system performance. Based on thorough analysis of the dynamic behaviour of real workloads, we design a novel NoC architecture that adapts to changing application demands. Our architecture uses ﬁne-grained bandwidth-adaptive bidirectional channels to improve channel utilization without negatively affecting network latency. Running PARSEC benchmarks on a cycle-accurate full-system simulator, we show that ﬁne-grained bandwidth adaptivity can save up to 75% of channel resources while achieving 92% of overall system performance compared to the baseline network; no performance is sacriﬁced in our network design conﬁgured with 50% of the channel resources used in the baseline. I . IN TRODUC T ION To meet the growing communication demands of future many-core applications, architectures will employ networkson-chip (NoC) to provide a scalable high-bandwidth, lowlatency communication substrate. Recent research demonstrates that there is no one size ﬁts all NoC architecture [12]. While chip multiprocessor (CMP) workloads behave inherently dynamically, there is little work that focuses on designing a network to accommodate and respond to this properly. For example, the bandwidth demands of the network can vary throughout the execution of a single application, yet the communication infrastructure remains a ﬁxed, rigid design. Prior work [12] focuses on inter-application variations and proposes a network that can adapt at a very coarse time granularity. In addition to coarse-grained changes in communication demand, we observe signiﬁcant intra-application variation. Applications and trafﬁc patterns are not ﬁxed and vary heavily over time. A NoC implementation that provides superior communication performance for one trafﬁc scenario might be underutilized or over-provisioned at a later point in the execution. 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.23 132 Figure 1. Channel utilization of uniform random trafﬁc in a 4 × 4 mesh network Figure 2. Channel utilization of Facesim trafﬁc in a 4 × 4 mesh network To efﬁciently utilize hardware resources and improve overall system performance, NoCs need to dynamically adapt to changing trafﬁc patterns. Although average injection rates for NoCs are generally low, today’s NoCs are designed to tolerate peak trafﬁc loads in order to avoid becoming the performance bottleneck during periods with high volumes of communication. The bandwidth resources of such a NoC are statically provisioned at design time with only limited knowledge about application requirements. This work explores whether a dynamically adapting communication infrastructure could be designed for the average case without losing performance during phases of high trafﬁc volume. widely considered to be abundant, there are constraints and limitations. Among these are the ability to route large numbers of wires. It is difﬁcult to route wires over dense logic which limits the placement and number of wires [10]. The metal layers devoted to interconnect wiring are also limited. Fewer wires can ease the layout and routing of NoCs. Finally, the wires that become available could also be used for different, performance enhancing purposes. Experimental evaluations on a cycle-accurate full-system simulator show that our solution signiﬁcantly decreases a NoC’s channel resource requirements while maintaining overall system throughput. To summarize, the main contributions of this paper are the following: • We analyze NoC bandwidth demands of PARSEC applications and observe that average bandwidth requirements are low. We observe that bandwidth requirements change dynamically during runtime and channel resources are generally over-provisioned. • We propose a novel, ﬁne-grained mechanism to dynamically adapt a NoC’s bandwidth to the current bandwidth demands and increase utilization without compromising network latency. We decouple ﬂit width from channel width and thereby signiﬁcantly reduce the required channel resources without incurring the increased serialization delay that normally accompanies narrow channels. • We compare our proposal to previous adaptive bandwidth solutions [3], [13]. Our solution can decrease the channel resources of a 4x4 mesh NoC by 50% without sacriﬁcing overall system performance and by 75% while still achieving 92% of the baseline performance. I I . MOT IVAT ION To understand the opportunities for an adaptive, reducedresource network, we ﬁrst analyze the behaviour of several CMP applications. Next, we analyze the impact of naively reducing channel width. A. Concept of Bandwidth Adaptivity  $	"" $	""                )* )% (* (% '* '% &* &% * % Previous analysis [1], [7] of different single-threaded (SPEC CPU2000 [8]) and multi-threaded workloads (SPLASH-2 [20], PARSEC [2]) for CMPs reveals very low average injection rates, generally under 5%, which is consistent with our own simulations. With such low injection rates, a dynamically adapting NoC could share resources across the network to improve performance and make the network more resistant to hardware failures. NoC designs are often evaluated using synthetic trafﬁc patterns, such as uniform random or tornado trafﬁc [4]. These patterns have constant packet injection rates and regular communication patterns; they stress the network and analyze performance under known, well-structured conditions. For example, Fig. 1 shows the channel utilization for uniform random trafﬁc over 100 million cycles on 16 cores with a 4x4 2D mesh NoC1 . The channel utilization for each of the 48 channels is sampled periodically; since this is a synthetic trafﬁc pattern, with a ﬁxed injection rate, there is modest variation across channels but no temporal variation on a given channel. However, real multi-threaded CMP applications do not exhibit the same regularity. Instead they have highly dynamic communication patterns. For example, Fig. 2 shows the channel utilization for Facesim, a PARSEC application [2]. Here, we observe substantial variations both across channels and in time for a single channel. The network’s performance with such dynamic, realistic trafﬁc will differ substantially from the same network handling synthetic trafﬁc patterns. Both temporal and spatial variations in application trafﬁc can impact interconnect and overall system performance; NoC designs that adapt to these variations will save power and area, producing more efﬁcient implementations. In this paper, we explore adapting a NoC’s bandwidth resources to the dynamically changing bandwidth demands of its applications. We introduce ﬁne-grained bandwidthadaptivity, which is provided by replacing the usual pair of unidirectional channels between two routers with narrow bidirectional channels. These channels can be dynamically conﬁgured in accordance with current bandwidth demands. We propose a novel router architecture to monitor the network’s bandwidth demands and to switch the channels accordingly. We target the optimization of link resources for several reasons. First, channel width has a signiﬁcant impact on the router area as wide links require wider crossbars and crossbar area grows quadratically with port width. Router area can be similar in size to core area [9]; saving router area can allow designs to add additional cores, increase core complexity or add additional caches. There are complex tradeoffs between these components and system performance; an area-efﬁcient router design can improve the design of the overall system. Second, while bandwidth on chip is  !     !     1Details about the simulation setup and methodology are in Sec. V. Figure 3. Channel utilization for PARSEC using the baseline 2D mesh NoC 133 	 	      	 To analyze the bandwidth demands of common CMP applications, we measure the channel utilization of different PARSEC benchmarks executed on a 16-core CMP. For a period of 100 million cycles, we determine the percentage of cycles each channel in a 4x4 2D mesh NoC is utilized. Sufﬁcient buffers and virtual channels are provided to eliminate all bottlenecks in the network; thus, maximal channel utilization can be achieved. Fig. 3 shows that the average channel utilization does not exceed 7% for any benchmark and is generally much lower. Although the maximum channel utilization is considerably higher and reaches 43% for Streamcluster, most of the time channels are idle and the network is overprovisioned in terms of its bandwidth resources. To reduce the channel resources and therefore increase their utilization, we propose to combine the data trafﬁc of two unidirectional channels into one narrower bidirectional channel.     (a) Two unidirectional channels        (b) One bidirectional channel (c) Four bidirectional channels Figure 4. The concept of bandwidth adaptivity In conventional NoCs, every two neighbouring routers are connected by two unidirectional channels of width b, as Fig. 4a illustrates. This design allows for one ﬂit of width b bits to be sent in each direction concurrently per cycle providing a bandwidth of 2b bits per cycle. Since the channels are idle most of the time, it makes sense to combine their trafﬁc on one bidirectional channel, which has the ability to transfer ﬂits in only one direction at a time. Fig 4b shows how one bidirectional channel of width b bits can replace the two unidirectional channels providing a bandwidth of b bits per cycle. The bidirectional channel switches its direction according to the bandwidth demands of the attached routers. In order to be able to transmit data simultaneously in both directions, the bidirectional channel can be split up into a number n channels, each with width b/n bits, which can switch their direction independently; this arrangement still provides a total bandwidth of b bits per cycle, but allows for a ﬁne-grained bandwidth-adaptive channel between two adjacent routers, as shown in Fig. 4c. B. Advantages of Exploiting Bandwidth Adaptivity Instead of combining the bandwidth of two unidirectional channels into bidirectional channels, one could also narrow the width of the unidirectional channels, to b/2 for example, as a simpler solution to increase channel utilization and save resources. Narrower channels, however, have the negative side effect of increasing the number of ﬂits in a packet, if we assume ﬁxed packet sizes and a ﬂit width corresponding to the channel width. Increasing the number of ﬂits per packet leads to a signiﬁcantly worse latency across all benchmarks due to increased serialization delay and congestion in the network, as Fig. 5 demonstrates for varying channel widths in our baseline NoC. To deliver high system performance, low communication latency is imperative; 2-byte wide links incur unacceptably high latency which degrades overall performance.             (  '  &  %  $  #  ""  !    		    ( $ ""  	  	    	 	   Figure 5. Average packet latency with different unidirectional channel widths Additional delays due to increased packet length can be avoided by using bidirectional channels. Since the effective channel width per direction remains unchanged, the ﬂit width can stay the same while the total channel resources are reduced. Therefore bidirectional channels allow for increased channel utilization without negatively affecting the length of a packet, which corresponds to greater serialization delay and latency due to congestion. However, while a bidirectional channel is transmitting a packet in one direction, another packet that might require bandwidth in the opposite direction would be stalled and additional delays would be introduced. We mitigate this added delay by allowing the simultaneous transfer of ﬂits in both directions through narrower channels, which independently switch their direction so that packets do not need to wait for a transmission in the opposite direction to ﬁnish. In order to transfer a ﬂit through narrower channels without changing its width, we divide a ﬂit into smaller subunits (phits) of width b/n for transmission across the channel. This is called phit-serial communication and is explained in more detail in Sec. III-A. I I I . BANDW ID TH -ADA P T IV E CHANN E L ARCH I T EC TUR E In this section, we present our modiﬁcations to the router architecture to support ﬁne-grained bandwidth adaptivity. In a standard virtual channel (VC) router, every output port is connected to an input buffer of a neighbouring router using a unidirectional channel. In our solution, the two unidirectional channels are combined into a set of bidirectional channels that can independently be driven from either one of the adjacent routers. Control logic and tristate 134 	                                   	                              Figure 6. Modiﬁcations to standard VC router buffers ensure that no channel is driven from both sides simultaneously. The necessary modiﬁcations to a standard VC router are shown in gray in Fig. 6 and will be discussed in the following sections. The remaining modules of the router, including allocation and ﬂow control are unchanged. A. Phit-Serial Communication Conventionally, the width of a ﬂit is considered to be identical to the channel width in NoCs. This allows for the transmission of exactly one ﬂit per channel every clock cycle. Therefore, the bandwidth between two adjacent routers is ﬁxed to one ﬂit in each direction per cycle. Because the bandwidth demands of a NoC change dynamically throughout application execution, a replacement of the ﬁxed bandwidth structure with a more ﬂexible solution is necessary. Previous approaches [13], [3] have introduced ﬂexible bandwidth allocation by adding more channels between routers, which enable the potential transmission of multiple ﬂits per cycle. These designs add signiﬁcant resources to the baseline architecture. Since channel width is directly coupled to the ﬂit width, a reduction of the over-provisioned bandwidth resources in these cases is only possible if the ﬂit width is reduced at the same time. However, reducing the ﬂit width leads to increased packet lengths and therefore increases the packet latency. To provide bandwidth ﬂexibility without increasing channel resources or harming communication performance by increasing packet serialization latency, we decouple the ﬂit width from the channel width. Flits are subdivided into smaller units called phits (physical transfer unit)2 . One phit represents the amount of data that can be transferred between two routers on one channel in one cycle. Flow control (e.g. buffer allocation) between routers is still managed at the granularity of a ﬂit, which allows us to change the channel width independently of ﬂit and buffer widths. Hence, we can reduce the channel width while keeping the packet length constant. Each ﬂit is broken down into multiple phits; phits are transferred sequentially across a narrow channel and reassembled on the receiving end. To reduce the serialization delay across the channel, multiple narrow channels can be 2 Phits are common in off-chip networks which are often constrained by limited pin bandwidth. used in parallel. The key difference between phits used in off-chip networks and our design is that multiple phits can proceed in parallel between two adjacent routers. Every channel is bidirectional and able to switch direction independently; bandwidth-adaptivity at the granularity of single phits is provided. Fig. 7 walks through an example of phit-serial communication between two adjacent routers. While Fig. 7a shows the entire bidirectional input and output port setup, subsequent ﬁgures only show communication in one direction for brevity. The ﬁgures only show the input stages (IS) and output stages (OS) of our design. Three 8-byte ﬂits (Flit A, B, and C) are divided into four 2-byte phits each, which are transferred across four bidirectional 2-byte channels. The transmission begins with an empty OS and IS; initially, the channels are conﬁgured with 3 channels pointing to the right and 1 pointing to the left (3:1) as shown in Fig. 7a. During the next clock cycle (Fig. 7b), Flit A arrives at the OS and the serializer divides it into four 2-byte phits (labeled F:A, P:x where x is the phit number) after bypassing the output buffer. Next, three of Flit A’s phits are transferred across the three available channels to the deserializer of the downstream router where they are temporarily stored until the ﬂit is reassembled. The remaining phit of A, (F:A, P:3) is shifted up in the serializer. At the same time, Flit B enters the OS and is written directly into the serializer as shown in Fig. 7c. The channel conﬁguration changes during the following clock cycle (2:2), so that only two phits can be transferred to the downstream router (Fig. 7d). Transferring the last phit of Flit A completes its reassembly in the deserializer, so it leaves the IS at cycle 4 (Fig. 7e). The proposed mechanism is able to adjust the number of channels in a particular direction on a per-cycle basis, while it guarantees in-order delivery of phits as well as ﬂits without introducing additional delays aside from a potential serialization delay, which occurs whenever the available bandwidth is smaller than the ﬂit width. The key to avoiding additional delays due to gaps between phits in the serializer and deserializer modules is a proper alignment of the phits towards the available bidirectional channels, while the number of transferred phits can change every cycle. When phits are allocated less channels, the packet will experience extra serialization; however as allocation is based on demand, this extra serialization does not impact performance. We avoid additional delays due to buffering by bypassing input and output buffers whenever they are empty. Output buffers are only used when the serializer is full. Fig. 8 demonstrates a low-overhead hardware implementation of our proposed phit-serial communication concept in which the proper phit alignment is accomplished by a barrel shifter in conjunction with an adjustable shift register. In our example, there can be a varying number of active channels at any given time. Whenever phits are present in one of the registers that are attached to an active channel, 135                       	       	  	                                  	              	     	               !               ""      	 (a) Clock cycle: 0; 3 channels to the right; 1 channel to the left   	     	                                                  (b) Clock cycle: 1; 3 channels to right; 1 channel to left   	     	                                                              (c) Clock cycle: 2; 3 channels to right; 1 channel to left   	     	                                                                          (d) Clock cycle: 3; 2 channels to right; 2 channel to left (e) Clock cycle: 4; 2 channels to right; 2 channel to left Figure 7. Concept of phit-serial transmission between OS and IS   	     	                                                                	     	    	  	                                  	    	  	  	  	  	  	  	 Figure 8. Implementation of the adaptive serializer/deserializer they are transferred to the IS of the downstream router. In order for the input registers to accept the new phits, any phits that were received during the previous cycle need to be shifted out of the registers that are connected to one of the channels. If all phits of an entire ﬂit have been assembled in the shift register, the reassembled ﬂit can be transferred into the input buffer. Because the reassembled ﬂit can be at any position within the shift register, we need a barrel shifter to align the ﬂit properly with the input buffer. In the meantime, the shift register on the sending side shifts forward by the exact number of phits that were sent out during the previous cycle, so that the registers, which are attached to the channels are always supplied with new phits as long as phits remain within the OS. As soon as an entire ﬂit ﬁts into the shift register, the next ﬂit is read from the output buffer, aligned through the barrel shifter and written into the shift register. Our proposed bandwidth-adaptive serializer/deserializer mechanism is able to manage an arbitrary change in the amount of available bandwidth on a cycle-per-cycle basis, while avoiding additional delays through proper phit alignment and buffer bypassing. This allows for an efﬁcient and low-overhead solution to decouple ﬂit width from channel width and adjust the bandwidth between every two adjacent routers in a ﬁne-grained manner. The overhead of our solution will be evaluated in Sec. V-D. 136           B. Bandwidth Allocation Each set of bidirectional channels connecting two adjacent routers contains a channel allocator module, positioned between the two routers as shown in Fig. 8. This module controls the direction of each individual channel by sending a control signal to the direction control logic in each router port. In order to generate this control signal, the allocator module utilizes a pressure-based control mechanism which takes as input the number of phits waiting to use either output port. The phit count is sent as sideband signal halfway across the channel. The allocator module then determines the next cycle channel allocation. The number of channels allocated in each direction depends on the phit count ratio between the two routers. The larger the difference between the phit counts, the more bandwidth allocated in one direction. In our example with 4 bidirectional channels, 2 channels are allocated in each direction if the two phit counts are equal. If one phit count is larger while the other is non-zero, 3 channels are allocated. All 4 channels are allocated in one direction only if one output port contains phits while the other does not; this policy is required for deadlock avoidance (Sec. III-C). We compared this pressure-based approach to different metrics, such as the ratio of credit counts between two routers and the ratio of sent and received ﬂits, but found our solution to be superior, since it directly expresses the bandwidth requirements of a particular connection. As described in Sec V, we implement the channel allocator module without affecting the critical path of the router. By removing bandwidth allocation from the critical path, we can adjust the channel conﬁguration on a cycle-by-cycle basis, which allows us to match the bandwidth demands of a given trafﬁc pattern. C. Deadlock Avoidance The introduction of bidirectional channels can theoretically cause network deadlock due to loss of connectivity; this type of deadlock cannot be prevented using a deadlockfree routing algorithm, such as DOR. Since bidirectional channels can potentially cause connections to disappear temporarily during runtime, the network might end up not being fully connected at all times. To prevent deadlock, we need to provide full connectivity when needed. Whenever at least one phit is ready to be sent to a downstream router, one bidirectional channel will be reserved in the respective direction. The channel cannot switch its direction as long as there are phits available in the OS. This mechanism ensures that every ﬂit in the network can progress to the next router, due to adaptively providing full connectivity. The beneﬁt of this adaptive full connectivity is that the entire bandwidth of all channels can still be allocated in one direction provided that no phits are ready for transmission in the opposite direction. IV. QUA L I TAT IV E COM PAR I SON O F BANDW ID TH -ADA P T IV E D E S IGN S Although bidirectional channels have been previously proposed [3], [13], our solution represents a novel architecture with the objective of reducing resources that compliments this design space. In this section, we discuss the qualitative differences between the concepts and show why our proposal presents a signiﬁcant improvement over both previous approaches. We present quantitative comparisons in Sec. V. BiNoC [13] introduces a bidirectional channel mechanism, which consists of exactly two bidirectional links between adjacent routers instead of the standard twounidirectional-links implementation. Since only two bidirectional links are available between each pair of routers and direction switching involves a signiﬁcant delay, only some coarse-grained bandwidth adaptivity is provided. Moreover, the ability to switch the direction of channels may lead to deadlock, which the paper does not address. A bandwidth-adaptive implementation by Cho et al. [3] reﬁnes the idea of bidirectional channels by using ﬁne-grained adaptive, pressure-based control of multiple channels. This design is able to hide BiNoC’s direction-switching delays and deals with the additional deadlock possibilities. However, this additional ﬂexibility comes at the cost of signiﬁcant area and timing overheads. Large crossbars and complicated arbitration logic are necessary to support multiple channels per port, which are all connected to the crossbar. Our solution mitigates the additional overhead by keeping the number of crossbar ports stable and solely adjusting the bandwidth distribution between adjacent routers. Both papers follow a common strategy of increasing bandwidth capacity; BiNoC [13] provides up to two times the baseline bandwidth in one direction. Cho et al. [3] increase the number of physical channels; multiples of the baseline bandwidth are available for sending in each direction. This strategy is questionable given our observation that bandwidth resources are mostly underutilized in real-world applications and thus additional bandwidth resources do not improve application performance. Our efforts, instead, are focused on reducing NoC bandwidth resources. V. EVA LUAT ION We evaluate our design of a bandwidth-adaptive router (BAR) using both synthetic and real workloads, comparing it to a typical virtual-channel router (STANDARD), a BiNoC router [13] (BINOC), and an existing bandwidth-adaptive router design [3] (BWADAPTIVE). We use FeS2 [16] for full-system x86 simulation with BookSim [4] for a detailed, cycle-accurate NoC model. We execute all PARSEC benchmarks with 16 threads on a 16core CMP, consisting of Intel Pentium 4-like CPUs, using the simmedium input set, executed for 100 million cycles within the regions of interest (ROI). The system runs Fedora 137 Table I S IMU LAT ION PARAM E T ER S # of Cores L1 Cache (D & I) L2 Cache Cache Coherence Network 16 private, 4-way, 32KB each, 64 Byte Blocks private, 8-way, 512KB each, 64 Byte Blocks MOESI distributed directory 4x4 2D-Mesh, DOR Routing ROU T ER CON FIGURAT ION S FOR OUR EX P ER IM EN T S Table II Architecture Total # Buf. Total Channels Each Buf. Size Total Buf. Size Crossbar STANDARD 5 5-in 5-out 32 ﬂits 160 ﬂits 5x5 BINOC 10 10-inout 16 ﬂits 160 ﬂits 10x10 BWADAPT. 20 20-inout 8 ﬂits 160 ﬂits 20x20 BAR 10 20-inout 16 ﬂits 160 ﬂits 5x5 Core Linux and each thread is explicitly pinned to one of the cores to eliminate thread scheduling side effects. Table I gives the simulation conﬁguration. Each core has private, inclusive L1 and L2 caches with a MOESI directory protocol. We use a 4x4 2D mesh NoC for communication; however, our solution is applicable to any topology that is based on bidirectional data transfer between neighbouring nodes. We use a 2D mesh because of its simplicity and popularity within the research community. In our baseline NoC, we use a standard virtual-channel (VC) router [18] with 2 pipeline stages, lookahead routing, speculative switch allocation, 2 VCs per port and a baseline unidirectional channel width of 8 bytes. A. Area Comparison We use ORION2.0 [11] to compare the area of STANDARD, BINOC, and BWADAPTIVE to our bandwidthadaptive router (BAR) for a 45 nm technology process. All routers are modelled using equal buffer resources and identical ﬂit widths of 8 bytes. BWADAPTIVE and BAR use 4 bidirectional channels. Table II summarizes the router conﬁgurations used for comparison and Fig. 9 shows a breakdown of the area for each router.                         ""$ ""# ""! & % $ # !      	    Figure 9. Router area comparison 138 The area for both BINOC and BWADAPTIVE is signiﬁcantly larger than that of STANDARD and BAR. Both designs increase the bandwidth adaptivity of the network by providing additional channels which leads to signiﬁcant area penalties. Next, we equalize the area of all routers to provide a fair performance comparison. We reduce the ﬂit size to 4 bytes for BINOC and 2 bytes for BWADAPTIVE for all further experiments. The area results for these conﬁgurations are shown in Fig. 10. It should be noted that our adaptive serializer/deserializer (SerDes) architecture does not add signiﬁcant area to the typical VC router, which was an important criteria for our design. A detailed analysis of the implementation overhead is presented in Sec. V-D.                         # ""' ""& ""% ""$ ""      	    Figure 10. Router area with reduced ﬂit size for BINOC and BWADAPTIVE B. Network Performance Comparison In Fig. 11, we compare the latency and throughput under uniform random, transpose, shufﬂe and bit-complement trafﬁc for the 4 router designs in a 8x8 mesh with equalized areas. We simulate a 8x8 mesh to demonstrate the scalability of our design to network sizes larger than 4x4. Furthermore, we use a packet size of 32 bytes, since we measured this to be the average packet size across all PARSEC benchmarks. For synthetic trafﬁc without temporal variation in bandwidth demands, BAR performs comparably to STANDARD. These routers have identical steady-state latency due to equallysized crossbar and buffers. BINOC and BWADAPTIVE improve the saturation throughput over STANDARD due to more available channels, especially for highly unbalanced trafﬁc patterns like transpose and shufﬂe. Both suffer from increased serialization delays caused by narrower channels resulting in higher zero-load latency. BAR keeps serialization latency low because the ﬂit size and therefore the packet size remains unchanged. At low injection rates, which are the common operating point for NoCs, BAR outperforms both other bandwidth-adaptive designs. C. Overall System Performance Synthetic trafﬁc patterns fail to demonstrate the full ﬂexibility of BAR. Therefore, we measure the overall system performance of PARSEC to gain deeper insight into the        	        	                                                      	           	      (a) Uniform Random  	         	                                                          	        	    (b) Transpose  	         	    (c) Shufﬂe (d) Bit-Complement Figure 11. Latency-throughput graphs under synthetic trafﬁc               , +'2 +'1 +'0 +'/  	( -$1 	(.$. (-$1 	  	(1$-  	( -$. (-$. 	(.$- 	  	(1$,  	( -$- (-$- 	  	(.$, Figure 12. Performance of PARSEC benchmarks with different channel conﬁgurations  %    ! % ! # "" "" dynamic real-world behaviour of our ﬁne-grained bandwidth adaptivity. Fig. 12 presents the normalized average instructions per cycle (IPC) across all 16 cores as a metric for application performance. All results are normalized to a STANDARD router with 8 byte unidirectional links. Each group of bars represents a different total channel bandwidth between two adjacent routers. For a standard VC router with two 4-byte unidirectional channels (STANDARD/2x4) the total bandwidth would be 8 bytes. BAR with four 4byte bidirectional channels (BAR/4x4) has 16 bytes of total bandwidth. The performance of STANDARD and BINOC decreases width results in ∼10% loss in performance. Using a quarter as the channel resources are reduced. Halving the channel of the original bandwidth results in more than 20% performance loss on average with a maximum loss of 30% for 3 of the workloads: canneal, streamcluster and vips. However, BAR has almost no reduction in application performance (99.3% on average) when the bandwidth is decreased by half (8 bytes). With only 4 bytes of bandwidth between adjacent routers (a 75% reduction in channel resources), BAR still achieves an average of 92% of the original performance. Increased average network latency for STANDARD and BINOC as bandwidth is reduced accounts for the performance difference. Increased serialization and congestion leads to an average latency of 50 cycles for STANDARD and BINOC, while BAR has a relatively low 20-cycle latency for the 4-byte conﬁguration. BAR increases the average channel 139         	    	 utilization by 4×, from about 3% to more than 12%. In terms of the number of bidirectional channels, some applications beneﬁt from a more ﬁne-grained channel conﬁguration. However, on average there is no signiﬁcant performance difference between 4 bidirectional channels and 8 bidirectional channels. Since more channels incur a slightly higher overhead regarding the number of phits to be handled and the number of channels to be arbitrated, the conﬁguration with fewer channels is preferable. D. Hardware Implementation BAR AND STANDARD POW ER AND AR EA Table III Router STANDARD BAR Area (μm2 ) 316194 319358 Power (mW) 17.31 17.63 Crit. path (ns) Wires 1.1 640 1.1 345 To ensure the feasibility of our design and analyze its area and power overhead, we implement a 5-port BAR router with 4 bidirectional 2-byte channels per port, as well as a typical VC router with 5 ports and 2 unidirectional 8byte channels per port in Verilog. Table III reports results from the Synopsys Design Compiler with a TSMC 65nm standard library; these results show that ﬁne-grained bandwidth adaptivity can be implemented without impacting cost or performance. The additional buffer requirements for the serializer/deserializer, as well as the control logic and tristate buffers add a negligible 1% area overhead. The same is true for the routers’ power consumption, which was averaged over several PARSEC application traces. While we do not incur a signiﬁcant area or power overhead, we reduce the overall amount of wiring resources by 46%. The channel bandwidth is reduced by 50% while 5 bits of sideband signal information are necessary for the channel allocator module (3 bits: binary encoded number of phits in shift register; 2 bits: channel conﬁguration). To guarantee that our design does not increase the critical path delay of the router, we separate the task of the channel allocator module into two separate cycles. First, the phit counts are sent to the allocator and their ratio is computed. During the following cycle, the current channel conﬁguration is sent back to the router ports and the channel direction is switched accordingly using tristate buffers. This design avoids the delay and complexity of handshaking between both router ports, such that the critical path delay can be maintained at 1.1ns. Repeaters are necessary for long wires, which require additional control signals to switch their direction. To avoid this additional overhead, bidirectional channels are best used for local interconnects of nearby routers, such as neighbouring routers in a 2D mesh, rather than global interconnects. 140 V I . R E LAT ED WORK Dynamically varying network requirements: Most NoC research assumes a static network architecture that is conﬁgured once at design time based on worst-case trafﬁc analysis and does not adapt to dynamic changes in network trafﬁc. Recently, the idea of adapting the NoC to changing workload properties has gained in popularity as it promises signiﬁcant performance improvements and enhanced resource utilization. Based on the observation that no single ﬁxed-design NoC efﬁciently handles different styles of trafﬁc, Kim et al. introduce Polymorphic On-Chip Networks [12], a special polymorphic fabric that offers per-application network customization. However, we believe the per-application granularity overlooks signiﬁcant opportunity to adapt to intra-application variations. Fine-grained approaches such as ViChaR [17] target dynamic adaptation of routers by providing uniﬁed buffer resources across their input ports. By utilizing buffer space more efﬁciently, they achieve similar network performance using 50% fewer buffers compared to the baseline. Our work also reduces network resources through sharing of underutilized resources. Bandwidth-adaptivity: Different approaches have been proposed to dynamically adjust a NoC’s bandwidth resources to match the varying workload requirements. We present a detailed comparison between our solution and the two most closely related proposals [3], [13] in Sec. IV. These two proposals follow a completely different strategy of providing more bandwidth resources instead of reducing the already over-provisioned resources. The Adaptive Physical Channel Regulator (APCR) which allows concurrent transmission of multiple ﬂits per channel also increases channel width [19]. Das et al. [5] and Mishra et al. [15] describe mechanisms to decouple ﬂit width from channel size. Their approaches combine two ﬂits when possible and then send the combined ﬂit together through a wider channel. Like us, Meng et al. [14] observe that NoC bandwidth resources are generally over-provisioned. In order to optimize the energyperformance trade-off across varying bandwidth requirements, they dynamically adjust the channel width across the entire network. All channels are adjusted to the same width. Our approach is more ﬁne-grained since bandwidth resources can be adjusted on a per-router basis. Phit-serial communication: Phit-serial communication in NoCs enables better routability due to fewer wires and higher clock frequency. Dziurzanski et al. use phit-serial communication for their lossless compression system [6]. All prior uses of phit-serial communication are based on unidirectional channels and rely on the transmission of a single phit per cycle. Therefore, they implement purely serial communication. We propose a hybrid approach, where we beneﬁt from the reduced data width of phits compared to ﬂits, but are able to adapt the parallelism, and therefore the bandwidth of a communication link, at the same time. This allows us to reduce channel width independent of ﬂit width without increasing packet lengths. V I I . CONC LU S ION In this work, we introduce ﬁne-grained bandwidth adaptivity in NoCs using bidirectional channels. By decoupling the ﬂit width from the channel width and utilizing a novel bidirectional phit-serial communication mechanism between routers, we are able to improve NoC channel utilization without signiﬁcantly increasing communication latency. Using a full-system simulator with a cycle-accurate NoC model to execute the PARSEC benchmark suite on a 16-core CMP, we achieve a 50% reduction in channel resources for a 4x4 mesh NoC without a measurable impact on overall system performance. When reducing the channel resources by 75%, ﬁne-grained bandwidth adaptivity is able to achieve 92% of overall system performance compared to the baseline network. ACKNOW L EDG EM EN T S We would like to thank the anonymous reviewers for their constructive feedback on this work. We also thank the members of Prof. Enright Jerger’s research group for their helpful suggestions. This work is supported in part by the University of Toronto, NSERC of Canada and a Connaught New Researcher Award. "A Hybrid Buffer Design with STT-MRAM for On-Chip Interconnects.,"As the chip multiprocessor (CMP) design moves toward many-core architectures, communication delay in Network-on-Chip (NoC) has been a major bottleneck in CMP systems. Using high-density memories in input buffers helps to reduce the bottleneck through increasing throughput. Spin-Torque Transfer Magnetic RAM (STT-MRAM) can be a suitable solution due to its nature of high density and near-zero leakage power. But its long latency and high power consumption in write operations still need to be addressed. We explore the design issues in using STT-MRAM for NoC input buffers. Motivated by short intra-router latency, we use the previously proposed write latency reduction technique sacrificing retention time. Then we propose a hybrid design of input buffers using both SRAM and STT-MRAM to hide the long write latency efficiently. Considering that simple data migration in the hybrid buffer consumes more dynamic power compared to SRAM, we provide a lazy migration scheme that reduces the dynamic power consumption of the hybrid buffer. Simulation results show that the proposed scheme enhances the throughput by 21% on average.","A Hybrid Buffer Design with STT-MRAM for On-Chip InterconnectsHyunjun Jang, Baik Song An, Nikhil Kulkarni, Ki Hwan Yum and Eun Jung KimDepartment of Computer Science and Engineering Texas A&M UniversityCollege Station, Texas,  77843-3112Email: {hyunjun,baiksong,nikhilvk,yum,ejkim}@cse.tamu.eduAbstract—As the chip multiprocessor (CMP) design moves toward many-core architectures, communication delay in Network-on-Chip (NoC) has been a major bottleneck in CMP systems. Using high-density memories in input buffers helps to reduce the bottleneck through increasing throughput. Spin- Torque Transfer Magnetic RAM (STT-MRAM) can be a suitable solution due to its nature of high density and near- zero leakage power. But its long latency and high power consumption in write operations still need to be addressed.  We explore the design issues in using STT-MRAM for NoC input buffers. Motivated by short intra-router  latency,  we  use the previously proposed write latency reduction technique sacriﬁcing retention time. Then we propose a hybrid design   of input buffers using both SRAM and STT-MRAM to hide the long write latency efﬁciently. Considering that simple data migration in the hybrid buffer consumes more dynamic power compared to SRAM, we provide a lazy migration scheme that reduces the dynamic power consumption of the hybrid buffer. Simulation results show that the proposed scheme enhances the throughput by 21% on average.Keywords-Network-on-Chip; STT-MRAM; router; input buffer;INTRODUCTIONWith the continued advance of CMOS technology, the number  of  cores  on  a  single  chip  keeps  increasing  at  a rapid pace. And it is highly expected that many-core architectures with more than hundreds of processor cores will be commercialized in the near future.  In  a  large-  scale chip multiprocessor (CMP) system, network overheads are more dominant than computation power in determining overall system performance. While shared buses provide networking performance enough for a small number of CMP nodes, they cannot be good solutions for many-core systems due to the limitation on scalability. Accordingly, switch- based networks-on-chip (NoCs) are being adopted as an emerging design trend in many-core CMP environments. Since all components in a chip including processors, caches and interconnects must compete for limited area and power budgets, resources available for NoCs are tightly constrained compared to off-chip interconnects. Moreover, network per- formance becomes more signiﬁcant with the increasing scale of CMP systems. Therefore, a new and innovative NoC design that can guarantee better performance with limited resources is necessary for many-core systems. The advance of memory technology has ushered in new non-volatile memory (NVM) designs that overcome the drawbacks of existing memories such as SRAM or DRAM. Among them, Spin-Torque Transfer Magnetic RAM (STT- MRAM) is being regarded as a promising technology for a number of advantages over the conventional RAMs. STT- MRAM is a next-generation memory that uses magnetic materials as the main information carrier. It achieves lower leakage power and higher density compared to the existing SRAM. Also, STT-MRAM shows higher endurance com- pared to other NVM techniques such as Phase Change Mem- ory (PCM) or Flash, which makes STT-MRAM more at- tractive for on-chip memories that must tolerate much more frequent write accesses compared to off-chip memories. However, one of the biggest weaknesses of STT-MRAM is long write latency compared to SRAM. Since the fast access time of memories on a chip must be guaranteed and cannot be negotiable, the slow write operations of STT-MRAM limit its popularity, even though it shows competitive read performance. Another serious drawback of STT-MRAM is high power consumption in write operations. This issue of high power consumption in STT-MRAM must be resolved in NoCs due to the limited power budgets.Despite these weaknesses, using STT-MRAM in the NoC design has signiﬁcant merits since an on-chip router can incorporate larger input buffers compared to SRAM with the same area budget because of the higher density of STT- MRAM. Larger input buffers contribute to improving the throughput of NoC, which results in the enhancement of overall system performance. However, the aforementioned challenges must be addressed ﬁrst to exploit the beneﬁt of STT-MRAM in NoC. Since the input buffer of an on-chip router must handle arriving ﬂits on time, it is impossible  in reality to use STT-MRAM without additional technique to hide the long write latency. Moreover, addressing the high write power issue of STT-MRAM is mandated in NoC environments.In this paper, we explore the design issues of adopting STT-MRAM in on-chip interconnects. First, by relaxing the non-volatility of STT-MRAM, the latency as well as  the power consumption in write operations can be reduced at the sacriﬁce of the retention  time  [1],  [2].  Based  on the observation of intra-router latency of ﬂits, we ﬁnd out			that the retention time needed for input buffers in NoC    can be signiﬁcantly shortened. We exploit the write latency reducing technique [1] in the input buffers of on-chip routers, and decrease the latency to less than 2ns that corresponds to 6 cycles in 3GHz clock frequency. Then we propose a hybrid design of input buffers combining both SRAM and STT-MRAM. By allowing each arriving ﬂit to be stored in the SRAM buffer ﬁrst and then migrated to STT-MRAM, the write latency of STT-MRAM is effectively hidden, thus increasing network throughput.Simply migrating each ﬂit from SRAM to STT-MRAM buffer causes signiﬁcant power consumption due to the high write power of STT-MRAM, compared to existing SRAM- based input buffers. So we design a lazy migration scheme that allows the ﬂit migration only when the network load exceeds a certain threshold, which helps to reduce the power consumption signiﬁcantly. Simulation results show that the hybrid input buffers improve the network throughput by 21% in synthetic workloads and 14% in SPLASH-2 parallel benchmarks on average compared to pure SRAM- based buffers with the same area overheads. Also, the lazy migration scheme contributes to power reduction by 61% on average compared to the simple migration scheme that always migrates ﬂits from SRAM to STT-MRAM.The remainder of this paper is  organized  as  follows. We discuss related work in Section II, followed by the performance and power model of STT-MRAM in Section III. In Section IV, we explain the hybrid buffer design using STT-MRAM in detail. Section V presents simulation results and analysis, and ﬁnally Section VI summarizes our work Low-Rexixtance	High-RexixtanceFigure 1: The Two States of An MTJ Modulemodule is coupled with a transistor to form a basic memory cell of STT-MRAM called a 1T-1MTJ cell.B. Utilizing NVMs in Processors and MemoriesSeveral schemes have been proposed to provide architec- tural support for applying NVMs to system components. Jog et al. [1] proposed to achieve better write performance and energy consumption of STT-MRAM-based L2 cache through adjusting data retention time of STT-MRAM. Similarly, Smullen et al. [2] reduced the write latencies as well as dynamic energy of STT-MRAM by lowering the retention time for designing on-chip caches. In [3], they integrated STT-MRAM into on-chip caches in a 3D CMP environment and proposed a mechanism of delaying cache accesses to busy STT-MRAM banks to hide long write latency. Prior to that, Sun et al. [4] stacked MRAM-based L2 caches on top of CMPs and reduced overheads through read-preemptive write buffer and hybrid cache design using both SRAMand makes conclusions.RELATED WORKSince there has been no prior work using STT-MRAM in NoC design, we only summarize the relevant studies of STT- MRAM technologies as well as the application of NVM to diverse system domains such as processors and memories.STT-MRAMSTT-MRAM is a next generation memory technology that takes advantage of magnetoresistance for storing data. It uses a Magnetic Tunnel Junction (MTJ), the fundamental building block, as a binary storage. An MTJ comprises a three-layered stack: two ferromagnetic layers and an MgO tunnel barrier in the middle. Among them, the ﬁxed layer located at the bottom has a static magnetic spin, the spin   of the electrons in the free layer  at  the top  is inﬂuenced by applying adequate current through the ﬁxed layer to polarize the current, and the current is passed to the free layer. Depending on the current, the spin polarity of the free layer changes either parallel or anti-parallel to that of the ﬁxed layer. The parallel indicates a zero state, and the anti-parallel a one state. Figure 1 depicts the two parallel and anti-parallel states of an MTJ module. A single MTJ and MRAM. Guo et  al.  [5]  resolved  the  design  issues of microprocessors using STT-MRAM in detail for more power-efﬁcient CMP systems.PCM also has been constantly explored to replace existing SRAM or DRAM-based memory systems. Due to its lower endurance compared to SRAM or STT-MRAM, PCM is mainly adopted for off-chip memories rather than on-chip caches. Several designs of PCM-based main memory were discussed in [6], [7], [8]. In [9], adaptive write cancellation and write pausing policies were proposed to reduce energy and improve performance. Zhou et al. [10] suggested a new memory scheduling scheme that allows Quality-of-Service (QoS) tuning through request preemption and row buffer utilization.PERFORMANCE AND POWER MODEL OFSTT-MRAMAs an area model of STT-MRAM, we use ITRS 2009 projections [11] as well as the model used in [5], where a 1T-1MTJ cell size is 30F2 in the 32nm technology. When we assume that an SRAM cell size is approximately 146F2 with the same technology, one SRAM cell can be substituted by at least four STT-MRAM cells under the same area budget. Also, about 3.2ns of write latency can be achievedwith 30F2 STT-MRAM cell size [5]. It corresponds to 10 cycles in 3GHz clock frequency, which is quite long for on- chip routers compared to SRAM that completes both read and write accesses in a single cycle. Reducing retention time from 10 years to 10ms guarantees the same write latency with one third of original write current needed [1]. Using lower current is beneﬁcial in terms of area overheads because it facilitates to implement STT-MRAM cells with smaller transistors, which reduces actual cell area.In this study, we slightly increase write current to reduce this write latency of STT-MRAM further. The write latency reduces from 3.2ns to 1.8ns through increasing the write current from 50μA to 75μA under 125 ◦C of a temperature. Note that even this increased current is far less than the original current needed for 10 years of retention time, while maintaining the same STT-MRAM cell size, 30F2. Also, the increased current does not hurt write energy consumption since the MTJ switching time decreases accordingly [5]. As a result, the write latency decreases from 10 to 6 cycles in 3GHz clock frequency. The increased write current may hurt the performance in terms of read latency. However, we verify that the reduction of write latency from 3 to 1.8ns affects the read latency to only a small extent [2]. Therefore, we can assume that the increased read latency can still be covered by a single cycle, considering the original read delay of 122ps [5], which is far shorter than 333ps, a cycle time in 3GHz clock frequency.The relaxed retention time of 10ms may hurt the reliability of data stored in an STT-MRAM buffer, if the retention time is shorter than the intra-router delay of a ﬂit, deﬁned by the time difference between arrival time at the buffer and departure time in a router. Figure 2 depicts maximum intra-router latency for different injection rates ranging fromto 0.7 with various SRAM buffer sizes per VC, under uniform random synthetic workloads. We observe that the latency does not go up beyond 16 cycles, and it is almost negligible compared to 10ms, which corresponds to more than 30 million cycles in 3GHz clock frequency 1. Hence, it is conﬁrmed that even the reduced retention time is completely enough to hold a ﬂit in STT-MRAM buffers safely. For the read and write energy model of STT-MRAM, we conservatively adopt the same parameters from [5], 0.01pJ and 0.31pJ per bit for read and write, respectively. Note that these are based on 3.2ns of write latency, so actual write energy becomes smaller after decreasing the latency to 1.8ns.AN ON-CHIP ROUTER ARCHITECTURE WITHHYBRID BUFFER DESIGNIn this section, we describe a generic router architecture and a buffer structure in NoC and present our hybrid buffer1Note that in deadlock situations, packets can stay in the network forever. In this study, we adopt deadlock-free routing algorithms, thus avoiding such situations. SRAM2	SRAM3	SRAM4	SRAM5	SRAM620f5 f0 500.f 0.2 0.3 0.32 0.34 0.36 0.38 0.4 0.5 0.6 0.7Injection Rate (titsƒnodeƒcycle)Figure 2: Maximum Intra-Router Latency of An On-Chip Router (SRAM#: SRAM Buffer Size per VC)Output LinhFigure 3: Generic Router Architecturedesign that maximizes the mutually complementary features of the two different memory technologies, SRAM and STT- MRAM, while minimizing the drawbacks of STT-MRAM, the long latency and high power consumption in write operations.Generic Baseline Router ArchitectureThe generic NoC router architecture is depicted in Fig- ure 3. It is based on the state-of-the-art speculative router architecture [12]. Each arriving ﬂit goes through 2 pipeline stages in the router: routing computation (RC), VC allo- cation (VA) and switch arbitration (SA) at the ﬁrst cycle, and switch traversal (ST) at the second cycle. A lookahead routing scheme [13] is adopted, which generates routing information of the downstream router for an incoming ﬂit prior to the buffer write, thus removing the RC stage from the critical path. Each router has multiple VCs per input port and uses ﬂit-based wormhole switching [14]. Credit- based VC ﬂow control [15] is adopted to provide the back-pressure from downstream to upstream routers, thus controlling ﬂit transmission rate to prevent packet loss due to buffer overﬂow.Due to the limited area and power resources and ultra-low latency requirements, on-chip routers rely on very simple buffer structure. VC-based NoC routers consist of a number of FIFO buffers per input port where each FIFO corresponds to a VC as illustrated in Figure 4(a). Each input port has vMigration LinhSRAM VC Identifier SRAM  	 		InputChannel Crossbar	Write Pointer Read Pointer Input Port Write Pointer STT-MRAM Read PointerGeneric SRAM Parallel FIFO Buffer a  H7brid Parallel FIFO Buffer b Figure 4: A Generic SRAM Input Buffer (a) and A Hybrid Input Buffer (b)Input Port H7brid Input BufferSRAM	STT-MRAM An On-Chip Router Architecture with Hybrid Buffer DesignIncoming Flit	Buffered FlitInput Write direction	Migration Direction	ossbar	1 In this section, we show an on-chip router architectureChannel Time Flow6     5    46     5A     6 ²  13    ²   14 3 ² 15 4 3 ² 1 with hybrid buffer design that combines SRAM and STT- MRAM. The hybrid design aims to maximize advantages inherent in different memory technologies in a synergis-   tic fashion for performance improvement while consumingFigure 5: Simple Flit Migration Scheme in Hybrid BufferDesignVCs, each of which has a k-ﬂit FIFO buffer. Current on- chip routers have small buffers to minimize area overheads, thus v and k are much smaller than in macro networks. The necessity for ultra-low latency leads to a parallel FIFO buffer design as shown in Figure 4. Contrary to a serial FIFO im- plementation, the parallel structure eliminates unnecessary intermediate processes for a ﬂit to traverse all buffer entries until it leaves the buffer [16]. This ﬁne-grained control requires more complex logic, which manages read and write pointers to keep the FIFO order. The read and write pointers in the parallel FIFO registers control an input demultiplexer and an output multiplexer. The write pointer points to the tail of the queue, and the read pointer points to the head of the queue. For a read operation, the ﬂit pointed by the head is selected and transmitted to a crossbar input port. Similarly, write operation leads the incoming ﬂit  to  be  written  to the location pointed by the tail pointer. The pointers are promptly updated after each read or write operation. After  a read operation, once the head is overlapped with the tail, the buffer becomes empty. After a write operation, likewise, if the tail moves to the same position pointed by the head, the buffer is full. power economically. The key idea is inspired by the nature of STT-MRAM that provides 4 times more buffer space than SRAM under the same area constraint due to its higher density characteristics [5], [17]. The increased buffer size contributes to making on-chip routers have spacious rooms for buffering, thus boosting the overall network throughput with no additional area overheads compared to a pure SRAM-based input buffer.Figure 4(b) depicts the  proposed  hybrid  input  buffer  of a VC. Compared to the pure SRAM buffer shown in Figure 4(a), the STT-MRAM is attached to each VC in parallel with the SRAM buffer.  Each SRAM buffer entry  is connected to m dedicated STT-MRAM buffer entries through separate migration links. The hybrid parallel FIFO buffer maintains read/write pointers. An incoming ﬂit is ﬁrst written to the SRAM buffer, thus the write pointer points to SRAM buffer entries only. But an outgoing ﬂit may leave from either SRAM or STT-MRAM and the read pointer covers the entire buffer, both SRAM and STT-MRAM buffer entries.A migration controller triggers the ﬂit migration and determines if a certain ﬂit is ready to be migrated to STT-MRAM. VC ﬂow control is performed based on the availability of SRAM in downstream routers, meaning that the availability of STT-MRAM is not considered, because  a write operation to STT-MRAM cannot ﬁnish in a single cycle.Figure 6: CMP LayoutSimple Flit Migration Scheme: The key design goal of the hybrid input buffer is to guarantee seamless read and write operations in every cycle to achieve higher throughput with an increased buffer size. To serve this purpose, we devise a ﬂit migration scheme, which seamlessly migrates buffered ﬂits from SRAM to STT-MRAM to secure more SRAM buffer space for incoming ﬂits, while hiding the long write latency of STT-MRAM.Figure 5 depicts an example of the migration scheme, where each VC consists of 6 SRAM and 12 STT-MRAM buffer entries. The STT-MRAM buffer write latency is assumed to be 6 cycles. When an incoming ﬂit arrives, it   is written to the SRAM buffer ﬁrst, and the migration from SRAM to STT-MRAM begins immediately. Supposing that a new ﬂit arrives every cycle, the SRAM buffer becomes full eventually in the 6th cycle. At  the  same  time,  the ﬁrst ﬂit is migrated to STT-MRAM successfully and one SRAM buffer entry becomes available. Then a subsequent incoming ﬂit occupies the released SRAM buffer entry with no additional timing delay. Note that Figure 5 illustrates the concept in a logical way, and no physical shift occurs except the migration from SRAM to STT-MRAM. The placement of ﬂits in STT-MRAM is logical and is not the physical placement described in Figure 4(b).Power-Efﬁcient Lazy Migration: In the simple migra- tion scheme explained in the previous section, the migration begins immediately as soon as an incoming ﬂit arrives at the SRAM buffer. The simple migration wastes lots of power in a low network load because most of the ﬂits initially written to SRAM leave the buffer in the middle of migration to STT- MRAM.Based on this observation, we propose a lazy migration scheme, which selectively triggers the migration of a ﬂit based on the estimated network load per VC in the on-   chip router. The network load is indirectly estimated by tracking the number of ﬂits in the SRAM buffer.  If the ratio of the number of ﬂits in the SRAM buffer to the total SRAM buffer size exceeds a certain predeﬁned threshold level, the ﬂit migration is performed for every subsequent incoming ﬂit as long as the the ratio exceeds the threshold. Table I: CMP System ConﬁgurationTable II: SRAM and STT-MRAM Parametersscheme, the migration controller is augmented to keep track of the ﬂits in the SRAM buffer and triggers the migration adaptively. The write power is reduced by up to 79% in a low network load compared to the simple migration, which will be discussed in detail in Secton V.PERFORMANCE EVALUATIONIn this section, we evaluate the proposed hybrid on-chip router to examine how much it improves the overall network performance while reducing the power consumption in NoC, using several benchmarks and synthetic workloads.System ConﬁgurationA cycle-accurate NoC simulator is used to conduct the detailed evaluation of the proposed scheme. It implements the pipelined router architecture with VCs, a VC arbiter,   a switch arbiter and a crossbar. Under the 32nm process technology, all simulations are performed in an 8x8 network having 32 out-of-order processors and 32 L2 cache banks on a single chip as shown in Figure 6. The network is equipped with 2-stage speculative routers with lookahead routing [13]. The router has a set of v VCs per input port. Each VC contains a k-ﬂit buffer with 16B ﬂit size. In our evaluation, we assume that v is 4, and k may vary with different buffer conﬁgurations. A dimension order routing algorithm, XY, and O1TURN [18] are used with wormhole switching ﬂow control.A variety of synthetic workloads are used to measure the effectiveness of the hybrid on-chip router: uniform random (UR), bit complement (BC) and nearest neighbor (NN). To evaluate the proposed schemes under realistic environments,In this way, we can save total write power associated with the migration operation. To implement the lazy migration we also use SPLASH-2 [19] parallel benchmark  traces. The traces are obtained using Simics [20], a full systemf20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)UR 0.4 f20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)BC f20 f00 806040200 0 0.f 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Injection Rate (titsƒnodeƒcycle)NNFigure 7: Performance Comparison with Synthetic Workloadsf20 f00 806040200 0	0.f	0.2	0.3Injection  Rate (titsƒnodeƒcycle) 0.4 f20 f00 806040200 0	0.f	0.2	0.3	0.4	0.5	0.6Injection Rate (titsƒnodeƒcycle) f20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)Figure 8: Performance Comparison with 2D-Torus Flattened ButterﬂyO1TURN Routing Algorithm Figure 9: Performance Comparison with Different Topologiesf20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)30 cycles 0.4 f20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)10 cycles 0.4 f20 f00 806040200 0	0.f	0.2	0.3Injection Rate (titsƒnodeƒcycle)6 cycles 0.4Figure 10: Performance Comparsion with Various STT-MRAM Write Latenciessimulation platform. Table I speciﬁes the detailed CMP conﬁguration we use to run benchmarks.We use Orion 2.0 [21] to estimate router power consump- tion. In addition, parameters shown in Table II are cited from [11], [5], for both SRAM and STT-MRAM. The unit of parameter for the leakage power is mW per 1-ﬂit buffer. Throughout this paper, the size of SRAM and STT-MRAM buffers are denoted by  SRAM# and  STT#,  respectively. As stated in Section IV-B, STT-MRAM provides 4 times more buffer space compared to SRAM under the same area budget, thus SRAM1 is equal to STT4. Unless otherwise stated, the write latency of STT-MRAM is 6 cycles based on the analysis in Section III.Performance Analysis with Synthetic Workloads and BenchmarksFigure 7 shows performance improvement for various hybrid input buffer conﬁgurations compared to the pure SRAM buffer, under UR, BC and NN trafﬁc patterns. All results are measured under the same area budget, SRAM6 per VC, for input buffers. In all cases, the hybrid design shows throughput improvement by 18% for UR, 28% for BC, and 17% for NN on average. These results indicate that although the STT-MRAM write latency is longer than that of SRAM, the performance loss is offset by the increased buffer size due to the high density of STT-MRAM, thus resulting in performance improvement.We also evaluate the hybrid design using O1TURN [18] routing algorithm as well as various topologies: 2D-torus and ﬂattened butterﬂy [22]. Figure 8 shows the performance with O1TURN in the 8x8 2D-mesh topology, where the overall throughput increases by 15% on average, while Figure 9 shows that the throughput is increased in 2D-torus and ﬂattened butterﬂy by 13% and 15%, respectively.To examine the impact of different write latencies of STT- MRAM on network performance, we conduct experiments under 2D-mesh and XY routing algorithm. Figure 10 shows the performance in terms of packet latency with 3 different write latencies of STT-MRAM: 30, 10, and 6 cycles. It clearly indicates that the overall network performance isSRAM6 SRAM2_STTf6  SRAM3_STTf2  SRAM4_STT8 SRAM5_STT4f.30 f.20f.f0 f.00 0.900.8030 cycles	f0 cycles	6 cycles Write Latency of STT−MRAMFigure 11: Throughput with Different STT-MRAM Write LatenciesSRAM4	SRAM3_STT4403020f0 0	Figure 12: SPLASH-2 Benchmark Resultsaffected by the duration of STT-MRAM write operation. Among the different hybrid conﬁgurations, SRAM2 STT16 shows the worst performance. This is because the SRAM buffer space is too small to retain the incoming ﬂits for sufﬁcient period of time for migration, 6 cycles, which makes the simple ﬂit migration scheme less efﬁcient. Thus, the long write latency of STT-MRAM is not effectively hidden, resulting in the early saturation of the network. As shown in Figure 2, every ﬂit stays in the buffer for at least 3 cycles. So the SRAM buffer size should be greater than or equal to 3 to run the migration scheme seamlessly.If the write latency is long, 30 cycles, the performance is mostly determined by the SRAM size. This is because the long write latency lowers the possibility for ﬂits to be migrated to the STT-MRAM buffer before network satu- ration. Therefore, SRAM5 STT4 shows the best through- put improvement. On the  contrary,  if  the  write  latency  is sufﬁciently short, 6 cycles, the performance is greatly impacted by the total buffer size including both SRAM   and STT-MRAM except the SRAM2 STT16 case. Thus, SRAM3 STT12 shows the highest throughput compared to other conﬁgurations.To make a clear quantitative comparison of relative per- formance of the 3 different write latencies, we show network throughput normalized to the SRAM6 in Figure 11, based on the results in Figure 10. Figure 11 conﬁrms the aforemen- tioned analysis. In case of a relatively long write latency,  30 cycles, the hybrid input buffer having the largest SRAM buffer outperforms the others by up to 11% compared to the SRAM	SIMPLE	LAZY (0.25)	LAZY (0.5)	LAZY (0.75)32.52f.5 f 0.500.f	0.f5	0.2	0.25	0.3	0.35	0.4Injection Rate (titsƒnodeƒcycle)Dynamic Power Consumption of Input BuffersSRAM	SIMPLE	LAZY (0.25)	LAZY (0.5)	LAZY (0.75)f.5 f 0.500.f	0.f5	0.2	0.25	0.3	0.35	0.4Injection Rate (titsƒnodeƒcycle)Total Power Consumption of RoutersFigure 13: Comparison of Power Efﬁciencypure SRAM6 buffer. Likewise, in case of a low write latency, 6 cycles, except the SRAM2 STT16 case, the one having the largest total buffer size, SRAM3 STT12 beats the other conﬁgurations by up to 18% in terms of network throughput. Figure 12 shows the average network latency with SPLASH-2 benchmark traces. We assume SRAM4 per VC as an area budget, the same as a cache block size. In general, the hybrid input buffer outperforms the pure SRAM- based one, by approximately 14% on average. Speciﬁcally, water-nsquared shows the best improvement by 34.5% while ocean shows the least improvement by 3.2%. The amount of improvement varies depending on the trafﬁc patterns. We observe that in the benchmarks showing higher im- provement, hot spots exist in their communication, whereas in the benchmarks with slight performance improvement, communication is evenly spread across the whole network. Finally, we make a  sensitivity  analysis  of the number  of buffer entries in NoC routers. Under two different area budgets, SRAM4 and SRAM6, we compare the throughput of the pure SRAM-based buffer and the hybrid buffer that shows the best performance. As the budget decreases from SRAM6 to SRAM4, the amount of improvement coming from the hybrid buffer increases by approximately 5.5%. This trend indicates that the hybrid buffer is more beneﬁcial asthe area budget in CMP environments becomes tighter.Power AnalysisSince power is one of the main issues in  the  NoC router design, we evaluate power consumption of the hybrid input buffer and compare the effect of the two migrationschemes explained in Section IV. Figure 13(a) compares the dynamic buffer power consumption of 4 different migration schemes in SRAM3 STT12: simple and lazy with 3 different thresholds (0.25/0.5/0.75). All results are normalized to that of the pure SRAM-based buffer, SRAM6. The lazy migration scheme with the threshold 0.75 consumes signiﬁcantly less amount of power, by 53% on average, compared to the simple migration scheme. In a low network load (0.1), the power consumption of the lazy migration scheme with the threshold 0.75 is almost equivalent to that of the baseline SRAM. In a high network load (0.4), however, the ﬂit migration occurs more frequently in the hybrid buffer due to the highly congested network. Accordingly, the migration lowers the possibility of reducing the dynamic power, thus increasing the power consumption of the lazy migration by up to 1.7x more than the baseline SRAM.Figure 13(b) compares the total router power consumption of the 4 migration schemes that  includes  both  leakage and dynamic power consumption of all routers across the network. In a low network load (0.1), the total power consumption of routers with the hybrid buffer is less than that of routers with the pure SRAM buffer by 16%. This is due to much less leakage power consumption of STT- MRAM compared to SRAM as shown in Table II. As the network gets more congested, however, the hybrid buffer consumes more power compared to the baseline SRAM buffer. In a high network load (0.4), for instance, the lazy migration scheme with the threshold 0.75 consumes more power by up to 4% compared to the baseline SRAM buffer. Note that as we increase the threshold value from 0.25  to 0.75 in the lazy migration scheme, the overall network throughput is slightly degraded but the amount of degrada-tion is around 0.5% on average, which is negligible.CONCLUSIONSIn this paper, we have proposed a hybrid input buffer design using STT-MRAM with SRAM to achieve better network throughput with marginal power overheads in on- chip interconnection networks. The high density of STT- MRAM facilitates to accommodate larger buffer compared to the conventional SRAM under the same area budgets. Through the ﬂit migration schemes, the long write latency of STT-MRAM is effectively hidden while minimizing the power overheads. Simulation results indicate performance improvement of around 21% and 14% on average under the synthetic workloads and benchmarks, respectively, compared to the conventional on-chip router with the SRAM input buffer.For future work, we intend to devise an STT-MRAM- aware routing algorithm and provide an architectural support to reduce the overall power consumption and latency further. "Synthesis of NoC Interconnects for Custom MPSoC Architectures.,"As technology continues to demand high performance, low power, and integration density, NoC system designers consider multiple aspects during the design phase. This paper addresses these issues and presents an NoC design methodology for generating high quality interconnects for custom Multiprocessor System-on-Chip (MPSoC) architectures. Our design methodology incorporates the main objectives of power and performance during topology synthesis while employing both analytical and simulation based automated techniques. A rendezvous interaction performance analysis method is presented where Layered Queuing Network models are invoked to observe the asynchronous interactions between NoC components and identify possible performance degradation in the on-chip network. Several experiments are conducted using various SoC benchmark applications to compare the power and performance outcomes of our proposed technique.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Synthesis of NoC Interconnects for Custom MPSoC Architectures  Gul N. Khan and Anita Tino  Electrical and Computer Engineering, Ryerson University  Toronto, ON Canada M5B2K3  e-mail: gnkhan@ryerson.ca  Abstract—As  technology  continues  to demand high  performance, low power, and integration density, NoC system  designers consider multiple aspects during the design phase.  This paper addresses these issues and presents an NoC design  methodology for generating high quality interconnects for  custom  Multiprocessor  System-on-Chip  (MPSoC)  architectures. Our design methodology incorporates the main  objectives of power and performance during topology synthesis  while employing both analytical and simulation based  automated techniques. A rendezvous interaction performance  analysis method is presented where Layered Queuing Network  models are invoked to observe the asynchronous interactions  between NoC components and identify possible performance  degradation in the on-chip network. Several experiments are  conducted using various SoC benchmark applications to  compare the power and performance outcomes of our  proposed technique.   Keywords-NoC  architectures;  Topology  MPSoCs; Power and Performance efficiency  Synthesis;  I. INTRODUCTION  NoC interconnects have transpired in response to the  performance limitations and overhead of current bus-based  systems, replacing busses and allowing simultaneous core  communication for improving  system performance [1].  Furthermore, power consumption is also reduced by using  shorter links as opposed to long shared busses. In this paper,  we address the synthesis of NoC interconnects for emerging  custom MPSoC architectures. Regular topologies have been  successful for regularity during the design phase [2].  However,  these  types of  topologies  consider  a  homogeneous, tile-based solution whereas current NoCs  often comprise of heterogeneous corees, variable core  locations and communication bandwidth requirements [3].  For this reason, application-specific [4] and adaptive NoC  [5] design have gained significant attention. Many high  performance SoCs use heterogeneous cores which are  mapped to tasks, traffic characteristics, and can be well  defined statically prior to the design phase [3]. Adaptive  NoCs can consume more power and sacrifice performance.  The routing time overhead, extra hardware, and regular  topological implementations can lead to lower performance  [6]. The application-specific custom  topology forfeits  scalability in order to provide the on-chip system with a  reduced amount of power and compact area by using the  specifications that are known prior to topology generation.   To generate efficient and custom topologies, synthesis of  application-specific NoCs will be the focus of this paper.   The application-specific NoC synthesis must consider  multiple objectives of power and performance. CAD tools  have undergone intensive research in NoC design and  optimization [3]. The design of these tools allows flexibility  in exploring power/performance aspects, and often provides  designers with effective solutions. Our NoC synthesis tool  incorporates multiple objectives to generate high quality  NoC topologies for emerging MPSoC architectures. It also  makes use of both analytical and simulation based methods.  II. DESIGN FLOW AND MODELING  A. Design Flow  Overall flow diagram of our NoC synthesis tool is shown  in Fig 1. The topology generator assesses all the NoC  characteristics provided by the system core graph as well as  the specified constraints. Orion models are called upon to  compute a quick analysis of power consumption, router port  constraints, and associated wire-lengths [7]. The initial  topology is run through the system-level floor-planner,  where a simulation is conducted to assess the wire-lengths  and NoC area to make use of time and accuracy in an  optimal way. When the objectives and constraints do not  meet the criteria, the method tries to find a new solution that  will satisfy the objectives. When the system requirements  are met for a given  temporary  topological solution,  performance degradation analysis is performed to determine  contention and deadlock. The on-chip network is modeled  as a Layered Queuing Network (LQN) [8], where potential  bottlenecks are identified and relieved using a temporary  virtual channel (VC) insertion method. Once a solution is  derived, the floorplanner minimizes the wire-length and  performs compaction on the layout, in addition to detecting  system level timing violations to generate a final topological  solution.  B. Performance Modeling  • Each vertex vi אV represents a core in the graph.   The application input model to the topology generator can  be specified as a directed graph G(V,E), where:  directed edge (vi, vj), expressed as ei,j אE.    • The communication between vertex i and j represent a  • The weight found on an edge ei,j denoted by b(ei,j)  characterizes the bandwidth.   978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.16 75             • A destination vertex core dx (where dx אV) may have  one or many source cores sx.  • The source vertex sx אV, and ׊x א 1...N, where N  represents the number of cores in the core graph.  The input graph is a model based on the individual core  communication  requirements and size, with specific  transaction details about the source and destinations. The  user can stipulate topology generation constraints such as  maximum frequency of operation, buffer and flit sizes, an  initial arbitration delay Tinit, router port constraints, and  anticipated packing/de-packing (Tpk, Tdpk) delays of the NIs  (Network Interfaces).   F l o o r P l a n n e r Power  Models  Core Graph  G(V,E)  Design  Constraints  Topology Generator  Layout  Topology  LQN  Model   LQNS  Performance Constraints  Figure 1. NoC Synthesis Design Flow  We adapt the network topology graphing technique  presented by Dally and Seitz  [9]  for preliminary  performance modeling. A Topology Solution Graph (TSG)  is based on the current topological solution, such that TSG =  G(V,L). The vertices of TSG represent cores and routers of  the topology, whereas the edges denote the communication  links. Associated with each edge, li is the expected  transactions.  A connection function (CNX) connects the  link, li to the next communicating link, lj in order to route  the source core (sx) to its destination (dx). We assume a  static wormhole routing network when evaluating delays,  throughput,  and  performance  degradation. Traffic  generation is based on fully specified temporal patterns as  application. Given a router, r (where ݎ א ܴ); there are R  determined by  the required bandwidth of  the  target  number of ports ݌ א ܲ . The mean latency for a message to  routers in the topology and a router may have different  traverse from source sx to its destination dx is expressed as  Tlat. This value incorporates packing Tpk and de-packing Tdpk  delays, arbitration delays Tarb in the router for each hop y,  and possible contention and blocking factors Tblk that can  hinder transmission of the packets. The latency can be  expressed as:  ܶ௟௔௧ ൌ ෍ ܶ௔௥௕ ௒ ௬ୀ଴ ൅ ෍ ܶ௕௟௞ ௒ ௬ୀ଴ ൅ ܶ௣௞ ൅  ܶௗ௣௞ ሺͳሻ  The blocking delay probability per router is obtained  through the LQNS (Layered Queuing Network Solver)  analysis [8] by evaluating the models for system contention.  The fully specified temporal patterns allow each router to  experience different arbitration delays, and each NoC router  76 is modeled in a unique way accordingly. The number of  expected transactions to be incurred by each router port is  calculated from  the specified bandwidth values, and  converted into flits/second. Given a traffic flow of ȟ from a  source sx to destination dx, the number of expected  transactions within a router r traversing through port p can  be expressed as:  Ƚ௥ ǡ௣ ൌ ෍ ෍ ȟሺݏ௫ ǡ ݀௫ ǡ ݎǡ ݌ሻ  ሺʹሻ  ׊௦ೣ ׊ௗೣ ׊ோ ௥ୀ଴ ׊௉ǡ௥ ௣ୀ଴ The basic router model used in this paper assumes one  input FIFO per port with static header decoding capabilities  and a respective output port. An input port p cannot output  data to its respective output port. Therefore, there are P(P-1)  traffic flow from a port i to port j (where i j) with ୧՜୨  possible connections within a router r. The expected overall  transactions can be formulated as:  ߠሺɌሺݏ௫ ǡ ݀௫ ǡ ݎ ǡ ݌ሻǡ ݅ ǡ ݆ሻ ൌ  ൜ ୧՜୨ ׌Ɍ݅ ݆ Ͳሺ͵ሻ  ܶ௔௥௕ೝ ൌ  ෍ ቎෍ Ƚ௥ ǡ௣ ൭ߠሺȟሺݏ௫ ǡ ݀௫ ǡ ݎ ǡ ݌ሻǡ ݅ ǡ ݆ሻ σ Given an initial expected arbitration value Tinit, the  arbitration delay in a router, r can then be expressed as:   ሺͶሻ  ൱ ܶ௜௡௜௧ ቏ ׊௉ǡ௥௣ୀ଴ Ƚ௥ ǡ௣       The arbitration delays per router and port are determined  given the traffic to be incurred, and are applied to the router  models for analysis using the LQNS tool [8]. Reflecting on  equation (1), where Tpk and Tdpk are determined at design  time, Tblck will then be analyzed and fine-tuned by the LQNS  tool during the contention/deadlock analysis and VC  insertion. This delay can then be incorporated to determine  the final frequency of operation for the NoC topology  solution under analysis.  1) Deadlock Modeling: As discussed by Dally and Seitz  [9], a Link Reliance Graph (LRG) is a deadlock solution  graph, where LRG = G(L, E). The vertices of the graph  represent the links between the vertices of the TSG, where  the edges signify link reliance and associated transactions  which are connected using CNX such that:                        ܧ ൌ ൛൫݈௜ ǡ ݈௝ ൯หܥܰܺሺ݈௜ ǡ ܸሻ ൌ  ݈௝ ǡ ݒ א ܸሽ                   (5)   A cycle in the LRG is defined as φj such that a set of links   φj = {l1, l2, …, lk}, where li ∈ L and LRG contains edge e(li,  li+1) where i = {1..k} and an edge e(lk, l1) directed back to  the initial vertex l1. Once the LRG has been determined, a  breadth first search (BFS) is performed from every vertex e  ∈ E to identify any cycles. A cycle exits when a vertex is  detected along a path more than once. In order to remove a  cycle, a vertex must be inserted into the LRG. A vertex  insertion is equivalent to the addition of a VC. All the  vertices are checked again after eliminating a cycle to  remove any other deadlocks.  A cost is associated with removing the deadlocks in  terms of power and performance. By inserting an extra  vertex in the LRG or a VC in the NoC topology, an increase  in power and performance may take place. Contrary to the                                          methodology of Seiculescu et al [10], our technique  evaluates the possibility of increasing performance due to  vertex (VC) insertion. In practice, the highest performance  achievement is attained by inserting VCs at the links, which  exhibit a large number of transactions. As a result, our  method first assesses high transactional links in order to  guarantee an increase in throughput while simultaneously  eliminating deadlock and contention.  2) Contention Modeling: The contention analysis is based  on link utilization and router port usage. The contention  model makes use of both the TSG and LRG. When a link is  heavily requested and utilized in the LRG, the requests can  result in a potential bottleneck within the on-chip network.   The degree of a given link-vertex li can be determined as:                                         ߪ௟೔ ൌ  σ                                   (6)  ா೗೔௞ୀ଴ ݁௞ Where Eli represents the total amount of edges entering and  leaving the link-vertex li in the LRG.   E and L are edges and link vertices in the LRG.  In order to determine the heavy utilized links ofߪ௟೔ :                                ܯܽݔι ൌ ݉ܽݔ൫ߪ௟೔   during topology generation, both GA and SA employ a  Pareto curve technique to produce multiple single-objective  mappings of power or performance.   Contrary to these optimization methods, the TS method  allows multiple objectives when searching for a high quality  solution, where TS is not limited by a cost function. The  method is able to keep track of previous through memory,  reducing run-times and using less computational effort in  comparison  to other  techniques. More advanced TS  methods, such as the one employed in this paper, candidate  list strategies are used to narrow the examination of  solutions to achieve a high quality solution for complex  NoC systems.    A. NOC Synthesis Flow  Given the constraints specified by the user, NoC  topology generator creates an initial topology, N(s, f, P).  Each vertex Vi in the core graph is assigned to an NI. Initial  NoC topology is a fully-connected, where all the NIs/vertex  cores are connected to one central router [4]. An initial NoC  frequency is based on all the connections in the on-chip  network, where Tarb_init, Tblck and Tlat have high values given  the application and size of the initial router. The iterations  within the TS divide the initial crossbar router, where  preference is given to grouping frequently communicating  cores for the same routers.  B. Neighbourhood Selection Method  Let N(s, f, P) represents the current feasible NoC  topology solution s consuming power P at a frequency f, and  N(s) express a new possible solution s’ within  the  neighborhood set. We define a Tabu list, TL(s) that contains  non-optimal solutions, and  an aspiration list AL(s) that is  responsible for consulting the TL(s) to ensure that s’ is  an optimal solution ߱(s), and a possible current feasible  optimal and more desirable than the previous encountered  solutions. Thus in order for the new solution N(s) = s' to be  solution N(s, f, P), the following condition must be satisfied:                                 ߱ሺݏሻ ൌ ሼܰሺݏሻ C. Power Modeling  Router components of the model library assume a  wormhole, static routing. Each router model consists of one  input buffer per port with header decoding capabilities,  requesting access from the arbiter to forward the flit to the  output port with round-robin scheme. The routers and NIs  employ a stop/go flow control strategy to avoid the  overflow. The power aspects of the models are based on  65nm TSMC technology library, established on NVT with a  Vdd of 1.0 V. The power model library invokes the Orion  library [7] to derive the power values of routers and wires  based on different router configurations. Specifically, the  link lengths obtained by the Parquet floor-planner [11], the  specified NoC frequency of operation and the amount of  VCs used per router have been alternated to cover various  scenarios in the power models. Routers assume a registerFIFO, cross-point fabric implementation to further reduce  the delay and static power consumption [12].    III. NOC TOPOLOGY GENERATION  Tabu search (TS) based multi-objective optimization  method is employed for NoC topology generation. The  models formulated in the previous section are applied to  NoC topology generation. TS is a meta-heuristic method  that employs an aggressive search procedure. It progresses  iteratively from one solution to another by moving in a  neighborhood space with the assistance of adaptive memory  [13]. The general TS algorithm commences with an initial  feasible solution and explores other possible neighborhood  space solutions. TS method inquires in its short term  memory lists in order to prevent the reversal of recent  moves. Memory in the TS method relies on explicit and  attributive memory types to guide the search in finding a  high quality solution. Explicit memory directs the search  towards an influential and quality-based solution, keeping  record of past solutions to avoid cyclic behavior. Attributive  memory acts as long term memory, which is also known as  Frequency Recency based memory (FR-Memory). This  memory is used during neighborhood exploration for a new  possible solution. FR-Memory encourages the search to  explore different regions within the neighborhood and allow  diversification amongst different feasible solutions.   Optimization methods such as Simulated Annealing (SA)  rely on minimum-cut partitioning to generate high-quality  solutions while being limited to a cost function. Given that  the cost function should invoke multiple objectives, as in the  case of analyzing various power and performance factors,  the function then becomes very expensive and slow to  compute. Genetic Algorithm (GA) is typically utilized in a  random nature for large solution spaces [14]. The solution  quality of GA is also limited by the fitness function and may  not necessarily lead to an optimal solution. GA based  methods commence with a initial random configuration,  however, NoC  constraints  and  target  application  characteristics are well defined a priori. Therefore, there is  no logic in generating random solutions and GA often acts  as an excessive and time consuming method in topology  generation. Moreover, in order to invoke multiple objectives  increased from the last N(s, f, P) move. If NoC frequency  exceeds the maximum possible frequency, or the power is  significantly increased. Then AL(s) is referred to and the last  optimal solution is restored. The undesired solution is  placed into TL(s) and the algorithm attempts to obtain  another neighborhood solution.   The CL(s) i.e. candidate list strategy is employed here,  which is known as the Successive Filter Strategy (SFS). It  assists the TS in finding a new solution N(s) within the  neighborhood. A high quality solution results in a low  power, high performance topological arrangement that  satisfies the maximum frequency of operation. The SFS  initially aims at first filtering the cores with low transaction  rates since their moves have little effect on the frequency  and performance of the topology as compared to highly  utilized cores. Power consumption is also decreased as  compared to the initial topology, where the router is divided  into smaller components. The head of CL(s) is the core  chosen by the SFS based on either the initial low transaction  cores, or the neighborhood cores being kept tracked by the  FR-Memory. Low  transaction cores are chosen by  employing the following expression:  ܯ݅݊ܶݎܽ݊ݏ ൌ  ݉݅݊௡ ඃܸ௡ ൫σ σ      (12)  ே׊௦ೣ ܰ௧௥ ൯ඇ   ே׊ௗೣ Where N is the total number of vertices/cores in the core  graph such that n = {1,2,…,N}.   Ntr is the number of source cores sx and/or destination dx  transactions.   Vn(z) represents vertex n and its expected total number of  transactions z.   X is the total amount of source/destination sets for the  respective source core n, where x = {1,2,…,X}.   Algorithm 1        Topology Generation Algorithm  1:  2:  3:  4:  5:  6:  7:  8:  9:  10:  11:  12:  13:  14:  15:  16:  17:  18:  19:  20:  21:  22:  23:  Generate initial topology solution N(s,f,P)  Evaluate N(s,f,P) for initial frequency f and power P  TL(s) = {}  //ensure empty Tabu List  WHILE constraints NOT met   DO       Identify s' = N(s)        Move to the temporary solution s'       Evaluate s' solution for f’ and P’ using Orion models       ConstraintCheck(s', AL(s), TL(s))       IF solution meets AL(s), TL(s),  f’, and P’ constraints              Create LQN models for sub-networks              Invoke LQNS tool for performance analysis              Place solution as last optimal TL(s) entry              Update current solution, N(s,f,P) = s’              Check for deadlock, contention, utilization                                 VCInsertion(); (Tarb)              IF f’ ൑ fmax THEN              Determine f’ based on updated Tarb                     Run through system-level floorplanner              ELSE                      Go to line 22              END              IF power/ perf constraints & router ports satisfied                     N(s,f,P) and EXIT              END        ELSE               Place as a non-optimal TL(s) entry              Refer to AL(s) to restore last optimal N(s)        END  END  79 The candidate list and SFS are also verified with the FR  memory whether the core has been previously selected,  which allows other low transaction cores to take the head  candidate position. The SFS then filters through the cores,  which communicate with the head of the candidate list.  Subset arrangements of these cores are formed in different  combinations, where the topology generator selects the  subsets and configures the topology such that the cores are  positioned. The subset core combinations are  then  evaluated, where:  • ߮(s) denotes the set of possible moves that core j can have,  • π(j) represents core j attaining the head candidate position.  when core j has occupied the position π(j).  • ߮(s) be divided into subsets ߮(1,s), ߮(2,s), ... ߮(m,s),  • m signifies all possible combination of moves formed by  where ߮(1,s) denotes the 1st subset move in the possible  the SFS.  set of total moves generated by the topology generator etc.   C. Performance Degradation Application  The contention and deadlock analyzer is given a finite  amount of VC resources and aims at equally distributing the  VCs according to the hot spots. Deadlocks are removed by  VC insertion, followed by contention analysis as specified  in ȍ[]. Algorithm 2 outlines the VC insertion technique for  deadlock and contention elimination. The  technique  commences by analyzing the performance and power  outcomes for temporary VC insertions of the generated  deadlock LQN models. There may be k ways to insert the  VC to avoid deadlock. The technique analyzes all possible  insertions for the best performance and power outcomes  where preference is given to highly utilized vertices. The  best solution is used to determine the VC insertion  permanently. The analyzer continues to check for all the  possible deadlock points.       Contention analysis is used by employing VCs from the  VC resource pool. Once a bottleneck point is identified, the  LQN and power models are used to determine the initial  throughput ȥinit and power Pinit factors respectively. Then  temporary VCs are inserted at the bottleneck points to  model the tasks once again and to evaluate the improvement  in performance ȥj on the jth iteration per contention point,  where j number of VCs inserted at the contention point. The  extra power dissipation Pj incurred due to VCs is also  analyzed. A temporary VC insertion is done permanently if:  • The performance improvement ȥj is greater than the  extra power dissipation Pj that the NoC will experience.  • There are enough VC resources available for insertion.  The algorithm inserts multiple VCs if all the criteria are  met and there are enough resources. The analyzer proceeds  to search and relieve other potential bottleneck points within  the topology as specified by the order of ȍ[]. For any  remaining contention points in the topology due to VC  resource pool  limitations,  the analyzer  reports any  performance improvements that can be achieved by the  additional VCs. When the solution has been analyzed for  performance, the updated value of Tarb is applied to Tlat. If                all contention points are eliminated then Tblk can also be  removed from equation (1). Otherwise, blocking probability  is obtained from LQNS and incorporated into Tlat that is  used to obtain the new frequency f’.  Algorithm 2                           VC Insertion   1:  2:  3:  4:  5:  6:  7:  8:  9:  10:  11:  12:  13:  14:  15:  16:  17:  18:  19:  20:  21:  22:  23:  24:  25:  26:  IF  ׌φ∈ ȍ[] THEN // Deadlock  Generate TSG, LRG based on N(s) solution  Evaluate graphs for Max°, ȝ, δ      IF  > 1 way to insert VC            Find k ways to insert, initialize i = 0             WHILE i  k                  Insert VC at point i                  Evaluate performance and power                  IF solution superior to previous insertion i-1                          Save solution, determine Tarb                  END                   i = i +1             END WHILE       ELSE == 1 way to insert VC              Insert VC at φ         END        WHILE ȍ[]  ׎ && ׌δ∈ ȍ[] //Contention  remove φ from ȍ[] list  ELSE              IF δ previously encountered || previous φ VC point                      Restore ȥ && P variables, VCs resources             END              Determine ȥinit and Pinit factors              Add 1 VC to δ point               Determine ȥj performance cost              IF ȥinit < ȥj || ȥj-1 < ȥj                        Determine Pj                       IF Pj && enough resources                              Insert VC && update VC resources                      END             END             Save solution, determine Tarb, f’, and search for next δ        END WHILE  END              IV. EXPERIMENTAL RESULTS  We employ various multimedia and network benchmark  applications  for our TS-based NoC synthesis  tool.  Performance and power results are compared to a mesh  based topology mapping (employing a similar method to  that of the work proposed in [15], as well as a previous  application-specific topology method for each individual  benchmark. We also compare the performance results of our  tool for deadlock removal with the well-known resource  ordering technique [16]. An analysis of our contention  alleviation method is also provided.  A. Experimental Setup  NoC topologies are generated for different benchmark  applications as given in Table I. We refer to the work of  various researchers for benchmark application core graphs  as referenced in the Benchmark column of Table I. The  application-specific topology, which is compared to our TSbased solution is also referenced in the Table as AppSpecific. The routers are modeled as individual components  in the floor-planner by taking VC area into account. Power  is also calculated with VC power consumption being  considered, where router port buffers are of size 4 flits.  Topologies were generated using a restriction of 6 ports per  80 router, and a maximum operating frequency of 2 GHz.  Network Interfaces are taken into account in the core area,  where each NI is estimated to 0.2 mm2 [17].  TABLE I. BENCHMARKS USED  Benchmark  d26_media [18]  Set-Top Box [20]  MWD [4]  Audio/Video (A/V) [4]  Layer-3 Switch [21]  NCS1 [18]  MPEG4 Decoder [4]  Graph ID Cores  B1  26  B2  25  B3  15  B4  21  B5  12  B6  21  B7  12  App-Specific  [19]  [14]  [4]  [4]  [14]  [20][22]  [4]  Performance results for the generated topologies by our  Tabu method and previous works were compared for  accuracy using an  in-house cycle-accurate (flit-level)  SystemC simulator, where the mesh regular topologies were  simulated using the worm_sim NoC simulator [23]. The  NoC routers employ round-robin arbitration amongst all the  ports  to prevent starvation. Livelock  is averted by  incorporating minimum  length paths from source  to  destination cores. The topology generator implementation  for the proposed method was tested using a 1.66 GHz  processor based Linux system. Simulations were run for  20,000 cycles with an initial warm-up period of 100 cycles.  Due to inconsistencies between power libraries used in the  previous works, we have re-designed layouts for each of the  application-specific topologies with the 65nm library used  in our work to accurately compare power dissipation and  area values.   B. Results and Discussion  The power and performance results are displayed in  Figures 3 and 4 for each of the seven benchmarks,  normalized to the average of four methods tested. Area  overhead comparisons between  the Tabu, mesh and  application-specific topologies are displayed in Table II. In  terms of performance, our Tabu based NoC designs were  able to achieve a 33.6% improvement as compared to other  methods. On average, our method was also able to design  NoC interconnects which consume 33.1% less power as  compared to the other two techniques. The run time for our  tool was 5 seconds for a 12 core application, while the A/V  and Set-Top box applications took approximately 15  seconds to generate the final topologies.   The app-specific method of B6 (referred as ANOC  technique [24]) was unable to determine a solution for the  NCS1 application due to a static floorplan implementation  and the respective timing violations. The symbol ‘--‘ is used  to denote no solution for the normalized metrics obtained in  Table II. The  throughput-oriented  topology synthesis  techniques [4, 14] surpassed the initial Tabu-based solution  in terms of performance for Layer-3 Switch (B1), MWD  (B3) and A/V (B4) applications. When the VC insertion  technique was applied, however, our Tabu technique was  able to surpass the performance outcomes of the throughputoriented method while dissipating less power. The A/V (B4)  topology employed in [4] also made use of smaller routers                    with excessive interconnections and power dissipation to  increase throughput. As a result, our NoC design was able to  consume 54% and 25% less power in comparison to the  app-specific and regular topologies respectively. Finally, the  method by Rahmati et. al [19] emphasized a QoS based  topology generation for the D26-Media (B1) application by  making use of fairly large size routers. Performance  outcomes of our technique with VC insertion in comparison  to Rahmati and others method [19] yielded fairly similar  results, with a slight throughput improvement lower power  dissipation. Figure 5 presents the custom topology generated  by our method for the A/V application, where multiple links  from a router or core signify VC insertion. As presented in  Figure 4, our (Tabu based) approach surpassed  the  application-specific and mesh topologies in performance.  Asterisks (*) present next to the value in the total VCs  column indicate a VC previously inserted during deadlock  analysis. The actual number of VCs inserted during  contention analysis is subtracted by one.  An example of the VC insertion power and performance  tradeoff for the A/V topology’s ASIC2-S and CMEM3 core  sub-networks are presented in Table III. The ASIC2-S  showed an increase in performance of approximately 18.2%  due to a 13.99% increase in power dissipation for the  insertion of one VC. When two VCs were temporarily added  to the system during the second iteration, the NoC system  showed a 22.47% performance improvement with a 19.77%  additional increase in power.                         Figure 3. Power comparison of the benchmarks  Figure 5: Tabu Based Topology for A/V Application  TABLE III. POWER/PERFOMANCE TRADEOFF FOR CONTENTION ANALYSIS  (A/V BENCHMARK)  CMEM2(14) ASIC2 Slave (8)  Sub-Network Performance  Power  (%)  (%)  0.6  12.7  18.2  13.99  22.47  19.77  24.52  27.35  CMEM3  19.48  12.70  (1)  20.95  20.78  Actual  Performance / Power Increase:  30.14 (%) / 10.01 (%)  Total  VCs  0  2  1*  Figure 4. Throughput comparisons of the benchmarks  TABLE II. AREA COMPARISONS  Benchmark  B1  B2  B3  B4  B5  B6  B7  Tabu  8.89  6.41  3.07  11.19  2.35  4.00  5.05  Area (mm2)  Mesh  App-Specific 14.93  18.22  13.4  6.54  22.03  10.87  32.35  22.92  16.3  9.129  32.35  --  16.3  22.1  C. Contention Analysis  Table III presents the specific experimental power and  performance tradeoff values for the rendezvous interaction  contention analysis method of the A/V benchmark subnetworks. Actual performance and power increases from  contention alleviation are outlined at the end of Table III.  81      An interesting observation during experimental results  was the several instances of deadlock which also occurred at  contention points within the topologies. This is an important  point to note that the dependence of resources, which both  deadlock and contention points endure are similar in nature.  Actual performance results of the generated topologies were  simulated using an  in-house SystemC simulator and  displayed a worst case deviation from the contention  analyzer of approximately 19.8%. Overall, the analysis  method was able to relieve various contention points and as  a result increase the performance of the initial topological  solution by an average of 25.3%.  D. Deadlock Removal   The resource ordering technique of Dally and Towel [16]  is able to remove deadlocks while often providing the  system with an increase in throughput. Both Dally-Towel  [16] and our deadlock removal technique were applied to  our initial TS based topologies. In comparison, the proposed                                                                            VC insertion method presented in this work was able to  surpass the throughput values of resource ordering method  slightly by 4%. Conversely, when power dissipation of the  methods were evaluated as displayed in Figure 5, our TSbased method was able to save approximately 9.35 times  less power in comparison to that of resource ordering  method. In terms of the number of VCs needed to relieve  both contention and deadlock, our method managed to save  84.8% resources.  Figure 5. Deadlock Avoidance Power Comparison  V. CONCLUSION  We presented an NoC topology generation method by  employing analytical models and simulation tools to design  low-power, high performance custom NoCs. An analysis  method  for evaluating performance, contention and  deadlock improvements due to VC insertions was presented.  In terms of power, a model library was created for various  topological components. Power values for the NoC are  obtained at significant stages of the synthesis approach  using an integrated floor-planner. We contrast our method to  previous application-specific topology generation methods.  Results indicted an average of 33.1% less power and 33.6%  more throughput as compared to various past techniques.  Our NoC designs are able to save resources, consume less  power with a slight performance improvement.  ACKNOWLEDGEMENTS  The authors acknowledge the financial support from an  NSERC Discovery Grant, and Ryerson University.  Equipment and CAD tool supports from CMC Canada is  also acknowledged.  "Heterogeneous NoC Design for Efficient Broadcast-based Coherence Protocol Support.,"Chip Multiprocessor Systems (CMPs) rely on a cache coherency protocol to maintain memory access coherence between cached data and main memory. The Hammer coherency protocol is appealing as it eliminates most of the space overhead when compared to a directory protocol. However, it generates much more traffic, thus stressing the NoC and having worse performance in terms of power consumption. When using a NoC with built-in broadcast support network utilization is lowered but does not solve completely the problem as acknowledgment messages are still sent from each core to the memory access requestor. In this paper we propose a simple control network that collects the acknowledgement messages and delivers them with a bounded and fixed latency, thus relieving the NoC from a large amount of messages. Experimental results demonstrate on a 16-tile system with the control network that execution time improves up to 17%, with an average improvement of about 7.5%. The control network has negligible impact on area when compared to the switches.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Heterogeneous NoC Design for Efﬁcient Broadcast-based Coherence Protocol Support Mario Lodde Jose Flich Universitat Polit `ecnica de Val `encia Spain Email: mlodde@gap.upv.es Universitat Polit `ecnica de Val `encia Spain Email: jﬂich@disca.upv.es Manuel E. Acacio Universidad de Murcia Spain Email: meacacio@ditec.um.es Abstract—Chip Multiprocessor Systems (CMPs) rely on a cache coherency protocol to maintain memory access coherence between cached data and main memory. The Hammer coherency protocol is appealing as it eliminates most of the space overhead when compared to a directory protocol. However, it generates much more trafﬁc, thus stressing the NoC and having worse performance in terms of power consumption. When using a NoC with built-in broadcast support network utilization is lowered but does not solve completely the problem as acknowledgment messages are still sent from each core to the memory access requestor. In this paper we propose a simple control network that collects the acknowledgement messages and delivers them with a bounded and ﬁxed latency, thus relieving the NoC from a large amount of messages. Experimental results demonstrate on a 16-tile system with the control network that execution time improves up to 17%, with an average improvement of about 7.5%. The control network has negligible impact on area when compared to the switches. I . IN TRODUC T ION AND MOT IVAT ION It is widely accepted that many-core chip multiprocessors (many-core CMPs) will be the norm in the near future. Manycore CMPs integrate tens or even hundreds of processor cores onto the same die. In such systems, all the processor cores and other support components (memory components, accelerators, memory controllers) are interconnected using a high-speed on-chip network (NoC) [1]. Even today, there are several examples of this kind of systems, including prototypes (such as the 48-core Single-chip Cloud Computer developed by Intel [17]) and real products (such as the Tilera’s 100-core TILE-Gx100 processor [27]). If current trends continue, future many-core CMP architectures will implement the hardware-managed, implicitlyaddressed, coherent caches memory model [9]. With this memory model, all on-chip storage is used for private and shared caches that are kept coherent in hardware using a cache coherence protocol. Communication between threads is performed by writing to and reading from shared memory. On the other hand, future many-core CMPs will probably be designed as arrays of identical or close-to-identical building blocks (tiles) connected over an on-chip switched direct network [27]. Each tile contains at least one processing core, caches and a connection to the on-chip network. These tiled architectures have been claimed to provide a scalable solution for managing the design complexity, and effectively using the resources available in advanced VLSI technologies. For the 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.14 59 Fig. 1. Tile-based CMP system. sake of clarity, Figure 1 illustrates the tile-based CMP that we assume for developing our proposal. More speciﬁcally, we show the case of a CMP system with 16 tiles, in which every tile has a core, private L1 caches (for data and instructions) and a slice of a shared but distributed L2 cache. Directory-based cache coherence protocols have been typically employed in systems with switched direct networks, as tiled CMPs are. The directory structure is distributed between the L2 cache banks, usually included into the L2 tags portion [25]. In this way, each tile keeps the sharing information of the blocks mapped to the L2 cache bank that it contains. This sharing information comprises two main components: the state bits used to codify one out of the three possible states the directory can assign to the line (uncached, shared and private), and the sharing code, that holds the list of current sharers. Most of the bits of each directory entry are devoted to codify the sharing code, and therefore the total size of the directory structure is mainly determined by it. This extra storage adds requirements of area and energy consumption to the ﬁnal design and could restrict the scalability of future many-core CMPs [12]. Among the organizations for the sharing code appeared a long time [22], two deserve special attention. On the one hand, a bit-vector (one bit per private cache) could be used to have an exact representation of the sharers in each moment. On the other hand, Dir0B does not dedicate any bits to the sharing code, requiring a broadcast on all coherence operations and giving rise to broadcast-based directory protocols. The Hammer protocol employed in systems built using AMD Opteron processors [5], [16] is the most representative example of the latter1 . Figure 2 and 3 show the basic NoC-level actions performed in both protocol implementations when dealing with a write miss operation in a L1. In the broadcast-based protocol, the corresponding L2 bank is notiﬁed which triggers a broadcast operation. To complete the operation, all the nodes need to notify the requestor with an ACK message (after invalidating their copy). In a bit-vector directory protocol, however, only the node who owns the block is notiﬁed (or the sharers, if they exist). Fig. 2. L1 cache write miss with hammer protocol. Fig. 3. L1 cache write miss with directory protocol. A signiﬁcant fraction of the messages injected on the NoC by a directory-based protocol is due to the coherence requests and their associated acknowledgements generated by the home directory. If we consider a bit-vector directory protocol, when an L2 cache receives a write request for a shared block the request is forwarded to the sharers, which in turn will send an acknowledgement to the requestor to let it know that the block has already been invalidated in their private caches. All the acknowledgements are sent to the same destination roughly at the same time, introducing serialization at the destination node that increases with the number of sharers. Similarly, in the case of the broadcast-based directory protocol, a write request for a shared block would cause the L2 to send invalidations, but this time to all cores (sharers and non sharers). Again, all acknowledgements should be collected by the requestor before the write operation can be ﬁnalized. In the latter kind of protocols, broadcasts also arise on read and write requests when the directory state of the block is private. The serialization issue in a broadcast-based protocol has much deeper impact on performance, since each time a request is broadcasted by the L2 it generates an amount of acknowledgement messages equal to the worst case of a directory protocol (a block shared by all cores), which could 1We use the terms “broadcast-based directory protocol” and “Hammer protocol” interchangeably along this work. 60 even exhaust buffers at the requestor if the number of cores is large enough. Summarizing, bit-vector directory protocols do not scale since the size of the directory grows linearly with the number of cores. However, these protocols generate the minimum amount of network trafﬁc on the NoC. On the other hand, broadcast-based protocols completely remove the onerous part of the directory structure (the sharing code) resulting in a low overhead, completely scalable directory structure (just the state bits are needed in each entry). The counterpart is that the number of messages on coherence events increases linearly with the number of cores, which limits its applicability to systems with a small number of cores. In an isolated design environment, where the NoC and the coherence protocol are designed separately, many optimization opportunities are lost. This is, for instance, the case when the NoC is usually relied as a transmission medium for higher system levels, as for instance hypervisors and coherence protocols. If, however, the NoC is co-designed with upper layers, the overall system can be optimized. In particular, the NoC can be designed to efﬁciently deal with the operations of a broadcast-based directory protocol. This is the approach followed in this work. Figure 4 shows a classiﬁcation of the messages that travel on the NoC of our simulated system when running nine different SPLASH-2 applications under the Hammer coherency protocol (simulation details are provided in the evaluation section). Network messages are divided in four categories: Requests includes all the requests sent by L1 to L2 caches; Responses includes all data messages sent to the requestor and writeback acknowledgements sent by the L2 back to the L1s; Coherence reqs includes the coherence requests (invalidations and cache-to-cache transfer commands) sent from the home L2 bank to all the L1s; and Coherence res includes all the acknowledgements from the L1 caches back to the requestor (another L1 cache). As it can be derived from the ﬁgure, 60% on average of the total trafﬁc is due to coherence requests and their associated acknowledgements. It is also important to note that this amount reaches almost 90% for some applications (Barnes, FFT and Water-nsquared). This highlights the importance of managing properly coherence request messages and their corresponding acknowledgements from the point of view of the NoC. One well-known strategy to reduce the trafﬁc generated by coherence requests is to use a NoC with multicast or broadcast support. With this kind of support, on every coherence event the home directory would inject a single multicast message instead of multiple serialized messages (one per destination). Obviously, this would save a very signiﬁcant fraction of the coherence requests. However, broadcast-based directory protocols cannot take full advantage of a NoC with multicast or broadcast support due to the fact that the acknowledgment messages (Coherence res) are still there and would penalize performance in a noticeable way (note from Figure 4 that these messages represent more than 40% of the total trafﬁc in several cases). In this way, dealing with these acknowledgement I I . H E T EROG EN ENOU S NOC D E S IGN W I TH GATH ER CON TRO L N E TWORK For the sake of clarity, we consider in this paper a tiled CMP system with 16 cores connected through a 4× 4 2D mesh network. Each tile includes a core, a private L1 cache, a bank of the shared L2 cache and a switch. Memory controllers are connected to peripheral switches of the mesh. In the evaluation section we also use a 64-node 8 × 8 system. A. Overall NoC Design The complete NoC switch design is shown in Figure 5. The network is made of 4-stage pipelined switches with the typical stages: IB (input buffer), RT (routing), VA/SA (virtual channel allocation and switch allocation) and XT (crossbar transversal). Each switch is connected to its neighbors through each dimension and direction and to the local nodes. Up to three nodes are located on each tile (the L1 memory, the L2 bank, and possibly a memory controller). Fig. 5. Switch design. Each input port has ﬁve virtual channels (VC0 - VC4), with separate buffering per virtual channel. Virtual channels are used to separate different trafﬁc classes (a message will be routed always through the same VC). Input buffers can store up to four ﬂits per VC. A Stop&Go ﬂow control protocol has been deployed in order to control the advance of ﬂits between adjacent switches. Additionally, the routing stage has been implemented to support the XY routing algorithm with an RT module at each input port. Similarly, there is a virtual channel allocator (VA) module at each input port. The VA module determines which virtual channel of this input port will compete for the crossbar. Additionally, a switch allocator (SA) module has been implemented at each output port, although all the ﬁve SA modules work in a coordinated way. Each SA module has been designed using a round-robin arbiter according to [21]. B. Gather Control Network In addition to this typical NoC switch design, an additional control logic is added to every switch, building the gather control network (GCN). Each switch has an additional set of wires along the link that implement the fast control network. A gather control network consists of a single-wire network made of AND gates, where 15 nodes are located at the leaves of the tree and the other node is at the root of the tree. The gather control network serves as an ACK of all the nodes. Once all 61 Fig. 4. Breakdown of NoC messages when using the Hammer protocol for different SPLASH-2 applications. messages in the Hammer protocol is still an open issue. In this work we tackle such challenge and take an heterogeneous design approach of the NoC in order to speedup the Hammer protocol. In particular, we design a fast and simple gather control network (GCN) associated with the NoC that collects all the acknowledgement messages of the L1 caches. The main NoC is still used to transport requests, responses (including messages with data) and coherence requests. However, all the acknowledgement messaging associated with coherence requests is separated from the NoC and sent through the gather network. In a 16-node system, the gather network consists of 16 one-bit wide subnetworks, each collecting 15 end nodes communicating with a different end node. Each subnetwork is made of few AND gates distributed over NoC switches and with no ﬂow control, routing, or switching in place (it is just a tree of AND gates). We provide different layouts of the gather network. Notice that the NoC environment allows for such heterogeneous design as nodes are close each other, as opposed to the case of off-chip high-performance networks. With this design, the hammer protocol performance is boosted and pairs and even improves the performance of a typical directory-based protocol. 2 This allows for an efﬁcient implementation of the coherence protocol without the need of expensive directory structures required by directory-based approaches. In particular, in mid-size systems (16 nodes) execution time of applications is reduced on average by 7.5% whereas messages in the network are reduced by 60%. In larger 64-node systems, beneﬁts still hold, obtaining reductions of execution time of 10% for Barnes and up to 65% for the FFT application. The rest of the paper is organized as follows. In Section II we describe the proposed NoC with the gather control network in place. Then, in Section III we provide performance and NoC results for different coherence protocols. Then, Section IV describes the related work, and ﬁnally, Section V concludes the paper. 2 In [10] we have also implemented a similar approach for directory-based protocols. logic at each switch for the gather control network. Notice that each subnetwork is made of a single wire, coming from different input ports. The goal of the logic block is simply to AND the corresponding input signals and to distribute the results through the corresponding output ports, depending on the location of the switch in the mesh topology and the selected layout. the nodes set the wire, the AND tree outputs the signal. The receiving node samples the input signal at the next clock edge. We implement one gather control network for each node. Figure 6 shows the number of wires of the gather control signals between switches. Each switch handles both input and output control signals through all its ports. For a N × N mesh NoC, the number of outgoing control signals through all the output ports is N − 1, in our case 15. Each control signal is a different one-bit subnetwork addressed to a different destination (N − 1 destinations). Notice that some output ports handle more control signals than others. This is due to the mapping we have performed for the control signals, following the Y-X layout. Figure 7 shows the layout for the subnetwork dedicated to node 1. ACK messages will be collected through Y columns and then collected through the ﬁrst row. At each switch control signals are grouped with AND gates. In particular, at each switch the incoming control signals and the control signal from the local cores are ANDed. Since there is at least one core connected to each switch, an AND gate is needed at each switch for the control network addressed to node 1, except for the last row of switches. Fig. 8. General GCN block at switches. The logic receives as input 15 control signals from the local core. Each control signal is addressed to a different destination node. XL means a control signal coming from the local port and addressed to destination X . Thus, we have from 0L up to 15L signals (excluding the one for the local core). In addition, we have up to 20 control signals coming from either north (N ) or south (S ) input ports and up to 5 control signals from either east (E ) or west (W ) input ports. switches at the boundaries of the mesh have lower number of input control signals. These signals are then distributed (based on the location of the switch in the mesh) and assigned to the corresponding inputs of the AND gate array. Notice that most input signals will not be required, thus simplifying the array. The outputs of the AND gates are then distributed over the output ports, again depending on the location of the switch and the layout. Notice that 15 output control signals are generated, one per destination in the system (except for the local node). Figure 8 shows the case for the AND gate generated for destination node 0 at switch 5 (second row and ﬁrst column of the mesh). This output signal will be forwarded through the north (N ) output port (following the YX mapping). Notice that the logic at each switch is minimum, consisting of 16 AND gates and most of them with simply two entries (AND gates for signals going through E and W output ports require three entries, collecting ACKs from the south, north, and local ports). The signal distribution blocks are simply a rearrangement of the input and output control signals to the appropriate inputs and outputs of the AND gates. Fig. 6. Control signals distribution for the gather network. YX layout. Fig. 7. YX layout for gather control network for end node 1. Each GCN logic at each switch is connected to its neighbors control logic blocks with dedicated wires. Figure 8 shows the 62 Alternatively, we can design a different mapping strategy, where XY is used instead. In this case we have a mirror effect on the distribution of wires between horizontal ports and vertical ports. To better balance the wires, we can use a mixed approach where wires for half the nodes are mapped YX and wires for the other half of nodes are mapped XY. The latency through the GCN does not change as the path follows the same manhattan distance. Figure 9 shows the case where nodes with the underlined ID number follow the YX mapping and the other follow XY mapping. In this case we achieve a perfect distribution of wires, where each bidirectional port handles 10 wires for a 4 × 4 mesh network. Fig. 9. Control wire distribution for mixed XY/YX mapping. As the system size increases, the number of wires increases, N × N network, this mapping strategy requires (N 2 + N )/2 but not the logic complexity at the switches. In practice, for a wires per direction and dimension. For a 8 × 8 system, the port widths of 256 bits. For a 16 × 16 system, the number number of wires per port is 36, well below typical CMP increases up to 136 wires. As future work we plan to address wire complexity for systems larger than 256-nodes with a hierarchical approach. C. Switch Design: Support for the gather control network In this section we provide an analysis of the overhead of the gather control network. The switch has been implemented using the 45nm technology open source Nangate [26] library with Synopsys DC. Cadence Encounter has been used to perform the Place&Route. Link width and ﬂit size are set to 8 bytes. Table I summarizes the delay and area for each of the modules of the switch (gather network circuits are excluded as it will be analyzed next). Note that the area numbers in the table are for a single instance of each module, thus some of them are replicated in the complete switch. Additionally, the area numbers in Table I have been obtained by independently synthesizing each module to work at its maximum frequency.3 3When the whole switch is synthesized at once, similar numbers are obtained. From the delay results, it is noteworthy to mention that the VA-SA stage becomes the bottleneck in our switch design. module IB RT VA-SA XB area (mm2 ) 3.08 ∗ 10−3 8.91 ∗ 10−5 1.09 ∗ 10−3 4.47 ∗ 10−3 critical path (ns) 0.58 0.30 0.74 0.43 AR EA AND D E LAY FOR TH E SW I TCH MODU L E S . TABLE I Both 4 × 4 and 8 × 8 2D meshes have been implemented using the switch previously described and the 45nm technology open source Nangate [26] library with Synopsys DC and Cadence Encounter. Links have been placed automatically by the Place&Route tool. Two scenarios are analyzed, a conventional 2D mesh without introducing the gather network circuit (conventional 2D mesh), and the same 2D mesh when the gather network circuit is added. When analyzing a conventional 2D mesh, although the VA-SA stage is the slowest one in a switch, the critical path of the whole network is ﬁxed by the delay of transmitting a ﬂit through a link. This delay involves the XB delay of the upstream switch, the delay of the link, and the delay of the logic to select the input VC at the receiving switch. This delay is reduced when the output ports are registered. This design option enlarges the pipeline but reduces the critical path of the whole network. As the buffering and the operating frequency is increased, the power consumption of the network is also increased. Table II shows critical path of a conventional 2D mesh network when both types of switches are used (with and without registers at output ports). In order to analyze the relationship between link length and critical path, two link lengths have been analyzed: 1.2mm and 2.4mm. As it can be seen, a registered switch has a lower critical path (higher operating frequency) than the non registered switch. By now on, the registered switch will be used to analyze the impact when introducing the GCN. Critical path (ns) link length (mm) conventional 2D mesh Non Registered Switch 1.2 2.4 1.86 2.17 TABLE II Registered Switch 1.2 2.4 1.35 1.75 CONV EN T IONA L 2D M E SH CR I T ICA L PATH . Table III shows the critical path of the gather network circuit when analyzed in isolation. The critical path of the registered switch has been added for clarity. The critical path of the gather network is ﬁxed by the dedicated logic that connects the two most physically separate nodes in a chip. Notice that the latency of this circuit depends on the mesh radix, since the gather network critical path crosses the whole network. Remember that the switch critical path remains unalterable for different 2D mesh conﬁgurations because the communication pattern is point to point. Then, as the number of nodes increases and the link length does too, the critical path of the gather network circuit increases. Note that for a 4 × 4 network with a link length of 1.2mm, 63 Critical path (ns) link length (mm) Gather control network conventional 2D mesh 4x4 Network 1.2 2.4 1.23 2.20 1.35 1.75 TABLE III 8x8 Network 1.2 2.4 2.65 4.32 1.35 1.75 GATH ER CON TRO L NE TWORK CR I T ICA L PATH . the gather network critical path is smaller than the critical path of a conventional network, and hence, the gather network is able to work at the same operating frequency than the conventional 2D mesh. That is, in that case, introducing the dedicated circuit does not affect the rest of the network. In contrast, if the link length is increased, the gather network has a higher critical path than the conventional 2D mesh. That implies two possibilities. First, the whole network increases its clock cycle to that ﬁxed by the gather network circuit. Second, the 2D mesh operates at the same clock cycle while the gather network circuit requires 2 clock cycles to operate. When designing an 8 × 8 network, it can be seen that the gather network circuit does not scale as well as the point-topoint communication protocol of the NoC. However, it can be noticed that the worst case (comparing the gather network circuit with the faster switch with long links) the gather control network is still able to cross the network in 4.32 ns (3 clock cycles). As we will see in the performance evaluation section, much higher delays in the gather network will not induce any penalty to performance. Table IV shows the area of a switch and the area occupied by the gather network circuitry in a single switch. Note that the area occupied by the gather network circuit is smaller than the area of the whole switch. In a non registered switch, the area overhead introduced by the gather network circuit is just an 1.54%. In a registered switch this overhead increases up to 2.81%. Area (mm2 ) switch gather network Non Registered Switch 0.132 0.20 ∗ 10−2 Registered Switch 0.165 0.45 ∗ 10−2 SW I TCH AND GATH ER CON TRO L N E TWORK C IRCU I T AR EA . TABLE IV I I I . EVA LUAT ION In this section we provide an evaluation of the resulting coherence protocol co-designed with the NoC. In particular, we compare the performance with different applications when using the Hammer protocol with neither broadcast nor gather support at NoC level (HAMMER), the Hammer protocol with NoC built-in broadcast support (HAMMER BC), the Hammer protocol with both broadcast and gather control network support (HAMMER BC GCN), and a MOESI directory-based protocol (DIRECTORY). We implemented the NoC and all the coherency protocols with an in-house memory/network simulator which is cycleaccurate and ﬂit-level accurate. The memory/network module has been integrated into the Graphite simulator [13] which allows us to run applications and capture all the memory accesses. With this system platform, we ran several applications of SPLASH-2 benchmark suite on a modeled tiled CMP system with 16 cores connected through a 4×4 2D mesh. The switches are modeled following a 4-stage pipelined approach using wormhole switching, XY routing, and stop&go ﬂow control. Flit size is set to 10 bytes. Broadcast is achieved by following the XY routing. By default, the control gather network is modeled with 2 cycle delays. Each tile has a 128KB private L1 cache (64KB for instructions and 64KB for data, tag and cache latencies are set to 1 and 2 cycles) and a 512KB L2 bank (tag and cache latencies are set to 2 and 4 cycles); the total size of the L2 cache is thus 8MB. Figure 10 shows the normalized execution time for different SPLASH-2 applications when using the different protocols and NoC implementations. As a ﬁrst result, we can see how the use of directory protocols achieves much lower execution time for applications when compared to a Hammer protocol with no NoC support. In some applications, execution time can be reduced by 17%. On average, execution time is reduced by 6%. Fig. 10. Normalized execution time for different coherence protocols and different applications. Now, as we provide NoC support for the Hammer protocol we can see how execution time reduces signiﬁcantly. As a ﬁrst measure, adding a broadcast facility helps in reducing execution time by %3 on average (in some applications execution time is reduced by %7 for the LU application). Furthermore, execution time is reduced again when the gather control network is in place. On average, execution time is now 8% lower than a naive Hammer protocol. Indeed, execution time is even lower that when using directories. This is due to the speedup in the notiﬁcation of the ACK messages that are needed also in directory-based protocols. This beneﬁt is much higher than the overhead in broadcasting all the nodes in the system. Now, let us focus our attention to the messages in the network. Figure 11 shows the normalized number of messages sent over the network for the different protocol-network support combinations. As can be seen, the directory-based approach achieves a signiﬁcant reduction in messages overhead. Indeed, only 40% of messages are needed (when compared with HAMMER). Notice now how the different NoC additions (broadcast and gather control network) help in reducing signif64 IV. R E LAT ED WORK In an isolated design environment, where the NoC and the coherence protocol are designed separately, many optimization opportunities are lost. Cache coherence protocols have traditionally maintained a ﬁrm abstraction of the interconnection network fabric as a communication medium, thus disregarding the opportunities of optimizing the NoC at design time to efﬁciently deal with the coherence operations. More recently, however, some proposals exploring on-chip network optimizations for cache coherence protocols have appeared. Cheng et al. [4] leveraged the heterogeneous interconnects available in the upper metal layers of a chip multiprocessor, mapping different coherence protocol messages onto wires of different widths and thicknesses, trading off their latency-bandwidth requirements. Subsequently, Flores et al. [8] propose to combine a protocol-level technique (called Reply Partitioning) with the use of a simpler heterogeneous interconnect. Eisley et al. [7] propose in-network cache coherence, an implementation of the cache coherence protocol within the network based on embedding directories in each switch node that manage and steer requests towards nearby data copies. This approach enables in-transit optimization of memory access delay and shows good scalability. In [2], it is presented a priority-based NoC, which differentiates between short control signals and long data messages to achieve a signiﬁcant reduction in cache access delay. Additionally, the authors propose to use more efﬁcient multicast and broadcast schemes instead of multiple unicast messages in order to implement the invalidation procedure and provide support for synchronization and mutual exclusion. Walter et al. [24] explore the beneﬁts of adding a low-latency, customized shared bus as an integral part of the NoC architecture. The bus is used for some transactions such as broadcast of queries, fast delivery of control signals, and quick exchange of small data items. More recently, Vantrease et al. [23] advocate nanophotonic support for building high-performance simple atomic cache coherence protocols. On the other hand, broadcast-based cache coherence protocols can completely remove the important overhead that the directory structure would entail in a many-core CMP system. The AMD’s Coherent HyperTransport (TH) [5] implements the Hammer broadcast-based protocol enabling the construction of small-scale multiprocessors. Subsequently, the HyperTransport Assist [6] developed by AMD for the 12-core AMD Opteron processor-based system code-named Magny Cours, added a directory cache to reduce the frequency of broadcasts, and therefore, to enable larger core counts. Also, the Intel’s QuickPaht Interconnect (QPI) implements two different protocol modes [11]. In one of them (source snoop protocol mode) coherence transactions are broadcasted and every core must respond to the home with a snoop response that indicates the state of the block at that core. This mode allows for lower-latency coherence transactions at the expense of not scaling well. JETTY [15] and Blue Gene/P [20] are two proposals to ﬁlter the broadcast requests that would miss at Fig. 11. Normalized number of injected messages for different coherence protocols and different applications. GCN signals are not included. icantly the number of messages in the network. Indeed, when combined, the Hammer protocol has similar trafﬁc overhead than DIRECTORY. Therefore, the Hammer protocol is able to cope with directory-based protocols without the overhead of managing directory structures, and with no signiﬁcant messaging overhead over the network. Figure 12 shows the performance achieved with HAMMER BC GCN when the gather control network has different delays. We run all the applications when using a gather control network ranging from a 2-cycle delay up to a 128-cycle delay. Notice that we showed realistic delays of up to 4 cycles can be achieved even for 8 × 8 conﬁgurations. Fig. 12. Execution time with different gather network delays. As we can observe, the results are quite insensitive to low and medium gather control network delays. Indeed, performance is not signiﬁcantly affected even for a 64-cycle GCN. Only, on average, execution time is increased by 1%. Indeed, for a 8 cycle delay (which gives a large slack in the design), performance is unaltered. Only when using large (and unrealistic) 128-cycle delays the performance is impacted and the beneﬁts of the GCN are cancelled (performance is similar to the HAMMER BCAST approach). Finally, we have obtained similar results for an 8 × 8 CMP system where the gather control network is set to 4 cycles. In particular, beneﬁts range from 10% in execution speedup for Barnes to 65% for FFT. Messages also get signiﬁcantly reduced. Results are not shown due to space limitations. 65 destination nodes in order to reduce energy consumption due to cache look-ups. Filtering has also been proposed at source nodes [3], [14] to save energy and bandwidth. Proposals for efﬁcient multicast support in on-chip networks have also appeared [18]. Additionally, it has been evaluated the case of using this kind of support in combination with a cache coherence protocol implementing imprecise directories (the Hammer protocol could be seen as using an inexact directory), demonstrating that multicast support is not enough to completely remove the performance degradation that the inexact sharing codes introduce [19]. V. CONC LU S ION In this paper we have addressed the co-design of the NoC and the coherence protocol. The Hammer protocol scales to a large number of cores as it does not require large memory structures to keep the sharer’s list typically found in directorybased protocols. However, the network trafﬁc requirements of Hammer impacts the performance signiﬁcantly. Therefore, a smart co-design of the network and the protocol is needed. We have redesigned a standard mesh-based XY-routed wormhole-based NoC and extended to support a dedicated and set-aside gather control network. The new control network is made of a set of AND gates along a tree wiring enabling the fast notiﬁcation of ACK messages from every node to a certain destination. We have seen that the wiring requirements for the gather control network keep low up to 256-core systems, and with a low delay of few network cycles. Area impact is lower than 3% of the switch area. With the gather control network support and built-in broadcast, the Hammer protocol is able to cope with the performance of directory based approaches, even achieving better performance. Also, network trafﬁc is reduced signiﬁcantly and put on par to directory-based protocols. Therefore, the Hammer protocol behaves as the directory-based protocols without the expensive control structures required, thus enabling Hammer to effectively scale. V I . ACKNOW L EDG EM EN T S This work was supported by the Spanish MEC and MICINN, as well as European Commission FEDER funds, under Grant TIN2009-14475-C04. It was partly supported by the project NaNoC (project label 248972) which is funded by the European Commission within the Research Programme FP7. "Dynamic Flow Regulation for IP Integration on Network-on-Chip.,"Flow regulation is a traffic shaping technique, which can be used to achieve communication performance guarantees with low buffering cost when integrating IPs to network-on-chip architectures. This paper presents dynamic flow regulation, which overcomes the rigidity of static flow regulation that pre-configures regulation parameters statically and only once. The dynamic regulation is made possible by employing a sliding window based online flow (σ,ρ) characterization technique, whereσbounds traffic burstiness andρreflects the average rate. The characterization method is effective and can be implemented in hardware with small area and high speed. The resulting dynamic regulation can adaptively adjust the traffic regulation strength in response to real traffic workload scenarios. As such, it makes more efficient use of the system interconnect resources, leading to significant improvement in network performance.","Dynamic Flow Regulationfor IP Integration on Network-on-ChipZhonghai Lu and Yi  WangDepartment of Electronic Systems, School for ICT KTH Royal Institute of Technology, Stockholm, Sweden{zhonghai, yiwang}@kth.seAbstract—Flow regulation is a trafﬁc shaping technique, which can be used to achieve communication performance guarantees with low buffering cost when integrating IPs to network-on-chip architectures. This paper presents dynamic ﬂow regulation, which overcomes the rigidity of static ﬂow regulation that pre-conﬁgures regulation parameters statically and only once. The dynamic regulation is made possible by employing a  sliding  window  based online ﬂow (σ, ρ) characterization technique, where σ bounds trafﬁc burstiness and ρ reﬂects the average rate. The characterization method is effective and can be implemented in hardware with small area and high speed. The resulting dynamic regulation can adaptively adjust  the  trafﬁc  regulation  strength in response to real trafﬁc workload scenarios. As such, it makes more efﬁcient use of the system interconnect resources, leading   to signiﬁcant improvement in network performance.Keywords: Flow Regulation, IP Integration, Network-on-ChipINTRODUCTIONTo manage ever increasing complexity, advanced System- on-Chip (SoC) designs typically encompass a system inte- gration process based on existing computation, storage and interconnect IPs . These IPs are designed separately, following a common interface, for example, AMBA, AXI, OCP-IP etc. Later, the computation and storage IPs are integrated together with the interconnect IP. While a common interface smooths the hardware integration, guaranteeing application performance during the integration remains an open challenge because of trafﬁc diversity and unpredictability. This is becom- ing a critical issue as advanced SoC interconnects are moving from bus to network-on-chip (NoC) architectures in order to accommodate hundreds of IPs, which raise complexity and performance uncertainty in orders of magnitude [1]. 	Fig. 1. Dynamic regulation with online characterizationTo address the problem of IP integration on NoCs, trafﬁc regulation was proposed by which trafﬁc ﬂows or streams can be shaped into desired characteristics before being admitted into the interconnect [2][3]. Figure 1 illustrates the ﬂow regulation where a regulator is inserted between an IP and the interconnect to perform trafﬁc shaping. Trafﬁc regulation brings two main advantages: (1) It turns unknown or unde- sired trafﬁc characteristics into desired ones, thus facilitating performance analysis and performance guarantees. (2) It can be used to decrease packet delay and buffer requirement since it can control network congestion status and reduce backlogs due to changing ﬂows’ timing and burstiness. To provide a solid foundation for the performance analysis and control, trafﬁc regulation is preferably conducted under a mathematical formalism. (σ, ρ) based regulation, where σ bounds a ﬂow’s burstiness and ρ sustainable rate, is such a technique studied under network calculus [4][5][6], a ﬂow-based queuing theory for performance guarantees in communication networks.The (σ, ρ) based regulation requires characterizing ﬂows’ σ and ρ values before applying them to losslessly control the trafﬁc admission. Depending on whether the parameters are determined statically or dynamically, we can apply static or dynamic regulation, respectively. While a static approach [2][3] is rigid and often cheaper to implement, it cannot adapt to dynamic scenarios. As the system gets more complex with more and more IPs communicating with each other, the system behavior is becoming more dynamic. In these cases, a dynamic approach is desired to enhance the network utilization and improve the application performance.To realize dynamic trafﬁc regulation requires dynamic trafﬁc characterization. Based on a sliding window concept, we propose an online ﬂow characterization technique, which samples incoming trafﬁc with overlapped time windows and makes predictions to obtain a ﬂow’s (σ, ρ) values. With design optimizations, this technique can be implemented as a lightweight hardware module. Figure 1 shows the module as a characterizer. In our experiments with MMP (Markov Modulated Process) trafﬁc ﬂows on an NoC, we show the ﬁdelity and adaptivity of our dynamic trafﬁc characterization via comparing it with ofﬂine trafﬁc proﬁling. We also demon- strate the delay efﬁciency of dynamic regulation in contrast to static regulation and no regulation.Note that the static regulation is inﬂexible and may be overly restrictive, but it can be used to ensure per-ﬂow worst- case delay bound through formal analysis. With the dynamic regulation, the trafﬁc ﬂows are not absolutely bounded, since the predication can only give an estimated upper bound. Violating the estimated bound is possible and allowed. As a consequence, the hard performance guarantee is lost and only			soft performance promise is possible.The rest of the paper is organized as follows. Section   II discusses related work. In Section III, we introduce the basics of network calculus and (σ, ρ) based regulation. Section IV details the sliding window based ﬂow characterization from concept, design to implementation. In Section V, we describe the dynamic ﬂow regulation. In Section VI, we report experiments and results. Finally, we conclude in Section VII.RELATED WORKNetwork calculus was pioneered by Cruz [6] and Chang [5]. Since its creation, it has become a very active research area and been successfully applied to design QoS networks for ATM [4], Internet with differentiated and integrated services [4] [7], and recently for WSNs [8] etc. The stochastic version of network calculus can be found in [5][9]. Realtime calculus [10] combines network calculus with real-time scheduling theory to reason about the worst-case delay and backlog bounds under various scheduling policies (ﬁxed priority, TDMA, EDF, FIFO, etc.) under different workload and service conditions.Based on network calculus, trafﬁc shaping was previously proposed for ATM and Internet to enable QoS guarantees for network users [4] [9], as it can control the trafﬁc burstiness and rate from users. If a user injects trafﬁc with a rate more than the agreed one, the excessive trafﬁc may be dropped or sent out as best effort trafﬁc. Based on such control mechanisms, network service providers can associate billing information with different service-level agreements for users.Inspired by this technique, trafﬁc shaping was introduced for SoC designs. In SymTA/S (Symbolic Timing Analysis for Sys- tems) [11], trafﬁc shapers were used to increase the minimum distance between events and thus reduce peak workload bursts so as to improve worst-case performance. In [12], greedy shapers were analyzed in distributed embedded systems and incorporated into a system-level modular performance analysis framework to allow deriving a timing guarantee for real-time trafﬁc streams.In [2][3], trafﬁc shaping was proposed as a trafﬁc regulation technique to deal with the QoS IP integration problem. As SoC applications typically do not allow dropping packets, lossless regulation is essential. In [2], regulation spectrum was formally deﬁned to give a valid range for regulation parameters so as to shape (σ, ρ) ﬂows without data loss. It is also shown that this spectrum can be exploited to improve delay and reduce backlog bounds. Because different ﬂows may have conﬂicting regulation requirements due to sharing network resources, the problem of optimized regulation arises. In [3], the regulation problem was formulated with buffer optimization as objectives, and signiﬁcant reduction in packet delay and required buffers can be achieved by solving the optimization problem. Both works [2][3] dealt with static regulation, requiring the characterization of trafﬁc ﬂows’ (σ, ρ) values ofﬂine at design time.Though ofﬂine characterization methods [13][14] such as static trafﬁc analysis and trace-based proﬁling are possible, they are inﬂexible and can not capture trafﬁc dynamism or only partially. Recently, dynamic trafﬁc characterization and prediction has received increasing attention, thanks to its potential in guiding quality system design in achieving high performance and low power, as demonstrated in [15][16][17]. With a table driven approach, Kaxiras and Young [15] used coherence communication prediction in distributed shared- memory systems to speculate data needed by several pro- cessors and to deliver the data as soon as possible. In [16], Huang et al. proposed a table-driven predictor to predict end-to-end communication behavior in NoCs. Their method only predicted one future time interval and the predicted communication volume is either zero or the current quantity. In [17], a fuzzy logic based algorithm is used to search for similar trafﬁc patterns in the trafﬁc history to predict prospective data points. Since it aims to pinpoint the actual trafﬁc volume and does not balance the consideration of current and past trafﬁc information, it is very difﬁcult to be accurate. The prediction accuracy for chaotic patterns with continuous behavior reaches up to 10 data points while for a discontinuous (high burstiness)real communication trace only up to two data points.This paper proposes a hardware-oriented online ﬂow charac- terization technique from concept, design to implementation. Our approach is based on a sliding window which can balance the consideration of current and history trafﬁc information. Very different from the others, we aim to predict an arrival curve (refer to Section III-A) which provides an upper bound for the future trafﬁc per time window rather than the actual data volume. Furthermore, our solution is designed in a way that is suitable for hardware implementation. Based on this online characterization, we further integrate it into a dynamic regulation architecture for NoCs.(σ, ρ) BASED FLOW REGULATIONNetwork  Calculus BasicsNetwork calculus deals with trafﬁc ﬂows and relies on two fundamental abstractions, arrival curve and service curve, for worst-case performance analysis.In network calculus [4][6], a unicast trafﬁc stream sent from a source to a destination is called a ﬂow, which is denoted as f (t) representing the accumulated number of bits transferred in the time interval [0, t]. Network calculusdeveloped the abstraction of cumulative arrival curve α(t),also called envelop process, to upper-bound a trafﬁc ﬂow. A well-studied arrival curve is the linear function α(t)= σ+ ρt, where σ bounds the trafﬁc burstiness, and ρ the average (sustainable) rate. This afﬁne function is very useful since it is simple and often enables to derive closed-form analytic results. Another abstraction is cumulative service curve β(t) that models the minimum service behavior of a network element. A commonly used service model is the latency-rate function βR,T (t)= R(t T )+, where R is the minimum service rate and T the maximum processing latency of the node [7]. Notation x+ = x if x > 0; x+ = 0, otherwise.Based on the two abstractions, network calculus deliversbasic results in per-ﬂow delay bound, backlog bound, and output arrival curve. Consider a single-node case. A ﬂow f (t)constrained by a (σ, ρ) arrival curve is served by a node guaranteeing a latency-rate service βR,T (t). According to [4], the maximum delay for the ﬂow is bounded by Eq. (1) and the buffer required for the ﬂow is bounded by Eq. (2): a communication task graph, for example, a radio processing application [19], may have trafﬁc bandwidth annotated on. This method is purely static, and thus only applicable to static SoC applications with good trafﬁc knowledge.Another way is to use static trafﬁc proﬁling. Based on¯D = T + R	(1)B¯   = σ+ ρ · T	(2)The output arrival  curve  can  be  described  as  α∗(t)= (σ+ ρT )+ ρt. As can be observed, the output characterization follows the same afﬁne model as the input ﬂow. This nice property enables that the single node analysis can be extended to analyze cascaded multiple-node cases.Flow RegulationThe (σ, ρ) trafﬁc model is a powerful characterization which helps to ensure QoS guarantees in networks [4][5]. However, in reality, trafﬁc ﬂows are diverse and often they do not follow the (σ, ρ) envelop process. Because of this reason, trafﬁc admission control has been extensively studied in order to shape an incoming trafﬁc into the desirable ﬂow characteristics. Speciﬁcally, the leaky bucket model [18] can be employed to shape an incoming trafﬁc ﬂow and make it conform to a (σ, ρ) envelop process. system simulation models, trafﬁc ﬂow characteristics may be obtained through analyzing simulated communication traces. However, each communication trace is speciﬁc to a particular system conﬁguration, and to obtain good characteristics re- sults, sufﬁciently long traces are required, leading the proﬁling process to be tedious and time-consuming.In this paper, we propose a dynamic trafﬁc proﬁling tech- nique, which characterizes a ﬂow’s (σ, ρ) values online using hardware. While trafﬁc proﬁling derives the characteristics of given trafﬁc history, online characterizing involves also a pre- diction process, which projects the future trafﬁc characteristics based on trafﬁc history.ONLINE FLOW CHARACTERIZATIONIn this section, we present our online ﬂow characterization from concept, design to implementation.A. Concept: Sliding Window based CharacterizationFig. 2. A (σ, ρ) regulator model.Figure 2 shows a leaky bucket (σ, ρ) regulator model.  It works as follows: The server conditionally processes the incoming data stream. The condition is that, to process one unit of data, there needs exactly one token from the bucket, i.e., one token per unit of data. There is a ”ready/stall” signal to control the service availability. The bucket has a maximum capacity of σ tokens. Initially, the bucket is full containing σ tokens and the token is ﬁlled at the rate of ρ. With this mechanism, during any time interval t, the generated number of tokens is bounded by σ+ ρt. Therefore, the amount of output trafﬁc isenveloped by σ+ ρt.C. Trafﬁc CharacterizationThe leaky bucket model can be employed to shape trafﬁc to conform to the (σ, ρ) characteristics. But, before shaping a trafﬁc ﬂow, we have to determine the two parameters, σ and ρ. This is the trafﬁc characterization problem investigated in the paper. One way is to use static trafﬁc analysis. Given an SoC application, the trafﬁc characteristics over communication channels may be statically analyzed and annotated. As a result, Fig. 3. Sliding window mechanism for trafﬁc characterizationThe characterization is based on a sliding window mecha- nism, as illustrated in Figure 3. The basic procedure involves three sequential steps: sampling, characterizing, and shifting. For each time window with length Lsw, an input ﬂow is sampled cycle by cycle with values (t, f (t)) recorded; At the characterizing step (Section IV-B), the (σ, ρ) values are computed and updated at the end of each sampling window, and they are used for online regulation (Section V). After this, the sampling window is shifted forward at a step of Lsw/N, where N is a natural number. The step window is called prediction window. Its length equals to Lpw = Lsw/N, which gives the valid period for each projected (σ, ρ) pair. N is called overlapping ratio. Then the three steps repeat through the system execution.Note that consecutive sampling windows may be over- lapped. If N = 1, two consecutive sampling windows have no overlapping; If N = 2, two consecutive sampling windows overlap with a length of Lsw/2. Figure 3 shows the case with N = 3, where three consecutive sampling windows overlapwith a length of Lsw/3. Window overlapping is important to ensure that the window-by-window characterized results enjoy high continuity, taking into account both current and past states when predicting the next state.Design: (σ, ρ) CharacterizationWe detail the characterizing process, ﬁrst on ρ and then σ. We also describe design optimizations for characterizing σ.Characterization of ρ: The online rate characterization consists of two steps: rate proﬁling and rate prediction. Both steps are conducted on a per-window timing basis.Based on the window mechanism, the online proﬁling of ρ is straightforward. Within each sampling window, a ﬂow is sampled at each time instant ti for its trafﬁc volume f (ti), where ti [1, Lsw], and these two values are maintained in two counters, one for each. At the end of each sampling window, ρ is computed by f (Lsw)/Lsw to obtain the per-window average rate of the ﬂow.The  task  of  rate  prediction  is  to  speculate  ρˆn+1  based  on previous  proﬁled  rate  results  ρn,  ρn−1,  ·· ·,  where  n  is  the sequence number of a sampling window. Speciﬁcally, ρn−1, ρn, ρn+1 represent the previous window, current window, and next window rate. The projected rate ρˆn+1  is used to regulate the ﬂow during time window n + 1. In our current approach, projecting  ρˆn+1,  we  use  σˆ n+1 = σn + (σn	σn  1) to  projectσˆ n+1.The critical check is a key step for burstiness character-ization. Condition 4 is recursive since it is performed for t	[tc, ti] whenever ti moves forward cycle by cycle. This  is too computation intensive and impractical to implement in hardware. To simplify the condition and thus reduce hardware complexity, we use ti to replace t, which turns the recursive check into a point-wise check. In this way, the condition isreduced to f (tc)+ f (ti)  (ti	tc)	f (ti). With simple transfor-mation, it can be rewritten asf (tc) · ti ≥ f (ti) · tc.	(5)This simpliﬁcation enables an efﬁcient hardware solution to conduct the critical check. We consider it acceptable, since the prediction itself is an approximation process.Implementation: The Characterizer in HardwareReset 0we use ρn−1 and ρn to project ρˆ n+1 in a simple way:ρˆn+1 = ρn + (ρn − ρn−1).	(3) The  projected  ρˆn+1  is  composed  of  a  base  value  of  ρnand an offset value of ρn ρn−1, which captures possiblerate variation. By using consecutive proﬁling results, thisprediction exploits the continuity brought by the sliding- window mechanism to avoid abrupt change.2) Characterization of σ: Similarly to rate characteriza- tion, the burstiness characterization also involves two steps: burstiness proﬁling and burstiness prediction.The burstiness proﬁling involves ﬁnding the maximum burstiness value online such that σ(t)= max  σ(ti) : σ(ti)= f (ti) ρ ti, 1 ti Lsw . This is done by ﬁrst recursively performing a check on the critical instant, tc, called critical check. A time instant t is considered critical if, at this instant, the trafﬁc volume f (t) surpasses the estimated trafﬁc bound calculated with the previous tc. The condition can beformulated as an inequality:f (tc)+ f (ti) · (t − tc) ≥ f (t), ∀ t ∈ [tc, ti].	(4)At the start of a sampling window, the ﬁrst point t1 is initiated as the ﬁrst critical instant. As the time advances, the check is performed at each and every clock cycle until the end of the sampling window. If the critical check gets passed (the condition satisﬁed), nothing happens. Otherwise, the ﬁrst t that fails the condition becomes the new tc and the corresponding f (t) will replace f (tc). At the end of thesampling window, the ﬁnal value of (tc, f (tc)) will be used to calculate σ  according to σ = f (tc) − ρ · tc. Similarly  to Fig. 4. The characterizer micro-architectureFigure 4 shows the structure of the characterizer imple- mentation. The sampling unit samples the values (t, f (t)) whenever an input packet arrives. Then this value is fed into N parallel Characterize units, where N is the overlapping ratio between the sampling window length Lsw and the prediction window length Lpw. The parallelization structure is to realize the overlapped window based concurrent characterization. The N Characterize units do not start simultaneously rather with an initial interval equal to Lpw one after another. This is realized by sequentially controlling the reset signal through the Delay unit. They sequentially compute the characterized (σ, ρ) values, based on the sliding window mechanism. Note that their executions are overlapping. A multiplexer with one- hot encoding as the control signal selects the results and feeds them to the Predict unit sequentially. This unit computes the estimated values of (σˆ , ρˆ) for each prediction window.To optimize the design, we choose the sampling windowlength, Lsw, to be 2m (m is a natural number). This brings two advantages. One is that we do not need to implement the boundary check and counter reset because overﬂow au- tomatically achieves such effect. The other is that this valuecan be directly used as the denominator for computing ρ and σ, and all divisions involved in the calculation of these two parameters can be replaced by a right shifter. The characterizer was synthesized in a 45 nm technology. It can run up to 2 GHz with an area of 12181 μm2.DYNAMIC REGULATIONWe integrate the online ﬂow characterization into (σ, ρ) based regulation, resulting in dynamic ﬂow regulation.Overview of Dynamic RegulationAs already shown in Section I, Figure 1 presents an overall architecture of the dynamic regulation. There is one char- acterizer and one regulator per IP. The characterizer does the window-based online (σ, ρ) characterization as described in the previous section and uses the predicted (σ, ρ) values to conﬁgure the regulation parameters in the regulator. The regulator becomes a reconﬁgurable module which performs the leaky bucket regulation. As ﬂows are continuously sent from IPs, (σ, ρ) values are dynamically updated window-by-window and used to shape trafﬁc at run time. In view of this dynamic structure, the static regulation is a special case where there are no online characterizers, and it contains only regulators which are pre-conﬁgured with regulation parameters obtained at design time.In fact, Figure 1 shows a distributed dynamic regulation architecture, where the characterizers make regulation de- cisions locally without global optimization. In contrast to such local-decision based distributed decision making, one can alternatively design and implement centralized decision making to globally optimize the regulation decisions [3]. In theory, this kind of global optimization is necessary as locally optimal decisions may lead to be globally suboptimal in per- formance and resource utilization due to lack of coordinations. However, such global optimization is not scalable for hardware implementation and thus preferably suits for static or semi- static regulation, since static decisions are made once via ofﬂine optimization.The Reconﬁgurable Regulator ImplementationFollowing the leaky-bucket control mechanism (Section III-B), we implemented a dynamically reconﬁgurable regulator in HDL to quantify its hardware speed and area. In [20], a regulator implementation was discussed but only in theory. No hardware design and synthesis results were reported.The reconﬁgurable regulator realizes dynamic regulation while admitting trafﬁc into the interconnect. It contains three conﬁguration hardware registers which store the regulation parameters, one for σ and two for ρ. A pair of integers is used to represent rate ρ, one as the numerator and the other denominator. All the three parameters are represented in 14- bit integers in the implementation. At the end of each time window, the (σ, ρ) values are updated by the characterizer.For the regulator design, it is important to ensure that, given the same rate, tokens are evenly generated so as to remove unwanted bursts. For example, for ρ = 0.5, the regulator may receive values 2048/4096 or 1024/2048. A regulator conﬁg- ured with these two pairs of values should result in exactly the same token generation behavior. We use the following Algorithm 1 to achieve this.Algorithm 1 Pseudo code of token generation	 Let num be the numerator of ρLet den be the denominator of ρnum	denStart:compare = 0;token = σ; //Admitting 1 packet consumes 1 token.while TRUE dowait for clock rising edge; compare = compare + num; if compare	den thencompare = compare	den;if token < σ thentoken = token + 1;end ifend if end while EndThe numerator is self-added every cycle, and whether to generate a token or not is determined by the comparison between the numerator accumulation and the denominator. The algorithm is very simple, so the resulting ASIC can enjoy a high speed. After synthesized in a 45 nm library, the reconﬁgurable regulator can reach a maximum speed of 1.4 GHz, consuming 2238 μm2.Per-ﬂow vs. Per-aggregate RegulationIn our system, there is one and only one pair of characterizer and regulator per IP. From an IP, there can be many ﬂows sent to many different destinations. Different ﬂows may have their own distinct characteristics. We can have two distinct regulation approaches:Per-ﬂow regulation: This means that each ﬂow is treated differently. This is achievable by equipping a ﬂow with a ﬂow ID. The characterizer can be designed as a multi- ﬂow characterizer that tracks and characterizes each ﬂow individually online. A multi-ﬂow regulator can be realized to implement the ﬂow-sensitive regulation, reg- ulating different ﬂows according to their own regulation parameters.Per-aggregate regulation: The characterizer and the reg- ulator do not distinguish ﬂows. All ﬂows sent from the same IP are treated as a single ﬂow aggregate. The characterizer proﬁles this aggregated ﬂow, sends regulation decisions for the aggregate, and the regulator realizes the aggregate regulation.Per-ﬂow regulation can be more precise than per-aggregate regulation, but apparently, is less scalable in implementation. Suppose there are n nodes in the system and each node maycommunicate with each other, then each node has up to n ﬂows, resulting in a total of up to n2 ﬂows. With the per-ﬂow regulation, the system implementation complexity is O(n2). While with the per-aggregate regulation, it is O(n). In the following, we have adopted the per-aggregate regulation.EXPERIMENTS AND RESULTSWe present experiments and results concerning the ﬁdelity of our online characterization technique followed by the ben- eﬁts of applying dynamic regulation in a NoC environment.Effectiveness of Online vs. Ofﬂine CharacterizationExperimental Purpose and Setup: The ﬁrst experiment is to (1) validate the sliding window based online charac- terization against ofﬂine characterization; (2) investigate the effect of sampling window overlapping, as it is a key to ensure continuity in the predicted results.We modeled the characterizer in Matlab. We used a two- state (on/off) MMP (Markov Modulated Process) [21] as the trafﬁc model in the experiment. The reasons are two fold:(1) To validate the effectiveness of an online characterization technique, we need to have the trafﬁc model known so that we can compare it with an ofﬂine characterization method. In other words, it is impossible or of no interest to validate a characterization method with completely random trafﬁc; (2) The MMP can well represent trafﬁc burstiness [21], which is a typical trafﬁc characteristics in SoCs.Effectiveness Result: We ran extensive MATLAB sim- ulations to compare the effectiveness of online and ofﬂine characterization methods. We report detailed results for one case where the MMP trafﬁc trace has a unit pattern length of 100 cycles, burst rate of 0.9 and burst proportion of 30%. The sampling window is set to 8192 cycles and the prediction win- dow 2048 cycles. This means that four consecutive sampling windows overlap with each other.400300200100Fig. 5. Online characterization and ofﬂine characterizationFigure 5 illustrates the difference between the two char- acterization methods. Given complete MMP traces (the blue line), the ofﬂine characterization statically computes their (σ, ρ) values. This is to mimic the typical ofﬂine proﬁling method. Therefore, for a single trace, the ofﬂine method will result in only one arrival curve (the back line). The online prediction differs in two aspects. First, it does not require complete MMP traces beforehand. The proﬁling is based on window- by-window sampling and prediction. As can be observed, this online method results in one arrival curve per window (the red line). The resulting connected curve follows closely and adaptively to the dynamic trafﬁc accumulation. Effect of Sliding Window Overlapping: To show the impact of overlapped sampling windows, we conduct another set of tests in which we vary the ratio between the sampling window length and the prediction window length. In order to quantify the effect, we deﬁne a deviation metric, which mea- sures the percentage of deviation occurrences among the total simulated cycles. We say that a deviation occurs whenever the real accumulated trafﬁc volume at instant ti is greater than that projected using the current arrival curve, i.e., f (ti) > α(ti).2520151050	1	2	4	8	16	32Sampling Window Length/Prediction Window LengthFig. 6. Deviation in relation to window overlappingFigure 6 shows the results with multiple overlapped win- dows. Clearly, as the overlapped windows increase, the devia- tion reduces. This means that with more historical information accounted, the projection can achieve higher continuity and thus higher ﬁdelity. However, higher overlapping means more computation, which implies higher design complexity and thus a design tradeoff.Effect of Dynamic vs. Static vs. No RegulationExperimental Purpose and Setup: The second exper- iment is to compare the network performance of dynamic regulation against static regulation and no regulation. We shall investigate the performance gain with dynamic regulation in not only the observed maximum packet delay but also the average packet delay.Our experimental platform is based on a cycle-accurate pin- accurate NoC using deﬂective routing [22] written in VHDL at the RT level. The synthesizable characterizer described in Section IV-C and the regulator in Section V-B are integrated onto the hardware modeling platform. With deﬂective routing, the router may transfer packets to unfavored links upon losing link arbitrations. Because of directing packets to other links rather than buffering them, the router requires only one buffer per port, minimizing area cost. The reason for using the deﬂective network is that the routing algorithm is fully adaptive to network congestion status. In comparison with a deterministically-routed network, the network performance  is less sensitive to trafﬁc regulation due to its capability of exploiting path diversity. Therefore it is a more challenging case for illustrating the effect of trafﬁc regulation. The network ejects packets immediately once they reach destinations.Figure 7 shows the experimental setup. It has 64 tiles (nodes) organized in an 8×8 mesh topology. Each tile can beFig. 7. A tile based 8×8 mesh with 56 masters and 8 slaveseither a master or slave tile. A master tile hosts a master IP, a router, together with a characterizer and a regulator to conduct regulation. A slave tile has a router connected to an off-chip memory module (DRAM controller plus DRAM). Master and slave tiles are numbered sequentially. In total, there are 56 masters numbered from 1, 2, to 56, and 8 slaves numbered from 1s, 2s, to 8s.The performance metric we use is the packet delay, which is counted from when a packet is sent by its master until it is received by its slave. This delay includes two parts: regulation delay (delay due to regulation) and network delay (delay in the network). a high trafﬁc injection rate before but near to the network saturation point.In the experiment, the injection rate per master is 0.166 packets/cycle. The temporal distribution of each aggregate ﬂow follows the two-state MMP process described in the ﬁrst experiment. We use the regulation parameters (σ = 256packets, ρ = 0.24 packets/cycle) obtained from the ofﬂinecharacterization for the static regulation. For the dynamicregulation, the sampling and prediction window sizes still use 8192 and 2048 cycles, respectively.3) Performance with Synthetic Trafﬁc: We use hot spot trafﬁc as the synthetic trafﬁc pattern in which all 56 masters send packets to all 8 slaves with equal probability. Therefore the two slave regions are hot spots. Each master generates 8 ﬂows, each targeting a slave. The 8 ﬂows from the same master are treated as 1 aggregate. In total there are 448 (56 8) ﬂows, 56 aggregates.30002500200015001000500In the following experiment, we use two kinds of trafﬁc 02	4	6 8 10 12 14 16 18 20 22 24 26 28patterns: Synthetic trafﬁc and Application benchmarks. For all 1	3	5	7 9 11 13 15 17 19 21 23 25 27trafﬁc patterns, we compare three conﬁgurations:No regulation: The characterizer is disabled and the regulator provides a bypass to admit packets into the network as soon as possible.Static regulation: The characterizer is disabled and the regulator is conﬁgured once with constant regulation parameters (σ, ρ).Dynamic (online) regulation: The characterizer is en- abled. The (σ, ρ) values are dynamically updated window by window and used to conﬁgure the regulator for runtime regulation.2) Selection of Operation Point: To show the effect of regulation, we need to select a proper network operating point Source of aggregates (master ID)Fig. 8. Per-aggregate maximum packet delay comparison for hot spot trafﬁc16001400120010008006004002000with respect to the trafﬁc injection rate. Flow regulation wouldmake no sense if the network workload is too low, since the 2	4	61	3	5 8 107	9 11 12 1413 1615 17 18 20 2219 21 2423 25 26 2827network has less contention and any holding back of trafﬁc due to regulation is unnecessary and may result in negative impact on performance. On the other hand, the network workload should not be too high. Otherwise, the network gets saturated in throughput. The packet delay becomes exponentially up, and the network enters into an unstable state [21].In view of the typical network behavior, we should select a proper trafﬁc injection rate at which the network operates stably in the linear region in which, when the trafﬁc injection rate increases, the network throughput linearly increases. From the regulation perspective, we should select a higher trafﬁc injection rate. Combining these two requirements, we select Source of aggregates (master ID)Fig. 9. Per-aggregate average packet delay comparison for hot spot trafﬁcFigures 8 and 9 compare the per-aggregate maximum and average packet delay for the 28 aggregates from the corre- sponding 28 masters, 1, 2, to 28. We report the results only for half of the aggregates, because the other half (the other 28 aggregates from the other 28 masters, 29, 30,   to56) have similar results due to the symmetric trafﬁc pattern. From  Figure  8,  we  can  observe:  concerning  the per-aggregate maximum packet delay: (1) The dynamic regulation outperforms the static regulation for 34 (61%) of the 56 aggregates, with the maximum and average reduction of 452cycles (16%) and 146.8 cycles (5.8%), respectively; (2) The dynamic regulation outperforms no-regulation for 46 (82%) of the 56 aggregates. The maximum and average improvement is 435 cycles (17.4%) and 167.5 cycles (6.3%), respectively.From Figure 9, we can observe: concerning the per- aggregate average packet delay: (1) The dynamic regulation outperforms the static regulation for all 56 aggregates, with the maximum and average reduction of 186 cycles (13.8%) and108.6 cycles (14.5%), respectively. This average improvement is called overall average packet delay improvement, which   is calculated by the sum of per-aggregate delay reduction number of aggregates; (2) The dynamic regulation outperforms no-regulation for 45 (80%) of the 56 aggregates. The maxi- mum and average improvement is 332.8 cycles (54.6%) and147.8 cycles (17.7%); (3) Neither static nor dynamic regulation improves the performance for nodes 2, 3, 7, 8, 9 and 10. This is because they are close to slaves, and the regulation delay occupies a larger portion of the total delay.In the next, we show results with application benchmarks. All results (per-aggregate maximum and average packet de- lays) are consistent with the synthetic trafﬁc study. Due to space limitation, we only report results for average packet delay.4) Performance with Application Benchmarks: We exper- imented with the Stanford ParalleL Applications for SHared Memory benchmark suite 2 (SPLASH-2) to conﬁrm the per- formance beneﬁts brought by dynamic trafﬁc regulation.The ﬁrst step is to obtain the communication traces for the SPLASH-2 benchmarks. We used the full-system simulator Simics together with GEMS [23] (for the memory system). According to Figure 7, we conﬁgured a CMP system with 56 cores (masters) and 8 slaves. Each core has L1 I/D Caches: 64KB, 4 way set-associative; L2 Cache: 256KB, 4 way set- associative, 64 Byte lines. The total memory size is 4 GB with each memory being 500 MB (4G/8). The cache coherency protocol follows the directory-based MOESI protocol. The conﬁgured CMP system runs Solaris 9 OS. After being com- piled, the benchmark programs ran on top of the OS. The traces were recorded during the parallel execution phase, with each core running about 10 million instructions.Dynamic	Static	No Reg.1400120010008006004002000 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56Source of aggregates (master ID)Fig. 10. Per-aggregate average packet delay comparison for FFTFigure 10 compares the per-aggregate average packet delayfor FFT. We can consistently observe: (1) Compared with the static regulation, the online regulation improves 53 (94.6%) of the 56 aggregates in per-aggregate average delay with a maximum of 183 cycles (14.5%) and an average of 90 cycles (26%). (2) Compared with no-regulation, the improvement is achieved for 54 (96.4%) of the 56 aggregates at maximum 259cycles (19.5%) and on average 136 cycles (32%).Dynamic vs Static	Dynamic vs No Reg.286420Fig. 11. Overall average packet delay reduction with dynamic regulationTo give a complete picture, we summarize the overall average packet delay improvement for the 14 benchmarks of SPLASH-2 in Figure 11. The dynamic regulation consistently improves the results of static regulation and no-regulation for all the benchmarks. Due to different trafﬁc characteristics, the improvement is different case by case. In comparison with static regulation, the improvement in overall average packet delay ranges from 12 to 90 cycles, from 10% to 26% in percentage. In comparison with no regulation, it is from 53 to 190 cycles, from 22% to 41% in percentage. The case with less improvement is due to trafﬁc locality which achieves much less network delay and the regulation delay takes a larger portion of the total delay.DiscussionStatic regulation can improve performance since it can control network admission and thus congestion [2][3]. The reason why static regulation performs worse than dynamic regulation in both maximum and average packet latency lies in its rigidity, which may result in two negative effects at some time intervals: (1) Over regulation: it does not allow supplying sufﬁcient trafﬁc even if the network can accept more workload; (2) Under regulation: it still injects more trafﬁc even if the network cannot accept more workload. In contrast, the dynamic method can capture and adapt to trafﬁc dynamism and thus avoid both over-regulation and under-regulation of the static method. With the dynamic regulation, the network workload can be adaptively adjusted reﬂecting closely the real workload scenarios. Consequently network contention is better mitigated, making more effective use of the interconnect resources.To support the analysis above, Figure 12 shows the packet delay reduction of dynamic regulation against static regulation for FFT. For each regulation, we ﬁrst recorded delays for 440,000 packets sent over time to the network. Then we sampled one packet every 100 packets, resulting in 44006005004003002001000-100-200 0 400 800 1200 1600 2000 2400 2800 3200 3600 4000 4400Sampled packets over time "Reservation-based Network-on-Chip Timing Models for Large-scale Architectural Simulation.,"Architectural simulation is an essential tool when it comes to evaluating the design of future many-core chips. However, reproducing all the components of such complex systems precisely would require unreasonable amounts of computing power. Hence, a trade off between accuracy and compute time is needed. For this reason most state-of-the-art tools do not have accurate models for the networks-on-chip, and rely on timing models that permit fast-simulation. Generally, these models are very simplistic and disregard contention for the use of network resources. As the number of nodes in the network-on-chip grows, fluctuations with contention and other parameters can considerably affect the accuracy of such models. In this paper we present and evaluate a collection of timing models based on a reservation scheme which consider the contention for the use of network resources. These models provide results quickly while being more accurate than simple no-contention approaches.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Reservation-based Network-on-Chip timing models for large-scale architectural simulation Javier Navaridas ∗ ∗ § , Behram Khan ∗ , Salman Khan ∗ , Paolo Faraboschi † , Mikel Luj ´an ∗ † School of Computer Science, The University of Manchester, UK Intelligent Infrastructure Lab, Hewlett Packard Corresponding author: javier.navaridas@manchester.ac.uk § Abstract—Architectural simulation is an essential tool when it comes to evaluating the design of future many-core chips. However, reproducing all the components of such complex systems precisely would require unreasonable amounts of computing power. Hence, a trade off between accuracy and compute time is needed. For this reason most state-of-the-art tools do not have accurate models for the networks-on-chip, and rely on timing models that permit fast-simulation. Generally, these models are very simplistic and disregard contention for the use of network resources. As the number of nodes in the network-on-chip grows, ﬂuctuations with contention and other parameters can considerably affect the accuracy of such models. In this paper we present and evaluate a collection of timing models based on a reservation scheme which consider the contention for the use of network resources. These models provide results quickly while being more accurate than simple no-contention approaches. I . IN TRODUC T ION The relentless improvement of electronic miniaturization has provided the possibility of integrating several processing cores into a single chip. Most modern general purpose processors have several processing cores and, indeed, we can ﬁnd processors with over 10 cores offered by several companies, such as the 10-core Intel Xeon processors [34], the 12-core AMD Opteron processors [15] or the 16-core Sparc processors [36], [38]. Even more processing cores are provided by the 48-core Larrabee processor from Intel [35] or the 64-core TILE64 processor offered by Tilera [9]. In fact, the design and development of new processor architectures able to integrate over one thousand cores in a single chip is a current hot topic [13]. Indeed, some authors speculate that such technologies may become a reality within this decade [21]. Several international projects are pursuing this objective, although with different perspectives and objectives. The ATAC system, for instance, explores the viability of using a broadcast optical medium as the communication infrastructure within a 1000-core chip [21]. In contrast, Rigel [20] has been devised as a programmable accelerator comparable to current GPUs because of its single-process multiple-data (SPMD) execution model. The main reason that such architectures have become so attractive for the scientiﬁc community is improved power and thermal characteristics by means of per-core frequency and voltage regulation (or even shutting off idle cores). They are also more resilient to failures due to their greater redundancy. When designing new chip architectures it is essential to select appropriate evaluation methodologies. For example in the ﬁrst phases of the design it is preferable to explore as much of the design space as possible. Thus, fast evaluation methodologies such as functional simulation or analytical modelling are favoured in these phases even when they offer limited accuracy. As the ﬁnal design approaches, we need to assess chip functionality and performance through more detailed simulations–a practice that can be seen as virtual prototyping. The high complexity of these models demands large amounts of computing power to carry out simulation. Our research group is currently working in the ﬁrst design phases of a future 1000-core architecture: TERAFLUX [31], [37]. Given that simulation speed is a valuable characteristic for us now, we have selected the COTSon simulator [3] as it provides adequate accuracy while being lightweight enough. To speed-up execution COTSon processes a block of events at a time and offers other facilities such as statistical sampling. When modelling a large chip we need to provide timing models that do not slow-down the simulation while still being representative of the execution. How to strike such a balance is the key insight provided by this paper. Our focus is on how we can produce timing models for NoCs that we can use when considering the architectural design of large many-core chips without slowing down the simulation. In this paper we propose a collection of models which improve accuracy, comply with COTSon restrictions and are lightweight enough to perform fast simulation. These models rely on the idea of reserving resources for the period of time that they are in use, allowing contention to be modelled. We perform an exhaustive evaluation using workloads of different nature: synthetic trafﬁc from independent sources, traces from cache coherence, transactional memory and message passing applications and cache-coherency like synthetic trafﬁc to simulate a 1024-core NoC. The wide variety of evaluation scenarios provide insights into the strengths and weaknesses of the different models. I I . S IMU LAT ION O F FU TUR E MANY-COR E SY S T EM As device scaling continues to track Moore’s law and with the end of corresponding performance improvements in outof-order processors, multicore systems have become the norm. If current trends continue, a chip with over 1000 cores may be available as early as 2016. Given manycores’ inherent complexity, simulation is essential for exploring the design space of a future architecture. Moreover, while simulators 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.18 91 have focused on microarchitecture in the past, high level architecture (modelling the on chip network, memory hierarchy and coherence, etc.) is becoming increasingly more important. Simulating a processor involves making decisions on trade-offs between simulation speed and accuracy. At one end of the spectrum, purely functional simulation provides little insight into system performance, but allows for fast simulation. At the other end, a cycle-accurate full-system simulation gives reliable estimates of performance, but at the cost of very long running simulations. There exists a range of options between these extremes, some of which are explored in this paper. A number of simulation tools can be used to model processor and system architectures. Simplescalar [4] has been popular amongst the processor architecture community. It provides detailed processor and memory models, but is not a full-system simulator. Simics [23] performs full-system simulation and has extensions such as Gems [24] and Simﬂex [17] which add considerable sophistication to the timing models. Graphite [26] is a Pin-based simulator that concentrates on running speed by parallelising execution and by using probabilistic models instead of cycle-accurate models. COTSon [3] is a full-system simulator that leverages AMD’s SimNow [8] and scales up to simulating 1000 cores [22]. To improve simulation speed it does not allow callbacks. Therefore, we can not simulate the NoC and stall an event execution until the corresponding packets are delivered. Instead, we must calculate the latency for an event as soon as it is encountered. This allows for simplicity and speed in the simulation infrastructure, but means that care must be taken in implementing timing modules that accurately reﬂect device behaviour. Network on Chip simulation is also supported by a number of tools. Hornet [19] is a cycle accurate NoC simulator that can be driven by traces, by a built-in MIPS simulator or by native applications instrumented by Pin [5]. Garnet [2] also provides a cycle accurate model, and can interface with Gems to model a full system, and with ORION [39] to provide power estimates. Noxim [29] is a more limited tool that models only 2D mesh interconnects and is driven through synthetic patterns rather than application-based trafﬁc. SICOSYS [32] has very detailed models of several router architectures, which allows obtaining very accurate performance measurements, close to those obtained with a hardware-level simulator, but at a fraction of the required computing resources. It can interface with RSIM [28] and SimOS [33] to perform fullsystem simulation. Topaz [1] is a recently released extension of SICOSYS capable of interfacing with Gems to perform full-system simulation of CMPs. The Gem5 simulator [12], which merges the well-known M5 [11] and Gems simulators, has support for most state-of-the-art processor technologies while providing ﬂexible models for the memory subsystem. Regarding the NoC, it allows two simulation models: a fast no-contention model and a detailed packet-level simulation. It does not provide an intermediate solution as those discussed in this paper. To our knowledge, the only alternative implementation of fast timing modules akin to those proposed here is FIST [30]. Instead of modelling contention, FIST uses load-latency curves obtained from training simulations to estimate packet latency. It has, however, several limitations: the load-latency curves need to be obtained speciﬁcally for each trafﬁc pattern, so if an application has a mixture of trafﬁc patterns, or a patternless trafﬁc it cannot be modelled properly. Also it requires tracking the load handled by the NoC. As instantaneous load tracking would be prohibitive in terms of synchronisation, a sweet-spot would need to be found for how often we calculate/estimate network load. Again this means reaching a trade-off between accuracy and speed. At any rate using FIST would involve executing a stand-alone network simulator to train the FIST estimator, which has a deﬁnite impact on the overall time required to perform simulation. I I I . FA S T T IM ING MODU L E S This Section is devoted to discussing the different timing models considered in this paper. We will start with some simplistic models that have been used by the community, but that in our opinion will not be appropriate once the number of on-chip cores goes over a few tens. Then we will present the reservation-based models we are proposing. Finally, we will consider some illustrative statistical models. In all cases we will discuss the strengths and limitations of the models. A. Simple No Contention Models The simplest models and, as discussed before, the most prominent in the literature, do not consider network contention. For instance, there are some models which provide a constant latency for all network accesses regardless of the packet size, the number of hops and so on (e.g. vanilla Simics [23]). It is clear that disregarding any knowledge about the network makes this model very inaccurate; it can be used to test functionality, but should not be used to evaluate the performance of a large system. In our experiments this model has been denoted as ‘ﬁxed’ and considers all communications to require 16 cycles to complete. An improvement of this model would consider the distance and the packet size to model the latency, but again without considering any contention in the NoC. This model is still very poor as the NoC is a common resource that has to be shared by all the cores and, hence, it is not likely that network packets travel through the NoC without encountering any contention. In the discussions below we will denote this model this module will return a latency of 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 + #𝑓 𝑙𝑖𝑡𝑠 − 1. ‘no contention’. As we are considering cut-trough switching B. Reservation-based Models We propose several models that consider contention for the use of network resources. The basic element in these models is a ‘resource’, which in general represents a communication channel. To use one of these resources it is necessary to reserve it for a given period of time, forcing other accesses to the same resource to wait until it is freed. Given the nocallback limitation of COTSon, when a packet wants to reserve a resource which is in use, it then will reserve the ﬁrst available 92 resource start:13 end:  29 start:47 end:  55 start:78 end:  91 timestamp=5 : reserve (32, 11) resource start:13 end:  29 start:32 end:  43 timestamp=40 : reserve (50, 14) resource start:13 end:  29 start:32 end:  43 start:47 end:  55 start:78 end:  91 start:47 end:  69 start:78 end:  91 Fig. 1. Example of the reservation data structure. The resource starts with three reservations (top). A new reservation which requires adding a new element–in grey (middle). A new reservation which requires modifying an existing element–in grey–and permits removing an old element–in dark (bottom). Fig. 2. Example of two packets which do not compete in a NoC but do in the ‘direction con’ model. period. Note that as COTSon has to provide the time for executing a complete operation, more than one packet may be needed to transmit trough the NoC, and therefore, we may need to make reservations for the future. We have implemented a sorted linked list in which every element in the list represents a period in which the resource is reserved (Fig 1). The only required operation allowed is to reserve the resource for a given period of time; given the timestamp the reservation should begin and the duration (in cycles). This operation searchs for a free period of time that can accommodate the required reservation and will return the timestamp when it ends. Therefore when a resource is reserved, the delay can be calculated instantaneously after reserving, just by subtracting the end of the granted period and the current timestamp. As a secondary effect, every time this operation is invoked, it will remove all outdated reservations from the list (those which have already ﬁnished). This helps to keep the list in a manageable size independently of the simulation length. To further reduce the number of elements in the list, a new element will be added to the list only if extending an existing one (i.e. increasing the timestamp it ends at) is not possible. Based on this data structure we have implemented four different timing modules. The ﬁrst two models are aware of the network topology – for the purpose of this paper we will consider a mesh – the other two are topology agnostic. The ﬁrst model has been denoted ‘direction con’. It considers each row and each column of the NoC as a shared resource in each direction. A core trying to inject a packet will reserve the row and the column as dictated by XY routing. First it will reserve the row for the number of hops required in the 93 X dimension starting in the current moment, the reservation of the column will start after the end of the previous one and will last for the number of hops in the Y dimension. To the ﬁnal latency obtained by reserving the column we will add the packet length. The main limitation of this model is that it only allows one packet travelling in each row/column and direction of the NoC, while an actual NoC may allow several packets travelling in the same row, provided they are not competing for resources. For example the two packets in Fig. 2 do not compete in a real NoC, but require one waiting for the other in this model as both of them are using the same row. As we will see later, this will be the reason for this model reaching congestion before the modelled NoC. The second model, ‘path con’, considers each link of the topology as a resource. A core has to reserve all the links towards the destination for a period equal to the packet length. Resembling the way that packets travel through a network, the end of a reservation will affect the starting timestamp of all subsequent reservations. As we are modelling a virtual cut-through network, a link can be reserved once the header #𝑓 𝑙𝑖𝑡𝑠 − 1 cycles before the end of the reservation of the has arrived to a router, in other words we can start reserving previous link. For simplicity we have considered that the packet always follow a XY path, but any routing algorithm could be easily implemented. In principle this is the model whose behaviour most closely mimics the actual network as it emulates packets movement. However as router arbitration is not modelled its behaviour can differ from the actual network. Also it is the most complex of the models as it requires modelling lots of the components of a network (all the links). The topology-agnostic models consider the network as a collection of channels or pipes without any particular arrangement. When a core wants to send a packet it randomly selects the communication (#𝑓 𝑙𝑖𝑡𝑠 + 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 − 1). This simpliﬁes one of the pipes and reserves it for the time required to perform simulation while still considering contention for the use of resources. We have implemented two different versions of this model. In the ﬁrst one, ‘pipes’, all the cores share all the pipes, so the contention in any part of the network will affect all the cores equally. The second one, ‘pipes dist’ is a distributed implementation in which the system is divided in groups of cores which share a collection of pipes. This way contention in an area of the network does not affect other areas. This model simpliﬁes distributed simulation as there is no need for a shared data structure. C. Statistical Models We will close our study with two timing modules that do not consider contention directly but assume that travelling packets will suffer some extra delay due to other in-transit packets. The ﬁrst model, ‘load estimation’, estimates the current network load and approximates the latency either as noncongested in which latency is barely affected, or as congested in which latency is greatly increased. The approximation uses a exponential distribution to select the latency, based on the estimated load and the distance the packet has to travel. In this independent trafﬁc sources which allows us to easily vary the pressure exerted on the communication infrastructure. Next we will test them using traces from applications. This will allow us to assess their accuracy for a range of applications of interest. The use of traces of applications simpliﬁes comparing the timing modules because network utilization will remain the same for each workload. If full-system simulation would have been used, each timing module could have had a different set of messages, as the performance of the NoC may affect the overall trafﬁc; e.g. in the case of two memory accesses to the same memory address, one for reading and the other for writing, the order in which they arrive to the cache will affect the subsequent communications: if the read arrives ﬁrst it may require an extra invalidation packet once the write is executed. For the sake of completeness we use traces from applications of a diverse nature: directory-based shared memory, transactional memory and message passing. The former two are of interest in our research as they are the ‘hot’ application models to be run in the manycore systems we investigate. The latter, used in parallel and distributed systems, has a two-fold purpose. On one hand, it provides speciﬁc trafﬁc characteristics that are not covered by the previous two models. On the other hand, it allows us to assess whether the proposed models may be used in other design domains such as, for instance, by the cluster computing community. Finally we will use synthetically generated coherency-like trafﬁc to evaluate systems composed of 1024 cores. This will provide some insights on the scalability of the evaluated timing models. The employed methodology is as follows: the results obtained by the different modules are contrasted with those from the time-accurate network simulator. We will consider the following three ﬁgures of merit: 1) Simulated execution time of the application. This provides a ﬁrst approximation to the accuracy of the different models. 2) Similarity score metric. A more profound assessment of accuracy. We measured the simulated time to execute every 100 trace events and compute the average difference with the actual simulator. A lower value means that the evolution of the application is closer to simulation, i.e. more accurate. 3) Actual running time. This ﬁgure gives an insight into simulation speed. To provide fair speed estimates we developed the timing models as stand-alone tools. Fig. 3 illustrates the similarity score metric. It shows the evolution of ferret with the simulator, the ‘path con’ model and the ‘no contention’ model. The average of the differences between the simulator and a model in each of these points is its similarity score. For instance, we can see how the evolution of the ‘path con’ model is always closer to the evolution of the simulator than the ‘no contention’ model. Therefore it will have a lower similarity score, meaning that it is more accurate. A. Random Uniform Trafﬁc The use of independent trafﬁc sources is a typical evaluation tool which allows to extract some raw characteristics of Fig. 3. Extract of the temporal evolution of ferret. Fig. 4. Average latency. Random uniform trafﬁc. a) 64 cores. b) 1024 cores. paper, our estimator for the load is calculated as the number of injected packets divided by the elapsed time. Note that this model follows the same idea as FIST [30] but in a simpliﬁed way. Finally there exists the possibility to estimating latencies from a real simulation of the network. Although very complicated models can be extracted we think that estimations from the average latency in simulation does sufﬁce for the purpose of this paper. We will use an exponential distribution in which the 𝜆 parameter depends on the average delay measured during an actual simulation and the distance to be traversed, so that it provides the same average latency as simulation. We denoted this model ‘exponential’. IV. EX P ER IM EN TA L WORK In this section we show how the different timing models for the NoC behave under different operating conditions and compare them with INSEE, a lightweight time-accurate network simulator [27]. For the purpose of this paper we considered minimal NoCs: simple mesh topologies using XY routing and a single virtual channel. First we will use synthetic trafﬁc from 94 a network [16]. In this study we use it with a different perspective, though. Since independent trafﬁc sources allow to adjust network load at will, we can use them to show the behaviour of each model with a wide range of communication needs. We model the trafﬁc sources as independent entities injecting packets randomly along time following an exponential distribution. Spatially, packets are distributed uniformly through the network. The average latencies reported by each model are shown in Fig. 4. In the plot we can see that when the network load is low all the models offer latency ﬁgures very similar to those of the real simulation. As network load increases all the models follow a trend similar to the real simulation. The only exceptions are the two simple models: ‘ﬁxed’ and ‘no contention’ as they are unaware of the network load. The main difference, though, is when the different models start to behave as saturated, i.e. having very high latency ﬁgures. For example the ‘direction con’ model reaches saturation very early when compared with the real simulation. This is because modelling each row/column as a single shared resource is an extremely restrictive model (see again Fig. 2). Anyhow the network of a many-core system is not likely to suffer from persisting states of saturation when running shared memory or transactional memory applications as the threads requesting the use of the network will commonly stall until the reception of an ack packet indicating that the operation has succeeded. In general we can state that the contention-aware timing modules produce latency ﬁgures that resemble the shape of those of a NoC. Notice that the ‘exponential’ model practically overlaps with the simulator in this experiment. This is because this model uses the simulation average latency to generate a latency distribution with the same value. However, we will see later that with the real applications it can not capture application dynamics properly, throwing worse results than the reservation-based models. B. Directory-based Cache Coherency Undoubtedly, the directory-based cache-coherent shared memory application model is the most important one to bear in mind for studying many-core architectures. To generate the traces we used the COTSon framework [3], extended to implement directory based cache coherence protocol. Table I presents the main parameters of the simulated architecture. The network simulation is driven through a trace generated from COTSon. This trace logs all coherence and data messages that enter the network. We utilized the Parsec benchmarks [10] with ‘simsmall’ input data run with 32 cores (arranged in a 8 × 4 mesh), the maximum available in our conﬁguration. As discussed in [7] the spatial patterns of the application composing this benchmark suite do not present any noticeable ‘hot spot’. We will see later that this may happen with the transactional memory applications. Fig. 5 shows the results obtained by each of the timing modules. We can see how the four reservation-based models offer noticeably more accurate results, both in terms of execution time and similarity score than the other models. Also Feature L1 Cache L2 Cache Network Main Memory Directory Description 32-KB, 64 byte cache line, 4-way associative, write-through, 1 cycle latency 512-KB, 64-byte cache line, 8-way associative, write-back, 16 cycle latency 2D mesh topology, 16 cycles link latency 150 cycles latency Full-bit vector sharer list; directory cache 10 cycle latency TABLE I M EMORY SY S T EM CON FIGURAT ION Benchmark Vacation Kmeans Genome Intruder Parameters -n2 -q90 -u98 -r8192 -t4096 -c32 -m40 -n40 -t0.05 -i random2048-d16-c16.txt -p32 -g128 -s32 -n8192 -t32 -a10 -l4 -n2038 -s1 -p32 STAMP B ENCHMARK SU I T E PARAM E T ER S TABLE II looking at the time to execute the different modules we can see that we can achieve noticeable acceleration with respect to the simulation, in par with the simpler models. C. Transactional Memory Transactional memory is a novel memory model devised to simplify the development of shared memory applications and, especially, thread synchronisation [18]. This scheme provides support for transparent atomic execution of instructions in a shared memory system. Since this application model is gaining interest in the many-core community and it is an essential characteristic of our design, our study includes some examples of transactional memory applications. We generated 32-core traces of some of the STAMP benchmarks [14] following a procedure similar to the one used for Parsec. The parameters for these benchmarks are listed in Table II. The trafﬁc generated by this kind of applications has similar nature to cache-coherency’s. However there is a noticeable difference: if the region of the memory which has to be accessed transactionally is small it will be likely located in a single core’s cache. This core will then become a ‘hot stop’ as all the transactions will require trafﬁc to and from this node. For this reason, the spatial patterns of these applications are more likely to have an unbalanced use of the network. The results for the transactional memory experiments are plotted in Fig. 6. In the plot we can appreciate that in general, for this kind of applications, the differences in simulation time are minute. This is because these applications generate very low trafﬁc into the network and therefore there is not much contention. However the similarity score shows that the reservation-based models are more accurate than the other ones. Again, the computation times required by all the timing modules remain similar and noticeably faster than simulation. D. Message Passing To conclude with the trace-based experimentation, we compare the different models using the message passing version of the NAS Benchmarks [6]. The main difference of this application model when compared with the previous two 95 Fig. 5. PARSEC benchmarks. 32 cores. a) Normalized simulated execution time. b) Similarity score. c) Required computation time. simulation fixed no contention load estimation exponential direction con path con pipes pipes dist simulation fixed no contention load estimation exponential direction con path con pipes pipes dist Fig. 6. STAMP benchmarks. 32 cores. a) Normalized simulated execution time. b) Similarity score. c) Required computation time. is that communications are determined explicitly within the applications. This does not prevent tasks from injecting as much trafﬁc as they want into the network. For this reason, network can suffer long-lasting periods of saturation. The traces were obtained from a real cluster of PCs interconnected by means of a Myrinet interconnect. The class A of BT, CG, IS, LU, MG and SP benchmarks were run using a customised version of MPI logging to capture the traces [25]. We use tasks. Hence, a 8 × 8 mesh was used in this experiment. the largest trace we were able to generate with our set-up, 64 The results for the different benchmarks are plotted in Fig. 7. We can see how, with this kind of applications, differences between models are more signiﬁcant than with the previous two. This is because, as discussed, network saturation does appear and persist. In fact most of NAS benchmarks are composed of different phases in which computation and communication are alternated. This means that during computation phases, network is barely used but, in turn, during communication phases the network suffers from severe saturation. We can see how, in general, all the models are very inaccurate as the simulation times differ greatly from the simulator. This is because, modelling the behaviour of a saturated network is nearly impossible without modelling the whole network. Some of the reservation-based models do a relatively good work with some of the benchmarks but fail with the others. For example, as stated before, ‘direction con’ is severely affected by restriction of only one packet per direction. In the case of BT and SP benchmarks, they use a near neighbour communication pattern which essentially allows 7 packets in each row or column whereas ‘direction con’ has to deliver the packets sequentially (Fig. 2). If we look at the actual computation time, we can see that the high pressure exerted over the network by this kind of applications makes the reservation-based models slower than the other models. This is because the large amount of packets in the network is translated into lots of reservations in the data structures containing our resources. As the resources are implemented as sorted lists, each time a reservation is called, the whole structure has to be browsed which lowers performance. At any rate the reservation-based modules are still noticeably faster than the simulator in this context. To close this subsection we want to remind the reader that this kind of application is not actually of interest in our research but were included to show the inadequacy of such models for saturated networks. For the rest of the paper, we do not consider the conclusions of this subsection, since this workload forces the network to operate in a state that is not likely to occur in our set-up with our target applications. E. Directory-like trafﬁc in a 1024-core chip As stated before, our experimental set-up restricts the size of the applications we can use to evaluate our models. However, the ultimate aim of our research is to evaluate chips with up to 1024-cores. For this reason, following a methodology akin to the one presented in [21], we have generated synthetic directory-like trafﬁc for a 1024-core system (32 × 32 mesh). We present the results for these synthetic communications in Fig. 8. In general, all the reservation-based models but ‘direction con’ consistently provide better accuracy than the other ones. The reason for the low accuracy of ‘direction con’ is clear: allowing only one packet per each 32-core row (or column) is extremely restrictive, and results in overly pessimistic performance. Regarding the computation time, we can see how all models except ‘path con’ execute more than two orders of magnitude faster than the simulator, which means good scalability levels. In the case of ‘path con’, it requires using roughly 4,000 ‘resource’ structures and use, on 96 Fig. 7. NAS benchmarks. 64 cores. a) Normalized simulated execution time. b) Similarity score. c) Required computation time. simulation fixed no contention load estimation exponential direction con path con pipes pipes dist simulation fixed no contention load estimation exponential direction con path con pipes pipes dist Fig. 8. Synthetic directory-like trafﬁc. 1024 cores. a) Normalized simulated execution time. b) Similarity score. c) Required computation time. average around 20 of them for each packet. This render this timing module noticeably slower than the others (but still one order of magnitude faster than actual simulations). This may not be acceptable for the simulation of large chips, as it may become an important simulation bottleneck. F. Discussion Looking at the results, we can see how ‘path con’ is consistently the most accurate. This is because it is the one having highest resemblance with the actual behaviour of the network. It models the topology and the way physical links are employed during normal operation, but avoids modelling all the complex logic within the router (requesting, VC management, arbitration, QoS, congestion control). However, we found that this model cannot scale well to our target of 1024 cores as it may slowdown simulation considerably. In the case of ‘direction con’ we ﬁnd out that the restriction of having a single packet per row or column is exceedingly restrictive, specially with high loads or large systems. For this reason we should discard this model. However, taking into account that it produces reasonably good results with the 32node systems we may consider an intermediate solution which splits a large network into smaller ‘direction con’ networks which should provide better results for large systems while still being lightweight enough. Another model which is worth mentioning is the ‘pipes dist’. It has shown good accuracy and reasonable speed and scalability, but the more interesting property is that it can be used in distributed simulations without the need for a shared resource for simulating the network, which would simplify parallelising simulation as no synchronization would be needed among different parallel instances. These two characteristics make it a good candidate to scale up our simulator up to the 1000-core systems subject of our research. In general, it is reasonable to state that the proposed models provide more accurate results than simpler no-contention models simply by considering packet interaction in a very simpliﬁed way. Moreover, these models do not only provide more accuracy when simulating the same amount of trafﬁc but may help to detect when the network becomes a bottleneck. For instance, consider two alternate architectural designs with different communication needs, one of them being communication biased. If we evaluate these two designs using a nocontention model the communication-biased design would be in clear advantage as network contention and saturation do not affect its performance. The same evaluation with a contentionaware model would provide a more sensible evaluation. Finally, although energy estimation is outside of the scope of this paper, it is fair to say that the proposed models should be more accurate than no-contention models because both are aware of the distance travelled by the packets but, in addition, the contention-aware models also provides an estimation of the buffering time employed by the packets, which has a deﬁnite impact on NoC consumption. V. CONC LU S ION S In this paper we have proposed and evaluated a collection of timing models for the NoC in the context of fast simulation of many-core systems. The main novelty of these models is that they implement a reservation scheme that allows modelling network contention without the need for tracking instantaneous network load, or implementing a complex and slow simulation requiring callbacks. Our study has found that the proposed models consistently show more accuracy than the no-contention model that we had previously implemented in our set-up and that is widely used by the community. The next logic step is to implement the most promising of those modules in the COTSon simulator we use in our 97 daily research. We have found that a reservation model which resembles network topology seems to be the most accurate model but it may be excessively demanding to provide very fast simulation. A good alternative is a reservation-based model which provides a number of ‘pipes’ which have to be reserved to submit packets. This also has the advantage of supporting distributed simulation seamlessly, which will simplify simulating the 1000-core chip we are designing. It is worth noticing that in this paper we have considered a simple NoC design (mesh topology, cut-through switching and deterministic XY routing). However, modelling other NoC designs based on the proposed ‘reservation’ structure is straightforward. It only requires to organize the pool of resources following the topology of choice, to reserve them as directed by the routing and to chain them as dictated by the switching technology. A proof of concept for different ﬂavours of NoCs remains as future work. To close the paper, authors want to emphasise that these models are intended to accelerate the simulation of manycore processors but should not replace a proper evaluation of the communication infrastructure when it comes to design such systems. Our experiments showed that, under scenarios of saturation, none of the models were able to emulate network behaviour in an appropriate way. ACKNOW L EDG EM EN T S Dr. Navaridas holds a Newton International Fellowship. This research is supported by the TERAFLUX project funded by the EU FP7 with grant agreement number ICT-FP7-249013. Dr. Luj ´an holds a Royal Society University Research Fellow. "Analytical Performance Modeling of Hierarchical Interconnect Fabrics.,"The continuous scaling of nanoelectronics is increasing the complexity of chip multiprocessors (CMPs) and exacerbating the memory wall problem. As CMPs become more complex, the memory subsystem is organized into more hierarchical structures to better exploit locality. During the exploration and design of CMP architectures, it is essential to efficiently analyze their performance. However, performance is highly determined by the latency of the memory subsystem, which in turn has a cyclic dependency with the memory traffic generated by the cores. This paper proposes a scalable analytical method to estimate the performance of highly parallel CMPs (hundreds of cores) with hierarchical interconnect fabrics. The method can use customizable probabilistic models and solves the cyclic dependencies by using a fixed-point strategy. The technique is shown to be a very accurate and efficient strategy when compared to the results obtained by simulation.","Analytical Performance  Modeling of Hierarchical Interconnect FabricsNikita Nikitin, Javier de San Pedro, Josep Carmona and Jordi Cortadella Universitat Polite`cnica de CatalunyaBarcelona, SpainAbstract—The continuous scaling of nanoelectronics is in- creasing the complexity of chip multiprocessors (CMPs) and exacerbating the memory wall problem.  As  CMPs  become  more complex, the memory subsystem is organized into more hierarchical structures to better exploit locality. During the exploration and design of CMP architectures, it is essential to efﬁciently analyze their performance. However, performance is (a) (b)highly determined by the latency of the memory subsystem, which in turn has a cyclic dependency with the memory trafﬁc generated by the cores. This paper proposes a scalable analytical method to estimate the performance of highly parallel CMPs (hundreds of cores) with hierarchical interconnect fabrics. The method can use customizable probabilistic models and solves the cyclic dependencies by using a ﬁxed-point strategy. The technique is shown to be a very accurate and efﬁcient strategy when compared to the results obtained by simulation.INTRODUCTIONThe continuous shrinking of CMOS technology has enabled the integration of multiple cores and distributed memory in one chip. Parallelism has also been one of the paradigms to make computations more power efﬁcient. In the last few years, multicore systems have evolved from having few cores [1], [2] to single-chip processors with tens or hundreds of computing units [3]–[5].Tiled CMPs are an effective approach to architect general- purpose processors under the intense time-to-market pres-  sure [6], [7]. The replication of tiles provides a rapid way of ﬂoorplanning many computing units in one chip and commu- nicating them with scalable interconnect fabrics. Figure 1(a) shows an example of a CMP with 16 tiles, each one including a computing core (C), two levels of private on-chip caches (L1, L2), and a router (R) that communicates with the on-chip interconnection network (a mesh). Two memory controllers (MC) provide access to the off-chip memory.To exploit the locality of memory references, hierarchical interconnects have been proposed [7], [8]. Several cores can be grouped into one cluster to share the on-chip cache, accessible through a local interconnect (e.g., bus, crossbar, ring, etc). Hierarchy increases the intra-cluster hit-ratio and reduces the trafﬁc in the top-level interconnect. Figure 1(b) shows an implementation of a CMP with 4 clusters. Each cluster has  two cores with private caches, a shared cache (L3) with tag directory (DIR), a local interconnect (IC), a router and a network interface (NI).Given the vast space of design parameters, CMP designers are faced with the complex problem of selecting the best ar- Fig. 1: CMP layouts: (a) ﬂat, (b) hierarchical.chitecture subject to a set of constraints. Many design options must be explored, such as the variety of core implementations, interconnect types, topologies, cache hierarchies and memory management policies. Moreover, the amount of conﬁgurations increases drastically as the technology advances, allowing more cores and memory to ﬁt into the chip area.Evaluating the performance of a CMP architecture is es- sential to take the correct decisions during design. Unfortu- nately, simulation imposes a prohibitive computational cost when the space of design points grows signiﬁcantly. In this scenario, analytical modeling becomes an effective alternative for rapidly pruning the design space during early exploration and selecting a small set of promising conﬁgurations. Along this line, several analytical models for CMP exploration have been recently proposed (e.g., [9], [10]).The fundamental problem in evaluating the performance of a CMP is the calculation of the latency for memory requests, given the parameters of the interconnect fabrics and memory hierarchy. A key phenomenon that is underestimated by the existing models is the contention effect of the interconnection fabric. This paper will show that contention has a major signiﬁcance in the analysis of CMP performance. Ignoring contention leads to optimistic latency and throughput measure- ments, and may overestimate the architectures with saturated interconnects. As a result, exploration may select inefﬁcient architectures and, even worse, discard the promising ones.Accounting for contention is particularly important when exploring hierarchical CMPs. Interconnects at different levels of hierarchy may deliver different throughput characteristics (e.g., a bus at the cluster level and a mesh at the top level). It is then essential to verify that the required bandwidth between the cores and the memory is delivered at all levels. This veriﬁcation discards architectures in which one level of the interconnect is saturated, while another remains underutilized and consumes resources unnecessarily.The purpose of this work is to emphasize the importance and provide analytical methods for modeling contention in			CMP exploration frameworks. The contributions of the paper can be summarized as follows. First, we formulate the cyclic dependency between the latency and the rate of memory requests as a system of non-linear equations that models the contention in the CMP interconnect. Second, we propose three methods to resolve this model: using a general-purpose solver, a ﬁxed-point iteration and a bisection method. The last two 	(a)  8  8 mesh	(b) 4 4 mesh withbus clusters (c) 2 2 mesh with bus clustersmethods show signiﬁcant runtime savings, trading-off accu- racy and convergence. More importantly, these methods can be parametrized with any black-box analytical model for the latency. This makes our strategy ﬂexible to incorporate novel models for on-chip interconnects. Third, we experimentally show the application of the methods for CMP exploration and conﬁrm the importance of evaluating contention.Next section describes a simple example to emphasize the importance of contention in performance evaluation of hier- archical CMPs. Section III reports related work. The models for CMP throughput, memory latency, trafﬁc and their inter- dependency are discussed in Section IV. In Section V, we propose three methods to resolve this dependency. The exper- imental evaluation of the methods is described in Section VI.THE IMPORTANCE OF CONTENTION: AN EXAMPLEConsider a CMP with 48 cores and 16 shared  on-chip  cache modules. Figure 2 presents three (of the many) possible architectures with such parameters. One of the architectures has an 8 8 structure of regular tiles connected with a mesh (Figure 2(a)). The cores and caches are shown as light and dark squares, respectively. Solid lines represent the mesh links.To take advantage of the locality of memory accesses, several cores and caches can be grouped in a cluster and communicate via the local interconnect. For instance, clusters with bus interconnects were shown to notably improve the average communication latency [8]. This fact encourages the exploration of hierarchical interconnects. Figure 2(b) describes the CMP organization with 16 clusters, each one having three cores, one cache and a shared bus. The clusters communicate via the top-level 4 4 mesh. Another option is to increase the cluster size up to 16 components (12 cores and 4 caches) and decrease the dimensions of the top-level mesh (Figure 2(c)).One of the problems of architectural exploration is to select the conﬁguration with the best performance. We ﬁrst estimated the throughput of each conﬁguration using only the static (hop- count) latency of the network, i.e., assuming no contention.   In this experiment we assumed the ideal throughput of cores (under the assumption of zero-latency memory) to be  2.0  IPC (instructions per cycle), and the number of memory references per instruction to be 0.5. The values of static latency (in cycles) and the estimated throughput (in IPC) are displayed in the columns Lest and θest of Table I. Hierarchical architectures show a higher performance due to the exploited locality: conﬁguration (c) has the largest size of local cache (per cluster), hence the increased local hit ratio. Therefore, (c) shows the highest estimated throughput.However, this conclusion is incorrect when network con- tention is taken into account. Simulation reveals rather distinct Fig. 2: Possible architectures for a 48-core CMP. TABLE I: Performance of architectures in Figure 2.performance numbers, reported in the Lsim and θsim columns  of Table I. For conﬁgurations (a) and (b), the estimated throughput with no contention is close to the one reported    by simulation. However, the performance of conﬁguration (c) drops by about 40%. In fact, simulation concludes that (c) is the worst in terms of performance.The reason of this signiﬁcant discrepancy is the fabric contention. It occurs because of the competition between memory requests for the shared resources of the interconnect. This results into longer latencies, decreasing the overall perfor- mance of the system. In this example conﬁguration (b), which incorporates hierarchy at some extent, is the one with the highest throughput and represents the best architectural trade- off between cache locality and communication parallelism.RELATED WORKThe topic of CMP design space exploration has been widely studied in the last years. Many simulation-based frameworks (e.g. [11], [12]) appeared to extensively investigate the param- eters of core architectures, memory hierarchies and emphasize the importance of their joint optimization for improving the power, performance and thermal characteristics.Analytical models aim at replacing costly simulations and provide instead a quick insight on the architectures.  How- ever, the modeling techniques in the literature signiﬁcantly underestimate the performance degradation caused by the con- tention in the communication fabric. The model in [9] studies the trade-off between the number of cores and the on-chip memory size for throughput optimization. The latency model used includes a contention penalty  with  linear  dependency on the number of cores. Still, apart from being inaccurate,   this approximation does not allow to compare interconnects with various parameters and topologies. In [10] the authors introduce an energy-performance analytical model for CMP architectures, however they only consider bus interconnects with a simpliﬁed contention model. The work in [13] analyzes ﬁnite cache penalties in memory hierarchies, but the intercon- nects are also restricted to buses.In this paper we propose  a  generic  method  for  analyti- cal modeling of contention in hierarchical interconnect fab- rics. The advantages of hierarchical topologies for many-core CMPs have been demonstrated in [7], [8]. Our method can be parametrized by an arbitrary latency model for on-  chip interconnect. This paper discusses the application of the latency model in [14] due to its ﬂexibility. Other models can be considered, such as [15]. It introduces an accurate model for heterogeneous NoCs that can be useful for modeling variable number of virtual channels and link capacities on the different levels of the hierarchical interconnect. In [16], an approach similar to [14] is proposed, offering an accurate backpressure analysis at the cost of the model efﬁciency. To overcome the limitations of queueing approaches, alternative latency models (e.g. using non-stationary trafﬁc analysis [17]) can be used.CMP PERFORMANCE MODELINGThis section introduces the models for the evaluation of CMP performance. First, we explain the assumptions and input parameters of the model. Next, the equation to model static latency is presented. This equation is then extended to between miss ratio and cache size. This model was proven to be a good approximation [18]:Miss(S)= κS−α,	(1)where S is the cache size, and κ, α are the model parameters. Since L3 is a distributed cache, its access latency depends on the cluster where the requested data is stored. In this work we assume the probability to ﬁnd the data in a particular cluster to be inversely proportional to the distance between the requesting core and the cluster. However, our method can be parameterized with any other model for distributed cache.B. Static latencyIn this section we describe how to calculate the average static latency of memory accesses for a core c in the presence of memory hierarchy. Given the probability pf  for each ﬂow  f ∈ F (c) and its latency Lf , the static latency Lst is:consider the contention component of communication. Finally, the throughput model is discussed and the formula for memory stcf ∈F (c) pf Lf .	(2)request rate is derived. The section concludes by emphasizing the cyclic dependency between memory trafﬁc and latency.A. Assumptions and input parameters of the analytical modelIn this paper we focus on systems with two-level hierarchi- cal interconnect fabrics. However, the approach can be applied for an arbitrary number of hierarchical levels, including the particular case of ﬂat interconnects. Several components are grouped into a cluster: cores, components of the memory sub- system and the local interconnect. The top-level interconnect provides communication between the clusters and access to the off-chip memory (Figure 1(b)).The system has in total N cores, each one with two user-deﬁned parameters. IPC0 is the ideal core throughput, i.e., the amount of instructions executed by the core in one cycle, assuming zero-latency memory. MPI is the average number of memory references generated per instruction. These parameters characterize both the core and the workload.Without loss of generality, we assume that the memory sub- system has four hierarchy levels. Every core has a private L1 Since requests to L3 and MC are sent via the communication fabric, its delay must also be considered. This delay represents the latency of the on-chip network traversal and is deﬁned using the routing function      : f    π(f ), that for any ﬂow      f returns its routing path π(f ). In this work we consider the XY-routing function [19], however any deterministic or even adaptive routing can be used, specifying the probabilities for certain paths. The total latency to access an L3 instance is    the sum of the network traversal latency along the path π(f ) and the L3 latency. The total latency of the off-chip memory accesses is calculated likewise.The ﬂow probabilities pf are obtained using the dependency of miss ratio on the cache size, Miss(S), given by (1). As- suming the sizes SL1, SL2 of the two low-level caches, the probabilities to access them are:pL1 = 1 − Miss(SL1),pL2 = (1 − pL1)(1 − Miss(SL2)).As L3 is shared, the miss ratio is deﬁned by the effectiveL3 size, Seff, seen by each core [20]. To estimate Seff we useL3	L3cache and possibly, a private L2 cache of larger size but higher latency. The clusters incorporate modules of a distributed L3 cache, shared by all cores. The off-chip memory is accessible via a set of memory controllers. The latencies of the caches and the off-chip memory are provided as parameters.We use the term memory ﬂow to denote a feasible com- munication between a core and a component of the memory subsystem. For example, each core may access its own L1 or L2 caches, or any of the L3 modules or MCs. The set of all possible memory ﬂows for core c is denoted as F (c).Every ﬂow f F (c) is realizable with probability pf , thatdeﬁnes the probability for c to request data from a certain memory component. These probabilities can be user-deﬁned  or calculated with some analytical model. In our work we calculate these probabilities using a model of cache miss behavior based on a power law that represents the dependency the concept of the average number of cores, sharing each line, as proposed by [20]. The probability to access L3 is then:pL3 = (1 − pL1)(1 − pL2)(1 − Miss(Seff)).Finally, pL3 should be multiplied by the probability to ﬁnd the data in a particular L3 instance (cluster). A similar strategy is used to calculate the probabilities of ﬂows to every memory controller.Queueing model for the on-chip interconnectEquation (2) describes the static latency of memory ac- cesses. Another important part of the communication delay is the dynamic or contention latency [19]. Contention happens  in the interconnect fabrics when several packets compete for the same shared resource, such as a bus  or  an  NoC  link. This results into additional delays experienced by packets in(a)   (b) (RemRate), and the cost of an access (RemCost), the average number of cycles for executing an instruction, CPI, is:CPI = CPI0 + RemRate · RemCost,	(5)where CPI0 = 1/IPC0 is the ideal CPI, derived under the assumption of zero-latency memory. For a single-threaded in- order core, the cost of a remote access is the average latency,Fig. 3: Queueing representation for (a) mesh NoC and (b) bus- based cluster. given by (3), and the remote rate is given by the MPI value. As throughput is typically measured in IPC, the reciprocal of CPI, from (5) we obtain:the buffers, distributed over the on-chip interconnect. One of the approaches to estimate the contention delays is to model 1θc = CPI 1= 1 + MPI · L .	(6)the CMP as a system of queues and apply queuing theory to calculate the buffer delays.Figure 3(a) shows the queueing representation of the top- level mesh interconnect. The mesh routers (R) have up to ﬁve input-buffered ports to store incoming ﬂits while the router     is busy.  The primary ports of the routers are connected to     the clusters of devices (CL), which in case of a ﬂat CMP organization may consist of one device (e.g. a core with private caches in Figure 1(a)). Figure 3(b) presents the queueing model of a cluster, corresponding to one tile of the hierarchicaldevices, communicating via a shared bus: two cores with private caches, an instance of an L3 shared cache, a directory and a network interface. Every device has a buffer to store   the requests to the bus. To distribute the off-chip memory trafﬁc uniformly over the mesh and avoid high contention     of certain routers, we assume that memory controllers have multiple connections to the mesh, as shown in Figure 3(a).Total latencyThe average total latency for core c, Lc, is obtained by adding the queue delays along the communication paths, denoted as wq, to the static latency. Hence, given the paths π(f ) for every ﬂow f , we extend equation (2) accordingly: The throughput of a CMP, θcmp, is then calculated as the total performance of individual cores:θcmp =	θc.	(7)cThe rate of memory accesses, λc, is the probability for a core to issue a remote memory request per cycle. λc is proportional to the core throughput and the MPI:MPIλc = θc · MPI =   1   + MPI · L .	(8)This equation can be extended for the case of more complex out-of-order and multithreaded cores. The difference in model- ing an out-of-order core is that the remote memory access does not force the core to stall, hence the effective remote latency Lc decreases [21]. Techniques discussing throughput modeling for out-of-order implementations can be found in [22]. A multithreaded core can be modeled as a group of single- threaded cores. The latency for each thread remains Lc, but  the total memory access rate becomes λmt = Mλc, where M  is the number of threads.F.   The cyclic dependency between memory latency and trafﬁcIn order to calculate the buffer delays, equation (4)  requiresstcf ∈F (c) pfq∈π(f ) wq.	(3) the injection rates at every input (source) of the interconnect, while equation (8) gives the rates of request generation per core. Note that the injection rates in a ﬂat interconnect areTo  ﬁnd the values for wq, an analytical model for the on-chipinterconnects can be used. In this work we apply the model from  [14],  that  permits  calculating  the  delays  for  a varietyof topologies. Given the vector of injection rates into the interconnect,  λ¯	RN ,  the  model  proposes  to  express  queuedelays in the form of a system of equations with a matrix W :w¯q = W (λ¯).	(4)The exact form of the matrix W is given by the expressions(5) and (18) in [14]. What only remains is to compute the injection rates λ¯, which is covered in the next sections. directly deﬁned by the core rates: for a CMP with N cores,λ¯ =  λ1, .., λN  . In case of a hierarchical interconnect fabric,the core rates will correspond to the injection rates at the sources of the cluster-level interconnects, such as the bus in Figure 3(b). The injection rates to the top-level mesh can be calculated, given the fraction of inter-cluster trafﬁc. The latter is deﬁned by the probabilities of access to the L3 and the off- chip memory, discussed in section IV-B. Below we directly consider the dependency of memory latency on the core rates.From (3), (4) and (8) we observe the following system of dependencies:E. Throughput modelThe throughput of a CMP and the trafﬁc of its interconnect c = 1, .., N :	Lc = L(λ1, .., λN )λc = λ(Lc). (9)are closely related. To derive the exact dependencies, we start with the performance model for a single core, given in [21]. For a core with the average rate of accesses to remote memory This result is quite intuitive: the latency of the memory requests traversing the interconnect depends on the rate of sentrequests, due to the network contention. In turn, the request rate is determined by the latency, as no new memory requests are issued if the execution of cores stalls due to the absence  of data. System (9) emphasizes the cyclic dependency between the latency and rate of memory requests. In the following section we describe the methods to resolve this dependency.ANALYTICAL METHODS FOR LATENCY ESTIMATIONIn this section we propose three methods that can be used  to resolve the dependency (9). Apart from the straightforward way to solve the system of equations, the ﬁxed-point iteration and bisection methods are discussed. The beneﬁt of the ﬁxed- point method is that it delivers the exact solution in case of convergence and can be applied to arbitrary conﬁgurations. The bisection always converges for our problem, but ﬁnds an approximate solution. However, it was found to be a good ap- proximation for tiled homogeneous CMPs (see Section V-C).Solving the system of nonlinear equationsGiven an analytical  model  for  the  interconnect  latency  as a closed form, the straightforward way to ﬁnd Lc is  to solve the system of nonlinear equations. We apply the model from [14], which offers a convenient deﬁnition of queue delays via the injection rates in the closed form (4). Hence, the dependencies (3), (4) and (8) create a system of equationswith respect to the vectors of variables L¯, λ¯, and w¯q.The system will always contain nonlinear equations because of (8). The solution to a nonlinear system can be found using a solver, such as MATLAB [23]. Although hard to be proved analytically, our conjecture is that the system has a unique solution. We veriﬁed this by running MATLAB with different initial vectors and observing convergence to the same solution. While this method is straightforward, it has two important drawbacks. First, it only works for  the  analytical  models that provide closed-form equations for latency. Second, unless properly tuned, the methods for solving general systems of nonlinear equations may exhibit poor performance. Since our objective is to apply the method for exploration of a large design  space,  this  limitation  is  critical.  In  conclusion,  thismethod is useful to validate the techniques described below.Fixed-point iterationThe algorithm proposed in this section is a popular numeri- cal method for solving the systems of nonlinear equations [24]. While the theoretical speed of convergence of this method is relatively slow, it performs well in practice due to its low cost for a single iteration. Given a system of equations in the form:x¯ = F (x¯),	(10)where x¯ is the vector of unknowns and F is the system matrix, and an initial guess x¯0, the following iterative procedure can be used to ﬁnd a solution x¯∗ (ﬁxed point) of the system:x¯n+1 = F (x¯n),  n ≥ 0.In  our  setting,  x¯  is  composed  of  the  variables  {L¯,  λ¯,w¯q} and  matrix  F  is  deﬁned  by  the  right-hand  terms  of  the equations  (3),  (4)  and  (8).  For  the  initial  guess,  x¯0,  we  usestatic latencies (2) and compute other values using the same equations: L¯0 = L¯st, λ¯0 = λ(L¯0), w¯q,0 = W (λ¯0).The beneﬁt of the proposed method is that it does not require closed-form analytical expressions for latencies. Furthermore, any black-box model for the dependency of NoC latency on injection rate can be used. The method hence maintains the modular structure of hierarchical interconnects and permits plugging independent models for different  topologies,  such as bus (cluster-level) and mesh (top-level). This makes the approach a valid tool for future interconnect modeling.As a numerical  method,  ﬁxed-point  iteration  is  subject  to convergence issues. For a system in the form (10), the sufﬁcient condition for convergence is [24]:∂Fi	∂xiIn our case, this requires the latency to grow slowly with the injection rate, and vice versa. This condition holds for the communication fabrics that perform far from their saturation throughput (for instance, see chapter 23 in [19]). Although this condition is quite strong, it is not necessary for convergence.  In practice we observe that for the majority of conﬁgurations the iterative procedure converges.A second issue of the ﬁxed-point iteration is due to the analytical models based on queueing theory: the queueing models work under the assumption of system being in the steady-state [25]. This means, that for any router with service time T and the sum of arrival rates to its inputs λ, the following condition must hold: λT  < 1. In other words, there should    be no packet accumulation in the input queues of the router. Unfortunately, this requirement may be not satisﬁed by the initial solution. From (8) we know that the latency Lc and the memory access rate λc are inversely proportional. Since static latency is taken for the initial value of Lc, it may be highly underestimated for the conﬁgurations with high contention. As a result, the initial value of λc will be overestimated and may violate the steady-state condition.To handle  this  situation  as  well  as  the  conﬁgurations  for which the ﬁxed-point iteration diverges, we propose the method based on bisection search of λc, to ﬁnd a reasonable and fast approximation to the solution.Bisection search for trafﬁc rateThe advantage of the bisection method is that it always converges for our model (due to the intermediate value the- orem [24]). Since every core generates trafﬁc at certain rate, λc, multidimensional bisection [26] can be applied to ﬁnd the exact rates. However, a good approximation to the exact rates can be obtained by using the less complex unidimensional bisection: by simulation we observed that tiled CMPs with uniform clusters tend to have trafﬁc rates that change pro- portionally to their estimates, obtained by the static latency.Hence,  we  initialize  the  vector  of  injection  rates  λ¯ with  thevalues estimated by static latency, and on every bisection step adjust all rates in the same proportion.To   introduce  the  bisection  more  formally,  let us  rewrite	50equation  (8)  by  isolating  Lc,  and  using  the  star symbol to	40distinguish from the latency  in (3):	30L∗(λ )= 1  	1		20−	.	(11)c	c	λc MPI · IPC0	f0From  (3)  and  (11)  we  deﬁne  the  average  latencies  L(λ¯) andL∗(λ¯) as the functions of the vector λ¯: 00	0.05	0.f	0.f5	0.2K, average in¡ection rate (Glitx/cycle)N	N	Fig. 4: Behavior of the latency functions L(λ¯) and L∗(λ¯).L(λ¯)=  1  Σ L (λ¯),	L∗(λ¯)=  1  Σ L∗(λ ).Finally we introduce the latency difference function, F(λ¯):F (λ¯)= L(λ¯) − L∗(λ¯).Figure 4 shows the typical behavior of these functions, emphasizing the cyclic dependency (9). To depict a 2D viewof this behavior, we plot L(λ¯) and L∗(λ¯) as a function of the added. This enables simulation of multi-level interconnect fabrics with arbitrary depth. Finally, we implemented bus and multibus topologies.Each simulation was run long enough to obtain a 2% relative error (the same value used for the analytical method) with       a 95% conﬁdence degree. The 95% conﬁdence interval iscalculated using the popular batch means method [27].average  rate  Λ =   1 ΣN     λc.  The  curve  L∗(λ¯) shows  thatthe average rate of memory requests increases as the latency decreases.  In  turn,  L(λ¯)  shows  that  the  average  latency  ofrequests grows with the injection rate. The real values for latency and trafﬁc are deﬁned by the intersection point A ofthese  curves,  that  can  be  found  as  a  root  of  F (λ¯).  Hence,we use the bisection as a root-ﬁnding method, that does not require the exact knowledge of the function F (λ¯) and can beused with any black-box analytical model for latency. Bisection   searches   for   λ¯   that   satisﬁes   the   condition F (λ¯)  < s, where s is the solution tolerance. The initial rangefor  λ¯  is  limited  by  the  trafﬁc,  obtained  with  static  latency:λ¯min   =  ¯0,  λ¯max   =  λ¯(Lst).  Assuming  the  proportionality  in variation  of  the  individual  components  of  λ¯,  all  componentsare updated simultaneously. For any pair of consecutive iter- In this section we compare the efﬁciency of the three ana- lytical methods for resolving the cyclic dependency presented in Section V: solver (MATLAB), ﬁxed-point iteration (FP) and bisection (BS). We generated a set of CMPs having ﬂat mesh topologies with various dimensions and contention degrees. The reason to select ﬂat interconnects is to demonstrate that even for rather simple architectures the obtained system of equations is hard to be tackled in a straightforward way.The test cases and the results of modeling are summarized in Table II. The ﬁrst three columns display the test name, mesh dimension and the ratio of contention latency, with respect to the average total latency. The fourth column represents the number of variables and equations in the obtained system.ations  i  and  i + 1,  either  λ¯i+1 =  λ¯i  when  F (λ¯i)  <  0,  or The ﬁfth column shows the time required to ﬁnd a solutionλ¯i+1  = λ¯i  when  F (λ¯i) > 0.  The  iteration  is  continued  until the required tolerance for F (λ¯) or λ¯ is met [24].EXPERIMENTAL RESULTSIn this section we describe several experiments used to validate the proposed analytical method for efﬁciency and quality. Validation is performed with respect to simulation. Section VI-A describes our simulation environment. Further sections focus on the experiments for the analytical model.A. Simulation environmentTo verify the analytical model we have developed a ﬂit-level simulator for hierarchical CMP interconnects on top of Book- Sim 2.0 [19]. In contrast to the analytical model, the simulator performs ﬂit-level modeling of contention in the interconnect fabric and cycle-accurate calculation of throughput.To simulate a hierarchical CMP, three enhancements were made to BookSim. First, BookSim purely probabilistic trafﬁc injection patterns were replaced with state machine models  for cores, caches and memory controllers. The cores inject memory requests based on the average workload parameters and stall waiting for the replies. Memories accept requests from cores and send replies after a predeﬁned latency. As using the general nonlinear solver provided by MATLAB. The last two columns show the time consumed by the FP and BS methods. For each test case the three methods converged to the same solution, within the given tolerance region of 2%.Test cases T1 to T5 accentuate how the MATLAB time grows with the  mesh  size.  The  solution  to  T5-T7  could  not been found within an hour. Clearly, this straightforward method is not acceptable for efﬁcient exploration of CMPs.The purpose of test cases T6 and T7 is to compare the FP and BS methods. Conﬁguration T6 has higher contention than T5. As a result, FP method takes more time to converge than BS. Conﬁguration T7 has even more contention, resulting in   a violation of the steady-state assumption for the queueing model (see Section V-B). Hence, FP can not be used in this case, so BS is the only option.We observed that FP typically outperforms BS, when the contention component of latency is moderate, i.e. does not exceed about 30-40% of the total latency. Hence we choose   to run FP ﬁrst and use BS only when the former method fails.Architectural exploration for CMPsTo validate the quality of the analytical methods in perfor- mance estimation of hierarchical architectures, we carry out anTABLE  II: Performance comparison of analytical methods.	TABLE III: Parameters of the exploration.experiment for CMP design space exploration. Our framework reads a setup ﬁle with the parameters for cores, memories   and workloads, generates a multitude of architectures, and for every architecture obtains the throughput, using both modeling and simulation. With this experiment  we  demonstrate  that the modeling selects a very similar set of best-throughput architectures as simulation, but in much shorter time.Table III shows the exploration setup. The  estimates  of chip and component area were taken from the Niagara 2 processor [2]. We scaled the core area and memory density down to the 16nm to allow hundreds of cores ﬁt into the chip area. The IPC0, MPI of cores and the miss ratio dependency on cache size were estimated from the benchmarks in [28]. The number of cores and cache sizes were varied to explore the trade-off between the computing units and the on-chip memory. All architectures were generated with the mesh- of-buses topology. The exploration of the mesh dimensions compromises the number of clusters and processors per cluster. Given these parameters, our framework generates 1062 fea- sible conﬁgurations. The simulation of all the conﬁgurations took 324 minutes, while performance modeling was done in just 16.8 seconds, delivering more than 1000x speedup. The best architecture by simulation is the conﬁguration #937, with a throughput of 30.81 IPC. It consists of 6 6 mesh (36 clusters, 5 cores per cluster), a total of 180 cores with 64KbL1, 256Kb L2 private caches and 68Mb shared L3 cache.In Figure 5 we sorted the conﬁgurations by throughput along the horizontal axis, as estimated by simulation. One can see that the modeling estimate follows well the simulation curve. The analytical model for latency underestimates the contention, and therefore deviation increases with its degree. Conﬁgurations with similar throughput may have various contention degrees, hence the noisy behavior of the modeling curve. The error in throughput varies up to 19% with the35302520f5 f0 50ConGigurationx xorted in dexcending order oG throughputFig. 5: Throughput comparison for modeling and simulation. average value being 10%, which corresponds to the error reported by the latency model [14].However, what really matters for exploration are the relative, rather than the absolute values of throughput. Indeed, when exploring the huge design space we would like to effectively prune suboptimal architectures and leave a moderate subset of promising solutions. These conﬁgurations can be simulated further to select the best one. Hence, we are interested in comparing the order of conﬁgurations by the highest through- put, as delivered by modeling and simulation. And here our technique demonstrates very accurate results: Figure 6 shows the comparison for the best-throughput order. To make the picture illustrative, we limit to consider the 50 best conﬁg- urations, however the explained tendency is maintained for  the whole set. The horizontal axis speciﬁes the number N     of best conﬁgurations chosen by simulation. The vertical axis indicates the minimum number of best conﬁgurations chosen by modeling, that include all the N best ones by simulation. For example, the point with coordinates (1; 2) means that the best conﬁguration by simulation (#937) has the second place in modeling. Furthermore, the throughput of #937 is 30.81 IPC, while the throughput of the best conﬁguration by modeling (#940) is 30.80 IPC, and is within our modeling tolerance. The rightmost point on the plot (50; 64) means that the 64 best conﬁgurations by modeling include all 50 best by simulation. This is actually a very accurate result for the analytical model, when comparing more than one thousand of conﬁgurations.We also demonstrate that approximation by the static latency delivers poor order. It biases the exploration towards the conﬁgurations with large clusters, given the fact that the long contention latency in the buses is not considered. The point  (1; 33) in Figure 6 means that the best conﬁguration (#937)    is on the 33rd position, when not considering contention.For comparison, we also checked a conﬁguration similar   to #937, having the same number of cores and cache size,    but exploiting less locality by using a 12 15 mesh with one core and one cache module per cluster. The throughput of this conﬁguration is 24.23 IPC, that is 21% less. This witnesses the importance of the hierarchical fabrics exploration, effectively using the locality of memory accesses.Scalability of the modelingTo investigate the scalability of the analytical model, we generated several CMPs with mesh-of-buses topology and706050403020f0 00	f0	20	30	40	50	60Number oG bext conGigurationx by ximulation (N)Fig. 6: Order comparison for modeling and simulation.similar structure. Each cluster contains four components (three cores and one cache module) and a bus interconnect. The top-level mesh dimensions are varied from 2 2 to 16 16, producing CMPs with 16 to  1024  components.  For  each  test case we executed both ﬁxed-point and bisection and compared the average runtime value of these two methods with simulation. Figure 7 shows the results of comparison.Our probabilistic simulator demonstrates very good per- formance. Simulation of the 16-component CMP takes just2.5 seconds, and for the 1024-component CMP about 600 seconds. However, the modeling yet brings about three orders of magnitude improvement in efﬁciency. For the 16- and 1024- component CMPs modeling took only 0.002 seconds and 3.3 seconds respectively. In one second our method handles a CMP with nearly 700 components. This result justiﬁes high scalability of the proposed method and its ability to efﬁciently explore architectures with many hundreds of cores.CONCLUSIONSAnalytical models for CMP performance are crucial to make the architectural exploration possible. This paper shows that such models need to incorporate the contention factor in order to adequately estimate performance. We have presented three analytical methods to model the contention of hierarchical interconnects, by resolving the cyclic dependency between the memory latency and trafﬁc. The validity and efﬁciency of the model were proved through extensive simulation and with an example of architectural exploration.fOOOfOO fO f O.fO.OfO.OOfO	fOO       2OO       3OO       4OO       5OO       6OO       7OO       8OO       9OO      fOOONumber oG componentx in CMPFig. 7: Performance comparison of modeling and simulation. ACKNOWLEDGMENTThis research has been funded by a grant from Intel Corp., project CICYT TIN2007-66523, and FPI grant BES-2008- 004612."Generic Monitoring and Management Infrastructure for 3D NoC-Bus Hybrid Architectures.,"Three-dimensional integrated circuits (3D ICs) achieve enhanced system integration and improved performance at lower cost and reduced area footprint. In order to exploit the intrinsic capability of reducing the wire length in 3D ICs, 3D NoC-Bus Hybrid mesh architecture was proposed which provides performance, power consumption, and area benefits. Besides its various advantages, this architecture has a unique and hitherto previously unexplored way to implement an efficient system-wide monitoring network. In this paper, an integrated low-cost monitoring platform for 3D stacked mesh architectures is proposed which can be efficiently used for various system management purposes such as traffic monitoring, thermal management and fault tolerance. The proposed generic monitoring and management infrastructure called ARB-NET utilizes bus arbiters to exchange the monitoring information directly with each other without using the data network. As a test case, based on the proposed monitoring and management platform, a fully congestion-aware and inter-layer fault tolerant routing algorithm named AdaptiveXYZ is presented taking advantage of viable information generated using bus arbiter network. In addition, we propose a thermal monitoring and management strategy on top of our ARB-NET infrastructure. Compared to recently proposed stacked mesh 3D NoCs, our extensive simulations with synthetic and real benchmarks reveal that our architecture using the AdaptiveXYZ routing can help in achieving significant power and performance improvements while preserving the system reliability with negligible hardware overhead.","Generic Monitoring and Management Infrastructure for 3D NoC-Bus Hybrid ArchitecturesAmir-Mohammad Rahmani1,2, Kameswar Rao Vaddina1,2, Khalid Latif1,2, Pasi Liljeberg1, Juha Plosila1, and Hannu Tenhunen11Computer Systems Lab., Department of Information Technology, University of Turku, Turku, Finland2Turku Centre for Computer Science (TUCS), Turku, FinlandEmail: {amir.rahmani, vaddina.rao, khalid.latif, pasi.liljeberg, juha.plosila, hannu.tenhunen}@utu.ﬁAbstract—Three-dimensional integrated circuits (3D ICs) achieve en- hanced system integration and improved performance at lower cost and reduced area footprint. In order to exploit the intrinsic capability of re- ducing the wire length in 3D ICs, 3D NoC-Bus Hybrid mesh architecture was proposed which provides performance, power consumption, and area beneﬁts. Besides its various advantages, this architecture has a unique and hitherto previously unexplored way to implement an efﬁcient system-wide monitoring network. In this paper, an integrated low-cost monitoring platform for 3D stacked mesh architectures is proposed which can be efﬁciently used for various system management purposes such as trafﬁc monitoring, thermal management and fault tolerance. The proposed generic monitoring and management infrastructure called ARB-NET utilizes bus arbiters to exchange the monitoring information directly with each other without using the data network. As a test case, based on the proposed monitoring and management platform, a fully congestion-aware and inter-layer fault tolerant routing algorithm named AdaptiveXYZ is presented taking advantage of viable information generated using bus arbiter network. In addition, we propose a thermal monitoring and management strategy on top of our ARB-NET infrastructure. Compared to recently proposed stacked mesh 3D NoCs, our extensive simulations with synthetic and real benchmarks reveal that  our  architecture using  the AdaptiveXYZ routing can help in achieving signiﬁcant power and performance improvements while preserving the system reliability with negligible hardware overhead.Index Terms—3D NoC-Bus Hybrid Architecture; 3D ICs; MonitoringInfrastructure; Adaptive Routing Algorithm; Fault Tolerance; Thermal Management;INTRODUCTIONWith shrinking feature sizes in today’s many-core chip multi- processors (CMPs), interconnects have become one of the limiting factors for modern system designs. To get full beneﬁts of parallel processing, a multiprocessor system needs an efﬁcient on-chip com- munication architecture [1]. Network-on-Chip (NoC) is a general concept, proposed for complex on-chip communication because of scalability, better throughput and reduced power consumption [2]. However, increasing the number of cores over a 2D plane is not efﬁcient enough due to long interconnects.The advent of 3D integration technology has opened a new horizon for new on-chip interconnect design innovations. In 3D integration technologies, multiple layers of active devices are stacked  above each other and vertically interconnected using Through-Silicon Vias (TSVs) [3]. The comparison of 2D and 3D NoC architectures show that, 3D NoCs deliver better system performance with signiﬁcantly lower energy per packet, as compared to the 2D implementations   due to increased package density and shorter wires [4]. However,    3D stacking of active devices can potentially increase power density leading to thermal problems and creation of new hotspots [5].The straightforward extension of popular planar 2D NoC structure is 3D Symmetric NoC created by simply adding two additional physical ports to each router; one for Up and one for Down [6]. Despite simplicity, this architecture does not exploit the beneﬁcial feature of a negligible inter-wafer distance in 3D chips, because in this architecture, inter-layer and intra-layer hops are indistinguishable. Also, due to two extra ports, a considerably larger crossbar is required. The Stacked (Hybrid NoC-Bus) mesh architecture presented in [7] is a hybrid architecture between the packet switched network and the bus architecture to overcome the above mentioned 3D Symmetric NoC challenges. It takes advantage of the short inter- layer distances that are characteristic of 3D ICs. In stacked mesh architecture, a six-port router is required  instead  of  a  seven  port one for typical 3D NoC router, and vertical communication is just  one hop away to any destination layer.  Due  to  these advantages, this architecture is efﬁcient enough in terms of power consumption and latency. Since the bus is a shared medium it does not allow concurrent communication in the third dimension, however it has been shown that the dTDMA bus outperforms a NoC for the vertical communication as long as the number of 2D planes is less than 9   [7]. Thus, bus medium offers a sufﬁcient degree of scalability for the third dimension.To obtain high performance on large on-chip systems, a careful design of on-chip communication platform and  efﬁcient utilization  of available resources is required. The effective resource utilization needs an efﬁcient system monitoring platform, which can monitor the system at run-time and manage the resource utilization dynamically. Due to large number of cores in 3D NoC based system, heavy trafﬁc loads can create congestion. Similarly, over-utilization of one core can produce thermal hotspots. Moreover, retaining system performance under faults is an essential challenge for 3D NoCs. Such hotspots  and congestions can halt the system operation. All these issues direct designers to follow a sophisticated monitoring platform. Except the thermal management system, the existing monitoring schemes for 2D NoCs can be extended for typical 3D NoC architectures. However, 3D NoC-Bus Hybrid mesh architecture necessitates its own customized monitoring platform considering the hybridization structure.In this paper, we address the monitoring issues of 3D NoC-Bus Hybrid mesh architectures to enhance the overall system performance and reliability and reduce the power consumption. The proposed platform can be efﬁciently used for any kind of system management and monitoring such as trafﬁc  monitoring,  thermal  management  and fault tolerance. As a test case, the trafﬁc and inter-layer fault monitoring and management infrastructure is presented. The proposed platform gracefully monitors and extracts trafﬁc and bus failure information from bus arbiters and provides valuable information for connected routers. Based on this information, for the ﬁrst time, a fully adaptive and inter-layer fault tolerant 3D routing algorithm called AdaptiveXYZ is implemented to globally balance the load. In addition, we present a thermal monitoring and management strategy which would help in mitigating hotspots across all dimensions.The remainder of the paper is organized as follows. Section II elaborates the demands for enhancement of the existing architectures, while Section III motivates this work. Section IV presents the proposed monitoring platform and the adaptive routing algorithm based on the proposed architecture. Later, we present our thermal monitoring and management approach in Section V. The simulation methodology and results are shown in Section VI and ﬁnally, Section VII concludes this paper.			BACKGROUND AND RELATED WORKOne of the many challenges for NoC community is to monitor the system with minimum possible overheads in power, performance, latency and area. It is not a simple task to monitor the traditional NoC-based systems especially the 3D NoC architectures because of their massively parallel and distributed nature.The stacked mesh architecture [7] discussed in Section I suffers from an important drawback. The issue with this  architecture  is  that, each packet is traversed through two intermediate buffers: the source output buffer and destination input buffer. The output buffer hinders implementing congestion-aware inter-layer communications. Rahmani et al. [8] have resolved this problem by removing the source output buffers and rearranging the buffers placement at destination input. Based on the new hybridization scheme, the authors introduced a congestion-aware inter-layer routing algorithm called AdaptiveZ for 3D stacked mesh NoC architectures.Chao et al. [9] proposed a thermal-aware adaptive routing using     a proactive downward routing to ensure thermal  safety  for  throt- tled 3D NoCs. The routing technique is limited to symmetric 3D NoCs, uses non-minimal path and increases the zero load latency. The work presented in [8] addressed these issues and proposed a hybrid adaptive routing algorithm to mitigate the thermal issues. Ramanujam et al. [10] proposed a layer-multiplexed 3D architec-  ture for inter-layer communication with the consideration of load balancing. The architecture is not power efﬁcient because packets traverse through two stage  crossbars.  In  addition,  the architecture is not high-performance because usually two hops per packet (in different directions) are required for vertical communication. We deal with both the issues by using single hop vertical communication without following non-minimal paths. A reconﬁgurable inter-layer routing mechanism (RILM) for irregular 3D NoCs was presented in [11]. The proposed architecture exchanges a set of messages during initialization to generate an information tree containing location of 3D routers. This information tree is stored at each node. The mechanism is repeated, whenever a fault on any vertical link occurs. Storing     the tree information on each node and message exchanging impose considerable overheads in terms of area, power and latency.In [12] a Hieratical Agent Monitoring (HAM) for complex, parallel and distributed systems was proposed. The authors proposed to have a separate design layer for monitoring operations, separated from com- putation and communication because adaptive monitors are required for adjusting the system performance under unpredictable situations. The approach provides high level of abstraction for system designers. However, there is no available physical platform to implement HAM approach for 3D NoC-Bus Hybrid mesh architectures.For all the discussed architectures, authors address a speciﬁc issue such as fault tolerance, thermal management or trafﬁc monitoring. From implementation point of view, if a designer considers all the system monitoring and management techniques, the design com- plexity and overheads will grow dramatically. Apart from design complexity, most of the aforementioned techniques were designed  for symmetrical 3D NoCs without consideration of trafﬁc congestion. These issues necessitate an efﬁcient system monitoring platform with minimum overheads capable of considering all the system constraints in parallel without adding extra workloads to the main interconnection network.MOTIVATION AND CONTRIBUTIONAs discussed in [8], a stacked mesh 3D NoC architecture enabling congestion-aware interlayer communication has been presented. Their detailed stacked mesh architecture is shown in Fig. 1. In this Fig. 1. Side view of the AdaptiveZ-based stacked mesh architecture [8]architecture, after receiving a bus request (req) along with destination layer ID (dest layerID) from the router, the bus arbiter generates the grant and wait signals on the basis of different parameters such as stress value of VC buffers, transactions going on and the transactions in the queue waiting for the bus grant. If the router receives the grant signal, it will proceed with the transaction. Instead of grant, if wait signal is received, the router will wait for the grant signal and the   bus arbiter will put the request in the queue. The problem arises  when no grant or wait signal is received. In this case, the router will withdraw the request and send the packet to one of the neighboring routers without any information of the status of their connected buses. The packet may have to be rerouted from next hop due to the similar situation. Forwarding a packet without having enough information about the next vertical hop (bus) increases average packet latency and power consumption of the system. In this kind of situations, if the   bus arbiter informs the router to choose proper direction for the next hop, it will globally balance the load across all vertical buses. This issue necessitates bus arbiters to have some monitoring information about their neighboring buses.The same kind  of problems  may arise for  fault tolerant  routing  or during dynamic thermal management. In fault tolerant routing techniques for 3D NoC architectures, such as the work presented      in [11], a lot of information is transferred using the data channels   and thus hindering the system performance. For dynamic thermal management, there could be sensors on the silicon die, which needs  to exchange thermal data using the existing network and further exacerbates the system performance.Independent implementation of the mentioned monitoring services imposes a considerable area, power, latency and design complexity overheads on the system. Among the available options, an optimal approach is to exploit a dedicated generic monitoring platform, where different monitoring and management services can  be  integrated.  The available monitoring frameworks for 3D NoC architectures have signiﬁcant overheads because monitoring data needs to be exchanged across multiple layers. In addition, the available frameworks cannot efﬁciently utilize the beneﬁts of hybrid architectures such as 3D NoC- Bus Hybrid mesh architecture.To deal effectively  with  the  mentioned  challenges,  we  propose a system monitoring platform for 3D NoC-Bus Hybrid mesh ar- chitectures. To the best  of  our  knowledge,  this  is  the  ﬁrst  effort in implementing generic monitoring platform for 3D NoC-Bus Hy- brid mesh architectures. In hybrid NoC-Bus architectures, the bus arbiters that are located in the middle layer are a precious source      of information that can be used for monitoring purposes. This     	    	        k	 n	m	TABLE IIP Block	Switch ARB-NET Bus Node Monitoring Interconnect (SMS) InterconnectBus Xdir Ydir LOOKUP TABLES(in/out)Fig. 2. ARB-NET-based 3D Hybrid NoC-Bus mesh architecturemonitoring data is available within the same layer. Bus arbiters can exchange the monitoring information directly with each other and form a monitoring platform called ARB-NET without using the data network.THE PROPOSED ARCHITECTUREThe proposed architecture of our 3D NoC including the  ARB- NET monitoring platform  is  shown  in  Fig.  2.  It  is  basically  a  3D stacked mesh architecture which is a hybrid between packet- switched network and a bus. The arbiters resolve the contention between different IP blocks for bus access and in our view are a  better source to keep track of monitoring information. Hence, the arbiters can be prudently used by bringing them together to form a network and thereby creating an efﬁcient monitoring and controlling mechanism. The arbiters exchange very short messages (SMS) among themselves regarding various monitoring services that are on offer. The calculation of SMS values are updated every cycle. In this paper we deal with SMS messages which include trafﬁc and thermal stress values (its a measure of how much the bus is busy and thermally stressed respectively) of the bus pillars and possible failures on the bus in order to effectively monitor and control the network of a 3D NoC system.If n is the number of layers in a 3D stacked mesh NoC and is an odd number, then the arbiter network is placed in the n/2 layer. If n happens to be an even number then the arbiter network can be placed in either n/2 or (n/2)+ 1 layer. The main reason for placing the bus arbiters in the middle of the stacked 3D NoC is to reduce the number of vertically connected TSVs by keeping wire distances as uniform as possible [7][8]. Since the routers are mandated to send extra packet information (like Xdir and Ydir which deﬁne possible directions a packet can take during the minimal path routing), the number  of TSVs that are being used are reduced drastically. Our monitoring mechanism that  is being  proposed and  implemented  in this  paper  is a low-cost and an optimal approach, as the control network is required to spawn across in only one layer. Since, the proposed monitoring infrastructure differentiates between data network and monitoring network, the system performance is not degraded. In the monitoring techniques using the data network, monitoring packets   are sent throughout the whole network. Generally due to their higher priority, these packets increase the average packet latency of the network particularly at places where the congestion in the NoC occurs. On the other hand, the overdue delivery of these critical monitoring packets can lead to late reactions and further negative impacts on the network. Handling all the monitoring information in a low-cost network separated from data network and located only in one layer deals with the both mentioned issues.Packet FormatThe packet format that is being used in our 3D NoC comprises of   a header ﬂit and a number of payload ﬂits. Each ﬂit is n bits wide.  The ﬁrst bit in the ﬂits is reserved for the bop (begin-of-packet) ﬂag and the second bit for the eop (end-of-packet) ﬂag respectively. In  the header ﬂit, m bits are reserved for the routing information (RI) and k bits are reserved for the monitoring information (MI). The remaining bits are allocated for Data and to implement some higher level protocols (HLP). The RI ﬁeld consists of ﬁve  ﬁelds:  Xdest, Ydest, Zdest,  Xdir  and  Ydir .  The  Xdest,  Ydest  and  Zdest  ﬁelds indicate the destination address. The Xdir and Ydir bits deﬁne the possible directions a packet can take during the minimal path routing within the layer of the network, and is described in Table I. The MI ﬁeld consists of several bits which can be used to effectively monitor the network. One such possible MI ﬁeld information can be the packet length bits (Ptype). In this paper, in order to classify different types    of packets in the  network,  we  use  the  packet  length  ﬁeld  (Ptype) as a differentiating unit between packets instead of packet type. The packet length bits can be ignored in our network for packets which are of constant length.ARB-NET Node ArchitectureThe ARB-NET node that is used in our 3D NoC is a central building block for our arbiter network. In its general form, it consists of a measuring unit, control unit and a unit which does the actual arbitration. The measuring unit senses different monitoring parame- ters (like thermal, faults, trafﬁc, etc), whereas a control unit acts on the parameters that were measured by the measuring unit, by passing actionable control signals to the bus arbiter. In this section, in the case of no faulty buses, we use bus stress values to select least stressed  bus pillars (bus pillars are depicted in Fig. 2), while adaptively  routing data packets in a more efﬁcient way. In the subsequent section we will extend this architecture to support thermal monitoring and management. The measuring unit (consisting of trafﬁc and thermal stress value calculator module along with the error detector module) and control unit (consisting of pillar selector module) is represented in Fig. 3. The signals to/from the routers to the ARB-NET node are also depicted in the Fig. 3.Bus Stress Value Calculator and Error Detector: The measur- ing unit which senses different monitoring parameters (in this section bus trafﬁc and failure) calculates stress value of the bus and detects the possible failures on the bus and then propagates them to the neighboring arbiters as short messages over the network. Such a measuring unit is depicted as part of the ARB-NET node in Fig.3. The bus stress values are calculated according to Algorithm 1 and is based on the request signal  (req)  and  packet  type  (Ptype) from the router, and queue length of all output buffers connected to the  bus within the destination layer. The total stress value of the bus  pillar is calculated by summing up packet type (e.g. length) and the status of the destination buffer associated with the routers request signal. The weight factor (α) is used to regulate the impact of the destination queue length in our algorithm, the optimal value of which is determined based on a series of experiments conducted as described in the experimental results section of this paper.Signals to/from RouterXY0k-1 Signals to/from RouterXY(N-1)   Queue length of 	all	 connected buffers   Algorithm  2 Pillar Selection Algorithm	Input: Xdir, Ydir, Stress valueZ E,W,N,S , Fault{E,W,N,S}Output: Arb rsp X/Y , Arb rsp Balance, Faulty Bus X,Faulty Bus Y1: if (Xdir Ydir = “00”) then2:	Faulty Bus X = FaultE ; Faulty Bus Y = FaultN ;			3:	if (Stress valueZE > Stress valueZN ) then4:	Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; 	 5:	else if (Stress valueZE < Stress valueZN ) then 6:		Arb rsp X/Y  = ‘0’; Arb rsp Balance = ‘0’; 7:	else8:	Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’;9:	end if10: else if (XdirYdir = “01”) then11:	Faulty Bus X = FaultE ; Faulty Bus Y = FaultS ;12:	if (Stress valueZE > Stress valueZS ) then	SMS_out	kGlobal for all 4 directions SMSE_in SMSW_in SMSN_in SMSS_in 13:	Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; 14: else if (Stress valueZE  < Stress valueZS ) then  15:	Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; 	 16:	else17:	Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’;18:	end if19: else if (XdirYdir = “10”) then20:	Faulty Bus X = FaultW ; Faulty Bus Y = FaultN ;			Vertical Bus 21:	if (Stress valueZW > Stress valueZN ) then	Fig. 3. ARB-NET node architecture 22:		Arb rsp X/Y = ‘1’; Arb rsp Balance = ‘0’; 23:	else if (Stress valueZW < Stress valueZN ) then 24:		Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’; 	  		25:	elseAlgorithm 1 Bus Stress Value Calculation Algorithm 26:	Arb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; 	 Input:  req1−N ,  Ptype	, queue length of all connected output buffers 27:	end ifOutput: 1 NStress valueZ 28: else29:	Faulty Bus X = FaultW ; Faulty Bus Y = FaultS ;			N: Number of layersα: Constant weight factor, 0 ≤ α ≤ 1 30:	if (Stress valueZW31:	Arb rsp X/Y  > Stress valueZS  ) then= ‘0’;1: Stress valueZ = 0;2: for i = 1 → N do 32:	else if33: = ‘1’; Arb rsp Balance(Stress valueZW < Stress valueZS ) then 	 3:	if (reqi  = ‘1’) then 34: Arb rsp X/Y = ‘0’; Arb rsp Balance = ‘0’;4:	Stress valueZ =	Stress valueZ + (P typei + α × 35: elseArb rsp X/Y = ‘X’; Arb rsp Balance = ‘1’; 	 min queue lengthreq dest );5:	end if6: end for 36:	end if37: end if7: SMS[STVZ ] = Stress valueZ ;The proposed ARB-NET monitoring platform employs Cyclic Re- dundancy Check (CRC) codes to detect possible faults in a received ﬂit that may have been corrupted. Before traversing the bus towards the input of the downstream intermediate buffer, each ﬂit is error encoded at the upstream router using payload bits to store a CRC checksum for error detection. The CRC characteristic  polynomial that has been used in this work is g(x) = x8 + x2 + x + 1. The polynomial adds 8 check bits to ﬂit data. When the Error Detector unit signals that the bus link is not able to provide services because  of either transient or permanent failures, the Error signal is asserted to ‘1’. When the bus arbiter receives any such signals it asserts ‘0’ on the wait signal accordingly. When the router checks the wait signal,  it will not drive any more trafﬁc for inter-layer communication to the corresponding vertical bus.Pillar Selector: The controlling unit (pillar selector unit) which selects the next bus pillar the packet has to take en route to its destination is being governed by Algorithm 2 and illustrated in Fig. 3. In this section it takes into account the stress and fault values of the neighboring buses, as well as the Xdir and Ydir bits which give two possible directions the packet can take depending on the response of the Arb rsp X/Y signal.For example if both the Xdir and Ydir  bits  are  low,  then  the pillar selector unit ﬁrst assigns the bits indicating the direction of faults (east and north bound) to the outputs Faulty Bus X and Faulty Bus Y , respectively. It then checks the stress values of the buses which are north and east bound to make a decision on the direction the packet has to take. Depending on the Arb rsp X/Y signal, the arbiter guides the router to send the packet towards the direction of the bus which is least stressed. When the stress values of buses in both the possible directions is same, then Arb rsp Balance signal is asserted and the choice of the direction which the packet has to take is left to the router. Similarly, for other values of Xdir and Ydir, the bus pillar is selected in accordance with Algorithm 2.AdaptiveXYZ Routing AlgorithmThe routing of packets that takes place in the 3D NoC architecture that has been described above is based on our AdaptiveXYZ routing algorithm which has been elaborated in Algorithm 3. In the Adap- tiveXYZ routing, the direction the data packet has to traverse, depends on the location of the current node, location of the destination, queue length of the buffers in the x-direction and in the y-direction, as well as the response of the arbiter which will be guiding the packet through the network. If the destination node is the same as the current node, then the packet is delivered to the local node and the algorithm exits. If the packet has to be delivered with-in the same layer, then ﬁrst stress values which are nothing but the queue lengths of the buffers in the neighboring nodes are calculated, and the node with least stress value is chosen to transmit the packet while on its journey to its destination. If the destination node is connected to the same bus as the current node, then the current node sends a request signal to the bus arbiter along with the destination layer ID. The packets waits at the current node until it receives a grant signal from the bus arbiter. If the destination node is not on the same layer and is not connected  to the same bus as the current node, then the packet is routed in the network after the current node sends a request signal to the bus arbiter along with the destination layer ID, packet type and Xdir and Ydir information. The packet either waits upon the receipt of the grant signal, or withdraws the request and then proceeds to send the packetAlgorithm 3 AdaptiveXYZ Routing AlgorithmInput: (Xcurrent , Ycurrent , Zcurrent ), (Xdestination , Ydestination , Zdestination), queue lengthX , queue lengthY , Arb rsp X/Y , wait Output: Next Hop (E, W, N, S, L, U/D)1: Xdiff = Xcurrent Xdestination ; 2:  Ydiff  = Ycurrent       Ydestination ; 3: Zdiff = Zcurrent Zdestination ;4: if (Xdiff = Ydiff = Zdiff = 0) then5:	Deliver the packet to the local node and exit;6: end if7: if (Zdiff = 0) then8:	Stress  valueX   = queue  lengthX ;9:	Stress  valueY   = queue  lengthY ;10:	Use Stress valueX and Stress valueY to choose the destination port;11: else if (Xdiff = Ydiff = 0) then12:	Send a request to the bus arbiter along with the destination layer ID;13:	Wait until receiving the grant;14: else if (Xdiff = 0 or Ydiff = 0) then15: Send a request to the bus arbiter along with the destination layer ID, X dir, Algorithm 4 Inter-Layer Fault Tolerant AdaptiveXYZAdditional Input: Faulty Bus X, Faulty Bus Y1: if (Faulty Bus X = ‘0’ and Faulty Bus Y = ‘0’) then2:	if (Arb rsp Balance = ‘0’) then3:	Stress  valueX  =  Arb  rsp  X/Y , queue  lengthX  ; 4:	Stress valueY   =   Arb rsp X/Y , queue lengthY   ;   5: end if6: else if (Faulty Bus X = ‘1’ and Faulty Bus Y = ‘0’) then7:	Send the packet to Y Axis towards the destination and exit;8: else if (Faulty Bus X = ‘0’ and Faulty Bus Y = ‘1’) then9:	Send the packet to X Axis towards the destination and exit;10: end ifYdir , Ptype ;16:	if (wait = ‘1’) then17:	Wait until receiving the grant;18:	else19:	Withdraw  the request;20:	if (Xdiff = 0) then21:	Send the packet to Y Axis towards the destination;22:	else if (Ydiff  = 0) then23:	Send the packet to X Axis towards the destination;24:	else if (Xdiff  = 0 and Ydiff  = 0) then25:	if (Arb rsp Balance = ‘0’) then26:		Stress valueX = Arb  rsp  X/Y , queue  lengthX  ; 27:		Stress valueY  =  Arb rsp X/Y , queue lengthY  ;   28:	end if29:	Use Stress valueX and Stress valueY to choose the output port;	30:	end if31:	end if32: end iftowards the destination along the x or y axis. It may also withdraw the request and use the Arb rsp X/Y and Arb rsp Balance signals provided by the ARB-NET node, to choose one output port among them through which the packet is propagated in the network. This     is done by calculating the stress values of the neighboring nodes       as a function of the queue length  and  the  Arb  rsp  X/Y  value. This solution is deadlock–free as a result of using typical virtual channel architecture for the interlayer routing as shown in Fig. 1. In order to simplify the hardware infrastructure, we simply ignore the queue length of the  buffers in the  neighboring buses and  just use  the Arb rsp X/Y  value provided by the ARB-NET node. In this  way, the problem which has been described in the motivation section of this paper about the routing issue when both the wait signal and  the grant signal are not received by the router has been solved in an innovative way with the help of the pillar selector unit.Fault Tolerant Inter-Layer RoutingThe fault tolerance feature being incorporated in our 3D archi- tecture is governed by a fault tolerant inter-layer routing algorithm   as enumerated in Algorithm 4. It should be noted in here that the Algorithm 4 is not the complete fault tolerant inter-layer routing algorithm, but actually represents the last portion  of  Algorithm 3 (i.e. lines 25-28 should be replaced by Algorithm 4 in order to arrive at a complete fault tolerant inter layer routing algorithm). In this subsection we will be describing only the last portion of Algorithm   3 after replacing it with Algorithm 4.The packet either waits upon the receipt of the grant signal, or withdraws the request and then proceeds to send the packet towards the destination along the x or y axis. It may also withdraw the request and depending on the direction of faulty buses will route packet data accordingly. So, if neither of the buses are faulty, then depending    on the Arb rsp X/Y  and Arb  rsp  Balance  signals provided by  the ARB-NET node, the output port through which the packet has Fig. 4. Temperature proﬁle using run-time thermal managementto be sent is chosen. This is done by calculating the stress values       of the neighboring nodes as a function of the queue length and the Arb rsp X/Y value. If either one of the bus is  faulty,  then  the packet is sent in the other direction towards the destination; or if   both the buses are faulty, then the output port is chosen depending   on the stress values of their respective routers.Normally, the minimal path routing will be used even in case of faulty bus without any modiﬁcation in the proposed routing algorithm. However there are few cases,  when  routing  mechanism  needs  to be modiﬁed. Situations, wherein the current node is exactly below   or above the destination node, packets cannot be delivered to the destination if the bus link is faulty. So, the packets will be rerouted   to the neighboring node within the layer, where the packet is currently residing. Then they will be delivered to the destination. Because we only forward a packet through a non-minimal path in case of a bus failure and only for source–destination pairs connected to a same bus, the lack of livelock in the routing algorithm is preserved.THERMAL  MONITORING   AND   MANAGEMENT Hotspots by their very nature are localized and can lead totiming uncertainties in the system. This reduces the performance and increases the cost of packaging and cooling solutions. Hence there    is a need to move towards run-time thermal management solutions which can effectively guarantee thermal safety thereby increasing the efﬁciency and performance of the system with low overhead. This can be done by detecting hotspots and responding to them at different levels of abstraction. In this section, we propose a thermal monitoring and management strategy on top of our ARB-NET infrastructure which effectively responds to thermal  hotspots  in  a  3D  NoC  at  the system architecture level by routing data packets which bypass the regions with greater density of  hotspots.  Thermal  monitoring and management strategy is just one of the possible applications of the ARB-NET infrastructure. We assume that a distributed thermal sensor network is embedded in the 3D NoC which regularly provides thermal feedback to the routers and bus arbiter network thereby aiding and controlling temperature with the following thermal control mechanism.Fig. 5. State diagram of the proposed thermal control unitAlgorithm  5 Bus Thermal State Measurement  Algorithm	Additional Input: Thermal State1−NAdditional Output: Sum Thermal Control Mode (SUM TCM )1:  SUM  TCM = 0;2: for i = 1	N do3:	if (Thermal Statei = ‘1’) then4:	SUM TCM = SUM TCM + 1;5:	end if6: end for7: SMS[SUM TCM ] = SUM TCM ;In order to control  the  temperature  of  a  3D  NoC  we  propose  to use threshold approach which when crossed, the thermal control mechanism kicks in. As shown in Fig. 4, when the temperature rises above a certain threshold trip level (Thermal trip), the thermal control unit (TCU) changes the control policy to thermal control mode until the temperature drops to a certain safe zone (Thermal safe). Having two threshold levels reduces frequently occurring thermal  cycles.  The state diagram of  the  proposed  thermal  control  unit  is shown in Fig. 5. The system stays in the normal mode of operation until    the thermal trip signal is asserted after which it goes into thermal control mode of operation and continues to remain in that state until the temperature falls below the safe level (i.e until thermal safe signal is deasserted).If the router’s temperature increases beyond the predeﬁned ther- mal trip state then a signal called Thermal State is set which will be sent to the respective bus arbiter for further processing. We measure the thermal state of the bus in terms of its thermal stress value in accordance with the Algorithm 5. The total thermal stress value of  the bus is the sum of the Thermal State values of the respective routers connected to the bus. The thermal stress value should not be confused with the trafﬁc stress value that we deﬁned in Algorithm 1. Both Algorithm 1 and Algorithm 5 provide us with the overall state of the bus which can be used as the overarching guiding principle to route data within the 3D NoC.Based on the thermal stress values of the bus pillars which are obtained using Algorithm 5, we use the thermal-aware pillar selector algorithm as deﬁned in Algorithm 6 to select the next pillar through which the data packets are traversed. Algorithm 6 is an extension of the main pillar selection algorithm which is deﬁned in Algorithm 2.   It takes into account the total thermal stress, trafﬁc and fault stress values of the neighbouring buses, as well as the Xdir and Ydir bits which give two possible directions the packet can take. Algorithm 7 Thermal-Aware and IL Fault Tolerant AdaptiveXYZAdditional Input: Arb rsp X/Y TCM , Arb rsp Balance TCM1: if (Faulty Bus X = ‘0’ and Faulty Bus Y = ‘0’) then2:	if (Arb rsp Balance TCM = ‘0’) then3:	if (Arb rsp X/Y TCM = ‘0’) then4:	Send the packet to X Axis towards the destination and exit;5:	else6:	Send the packet to Y Axis towards the destination and exit;7:	end if8:	else9:	if (Arb rsp Balance = ‘0’) then10:		Stress valueX = Arb  rsp  X/Y , queue  lengthX  ; 11:		Stress valueY  =  Arb rsp X/Y , queue lengthY  ;   12:	end if13:	end if14: else if (Faulty Bus X = ‘1’ and Faulty Bus Y = ‘0’) then15:	Send the packet to Y Axis towards the destination and exit;16: else if (Faulty Bus X = ‘0’ and Faulty Bus Y = ‘1’) then17:	Send the packet to X Axis towards the destination and exit;18: end ifFor example if both the Xdir and Ydir bits are low, then the thermal-aware pillar selector unit ﬁrst checks the total thermal stress values of the east and north bound bus pillars and then makes a decision on the direction the packet has to traverse. Depending on   the Arb rsp X/Y TCM signal, the  arbiter  guides  the  router  to send the packet towards the pillar with least thermal stress value. When the thermal stress values of both the pillars are the same then Arb rsp Balance TCM signal is asserted. Similarly, for other  values of Xdir and Ydir bits, the bus pillar is selected in accordance with Algorithm 6.The thermal-aware and inter-layer fault tolerant AdaptiveXYZ al- gorithm is described in Algorithm 7. This algorithm gives the routing policy directive that governs the entire network. The priority in which the reliability of the network is addressed in this policy directive       is fault tolerance, thermal and trafﬁc management in that order. It should be noted that Algorithm 7 is not the complete thermal-aware, fault tolerant inter-layer routing algorithm, but actually represents the last portion of Algorithm 3 (i.e. lines 25-28 should be replaced by Algorithm 7 in order to arrive at a complete thermal-aware, fault tolerant inter-layer fault tolerant routing strategy). In the following paragraph we will be describing only the last portion of Algorithm    3 after replacing it with Algorithm 7.The packet either waits upon the receipt of the grant signal, or withdraws the request and then proceeds to send the packet towards the destination along the x or y axis. It may also withdraw  the  request and depending on the direction of the  faulty  buses  will  route data accordingly. So, if neither of the buses are faulty, then     we check the thermal stress values of the buses and if they are different (i.e Arb rsp Balance TCM = ‘0’) then the  packet  is routed through the pillar with least  thermal  stress  value (depend- ing on the signal Arb  rsp  X/Y  TCM ).  That  is,  if  the  signal  Arb rsp X/Y TCM is ‘0’ then the packet is routed through the x  axis or else it is routed through y axis. If the thermal stress values of 		both the bus pillars are the same (i.e Arb rsp Balance TCM =Algorithm  6 Thermal-Aware Pillar Selection Algorithm	Additional Input: SUM TCM E,W,N,SAdditional Output: Arb rsp X/Y TCM , Arb rsp Balance TCM1: if (XdirYdir = “00”) then2:	...3:	if (SUM TCME > SUM TCMN ) then4:	Arb rsp X/Y TCM = ‘1’; Arb rsp Balance TCM = ‘0’;5:	else if (SUM TCME < SUM TCMN ) then6:	Arb rsp X/Y TCM = ‘0’; Arb rsp Balance TCM = ‘0’;7:	else8:	Arb rsp X/Y TCM = ‘X’; Arb rsp Balance TCM = ‘1’;9:	end if10:	...11:   end if ‘1’) then we check for the trafﬁc stress values of the bus pillars and   if they are different (i.e Arb rsp Balance = ‘0’), then the packet is routed through the pillar with least amount of trafﬁc stress. If either of the bus is faulty then the packet is sent in the other direction towards the destination; or if both the buses are faulty then the output port     is chosen depending on the trafﬁc stress values of their respective routers.EXPERIMENTAL RESULTSIn order to demonstrate the efﬁciency of the proposed monitoring platform vis-a-vis its average network packet latency and powerreduction, a cycle-accurate NoC simulation environment was imple- mented in VHDL. Symmetric 3D-mesh NoC [6], Typical 3D NoC- Bus Hybrid mesh [7], AdaptiveZ-based 3D NoC-Bus Hybrid mesh[8] and the proposed architecture were analyzed for synthetic and realistic trafﬁc patterns. For Symmetric and Typical 3D NoC-Bus Hybrid mesh  architectures,  Z-DyXY  wormhole  routing  was  used. In Z-DyXY routing, a ﬂit will ﬁrst travel statically along the Z direction, then DyXY algorithm [13] will be used for the intra-layer wormhole routing. For the architecture proposed in [8], AdaptiveZ- DyXY routing was used. We used the proposed AdaptiveXYZ routing for the presented ARB-NET 3D NoC. We take advantage of the presence of two VCs per each input port that can be effectively utilized.In synthetic trafﬁc analysis of our experiments, we  use  a  36-  node (four layers of 3   3 mesh) network which models a single-   chip CMP using network-in-memory. We have assumed  that  the ﬁrst layer consists of processor arrays and other layers are shared cache memories. We followed SNUCA cache hierarchy and MOESI cache coherency protocol. The ﬂit size was set to  128  bits.  Our cache coherency protocol (MOESI) is a 3-phase protocol, which means a request(R)/response(S) control message is issued to the memory/cache. The cache responds, and then the cache controller sends the actual data. This avoids any race conditions. In conclusion, each of the R/S message is used just for control, short enough to ﬁt     a ﬂit (e.g. 1 R/S message = 1 ﬂit). We used 5-ﬂit packets to transfer on each 64-Byte cache line (read response or write request data). Because there are only two packet types (1-ﬂit and 5-ﬂit packets), we used the actual packet length instead of packet type (Ptype) in packet headers. For each simulation, the packet latencies were averaged over 500,000 packets. It was assumed that the buffer size of each FIFO is four ﬂits.In Algorithm 1 line 4, α is used as a criterion to regulate the impact of destination queue length on the bus stress value calculation. The greater the value of α, the greater is the stress value that is assigned  to the corresponding bus. Balancing the inﬂuence between the source part (req and Ptype) and the destination part (queue  lengthreqi  dest) is a critical issue for optimizing bus stress value. Fig. 6 showsthroughput comparison between different α values under the uniform random trafﬁc pattern. For this analysis, STV α 0 denotes param- eter α = 0. Small α leads to reduction in network throughput due to the reduction in the impact of FIFOs in the destination part. Large     α might excessively reduce the source part impact and negatively affect the throughput. The case where α = 0.4, sustains the most throughput for the proposed architecture. Since a comparison between the neighboring bus stress values is needed the division operation   can be eliminated by quantizing the variables used in Algorithm 1. Based on the number of layers, packet types, intermediate buffer depth, and the selected alpha value in our simulation environment, after quantization, 11 bits are enough to exchange the monitoring messages (SMS).To perform the simulations, we used uniform, hotspot 10% (2 nodes) and Negative Exponential Distribution (NED) [14] trafﬁc pat- terns. The average packet latency (APL) curves for uniform, hotspot 10% and NED trafﬁc patterns with varying average packet arrival rates (APAR) are shown in Fig. 7. It can be observed for all the trafﬁc patterns, that the network with proposed architecture saturates at higher injection rates. The reason being that, by utilizing the provided information via the ARB-NET, the AdaptiveXYZ routing increases  the bus utilization and makes the load, balanced. In the case of Z- DyXY routing and AdaptiveZ-DyXY routing, the 3D NoC-Bus Hybrid mesh architecture cannot deliver the desired performance because Fig. 6. Average throughput for different α valuesof bandwidth limitations. For the proposed architecture, bandwidth limitations are managed by the separated monitoring network without increasing the main 3D NoC communication workload.For realistic trafﬁc analysis, we used the encoding part of video conference application with sub-applications of H.264 encoder, MP3 encoder and OFDM transmitter presented in [8]. The video stream used for simulation purposes was of size 300 225 pixels and each pixel consists of 24 bits. Thus each video frame is composed ofMbits and can be broken into 8400 data packets each of size 7 ﬂits including the header ﬂit. The data width was set to 64 bits. We modeled the application graph, mapping strategy, frame rate, buffer size, number of nodes, layers and generated packets, supply voltage and clock frequency used in [8] for the real application simulation.   In order to implement adaptive routing, routers with 2 VCs per input port were used.We extended the high-level NoC power simulator presented in [15] to estimate the power consumption of the 3D NoC architectures. The simulation results for power and performance of the video conference encoding application are shown in Table II. The proposed ARB-NET- based architecture using the AdaptiveXYZ routing algorithm showed 19%, 9%, and 4% drop in power consumption over the Symmetric 3D-mesh NoC, typical 3D NoC-Bus Hybrid mesh, and AdaptiveZ 3D NoC-Bus Hybrid mesh architectures, respectively. Similarly, 29%, 17%, and 10% reduction in APL over the  Symmetric  3D-mesh  NoC, typical 3D NoC-Bus Hybrid mesh, and AdaptiveZ 3D NoC-  Bus Hybrid mesh architectures was also observed for our proposed architecture. Since the proposed architecture using AdaptiveXYZ routing algorithm can balance the load distribution among all vertical buses much better than Z-DyXY and AdaptiveZ-DyXY routing, the average packet latency for all routers is smaller when compared with other counterparts. The achieved latency and power reductions also lead to a considerable decrease in power-delay product (PDP) of the system.To demonstrate the efﬁciency of the ARB-NET monitoring plat- form for system reliability, the network running the video conference application presented in [8] with one faulty vertical bus was simu- lated. The rest of the system parameters were kept unchanged. For this application, we assumed that the bus connecting Stream Mux Mem, Padding for MV Computation, and YUV Generator components is faulty because there is a high inter-layer communication via this bus. Thus, the ARB-NET node of the faulty bus asserts ‘0’ on the wait signal and informs the neighboring ARB-NET nodes not to consider this faulty bus in their intra-layer routing. The power consumption and average packet latency of the AdaptiveZ 3D NoC-Bus Hybrid mesh and the proposed ARB-NET-based architectures with one faulty bus are shown in Table II (last two rows of the table). The ﬁgures given  in the table show that, the proposed architecture can tolerate a single faulty bus with lower latency and power overhead.We conclude the simulation results with a qualitative area compar- ison with the other work, most closely related to ours. The area ofUnder uniform trafﬁc proﬁle Under hotspot 10% trafﬁc proﬁle Under NED trafﬁc proﬁleFig. 7. Latency versus average packet arrival rate on a 3×3×4 mesh for different 3D NoC-Bus Hybrid mesh architecturesmanagement strategy based on the ARB-NET infrastructure which weTABLE IIPOWER CONSUMPTION AND AVERAGE PACKET LATENCYTABLE IIIHARDWARE IMPLEMENTATION DETAILSthe different routers was computed once synthesized on CMOS 65nm LPLVT STMicroelectronics standard cells using Synopsys Design Compiler. The typical, the AdaptiveZ-based, and the proposed bus arbiters were also synthesized to illustrate the area overhead of the controllers. The layout area of a typical 6-port NoC router, the AdaptiveZ router, the proposed AdaptiveXYZ router, and the bus controllers are listed in Table  III. Note that for all the routers, the  data width and buffer depth were set to 32 and 8 bits, respectively. The ﬁgures given in the table reveal that the area overheads of the proposed routing unit and the bus control module (ARB-NET node) are negligible.CONCLUSION AND FUTURE WORKIn this paper, a generic monitoring and management infrastructure for 3D NoC-Bus Hybrid mesh architectures was proposed to enhance the system management capability, thereby improving the overall NoC performance, power consumption, and reliability. The proposed monitoring platform called ARB-NET comprises of a network of monitoring nodes in the middle layer of the 3D NoC, which efﬁciently manages the entire network without any overhead on the main data network. The monitoring nodes are integrated in the bus arbiters and beneﬁt from the available monitoring information generated by each transaction as well as the information provided by the connected routers. Further, to assess the efﬁciency of the proposed ARB-NET in- frastructure, a fully adaptive and bus failure tolerant routing algorithm named AdaptiveXYZ was demonstrated. The results showed lower latencies and increased reliability for the proposed routing algorithm for high packet injection rates at places where the congestion in the NoC occurs. In addition, we have proposed thermal monitoring and believe would provide signiﬁcant peak temperature improvement. In this regard, we have performed some preliminary implementations of our thermal monitoring and management strategy which guide us to believe about the reductions in on-chip peak temperatures. The future work would include supplementing and verifying our preliminary work on thermal monitoring and management strategy by simulating our network using a set of realistic workloads.ACKNOWLEDGMENTThe authors wish to acknowledge the ﬁnancial support by the Academy of Finland and Nokia Foundation during the course of this project."Engineering a Bandwidth-Scalable Optical Layer for a 3D Multi-core Processor with Awareness of Layout Constraints.,"The performance of future chip multi-processors will only scale with the number of integrated cores if there is a corresponding increase in memory access efficiency. The focus of this paper on a 3D-stacked wavelength-routed optical layer for high bandwidth and low latency processor-memory communication goes in this direction and complements ongoing efforts on photonically integrated bandwidth-rich DRAM devices. This target environment dictates layout constraints that make the difference in discriminating between alternative design choices of the optical layer. This paper assesses network partitioning options and bandwidth scalability techniques with deep technology and layout awareness, the main contribution lying in the characterization and precise quantification of such interaction effects between the technology platform, the layout constraints and the network-level quality metrics of a passive optical NoC.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Engineering a Bandwidth-Scalable Optical Layer for a 3D Multi-core Processor with Awareness of Layout Constraints Luca Ramini Engineering Department University of Ferrara Ferrara, 44122 Italy Email: luca.ramini@unife.it Davide Bertozzi Engineering Department University of Ferrara Ferrara, 44122 Italy Email: davide.bertozzi@unife.it Luca P. Carloni Computer Science Department Columbia University New York, 10027 USA Email: luca@cs.columbia.edu Abstract—The performance of future chip multi-processors will only scale with the number of integrated cores if there is a corresponding increase in memory access efﬁciency. The focus of this paper on a 3D-stacked wavelength-routed optical layer for high bandwidth and low latency processor-memory communication goes in this direction and complements ongoing efforts on photonically integrated bandwidth-rich DRAM devices. This target environment dictates layout constraints that make the difference in discriminating between alternative design choices of the optical layer. This paper assesses network partitioning options and bandwidth scalability techniques with deep technology and layout awareness, the main contribution lying in the characterization and precise quantiﬁcation of such interaction effects between the technology platform, the layout constraints and the network-level quality metrics of a passive optical NoC. I . IN TRODUC T ION The performance of future multi-core processors will only scale with the number of integrated cores if there is a corresponding increase in memory bandwidth. For this purpose, silicon photonic technology is being investigated as a way to improve pin bandwidth density and power of DRAM memory devices [12]. In parallel, it is necessary to innovate the memory access architecture on the processor side. Processor-memory communication in chip multiprocessors (CMPs) and high-performance multi-core embedded systems is typically accommodated via an on-chip electronic network, which provides larger communication parallelism and higher aggregate bandwidth when compared to shared bus and multi-layer interconnect solutions. However, there is a clear gap between performance of such electronic NoCs and that of the high-bandwidth density, datarate transparent and distance- independent off-chip optical links. The only way to bridge this gap is to bring the photonic interconnect technology deeper into the chip. In recent years, a signiﬁcant amount of work has been done on exploring various optical NoC topologies (e.g., [19], [20], [30], [29], [23], [24]. Many of these rely on active optical devices [7], where a dual electronic NoC is required to establish and manage optical paths across broadband optical switches. Setting up and tearing down optical circuits, however, are time consuming tasks that call for expensive architectural support such as the management of dropped requests or the capability of adaptive routing. Above all, applications may have different types of memory access requirements, ranging from the highbandwidth processor-memory communication for streaming media decoding to the latency-constrained communications of those control applications for which response time is the key metric [15]. Active optical NoCs are not the best match for this latter kind of applications. Another set of works envisions fully optical interconnect solutions such as [14], however their efﬁciency depends on the availability of optical devices with stringent power consumption and signal loss features that are hardly achievable by current silicon photonic technology. The above shortcomings motivate the main idea of this paper of using passive photonic NoCs (PPNoCs) for processormemory communication. In PPNoCs, the route followed by a packet depends solely on the wavelength of its carrier signal (wavelength routing), and not on the information either contained or traveling along with it. In this way, the expensive O-E/E-O conversions required by some all-optical approaches like [11] are not needed. Also, if the routing pattern is set at design time and the wavelength employed for a sourcedestination pair is invariant for that pair, it does not depend on ongoing transmissions by other nodes and no time is lost in routing/arbitration. This is an appealing property for a processor-memory network in mixed criticality systems. In order to make silicon photonic technology affordable also for more cost-constrained multi-core systems (e.g., in the high-end embedded computing domain), we envision its implementation through a separate, vertically stacked optical layer with no integrated electronic devices. The 3D-stacking approach is the reference solution for cost-effective integration of heterogeneous technologies [26]. Although PPNoCs have been studied before in the literature, their use for processor-memory communication is challenged by a number of constraints that are typically overlooked: addressing these constraints is the main contribution of this paper. First, a processor-memory network poses speciﬁc layout constraints to the placement of the electro-optical network 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.29 185 interfaces and hence of the gateways to the optical layer. Moreover, memory controllers are typically distributed around the processor chip periphery to remove centralized communication bottlenecks from the on-chip network. Such layout constraints radically question the practical feasibility of appealing logic connectivity schemes proposed for passive NoCs and make the design of their associated physical topology mandatory. This latter incurs a hardly-predictable number of additional waveguide crossings depending on the speciﬁc layout constraints of the design at hand. The resulting unexpected insertionloss may offset the theoretical properties of the topology and change relative comparison results among them. We borrow logic schemes for passive NoC topologies from [20] and derive the corresponding physical topologies when accounting for the layout constraints of a processor-memory network. We then quantify insertion-loss deviations and gain practical insights. Second, network trafﬁc is inherently heterogeneous and comprises several trafﬁc classes. For a processor-memory network, it is possible to discriminate memory requests from responses and both of them from core-to-core communications. Awareness of such trafﬁc classes inspired us alternative PPNoCs partitioning options, and led us to assess their layout implications. We contrast a global connectivity solution with multiple network partitions associated with different trafﬁc classes. Again, we bring layout awareness to this comparative evaluation. Third, the engineered PPNoC solution should be conceived with scalability in mind from the ground up, not as an afterthought. While scalability with the number of network nodes has been addressed in the literature (e.g., see the hierarchical approach in [23]), bandwidth scalability under a ﬁxed number of network gateways and memory controllers is a largely unexplored topic. Yet, this is a key concern for those systems scaling to a larger number of cores. Bandwidth provided by photonically integrated DRAMs will most likely enable to preserve the number of memory controllers across a few device generations. Similarly, the need to amortize electro-optical conversion will cause the aggregation factor of processor cores around an optical gateway to increase, without immediately reverting to more gateways. In this context, the optical network must serve a larger number of memory access requests from its gateways. Two known techniques can provide such bandwidth scalability: spatial parallelism (SPM) and broadband passive switching (BPS). Many works choose one technique over the other based mostly on qualitative reasons, and some of them end up selecting inefﬁcient solutions. Instead we present the ﬁrst quantitative comparative analysis between SPM and BPS for passive optical NoCs. Combining the above contributions yields the complete engineering of a passive optical layer stacked on top of a multicore processor for high-bandwidth and low-latency processormemory communication. This additional paper contribution stems directly from the previous one: layout awareness drives the key choices for planning the entire optical layer of a 3D multicore processor, thus providing well-grounded design guidelines. In this direction, we engineer wavelength reuse strategies and low cost implementation techniques. In order to preserve technology-awareness in the analysis in spite of the focus at the network level we rely on a SystemC modeling and simulation environment where routing functionality is merged with FDTD-derived technology annotations in the models of the optical devices. I I . R E LAT ED WORK Shacham et al. [13] propose a circuit-switched on-chip photonic network with reconﬁgurable broadband optical switches. Circuit management via a dual electronic NoC may cause unpredictable communication latencies. Cianchetti et al. [11] propose another switch-based on-chip photonic network. It uses source-based routing and reconﬁgurable optical switches to route data. Switch setup is performed by converting the optical control signals that travel along the data to electrical form, and setting up the switch accordingly. Unlike the works above, Vantrease et al. [14] propose a fully optical solution. The large number of components, especially for high node counts, makes the viability of this architecture highly dependent on its ability to rein in the power consumption and signal losses of optical components, which will be heavily dependent on the maturity and efﬁciency of the optical technology employed. An approach with milder technology assumptions comes from [10] where a fully passive optical NoC is suggested. While the authors discuss bandwidth scalability options, they chose to use spatial parallelism without fully motivating the reason behind this decision. The key contribution of this paper is to provide a quantitative comparison between bandwidth scalability techniques for passive optical NoCs. In literature there are many works on passive optical topologies. In particular, Connor et al. in [19] is one of the ﬁrst examples. A basic element such as an add-drop ﬁlter was used to build the 4x4 network deﬁned as lambda-router on which wavelength routing was applied. Scandurra and Connor presented a scalable and fully connected Optical NoC topology for multiple cores and heterogeneous systems-on-chip [20]. A full 8x8 ONoC topology is proposed and analyzed in two communication scenarios such as total and grouped connectivity. Grouped connectivity makes it clear that if total connectivity is not required, signiﬁcant reductions in complexity can be achieved. Differently from these works, we provide total connectivity while capturing the layout implications on the physical topology. Le Beux et al. in [21] further reﬁne the topology. In particular, a single 8x8 interconnection network has been transformed into two optical sub-networks to reduce the number of crossings on the critical path. However, the number of optical sources stays the same. In contrast, we exploit network partitioning as a way to reuse optical sources. Also, the layout constraints assumed in [21] do not match those posed by processor-memory networks. Layout design rules for a 3D environment have been analyzed in [22], even considering a variable number of optical network interfaces, waveguides and electronic layers 186 Array  of  off-chip CW Lasers   CW CW CW CW  3  3 1  1 M3  M1  Photonic Layer  Network Interface of the Photonic Layer  Network Interface of the Electronic Layer  Coupler  H1  T S  V  T S  V  4  2  H3  H4  M4  T S  V  M2  Fiber Ribbon  Cluster C4  H2  T S  V  Cluster C3  Cluster C1  Cluster C2  PE   Electronic Layer        -	.	    !""#$""%&'(""# '"")(*%""(""+',*#   	  	  	  	  	  	  	  	                               	 	  			 	    	 	 	 	    Fig. 1. The proposed 3D-Architecture respectively. This analysis is limited to a ring topology and has the typical ﬂoorplanning of a regular homogeneous multicore system. Therefore, they ignore the intricacy of connecting central hubs to the chip periphery, which severely challenges the physical topology for non-ring structures. The ring topology was improved in [24] by upgrading it to the Spidergon topology for all-optical wavelength routing. Despite constant node degree of four, network diameter of the Spidergon topology limits the efﬁciency of large scale optical NoCs. The drawbacks of Spidergon have been overcome in [23], where a two dimensional hierarchical expansion of ring topology has been proposed. This is an effective way of providing scalability to large scale optical networks. This paper is complementary to our work, which is focused on bandwidth scalability for a ﬁxed number of topology nodes and leaves scalability issues with the number of nodes for future work. With respect to the illustrated literature, our work quantiﬁes to which extent layout effects and placement constraints in a realistic processor-memory communication setting cause the insertion-loss of the physical topology to deviate from its logic one. I I I . TARG E T 3D -ARCH I T EC TUR E In this section we describe the 3D-architecture of a multi-core processor including the passive optical layer for processor-memory communication whose detailed design is presented in section V. As illustrated in Fig.1, an electronic layer positioned at the bottom of this 3D-integrated system consists of an array fabric of homogeneous processor cores. Similar architectures are already available in the market, e.g. the Tilera family of multi-core processors [1], which currently features arrays of 16, 36, 64 and 100 cores. We consider an electronic layer that consists of 64 cores connected by an electronic NoC with a 2D mesh topology. We assume that cores are grouped into 4 clusters Ci of 16 cores each. Every cluster has its own access to the optical Fig. 2. Electronic Network Interface: Transmission Part layer which is vertically stacked on top of the electronic layer. We refer to the network interfaces between the electronic and the optical layer as the hubs Hi and the number of cores inside each cluster as the aggregationf actor . This factor is design- and technology- dependent, since the cost (power and latency) for domain crossing dictates the most convenient boundary between the electronic and the optical NoC for cost-effective long range communication. Identifying this boundary is outside the scope of this paper. The optical layer accommodates three kinds of communications: (a) between a pair of clusters; (b) from a cluster to a memory controller of an off-chip DRAM DIMM; (c) from a memory controller to a cluster. We assume that the optical power is provided by an array of off-chip continuous wave (CW) lasers and that multiwavelength signals are coupled into the chip and brought to the initiators for modulation. As demonstrated later on, the same array of CW lasers can be shared by all the initiators. For the given system conﬁguration, 4 lasers are sufﬁcient since every initiator modulates the same 4 wavelengths. This way, we are able to connect 8 initiators (4 hubs, 4 memory controllers) with 8 targets (the target interface of the same 4 hubs and 4 controllers). The microarchitecture of memory controllers depends on the speciﬁc implementation of the memory sub-system. As an example, in [12] optical command, read and write busses connect the controller to the off-chip photonically integrated DRAM (PIDRAM) DIMMs via a ﬁber ribbon. In any case, the memory controllers are typically placed all around the chip. Their speciﬁc location depends on the position of the PIDRAM DIMMs on the board and can be optimized to reduce contention (hot spots) in the on chip interconnect fabric. We assume that memory controllers are located pairwise at opposite positions of the chip like in the architecture in [1] thus reﬂecting a common industrial practice (see Fig.1). The speciﬁc topology of the passive optical NoC is discussed in section V. The electro-optical network interface (NI) resides partly in the electronic layer and partly in the optical one. Fig.2 shows 187                    	                MODULATORS  OUTPUT  Modulator   With   Resonance  Modulator   with   Resonance  Modulator   With   Resonance  Modulator   with   Resonance  λ1  λ2  λ3  λ4  T  S  V  T  S  V  T  S  V  T  S  V  INPUT  Carriers  From  Lasers  Bias  Signals  Fig. 3. Array of modulators in the optical layer the transmission part that is implemented in the electronic layer. Packets coming from the cluster’s electronic NOC are buffered at the network interface front-end. Based on their destination, they are stored in distinct buffers (in our target system, there are 7 buffers associated with the other clusters and with the memory controllers). A serializer reads packets from the buffers and feeds them to the drivers. We assume that drivers are directly connected to the through-silicon vias (TSVs) and through them to the modulators on the optical layer. The latest technological developments about 3D-integration enable TSVs with a pitch of 5um x 5um and therefore a large TSV integration density (up to 160K TSVs in a 10mmx10mm die). As reported in [2] [5], the TSVs can deliver high-speed transmission from 1 Gbit/s to 10 Gbit/s. This performance motivates our choice of using them to provide the biasing signal to the optical modulators in the optical plane (see Fig.3). The rationale behind this choice is to avoid integrating electronic devices in the optical layer. In turn this enables low-cost fabrication of this layer, a key requirement to make silicon photonics affordable in the future also for the embedded multi-core computing domain. In line with current technology, we assume modulation rates of 10 Gbit/sec for each wavelength. Therefore, the injection rate of every hub peaks at 40 Gbit/sec. In the optical layer we use passive, wavelength-routed networks: every destinationspeciﬁc buffer in the electronic NI is associated with a different wavelength in the modulation array. By complementing this with a network made up of add-drop ﬁlters, contention-free optical communication is achieved with no latency overhead for arbitration, routing or circuit setup. The reception part is specular. In the optical layer an array of add-drop ﬁlters for each hub feeds photodiodes that convert the optical signal back into the electrical domain. The photodiodes’ outputs are conveyed to the transimpedance ampliﬁers in the electronic layer by means of TSVs. Again, we opt for not placing the electronic devices in the optical layer. Digital comparators and de-serializers complete the domain conversion. Buffers are associated with packet source and from here on the electronic network interface functions come into play (i.e., association of memory responses with memory requests, packetization for the electronic NoC). IV. T ECHNO LOGY-AWARE N E TWORK - L EV E L S IMU LAT ION FRAM EWORK In order to design a passive optical layer for processormemory communication, it is necessary to explore interconnect solutions at the network level. For optical NoCs, this calls for a simulation environment with modeling capability of the routing functionality (which is the key feature of a wavelength-routed network) and of wavelength division multiplexing. However, in optical NoCs the characteristics of the technology platform cannot be completely abstracted away because they determine the viability of a logic scheme. Finding this out during layout design is too late and an effective design iteration is not possible any more. Unlike electronic NoC design, where such an iteration may simply consist of a new link inference technique (e.g., insertion of a pipeline stage on a slow link), in optical NoCs the number of waveguide crossings in the physical topology may be so large that the initial logic scheme may have to be entirely replaced by a more technology-friendly one. We propose a SystemC modeling and simulation environment as a good candidate to accommodate multiple requirements: efﬁcient network-level simulation, support for technology annotations, compatibility with industrial frameworks for system-level design of the electronic part. Our modeling strategy is based on four main steps: (a) Finite-Difference Time-Domain (FDTD) simulations of basic building blocks of optical NoCs, including straight and bending waveguides, 1x2 and 2x2 photonic switching elements (PSEs). (b) derivation of the equations of the analytical governing models describing the optical behaviour of PSEs. (c) back-annotation of the analytical model in SystemC and validation with FDTD. Accuracy validation of abstract models was proven in [18]: the mean-squared error is below 2% across the entire optical spectrum (1500-1600nm). (d) modeling of higher-order switching structures in SystemC by means of a compositional approach. FDTD simulations of such higher-order structures are not practical because require very long simulation times. A SystemC modeling framework is reported in [27], where separate channels are used to model wavelength and power information of optical signals. In [28], a new SystemC class is created to manage analog signals transmitted between modules. We leverage the existing port-interface-channel constructs of SystemC, thus making the top-level view of an optical NoC look like the same of an electronic NoC: the difference lies just in module implementations and in the data types exchanged through the pre-deﬁned SystemC channels. The optical link model is at the core of our SystemC modeling framework. The sc signal channel is instanced with a data type modeling the relevant features of an optical link: logic value, optical wavelength and signal amplitude. The optical wavelength is used by the router model for routing decisions, while the signal amplitude is used to preserve technology awareness inside the same router model. In fact, the analytical 188 Fig. 4. Logic scheme of the 8x8 PPNoC topology from [20]. model outputs insertion-losses and crosstalk noise that affect the value of such amplitude. Back-annotated losses from FDTD simulations include waveguide crossing loss, dropinto-a-ring-loss, waveguide propagation and bending loss. By comparing the logic value with the signal amplitude the optoelectronic receiver reads out of the optical link, we get biterror rate indications. The link model can support WDM by simply extending the user-deﬁned data type to represent multiple wavelengths (and associated logic values and signal amplitudes) which might be propagating at the same time across an optical link. V. PA S S IV E O P T ICA L NOC D E S IGN The most straightforward solution to interconnect the hubs with each other and with the memory controllers consists of an 8x8 passive optical NoC. We use the NoC topology proposed by O’Connor et al. [20] (see Fig.4), which we will hereafter refer to as the 8x8 PPNoC. In order to interconnect 8 initiators with 8 targets, the topology instantiates 8 stages of 4 and 3 add-drop ﬁlters. The only difference with respect to the original scheme is that we replace their 2x2 add-drop ﬁlters with a 2x2 photonic switching element [7]. This consists of 2 micro-ring resonators and two straight waveguides in orthogonal position. This choice eases layout design while leaving the routing functionality unchanged. Unfortunately, the appealing logic scheme of the topology does not match the actual ﬂoorplan in real-life systems. This is subject to speciﬁc constraints which may lead to a physical topology radically different from the logic one. In our target architecture, location of the hubs is dictated by the position of the electro-optical network interface in the electronic layer, which in turn depends on the aggregation factor. As a consequence, we have four hubs that are located along a square in the middle of the optical layer. In contrast, memory controllers are placed pairwise in opposite directions at the boundary of the chip. Since the layout of the optical NoC must satisfy these physical constraints, it necessarily results in a larger number of waveguide crossings than in the logic scheme. Such crossings increase the total insertion-loss and hence the power of the optical laser which is required by the optical signal to stay above the minimum detection threshold at the photodiodes. Such layout constraints, which are sometimes overlooked by theoretical studies of optical NoC topologies, are instead the main target of this paper. In Fig.5 we report the actual layout of the 8x8 PPNoC that we obtained given the layout constraints of our target system. It should be observed that each hub and TX M1 RX M1 TX M2 M2 RX TX M3 M3 RX TX H1 RX H1 TX H2 RX H2 TX H3 H3 RX TX H4 H4 RX TX M4 M4 RX Fig. 5. Layout of Global 8x8 PPNoC each memory controller is both initiator and target for the network, which should be therefore folded accordingly. We adopted design guidelines similar to those followed for the layout of fat-tree topologies in electronic NoCs [25]. As a result, we were able to minimize waveguide crossings, homogeneously exploit the ﬂoorplan and avoid intricate routes for the silicon waveguides. The deviation of the physical topology from the logic one can be quantiﬁed by the insertionloss critical path. It grows from 7 crossings in the logic scheme to 64 in the physical one. This effect is then reﬂected in the total insertion-loss of the topology. Fig.6(a) and Fig.6(b) report total losses for the logic and physical topology, respectively. Such worst case losses account for all the wavelengths, which are therefore assumed to be used by all initiators at the same time. We consider both elliptical tapers [31] at waveguide crossings and MMI (Multimode Interference) tapers [32], two common physical implementation options. The experiments have been carried out by means of the SystemC modeling framework of optical devices illustrated in section IV. In Fig.6(b), the breakdown of the losses into request path (i.e., memory access requests from the hubs to the memory controllers), response path and inter-cluster communication allows us to assess the quality of the layout. In fact, request and response paths show similar losses denoting the symmetric layout of the topology. Inter-cluster losses are lower due to the physical proximity of the hubs. Notice the signiﬁcant amount of total losses and the relevant savings obtained with MMI tapers, which are however not capable of cutting down losses below 48 dB in the ideal case. The picture becomes worse when we move to the real layout in Fig.6(b), where even with MMI tapers the total losses are more than 7 times higher, achieving 331 dB. This is clearly unacceptable for power-efﬁcient implementations. This experimental insight suggests that other solutions should be researched. One idea is to partition the global PPNoC into three sub-networks, each dedicated to a different trafﬁc class. By scaling down the same topology to 4 initiators and 4 targets, we derived the network for memory access requests. Similarly, we designed the network for memory responses, which features initiators and targets at ﬂipped positions in the layout. Finally, we opted for a different topology for inter-cluster communication, where 189 INSERTION LOSSES    M4 M3 160 140 120 100 80 60 40 20 0 ] B d [ 1200 1000 800 ] B d 6[ 600 141,6  Elliptical Taper MMI Taper 52,23  51,71  48,47  17,87  17,7  37,65  12,9  Request Part Response Part Inter-Cluster Total losses (a) Global 8x8 PPNoC with ideal layout INSERTION LOSSES   969,85  Elliptical Taper MMI Taper 400 369,64  371,61  331,41  126,31  126,97  228,6  78,13  200 0 Request Part Response Part Inter-Cluster Total Losses (b) Global 8x8 PPNoC with real layout Fig. 6. Insertion-loss analysis for the 8x8 PPNoC. a scheme better matching the hub-delimited square shape of the available ﬂoorplan is needed. For this purpose, we selected the 4x4 GWOR topology [8]. This is a scalable and non-blocking passive optical router design using micro-ring resonators with four bidirectional ports located in the North, West, South and East directions. Also, two horizontal and two vertical waveguides are used, which is a valuable property for a squared ﬂoorplan. The resulting layout is shown in Fig.7. It is evident that this layout is much less intricate with a lower number of additional crossings even for the 4x4 request and response PPNoCs. More precisely, the request PPNoC has the same number of crossings both in the logic and in the physical topology (3), while the response one features just a couple of additional crossings due to the opposite placement of initiators and targets on the optical layer. The improvement of the partitioned network with respect to the global network is quantiﬁed in Fig.8. Regardless of the speciﬁc taper conﬁguration, the total insertion-losses are reduced by 21x in the partitioned PPNoC. Another advantage of partitioned networks which should not be overlooked is the reduction of the number of CW lasers. In the 8x8 PPNoC, every initiator modulates the same 8 wavelengths, thus requiring 8 external laser sources. With the H4 H1 H3 H2 M1 M2 Fig. 7. Layout of the optical layer with network partitioning partitioned solution, wavelengths can be reused across the multiple networks, thus only 4 CW lasers are needed. The partitioned architecture is assumed in the following sections. V I . BANDW ID TH SCA LAB I L I TY IN PA S S IV E N E TWORK S In our target system, we have so far assumed an aggregation factor of 16 resulting in 4 clusters for the 64 core system. Successive generations of the same system will integrate more cores. The number of hubs, however, does not necessarily need to be increased accordingly to amortize the cost for electrooptical conversion and for the optical NoC infrastructure support (e.g., laser sources, distribution network of the optical power). Instead, bandwidth scalability techniques to increase the peak injection rate of the hubs could be implemented. The same considerations hold for the number of memory controllers, which could stay the same for a few generations. In fact, congruent multiples in memory bandwidth could come from the deeper integration of silicon photonics into the DRAM chip before reverting to multiple DRAM memory channels. Identifying the new aggregation factor that justiﬁes the increase in the number of network hubs and of memory controllers remains as future work. In contrast, we engineer the optical NoC for bandwidth scalability, so that the peak bandwidth can be increased to accommodate the memory trafﬁc that the hubs aggregate from a larger number of cores. With the assumptions made in section III, the peak injection rate of each hub is 40 Gbit/sec. A cost-effective way to augment this network’s bandwidth is to embed multiple virtual networks in the same set of waveguides, using spare wavelengths which may be available depending on the maturity of the technology. One possibility is to employ the technique proposed by Small et al. [9], which essentially places several wavelengths in the resonance band of a microring resonator. In that case, it is possible to route multiple bits of a message in parallel with little extra hardware: at each node, multiple modulators/detectors must tap separately on each of these wavelengths in order to inject/extract the bits of information; however the only 190     INSERTION LOSSES    M4 M3 1200 1000 800 ] B d [ 600 66 400 200 0 969,85  x21  Elliptical Taper MMI Taper 331,41  x20,5  46,19  16,12  Global 8x8 PPNoC Partitioned solution Fig. 8. Global 8x8 PPNoC vs Partitioned solution change for the routers and ﬁlters is necessary to broaden the resonance band of their microrings, in order to correctly route such wavelength bundles. This solution, which we call broadband passive switching, (BPS). obviously requires a larger number of off-chip sources to provide optical power to the new wavelengths. Another way to achieve higher network bandwidth is simply to replicate the network (spatial parallelism, SPM). Notice that all replicated networks must be laid out in a way that minimizes waveguide crossings, which are a signiﬁcant source of optical power losses. Multiple physical networks can be used to transfer more bits of the same message, or alternatively, more messages. With respect to BPS, SPM uses the same additional number of modulators and detectors but on different waveguides, while in BPS they increase the degree of wavelength division multiplexing on the same waveguides. Also, unlike BPS the number of laser sources stays the same, since the further replicated networks share the same wavelengths with the baseline ones. The output of the optical sources is just split among the individual network partitions. This advantage is compensated by the need to provide enough optical power for each wavelength to feed all network partitions. In principle, the total power provided by the optical source sub-system should be more or less the same, since in all cases the networks are (virtually or physically) replicated: the insertion-loss comes either from new wavelengths on the same waveguides (BPS) or from the same wavelengths on new waveguides (SPM). We experimentally measured the total insertion-loss of PBS and SPM to be around 12 dB, with only minor (less than 0.3dB) differences associated with the slightly different response of switching elements as a function of the signal wavelength. The two solutions differ for the actual layout implementation, which has a critical impact on power losses, particularly for SPM. To quantify such effect we designed the layout of the SPM solution and accounted for the additional waveguide crossings in the technology-annotated SystemC simulation. H4 H1 H3 H2 M3 M1 M2 Fig. 9. Layout of the Request Passive Network with Spatial Parallelism For the sake of clarity, Fig.9 reports only the layout for the replicated request PPNoC. Similar considerations apply for the response and the inter-cluster networks. For BPS, the layout remains the same as in Fig.7. Total insertion-losses across all wavelengths/network partitions in the bandwidth-scaled request PPNoC are reported in Fig.10. These losses are not comparable with those in Fig.8 since the new plot refers to an injection rate from each hub that has been doubled and now peaks at 80 Gbit/sec. The plot clearly shows that only BPS preserves the nominal insertion-loss of around 12 dB, while it grows up to 3x in SPM because of the waveguide crossings that the real layout constraints impose. Even with the MMI taper optimization, SPM is not able to go below 39 dB of total insertion-loss. This behaviour is reﬂected into the critical path of the two solutions: SPM has a critical path insertion-loss which is 4 times larger than BPS. INSERTION LOSSES   116,41  Elliptical Taper MMI Taper 39,75  x3,1  37,78  x3,1  12,93  Spatial Parallelism Broadband Passive Switching Fig. 10. Contrasting SPM vs BPS: total insertion-loss ] B d [ 140 120 100 80 60 40 20 0 191     [11] M. J. Cianchetti, J. C. Kerekes, and D. H. Albonesi “Phastlane: A rapid transit optical routing network”, ISCA’09: International Symposium on Computer Architecture, June 2009. [12] S. Beamer et al., “Re-Architecting DRAM Memory Systems with Monolithically Integrated Silicon Photonics”, ISCA’10: International Symposium on Computer Architecture, June 2010. [13] A. Shacham, K. Bergman, and L P. Carloni “Photonic Networks-onChip for Future Generations of Chip Multiprocessors”, IEEE Trans. on Computers, vol.57, n.9, pp. 1246-1260, September 2008. [14] D. Vantrease et al., “Corona: System implications of emerging nanophotonic technology”, ISCA’08: International Symposium on Computer Architecture, June 2008. [15] B. Akesson et al., “Memory Controllers for High-Performance and Real-Time MPSoCs Requirements, Architectures, and Future Trends”, CODES+ISSS’11: International Conference on Hardware/Software Codesign and System Synthesis, October 2011. [16] C. Batten et al., “Building manycore processor-to-DRAM networks with monolithic CMOS silicon photonics”, HOTI’08: IEEE Symposium on High Performance Interconnects, September 2008. [17] A. Hadke et al., “OCDIMM: Scaling the DRAM memory wall using WDM based optical interconnects”, HOTI’08: High-Performance Interconnects Symposium, August 2008. [18] A. Parini, L. Ramini, G. Bellanca and D. Bertozzi “Abstract Modelling of Switching Elements for Optical Networks-on-Chip with Technology Platform Awareness”, INA-OCMC’11:International Workshop on Interconnection Network Architecture on Chip, Multi-Chip, January 2011. [19] I. O’Connor et al., “Towards Reconﬁgurable Optical Networks on Chip”, ReCoSoC 2005, pp.121-128. [20] A. Scandurra and I.O’Connor, “Scalable CMOS-compatible photonic routing topologies for versatile networks on chip”, Network on Chip Architecture, 2008. [21] S. Le Beux et al., “Multi-Optical Network-on-Chip for Large Scale MPSoC”, IEEE embedded systems letters, vol.2, n.3, pp. 77-80, September 2010. [22] S. Le Beux, J.Trajkovic, I.O’Connor and G.Nicolescu, “Layout Guidelines for 3D Architectures including Optical Ring Network-onChip (ORNoC)”, VLSI-SoC’11: International Conference on VLSI and System-on-Chip, October 2011. [23] S. Koohi, M.Abdollahi, S.Hessabi, “All-Optical Wavelength-Routed NoC based on a Novel Hierarchical Topology”, NOCS’11: International Symposium on Networks-on-Chip, May 2011. [24] S. Koohi, S.Hessabi,S.J.B.Yoo, “An Optical Wavelength Switching Architecture for a High-Performance Low-Power Photonic Network on Chip”, Advanced Information Networking and Applications (WAINA), 2011 IEEE Workshops of International Conference, March 2011. [25] D. Ludovici et al., “Assessing Fat-Tree Topologies for Regular Networkon-Chip Design under Nanoscale Technology Constraints”, DATE’09: Conference on Design, Automation and Test in Europe, April 2009. [26] Y. Xie, “Processor Architecture Design Using 3D Integration Technology”, VLSID’10: International Conference on VLSI Design, January 2010. [27] E. Drouard et al., “Optical Network on chip Multi-Domain modelling using SystemC”, FDL 2004, pp.123-135. [28] M. Briere et al., “Heterogeneous Modelling of an Optical Network-onChip with SystemC”, IEEE Int. Workshop on Rapid System Prototyping, pp.10-16, 2005. [29] Y. Pan et al., “Fireﬂy:Illuminating future network-on-chip with nanophotonics”, ISCA’10: International Symposium on Computer Architecture, June 2009. [30] A. Joshi et al., “Silicon-photonic Clos networks for global on-chip communication” NOCS’09: International Symposium on Networks-onChip, May 2009. [31] N. Sherwood-Droz et al., “Optical 4x4 hitless silicon router for optical Networks-on-Chip (NoC)”, Opt. Expr., vol. 16, n. 20, pp. 15915-15922, 2008. [32] H. Chen and A.W Poon, “Low-Loss Multimode-Interference-Based Crossings for Silicon Wire Waveguides”, Photonics Technology Letters,IEEE., vol. 18, n. 21, pp. 2260-2262, 2006. V I I . CONC LU S ION In this paper we have quantiﬁed the deviation between quality metrics of logic topology as opposed to physical ones. This latter stems from the mapping of the logic connectivity scheme onto the real layout subject to placement constraints of communication actors and their network interfaces. As a case study, we consider processor-memory communication in a 3D system. We came up with three sets of results. First, insertion losses in the physical topology were one order of magnitude larger than expected because of the high number of waveguide crossings needed to lay it out. Second, we provided wellgrounded results supporting optical NoC partitioning instead of full connectivity, materializing around 20x lower insertion losses as well as an effective reuse of wavelengths and offchip laser sources. Third, we compared SPM with BPS as bandwidth scalability techniques. Real layout constraints heavily penalized SPM, since the additional waveguide crossings made insertion-losses 3x larger than in the nominal case. In contrast, BPS preserved such nominal values at the cost of more optical sources. The above practical insights enabled us to engineer a complete optical layer for high-bandwidth, lowlatency and low-cost processor-memory communication in a 3D multi-core system. ACKNOW L EDG EM EN T S This work has been partially supported by the PHOTONICA project (under the “FIRB-Futuro in Ricerca” program, funded by the Italian Government) and by the National Science Foundation (under Award number: 5-25083). "Modeling and Power Evaluation of On-Chip Router Components in Spintronics.,"On-chip routers are power hungry components. Besides exploiting current CMOS-based power-saving techniques, it is also desirable to investigate the power saving potential enabled by new technologies and devices. This paper investigates the potential of exploiting the emerging spin-electronics based MTJ (Magnetic Tunnel Junction) devices with application to on-chip router modules, in particular, buffers and crossbars. To this end, we build MTJ models, design circuits based on mixed MTJ-CMOS devices, and evaluate their switching power consumption, using their pure CMOS counterparts as the baseline. Our study shows that the new technology can significantly improve power efficiency for buffers but the gain for crossbars is less clear.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Modeling and Power Evaluation of On-Chip Router Components in Spintronics Pierre Schamberger and Zhonghai Lu Dept. of Electronic Systems, School for ICT KTH Royal Institute of Technology, Sweden {pierresc, zhonghai}@kth.se Xianyang Jiang Meikang Qiu Institute of Microelectronics and IT Wuhan University, China jiang@whu.edu.cn Department of ECE University of Kentucky, USA mqiu@engr.uky.edu Abstract—On-chip routers are power hungry components. Besides exploiting current CMOS-based power-saving techniques, it is also desirable to investigate the power saving potential enabled by new technologies and devices. This paper investigates the potential of exploiting the emerging spin-electronics based MTJ (Magnetic Tunnel Junction) devices with application to onchip router modules, in particular, buffers and crossbars. To this end, we build MTJ models, design circuits based on mixed MTJCMOS devices, and evaluate their switching power consumption, using their pure CMOS counterparts as the baseline. Our study shows that the new technology can signiﬁcantly improve power efﬁciency for buffers but the gain for crossbars is less clear. Keywords: Spintronics, MTJ, Network-on-Chip, Router I . IN TRODUC T ION Spintronics [1], [2], [3] is recently proposed for integrated circuits and systems to maintain states without consuming power. Spintronics implies a new type of logic, nonvolatile magnetic-based logic, which relies on magnetism instead of electricity to save the circuit or system states. As no power is required to store data with this technology, this may enable computing devices and equipment to be booted almost instantaneously. Thanks to the power saving potential provided by the nonvolatile magnetic-based logic, which allows bridging logic and memory in an energy efﬁcient way [3], Spintronics will likely ﬁnd more applications in the design of circuits and systems. Routers are a key component for scalable on-chip communication infrastructures [4], [24]. As the scales of multicore processors and multiprocessor SoCs rapidly grow, the role of routers, which enable communication via routing packets rather than using hard wires, is becoming essential. However, routers are power-hungry hardware modules due to buffering and crossbar switching activities. It is shown in [6] that the on-chip interconnect (routers and links) takes more than 20% of system power. It is therefore important to reduce power consumption in routers. This paper intends to apply the nonvolatile magnetic-based logic to a typical router datapath design in order to investigate its feasibility and power-saving potential. Our focus is on applying the new type of logic for CMOS integration, and in particular, we look into the potential of reducing dynamic switching power (rather than static power). If positive, this potential would bring much added value beyond its power reduction capability in static power. The paper is organized as follows: Section II discusses related work. Section III introduces the MTJ device, its working principle, and then basic router components. In Section IV, we present MTJ models. Based on these models, Sections V and VI describe the buffer and crossbar circuit designs, and report Spice simulation results showing the power gains in comparison with pure CMOS counterparts. Finally, we conclude in Section VII. I I . R E LAT ED WORK The spin effect of electronics was ﬁrst discovered in the 70s [7] when M. Julli `ere wrote an article about tunneling between ferromagnetic ﬁlms. This raised the interest of a number of researchers in the ﬁeld [8]. In 1988, Fert et al. [9] and Gr ¨unberg et al. [10] independently discovered the giant magnetoresistance effect, which is the phenomenon currently used because it allows better results than simple magnetoresistance. Crystalline MgO (001) is the crystal barrier commonly used in between the magnetic layers due to its perfect alignment making the structure more spin restrictive, compared to older amorphous Al-O barriers [11]. The barrier thickness inﬂuences the MTJ characteristics [12], making it more stable while increasing the thickness, but meanwhile more power-consuming. In March 2011, a 90 nm width MTJ device [13] was reported which greatly eases integration. The MTJ device has several advantages: First, the structure has a very good integration property [12] since it can be combined and put on an enable CMOS transistor (no extra surface is required). Moreover the smaller the technology is, the less amount of energy will be needed to switch an MTJ device since the magnetic ﬁeld is lower. Also, the resistance to power failures makes it very interesting: Once the value is set into the MTJ, it does not need any energy to keep it. A power failure or simply a circuit shutdown does not disturb the value [14]. This feature is particularly useful for memory units. Finally, lower dynamic and no static current are the main advantages compared to the rising CMOS leakage power when decreasing technology. A new domain, called Logic-inMemory (LiM), has just been created with that technology which combines MTJ units with logic blocks. The main goal of LiM is to look for MTJ-based electronic circuits to replace CMOS ones and to achieve higher performance with smaller 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.13 51 when the voltage is rising (particularly the RAP state). This shall be taken into account when modelling the MTJ. area and lower power. A power-efﬁcient MTJ adder [14] and LUTs for FPGA [15] have been successfully built. Compared to a CMOS adder, the MTJ adder consumes 16 μW dynamic power and 0.084 nW static power, against respectively 51 μW and 55 nW , when both working with a 310 ps period. To our knowledge, our study is the ﬁrst attempt to apply MTJ technology to on-chip routers, investigating its potential in saving switching power. I I I . MT J D EV IC E AND ON -CH I P ROU T ER A. MTJ (Magnetic Tunnel Junction) Device Fig. 2. The resistance depends on the crossing voltage V . The upper curb corresponds to RAP and the lower curb to RP . (Redrawn from [19]) Fig. 1. The MTJ ”sandwich structure” in its two states (parallel (a) and anti-parallel (b)) [17]. 1) Theory: Figure 1 shows the three-layer ”sandwich structure” of the MTJ device [16], which uses the spin of electrons and its associated magnetic moment in solid-state devices to store a logic value (0 or 1). It consists of two magnetic layers separated by a thin spacing layer (insulator). One of the magnetic layers (the ﬁxed layer) has always the same magnetization vector. The other layer (the free layer) vector can switch in between a parallel direction (ﬁgure 1a), and the opposite one (ﬁgure 1b). By applying a small current between the two layers, one can notice a difference of resistivity depending on the direction of the free layer. In a parallel state, the electron will tunnel easily between the two layers, and then show a ”low” resistivity (RP , ”1” logic state) crossing the device. However, in the anti-parallel state, a bigger resistance (RAP , ”0” logic state) will appear. This difference can thus be seen as a memory unit. In Figure 1, part (a) shows the MTJ in the parallel state (RP , ”1”), and part (b) in the anti-parallel state (RAP , ”0”). The ratio between the two resistance states is deﬁned by the Tunnel Magneto-resistance Ratio (TMR) which is calculated as follows: T MR = RAP − RP RP TMR can vary from 30% up to 500% at room parameters. It is also temperature sensitive: When the temperature is rising, TMR drops down [18]. In fact, during a write action which usually consists of a strong current crossing the MTJ, heat appears rapidly. The resistance for a given state is then not a simple ﬁxed value, neither is the TMR. It actually depends on the voltage going through the MTJ. Figure 2 shows the resistance change in response to the switching crossing voltage. We can see that the ratio is dropping down quickly Fig. 3. Spin Torque Transfer (STT) principle: (a) shows a RAP to RP switching, and (b) the opposite [17]. 2) Switching Energy: To change the state stored, several techniques exist [17], [19], [11], [20], [21]. A common technique is Spin-Torque Transfer (STT), which injects a strong current through the device. Depending on the direction, the MTJ switches its state as shown in ﬁgure 3. The faster the switching is, the higher the required current is (ES in ﬁgure 4). Fig. 4. Required switching energy depending on switching time with three regions: precessional switching, dynamic reversal and thermally activated switching [17]. 52 power consumption can be derived in a more accurate way. This means that our circuits will simulate only reading MTJ actions on random pre-assigned MTJs, and writing energy will be computed using the value of 1 f J for switching energy consumption from Section III-A2. This value will be converted to a power value depending on the frequency and then multiplied by the number of MTJ devices. The chosen reading model consists of a variable resistance, with an anti-parallel value (around 0.9 kΩ) and a parallel one (around 0.5 kΩ). But these values also depend on the voltage crossing the device while reading, as suggested in [25]: 1 = 1 1 a · exp(−b · V ) R c with a, b and c described in Table I. (2) + VA LU E S U S ED FOR S IMU LAT ION IN TH E ABOV E FORMU LA TABLE I a (kΩ) b (V −1 ) c (kΩ) parallel 2.43461 (kΩ) 5.567337 (V −1 ) 0.633625 (kΩ) anti-parallel 4.334744 (kΩ) 5.792084 (V −1 ) 1.140817 (kΩ) Depending on the current pulse width, three switching energy regions exist, namely, precessional switching, dynamic reversal and thermally activated switching, as shown in Figure 4. In the precessional switching region, the energy consumption decreases as the switching time increases. While in the thermally activated switching region, the energy consumption increases as the switching time increases. The long switching time for the thermally activated switching region makes it less interesting. The dynamic reversal region is the transition region from the precessional switching region to the thermally activated switching. As can be observed in ﬁgure 4, it achieves the lowest energy (the lowest points in the ﬁgure) with a reasonably fast switching time (around 10 ns). Formally, the relation between the energy consumption and its related parameters can be represented with the following formula [17], [22]: (1) ES = (JC0 + C )2 · A2 · R · tS tS where the switching energy (ES ) depends on the critical current density (JC0 ), the switching time (tS ), the cross sectional junction area (A), the MTJ current state resistance (R) and a material constant (C). In November 2010, Zhao et al. [20] obtained a RAP to RP (resp. RP to RAP ) switching at 1.54 ns (resp. 0.68 ns) consuming 0.286 pJ (resp. 0.706 pJ ). At 500 MH z, it gives a 0.24 mW power consumption against 8 μW for a CMOS ﬂip-ﬂop [23]. However, 1 f J energy consumption per switching is expected soon for new MTJ devices [17]. Our work will assume this last value, which corresponds to 0.05 μW at 500 MH z. Also, in order to simplify the Spice models, only the reading power consumption is simulated, and the writing (switching) energy will be computed using this value. B. On-Chip Router A router consists of a data path and a control path. A typical data path is composed of buffers and a crossbar, while the control path of routing, arbiter, and ﬂow controller. As the data path is most power hungry and the control path not always standard, we focus on studying the potential of reducing power consumption for buffers and crossbar components enabled by MTJ devices. The result is a kind of mixed MTJ-CMOS circuitry. In fact, buffers are typically implemented as hardware registers in on-chip routers. Their size can easily reach 128-bit width and the speed is usually around 200 MH z - 2 GH z [24]. Crossbars should work at the same speed and are usually built with CMOS-tristate gates in a matrix style (ﬁgure 14). IV. MT J D EV IC E MOD E L ING A. The MTJ Model Complete MTJ models can be found in open sources [17], [25], simulating both writing and reading (in use) power consumption. However, the simulation time increases nonlinearly as the complexity of the models increases. In this study we used a simple model yet giving accurate results. The model is the double variable controlled resistance model [25], and we only used the reading part of the model because the writing Fig. 5. RAP (green) and RP (red) according to the values in [26] and the model in [25]. These resistance values are adapted from [25] to ﬁt best the needs of the simulation. In fact, resistance values can widely vary from one paper to another ([17], [26], [27], [28]), taking values from hundreds of Ω to a couple of MΩ. This can be explained as it depends mostly on the MTJ tunnel thickness. Figure 5 shows the resistance depending on the crossing voltage. This voltage is applied when the device is read. The stronger the voltage is, the faster the device is, but also the more power-consuming. The numbers used in [25] are issued from an MTJ conductive atomic force microscope study [26], which reported resistance values depending on Vt hrough in between [−1V ; 1V ]. 53 This is more than the real voltage during reading stages (10 50 mV ), making the approximation valid in this case. B. The MTJ Reader The evaluation of an MTJ state is done by comparing two MTJs, storing opposite values, as shown in ﬁgure 6(a). The left MTJ stores the real value, and the right one the opposite one. Due to the resistance difference, more current will pass by a branch, triggering the comparator to set the out and ¯out signals. Fig. 6. MTJ reader principle with 2 MTJ units (a) or hybrid, with 1 MTJ unit and 1 resistor (b). But using two MTJs requires switching two MTJs instead of one each time a new value is written. This structure consumes twice the MTJ switching power. Also, by replacing an MTJ (ﬁgure 6(b)) by a simple reference resistor in the real implementation, one can avoid this drawback. This reference resistor should be always seen by the current comparator as the ”opposite value” of the MTJ resistance. Thus, the reference resistor value should be chosen between RAP and RP . This study will consider the average value, RAP+RP . In our case, since we are working with MTJs providing either 0.5 kΩ (RP ) or 0.9 kΩ (RAP ), the reference resistor value is 0.7 kΩ. 2 Fig. 7. The Spice simulation result of an MTJ reader, with 2 MTJs and with 1MTJ+Resistor. The circuit with a resistor is represented in pink and red (resp. out and ¯out ), and the circuit with 2 MTJs is represented in blue and light blue (again, resp. out and ¯out ). Figure 7 shows the read action for an MTJ reader with two MTJ devices (blue), against a reader with one MTJ and a reference resistor (pink & red). The ﬁrst line is the system clock, enabling MTJ evaluation at the high level. One can see that the system requires a few hundreds of ps to stabilize, and that the combined system is slightly slower. After about 0.4 ns, the output value stabilizes at ”0”. 54 Fig. 8. Dynamic Current Model Logic circuit (DyCML). Figure 8 shows a circuit in Dynamic Current Model Logic (DyCML) [17], [14] following the previous principles with a Cross-Coupled Keeper (CCK) as current comparator and a Dynamic Current Source (DCS). The CCK is represented by the 4 PMOS and 2 NMOS above the MTJ/resistor. The DCS avoids steady current from VDD to GND, resulting in low power consumption. Throughout the paper, we use the Spice model from [29] for the CMOS transistors. V. BU FF ER MOD E L S AND POW ER EVALUAT ION A. Buffer Modeling We present how buffers were implemented, with both MTJ and CMOS technologies, to get a power consumption reference and compare the simulation results. To facilitate reading the following, we make two notes: • The ﬁgures presenting the MTJ buffer implementations will only show the reading circuitry, as the writing one has been explained in section III-A2. The circuits presented do not describe usual serially linked register cells. As the main advantage of Spintronics is to keep values in an energy-free way, writing the value in the next cell at each clock cycle will be a waste of energy. Since the MTJ-based buffer is not in favor of the register-type implementation, it should be implemented as a RAM-type buffer such as a circular buffer, which moves pointers rather than the data in the storage cells when accessing the buffer. Thus, the main idea of this study was to write and get values, as a memory unit. To access the MTJbased buffer, random access is preferably assumed. • The term buffer size refers to buffer depth D as all the presented designs have a data width of 1 bit (W = 1). To build a buffer with a depth of D and a width of W bits, i.e., a D ×W buffer, one needs to use as many as W parallel structures, each structure with a depth of D. B. CMOS-based Buffer Model To compare the power consumption, we use CMOS dynamic two-phase ﬂip-ﬂops (ﬁgure 9) as the CMOS buffer model to give an estimation. The ﬂip-ﬂops are in parallel and the number corresponds to the buffer size (up to 1000 bits/ﬂipﬂops). buffer size (over 50 bits) with a large stabilization time and power consumption. To improve this, we designed a 3dimensional reading circuit for bigger buffers. The structure of the CCK remains unchanged. The middle part of the circuit is enhanced with new super-enable NMOS transistors (Sci) to control, not only an MTJ device as before, but a whole branch of MTJ-NMOS couples (ﬁgure 12). Each superenable NMOS transistor drives 10 MTJ devices, each of them individually driven by a classical enable NMOS transistor. A small logic circuit will be required to determine which superenable transistor to switch, depending on the MTJ device targeted. Fig. 12. The MTJ buffer reading circuit for storage size over 50 bits (up to 1000 bits). This improved implementation avoids the current to be spread through all the branches, since only one will be ﬁnally evaluated. As an extension of this circuit, one can even think about implementing a 4-dimensional circuit, for even bigger size devices. The transistor sizes and the capacitance have to be scaled with the buffer size, and both are rising when growing the buffer size. D. Buffer Power Evaluation The power consumption was evaluated with the following parameters: 4 ns clock period, 25% duty cycle, 50% switching probability, 1 V voltage supply, 50 nm technology and 1 f J energy per MTJ switching. At each clock cycle, a bit is read out from the device, and other bits have a 50% switching probability. The frequency chosen for the simulation may appear to be slow compared to high-speed routers but it is however a reasonable value for power-sensitive routers [24]. MTJ devices have no fundamental frequency limitations, and the coming smaller devices will help speed-up frequency. Power consumption for the CMOS device is around N times Fig. 9. A typical CMOS ﬂip-ﬂop. C. MTJ-based Buffer Models We detail a scalable MTJ buffer device which could work with a data width up to 1000 bits. The idea was ﬁrst to implement as many DyCML cells as the number of bits (ﬁgure 10). However, the whole circuit would be duplicated many times, and thus very power and area consuming. Fig. 10. First buffer implementation by duplicating as many DyCML as needed. Only the reading circuitry is shown on the ﬁgure. We observe that, for a buffer with bit width of 1, the values can be stored quite a long time, and only one value is read at each clock cycle. Then, one circuit with one output can be shared for all the storage devices. Figure 11 shows the improved solution, which keeps one reading circuit (CCK and DCS) and in between, several MTJ devices enabled with an NMOS transistor to select them. Fig. 11. The MTJ buffer reading circuit for storage size under 50 bits. However, this implementation becomes unusable for big 55 the ﬂip-ﬂop power consumption, where N is the bit size of the buffer. This is the value reported in ﬁgure 13 for the CMOS ﬂip-ﬂop implementation. HSpice tool was used to compute the instant power, and the average power using CScope Waveviewer tool. The process was quite the same for MTJ implementations to get the reading energy. One needs to add the switching power to this value. As discussed previously, the switching power value is 0.1 μW per MTJ. V I . CRO S SBAR MOD E L S AND POW ER EVALUAT ION A. A Crossbar Model Fig. 14. A 2×2 crossbar. Control bits (C1 and C2) can be either ﬂip-ﬂops or MTJ reader devices. A crossbar in a router switches packets from inputs to outputs. One way to implement it is to use tri-state buffers enabling or disabling an input to drive an output. Let’s assume for instance a 2-input and 2-output tri-state crossbar, as shown in Figure 14. For each output, a 1-bit control signal asserts which input is allowed to forward the data to the output. Two control bits are then stored, one for each output. In the following, we show how they are implemented in CMOS, and we propose two MTJ implementations. As a matter of simplicity, only 1 bit width crossbar is shown, but a real implementation of real wide-width crossbar is just a parallel use of the circuit. A crossbar may be alternatively implemented using pass gates. A pass-gate style of crossbar implementation requires one pass gate at each matrix crosspoint. A N × M crossbar has N × M crosspoints and thus requires N × M pass gates. However, as each input and output line is connected to a large number of parallel transistors, which are in turn connected to the pull-up and pull-down transistors of the drivers, it presents a large load capacitance and multiple low-resistance leakage paths to the ground [30]. Though demanding less number of transistors, it consumes signiﬁcant static power and reduces signal throughput. Due to this reason, we did not consider the pass-gate implementation for the crossbar. B. CMOS-based Crossbar Fig. 15. A CMOS tri-state buffer structure. 56 Fig. 13. Results of the buffer implementations comparing both CMOS (blue) and MTJ (red) implementations. Figure 13 shows the average power including both read and write actions. The blue curve corresponds to the CMOS implementation, and the red one to the MTJ implementation. We can see that the CMOS regular ﬂip-ﬂop leads to better results for small size devices (up to 10 bits). After that, the MTJ implementation becomes more power efﬁcient. For 96bit and above buffers, the MTJ power is only about 44% of the CMOS power. This percentage remains almost constant when increasing the size up to 1000 bits, which is the maximum value simulated in this study, however not a boundary reached. In fact, bigger buffer implementations are still possible, but the depth of on-chip buffers is usually smaller than that [24]. The critical size value of 10 bits is the limit over which MTJ buffers are more power saving than CMOS circuits. However, the device still needs around 0.5 ns for big buffer sizes to stabilize the value. Also one should not forget that the value used to compute MTJ switching energy is not yet reached, but should be achieved within a short time, when technology will scale down to 20 nm [17]. Of course, one may achieve further power reduction since it is possible to cut off the VDD power supply instantly for some unused parts of the buffer, and supply it again when required as it needs no set up time. A CMOS tri-state buffer (ﬁgure 15) implies 1 PMOS + 1 NMOS for the data and 1 PMOS + 1 NMOS for the control bit. It is very scalable and then easy to use in bigger case than 2 × 2 crossbars because one needs only to add a new PMOS/NMOS couple in the structure to add another control bit. This structure is very simple and low power consuming. But to use it for crossbar design, one needs a ﬂip-ﬂop to store the control bit (cf. ﬁgure 14). Moreover, according to the length of the packets, this stored enable signal will be often evaluated but only a few write actions will be processed. Thus, one can suppose that storing this data with a CMOS ﬂip-ﬂop is not the most power efﬁcient implementation, comparing it to an MTJ implementation for instance. C. MTJ-based Crossbars Two studies have been conducted, working on two different implementations. 1) MTJ Enhanced CMOS-based circuit: A ﬁrst idea would be to enhance the CMOS-based circuit (ﬁgure 14) by replacing the control bits of the CMOS ﬂip-ﬂops with MTJ readers (C1 and C2 blocks). The behavior will remain unchanged and the same style can be applied for larger crossbars. However, an MTJ reader (a.k.a. a buffer of size 1) is more power consuming than a simple ﬂip-ﬂop according to ﬁgure 13, even if there would be less write access than in the previous study. Moreover, even if several control bits have to be stored, it is no longer possible to use the previously described MTJ buffers, since all the control bits are read at the same time. Also, an MTJ switching (write) action takes more time to proceed than a CMOS switching, and it will delay the crossbar switching, which is more crucial than in the buffer case. Fig. 16. A Logic-in-Memory 2-in-1-out Mux structure. 2) Logic-in-Memory (LiM) implementation: Another idea would be to rethink the whole circuitry by looking into the LiM domain. Figure 16 shows a circuit corresponding to a so-called LiM mux device. It looks like a usual MTJ reader, except the transistors driven by A and B. The idea is to store in an MTJ the most stable value between inputs A and B and the control bit C. Since the control bit is the most stable, it is the one stored into the MTJ device. Then the output signal 57 S represents the Boolean function S = A · C + B · ¯C. However, even if this method would get rid of both ﬂip-ﬂop storage and the CMOS tri-state buffer, this solution works for a 2-input 2output crossbar, but the main problem of this implementation is scalability. Usual crossbars are bigger than 2 × 2, and there is no evident scalable LiM structure to implement. D. Crossbar Power Evaluation POW ER CON SUM P T ION O F A 2 × 2 CRO S SBAR TABLE II Implementation Average total power CMOS based MTJ Enhanced CMOS-based MTJ Logic-in-Memory Mux 0.62 μW 2.4 μW 2.6 μW Table II lists the power evaluation results. They are given for the same parameters described previously, but with a 300 ns switching period and a 2 ns clock period. Wire dissipation power was not taken into account in this simulation, it is only the components that are evaluated. The MTJ model was good for the buffer implementation, but it appeared that for the crossbar, the CMOS implementation has better scalability, and consumes less power. Thus, at least for now, it is not interesting to use a simple MTJ reader instead from a 2 × 2 crossbar to a larger one. But a smart scalable of the ﬂip-ﬂop. It is then pointless too to increase the size implementation using a LiM unit may lead to good results. In fact some studies [14] predicted this kind of circuit to be better than CMOS implementations. But this kind of scalable structure awaits to come in the future, and by extrapolating the 2 × 2 LiM crossbar, we could expect functional and power saving MTJ-based crossbars soon. V I I . CONC LU S ION In this paper, the MTJ device behavior was described and modeled in a Spice model. Then, the switching energy was discussed, and we saw that the actual values are still too high (around pJ ). However, the coming generation will eliminate this drawback when starting to process 20 nm technologies and below. We will obtain values around f J , which is in the same order as the CMOS switching energy. Based on this assumption, MTJ-based implementations of router datapath components, buffers and crossbar, were detailed, and their power consumption was studied against their pure CMOS counterparts. It appears that buffers can be replaced by scalable MTJ devices, showing over 44% of power reduction for 96bit and above buffers. This tells us that MTJ devices are good candidates for storage dominated structures like buffers. But there is no simple power-efﬁcient MTJ implementation for the crossbar yet, as the crossbar is wire dominated. With this study, we have aimed to present the different possibilities and show the potential of integrating MTJ devices in routers and thus to pave a way for further studies in this new direction. Focusing on investigating the dynamic power, we have not considered the leakage power in this study. As reported in [17], MTJ devices can achieve four times less leakage power [20] H. Zhao, A. Lyle, Y. Zhang, P.K. Amiri, G. Rowlands, Z. Zeng, J. Katine, H. Jiang, K. Galatsis, K.L. Wang, I.N. Krivorotov and J-P. Wang, Low writing energy and sub nanosecond spin torque transfer switching of inplane magnetic tunnel junction for spin torque transfer random access memory, Journal of Applied Physics, vol. 109, no. 7, 2011. [21] Toshiba, High-Density and Low-Power Spin-Transfer-Torque MRAM with Perpendicular MTJs, http://www.toshiba.co.jp/rdc/rd/ﬁelds/08 e06 e.htm. [22] Y. Huai, Spin-Transfer Torque MRAM (STT-MRAM): Challenges and Prospects, AAPPS Bulletin, vol. 18, no. 6, 2008. [23] D. Markovi ´c, B. Nikoli ´c, R.W. Brodersen, Analysis and Design of LowEnergy Flip-Flops, International Symposium on Low Power Electronics and Design, 2001. [24] E. Salminen, A. Kulmala and T. D. Hamalainen. Survey of Network-onchip Proposals. OCP-IP white paper. March 2008. [25] S. S. Mukherjee and S.K. Kurinec, A Stable SPICE Macro-Model for Magnetic Tunnel Junctions for Applications in Memory and Logic Circuits, IEEE Transactions on Magnetics, vol. 45, no. 9, 2009. [26] M-F. Shua, A. Canizo-Cabreraa, C-C. Hsua, C.C. Chenb, J.C. Wub, C-C. Yangc and T-H. Wu, The magnetoresistance ratio of an MTJ device and the inﬂuence of ramping DC bias voltage rate measured by conducting atomic force microscope, Journal of Magnetism and Magnetic Materials, vol. 304, no. 1, 2006. [27] C-H. Cho, J.H. Ko, D. Kim, A CMOS macro-model for MTJ resistor of MRAM cell, Physica Status Solidi A. Applied Research, vol. 201, no. 8, 2004. [28] S. Ikeda, J. Hayakawa, Y.M. Lee, F. Matsukura, Y. Ohno, T. Hanyu and H. Ohno, Magnetic Tunnel Junctions for Spintronics Memories and Beyond, IEEE Transactions on Electron Devices, vol. 54, no. 5, 2007. [29] UC. Berkeley, BSIM4. Berkleley EECS device, 2009, http://www- device. eecs.berkeley.edu/∼bsim3/bsim4.html. [30] E. Coen-Alfaro and G. W. Donohoe, A Comparison of Circuits for On-Chip Programmable Crossbar Switches, Proc. of the 10th NASA Symposium on VLSI Design, March 2002. than CMOS devices. In the future, we shall take the leakage power into consideration to build a complete power model, and then apply it for real applications to assure the potential power saving gains with MTJ devices. "A Statically Scheduled Time-Division-Multiplexed Network-on-Chip for Real-Time Systems.,"This paper explores the design of a circuit-switched network-on-chip (NoC) based on time-division-multiplexing (TDM) for use in hard real-time systems. Previous work has primarily considered application-specific systems. The work presented here targets general-purpose hardware platforms. We consider a system with IP-cores, where the TDM-NoC must provide directed virtual circuits - all with the same bandwidth - between all nodes. This may not be a frequent scenario, but a general platform should provide this capability, and it is an interesting point in the design space to study. The paper presents an FPGA-friendly hardware design, which is simple, fast, and consumes minimal resources. Furthermore, an algorithm to find minimum-period schedules for all-to-all virtual circuits on top of typical physical NoC topologies like 2D-mesh, torus, bidirectional torus, tree, and fat-tree is presented. The static schedule makes the NoC time-predictable and enables worst-case execution time analysis of communicating real-time tasks.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip A Statically Scheduled Time-Division-Multiplexed Network-on-Chip for Real-Time Systems Martin Schoeberl, Florian Brandner, Jens Sparsø, and Evangelia Kasapaki Department of Informatics and Mathematical Modeling Technical University of Denmark Email: masca@imm.dtu.dk, ﬂbr@imm.dtu.dk, jsp@imm.dtu.dk, evka@imm.dtu.dk Abstract—This paper explores the design of a circuitswitched network-on-chip (NoC) based on time-divisionmultiplexing (TDM) for use in hard real-time systems. Previous work has primarily considered application-speciﬁc systems. The work presented here targets general-purpose hardware platforms. We consider a system with IP-cores, where the TDM-NoC must provide directed virtual circuits – all with the same bandwidth – between all nodes. This may not be a frequent scenario, but a general platform should provide this capability, and it is an interesting point in the design space to study. The paper presents an FPGA-friendly hardware design, which is simple, fast, and consumes minimal resources. Furthermore, an algorithm to ﬁnd minimum-period schedules for all-to-all virtual circuits on top of typical physical NoC topologies like 2D-mesh, torus, bidirectional torus, tree, and fat-tree is presented. The static schedule makes the NoC timepredictable and enables worst-case execution time analysis of communicating real-time tasks. Keywords-real-time systems; network-on-chip I . IN TRODUC T ION Network-on-chip (NoC) design has been an active area of research in academia and industry for the past decade. Today the interconnect fabric of single-chip multi-core systems is typically some form of packet or circuit switched interconnection network. This is the case for general-purpose chipmulti-processors (CMP) as well as for application-speciﬁc multi-processor systems-on-chips (MPSoC) used in a large variety of embedded systems. Despite the growing similarity in the way CMPs and MPSoCs are architected, there are some fundamental differences as well: CMPs are typically homogeneous, i.e., built from many identical processors, and the focus is generally on providing the highest possible performance. MPSoCs are typically used in embedded systems and hence dedicated to a speciﬁc product or class of products. Thus, they often use a heterogeneous set of processing cores in order to meet energy and performance constraints. In many cases, these systems additionally have to provide hard real-time guarantees. Much of the recent research in NoCs for real-time MPSoCs is targeting the creation of application-speciﬁc hardware platforms. However, the very high and growing cost of developing and fabricating large integrated circuits makes this relevant only for high volume (consumer) products. IP IP IP IP IP IP Network−on−chip − TDM−based − Virtual circuits; all−to−all − Topologies: 2D−mesh, torous, tree Figure 1. A TDM-based NoC providing all-to-all connections. Systems that are to be manufactured in smaller quantities must therefore be based on more general-purpose platforms. In this paper we explore the design of a statically scheduled, time-division-multiplexed (TDM) network-on-chip for such general-purpose multi-processor platforms for use in realtime systems. In order to give hard guarantees on the real-time properties of a system, the designer must be able to analyze and safely estimate the worst-case execution time (WCET) of the different tasks in the system. The fact that the NoC is a shared communication medium comprising multiple independentlyarbitrated resources (routers and links) severely complicates timing analysis, and makes it hard to compute the WCET and guarantee real-time behavior. Real-time NoCs have the following options: (i) non-blocking routers with rate control (e.g. Mango [1]); (ii) circuit switching where a connection owns the resources providing the connection (SoCBUS [2]); and (iii) virtual circuit switching, for example using TDM (Æthereal [3], aelite [4], and Nostrum [5]). The work presented in this paper is inspired by the aelite NoC as well as initial discussions within the T-CREST1 project. In contrast to aelite and its associated CAD tools – aiming at creating application-speciﬁc platforms derived from use-cases and task-graphs – our aim is to explore the design of a TDM network-on-chip for use in generalpurpose, real-time MPSoC platforms. Additional goals are: hardware simplicity, speed, and an FPGA-friendly design enabling both chip and FPGA implementations. We assume a system with n IP-cores, where the TDMNoC must provide directed virtual circuits – all with the 1 http://www.t-crest.org/ 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.25 152 same bandwidth – between all nodes, as shown in Figure 1. In the following we will address the hardware design of such a general-purpose TDM-NoC, and we will explore how to schedule packets and provide all-to-all point-to-point virtual circuits on top of typical physical NoC topologies like 2Dmesh, torus, bidirectional torus, tree, and fat-tree. One of the questions that we try to answer is the following: “What is the minimum period of a schedule that provides virtual circuits with identical bandwidth between all pairs of nodes?” The contributions of the paper are as follows: • Design of a time-predictable NoC for real-time systems • An algorithm to generate static TDM schedules • Study of different topologies with respect to the minimum period of static schedules • A small, fast, and FPGA-friendly router design An all-to-all communication schedule might lead to a low utilization. However, a ﬁxed schedule can be implemented in hardware. Therefore, our NoC uses less resource than one with programmable schedule support or additional besteffort trafﬁc support. The performance/cost ratio is more important than fully utilization. The paper is organized as follows. In the next section, we discuss related work. In Section III we motivate our design with a problem statement. The design of the time-predictable NoC is described in Section IV. In Section V we present an ILP-based algorithm to ﬁnd minimum-period schedules. Analytical lower bounds of schedule periods are presented in Section VI. In Section VII we present evaluation results for a range of NoC topologies (2D-mesh, torus, bidirectional torus, tree, and fat tree) and provide details of a speciﬁc FPGA-based implementation. Section VIII concludes the paper. I I . R E LAT ED WORK In the following we describe approaches to use NoCs for real-time systems and how to generate schedules for those. A. Real-Time Network-on-Chips MANGO [1] is an asynchronous NoC, which supports both guaranteed service (GS) and best effort (BE) trafﬁc, using non-blocking routers and rate control. A non-blocking router requires a separate physical buffer for each virtual circuit, an elaborate arbitration mechanism for each router output port, and a credit-based ﬂow control mechanism among output buffers in neighboring routers. This indicates that the hardware cost of rate-controlled routers considerable. SoCBUS [2] and Wolkotte [6] use a pure circuit-switching NoC, i.e., no resources, such as wires and router buffers, are shared between connections. This lowers utilization and increases costs. However, once a connection has been established, real-time guarantees are trivially achieved. It is, however, possible that a requested connection cannot be set up due to lack of resources (links) – this may compromise the real-time properties. Æthereal [3] uses TDM, i.e., reserves resources for certain points in time. In each slot of time a block of data is forwarded through a router without waiting or blocking trafﬁc, hence, contention cannot occur. Slot tables with routing information are contained in the routers and no arbitration or link-to-link ﬂow control is required. Instead, a credit-based ﬂow control is applied for end-to-end control, saving buffer space between links. Guaranteed services are combined with best effort routing in order to utilize unreserved resources. aelite, a light version of Æthereal, only offers guaranteed services resulting in a simpler router design [4]. Slot tables are placed in the network interfaces and routing is done through message headers. In the latest version of aelite, called dAElite [7], the static routing tables are back in the routers to support multicast routing. A time-triggered NoC (TT-NoC) applies the concepts of the time-triggered architecture (TTA) [8] to NoCs [9]. The TT-NoC consists of a ring structure and is therefore only intended for a small number of IP-cores. As the ring is built out of simple multiplexers and registers, it is clocked at double the frequency of the computation nodes. Similar to our presented design, the communication schedule is static and predetermined. Paukovits and Kopetz use a time-triggered NoC for the time-triggered system-on-chip (TTSoC) architecture [10]. The main difference to other NoC designs is the absolute time format for the TDM slotting, which is called a macro tick. The macro tick is not directly related to the clock frequency. The network interfaces (called TISS in TTSoC) are synchronized at the macro tick, which is distributed separately. The TDM-based slot at macro ticks may last several clock cycles at the network clock. The slotting ignores any pipeline effects within the NoC and therefore results in some idle time within links. Our design shares the idea of static scheduling based on TDM. However, we base our schedule on the ﬁner granular network clock and take pipeline effects into account in the network. Static scheduled communication between processors was explored in the NuMesh project [11]. It is argued that static schedules result in less hardware complexity and the control circuit for the static decisions can be arbitrarily deep pipelined. In contrast to NuMesh, we further simplify the hardware as the schedule is completely ﬁxed and no hardware to support the reprogramming is needed. The Raw machine [12] is a 16-tile architecture, where each tile contains a MIPS-like processing pipeline and a network router. Similar to our router architecture, the Raw router also contains static routes (besides dynamic routes). The network in Raw is tightly connected to the processing core: the network input and output buffers are mapped to processor registers. This allows a very low latency where an output of the ALU can be routed to the ALU input of 153 R R R IP IP IP Router (3 x 3) Bi−directional link IP−core Router (4 x 4) Bi−directional link IP−core (a) Mesh (b) Torus (c) Bi−torus (d) Binary tree (e) Fat−tree R R (f) Ring IP (g) Bi−ring IP Figure 2. The NoC topologies considered: (a) mesh, (b) torus, (c) bidirectional torus, (d) binary tree using 3 × 3 routers, (e) fat-tree built from identical 4 × 4 routers, (f) ring, and (g) bidirectional ring. A ﬁlled circle represents a node, comprising a router and an IP-core as shown. the neighbor processor in 3 cycles. This tight integration of the network and the processor pipeline is the basis for, so-called, software circuits, i.e., applications that resemble ASIC circuits. B. Routing Schedule Construction Lu and Jantsch [13] propose a conﬁguration technique for the Nostrum NoC [5] that allows multiple virtual circuits to share buffers of the network. They present a problem formulation that deﬁnes a legal allocation of TDM time slots using a backtracking search algorithm. In contrast to our problem, only a single assignment of a given set of virtual circuits is needed that satisﬁes the required bandwidth and a conﬂict-free operation of the NoC. A similar slot allocation problem appears for the Æthereal NoC. The allocation here proceeds in two steps. First, routing paths are determined through the NoC depending on a mapping of an application to the network and the application’s communication requirements [14]. Given these paths, TDM time slots are allocated for each virtual circuit in turn [15]. This technique has been extended to split packets and deliver the individual fragments of the packet over multiple paths in order [16]. This approach provides a single solution satisfying the application-speciﬁc communication and bandwidth requirements. The scheduling problem considered in this work can formally be stated as a dynamic multi-commodity ﬂow problem over time. A seminal work by Ford and Fulkerson introduced time-expanded ﬂow networks to model dynamic ﬂow problems using equivalent static problems [17]. A timeexpanded network is a structure containing replications of the network for several time instants (e.g., clock ticks). Fleischer and Skutella study variants of the NP-hard quickest multi-commodity ﬂow problem [18] and present a polynomial 2-approximation algorithm. Although closely related, these results apply to general multi-commodity ﬂow problems, where fractional solutions are acceptable. In the context of this work, however, integer solutions are required since the physical hardware resources are indivisible. I I I . R EA L -T IM E N E TWORK -ON -CH I P In dependable real-time systems it needs to be guaranteed that all deadlines will be met. This guarantee is performed by schedulability analysis. The input to this schedulability analysis is the worst-case execution time (WCET) of the tasks. To enable WCET analysis, all components of the system (the application software, the processor, the memory subsystem, and the communication network) need to be time-predictable. We aim for a time-predictable NoC that supports WCET analysis. To enable time-predictable usage of a shared resource the resource arbitration has to be time-predictable. In the case of a NoC, statically scheduled TDM is a time-predictable solution. This static schedule is repeated and the length of the schedule is called the period. Like tasks in real-time systems, also the communication is organized in periods. One optimization point of the design is minimizing the period to minimize the latency of delivering ﬂits and the size of the schedule tables. Many NoCs are intended to be optimized for a given application or application domain. The NoC structure and/or the routing schedules are then optimized and are then 154 application-speciﬁc. While our proposed network can be optimized this way, we aim at a general-purpose solution. The general-purpose solution allows each core to communicate to every other core and the bandwidth is identical for each communication channel. For a general-purpose solution we need to ﬁnd a single static schedule, which can then be implemented in hardware. We look at several different NoC topologies and evaluate how well they support this general-purpose static schedule. We consider mesh, torus, torus with bidirectional links (bitorus), tree, fat-tree, ring, ring with bidirectional links (biring), and bus topologies. Figure 2 shows these topologies. Except for the tree, the fat-tree and the bus, we assume that each topology is composed of n nodes each consisting of an IP-core and a router. With tree structures the IP cores and router do not have a one-to-one mapping. The routers range from 2-ported routers (2 in and 2 out) to 5-ported routers: the mesh (inner nodes) and the bi-torus use 5-ported routers; the fat-tree uses 4-ported routes; the torus, the bi-ring, and the tree use 3-ported routers; and the ring use 2-ported routers. In this paper we concentrate on the network itself and consider it as a structure that supports communicating streams of ﬂits. Designing the network interface and the ﬂow control are out of the scope of this paper. IV. N E TWORK D E S IGN A static schedule guarantees latency and bandwidth for sending data over the NoC, which itself enables WCET analysis of tasks. Furthermore, this static schedule allows optimization of the routers. With our design we avoid transmitting the packet route via the network, but keep the network schedule in the routers. With a predeﬁned schedule there are no collisions possible. The simple router, as shown in Figure 3, consists of multiplexers and registers. This structure ﬁts very well to the structure of a logic cell (LC) in an FPGA. A LC usually contains a lookup table (LUT) and a register. The LUT is used to build combinational logic (e.g., the multiplexer). Although we do not want to restrict our NoC to FPGAs, we consider FPGAs as an important platform and aim for an FPGA-friendly design. A. Packet Organization As the static schedule is contained in the router, the ﬂits traveling through the network contain only data and no routing information. The time slot the ﬂit is injected to the network implicitly gives the destination address. Without the address header we are less constrained on packet lengths – we do not need to amortize for the address overhead. Therefore, we deﬁne that a packet is a single ﬂit long and use schedules for single clock/ﬂit granularity. This short packet length minimizes the latency for short data transportation. The link width is usually determined by the length of the address ﬁeld (the packet header) and therefore depends on L N S E W L N S E W L ST N ST S ST E ST W ST Figure 3. Connections of the multiplexer based router Slot Cnt the network size. Free of these constraints, we can use any link width that is needed for the bandwidth requirements. The resource consumption of a router is directly related to the link width. Therefore, we can trade bandwidth with resource consumption. At the lowest level the individual ﬂits are considered to be individual words of a data stream – similar to a serial line connection. The only control structure is a valid bit in the ﬂit. The framing of the data, the forming of longer packets (e.g., cache line ﬁlls), and the meaning of the stream is deﬁned in the network interface. Or in other words: the NoC just represents a transport layer with end-to-end channels. Using single ﬂit packets and a static schedule results in a deadlock free design. B. Router One of the beneﬁts of a static scheduled NoC is the simpliﬁcation of the network routers. Figure 3 shows the router for our NoC. A router consists of registers for a single ﬂit at the output ports and a multiplexer feeding this register from each input port. Due to the static schedule there is no need for dynamic arbitration or additional buffers. Furthermore, there are no ﬂit collisions or deadlocks possible. Flits move each clock cycle one hop. They cannot stay within the routers’ registers. 155 Figure 3 shows a router for a mesh or bi-torus NoC with 5 input and 5 output ports. Input and output L stands for the local connection to the IP-core; N, S, E, and W represent the four directions in a mesh-style network. Each multiplexer is connected to four inputs. The black dots represent a connection point. The box with ST represents the schedule table for each multiplexer. A slot counter drives the schedule table. This counter can be a global one. To avoid long wires from this global schedule counter the distribution can be pipelined or the counter can simply be replicated locally in every router. C. Schedule Tables In [4] schedule tables in the routers have been avoided due to high resource consumptions. However, in our design two properties make schedule tables in routers attractive: (a) a ﬁxed schedule (compared to a conﬁgurable) is cheaper to implement (in a ROM); and (b) the size of the table (the schedule period) is relatively small for the symmetric bandwidth conﬁguration. Each router has a table for the static schedule, which basically includes just the multiplexer selection. The schedule period is quite short. Therefore, the resource consumption for this table is low. Let us consider a small network, a 3x3 mesh network, as an example. For this network the ideal static schedule is 10 entries long – in each clock cycle each IP-core injects a ﬂit into the network and receives one ﬂit. Let us assume that we cannot ﬁnd this ideal schedule, but one that ﬁts into a maximum of 16 entries. With FPGA technology, 16 entries of a schedule table ﬁt perfectly into the available 4-bit LUT. Therefore, for each multiplexer, which has 2-bit select lines, 2 LUTs are needed to represent the schedule table. D. Router Size Estimation To set the size of the schedule table in relation to the complete router resource consumption we assume for now a 16-bit link. A 4:1 multiplexer can be built out of two 4-bit LUTs. Each LC contains one register. The LUT used for the multiplexers can share the same LC for the registers. Therefore, a 16-bit multiplexer plus register consumes 32 LCs. The overhead for the schedule table for up to 16 schedule entries is 2 LCs, in other words 6.25%. A 4port router with 16-bit links consumes overall 136 LCs, which is way smaller than other designs that need additional buffering, dynamic arbitration, and/or pipeline stages. V. F IND ING TH E SCH EDU L E A corner stone of our NoC is the assumption of a periodic communication schedule that allows every node in the network to communicate with any other node. This schedule is ﬁxed, does not change during the operation of the network, and can thus be pre-computed off-line. Computing optimal schedules is rather time consuming, however, it needs to be done only once for every network conﬁguration. 156 We formulate the problem of ﬁnding an optimal schedule as a generic, linear optimization problem (ILP) that is solved using CPLEX. The input to our formulation is an instance of an arbitrary network topology, specifying the number of nodes and routers, and the interconnect structure. Such an instance is described by the graph G = (R, I , L), where R represents the set of routers available in the NoC, I denotes the set of IP-cores connected to the network, and L ⊆ (R ∪ I ) × (R ∪ I ) represents the set of directed network links. We assume that the network operates synchronously and transmits ﬂits between routers and/or IP-cores on every clock tick. Links may only be used to transfer one ﬂit at a time, while we assume that routers and IP-cores do not impose any constraints on the schedule, e.g., routers and IPcores are capable of processing all incoming and outgoing ﬂits at every time instant. Furthermore, ﬂits cannot be stalled at a link, which implies that a ﬂit has to be routed to another link on the next clock tick. Given a network instance G = (R, I , L) we generate a set of linear equations that model the ﬂow of ﬂits in the network over a time period [0, T ], where T represents an upper bound of the schedule length. In a ﬁrst step we generate a timeexpanded network GT = (RT , I T , LT ) which is deﬁned as follows: (1) RT = {rt |t ∈ [0, T ], r ∈ R}, (2) I T = {ct |t ∈ (u, v) ∈ L}. This approach is very similar to time-expanded networks of dynamic multi-commodity ﬂow problems [17]. [0, T ], c ∈ I }, (3) LT = {lt = (ut , v t+1 )|t ∈ [0, T − 1], l = A. Variables We then introduce ILP variables for every link in the timeexpanded network GT in order to express the ﬂow of ﬂits l,c , where lt ∈ LT and c ∈ I , represents the use of the network link l at time within the network. A binary variable (cid:2)t instant t in order to send a ﬂit to IP-core c. Note that the variable does not cover the source IP-core sending the ﬂit. More formally we introduce the following variables: V (GT ) = {(cid:2)t l,c |lt ∈ LT , c ∈ I } (1) B. Constraints Next, we deﬁne constraints on the variables in V (GT ) expressing legal ﬂows of ﬂits through the NoC. These constraints ensure three properties: (1) every ﬂit on a network link leading to a router at one time instant will be forwarded to another network link leading from the router on the next time instant, (2) every link is used to transmit at most one ﬂit at any moment in time, and (3) every IP-core sends and receives a ﬂit to/from every other IP-core on the network. In the following we will make use of the functions I n(nt ) = {lt |lt = (ut−1 , nt ) ∈ LT } and Out(nt ), which is deﬁned analogously, that return the incoming and outgoing links of a router or IP-core in the time-expanded network. The ﬁrst constraint ensures that ﬂits are never lost within the network. For every router rt ∈ Rt , t ∈ [0, T − 1] the following equation has to be fulﬁlled: (cid:2) (cid:2) − (cid:2) (cid:2)t i,c it∈I n(rt ) c∈I ot+1∈Out(rt+1 ) (cid:2) c∈I (cid:2)t+1 o,c = 0 (2) The second constraint ensures that every network link lt ∈ LT is used to transmit at most one ﬂit on every time instant t ∈ [0, T ]: (cid:2) c∈I (cid:2)t l,c = 1 (3) Figure 4. nodes. hops: 1 2 3 4 5 6 4 × 4 Torus: No. of hops to reach from one node to all other Finally it remains to ensure that ﬂits are actually sent from every IP-core on the network to every other IP-core, we thus add the following equations to the optimization problem for every IP-core c ∈ I and every destination IPcore s ∈ I , s (cid:5)= c: (4) (5) (cid:2) lt∈I n(c),t∈[0,T ] (cid:2) = |I | − 1 (cid:2)t l,c lt∈Out(c),t∈[0,T ] (cid:2)t l,s = 1 C. Solving We then ask the CPLEX optimizer to ﬁnd a schedule under the given constraints that minimizes the schedule length, which can easily be derived from the time index of the ILP variables in V (GT ) representing the usage of network links from above. In addition to minimizing the schedule length, we also assign a small weight to every use of a link to transmit a ﬂit. This minimizes the overall length of the paths within the schedule and, among others, helps to avoid useless cycles in the schedule. Note that, due to the intrinsic complexity of solving ILP problems, the size of the network, and the use of a time-expanded network to model the network behavior over time, solving the equation system from above is time consuming. However, this approach is simple to realize and very powerful. It allows to model arbitrary network topologies, to model constraints on the usage of the network and its links, and to express constraints on time instants when ﬂits are sent and/or received, et cetera. V I . ANA LY T ICA L BOUND S ON TH E P ER IOD By considering different bandwidth limits of the various network topologies, we can derive a set of analytical lower bounds on the schedule’s period P . We assume a system with n IP-cores that are connected to the network using bidirectional links, such that an IP-core can send and receive one ﬂit per clock period. During a period P each node is assumed to send a ﬂit to all the other IP-cores in the system. This means that the total number of ﬂits injected into the network during a period P is n(n − 1). Based on these simple assumptions we can derive three fundamental lower bounds on the period: 157 • IO-bound, BIO : The total amount of ﬂits communicated during a period has to be communicated across the n links connecting the IP-cores to the network. For a given number of IP-cores, n, this ﬁgure is the same for all the different topologies: n(n − 1) n = n − 1 BIO = (6) • Link capacity bound, BC ap : The total number of link-hops required to communicate the total amount of ﬂits through the network (assuming the shortest paths), divided by the total number of links in the network. • Bisection capacity bound, BB isect : The total amount of ﬂits communicated across a bisection boundary ((n/2)2 ), divided by the number of links crossing the bisection boundary. In the following we will derive the above three bounds for a range of common NoC topologies, as shown in Figure 2. These bounds, and actual minimal periods for speciﬁc schedules are shown in Table I which is introduced and discussed later in Section VII. For some of the topologies we derive analytical expressions, for others we calculate actual ﬁgures. When counting the number of links in the mesh, the torus, the bi-torus, the ring, and the bi-ring, we do not consider the links that connect IP-blocks to routers or routers to IPcores. This is because the full amount of trafﬁc is also carried across the links in the “core network”, i.e., by the links that connect pairs of routers. In this way we get a tighter bound than if the IP-router and router-IP links were also included core network is 2 · 2 · m · (m − 1) for a mesh, 2 · m2 for a in the count. Assuming n = m2 the number of links in the torus and 4 · m2 for a bi-torus. Torus: Due to its symmetry, every node in a torus sees the same distances to the n − 1 nodes to which it transmits ﬂits. With the help of Figure 4 we can determine the capacity bound BC ap,T orus for a 16-node, 4x4 torus. For simplicity we consider the upper left node that does not make use of the links that wrap around the edges. We ﬁrst compute the total number of hops made by all the ﬂits send from the node considered, to all the other n − 1 nodes. From the ﬁgure we see that in 1 hop 2 nodes are reachable, in 2 hops 3 nodes are reachable, etc. resulting in a total hop count of: 1 · 2 + 2 · 3 + 3 · 4 + 4 · 3 + 5 · 2 + 6 · 1 = 48 (7) Multiplying this ﬁgure by the number of nodes and dividing by the total number of uni-directional links (2 per node in a torus) we get: BC ap,T orus,4×4 = = 24 (8) 48 × 16 32 Assuming quadratic structures with a side-length of m, i.e. n = m2 , it is possible to derive a general expression: BC ap,T orus = m3 − m2 2 (9) Finally, assuming that m is even (resulting in equal-sized bisection partitions), we derive a general expression for the minimum period determined by the bisection capacity. It is calculated as the number of ﬂits transmitted from one partition to the other(m2 /2 · m2 /2), divided by the number of links crossing the bisection boundary in that direction (m): BB isect,T orus = (for m even) (10) m3 4 Mesh: Due to the many special cases, which have to be considered in the non-symmetric mesh topology, it is more involved to derive an analytical bound. The basic idea is to sum the minimal distances between all pairs of IP-cores in the NoC, which gives after simpliﬁcation: BC ap,M esh = m2 (m + 1) 6 (11) The bisection bound is the same as for the torus (again assuming that m is even): BB isect,M esh = m3 4 (for m even) (12) Bi-torus: For the bi-torus, the capacity bound is determined in the same way as for the torus, but as the links are bidirectional the analysis considers a center node transmitting to all other nodes. When m is odd, the situation is fully symmetrical, and for this we have the following general expression: m3 − m 8 BC ap,B i−torus = (for m odd) (13) The bi-torus has double the bisection bandwidth of a torus, and therefore the period-bound is half: BB isect,B i−torus = m3 8 (for m even) (14) Trees: In the binary tree there is one link to support the trafﬁc that crosses the bisection boundary. When the number of IP-cores increases this obviously becomes the bottleneck and the period will be limited by the (constant) bisection capacity. In the fat-tree the bisection bandwidth increases with the number of IP-cores. In the fat tree with 4 IP-cores there are 2 links crossing the bisection boundary, in a system with 8 158 IP cores there are 4 links, and in a system with 16 IP-cores there are 8 links, etc. Ring: It is possible to devise an optimal schedule for a ring, and thereby determine the minimum period, analytically. In a unidirectional ring each router either forwards a ﬂit from its neighbor router or it injects a ﬂit from its attached IP-core. As the ring is symmetric we can easily derive a static schedule, where each node performs the same communication pattern. Within the ﬁrst cycle the ﬂit is sent to the next neighbor, which can be performed in a single hop. During the next two cycles a ﬂit is sent to the node travels n − 1 hops. The resulting period P is: two hops away. This is continued until the last ﬂit, which n−1(cid:2) n(n − 1) 2 (15) PM in,Ring = i = i=1 A unidirectional ring results in the minimum hardware cost, but ﬂits that travel almost the whole ring consume a lot of those communication resources. Adding a second ring in the other direction reduces the maximum travel length of a ﬂit to the half. The period for a double ring is: PM in,B i−ring = (cid:3) n−1 2 (cid:4)(cid:2) i=1 ((cid:6) n−1 2 (cid:7) + 1)(cid:6) n−1 2 2 (cid:7) i = (16) With the above given schedule, and period of that schedule, all links are used. Therefore, the capacity bound is equal to the derived period. Furthermore, as we have the optimal period there is no incentive to show a bisectional bound of the period. V I I . EVALUAT ION R E SU LT S For all the topologies shown in Figure 2, and for a range of network sizes, Table I lists: the amount of resources (number of routers and links), the 3 analytical lower bounds on the period: BIO , BC ap and BB isect , and the smallest period, PM in , for which we have been able to ﬁnd an actual schedule using the ILP formulation. It should be noted that BB isect can only be calculated when n = m2 is even using one of the equations from Section VI. The ﬁgures for odd values of n have been calculated using the same equations, rounded up, and as a reminder that the values are not exact; they have been put in parenthesis. For easy comparison with Pmin , we have highlighted the largest of the 3 bounds. A. Discussion We see that mesh and torus topologies become very fast bisection and capacity bounded when the number of nodes increases to a medium sized 4x4 network. The practical schedules that we found also reﬂect this. With larger structures we see a considerable growth of the period. For large m the total number of hops is 4 times lower in a bi-torus than in a torus. Comparing BC ap,T orus and R E SOURC E S , P ER IOD BOUND S , AND M IN IMA L P ER IOD S FOR S TAT IC SCH EDU L ED N E TWORK S . Table I R E SOURC E S AND FR EQU ENC I E S FOR D I FF ER EN T S I Z E S O F M E SH NOC S W I TH PROC E S SOR COR E S Table II NoC Resources Period Bounds Links 8+8 24+18 48+32 80+50 8+8 18+18 32+32 50+50 72+72 16+8 36+18 64+32 100+50 144+72 196+98 256+128 324+162 2x5 2x13 2x29 2x8 2x24 4+8 9+18 16+32 25+50 2x4 2x9 2x16 2x25 Topology Mesh Torus Bi-torus Tree Fat tree Ring Bi-ring Bus IPs 2x2 3x3 4x4 5x5 2x2 3x3 4x4 5x5 6x6 2x2 3x3 4x4 5x5 6x6 7x7 8x8 9x9 4 8 16 4 8 4 9 16 25 4 9 16 25 4 9 16 25 Routers 4 9 16 25 4 9 16 25 36 4 9 16 25 36 49 64 81 2 6 14 4 8 4 9 16 25 4 9 16 25 4 9 16 25 IO 3 8 15 24 3 8 15 24 35 3 8 15 24 35 48 63 80 3 7 15 3 7 3 8 15 24 3 8 15 24 3 8 15 24 Cap. 2 6 14 25 2 9 24 50 90 1 3 8 15 27 42 64 90 16 (32) Bisect. Min. 2 5 7 10 18 34 5 11 26 52 n.a. 4 10 18 28 n.a. n.a. n.a. n.a. 2 (7) 16 (32) 54 1 (4) 8 (16) 27 (43) 64 (92) 4 16 64 2 4 3 9 6 36 120 300 3 10 36 78 12 72 240 600 6 36 120 300 3 10 36 78 12 72 240 600 BB isect,T orus one can see that for increasing values of m, the torus is always limited by the capacity bound, and never the bisection bound. For all other structures, the bisection bound is always the larger one (for large enough values of m). If links are pipelined, the network capacity increases and topologies that are not limited by IO or bisection bounds may beneﬁt from pipelining. The bidirectional torus and the fat tree provide enough link capacity to build reasonable sized networks where the schedule period is still bounded by the IO capacity. Finding an optimal solution for torus and bi-torus networks for sizes larger than 5x5 were not yet possible, due to the computational complexity. We thus plan to investigate fast optimal, or near-optimal, algorithms to derive schedules for larger problem instances. For instance, symmetric topologies such as the bi-torus permit schedules where every router 159 Size 2x2 3x3 4x4 5x5 6x6 7x7 8x8 9x9 Total 1132 2726 5166 8287 12095 16662 22006 28113 Logic cells NoC 48–105 48–112 48–145 48–146 48–150 48–160 48–160 48–160 Processor 217–222 217–223 218–222 218–226 217–223 217–224 217–225 219–224 Frequency (MHz) Processor NoC 127 371 130 327 130 280 134 264 128 254 125 223 125 230 127 227 in the NoC performs the same action. The problem of ﬁnding a good schedule thus could be reduced to ﬁnding a spanning tree that maximizes the number of ﬂits in-ﬂight at the same time. This idea could also be adapted for other topologies that are somewhat symmetric, e.g., mesh or torus, where almost all routers could take identical routing decisions. It is important to note that application-speciﬁc optimizations, as found in other real-time NoCs, can also be realized in our framework, e.g., by adding constraints to the ILP formulation or providing a tailored network topology. B. FPGA Implementation We have implemented the mesh network topology in an FPGA and used a small processor (Leros [19]) as IP core. The number of nodes is conﬁgurable and we explored networks of different size. We used a ﬂit/link width of 16 bits. We evaluated the maximum attainable clock frequency and its dependency on the size of the design. To explore the NoC limits we use a PLL to generate two clocks: a 100 MHz clock for the processor and a 400 MHz clock for the NoC. For our experiments we use a Cyclone III device from Altera. The Cyclone device belongs to the low-cost FPGA family from Altera. We left the synthesis tool Quartus select the smallest FPGA for each conﬁguration. To report the maximum frequency of the NoC we use the slow timing model at 85° C and 1200 mV core voltage. The results of the experiments of different sizes of mesh NoCs are shown in Table II. The processor node consumes about 220 LCs, as it is a processor optimized for low resource consumption. Simple 32-bit RISC pipelines in an FPGA consume about 2000 LCs. The router, as it appears on the table, occupies a small number of LCs (48-160), for all implementation sizes, as it is a simple design. A router in the middle of the network consumes, as expected, about 150 LCs. Routers at the sides are considerable smaller. The operation frequency is also high (223-371 MHz) compared to the processor frequency (125-134 MHz). It is double the frequency of the processor even for larger NoCs. Therefore, one design option is to clock the NoC at double the clock frequency of the IP-cores and reduce the link width to half. V I I I . CONC LU S ION S Hard real-time systems need an architecture with processors and communication channels where upper bounds on the worst-case execution time (WCET) can be statically derived. To provide such bounds, all resources have to be arbitrated using a static TDM schedule. This schedule has to be considered during the WCET analysis. In this paper we explored statically scheduled, timedivision-multiplexed network-on-chips for such real-time systems. We investigated different topologies and derived optimal static schedule tables by solving an integer linear programming problem. Static schedules that allow communication between all processing nodes result in a considerable pressure on the network bandwidth. For networks above 16 nodes, only bi-torus and fat trees have enough link capacity to enable a schedule period that is in the same range as the IO capacity of the IP cores. The presented NoC design and the program for the schedule generation are provided in open-source under the BSD license. The source can be found at https://github.com/t-crest/s4noc. ACKNOW L EDGM EN T S This work received funding from the FP7-ICT Project 288008 Time-predictable Multi-Core Architecture for Embedded Systems (T-CREST). "DSENT - A Tool Connecting Emerging Photonics with Electronics for Opto-Electronic Networks-on-Chip Modeling.,"With the rise of many-core chips that require substantial bandwidth from the network on chip (NoC), integrated photonic links have been investigated as a promising alternative to traditional electrical interconnects. While numerous opto-electronic NoCs have been proposed, evaluations of photonic architectures have thus-far had to use a number of simplifications, reflecting the need for a modeling tool that accurately captures the tradeoffs for the emerging technology and its impacts on the overall network. In this paper, we present DSENT, a NoC modeling tool for rapid design space exploration of electrical and opto-electrical networks. We explain our modeling framework and perform an energy-driven case study, focusing on electrical technology scaling, photonic parameters, and thermal tuning. Our results show the implications of different technology scenarios and, in particular, the need to reduce laser and thermal tuning power in a photonic network due to their non-data-dependent nature.","DSENT – A Tool Connecting Emerging Photonics with Electronics for Opto-ElectronicNetworks-on-Chip ModelingChen Sun, Chia-Hsin Owen Chen, George Kurian, Lan Wei, Jason Miller, Anant Agarwal, Li-Shiuan Peh and Vladimir StojanovicDepartment of Electrical Engineering and Computer Science Massachusetts Institute of TechnologyCambridge,  MA 02139Email: {sunchen, owenhsin, gkurian, lanwei, jasonm, agarwal, peh, vlada}@mit.eduAbstract—With the rise of many-core chips that require sub- stantial bandwidth from the network on chip (NoC), integrated photonic links have been investigated as a promising alternative to traditional electrical interconnects. While numerous opto- electronic NoCs have been proposed, evaluations of photonic architectures have thus-far had to use a number of simpliﬁca- tions, reﬂecting the need for a modeling tool that accurately captures the tradeoffs for the emerging technology and its impacts on the overall network. In this paper, we present DSENT, a NoC modeling tool for rapid design space exploration of electrical and opto-electrical networks. We explain our modeling framework and perform an energy-driven case study, focusing on electrical technology scaling, photonic parameters, and thermal tuning. Our results show the implications of different technology scenarios and, in particular, the need to reduce laser and thermal tuning power in a photonic network due to their non-data- dependent nature.Index Terms—photonics; networks on chip; powerINTRODUCTIONAs CMOS technology scales into the deep sub-100 nm regime, improvements in transistor density have resulted in greater processor parallelism as the means to improve proces- sor performance, leading to ever-higher processor core counts. The rise of the many-core era, however, comes with the challenge of designing the on-die interconnect fabric to allow for efﬁcient delivery of bits between an ever increasing number of processor cores, memories, and specialized IP blocks both on- and off-chip. Traditional approaches, such as the shared bus or global crossbars, scale poorly in either performance or cost for large numbers of network endpoints, driving the need for efﬁcient Network-on-Chip (NoC) architectures to tackle the communication requirements of future many-core machines.Recognizing the potential scaling limits of electrical interconnects, architects have recently proposed emerging nanophotonic technology as an option for both on-chip and off-chip interconnection networks [1, 2, 3, 4]. As optical links avoid the capacitive, resistive and signal integrity constraints imposed upon electronics, photonics allows for efﬁcient real- ization of physical connectivity that is costly to accomplish electrically. Photonics technology itself, however, remains immature and there remains a great deal of uncertainty in its capabilities. Whereas there has been signiﬁcant prior work on electronic NoC modeling (see Section II-C), evaluations of photonic NoC architectures have not yet evolved past the use of ﬁxed energy costs for photonic devices and interface circuitry [1, 3, 4, 5], whose values also vary from study to study. In  order  to  gauge the true potential of this emerging technology, inherent interactions between electronic/photonic components and their impact on the NoC need to be quantiﬁed.In this paper, we propose a uniﬁed framework for photonics and electronics, DSENT (Design Space Exploration of Net- works Tool), that enables rapid cross-hierarchical area and power evaluation of opto-electronic on-chip interconnects1. We design DSENT for two primary usage  modes.  When  used standalone, DSENT functions as a fast design space exploration tool capable of rapid power/area evaluation of hundreds of different network conﬁgurations, allowing for impractical or inefﬁcient networks to be quickly  identiﬁed and pruned before more detailed evaluation. When integrated with an architectural simulator [6, 7], DSENT can be used to generate trafﬁc-dependent power-traces and area estimations for the network [8].Through DSENT, our paper makes the following contribu- tions:Presents the ﬁrst tool that is able to capture the interac- tions at electronic/photonic interface and their implica- tions on a photonic NoC.Provides the ﬁrst network-level modeling framework for electrical NoC components featuring integrated timing, area, and power models that are accurate (within 20%)   in the deep sub-100 nm regime.Identiﬁes the most proﬁtable opportunities for photonic network optimization in the context of an entire opto- electronic network system. In particular, we focus on1We focus on the modeling of opto-electrical NoCs in this paper, though naturally, DSENT’s electrical models can also be applied to pure electrical NoCs as well			source and coupled into an on-chip waveguide. Each wave- length is modulated by a resonant ring modulator dropped at the receiver by a matching ring ﬁlter. Using WDM, a single waveguide can support dozens of independent data-streams on different wavelengths.Fig. 1: A typical opto-electronic NoC including electrical routers and links,  and a wavelength devision multiplexed intra-chip photonic link.the impact of network utilization, technology scaling and thermal tuning.The rest of the paper is organized as follows. Section II introduces the main building blocks of photonic NoCs and recaps existing work in photonic architectures and NoC mod- eling. We describe the DSENT framework in Section III and present its models for electrical and optical components in Sec- tions IV and V, respectively. Validation of DSENT is shown in Section VI. Section VII presents an energy-efﬁciency-driven network case-study and Section VIII concludes the paper.BACKGROUNDSilicon Photonics TechnologyWaveguides, Couplers, and Lasers: Waveguides are the primary means of routing light within the conﬁnes of a chip. Vertical grating couplers [9] allow light to be directed both into and out-of the plane of the chip and provide the means to bring light from a ﬁber onto the chip or couple light from the chip into a ﬁber. In this paper, we assume com- mercially available off-chip continuous wave lasers, though we note that integrated on-chip laser sources are also possi- ble [10, 11].Ring Resonators: The optical ring resonator is the primary component that enables on-chip wavelength division multiplexing (WDM). When coupled to a waveguide, rings perform as notch ﬁlters; wavelengths at resonance are trapped in the ring and can be potentially dropped onto another waveg- uide while wavelengths not at resonance pass by unaffected. The resonant wavelength of each ring can be controlled by adjusting the device geometry or the index of refraction. As resonances are highly sensitive to process mismatches and temperature, ring resonators require active thermal tuning [12].Ring Modulators and Detectors: Ring modulators modulate its resonant wavelength by electrically inﬂuencing the index of refraction [13]. By moving a ring’s resonance in and out of the laser wavelength, the light is modulated (on-off keyed). A photodetector, made of pure germanium or SiGe, converts optical power into electrical current, which can then be sensed by a receiver [14] and resolved to electrical ones and zeros. Photodetectors standalone are generally wideband and require ring ﬁlters for wavelength selection in WDM operation.Photonic Links: The dynamics of a wavelength- division-multiplexed (WDM) photonic architecture are shown in Figure 1. Wavelengths are provided by an external laser Prior Photonic NoC ArchitecturesMany photonics-augmented architectures have been pro- posed to address the interconnect scalability issue posed by rapidly rising core-counts, The Corona [4] architecture uses    a global 64x64 optical crossbar with shared optical buses employing multiple matching ring modulators on the same waveguide. Fireﬂy [3] and ATAC [2] also feature global cross- bars, but with multiple matching receive rings on the same waveguide in a multi-drop bus conﬁgurations. The photonic clos network [5] replaces long electrical links characteristic   of clos topologies with optical point-to-point links (one set    of matching modulator and receiver  ring  per  waveguide)  and performs all switching electrically. Phastlane [15] and Columbia [16] networks use optical switches in tile-able mesh- like topologies. While each of these prior works performs evaluations of their respective networks, we note that the analyses in these prior works all rely on ﬁxed numbers for active photonic devices and electronic components, making it difﬁcult to explore design tradeoffs and interactions between photonics and electronics.Existing NoC Modeling ToolsSeveral modeling tools have been proposed to estimate the timing, power and area of NoCs. Chien proposed a timing and area model for router components [17] that is curve-ﬁtted to only one speciﬁc process. Peh and Dally proposed a timing model for router components [18] based on logical effort that is technology independent; however, only one size of each logic gate and no wire model is considered in its analysis. These tools also only estimate timing and area, not power.Among all the tools  that  provide  power  models  for  NoCs [19, 20, 21, 22], Orion [19, 22], which provides parametrized power and area models for routers and links, is the most widely used in the community. However, Orion lacks a delay model for router components, allowing router clock frequency to be set arbitrarily without impacting energy/cycle or area. Furthermore, Orion uses a ﬁxed set of technology parameters and standard cell sizing, scaling the technology through a gate length scaling factor that does not reﬂect the effects of other technology parameters. For link components, Orion supports only limited delay-optimal repeated links. Orion does not model any optical components.PhoenixSim [23] is the result of recent work in photonics modeling, improving the architectural visibility concerning the trade-offs of photonic networks. PhoenixSim provides param- eterized models for photonic devices. However, PhoenixSim lacks electrical models, relying instead on Orion for all electrical routers and links. As a result, PhoenixSim  uses  ﬁxed numbers for energy estimations for electrical interfacecircuitry, such as modulator drivers, receivers, and thermal tun- ing, losing many of the interesting dynamics when transistor technology, data-rate, and tuning scenarios vary. PhoenixSim in particular does not capture trade-offs among photonic device and driver/receiver speciﬁcations that result in an area or power optimal conﬁguration.To address shortcomings of these existing tools, we propose ring tuning. Ptotal = Pelectrical + Poptical	(1a)Pelectrical = Prouter + Plink+Pinterface + Ptuning	(1b)Poptical = Plaser	(1c)DSENT to provide a uniﬁed electrical and optical framework that can be used to model system-scale aggressive electrical and opto-electronic NoCs in future technology nodes.DSENT FRAMEWORKIn our development of the generalized DSENT modeling framework, we observe the constant trade-offs between the amount of required user input and overall modeling accuracy. All-encompassing technology parameter sets can enable pre- cise models, at the cost of becoming too cumbersome for predictive technologies where only basic technology param- eters are available. Overly simplistic input requirements, on the other hand, leaves signiﬁcant room for inaccuracies. In light of this, we design a framework that allows for a high degree of modeling ﬂexibility, using circuit- and logic-level techniques to simplify the set of input speciﬁcations without sacriﬁcing modeling accuracy. In this section, we introduce the generalized DSENT framework and the key features of  our approach.Framework OverviewDSENT is written in C++ and utilizes the object-oriented approach and inheritance for hierarchical modeling. The DSENT framework, shown in Figure 2, can be separated into three distinct parts: user-deﬁned models, support models, and tools. To ease development of user-deﬁned models, much of the inherent modeling complexity is off-loaded onto support models and tools. As such, most user-deﬁned models involve just simple instantiation of support models, relying on tools to perform analysis and optimization. Like an actual electrical chip design, DSENT models can leverage instancing and multiplicity to reduce the amount of repetitive work and speed up model evaluation, though we leave open the option to allow, for example, all one thousand tiles of a thousand core system to be evaluated and optimized individually. Overall, we strive to keep the run-time of a DSENT evaluation to a few seconds, though this will vary based upon model size and complexity.Power, Energy, and Area BreakdownsThe typical power breakdown of an opto-electronic NoC can be formulated as Equation 1. The optical power is the wall-plug laser power (lost through non-ideal laser efﬁciency and optical device losses). The electrical power consists of   the power consumed by electrical routers and links as well     as electric-optical interface circuits (drivers and receivers) and Power consumption can be split into data-dependent and non-data-dependent parts. Non-data-dependent power is deﬁned as power consumed regardless of utilization or idle times, such as leakage and un-gated clock power. Data-dependent power  is utilization-dependent and can be calculated given an energy per each event and frequency of the event. Crossbar traversal, buffer read and buffer write are examples of high-level events for a router. Power consumption  of  a  component  can thus be written  as  P  =  PNDD +  Ei · fi,  where  PNDD  is  the total non-data-dependent power of the module and Ei, fi are the energy cost of an event and the frequency of an event, respectively.Area estimates can be similarly broken down into their respective electrical (logic, wires, etc.) and optical (rings, waveguides, couplers, etc.) components. The total area is the sum of these components, with a further distinction made between active silicon area, per-layer wiring area, and photonic device area (if a separate photonic plane is used).We note that while the area and non-data-dependent power can be estimated statically, the calculation for data-dependent power requires knowledge of the behavior and activities of the system. An architectural simulator can be used to supply the event counts at the network- or router-level, such as router or link traversals. Switching events at the gate- and transistor- level, however, are too low-level to be kept track of by these means, motivating a method to estimate transition probabilities (Section IV-D).DSENT MODELS AND TOOLS FOR ELECTRONICSAs the usage of standard cells is practically universal in modern digital design ﬂows, detailed timing, leakage, and energy/op characterization at the standard-cell level can enable a high degree of modeling accuracy. Thus, given a set of technology parameters, DSENT constructs a standard cell library and uses this library to build models for the electrical network components, such as routers and repeated links.Transistor ModelsWe strive to rely on only a minimal set of technology pa- rameters (a sample of which is shown in Table I) that captures the major characteristics of deep sub-100 nm technologies without diving into transistor modeling. Both  interconnect and transistor properties are paramount at these nodes, as interconnect parasitics play an ever larger role due to poor scaling trends [25]. These parameters can be obtained and/or calibrated using ITRS roadmap projection tables for predictive technologies or characterized from SPICE models and process design kits when available.Fig. 2: The DSENT framework with examples of network-related user-deﬁned models. TABLE I: DSENT electrical parametersShown values are for NMOS transistors and the global wiring layerC. Delay Calculation and Timing OptimizationTo allow models to scale with transistor performance and clock frequency targets, we apply a ﬁrst-order delay estimation and timing optimization method. Using timing information in the standard cell models, chains of logic are mapped to stages of resistance-capacitance (RC) trees, shown in Figure 4a. An Elmore delay estimate [30, 31] between two points i and k can be formed by summing the product of each resistance and the total downstream capacitance it sees:Fig. 3: Standard cell model generation and characterization. In this example,	k	ka NAND2 standard cell is generated.Currently, DSENT supports the 45 nm, 32 nm, 22 nm, 14 nm and 11 nm technology nodes. Technology parameters for the 45 nm node are extracted using SPICE models. Models for the 32 nm node and below are projected [26] using the virtual- source transport of [27] and the parasitic capacitance model   of [28]. A switch from planar (bulk/SOI) to tri-gate transistors is made for the 14 nm and 11 nm nodes.Standard CellsThe standard-cell models (Figure 3) are portable across technologies, and the library is  constructed  at  run-time  based on design heuristics extrapolated from open-source libraries [29] and calibrated with commercial standard cells.We begin by picking a global standard cell height, H =  Hex + α · (1 + β) · Wmin, where β represents the P-to-N ratio, Wmin is the minimum transistor width, and Hex is the extra height needed to ﬁt in supply rails and diffusion separation. α is heuristically picked such that large (high driving strength) standard cells do not require an excessive number of transistor folds and small (low driving strength) cells do not waste too much active silicon area. For each standard cell, given a drive strength and function, we size transistors to match  pull-up  and pull-down strengths, folding if necessary. As lithography limitations at deep sub-100 nm force a ﬁxed gate orientation and periodicity, the width of the cell is determined by the max of the number of NMOS or PMOS transistors multiplied by the contacted gate pitch, with an extra gate pitch added for separation between cells. td,i−k = ln(2) · Σ Σ Rn · Cm	(2)n=i m=nNote that any resistances or capacitances due to wiring parasitics is automatically factored along the way. If a register- to-register delay constraint, such as one imposed by the clock period, is not satisﬁed, timing optimization is required  to  meet the delay target. To this end, we employ a greedy incremental timing optimization algorithm. We start with the identiﬁcation of a critical path. Next, we ﬁnd a node to optimize to improve the delay on the path, namely, a small gate driving a large output load. Finally, we size up that node and repeat these three steps until the delay constraint is met   or if we realize that it is not possible and give up. Our method optimizes for minimum energy given  a  delay  requirement, as opposed to logical-effort based approaches employed by existing models [18, 32, 33], which optimize for minimum delay, oblivious to energy. Though lacking the rigorousness of timing optimization algorithms used by commercial hardware synthesis tools, our approach runs fast and performs well given its simplicity.Expected TransitionsThe primary source of data-dependent energy consumption in CMOS devices comes from  the  charging  and  discharg- ing of transistor gate and wiring capacitances. For every transition of a node with capacitance C to voltage V , we dissipate an energy of E = 1 C · V 2. To calculate data- dependent power usage, we sum the energy dissipation of all such transitions multiplied by their frequency of occurrence, PDD =   Ci · V 2 · fi. Node capacitance Ci can be calculated for each model and, for digital logic, Vi is the supply voltage. The frequency of occurrence, fi, however, is much more		Mapping standard cells to RC delaysTiming optimizationFig. 4: DSENT’s delay calculation and timing optimization framework. In (a), delay is estimated by mapping standard cells to sets of input capacitances and output drive resistances. Using these delay calculations, timing optimization in (b) may begin by incrementally sizing up cells until all delay constraints are     met.difﬁcult to estimate accurately as it depends on the pattern     of bits ﬂowing through the logic. As event counts and signal information at the logic gate level are generally not available except through structural netlist simulation, DSENT uses a simpliﬁed expected transition probability model [34] to esti- mate the average frequency of switching events. Probabilities derived using this model are also used with state-dependent leakage in the standard cells to form more accurate leakage calculations.SummaryDSENT models a technology-portable set of standard cells from which larger electrical components such as routers and networks are constructed. Given a delay or frequency con- straint, DSENT applies (1) timing optimization to size gates for energy-optimality and (2) expected transition propagation to accurately gauge the power consumption. These features allow DSENT to outpace Orion in estimating electrical com- ponents and in projecting trends for future technology nodes.DSENT MODELS AND TOOLS FOR PHOTONICSA complete on-chip photonic network consists of not only the photonic devices but also the electrical interface circuits and the tuning components, which are a signiﬁcant fraction of the link energy cost. In this section we present how we model these components in DSENT.Photonic Device ModelsSimilar to how it builds the electrical  network  model  using standard cells, DSENT models a library of photonic devices necessary to build integrated photonic links. The library includes models for lasers, couplers, waveguides, ring resonators, modulators and detectors. The total laser power required at the laser source is the sum of the power needed   by each photodetector after applying optical path losses:Plaser  = Σ Psense,i · 10lossi /10	(3) where Psense,i is the laser power required at photodetector i and lossi is the loss to that photodetector, given in dB. Note that additional link signal integrity penalties (such as near- channel crosstalk) are lumped into lossi as well.Interface CircuitryThe main interface circuits responsible for electrical-to- optical and optical-to-electrical conversion are the modulator drivers and receivers. The properties of these circuits affect not only their power consumption, but also the performance of the optical devices they control and hence the laser power [12].Modulator Driver: We adopt the device models of [12] for a carrier-depletion modulator. We ﬁrst ﬁnd the amount of charge ΔQ that must be depleted to reach a target extinction ratio, insertion loss, and data-rate. Using equations for a reverse-biased junction, we map this charge to a required reverse-biased drive voltage (VRB) and calculate the effective capacitance using charge and drive voltage Ceff = ΔQ/VRB. Based on the data-rate, we size a chain of buffers to drive   Ceff . The overall energy cost for a modulator driver can be expressed as:1Edriver = γ ΔQ · max(VDD, VRB)+ Ebuf (Ceff ,f)	(4)where γ is the efﬁciency of generating a supply voltage of  VRB and Ebuf (Ceff ,f) is the energy consumed by the chain of buffers that are sized to drive Ceff at a data-rate f .Receiver: We support both the TIA and integrating receiver topologies of [12]. For brevity, we focus the following discussion on the integrating receiver, which consists of a photodetector connected across the input terminals of a current sense-ampliﬁer. Electrical power and area footprints of the sense-ampliﬁer is calculated based on sense-ampliﬁer sizing heuristics and scaled with  technology,  allowing calculation of switching power. To arrive at an expression for receiver sensitivity (Psense), we begin with an abbreviated expression for the required voltage buildup necessary at the receiver senseamp’s input terminal:Vd = vs + vos + vm + Φ(BER) · .Σ σ2 (5) process mismatches have been compensated for during post- processing.D. Optical Link Optimizationwhich is the sum of the sense-amp minimum latching input swing (vs), the sense-amp offset mismatch (vos), a voltage margin (vm), and all Gaussian noise sources multiplied by the number of standard deviations corresponding to the receiver bit error rate. The required input can then be mapped to a required laser power requirement, Psense at the photodetector: Equations 4 and 6 suggest that both the modulator driver’s energy cost and the laser power required at the photodetector depend on the speciﬁcation of extinction ratio and insertion loss of the modulator on the link. This speciﬁcation can be used to tradeoff power consumption of the modulator driver circuit with that of the laser. This is an optimization degree1	ER 2 · f of freedom that DSENT takes advantage of, looping throughPsense =pd · ER − 1 · Vd · Cin · 1 − 2 · f · t (6) different combinations to ﬁnd one that results in the lowestwhere Rpd is the photodetector responsivity (in terms of overall power consumption.amps/watt), ER is the extinction ratio provided by the modula- tor, Cin is the total parasitic capacitance present at the receiver input node, f is the data rate of the receiver, and tj is the clock uncertainty. The factor of 2 stems from the assumption that the photodetector current is given only half the clock period to integrate; the sense-amp spends the other half in the precharge state.Serializer and Deserializer: DSENT provides models for a standard-cell-based serializer and deserializer (SerDes) blocks, following a mux/de-mux-tree topology [35]. These blocks provide the ﬂexibility to run links and cores at different data-rates, allowing for exploration of optimal data-rates for both electrical and optical links.C. Ring Tuning ModelsAn integrated WDM link relies upon ring resonators to perform channel selection. Sensitivity of ring resonances to ring dimensions and the index of refraction leaves them particularly vulnerable to process- and temperature-induced resonance mismatches [36, 37, 38], requiring active closed- loop tuning methods that add to system-wide power con- sumption [5]. In DSENT, we provide four models for four alternative ring tuning approaches [12]: full-thermal tuning, bit-reshufﬂed tuning, electrically-assisted tuning, and athermal tuning.Full-thermal tuning is the conventional method of heating using resistive heaters to align their resonances to the desired wavelengths. Ring heating power is considered non-data- dependent, as thermal tune-in and tune-out times are too slow to be performed on a per-ﬂit or per-packet basis  and thus must remain always-on. Bit-reshufﬂers provide freedom in the bit-positions that each ring is responsible for, allowing rings  to tune to its closest wavelength instead of a ﬁxed absolute wavelength. This reduces ring heating power at the cost of additional multiplexing logic. Electrically-assisted tuning uses the resonance detuning principle of carrier-depletion modu- lators to shift ring resonances. Electrically-tuned rings do not consume non-data-dependent ring heating power, but is limited in tuning range and requires bit-reshufﬂers to make an impact. Note that tuning distances too large to be tuned electrically can still be bridged using heaters at the cost of non-data- dependent heating power. Athermal tuning represents an ideal scenario in which rings are not sensitive to temperature and all E. SummaryDSENT provides models not only for optical devices but also for the electrical backend circuitry including modulator driver, receiver and ring tuning circuits. These models enable link optimization and reveal tradeoffs between optical and electrical components that previous tools and analysis could not accomplish using ﬁxed numbers.MODEL VALIDATIONWe validate DSENT results against SPICE simulations for  a few electrical and optical models. For the receiver and modulator models, we compare against a few early prototypes available in literature (fabricated at different technology nodes) to show that our results are numerically within the right range. We also compare our router models with a post-place-and- route SPICE simulation of a textbook virtual channel router and with the estimates produced by Orion2.0 [22] at the 45 nm SOI technology node. To be fair, we also report the results ob- tained from a modiﬁed Orion2.0 where we replaced Orion2.0’s original scaling factors with characterized parameters for the 45 nm SOI node and calibrated its standard cells with those used to calibrate DSENT. Overall, the DSENT results for electrical models are accurate (within 20%) compared to the SPICE simulation results. We note that the main source of inaccurate Orion2.0 results is from the inaccurate technology parameters, scaling factors, and standard cell sizing. The re- calibrated Orion2.0 reports estimations at the same order of the SPICE results. The remaining discrepancy is partly due to insufﬁcient modeling detail in its circuit models. For example, pipeline registers on the datapath and the multiplexers neces- sary for register-based buffers are not completely modeled by Orion2.0.EXAMPLE PHOTONIC NETWORK EVALUATION Though photonic interconnects offer potential for improvednetwork energy-efﬁciency, they are not without their draw-backs. In this section, we use DSENT to perform an energy- driven photonic network evaluation. We choose a 256-tile version of the 3-stage photonic clos network proposed by [5] as the network for these studies. Like [5], the core-to-ingress and egress-to-core links are electrical, whereas the ingress- to-middle and middle-to-egress links are photonic. The net- work conﬁguration  parameters are shown in Table  III.  WhileTABLE  II: DSENT validation points.TABLE III: Network ConﬁgurationTABLE IV: Default Technology ParametersDSENT includes a broader selection of network models, we choose this topology because there is an electrical network that is  logically  equivalent  (an  electrical  clos)  and  carries a reasonable balance of photonic and electrical components. To obtain network-level event counts with which to animate DSENT’s physical models, we implement the clos network in Garnet [6] as part of the GEM5 [40] architecture simulator. Though the GEM5 simulator is primarily used to benchmark real applications, we assume a uniform random trafﬁc pattern to capture network energy at speciﬁc loads. Given network event counts, DSENT takes a few seconds to generate an estimation.In the following studies, we investigate the impact of different circuit and technology assumptions using energy cost per bit delivered by the network as our evaluation metric. Unless otherwise stated, the default parameters set in Table IV are used. The parameters we sweep are organized by section in Table V. TABLE V: Sweep Parameters Organized by SectionScaling Electrical Technology and Utilization TradeoffWe ﬁrst compare the photonic clos network with an elec- trical equivalent, where all photonic links are replaced with electrical links of equal latency and throughput (128 wires, each at 2 GHz). We perform this comparison at the 45 nm SOI and 11 nm Tri-Gate technology nodes, representing present and future electrical technology scenarios, respectively. Energy per bit is plotted as a function of achieved network throughput (utilization) and a breakdown of the energy consumption at three speciﬁc throughputs is shown in Figure 5.Note that in all conﬁgurations, the energy per bit rises sharply at low network utilizations, as non-data-dependent (NDD) power consumption (leakage, un-gated clocks, etc.) is amortized across fewer sent bits. This trend is more prominent in the photonic clos as opposed to the electrical clos due to       a signiﬁcantly higher NDD power stemming from the need    to perform ring thermal tuning and to power the laser. As a result, the electrical clos becomes energy-optimal at low uti- lizations (Figure 5b). The photonic clos presents smaller data- dependent (DD) switching costs, however, and thus performs more efﬁciently at high utilization (Figure 5d).Comparing 45 nm and 11 nm, it is apparent that both pho- tonic and electrical clos networks beneﬁt signiﬁcantly from electrical scaling, as routers and logic become cheaper. Though wiring capacitance scales slowly with technology, link energies still scale due to a smaller supply voltage at 11 nm (0.6 V). Laser and thermal tuning cost, however,  scale marginally, if  at all, allowing the electrical clos implementation to beneﬁt more. In the 11 nm scenario, the electrical clos is more efﬁcient up to roughly half network of the saturation throughput. As networks are provisioned to not operate at high throughputs where contention delays are signiﬁcant, energy efﬁciency at lower utilizations is critical.Photonics Parameter ScalingFor photonics to remain competitive with electrical alterna- tives at the 11 nm node and beyond, photonic links must sim-54.543.532.521.510.5 54.543.532.521.510.5 54.543.532.521.510.5 54.543.532.521.510.500	5	10	15	20	25	30	35Achieved Throughput [Tb/s] 0E45 P45 E11 P11Configuration 0E45 P45 E11 P11Configuration 0E45 P45 E11 P11ConfigurationEnergy/bit vs throughput	(b) 4.5 Tb/s (Low Throughput) (c) 16.5 Tb/s (Med Throughput) (d) 33 Tb/s (Max Throughput)Fig. 5: Comparison of network energy per bit vs network throughput (a) and the energy per bit breakdown at various throughputs (b–d) for the electrical clos (EClos) and photonic clos (PClos) at both the 45nm and 11nm technology nodes. Utilization is plotted up to the point where the network saturates (deﬁned          as when the latency reaches 3× the zero-load latency.54.543.532.521.5 54.543.532.521.51	10.5	0.500	5	10	15	20	25	30	35Achieved Throughput [Tb/s] 00.0	0.5	1.0	1.5	2.0	2.5Waveguide Loss [dB/cm]Sensitivity to waveguide loss. Energy per bit vs throughput (left) and energy per bit breakdown at 16 Tb/s throughput (right)54.543.532.521.5 54.543.532.521.51	10.5	0.500	5	10	15	20	25	30	35Achieved Throughput [Tb/s] 0400       200       100	50	25	10Heating Efficiencies [K/mW]Sensitivity to heating efﬁciency. Energy per bit vs throughput (left) and energy per bit breakdown at 16 Tb/s throughput (right)Fig. 6: Sensitivity of the photonic clos network to a few select technology parameters. All plots assume the 11 nm electrical technology model.ilarly scale. The non-data-dependent laser and tuning power  as particularly problematic, as they are consumed even when links are used sporadically.In Figure 6, we evaluate the sensitivity of  the  photonic clos to waveguide loss and ring heating efﬁciencies, which affect laser and tuning costs. We see that our initial loss assumption of 1 dB/cm brings the photonic clos quite close   to the ideal (0 dB/cm) and the network could tolerate up to around 1.5 dB/cm before laser power grows out of proportion. Ring tuning power will also fall with better heating efﬁciency. However, it is not clear whether a 400 K/mW efﬁciency is physically realizable and it is necessary to consider potential alternatives.Thermal Tuning and Data-RatePer wavelength data-rate of an optical link is a particularly interesting degree of freedom that network designers have con- trol over. Given a ﬁxed bandwidth that the link is responsible for, an increase in data-rate per wavelength means a decrease in the number of WDM wavelengths required to support the throughput. This affects the number of ring resonators and, as such, can impact the tuning power.Under the more conservative full-thermal (no bit- reshufﬂing) tuning scenario (Figure 7a), the energy spent54.543.53 54.543.53 54.543.532.5 2.5 2.52	2	21.5 1.5 1.51	1	10.5 0.5 0.502	4	8	16	32Data Rate per  [Gb/s]Full-Thermal Tuning (conservative) 02	4	8	16	32Data Rate per  [Gb/s]Bit Reshufﬂed Tuning (default) 02	4	8	16	32Data Rate per  [Gb/s]Electrically-Assisted Tuning (optimistic)Fig. 7: A comparison of three thermal-tuning strategies discussed in Section V-C. Link data-rate is used as a degree of freedom to balance tuning power with  other sources of power consumption. Since the throughput of each link is 128 bits/core-cycle at a 2 GHz core clock, a data-rate of 2, 4, 8, 16, 32 Gb/s per wavelength (λ) implies 128, 64, 32, 16, 8 wavelengths per link, respectively. All energy breakdowns are shown for half of saturation throughput (16.5 Tb/s.on ring heating is dominant and will scale  proportionally  with the number of WDM channels (and thus inversely with per wavelength data-rate). Modulator and receiver energies, however, grow with data-rate as a result of more aggressive circuits. Laser energy cost per bit grows with data-rates due   to a relaxation of modulator insertion loss/extinction ratios as well as clock uncertainty becoming a larger fraction of the receiver evaluation time. Routers and electrical links remain the same, though a small  fraction  of  energy  is  consumed for serialization/deserialization (SerDes) at the optical link interface. These trends result in an optimal data-rate between 8-16 Gb/s, where ring tuning power is balanced with other sources of energy consumption, given the full-thermal tuning scenario.This trend is no longer true once bit-reshufﬂing (the de- fault scenario we  assumed  for  Sections  VII-A  and  VII-B) is considered, shown in Figure 7b. Following the discussion  in V-C, a bit-reshufﬂer gives rings freedom in the channels they are allowed to tune to. At higher data-rates, there are fewer WDM channels and hence rings that require tuning. However, the channel-to-channel separation (in wavelength) is also greater. Given the presence of random process variations, sparser channels means each ring requires, on average, more heating in order to align its resonance to a channel. These   two effects cancel each other out. Since the bit-reshufﬂer logic itself consumes very little power at the 11 nm node, ring tuning costs are small and remain relatively ﬂat with data-rate.If electrical-assistance is used (Figure 7c), tuning power favors high WDM channel counts (low data-rates). This is a consequence of the limited resonance shift range that carrier- depletion-based electrical tuners can achieve. At high WDM channel counts where channel spacing is dense, rings can align themselves to a channel by electrically biasing the depletion- based tuner without a need to power up expensive heaters.   By contrast, when channels are sparse, ring resonances will often have to be moved a distance too far for the depletion tuner to cover and costly heaters must be used to bridge the distance. As such, the lowest data-rate, 2 Gb/s per wavelength, is optimal under this scenario. A well-designed electrically- assisted tuning system could completely eliminate non-data- dependent tuning power. Hence, it is a promising alternative  to aggressive optimization of ring heating efﬁciencies.CONCLUSIONIntegrated photonic interconnects is an attractive intercon- nect technology for  future  manycore  architectures.  Though it promises signiﬁcant advantages over electrical technology, evaluation of photonics in existing proposals have relied upon signiﬁcant simpliﬁcations. To bring additional insight into the dynamic behavior of these active components, we developed   a new tool – DSENT – to capture the interactions between photonics and electronics. By introducing standard-cell-based electrical models and interface circuit models, we complete the connection between photonic devices and the rest of the opto- electrical network. Using our tool, we show that the energy- efﬁciency of a photonic NoC is poor  at  lower  utilizations due to non-data-dependent laser and tuning power. These two components do not scale with electrical process technology and, in the case of thermal tuning, limited in photonics scaling potential. Using DSENT’s tuning models, we show that an electrically-assisted tuning scheme can eliminate non-data- dependent ring heating power for an NoC, signiﬁcantly low- ering the overhead of photonics and improve network energy efﬁciency. We will be releasing DSENT open-source [42].ACKNOWLEDGMENTThe authors would like to thank the Integrated Photonics teams at both University of Colorado, Boulder and MIT. This work was supported in part by DARPA, NSF, FCRP, MARCO IFC, SMART LEES, Trusted Foundry, Intel, APIC, MIT CICS, and NSERC."A Mixed Verification Strategy Tailored for Networks on Chip.,"This paper targets the development of a verification methodology for Networks on Chip. We advocate the use of formal methods to guarantee an unambiguous expression of the specifications. A previous theorem proving based solution enables the verification of high-level properties for NoC communication algorithms, it deliberately addresses abstract NoC descriptions and ignores implementation details. We suggest here a complementary approach, oriented toward Assertion-Based Verification, that focuses on the verification of RT level implementations, also applicable to the on-line checking of robustness properties.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip A Mixed Veriﬁcation Strategy Tailored for Networks on Chip Georgios TSILIGIANNIS† , Laurence PIERRE TIMA Laboratory (CNRS-GrenobleINP-UJF) 46 Av. F ´elix Viallet - 38031 Grenoble cedex - France Email: Georgios.Tsiligiannis@lirmm.fr, Laurence.Pierre@imag.fr † Was with TIMA when this work was done Abstract—This paper targets the development of a veriﬁcation methodology for Networks on Chip. We advocate the use of formal methods to guarantee an unambiguous expression of the speciﬁcations. A previous theorem proving based solution enables the veriﬁcation of high-level properties for NoC communication algorithms; it deliberately addresses abstract NoC descriptions and ignores implementation details. We suggest here a complementary approach, oriented toward Assertion-Based Veriﬁcation, that focuses on the veriﬁcation of RT level implementations, also applicable to the on-line checking of robustness properties. I . IN TRODUC T ION With the advance of Systems on Chip technology, the need for more efﬁcient communication inside a SoC is inevitable. Networks on Chip (NoC) are a promising solution for the interconnection of IPs on SoC [1]. Although there has been a lot of research concerning NoCs, there is no golden reference for their design and even more, each NoC is designed and implemented according to the SoCs speciﬁcations. As a result having a veriﬁcation method as a guide in order to ensure NoC correctness is more than necessary. The solution described here addresses that issue, and also targets the possibility to perform on-line robustness veriﬁcations. We have originally developed a theorem proving based method [2]: its underlying model called GeNoC enables a generic characterization of Networks on Chip, together with correctness theorems and their associated proof obligations. A theorem prover is used to discharge these proof obligations when the generic model is instantiated for a given NoC. This solution targets the veriﬁcation of high-level (algorithmic) properties about the complete network. Hence the NoC description is made at a high level of abstraction, ignoring several implementation details. The purpose of the work presented here is to develop a complementary method using AssertionBased Veriﬁcation (ABV) [3], in order to offer a complete scheme for the veriﬁcation of NoC. This scheme will provide the designer with the means to verify the correct functionality of the NoC from the design phase up to the run-time phase. We propose a comprehensive and abstract deﬁnition of properties NoC implementations should satisfy, and we illustrate their possible instantiations with corresponding PSL [4] assertions for the VHDL descriptions of two state-of-the-art NoCs, Nostrum [5] and Hermes [6]. Those assertions can be used during simulation, to check design correctness, and can also be synthesized into hardware monitors [7], [8] that enable to check on-line whether the properties still hold for instance in the presence of transient faults. Few attempts have been done towards the formal veriﬁcation of Networks on Chip. The abstraction of NoC characteristics is made in [9], in order to create Models of Computation (MoC), with which properties can be veriﬁed at the modeling level. The Nostrum case study is used to verify through its MoC that guaranteed bandwidth (GB) and best effort (BE) trafﬁcs are achieved. A model checking approach is described in [10]; the FAUST asynchronous network is used for experimental purposes. Veriﬁcation is performed with the use of a state model of the network described in CHP which is in turn translated into LOTOS. The LOTOS model of the input controller is checked using the CADP model checker so as to verify properties like data integrity and protocol correctness. In [11] a part of a wormhole XY-routing NoC router is validated for a set of properties concerning mutual exclusion, starvation, deadlock and trafﬁc congestions. Veriﬁcation is performed with the State Graph Manipulator tool, using a speciﬁcation of the router in terms of extended timed automata. An interesting study has been performed in [12] where the correctness of a Hierarchical Ring network is veriﬁed through the use of ABV. The assertions are translated into synthesizable monitors and participate in the debugging process of the network through the injections of a special ﬂit containing debugging information. Close to this latter approach, the work presented here also avoids the state explosion problem inherent to model checking. However we do not address the design of an ABV-oriented debugging infrastructure, but the deﬁnition of a general assertionbased speciﬁcation framework. I I . I L LU S TRAT IV E CA S E S TUD I E S A. Nostrum Nostrum [5] is a NoC implemented in a 2D-mesh topology with a packet switching ﬂow control, and hot-potato routing policy (deﬂective routing). Each router has ﬁve input and output ports, North, South, West, East, and Local (connection of an IP). Each router implements the same routing algorithm, driven also by the QoS of the design. Nostrum achieves a best effort QoS with the use of latency and throughput fairness mechanisms. Therefore a path is assigned to a packet based on two types of priorities: the age of the packet and the load of 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.26 161 each neighbor router. A packet that enters the router receives a priority, based on the number of hops. According to this priority, each packet selects an output port to which it will be driven. Each output port has also a priority based on the load of the corresponding neighbor switch (the output port with the smallest load is given the highest priority). For Nostrum to achieve an overall minimum network load, another priority mechanism is applied: an IP is allowed to inject a packet in the network only if at least one input port is not occupied with a packet. B. Hermes Hermes [6] also has a 2D-mesh topology, with a conﬁgurable routing algorithm (here we consider the nondeterministic minimal Negative First routing) and a wormhole ﬂow control. The Hermes router has ﬁve input and ﬁve output ports. The Negative First routing is based on an arbitration mechanism in which the ﬁrst ﬂit of the packet (header ﬂit) entering the router requests for arbitration. It will ﬁrst try to follow a path of negative direction (south and west) and then will try to follow a path in a positive direction. According to the availability of the requested output port the header ﬂit is either transmitted or waits in the buffer until the port is freed. Once the header ﬂit is transmitted, the body ﬂits of the packet follow until the complete transmission. Since the ﬂow control is wormhole, the transmission of a packet cannot be interrupted, meaning that once the header ﬂit allocates an output port, this port will remain allocated to the packet ﬂow until the complete transmission of the packet. I I I . H IGH -L EV E L V ER I FICAT ION S In the original theorem-proving based method, a metamodel represents the transmission of messages from their source to their destination, on a generic communication architecture, with an arbitrary network characterization (topology and node interfaces), routing algorithm and switching technique [2]. The model is composed of a collection of functions together with their characteristic constraints. These functions represent the main constituents of the network meta-model: interfaces, controls of network access, routing algorithms, and scheduling policies. The main function F of this model is recursive. Each recursive call represents one step of execution, where messages progress by at most one hop, see Fig. 1. Correctness theorems are associated with this function. They state that for all topology T , interfaces I , routing algorithm R, and scheduling policy S that satisfy constraints expressed in the model (proof obligations), the representative function F fulﬁlls correctness properties: • every message arrived at some node n was actually issued at some source node s and originally addressed to node n, and it reaches its destination without modiﬁcation of its content, • no message is lost (e.g., by the transmission protocol). The proof obligations are discharged when the model is instantiated for a given NoC, thus guaranteeing the satisfaction of the properties above. This method deﬁnitely addresses Fig. 1. Modeling for High-level Veriﬁcations algorithmic features and ignores implementation details at the signal level. For instance, it takes into account the routing algorithm but does not focus on its actual implementation. Similarly, at the data link layer, an abstract view of the transmission protocol is used. IV. LOW-L EV E L V ER I FICAT ION S - MOD E L To complement with ABV for the veriﬁcation of implementation features, we propose a classiﬁed set of assertions for the detection of typical errors in NoCs. Those assertions are given in plain English format and some are exempliﬁed here by their PSL formalization [4] for Nostrum or Hermes. First of all, we consider a general model of the router, that aims at representing fundamental architecture characteristics common to a variety of routers. Fig. 2 shows this model consisting of four basic elements: inputs, outputs (their number is non signiﬁcant), control logic and switch. Fig. 2. General Router Model The input and output units abstract all the components related to the router I/O such as ports, buffers, virtual circuit allocators, credit control logic, communication control logic as well as other control logic that might exist at the I/O of the router. The control logic unit abstracts the functional core of the router consisting of the routing functions, scheduling, arbitration and control ﬂow. Finally the switch element represents the multiplexing components of a router which are responsible for the forwarding of the packets to the output ports. 162 The packet ﬂow in the router is depicted according to this model. Packets A and B are initially introduced to the input ports. The control logic unit extracts the required information from these packets and runs the routing algorithm. It then forwards the control signals (ctrl) to the switch unit which establishes the correspondence between the input units and the output units. Finally the packets are released by the output units to the neighbor routers. Using our two case studies, let us assess the relevance of this model. Nostrum. The Nostrum router (Fig. 3) is very similar to this structure. The input unit of the model corresponds to the sort4inputOrder and the sort4outputOrder components. The ctrl box and the loadAverager components correspond to the control logic of the model. Multiplexing in Nostrum differs from the generic router model in the sense that each output unit has its own multiplexer (switch in the model) controlled by the ctrl box. The output units of the model correspond to the packetJustiﬁer component which modiﬁes the packet so as to respect the packet protocol (change of relative address of the packet, modiﬁcation of the empty bit, etc). In the assertions that will be presented, some control signals will be used: empty temp is a vector denoting that the packet at the ith input port is valid i.e., a new packet is present, if the value of the ith bit is ’1’. N select is a vector controlling the North output port (and similarly for the 4 other ports): if the ith bit of this vector is set to ’1’, the North output will forward a packet from the ith input port. N empty is a signal denoting that the North output port has actually something to transmit (and similarly for the 4 other ports). Fig. 3. Nostrum Router Top Level Architecure Hermes. The relation to the router model is straightforward (Fig. 4). Since the ﬂow control is wormhole, the input units of the model correspond to the buffers of the input ports. The control unit of the Hermes router is the switch control component in which the arbitration of the requesting for transmission takes place. Also the control unit is responsible for the implementation of the Negative First routing algorithm. Switching is performed by the mux component, which is controlled by the switch control block. At the arrival of the header ﬂit, a request signal for arbitration h is transmitted. If the requested resources are available, the input unit receives 163 Fig. 4. Hermes Router Top Level Architecture an acknowledgment ack h, and begins the transmission of the rest of the packet. Before the transmission of a ﬂit, the sender sends a transmission request signal tx to the receiver. The receiver receives a request signal for reception, rx. If the receiver is available for reception (the buffer is not full) the ﬂit is transmitted and an acknowledgment ack rx is transmitted by the receiver. The mux in and mux out vectors hold the position of the output port with which an input port is associated. The free vector denotes whether the ith output port is occupied by a packet or not, and the sender vector shows when an input unit is in transmission state or not. V. G EN ERA L PUR PO S E A S S ERT ION S AND IN S TAN T IAT ION A. Category 1 - No Packet Loss A ﬁrst fundamental property is the absence of packet loss. We divide this class into two subclasses depending on the possible loss: inside the router, and between two routers. 1) Inside the Router: This subclass includes properties verifying that if a packet appears at an input unit of the router then it will appear at one of its output units. Case of buffered communications. A general property to be veriﬁed is: (NL1) A packet/ﬂit that enters the router will eventually leave the router Let us see more precisely the interpretation of this property according to the various switching techniques. Store and Forward. In store and forward ﬂow control, it is important to know that the packet will be transmitted after it has been completely received. (N LSF 1 ) A packet will not be transmitted until its last ﬂit arrives at the input The failure of this property will translate the break of the packet into two pieces resulting either in the reception of the reduced packet, or in the endless waiting of the rest of the packet (deadlock). (N LSF 2 ) The input will not receive a new packet until all the previous packet ﬂits are transmitted Cut Through. Here we have the following essential property: (N LC T 1 ) The input unit will accept a new packet only after the complete transmission of the previous one The failure of this property can be produced by the attachment of two consecutive packets which will mean their loss. There is an important difference with store and forward. In store and forward the input unit holds all the packet, while in cut through the input unit might hold only some ﬂits of the packet since the transmission is done in a pipelined fashion. Wormhole. In cut-through the router reserves the input units in terms of packets; in wormhole it is allowed to receive a new packet before the complete transmission of the previous one. However the allocation of the output unit through the switch is performed similarly in wormhole and cut-through. A property verifying the functionality of the reservation is as follows: (N LW 1 ) The allocated resources will remain allocated to the same packet ﬂow until the last ﬂit is transmitted The essence of this property is for the input and the control logic element not to deallocate the reserved resources for the current ﬂow. Otherwise the packet will be split, which will result in a loss. Example. The previous property is instantiated inside the Hermes SwitchControl component as follows: property verify_deallocate is forall i in {0 to 4}: always ( (free(CONV_INTEGER(source(i)))=’0’ and sender(i)=’1’) -> (free(CONV_INTEGER(source(i)))=’0’ and sender(i)=’1’) until! sender(i)=’0’); The free vector, keeps track of the free output ports of the router, while the source vector keeps track of the associations of the input ports to the output ports. Thus the ith location of the source vector, indicates the location of the output port the ﬂit must travel. The sender vector indicates whether the ith input port is in transmission state or not. This assertion says that if an output port is related to an input port, it will remain occupied until the input port leaves the transmission state. Virtual Channel. In virtual channels, there are more than one buffer at the I/O ports handling several packet ﬂows. A key property is not to mix the virtual channels: (N LV C 1 ) Once a virtual channel is allocated to a packet/ﬂow, it will not be deallocated until the complete transmission of the packet The failure of this assertion can translate the mix of two different packet ﬂows, which results in their loss. Case of bufferless communications. In bufferless ﬂow control the packet is moving from one router to the other every clock cycle. Since there are no buffers, there are two main heuristics for packets requesting the same resources: packet the requesting drop, and misroute. Hence verifying the non loss of a packet can be expressed as follows: (NL2) A packet will not be dropped if destination is available The next property veriﬁes that a packet will leave the router at the next clock cycle if misroute policy is applied. It is important to point out the key difference between this property and its buffered version. In the buffered version an “eventually” statement is used to express the fact that the packets are allowed to be stalled, while in non buffered control ﬂow the packet must leave the router at the next clock cycle. (NL3) If a packet enters the router, it will be ready to leave the router at the same cycle Example. For Nostrum, this property is instantiated as: property verify_loss is forall i in {0 to 3}: always (empty_temp(i)=’1’ -> (N_select(i)=’1’ and N_empty=’0’) or (S_select(i)=’1’ and S_empty=’0’) or (E_select(i)=’1’ and E_empty=’0’) or (W_select(i)=’1’ and W_empty=’0’) or (R_select(i)=’1’ and R_empty=’0’)); This property veriﬁes that if there is a packet at the input of the router, then this packet will be routed to an output port. The select signals of the multiplexers are used to express the presence of packets. 2) Between two Routers: Between two routers packet loss may occur mainly due to communication protocol failures. There are two categories of protocols, synchronous and asynchronous, according to which this subclass will be divided. Synchronous. The routers exchange ﬂits every clock cycle if there is a ﬂit to be transmitted and if the neighbor router is available. The availability of the neighbor router is expressed as the ability of the buffer to receive packets i.e., it is not full. In every protocol there is a notiﬁcation mechanism (transmission request signal, etc) which indicates to the neighbor the existence of a packet. The question is whether this mechanism will be triggered according to the control signals at the corresponding cycle. This condition can be expressed as: (NL4) If a ﬂit is ready to leave, it will stay at the router until the neighbor router is ready for reception Since the protocol is synchronous, at each clock cycle the packet is tested for departure. This property complements the ones previously described, where the packets where supposed to be ready for departure. Example. For the Nostrum network the instantiation of this property can be made as follows: property verify_packet_transmit is always ((N_empty=’0’ -> next(qNorth.Payload(PacketLength - 2*AddressLength -1)=’1’)) and (E_empty=’0’ -> 164 next(qEast.Payload(PacketLength - 2*AddressLength -1)=’1’)) and (W_empty=’0’ -> next(qWest.Payload(PacketLength - 2*AddressLength -1)=’1’)) and (S_empty=’0’ -> next(qSouth.Payload(PacketLength - 2*AddressLength -1)=’1’)) and (R_empty=’0’ -> next(qResource.Payload(PacketLength - 2*AddressLength -1)=’1’))); The meaning of this assertion is straightforward. For instance, regarding the north port: the N empty signal indicates the existence of a valid packet ready to depart from this output port. qNorth.Payload(empty bit location) is the empty bit of the output packet indicating the validity of the packet to be transmitted at the next clock cycle. Asynchronous. When the communication is asynchronous, requests and acknowledgments are responsible for the proper exchange of packets between two neighbor routers. The following properties will help verify the correct functionality of the general asynchronous protocol principles. (NL5) If a router requests to send a message it will not stop requesting until it receives an acknowledgment for its reception The previous property does not cover the case where packet drops are allowed by the protocol, but it can be extended to be valid during the threshold before the packet drop. If the router terminates the request for transmission, there are three possible scenarios. First, another ﬂit from another packet might be accepted resulting to a packet loss. Also the request for transmission might start after a few clock cycles which means that the ﬂit might be transmitted ﬁnally, but the property will have failed. Finally the failure of this property can also reveal deadlocks (the buffer for example will stop requesting for resources and so, it might stop sending and receiving since the ﬂit will remain there forever). Example. Property (NL5) can be instantiated for Hermes in two ways: either from the receiver point of view with the rx signal, or from the transmitter point of view with the tx signal. The rx case is presented below: property verify_rx_ack is forall i in {0 to 4}: always (rx(i) -> rx(i) until! ack_rx(i)); Finally there is a case where a router might send an acknowledgment for reception without a request for transmission. This means that the router received a ﬂit that does not exist or has already been received, which may cause a packet loss in the buffer or an unexpected packet generation. (NL6) An acknowledgment is sent only if the router receives a request and the corresponding packet/ﬂit Example. This property is instantiated for Hermes as follows: property verify_rcv_empty is forall i in {0 to 4}: always (ack_rx(i) -> rx(i)); 165 B. Category 2 - No Packet Duplication Packet duplication errors occur either due to communication errors or due to internal router errors. This family of properties can be further classiﬁed as the packet loss class that is, based on the location of the duplicated packet. 1) Inside the Router: One of the most important properties of a network is the one-to-one relationship between the input ports and the output ports of the router. Failure of this oneto-one relationship will cause packet duplication and other bad effects. The following property describes this behavior (it is also related to the routing decision integrity family of properties presented in section V-D). (ND1) The input of a router will be assigned to one and only output of the router The failure of this property can also correspond to packet loss. Example. The instantiation for Hermes is: property verify_unique_output is forall i in {0 to 4}: always tx(i) -> ((mux_out(i)=mux_out((i+1) mod 5) -> free((i+1) mod 5)) and (mux_out(i)=mux_out((i+2)mod 5) -> free((i+2) mod 5)) and (mux_out(i)=mux_out((i+3)mod 5) -> free((i+3) mod 5)) and (mux_out(i)=mux_out((i+4)mod 5) -> free((i+4) mod 5))); At the mux out(i) vector, the ith position contains the number of the input port direction (0 East, 1 West etc) that will be assigned to the ith output port. The free(i) vector, indicates whether the ith output port is occupied or not. Therefore, this property veriﬁes that if an input port is assigned to two different output ports, then one of the two output ports must be in non transmitting state (i.e. it is not occupied by any packet). 2) Between two Routers: Packet duplication between two routers can be further classiﬁed according to the implemented communication protocol. Thus this class is divided into synchronous and asynchronous communication protocols. Synchronous. When synchronous communication occurs, it is important to verify that the implemented mechanism indicating the existence of a packet will be deactivated if there is no packet at the input port of the router. In many cases the failure of such a property can result in the duplication of a packet, or the generation of an empty packet, since the neighbor router will be notiﬁed for a packet that does not exist. (ND2) The packet indication mechanism will be deactivated if there is no packet to transmit at the next clock cycle after the last transmission This notiﬁcation mechanism can be implemented using several techniques such as a signal between the routers with a stop/go functionality or a bit in the packet protocol indicating the validity of the packet. Example. For Nostrum, this property is expressed as: property verify_packet_kill is always ((N_empty=’1’ -> next(qNorth.Payload(PacketLength - 2*AddressLength -1)=’0’)) and (E_empty=’1’ -> next(qEast.Payload(PacketLength - 2*AddressLength -1)=’0’)) and (W_empty=’1’ -> next(qWest.Payload(PacketLength - 2*AddressLength -1)=’0’)) and (S_empty=’1’ -> next(qSouth.Payload(PacketLength - 2*AddressLength -1)=’0’)) and (R_empty=’1’ -> next(qResource.Payload(PacketLength - 2*AddressLength -1)=’0’))); In this assertion it is veriﬁed that if there is no packet to be switched (indicated by the N empty signal of the north output port for example), then at the next clock cycle, the empty bit of the previous packet will be set to ’0’ at the corresponding output port. If this property does not hold, the same packet will be transmitted twice resulting in packet duplication. Asynchronous. It is important to verify that the handshake protocol will not produce a packet duplication related error. (ND3) If the sender receives an acknowledgment it will stop requesting for transmission As before this property must hold if there is no other packet to transmit in the case of a three phase protocol, or it can be always true in a four phase protocol. In the three phase protocol, if there are two consecutive packets/ﬂits to be transmitted, the requesting signal does not have to stop (only if there is no other packet) while in the four phase protocol of Hermes for example, the request signal must return to its initial state (non requesting state) before it starts requesting transmission for the next packet/ﬂit. property verify_four_phase is forall i in {0 to 4}: always {rx(i)} |-> {rx(i)[*] ; ack_rx(i) and rx(i) ; not(ack_rx(i)) and not(rx(i))}; C. Category 3 - Delivery Upon Arrival This class of properties aims at verifying the correct reception of packets from the local IPs when they reach their destination. In deterministic (or minimal adaptive) routing architectures, when the packet reaches its destination, it waits until the local IP becomes available. Fully adaptive architectures are allowed to misroute the packet if the local IP is occupied. 1) Deterministic: Here a major property is the following: (Del1) If a packet/ﬂit arrives to the ﬁnal destination router it will remain there until it is received by the local IP Example. For Hermes, this property is instantiated as property verify_arrival is forall i in {0 to 3}: always (ack_h(i)=’1’ and tx=lx and ty=ly) -> source(i)=LOCAL; This assertion expresses that, if an incoming packet grants arbitration from the router and it is located on the ﬁnal destination then this packet must be routed to the local IP. 2) Fully Adaptive Routing: It is difﬁcult to deﬁne a generic property for the veriﬁcation of the delivery of a packet due to the fact that misrouting is allowed. However the following property is common for all the fully adaptive networks. (Del2) If the local IP is free and at least one packet arrives to its destination, one packet will be delivered to the local IP Example. The instantiation of this property for Nostrum can be given as follows: property verify_delivery is always (( (empty_temp(North)=’1’ and (iNo.d_addr=’0’)) or (empty_temp(South)=’1’ and (iSo.d_addr=’0’)) or (empty_temp(West)=’1’ and (iWe.d_addr=’0’)) or (empty_temp(East)=’1’ and (iEa.d_addr=’0’))) -> R_empty=’0’); If there is at least one packet at the input ports of the router destinated to the local IP (the ﬁeld d addr is ’0’), it will be delivered to the local IP. D. Category 4 - Routing Decision Integrity The purpose of this class of low-level properties is to verify that the decisions of the routing algorithm are actually fulﬁlled. Properties below apply to both minimal and nonminimal routing architectures. The following property is interesting for many architectures since the output ports of the routers are usually controlled by multiplexers and thus it could be the case that an error in the multiplexing signals may result in the assignment of two different packets to the same port. (RD1) Packets waiting to be transmitted will not be assigned to the same port Example. In the Nostrum network ﬁve multiplexers are responsible for the assignment of the input ports to one of the output ports. Each multiplexer corresponds to one output port and is driven by a 5-bit input signal. Each bit of this signal corresponds to one input port. Thus at each step, only one bit of the multiplexer control signal must be high, otherwise the output port should multiplex two input signals. property verify_one_hot_sel is always (onehot(S_select) and onehot(N_select) and onehot(W_select) and onehot(E_select) and onehot(R_select)); Complementing the previous assertion, the next property veriﬁes that, after the assignment of the control signals of the switch, the ﬂits will be transmitted by the designated outputs. (RD2) The switch will route the packets according to the control signals Example. In the Nostrum network, this property can easily be 166 instantiated as follows: property verify_multiplex_route is forall i in {1 to 4}: always ( ((R_select(i)=’1’ and empty_temp(i)=’1’) -> R_empty=’0’) and ((E_select(i)=’1’ and empty_temp(i)=’1’) -> E_empty=’0’) and ((W_select(i)=’1’ and empty_temp(i)=’1’) -> W_empty=’0’) and ((N_select(i)=’1’ and empty_temp(i)=’1’) -> N_empty=’0’) and ((S_select(i)=’1’ and empty_temp(i)=’1’) -> S_empty=’0’)); E. Category 5 - Quality of Service Depending on the performance goals of the application, QoS is usually classiﬁed into two categories: Guaranteed Services and Best Effort Services. 1) Guaranteed Services: NoCs implementing guaranteed service methods can achieve a minimum expected performance if the network satisﬁes some restrictions. Among many techniques that have been proposed, aggregate resource allocation and resource reservation are the most popular. In aggregate allocation, each port (input or output) has a limited bandwidth, in order to achieve an overall bandwidth for the network, not inﬂuenced by bursty trafﬁc. To achieve this upper limit of bandwidth the packet ﬂow is split and reconstructed before its arrival to destination. Essentially this method works as an indication to the routing algorithm, but it is expected that the router will respect these instructions. (QS1) The router will not accept more trafﬁc than its bandwidth Reserving resources allows the network to experience more strong guarantees over delay and jitter. Reservation in space and/or time implies that several network resources (ports, buffers, etc) will be allocated to a ﬂow. Several heuristics have been proposed providing a fair resource reservation among the ﬂows. It is not part of this study to analyze these heuristics, however a certain behavior is expected to appear at each node implementing these methods: (QS2) An allocated route in the network will not be allocated by another ﬂow before it is allowed to do so 2) Best Effort Services: Best effort services guarantee the fair utilization of the resources. Latency fairness services give priority to the oldest packet while throughput fairness services give priority to the output ports with the smallest load. Depending on the details of the architecture, these services can work as a guide to the routing algorithm or as a restriction. However when there is a conﬂict for the same resource, the priorities between the requesters should never be the same. (QS3) Two competing resources will never have the same priority order Example. Nostrum implements both latency and throughput fairness services. The resource priorities must be unique, which is described by the following two assertions: property verify_OPO is always ((OPO(0)/= OPO(1)) and (OPO(0)/= OPO(2)) and (OPO(0)/= OPO(3)) and (OPO(1)/= OPO(2)) and (OPO(1)/= OPO(3)) and (OPO(2)/= OPO(3))); property verify_Prio_Row is always ((PrioRow(0) /= PrioRow(1)) and (PrioRow(0) /= PrioRow(2)) and (PrioRow(0) /= PrioRow(3)) and (PrioRow(1) /= PrioRow(2)) and (PrioRow(1) /= PrioRow(3)) and (PrioRow(2) /= PrioRow(3))); OPO and PrioRow are the output and input priority vectors. F. Category 6 - Packet Flow Deadlocks and livelocks are crucial problems for NoC. Since deadlock avoidance is an algorithmic and design issue, it is out of the scope of this study. However properties can be described to ease the detection of deadlocks and livelocks at the router level mostly concerning the packet ﬂow inside the router. Thus this class focuses on the deadlock caused by internal errors of the router and not by the dependencies between the routers. When the observation point is the router itself and not the global network state, it is not always possible to know if the packet ﬂow waits to be forwarded or if it is in a deadlock state. Having available resources though, it is expected that the packet will move forward. (PF1) A packet/ﬂit will not stay in a router if the neighbor nodes are available Several networks adopt packet dropping policies in order to avoid deadlocks, congestions and other issues. It is important to verify that the router will always respect this policy. (PF2) A packet will be dropped if it exceeds the time limit for waiting According to the modeling of the router, the resources refer to I/O buffers, ports, virtual channels etc, which are allocated by the control logic unit. Essentially it is expected for the control logic to deallocate these resources after the transmission of the ﬂow, otherwise deadlocks might occur. (PF3) The router will deallocate the resources if there is no ﬂow using them Example. In Nostrum, each allocation lasts by deﬁnition only one clock cycle; property (ND2) participates in the instantiation of (PF3). Its instantiation for Hermes is: property verify_port_freedom is forall i in {0 to 4}: always (not(free(CONV_INTEGER(source(i)))) and sender(i)) -> next_event(not sender(i)) (next(free(CONV_INTEGER(source(i))))); This assertion denotes that if an input unit is transmitting through an output port, then when the input unit terminates the transmission, the output port will be freed (next event(a)(b) means that when a holds, then b must also hold). Livelock is a situation in which a packet does not move towards its destination. Livelocks are more likely to occur 167 Fig. 5. Excerpt of Hermes Simulation with Fault Injection in non-minimal adaptive networks. Several techniques have been proposed to avoid livelocks such as age-based priorities, misroute count or probabilistic routing. We have already described properties which check the correct functionality of these priority mechanisms. However combining these methods could result in using misroute count in order to settle a threshold for the maximum number of misroutes. When this threshold is exceeded the ﬂow must be routed in a minimal routing fashion. The following property veriﬁes this heuristic. (PF4) A packet which exceeds a given threshold will be no more misrouted until it reaches its destination Example. Nostrum does not guarantee livelock freedom. However it is interesting to describe a property that is able to detect a livelock for the monitoring of the network behavior: based on the packet hop count, this property veriﬁes that the packet will not reach the maximum value of the counter. property verify_HC is always ( (iNo.HC /= (others => ’1’)) and (iSo.HC /= ( others => ’1’)) and (iEa.HC /= ( others => ’1’)) and (iWe.HC /= ( others => ’1’))); V I . EX P ER IM EN T S For Nostrum and Hermes, runtime veriﬁcations have been performed by automatically transforming the PSL assertions into synthesizable monitors and instrumenting the original VHDL descriptions with these checkers: a total of 39 PSL assertions have been created for Nostrum, and 30 for Hermes. A ﬁrst set of experiments was realized during simulation. Nostrum’s longest simulations lasted 3 ms with a clock period of 100 ns and the number of packets injected in the network was 107200. Hermes’longest simulation was 15 ms with a clock period of 100 ns; the overall number of packet injected was 7600. Without fault injection, no property violation occurred, as the NoC’s implementations are correct. Saboteurbased fault injection (of Single Event Upsets) has shown that misbehaviors can readily be revealed by the checkers. An example for Hermes is depicted in Fig. 5: since the ack rx signal remains high for two clock cycles instead of one, a violation is caused for assertions verif y rcv empty and verif y f our phase (sections V-A2 and V-B2): the “valid” outputs of the corresponding checkers become low. The assertions have also been synthesized with their NoC into a Virtex 5 Xilinx FPGA, and ChipScope Pro [13] was used to observe the monitoring signals. For instance, a 4x4 version of Hermes has been synthesized with 7 monitors connected to each of six central routers (and with trafﬁc generators). The area and speed with the monitors are 22581 LUT and 48.25 MHz (21173 LUT and 64 MHz without monitors). V I I . CONC LU S ION We have determined and categorized various assertions for the detection of typical implementation errors in NoCs, to complement algorithmic veriﬁcations. They have been applied to two state-of-the-art networks, previously veriﬁed from the algorithmic point of view [14], [15]. The high-level theorem proving based solution is generic and reproducible. We have taken into account a large variety of NoC characteristics when deﬁning the assertions, to make them also largely instantiable. "CCNoC - Specializing On-Chip Interconnects for Energy Efficiency in Cache-Coherent Servers.,"Many core chips are emerging as the architecture of choice to provide power efficiency and improve performance, while riding Moore's Law. In these architectures, on-chip inter-connects play a pivotal role in ensuring power and performance scalability. As supply voltages begin to level off in future technologies, chip designs in general and interconnects in particular will require specialization to meet power and performance objectives. In this work, we make the observation that cache-coherent many core server chips exhibit a duality in on-chip network traffic. Request traffic largely consists of simple control messages, while response traffic often carries cache-block-sized payloads. We present Cache-Coherence Network-on-Chip (CCNoC), a design that specializes the NoC to fit the demands of server workloads via a pair of asymmetric networks tuned to the type of traffic traversing them. The networks differ in their data path width, router micro architecture, flow control strategy, and delay. The resulting heterogeneous CCNoC architecture enables significant gains in power efficiency over conventional NoC designs at similar performance levels. Our evaluation reveals that a 4×4 mesh-based chip multiprocessor with the proposed CCNoC organization running commercial server workloads is 15-28% more energy efficient than various state-of-the-art single- and dual-network organizations.","CCNoC: Specializing On-Chip Interconnects for Energy Efﬁciency in Cache-Coherent ServersStavros Volos§, Ciprian Seiculescu†, Boris Grot§, Naser Khosro Pour‡, Babak Falsaﬁ§, and Giovanni De Micheli†§EcoCloud	†LSI	‡ELABE´ cole Polytechnique Fe´de´rale de LausanneAbstract—Manycore chips are emerging as the architecture of choice to provide power efﬁciency and improve performance, while riding Moore’s Law. In these architectures, on-chip inter- connects play a pivotal role in ensuring power and performance scalability. As supply voltages begin to level off in future technolo- gies, chip designs in general and interconnects in particular will require specialization to meet power and performance objectives. In this work, we make the observation that cache-coherent manycore server chips exhibit a duality in on-chip network trafﬁc. Request trafﬁc largely consists of simple control messages, while response trafﬁc often carries cache-block-sized payloads. We present Cache-Coherence Network-on-Chip (CCNoC), a design that specializes the NoC to ﬁt the demands of server workloads via a pair of asymmetric networks tuned to the type of trafﬁc traversing them. The networks differ in their datapath width, router microarchitecture, ﬂow control strategy, and delay. The resulting heterogeneous CCNoC architecture enables signiﬁcant gains in power efﬁciency over conventional NoC designs at similar performance levels. Our evaluation reveals that a 4x4 mesh-based chip multiprocessor with the proposed CCNoC organization running commercial server workloads is 15-28% more energy efﬁcient than various state-of-the-art single-and dual-network organizations.INTRODUCTIONToday’s server chips feature up to one hundred cores [20], and as chip integration levels keep increasing, future chips are expected to accommodate hundreds of cores [11]. Manycore chips rely on a network-on-chip (NoC) to lower design com- plexity and improve scalability. Recent research has identiﬁed high NoC power consumption as a signiﬁcant obstacle in a quest for efﬁcient manycore chips [5]. For example, in the MIT RAW processor, NoC power accounts for 40% of the overall chip power [22]. With supply voltages leveling off [7], the key design criteria for chips in general and NoCs in particular will be power.A number of techniques have been proposed for improving NoC power consumption [1, 4, 13, 15, 23]. While the majority of these approaches focus on improved efﬁciency in the context of a single on-die network, Balfour and Dally [1] ad- vocate using multiple networks as a practical way to improve performance, power, and area in tiled chip multiprocessors (CMPs). Using simple read/write messaging protocols, Balfour and Dally conclude that a homogeneous dual-network NoC,  in which the two networks use identical microarchitectures and datapath width, balances the load among short and long messages and is preferred to a heterogeneous design. Whereas prior work examined NoC efﬁciency in general- purpose chips, we target efﬁciency through network-level specialization in the context of server CMPs. Server  chips rely on cache-coherent architectures for software transparency, and for facilitating software development and porting. Our analysis of commercial server workloads in a cache-coherent CMP reveals that network trafﬁc does not follow simple read/write messaging protocols, commonly assumed in prior work, including Balfour and Dally [1]. In fact, trafﬁc is highly skewed among short and long messages with (short) request messages primarily consisting of block fetch requests and clean replacement notiﬁcations and (long) response messages carrying a cache block.These observations motivate us to propose Cache- Coherence Network-on-Chip (CCNoC), a heterogeneous dual- network architecture for manycore server chips. CCNoC op- timizes power and performance based on the  characteristics of the two dominant message classes. The networks are asymmetric in their datapath width and router architecture. The request network is optimized for short messages, and thus it has a narrow datapath. As requests (e.g., reads) and the associated coherent requests (e.g., downgrades) travel through the same network, the network relies on virtual channels to segregate these message classes for deadlock avoidance. In contrast, the response network does not require any virtual channels and is customized for cache block transfers via a wide datapath and low-complexity wormhole routers.We use Flexus [24] for cycle-accurate full-system multipro- cessor simulation running commercial server workloads and augment it with custom power models to show that:Network trafﬁc in server workloads is highly skewed among short and long messages. Short requests account for 94% of all requests and long responses account for 95% of all responses;Unlike NoCs for CMPs with simple messaging proto- cols that favor homogeneous networks, NoCs for cache- coherent CMPs require specialization and heterogeneity to best take advantage of the network trafﬁc.CCNoC is more energy efﬁcient than various state-of-the- art single- and dual-network organizations by 15-28%.BACKGROUNDWe ﬁrst describe the chip architecture that we consider in this paper. Next, we qualitatively explain how trafﬁc duality			Reader    Directory	Writer	Writer	Directory Reader 1, 2Figure 1: Tiled processor with mesh topology. Each network router uses three virtual channels (VCs); Request, Coherence request, and Response. The network router has six ports: N(orth), S(outh), W(est), E(ast), NI1 for the L1 controllers, and NI2 for the L2 slice.arises in cache-coherent CMPs and, ﬁnally, describe protocol- level deadlock issues.Figure 1 depicts a canonical cache-coherent tiled CMP chip architecture. Each tile consists of a processor core with L1 data and instruction caches, an L2 slice, a directory slice, a network router, and two network interfaces (NIs). We assume a shared L2  cache  organization, in  which  each  L2  slice  is a part of a shared last-level cache  (LLC) and  cache blocks are address-interleaved among the L2 slices. While the shared organization is preferred for server workloads, as it is more effective at capturing their large instruction and data footprints, our results equally hold for systems with a private LLC.An invalidation-based directory protocol maintains coher- ence among the L1 caches. We assume, without loss of generality, a duplicate-tag directory scheme where each di- rectory slice is responsible for the same range of address- interleaved cache blocks as the co-located L2 slice. The choice of directory encoding [26] may also affect the overall network trafﬁc, but does not fundamentally affect the breakdown of the request and response message types.Each tile uses two NIs, one for the core (i.e., L1 controllers) and the other for the L2 slice and the  directory controller. This allows parallel accesses to the tile’s caches and directory from remote cores. We assume a router architecture with four network ports and two local ports (connected to the NIs).Coherence protocol activityCoherence protocols were introduced as a way to ensure that any request for a cache block will get the most recent state of that cache block. Figure 2 depicts the protocol transitions for reading (data and instruction) and writing into cache blocks.Figure 2 (left) shows the communication between a reader core, a directory slice, and a potential writer. Control messages are short and depicted with narrow dashed lines. Data mes- sages carry a cache-block sized payload and are depicted with thick solid lines. In the common case, a reader sends a request for the read-only copy of a cache block, followed by a response from the L2 cache with the data. Similarly, to keep the Figure 2: Read (left) and Write (right) protocol activity.directory up-to-date with sharer information, clean cache block replacements are also notiﬁed with small request messages. In the less frequent case of a read from an active writer of a block, the protocol implements a 3-hop transition. The read request  is forwarded to the writer, which then responds directly to the reader and the directory with a data message and a notiﬁcation response, respectively. Thus, most requests (reads or eviction notiﬁcations) for clean blocks are short messages and most responses carry a cache block.Figure 2 (right) depicts the protocol transitions for writing into cache blocks. In general, write requests (i.e., fetch-block or upgrade requests) are less frequent. Moreover, in server workloads, writebacks account for a negligible fraction of the overall trafﬁc because data are rarely updated and instructions are virtually never modiﬁed at runtime [6]. Finally, unlike cache block fetches, writebacks are not latency sensitive and therefore do not impact performance directly.Even among the writes, there are  common transitions that ﬁt the duality in trafﬁc. For example, write misses to blocks that are not actively shared have the same request/response behavior as reads of clean blocks. Other transitions include upgrade requests to a non-shared block, requiring a short request message and a short response message as well. Among write requests, those involving other readers and consequently a large number of control messages for both requests and responses are quite rare. In server workloads, data sharing and migration across threads happen over large windows of time– well beyond a typical L1 residency period [6]. As a result, writers rarely modify blocks shared by other cores [10].Commercial server [6] and emerging scale-out cloud [3] applications are optimized for high reuse in the L1 data cache, while their instruction working sets exceed the L1 instruction cache capacity. Therefore, in server CMPs, coherence activity is dominated by short requests (clean data and instruction block fetches) and the associated long responses.Protocol-level deadlock avoidanceVarious message classes co-exist in cache-coherence proto- cols (i.e., requests, coherence requests, and responses). Con- sequently, protocol-level cyclic dependencies and deadlocks(a)CMP Size	16-core for server workloads8-core for multiprogrammed workloadsProcessing Cores	UltraSPARC III ISA; 2GHz 8-stage pipeline2-wide dispatch / retirement, OoOL1 Caches	Split I/D, 32KB 4-/2-way, 1-/2-cycle load-to-use3 ports, 32 MSHRs, 8-entry victim cacheL2 NUCA Cache	512KB per core, 8-way, 10-cycle latency, 64-bytelines, 1 port, 32 MSHRs, 16-entry victim cacheMain Memory	3 GB total memory, 45 ns access latencyMemory Controller	one per 4 cores, round-robin page interleaving (b)Table I: System parameters for the 16-core and 8-core CMPs (a) and application parameters (b).may occur due to messages sharing network resources. To avoid protocol-level deadlocks, conventional NoCs partition the physical resources at each router’s input port among mul- tiple virtual channels to allow independent routing of different message types. An alternative organization consists of multiple physical networks, with a dedicated network for each message class. In both cases, deadlock is avoided by routing messages of each class on a dedicated network (virtual or physical), thereby preventing the formation of cyclic dependencies across 100%80%60%40%20%0% Request Short	Request Long		Coherence Request Response Short	Response Longmessage classes.CHARACTERIZATION OF NETWORK TRAFFICMethodologyWe use Flexus [24] for cycle-accurate full-system sim- ulation of a tiled CMP executing unmodiﬁed  applications  and operating systems. Flexus extends the Virtutech Simics functional simulator with timing models of processing tiles with out-of-order cores, a detailed cache hierarchy, memory controllers, and a NoC. We simulate a tiled CMP similar to  the one described in Figure 1 with a shared LLC.Table I  (a) summarizes our system architecture. We  model a server chip composed of 16 cores and an 8 MB last-level cache. Recent work has demonstrated that cache capacities up to 4-8 MB are beneﬁcial for capturing the instruction footprint and the small amount of shared data in server workloads [6, 8]. Cache capacities beyond 8 MB have a much lower utility due to the enormous memory footprints of these applications. We model a distributed (NUCA) LLC with 512 KB at each tile, with cache coherence based on the MESI protocol.Our simulated system runs the Solaris 8 operating system and executes the workloads listed in Table I (b). We include a wide range of commercial server workloads from the domains of online transaction processing, decision support systems, and web servers. We use the TPC-C v3.0 OLTP  benchmark [21] on IBM DB2 v8 ESE and Oracle 10g Enterprise Database Server. We run a mix of queries 1, 6, 13, and 16 from the TPC-H benchmark [21] on DB2. Queries 1 and 6 are scan- bound, Query 16 is join-bound, and Query 13 exhibits a hybrid behavior. To evaluate web server performance, we use the SPECweb99 benchmark on Apache HTTP Server v2.0 and Zeus Web Server v4.3. We use a separate client system to drive the web servers, and hence do not include client activity in our measurements. Figure 3: Network trafﬁc distribution in server and multiprogrammed workloads.For comparison, we also simulate a multiprogrammed work- load that consists of SPEC CPU2000  applications  running the reference input set. Because desktop applications lack concurrency and do not beneﬁt from many-core execution substrates [2], we model an 8-core CMP for this study.Characterization of network trafﬁcFigure 3 illustrates the distribution of short and long mes- sages across on-chip request, coherence request, and response messages. The request trafﬁc primarily consists of instruction fetches, cache-block reads, and clean eviction requests (i.e., short messages). Across server workloads, short messages account for 94% of the request trafﬁc. In OLTP and Web workloads, which have big instruction footprints, instruction block requests dominate the request trafﬁc. In DSS and the multiprogrammed workload, all with small instruction foot- prints, read requests are the majority of the request trafﬁc.In server workloads, the writeback trafﬁc accounts only for 6% of the request trafﬁc, implying that the clean eviction requests dominate the dirty eviction requests. In contrast, in the multiprogrammed workload, the writeback trafﬁc accounts for 19% of the request trafﬁc.Coherence requests (i.e., invalidate and downgrade requests) are short and account for a small fraction of the request trafﬁc. For the server workloads, coherence requests account for only 5% of the request trafﬁc. The multiprogrammed workload does not exhibit such trafﬁc because each core runs a different application and hence there is not any sharing among the cores. Figure 3 indicates that the response trafﬁc is also highly skewed. On average, long response messages account for 95%Figure 4: CCNoC with mesh topology. CCNoC uses a pair of asymmet- ric routers to optimize power and performance for the dominant trafﬁc type of the corresponding network.of the response trafﬁc. The majority of long responses are messages carrying instruction or read data blocks.Overall, our results reveal that cache-coherent tiled CMPs exhibit duality in the network trafﬁc when running server workloads, with trafﬁc primarily consisting of short requests and long responses. This distribution differs from that in desktop workloads, where 19% of the request messages are long due to frequent writebacks of dirty cache blocks.CACHE-COHERENCE NETWORK-ON-CHIPIn this paper, we propose Cache-Coherence Network-on- Chip (CCNoC), a design that capitalizes on the duality in network trafﬁc of cache-coherent server chips. CCNoC uses two asymmetric networks to achieve higher efﬁciency as compared to existing designs.  Each  network  is  optimized for the dominant trafﬁc type and hence the  two  networks have different datapath widths, different buffer architectures, and different pipeline lengths. Unlike NoCs for CMPs with simple messaging protocols that favor homogeneous networks, NoCs for cache-coherent CMPs require specialization and heterogeneity to best take advantage of network trafﬁc.Figure 4 depicts the CCNoC architecture with a mesh topology. Each tile consists of two routers, one of them specialized for the request and the other for the response network. Because requests primarily consist of short messages, the request network can be built with a narrow datapath width to reduce switch area and power with minimal impact on the system performance. The response messages usually carry a cache block and as such  beneﬁt  from  wider channels. The NI connecting the core to the network has physical interfaces to both request and response routers. The NI routes requests into the narrow request network and responses into the wide response network. To ensure that the cache coherence protocol is not affected, the receiver NI keeps the order between the request and response messages coming from the same source. Our system features three types of network messages: (a) normal requests (e.g., read or instruction fetch requests); (b) coherence requests (invalidate or downgrade requests); and (c) responses. Avoiding protocol-level deadlock among different messages classes requires either dedicated virtual channels (VCs) or different physical networks. Thus, single-network topologies require three VCs per input port to guarantee deadlock freedom. The same is true for homogeneous dual- network schemes and heterogeneous organizations that split the trafﬁc based on message size (short, long). In contrast,   the proposed CCNoC organization segregates requests and responses via dedicated networks, and as a result, requires virtual channels only on the narrow request network to avoid any cyclic dependencies between normal and coherence re- quests. The wide response network is deadlock-free by default, since all response messages are guaranteed to be consumed at the destination. As such, it does  not  need  virtual  channels for deadlock freedom, thereby improving area and energy efﬁciency through reduced buffer requirements.Specializing the CCNoC networks to trafﬁc class enables further optimizations at the router level. Conventional single- and dual-mesh topologies use a three-stage router pipeline that consists of virtual channel allocation (VA), switch allocation (SA), and switch traversal (ST) stages. In CCNoC, the VC- enabled request network features the same router pipeline; however, wormhole routers in the response network can be simpliﬁed by eliminating the VC allocation stage. This opti- mization reduces communication delay and diminishes router complexity in the CCNoC response network.While speculation may also be used to  reduce router de- lay [17], existing schemes tend to increase router complexity and adversely impact cycle time. More generally, speculation and other potential microarchitectural mechanisms do not change the beneﬁts that CCNoC provides. The advantages of the CCNoC design hold due to the server trafﬁc characteristics in cache-coherent CMPs.EVALUATIONWe compare CCNoC to traditional single-network topolo- gies and other dual-network topologies proposed by prior re- search [1] and show that CCNoC provides signiﬁcant area and power savings while achieving similar or better performance.MethodologyWe compare the performance, area, and energy efﬁciency  of CCNoC to state-of-the-art single- and dual-network topolo- gies. Our reference single-network organization is a mesh with a 176-bit datapath (Mesh-176). We also evaluate a single- network 128-bit mesh (Mesh-128) with the  understanding  that it offers inferior performance due to lower bisection bandwidth, but better energy efﬁciency than the wide mesh.We consider two dual-network schemes. The ﬁrst is a Homogeneous (i.e., replicated) organization that features two identical 88-bit mesh networks. In this design, trafﬁc is evenly distributed among the two networks to maximize load balance. The other organization is Heterogeneous, featuring a wide network for long messages and a narrow network for short packets. The networks are 112 and 64 bits wide, respectively. Both single- and dual-network NoCs feature a three-stage router pipeline and three VCs per router input port to avoid protocol-level deadlocks.1.00.80.60.40.20.0 Links(Dynamic)	Buffers(Dynamic)	Buffers(Leakage) Crossbar(Dynamic)	Crossbar(Leakage)a. Mesh-176 b. Mesh-128 c. CCNoC 1.21.00.80.60.40.20.0 Mesh-176 Mesh-128 CCNoCFigure 5: NoC power consumption normalized to Mesh-176 for (a) Mesh-176, (b) Mesh-128, and (c) CCNoC topologies. Figure 7: System performance (User-IPC) normalized to Mesh-176.cases that the width of the transmitted ﬂit is smaller than the0.120.090.060.030.00 Mesh-176 Mesh-128 CCNoC Response CCNoC Request20151050 width of the network’s datapath [1].We use Orion 2.0 [9] to estimate the area of router buffers and use our custom methodology to estimate the area of links and crossbar.Comparison to single-mesh NoC designsWe measure the total network power consumption and break it down into major network components: buffers, crossbars, and links. For buffers and crossbars, we track both dynamic and leakage power consumption.Figure 6: Injection rate (left) and average network latency (right).Our proposed CCNoC architecture also features a narrow (64-bit) request and a wide (112-bit) response networks. However, as discussed in Section IV, the networks are spe- cialized. The request network carries data request messages and coherence protocol trafﬁc. As such, it requires two VCs per router input port for deadlock avoidance and a three-stage router pipeline similar to reference NoC organizations. The wide response network, on the other hand, is dedicated to  only one message class. This feature enables a simpler router design based on wormhole ﬂow control with no VCs and a two-stage pipeline.We use a custom methodology to assess the energy efﬁ- ciency of the examined NoC organizations. We target 32 nm technology with an on-chip voltage of 0.9 V and a frequency of 2 GHz. We use detailed wire parameters derived from published sources [1, 19] to model the energy expanded in links and router switch fabrics. To reduce link power, we employ differential signaling with  125  mV  swing  voltage  in network channels routed on an intermediate metal layer [18]. Our crossbars are segmented [23] and use full-swing signaling on local wiring with  2x  spacing.  Each  VC  uses six ﬂit buffers. We estimate the energy expanded in  ﬂit buffers by modifying CACTI 6 [16] to model shallow FIFO conﬁgurations representative of typical NoC routers. We also measure leakage power in router buffers and switch fabrics using models derived from CACTI.In all network organizations, we assume that power gating techniques are applied to eliminate spurious toggling of inac- tive portions of the datapath. This feature saves power in the Figure 5 illustrates the network power breakdown for all topologies across our server workloads normalized to Mesh-176. The ﬁgure shows that CCNoC reduces total network power by 28% and 18% when compared to Mesh-176 and Mesh-128 respectively. The power savings of CCNoC com- pared to these network topologies are two-fold.First, CCNoC requires less total ﬂit storage by virtue of requiring fewer VCs than both Mesh-128 and Mesh-176. This feature reduces combined dynamic and leakage buffer power compared to the reference designs by 42% on average. As buffer power accounts for up to 35% of the network power, the savings are signiﬁcant.Second, as noted in Section III-B, a signiﬁcant fraction of the trafﬁc are short request messages that travel through the narrow request network. The compact crossbar in the associ- ated routers diminishes CCNoC’s switch power by 40% and 13% when compared to Mesh-176 and Mesh-128, respectively. As crossbars account for 24-30% of the network power in single-mesh topologies, CCNoC considerably reduces their effect on the NoC power consumption.We evaluate CCNoC’s impact on performance by showing both network and system1 performance across our benchmark suite in Figures 6 and 7, respectively. Figure 6 (left) shows the injection rate (ﬂits/node/cycles) and Figure 6  (right) shows the network latency (i.e., number of cycles to transfer a message from source to destination). Compared to Mesh-  176, Mesh-128  has  a  narrower  channel  width  resulting  in a higher effective load and, consequently, higher network latency, resulting in a minor loss of performance of 5%.1We use User-IPC which is proportional to system throughput [24].1.21.00.80.60.40.20.0 Links(Dynamic)	Buffers(Dynamic)	Buffers(Leakage) Crossbar(Dynamic)	Crossbar(Leakage)Homogeneous b. Heterogeneous c. CCNoC 1.21.00.80.60.40.20.0 Homogeneous	Heterogeneous	CCNoCFigure 8: NoC power consumption normalized to Homogeneous for (a) Homogeneous, (b) Heterogeneous, and (c) CCNoC topologies. Figure 10: System performance (User-IPC) normalized to Homoge- neous.1.000.750.500.250.00 Homogeneous	Heterogeneous	CCNoC20151050 the single-network Mesh-176 design, all three dual-network organizations are effective in reducing switch power by 40- 47% (not shown in the ﬁgure).Figure 9 shows the impact of CCNoC on network per- formance. The ﬁgure on the left illustrates the load balance between the two networks. We deﬁne load balance as the ratio of the injection rate, in ﬂits, in the narrow network over the injection rate in the wide network. The closer to one this ratio is, the better is the achieved load balance. The ﬁgure on the right illustrates the average latency across both networks.By design, the Homogeneous organization evenly splits theFigure 9: Load balance (left) and average network latency (right).Long responses in a CCNoC system require one (two) addi- tional ﬂits when compared to Mesh-128 (Mesh-176). However, the extra serialization delay is offset by the reduced load on the response network thanks to the separate request NoC, as well as the shallower pipeline of wormhole routers in the response network. Figure 6 (right) shows that Mesh-128 and  Mesh- 176 exhibit, respectively, worse and similar injection rate (and network latency), compared to CCNoC’s response network. The request network exhibits lower injection rate, because the majority of injected messages are single-ﬂit. Consequently, the request network latency is slightly lower. Overall, as Figure 7 shows, a CCNoC-enabled CMP matches the system performance of a Mesh-176 organization and outperforms a design based on Mesh-128 by 5%.Comparison to dual-mesh NoC designsWe compare CCNoC to dual-mesh network topologies proposed by Balfour and Dally [1]. As explained in Section V-A, the Homogeneous organization splits the trafﬁc across two identical networks to maximize the load balance and thus reduce network  congestion. The  Heterogeneous design  uses a narrow network to transport short messages and a wide network to transport long messages.Figure 8 shows that CCNoC consumes 11% and 18% less power than Homogeneous and Heterogeneous organizations, respectively, across our server workloads. The gains are largely due to the efﬁcient buffer architecture of CCNoC. Compared to other dual-network designs, CCNoC features lower VC and buffer requirements across the two networks reducing dynamic and leakage power draw by 44%, on average. Compared to trafﬁc across the two networks and achieves a perfect load balance. In comparison, CCNoC and Heterogeneous designs achieve a load balance ratio of 0.55 and 0.33, respectively, across the two networks. CCNoC signiﬁcantly improves the load balance over the Heterogeneous design due to the pres- ence of long request (dirty block eviction) messages, which comprise 6% of the request trafﬁc on average. In a CCNoC system, these multi-ﬂit messages travel on the narrow request network, which improves its utilization. As dirty block evic- tions are often not on the critical path, the impact on the system performance is negligible. In contrast, long requests must traverse the wide network in a Heterogeneous design, which diminishes load balance and consumes more power.In terms of latency, CCNoC bests other dual-network de- signs due to its efﬁcient architecture. The majority of long messages traverse the wide response CCNoC, which improves performance compared to the narrower networks in the Ho- mogeneous design. The performance is also improved against other dual-network organizations thanks to the shallower 2- cycle router pipeline in the CCNoC response network.Figure 10 plots the system performance of the dual-mesh topologies normalized to Homogeneous. CCNoC slightly out- performs both Homogeneous and Heterogeneous NoC designs by 4% and 3%, respectively, thanks to its architecture that specializes the networks to trafﬁc type.Area analysisFigure 11 plots the NoC area for various single- and dual- network designs. Compared to designs with same bisection bandwidth, CCNoC reduces network area by 31-39%. As buffers occupy 52-58% of the area in single- and dual-network3.02.52.01.51.00.50.01.61.20.80.40.0 Links  Buffers  CrossbarMesh-176	Mesh-128	Homogeneous    Heterogeneous	CCNoCFigure 11: NoC area for various network topologies.Mesh-176	Homogeneous	Heterogeneous The ﬁgure shows that CCNoC improves the energy-delay product by 43%, 23%, and 28% compared to Mesh-176, Homogeneous, and Heterogeneous.Our ﬁndings show that an asymmetric dual-network topol- ogy which splits the network trafﬁc according to the coherence protocol patterns is superior to other single- or dual-network topologies. In particular, across our benchmark suite, CCNoC is more energy (area) efﬁcient compared to single- and dual- network topologies by 15-28% (31-39%).RELATED WORKThe MIT RAW architecture [22] uses four symmetric NoCs (two static networks and two dynamic) which use packet- switched ﬂow control. The Tilera chip [20] extends the MIT RAW architecture and uses ﬁve identical wormhole-routed net- works to isolate: (a) communication to different sub-systems,(b) memory trafﬁc, and (c) user-speciﬁed trafﬁc. In contrast, CCNoC requires only a pair  of  networks to  divide  packets at  the  protocol  level.  In  addition,  the  networks  in CCNoCare  asymmetric, yielding  greater efﬁciency and  performanceFigure 12: Energy-delay product for server workloads (averaged) andthe multiprogrammed workload for various networks.designs, the gains are largely due to the efﬁcient buffer architecture of CCNoC. In particular, CCNoC reduces the buffer area by 55% compared to Mesh-176, Homogeneous, and Heterogeneous designs. Compared to the single-network Mesh-176 design, all three dual-network organizations are effective in reducing crossbar area by 48-50%.Compared to a single-network NoC design with smaller bisection bandwidth (i.e., Mesh-128), CCNoC is able to reduce the NoC area by 10%. The gains are largely due to the efﬁcient buffer architecture of CCNoC which offsets the increase in the link area. CCNoC reduces the buffer area by 38%. As crossbar area is proportional to the square of the datapath width, the cost of adding a narrow network is relatively low; as such, CCNoC and Mesh-128 feature similar crossbar footprints.SummaryWe summarize our results by calculating the energy-delay product of various network topologies which have same bi- section bandwidth. We calculate  the  energy-delay  product  of CCNoC, Mesh-176, Homogeneous, and Heterogeneous designs as the product of the total network energy and the CPI (i.e., the inverse of IPC). Figure 12 illustrates the energy-delay product of all network organizations normalized to CCNoC (i.e., dashed line). Across the server workloads, Mesh-176, Homogeneous, and Heterogeneous achieve 37%, 16%, and 21% higher energy-delay product compared to CCNoC.In workloads with lower network utilization, such as the multi-programmed workload, leakage power constitutes a much higher fraction of the overall buffer power. Because the combined storage footprint of CCNoC networks is lower than that of conventional organizations, CCNoC offers a signiﬁcant reduction in buffer power. Thus, CCNoC achieves higher network power savings when network utilization is lower. through specialization.Balfour and Dally [1] propose  splitting  network  trafﬁc into two heterogeneous or homogeneous networks to improve performance and power efﬁciency. The former splits trafﬁc based on message size and the latter strives for load balance across the two networks. Using simple read/write messaging protocols, the authors conclude that the homogeneous design is preferred to  a heterogeneous. In this  paper, we  show that  a heterogeneous design which splits network trafﬁc based on message class (request, response) leads to better performance and power efﬁciency than a homogeneous design for the case of cache-coherent CMPs, as it requires fewer VCs for deadlock avoidance. In addition, we show that a design which splits network trafﬁc based on message class rather than message size leads to better performance and power efﬁciency, as it achieves better load balance and requires fewer VCs.Yoon et al. propose using four networks - one for each message class of the  MOESI  protocol  -  as  an  alternative  to using virtual channels [25]. In contrast, the dual-network hybrid organization, proposed in our work, makes better use of wire resources by requiring fewer networks, improves network load balance, and boosts performance under a ﬁxed wire budget by supporting a wider response network as compared to a design with multiple dedicated NoCs.Manevich et al. propose using a hybrid NoC architecture with a bus to broadcast the short commands of the coherence protocol [12]. This work can be considered similar to the heterogeneous approach of Balfour and Dally  [1],  except that it relies on a bus for the short messages instead of a second network. While appealing for CMPs integrating a small number of cores, a bus-based architecture is hard to scale to many-core conﬁgurations that are likely in future server chips. Prior work has proposed multi-NoC interconnects where one NoC is packet-switched (used for non-localized trafﬁc) and the other is circuit-switched (used for localized trafﬁc) [14]. This optimization targets applications with localizedtrafﬁc patterns and is not applicable to server workloads running on cache-coherent CMPs, as the network trafﬁc is uniformly distributed.Higher radix topologies [4] are considered a viable approach for reducing NoC power consumption. These topologies rely on rich physical connectivity to eliminate a fraction of router traversals. However, the duality-based concept is orthogonal  to the network topology and hence the CCNoC concept can  be extended to higher radix topologies.Much research has focused on reducing router buffer power which accounts for a substantial fraction of overall NoC power. The techniques range from those that target a more efﬁcient implementation of buffers [13], bypass buffers [23], or eliminate buffers all together [15]. While many of these techniques increase complexity, they are equally applicable and complementary to CCNoC, as CCNoC targets both buffer and crossbar power consumption.CONCLUSIONServer chips are increasingly relying on large number of low-complexity cores to achieve power and performance scal- ability. In these manycore designs, communication power is becoming a signiﬁcant fraction of total chip power, calling for improvement in NoC efﬁciency.In this work, we introduced Cache-Coherence Network- on-Chip (CCNoC), a heterogeneous dual-network design that capitalizes on the duality in on-chip network trafﬁc observed in cache-coherent multiprocessors and optimizes the routing nodes accordingly. CCNoC employs two asymmetric networks with different datapath widths and  router microarchitectures to separate requests and responses. The specialization allows signiﬁcant power savings with no impact on performance. Through full-system simulation, we showed that CCNoC is more energy efﬁcient than single- and dual-network topologies by 28% and 15%, respectively.ACKNOWLEDGEMENTSThe authors would like  to thank the members of PARSA   at EPFL and the anonymous reviewers for their feedback on drafts of this paper. This work was supported by Eurocloud, Project No 247779 of the European Commission 7th RTD Framework Programme Information and Communication Technologies: Computing Systems, the Cyprus Research Pro- motion Foundations Grant TΠE/ΠΛHPO/0609(BIE)/09 (co- funded by the Republic of Cyprus and the European Regional Development Fund), and the project AdG-246810-NANOSYS of the European Research Council."Efficient Timing Channel Protection for On-Chip Networks.,"On-chip network is often dynamically shared among applications that are concurrently running on a chip-multiprocessor (CMP). In general, such shared resources imply that applications can affect each other's timing characteristics through interference in shared resources. For example, in on-chip networks, multiple flows can compete for links and buffers. We show that this interference is an attack vector through which a malicious application may be able to infer data-dependent information about other applications (side channel attacks), or two applications can exchange information covertly when direct communications are prohibited (covert channel attacks). To prevent these timing channel attacks, we propose an efficient scheme which uses priority-based arbitration and a static limit mechanism to provide one-way information-leak protection. The proposed technique requires minimal changes to the router hardware. The simulation results show that the protection scheme effectively eliminates a timing channel from high-security to low-security domains with minimal performance overheads for realistic traffic patterns.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Efﬁcient Timing Channel Protection for On-Chip Networks Yao Wang and G. Edward Suh Computer Systems Laboratory Cornell University Ithaca, NY 14853, USA {yao, suh}@csl.cornell.edu Abstract—On-chip network is often dynamically shared among applications that are concurrently running on a chipmultiprocessor (CMP). In general, such shared resources imply that applications can affect each other’s timing characteristics through interference in shared resources. For example, in onchip networks, multiple ﬂows can compete for links and buffers. We show that this interference is an attack vector through which a malicious application may be able to infer datadependent information about other applications (side channel attacks), or two applications can exchange information covertly when direct communications are prohibited (covert channel attacks). To prevent these timing channel attacks, we propose an efﬁcient scheme which uses priority-based arbitration and a static limit mechanism to provide one-way information-leak protection. The proposed technique requires minimal changes to the router hardware. The simulation results show that the protection scheme effectively eliminates a timing channel from high-security to low-security domains with minimal performance overheads for realistic trafﬁc patterns. Keywords-on-chip network, security, side channel, covert channel I . IN TRODUC T ION Future computing systems are expected to integrate a large number of processing and memory elements on a die in order to continue scaling the overall throughput within a limited power budget. Today’s processors often contain two to eight cores on a chip. More specialized processors such as the Intel Single-chip Cloud Computer (SCC) [7] or the Tilera TILE64 family [14] already contain several tens of cores. For efﬁcient communications, such many-core systems are likely to rely on an on-chip interconnect network and often called network-on-chip (NOC). To fully utilize a large number of processing elements, many-core systems need to execute a number of parallel workloads and share resources dynamically. For example, cloud computing infrastructures may support multiple virtual machines with shared physical resources. Therefore, applications on a NOC system can interfere with one another’s execution through contention on the network; an application’s communication trafﬁc may experience contention and delays that would not have occurred if each application had been run individually. The network contention over shared channels is obviously a concern from the performance, and also introduces fairness and quality-of-service issues, which have been studied recently [11], [5]. In this paper, we study the implications of the shared on-chip network from a security point of view, namely information leak through network interference, and proposes an efﬁcient protection mechanism. In general, the latency and throughput variations from network interferences can be used as timing channels by an attacker either to infer conﬁdential information from a protected high-security program (side channel attacks) or to have a malicious program deliberately leak information covertly when direct communication channels are protected (covert channel attacks). While a potential denial-of-service (DoS) attack on the network has been studied, to the best of our knowledge, this paper represents the ﬁrst study on timing channels. Handling of both side channel and covert channel concerns is critical for systems that require high levels of assurance. For example, consider cloud computing infrastructures that allow virtual machines from multiple customers to share physical hardware. In order for such systems to be viable for business or military customers, a system must provide an assurance that critical trade secret cannot be leaked. In fact, today’s cloud computing service contracts may prevent a service provider from sharing physical systems among multiple customers to handle such a concern. Similarly, safety critical systems such as an automobile engine controller cannot make use of multi-core platforms for multiple tasks unless there is a strong guarantee of isolation in order to meet timing deadlines. Unfortunately, eliminating the network interference can result in a signiﬁcant performance degradation due to inefﬁcient resource management even when there is no malicious trafﬁc. For example, static partitioning implies that a network can never be fully utilized by one program. In essence, non-interference requires that resource allocation decisions should be independent from application demands. In this paper, we present an efﬁcient solution to remove the timing channel based on the observation that practical systems often need to prevent only one-way information ﬂows from high security to low security levels. In this case, we found that the network capacity can be dynamically allocated based on application demands without a security concern by giving a strict priority to the low-security trafﬁc. The priority ensures that the low-security trafﬁc is not affected by the high-security trafﬁc demands. The priority 978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.24 142 (a) Network and ﬂow setup. (a) A platform setup. m a r g o r P r e k c a t t A ) i l i t n s s e n c o y c a 0 e 0 0 0 1 0 0 1 t i ( / r (b) A’s throughput, varying B’s demand. (c) B’s throughput, varying A’s demand. Figure 1. A simple network-interference example. e t u c e x e o t e m i T 280 270 260 250 240 230 220 0 0.2 0.4 0.6 Fraction of bit ’1’ in RSA key 0.8 1 scheme can be easily incorporated into a network router in a manner similar to traditional quality-of-service (QoS) mechanisms. To prevent the low-security trafﬁc to cause a denial-of-service (DoS) attack, we throttle the low-security trafﬁc based on conditions that do not depend on the highsecurity trafﬁc. For example, the network can enforce a static bandwidth limit on the low-security trafﬁc. Simulation studies show that the proposed technique indeed eliminates one-way information ﬂows from highsecurity to low-security levels. Also, the technique only requires minor changes to traditional router designs to add a simple priority-based arbitration and a static virtual channel allocation along with a bandwidth limit. Moreover, the simulation results indicate that the performance overhead of the proposed protection can be minimal, especially when communication demands change over time. The technique allows high-security trafﬁc to fully utilize the network capacity as long as there is no low-security trafﬁc. Similarly, the low-security trafﬁc can utilize the network up to its limit. The rest of the paper is organized as follows. Section II discusses the timing channel problem in on-chip networks. Section III and Section IV present simple static partitioning approaches as well as our proposal to address this problem. Section V evaluates the effectiveness and performance overheads of the protection techniques. Section VI discusses related works, and Section VII presents our conclusions. I I . T IM ING CHANN E L S IN ON -CH I P N E TWORK S A. On-chip Network Interference Network interference happens when ﬂows from multiple applications contend for shared resources such as links and buffers. As an example, Figure 1(a) illustrates a simple scenario where two ﬂows, Flow A and Flow B, share a common link between Node 1 and 2 on a typical 1-D mesh with 4 virtual channels. Because only one ﬂow can use (b) Attack program’s execution time. Figure 2. A side-channel attack on RSA. the link in each cycle, the network performance of one ﬂow, such as throughput and latency, can be affected by the other ﬂow’s demand. Figure 1(b) and Figure 1(c) show the throughput of Flow A and B respectively as a function of the other ﬂow’s injection rate. In the experiments, we ﬁxed one ﬂow’s injection rate (0.2, 0.5, or 1 ﬂit/cycle) and measured its delivered throughput while varying the other ﬂow’s injection rate from 0 to 1 ﬂit/cycle. The experiments uses a cycle-level network simulator, named Darsim [12]. As shown in the ﬁgure, when a ﬂow has a low injection rate (0.2 ﬂit/cycle), its throughput is not signiﬁcantly affected by the other ﬂow’s injection rate because a round-robin arbitration allocates roughly half of the link capacity to each ﬂow, which is enough to satisfy the demand. However, when the injection rate is high (1 ﬂit/cycle), a ﬂow’s throughput is highly dependent on the other ﬂow’s injection rate. The network interference creates a potential timing channel where sensitive information can be leaked intentionally or unintentionally even when legitimate communication channels are disallowed. For example, a program with a high security clearance may covertly leak conﬁdential information to a low-security program by controlling the amount of network trafﬁc to reﬂect the secret (say, high demand for 1 and low demand for 0). The low-security program can obtain the secret by measuring the throughput or latency of its own ﬂow through the shared network. This covert channel allows the two programs to communicate covertly even when they are not allowed to explicitly send messages to each other or access the same memory locations. The timing channel can also be exploited to extract secrets from a program, which leaks information unintentionally. This scenario is often called a side channel. As an example, Figure 2(a) shows a case where an RSA algorithm (Core 143                   0) runs on a multi-core platform along with a malicious program (Core 1). RSA [13] is a public key cryptographic algorithm that is widely used to secure electronic communications through encryption or digital signatures. The core of the RSA algorithm performs a modulo multiplication of two large numbers (often 1024 or 2048 bits) depends on each bit in a secret key, and is shown to be prone to timing attacks [8]. Essentially, the algorithm examines each bits in the key and only performs a multiplication if the bit is 1. In our example, we studied a case where the multiplication operation in RSA causes additional network trafﬁc to the memory controller 2 (MC2) due to cache conﬂicts. Meanwhile, the attack program runs a loop which causes cache misses in every iteration, which also sends memory requests to MC2. Packets from the RSA program and the attack program share the output port of the crossbar and experience network interference. The experiments were performed using McSim [1], which provides timing models for a multi-core platform based on Intel’s Pin tool. Figure 2(b) shows the attack program’s execution time as the number of 1s in the RSA key varies. As shown in the ﬁgure, the execution time has a high correlation with the fraction of 1s in the key. This result suggests that the attacker can roughly estimate the number of 1s from its own execution time, which can greatly reduce the search space to ﬁnd the correct key. B. Security Model The objective of our timing channel protection technique is to prevent sensitive information ﬂows from a high-security domain to a low-security domain through network interference. In particular, our goal is to remove dynamic datadependent network interference. For example, an application in the low-security domain may still be able to observe that there is another application, but should not be able to observe run-time changes in the high-security domain. In this way, an on-chip network can still be shared among multiple applications. Instead of referring to individual applications, we use the term domain to refer to a set of applications that do not need to be isolated from each other. The security model is based on the traditional multi-level security model (MLS) for conﬁdentiality where information can ﬂow from a lower security level to a higher security level, but not in the other way. As we will discuss later, only preventing high-to-low information ﬂows instead of ﬂows in both directions is essential in enabling efﬁcient techniques that can allow dynamic resource management. Fortunately, the multi-level security model with the one-way protection is applicable to a wide range of applications. In fact, the model is the standard for military and is also widely adopted by industry. For example, the model can be applied to cloud computing by placing security-sensitive corporate virtual machines in a high-security level and common compute workloads in a low-security level, allowing more efﬁcient utilization. While our technique is applicable to multiple 144 (a) Baseline. (b) Spatial partitioning. Figure 3. Non-interference through spatial network partitioning. security levels, for simplicity, our discussion will focus on the case with two levels - a high-security level and a lowsecurity level. In this work, we assume that an attacker controls software on a subset of processing cores along with its placement and scheduling. We also assume that the attacker knows the characteristics of high-security programs including their placement, scheduling, and trafﬁc patterns. In that sense, we assume that attackers can cause its ﬂow to share the network resources with arbitrary ﬂows in the high-security domain. However, the attack program is assumed to be in the lowsecurity domain. Because he only has access to software, the attacker can only observe the network trafﬁc through the latency and the throughput of its own ﬂows. I I I . S TAT IC N E TWORK PART I T ION ING To remove a timing channel via on-chip networks, the communication latency and throughput of the low-security domain should be independent from the dynamic behavior of applications in the high-security domain. This independence condition implies that all shared network resources, both link bandwidth and buffer space, must be allocated between the security domains in a way that is independent from run-time demands. The most straightforward approach to achieve this goal is to statically partition resources. Spatial Network Partitioning (SNP) As shown in Figure 3, a network can be partitioned in space to remove interference. The example shows two security domains, A and B, where each domain is allocated with a subset of cores. In the baseline without any timing channel protection, some ﬂows from the two domains can use the same link using the traditional Y-X routing as shown in Figure 3(a). The interference can be removed by requiring each domain to only use the routers that are associated with the nodes that are allocated to itself as shown in Figure 3(b). Temporal Network Partitioning (TNP) Instead of restricting routers for each security domain, a more ﬂexible partitioning approach can allow sharing of the routers and links while still statically allocate their resources. In this approach, we statically allocate virtual channels (VCs) in each input port to security domains and use temporal scheduling to have security domains take pre-determined turns on a per-cycle basis. For example, each security domain can be allocated with a half of input virtual channels for exclusive use, and be allowed in the switch allocation and link traversal in every other cycle. Effectively, this scheme statically allocates a half of network resources to each domain. Strengths and Weaknesses Both spatial and temporal partitioning schemes eliminate network interference between the security domains. In fact, the static partitioning remove a timing channel in both directions, not only from highsecurity to low-security domains. However, both schemes can incur signiﬁcant performance overheads because resource allocation cannot be dynamically adjusted to match the actual demands from each security domain. For example, even if one security domain has a large bandwidth demand while the other domain has a very small demand, the simple partitioning only allows the domain with the large demand to use at most the half of the output bandwidth. Even if application’s trafﬁc patterns are known, the static partitioning cannot efﬁciently deal with bursts or changes in application phases. IV. ON E -WAY IN FORMAT ION L EAK PROT EC T ION : R EV ER S ED PR IOR I TY W I TH S TAT IC L IM I T S (RPSL ) In this section, we propose an efﬁcient mechanism to eliminate a timing channel from high-security to lowsecurity domain. As shown in the previous section, eliminating network interference in both directions suffers from inefﬁciency because resource managements cannot reﬂect run-time demands from either domain. On the other hand, protection in one direction allows at least the low-security domain’s demand to be used in resource allocation. A. Overview In the multi-level security model, the goal is to prevent information ﬂows from the high-security domain to the lowsecurity domain. To achieve this goal without statically allocating resources, our scheme uses a priority-based arbitration for resources such as the router crossbar along with static allocation of virtual channels. The basic idea is to assign a high priority to low-security trafﬁc so that its behavior is not affected by high-security trafﬁc. In other words, when ﬂows from two different security levels compete for the switch traversal, the low-security ﬂow always wins, in both input port and output port arbitrations in a separable allocator. To remove interference in buffers, virtual channels are statically allocated to each security domain. This static allocation removes head-of-line blocking between packets from the two different security domains. In this way, the low security-level ﬂows are not affected by the dynamic demands of the highsecurity domain. At the same time, the approach allows each security domain to use more network resources dynamically when the other domain has low demands. While assigning a higher priority to a lower-security domain can prevent the information leak without statically restricting network resources, the approach introduces an obvious concern for fairness. In fact, the strictly higher priority allows a malicious program in the low-security 145 r0 r1 r2 r3 Round-robin  Arbiter Round-robin  Arbiter r0 r1 r2 r3 G0 G1 G2 G3 h0 h1 h2 h3 h0h1h2h3 Round-robin  Arbiter G0 G1 G2 G3 (a) Input arbiter. (b) Output arbiter. Figure 4. A priority-based switch allocator design. domain to easily perform a Denial-of-Service (Dos) attack on high-security programs by sending packets at a high injection rate and occupying all network resources. To prevent the DoS attack, we add an additional mechanism that monitors and limits the trafﬁc amount of the low-security domain in a way that is independent from the demand from the high-security domain. This mechanism sets a static limit per port on the number of ﬂits that can be sent by the low-security domain over a certain interval. Once the limit is reached, the port does not send the low-security ﬂits until the next interval, allowing the high-security ﬂits to go through. Note that the static limit does not create a timing channel because it does not depend on the high-security trafﬁc. Also, the limit can change over time as long as it does not reﬂect sensitive information - network demands from the high-security level. The rest of the section discusses in detail how this overall approach can be efﬁciently realized in a modern router architecture, and how the technique differs from traditional quality-of-service schemes. B. Information Leak Protection To provide one-way information leak protection, our scheme requires a priority-based allocator that gives a high priority to a lower security trafﬁc. Here, we describe how this mechanism can be implemented based on a typical separable allocator, which consists of input arbiters and output arbiters [3], with statically allocated virtual channels. The input arbiter decides which virtual channel can use each input port to the crossbar, and the output arbiter decides which input port can use each output channel. For simplicity, we illustrate our design for a router with four virtual channel buffers per input port, which are numbered from 0 to 3. To avoid the interference from head-of-line blocking as discussed above, we statically allocate virtual channel 0 and 1 to the lowsecurity domain (domain A), and 2 and 3 to the high-security security domain (domain B). Figure 4(a) shows our design of an input arbiter. In the diagram, r0 -r3 are request signals from virtual channels, which compete for the input port to the crossbar. G0 -G3 O0 O1 O2 O3 O0 O1 O2 O3 I0 r0 D0 D1 I0 r1 r2 r3 Round-robin  Arbiter Round-robin  Arbiter G0 G1 G2 G3 h0 h1 h2 h3 r0 r1 r2 r3 O0 h0h1h2h3 Round-robin  Arbiter G0 G1 G2 G3 (a) Input arbiter. (b) Output arbiter. Figure 5. A switch allocator design with a static bandwidth limit. are grant signals indicating which virtual channel wins the input arbitration. The arbiter uses a round-robin arbitration among virtual channels within the same security domain, but gives a priority to the ﬁrst two virtual channels (0 and 1). A request from a low-priority channel (2 or 3) is given to the round-robin arbiter only if there is no request from the high-priority virtual channel (AND gates in the ﬁgure). Figure 4(b) shows our design of an output arbiter for a 4-by-4 crossbar. Here, r0 -r3 represent request signals from four input ports, and h0 -h3 are signals indicating whether the request signals are from the high priority virtual channels (’1’) or from the low priority virtual channels (’0’). Again, G0 -G3 are grant signals indicating which request wins the arbitration. Similar to the input arbiter design, the output arbiter masks a request signal from a low-priority virtual channel if there is any high-priority request. As shown in the ﬁgures, adding a priority to the switch arbitration is quite simple and only adds a couple of gate delays to the typical round-robin arbiter. C. Denial-of-Service Protection To prevent a potential DoS attack, our approach enforces a static limit on the number of low-security ﬂits that can use each input and output port over a certain interval. For this purpose, a counter is added to each input and output port and records the number of ﬂits from the low-security domain. The counter is compared to a static limit C , and resets every N cycles. C/N represents the maximum fraction of the network capacity that can be used by the low-security domain. For example, if C = 80 and N = 100, the lowsecurity ﬂows can at most send 80 ﬂits every 100 cycles. The remaining cycles can only be used by the high security ﬂows. The switch allocator design with this capability is shown in Figure 5. For the input arbiter, we add a signal I0 to indicate whether the low-security packets has reached its limit for this input port. The signal comes from the input port counter. In addition, we also add control signals from the output ports to prevent low-security ﬂows from exceeding the limit on each output port; Oi indicates whether the output port i reached the limit. Finally, D0 -D1 indicate the output port of each low-security ﬂow, which is used to select a proper output limit signal. For the input arbiter, a request from a low-security ﬂow is invalided (AND) once the limit is reached for either the input or output ports. Similarly, for the output arbiter, we add O0 to invalidate the requests from low-security ﬂows once the output limit is reached. D. Extension to More Security Domains It is relatively straightforward to extend the RPSL scheme to support more than two security domains. Suppose we have N security domains, and we rank them based on their security levels. The ﬂows from the lowest security domain get the highest priority to use the router crossbar and vice versa. In this way, the trafﬁc from the lower security domain is not interfered by the trafﬁc from the higher security domain, thus preventing information ﬂow from higher security domain to lower security domain. To extend to N security domains, RPSL needs N-1 static limits to avoid DoS attacks, one for each domain except for the highest security domain. The limits may specify the maximum bandwidth usage over a certain period for corresponding security domains or the maximum usage for each security level and below. The arbiter designs can be extended to support more priority levels and static limits with simple additions to the design for two security domains. We need N round-robin arbiters and similar priority control signals for input arbiter. For output arbiter, we need to add compare logic for h0 -h3 signals to decide which ﬂow has the highest priority. E. Comparison with QoS schemes The proposed scheme uses the priority-based allocation, which is a common technique in network quality-of-service (QoS) schemes. In fact, at a glance, the timing channel protection may seem similar to guaranteeing a certain level of services in terms of latency and throughput in the traditional QoS context. However, a closer inspection reveals that the QoS properties are insufﬁcient to prevent information leak through network interference. In particular, in a traditional QoS system, a ﬂow can utilize the full capacity of a network beyond its allocation if there is no competing ﬂow. However, such an optimization creates a timing channel in the context of security by reﬂecting the demand from the high-security domain. For example, the fact that a ﬂow can use more resources than its QoS allocation reveals that other (highpriority) ﬂows have low resource usage at the time. V. EVALUAT ION A. Experimental Setup We evaluate proposed timing channel protection scheme, named RPSL, compared to the baseline without protection and the static temporal network partitioning (TNP, see Section III). The simulations are performed by modifying Darsim [12], which is an open-source cycle-level NOC simulator. The network is conﬁgured to be a standard mesh 146 ) l e c y c / t i l f ( t u p h g u o r h t e g a r e v a A w l o F 1 0.9 0.8 0.7 0.6 0.5 0.4 0 Baseline TNP RPSL 20 Timeline/(1000 cycles) 40 60 80 100 Figure 8. Flow A’s throughput over time with varying demands from Flow B. Flow A: low security, Flow B: high security. (a) Flow A (low security). (b) Flow B (high security). Figure 6. The effectiveness of TNP on a simple example. (a) Flow A (low security). (b) Flow B (high security). Figure 7. The effectiveness of RPSL on a simple example. with four virtual channels per port, and uses iSLIP [3] as the baseline allocator. For TNP, we allocate half of the virtual channels and switch time slots to each security domain. For RPSL, we allocate half of the virtual channels to each domain while prioritizing the low-security ﬂows in arbitration. The static limit for the low-security domain in RPSL is set to be 80 ﬂits per 100 cycles. Each packet consists of eight data ﬂits and one head ﬂit. All our experiments are done with a warm-up period of 20000 cycles, followed by simulation of 100000 cycles. B. Security 1) Timing Channel Protection: We ﬁrst study the security of each protection scheme using a simple scenario, which is discussed in Section II (see Figure 1). In this scenario, two ﬂows share a link in the network, causing interference. In the experiments, we assign Flow A to the low-security domain and B to the high-security domain. Then, we observe the throughput of each ﬂow, with a ﬁxed injection rate, while varying the other ﬂow’s injection rate. Figure 6 and Figure 7 show the results for the two protection schemes, TNP and RPSL, respectively. The results shows that both schemes can eliminate the timing channel from the high-security domain to the low-security domain; the throughput of Flow A does not depend on the injection rate of Flow B. In fact, the throughput of both ﬂows are constant under the TNP scheme, indicating that the static partitioning prevents information ﬂows in both directions. On the other hand, Flow B’s throughput under RPSL is sensitive to the injection rate of Flow A because the technique only removes a timing channel in one direction. Another interesting observation from Figure 6 is that (a) Flow A (low security). (b) Flow B (high security). Figure 9. Flow throughputs with a static limit on the low-security domain. neither of the two ﬂows’ throughput can achieve higher than 0.5 ﬂit/cycle. This is the consequence of statically allocating virtual channels and time slots evenly between two ﬂows, so each of them can at most use half of the network bandwidth. In contrast, the results for RPSL in Figure 7 show that each ﬂow’s throughput can be higher than 0.5 ﬂit/cycle because the network resources can be dynamically allocated. For the experiments above, the injection rates of both ﬂows are kept constant during each simulation run in order to obtain the average throughput. However, in practice, an attack program will measure its throughput over time while keeping its injection rate high. The variations in the observed throughput is used to obtain information about the high-security domain’s behaviors. To mimic such an attack process, we also measured the dynamic change in the Flow A’s throughput while keeping Flow A’s injection rate at 1 ﬂit/cycle and varying the injection rate of Flow B over time. More speciﬁcally, we randomly change the injection rate of ﬂow B among 1/3, 1/2, and 1 ﬂit/cycle every 1000 cycles. Figure 8 shows the dynamic throughput changes of Flow A. As the results show, the throughput of Flow A varies signiﬁcantly for the baseline scheme, depending on the injection rate of Flow B. As expected, the throughput of Flow A stays constant under TNP and RPSL, which further illustrates the two schemes can provide informationleak protection against timing channel attacks. 2) DoS Protection for RPSL: To study the effectiveness of having the limit on the low-security domain, Figure 9 shows experimental results with a few different limits. In the ﬁrst experiment, we ﬁxed Flow A’s injection rate at 1 147             (a) Baseline (b) TNP (c) RPSL (limit = 0.8 ﬂit/cycle) Figure 11. Low-security (domain A) throughput as a function of the high-security (domain B) demand for the transpose trafﬁc pattern. (a) Baseline (b) TNP (c) RPSL (limit = 0.8 ﬂit/cycle) Figure 12. Low-security (domain A) throughput as a function of the high-security (domain B) demand for the hotspot trafﬁc pattern. effectively avoid DoS attacks. Also, the results suggest that the limit can be chosen to be an arbitrary value without introducing a timing channel. Therefore, the limit can be optimized to match the general performance demand of each domain. For example, if the high security domain also has a high bandwidth requirement, we can set the limit to a low value such as 0.2 ﬂit/cycle. In this way, the high secuirty domain can get at least 80% of the bandwdith while still maintaining the security guarantee. 3) Complex Trafﬁc Patterns: To evaluate the protection schemes under more complex interference patterns, we simulated more complicated trafﬁc patterns on a 6-by-6 mesh network with two security domains. As shown in Figure 10, Domain A, which is the low-security domain, was given one-fourth of the cores. We place the cores in contiguous locations because this minimizes the communication cost within a security domain. We simulated two trafﬁc patterns, namely transpose and hotspot. In transpose, a node communicates with the node that is symmetric with respect to the diagonal. In hotspot, all nodes communicate with a single hotspot node, which are marked red in Figure 10. We use Y-X routing for all conﬁgurations. As a result, ﬂows from different domains share some links in the network, potentially causing interference. As before, we plot the average throughput of one domain while varying the average injection rate of the other domain. Due to the space limit, we only show domain A’s throughput results, which show if there exists a timing channel from high-security to lowFigure 10. Experimental setup in the 6-by-6 mesh network. ﬂit/cycle, and measured its throughput while varying Flow B’s injection rate under three different limits for Flow A: 0.6, 0.8, and 1.0. Figure 9(a) shows that the throughput of Flow A stays constant and matches the speciﬁed limit, which conﬁrms the bandwidth limit for the low-security domain is effective and does not introduce a timing channel. Figure 9(b) shows that the throughput of Flow B while varying Flow A’s injection rate. As shown in the ﬁgure, Flow B’s throughput decreases with the increasing injection rate of Flow A. However, the bandwidth limit on Flow A allows Flow B’s to utilize the network even when Flow A’s injection rate is at 1 ﬂit/cycle. Also, note that when the limit is 1.0 ﬂit/cycle, which effectively means no limit, Flow A utilizes the full bandwidth of the network and Flow B’s throughput drops to zero, which illustrates a DoS attack. The experimental results show that the bandwidth limit can 148 l s e c y c n o i l l i m y r e v e s t i l f t d e c e n j I 3000 2500 2000 1500 1000 500 0 0 FFT OCEAN 20 40 Timeline/ (million cycles) 60 80 100 Figure 13. On-chip network trafﬁc for SPLASH-2 benchmarks. security domains. Figure 11 shows the results for the transpose trafﬁc. As expected, in the baseline scheme, the throughput of domain A (low security) changes with the injection rate of domain B (high security). In contrast, domain A’s throughput stays constant under both TNP and RPSL regardless of the injection rate of domain B. Therefore, both schemes provides an effective protection against timing channels. However, a distinct difference between the results of TNP and RPSL is that domain A achieves higher average throughput in RPSL than in TNP when domain A’s injection rate is high (0.5 or 1 ﬂit/cycle), which again shows the performance advantage of RPSL over TNP scheme. The results of the hotspot trafﬁc pattern in Figure 12 show similar trends. C. Performance To provide the capability of information-leak protection, we put restrictions on virtual channel usage and crossbar arbitration, which can potentially decrease network performance. To explore how much performance overhead is incurred, we simulated the three schemes (baseline, TNP, and RPSL with the low-security limit of 0.8) with transpose and hotspot trafﬁc patterns mentioned above. To mimic the real application behavior, we studied the network trafﬁc for program in the SPLASH-2 benchmark suite using McSim [1]. The experiments collected the trafﬁc statistics for each network ﬂow. From the study, we found that in most programs, the injection rates of network ﬂows vary signiﬁcantly over time. Figure 13 shows the injection rate of one network ﬂow for FFT and OCEAN benchmarks over time. As shown in the ﬁgure, the injection rate changes with time and shows a large dynamic range. Based on this observation, in our experiments, we randomly changed the network ﬂow’s individual injection rate every 1000 cycles, for both transpose and hotspot trafﬁc patterns. For the performance metric, we use actual throughput over throughput demand, which equals to the number of ﬂits received divided by the number of ﬂits injected. Our results for transpose and hotspot trafﬁc are shown in Figure 14(a) and Figure 14(b), respectively. For domain A, RPSL scheme achieves higher performance than that of the baseline scheme. This is because RPSL gives domain A a higher prority to use the crossbar. For domain B, the (a) Transpose. (b) Hotspot. Figure 14. Impacts of timing channel protection mechanisms on the network throughput. performance of RPSL is a slightly lower than that of the baseline. However, the aggregated performance of RPSL is comparable to the performance of the baseline scheme. For the hotspot pattern, we even see an performance improvement over the baseline scheme. Overall, the results show that RPSL has a small performance overhead over the baseline because RPSL can usually utilize available bandwidth at runtime. The only case that the available bandwidth is wasted in RPSL is when the low-security domain has reached its limit and the other domain is sending very little trafﬁc which is insufﬁcient to utilize the remaining bandwidth. Furthermore, the cap can be reconﬁgured to ﬁt the demand if this situation does happen. By contrast, the TNP scheme incurs a signiﬁcant performance overhead. In fact, the performance suffers in TNP scheme whenever one security domain’s demand on a single link exceeds a half of the capacity and the other domain’s demand is less than half. Therefore, TNP cannot allow each ﬂow to fully utilize the network even for a relatively short period for a burst. Although TNP can provide twoway information-leak protection, the performance overhead is likely to be too signiﬁcant to be practical in most applications. V I . R E LAT ED WORK A. Side-channel Protection Previous research has studied the possibility of side channel attacks caused by contention over other shared resources such as caches. Wang and Lee showed how SMT, control speculation, and shared caches can create such interference between threads and thereby promote side and covert channel attacks [15]. They then suggested architectural partitioning and randomization schemes to eliminate or decorrelate cache interference from data dependencies [16]. Kong also suggested software solutions to the cache interference problem [9]. However, the cache side-channel protection schemes cannot be directly applied to NOC interference. Also, the side-channel protection does not handle covert channels where a program intentionally leaks information through timing channels. 149             B. On-chip Network Security Regarding NOC security, previous work [4] has focused on preventing unauthorized memory operations and denialof-service. This paper studies a new security problem of timing channels, and show how to efﬁciently address the problem using a priority with a static limit. To the best of our knowledge, we are not aware of previous work on network timing channels. C. Quality of Service Many QoS techniques have been proposed to provide performance isolation and differentiated services to different network ﬂows. Virtual Clock [17] provides performance isolation by assigning virtual clock values to packets based on the reserved rates, and the virtual clock value is then used for crossbar arbitration, thus guaranteeing the reserved rates for each ﬂow. The idle resources are dynamically reallocated among ﬂows. Preemptive Virtual Clock [5] keeps track of each ﬂow’s bandwidth consumption and prioritizes packets based on the consumed bandwidth and established rate of service, thus providing guaranteed performance. Another QoS technique, Globally-Synchronized Frames [11], achieves performance isolation by reserving slots in a global frame for each ﬂow. A key difference between these QoS techniques and the RPSL scheme is that the QoS techniques traget at using network bandwidth efﬁciently and do not keep the available bandwidth unused if there is demand. In RPSL, to provide information-leak protection, the lower security-level ﬂow cannot use the available bandwidth once it exceeds the limit. D. Composability Due to interference between applications, the integration and veriﬁcation complexity of multi-processor SoC grows exponentially with the number of applications. Composability is proposed to remove all interference between applications so that each application can be veriﬁed individually without slow system-level simulation of all use cases. Unlike our RPSL security scheme which utilizes the one-way information leak protection, a composable system requires bi-directional non-interference between applications, thus incurring a larger performance or hardware overhead. An approach of achieving composability is not sharing any resources, which is expensive and mostly used by federated architectures in aerospace industries. Kopetz [10] and Hansson [6] proposed the Time Division Multiple Access (TDMA) approach, which schedules the network communication based on a static time table, similar to the TNP approach in our paper. As shown in our experiments, the TDMA approach suffers from large performance overhead compared to our RPSL scheme. Akesson [2] used an approach based on latency-rate servers. Although it overcomes some shortcomings of the TDMA approach, it adds signiﬁcant hardware to achieve the goal of composability. In summary, due to the fact that composability needs a bidirectional non-interference, the composability approaches have larger performance overhead than our RPSL scheme. Also, because the goal of composability is to reduce the complexity of system integration rather than removing timing channels, some composability approaches allow dynamically reconﬁguring the resources to fulﬁll the changing application behavior. As a result, these approaches are not secure from the information ﬂow perspective. V I I . CONC LU S ION In this paper, we proposed a security technique RPSL to prevent timing channel attacks caused by interference in onchip networks. We demonstrated that the network interference among applications executing concurrently on the same CMP can be used as timing channels to obtain conﬁdential information. The RPSL mechanism eliminates a one-way timing channel from high-security to low-security domains by assigning a high priority to the low-security domain. For complete elimination of timing channels, we also discussed two static network partitioning schemes in either space or time. These protection schemes are compared through cycleaccurate network simulations. The results suggest that both RPSL and static partitioning provide effective protection against timing channels, and the one-way protection through RPSL can be realized with minimal performance overheads. V I I I . ACKNOW L EDGM EN T S This work was partially supported by the National Science Foundation grants CNS-0746913, CNS-0708788, and CNS0905208, and an equipment donation from Intel Corporation. We thank Skyler Schneider and Kun Ki for their contributions to the experimental framework, and anonymous reviewers for constructive comments. "Transient and Permanent Error Control for High-End Multiprocessor Systems-on-Chip.,"High-end MPSoC systems with built-in high-radix topologies achieve good performance because of the improved connectivity and the reduced network diameter. In high-end MPSoC systems, fault tolerance support is becoming a compulsory feature. In this work, we propose a combined method to address permanent and transient link and router failures in those systems. The LBDRhr mechanism is proposed to tolerate permanent link failures in some popular high-radix topologies. The increased router complexity may lead to more transient router errors than routers using simple XY routing algorithm. We exploit the inherent information redundancy (IIR) in LBDRhr logic to manage transient errors in the network routers. Thorough analyses are provided to discover the appropriate internal nodes and the forbidden signal patterns for transient error detection. Simulation results show that LBDRhr logic can tolerate all of the permanent failure combinations of long-range links and 80% of links failures at short-range links. Case studies show that the error detection method based on the new IIR extraction method reduces the power consumption and the residual error rate by 33% and up to two orders of magnitude, respectively, compared to triple modular redundancy. The impact of network topologies on the efficiency of the detection mechanism has been examined in this work, as well.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Transient and Permanent Error Control for High-End  Multiprocessor Systems-on-Chip  Qiaoyan Yu1, José Cano2, José Flich2,  Paul Ampadu3  1Department of Electrical and Computer  Engineering  University of New Hampshire  Durham, NH, USA 03824  qiaoyan.yu@unh.edu  2Parallel Architectures Group  Department of Computer Engineering  Universitat Politècnica de València  46022 Valencia, Spain  jocare@gap.upv.es, jflich@disca.upv.es  3Department of Electrical and Computer  Engineering  University of Rochester  Rochester, NY, USA 14627  paul.ampadu@rochester.edu Abstract— High-end MPSoC systems with built-in high-radix  topologies achieve good performance because of the improved  connectivity and the reduced network diameter. In high-end MPSoC  systems, fault tolerance support is becoming a compulsory feature. In  this work, we propose a combined method to address permanent and  transient link and router failures in those systems. The LBDRhr  mechanism is proposed to tolerate permanent link failures in some  popular high-radix topologies. The increased router complexity may  lead to more transient router errors than routers using simple XY  routing algorithm. We exploit the inherent information redundancy  (IIR) in LBDRhr logic to manage transient errors in the network  routers. Thorough analyses are provided to discover the appropriate  internal nodes and the forbidden signal patterns for transient error  detection. Simulation results show that LBDRhr logic can tolerate all  of the permanent failure combinations of long-range links and 80%  of links failures at short-range links. Case studies show that the error  detection method based on the new IIR extraction method reduces  the power consumption and the residual error rate by 33% and up to  two orders of magnitude, respectively, compared to triple modular  redundancy. The impact of network topologies on the efficiency of  the detection mechanism has been examined in this work, as well.  Keywords-Networks-on-chip; transient error; permanent error;  information redundancy; arbiter; reliability; fault tolerant  INTRODUCTION  I.  Current multi-processor systems-on-chip (MPSoCs) are  usually formed by  tens of microprocessors and other  components (such as memories) in the same chip. As  technology advances, more and more components are included  in MPSoCs; hundreds of components are expected to integrate  into a single chip in the near future. One side effect of  increasing component densities  is  the need of  long  interconnections, which creates a well-known communication  bottleneck. Networks-on-Chip (NoCs) have demonstrated  potential to manage this communication problem [1, 2].   In MPSoCs, it is worth differentiating between applicationspecific MPSoCs and high-end MPSoCs. In the former (e.g.  Spidergon STNoC [3]), fully irregular topologies are derived as  the system design is totally customized for the applications. In  contrast, in high-end MPSoCs (e.g. Tilera [4]), regular  structures (2D mesh-based topologies) are used. In this paper  we focus on high-end MPSoCs.  Reliability is one of the most critical emerging challenges,  caused by ever increasing chip densities [5]. Nanoscale  978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.27 169 fabrication processes inevitably result in defective components,  which lead to permanent errors. Without a proper solution, a  simple failure may render the chip useless, impacting the yield  and cost of manufacturing. One common approach to handle  permanent errors in NoCs is to use fault-tolerant routing  algorithms, which assume that the permanently unusable links  and/or routers are isolated at the testing stage [6]. One recent  solution that offers compact routing implementation and high  fault tolerance is the Logic-Based Distributed Routing (LBDR)  approach [7]. Transient errors are becoming equally important.  As the critical charge of a capacitive node decreases with  technology scaling, the probability that a high-energy particle  strike flips the value in a storage node increases [8]. Moreover,  transient error rates in logic gates are expected to increase  because of higher frequencies and lower supply voltages [9].  Recently, inherent information redundancy (IIR) has been used  to manage transient errors in a NoC based on XY routing [10].   These two approaches combined (LBDR and IIR) can thus  be envisioned as a good solution for addressing both permanent  and transient errors in NoCs. However, as they were originally  proposed, both were not extensively applied to the topologies  and configurations for realistic applications. On one hand, the  LBDR approach is designed for 2D meshes where routers are  connected to one neighbor on each dimension and direction.  On the other hand, because the uniqueness of XY routing used  in [10], that approach is not suitable for the router designs with  more advanced routing solutions. Indeed, recently, different  topologies have been presented. 2-D meshes suffer from an  increasing diameter and a limited bisection bandwidth as the  system size increases. To address these issues, the new  topologies add extra links connecting routers in the 2D mesh,  thus providing additional connectivity and lower diameters.  Three advanced topologies are shown in Fig. 1. The initial 2D  mesh is the underlying topology.   Fig. 1 Advanced solutions for 2D-meshes.             B. Permanent Error Management  Permanent fault support in NoCs can be achieved by  means of  three main approaches: fault-tolerant routing  algorithms,  reconfiguration  techniques and component  redundancy. Routing algorithms for faulty NoCs have been  extensively  investigated  to support  irregular  topologies  derived from regular ones after permanent link failures. Some  solutions [19, 20] are implemented using routing tables,  suffering the scalability problems and routing costs derived  from them. On the contrary, other solutions are logic-based.  FDOR [21] is based on the idea of dividing an irregular mesh  topology into regular sub-meshes. Although a large variety of  irregular mesh topologies are supported, it does not offer full  coverage. The uLBDR approach [7], however, is proposed as  an efficient logic-based mechanism offering 100% coverage to  faulty 2D-meshes. The routing information is condensed into a  small set of bits distributed over the switches. Reconfiguration  techniques are focused on adequately updating the routing  tables whenever failures occur. ARIADNE [22]  is a  distributed reconfiguration solution for agnostic NoCs capable  of circumventing large numbers of simultaneous faults.  Compared with other solutions [23, 24], it guarantees higher  performance during normal operation since no virtual channels  are restricted to deterministic routing. Component redundancy  is the easiest way for providing fault tolerance, but needs to  consider the extra cost derived from redundant components.  To the best of our knowledge, there are no solutions that  are able to allow distributed routing implementation in faulty  high-radix topologies with no routing tables and minimum  logic. The LBDRhr approach we proposed in this work  replaces the routing tables with a small and bounded set of bits  that are finally hardwired, offering support for a great number  of irregular shapes derived from high-radix topologies. We  also exploit the inherent information redundancy in LBDRhr  logic to manage transient errors in high-end MPSoC systems.  III. LBDRHR MECHANISM FOR HIGH-RADIX TOPOLOGIES  A. LBDRhr Description  In this section we describe the LBDRhr mechanism, an  extension of LBDRx [25], which is able to tolerate permanent  link and router failures. We assume that links are bidirectional  in this work. As shown in Fig. 3, this method is implemented  with three basic logic blocks, each addressing different port  types: 3-hop, 2-hop and 1-hop ports. 3-hop ports are those  connected to long-range links that connect two routers with a  distance of 3-hops (through the basic 2D-mesh). 2-hop ports  are connected to links that connect to routers at 2-hop  distance. 1-hop ports are connected to 2D-mesh links.   LBDRhr uses a few configuration bits to store local  information about the neighboring links: 8 configuration bits  for routing purposes (Rne, Rnw, Ren, Res, Rwn, Rws, Rse,  and Rsw); 2 bits (DR1 and DR0) for two deroute options  (special cases) at every input port; and one connectivity bit per  output port (Cx for port X), which can be hardwired. With this  compact information per router, LBDRhr addresses fault  tolerance without using routing tables, achieving important  savings in critical aspects such as area, latency and power.   Fig. 2 Block diagram of proposed router.  In this work, we address fault tolerance for advanced  topologies by redesigning  the LBDR mechanism. The  proposed LBDRhr (LBDR for high-radix networks) contains  two virtual channels and  includes an adaptive routing  algorithm. We prove that LBDRhr is deadlock-free for any of  the high-radix topologies even when certain links and/or  routers permanently fail. We further couple the LBDRhr  mechanism with a new error control method to detect the  transient errors in LBDRhr logic. This new error control  method exploits the inherent information redundancy in  LBDRhr to significantly reduce the error control overhead. The  overview of the proposed router is shown in Fig. 2.   The remainder of this paper is organized as follows. The  related work is discussed in Section II. In Section III we  describe the new routing mechanism LBDRhr. The new  method to extract inherent information redundancy in the  LBDRhr approach is proposed in Section IV. Performance,  area and power consumption are evaluated in Section V.  Conclusions are provided in Section VI.  II. RELATED WORK  A. Transient Error Detection for NoC Router  Because of its temporary behavior, transient link errors can  be managed with various error control coding methods—such  as forward error correction (FEC), error detection combined  with automatic repeat request (ARQ) retransmission, and  hybrid ARQ (HARQ). ARQ and HARQ are not suitable for the  route computation unit because no retransmission is allowed  after the packet arrives in input FIFOs. Transient errors in NoC  links can be managed using error control coding (ECC) [1113]. FEC methods are also used in the data path of NoC routers  [14, 15]. Unfortunately, FEC techniques are only suitable for  linear systems. In a NoC router, the control path (in charge of  the routing of packets and the allocation of resources to  packets) is not a linear system. Triple-modular redundancy  (TMR) duplicating the unit under protection and selecting the  output through majority voting is attractive for its simplicity  and it has been applied to the router control path [16, 17].  Theoretically, TMR functions correctly when up to one-third of  the received copies are wrong. Furthermore, the potential errors  in the majority voter further reduce the effectiveness of the  TMR approach, particularly when the number of units being  protected is small [18]. In [10], the inherent information  redundancy in the router using XY routing algorithm is  investigated to reduce the packet loss and misrouting without  significantly sacrificing power and area overhead. It has been  shown that the residual error rate achieved by the inherent  information redundancy based error control method is smaller  than that of TMR.   170   Fig. 3 Diagram of LBDRhr  First of all, the logic computes the relative position of the  message’s destination. Note that packets forwarded to the  local port are excluded from the routing logic. With this logic,  the directions to take for the packet are computed. Signals  NNN’, SSS’, EEE’ and WWW’  indicate whether  the  destination is at least 3 hops in the give direction. Similarly,  the signals of 2 hops and 1 hop are computed.  In the first logic block, if the packet’s destination is at least  three hops along the same direction away from the current  router, then the corresponding 3-hop port is selected. A simple  AND gate per 3-hop output port is needed to AND both the  proper comparator signal and the connectivity bits (the  presence) of the 3-hop port (e.g., NNN’ signal and the Cnnn  bit). Notice that the logic may select more than one output port  as a candidate for routing. For example, in a NoC with the  flattened butterfly topology, a packet at router 0 (located in the  upper left corner) with destination at router 15 (located in the  lower right corner) can use both EEE and SSS output ports.  The second logic block deals with 2-hop ports. In this case,  3-hop ports have priority over 2-hop ports. Therefore, the 3hop signal is considered in this block (the 3-hop signal is  easily computed by ORing all the output signals from the 3hop logic block). The 2-hop block behaves similarly to the  previous one: 2-hop ports are selected if the destination is at  least two hops away along the intended direction. Directions  that are covered in this block are NN, EE, WW, and SS, for  the mesh with express channels, and NE, ES, WS, and WN,  for the diagonal mesh topology. A simple 4-entry AND gate is  required for every output port (ANDing the 3-hop signal, the  availability of the output port, and the combined outputs of the  comparators). Notice that 3-hop ports and 2-hop ports will  never be selected at the same time for the same packet.  Finally, the third logic block deals with 1-hop ports. In this  case, complexity is higher. The logic is replicated in two  blocks. The first block (1-hop-adap) is filtered by the 3-hop  and 2-hop signals. Thus, selecting 3-hop or 2-hop ports  prevents using 1-hop ports. The second block (1-hop-escape),  however, is not filtered out by the previous blocks and works  independently. The 1-hop-adap and 1-hop-escape blocks  consider both routing and deroute bits in the same manner  they are configured and used in [7]. Notice that these bits are  only used in the underlying 2D-mesh to provide non-minimal  path support. In particular, the logic forces messages to take a  non-minimal port whenever the second part of the logic fails  in routing the message.  B. Deadlock-free Routing  With the provided logic and with no further support,  deadlocks can easily form. In order to avoid deadlocks, and  still keeping the LBDRhr approach simple, we combine the  mechanism with a routing algorithm using two virtual  channels. The first virtual channel (VC0) is used to route  packets freely using minimal paths through long-range links  (3-hop and 2-hop ports). Also, packets can be routed through  1-hop links but taking into account both routing bits and  deroute bits. The second virtual channel, however, can be used  only for routing packets through 1-hop links and by taking  into account routing and deroute bits. Therefore,  the  underlying 2D-mesh topology must provide a deadlock-free  routing implementation.  The arbiter needs also to be codesigned with LBDRhr.  Notice that for some packets different routing options may be  available at the same time: some output ports through VC0  and some outputs through VC1. To avoid deadlocks, the  arbiter must filter out those routing options that could allow  packets moving from VC1 to VC0 again. That is, once a  packet moves to VC1 it will stay in VC1 along its route until it  reaches its destination. Therefore, VC1 implements an escape  path through the underlying 2D-mesh topology. When the  packet is at VC0, adaptive and escape options are available  because of the LBDRhr logic. The arbiter should give higher  priority to adaptive output ports. In the case the adaptive  output port is busy, the escape output port must be selected.  Finally, packets will be injected always through VC0 (to  maximize the flexibility).   In order to clarify how the arbiter and the virtual channels  are used, an example is shown in Fig. 4. The right part of the  figure represents VC1 (the escape layer) through a 2D-mesh  topology with some faulty links. On the other hand, the left  part represents VC0 (escape and adaptive layers) through the  previous faulty 2D-mesh  topology along with express  channels, where there also exist faulty express channels.  171 TABLE 1. Five request failures  Fig. 4 An example of routing using virtual channels.  As an example we route a packet from router 0 to router 15.  At a first sight, we observe that there exist several options in  both layers for reaching our destination.  On VC1, four escape paths are possible. From router 0 to  router 2 there is only one valid option, but at router 2 two  options are possible (through router 6 and through routers 3, 7  and 6). At router 10, there are two new options again (through  router 11 and through router 14). By combining these options,  up to four paths are possible. It is important to note that, when  router 7 is reached (through router 3), there is no minimal path  available for reaching router 15 because there is a faulty link  between routers 7 and 11. Therefore, at Router 7, a deroute  option is needed (in this case going West to router 6). At  router 6, it is possible to reach the destination without using  deroutes (through a minimal path) with the options previously  commented. Note that the sub-path using the deroute option is  marked with dashed line. Note also that routing in VC1 is  deadlock-free based on LBDR.   On VC0, the previous four escape paths and new four  adaptive paths are available. Concerning the adaptive paths,  the first two options share the path until router 10, but in this  case using 2-hop links (express channels). Options 3 and 4  share the path until router 8 using a 2-hop link. Therefore, in  this example the arbiter will select between the eight options  available through VC0. Remember that options with longer  links have priority over shorter ones, so at the end, the arbiter  will choose between the four adaptive paths available. Note  that a change between VC0 and VC1 directly depends on the  dynamic conditions of the network (explained previously) that  does not affect the conclusions drew from our example.  IV. PROPOSED IIR-BASED TRANSIENT ERROR  DETECTION  METHOD FOR LBDRHR LOGIC   A. Inherent Information Redundancy (IIR) Extraction  In this work, the forbidden signal patterns in routers are  regarded as inherent information redundancy. LBDRhr logic  determines the next-hop on the path for the packet at the  current node. If one or more logic gates in the LBDRhr logic  fail because of transient errors, the packet may be routed to  wrong directions, thus causing either additional latency or  deadlocks. For a router using XY routing, the route path  determination unit is simple: only one output request is valid  per each packet. As a result, detecting the number of valid  output requests can identify transient errors in the CMP unit.  Sigma-and-branch error detection [10] has been successfully  applied to routers using the XY routing algorithm.  To facilitate fault-tolerant routing, LBDRhr logic may  generate multiple valid requests to use the output ports.  Consequently, the method in [10] is not feasible. However, the  principle of inherent information redundancy extraction is  applicable to the LBDRhr-based routers. Different with [10],  we classify the forbidden signal patterns existing in the output  port requests into four general categories:  • Mute-Request: None of the output ports is requested when  a packet header flit arrives at the input port. As shown in  Table 1-case (e), the valid request (high logic value) is  muted by logic gate failures.  • Multiple-Request:  One or more non-valid requests are  flipped to be ‘valid’ requests, resulting in accessing two  opposite directions, as shown in Table 1-cases (b) and (c).  Those are illegal signal combinations, which can be used  to detect the malfunction of the LBDRhr logic.   Switched-Single-Request: A non-header flit arrives but  there is a request to build a input port-output port  connection. Or, the destination and the current nodes have  the same horizontal or vertical coordinate, nevertheless  the next-hop route deviates from horizontal or vertical  direction. As shown in Table 1-case (a), the transient error  in LBDRhr logic causes an non-reasonable request to  access the output port.   • Bidirectional-Switched-Request: The  request  to  the  intended output port is muted while another opposite  output port is requested, as shown in Table 1-case(d).   The summary of Table 1 is based on the routing  phenomena that two opposite directions cannot be enabled at  the same time. Without losing generality, we use the pair of  east and west ports to introduce the error detection approach  •  based on the different error scenarios shown in Table 1.  B. IIR-Based Error Detection for LBDRhr Logic  We propose to detect transient errors in each function unit.  As shown in Fig. 5, a CMP error detection unit is used to  detect the forbidden signal patterns in the coordinator  comparison unit; the multi-hop logic (MHL) error detection  unit is used to examine the transient errors in the 24 output  port request; the deroute error detection unit is used to identify  the wrong deroute path. Any of the detected errors invokes to  re-compute the direction for the next hop. The details for these  three error detection units are discussed in the following  subsections.   172     e t a R n o i t c e e t D r o r r E 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 EEE−WWW EE−WW E−W 4*4 5*5 10*10 20*20 Dimension of Mesh Network−on−Chip Fig. 5 Overview of proposed error detection method for LBDRhr logic.  Fig. 7 Error detection rate breakdown for each signal combination in CMP  1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 e a t R n o i t c e t e D r o r r E Single Error Double Errors Triple Errors 0.2 9 16 25 100 225 400 Number of Nodes in MPSoC Fig. 8 Error detection rate of the proposed CMP.  Fortunately, we discovered  the  third  type  inherent  information redundancy to detect the rest of request failure:  the information consistency between the internal node A (in  Fig. 6) and the CMP outputs. If the subtraction is not zero, E’  and W’ cannot both be zero because the value of A is either  larger than zero or less than zero. When the subtraction is  zero, neither E’ nor W’ can be zero because of the nature of  ‘>2’, ‘<-2’, ‘>1’, ‘<-1’, ‘>0’ and ‘<0’ comparison units.   Err3 = (A_n & (E’ | W’))  |  (A & E’_n & W’_n)           (8)  The three IIR-based error detection outputs are ORed to  obtain the overall error detection decision, ErrCMP, as shown in  Fig. 6(b). We verify the error detection coverage of the error  detection logic by flipping logic values of some gates in CMP  unit. The error detection rate is the ratio of the number of errordetectable cases over the number of the total error injection  cases. As shown in Fig. 7, no matter how the NoC size  changes, the error detection rate for E’ and W’ is 100% because  of the use of the internal node. Since the zero/non-zero  subtraction output does not contribute to detect the errors  causing wrong EE’, WW’, EEE’ and WWW’, only the  occurrence of opposite direction pairs helps to detect errors in  EE’, WW’, EEE’ and WWW’. Therefore, the error detection  rate for EE’, WW’, EEE’ and WWW’ is less than 1. A larger  NoC size is more likely to obtain a nonzero subtraction output  (i.e. A!=0). As a result, the proposed method achieves a higher  error detection rate in smaller NoCs, as shown in Fig. 7.   We exhaustively examined the impact of the number of  failure gates on the CMP error detection rate. As shown in  Fig. 8, the proposed method can successfully captures most of  the single errors. As the number of failure gates increases,  more errors occur on EE’-WW’ and EEE’-WWW’;  consequently, the error detection rate slightly decreases.  (a)                                (b)  Fig. 6 Coordinate comparison unit (a) without error detection and (b) with the  proposed error detection.   1) Error Detection for CMP Unit  The coordinate comparison module (CMP) is implemented  with a subtraction unit and six threshold comparison units (>2,  <-2, >1, <-1, >0 and <0), as shown in Fig. 6(a). Xcurr and  Xdest are the x coordinates for the current and destination  nodes, respectively. This CMP provides the prime signals for  3-hops, 2-hops and 1-hop units in the LBDRhr logic (shown in  Fig. 3). Similar to the CMP for XY routing, the opposite  directions cannot be enabled at the same time. As a result,  multiple-request error detection is still applicable in LBDRhr.  We use the expressions (1)-(3) to detect multiple-requests.  Err1.1 = WWW’ & EEE’             (1)  Err1.2 = WW’ & EE’              (2)  Err1.3= W’ & E’             (3)  The second inherent information redundancy in the threestage LBDRhr  logic  is  the  information downward  compatibility. If the subtraction output of Xcurr and Xdest is  greater than 2 (i.e. WWW’=1), WW’ and W’ should be true, as  well; otherwise, it indicates that the threshold comparison  units have errors. Meanwhile, we keep the forbidden signal  patterns of the opposite directions in mind to further constrain  the signal patterns among E’, W’, EE’, WW’, EEE’ and  WWW’, as expressed in (4)-(7):  Err2.1= WWW’ & !(WW’ & W’ & EE’_n & E’_n)             (4)  Err2.2 = EEE’ & !(EE’& E’ & WW’_n & W’_n)               (5)  Err 2.3= WW’ & !(W’ & E’_n)          (6)  Err2.4= EE’ & ! (E’ & W’_n)          (7)  in which, EE’_n, E’_n, WW’_n and W’_n are the inverted  values of EE’, E’, WW’ and W’, respectively. With  expressions (1)-(7), we can successfully identify the multiple  requests, rather than the switched single-request, bidirectional  switched-request and mute-request.   173                                           The inherent information redundancy (i) inspires us to use  the expression (9) to detect errors in the 3-hops logic.  Err3-hops = (NNE & SSW) | (EEN & SSW) | (EES & WWN) | (SSE &  NNW) | (NNN & SSS) | (EEE & WWW) | (NNE & NNW) | (SSW &  SSE) | (EEN & EES) | (EES & EEW) | (WWN & WWS)            (9)  However, this logic can only detect the errors for the  multiple request scenarios. As shown in Fig. 3, the connection  vector Cdest also plays an important role to determine the  possible route for the packet’s next hop. Given a network  topology, we can eliminate some 3-hops paths. With this prior  knowledge, we further detect the errors in the 3-hops logic.  For the 4x4 diagonal and mesh with express lanes topologies,  all outputs of the 3-hops logic are zero. This simplifies the  error detection and improves the error detection rate.   The inherent information redundancy (ii) and (iii) inspire  us to use the expression (10) to detect errors in the 2-hops unit.  Again, we also use the prior-knowledge of Cdest to detect  switched-request and mute-request.   Err2-hops = (NN & SS)  |  (EE & WW)  |  (NE & SW)  |  (SE  & NW) |  (NE & SE) |  (SW & SE)          (10)  Whenever any of the previous error detection is true, it will  invoke to re-compute the output port request. Fig. 11 shows  the error detection rate of the proposed method is above 0.8,  which maintains as the number of injected error increases. We  also examine the impact of different topologies on the error  detection rate. As shown in Fig. 11, the proposed method has  minor variation on the error detection rate. This is because an  all-zero Cdest vector simplifies the error detection circuit and  thus reduces the probability of detection circuit failure.  The  error detection efficiency for the 2-hops is slightly less than  that for 3-hops logic. The interesting phenomena we observed  was that the proposed method improves the error detection  rate of 2-hops logic as the number of error injected to the logic  increases. A similar approach is applied to detect the error in  1-hop logic. Because of the sufficient IIR, we achieve the  error detection rate of 1 in the 1-hop logic unit.   3) Error Detection for Deroute Logic   The deroute logic provides a non-minimal path in case the  minimal path is not available, because of permanent errors.  Based on the DR vector, the alternative paths are constructed  with regular logic. For example, the Nderoute, Sderoute, Ederoute and  Wderoute are expressed in (11)-(14).  Nderoute= ~DR[0] &~DR[1]         (11)  Ederoute= ~DR[0]& DR[1]          (12)  Wderoute =DR[0]&~DR[1]           (13)  Sderoute =DR[0]&DR[1]               (14)  The fact that the four directions are exclusive is regarded  as a new inherent information redundancy to detect errors in  the deroute logic. The simulation performed also considers the  impact of failures in the error detection circuit. As shown in  Fig. 13, the proposed error detection method can even tolerate  the errors in the error detection circuit, as long as the number of  errors injected is no more than four. Fig. 13 also shows that the  decrease of the error detection rate is negligible.                                 (a)                                                     (b)  Fig. 9 Possible directions in LBDRhr (a) 1- and 2-hop (b) 3-hop directions  Fig. 10 Pairs of the forbidden concurrent routing directions. The center point  is where the packet is currently located. Dash line and dot line are overlapped  with solid line on the N/S/W/E directions.  2) Error Detection for Hop Logic   The LBDRhr is designed to facilitate the multi-hop  routing, as shown in Fig. 9. In the XY routing, we found only  two pairs of opposite directions; however, in the LBDRhr  routing logic, we found ten pairs of opposite directions, each  one on the 180-degree line shown in Fig. 10. Although  adaptive routing algorithms can provide multiple routing  paths, two directions opposite to each other are not likely  offered at the same time. Consequently, we obtain three new  inherent information redundancies in the multi-hop logic  (MHL):  The opposite direction pairs (1)-(8) are eight strong  opposite cases. For example, in the pair of NNE-SSW, N is  opposite to S and E is opposite to W, therefore these two  constraints further ensure the impossible occurrence of NNE=1  and SSW=1.   i. As shown in Fig. 3, NN’, SS’, WW’ and EE’ are generated  with the use of the 3-hop signals. As a result, pair (1)  should inherently match to pair (9); similarly, pair (5)  should inherently match to pair (10) in Fig. 10.   ii. Other than the 180 degree line, the symmetric directions  belonging to different quadrants are also exclusive,  because part of the logic is exclusive. For example, NNE  and SSE cannot be true as the same time. To enable NNE,  the 2-hop north direction (i.e. NN’) has to be true, which  means 2-hop south direction (i.e. SS’) is not enabled. As a  result, NNE and SSE cannot be selected in the same router  port. Consequently, we have more forbidden signal  patterns to detect errors.   174                       1 2 3 4 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of Injected Errors E r r o r D e t c e i t n o R a t e Flattened Express Diagonal 0.7 1 2 3 4 0.75 0.8 0.85 0.9 0.95 Number of Injected Errors E r r o r D c e e t i t n o R a t e Diagonal Express Flattened 0.9997 1 2 3 4 5 6 0.9998 0.9998 0.9999 0.9999 1 Number of Errors Injected in Deroute logic E r r o r D e t c e i t n o R e a t Without Error Detection Circuit Failure With Error Detection Circuit Failure         Fig. 11. 3-hops logic error detection rate.                            Fig. 12. 2-hops logic error detection rate.               Fig. 13. Error detection rate of the deroute logic.  0 1 2 3 4 5 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Number of Errors Injected R a u d s e i l E r r o r R a t e TMR Proposed 10−8 10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−6 10−4 10−2 100 102 104 106 Gate Error Rate R a u d s e i l E r r o r R a t e o f R D B L h r c g o L i TMR Proposed Diagonal Express Flattened 0 0.2 0.4 0.6 0.8 1 Network Topologies E r r o r D c e e t i t n o R e a t TMR Proposed Fig. 14 Residual error rate comparison.           Fig. 15 Impact of gate error rate on LBDRhr reliability.       Fig. 16 Impact of network topologies on reliability.  V.  A. Experimental Setup  We evaluated the performance and overhead of the routers  without protection, using TMR, and using proposed methods.  The experiments were performed on topologies derived from a  4x4 mesh topology, TMR was applied on the entire LBDRhr  logic; each output port uses a majority voter to guess the most  likely output. The residual error rate is simulated on the gatelevel description of the LBDRhr logic. Each data point was  obtained from few million random simulations. Area, power  and delay are obtained based on the synthesized netlist from  Synopsys Design Compiler using a TSMC 65nm typical  technology and 1 GHz clock frequency.    EXPERIMENTAL RESULTS  B. Reliability  In this experiment, we randomly flipped the logic gate  value to simulate the transient error in LBDRhr logic. The  residual error rate is the ratio of the number of cases that the  error detection method cannot identify the error over the total  number of the error injection cases. We vary the number of the  error injected in this experiment. As shown in Fig. 14, the  residual error rate of the proposed method for 1-bit error is  higher  than that of TMR. This  is because TMR can  successfully tolerate 1-bit error, unless the error happens to the  majority voter. In contrast, the proposed method has limited  capability to detect the switched single-request and muterequest. However, the error detection circuit is not error free; if  the number of injected errors increases, our method reduces the  residual error rate by up to 90%. If the ratio of failed gates over  the total logic unit is fixed, our approach is better than TMR.  Because TMR has three times the logic gates as the original  design, more logic gates potentially encounter more gate  failures. As shown in Fig. 15, our method reduces the residual  error rate by two orders of magnitude over TMR.  In Fig. 16, we show the impact of network topologies on  the error detection rate. TMR has negligible impact by the  network topologies; our approach slightly varies the error  detection rate because we use the Cdest vector as redundancy  information to detect errors. The proposed method also  achieves a 30% higher error detection rate than the TMR.   C. Flit Throughput and Latency  For the next experiments we studied the behavior of the 3  proposed topologies using the gNoCsim simulator (an inhouse cycle accurate simulator). We first performed the  experiments without considering link failures, and then  incrementally add faulty links to each topology up to obtain  the underlying 2D-mesh. Fig. 17 compares  the  traffic  generated with the maximum mean value of traffic actually  received. In all of the cases, the flit throughput decreases with  the number of failures up to 0.33 flit/cycle/nic. We also  compared the traffic generated with the average network flit  latency at the first traffic point (low network load). As shown  in Fig. 18, a larger number of link failures leads to a higher  latency. Finally, we redid the experiments and additionally  considered a faulty underlying 2D-mesh. Similar results were  obtained. After examining all the failure combinations of longrange links for the 3 proposed topologies, we found that the  LBDRhr logic successfully facilitates all nodes to reach any  other nodes in the network; the topologies with a higher radix  offer better performance for a given number of failures. In  addition, as demonstrated in [7], 80% of the failure patterns of  the underlying 2D-mesh were covered by the LBDR approach.   D. Area  The TMR approach need three copies of the core elements  in LBDRhr, so consumes more than two times the area  compared to the LBDRhr logic without error detection. Our  method exploits the inherent information redundancy and does  not dramatically induce overhead, only causing a 6.1% area  overhead, as shown in Table 2.   175                                                                                                        ACKNOWLEDGMENTS  This work was supported by the Spanish MICINN,  Consolider Programme, as well as European Commission  FEDER funds, under Grant CSD2006-00046. It was also partly  supported by the project NaNoC (project label 248972), which  is funded by the European Commission within the Research  Programme FP7.  "Overlaid Mesh Topology Design and Deadlock Free Routing in Wireless Network-on-Chip.,"To bridge the widening gap between computation requirements of terascale application and communication efficiency faced by many-core processor chips, wireless Network-on-Chip (WiNoC) has been proposed by using the recently developed CMOS ultra wideband interconnection. In this research, we propose an unequal RF nodes overlaid mesh topology design to improve the on-chip communication performance. A network capacity model is developed for fast searching of optimal topology configuration. A high-efficient, low-cost zone-aided routing scheme is designed to facilitate deadlock freedom. The simulation study demonstrates topology modeling effectiveness, routing efficiency, and promising network performance of the overlaid mesh WiNoC over a regular 2D mesh baseline.","2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip Overlaid Mesh Topology Design and Deadlock Free Routing in Wireless Network-on-Chip Dan Zhao and Ruizhe Wu The Center for Advanced Computer Studies University of Louisiana at Lafayette Lafayette, LA 70503 {dzhao,rxw2563}@cacs.louisiana.edu Abstract—To bridge the widening gap between computation requirements of terascale application and communication efﬁciency faced by many-core processor chips, wireless Network-on-Chip (WiNoC) has been proposed by using the recently developed CMOS ultra wideband interconnection. In this research, we propose an unequal RF nodes overlaid mesh topology design to improve the on-chip communication performance. A network capacity model is developed for fast searching of optimal topology conﬁguration. A high-efﬁcient, low-cost zone-aided routing scheme is designed to facilitate deadlock freedom. The simulation study demonstrates topology modeling effectiveness, routing efﬁciency, and promising network performance of the overlaid mesh WiNoC over a regular 2D mesh baseline. Keywords-wireless network-on-chip; overlaid mesh topology; zone aided routing; octagon turn model; deadlock avoidance. I . INT RODUCT ION With silicon technology scaling, multi-processor chips (CMPs) are moving towards many-core structures to achieve energy-efﬁcient performance. Network-on-chips (NoCs) are in replace of conventional shared-bus architectures to provide scalable and energy-efﬁcient communication for CMPs using integrated switching network. In the meantime, RF/wireless interconnect technology [1]–[3] has emerged recently to address future global routing needs and surpass the fundamental limitation of hard-wired electronic interconnects. Among them, the UWB interconnect (UWB-I) by using the carrierless impulse-radio ultra wideband (UWB) technology [4], [5] brings in new opportunity for lowpower, high bandwidth, short-range communication. The high ﬂexibility and free-of-wiring make UWB-I an attractive solution for the on-chip inter-core communication. The UWB-I is based on transverse electromagnetic wave propagation by using on-chip antennas. Since its signal has very short pulse duration, high data rate with constant signalto-noise ratio is achieved by increasing the bandwidth. It consumes low power (e.g., a few milliwatts), attributed to its very low duty cycle (typically < 0.1%). With carrier free implementation, the RF circuits can be very simple. The pulse This research was supported in part by the National Science Foundation under grant number CNS-0821702 and CCF-0845983. position modulation is typically used to modulate a sequence of very sharp Gaussian monocycle pulses. The received signal and the template signal is automatically correlated during the demodulation. Various CMOS-integrated high transmission gain printed antennas such as linear dipole, meander dipole, folded dipole, zigzag dipole and loop antenna [6], have been proposed for on-chip implementation. Following ITRS projection, it is possible to build RF circuits operating at ∼20GH z , achieving a data rate of ∼20Gbps/band (with 1bps/H z bandwidth efﬁciency) in 32nm CMOS technology. With multiple bands, the aggregate data rate can be further improved for tera-scale computing. With RF technology scaling, the required RF circuitry and antenna sizes will scale down, tremendously reducing implementation cost and increasing on-chip interconnection ﬂexibility. Moreover, the energy consumption per bit is expected to scale down too. Table I [7] summarizes the UWB interconnect scalability which facilitates the fundamental architectural shift to many-core and tera-scale computing. Table I UWB I N T E R CONN E C T S CA L I NG Technology (nm) Cut-off freq. (GH z ) Data rate per band (Gbps) Dipole antenna length (mm) Meander type dipole antenna area (mm2 ) Power (mW ) Energy per bit (pJ ) 90 105 5.25 8.28 65 170 8.5 45 280 14 32 400 20 22 550 27.5 5.12 3.11 2.17 1.58 0.738 33 6 0.459 40 4.7 0.279 44 3.1 0.194 54 2.7 0.14 58 2.1 With the high bandwidth, low power and ultra-short range communication provided by UWB-I, the wireless radios are deployed on chip in replace of wires to establish a wireless Network-on-Chip (WiNoC). A WiNoC consists of a number of RF nodes, i.e., wireless routers, each associated with a processor tile. The RF node has a predetermined transmission range. The processor tiles access the network via RF nodes, and their packets are delivered to destinations through multi-hops across the network. Multiple channels are assigned among the nodes to ensure transmission paral978-0-7695-4677-3/12 $26.00 © 2012 IEEE DOI 10.1109/NOCS.2012.11 27 lelism and reduce channel contention. A node may receive packets from its neighbors fall within its transmission range along dedicated channels, and thus are connected by wireless links. A collection of RF nodes connected by high BW wireless links, forming WiNoC topology. RF node architecture mainly implements routing, channel arbitration, virtual output queuing and congestion control mechanisms for highspeed cost-efﬁcient on-chip communication. In this work, we design an overlaid mesh wireless NoC, where unequal RF nodes are dispersed on-chip as wireless routers to forward data (Sec. II-A). By conﬁguring the RF nodes placement, various topologies may be formed. An effective network capacity modeling scheme is designed for fast allocation of an optimal topology from a large searching space (Sec. II-B). Beneﬁted from long link transmission, we further develop a simple and efﬁcient logic-based zoneaided routing scheme for distributed and deadlock-free routing (Sec. III-A, Sec. III-B). Routing efﬁciency is further improved by an enhancement technique which greatly improves transmission concurrency by evenly distributed trafﬁc density. The thus induced deadlock problem is resolved by a simple buffer ordering scheme. (Sec. III-C). I I . OVERLA ID M E SH TO POLOGY D E S IGN A. Architectural Topology Design To improve network performance of WiNoC where the RF nodes are placed in a regular mesh, unequal RF nodes, i.e., wireless routers, are deployed in a way that small RF nodes (with shorter transmission range T and lower link bandwidth) form a mesh while big RF nodes (distributed at distance of nT with longer transmission range of 2nT and higher link bandwidth) are overlaid to form a fully connected mesh as shown in Fig. 1. As thus, we introduce two types of meshes, a base mesh which is simply a regular 2D mesh formed by both small and big nodes, and a full mesh formed only by big nodes where the big nodes within a grid are fully connected to each other. Further, the big node has starry ends constituting of unidirectional direct links to the nearby small nodes which fall within its transmission range. √ 14 14 A 24 14 D 30 12 E F 32 12 G 30 16 14 H 24 12 C 14 14 B The overall topology is a ﬂat structure with both short and long wireless links formed by small and big RF nodes. It looks like a full mesh overlaid on a base mesh, thus we simply name it an overlaid mesh topology. It’s worth mentioning that more channel bandwidth is allocated to the big nodes in order to alleviate denser trafﬁc congestions at the big nodes (due to higher node radix). An overlaid mesh potentially improves WiNoC performance from two aspects, reduced hop count with long range wireless links and reduced trafﬁc congestion due to efﬁcient trafﬁc distribution. On one hand, routing cost can be reduced due to signiﬁcant reduction in the hop count by using long links as much as possible in overlaid mesh. The direct links from the big nodes to its nearby small nodes further reduce the hop count. For example, as shown in Fig. 1, only two hops are needed to deliver a packet from A to B via C while it requires a total of 7 hops to route the packet along a regular 2D mesh which results in 71% hop count reduction. On the other hand, using long links may relief the trafﬁc congestion problem in the network specially for steaming data ﬂows. For instance, considering all-toall communication in a regular 2D mesh and an overlaid mesh, respectively. Without loss of generality, we compare the trafﬁc density on selected links after applying shortest path routing for all-to-all communication. As we can see in a regular 2D mesh, the trafﬁc hot spots may be formed at nodes E, F, G and H while the trafﬁc is more evenly distributed in the overlaid mesh. B. Overlaid Mesh Topology Conﬁguration and Optimization In an overlaid mesh, big RF nodes are placed in a way to tradeoff between routing path cost and network congestion. For a N × N WiNoC with big nodes deployed at distance of nT , we may generate several different topologies by changing the big nodes placement distance. When increasing n (i.e., big nodes are separated farther), a packet may be delivered to the destination with less hops by using longer links formed by big nodes. However, as farther separated big nodes have higher radix (due to more starry ends), the trafﬁc would be more congested at big nodes, thus increasing the end-to-end delay. In the meantime, with less number of big nodes in the network, the number of channels needed may be reduced. As a result, per-link BW may be improved1. It becomes essential to study the impact of big nodes placement and the corresponding number of big nodes on topology formation, and consequently the impact on network performance. Various conﬁgurations of overlaid mesh topology can be formed with varying big nodes placement for a given network scale. It is important to ﬁnd out an optimal or near-optimal conﬁguration which results in the best possible Figure 1. Illustration of an overlaid mesh WiNoC. 1With a single band implementation, the overall bandwidth will be split among the number of channels assigned in the system. 28 network performance at given network scale. Such a topology design problem is basically an big nodes placement problem. As big nodes may likely form trafﬁc hot spots in the network, they will be distributed at the center of their neighboring small nodes where their starry ends may reach out as equally as possible. Now the question is: which placement may result in the best possible network capacity? We derive an efﬁciency network capacity modeling scheme to fast approach an optimal topology conﬁguration without running comprehensive WiNoC network simulation. Network Capacity Modeling: Considering a wireless link lAB with node radix of nA (or nB ) for node A (or B ), the possibility of transmitting a packet between A and B is nA ·nB . Assume that m paths are routed through PAB = it from n1 sources S = {S1 , S2 , ...Si ..., Sn1 }, i ∈ n1 to n2 destinations D = {D1 , D2 , ...Dj ..., Dn2 }, j ∈ n2 . 1 Considering the worst case trafﬁc contention in the collision domain of nodes A and B where all neighbors are involved in potential data transmission and only one transmission is allowed per frame time to avoid collision, the expected transmission time on link lAB for m paths passing through it would be ET T (m) = m · nA · nB · F rameT ime. Thus the possibility of transmitting a packet for all m paths passing through link lAB is: PAB (m) = 1 m · nA · nB (1) BWAB A link capacity, i.e., the maximum packet carrying capacity of a link, is strictly coupled with the effective bandwidth of the wireless link by considering channel contention overhead. It is determined by C ap(lAB ) = PAB · BWAB = nA ·nB . When excessive packets ﬂow in a path Si → Di , at least one packet will be queued at the bottleneck wireless router and keeps the bottleneck router saturated. Thus pushing more packets into the path cannot improve the ﬂow’s along path Si → Di : throughput. We may ﬁnd the bottleneck capacity of a link Link C ap(Si → Di ) = min{ BWAiBi mj · nAi · nBi (2) where i ∈ any link lAiBi on path Si → Di and j ∈ all paths passing through lAiBi . Therefore, the minimum capacity of path Si → Di is determined by: } P ath C ap(Si → Di ) = H opC ountSi→Di × Link C ap(Si → Di ) (3) The overall network capacity can be simply calculated as the sum of all path capacity. N et C ap(N × N ) = i=1...N(cid:2) j=1...N P ath C ap(Si → Dj ) (4) Based on the above modeling, a simple and fast simulator is developed to estimate the overall network capacity under different topology conﬁguration. Under certain trafﬁc pattern such as uniform, the estimator will record the total number of trafﬁc ﬂows passing through each link and calculate the overall network capacity. The one which deliver the maximum capacity is chosen to be the optimized topology design under given network scale. I I I . DEADLOCK -FRE E OVERLA ID ROUT ING With the great freedom and large bandwidth offered by this emerging WiNoC technology, a deadlock-free logicbased routing scheme is developed for low latency on-chip application. Aiming at low-cost and high-efﬁcient implementation, we propose a zone-aided routing scheme, where the whole chip plane is virtually divided into several zones, each associated with one big node serving as the zone header and a group of small nodes. To take the advantage of overlaid mesh topology, routing is performed in a way to use long links as much as possible to shorten routing path lengths. The proposed routing scheme mainly includes three steps, virtual zone division, basic routing scheme, and enhancement techniques. A. Virtual Zone Division In order to efﬁciently utilize the long links in the full mesh, a small source node will ﬁrst deliver its packet to the closest big node. In other words, it will be shortestXY routed to a neighboring big node. All the small nodes which will forward their packets to the same big node will be grouped into one virtual zone with the big node as the header of this zone. The whole network can thus be divided into several virtual zones where the zone headers are located at the center of these zones. Note that some zones at the edge of the network are just in partial size. The zone division for a 8 × 8 WiNoC is illustrated in Fig. 2. Sh3 Dh3 Dh1 S3 D3 D1 Ih D2 S2 S1 Sh1 Figure 2. Illustration of zone aided routing. Assume the XY-coordinate of the left bottom node (1, 1) and the right top node (N , N ) in a N × N network with virtual zone size of n × n. For any small RF node S with its XY-coordinates (xS , yS ), a fast process is developed to determine its zone Z (XR , YR ) and zone header H (xB , yB ): 29 XR = (cid:6) xS n (cid:7); YR = (cid:6) yS (cid:6) n n (cid:7); xB = (cid:6) n 2 (cid:7) + (YR − 1)· n. For example, node A(7, 6) in Fig. 2 is located in zone Z (III, II) with its header H (8, 5). 2 (cid:7) + (XR − 1)· n; yB = B. Basic Routing Scheme Intuitively, the shortest-path routing can be performed on the overlaid mesh. However, the shortest-path routing involves high implementation cost in order to maintain the forwarding table at routers. Further, the shortest-path routes can easily form cyclic dependency, inducing deadlocks which are not easy to get resolved. The proposed zone-aided routing scheme facilitates simple and efﬁcient logic-based implementation and at the same time shortens the routing paths to the maximum possible by taking the advantage of long links in the overlaid mesh. Most importantly, it ensures deadlock freedom. Without loss of generality, the basic routing scheme makes the packet forwarding decision at the routers based on the source and destination nodes’ position in the zones. When we transmit packets between a source and destination pair, the source will ﬁrst check where the destination is located based on the destination’s address contained in the packet header. • If the source and destination nodes fall within the same zone, the packet is forwarded to the destination using XY-routing along the short links of the base mesh, such as the path S2 → D2 with 3 hops in Fig. 2. • If they belong to different zones, the packet is ﬁrst XY-routed to the source zone header (if the source is a small node). The packet is then routed along the full mesh with long links using turn-restricted shortestpath routing to reach destination zone header. More speciﬁcally, the tilted long links are used to forward the packet until reaching the header located on the same raw/column of the destination zone header. The packet is continuously forwarded along the horizontal/vertical long links towards the destination zone header. Finally, the packet directly hops to its destination small node from the destination zone header. For example, a packet generated at source S1 is ﬁrst XY-routed to the source header Sh1 and then routed to an intermediate header Ih located on the same column of destination header Dh1 . The packet is then vertically routed up to Dh1 and reaches the destination D1 via the direct link from Dh1 , resulting in a total of 5 hops. 1) Octagon Turn Model Design: To ensure deadlock free routing in full mesh, we propose a new octagon turn model as illustrated in Fig. 3. The model involves two abstract cycles, a clockwise cycle and a counter clockwise cycle, each formed by eight turns. It prohibits certain turns in each cycle to break all the cyclic dependencies. The routing employing the remaining turns prevents circular waiting on network resources, such as buffers or channels, and is thus deadlock free. N E → W , E → N W , SE → N , S → N E , and N → SE , N W → E , W → N E , SW → N , S → N W , Essentially, a turn involves a 135 degree change of clockwise cycle are W → SE , N W → S , N → SW , traveling direction. The eight types of turns deﬁned in the SW → E . Similarly, the other eight types of turns deﬁned in the counter clockwise cycle are E → SW , N E → S , and SE → W . As thus, two rules are restrictively followed by the octagon turn model. Rule 1. Any packet is not allowed to make the four turns i.e., W → SE , N → SW , E → N W , and S → N E at a node as in the clockwise abstract cycle. Rule 2. Any packet is not allowed to make the four turns i.e., N E → S , N W → E , SW → N , and SE → W at a node as in the counter clockwise abstract cycle. Figure 3. The octagon turn model. 2) Turn-Restricted Shortest-Path Routing: The turnrestricted shortest-path routing is performed in full mesh to deliver packets between big nodes or the zone headers. As designed in the above basic routing scheme, all 45degree and 90-degree turns are prohibited when routing in full mesh besides the prohibited turns designated in the octagon turn model. The 0-degree and 180-degree turns are incorporated without introducing cycles. The routing is minimal as it routes a packet via a shortest path between a source-destination pair. The turn-restricted shortest-path routing is deadlock free as it follows the rules of octagon turn model and prohibits all 45-degree and 90-degree turns in full mesh. While the XY-routing on the base mesh and the turnrestricted shortest-path routing on the full mesh are deadlock free respectively, it can be derived that the basic zone-aided routing scheme in overlaid mesh is deadlock free by contradiction. Assume a set of packets form a resource usage dependency cycle. The formed waiting path must interweave all types of routing paths. According to zone-aided routing, short links for routing within the zone and long links for traversing the zones. So any packet may enter and traverse the full mesh at most once. In order to form a cycle, the waiting path must include a deadlocked path segment where a packet gets off the full mesh and is forwarded to the small destination node. However, when a packet leaves the full mesh, it is consumed immediately by the destination small node via a direct link from the destination’s zone header. So no circular waits will be formed. 30 C. Routing Efﬁciency Enhancement Applying the basic routing scheme reduces routing cost in terms of hop count by using long links as much as possible, which however may cause severe trafﬁc congestion at big nodes. In order to achieve more even trafﬁc distribution in a network so as to alleviate trafﬁc congestion at big nodes, we propose a routing enhancement technique. The basic idea is that if any pair of source and destination are not located in the same zone while their Manhattan distance |yD − yS | + |xD − xS | falls within a threshold distance, XY-routing is performed instead of the turn-restricted shortest-path routing to deliver packets between them. The key is the threshold distance setting. With the primary purpose to even out the trafﬁc density with proper threshold setting, routing enhancement may reduce hop count if a source-destination pair is located near the border of two adjacent zones. For example in Fig. 2, a packet is sent from source S3 to destination D3 which are in two adjacent zones. Following the basic scheme, a packet at S3 will ﬁrst be forwarded to Sh3 and then is connected to Dh3 before reaching D3 , which takes 4 hops. It only requires 1 hop by applying XY-routing between S3 and D3 , a 75% reduction in path length. 1) The Setting of Threshold Distance: The routing efﬁciency with enhancement varies with different setting of threshold. For example in Fig. 4, when the threshold is set at 7, averagely 29.7% more trafﬁc is distributed at a big node than a small node. When the threshold arises to 15, the trafﬁc is quite evenly distributed in the whole network. In general, the trafﬁc density would be evened out at higher threshold. However, a larger threshold may lead to longer routing paths without taking the beneﬁt from long link transmission. It becomes essential to study the impact of threshold distance setting on the network performance and strike a balance between routing cost reduction and trafﬁc congestion alleviation. 2500 2000 1500 1000 500 0 1200 1000 800 600 400 200 0 1 2 3 4 5 6 7 8 9 10 7 6 5 4 3 2 1 10 9 8 1 2 3 4 5 6 7 10 9 8 8 9 10 7 6 5 4 3 2 1 (a) Trafﬁc density @ Thr=7. (b) Trafﬁc density @ Thr=15. Figure 4. Trafﬁc density under various thresholds. To quickly latch on the best threshold setting under certain topology conﬁguration with big nodes separation distance nT and network scale N × N , we determine the upper and lower bonds of its searching range. Roughly, the upper bound is set at 2(N − 1 − n 2 ), where about 96% of all-to-all trafﬁc will perform XY-routing on the base mesh. Thus the overlaid mesh seldom plays a role in routing. The threshold is lower bonded by the point where the XY-route between a source-destination pair is not a shortest path between them. Roughly, when the threshold is set less than n, the packets will simply reach the adjacent zone. Thus, routing with long links may result in more hop count. In a nutshell, the threshold searching space is bounded by n < T hr < 2(N − 1) − n (5) 2) Deadlock Avoidance: Improving routing efﬁciency by enhancement however may cause deadlock in routing due to the fact that the introduced XY-routes cross the borders of multiple zones. For example as shown in Fig. 5, data streams are delivered along ﬁve routing paths. Among them, four are enhanced XY-routes. Node A wants to send packets p1 to H , but H ’s dedicated virtual output queue2 (VQ) is occupied by packets p2 from G and A has to wait. Similarly, nodes H , E , D and C are waiting to send their packets p2 , p3 , p4 and p5 as their dedicated VQs at nodes F , D , C and B are blocked by p3 , p4 , p5 and p1 respectively. Thus the set of packets p1 , p2 , ..., p5 generate a cyclic dependency and wait on each other to release the buffers. A deadlock occurs. G P2 H P2 F E P3 P3 D P4 P4 A B P1 P1 C P5 Figure 5. Illustration of deadlock avoidance. Deadlock can be avoided by a simple buffer ordering scheme. Each VQ will maintain two units of buffer which are ordered into two numbered buffer classes. The 1st class buffer is used to store the packets delivered along the basic zone-aided routing paths while the 2nd class buffer is reserved for storage of packets set along enhanced XYroutes. With buffer ordering, the circular waits on the waiting path created in Fig. 5 will be broken in a way that p5 can be forwarded to B and stored in 2nd class buffer as p1 is stored in the 1st class buffer of B . Similarly, p2 in 2nd class buffer won’t block p1 at H. Consequently, D and F can send out p4 and p3 respectively. As a result, deadlock freedom is achieved. 2A dynamic virtual output queuing strategy [8] is employed here for buffering. 31 IV. PER FORMANCE EVALUAT ION A. Simulation Setup A simulator is developed to evaluate the performance of the proposed WiNoC platform under various network conﬁgurations, trafﬁc patterns and network scales. A WiNoC with omnidirectional radio range is built to cover the communication among the processor tiles. Unequal RF nodes (i.e., small/big nodes with short/long transmission range and low/high link BW) are properly distributed to construct an overlaid mesh topology. At a given network scale, an overlaid mesh is conﬁgured by varying big nodes placement. A topology generator is further developed to automatically generate a variety of topology conﬁgurations under different network scales. The overlaid routing scheme (with both basic and enhanced zone-aided routing as discussed in Sec. III) is developed for efﬁcient and cost-effective routing in the overlaid mesh WiNoC. Multi-channeling which is not the focus of this paper, is facilitated with different RF nodes transmitting in parallel on distinct channels. A virtual output queuing strategy is used for cost efﬁcient buffering. A backpressure based ﬂow throttling scheme is implemented for congestion control. The overlaid mesh WiNoC network performance is evaluated in terms of end-to-end delay and network throughput at various trafﬁc injection rate. The network throughput is the average rate of successful message delivery over the network. The end-to-end delay is deﬁned as the time needed to deliver a packet successfully from a source to a destination node. The injection rate is set as a fraction of the total number of packets injected in a certain trafﬁc pattern. At each injection rate setting, a total of 8000 packets are generated and injected into the network. For synthesis trafﬁc simulation, the wireless bandwidth of small node is set at 1Gbps while the big node’s bandwidth quadruples. B. Topology Conﬁguration Performance Impact We will study the performance impact of topology conﬁguration granting to big nodes placement. Fig. 6 veriﬁes how accurate of our network capacity modeling scheme proposed in Sec. II-B and how effective of using it for fast and efﬁcient topology conﬁguration. We compare the network capacity obtained under the fast estimator with the network throughput under the complex WiNoC simulator as described in Sec. IV-A, where the performance trend under various topology conﬁguration in terms of different RF nodes placement is studied for a 10 × 10 WiNoC. As we can see, the best performance is achieved when the big nodes separation distance is set at 6T , forming a 2 × 2 full mesh. More important, the estimated network capacity follows exactly the same trend as the simulation result and reaches its peak performance at 6T . We further compare the selections of best-performance topology conﬁguration obtained under both the estimation n o i t a m i t s E y t i c a p a C 450 400 350 300 250 Estimation Result Simulation Result 3 4 5 6 RF Node Seperation Distance 7 3.5 3 2.5 2.0 1.5 T h r o u g h t p u t S i m u a l t i o n Figure 6. The comparison of estimated and simulated throughput. and simulation under various network scales. As we can see in Fig. 7, the estimator’s topology conﬁguration choice a 3 × 3 RF nodes placement where its separation distance matches well with the choice of the simulator. For example, is 3T , is chosen as the optimal conﬁguration for the 8 × 8 overlaid mesh WiNoC. We argue that the network capacity estimator can be employed for fast searching of an optimal topology conﬁguration at given network scale in favor of its estimation accuracy and searching speed without comprehensive network simulation. Estimator choice Simulator choice Rule Out l e a c S h s e M l l u F 5x5 4x4 3x3 2x2 8x8 10x10 12x12 Network Scale 14x14 Figure 7. Selection of optimal topology conﬁguration. C. Routing Efﬁciency and Routing Cost We evaluate the routing performance in terms of hop count. As discussed in Sec. III, the overlaid routing improves routing efﬁciency and reduces routing cost with a logicbased routing scheme instead of a table-based scheme, which however doesn’t guarantee shortest path. We compare the average hop count with the shortest-path routing. As shown shortest-path routing. For example, under 8 × 8, 10 × 10 and in Fig. 8, the basic routing has a hop count very close to the 12 × 12 WiNoC, the hop count is increased by 1%, 4.8% and 9% respectively. The hop count is further increased by 0.2%, 2.8% and 14% respectively by applying the enhancement. Even that, the overlaid mesh demonstrates its advantage in hop count reduction (by using long links) over the regular 2D mesh (i.e., baseline). As we can see, the reduction may reach 17% in average and as high as 20.8%. We further study the average hop count changing with the threshold under different network scales and the results are given in Fig. 9. The impact of threshold setting on the hop count are in two folds. On one hand, using enhanced XY-routes may reduce hop count for those nodes located near the border of two adjacent zones. For a small threshold setting, such reduction in hop count is more obvious. For 32         8x8 10x10 12x12 2 3 4 5 6 7 8 9 10 A e v r e g a H p o C n u o t Scale Basic Zone−aided Routing Enhance Zone−aided Routing Shortest−path Routing Baseline Figure 8. Hop count comparison with shortest-path and baseline. example, the average hop count of the shortest-path routing under 10 × 10 WiNoC is 4.98 as in Fig 8. The hop count of the enhanced routing at threshold=7 is increased by 4.8% when compared with shortest-path routing. However, the increment may reach up to 20% when threshold doubles (=15). On the other hand, increasing the threshold means more packets will transit over the enhanced XY-routes instead of using long links to shorten the routing path which when the threshold changes from 7 to 11 under the 8 × 8 ﬁnally leads to the increase of hop count. For example, WiNoC, the hop count is increased by 11%. However such hop count increment will gradually slow down when the threshold increases further. For instance, when the threshold changes to 15, the hop count is increased just by 2%. 7 11 15 2 3 4 5 6 7 8 A e v . H p o C n u o t Threshold 8x8 10x10 12x12 Figure 9. Hop count under different threshold. D. WiNoC Network Performance Threshold Performance Impact: We study the impact of threshold setting on the network performance of an example 12 × 12 overlaid mesh WiNoC where the optimal conﬁgured topology with 3 × 3 full mesh (RF nodes separation distance=5) is used for experiment under uniform trafﬁc. As we can see from ﬁgures 10 and 11, a better performance is achieved at a higher threshold by taking the advantage of congestion alleviation at big nodes. For example, the best performance is obtained when threshold is 15. Although higher threshold increases hop count in general, more evenly distributed trafﬁc (e.g., the trafﬁc density is dropped by 73.6% and 31.3% respectively at threshold of 7 and 15 when comparing to the density at threshold=2) may greatly improve the transmission concurrency while relieving trafﬁc congestion, resulting in better end-to-end performance. 0 0.05 0.1 0.15 0.2 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Injection Rate A g v . N e t w o r k h T r u p h g u o t ( G s p b ) 2 5 7 11 15 Base Figure 10. Network throughput under various threshold setting. 0 0.05 0.1 0.15 0.2 0 100 200 300 400 500 600 700 Injection Rate E 2 d n E d n D y a e l ( C s e c y l ) 2 5 7 11 15 Base Figure 11. End-to-end delay under various threshold setting. When comparing the performance with the baseline, we can see that the overlaid mesh outperforms the 2D mesh. Its end-to-end delay is reduced by 25% at the best threshold setting (=15) while the network throughput is improved by 40%. Such performance improvement was attributed to three key factors: improved transmission concurrency, trafﬁc redistribution and reduced hop count. We have discussed hop count reduction in Sec. IV-C, we will brieﬂy discuss the other two factors. The transmission concurrency is improved by the increased wireless connectivity in overlaid mesh and consequently more effective bandwidth distribution with multi-channeling. In the meantime, the overlaid mesh shows a more evenly distributed trafﬁc when comparing the trafﬁc density at threshold=15 with baseline, thus further alleviating trafﬁc congestion. Trafﬁc Pattern Performance Impact: The network performance under two synthetic trafﬁcs are studied: uniform and transpose. Under uniform trafﬁc, a.k.a., all-to-all, each RF node uniformly injects packets into the network with randomly generated destinations. A node at (x, y ) sends its packets to the node at (y , x) generates the transpose trafﬁc. As we can see from ﬁgures 12 and 13, overlaid mesh demonstrated its performance advantage over long range communication oriented applications. For example, there are sufﬁcient long traverse range trafﬁcs under uniform trafﬁc. The overlaid mesh may thus reduce the delay with the use of long links and at the same time evenly distributing the trafﬁc with enhancement technique. The reduction in delay may reach as high as 33% over the baseline while the throughput is improved by 51%. Meanwhile in simplex transpose trafﬁc, most trafﬁcs cannot get beneﬁt from long link transmission, as only the trafﬁc between the top-left and bottom-right 33                               nodes will traverse through the tilted long links. Even that, the overlaid mesh can still improve the throughput by 11% and reduce the delay by 9%. 0 0.05 0.1 0.15 0.2 0 100 200 300 400 500 600 700 Injection Rate E 2 d n E d n D y a e l ( C s e c y l ) Figure 12. End-to-end delay under different trafﬁc patterns. 0 0.05 0.1 0.15 0.2 0 1 2 3 4 5 6 Injection Rate N e t w o r k h T r u p h g u o t ( G s p b ) TR TRB Un UnB OHB OH Figure 13. Network throughput under different trafﬁc patterns. Scalability Study: Moreover, the scalability of overlaid mesh WiNoC is investigated with uniform trafﬁc under three different network scales: 8 × 8, 10 × 10 and 12 × 12. As we can see from ﬁgures 14 and 15, the network throughput scales up while the end-to-end delay scales down with the from 10 × 10 to 12 × 12 WiNoC, the delay is increased by increase of the network size. For instance, when scaling 33%. It’s mainly due to the hop count increment by 25%. The throughput reﬂects the performance changing with the communication concurrency level at various network scale. Roughly, the maximum achievable packet transmission concurrency scales with the network scale ratio. 0 0.05 0.1 0.15 0.2 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Injection Rate N e t w o r k h T r u p h g u o t ( G s p b ) 10x10 12x12 8x8 Figure 14. Network Throughput at different network scales. V. CONCLU S ION We have proposed an overlaid mesh WiNoC platform to improve the on-chip communication of future many-core 0 0.05 0.1 0.15 0.2 0 50 100 150 200 250 Injection Rate E 2 d n E d n D y a e l ( C s e c y l ) 10x10 12x12 8x8 Figure 15. End-to-end delay at different network scales. chips. The WiNoC topology was designed in a way to achieve a performance optimized conﬁguration by proper big nodes placement. An effective and efﬁcient topology conﬁguration model has been developed for fast searching through the design space without comprehensive network simulation. A high-efﬁcient, low-cost zone-aided routing scheme has been designed to facilitate deadlock freedom while ensuring routing efﬁciency. The simulation study has demonstrated the promising network performance of the overlaid mesh WiNoC over a regular mesh. "