title,abstract,full_text
Scalability-oriented multicast traffic characterization.,"Multicast on-chip communications are expected to become an important concern as the number of cores grows and we reach the manycore era. The increasing importance such traffic flows directly contrasts with the diminishing multicast performance of current Network-on-Chip (NoC) designs, and has lead to a surge of research works that seek to improve on-chip multicast support. Within this context, one-to-many traffic models may become useful for the early-stage design and evaluation of these proposals. However, existing models do not distinguish between unicast and multicast flows and often do not consider different multiprocessor sizes. To bridge this gap, a multicast scalability analysis is presented, aiming to provide tools for the modeling of multicast communications for NoC design and evaluation purposes.","Scalability-Oriented Multicast Trafﬁc Characterization Sergi Abadal∗ , Ra ´ul Mart´ınez† Eduard Alarc ´on∗ , and Albert Cabellos-Aparicio∗ ∗NaNoNetworking Center in Catalonia (N3Cat) Universitat Polit `ecnica de Catalunya, Barcelona, Spain Corresponding Email: abadal@ac.upc.edu † INTEL Barcelona Research Center, Barcelona, Spain Abstract—Multicast on-chip communications are expected to become an important concern as the number of cores grows and we reach the manycore era. The increasing importance such trafﬁc ﬂows directly contrasts with the diminishing multicast performance of current Network-on-Chip (NoC) designs, and has lead to a surge of research works that seek to improve on-chip multicast support. Within this context, one-to-many trafﬁc models may become useful for the early-stage design and evaluation of these proposals. However, existing models do not distinguish between unicast and multicast ﬂows and often do not consider different multiprocessor sizes. To bridge this gap, a multicast scalability analysis is presented, aiming to provide tools for the modeling of multicast communications for NoC design and evaluation purposes. Index Terms—Multicore Processors, Multicast, Broadcast, OnChip Trafﬁc Analysis, Network-on-Chip, Scalability I . IN TRODUC T ION Networks-on-Chip (NoCs) are emerging as the way to interconnect the components of a multiprocessor. As recent years have seen a rapid increase in the core density, it is crucial to guarantee the scalability of NoCs to avoid communication to become the performance bottleneck in next-generation multiprocessors. Among other issues, NoCs suffer a signiﬁcant performance drop in the presence of multicast (one-to-many) or reduction (many-to-one) ﬂows [1]. In the particular case of multicast, messages are generally broken down into multiple unicast packets and served independently. Such approach is not only highly power-inefﬁcient, but also the cause of the aforementioned network performance drop that typically implies a reduction of the multiprocessor performance. It is expected that this effect will exacerbate in denser networks since, as it is shown in this work, multicast ﬂows grow in intensity and number of destinations with the core count. Also, multicast is intensively used in new architectural innovations for many-core processors [2]. For all this, signiﬁcant research efforts have been recently devoted to improving on-chip multicast support (see [1]–[4] and references therein). Perhaps due to the lack of realistic multicast models, these approaches have been generally tested using synthetic trafﬁc and considering a ﬁxed network size. As a result, their impact upon the network performance is imprecise and their scalability remains largely unknown. Given that one-to-many trafﬁc models may become useful for the TABLE I S IMU LAT ION PARAM E TER S Number of cores L1 Cache (I & D) L2 Cache Coherency Main Memory Latency Network-On-Chip 4, 8, 16, 32, 64 32 KB, 2-way, 2 cycles 512 KB, 8-way, 10 cycles MESI, HyperTransport (HT) 30 ns 2D-Mesh, 1-cycle link, 5-cycle router early-stage design and evaluation of NoCs, in this paper we characterize multicast trafﬁc from a scalability perspective as a ﬁrst step towards complete trafﬁc models. Unlike existing characterization efforts [5]–[7], our approach differentiates between unicast and multicast ﬂows and is not bound to a given network size. I I . FRAM EWORK To perform a scalability-oriented multicast trafﬁc analysis, we simulate different multiprocessors running SPLASH-2 and PARSEC (simmedium input set) in order to obtain a set of traces. All multiprocessors share the same basic architecture (see Table I) and use the number of cores and coherency mechanism as parameters. Simulations are carried out with GEM5 [8], which has been slightly modiﬁed so that the network interfaces would register the time of arrival, origin, destinations, type and size of each multicast message that needs to be injected into the NoC. Since GEM5 only admits up to 64 cores thus far, scalability trends obtained with such methodology could be used to extrapolate a ﬁrst approximation of the multicast trafﬁc requirements of manycore processors. I I I . R E SU LT S Multicast Trafﬁc Intensity: The number of multicast messages per instruction is a NoC-agnostic measure of the multicast intensity. Figure 1 (left) plots the number of multicast messages per one million instructions for both MESI and HyperTransport (HT) coherence schemes in the test conﬁguration. It is observed that HT has multicast requirements one order of magnitude larger than that of MESI. More importantly, it is shown that most applications become more multicast intensive as the number of cores grows. Although such increase is application-dependent and does not follow a common scaling Fig. 2. Averaged coefﬁcient of variation of the spatial injection distribution for MESI and HT as a function of the number of nodes. Fig. 1. Number of multicast messages per 106 instructions (left) and number of destinations per multicast (right) as a function of the number of cores, assuming MESI (top) or HT coherence (bottom). N (cid:1) trend, ﬁtting methods on the average values yield a logarithmic relation between multicast intensity and number of cores. Application scalability limitations may explain such tendency. Number of Destinations: This is an important metric given that the performance of conventional NoCs is inversely proportional to the number of destinations per message. Figure 1 of cores N . The number of destinations scales as O(cid:0)√ (right) shows the how the number of destinations per multicast and as O(cid:0)N (cid:1) when assuming MESI and HT coherence, (averaged over all the applications) scale with the number respectively. In the former case the metric is applicationdependent; whereas, in the latter case, the trend is applicationindependent since the coherence protocol issues a broadcast for each coherence operation. Spatial Distribution: The study of the injection spatial distribution may be useful for the identiﬁcation of hot spots, which are especially concerning in the case of multicast. To express this trafﬁc characteristic, we calculate the coefﬁcient of variation (COV) of the number of injected multicast per node as cv = σ/µ, where σ and µ are the standard deviation and mean of the multicasts injected by each node. A higher COV means a higher concentration of the multicast injection over given cores. Figure 2 shows how the COV (averaged over all the applications) scales with the number of cores in MESI and HT. In both cases, the concentration increases with the core count. Temporal Distribution: Besides enabling the identiﬁcation of periodic multicast-intensive phases, studying the temporal distribution of multicast message injection provides knowledge on the burstiness of such trafﬁc. Related works have shown that on-chip trafﬁc is bursty (self-similar) in general [5] and, provided that multicast trafﬁc is a subset of the on-chip trafﬁc, it is reasonable to deduce that multicasts will also exhibit selfsimilarity. We calculated the Hurst exponent H (0.5 < H ≤ 1, a value close to 1 denotes strong self-similarity) applying the RS plot method [5] to the full-system traces and the averaging it over all the applications. In light of the results of Figure 3, it can be concluded that multicast trafﬁc is self-similar and that burstiness increases with the core count. Fig. 3. Averaged Hurst exponent for MESI and HT as a function of the number of nodes. IV. CONCLU S ION S Our multicast trafﬁc characterization shows that the oneto-many communication requirements of MESI and HT grow with the number of cores due to an increase of both the number of multicasts per instruction and the destinations per multicast. Further, spatial injection unbalance and temporal burstiness worsen with the core count. In light of this, the need for multicast-efﬁcient NoCs becomes patent as we reach the manycore era. ACKNOW L EDGM ENT S The authors gratefully acknowledge support from INTEL through the Doctoral Student Honor Program. "
DiAMOND - Distributed alteration of messages for on-chip network debug.,"During emulation and post-silicon validation of networks-on-chip (NoCs), lack of observability of internal operations hinders the detection and debugging of functional bugs. Verifying the correctness of the control-flow portion of the NoC requires tests that exercise its functionality, while abstracting the data content of traffic. We propose a methodology where network packets are repurposed for the storage of debug information collected during execution. Debug data pertaining to each packet is collected at routers along its path and stored by replacing the packet's original data content. Our solution is coupled with a detection scheme consisting of small checkers that monitor execution and flag bugs. Upon bug detection, we analyze the debug information to reconstruct network traffic.We also provide relevant statistics for debugging, such as packet interactions and packet latencies, per router. In our experiments, this approach allows us to reconstruct over 80% of the packets' routes. Moreover, the obtained statistics facilitate debugging erroneous network behavior and identifying performance bottlenecks.","DiAMOND:Distributed Alteration of Messages for On-Chip Network Debug Rawan Abdel-Khalek and Valeria Bertacco Computer Science and Engineering Department University of Michigan (rawanak, valeria)@umich.edu Abstract—During emulation and post-silicon validation of networks-on-chip (NoCs), lack of observability of internal operations hinders the detection and debugging of functional bugs. Verifying the correctness of the control-ﬂow portion of the NoC requires tests that exercise its functionality, while abstracting the data content of trafﬁc. We propose a methodology where network packets are repurposed for the storage of debug information collected during execution. Debug data pertaining to each packet is collected at routers along its path and stored by replacing the packet’s original data content. Our solution is coupled with a detection scheme consisting of small checkers that monitor execution and ﬂag bugs. Upon bug detection, we analyze the debug information to reconstruct network trafﬁc. We also provide relevant statistics for debugging, such as packet interactions and packet latencies, per router. In our experiments, this approach allows us to reconstruct over 80% of the packets’ routes. Moreover, the obtained statistics facilitate debugging erroneous network behavior and identifying performance bottlenecks. I . INT RODUCT ION Networks-on-chip (NoCs) have become the prevalent communication paradigm for current and future chipmultiprocessor (CMP) and system-on-chip (SoC) architectures. In today’s market, large chip-multiprocessors, incorporating hundreds of processing elements, are developed to provide the computational power needed to run parallel and high performance computing applications. Concurrently, SoC design is experiencing an increasing trend of high integration, where a number of IP blocks, commonly obtained from third party vendors, are integrated on to a single chip through a communication substrate. The network-on-chip model constitutes a distributed and generalized architecture that can meet the growing communication needs of SoCs and CMPs. A NoC consists of a set of routers connected according to a chosen topology. In the mainstream approach of wormhole routing, messages sent over the NoC are divided into packets, which in turn are divided into equal size segments of data called ‘ﬂits’. A generic router design includes several storage buffers and arbitration and allocation units to assign resources to packets-in-ﬂight. Router designs are often complex, as router architectures may include a number of advanced features, such as virtual channels and intricate arbitration units. Moreover, a number of regular and irregular topologies can render the packet ﬂow and the overall network subsystem extremely complex. In addition, network complexity is further increased when deploying elaborate routing protocols that utilize network state to guide routing decisions. For these large CMP and SoC architectures, a great deal of effort is spent in the functional veriﬁcation process of the individual cores and IP blocks, the NoC interconnect itself, as well as the entire system. With the increase in the size and complexity of these systems, along with shrinking timeto-market windows, a lot of this effort is shifting towards the heavy use of emulation and post-silicon validation to ensure functional correctness. A key underlying reason for this trend is the vast complexity of modern CMP and SoC designs and their communication subsystem, which are often too large for formal and software-based validation solutions. In emulation, the design under test is mapped onto conﬁgurable hardware units, such as FPGA-based platforms. Tests are run on the emulation platform, which provides orders of magnitude speedups relative to software-based simulations of the design’s RTL description. On the other hand, post-silicon validation comes at a later stage, when the ﬁrst few silicon prototypes of the chip become available. During this phase, tests run directly on the hardware and at-chip speed, enabling a faster and hence more thorough validation of the system’s functionality. In the context of validating the NoC design, emulation and postsilicon validation provide great advantages in speed but suffer from limited observability of the design under test. Lack of visibility of internal operations makes the detection, diagnosis and debug of errors an extremely challenging process. Moreover, general solutions that enhance observability, such as scan chains, do not always provide sufﬁcient debug data to permit a fast and efﬁcient functional validation of the NoC. In our work, we aim to address the challenge of validating the complete network subsystem on these fast platforms (emulation and post-silicon). In performing the functional validation of NoCs, we can consider any NoC design to consist of two components: data ﬂow and control ﬂow. Validating the data ﬂow correctness means ensuring that data sent over the network is not corrupted in transfer. Verifying the correctness of the control ﬂow portion essentially means validating all functionality, and hence control decisions made in the network. The NoC functionality is entirely dependent on the trafﬁc patterns observed and it is agnostic to the data content of the messages. Therefore, specialized test cases can be run with the goal of exercising as much of the network’s functionality as possible. When running such tests, the packets’ data contents are effectively irrelevant. In this context, we propose a methodology to greatly enhance the observability of the network trafﬁc and its internal state to facilitate the detection and debug of control-ﬂow functional errors. Our solution, called DiAMOND, proposes to replace packets’ data contents with debug information collected during the network’s execution. At every router along a packet’s path, we gather debug data that encapsulate the packet’s current state and we systematically substitute the data ﬂits of the packet with this information. Once packets arrive to their destinations, they are stored at the local cache or memory associated with those nodes. Along with this data collection mechanism, we instrument routers with small checkers that can detect various functional errors. Upon error detection, the collected debug data accumulating throughout the network is analyzed by software-based algorithms running on the CMP/SoC cores or off-chip. The information that can be extracted from this data includes a detailed overview of the packets’ paths, analysis of performance metrics at internal routers, as well as the sequence of events observed at a given router during a given interval. Armed with this enhanced visibility into the network behavior, veriﬁcation engineers can more promptly localize and debug functional (and in some cases performance) bugs. A. Contributions • We introduce a novel solution to gather debug data during the emulation or post-silicon validation of NoC interconnects and, speciﬁcally, while running tests targeting the validation of the NoC’s control-ﬂow. • We present a complete framework that couples our debug data collection mechanism with a bug detection scheme. We also present a debug data processing methodology to extract relevant information that facilitates the diagnosis and debug of functional errors in the NoC design. • Our solution introduces minimal perturbations and requires minor hardware additions. We also provide three modes of operations to conﬁgure our data collection mechanism, tradingoff the exhaustiveness of the debug data gathered with the degree of perturbation to the original system. • In addition to detecting and debugging functional errors, our solution can also detect performance bugs, such as starvation and misroutes. It can also provide performance statistics at internal network routers, which in turn aids in analyzing the NoC’s overall performance. I I . RE LAT ED WORK Previous work on post-silicon validation of NoCs have proposed various approaches to increase NoC observability. In [1], authors instrument routers and network interfaces with monitors. These monitors ﬁlter network trafﬁc to identify transactions of interest, as well as analyze performance and validate data ﬂow errors. Similarly, approaches proposed by [2], [3] add monitors to network routers that observe trafﬁc and abstract it into events or transactions. The extracted events and transactions are then transferred over the network for further analysis. Our solution differs from these approaches by focusing on the validation of functional bugs in the controlﬂow portion of NoC designs. It can also detect and debug some types of performance bugs. In contrast to the above works, we provide a complete framework that can collect debug data, detect functional bugs, and then analyze the data for diagnosis and debugging. The collected debug data is transferred by substituting the original data content of packets. Moreover, the type of debug data collected and how it is analyzed is independent of network topology and router architectures. Other recent work proposes a post-silicon solution that relies on taking periodic snapshots of trafﬁc to reconstruct packet paths and identify functional errors related to forward progress [4]. By periodically sampling trafﬁc, this solution can only provide low error detection probability for those bugs that are transient in nature, such as misroutes or starvations. It also fails to detect other types of bugs, such as dropped packets. On the other hand, our approach achieves a much better observability of network trafﬁc, as all data packets can be observed, for most or all of their paths. We can also reconstruct longer and more uniform routes for each packets, achieving a reconstruction rate of more than 83%. In addition, since debug data is collected per packet and then stored within each packet’s body ﬂits, the amount of debug data logged is proportional to the amount of trafﬁc and not the test execution length. A number of works have targeted emulation of networkson-chip [5]–[9], where authors proposed different ways of implementing an emulation platform that allows the modeling and exploration of various NoC designs. The emulated NoC is evaluated by relying on trafﬁc generators and receptors that can be conﬁgured to inject different trafﬁc patterns and analyze received packets. These works focus mostly on designing an emulation platform and a methodology that allows modeling different NoCs to speed up design exploration and validation. Our work is complementary to these approaches. Independently of how the NoC is emulated, we provide a methodology to collect debug data from network routers to facilitate the debugging of functional errors in the control ﬂow portion of the design. Moreover, the trafﬁc receptors proposed are limited to analyzing end-to-end correctness and performance metrics and can not provide insights regarding the internal events that occurred in the network. In contrast, our debug data collection mechanism provides a detailed view of internal network behavior, generating results about packet interactions within routers, packet latencies observed per router, as well as the routes followed by the packets. NoC execution Analysis debug data from R1  replaces data flit R1 H R2 bug  detected 1) Traffic reconstruction – local analysis p1 R1 R7 p2 R10 R4 p3 H NoC  router R2 R4 hardware checkers to local  storage post-silicon/emulation platform  2) Statistics – global analysis - latency per router - sequence of      events per router y c n e t a l - relative order of  events across routers Rn R1 time Fig. 1. Overview of our solution. During NoC execution, debug data is collected at every hop and stored in the packet, overwriting data ﬂits. Routers are instrumented with hardware checkers that monitor execution and ﬂag functional errors. Upon error detection, the debug data is analyzed to reconstruct trafﬁc, as well as provide a number of relevant statistics. I I I . THE D IAMOND SOLUT ION A. Overview The aim of DiAMOND is to provide observability of the network’s operation to facilitate the diagnosis and debug of functional errors in the NoC’s control ﬂow. Tests used to validate the NoC functionality aim at creating various network trafﬁc scenarios, while abstracting away the data content of messages. When running such tests, our solution relies on using the contents of packets to store debug data collected during execution. Network execution is partitioned into epochs, during which the network is instrumented for bug detection, as well as debug data collection. As packets traverse the network, their data content is substituted with debug information collected at every hop, as illustrated in Figure 1. Once a packet is delivered to its destination node, it is stored in the local cache or memory associated with that node. In parallel with debug data collection, small hardware checkers monitor the network’s execution and detect functional bugs. Upon ﬂagging an error, execution is halted and the debug data that has been collected in the caches is analyzed. This analysis process is ﬁrst carried out locally, where each processor core examines the debug data of packets that were destined to itself, and then globally, where debug data from all nodes are aggregated at a central location for a global overview of the network’s behavior. On the other hand, if the epoch ends without the detection of any bugs, the collected debug information in the caches is simply overwritten in the following epoch. Data collection During NoC execution debug  logs debug  logs NoC  router NoC  router • local cache • on-chip  trace buffers Analysis Post-silicon validation CMP/SoC core local cache debug data  analyzed by  on-chip cores Emulation on-chip  trace buffer debug  logs NoC  router NoC  router debug  logs hardware checkers post-silicon validation or emulation platform  local cache debug logs  analyzed  off-chip on-chip  trace buffer Fig. 2. Functional validation ﬂow. During the emulation or post-silicon validation of NoC designs, debug data is collected during testbench execution. The debug logs can be stored in the local cache or memory associated with each node or in on-chip trace buffers. Upon the detection of an error, execution is halted and the collected debug data is analyzed by the on-chip processor cores or can be transferred off-chip and analyzed separately. Since this approach relies on utilizing each packet’s ﬂits to carry debug information, the amount of debug data that can be collected is limited by the packet’s size and the length of its path through network. However, in practice, this limitation does not always cause a lack of scalability. With low packet latency being a primary concern, NoC designs, even if large in topology, commonly incur a low average hop count for packets in ﬂight. Moreover, in this paper, we consider a baseline CMP system, where each node consists of a processing element and a local cache, but our solution is also adaptable to SoC designs, where the system consists of general purpose processors, as well as other IP modules. In this context, only nodes with memory/cache modules store debug data. It is also common during post-silicon validation and emulation to have additional on-chip trace buffers, which can be utilized to store debug data for nodes without caches. The debug data analysis process can be carried out by running a software program on the on-chip cores or by performing the analysis off-chip. Figure 2 shows the complete validation ﬂow of our proposed solution. B. Debug Data Collection debug data associated with each packet is stored in one of the packet’s ﬂits replacing the original data content. For every input buffer within the router, we add a register, called log buffer, to store the collected debug information. In addition, we require each router to include a packet counter (pckt cntr) that is incremented upon receiving a packet. The log buffer is updated everytime a new packet is at the head of its corresponding input buffer. The information collected and stored in the log buffer consists of: 1) The router ID 2) Arrival timestamp (timestampA) that indicates the value of the pckt cntr when the header ﬂit of the packet was received by the router. 3) Departure timestamp (timestampD) that indicates the value of pckt cntr when the header ﬂit was sent from the router. Logging timestampA and timestampD allows us to order packets passing through each router, as well as reason about packet interactions within a router. 4) A third timestamp (pckt latency) that indicates the amount of time (in cycles) the packet’s header ﬂit remained in the router. This timestamp allow us to analyze packet latencies observed at interval routers. 5) The packet’s input port and input virtual channel. 6) The output port and virtual channel the packet requests. Once the log buffer is complete, the debug data is written to one of the packet’s body ﬂits. The index of the ﬂit to be written is maintained by a counter that is added to the packet’s header ﬂit. When a packet arrives to a router and reaches the head of one of its input buffers, we ﬁrst extract the ﬂit index where the debug data will be written. Then, timestampA, the input port and the input virtual channel are logged in the log buffer. When the header ﬂit completes its route computation and virtual channel allocation, the requested output port and requested virtual channel are logged. Finally, when the header ﬂit is sent to the next router (or ejected if it is at a destination router), timestampD and pckt latency are logged. Once the packet’s header has been routed to the next hop, the packet’s body ﬂits follow. Based on the ﬂit’s write index, the log buffer is simply written in the appropriate ﬂit. A typical ﬂit width in NoCs is between 128 and 256 bits [10]. In our evaluation, we assume a ﬂit width of 128 bits and a log buffer size of 64 bits. Therefore, the debug data collected at every hop occupies only half a ﬂit, with the remaining half written at the next hop. In order to implement this functionality, the ﬂit write-index ﬁeld in packet headers is extended by 1 bit, which indicates whether the debug data will replace the ﬁrst or second half of a body ﬂit. Moreover, the ﬂit write-index ﬁeld is incremented once every two hops. Figure 4 shows the various ﬁelds of packets’ header ﬂits. In the case of packets that do not have enough ﬂits to store the collected debug information, we provide three solutions for our approach, depending on the needs of the veriﬁcation methodology in use: drop remaining, drop at alternate hops, and append. Figure 3 illustrates the behavior of each mode. In these three modes, the veriﬁcation process can be tuned to trade-off debug capabilities with the degree of perturbation introduced to the original system. Debug data is collected for every packet injected into the network and at every hop during its ﬂight. At each hop, the 1) Drop Remaining: During drop remaining, when the number of hops in a packet’s path exceeds the available ﬂits original packet header data flit1 data flit2 tail R1 R2 R3 R4 R5 R6 R7 dd-Rx = debug data collected at router x header dd-R1 header dd-R1 dd-R2 R2 R4 tail tail mode 1: Drop remaining  header dd-R1 dd-R2 dd-R3 dd-R4 tail header dd-R2 dd-R1 R7 dd-R3 dd-R4 tail header dd-R1 tail header header R4 dd-R2 dd-R1 dd-R3 dd-R4 R5 dd-R1 dd-R5 dd-R3 dd-R4 R7 tail tail header dd-R1 dd-R5 dd-R3 dd-R6 tail R1 R1 mode 2: Drop at alternate hops header dd-R1 R1 R4 tail header dd-R2 dd-R1 dd-R3 dd-R4 tail mode 3: Append header R5 dd-R1 dd-R2 dd-R3 dd-R4 R7 dd-R5 tail header dd-R1 dd-R2 dd-R3 dd-R4 dd-R5 dd-R6 dd-R7 tail Fig. 3. Debug data collection. In drop remaining, if the number of hops exceeds the available data ﬂits, additional debug data is simply dropped. During drop at alternate hops, additional debug data replaces alternating entries providing a more uniform overview of the packet’s path. The last mode, append, creates new ﬂits as needed and appends them to the packet. in the packet, additional debug data is dropped. This mode of operation is simple to implement at the expense of low observability for packets with long routing paths. 2) Drop at Alternate Hops: In this mode of operation, debug data collection is implemented as before. However, when the space in the packet is exhausted, new debug data overwrites older debug data, creating an every-other-hop scheme. For example, as shown in Figure 3, the debug data collected at router R5 replaces older debug data collected at router R2. As opposed to drop remaining, this mode provides a longer and more uniform overview even of long routing paths. Moreover, data belonging to the alternating missing hops can be partially reconstructed or extrapolated from the debug data that remains. For example, the output port requested along with the router ID can be used to determine the downstream router, for which we have no log. Similarly, the input port and router ID can be used to determine the missing upstream router. 3) Append: We also provide a mode of operation that allows routers to append new ﬂits to the packet. New ﬂits are appended before the tail ﬂit, as illustrated in Figure 3. While this mode provides the complete path of a packet, it requires additional hardware to add the new ﬂits. It also alters the network’s original execution by creating longer packets, potentially masking bugs. It is also possible for the perturbation created by increasing the length of some packets, to expose bugs that would have not been observed otherwise. C. Error Detection Our debug data collection methodology is orthogonal to the mechanism by which functional bugs are detected in the NoC. We propose the use of a ﬁne-grain detection approach that relies on adding small checkers to the NoC routers. These checkers monitor runtime execution for signs of erroneous behavior, while targeting a wide range of functional bug manifestations detailed below. Instead of localizing the root cause of a functional bug, our detection mechanism targets the bug’s manifestation on the trafﬁc in-ﬂight. Whereas, our debug collection mechanism stores low level debug data pertaining to each stage of a router’s control path (route computation, virtual channel allocation and switch allocation), which later permits a more detailed diagnosis and debugging analysis. In terms of detection, irrespective of its exact location, a functional bug in a NoC can manifest by affecting the trafﬁc in-ﬂight in a ﬁnite number of ways. First, a functional bug can lead to bit corruptions in the transferred data, which can be detected by including an error correction code (ECC) in each ﬂit. However, in this work, we target the validation of the control ﬂow portion of the NoC design and therefore only focus on that subset of functional bug manifestations. In terms of control ﬂow and at the level of packets, a functional bug can hinder the forward progress of packets through the network, such as in the cases of deadlocks, livelocks, starvations, and misroutes. It could also lead to entire packets being dropped or duplicated. At the level of ﬂits, a functional bug in the control ﬂow can manifest as dropped or spurious ﬂits. Deadlock and starvation: Packets involved in a deadlock are permanently blocked waiting on each other to free needed resources. On the other hand, starvation occurs when a packet is temporary blocked waiting for resources that are allocated to other packets due to unfair arbitration and allocation schemes. Note that unless bounded packet delivery is a system requirement, starvation does not always affect the correctness of execution and it is often considered a performance bug. A common technique to detect blocked packets adds counters to routers, one associated with each input buffer. After a header ﬂit, marking the beginning of a new packet, reaches the head of an input buffer, its corresponding counter is incremented in every cycle. The counter is reset when the tail ﬂit is observed. If the counter exceeds a user-deﬁned threshold, it ﬂags an error [11]. In our work, we differentiate between deadlock and starvation by allowing the network to drain after the error is ﬂagged. In contrast to deadlock, starved packets will eventually acquire the resources they need to move forward and their corresponding counters reset to zero. Livelock: Packets are in livelock if they are continuously transferred between routers without making forward progress to their destinations. A common approach to detecting a livelock adds a hop counter to the header ﬂit of every packet. The counter is incremented at every hop and a livelock is ﬂagged if the counter exceeds a pre-deﬁned threshold [12]. Dropped and duplicated packets: To detect dropped packets, we utilize the approach proposed by [13], where a packet counter is maintained per router. The counter is incremented upon receiving a tail ﬂit and decremented upon sending one. If packets are not dropped within the router, then the counter should reach a value of zero at some point within a checking window. Similarly, a packet counter reaching a negative value can be used to identify packet duplication or spurious packet creation. In the case of dropped packets, it is possible for this approach to exhibit false positives, particularly under high congestion trafﬁc. High congestion can also mask packet duplications, causing this technique to exhibit false negatives. However, [13] shows that choosing a suitable checking window size can practically eliminate false positives. Moreover, false negatives are rare and duplications will eventually be detected. Dropped and duplicated ﬂits: In order to identify dropped and duplicated ﬂits within packets, we require the addition of a size ﬁeld to the header ﬂit. A simple counter and comparator added to every input buffer are then used to keep track of the number of ﬂits observed. If the tail ﬂit is reached and the counter does not match the size ﬁeld, then a ﬂit must have been duplicated, created or dropped. partial order of events by using packets as points of reference. A packet transferred from routerA to routerB serves as a synchronization point, where events observed in routerA before the packet was sent can be classiﬁed to have happened before the events occurring in routerB after the arrival of the packet. Misrouting: A packet is misrouted if it is sent to the wrong destination. To detect such errors, we perform a check upon packet delivery to ensure that the destination ﬁeld in the header ﬂit matches. Misrouting could also occur if a packet is delivered to the correct destination, but takes incorrect routes along its path. In such cases, misroutes can be detected by adding simple checkers to internal routers. The exact checker implementation is dependent on the routing protocol. For example, for deterministic or minimal routing algorithms, a simple lookup table or assertion can detect such errors [13]. D. Debug Data Analysis Once an error has been ﬂagged, execution is halted and the network is allowed to drain. Packets blocked, due to deadlocks or livelocks, are permitted to drain to the closest node. At this point, all the debug data that was collected during execution is residing in the content of packets, which are stored in the local caches or trace buffers across the network. This data is processed in two steps: local and global, each providing a different overview of the network’s execution. 1) Local Processing: The content of each local cache is individually analyzed by a software application running on the corresponding core. The data can also be loaded off-chip for a similar analysis. By examining the contents of every packet, its path through the network can be reconstructed. The path overview allows the identiﬁcation of any livelock cycles, as well as any misrouted segments along its route. In the case of adaptive routing algorithms, the reconstructed paths provide insights regarding the performance and effectiveness of the routing protocol. In addition, by examining the recorded pckt latency timestamps, network performance can be analyzed. Periods of high packet latency can be identiﬁed along with the routers where this high latency was recorded. Finally, by comparing a packet’s requested output port and output virtual channel within a router relative to the input port and input virtual channel of the downstream router, functional bugs in switch arbitration logic can be ﬂagged. 2) Global Processing: Through the local processing step, execution intervals or routers of interest are identiﬁed. Then, data from all local caches are aggregated at a central location, where another software algorithm, running on one of the cores or running off-chip, groups this data on a per router basis. Then, using the timestampA and timestampD counters, each router’s data is sorted by increasing time. The sorted information basically encapsulates the series of packets and events witnessed by each router during execution. This, in turn, gives insights regarding packet interactions within routers, allowing us to reason about the source of the error observed. Since each router’s timestampA and timestampD represent the value of the router’s packet counter, these timestamps do not have a notion of physical time. Therefore, the arrival and departure of packets from different routers can not be correlated. However, by leveraging techniques similar to those used in ordering events for distributed systems [14], we can still construct a PID src dest size flit_wr_index header flit rID timestampA timestampD latency input  port input  VC output  portreq ouput  VCreq  input port 1 H input buffer 1 log_buffer1 virtual channel (VC)  allocation switch allocation route  computation  input buffer 2 output port 1 input port 5 xbar pckt_cntr input buffer 10 output port 5 Fig. 4. Debug data collection - hardware implementation. Additional ﬁelds are added to the header ﬂits of packets. A register, log buffer, is added to each input buffer to store debug data. A packet counter is required per router to provide the timestampA and timestampD values. IV. IM PL EMENTAT ION O F DE BUG DATA COL L E CT ION In order to implement DiAMOND’s debug data collection solution, we include additional ﬁelds in the header ﬂits of packets, as shown in Figure 4. A header ﬂit commonly carries the router IDs of the packet’s source and destination nodes. It also commonly has unused bits, which we can utilize for our solution. Therefore, we include a small counter, which along with the source and destination, serves as an ID that can uniquely identify the packet in the network. Moreover, to keep track of the ﬂit ID where the debug data will be stored at each router, we require an additional ﬂit write index ﬁeld. The ﬂit write index is a counter that is incremented every two hops. It also has an extra bit, which indicates whether the debug data will replace the ﬁrst or second half of the ﬂit, as explained in Section III-B. Note that the length of the ﬂit write index is determined by the number of body ﬂits in a packet and would typically be 3-4 bits. Table II shows the various ﬁelds added to header ﬂits and their length. We also require some minor additions to the NoC routers. Debug data collected at each router is stored in a register, the log buffer, before its written to the appropriate data ﬂit. We require one log buffer for every input buffer. The size of the log buffer register depends on the network size and router architecture. Table I lists the various entries of a log buffer and their lengths. In addition, we add a packet counter per router, which tracks the number of packets received by the router and which provides the timestampA and timestampD values. In order to record the pckt latency, we make use of the same deadlock/starvation counter needed for bug detection (Section III-C). As for the remaining entries in the log buffer, they can be recorded directly from the original router implementation and do not require any additions. Once a packet is received by a router, the ﬂit write index ﬁeld of the header ﬂit is extracted. Then, a simple combinational logic counts the number of data ﬂits observed and when it matches the ﬂit write index, it copies the log buffer to the appropriate half of the data ﬂit. We implemented these additions and the detection checkers in the Verilog model of the baseline router architecture that is described in Section V. Synthesis results show an area overhead of 2%. Moreover, the incurred power overhead is minor and is in itself not a signiﬁcant concern during the emulation and post-silicon validation of the NoC. These hardware modiﬁcations are also decoupled from the router’s functionality and can be disabled when the chip is released. V. EX PE R IMENTAL EVALUAT ION To evaluate our solution, we modeled an 8x8 mesh network using the cycle-accurate Booksim simulator [12]. Our baseline router architecture consisted of a general input-queued virtual channel router, with 5 input ports and 2 virtual channels per port. We ran both random directed trafﬁc, as well as network ﬂow traces from the PARSEC benchmark suite [15]. For uniform random trafﬁc we varied the packet size between 5 ﬂits/packet (a header, a tail and 3 body ﬂits) and 7 ﬂits/packet. As for the PARSEC network ﬂow, trafﬁc consisted of both control packets and data packets. While data packets consisted of 5 ﬂits, control packets were only 1-ﬂit long. log buffer entries routerID timestampA timesatampD pckt latency input port input virtual channel output port requested output virtual channel requested total number of bits 6 bits 15 bits 15 bits 10 bits 3 bits 1 bit 3 bits 1 bit 64 bits TABLE I. L OG BU FFE R . ﬁelds PID ﬂit write index size total additions number of bits 8 bits 3-4 bits 4 bits 14-15 bits TABLE II. A DD I T I ON S TO H EAD E R FL I T S . We modiﬁed Booksim to implement the three modes of the data collection mechanism: drop remaining, drop at alternate hops and append. Based on the network size and router architecture, we determined the length of the log buffers required at every router, as shown in Table I. The lengths of timestampA and timestampD (and hence the pckt cntr) were chosen to be 15 bits, to ensure that the packet counter does not wrap around too frequently. In the event that a wrap-around occurs at any router, we force the epoch to end early, which permits clearing the previously collected debug data from the caches and restarting the counters. We chose a length of 10 bits for the pckt latency ﬁeld, limiting the maximum latency value that can be logged to 1,024 cycles. Finally, we assume a ﬂit size of 128 bits, which is a common ﬂit length [10]. Therefore, we are able to store the log buffers collected along two hops in each ﬂit, as explained in Section III-B. Our implementation also requires adding several ﬁelds to each packet’s header ﬂit, which are listed in Table II. We chose the packet ID to be an 8 bit counter, which along with the packet’s source and destination node IDs forms a unique identiﬁer of each packet. The length of size, which is used for the detection of dropped and duplicated ﬂits (Section III-C), depends on the number of ﬂits in a packet. In our evaluation, we consider packets of size 5 and 7 ﬂits, making the size ﬁeld 4 bits long. Similarly, ﬂit wr index depends on the number of body ﬂits in a packet, with an additional bit to indicate which half of the ﬂit is to be written. A. Path Reconstruction Results We ﬁrst examined the observability gained from utilizing our debug data collection solution by evaluating the fraction of the path that can be observed for each packet. In our validation platform, the path reconstruction process is completed during the local processing phase, where body ﬂits of packets are examined and the sequence of routers, through which each packet passed, is reconstructed. Table III shows the average percentage of each path that can be reconstructed under all three modes of operation. For the PARSEC network ﬂow, data packets consist of 3 body ﬂits and could carry complete debug data from 6 routers along their path. Therefore, under drop remaining, we are able to achieve full observability (100% path reconstruction) over packets whose path traverses 6 or fewer routers. Remaining packets have smaller path reconstruction fractions depending on their path length. For all PARSEC benchmarks, the percentage of path reconstruction is 83.76% on average. During the drop at alternate hops mode, when all body ﬂits have been utilized, new debug data replaces older data by over-writing only the second half of each body ﬂit, as illustrated in Figure 3. Moreover, routers pertaining to the alternate missing hops can be extrapolated. For the PARSEC network ﬂow, in addition to the 6 routers that can be extracted directly from the debug data, the 3 alternate routers that were overwritten can be deduced from the recorded router IDs and input ports. Therefore, paths consisting of up to 9 routers can be fully observed. This mode provides a higher path reconstruction of 96.54%, on average. Note that, the PARSEC network ﬂow also consists of 1-ﬂit control packets that do not have any body ﬂits. For such packets, we are not able to collect any debug data during these two modes. Finally, for the append mode, we achieve 100% path reconstruction for both control and data packet, as expected, since packets can append as many new ﬂits as needed to store debug information. Similar results are also observed for uniform random trafﬁc. Results are averaged over a sweeping injection rate from low injection, 0.04 ﬂits/cycle/node, to high injection, 0.24 ﬂits/cycle/node. PARSEC network ﬂow blackscholes bodytrack dedup ferret freqmine streamcluster swaptions vips x264 average uniform trafﬁc packet size = 5 ﬂits uniform trafﬁc packet size = 7 ﬂits drop remaining 83.2% 85.0% 84.4% 84.5% 83.8% 84.3% 84.2% 81.4% 83.0% 83.76% drop at alternate hops 96.3% 97.1% 96.8% 96.9% 96.6% 96.8% 96.8% 95.4% 96.2% 96.54% 87.1% 97.8% append 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 100% 98% 100% 100% TABLE III. AV E RAG E PATH R E CON S T RU C T I ON B. Performance Analysis Under drop remaining and drop at alternate hops, our solution does not introduce any additional performance overhead. However, the append mode can increase the length of packets to create space for storing new debug data. Longer packets increase network congestion and can slow down execution. Figure 5 shows the average network latency for the PARSEC network ﬂow during the 3 modes of operation of DiAMOND, as compared to a baseline system without our solution. As expected, the network does not incur a performance impact when operating during drop remaining and drop at alternate hops. However, when operating with the append mode, average ) s e l c y c ( y c n e t a l . g v a 140 120 100 80 60 40 20 0 baseline drop remaining drop at alternate hops append PARSEC network flow     analysis, such as the period highlighted in Figure 10, where we record the ﬁrst signiﬁcant increase in latency. By examining packet interactions and path reconstruction results for the dedup benchmark, we identify the packet associated with this latency and ﬁnd that it is blocked in router 49 due to congestion in the downstream router 41. Router 41, in turn, has several packets that are also waiting for busy virtual channels. By means of inspection, we realized that the simulated router design was setup to utilize a basic credit-ﬂow mechanism that releases the output virtual channel only after the entire packet is transferred, which ampliﬁes the packet latencies in the presence of congestion. This example highlights how DiAMOND provides high quality diagnosis and trafﬁc-inspection capabilities in post-silicon or emulation environments, inculding the ability to investigate performance ﬂaws in the network. This technique can also be utilized for non-veriﬁcation purposes early on in design process, particulary during NoC design exploration and performance proﬁling. latency per router - averaged over all routers 5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 0 ) l s e c y c ( y c n e t a l . g v a Fig. 8. Average packet latency at internal routers. Results are shown for the PARSEC network ﬂow, averaged over all routers PARSEC network flow  avg. latency for dedup network flow router 49 12 10 8 6 4 2 0 ) l s e c y c ( y c n e t a l . g v a 0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 router # Fig. 9. Average packet latency for a sample benchmark. For the dedup network ﬂow, we show the average packet latency observed at each router. V I . CONCLU S ION We presented DiAMOND, a debug solution for the postsilicon validation and emulation of networks-on-chip. Targeting the functional validation of the control-ﬂow portion of NoCs, we log debug data during network execution and store it by replacing the data content of packets. Debug information is collected for every packet, at each router along its path, and then systematically written in one of its body ﬂits. In addition, simple hardware checkers are added to routers to monitor execution and ﬂag functional bugs. Upon bug detection, the ) s l e c y c ( y c n e t a l 700 600 500 400 300 200 100 0 dedup network flow timestampA Fig. 10. Packet latency at router 49 for dedup benchmark. collected debug data provides increased observability of network trafﬁc. The analysis process reconstructs the packets’ paths, achieving, in most cases, over 80% reconstruction. We also provide several functional, as well as performance, statistics regarding the network’s operation, including the sequence of events that occurred within routers and packet latencies observed per router and over time. Acknowledgments. This work was supported in part by CFAR, one of the six SRC STARnet Centers, sponsored by MARCO and DARPA, and NSF grant #1217764. "
High-performance energy-efficient NoC fabrics - Evolution and future challenges.,"As exa-scale microprocessor and SoC core and IP block counts increase, networks-on-chip are increasingly becoming performance and power limiters. Recent scaling and integration trends pose further challenges for on-die communication networks with topologies that have evolved from crossbars to rings to 2D meshes. These future challenges include i) reducing energy associated with global clock distribution, synchronization, and data storage, ii) adapting to process, voltage, and temperature variations, and iii) flexibility for different operating voltages, frequencies, and IP blocks. In this presentation, we will review some of the key network-on-chip scaling trends and challenges as well as discuss architecture and circuit solutions. Recent advancements implemented in 22nm tri-gate CMOS to demonstrate the combination of hybrid packet/circuit switching with source-synchronous operation to address these challenges by removing intra-route data storage and costly global clock distribution power will be presented.","High-Performance Energy-Eﬃcient NoC Fabrics: Evolution and Future Challenges Mark A. Anders Circuit Research Lab - Intel, United States Abstract. As exa-scale microprocessor and SoC core and IP block counts increase, networks-on-chip are increasingly becoming performance and power limiters. Recent scaling and integration trends pose further challenges for on-die communication networks with topologies that have evolved from crossbars to rings to 2D meshes. These future challenges include i) reducing energy associated with global clock distribution, synchronization, and data storage, ii) adapting to process, voltage, and temperature variations, and iii) ﬂexibility for diﬀerent operating voltages, frequencies, and IP blocks. In this presentation, we will review some of the key network-on-chip scaling trends and challenges as well as discuss architecture and circuit solutions. Recent advancements implemented in 22nm tri-gate CMOS to demonstrate the combination of hybrid packet/circuit switching with source-synchronous operation to address these challenges by removing intra-route data storage and costly global clock distribution power will be presented. Biography Mark A. Anders received his B.S. degree in 1998 and M.S. degree in 1999, both in Electrical Engineering from the University of Illinois at Urbana-Champaign. Since then, he has been with Intel Corporation’s Circuit Research Lab in Hillsboro, Oregon, where he is a Senior Staﬀ Research Scientist in the High-performance Circuits Research Group. He has published over 60 conference and journal papers and holds 40 issued patents. His research interests are in high-speed and low-power data-path, DSP, and on-chip interconnects and networks. He has been recognized as a top ISSCC paper contributer and received the 2012 ISSCC Distinguished Technical Paper award. "
Augmenting manycore programmable accelerators with photonic interconnect technology for the high-end embedded computing domain.,"There is today consensus on the fact that optical interconnects can relieve bandwidth density concerns at integrated circuit boundaries. However, when it comes to the extension of this emerging interconnect technology to on-chip communication as well, such consensus seems to fall apart. The main reason consists of a fundamental lack of compelling cases proving the superior performance and/or energy properties yielded by devices of practical interest, when re-architected around a photonically-integrated communication fabric. This paper takes its steps from the consideration that manycore computing platforms are gaining momentum in the high-end embedded computing domain in the form of general-purpose programmable accelerators. Hence, the performance and energy implications when augmenting these devices with optical interconnect technology are derived by means of an accurate benchmarking framework against an aggressively optimized electrical counterpart.","Augmenting Manycore Programmable Accelerators with Photonic Interconnect Technology for the High-End Embedded Computing Domain Marco Balboni † , Marta Ortin Obon § , Alessandro Capotondi ± , Herv ´e Fankem Tatenguem † , Alberto Ghiribaldi † , Luca Ramini † , Victor Vi ˜nal § , Andrea Marongiu ‡ , Davide Bertozzi † †University of Ferrara, ITALY; ±University of Bologna, ITALY; §University of Zaragoza, SPAIN; ‡ETH Zurich, SWISS emails: alessandro.capotondi@unibo.it; {ortin.marta, victor}@unizar.es, a.marongiu@iis.ee.ethz.ch; {marco.balboni, herve.tatenguemfankem, alberto.ghiribaldi, luca.ramini, davide.bertozzi}@unife.it; Abstract—There is today consensus on the fact that optical inprogrammable accelerator. In fact, driven by ﬂexibility, terconnects can relieve bandwidth density concerns at integrated performance and cost constraints of demanding modern circuit boundaries. However, when it comes to the extension of applications, heterogeneous Systems-on-Chip (SoCs) are this emerging interconnect technology to on-chip communication the dominant design paradigm in the embedded computing as well, such consensus seems to fall apart. The main reason domain. SoC architecture and heterogeneity clearly provide consists of a fundamental lack of compelling cases proving the a wider power/performance scaling, combining host CPUs superior performance and/or energy properties yielded by devices along with massively parallel general purpose programmable of practical interest, when re-architected around a photonicallyaccelerator (GPPA) fabrics. These latter hold potential of integrated communication fabric. This paper takes its steps from the consideration that manycore computing platforms are gaining bridging the gap between the energy efﬁciency (GOPS/W) momentum in the high-end embedded computing domain in the of hardwired hardware accelerators and the computational form of general-purpose programmable accelerators. Hence, the power delivered by throughput computing. In contrast to performance and energy implications when augmenting these graphics processing units, applicability of optical interconnect devices with optical interconnect technology are derived by means technology to GPPAs is faced with a more balanced trade-off of an accurate benchmarking framework against an aggressively between latency and throughput requirements, and by a optimized electrical counterpart. different usage model of the manycore device. The distinctive contributions of this paper are as follows: IN TRODUC T ION I . Optical interconnect technology has yielded a rich design space for on-chip communication architectures [1], [17], [19], [28]–[30], including the re-architecting of the DRAM memory sub-system [15], the revision of the processor-memory interface [27], or the development of new coherence protocols custom-tailored for the optical transport medium [28]. This signiﬁcant amount of work has ﬁnally contributed to the foundation of cross-layer design methodologies for designing new optical networks [5]. Unfortunately, the above experimental evidence has not translated into a stabilization of roadmaps for industrial uptake of this on-chip communication technology yet. This consideration is further exacerbated by the high cost targets for introducing it, and by the far-from-consolidating maturity of basic optical components. Fundamentally, the main challenge to revert this trend consists of showing a compelling advantage (if any) for on-chip nanophotonic interconnection networks (ONoCs) with accurate modeling assumptions, and while meeting the requirements and operating conditions of real-life user devices and workloads. A milestone contribution in this direction comes from [13], which aims at tackling the bandwidth and latency bottlenecks in on-chip interconnect and off-chip memory access in graphics processing units (GPUs) [14] by means of optical links and 3D-stacked technology. This paper follows the same track, and aims at extending the feasibility analysis of optical interconnect technology when integrated into industry-relevant objects. In particular, the focus is on the high-end embedded computing domain, where photonic networks have already been proven to be promising for DRAM memory access [40]. In this paper, a key component to sustain the performance-per-watt metric of embedded computing platforms is investigated as a candidate for photonic integration, namely a general-purpose manycore 1) We re-architect the communication infrastructure and the processor-to-interconnect interface in a GPPA architecture inspired by real devices, driven by the requirements of the system at hand. To our knowledge, this is the ﬁrst time insights and guidelines are given to exploit optical technology in emerging GPPAs. 2) Aware of the difﬁculty in making the case for purely optical interconnect fabrics, we conservatively and realistically come up with a hybrid architecture where speciﬁc kinds of transactions are selected for switching on the optical transport medium. 3) We carry out a performance characterization of system operations with the hybrid interconnect fabric, and benchmark it against a competitive electrical baseline. Our focus is not just on the performance of NoC read and write transactions, but rather on their aggregation into higher-order operations relevant for the system at hand (e.g., computation ofﬂoad, instruction cache reﬁlls, explicit data memory management). As a side effect, performance of such operations is not just determined by the system interconnect, but rather by the cooperation of several components (e.g., the DRAM subsystem, DMA architecture, memory hierarchy). This work captures such interdependency. 4) The ONoC architecture is designed by following a crosslayer design methodology, where the quality metrics of the selected design point include awareness of the degradation effect of place&route constraints over insertion loss, and the overhead of the upper layers of the optical network interface beyond the domain conversion circuits (e.g., buffering, ﬂow control, virtual channels, synchronization). 5) Energy efﬁciency ﬁgures are provided by accounting for the execution time of real-life workloads, for a parametric set of quality metrics for the fast-evolving optical devices, and for the energy of electrical components on a 40nm low-power industrial technology library (which makes the electrical counterpart extremely competitive). (cid:5)(cid:17)(cid:22)(cid:17)(cid:25)(cid:13)(cid:20)(cid:30)(cid:8)(cid:28)(cid:25)(cid:24)(cid:23)(cid:26)(cid:17)(cid:1) (cid:8)(cid:25)(cid:23)(cid:18)(cid:25)(cid:13)(cid:21)(cid:21)(cid:14)(cid:20)(cid:17)(cid:1) (cid:2)(cid:15)(cid:15)(cid:17)(cid:20)(cid:17)(cid:25)(cid:13)(cid:27)(cid:23)(cid:25)(cid:1) (cid:6)(cid:23)(cid:26)(cid:27)(cid:1)(cid:10)(cid:29)(cid:26)(cid:27)(cid:17)(cid:21)(cid:1) (cid:10)(cid:19)(cid:13)(cid:25)(cid:17)(cid:16)(cid:30)(cid:21)(cid:17)(cid:21)(cid:23)(cid:25)(cid:29)(cid:1)(cid:3)(cid:7)(cid:12)(cid:10)(cid:11)(cid:4)(cid:9)(cid:1) (cid:13)(cid:17)(cid:18)(cid:23)(cid:7)(cid:14)(cid:21)(cid:14)(cid:15)(cid:1)(cid:6)(cid:9)(cid:13)(cid:5)(cid:11)(cid:3)(cid:10)(cid:9)(cid:9)(cid:5)(cid:3)(cid:13)(cid:1) (cid:12)(cid:22)(cid:19)(cid:20)(cid:14)(cid:16)(cid:1)(cid:4)(cid:8)(cid:2)(cid:1) (cid:16)(cid:14)(cid:16)(cid:1)(cid:3)(cid:13)(cid:11)(cid:7)(cid:1) (cid:4)(cid:4)(cid:11)(cid:24)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:4)(cid:10)(cid:17)(cid:15)(cid:16)(cid:6)(cid:14)(cid:1) (cid:4)(cid:13)(cid:14)(cid:6)(cid:15)(cid:1) (cid:16)(cid:4)(cid:5)(cid:11)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:10)(cid:18)(cid:1)(cid:3)(cid:2)(cid:12)(cid:9)(cid:1) (cid:7)(cid:2)(cid:3)(cid:14)(cid:8)(cid:4)(cid:1) (cid:4)(cid:16)(cid:14)(cid:10)(cid:1) DUAL NoC  I/O INTERFACE  MASTER  PORT  SLAVE  PORT  Fig. 1. Heterogeneous (many-core accelerator-based) MPSoC architecture. Fig. 2. GPPA Architecture. I I . GPPA MOT IVAT ION In the latest heterogeneous Systems-on-Chip (SoC), and even more in future ones, the quest for processing specialization to deliver ultra-high performance acceleration at reduced energy cost does not necessarily imply hundreds of dedicated hardware accelerators [2]. There are at least a couple of reasons against that approach. On one hand, the performance of a specialized processing engine may in many cases be equally achieved by the parallel computation of programmable processing units [25]. Execution efﬁciency can thus be achieved without sacriﬁcing programmability. On the other hand, the trend towards simplifying the microarchitecture design of system building blocks is becoming increasingly strong. Only a replication-driven approach ultimately pays off in terms of design productivity. There are two main architecture families that might in principle suit the need for manycore programmable accelerators: the former one consists of GP-GPUs [47] and is optimized for the single instruction multiple data/thread execution model (SIMD/SIMT), while the latter one relies on the multiple instruction multiple data (MIMD) model (although not limited to it). MIMD programmable accelerators do not implement GPUlike data-parallel cores, with common fetch/decode phases which imply performance loss when parallel cores execute out of lock-step mode. They are rather independent RISC cores, well suited to execute both SIMD and MIMD types of parallelism. When coupled with a hierarchical organization into clusters like [14], [20], [35], such accelerators lend themselves to powerful programming abstractions such as nested parallelism [26]. One reason for the growing interest in manycore accelerators in the embedded computing domain is that there is a rapidly growing demand for a new type of interactions between the user and the device, based on understanding of the environment sensed in multiple manner (image, motion, sound, etc.) striving to create more friendly user interfaces (augmented reality, virtual reality, haptics, etc.). Despite the good degree of data parallelism, parallel threads in this class of applications usually expose a behavior which is heavily dependent on the local data content, resulting into many truly independent parallel computations [20]. In such a situation, GP-GPUs lose efﬁciency due to large divergence between threads. The above motivations are at the core of this paper’s decidion to investigate the potentials of optical interconnect technology in the context of ﬂexible MIMD/SIMD GeneralPurpose Programmable Accelerators for the high-end embedded computing domain. I I I . TARG E T ARCH I T EC TUR E A common embodiment of architectural heterogeneity is a template where a powerful general-purpose processor (usually called the host), is coupled to a general-purpose programmable manycore accelerator (GPPA) composed of several tens of simple processors, where critical computation kernels of an application can be ofﬂoaded to improve overall performance/watt [20], [36]–[38]. Figure 1 shows a block diagram of such a system. The focus of this paper is on GPPA manycore design, which we describe in details in the following subsections. A. Cluster Architecture The GPPA is a cluster-based many-core computing system. Clusters are the central building block of several recent manycores [34] [35] [20]. These processors consider a hierarchical design, where simple processing units (PU) are grouped into small-medium sized subsystems (the clusters) sharing highperformance local interconnect and L1 data memory. Scaling to larger system sizes is enabled by replicating clusters and interconnecting them with a scalable medium like a networkon-chip (NoC). The simpliﬁed block diagram of the target cluster is shown in the rightmost part of Figure 1. It contains several simple RISC32 processor cores (typically up to 16), each featuring a private instruction cache. Processors communicate through a multi-banked, multi-ported Tightly-Coupled Data Memory (TCDM). This shared L1 TCDM is implemented as explicitly managed SRAM banks (i.e., scratchpad memory), to which processors are interconnected through a low-latency, highbandwidth data interconnect. This is a very common design choice for constrained embedded manycores, as the area and power overheads of hardware-managed caches (as compared to scratchpads) is very signiﬁcant, and coherency protocols encounter severe scalability issues when interconnecting a large number of nodes. Figure 2 depicts the global GPPA architecture. It consists of a conﬁgurable number of computing clusters (up to 12 in our setup), interconnected by a 2-D mesh network-on-chip. The topology of the NoC is a simple n × n mesh. Each of the ﬁrst   12 nodes includes a computing cluster and a L2 bank. Another node hosts the “Fabric Controller”, a special cluster instance with a single processor acting as a main controller for the whole many-core platform. This node interacts directly with the host system, and is in charge of the boot sequence of other clusters and their operation control. It has the fundamental role of managing NoC routing reconﬁguration, setting up partitions and starting applications. Among the remaining three nodes, one switch is reserved to communications with an I while SPONoCs are more suitable for scenarios featuring longlasting connections. Among the possible WRONoC topologies, we selected an optical ring inspired by [12], which has been proven in [7] to minimize the design predictability gap between logic scheme and physical implementation. 2) Network Interface Architecture: This section describes the network interface (NI) architecture for the optical network as depicted in Figure 4. The reader is referred to [44] for the basic NI design principles, while only the customizations for a GPPA setting are hereafter discussed. Clearly, WRONoCs move most of their control logic to the NIs, since the switching fabric is in itself a non-blocking crossbar. The upper layers of NI architectures, beyond basic domain conversion circuitry, should therefore not be oversimpliﬁed with overly abstract models. To avoid message-dependent deadlock, every network interface needs separate buffering resources (virtual channels, VCs) for each message class. The requirement is instead met by construction in the optical switching fabric, since the lack of optical storage and the contention freedom automatically deliver the consumption assumption needed for deadlock freedom [9]. The ﬁnal buffering architecture stems from considering another requirement of wavelength routing: each initiator needs an output port for each possible target, and each target needs an input port for each possible source. Overall, each target comes with 2 FIFOs (the 2 VCs) for each potential initiator. At the transmission side, one optimization is feasible: the same 2 FIFOs are shared for all destinations and ﬂits are sent to different optical paths afterwards. For each one of those paths, there is an arbiter that grants access to the ONoC and keeps a credit count of the empty slots at the reception buffers. Therefore, by only replicating buffers at the target while sharing those at the initiator it is still possible to associate ﬂow control credits between every initiator-target pair. All the FIFOs at both the transmission and the reception side must be dual-clock to move data between the processor frequency domain (we assume 700MHz) and the one used inside the NI. As hereafter explained, the latter depends on the optical bit parallelism. We used the dc-FIFO architecture presented by [23] with a size that guarantees maximum throughput (5 slots at the transmission side). However, at the reception side, we must consider the round-trip latency in order to allow uninterrupted communications, ending up with 15-slot dc-FIFOs. After ﬂits are sent to the appropriate path depending on their destination, they need to be translated into a 10 GHz bit stream in order to be transmitted through the optical NoC. In fact, we assume 10 Gbit/sec modulation on each wavelength. This serialization process is parallelized to some extent to increase bandwidth and reduce latency. 3-bit parallelism means that 3 serializers of 11 bits each work in parallel to serialize the 32 bits of a ﬂit, resulting on a bandwidth of 30 Gbps. The bitparallelism determines the frequency inside the optical NI: 1.1 ns (0.1*number of bits) are needed to serialize a ﬂit with 3-bit parallelism, but only 0.8 ns are needed with 4-bit parallelism. In turn, this also impacts the size of the reception dc-FIFOs based on round-trip latency, which increases from 15 to 17 slots when moving from 3 to 4-bit parallelism. Another key issue to be considered in NIs is the resynchronization of received optical pulses with the clock signal of the electronic receiver. In this paper we assume sourcesynchronous communication, which implies that each pointto-point communication requires a strobe signal to be transmitted along with the data on a separate wavelength. With DC_FIFO 5slots dest DECOD D X U 6 M 1 x E 1 DC_FIFO 5slots dest DECOD D X U 6 M 1 x E 1 ELECTRONIC  TRANSMISSION  SIDE 1 x 3 X U M 11 BITSER 11 BITSER 11 BITSER Driver Driver Driver TSV TSV TSV OPTICAL SIDE RING MODULATOR (cid:1)11 (cid:1)12 (cid:1)13 credit (cid:2)it ARBITER credits MESOCHRONOUS SYNCHRONIZER CREDIT COUNTER BRUTE FORCE SYNC H1 H1 DC_FIFO 15slots DC_FIFO 15slots ARBITERs master slave L2 s e X U M H16 H16 ELECTRONIC CLOCK SOURCE TSV (cid:1)1clk OPTICAL NOC 3 x 1 X U M E D CLOCK DIVIDER 11 BITDESER 11 BITDESER 11 BITDESER COMP TIA COMP COMP COMP TIA TIA TIA TSV TSV TSV TSV PD PD PD PD (cid:1)1clk (cid:1)11 (cid:1)12 (cid:1)13 ELECTRONIC  RECEPTION SIDE 700 MHz (1.43 ns/cycle) 0.9 GHz (1.1 ns/cycle) 10 GHz (0.1 ns/cycle) Fig. 4. Optical Network Interface Architecture with 3-bit parallelism. current technology, this seems to be the most realistic solution, especially considering the promising research effort that is currently being devoted to transmitting clock signals across an optical medium [24]. The received source-synchronous clock at the reception side of the NI is then used to drive the deserializers and, after a clock divider, the front-end of the dcFIFOs. We assume that a form of clock gating is implemented, therefore when no data is transmitted, the optical clock signal is gated. Similarly, we assume clock gating for all electronic components, both in the baseline and in the hybrid interconnect solution. When it comes to backpressure management, we opt for creditbased ﬂow control because credit tokens can reuse the existing communication paths. E. The Top-Level NoC In order to model the interconnection of the GPPA with host processor, L3 memory and system DMA at the top-level of the hierarchy, we use another xpipesLite switch connecting the above blocks together. IV. U SAG E MOD E L This section describes all kinds of communication scenarios investigated in this work. A. Ofﬂoading Scenario When a host application wants to ofﬂoad some computational kernels to the GPPA, it needs to collect code (the kernel executable) and data (e.g., pointers to data in L3) into metadata structures that are forwarded to the fabric controller (FC). This is done on the host processors, which initiates a copy of the kernel executable through the system DMA into the GPPA L2 memory, from which the cores inside the cluster can fetch instructions. It has to be underlined that the cost of ofﬂoading computation to the GPPA should be kept as small as possible, otherwise it may completely hide all the beneﬁts introduced by code acceleration. Since a relevant portion of the ofﬂoad cost is in the executable copy, it is important that the sustainable bandwidth to accomplish this operation is high. B. Partitioning Scenario The heterogeneous MPSoC system described in Section III features a powerful, virtualization-ready host processor. The host is capable of running multiple guest operating systems (or virtual machines, VM), each of which can potentially require the GPPA to accelerate parts of the applications it is     (cid:33)(cid:1) (cid:33)(cid:30)(cid:35)(cid:1) (cid:33)(cid:30)(cid:37)(cid:1) (cid:33)(cid:30)(cid:39)(cid:1) (cid:33)(cid:30)(cid:40)(cid:1) (cid:34)(cid:1) (cid:34)(cid:30)(cid:35)(cid:1) (cid:34)(cid:30)(cid:37)(cid:1) (cid:34)(cid:30)(cid:40)(cid:1) (cid:34)(cid:30)(cid:39)(cid:1) (cid:39)(cid:1) (cid:38)(cid:1) (cid:37)(cid:1) (cid:36)(cid:1) (cid:35)(cid:1) (cid:34)(cid:1) (cid:39)(cid:1) (cid:20)(cid:21)(cid:30)(cid:1)(cid:21)(cid:15)(cid:1)(cid:28)(cid:9)(cid:21)(cid:4)(cid:1)(cid:16)(cid:21)(cid:22)(cid:24)(cid:1) (cid:38)(cid:1) (cid:37)(cid:1) (cid:36)(cid:1) (cid:35)(cid:1) (cid:34)(cid:1) (cid:21) (cid:20) (cid:23) (cid:19) (cid:11) (cid:17) (cid:18) (cid:28) (cid:11) (cid:13) (cid:13) (cid:14) (cid:1) (cid:25) (cid:11) (cid:1) (cid:15) (cid:14) (cid:12) (cid:25) (cid:26) (cid:16) (cid:1) (cid:19) (cid:14) (cid:1) (cid:28)(cid:9)(cid:21)(cid:4)(cid:1)(cid:3) (cid:75)(cid:9)(cid:21)(cid:4)(cid:1) (cid:7)(cid:10)(cid:4)(cid:2)(cid:7)(cid:1)(cid:5)(cid:8)(cid:2)(cid:1) (cid:6)(cid:7)(cid:10)(cid:3)(cid:2)(cid:7)(cid:1)(cid:5)(cid:8)(cid:2)(cid:1) Fig. 7. Fetching time for 16B data chunks. (cid:33)(cid:1) (cid:33)(cid:30)(cid:35)(cid:1) (cid:33)(cid:30)(cid:37)(cid:1) (cid:33)(cid:30)(cid:39)(cid:1) (cid:33)(cid:30)(cid:40)(cid:1) (cid:34)(cid:1) (cid:34)(cid:30)(cid:35)(cid:1) (cid:34)(cid:30)(cid:37)(cid:1) (cid:39)(cid:1) (cid:38)(cid:1) (cid:37)(cid:1) (cid:36)(cid:1) (cid:35)(cid:1) (cid:34)(cid:1) (cid:39)(cid:1) (cid:20)(cid:21)(cid:30)(cid:1)(cid:21)(cid:15)(cid:1)(cid:28)(cid:9)(cid:21)(cid:4)(cid:1)(cid:16)(cid:21)(cid:22)(cid:24)(cid:1) (cid:38)(cid:1) (cid:37)(cid:1) (cid:36)(cid:1) (cid:35)(cid:1) (cid:34)(cid:1) (cid:21) (cid:20) (cid:23) (cid:19) (cid:11) (cid:17) (cid:18) (cid:28) (cid:11) (cid:13) (cid:13) (cid:14) (cid:1) (cid:25) (cid:11) (cid:15) (cid:1) (cid:14) (cid:12) (cid:25) (cid:26) (cid:16) (cid:1) (cid:19) (cid:14) (cid:1) (cid:28)(cid:9)(cid:21)(cid:4)(cid:1)(cid:3) (cid:75)(cid:9)(cid:21)(cid:4)(cid:1) (cid:7)(cid:10)(cid:4)(cid:2)(cid:7)(cid:1)(cid:5)(cid:8)(cid:2)(cid:1) (cid:6)(cid:7)(cid:10)(cid:3)(cid:2)(cid:7)(cid:1)(cid:5)(cid:8)(cid:2)(cid:1) Fig. 8. Fetching time for 5kB data chunks. larger number of hops (2), as showed in the ﬁgure. However, this prevents application of dynamic voltage and frequency scaling policies that decouple processor speed from network speed. C. Data Fetching from L3 There are two options for explicitily-managed data fetching at runtime. First, the within-cluster local DMA can be programmed to perform a read transaction from L3. Second, the system DMA can be instructed to do the same thing. However, data is then written (not read) into the GPPA (either in L2 or in L1). Figure 7 reports normalized data fetch time for small data sets (16 bytes per fetch) when the local DMA is used as opposed to the global DMA, as a function of the number of hops to the GPPA I/O interface. DMA programming time is included in the reported results. Clearly, the system DMA is not effective for this case since the programming time of the DMA cannot be amortized over a large data transfer time, except for short range communications, where the performance difference is not signiﬁcant. Interestingly, the ONoC can preserve this condition and make it independent of fetching core position in the network. As a result, the choice of the local vs. global DMA is almost irrelevant in the presence of an ONoC as global transport medium. In contrast, when the fetched data set is signiﬁcant (5k bytes, see Figure 8), the system DMA is clearly the right choice for a twofold reason. First, the programming time can be more easily amortized. Second, the DMA is closer to the L3, hence preventing read requests for L3 from going through the GPPA interconnect. Consequently, only read responses are forward by the system DMA to the GPPA in the form of write transactions to cluster L1 (or L2). (cid:34)(cid:28)(cid:34)(cid:34)(cid:3)(cid:44)(cid:34)(cid:34)(cid:1) (cid:36)(cid:28)(cid:34)(cid:34)(cid:3)(cid:29)(cid:34)(cid:35)(cid:1) (cid:38)(cid:28)(cid:34)(cid:34)(cid:3)(cid:29)(cid:34)(cid:35)(cid:1) (cid:40)(cid:28)(cid:34)(cid:34)(cid:3)(cid:29)(cid:34)(cid:35)(cid:1) (cid:42)(cid:28)(cid:34)(cid:34)(cid:3)(cid:29)(cid:34)(cid:35)(cid:1) (cid:35)(cid:28)(cid:34)(cid:34)(cid:3)(cid:44)(cid:34)(cid:34)(cid:1) (cid:35)(cid:28)(cid:36)(cid:34)(cid:3)(cid:44)(cid:34)(cid:34)(cid:1) (cid:4)(cid:18)(cid:20)(cid:11)(cid:10)(cid:18)(cid:1)(cid:8)(cid:20)(cid:2)(cid:30)(cid:1)(cid:8)(cid:13)(cid:23)(cid:25)(cid:20)(cid:21)(cid:17)(cid:1)(cid:6)(cid:19)(cid:23)(cid:13)(cid:21)(cid:14)(cid:10)(cid:12)(cid:13)(cid:22)(cid:31)(cid:1) (cid:4)(cid:18)(cid:20)(cid:11)(cid:10)(cid:18)(cid:1)(cid:8)(cid:20)(cid:2)(cid:30)(cid:1)(cid:8)(cid:13)(cid:23)(cid:25)(cid:20)(cid:21)(cid:17)(cid:31)(cid:1) (cid:7)(cid:20)(cid:12)(cid:10)(cid:18)(cid:1)(cid:3)(cid:8)(cid:20)(cid:2)(cid:1) (cid:3)(cid:8)(cid:20)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:26)(cid:11)(cid:28)(cid:1) (cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:35)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:36)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:37)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:38)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:35)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:36)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:37)(cid:11)(cid:16)(cid:23)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:38)(cid:11)(cid:16)(cid:23)(cid:1) (cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:20)(cid:19)(cid:22)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:20)(cid:19)(cid:22)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:20)(cid:19)(cid:22)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:12)(cid:20)(cid:19)(cid:22)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:15)(cid:15)(cid:21)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:15)(cid:15)(cid:21)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:15)(cid:15)(cid:21)(cid:28)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:15)(cid:15)(cid:21)(cid:28)(cid:1)(cid:1) Fig. 9. Static power for the compound ENoC vs. hybrid ONoC variants. (cid:9)(cid:10)(cid:24)(cid:22)(cid:1) (cid:34)(cid:27)(cid:37)(cid:40)(cid:41)(cid:1) (cid:34)(cid:27)(cid:40)(cid:34)(cid:40)(cid:1) (cid:34)(cid:27)(cid:41)(cid:40)(cid:43)(cid:1) (cid:34)(cid:27)(cid:43)(cid:37)(cid:38)(cid:1) (cid:35)(cid:27)(cid:35)(cid:35)(cid:41)(cid:1) (cid:34)(cid:27)(cid:37)(cid:42)(cid:37)(cid:1) (cid:34)(cid:27)(cid:38)(cid:37)(cid:39)(cid:1) (cid:34)(cid:27)(cid:38)(cid:42)(cid:42)(cid:1) (cid:34)(cid:27)(cid:39)(cid:40)(cid:34)(cid:1) (cid:38)(cid:1) (cid:40)(cid:38)(cid:38)(cid:1) (cid:42)(cid:38)(cid:38)(cid:1) (cid:44)(cid:38)(cid:38)(cid:1) (cid:45)(cid:38)(cid:38)(cid:1) (cid:39)(cid:38)(cid:38)(cid:38)(cid:1) (cid:39)(cid:40)(cid:38)(cid:38)(cid:1) (cid:39)(cid:42)(cid:38)(cid:38)(cid:1) (cid:39)(cid:44)(cid:38)(cid:38)(cid:1) (cid:8)(cid:23)(cid:15)(cid:13)(cid:21)(cid:1)(cid:4)(cid:9)(cid:23)(cid:3)(cid:1) (cid:5)(cid:21)(cid:23)(cid:14)(cid:13)(cid:21)(cid:1)(cid:4)(cid:9)(cid:23)(cid:3)(cid:1)(cid:6)(cid:31)(cid:14)(cid:33)(cid:1)(cid:39)(cid:35)(cid:14)(cid:20)(cid:27)(cid:1)(cid:34)(cid:34)(cid:1)(cid:6)(cid:31)(cid:14)(cid:33)(cid:1)(cid:40)(cid:14)(cid:20)(cid:27)(cid:1)(cid:34)(cid:34)(cid:1)(cid:6)(cid:31)(cid:14)(cid:33)(cid:1)(cid:41)(cid:35)(cid:14)(cid:20)(cid:27)(cid:1)(cid:34)(cid:34) (cid:1)(cid:6)(cid:31)(cid:14)(cid:33)(cid:1)(cid:42)(cid:35)(cid:14)(cid:20)(cid:27)(cid:1)(cid:34)(cid:34)(cid:1) (cid:39)(cid:1)(cid:6)(cid:10)(cid:11)(cid:1) (cid:40)(cid:1)(cid:6)(cid:10)(cid:11)(cid:26)(cid:1) (cid:41)(cid:1)(cid:6)(cid:10)(cid:11)(cid:26)(cid:1) (cid:42)(cid:1)(cid:6)(cid:10)(cid:11)(cid:26)(cid:1) (cid:43)(cid:1)(cid:6)(cid:10)(cid:11)(cid:26)(cid:1) (cid:44)(cid:1)(cid:6)(cid:10)(cid:11)(cid:26)(cid:1) (cid:3)(cid:23)(cid:22)(cid:26)(cid:16)(cid:25)(cid:30)(cid:13)(cid:28)(cid:30)(cid:16)(cid:1)(cid:10)(cid:24)(cid:28)(cid:15)(cid:13)(cid:21)(cid:1)(cid:12)(cid:16)(cid:15)(cid:19)(cid:22)(cid:23)(cid:21)(cid:23)(cid:18)(cid:31)(cid:1) (cid:2)(cid:18)(cid:18)(cid:25)(cid:16)(cid:26)(cid:26)(cid:20)(cid:30)(cid:16)(cid:1)(cid:10)(cid:24)(cid:28)(cid:15)(cid:13)(cid:21)(cid:1)(cid:12)(cid:16)(cid:15)(cid:19)(cid:22)(cid:23)(cid:21)(cid:23)(cid:18)(cid:31)(cid:1) (cid:17)(cid:7)(cid:34)(cid:14)(cid:20)(cid:27)(cid:1) Fig. 10. Dynamic power for the NoCs under test. The compound ENoC is broken down into its local and global networks, and so is the hybrid NoC. D. Power Analysis All of the electronic components (both in the ENoC and in the hybrid NoC) have been synthesized, placed and routed on a low-power 40nm industrial technology library in order to provide realistic power measurements. Clock gating was applied. Packetizers and depacketizers have not been considered since they are the same in both interconnects under test. The static power of the hybrid NoC is derived from the composition of the power consumption of all its subblocks such as local ENoC plus ONoC NI components (frequency converters, muxes and demuxes, SERs and DESERs as well as all blocks required for ﬂow control). The static power contribution of all optical devices is then given by: laser sources (assumed to be off-chip, yet included in the power budget), thermal tuning, transmitters, receivers, and optical clock support. In order to deliver contention-free full connectivity across 256 optical paths, 13 laser sources, 256 transmitters and receivers as well as 768 MRRs are needed. This hardware cost should be replicated for each bit of parallelism, except for the optical clock support, which is shared among the bit-parallel streams. Figure 9 compares the total static power of ENoC vs. hybrid NoC assuming two distinct sets of parameter values for the basic optical components, namely conservative and aggressive technologies (we use the same physical parameter values reported in [8]). They reﬂect state-of-the-art silicon photonics as opposed to optimistic predictions for future device evolution, and are based on the projections in [15]. With a conservative optical technology, the ENoC is clearly more power efﬁcient than ONoC regardless the bit parallelism. (cid:38)(cid:33)(cid:41)(cid:1) (cid:38)(cid:33)(cid:39)(cid:1) (cid:38)(cid:1) (cid:37)(cid:33)(cid:45)(cid:1) (cid:37)(cid:33)(cid:43)(cid:1) (cid:37)(cid:33)(cid:41)(cid:1) (cid:37)(cid:33)(cid:39)(cid:1) (cid:37)(cid:1) (cid:38)(cid:1) (cid:39)(cid:1) (cid:40)(cid:1) (cid:41)(cid:1) (cid:42)(cid:1) (cid:43)(cid:1) (cid:44)(cid:1) (cid:34)(cid:16)(cid:21)(cid:30)(cid:26)(cid:27)(cid:18)(cid:25)(cid:1)(cid:7)(cid:5)(cid:1) (cid:45)(cid:1) (cid:46)(cid:1) (cid:38)(cid:37)(cid:1) (cid:38)(cid:38)(cid:1) (cid:38)(cid:39)(cid:1) (cid:9) (cid:16) (cid:31) (cid:16) (cid:21) (cid:18) (cid:26) (cid:1) (cid:3)(cid:24)(cid:24)(cid:27)(cid:1) (cid:10)(cid:19)(cid:24)(cid:15)(cid:17)(cid:1) (cid:10)(cid:9)(cid:11)(cid:1)(cid:12)(cid:30)(cid:28)(cid:22)(cid:18)(cid:1) (cid:4)(cid:13)(cid:4)(cid:47)(cid:14)(cid:6)(cid:12)(cid:47)(cid:9)(cid:10)(cid:9)(cid:1)(cid:8)(cid:18)(cid:25)(cid:23)(cid:18)(cid:21)(cid:1) (cid:13)(cid:31)(cid:23)(cid:16)(cid:20)(cid:1) (cid:2)(cid:5)(cid:5)(cid:1)(cid:8)(cid:18)(cid:25)(cid:23)(cid:18)(cid:21)(cid:1) (a) ENoC. (cid:37)(cid:1) (cid:37)(cid:33)(cid:39)(cid:1) (cid:37)(cid:33)(cid:41)(cid:1) (cid:37)(cid:33)(cid:43)(cid:1) (cid:37)(cid:33)(cid:45)(cid:1) (cid:38)(cid:1) (cid:38)(cid:33)(cid:39)(cid:1) (cid:38)(cid:1) (cid:39)(cid:1) (cid:40)(cid:1) (cid:41)(cid:1) (cid:42)(cid:1) (cid:43)(cid:1) (cid:44)(cid:1) (cid:34)(cid:16)(cid:21)(cid:30)(cid:26)(cid:27)(cid:18)(cid:25)(cid:1)(cid:7)(cid:5)(cid:1) (cid:45)(cid:1) (cid:46)(cid:1) (cid:38)(cid:37)(cid:1) (cid:38)(cid:38)(cid:1) (cid:38)(cid:39)(cid:1) (cid:9) (cid:18) (cid:16) (cid:31) (cid:16) (cid:21) (cid:26) (cid:1) (cid:3)(cid:24)(cid:24)(cid:27)(cid:1) (cid:10)(cid:19)(cid:24)(cid:15)(cid:17)(cid:1) (cid:10)(cid:9)(cid:11)(cid:1)(cid:12)(cid:30)(cid:28)(cid:22)(cid:18)(cid:1) (cid:4)(cid:13)(cid:4)(cid:47)(cid:14)(cid:6)(cid:12)(cid:47)(cid:9)(cid:10)(cid:9)(cid:1)(cid:8)(cid:18)(cid:25)(cid:23)(cid:18)(cid:21)(cid:1) (cid:13)(cid:31)(cid:23)(cid:16)(cid:20)(cid:1) (cid:2)(cid:5)(cid:5)(cid:1)(cid:8)(cid:18)(cid:25)(cid:23)(cid:18)(cid:21)(cid:1) (b) Hybrid NoC. Fig. 11. Color Tracking execution distribution among clusters. This is mainly due to the higher static power overhead consumed by all of optical devices in the network, especially by laser sources. In constrast, such an overhead is mitigated when an aggressive optical technology is considered. More precisely, the ONoC differs from only 4% up to 35% with respect to the ENoC counterpart depending on the bit parallelism. We then computed energy-per-bit required for transmitting data over the alternative switching fabrics. For the ENoC, it amounts to 209 fJ/bit/switch. Figure 10 shows the energy-perbit comparison between the ENoC and the hybrid NoC for each bit parallelism (1,2,3 and 4). As can be seen, the hybrid NoC has position-independent results and turns out to be more energy efﬁcient than ENoC (up to an order of magnitude) regardless the speciﬁc optical technology, thus conﬁrming that, at least in terms of enegy-per-bit, the ONoC is deﬁnitely outof-reach. V I . A P P L ICAT ION B ENCHMARK ING We compare execution time between ENoC- vs. hybrid NoC-based GPPA platforms for real workloads. Our benchmarks are two common computer vision applications: color tracking, implemented from the open source computer vision library (OpenCV) for single color tracking, and FAST [4], which is a corner detection for image features extraction. Color tracking consists mainly of four kernels: color space conversion (CSC), threshold (THR), moments computation (MOM), and ﬁnally a pixel-wise addition (ADD) on the input image of the tracking segments. Without lack of generality, both applications are mapped on the GPPA as a whole, thus emulating a 12-cluster partition that cooperatively process the computational task. 2 cores are active in each cluster. The same program is executed on each cluster, but fed by different image portions. The benchmarks should be monitored with respect to different features. In color tracking the execution is independent of the processing data, hence NUMA effects are in principle more visible. In FAST, although (cid:30)(cid:1) (cid:30)(cid:26)(cid:35)(cid:1) (cid:31)(cid:1) (cid:31)(cid:26)(cid:35)(cid:1) (cid:32)(cid:1) (cid:32)(cid:26)(cid:35)(cid:1) (cid:33)(cid:1) (cid:31)(cid:1) (cid:32)(cid:1) (cid:33)(cid:1) (cid:34)(cid:1) (cid:35)(cid:1) (cid:36)(cid:1) (cid:37)(cid:1) (cid:38)(cid:1) (cid:39)(cid:1) (cid:31)(cid:30)(cid:1) (cid:31)(cid:31)(cid:1) (cid:31)(cid:32)(cid:1) (cid:8) (cid:22) (cid:16) (cid:14) (cid:25) (cid:14) (cid:18) (cid:1) (cid:27)(cid:14)(cid:18)(cid:24)(cid:22)(cid:23)(cid:16)(cid:21)(cid:1)(cid:6)(cid:4)(cid:1) (cid:3)(cid:20)(cid:20)(cid:23)(cid:1) (cid:9)(cid:17)(cid:20)(cid:13)(cid:15)(cid:1) (cid:9)(cid:8)(cid:10)(cid:1)(cid:90)(cid:437)(cid:374)(cid:410)(cid:349)(cid:373)(cid:286)(cid:1) (cid:5)(cid:2)(cid:11)(cid:12)(cid:1)(cid:7)(cid:16)(cid:21)(cid:19)(cid:16)(cid:18)(cid:1) (a) ENoC. (cid:35)(cid:1) (cid:35)(cid:31)(cid:40)(cid:1) (cid:36)(cid:1) (cid:36)(cid:31)(cid:40)(cid:1) (cid:37)(cid:1) (cid:37)(cid:31)(cid:40)(cid:1) (cid:38)(cid:1) (cid:36)(cid:1) (cid:37)(cid:1) (cid:38)(cid:1) (cid:39)(cid:1) (cid:40)(cid:1) (cid:41)(cid:1) (cid:42)(cid:1) (cid:43)(cid:1) (cid:44)(cid:1) (cid:36)(cid:35)(cid:1) (cid:36)(cid:36)(cid:1) (cid:36)(cid:37)(cid:1) (cid:8) (cid:24) (cid:17) (cid:15) (cid:29) (cid:15) (cid:19) (cid:1) (cid:32)(cid:15)(cid:19)(cid:28)(cid:24)(cid:25)(cid:17)(cid:23)(cid:1)(cid:6)(cid:4)(cid:1) (cid:3)(cid:22)(cid:22)(cid:25)(cid:1) (cid:9)(cid:18)(cid:22)(cid:14)(cid:16)(cid:1) (cid:9)(cid:8)(cid:10)(cid:1)(cid:11)(cid:28)(cid:21)(cid:26)(cid:20)(cid:17)(cid:1) (cid:5)(cid:2)(cid:12)(cid:13)(cid:1)(cid:7)(cid:17)(cid:23)(cid:21)(cid:17)(cid:19)(cid:1) (b) Hybrid NoC. Fig. 12. FAST execution distribution among clusters. clusters process the same amount of pixels, the execution ﬂow depends on the actual pixel content, hence potentially leading to divergence between clusters. The plots in Figure 11 compare the execution time on each cluster, expressed in Mcycles, to perform the color tracking on a single QVGA 24-bit input frame on the two platforms under test. The hybrid NoC improves the execution time by 18.1% with respect to the baseline ENoC. The hybrid NoC execution beneﬁts also from an improvement of alignment due the abatement of NUMA effects. This is more evident on the ADD kernel, which is memory dominated. Please note that the OMP contribution consists of the overhead for the OpenMP runtime environment (OMP) [46], and that the ofﬂoad time is considered as well. Figure 12 shows the same analysis on FAST. The application consists of a single kernel which works using a stencil pattern of accesses. Even in this case, the image is splitted into independent stripes, so that the computation is distributed among the clusters. Also the runtime (OMP) overhead is still there. The bottom chart shows the execution time in Mcycles on each cluster of the hybrid NoC platform on a QVGA 24-bit image. At the top, the same application is using the ENoC platform. In this case, the hybrid NoC improves the execution time by 16.5%. V I I . CONC LU S ION S The paper proposes the ﬁrst assessment of optical interconnect technology in the context of GPPA devices for the highend embedded computing domain. The system is re-architected around an optical interconnect fabric, under a realistic hybrid integration strategy. When put at work with realistic workloads, the photonically-integrated GPPA turns out to be extremely effective in speeding up application execution by at least 15%. This translates into a static power overhead of 2.5x, which is however expected to go down to 1.3x with future silicon photonic technology. In contrast, the ONoC results more energy efﬁcient than the ENoC counterpart (up to an order of magnitude) regardless the speciﬁc optical technology, thus conﬁrming that in terms of energy-per-bit, the ONoC is deﬁnitely hard to beat. Overall, the above quality metrics paint a promising picture for augmenting GPPAs with optical devices, while clearly pointing to the most important candidate for optimization: static energy reduction through technology evolution as well as gating techniques. V I I I . ACKNOW L EDG EM EN T This research has been partly supported by the 7th Framework Program of the European Union through the vIrtical Project, under Grant Agreement 288574, and by the PHOTONICA project (RBFR08LE6V) under the FIRB 2008 program, funded by the italian government. [6] "
Dynamic synchronizer flip-flop performance in FinFET technologies.,"The use of fine-grain Dynamic Voltage and Frequency Scaling (DVFS) has increased the number of distinct clock domains on a given Network-on-Chip (NoC). This necessitates robust synchronizers to prevent clock domain communication failures, even as FinFET devices have begun to replace planar devices. This paper presents simulation results and comparisons between dynamic (requiring reset) and non-dynamic synchronizer flip-flops implemented in predictive models for both planar technologies and future FinFET technologies. Results demonstrate that synchronizers built with FinFET devices 1) exhibit a tau value which continues to scale with fan-out of four delay and 2) can be improved with forward biasing, but 3) are more sensitive to temperature. Dynamic flip-flops settled metastability fastest when using standard technology voltages, but previously couldn't be used in non-dynamic systems. For this reason, a new synchronizer design is also presented which exploits the benefits of dynamic flip-flops without the need for a dedicated reset signal.","Dynamic Synchronizer Flip-Flop Performance in FinFET Technologies Mark Buckler‡∗, Arpan Vaidya†, Xiaobin Liu†, and Wayne Burleson†‡ †University of Massachusetts, Amherst, MA, USA ‡AMD Research, Boxborough, MA, USA ∗Cornell University, Ithaca, NY, USA mab598@cornell.edu, {avaidya, and xlu}@umass.edu Abstract—The use of ﬁne-grain Dynamic Voltage and Frequency Scaling (DVFS) has increased the number of distinct clock domains on a given Network-on-Chip (NoC). This necessitates robust synchronizers to prevent clock domain communication failures, even as FinFET devices have begun to replace planar devices. This paper presents simulation results and comparisons between dynamic (requiring reset) and non-dynamic synchronizer ﬂip-ﬂops implemented in predictive models for both planar technologies and future FinFET technologies. Results demonstrate that synchronizers built with FinFET devices 1) exhibit a tau value which continues to scale with fan-out of four delay and 2) can be improved with forward biasing, but 3) are more sensitive to temperature. Dynamic ﬂip-ﬂops settled metastability fastest when using standard technology voltages, but previously couldn’t be used in non-dynamic systems. For this reason, a new synchronizer design is also presented which exploits the beneﬁts of dynamic ﬂip-ﬂops without the need for a dedicated reset signal. Keywords—NoC, GALS, Synchronizers, Metastability, FinFET IN TRODUC T ION I . Both the increase in the number of independent cores per system, and the use of ﬁne grained Dynamic Voltage and Frequency Scaling (DVFS), has led to an increase in the number of clock domains per Network-on-Chip (NoC). Communication across these clock domains often requires the use of ”brute force” synchronizers which rely on highly optimized synchronizer ﬂip-ﬂops connected in series. As CMOS continues to scale, circuit designers must re-evaluate design trade-offs for each new technology node. Now that semiconductor manufacturers are moving from planar CMOS to FinFET CMOS, circuit designers must prepare for changes of an even larger scale. Previous work has focused on the effect of technology scaling on synchronizers in planar technology [1]–[5], but this paper aims to demonstrate the effects of FinFET devices. To evaluate their effects, HSPICE simulations using predictive technology models developed at ASU [6] were performed and compared with theoretical analysis. While onchip synchronizer evaluation techniques have been developed [7], building test chips for multiple technology nodes would be both prohibitively expensive and impossible for nodes not yet in production (such as 10nm and 7nm). This paper’s key contributions include simulation results and theoretical analysis on three high performance synchronizer ﬂip-ﬂops in ﬁve different technology nodes. The simulation results presented in this paper demonstrate the continual relationship between tau (τ ) and fan-out of four (F O4) delay even in FinFET devices. It was also found that body biasing synchronizer feedback loops is still effective in FinFET devices, but not as signiﬁcant as with planar devices. FinFET synchronizers were also observed to be more sensitive to temperature variations. The Dynamic Latch Flip-Flop performed the best at standard voltage levels. For this reason, this paper also presents The Dynamic Synchronizer, which enables the use of dynamic synchronizing ﬂip-ﬂops in non-dynamic systems. I I . BACKGROUND When communicating across clock domains the transmitted data and receiving clock are not aligned. Therefore setup and hold times are likely to be violated, possibly causing metastability. Metastability occurs when the bi-stable element within the ﬂip-ﬂop does not have enough time to fully charge or discharge, resulting in a half-charged element. This halfcharge is usually very close to ( VDD 2 ), and is not logic high or logic low. The element will eventually settle, but it is impossible to predict when the settling will occur or which state it will settle to. Brute force synchronizers are designed as a chain of ﬂip-ﬂops connected in series, and reduce the possibility of metastability entering the receive domain’s logic. Each additional ﬂip-ﬂop reduces the possibility of metastability, but the synchronizer will always retain a ﬁnite chance of failure (metastability manifesting on the output). Since additional ﬂip-ﬂops also result in an increased latency through the synchronizer, a trade-off between latency and reliability exists. To quantify the frequency of these failures, brute force synchronizer reliability is commonly expressed as the Mean Time Between Failures (MTBF). The expression in Equation (1) shows dependence on the receiver clock frequency (FC ) and incoming data frequency (FD ). These factors are independent of the ﬂip-ﬂops within the synchronizer, and so these factors are assumed to not change with technology scaling. The factor S is deﬁned as the settling time, and is often simpliﬁed as the receiver clock period multiplied by the number of ﬂipﬂop stages. This is generally a design parameter which can be varied to achieve the desired MTBF. Since this factor is not dependent on the properties of the individual ﬂip-ﬂops, it is unaffected by process technology. M T BF = τ e S TW FC FD (1) TW (the sensitive region) and τ (evaluation time constant) are both dependent on the individual ﬂip-ﬂop however, and are therefore affected by changes in process technology. Although some papers have considered the effects of TW [7], TW only affects MTBF linearly while τ is exponential. For this reason, much of the past work has chosen to focus on τ rather than TW [8], [9]. More complex expressions have been proposed for accurately calculating the MTBF of multi-stage synchronizers; but these expressions still show that τ is the most dominant factor [10]. The value of τ can be affected by many environmental factors, but circuit structure plays a central role. Two circuit parameters of particular importance are transistor strength and node capacitance. A number of circuit topologies have been designed to minimize the necessary node capacitance, while also improving the gain of the ﬂip-ﬂop feedback loop [2], [9], [11]–[14]. Device sizing is also important, and so a multitude of techniques have been proposed to effectively manage the tradeoff between device strength and size [2], [13], [15]. Some techniques have also considered adding supplementary devices but these studies have had mixed results [14], [16]. Naturally, both transistor strength and node capacitance change as technology scales. One key parameter used to compare technology nodes is the fan out of four delay (commonly referred to as F O4). Since the early days of CMOS, designers have tracked both F O4 and τ , ﬁnding them to scale together [1]. As planar technology has continue to scale, some researchers claim that this trend has continued [2], [3] while others claim that after 65nm this is no longer the case [4], [5]. In this paper we demonstrate through theoretical analysis and simulation results that F O4 and τ , do continue to scale together both in planar and in FinFET technology. FinFET devices offer a number of signiﬁcant beneﬁts in addition to allowing the continuation of Moore’s Law. Two of the most well-known beneﬁts of FinFET technology are decreased leakage current and less variation in V t between process corners [17], [18]. All circuits will beneﬁt from a decrease in leakage current, including synchronizers. Leakage is not a dominant effect on synchronizer performance however, and so this particular improvement is not considered in this work. The effect of tighter process corners will also have an impact of synchronizers since they must be designed for the worst case. The end result of this is expected to be an overall improvement in synchronizer performance, but would not affect design trade-offs. Process corners are also heavily deﬁned by laboratory experimentation, and therefore future technology simulation models do not consider them [6]. I I I . EX PER IM EN TA L M E THODO LOGY A. Simulating τ The HSPICE simulation strategy used to generate the results found in this paper was modeled on a hardware testing methodology developed by Dike and Burton [9]. A similar simulation based adaption of this technique has recently been used in other synchronizer design research [8]. In the original hardware technique, memory cell nodes are ﬁrst forced into metastable levels. For simulation, this is accomplished with initial conditions. Once forced into metastability, measurements are taken as the circuit begins to self-correct back to a stable Fig. 1: Synchronizer Bi-stable Element Circuit Response value (a logical high or low value). The speed at which the circuit can self-correct is recorded, and used to calculate the value of τ . This gives an accurate value without needing to rely on simulation methods which require data vs. clock time sweeps. Data vs. clock time sweep τ simulation methods have proven to be unreasonable given HSPICE’s sensitivity [9]. Bi-stable devices within synchronizer ﬂip-ﬂops generally have two sides. This is most easily understood when considering an inverter loop. When stable, the voltage of one side of the loop is at VDD while the other is GN D . When both sides are forced to be VDD 2 , the voltage difference between the two nodes is zero. As the bi-stable element begins to settle, this voltage difference begins to increase or decrease depending on the polarity. Fig.1 shows how this occurs in simulation. The properties of this change are used to calculate τ , as shown in equation (2). Two points which represent the linear section in Fig. 1 are selected, and these two points are entered into the formula as (t1 ,V1 ) and (t2 ,V2 ). τ = t1 − t2 ln(V2 /V1 ) (2) B. Flip-ﬂop Initialization As described, the ﬁrst step in simulation is forcing the ﬂipﬂop under test into metastability. This paper considers three synchronizer ﬂip-ﬂops which each utilize different bi-stable element structures. Each bi-stable element structure requires a different method of initialization. The ﬁrst ﬂip-ﬂop considered was the Dynamic Latch FlipFlop (DLFF), sometimes referred to as a jamb latch ﬂip-ﬂop [9]. The DLFF is so named due to the dynamic nature of its circuit, which requires a separate reset signal before the device can be re-evaluated (refer to Fig. 3). This modiﬁcation reduces the capacitance on the element, which enables the circuit to outperform standard synchronizer ﬂip-ﬂops. Unfortunately it also limits the usabililty of the circuit in modern devices which very rarely contain a dynamic reset signal. Since the DLFF utilizes a simple cross-coupled inverter pair, initializing the circuit into metastability is similar to what has been just described. The second circuit simulated was the PowerPC ﬂip-ﬂop, recently considered as a suitable sub-threshold voltage synchronizer ﬂip-ﬂop [11]. The PowerPC ﬂip-ﬂop as shown in Fig.4, was initialized by setting the pass gate transistor switch and all W p2 and W n2 devices in the ON state. The data input is then set to VDD 2 . The voltage difference is then considered between node X and output of the inverter consisting of W p1 and W n1. The Pseudo-NMOS ﬂip-ﬂop is the third to be simulated, and is shown in Fig.5. This ﬂip-ﬂop was initially proposed as a bi-stable element which did not sharply degrade in speed as VDD was reduced. To initialize the Pseudo-NMOS ﬂip-ﬂop, the q and q nodes were set at VDD 2 , and then the clock was set to low such that that mm and mm were in the same state. The voltage difference was taken between q and q . IV. R E SU LT S A. τ and F O4 delay The link between synchronizer ﬂip-ﬂop τ values and F O4 delay has been used in the past to predict synchronizer τ in new technologies. Both factors rely on the device size/strength tradeoff deﬁned by the technology node. However, the research community has disagreed about whether or not this trend continues [1]–[5]. Ginosar et al. have proposed that after 65nm, each technology node will see higher τ while F O4 delay will continue to decrease [4]. They justify this claim by ﬁrst creating a mathematical relationship between F O4 delay and τ in (3). Fig. 3: Dynamic Latch Flip-Flop [9] τ = η 4 td,F O4 A (3) Fig. 4: PowerPC Flip-Flop [11] Equation 3 represents τ as a function of F O4 delay, A (inverter gain), and η (derived from a combination of VDD , Vt , and λ). It was proposed that while F O4 will continue to scale, the inverter gain will not, creating a degradation in τ [4]. Other researchers have released results demonstrating τ values which continue to track F O4 delay [2]. Until now, the Fig. 2: τ F O4 For Planar and FinFET Nodes Fig. 5: Pseudo-NMOS Flip-Flop [2] (a) 22nm Planar (b) 20nm FinFET Fig. 6: DLFF τ with T emperature and VDD sweep (a) 22nm Planar (b) 20nm FinFET Fig. 7: Pseudo-NMOS τ with T emperature and VDD sweep effect of FinFET devices has not yet been considered. Analysis from UC Berkeley has found that propagation delay in FinFET circuits is independent of electrical width [19]. This serves to maintain the gain of inverter built with FinFET devices, even as they are scaled. The factor of consideration relating τ and F O4 delay is inverter gain, and so it stands to reason that τ will continue to track F O4 delay even when using FinFETs. This analysis is validated by the simulation results shown in Fig.2. Each of the three synchronizer ﬂip-ﬂops were simulated with a nominal VDD of 0.7V and temperature of 22◦C. The expected range for τ delay is between 0.2 and 1.5 times the given technology node’s FO4 delay [4], and each of the three ﬂip-ﬂops tested fall within this range. While there is some variation between technology nodes, the results still match physically measured results by other researchers for planar technology nodes [2]. The FinFET nodes (7nm, 10nm, and 20nm) stay within the same F O4 range as the values found for planar nodes (22nm, 50nm). τ B. τ and Temperature As the temperature of a CMOS circuit increases, the carrier mobility is reduced (due to an increase in particle scattering) and the threshold voltage decreases. While these effects have opposite effects on CMOS speed, the decrease in mobility is more dominant for most circuits (higher temperatures mean slower logic) [20]. However, previous work has veriﬁed that the threshold voltage’s temperature sensitivity is more important than mobility’s sensitivity for synchronizer circuits, resulting in an increase in synchronizer τ with lower temperatures [7], [21]. FinFET devices are not affected by temperature in the same way as planar devices however. Recent work has found that the previously weaker voltage threshold effect is actually dominant in FinFET logic gates, which has led to an increase in logic gate performance at higher temperatures [22]. Unfortunately, this suggests a sharp increase in τ at lower temperatures for synchronizers built with FinFET devices. The simulation results, shown in Fig. 6 and Fig. 7, do validate the theory predicting signiﬁcantly higher τ values for FinFETs at low temperatures. While simulation was completed for all nodes mentioned before, 22nm planar and 20nm FinFET are shown here due to their similar physical dimensions. There is a more stark degredation (increase) of synchronizer τ at lower temperatures for circuits built with FinFET devices (7nm and 20nm). When built with planar devices in 22nm and 50nm technology, the DLFF and Pseudo-NMOS ﬂipﬂop are generally stable over temperature, even with low supply voltages. Therefore a general increase in temperature dependence with scaling can also be observed. The PowerPC ﬂip-ﬂop experienced these same effects, but its associated data is not displayed here in order to conserve space. (a) 22nm Planar (b) 20nm FinFET Fig. 8: DLFF and Pseudo-NMOS τ when Non-Biased or Forward-Biased with VDD sweep These results also show that increases in τ due to lower temperatures can be prevented with a higher supply voltage. This further demonstrates that the dominant factor in synchronizer dependence on temperature is the change in threshold voltage. In modern SoCs, maximum restrictions on the supply voltage are generally enforced to prevent an increase in power consumption which leads to heat. Fortunately, this means that raising the supply voltage in a low temperature state to prevent τ degradation is not unreasonable. C. Forward Biasing Designers must also consider how FinFET devices will affect the trade-offs between various synchronizing ﬂip-ﬂop types. When all circuits were implemented with FinFET devices, the Power-PC ﬂip-ﬂop continued to be out-performed by both the DLFF and Pseudo-NMOS. However, the choice between the DLFF and Pseudo-NMOS becomes more complex when FinFET devices are used. The DLFF was one of the ﬁrst synchronizing ﬂip-ﬂops to be proposed, and relies on inverter feedback loops to settle metastability [9]. When near-threshold operation became more common, it was shown that inverter loops provided very poor τ at low voltages. For this reason, the Pseudo-NMOS ﬂipﬂop was proposed as a synchronizing ﬂip-ﬂop without the need for an inverter feedback loop [2]. Around the same time, another solution was proposed. This solution applied forward body bias to increase DLFF performance at low voltages [8]. It was demonstrated that forward body biasing increases the transconductance of the bi-stable inverter pair, resulting in a decrease in τ . While the Pseudo-NMOS has been compared to the unbiased DLFF, it has not yet been compared when using this technique. It is important to determine which design outperforms the other at low voltages, as well as veriﬁcation of the technique’s effectiveness when using FinFET devices. For 22nm planar technology, Fig.8(a) shows that the nonbiased DLFF has the worst τ . However, when forward body biasing is applied the DLFF τ is signiﬁcantly decreased. It even surpasses the biased Pseudo-NMOS ﬂip-ﬂop, which is relatively unchanged. This demonstrates that forward biasing a planar DLFF is the best solution for both high and nearthreshold voltage operation. In FinFET technology, not only are all τ values significantly reduced (observe the scale in Fig.8(b) vs Fig.8(a)), the tradeoffs change. Since FinFET devices sit on top of the bulk, rather than being deposited inside the bulk (as in planar technologies), the effect changes. As can be seen in Fig.8(b), both the DLFF and Pseudo-NMOS receive an increase in τ rather than a decrease in τ when forward biasing is applied at low voltages. This effectively removes biasing as a suitable technique for near threshold operation. It can also be seen however that DLFF maintains a signiﬁcant advantage over the Pseudo-NMOS at higher supply voltages, and can still be improved with forward body biasing. This presents a switching point (around 0.43V for 20nm) when above that voltage the DLFF is superior, and below that voltage the Pseudo-NMOS is superior. This is because inverter feedback loops offer a signiﬁcant advantage at higher voltages. If the supply voltage must decrease, this advantage wears off and the Pseudo-NMOS achieves the lowest τ for near threshold operation. V. TH E DYNAM IC SYNCHRON I Z ER Comparing the raw τ values of synchronizer ﬂip-ﬂops is important, but not the whole story. There are some limitations when these designs are used in real SoCs. While this work has found the DLFF to outperform the PowerPC and PseudoNMOS ﬂip-ﬂops when operated with a higher supply voltage, it requires a separate reset signal. Modern SoCs do not use reset, and so the use of this ﬂip-ﬂop has been limited. This (a) Schematic (b) Timing Diagram Fig. 9: Simple Dynamic Synchronizer paper presents a new circuit which automatically generates the required reset signal: The Dynamic Synchronizer. A simple version of the Dynamic Synchronizer is shown in Fig. 9(a), consisting of a single DLFF to sample data and resolve metastability. A normal D ﬂip-ﬂop is used to sample the output of the DLFF so that the output of the system is no longer affected by the periodic resets experienced by the DLFF. The output of this ﬂip-ﬂop is updated three-quarters of the way through the clock cycle. The behavior of this circuit is described in more detail in Fig. 9(b). In timing event 1, a rising edge on Rclk causes DLFF1 to correctly sample a high value. Then, three-quarters of the way through the clock cycle, a rising edge on the Delayed, inverted, rclk, causes both the Reset of the DLFF1 (via a pulse shown in event 2) and the D ﬂip-ﬂop to sample the output of the DLFF1. While resetting and sampling the output of a DLFF at the same time might seem like it violates timing conditions, this is not the case and has been veriﬁed in both schematic and layout level simulations. This is similar to how each ﬂip-ﬂop in a brute-force synchronizer samples the ﬂip-ﬂop ahead of it, which is also sampling at that same time [23]. The data on the D ﬂip-ﬂop is now aligned and can be sampled by the receive clock domain on the next rising edge of Rclk. Although the support structure around the DLFF has nearly hidden the dynamic behavior of this synchronizer, there is still one more eccentricity in the circuit’s behavior. For one speciﬁc input combination, the Simple Dynamic Synchronizer will not sample the input properly. The situation in question is when Tdata experiences a falling edge in-between the falling edge of Reset and rising edge of Rclk. The circuit fails in this case because this input combination violates the monotonicity input restriction required of all dynamic circuits. The Simple Dynamic Synchronizer needs the Reset signal on the ﬁrst ﬂipﬂop to pull its output down. If Reset is not asserted while the data experiences a falling edge, the synchronizer will keep sampling a high value until the reset can be reasserted. The goal is to use this synchronizer in non-dynamic systems which do not maintain monotonic signals, so monotonic input cannot be assumed. In the case shown in Fig. 9(b) circled in red, Tdata experiences a falling edge in-between the falling edge of Reset and rising edge of Rclk. This is the speciﬁc situation which causes a problem. Since the DLFF circuit has no pull up transistors on its data input (see Fig. 3), the output of the inverter pair can only be truly pulled low with a Reset signal. In this case the falling edge on the data occurred after the reset, so the latches inside the DLFF remained in their high state. This can be observed at timing event 3, where the DLFF1 output incorrectly samples a logic high. This is then sampled by the conventional ﬂip-ﬂop, and the Aligned data incorrectly displays another high value. The output of DLFF1 is ﬁnally pulled low when the next Reset pulse occurs (timing event 4), and on the next rising edge of the Rclk the real data value will be seen. This problem can be solved with a more robust design shown in Fig. 10(a). Since the problem is caused by a falling edge on Tdata between the falling edge of the Reset and the rising edge of Rclk, it is known that the false value caused will always be a false high value. For this reason, an AND gate has been inserted in between DLFF1 and the conventional ﬂipﬂop. This gates the output of DLFF1 and allows for a detection (a) Schematic (b) Timing Diagram Fig. 10: Robust Dynamic Synchronizer circuit to force the output low when it detects a failure. The detection circuit consists of two DLFFs, labeled DLFF2 and DLFF3 as shown in Fig. 10(a). The data input of DLFF2 is connected directly to VDD while its clock input is connected to inverted Tdata. This means that a falling edge on Tdata will cause DLFF2 to always sample a high value. So that DLFF2 is only active during the period of interest, its Reset is connected to Delayed rclock. This means that DLFF2 is only active in-between the falling edge of Delayed rclock and when DLFF3 samples the output of DLFF2 with the rising edge of Rclk. The output of DLFF2 will only be high when Tdata experiences a falling edge (causing a rising edge on the DLFF2 clk input), causing VDD to be sampled. This is then transferred over to DLFF3 on every rising edge of Rclk for evaluation. If the output of DLFF3 (Monotonicity Failure) is high, that means that there has been a monotonicity failure, and so this signal forces the output of the AND gate low. This supplies the conventional ﬂip-ﬂop with the correct data. So when the rising edge of the Delayed, inverted, rclk occurs, the conventional ﬂip-ﬂop supplies the correct Aligned data even though there was a monotonicity violation. DLFFs are used purposefully in the detection circuit, since metastability could occur if Reset was released at the same time as a falling edge of Tdata. The behavior of the Dynamic Synchronizer can be seen in Fig. 10(b). With the new Monotonicity Failure signal, the output of the DLFF1 can be corrected. The green arrows show the sampled values at each Rclk appearing correctly, ready to be sampled from Aligned data one cycle later. While using DLFFs decreases the chance for metastability to be found on the output of the synchronizer, it does not eliminate that chance. For this reason, longer periods to settle metastability may be beneﬁcial. The time period given to settle metastability is shown as S in the Equation (1). To accomplish this increase in S , additional stages can be added to add extra clock cycles of latency. V I . CONC LU S ION S The work in this paper has demonstrated and evaluated the changes expected as synchronizers begin to be designed exclusively with FinFET devices. First, results have been presented which demonstrate a continuing relationship between τ and F O4 delay as technology scales. This includes synchronizers built with both planar and FinFET devices. Forward body biasing has proven to still be effective in reducing τ , but its beneﬁt has decreased. FinFET devices have also been shown to exacerbate τ degradation at low temperatures. Recommendations have been made to increase the supply voltage during periods of low temperature operation. A comparison between high performance synchronizer ﬂip-ﬂop designs has also been presented. The DLFF has demonstrated lower τ at standard supply voltages, and the Pseudo-NMOS ﬂip-ﬂop has lower τ at lower supply voltages. Previously a separate reset signal was needed to use the DLFF, but the new synchronizer circuit presented here solves this problem. The Dynamic Synchronizer uses a locally generated reset and monotonicity detector to leverage the speed of the DLFF while hiding dynamic behavior. This allows systems which employ NoCs to beneﬁt from the high performance DLFF (while at standard voltages) without the need for a separate reset signal. [8] [14] [12] "
DyAFNoC - Characterization and analysis of a dynamically reconfigurable NoC using a DOR-based deadlock-free routing algorithm.,"Several simulations have been performed in order to verify the system architecture behavior, showing that the dynamic reconfiguration time overhead is mainly due to the packet draining time. The synthesis results obtained a maximum frequency of 182.02MHz (Virtex 4) and 162.86MHz (Virtex 6) for both 5×5 and 8×8 meshes respectively. Table I shows the synthesis results for a 5-port 16-bit router and MRCS logic for a 5×5 mesh.","DyAFNoC: Characterization and Analysis of a Dynamically Reconﬁgurable NoC using a DOR-based Deadlock-Free Routing Algorithm Ernesto Villegas Castillo∗ , Gabriele Miorandi† and Wang Jiang Chau‡ School of Engineering - Electronic Systems Department University of Sao Paulo Sao Paulo, Brazil Email: {cristovc∗ , jcwang‡ }@lme.usp.br, mrngrl@unife.it† I . IN TRODUC T ION Some related works about Dynamically Reconﬁgurable Networks on-Chips (DRNoCs) implementations over FPGA devices have been presented in the technical literature in the last decade [1]–[6], presenting different NoC topologies, communication mechanisms and switching modes. The routing algorithms play an important role, since they may need to adapt themselves to the new conﬁgurations in order to guarantee safer communication avoiding deadlock and/or livelock. As far as the distributed routing concerns, adaptability usually comes at cost of higher area and additional complexity. In previous widely-used approaches this problem is addressed by providing the routing logic of huge routing tables. However, in terms of reconﬁguration time, this means that a big overhead may be experienced when tents of bits must be updated inside each switch. A suitable approach for adaptable routing is the Flexible Direction Order Routing (FDOR) [7], which guarantees connectivity and deadlock freedom in irregular shaped topologies obtained from the distribution of processing modules over the network. These topologies are composed of one core and one or two ﬂanks each one implementing a simple DOR algorithm. The FDOR hardware implementation requires just one conﬁguration bit per router, which has a signiﬁcantly lower complexity in comparison with the Logic-Based Distributed Routing (LBDR) and Table-Based Routing (TBR) implementations used in the related works. In this work, we adopt a logic-based implementation of the FDOR algorithm in a DRNoC, to automatically adapt the routing of packets to new context topologies. This DRNoC is named as DyAFNoC (for Dynamic Automatic FDOR-based NoC). Its structure is based in a 2-D Mesh composed of 5port routers based on HERMES router architecture [8], with a Processing Element (PE) connected to the local port. With this arrangement, any FDOR-compatible topology may be easily obtained when new processing modules are created using the PEs logic resources as shown in the example of a DRNoC processing in Figure 1. I I . DYNAM IC AFDOR -BA SED R ECON FIGURAT ION SY ST EM A. Mesh Reconﬁguration Control System The MRCS is illustrated in Figure 2 for a 5x5 mesh. It is in charge of receiving the conﬁguration map of the network when a reconﬁguration is requested by the Partial Dynamic Reconﬁguration Control System (PDRCS) and the Automatic FDOR (AFDOR) logic circuit precalculates the new routing conﬁguration. Concurrently, the 2-D Mesh controller drains the network from the trafﬁc by holding any new packet at the network interface level. When the network is out of trafﬁc, the new conﬁguration is rapidly broadcasted into the network. Thus, each router will receive an input control signal determining what is the new routing mechanism (XY or YX). This functioning exploits the Typically Static Reconﬁguration (TSR) approach, by non overlapping the two routing algorithms. Fig. 2. An allowed FDOR topology formed after module placements showing the core and the two ﬂanks regions. B. Partial Dynamic Reconﬁguration Control System The PDRCS is responsible for the partial dynamic reconﬁguration process of the network. It receives signals from all the processing modules and PEs, indicating the end of their local tasks, this way the resources are available to apply for a new reconﬁguration. It is also in charge of sending the conﬁguration maps to the MRCS. Isolation Switches are positioned in the router local ports where the processing modules will be conﬁgured and placed as shown in Figure Fig. 1. Time Diagram of the different scenarios in the DyAFNoC processing. 3, simulating the presence or absence of PEs or processing modules in the reconﬁgurable fabric through the Dynamic Circuit Switching technique [9]. Figure 3 also shows the PDRCS functioning in 5 steps. Fig. 3. Partial Dynamic Reconﬁguration Control System block diagram showing the different processing steps. I I I . R E SU LT S Several simulations have been performed in order to verify the system architecture behavior, showing that the dynamic reconﬁguration time overhead is mainly due to the packet draining time. The synthesis results obtained a maximum frequency of 182.02MHz (Virtex 4) and 162.86MHz (Virtex 6) for both 5x5 and 8x8 meshes respectively. Table I shows the synthesis results for a 5-port 16-bit router and MRCS logic for a 5x5 mesh. IV. CONC LU S ION S AND FU TUR E WORK S From the results obtained, it can be observed that the MRCS based on FDOR logic requires a very low quantity of logic resources in comparison with with respect to a simple 5port router. Also, for refreshing the routing scheme in a 2D Mesh, it requires just one clock cycle, which makes that the reconﬁguration overhead depends only of the network TABLE I SYN TH E S I S R E SU LT S FOR V IRTEX 4 Logic Utilization Slices Slice FFs LUTs 5-port Router MRCS (5x5) 583 (3%) 163 (1%) 346 (1%) 81 (1%) 641 (2%) 188 (1%) draining time. These results lead to the conclusion that using the FDOR algorithm for a DRNoC implementation results in a more suitable approach than other alternatives as Table-Based Routing (TBR) and Logic-Based Distributed Routing (LBDR). ACKNOW LEDGM ENT To the Coordination for the Improvement of Higher Education Personnel (CAPES, Brazil) for supporting this work. "
Sharing and placement of on-chip laser sources in silicon-photonic NoCs.,"Silicon-photonic links are projected to replace the electrical links for global on-chip communications in future manycore systems. The use of off-chip laser sources to drive these silicon-photonic links can lead to higher link losses, thermal mismatch between laser source and on-chip photonic devices, and packaging challenges. Therefore, on-chip laser sources are being evaluated as candidates to drive the on-chip photonic links. In this paper, we first explore the power, efficiency and temperature tradeoffs associated with an on-chip laser source. Using a 3D stacked system that integrates a manycore chip with the optical devices and laser sources, we explore the design space for laser source sharing (among waveguides) and placement to minimize laser power by simultaneously considering the network bandwidth requirements, thermal constraints, and physical layout constraints. As part of this exploration we consider Clos and crossbar logical topologies, U-shaped and W-shaped physical layouts, and various sharing/placement strategies: locally-placed dedicated laser sources for waveguides, locally-placed shared laser sources, and shared laser sources placed remotely along the chip edges. Our analysis shows that logical topology, physical layout, and photonic device losses strongly drive the laser source sharing and placement choices to minimize laser power.","Sharing and Placement of On-chip Laser Sources in Silicon-Photonic NoCs Chao Chen, Tiansheng Zhang, Pietro Contu, Jonathan Klamkin, Ayse K. Coskun, Ajay Joshi IN TRODUC T ION Electrical and Computer Engineering Department, Boston University, Boston, MA, USA {chao9810, tszhang, contu, klamkin, acoskun, joshi}@bu.edu Abstract—Silicon-photonic links are projected to replace the power and temperature of the laser source. The optical power electrical links for global on-chip communications in future that needs to be output by a laser source depends on the manycore systems. The use of off-chip laser sources to drive physical layout of the silicon-photonic NoC and the bandwidth these silicon-photonic links can lead to higher link losses, thermal (i.e., number of required wavelengths) of the NoC channels. mismatch between laser source and on-chip photonic devices, The laser source efﬁciency is low when the optical output and packaging challenges. Therefore, on-chip laser sources are power is too high or too low. Considering existing laser and being evaluated as candidates to drive the on-chip photonic silicon-photonic device technologies, the optical output power links. In this paper, we ﬁrst explore the power, efﬁciency and temperature tradeoffs associated with an on-chip laser source. corresponding to the laser source’s peak efﬁciency is likely higher than that required for each DWDM channel. Therefore, Using a 3D stacked system that integrates a manycore chip with the optical devices and laser sources, we explore the design space sharing of a laser source across parallel waveguides would for laser source sharing (among waveguides) and placement to be required to operate the laser source at its peak efﬁciency. minimize laser power by simultaneously considering the network At the same time, the temperature of each laser source is bandwidth requirements, thermal constraints, and physical layout deﬁned by the power consumed by the hardware in the laser constraints. As part of this exploration we consider Clos and source’s neighborhood. As the laser temperature increases, crossbar logical topologies, U-shaped and W-shaped physical the efﬁciency of the laser source decreases. Hence, the laser layouts, and various sharing/placement strategies: locally-placed sources needs to strategically placed to operate them at as dedicated laser sources for waveguides, locally-placed shared laser minimum a temperature as possible to maximize the laser sources, and shared laser sources placed remotely along the chip edges. Our analysis shows that logical topology, physical layout, source efﬁciency. and photonic device losses strongly drive the laser source sharing In this paper, we explore the design space of sharing and and placement choices to minimize laser power. placement of on-chip laser sources by simultaneously considI . ering the NoC bandwidth constraints driven by the applications running on the manycore system, thermal constraints driven by Silicon-photonic link technology has been extensively exthe power consumed by the cores and the laser sources, and the plored as a potential replacement to electrical link techphysical layout constraints driven by the losses in the photonic nology in the design of network-on-chip (NoC) for manydevices. The goal is to optimize the sharing as well as the core systems [1]–[6]. The key advantages of silicon-photonic placement of on-chip laser sources to maximize laser efﬁciency link technology include up to an order of magnitude higher and in turn minimize the electrical power consumption of the bandwidth density (compared to electrical link technology) lasers. Using a 256-core 3D-integrated system consisting of through dense wavelength division multiplexing (DWDM), and separate processor logic layer, photonic device layer, and laser link length independent data-dependent power. However, this source layer, we show that, for various NoC logical topologies silicon-photonic link technology has not yet been adopted for designing the NoC of commercial manycore systems as the and physical layouts, laser power consumption can be lowered by sharing laser sources across the various silicon-photonic ﬁxed power consumed by the laser sources and the power waveguides and by intelligently placing these laser sources. consumed in thermal management of the silicon-photonic It should be noted that our proposed approach is also valid links can be signiﬁcant and can negate the bandwidth density advantages. Moreover, packaging such a system where several for a system where the photonic devices are monolithically integrated with the CMOS devices. off-chip laser sources drive the photonic NoC can be extremely challenging. Using on-chip laser sources to drive the photonic The rest of the paper is organized as follows. Section II NoC is therefore being considered as a potential alternative [7], discusses the related work, followed by a description of the [8]. Although on-chip lasers require further technological experimental setup that is used for our case study in Section III. development, they already provide advantages including the In Section IV, we describe our methodology for optimizing the elimination of coupling losses and simpliﬁed packaging. Onoperating points for on-chip laser sources, and then we evaluate chip lasers can also be switched on and off relatively rapidly our methodology for various logical topologies and physical and at a lower energy cost, which makes them compatible with layouts of the silicon-photonic NoC in Section V. various run-time laser power management techniques [9]–[11] that have been proposed. Lastly, given that laser sources and photonic component can exist in adjacent layers, it is possible to match the temperatures among these components, which would simplify thermal management techniques [12]–[15]. A current limitation of on-chip laser sources is the low wall-plug efﬁciency, which depends on both the optical output Several on-chip laser source technologies have been developed in recent years [16]–[19]. Although these technologies require further development, they show promise for simplifying packaging and enhancing the performance of photonic NoCs. On-chip laser sources can also be switched on and off I I . R E LAT ED WORK     	   (a) 	 	  	  	 	 	  	  	 	 	   	   	      % '(  &  	   ! ""	#  !	""	#""$  !	#  ! (b) ) * Fig. 1: (a) Overview of the 3D manycore system and layouts for each layer. Output from the laser sources is routed into the waveguides through couplers. The ring modulator is driven through TSVs by the transmitter in the logic layer. Photodetector’s output is fed to the receiver on the logic layer through TSVs. (b) Cross-sectional view of our target 3D manycore system. more efﬁciently compared to off-chip laser sources. This eases the process of run-time laser power management. In [7], the authors evaluate their ATAC photonic NoC architecture that is driven using Ge-based on-chip laser sources and propose that on-chip laser sources that enable rapid power gating need to be developed to enable the adoption of photonic NoC in mainstream systems. Similarly, in [8], the authors use Clos and crossbar topologies to make an argument for using on-chip laser sources in manycore NoC from an energy-efﬁciency and energy-proportionality perspective. Several design ﬂows have been developed to enable rapid exploration of the silicon-photonic link design space as well as the silicon-photonic NoC design space. In [20], the authors propose a linear programming technique to design the physical layout of the photonic devices on a separate photonic layer with the goal of minimizing the power consumed by an offchip laser source. Similarly, in [21], the authors propose a methodology to route the photonic waveguide such that the number of waveguide crossings is minimized. For a comprehensive evaluation of the photonic network design space, the authors in [22] propose a methodology and a tool that jointly explores the link-level and system-level designs of the network topologies. In [23], the authors propose a design ﬂow for placement and routing of photonic devices to hierarchically design large complex photonic networks. In [24], the authors have proposed a tool for placement and routing of optical NoC topologies with the goal of enabling a realistic analysis of the optical NoC design space. In our paper we explore the design space for sharing and placement of on-chip laser sources. Our methodology simultaneously considers NoC bandwidth constraints, thermal constraints and physical layout constraints to determine onchip laser source sharing and placement that reduce laser power consumption. This methodology can be readily applied to any of the on-chip laser source technologies under development. It is also possible to integrate our design strategy with design automation approaches that focus on optimizing other aspects of silicon-photonic NoC. I I I . EX P ER IM EN TA L S E TU P To explore the limits and opportunities for sharing and placement of laser sources, we consider a 3D stacked manycore system (see Fig. 1(a)) with a logic layer containing 256 cores fabricated using standard bulk CMOS process, a photonic NoC layer next to the metal stack and a layer for on-chip laser sources. The logic layer and photonic layer are connected using vertical metal vias. The architecture of each core in the logic layer is similar to an IA-32 core used in the Intel Single-Chip Cloud Computer [25]. We scale the core power and dimensions from 45 nm to 22 nm technology [26], resulting in a total chip area of 366.1 mm2 (0.93 mm2 per core, including L1, and 0.50 mm2 for each 256 KB private L2 cache). We choose the operating frequency as 800 M H z and the supply voltage as 0.65 V , and scale the per-core power based on the reported data of Intel 22 nm Tri-Gate technology. The average per core power is 0.46 W , and the average per L2 cache power is 0.01 W (based on ITRS-LSTP cache model in CACTI [27]). There are 16 memory controllers uniformly distributed along the two edges of the chip. The silicon-photonic NoC in our system is used for connecting the private L2 caches of the cores and the memory controllers. It should be noted that our proposed methodology for sharing and placement of laser sources is also applicable for a shared L2 architecture where the photonic NoC provides connectivity between L1 cache and shared L2 cache. For evaluation, we use Sniper [28] simulator for systemlevel simulations and then use McPAT [29] to derive dynamic core power values. We select a representative set of multithreaded benchmarks from SPLASH2 [30] (barnes, ocean, radix, lu contiguous, fft and water nsquare) and PARSEC [31] (blackscholes, canneal and swaptions) suite. We run each benchmark with sim medium input and 256 threads, and then calibrate the core power consumption based on the average power consumption given above. As for the bisection bandwidth of target system, we choose 512 GB /s based on canneal, which has the highest bandwidth requirements among all benchmarks we evaluated. Based on the calculated core power consumption when running the above listed benchmarks, we select 0.4 W and 0.7 W as the lower bound and upper bound, respectively, for core power consumption in the following case studies. For our target system, light waves emitted by one or more single-mode indium phosphide (I nP )-based laser sources located on the laser layer are routed into silicon (S i) waveguides located on the photonic device layer. Instead of singlemode lasers, integrated multi-wavelength lasers [32] or comb lasers [33] could be utilized. However, these alternatives have a large footprint or require further technological development. Therefore only single-mode laser sources are considered in this work. We place the single-mode laser sources over the lower power-density L2 cache blocks to minimize the impact of the core power on the laser source temperature. The laser source is driven by a driver located in the logic layer. With the exception of the photodetector which is made of germanium (Ge), all other photonic devices in the photonic device layer are made of S i. The cladding material is silicon dioxide (S iO2 ). The light waves pass next to a ring modulator that is driven through the metal vias by the link transmitter circuit located on the logic layer. The modulators convert data from the electrical domain to the photonic domain. The modulated light waves propagate in the waveguides and can pass through zero or more ring ﬁlters. At the receiver side, the light waves are ﬁltered by ring ﬁlters and then are absorbed by Ge photodetectors. The current generated by the photodetectors passes through the metal vias and is fed to the link receiver circuit located on the logic layer. To explain the various tradeoffs associated with choosing the laser source conﬁguration across various logical topologies, we compare an 8-ary 3-stage Clos topology, a 16-ary 3-stage Clos topology and a 16 × 16 crossbar mapped to a U-shaped physical layout of the waveguides in the photonic layer of our target system. The choice of these topologies is driven by the fact that silicon-photonic link technology is most appropriate for high-radix low-diameter topologies like Clos and crossbar. We also investigate the tradeoffs associated with choosing the laser source conﬁguration when the 16 × 16 crossbar and the 16-ary 3-stage Clos are mapped to U-shaped and W-shaped layouts shown in Fig. 1(a). For our analysis, we use the projected photonic losses listed in Table I. It should be noted that our proposed methodology for choosing a laser source is generally applicable to any physical layout and any logical topology mapped to that layout. To evaluate the impact of temperature variations (due to variations in core power and laser power) on the laser efﬁciency, we use the 3D extension of HotSpot 5.02 [34] for our thermal simulations. We set the ambient temperature to 35oC and use the default package conﬁgurations in HotSpot. The cross-sectional view of the 3D system that we evaluated is shown in Fig. 1(b). For thermal analysis, the laser sources are modeled individually on the laser layer. However, for waveguides and rings on the photonic layer, we aggregated them into larger-sized blocks in the ﬂoorplan as using a separate model for every waveguide and ring leads to large simulation time without any signiﬁcant improvement in precision. Our aggregation methods provide desirable accuracy-simulation time tradeoffs in thermal simulation [35]. We compute the joint thermal resistivity for waveguide blocks and ring blocks using Rj oint = Vtotal /Σ(Vi /Ri ), where Ri and Vi refer to the thermal resistivity and volume of material i in the blocks. The dimensions of our system are shown in Fig. 1(b). All the thermal results we report in this work are from steady state analysis. IV. ON -CH I P LA S ER SOURC E O P T IM I ZAT ION This section describes the laser source model used to evaluate the laser source operating regimes. A discussion of the optical power, efﬁciency and thermal tradeoffs is provided, TABLE I: Optical Loss per Component [2] Photonic device Coupler Splitter Non-linearity (at 30 mW) Modulator Insertion Waveguide (per cm) Waveguide crossing Filter through Filter drop Photodetector Optical Loss (dB) 1 0.2 1 0 1∼5 0.05 1e-4 1.5 0.1 followed by a description of the methodology to determine the sharing and placement of on-chip laser sources for minimizing laser power consumption. A. Laser source model The laser wall-plug efﬁciency (ηW P E ) is given by the ratio of optical output power (Po ) to electrical input power (PIN ): ηW P E = Po PIN , where Po is given by Po = ηi ηd hc λq (I − Ith ), (1) (2) where ηi and ηd are the laser internal efﬁciency and differential quantum efﬁciency, respectively; h, c and q are Planck’s constant, the speed of light and the elementary electric charge, respectively; λ is the laser operating wavelength; I and Ith are the drive and threshold currents, respectively [36]. The electrical input power of the laser is the product of the drive current and the total voltage across the laser’s terminals and can be calculated as: PIN = I 2Rs + I Vd , (3) where Rs is the laser series resistance and Vd represents the diode voltage. One of the shortcomings of semiconductor lasers is the strong dependence of Po on temperature. Fortunately, simple empirical formulae match quite well with the measured characteristics of many different semiconductor diode lasers [36]. These empirical formulae are: Ith = I 0 ηd = η0 th eT /T0 , d e−T /Tη , (4a) (4b) where T0 and Tη are the characteristic temperatures of the threshold current and the differential quantum efﬁciency, respectively, while I 0 d are the threshold current and the differential quantum efﬁciency projected to a reference temperature. Additionally, the diode voltage Vd depends on temperature through the Shockley diode equation: th and η0 Vd = kB T q ln ! I Is "" , (5) where kB is the Boltzmann constant and Is is the reverse bias saturation current. By substituting Eqs. (4) and (5) into Eqs. (2) and (3), simple relationships are expressed for the dependence ) W m ( o P r e w o P l a c i t p O 180 150 120 90 60 30 0 0 E P W η 12% 10% 8% 6% 4% 2% 0% 0 E P W η m u m i x a M 12% 10% 8% 6% 4% 2% 0% 200 T=65°C T=70°C T=75°C T=80°C T=85°C T=90°C T=95°C 400 600 Laser source length (µm) (c) 800 100 200 300 Current I (mA) (b) 400 500 100 200 300 Current I (mA) (a) 400 500 Fig. 2: (a) P-I characteristics of a laser source at various temperatures (b) Wall-plug efﬁciency vs. Input current at various temperatures (c) Wall-plug efﬁciency vs. Laser Source Lengths at various temperature. ) C ° ( e r u t a r e p m e T 100 90 80 70 60 50 0 Pcore=0.4W Pcore=0.5W Pcore=0.6W Pcore=0.7W 200 400 600 800 1000 Electrical input Power PIN (mW) 1200 Fig. 3: Laser source temperature vs. electrical input power for a 300 µm × 50 µm laser source. of the laser performance on operating temperature. This model is well established and the parameters used in simulations are extracted from measurement results [37], [38]. In this paper, we consider a strained I nP -based multi-quantum well (MQW) laser structure. B. Laser source optical power, wall-plug efﬁciency and temperature tradeoffs Figure 2(a) presents the optical output power of the laser source versus the input current (P-I characteristic) for various temperatures and demonstrates that the threshold current Ith increases with temperature while the laser optical output power decreases with increase in temperature for a given current. Figure 2(b) demonstrates that at a given temperature, the efﬁciency of the laser sources initially increases, reaches a peak value and then decreases as the input current is increased. The peak efﬁciency decreases at higher temperature as expected from the model presented in the previous section (Eqs. 4). It is therefore desirable to operate the laser source at a low temperature and ensure that the input current is such that the laser efﬁciency is maximized. Figure 2(c) shows the variation of laser source efﬁciency with laser source length (for lasers operating at the optimal current level) for various laser source temperatures. Here the laser source width is ﬁxed at 50 µm while the laser source length is varied from 200 µm to 700 µm. Figure 2(c) shows that a laser source that is 300 µm long has the highest efﬁciency at any given temperature. This is because for shorter cavity length, high-order effects result in a reduction of the carrier density above threshold, which in turn decreases ηi . For longer cavity length, ηd dominates and the efﬁciency decreases. This behavior is typical of the strained I nP laser structures that are used for our simulations. Therefore a laser source that is 300 µm × 50 µm is used for the remaining analysis. To determine the impact of electrical input power of the laser sources on laser source temperature, we ran Hotspot simulations for the 256-core target system with each core consuming 0.4 W , 0.5 W , 0.6 W and 0.7 W of power. As shown in Fig. 3, as the laser input power increases the laser source temperature increases, which reduces the laser source efﬁciency. The increase in the core power consumption in the logical layer also increases the temperature of the laser source, which further lowers the laser source efﬁciency. Based on the power-temperature-efﬁciency tradeoffs of the laser source shown above, Fig. 4 shows the laser source efﬁciency and electrical input power as a function of optical output power of the laser source for two systems – one where each core is dissipating 0.4 W of power and the second where each core is dissipating 0.7 W of power. The required optical output power from a laser depends on the optical losses in the photonic link being driven by that laser source. For the case where each core’s power consumption is 0.4 W , Fig. 4(a) shows that the optimal operation point is for a laser output power of 23 mW per wavelength, where the laser source achieves a peak efﬁciency of 8.2%. This results in a laser electrical input power of 268 mW . When each core’s power consumption is 0.7 W , the optimal laser output power is still approximately 23 mW , but the laser efﬁciency decreases to 6.2% due to the higher laser temperature. This results in an electrical input power of 355 mW . This analysis demonstrates that the electrical input power of the laser increases with increase in core power consumption. Hence, one needs to develop run-time techniques that can minimize core and laser power to minimize the peak temperature of the laser and in turn maximize laser efﬁciency and minimize the laser input power. As shown in Fig. 4, a laser source outputs a speciﬁc optical output power at its maximum efﬁciency. Depending on the optical power required per λ and the laser technology used, it may be desirable to share laser source power across two or more waveguides. Figure 5 shows two methods for sharing single-mode laser sources across multiple waveguides. Figure 5(a) uses ring ﬁlters at the crossing of waveguides to ﬁlter and route each wavelength into the waveguide such that each waveguide propagates a number of wavelengths. Assuming each waveguide crossing incurs 0.05 dB loss, 1e-4 dB through loss per ring [39] and 64 waveguides share each laser source, this sharing approach introduces an overhead of 3.2 dB of optical loss. Figure 5(b) shows an alternate method that ﬁrst couples the light waves from a set of laser sources (each emitting one wavelength), and then splits the light into       E P W η 10% 8% 6% 4% 2% 0% 0 1000 800 600 400 200 ) W m ( N I P r e w o P t u p n I 0 50 E P W η 10% 8% 6% 4% 2% 0% 0 1000 800 600 400 200 ) W m ( N I P r e w o P t u p n I 0 50 ηWPE PIN 10 20 30 40 Optical Power per wavelength Po (mW) (b) 10 20 30 40 Optical Power per wavelength Po (mW) (a) Fig. 4: Wall-plug efﬁciency vs. Optical output power by the laser source for different granularity of sharing while a background logical layer operates at 0.4 W per core (a) and 0.7 W per core (b) respectively. 0 1 n-1 0 1 n-1 0 1 n-1 WG0 WG1 WGm-1 0 1 n-1 WG0 WG1 WGm-1 (a) (b) Fig. 5: (a) The laser source sharing through ring ﬁlters at the crossings of waveguides (b) The laser source sharing through waveguide couplers and splitters. multiple waveguides. Assuming a loss of 0.2 dB /split, if 64 waveguides are sharing these laser sources, this sharing method causes an overhead of 1.2 dB optical loss that is lower than the overhead of the other method. Therefore, we choose the latter sharing method in this paper. Figure 6 shows a plot of variation in laser efﬁciency with optical output power per wavelength for different granularity of sharing a single-mode laser source across multiple waveguides. We consider two cases with core power of 0.4 W and 0.7 W . For the 0.4 W case, if the total optical power required per wavelength is small, say 1 mW , then using a laser source per wavelength per waveguide results in a laser source efﬁciency of 1%. If we were to share the laser source across two waveguides, the total required optical output power of the laser source increases, which increases the efﬁciency to 2%. For this 1 mW optical power per wavelength per waveguide case, it makes most sense to share the laser source across 16 waveguides as it provides the maximum efﬁciency. Sharing of laser sources across more than 16 waveguides increases the sharing cost and hence the laser source needs to emit larger optical power, which decreases the laser source efﬁciency. Thus, an optimal sharing of laser sources is critical for operating laser sources at maximum efﬁciency and minimizing the electrical input power. A similar argument can be made for the case where the core power is 0.7 W . The only difference is that in this case the laser source efﬁciency is lower due to the higher core power. C. On-chip laser source sharing and placement strategy To determine the sharing and placement of the laser source, we propose a cross-layer approach where we jointly consider the NoC bandwidth constraints driven by the applications running on the manycore system, thermal constraints driven by the power consumed by the cores and the laser sources, physical layout constraints driven by the losses in the photonic devices and laser source designs that are compatible with our proposed 3D system. Figure 7 shows a ﬂowchart describing the strategy for determining the sharing and placement of laser sources in the 3D system. The number of cores in the target system and the type of applications that are expected to run on the target system can be used to determine the amount of trafﬁc that will be injected into the NoC. This in turn can be used to determine the optimal NoC topology at the logic level and the bandwidth of each channel in the NoC. Depending on the bandwidth per channel, operating frequency of cores and bandwidth of each silicon-photonic link, we can determine the number of siliconphotonic links, i.e. number of wavelengths required by the target system. The chosen logical topology for an NoC can be mapped to several different layouts [40]. Depending on the loss components (including waveguide loss, through loss, crossing loss, etc.) in a silicon-photonic link, we identify various potential physical layouts of the NoC with three candidates for the placement and sharing of the laser source – 1) all laser sources are placed locally next to the router with a laser source emitting one wavelength for one waveguide (no sharing); 2) all laser sources are placed locally next to the router with each laser source emitting one wavelength that is shared across two or more waveguides; and 3) all laser sources are placed along the edge with each laser source emitting one wavelength shared by two or more waveguides. For these candidate layouts and the target bandwidths per channel, depending on the thermal properties, i.e. expected peak temperature, of the laser source at runtime and the laser source efﬁciency characteristics, we can decide the placement of the laser source and its sharing granularity such that the total laser power consumption is minimized. V. CA S E S TUD I E S In this section, we present two case studies to show how the sharing and placement of laser sources change with logical topologies and physical layouts. For this analysis we assume the target system bisection bandwidth is always 512 GB /s, all cores are consuming 0.7 W of power and we use the results in Fig. 4(b) and 6(b) to determine the laser source efﬁciency. A. Laser source placement and sharing across various logical topologies Figure 8(a) shows the total electrical input power of the laser sources for an 8-ary 3-stage Clos topology for different placements of laser sources. This 8-ary 3-stage Clos network uses 24 routers (8 routers in each stage), and each router in             E P W η e v i t c e f f E 10% 8% 6% 4% 2% 0% 0 0.5 E P W η e v i t c e f f E 10% 8% 6% 4% 2% 0% 0 0.5 4.5 5 s=1 s=2 s=4 s=8 s=16 s=32 s=64 4.5 5 1 1.5 2 2.5 3 3.5 4 Optical power per wavelength Po (mW) (a) 1 1.5 2 2.5 3 3.5 4 Optical power per wavelength Po (mW) (b) Fig. 6: The effective WPE while increasing the sharing of laser sources as the logical layer operates at 0.4 W (a) and 0.7 W (b) per core, respectively. s = x indicates that x waveguides are sharing the output of the laser source. Splitter loss is accounted. NoC BW Constraints Thermal Constraints Physical Layout Constraints # of Cores,  Operating Freq. Applications Photonic Devices Losses BW Requirement, NoC Topology Expected TPeak of Cores and  Laser Sources Select: Laser Source  Locations Laser Source  Sharing Scheme # of Silicon Photonic Links, # of Wavelengths per Link NoC design that has minimum  laser source power consumption Fig. 7: Flowchart for deciding the sharing and placement conﬁgurations of on-chip laser sources. the 1st and 3rd stage is connected to 32 cores (4 cores per router input). There are 64 photonic channels connecting the 1st and 2nd stage of routers and another 64 connecting the 2nd and 3rd stage of routers. The detailed speciﬁcations of the 8-ary 3-stage Clos are given in Table II. The photonic channels are mapped to a U-shaped layout shown in Fig. 1(a). In the physical layout of this 8-ary 3-stage Clos topology, one router each from the 1st stage, 2nd stage and 3rd stage are placed next to each other. For a typical waveguide loss of 2 dB /cm, 0.15 mW of optical output power per wavelength is required. If local laser sources are not shared, the efﬁciency of each local laser source for 0.15 mW optical output power is 0.12%. This results in a total electrical input power for laser sources of 243 W (119 mW per laser source). Given that the routers in the 1st , 2nd and 3rd stage of the Clos network are placed next to each other, there is an opportunity for sharing the local laser sources among the 16 photonic channels (8 from the 1st stage router and 8 from the 2nd stage router), whereby the optical output power of a laser source is split and routed into the waveguides associated with these photonic channels. This sharing of the local laser sources increases the total optical output power of each laser source, which improves its efﬁciency. For this particular example, each one of the 16 photonic channels is mapped to a waveguide with 16 λ per photonic channel, i.e. 16 λ per waveguide. If each laser source is shared across these 16 waveguides, the total optical output power is 2.4 mW for each λ, which corresponds to a laser source efﬁciency of 1.3% and a total electrical input power of 23.63 W (185 mW per laser source). For the laser source example considered in this paper, the maximum efﬁciency is achieved at an optical output power of 23 mW . If we want to use a laser source that outputs 23 mW of power, then we propose to place the laser source along the edge so that there are more opportunities for sharing. The 128 photonic channels in the 8-ary 3-stage Clos topology correspond to 128 waveguides. For the case where the laser sources are placed along the edge, the optical output power required for each waveguide is 0.18 mW for each λ. This value is higher than 0.15 mW due to longer waveguide lengths. We can share 16 laser sources (1 for each λ) across these 128 waveguides so that they can operate at 6.38% efﬁciency. This results in a total electrical input power of the lasers of 5.74 W (359 mW per laser source). As the waveguide loss per cm increases, the total optical output power required increases. In the case of nonshared local laser sources, this increment in optical output power improves the efﬁciency. As a result, the total electrical input power does not increase signiﬁcantly. In the case of shared local laser sources, the increase in the optical power requirement per waveguide pushes the efﬁciency of laser sources towards the peak value. Hence, similar to non-shared local laser sources, there is only a marginal increment in the electrical input power of the laser sources. The layout with laser sources located along the edge has longer waveguides, and so the optical loss increases signiﬁcantly when waveguide loss per cm increases. Hence, the required optical power per waveguide increases, which in turn lowers the laser source efﬁciency. Overall, if the waveguide loss is low (< 3 dB /cm), using shared laser sources located along the edge is the best option. On the other hand, if the waveguide loss is high (> 3 dB /cm), placing shared laser sources locally is beneﬁcial. For the same 256-core target system, we could use a 16-ary 3-stage Clos network for less contention among cores at the input of each router. This 16-ary 3-stage Clos topology has 48 routers (16 routers in each stage) with each router in the 1st and 3rd stage connected to 16 cores (1 core per router input). This network topology requires a total of 512 channels. To match the bisection bandwidth of this topology with the 8-ary 3-stage Clos, each channel needs 4 λ, and the system has a total of 128 waveguides with 4 channels (4 λ for each channel) sharing one waveguide (16 λ in each waveguide). In general, the trends for the electrical input power of the laser for the 16ary 3-stage Clos are similar to the trends for the 8-ary 3-stage Clos. One exception is that the electrical input power for the case using shared local laser sources is higher for the 16-ary 3-stage topology due to the decrease in the degree of sharing of laser sources. We could also use a 16 × 16 Single-Write-Multiple-Read (SWMR) crossbar topology having a concentration of 16, i.e. TABLE II: Architectural-level parameters for 5 NoCs under consideration. U-shaped and W-shaped layouts are shown in Fig. 1. Logical topology Physical layout Dimension Concentration λ/Channel Number of channels Clos Clos Crossbar Clos Crossbar U-shaped U-shaped U-shaped W-shaped W-shaped 8-ary 3-stage 16-ary 3-stage 16 × 16 16-ary 3-stage 16 × 16 4 1 4 1 4 16 4 64 4 64 128 512 32 512 32 1 0 1 2 3 4 Waveguide Loss (dB/cm) 5 10 100 1000 A l l I u p n t P o w e r P I N ( W ) (a) 8-ary 3-stage Clos with U-shaped layout 1 0 1 2 3 4 Waveguide Loss (dB/cm) 5 10 100 1000 A l l I u p n t P o w e r P I N ( W ) (b) 16-ary 3-stage Clos with U-shaped layout 1 0 1 2 3 4 Waveguide Loss (dB/cm) 5 10 100 1000 A l l I u p n t P o w e r P I N ( W ) (c) SWMR crossbar with U-shaped layout 1 0 1 2 3 4 Waveguide Loss (dB/cm) 5 10 100 1000 A l l I u p n t P o w e r P I N ( W ) (d) 16-ary 3-stage Clos with W-shaped layout 1 0 1 local non−share local share edge share 2 3 4 Waveguide Loss (dB/cm) 5 10 100 1000 A l l I u p n t P o w e r P I N ( W ) (e) SWMR crossbar with W-shaped layout Fig. 8: Total laser power vs. waveguide loss for various sharing scenarios and placements of on-chip laser sources. (a), (b) and (c) compare various topologies with U-shaped layout. (b) and (d) compare various layouts for 16-ary 3-stage Clos topology. (c) and (d) compare various layouts for SWMR crossbar topology. We assume each core in the logical layer operates at 0.7 W . each input of the crossbar can be accessed by 16 cores (similar to 16-ary 3-stage Clos) for the NoC and map it to the U-shaped physical layout. In this case, to match the bisection bandwidth of the crossbar with the Clos networks, we need 64 λ for each channel (4 waveguides per channel). Figure 8(c) shows the total electrical input power for the lasers. Compared to the Clos network, the SWMR channels are shared by more receivers, therefore the large number of rings along longer waveguides results in higher laser power consumption than the Clos networks. For smaller waveguide losses (< 2 dB /cm), shared laser sources located along the edge are preferable. On the other hand, for larger waveguide losses (> 2 dB /cm), using shared local laser sources are preferable. Overall, the best sharing and placement of on-chip laser sources depend on the network topology. For example, at 3.5 dB /cm waveguide loss, for 8-ary 3-stage Clos and SWMR crossbar mapped to U-shape physical layout, using shared local laser sources minimizes the electrical input power, while for 16-ary 3-stage Clos using shared laser sources located along the edge is the better choice. On the other hand for a 2 dB /cm waveguide loss, shared local laser sources are preferable for all three logical topologies. B. Laser source placement and sharing across various physical layouts Depending on alternate power, performance and area design constraints, the placement and routing tools may generate physical layouts that are different from the U-shaped layout that we considered in the earlier subsection. The choice of laser source sharing and placement changes with a change in the physical layout. For example, mapping the 16-ary 3stage Clos topology and the 16 × 16 SWMR crossbar to a W-shaped layout (see Fig. 1(a)) increases optical waveguide losses, and hence the electrical input power required for a laser source. Figure 8(d) and 8(e) shows the total electrical input power for the lasers varying with waveguide loss per cm for the 16-ary 3-stage Clos and 16 × 16 SWMR crossbar with W-shaped layout, respectively. Similar to the U-shaped layout, for low waveguide loss shared laser sources placed along the edge are preferable, while for high waveguide loss shared local laser sources are preferable. The crossover point (i.e. where the choice of laser source placement and sharing changes from shared laser sources placed along the edge to shared laser source placed locally) is different for the two layouts. For the 16-ary 3-stage Clos topology, the crossover points are 3.6 dB /cm and 2.2 dB /cm for the U-shaped and W-shaped layouts, respectively. On the other hand, for the 16 × 16 SWMR crossbar topology, the crossover points are 2.2 dB /cm and 1.5 dB /cm for the U-shaped and W-shaped layouts, respectively. V I . CONC LU S ION In this paper, we explored limits and opportunities for sharing and placement of on-chip laser sources that drive the silicon-photonic NoC with the goal of minimizing the total laser power consumption. We ﬁrst explored the power, efﬁciency and temperature tradeoffs associated with on-chip laser sources. Then we proposed a cross-layer methodology                                         that jointly considers NoC bandwidth constraints, thermal constraints and physical layout constraints to determine the sharing and placement of laser sources. We explored the application of our methodology to a 256-core system by considering three different logical topologies, two different physical layouts and three different sharing/placement strategies for its NoC. Our analysis shows that the choice of laser source placement and sharing changes with the choice of logical topology, physical layout and waveguide loss. For a 8-ary 3-stage Clos mapped to a U-shaped layout, with waveguide loss lower than 3 dB /cm, shared laser sources placed along the edge consume the least laser power, while at waveguide loss higher than 3 dB /cm, local shared laser sources provide the least laser power. For a 16 x 16 SWMR crossbar topology with matching bisection bandwidth and mapped to the U-shaped layout, shared laser sources placed along the edge are preferable for waveguide losses less than 2.2 dB /cm, while local shared laser sources are preferable for waveguide losses greater than 2.2 dB /cm. For the same crossbar topology mapped to a W-shaped layout, the shared laser sources placed along the edge are preferable for waveguide losses less than 1.5 dB /cm, while local shared laser sources are preferable for waveguide losses greater than 1.5 dB /cm. ACKNOW LDG EM EN T This work has been partially supported by the NSF grants CCF-1149549 and CNS-1149703. "
Sampling-based approaches to accelerate network-on-chip simulation.,"Architectural complexity continues to grow as we consider the large design space of multiple cores, cache architectures, networks-on-chip (NoC) and memory controllers. Simulators are growing in complexity to reflect these system components. However, many full-system simulators fail to utilize the underlying hardware resources such as multiple cores; consequently, simulation times have grown significantly. Long turnaround times limit the range and depth of design space exploration. Communication has emerged as a first class design consideration and has led to significant research into NoCs. NoC is yet another component of the architecture that must be faithfully modeled in simulation. Here, we focus on accelerating NoC simulation through the use of sampling techniques. We propose NoCLabs and NoCPoint, two sampling methodologies utilizing statistical sampling theory and traffic phase behavior, respectively. Experimental results show that NoCLabs and NoCPoint estimate NoC performance with an average error of 7% while achieving one order of magnitude speedup.","Sampling-based Approaches to Accelerate Network-on-Chip Simulation Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto {daiwenbo, enright}@ece.utoronto.ca Wenbo Dai, Natalie Enright Jerger Abstract—Architectural complexity continues to grow as we consider the large design space of multiple cores, cache architectures, networks-on-chip (NoC) and memory controllers. Simulators are growing in complexity to reﬂect these system components. However, many full-system simulators fail to utilize the underlying hardware resources such as multiple cores; consequently, simulation times have grown signiﬁcantly. Long turnaround times limit the range and depth of design space exploration. Communication has emerged as a ﬁrst class design consideration and has led to signiﬁcant research into NoCs. NoC is yet another component of the architecture that must be faithfully modeled in simulation. Here, we focus on accelerating NoC simulation through the use of sampling techniques. We propose NoCLabs and NoCPoint, two sampling methodologies utilizing statistical sampling theory and trafﬁc phase behavior, respectively. Experimental results show that NoCLabs and NoCPoint estimate NoC performance with an average error of 7% while achieving one order of magnitude speedup. IN TRODUC T ION I . As the number of cores in contemporary processors continues to scale, the criticality of NoC design to overall performance increases accordingly. NoC designers are relying more heavily on full-system simulation to faithfully evaluate their designs. In full-system simulation, the interaction between applications and the NoC is fully exercised; the performance of new designs is accurately evaluated. Although full-system simulation enjoys the beneﬁt of high ﬁdelity, it suffers from prohibitively long turnaround times. Applications in full-system simulation experience up to 100,000× slow down compared to native execution [17], limiting the range and depth of design space exploration. Sampled full-system simulation [5][13][14][23][25][28] [30] is an effective technique to reduce simulation turnaround times for single-, multi-threaded and multiprogrammed applications. In sampled full-system simulation, only a small but representative portion of the application is simulated in detail, the un-sampled intervals are fast forwarded. Existing work mainly focuses on evaluating micro-architecture designs, and report metrics such as CPI or application run time. To the best of our knowledge, there is no existing work exploring sampling methodologies for NoC simulation. Two major challenges exist for sampled NoC simulation1 . First, NoC simulation focuses on different metrics compared to core simulation, so it requires a new sampling methodology. This research was funded by a gift from Intel. Additional support was provided by the Canadian Foundation for Innovation and the Ontario Research Fund. 1 In the rest of our paper, NoC simulation refers to the full-system simulation where NoC performance is the focus of the evaluation. In processor simulation, CPI [5][13][30] or program basic block information [23] is used to select samples. In contrast, in NoC simulations, the designers are concerned with the trafﬁc behavior and network performance metrics including average packet latency. Therefore, the trafﬁc behavior or statistical characteristics of network metrics should be studied and utilized. Second, a new measure is needed to characterize applications and mark the locations of sample intervals. Such a measure must be network architecture independent, so that its marked samples are consistently representative across different network conﬁgurations. In single-threaded and multiprogrammed sampled simulations [23][30], instruction count is used to measure the application; however, it is unreliable and architecture-dependent for multi-threaded applications: multithreaded applications manifest execution divergence as the timing of inter-thread synchronization changes with the hardware conﬁguration, causing instruction count to vary signiﬁcantly. In this paper, we address the aforementioned challenges and propose sampling methodologies for NoC simulation. Speciﬁcally, we introduce two sampling methodologies: NoCLabs and NoCPoint. They are based on statistical sampling theory and trafﬁc behavior information, respectively. We make the following primary contributions: • Demonstrate that using total instruction count (TIC) to sample multi-threaded applications results in an unrepresentative sample; instead, user mode instruction count (UMIC) is a NoC architecture-independent measurement for trafﬁc characterization and sample selection; • Offer insights on parameter selection by exploring the impact of different parameters including the sample size, unit size and the type of vector used to characterize spatial behavior of trafﬁc; • Provide concrete implementations of NoCLabs and NoCPoint. These techniques estimate NoC performance with an average error of less than 7% while speeding up simulation by one order of magnitude. I I . NOC ARCH I TEC TUR E IND E PEND EN T M EA SUR EM EN T When sampling an application, characterizing the application and marking the locations of sample intervals are essential steps. Selected samples must be representative of the application even when it is running with different architectural conﬁgurations. Therefore, one must use an architecture-independent measurement to mark the locations of samples. For sampling a single-threaded application [23][30], total instruction count (TIC) has been effectively used as it is architectureindependent. However, the TIC of a multi-threaded application is not an architecture-independent measurement; it varies when the architectural conﬁguration changes. Fig. 1 plots the  16  14  12  10  8  6  4  2  0 barnes blackscholes bodytrack canneal ﬀt ﬂuidanimate ocean_cp radix raytrace swaptions vips water_nsquared avg a v r i a t i n o ( % ) TIC_variation UMIC_variation Fig. 1. Variation in total instruction counts (TIC) and user mode instruction counts (UMIC) when running with different network conﬁgurations.  0  420  0.02  0.04  0.06  0.08  0.1  430  440  450  460  470 i c e n j i t n o r a t e TIC (in million) (a) injection rate over TIC for network1   0  0.5  1  1.5  2  228  230  232  234  236 i c e n j i t n o r a t e UMIC (in million) (c) injection rate over UMIC for network1  0  420  0.02  0.04  0.06  0.08  0.1  430  440  450  460  470 i c e n j i t n o r a t e TIC (in million) (b) injection rate over TIC for network2  0  0.5  1  1.5  2  228  230  232  234  236 i c e n j i t n o r a t e UMIC (in million) (d) injection rate over UMIC for network2 Fig. 2. Injection rates vs total instruction count and user mode instruction count for canneal with different network conﬁgurations. variations of TIC for several PARSEC [3] and SPLASH2 [29] applications when running with two different NoC conﬁgurations: net1 and net2 (described in Sec. IV). The maximal variation is as high as 14.31% (swaptions). This raises one major issue for TIC-based sampling: using TIC to specify the start of a sample interval for net1 will lead to a different piece of application code being run on net2 due to the large variance; as a result, sample intervals with net2 are not representative. Fig. 2(a) and (b) further demonstrate how TIC-based multithreaded sampling can result in an unrepresentative sample. We plot the trafﬁc injection rates against TIC for canneal when running with two different networks. The trafﬁc intervals marked in grey are caused by the same piece of code in the program. However, the TIC corresponding to that trafﬁc is skewed signiﬁcantly (by 37 million) when a different network is used. Therefore, as a measurement closely tied to architecture, TIC can cause sampled multi-threaded simulation to be inaccurate, and lead to misleading conclusions about performance and design choices. An architecture-independent measurement for multi-threaded applications is needed. To this end, we propose to use the user mode instruction count (UMIC) instead, which is observed to be stable when different networks are used. Fig. 1 shows a maximal variation of only 0.67% for UMIC (canneal). Moreover, looking at the trafﬁc injection rates versus UMIC for canneal in Fig. 2(c) and (d), where the grey intervals are the same segment of application code as in Fig. 2(a) and (b), one sample interval will execute the same piece of code across different network architectures. The rationale behind the stability of UMIC is two-fold. For a multi-threaded application, execution time is spent mainly on two aspects: useful work and scheduling/synchronization [20]. The amount of useful work does not vary a lot with the machine it is running on. Since useful work is largely performed by user mode instructions, it is straightforward that UMIC is stable. However, the timing and duration of scheduling and synchronization can easily change along with architectural factors such as inter-core communication latency. The number of instructions used to perform scheduling/synchronization ﬂuctuates, especially for spin-lock type synchronization. As scheduling and synchronization are handled by the operating system in kernel mode (for instance, pthreads are implemented by Native POSIX Thread Library within linux kernel), its variation will be reﬂected in the kernel mode instruction count, leaving UMIC unaffected. Given the network architecture independent property of UMIC, we use it in application characterization and sample selection. User mode instructions have been used to sample multiprocessor throughput applications [28]. In throughput applications, user-instruction throughput is proportional to transaction throughput, therefore user-instructions per cycle (U-IPC) is sampled to assess transaction throughput. In contrast, we use user mode instruction count as a network architecture independent measurement, trafﬁc characteristics are examined for intervals of U user-instructions. The fundamental difference is that user mode instruction count is used as a performance metric and the estimation target of the sampling [28], while we use it as a measurement to proﬁle network trafﬁc. I I I . Sampled NoC simulation looks at how to select a minimal but representative portion of the full application trafﬁc to stimulate the network. Network performance metrics measured with the sampled trafﬁc should be an accurate estimate of the true values. We introduce two NoC simulation sampling methodologies: NoCLabs and NoCPoint. They utilize statistical sampling theory and trafﬁc phase behavior information, respectively. Before diving into the details, we introduce several common aspects for both NoCLabs and NoCPoint including the trafﬁc unit size U , population size N , and sample size n. • Trafﬁc unit size U designates the scale for studying trafﬁc. We divide the full application trafﬁc into non-overlapping units of size U , and then examine network performance metrics such as average packet latency of each unit. U is speciﬁed in terms of UMIC. • Population size N is the total number of U -sized units in the full application trafﬁc (N = total UMIC ). • Sample size n is the number of trafﬁc units that will be included in the sample. SAM P L ING NOC S IMULAT ION U Trafﬁc unit size U and sample size n are decided by NoCLabs and NoCPoint users. In the rest of this section, we describe how NoCLabs and NoCPoint select samples and discuss the decision for U and n. A. NoCLabs: Latency based Statistical Sampling Inferential statistical sampling [9] estimates a given accumulative property of a population by only measuring a sample. It prescribes a mathematically-sound procedure to select a         n ≥ ( · ˆV )2 (1) ˆV = σ minimal but representative sample. The minimal sample size n needed is quadratically proportional to the target metric’s variation. Since average packet latency is a commonly reported metrics, we base our sample selection on its variation, resulting in Latency based Statistical Sampling for NoC simulation, or NoCLabs. In particular, the minimal sample size n to represent the population depends upon three variables: • The coefﬁcient of variance of the packet latencies per unit in the population, ˆV ; • Conﬁdence level (1 − α); • Conﬁdence interval ±ε. µ , where σ and µ are the standard deviation and mean value respectively. Conﬁdence level and interval are to be given by NoCLabs users. Informally, they indicate that one can be (1 − α) conﬁdent that the estimated value is within ±ε of the true value. The minimal sample size n is deﬁned as: z ε where z is the 100[1 − α 2 ] percentile of the standard normal distribution. Such an equation indicates that applications that show larger degrees of variation in their trafﬁc will require a larger sample size. After n is decided, systematic sampling is performed on the population: one trafﬁc unit out of every k units is picked as sample, where k = N n . If (1 − α), ε and ˆV are all known, sample size n is easily calculated from Eqn. 1. However, as it is related to population latencies, ˆV can only be obtained after ﬁnishing the simulation of the full application. But simulating the full application is exactly the burden that sampling is attempting to avoid. To solve this dilemma, the variation of an initial sample’s per unit packet latencies, ˆVinit , is used in place of ˆV . The steps of performing sampling using NoCLabs are as follows: 1) Specify an accuracy requirement in terms of conﬁdence level (1 − α) and conﬁdence interval ±ε; 2) Take an initial sample of size ninit on the population; simulate and measure the sample; 3) Calculate the initial sample’s ˆVinit , and check whether the initial sample’s resulting conﬁdence interval εinit meets the speciﬁed requirement; εinit is calculated as εinit ≥ z · . 4) If εinit is within the desired ±ε, the initial sampling is a success; measured network performance metrics are reported to estimate the true values; 5) If εinit does not meet the requirement, re-sample with a new sample size nadjust . nadjust is calculated by using the initial sample’s ˆVinit , the desired (1 − α) and ±ε and applying Eqn. (1); 6) Simulate and measure the adjusted sample, and report the network performance. Among these steps, carefully choosing the initial sample size is vitally important: an ninit that is too small can easily result in the ﬁrst trial failing to meet the accuracy requirement; extra simulation is required. Too large ninit , on the other hand, may include more units in the sample than necessary. As a result, potential speedup will be sacriﬁced. We discuss how to decide ninit for PARSEC and SPLASH-2 in Sec. IV. ˆVinit √ninit B. NoCPoint: Exploiting Trafﬁc Phase Behavior Phase behavior exists in program execution: a set of execution intervals share a greater amount of similarity within themselves compared to other execution intervals. There is a correlation between the program phase and the processor’s architectural performance [1][23]. Similarly, application trafﬁc also exhibits phase behavior [2] and there is a correlation between trafﬁc phase and network performance [12]. NoCPoint performs sampling by exploiting an application’s trafﬁc phases. There are 3 steps: 1) characterizing and classifying the full application trafﬁc into phases, 2) sampling the full application trafﬁc based on phase information and 3) using the sampled trafﬁc to stimulate the network and measure its performance. 1) Characterizing and classifying trafﬁc: A trafﬁc phase refers to a set of trafﬁc units that behave similarly. To reveal the phases, we ﬁrst characterize the spatial and temporal trafﬁc behavior. We proﬁle each trafﬁc unit by an injection-ejection rate vector (IERV). In an IERV, each element represents the injection or ejection rate for one node or multiple nodes in the NoC. Two decisions must be made regarding IERVs: 1) how many user mode instructions each IERV covers (trafﬁc unit size U ); 2) what is the granularity of injection/ejection rate information in the IERV. In Sec. IV, we discuss the impacts of choosing different U s and types of IERV. After the behavior of the full application trafﬁc is characterized, one can cluster the trafﬁc units into classes. We use hierarchical clustering [27] to classify trafﬁc: during the clustering, Manhattan distances between IERVs are ﬁrst calculated, then classes are formed so that intra-class IERVs are closer to each other than inter-class IERVs. That is to say, trafﬁc within the same class manifests similar behavior. 2) Sampling the trafﬁc and measuring the network performance: Once the full application trafﬁc is clustered, we sample the trafﬁc by selecting one unit from each class: each chosen unit has the closest distance to the centroid of that class, and represents the trafﬁc behavior of that class. If there are multiple units with the same minimal distance, the earliest one is chosen. As a result, the sample size n equals to the number of classes formed in the clustering process. The last step is to measure network performance by simulating the sampled trafﬁc. As the classes formed through clustering may vary in size, the packet latencies of each measured unit are weighted to generate the overall average packet latency. Other network performance metrics, such as latency distribution and network power can also be collected and calculated in a similar manner. IM P LEM EN T ING NOCLAB S AND NOCPO INT IV. In this section, we describe our implementation of NoCLabs and NoCPoint. By carefully selecting the sampling parameters such as trafﬁc unit size U and sample size n, we can perform sampling in a cost-effective way. A. Simulation infrastructure We simulate 16 cores using FeS2 [18], a full-system, cycleaccurate x86 simulator. Booksim [11] is used to simulate the NoC. The conﬁgurations of FeS2 and Booksim are given in Table I. We run unmodiﬁed Linux on top of FeS2 and evaluate PARSEC [3] and SPLASH-2 [29] applications. All applications run with 16 threads, use simsmall input, and are run to completion. Only the regions of interest (ROI) are measured. N et0 is an ideal network with a one-cycle ﬁxed latency between all nodes. It exposes the inherent communication behavior of applications (e.g. spatial behavior). To sample TABLE I. S IMU LAT ION CON FIGURAT ION S cores L1 Cache (I & D) L2 Cache Coherence Protocol net0 net1 net2 net3 net4 16, P4-like private, 4-way, 32KB each, 64 Byte Blocks private, 8-way, 512KB each, 64 Byte Blocks MOESI distributed directory 1 cycle ﬁxed latency between all nodes, unlimited network interface buffer, 8 Byte ﬂit 4x4 2D-Mesh, adaptive XY/YX routing, 8 VCs, 8 buffers/VC, 8 Byte ﬂit 4X4 2D-Mesh, dimension-order routing, 4 VCs, 4 buffer/VC, 4 Byte ﬂit 4-ary, 2-ﬂy butterﬂy, destination tag routing, 4 VCs, 4 buffers/VC, 8 Byte ﬂit 4-ary, 2-ﬂy fat tree, nearest common ancestors routing, 4 VCs, 8 buffer/VC, 8 Byte ﬂit F W D F W D F Fig. 3. Simulation state transitions in sampled NoC simulation. Three states are functional simulation (F), detailed timing simulation (D) and warmup (W). an application, full-system simulation with net0 is performed ﬁrst to obtain the total UMIC and IERVs. This data is used in the steps described in Sec. III. After that, one can run sampled NoC simulation with other network conﬁgurations and estimate their performance without needing to simulate the entire application again. N et1 − 4 are the candidate networks to be evaluated using NoCLabs and NoCPoint. N et1 and net2 are direct networks; they are well- and underprovisioned respectively. N et3 and net4 are indirect networks. These conﬁgurations verify the effectiveness of NoCLabs and NoCPoint when applied to different types of networks. B. Fast forward and warmup In sampled NoC simulation, only sampled trafﬁc units are simulated in detail by the timing module (D); an unsampled interval between two sampled units is fast forwarded by running faster, functional simulation (F). To keep track of an application’s progress, user mode instructions are counted during both D and F. After each D period, the simulator drains all the outstanding packets in the network without measuring them, and then switches to F. As a result, when it switches back from F to D later on, the network is cold, introducing bias into performance measurement. To alleviate the effects of cold start, we warmup the network by adding a warmup (W) period before each D. A warmup period simulates the application in detail but discards all the performance statistics. As a result, the sampled NoC simulation transitions among three simulation states as illustrated in Fig. 3. A sample of size n contains n timing periods and n corresponding warmup periods; as a result, the number of user mode instructions simulated in detail is: U M I Csim = n × (W + D) (2) where W and D are the length of warmup and detailed timing period respectively. The value of U M I Csim largely affects how quickly the simulation ﬁnishes. The length of each warmup period W is left to the users to decide. A small W does not warmup the state enough, and an unnecessarily large W hurts the speedup. Dally and Towles [8] provide a heuristic procedure to detect the warmup length, and recognize that event counts on the order of 100 to 1,000 can bring the network to a steady state. We empirically set the length of warmup period to 1000 user mode instructions for both NoCLabs and NoCPoint. This generates ∼100 messages t i n u r e p i s e c n e t a l t e k c a p e g a r e v a f o n o i t a i  5  4.5  4  3.5  3  2.5  2  1.5  1  0.5  0 10^1 r a v barnes blackscholes bodytrack canneal ﬀt ﬂuidanimate ocean_cp radix raytrace swaptions vips water_nsquared avg 10^2 10^4 10^5 traﬃc unit size U Fig. 4. Coefﬁcient of variance of average packet latency across different trafﬁc unit sizes. 10^8 10^3 10^6 10^7 to warmup the network, because the average message per userinstruction is 0.097 across all our applications. Note that the cache is warmed up by the same 1000 instructions used to warm up the network. Such period of warmup is short for cache and may introduce cold start bias. However, our large trafﬁc unit size U (up to 10 million) mitigates the cold start effect. To completely eliminate the cold start effect, one must keep the cache warmed during fast-forward, but this slows down fast-forward and reduces the simulation speedup. C. Reﬁning sampling parameters for NoCLabs NoCLabs users need to determine the initial sample size ninit . To optimize ninit , we run an analysis on the coefﬁcient of variance of the full application’s packet latencies with net1. Note that this population information is obtained by running the full-system simulation with net1; our sampling method does not require this full-system simulation step. We include the data to illustrate our choice of parameters. Fig. 4 plots the variations of the full trafﬁc’s packet latency, ˆV for different trafﬁc unit sizes U . We observe that for each application, its ˆV decreases as U grows. As U increases, short term variation is hidden by a larger window. The knee point of the curve exists near U equal to 100,000 for all the applications. Beyond that point, increasing U does not reduce ˆV signiﬁcantly. By considering both Eqn. (1) and (2), this implies using an U larger than the knee point causes U M I Csim to grow proportionally without improving the sampling accuracy. We use the knee point as a guideline for U and note that the U should not exceed the knee point. However, this does not give a deﬁnitive conclusion on what is the best U . We show in Sec. V that a point just to the left of the knee point (e.g. U =10,000) achieves both high accuracy and speedup. Closer examination of the data shows that selecting a U between 10,000 and 100,000 can more accurately locate the knee point. Based on the analysis above, we recommend the trafﬁc unit size U to be 10,000. The average ˆV when U =10,000 of all applications is 0.29. If we set an accuracy requirement of 0.99 conﬁdence level with a ±3% conﬁdence interval, an initial sample size ninit of 841 can be obtained by using Eqn. (1). D. Reﬁning sampling parameters for NoCPoint NoCPoint users also need to answer several questions in regard to parameter selection. They are: 1) What is the trafﬁc unit size U : Since each IERV abstracts the behavior of one trafﬁc unit, using a small U preserves trafﬁc temporal behavior with great detail. However, the beneﬁt comes at a price: 1) because population size N = total UMIC , a small U results in a large N for a U             TABLE II. IN J EC T ION - E J EC T ION RATE V EC TOR S ( IERV S ) IERV type detail level per-ﬂow per-node row-column highest high medium vector length for an X-node network X 2 2X 4√X given application, which implies a time-consuming clustering process; 2) after the clustering is ﬁnished, a small U may require more than one unit from each class to fully represent the trafﬁc. On the other hand, an extremely large U yields a very small population size. Zhang et al. [32] point out that clustering on a population smaller than 1000 can lead to poor clustering quality. Using these principles as a guideline, we experimentally decide U in Sec. V. 2) How much detail to include in the IERV: To abstract spatial trafﬁc behavior, we consider three different types of IERV. Table II lists their detail levels and the vector lengths needed to characterize an X-node network. In a per-ﬂow IERV, each element represents the injection rate of one source-destination pair, or ﬂow, in the network. It is the ﬁnest way to proﬁle trafﬁc spatial behavior. Alternatively, one can use a per-node or rowcolumn IERV. One element in a per-node IERV represents the injection/ejection rate for one node, and each element in a rowcolumn IERV summarizes the injection/ejection rate for all the nodes within the same row/column. As shown in the table, the three candidates require different vector lengths to represent a network of the same size. For our 16-node network, they need 256, 32 and 16 elements respectively; these differences inﬂuence the clustering process. Clustering vectors whose dimension is as high as 256 raises two issues. The ﬁrst issue is the curse of dimensionality. When dimensionality increases, the data become so sparse that they are no longer statistically signiﬁcant; consequently, they cannot be clustered effectively. The second is a prohibitively slow clustering process. Clustering the per-ﬂow IERVs is over 100× slower than clustering the row-column IERVs on average. When running on a machine with Intel i5 processor and 4GB memory, the longest clustering time is over 12 hours. We rule out per-ﬂow IERV since it is too time-consuming, and only consider per-node and row-column IERV. Fig. 5 visually compares the clustering results for raytrace when per-node and row-column IERV are used. In each ﬁgure, different colors denote different classes formed. When one compares how the application is divided into different classes (Fig. 5(a) vs Fig. 5(b)), no major difference can be noted; the same results are found for other applications. In Sec. V, we quantitatively compare the performance of per-node and rowcolumn IERV-based sampling. 3) How many classes to form: A stopping rule in hierarchical cluster analysis refers to a procedure for determining the number of classes, K , in a data set. Clustering the data set into more than K classes indicates a solution containing too many clusters; constructing less than K classes, on the other hand, loses information by merging distinct classes. We use the Calinski and Harabasz formal method [4] to guide our trafﬁc clustering process. The basic idea of the Calinski and Harabasz method is to ﬁnd a number of classes K that can maximize the ratio between the inter-class variance and the intra-class variance: it calculates the ratio of all the K s ranging from 1 to the population size N , and then ﬁnds the maximum ratio and reports its corresponding K . In NoCPoint, after characterizing the trafﬁc by IERVs, we use the Calinski and Harabasz e a t r n o i t c e n j i e a t r n o i t c e n j i 0 200 400 600 800 UMIC (in million) (a) per-node IERV 0 3 0 . 0 2 0 . 0 1 0 . 0 0 0 . 0 3 0 . 0 2 0 . 0 1 0 . 0 0 . 0 0 200 400 600 800 UMIC (in million) (b) row-column IERV Fig. 5. Comparison of the clustering result based on per-node and row-column IERV. raytrace is used as an example. % r o r r e  14  12  10  8  6  4  2  0 net1 net2 net3 net4 barnes blackscholes bodytrack canneal ﬀt ﬂuidanimate ocean_cp radix raytrace swaptions vips water_nsquared avg Fig. 6. Error for average packet latency estimation of NoCLabs. The predicted conﬁdence intervals are shown by the error bars. method to analyze the IERVs and calculate the optimal number of classes; then we cluster using the hierarchical clustering algorithm, and sample the trafﬁc following the steps described in Sec. III. Prior work has not used this approach to determine the number of classes in a mathematically-rigorous manner. V. EX PER IM EN TA L R E SU LT S In this section, we demonstrate how accurately NoCLabs and NoCPoint estimate NoC metrics and compare the two techniques. Insights on parameter selection are provided for both techniques. As network designers are not only interested in average latencies, but also care about the network congestion level and power, we also provide results of latency distribution and power estimation to provide a more comprehensive comparison. A. NoCLabs As noted in Sec. IV, we set an accuracy goal of 0.99±3%, use U =10,000 and an initial sample size of 841 to sample NoC       % r o r r e  14  12  10  8  6  4  2  0 U=100,000 U=1,000,000 U=10,000,000 rowCol-net1 rowCol-net2 rowCol-net3 rowCol-net4 perNode-net1 perNode-net2 perNode-net3 perNode-net4 p u d e e p s  14  12  10  8  6  4  2  0 U=100,000 U=1,000,000 U=10,000,000 rowCol-net1 rowCol-net2 rowCol-net3 rowCol-net4 perNode-net1 perNode-net2 perNode-net3 perNode-net4 Fig. 7. Error in average packet latency estimation for the four candidate networks when using different trafﬁc unit sizes and IERV types. simulation with all the candidate networks. Fig. 6 illustrates the measured estimation errors for average packet latency. The errors are with respect to the true values measured from full-system simulation. The conﬁdence intervals are also plotted. For all the applications, the results generated by the initial sample provide an accurate estimation without having to perform step 5 described in Sec. III-A. The least accurate estimation comes from blackscholes and net2, with an error of 5.94% and a predicted conﬁdence of ±4.45%. From Fig. 4, we see that this is caused by blackscholes’s high variation. Our initial sample size ninit is selected based on the average ˆV of all applications at U =10,000, which is lower than that of blackscholes. As a consequence, blackscholes’s variation was not fully captured. By sampling using an nadjust of 1878, the error can be reduced to 3.69%. Similarly, resampling other applications using an nadjust can further reduce their estimation errors. On average, we estimate packet latency with an error of 1.75%, 2.45%, 1.83% and 1.68% for the four candidate networks, respectively. Such high accuracy demonstrates that choosing a trafﬁc unit size U just to the left of the knee point in Fig. 4 works well in practice. B. NoCPoint The procedure of selecting trafﬁc unit size U for NoCPoint is empirical. As a rule of thumb, we prefer a large U over a small U . The rationale for this is that after the clustering step, each class is represented by only one unit in the sample; a small trafﬁc unit may not stimulate the network long enough to represent the underlying class. We explore three different unit sizes: 100,000, 1 million and 10 million. They are tested with both row-column and pernode IERV. Fig. 7 shows the errors for different U s on net1−4. For a given U , the results vary across different applications; on average, using U of 1 million and 10 million achieves low error percentages: 3.37% for 1 million and 2.75% for 10 million. In contrast, the error increases by ∼1.5× when U =100,000 is used. This validates our preference for large U s. Although larger U is generally better, an excessively large value reduces the achievable simulation speedup. Fig. 8 shows the speedups when using different unit sizes with different sampling conﬁgurations. Wall-clock time is used to compute the speedup with respect to the full-system simulation. The common trend is that, larger unit size results in smaller speedup. For U =10 million, the average speedup is 6.31. If one continues to increase U beyond that, the speedup will keep dropping, as a result, the sampled simulation will not be fast enough to compensate for the accuracy that is sacriﬁced. Based Fig. 8. Simulation speedup achieved by different unit sizes on different sampling conﬁgurations. on our analysis, we recommend U =1 million to achieve a balance between estimation accuracy and simulation speedup. The other observation is that for the conﬁguration of pernode IERV-net2, the highest speedup is achieved by U =1 million, instead of the minimal U of 100,000. By examining the speedups on per-application basis, we ﬁnd that v ips reports a larger speedup (23.1) when U =1 million, compared to 10.1 when U =100,000. Although more instructions are simulated when U =1 million, the last sampled trafﬁc unit occurs signiﬁcantly earlier than the end of the application. As a result, the simulation can terminate before reaching the end of the application. In contrast, when U =100,000, the sampled trafﬁc includes fewer user mode instructions, but its last sampled unit is near the end of the application; this postpones the termination of the simulation. As a beneﬁt of NoCPoint, if the sampled units do not last until the end of the application, the sample simulation can terminate as soon as the last unit is ﬁnished which will increase the speedup opportunity. Fig. 7 also compares the effects of using row-column and per-node IERV. For estimation accuracy, there is no clear winner. For example, when U =1 million is used, per-node IERV is more accurate than row-column IERV for net2 (3.2% error vs 3.75% error), but it is the opposite for net4 (3.82% vs 3.06%). Therefore, our quantitative comparison shows that a row-column IERV is as accurate as a per-node IERV in characterizing the trafﬁc behavior. However, clustering on rowcolumn IERVs is 2× faster than clustering per-node IERVs. We recommend row-column IERV as a cost-effective solution. C. NoCLabs vs NoCPoint We compare the estimation accuracy and speedup of NoCLabs and NoCPoint. NoCLabs uses U =10,000 and the initial sample size n=841, NoCPoint uses U =1 million and the rowcolumn IERV. 1) Comparing different networks: NoC designers rely on simulation results to draw conclusions about whether one network outperforms another. Sampled NoC simulation should provide correct conclusions. Fig. 9 plots the absolute packet latency of the four candidate networks collected from full system simulation, NoCLabs and NoCPoint. It veriﬁes that NoCLabs and NoCPoint are drawing correct conclusions when comparing the performance of different networks. For instance, full system simulation suggests that net1 has lower packet latency than net2 across all applications. Both NoCLabs and NoCPoint agree with full system simulation when comparing the two networks and concludes that net1 outperforms net2; when comparing the performance of net3 and net4 based   y c n e t a l t e k c a p  50  40  30  20  10  0 e barnes blackscholes bodytrack canneal ﬀt ﬂuidanimate ocean_cp radix raytrace swaptions vips water_nsquared avg % r o r r  15  10  5  0 net1_full net1_NoCLabs net1_NoCPoint net2_full net2_NoCLabs net2_NoCPoint net3_full net3_NoCLabs net3_NoCPoint net4_full net4_NoCLabs net4_NoCPoint noclabs-net1 noclabs-net2  15  10  5  0 p u d e e p s power speedup nocpoint-net3 nocpoint-net4 packet_latency latency_dist estmiated metric/simulation speedup noclabs-net3 nocpoint-net1 noclabs-net4 nocpoint-net2 Fig. 9. Packet latency comparison for full system, NoCLabs and NoCPoint. s t e k c a p f o %  30  25  20  15  10  5  0 full_traﬃc NoCLabs_traﬃc NoCPoint_traﬃc NoCLabs_error% NoCPoint_error% 0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 all_other error_sum latency Fig. 10. Packet latency distribution of full vs sample trafﬁc for raytrace. on full system simulation, there is no immediate winner on per-application basis; on average, net4 slightly outperforms net3. Again, NoCLabs and NoCPoint agree with full system simulation both on per-application basis and average. 2) Estimating NoC congestion: Network designers are also concerned with the network congestion level. To determine if our sampled trafﬁc causes the same level of network congestion as the full trafﬁc does, we quantify the difference in latency distribution between sampled and full trafﬁc. In Fig. 10, we use raytrace as an example. The sampled trafﬁc generated by NoCLabs and NoCPoint in net1 are both compared against the original trafﬁc collected for full system simulation. Each group of bars represents the percentage of packets that falls into a range of latency. We compare the two bars representing the same range of latency, and measure the difference. The network congestion estimation error is obtained by summing the differences of all the bars. We ﬁnd only an 1.56% error for raytrace. Fig. 11 compares NoCLabs and NoCPoint’s average error of latency distribution estimation for all the applications and networks. Both techniques exhibit low error. 3) Estimating NoC power: Power is another important metric designers should take into consideration. We validate the power estimation accuracy of NoCLabs and NoCPoint by collecting data from both sampled and full-system simulation. We record network activity including buffer read/write, switch and link traverse during the simulation, and fed them into the DSENT [24] power model to calculate dynamic and leakage power. DSENT is conﬁgured to use a 45nm process and 1 GHz frequency. The average errors are reported in Fig. 11. 4) Putting all together: In Fig. 11, we plot NoCLabs and NoCPoint’s average estimation error of packet latency, latency distribution (i.e. network congestion) and power for net1 − 4. Their speedup is also compared. For NoCLabs, we use error Fig. 11. Comparison between NoCLabs and NoCPoint. Results are averaged across all applications. bar to demonstrate the predicted conﬁdence interval of latency estimation; for the rest of the data points, error bar implies the coefﬁcient of variance of all the applications. NoCLabs and NoCPoint both show high accuracy in NoC performance estimation. The errors of latency and power estimation are all less than 5%. The latency distribution errors are higher (maximal average error is 6.57%) due to the accumulation of multiple comparison points as explained earlier. For all the performance estimation errors, the coefﬁcients of variance are less than 2 (with the exception of 3.2 for the latency estimation with NoCLabs-net2), suggesting a stable accuracy across all the applications. In terms of simulation speed, NoCLabs reports a 8.92× speedup with net2; it offers almost one order of magnitude speedup over full-system simulation. Simulations previously lasting for one week now can be ﬁnished within a day. All speedups have a variation of less than 1 which implies stable simulation performance for NoCLabs and NoCPoint. Although NoCLabs and NoCPoint achieve comparable speedup on average (7.57 vs 7.54), they have different strengths and application scenarios. NoCLabs offers lower error than NoCPoint on average across all the candidate networks (1.92% vs 3.3% for latency, 5.31% vs 5.52% for congestion, 1.61% vs 2.36% for power) due to its rigorous mathematical nature. On the other hand, NoCPoint provides a better understanding of application trafﬁc behavior by exploring its phases (e.g. Fig. 5 illustrates how raytrace stresses the network differently over time). Therefore, users can choose between these two approaches according to their needs. V I . R ELAT ED WORK SMARTS [30][31] and SimPoint [22][23] apply sampling techniques to the micro-architecture simulation running singlethreaded applications. SMARTS utilizes statistical sampling theory to select samples. The sample size is decided by the coefﬁcient of variance of the cycles-per-instruction (CPI) values. In SimPoint, the samples are picked based on the knowledge of program phase behavior, which is detected by analyzing program basic blocks. Both SMARTS and SimPoint sample the dynamic instruction stream, and estimate microarchitectural statistics including IPC, branch prediction and cache miss rate. They were ﬁrst proposed in the context of single threaded application simulation; they have been extended to multiprogrammed workloads [25][28]. Recently, Ardestani and Renau [13] further explore statistical sampling for multi-threaded applications. They propose         time-based sampling to handle the problem of execution variation of multi-threaded applications. Fast-forwarding threads using time information rather than instruction count can maintain the correct relative progress among threads. Carlson et al. [5] provide a general-purpose sampling methodology for multi-threaded application. They highlight the importance of maintaining thread interaction and synchronization events during fast forward to achieve correct relative thread progress. For phase-based multi-threaded application sampling, prior work concentrates on effectively detecting application phases. Instruction counts and trafﬁc count information can be combined to better identify execution phases of multi-threaded applications [32]. Jin et al. [12] examine the characterization and clustering procedure for network trafﬁc. They cluster the trafﬁc by using different types of trafﬁc feature vector. By comparing the intra-class variations of all the cluster results, they also conclude that the row-column vector is efﬁcient. In our paper, we choose row-column IERV over per-node IERV based on the measured results from simulation. Besides sampling, other approaches can accelerate simulation. FPGA-based acceleration can be used for both fullsystem [7][15][19] and NoC simulation [10][26]. FPGA-based simulation introduces additional complexity in building hardware models for all the components. Parallelizing simulation is another approach to exploit multi-core hardware for faster simulation [6][16][17][21]. Synchronization can be a bottleneck and some approaches propose to relax synchronization across simulator threads to improve scalability [17][21]. V I I . CONCLU S ION NoC designers often rely on full-system simulation to faithfully evaluate their designs. In this paper, we propose two sampling methodologies to accelerate NoC simulation: NoCLabs and NoCPoint. For NoCLabs, we use statistical sampling to derive a latency-based NoC simulation sampling procedure. It selects a representative sample from the full trafﬁc based on variations in packet latency. By characterizing the trafﬁc using a network architecture-independent measurement and an IERV, NoCPoint captures the intrinsic temporal and spatial behavior of application communication to identify trafﬁc phases. This phase information is used to select simulation points that represent the full trafﬁc. By applying NoCPoint and NoCLabs, only a small portion of the full system simulation is needed to faithfully evaluate various NoC designs. As NoCLabs and NoCPoint users are faced with rich options in terms sampling parameter selection, we provide guidelines for these parameters. We provide concrete implementations for NoCLabs and NoCPoint, and evaluate them against the full system simulation. Evaluation results show that they estimate network performance metrics including average packet latency, latency distribution and power within an error of only 7%. Meanwhile, they speed up simulation by one order of magnitude. With a reduced simulation turnaround time, the range and depth of NoC design space exploration can be enhanced. ACKNOW L EDGM ENT The authors thank Mike Kishinevsky and Umit Ogras for their invaluable feedback and insight. Additional thanks go to the members of the Enright Jerger research group and the anonymous reviewers for their thoughtful and detailed feedback on this work. [6] [9] [27] [17] [14] "
An analytical model for worst-case reorder buffer size of multi-path minimal routing NoCs.,"Reorder buffers are often needed in multi-path routing networks-on-chips (NoCs) to guarantee in-order packet delivery. However, the buffer sizes are usually over-dimensioned, due to lack of worst-case analysis, leading to unnecessary larger area overhead. Based on network calculus, we propose an analysis framework for the worst-case reorder buffer size in multi-path minimal routing NoCs. Experiments with synthetic traffic and an industry case show that our method can effectively explore the traffic splitting space, as well as the mapping effects in terms of reorder buffer size with a maximum improvement of 36.50%.","An Analytical Model for Worst-case Reorder Buffer Size of Multi-path Minimal Routing NoCs Gaoming Du∗ , Miao Li∗ , Zhonghai Lu† , Minglun Gao∗ and Chunhua Wang∗ ∗ Institute of VLSI design, Hefei University of Technology, Hefei, China, 230009. Email: dugaoming@hfut.edu.cn † Department of Electronic Systems, KTH Royal Institute of Technology, Stockholm, Sweden. Email: zhonghai@kth.se Abstract—Reorder buffers are often needed in multi-path routing networks-on-chips (NoCs) to guarantee in-order packet delivery. However, the buffer sizes are usually over-dimensioned, due to lack of worst-case analysis, leading to unnecessary larger area overhead. Based on network calculus, we propose an analysis framework for the worst-case reorder buffer size in multi-path minimal routing NoCs. Experiments with synthetic trafﬁc and an industry case show that our method can effectively explore the trafﬁc splitting space, as well as the mapping effects in terms of reorder buffer size with a maximum improvement of 36.50%. I . IN TRODUC T ION Unlike those routing algorithms providing only a single path per ﬂow, multi-path routing allows a ﬂow to use different paths to get better trafﬁc balance. As a ﬂow may take diverse paths from source to destination, it can be seen as being split into sub-ﬂows which is called trafﬁc splitting. Multi-path routing outperforms single path in the following aspects: 1) minimize network congestion and packet delay, owning to exploit the path diversity; 2) improve the load balance and overall resource utilization; 3) reduce NoC frequency requirement and power consumption because of balanced network congestion; 4) offer more options to fault tolerant routing. However, out-of-order problems arise. It is very important to guarantee in-order packet delivery for many applications, e.g., multimedia or cache coherence protocols. Various approaches have been proposed. Flow control and reorder buffers are two typical candidates. Murali et al. [1] use ﬂow control to ensure in-order packet delivery. An example is shown in Fig.1(a). A ﬂow with the packet order of P1, P2, P3 and P4 is transferred. It is split into two sub-ﬂows of (P1, P2) and (P3, P4). They traverse through different paths and arrive at the convergence router R4, where a look-up table (LUT) is put for packet reordering. If (P3, P4) arrive at the destination earlier than (P1, P2), their request to the local network interface (NI) will not be granted by the arbiter in R4. Thus, they will be stalled in the router buffers, until the packets in need, i.e., P1 and P2, are transferred to the local network interface (NI). The side effect of this approach lies in that when out-of-order packets are stalled in the router buffers, they will reduce resource utilization and exaggerate the NoC congestion. Furthermore, before the stalled packets are retransmitted, re-arbitration is needed. So additional time will be added to the packet delay. Reorder buffer is another way to handle the out-of-order problem. As shown in Fig.1(b), reorder buffers are put at the local network interface. Suppose the same case as Fig.1(a) occurs. Out-of-order packets, P3 and P4, will be straightly Fig. 1. Motivating examples of two in-order packet delivery schemes for multi-path routing using: (a) ﬂow control, and (b) reorder buffer. stored into the reorder buffer until they become the packet in need. In this approach, there are no packets stalled which enhanced the resource utilization. Furthermore, there is no added re-arbitration time. However, the main shortcoming is the area overhead. In traditional approaches, much pessimistic size of reorder buffer has been set to give performance guarantee, which leads to low hardware utilization. In fact, with worst-case analysis, it can reduce the reorder buffer size with proper ﬂow splitting conﬁguration effectively. So we rethink this problem through analyzing the worst-case reorder buffer size for multi-path minimal routing NoCs, based on network calculus (NC) [2] theory. To the best of our knowledge, this is the ﬁrst work that presents the worst-case analysis of reorder buffer in multi-path routing NoC using network calculus. The proposed analytical model can not only model the upper size bound of the reorder buffer for a given application-speciﬁc NoC, but also resize the reorder buffer by varying the trafﬁc splitting proportion on sub-ﬂows. The contribution is as follows: (1) We ﬁrst present a general analytical model for the worstcase reorder buffer size, with the prior knowledge of upper and lower delay bounds of two non-intersecting sub-ﬂows. The upper delay bound reﬂects the worst-case delay for a given tag ﬂow, which can be calculated using NC. This model is suitable for any tag ﬂow split into two non-intersecting subﬂows. (2) Then we deduce the speciﬁc analytical model for multipath minimal routing NoCs, where the input of both tag ﬂow and contention ﬂows are represented by ﬂow rate and burstiness. (3) Finally, the relationship between trafﬁc splitting proportion and the worst-case reorder buffer size is studied. Through proper trafﬁc splitting conﬁguration, we can not only guarantee in-order packet delivery, but reduce the reorder buffer size. Furthermore, deadlock problem may also arise in multipath routing. So deadlock-free design is needed. Various approaches have been proposed to avoid deadlock such as the turn model [3] and virtual channel. However, the detailed discussion is beyond the scope of this paper. The rest paper is organized as follows. We review previous work in section 2, and introduce the router architecture with reorder buffer in section 3. Analysis of worst-case reorder buffer size using network calculus together with an example is presented in Section 4. Section 5 presents experiments and results. Finally, we conclude in Section 6. I I . R ELAT ED WORK Network calculus is a formal tool to evaluate the network performance, including worst-case delay and backlog bound. It was initialized by Cruz [2]. Recently, it is used in onchip communication. Lu et al. have derived the closed-form performance bound in per ﬂow case [4] and applied trafﬁc regulation policy for controlling the NoC performance [5]. Prior work has demonstrated the effectiveness of multi-path routing in NoCs. Stefan et al. [6] proposed a method on multipath slot allocation with static resource reservation, especially in TDM NoC, which can guarantee bandwidth and delay by employing circuit switching. Jiao et al. proved that multi-path routing can improve throughput and shorten average message latency [7]. Murali et al. [1] proposed a multi-path routing strategy and proved that multi-path routing has advantages in power saving and fault tolerance. Out of order is one of the problems coped with multi-path routing. The work in [1] used ﬂow controlling to guarantee inorder packet delivery. Reorder buffer is another approach. Kwon et al. [8] proposed an idea of transaction id renaming and distributed soft arbitration in the context of multiple memories, with a reorder buffer in the network interface. However, it suffered from high area overhead and low utilization. The work in [9] improved this idea by moving the reorder buffer from network interface to on chip router. Although the resource utilization was enhanced, the performance was degraded with the increasing of NoC size. Yang et al. [10] proposed a reordering mechanism based on a look-up table, but the reorder buffer was partitioned statically. Thus, it suffered from low utilization. Masoud et al. [11] proposed a dynamic buffer allocation architecture for the reorder buffer to increase the utilization and the overall NoC performance. Daneshtalab et al. [12] extended the work in [11], and proposed a streamlined adaptive reordering mechanism, i.e. dynamic reorder buffer allocation, to enhance the resource utilization. The work above demonstrated that reorder buffer is both important and practical to parallel communication in NoC, especially to improve memory efﬁciency. Nevertheless, most of them lacked the discussing of worst-case reorder buffer size. Du et al. [13] proposed a formal approach to derive the worst-case Fig. 2. Multi-path routing architecture. delay bound for multi-path minimal routing NoCs. However, they did not discuss how to calculate the worst-case size of reorder buffer. In this paper, based on a basic reorder buffer architecture, we will derive the analytical model using NC and ﬁnd approaches to diminish the worst-case reorder buffer size. I I I . ROU TER ARCH I T EC TUR E W I TH R EORD ER BU FFER We ﬁrst give an overview of the router architecture, as well as the reorder buffer structure. The router is assumed to have multi-path minimal routing with 2-D mesh topology, shown in Fig. 2. In this NoC, a ﬂow may have different paths between the source and destination, which is called trafﬁc splitting. Minimal routing guarantees the shortest routing path. Take ﬂow f (1, 16) as an example, it can be split at every possible router between source node 1 and destination node 16. Let the trafﬁc split proportion in the X axis be Px . Then the Y axis Py is 1 − Px . For full trafﬁc splitting, the tag ﬂow is split into 20 sub-ﬂows altogether. The sub-ﬂows can be classiﬁed into intersecting and nonintersecting. For non-intersecting sub-ﬂows, they do not have crossing nodes except the source and destination. The simplicity nature ensures that a link failure on one path does not affect the packets that are transmitted to the others. Many designs have used non-intersecting paths to achieve fault tolerant routing. For example, the IBM Vulcan[14]. Furthermore, nonintersecting means that no out-of-order issue is needed to be concerned in the intermediate router nodes, which will lower the area cost. So We consider this kind of sub-ﬂow set. After trafﬁc splitting, packets traverse through different paths. Out-of-order problem may occur since different paths have different contention or delay. Assuming worm-hole routing is applied, packets in the same sub-ﬂow may arrive at the destination in the same order. So out-of-order problem will only occur at the convergent node. We place a reorder buffer in this router, which is located at the local NI connected with the destination processing elements (PE). The basic architecture of reordering buffer is shown in Fig.2. While a packet is received from the convergent router, it will ﬁrst be determined whether it is the desired one that equals to the ongoing packet number. A counter is implemented to store the current packet. If it matches the number of the current packet, it will be transferred directly to local PE. Otherwise, it will be stored into the reorder buffer, with the packet indexing information updated into a look-up table. They will be sent out from the reorder buffer only when they become the packet in need. It is needed to point out that this is only a basic reorder buffer architecture. Many more complex solutions exist such as the work in [11], [12] and [9]. We choose the basic architecture as a start point to see how effective our analytical approach can be used in worst-case reorder buffer size analyzing. The object of this work is to study the relationship between the worst-case reorder buffer size and ﬂow conﬁgurations through trafﬁc splitting, then to minimize the worst-case reorder buffer size. One of the parameters that affects the reorder buffer size is the number of sub-ﬂows. Du et al. [13] found that full splitting leads to too many trafﬁc contention, and constraints are needed to prevent over congestion. For simplicity, we assume only two sub-ﬂows are allowed for trafﬁc splitting. Another factor is the trafﬁc splitting proportion. Since different proportion also leads to different trafﬁc congestion, the problem becomes to ﬁnd the sub-ﬂow pair that has the least reorder buffer size, given an application-speciﬁc NoC. IV. WOR S T-CA SE S I Z E O F R EORD ER BU FFER We ﬁrst derive the worst-case reorder buffer size model in a general way, and then use network calculus theory to derive analytical model for application speciﬁc NoCs. 2 → 3 → 4 → 8 → 12 → 16) and f2 (1 → 5 → 9 → 13 → A. General Analysis Still take Fig. 2 as an example. The tag ﬂow which starts from node 1 to node 16 is split into two sub-ﬂows f1 (1 → 14 → 15 → 16 ). Let the transient delays of packets for ﬂow f1 and f2 be D1 and D2 , and the packet injection intervals be ∆t1 and ∆t2 , with ﬁxed packet delay and uniform injection rate. Then the reorder buffer size Srb is, (1) |D1 − D2 | ∆t If D1 > D2 , ∆t = ∆t2 . Or if D1 < D2 , ∆t = ∆t1 . Srb = Otherwise, Srb equals zero, which means that packets from both sub-ﬂows have the same path delay, so no packets are needed to be stored into the reorder buffer. The above equation is for a transient case with a certain period. D1 and D2 can be considered as a ﬁxed value under this constraint. In order to get the worst-case buffer size, we need to know the worst-case delay of two sub-ﬂows. Deﬁnition 1: We denote by Smax the worst-case reorder buffer size of two sub-ﬂows through trafﬁc splitting. Let the upper bounds of D1 and D2 be ¯D1 and ¯D2 , and the lower bounds be D1 , D2 . Then Smax can be expressed as, rb . (cid:40) rb (cid:41) Smax rb = max ( ¯D1 − D2 ) ∆t2 , ( ¯D2 − D1 ) ∆t1 . (2) By using network calculus theory, we can obtain the delay bound for each sub-ﬂow, which is described in the next section. TABLE I D E FIN I T ION O F PARAM E TER S . R∗ α∗ P arameter Def inition D1 , D2 ¯¯D1 , ¯¯D2 D1 , D2 ∆t1 , ∆t2 R1 (t), R2 (t) 1 (t), R∗ 2 (t) α1 (t), α2 (t) 1 (t), α∗ 2 (t) β1 (t), β2 (t) r1 , r2 b1 , b2 R T The transient delay of packets for ﬂow f1 and f2 The upper bound of D1 and D2 The lower bound of D1 and D2 The packet injection interval of ﬂow f1 and f2 The input accumulative curve of ﬂow f1 and f2 The output accumulative curve of ﬂow f1 and f2 The input arrival curve of ﬂow f1 and f2 The output arrival curve of ﬂow f1 and f2 The equivalence service curve of ﬂow f1 and f2 The average rate of ﬂow f1 and f2 The burstiness of ﬂow f1 and f2 The minimum rate of route node The maximum processing latency of router node Min-plus convolution Min-plus de-convolution Function to compute the equivalent service curve The minimum operation. f ∧ g = min{f , g} Worst-case reorder buffer size of a pair of sub-ﬂows Worst-case reorder buffer size of a tag ﬂow Smax rb ¯¯S ⊗ (cid:11) ∧ (., .) Fig. 3. Abstraction of multi-path routing. B. Analytical Model on Network Calculus To be more general, for every sub-ﬂow, we abstract each path of routers into stand alone systems S1 , S2 , with input accumulative curve R1 (t), R2 (t), and output R∗ respectively, shown in Fig. 3. Let the system delay of S1 be D1 , and that of S2 be D2 . (For readability, these parameters are deﬁned in Table I). Suppose D1 > D2 , then the worstcase reordering appears when packets on S1 are sent ﬁrst, but arrive last. So the packets in S2 arriving ﬁrst should be stored in the reorder buffer, and the number of packets stored can be calculated by the output accumulative curve R∗ 2 (t) during timing period t, s, ∀t, s belongs to t − s = D1 − D2 : 1 (t), R∗ 2 (t), rb = max{R∗ 2 (t) − R∗ 2 (s)}. Smax (3) Moreover, let the input arrival curve of S1 be α1 (t), and output be α∗ 1 (t). Similarly, that of S2 are α2 (t) and α∗ We assume that the service curve of S1 and S2 are β1 (t) and β2 (t). By the deﬁnition of NC, ∀t ≥ s, R(t)−R(s) ≤ α(t−s), R∗ (t) − R∗ (s) ≤ α∗ (t − s). Equation 3 can be expressed as 2 (∆t), in which ∆t = D1 − D2 . Similarly, if D1 < 1 (∆t), with ∆t = D2 − D1 . To generalize, let ¯D1 , ¯D2 be the delay bounds of S1 , S2 , ¯D1 = h(α1 , β1 ), ¯D2 = h(α2 , β2 ), so the worst-case reorder buffer size is: rb = α∗ rb = α∗ Smax D2 , Smax 2 (t). Smax rb = max{α∗ 1 (∆t1 ), α∗ 2 (∆t2 )}, where ∆t1 = h(α2 , β2 ) − D1 , ∆t2 = h(α1 , β1 ) − D2 . Since the calculation of h(α1 , β1 ) and h(α2 , β2 ) needs to calculate the equivalence service curve for multi-path routing with trafﬁc splitting, we can use the results obtained in [13]. (4) Deﬁnition 2: We denote by ¯S the worst-case reorder buffer size of a given tag ﬂow that has many sub-ﬂow pairs. The analysis framework to calculate ¯S is as follows. Step 1: Classiﬁcation of Sub-ﬂows. Let the number of sub-ﬂows splitted by tag ﬂow is K. Since only two sub-ﬂows are allowed in the current model, there are C 2 K kinds of combinations altogether. As described in section 3, only non-intersecting sub-ﬂows are considered. The identiﬁcation of non-intersecting sub-ﬂows in 2D mesh NoC is implemented by the following equation: Pn = 2(l − 1), (5) where Pn is the total number of routers of sub-ﬂow pair, and l is that of one sub-ﬂow traversing through source to destination. Secondly, to calculate the trafﬁc split proportion of the selected non-intersecting sub-ﬂow pair. After that, the arrival curves of the tag sub-ﬂows are calculated. Assume that linear arrival curve α(t) is used to model both tag ﬂow and contention ﬂows, which has the form of α(t) = rt + b, where b bounds the burst and r is the average (sustainable) rate. Step 2: Equivalent Service Curve (ESC) Calculation. After calculating the average rate and burstiness of all subﬂows for both tag and contention ﬂows, we can deduce the ESC for each sub-ﬂow. We consider the latency-rate server function for NoC routers, i.e. βR,T = R(t − T )+ , where R is the minimum service rate, and T is the maximum processing latency of the network element. Here x+ equals 0 if x ≤ 0, else x. For ESC derivation, the equivalent R and T are obtained. Then we get the delay bound using [15]: ¯D = b R + T (6) α1 (t) = r1 t + b1 and α2 (t) = r2 t + b2 , and the two Step 3: Calculation of Worst-case Reorder Buffer Size. Assuming that the two sub-ﬂows have arrival curves of routers provide service curves of β1 = R1 (t − T1 )+ and β2 = R2 (t − T2 )+ , respectively. The output arrival curves of both ﬂows are α∗ α∗ According to Equation 6, the upper delay bounds are ¯D1 = 1 (t) = α1 (cid:11) β1 (t) = r1 t + b1 + r1T1 and 2 (t) = α2 (cid:11) β2 (t) = r2 t + b2 + r2T2 . h(α1 , β1 ) = T1 + b1 /R1 and ¯D2 = h(α2 , β2 ) = T2 + b2/R2 , time intervals are ∆t1 = h(α2 , β2 ) − D1 = T2 + b2 ∆t2 = h(α1 , β1 ) − D2 = T1 + b1 and the lower delay bounds are D1 = T1 and D2 = T2 . The − T1 , − T2 . Regarding Equation 4, we calculate the worst-case reorder buffer size Smax for a pair of non-intersecting sub-ﬂows as, R2 R1 rb Smax rb = max{R∗ 2 (t) − R∗ 2 (s)} = max{α∗ 1 (∆t1 ), α∗ 2 (∆t2 )} = max{r1 (∆t1 ) + b1 + r1T1 , r2 (∆t2 ) + b2 + r2T2} = max{r1 (T2 + b2 − T1 ) + b1 + r1T1 , R2 b1 − T2 ) + b2 + r2T2 }, r2 (T1 + R1 = max{r1 (T2 + b2 R2 ) + b1 , r2 (T1 + ) + b2 }. b1 R1 (7) Finally, the worst-case reorder buffer size of tag ﬂow ¯S can be obtained by calculating every non-intersecting ﬂow pair. C. Analysis Algorithm The whole procedure is summarized in Algorithm 1. Three main steps have been implemented. Firstly, the number of such ﬂow pairs M is obtained by judging whetherPn equals 2(l−1), which means no shared routers exist except for the source and destination. Then, the ESC of each sub-ﬂow from the nonintersecting ﬂow pairs is calculated, with R1 , R2 obtained as the minimum equivalent service rate and T1 , T2 the equivalent delay. Finally, the worst-case reorder buffer size of per pair is calculated. Executing step 2 and 3 for M times, we can obtain ¯S of the given tag ﬂow. ﬂow Smax rb Algorithm 1 Algorithm of the worst-case reorder buffer size. Input: An N × N 2D mesh NoC, K sub-ﬂows through full trafﬁc splitting, l routers for one tag sub-ﬂow and M pair of non-intersecting sub-ﬂows. Output: The worst-case reorder buffer size ¯S for tag ﬂow. 1: M ← 0; ¯S ← 0 2: for all (j = 1; j <= C 2 K ; j + +) do if Pn == 2(l − 1) then 3: M ← M + 1 4: 5: end if 6: end for 7: for all (i = 1; i <= M ; i + +) do ESC (f1 ) ← R1 (t − T1 )+ // ESC of sub-ﬂow f1 8: ESC (f2 ) ← R2 (t − T2 )+ // ESC of sub-ﬂow f2 9: S1 ← r1 × (T2 + b2 /R2 ) + b1 10: S2 ← r2 × (T1 + b1 /R1 ) + b2 11: i ← max(S1 , S2 ) 12: ¯S ← max(Smax , ¯S ) 13: Smax i 14: end for 2 f[a,b] The time complexity of the proposed algorithm consists of two parts. One is the calculation for the number of nonintersecting sub-ﬂows. Since C 2 K times are consumed, the time complexity is O( K×(K−1) ). The other is for step 2 and step 3. In the calculation of ESC, a uniform expression of is used to calculating the contention scenario, where a is the source node and b the destination. Supposing the number of nodes that sub-ﬂow ftag goes through is l. f[a,b] is obtained by traversing ﬂow ftag , with a maximum executing time of l×(l+1) . Let the maximum number of nodes for all the trafﬁc ﬂows be n, then the time complexity of computing ESC for each sub-ﬂow is O(( l×(l+1) )n ). The above procedure has been executed for 2M times, so the complexity for ¯S )n ). Finally, the algorithm complexity is ). is O(2M ( l×(l+1) O(2M ( l×(l+1) )n ) + O( K×(K−1) 2 2 2 2 2 D. An Example The above algorithm shows that the maximum reorder buffer of all non-intersecting sub-ﬂows is chosen as the worstcase size. This is deﬁnitely necessary to ensure no packet lose. However, in application speciﬁc NoCs, ﬂow contention R calculated as αf2 (1,12) = r2 t + b2 + 6r2T . So 16 ) = (R − r2 )[t − (T + b2+r2 6T (β16 , αf2 )]+ . 1 (cid:11) β f2 16 = αf2 Then we calculate the service that node 1, 2, 3, 4, 8 and 12 provide for f1 through the following: (a) Calculate the the left-over service that node 3 and 4 provide for f1 and f3 , R )]+ . (b) Calculate the equivalent service curve of node 2, 3, 4, and 8 provided R )]+ . (c) Remove the service of ﬂow f3 from router node 2 to node 8, and get the ESC for f1 from node 2 to node 8, β f1 (β3 ⊗ β4 , αf4 3 ) = (R − r4 )[t − (2T + b4 for both f1 and f3 , β2 ⊗ (β3 ⊗ β4 , αf4 3 ) ⊗ β8 = R ∧ (R − r4 )[t − (4T + b4 R )]+ = (R − r4 )[t − (4T + b4 (2,8) = (β2 ⊗ (β3 ⊗ 3 )⊗ β8 , αf3 2 ) = (R− r4 − r3 )[t− (4T + b4 β4 , αf4 β1 ⊗ β f1 (2,8) ⊗ β12 = (R − r3 − r4 )[t − (6T + b4 )]+ . (1,16) )+ , where (1,16) = (R − r3 − r4 ) ∧ (R − r2 ), b4 b3 b2 + 6r2T R + R − r4 + R )]+ . (d) Calculate the ESC of node 1 to node 12 for ﬂow f1 , β f1 (1,12) = R + b3 R−r4 (1,16) (t−T f1 ,eq eq = Rf1 ,eq Finally, the ESC for f1 is β f1 T f1 ,eq (1,16) = 7T + R + b3 R−r4 Rf1 ,eq (9) . Similarly, the equivalent service of f2 is calculated in the form of β f2 (1,16) )+ , where (1,16) (t − T f2 ,eq eq = Rf2 ,eq Rf2 ,eq (1,16) = R − r1 , T f2 ,eq (1,16) = 7T + b1 + r1 (6T + b4 R R + b3 R−r4 ) . (10) Step 3: Worst-case reorder buffer size. Finally, based on Equation 7, the worst-case reorder buffer size can be calculated as follows, (cid:26) (cid:32) (cid:33) (cid:27) b2 (cid:33) Rf 2,eq (1,16) Smax rb = max (cid:32) r1 T f 2,eq (1,16) + + b1 , b1 r2 T f 1,eq (1,16) + Rf 1,eq (1,16) (1,16) , Rf 1,eq (1,16) , T f 2,eq (1,16) , Rf 2,eq + b2 . (11) (1,16) deﬁned in with parameters T f 1,eq Equation 9 and 10. V. EX PER IM EN T S AND R E SU LT S Both synthetic pattern and industry case are used for the proposed analytical model to explore the following aspects: (a) to reveal the relationship among the path delay difference (or delta delay) with any pair of non-intersecting sub-ﬂows; (b) how the worst-case reorder buffer size is varying with trafﬁc splitting proportion of both tag ﬂow and contention ﬂows, and how to set the proper splitting proportion to minimize the reorder buffer size. A. Analysis with Synthetic Pattern 1) Setup: As shown in Fig. 4.(a), four ﬂows are injected into a 4 × 4 NoC. For tag ﬂow f (1, 16), it has 20 sub-ﬂows after trafﬁc splitting, which has 190 (C 2 20 ) pairs, but only 40 of them are non-intersecting. An reorder buffer is placed at the convergent router node 16, and its buffer size will be explored Fig. 4. An example for reorder buffer size analysis. is known at design time. So the congestion state can be considered as prior knowledge, which means we can avoid the most congestion links by appropriate conﬁguration, or limit the ﬂow amount for those most congested sub-ﬂows. Through this way, smaller size of reorder buffer can be achieved by selecting proper sub-ﬂow pairs, or by conﬁguring the trafﬁc splitting proportion. In this section, we use a detailed example to show the analysis procedure with unbalanced trafﬁc. As shown in 4.(a), four ﬂows are injected into a 4 × 4 NoC. Take ﬂow f (1, 16) as the tag ﬂow, and the others as contention ﬂows. Uniform patterns are applied to both tag ﬂow and contention ﬂows, with arrival curves of α(t) = 0.08t + 1.92, α(t) = 0.08t + 58.96, respectively. The NoC routers use a ﬁrst-come-ﬁrst-served (FCFS) policy and have a models of β (t) = 0.33 · (t − 3)+ . 1 = r1 t+b1 , αf2 1 = r2 t+b2 , αf3 2 = r3 t+b3 In Fig. 4.(b), all the ﬂows have been split into sub-ﬂows, and the non-intersecting sub-ﬂow pair (f1 , f2 ) is chosen as an example. There are also two contention ﬂows f3 , f4 . Let the arrival curves be αf1 and αf4 3 = r4 t + b4 , respectively. It is need to point out that ﬂow f (6, 11) is not considered as a contention ﬂow, since it does not have any interference with f1 or f2 . The worst-case reorder buffer size can be deduced step-by-step as follows. Step 1: Trafﬁc splitting conﬁguration. Let the splitting proportion of f1 be ρ. So the arrival curves of f1 and f2 are αf1 Step 2: ESC Calculation. In this step, the ESCs of f1 and f2 are calculated. Their path contention is represented in Fig. 4.(c) and (d). For f1 , it traverses through nodes 1, 2, 3, 4, 8, 12 and 16, and has contention with f2 , f3 and f4 at node 16, 2 and 3, respectively. According to the convolution theory, we can get the ESC (β f1 eq ) of f1 as follows, 1 = (1 − ρ)rt + (1 − ρ)b. 1 = ρrt + ρb, αf2 β f1 eq = β1⊗(β2⊗(β3⊗β4 , αf4 3 )⊗β8 , αf3 2 )⊗β12⊗(β16 , αf2 16 ), (8) where ⊗ and (., .) are min-plus convolution and left-over service operation [15], with basic results of β1 ⊗ β2 = (R1 ∧ R2 )[t − (T1 + T2 )]+ and (β , α) = (R − r)[t − (T + b R )]+ . We ﬁrst calculate the left-over service (β16 , αf2 16 ) that node 16 provides forf1 , by removing the service that node 16 provides for f2 . The arrival curve of f2 at node 16 can be delta delay, it may lead to the two curves inconsistent. 3) Resize reorder buffer size by trafﬁc splitting: As described above, not only ﬂow pairs and delay difference impact the worst-case reorder buffer size, but the ﬂow parameters such as burstiness and ﬂow rate also have inﬂuence on it. So we designed experiments with trafﬁc splitting to reveal the intrinsic relationship, because the splitting proportion has strong connection with all the three factors above. By varying the trafﬁc splitting proportion for both tag ﬂow and contention ﬂows, we can directly change the ﬂow parameters and the related contention status. The results are shown in Fig. 6 and Fig. 7, representing the variations of the tag ﬂow and contention ﬂows, respectively. In Fig. 6, the X axis is the variation of the trafﬁc splitting proportion of all the 40 non-intersecting sub-ﬂow pairs, with that of contention ﬂows using an even splitting strategy. The Y axis is the corresponding worst-case reorder buffer size, and all of them exhibit the same behavior: the reorder buffer sizes decrease when the proportion increases from 0.1 to 0.5, and increase when it grows from 0.5 to 1. The minimum value appears when the splitting proportion is around 0.5. This means that the more balanced the two sub-ﬂows are, the smaller the reorder buffer size is. The sub-ﬂow pair 4 and 11 has the maximum reorder buffer size. and there are four sub-ﬂow pairs have a minimum buffer size, i.e. 8 and 19, 8 and 20, 1 and 19, 1 and 20, which have an average improvement of 57.04% to the maximum buffer size. So the second conclusion is that the more balanced trafﬁc load, the less worst-case reorder buffer size. This can also be explained through the analytical model. When the proportion is less than 0.5, the delay of f1 is larger than that of f2 , so Equation 11 can be simpliﬁed is a monotone decreasing curve when the proportion increases from 0.1 to 0.9). When the proportion is greater than 0.5, the delay of f1 is less than that of f2 , the Equation 11 can be simpliﬁed as )+ b1 (It is a monotone increasing curve when the proportion grows from 0.1 to 0.9). So when the proportion increases from 0.1 to 0.9, the reorder buffer size has a curve as shown in Fig. 6. We then alter the trafﬁc splitting proportion variation to the contention ﬂows, and set the tag ﬂow with a ﬁxed trafﬁc splitting proportion 0.5. The corresponding worst-case reorder buffer size is shown in Fig. 7. Unlike previous results that all 40 non-intersecting ﬂow pairs exhibit the same behavior, they vary a lot. Some of them increase as the contention trafﬁc splitting proportion changing from 0 to 1, while others decrease. When the splitting proportion is between 0 to 0.5, two sub-ﬂow pairs (1 and 19, 1 and 20) have the same minimum reorder buffer size. However, from 0.6 to 1, other two pairs (8 and 19, 8 and 20) have the minimum size. All the four pairs have an average improvement of 79.08% to the sub-ﬂow pair with the maximum reorder size, as well as a maximum improvement of 95.64% and a minimum of 56.99%. Smax rb = r1 (T f2 ,eq (1,16) + b2 = r2 (T f1 ,eq (1,16) + b1 as Smax rb ) + b2 (It Rf1 ,eq (1,16) Rf2 ,eq (1,16) Fig. 5. Delta delay VS. buffer size. Fig. 6. Variation to splitting proportion of tag ﬂow. Fig. 7. Variation to splitting proportion of contention ﬂows. for different conﬁgurations of sub-ﬂow pairs. In other words, if other ﬂow such as f (2, 12) is set as tag ﬂow, a reorder buffer will also be placed at node 12. 2) Delta Delay and the Worst-case Reorder Buffer Size: The ﬁrst question coupled with the proposed analytical model is how to use this model to minimize the size of reorder buffer. So we need to ﬁnd out what factors mostly affect the reorder buffer size. The initial intuition is that the bigger the delay difference is, the larger the reorder buffer size should be. We designed experiments to verify this trend. The results are shown in Fig. 5, where the X axis is the number of 40 subﬂow pairs (from 0 to 39). The worst-case reorder buffer size of it is denoted with green circles (in order to observe more clearly, the buffer size value has been expanded 10 times). The delta delay, which is deﬁned as the maximum delay difference between the upper delay bound on one ﬂow and lower delay bounds of the other ( ¯D − D), is also plotted in blue triangles. From the ﬁgure we can see that the two curves are almost in the same trend, which prove that delay variation contributes a lot to the worst-case reorder buffer size. So we can conclude that in order to design the reorder buffer size as small as possible, we can limit to those sub-ﬂows pairs with less delay, with customized trafﬁc splitting. In this case, ﬂow pairs 4, 5, 33 and 34 are the most desired candidates, sine they all have the least worst-case buffer size and have maximum reduction of 56.99%, compared to ﬂow pair 17 which has the largest reorder buffer size. However, there is an exception that number 11 to number 12 do not comply with the same trend, with the delta delay of ﬂow 11 being greater than ﬂow 12, while the reorder buffer size of ﬂow 11 being smaller than that of ﬂow 12. By looking into Equation 7 and rewriting the ﬁrst item into r1 ( ¯D2 −D1 )+ b1 + r1T1 , we can see that besides the delta delay, there are elements such as ﬂow burstiness and ﬂow rate also impacting the reorder buffer size. When they contribute more than the Fig. 8. Simulation segment with conﬁguration of Px = 0.1. Fig. 9. Simulation segment with conﬁguration of Px = 0.8. B. Simulation with Synthetic Pattern 1) Setup: We use SoCLib [16] to realize a cycle-accurate simulation platform for the 2-D mesh NoC in Fig. 4.(b). The network performs wormhole switching. Each router guarantees the service curve of β (t) = 0.33 · (t − 3)+ and uses the packetby-packet round-robin scheme for link scheduling. We assume no delay on the link wires. Experiments are carried out using synthetic trafﬁc used in the last section, with the exception that ﬂow (6,11) has been removed. So in this conﬁguration, we can explore the partial contention scenario. That is to say, for a sub-ﬂow pair, one may occur contention while the other not. 2) Simulation Results: Two of the simulation segments are shown in Fig. 8 and 9. One is for the largest reorder buffer size, where the trafﬁc splitting proportion of tag ﬂow in X axis Px is 0.1. The other is the least one with Px equals 0.8. In this simulation process, every value is obtained through a simulation process of almost 108 cycles to guarantee an appropriate exploration of the network with all the possible splitting ratios. The simulation segment is shown in Fig. 8, in which only two thousand points of simulation are presented. The upper line is the analysis results for worst-case reorder buffer size, and the dotted curve is the number of packets observed in the reorder buffer. It shows that there is no packet loss at the packet reorder buffer, and the reorder buffer size calculated through NC does guarantee packet reordering without any packet loss. Both analysis and simulation results show that when trafﬁc splitting proportion Px increase from 0.1 to 0.8, the reorder buffer size is effectively decreased by 75.8% at most. Although all the simulation results are bounded by the analysis results, there is a gap between them. This seems to mean that the analysis results are not so tight. Actually, this is because there are challenges in achieving the worse-cases in simulation. Take Fig. 9 as an example, where the analytical result is that the worst-case reorder buffer size is eight-packet length. In this situation, the trafﬁc splitting proportion of ﬂow f1 in Fig. 4.(b) is 0.8. In other words, for every 10 packets from the tag ﬂow, eight packets are distributed to f1 , and two to f2 . The arrival curves of tag ﬂow and contention ﬂows are α(t) = 0.08t + 1.92 (for f1 , f2 ) and α(t) = 0.08t + 58.96 (for f3 , f4 ). In simulation, we set up this conﬁguration by injecting two successive packets every 25 cycles for the tag ﬂow, and Fig. 10. Two kinds mappings of Ericsson Radio System with conﬁgurations on: (a) shorter long-path, and (b) less number of reorder buffers. 64 consecutive packets every 800 cycles for the contention ﬂows. From Fig. 4.(b), we can see that the worst-case occurs in the following situation: eight out-of-order packets transverse through f2 and are stored in the reorder buffer, while thirtytwo packets containing the packet in-need are still remained in f1 , i.e., at least 40 packets are needed for tag ﬂow, with at least 500 simulation cycles in total. So the number of test cases for generating tag ﬂow is 2420 . As to the contention ﬂows, (737)2 test cases for two ﬂows are needed. Thus, the total number of test cases for generating the worst-case reorder buffer is 2.18 × 1033 , which is impossible to carry out within limit time and resource. In fact, with analytical model using trafﬁc splitting strategy, we can feed hundreds of possible different sub-ﬂows, but simulating all the situations would be impossible due to timing constraints and is hard to detect the worst case. As stressed in [17], this actually underlines the importance for our research into analytical methods for evaluating NoC performance and quality of service (QoS). C. Industry Case 1) Setup: We use the Ericsson Radio System (ERS) application [18] as an example to verify the effectiveness of the proposed model, as shown in Fig. 10. According to practical design constraints and performance/energy trade-off, two kinds of mappings are explored. One is the shorter longpath orienting, the other is the less number of convergent node orienting. Fig.10.(a) is of the former, where node 5, 6, 7, 10, 14 and 15 are ASICs, node 1, 2, 3, 8, 11 and 16 are DSPs, node 4, 9 and 12 are FPGA, and node 13 is a device processor which initiates the data transmission and controls the normal operations. There are 26 dedicated ﬂows altogether, and they can be classiﬁed into nine categories in terms of bandwidth requirements represented as {a, b, c, d, e, f , g , h, i}. We characterize ﬂows according to their bandwidth requirement and set their burstiness to be 128 bits. In this mapping, the longest path has three hops. Five reorder buffers are placed at nodes 4, 6, 7, 14 and 15(No reorder buffers are needed for other nodes since no trafﬁc splitting existed in other ﬂows). The second mapping is of the latter and shown in Fig. 10.(b), where only three reorder buffers are needed, i.e. node 9 → 13, node 10 → 9 and node 13 → 10. 2) Results: The results are shown from Fig.11 to Fig.13. Five nodes in mapping 1 and three nodes in mapping 2 are presented. Both of these two mapping have the same trend Fig. 11. Worst-case reorder buffer Size of node 4 (left) and 5 (right) in two kinds of mappings. Fig. 12. Worst-case reorder buffer size of node 7 in two mappings. Fig. 13. Total size of worst-case reorder buffer in two kinds of mapping. that they have the minimum worst-case reorder buffer size (S.worst) when the splitting proportion is at about 0.5, which coincides with the conclusion above. In both mappings, node 4 has the minimum worst-case reorder buffer size. But the value in mapping 2 is slightly larger than that in mapping 1. Because in mapping 2, the sub-ﬂows have to travel more nodes than in mapping 1, resulting in longer worst-case delay. Furthermore, this ﬂow has encountered longer contention links with possible more contention, implying possibly larger reorder buffer. It is the same reason that node 6 and node 7 have greater worstcase reorder buffer size in mapping 2 than in mapping 1. However, the total size of the reorder buffer in mapping 2 is smaller than mapping 1 (shown in Fig. 13), with reduction of maximum 36.50% (76 packets reduced), average 29.20% (61 packets reduced) and minimum 22.12% (46 packets reduced). Some nodes may have greater sizes for reorder buffers in mapping 2, but the total size has been signiﬁcantly reduced. V I . CONC LU S ION The analytical model of worst-case reorder buffer size in multi-path minimum routing NoC has been presented. Both synthetic patterns and industry case have been utilized for experiments. The results show that not only the worst-case delay has direct inﬂuence on the worst-case buffer size, but also ﬂow parameters such as burstiness and ﬂow rate affect this value. Further experiments of using trafﬁc splitting for both tag ﬂow and contention ﬂows show that for tag ﬂow, the more balanced trafﬁc, the less size of reorder buffer. Moreover, different sub-ﬂow pairs have great difference in the reorder buffer size. Through controlling different parameters, the maximum improvement is up to 95.64%, as well as an average of 79.08% when varying contention trafﬁc splitting proportions. An industry case is studied with two kinds of mappings. The results show that worst-case reorder buffer analysis is useful in architecture exploration such as application mapping, with a maximum improvement of 36.50%, which may be considered as one cost in routing/mapping problems. Simulation is also performed to verify the correctness of the analytical approach under different splitting proportion of tag ﬂow. ACKNOW L EDGM ENT The authors would like to thank the projects sponsored by National Nature Science Foundation of China (Grant No. 61106020, 61204024). "
ICARO - Congestion isolation in networks-on-chip.,"The growing demand of computing power and the emerging trend towards heterogeneity lead to integrate more and more cores and specialized modules into a single chip. As the number of cores per chip increases, the network interconnecting them must satisfy the growing communication needs. However, several factors as aggressive traffic patterns, power-saving and fault-tolerance mechanisms may lead to oversubscribed resources in the network, thereby generating congestion and so degrading the overall network performance. In this paper we propose ICARO, a mechanism to dynamically isolate the traffic flows contributing to congestion making use of dedicated virtual networks. In this way, ICARO prevents the head-of-line blocking effect derived from congestion, thereby improving overall network performance. We analyze thoroughly our proposal, especially from the robustness point of view, showing that it effectively manages to identify and isolate congested flows, improving network performance up to 82% with respect to previous proposals.","ICARO: Congestion Isolation in Networks-On-Chip Jos ´e V. Escamilla and Jos ´e Flich Grupo de Arquitecturas Paralelas, DISCA Universitat Polit `ecnica de Val `encia joseslo@gap.upv.es, jﬂich@disca.upv.es Pedro Javier Garc´ıa Escuela Superior de Ingenier´ıa Inform ´atica, Universidad de Castilla-La Mancha pedrojavier.garcia@uclm.es Abstract—The growing demand of computing power and the emerging trend towards heterogeneity lead to integrate more and more cores and specialized modules into a single chip. As the number of cores per chip increases, the network interconnecting them must satisfy the growing communication needs. However, several factors as aggressive trafﬁc patterns, power-saving and fault-tolerance mechanisms may lead to oversubscribed resources in the network, thereby generating congestion and so degrading the overall network performance. In this paper we propose ICARO, a mechanism to dynamically isolate the trafﬁc ﬂows contributing to congestion making use of dedicated virtual networks. In this way, ICARO prevents the head-of-line blocking effect derived from congestion, thereby improving overall network performance. We analyze thoroughly our proposal, especially from the robustness point of view, showing that it effectively manages to identify and isolate congested ﬂows, improving network performance up to 82% with respect to previous proposals. INTRODUCT ION AND MOT IVAT ION I . Nowadays, High-Performance Computing (HPC) and multimedia-oriented applications and services demand increasing computing power to the systems supporting them. Moreover, achieving this performance with minimum power consumption has become almost mandatory due to cost and power constraints. In order to satisfy both requirements, manufacturers beneﬁt from the advances in integration-scale technology, and include as many computing modules as possible into the same die. This leads to the design of chip multiprocessors (CMPs) or multiprocessor systems-on-chip (MPSoCs). To date, it is common to ﬁnd devices with tens or hundreds of modules [5][6][1]. Although this approach offers ﬂexibility, it does not offer high performance for speciﬁc usual computing tasks. As a consequence, heterogeneous designs are being revisited as they may include both multi-purpose and speciﬁc-purpose modules such as cache pre-fetchers [11], accelerators [22], etc. Regardless the speciﬁc design, these platforms require an interconnection network to support communication between all the processing nodes. In general, this network must provide high-bandwidth and low-latency to avoid slowing down the processing nodes while waiting for remote data. In that sense, Networks-on-chip (NoCs)[7] are well suited to systems with a high number of processing nodes. NoC design presents interesting challenges due to the tight constraints found in this environment. Among these challenges, an still open issue is how to efﬁciently deal with congestion situations, i.e. scenarios where any number of the network internal paths are clogged, mainly due to oversubscribed ports (hotspots). Basically, an oversubscribed port is a port requested concurrently by several trafﬁc ﬂows, so only one is granted access at a given moment while the rest will be temporarily blocked. This competition 0 This work was supported by the Spanish Ministerio de Econom´ıa y Competitividad (MINECO) and by FEDER funds under Grant TIN201238341-C04-01 and by Ayudas para Primeros Proyectos de Investigaci ´on from Universitat Polit `ecnica de Val `encia under grant ref. 2370. to access the ports is usually known as contention, congestion being actually the result of persistent contention, whose effects (i.e. blocked ﬂows) are propagated throughout the network due to the backpressure of ﬂow control. Indeed, congestion may lead to a severe degradation of network performance if no countermeasures are taken. Such congestion situations are likely to appear in NoCs due to several causes. For instance, the nodes in heterogeneous systems may generate bursty trafﬁc, thereby increasing the probability of hotspot appearance. In addition, in such systems, due to the different nature of each node, the trafﬁc generated is inherently unbalanced, so being prone to create hotspots that degrade network performance. Nevertheless, bursty trafﬁc is not the only case of trafﬁc patterns creating hotspots; indeed, any application involving intense communication towards more-preferred destinations may create them. Besides, some normal operations prone to be concurrently demanded, such as accessing the main memory controllers, may ”naturally” create hotspots. In addition, fault-tolerance mechanisms may re-route trafﬁc ﬂows to recover from network failures [21], thereby probably causing an unbalanced trafﬁc distribution, and so leading to oversubscribed links and ports. Moreover, as mentioned above, power consumption has become critical in HPC systems due to their large number of nodes and their high clock frequency, which lead to high costs, even to dissipate the heat generated by the machines. In the case of portable computing devices (like multimedia devices, smartphones, etc), power consumption is the main pending issue due to the insufﬁcient capacity of their batteries. Indeed, in such devices, one of the main power-hungry parts is the MPSoC [3], becoming mandatory to improve its power consumption. To achieve this, CMP and MPSoC manufacturers have developed and implemented mechanisms to reduce the energy necessary to keep working these devices while reducing their performance as less as possible. One of the most extended mechanisms to carry this out is Dynamic Voltage and Frequency Scaling (DVFS) [15][13]. However, switching from low to high voltage/frequency values may not be fast enough to satisfy a sudden increase in network performance demand, thereby causing transitory network congestion as the network offered bandwidth is temporarily insufﬁcient. Overall, any of the aforementioned causes of congestion may end up degrading network performance, especially due to the Head-of-Line (HoL) blocking effect associated to congestion situations. Speciﬁcally, HoL-blocking arises when ﬂits belonging to messages requesting oversubscribed ports precede in FIFO queues to ﬂits belonging to messages requesting other ports, which may be free. While the head-of-line ﬂits do not win their requested ports, such ﬂits force the ones stored behind to stay blocked in the queue, even if the latter request available ports. It is worth describing this effect because the mechanism we propose in this paper is based on the premise that congestion is not a problem by itself, the actual problem being the HoL-blocking caused by congestion. Indeed, if HoL-blocking is completely prevented, network performance is not affected even though the trafﬁc ﬂows contributing to congestion (hereafter referred to as congested ﬂows) are not removed, as some techniques proposed for off-chip networks have demonstrated (see Section II). By contrast, this principle has not been yet efﬁciently applied in the NoCs context. To ﬁll this gap, in this paper we propose ICARO (Internal-Congestion-Aware Hol-blocking RemOval), a novel mechanism to prevent HoL-blocking in NoCs, which uses special Virtual Networks (VNs) to dynamically isolate congested ﬂows, separating them from non-congested ones. In this way, the HoL-blocking derived from congestion is prevented by using a reduced set of network resources (VNs), thereby cost-efﬁciently improving network performance. The rest of the paper is organized as follows. In Section II, we offer an overview of existing approaches to deal with HoLblocking. Next, ICARO is described, dividing the mechanism into three clearly differentiated stages. In Section IV a deep behavioral analysis of ICARO is performed by testing its robustness and scalability, and by comparing its performance improvement against other similar proposals. Next, we analyze the area and power overhead of ICARO through HDL implementation results. Finally, in Section VI, some conclusions are drawn and the future work on ICARO is indicated. I I . R ELATED WORK Due to the negative impact of congestion, and also to the growing popularity of NoC-based systems, the number of proposals for congestion management in NoCs has quickly increased during the last years. Although some congestionmanagement mechanisms have been proposed for bufferless NoCs, such as the one presented in [18], hereafter we focus on the solutions oriented to buffered NoCs, as ICARO has been designed for this type of NoC architecture. Many of the solutions for buffered NoCs are based on collecting congestion information from neighboring nodes through the routing process and monitoring buffer occupancy, in order to offer an alternative path to route around congested areas (i.e. hotspots). Among them, a mechanism called RCA is proposed in [10] for congestion avoidance in NoCs with adaptive routing. RCA uses a composition of multiple global metrics collected from the whole network to select at each router the output port which messages are forwarded through, so that hotspots are avoided. Speciﬁcally, these metrics are: the count of free Virtual Channels (VCs), the count of free buffers and the crossbar demand. In order to collect the metrics from the whole network, such metrics are aggregated (piggybacked) from a router to the next one and so on. In a heavy-congestion situation this mechanism may collapse since the information used to avoid the congested areas is aggregated in the same messages that are congested, so a vicious cycle may be created. However, adapting the routes to avoid hotspots may result in moving the location of such hotspots from one place to another, so the problem would remain unsolved. Moreover, avoiding hotspots may be impossible if all the congested ﬂows have the same target (e.g. the memory controller). Another solution based on adaptative routing policy is PARS, proposed in [4], which uses a dedicated subnetwork for sending congestion metrics based on the buffer state at certain routers. Like RCA, PARS uses such metrics to select proper paths in order to avoid hotspots. Although in this case the information is sent through the dedicated subnetwork, the problems regarding unavoidable hotspots or “hotspot reallocation” may still appear. Similarly, in [14] authors propose a token-based ﬂow-control mechanism which uses dedicated wires to send routers status information (token) which is used to take routing decisions and bypass routers pipeline. However, this proposal is focused on reducing network latency by skipping routers pipelining, but not by facing congestion harmful effects. In [20] authors propose to collect congestion information from the whole network and to take routing decisions based on network status. However, in this proposal the congestion information is collected piggybacking the links status into the packets header. Following a different approach, a predictive-based ﬂow control mechanism is proposed in [19]. Authors propose an end-to-end ﬂow control mechanism based on predictionmodels to control the injection rate at the source node. Predictions are computed in every switch using its state and its neighbors state. In order to exchange the necessary data for computing the prediction, routers implement additional wires interconnecting them. This solution actually corresponds to one of the “classical” approaches to congestion management, usually known as injection throttling, which, like any control strategy based on closed-loop theory, may present performance oscillations and become inefﬁcient if the source nodes react too late. Similarly, in [12] authors propose HPRA, a hotspotformation prediction mechanism, that makes use of an Artiﬁcial Neural Network-based (ANN) hardware that gathers buffer utilization data to predict the formation of hotspots. Then, HPRA classiﬁes the trafﬁc into two classes: hotspot-destined trafﬁc (HSD) and non-hotspot-destined trafﬁc (nonHSD). HSD trafﬁc is throttled at source while the nonHSD trafﬁc is routed avoiding paths containing hotspots routers. This proposal may suffer from the same problems as injection throttling. An alternative approach is to speciﬁcally deal with the HoL-blocking derived from congestion, mainly by mapping different trafﬁc ﬂows to different queues in the buffers, so that the interaction between ﬂows is minimized. A solution that follows this approach has been proposed in [23]. Actually, authors propose two policies to map trafﬁc ﬂows to VCs: FVADA and AVADA. Both proposals establish a correspondence between the output port requested on the router x+1 and the output VC assigned in the router x (note this requires lookahead routing). The main difference between both policies is that FVADA establishes a direct and constant correspondence between the requested output port and the assigned VC, while AVADA starts establishing a direct correspondence but later this correspondence can be dynamically adapted, based on the output port load, making use of a correspondence table (a CAM-based table). Note that, while FVADA is simpler to implement, it requires exactly as many VCs as the router radix (cid:0) 1 value, thereby the number of required VCs depending on the router radix. Moreover, both policies require routers implementing lookahead routing and a credit-based ﬂow-control in order to quantify the output port load an adapt their behavior when the load in a given VC is too high. Note that neither FVADA nor AVADA are actually aware of which trafﬁc ﬂows are contributing to a hotspot, as they only consider one hop (i.e. the next requested output port) in the path of the messages, while hotspots may be located further away. Thus, congested ﬂows may still share queues with non-congested ones, thereby still causing HoL-blocking in some degree. A different approach to deal with HoL-blocking is proposed in [17], based on an Uniﬁed Buffer Structure in which the number of buffers per port and their depth are allocated dynamically depending on the trafﬁc load, dispensing fewer but deeper VCs under low trafﬁc loads and more but shallower VCs under heavy trafﬁc. Thus, we believe that an efﬁcient HoL-blocking-avoidance mechanism must explicitly identify congested ﬂows in order to isolate them completely and dynamically. This is the approach followed by the Regional Explicit Congestion Management (RECN) mechanism, proposed for off-chip networks [9]. Among the plethora of proposals for congestion management in off-chip networks, RECN can be considered as one of the most efﬁcient as it completely prevents HoL-blocking while requiring a reduced set of queues. However, adapting the RECN basics to NoCs requires a very different way of implementing it, due to the tight limitations in area and power in this context. In that sense, in [8] a solution is presented to isolate bursty trafﬁc, but not congested ﬂows. In this paper we ﬁnally propose a solution, ICARO, that adapts the RECN strategy to the NoCs context, as explained in the next section. ICARO D E SCR I PT ION I I I . A. ICARO Principles As mentioned in the previous sections, the purpose of ICARO is not removing congestion but preventing the HoLblocking caused by congestion. Indeed, ICARO manages to solve this problem by identifying congested ﬂows, then isolating them into special Virtual Networks (VNs) while keeping the non-congested data ﬂows in different regular VNs. By doing this, ICARO separates congested ﬂows from non-congested ones, thus preventing HoL-blocking and so increasing network performance. Note that ICARO needs at least two VNs: one regular-VN (for non-congested trafﬁc) and one slow-VN (for congested trafﬁc). Nevertheless, ICARO may be conﬁgured to work with several regular-VNs and also with several slowVNs. Note that ICARO is a reactive mechanism in the sense that all the system works normally in absence of congestion, keeping the system performance in the same values as the baseline (i.e. the same scenario without ICARO). However, when congestion is detected ICARO reacts to keep network performance by preventing the congestion harmful effects. ICARO functionality can be divided into three stages: ﬁrst, congested points in the network are detected at routers; then routers notify the sources of this detection; ﬁnally, the sources map the trafﬁc ﬂows either to a slow-VN or to a regular-VN depending on whether or not the injected ﬂow will traverse congested points. These three stages are thoroughly described in the next subsections. B. Congestion Detection ICARO is based on detecting congested points, deﬁned as output ports persistently oversubscribed. According to the deﬁnition of contention given before, ICARO considers that exists contention for an output port if two or more ﬂows request that output port from different input ports. In order to detect whether this contention is persistent, an additional metric is used. This metric consists in counting the number of messages requesting the contended output port. This count is computed per VN at input ports, increasing its value when a new message requesting the output port arrives to the input queue, and decrementing it when the whole message leaves the queue. Every time this count is modiﬁed, it is compared with a threshold (SAT THR) whose value is a conﬁgurable parameter of ICARO. Depending on the value of SAT THR, the congestion-trigger sensitivity of ICARO is lower or higher. As each input port may contain several VNs, an input port is considered as exceeding SAT THR for an output port for each output port do for each input port do port saturated = F ALSE ; for each vn do if isVNsaturated(input port, vn)==TRUE then if getNumRequests(vn, output port) < UNSAT THR then port saturated = F ALSE ; markVNasUNSaturated(input port, vn); port saturated = T RU E ; break; else end else else end end if getNumRequests(vn, output port) > SAT THR then port saturated = T RU E ; markVNasSaturated(input port, vn); break; port saturated = F ALSE ; end end if port saturated==TRUE then num ports saturated++; if num ports saturated >= 2 then markAsCongested(output port); markAsNoCongested(output port); end else end end Algorithm 1: Congestion/no-congestion detection algorithm. if anyone of its VNs does it. Therefore, an output port is considered as a congested point when, in two or more input ports, there are VNs exceeding SAT THR for that output port. ICARO must also detect the end of congestion. For this, an hysteresis technique is used. In a few words, once an output port is detected as congested, ICARO detects the end of congestion when the number of messages requesting that output port (in all the VNs) falls below the UNSAT THR threshold, being UNSAT THR < SAT THR. The pseudo-code in Algorithm 1 describes the whole mechanism. C. Congestion Notiﬁcation Once a congested point is detected (or when a previously congested point is no longer congested), sources must be notiﬁed in order to isolate (or stop isolating) congested ﬂows into slow-VNs. To deliver this notiﬁcation ICARO employs a simple dedicated network called CNN (Congestion Notiﬁcation Network) to send data about the status of the ports to all NIs in the network. The CNN consists in a P-bits-width ringnetwork to which all NIs and routers are connected, being P= log2 (N umN odes) + Router Radix + 1. Obviously, in the CNN routers act always as injectors and NIs as receivers so there is no media-access conﬂicts between NIs, but there may be conﬂicts between routers. To solve this, this ring is segmented by registers, so that each router has an associated register which separates the signals coming from the previous router from the signals being injected from the current router to the next one. The signals at each register are propagated to the next one at each clock cycle. An schematic deﬁnition of two consecutive routers is shown in Figure 1. When a router needs to inject data into the network, it waits until its associated register gets free. In practice, the router just keeps the port-status signal at the input of a multiplexer, which selects the register input signal depending on the busy bit sent by the previous register. When the current router register gets free, the multiplexer injects the signal to the register and such signal is propagated to the next register at the next cycle, and read by the next NI at the same time. When the port status data module follows a modulo-mapping [16] strategy. Since we make use of VNs instead of VCs, messages injected to the network through a given VN are never moved to other VN, as this is the key of isolation mechanism. Messages are provided in their header with a VN id prior injection. Such VN id is read by routers along the path in order to know which VN the message must be mapped to. The NI arbiter can be a typical arbiter (such as a round-robin arbiter). Note that the re-mapping mechanism can be executed in parallel with the injection of a message from other VNs except the one from which a message is being re-mapped. Note that ICARO is intended to be used with deadlock-free deterministic routing algorithms (e.g. XY), so no deadlocks can arise. 1) Congested-points cache: NIs must implement a mechanism to manage and store the congested-points data that must be available for the post-processor. This mechanism mainly consists in a cache memory and some additional logic. Figure 3 shows a diagram explaining the notiﬁcation storing process. When a notiﬁcation arrives to the NI, ﬁrst this notiﬁcation is deserialized, then traverses some ﬁlters (contained in the Notiﬁcation-processing module in Figure 3), and is ﬁnally stored in the cache. This cache may be implemented as ﬂip-ﬂop registers and it is arranged in several rows and two columns, each row corresponding to a notiﬁcation while each column corresponds to the data ﬁelds contained in the notiﬁcation (router ID and port). As explained previously, notiﬁcations arrive through the CNN as a router identiﬁer coded in binary and a bitmap describing the status of each port. However, not all the port-status data is relevant to the NI since not all ports of a given router are reachable from the receiver NI. Because of this, and also to speed up the cache queries, each notiﬁcation received through the CNN is split internally into as many notiﬁcations as the router radix value (for being able to discard individual port-status notiﬁcations). Once the router notiﬁcation is split into port notiﬁcations made of router, port status paired values, each notiﬁcation is stored in a temporary buffer (deserializer buffer). This buffer receives a notiﬁcation containing the status of the whole ports of the router at once, allowing the next functional module to read and process each port notiﬁcation one by one at each cycle. This allows to receive and process properly one notiﬁcation (containing all ports status) at each clock cycle through the CNN (in extreme cases) during a lapse of time, depending on this temporary buffer size. In case of deserializer-buffer overﬂow, notiﬁcations are dropped relying on the re-sync mechanism that allows to receive and process them later safely. Once the notiﬁcations are stored in the deserializer buffer, each notiﬁcation traverses a ﬁlter which discards unreachable points from this NI, thereby optimizing cache utilization. Congestion notiﬁcations which pass the ﬁlter are stored in the cache. End-of-congestion notiﬁcations which pass the ﬁlter trigger a matching-mechanism that removes from the cache congestion notiﬁcations which match the same router and port. In Figure 4 an example of an ICARO NI working in a 4x4 2D mesh is shown. The example shows the behavior of the NI 8 in the network. As can be seen, in Figure 4a the NI receives a notiﬁcation from router 9. This notiﬁcation contains the router ID and the port status bitmap which informs that the East port of such router is congested while the other ports are in normal state. As shown, the notiﬁcation is processed and stored in the cache. Next, in Figure 4b the message destined to the NI 2 tries to be injected into the network but, as the message will traverse the East port of router 9 (in order to arrive to the NI 2), the message is not injected, instead being re-mapped by Fig. 1: CNN registers example. Fig. 2: Complete congestion notiﬁcation network (CNN). generated from router x returns to the router x (the data has completed the loop along the ring), such data is dropped and the register x is freed. A complete CNN is shown in Figure 2. The congested-point data sent through the network consists of the router ID coded in binary, a bitmap corresponding to all ports in the router (a bit set to 1 means that the port corresponding to the bit position is congested, otherwise the port is not congested), and an additional bit set to 1 to indicate a valid signal (busy bit). All data is transmitted in parallel, so the CNN must be P-bits-wide. As some congestion notiﬁcations may be dropped at Network Interfaces (NIs) (explained later) or simply lost due to transient failures, the status of the ports is transmitted regularly (re-sync mechanism) to keep the congested-points data coherent at NIs. The frequency at which such data is transmitted is a conﬁgurable parameter of ICARO. Note that this mechanism may not scale for very large systems, as notiﬁcations may take too much time to reach all nodes in the network, this delay spoiling the performance improvement achieved by ICARO. Thus, for large systems, instead of using an unidirectional ring to deliver notiﬁcations, an hierarchical rings arrangement could be used. However, the evaluation of this option are left for further work. D. Congestion Isolation Congestion isolation is performed at NIs. As commented in the previous sections, ICARO makes use of at least two VNs: one slow-VN and one regular-VN. All ﬂows are always mapped ﬁrst to regular-VNs, but a module called postprocessor is in charge of checking the head of all regularVNs to ﬁnd messages that should be re-mapped to slow-VNs. The post-processor checks all queues each cycle: If it ﬁnds a head of message, the destination is analyzed in order to check whether or not this message will traverse a congested point. If so, the message is re-mapped to a slow-VN. In case of having more than one slow-VN available, the re-mapping Fig. 3: Notiﬁcation management. (a) ICARO receives a congestion notiﬁcation. (b) A message is reallocated to the slow-VN. (c) ICARO receives an end-of-congestion notiﬁcation. Fig. 4: ICARO NI module mechanism description. the post-processor to the slow-VN, which the message will be later injected through. Finally, in Figure 4c the NI receives a new notiﬁcation from router 9 informing that none of its ports is congested, so the stored notiﬁcation is removed, therefore no more messages will be re-mapped to the slow-VN. Note that, although ICARO makes no use of the slowVNs in absence of congestion, this does not necessarily lead to lower performance in terms of latency, as the number of VNs only affects latency when network capacity reaches its limit, and ICARO would start using all VNs in this case. 2) Optimizations: To optimizing even more the cache utilization, a congested-points merge mechanism can be used. Congestion tends to spread over the network starting from a root point. If congestion spreads towards a given NI, routers contained in the path to the root may notify for congestion sequentially. Due to this, the NI cache may be populated of multiple congested points contained in the same route so all of them but one are redundant since with only the one closer to the NI, the congested route would be covered completely. Thus, an optimization of ICARO consists in a merge mechanism in order to allow the redundant notiﬁcations to be discarded in a second ﬁlter before to storing new congested points. Let us suppose that we have a notiﬁcation already stored in the cache (notiﬁcation A). When a new notiﬁcation arrives (notiﬁcation B) three circumstances may occur: the new congested point is covered by another, already stored congested point (Figure 5a), the new congested point covers one or more already stored congested points (Figure 5b), the new congested point belongs to a new route (Figure 5c). In the ﬁrst case, the new congested point is useless because all messages crossing B will cross A so the new notiﬁcation is redundant, therefore can be discarded safely. In the second case, A is contained in B, so A becomes useless if we store B, therefore, A is replaced by B. Also, in such case, more rows may be affected by B. B may cover several already stored congested points, so such congested points can be safely merged, discarding them and storing B instead. However, despite this optimization achieves good results in minimizing the cache utilization, in some scenarios may be counter-productive in performance terms. When using the merge mechanism, the stored congested points tend to get closer to the NI, thereby isolating too much trafﬁc into the slow-VN. Also, since congested points belonging to branches of the congestion tree are usually more volatile than the congestion root, such congested points disappear quickly, thereby removing the congested points at NIs and so becoming the mechanism unstable in some cases. Therefore, this optimization should be used only when the cache size must be critically small due to the lack of silicon area available. IV. P ER FORMANC E EVALUAT ION For evaluating the performance we use a network-on-chip cycle-accurate simulator developed in our group. The ICARO results are compared with FVADA and AVADA [23]. First, the scenarios used for the simulations are described. Next, ICARO is evaluated in different scenarios varying critical parameters that may affect its performance. Then, a performance analysis is carried out comparing ICARO with FVADA and AVADA. A. Simulation Environment For our simulations we use a 4-stage pipelined router: IB (data storing into the buffer), RT (routing computation), VA/SA (VC allocation/switch allocation, both running in parallel), X (crossbar). Our router uses wormhole switching with ﬂit-level crossbar switching and implements credit-based ﬂow control. The size of the router queues is 16. The amount of queues varies depending on the strategy used, e.g. if the amount of queues is 4, the total slots for a given input port will be 4 (cid:3) 16 = 64f lits. A regular 8x8 2D mesh network is used (with XY routing), so router radix=5. It is noteworthy that, both FVADA and AVADA make use of virtual channels (VCs) instead of VNs as ICARO does. So, for the baseline scenario we decided to make use of VCs as well. Messages are 5-ﬂits long with a ﬂit size of 128 bits. Regarding trafﬁc patterns, two types of synthetic trafﬁc patterns are used. On one hand, typical synthetic trafﬁc patterns are used like uniform, tornado, bit-reversal, etc. On the other hand, as ICARO is a proposal intended to deal with irregular, bursty, hotspot-prone trafﬁc patterns, we drew up a combined trafﬁc pattern. This trafﬁc (a) A already covers B, so B is discarded. (b) B covers A, so A is replaced by B. (c) A and B belong to separated routes. Fig. 5: Merge opportunities. pattern is composed of a light uniform background trafﬁc and a hotspot component. Hotspots consist in several nodes (the amount depends on the network size) receiving each one high data rates from 4 nodes (4-to-1 hotspots). Hotspots are active only from cycle 10k to 20k. In this way, we have a background trafﬁc which generates no congestion, and another component of aggressive trafﬁc which causes congestion, causing HoLblocking to the background trafﬁc. B. Robustness Analysis In previous sections we stated that ICARO needs at least 2 VNs, one for regular trafﬁc and one for congested trafﬁc. However, the amount of VNs can be increased as much as we need, arranging the VNs in several conﬁgurations: 1+1VNs, 2+2VNs, 4+1VNs, 4+4VNs, etc. Also, for ICARO, the cache size is a critical parameter depending on the trafﬁc pattern and network size. So, for the purpose of evaluating the impact of such variables, an analysis is performed. For this analysis the combined hotspot trafﬁc patterns have been used with a background trafﬁc of 0.3ﬂits/cycle/node. The evaluation is performed with different VNs conﬁgurations. In order to graph the network latency for the different conﬁgurations, each VNs arrangement has a number assigned that identiﬁes such conﬁguration. Identiﬁer XY is for a conﬁguration with X regular-VNs and Y slow-VNs. We will play with conﬁgurations 11, 22, 31, 44 and 71. For the ICARO notiﬁcations we assume a propagation delay of 2 cycles for each hop. Regarding the baseline conﬁguration we use exactly the same conﬁguration as the ICARO scenario with the same number of VNs as in the ICARO case (considering both VN types: regular and slow VNs). Regarding the SAT THR and UNSAT THR thresholds used in the congestion-detection mechanism (see Section III), we performed simulations using different values for these thresholds, in order to evaluate their impact in ICARO and to obtain the optimal values. From the results obtained (not shown due to lack of space) we conclude that the thresholds values do not have a great impact on the ICARO behavior while they are conﬁned in a reasonable range. Nevertheless, the best results are achieved for SAT THR=4 and UNSAT THR=2, so these values are assumed in the current analysis. ICARO can work with the re-sync mechanism and/or the merge mechanism. Nevertheless, both mechanisms may cause counter-productive effects, so this analysis has been performed with three combinations of these mechanisms: no re-sync/no merge, re-sync/no-merge and re-sync/merge. The combination no re-sync/merge has not been considered as the purpose of the merge mechanism is to save cache slots in scenarios with a high number of notiﬁcations due to the re-sync mechanism. Regarding the congested points cache, the following sizes have been used: 2, 4, 8, 16, 32 and 360 (theoretical limit due to the maximum possible congested points that can be given in a 8x8 network). In the graphs, the 360 value has been replaced we adopted the policy of using 2 (cid:3) cache size entries. by 64 value for better viewing. For the deserializer buffer size In Figure 6 the results are shown. The metric used for measuring the performance is the latency area, which consists of the sum of the latency overhead during the whole simulation. The latency overhead is measured as the difference of latency values between the case with congestion present and the case with only background trafﬁc running. As can be seen, in all cases, with 4 or more cache slots, there is no latency area increase as ICARO has room enough to store all relevant congested points, thus it is able to isolate all the congested trafﬁc properly. However, as the number of cache slots available falls below 4, ICARO is not able to isolate congestion properly. However, as can be seen in Figure 6b, the re-sync mechanism helps to alleviate the shortage of cache entries. This is because congested points dropped due to the lack of room are re-notiﬁed periodically, so they have more opportunities to be stored. Besides, if we add the merge mechanism (Figure 6c), the latency falls even more as the congestion slots are better managed, so that there are more free slots to store congested points. However, the conclusion from this analysis is that with at least 4 cache entries HoL-blocking is completely removed regardless of VN conﬁguration. C. Overall Results In this section the ICARO performance is compared against AVADA and FVADA. First, all techniques are simulated using common trafﬁc patterns. As can be seen in Figure 8, for most of the common trafﬁc patterns ICARO keeps the results in similar values to the baseline and the other techniques. In the case of ICARO there is a slight overhead close to saturation. This is due to the fact that it employs VNs instead of VCs. VCs gives more ﬂexibility at the arbitration stage so is expected to perform better than using VNs. However, our proposal goal is not to improve the performance over static trafﬁc patterns but with the combined hotspot one1 . In Figure 7 we can see the latency results for the different mechanisms over combined hotspot trafﬁc pattern. Note that all mechanisms but ICARO use VCs while ICARO uses VNs. In Table I the ICARO parameters conﬁguration is shown. Let us recall that ICARO aim is to isolate harmful trafﬁc into the slow-VN in order to avoid non-harmful trafﬁc to be affected by the former. To better appreciate the ICARO behavior, latency results for our proposal are shown in two graphs: one for network latency average of all regular-VNs and another one for network latency of the slow-VN. As can be seen, ICARO outperforms all other mechanisms achieving an improvement of up to 82% for the 8VN conﬁgu1 In the case of FVADA and AVADA, despite of reproducing exactly the same scenarios the authors used in their evaluations, we could not obtain the results exposed by them for common synthetic trafﬁc patterns. (a) No re-sync, no merge. (b) Re-sync, no merge. (c) Re-sync and merge. Fig. 6: ICARO conﬁguration analysis. Parameter VNs conﬁg. SAT THR UNSAT THR Cache size Deserializer buffer size Re-sync Merge Value 1+1, 3+1 and 7+1 4 2 4 8 No No TABLE I: ICARO conﬁguration. ration. In the case of the 2VNs and 8VNs simulation FVADA is not shown because FVADA requires exactly r-1 VCs (r=router radix). Notice that congestion injection lasts from 10k-cycle to 20k-cycle. The HoL-blocking effects in ICARO are minimized and removed after the congestion builds. However, for the other conﬁgurations, congestion remains beyond the 20k-cycle point. They recover performance point only beyond 60k-cycle. IM P LEM EN TAT ION ANA LY S I S V. In this section, the area and power overhead of ICARO is analyzed. To perform this, the ICARO mechanism has been implemented in Verilog using a canonical NI and a wormhole router, both with support for 4 VNs. The router queues have a 4-ﬂit size with a 128-bit ﬂit size. For the NIs the queues have a size of 8 ﬂits. Regarding the ICARO conﬁguration, it has been implemented with support for merge and re-sync mechanisms with a cache size of 4 slots and a deserializer buffer size of 8 slots. To synthesize the Verilog designs, Design Vision tool from Synopsys with 45nm Nangate open cell library [2] (typical conditional) has been used. Then, we performed the place&route process with Encounter tool (from Synopsys too) to estimate accurately the area overhead. Figure 9 shows the results for the area and power overhead of a NI implementing ICARO compared with the baseline NI for different network sizes. In Figure 10, the area and power overhead results of our proposal for the router are shown. ICARO needs additional hardware in order to implement the CNN. However, this hardware is not strictly located either at the router or the NI. This hardware consists of wires interconnecting nodes and the logic associated to these wires (shown in Figure 1). In order to fairly evaluate all the hardware overhead imposed by ICARO, the logic associated to the CNN is included in the router overhead. Wires are not taken into account as they do not actually impose area overhead. Indeed, such wires use metalization layers. However, in the design ﬂoor-plan, little empty gaps always exist between all tiles to physically isolate each tile from its neighbors. Provided that these gaps are big enough to physically place all links between tiles, the area spent by the whole CMP remains the same even including the CNN wires. Fig. 9: NI area and power overhead. Fig. 10: Router area and power overhead. As can be seen in Figure 9, for all cases, the area overhead for the NI varies between 3.8% for a 16-node network, and 6% for a 1024-nodes network. For the power overhead it varies from 4.5% to 5.4%. In Figure 10 the overhead results for the router are shown. For the area, ICARO has an overhead of 6.7% for all cases. In the case of power, values from 6% to 10% have been obtained. As shown in such results, ICARO demonstrates an acceptable area and power overhead either for the NIs or the routers. In addition, taking into account the results for different networks sizes, seems clear that ICARO scales with the network size with no signiﬁcant extra area or power overhead. V I . CONC LU S ION S AND FUTURE WORK In this paper a mechanism for avoiding HoL-blocking in NoCs has been presented. ICARO manages to identify harmful trafﬁc and properly separates it from non-harmful one making use of VNs. This way, ICARO achieves improvements of up to 82% on the overall network latency with no signiﬁcant area and power overhead. As future work we plan evaluate proposals for scaling the CNN for very large systems. "
SpinNNaker - The world's biggest NoC.,"The SpiNNaker (Spiking Neural Network Architecture) project will soon deliver a machine incorporating a million ARM processor cores for real-time modelling of large-scale spiking neural networks. Although the scale of the machine is in the realms of high-performance computing, the technology used to build the machine comes very much from the mobile embedded world, using small integer cores and Network-on-Chip communications both on and between chips. The full machine will use a total of 10 square meters of active silicon area with 57,600 routers using predominantly multicast algorithms to convey real- time spike information through a lightweight asynchronous packet-switched fabric. In this talk I will focus on the NoC aspects, including novel approaches to fault-tolerance and deadlock avoidance.","SpinNNaker: the World’s Biggest NoC Steve Furber University of Manchester, United Kingdom Abstract. The SpiNNaker (Spiking Neural Network Architecture) pro ject will soon deliver a machine incorporating a million ARM processor cores for real-time modelling of large-scale spiking neural networks. Although the scale of the machine is in the realms of high-performance computing, the technology used to build the machine comes very much from the mobile embedded world, using small integer cores and Network-on-Chip communications both on and between chips. The full machine will use a total of 10 square meters of active silicon area with 57,600 routers using predominantly multicast algorithms to convey realtime spike information through a lightweight asynchronous packet-switched fabric. In this talk I will focus on the NoC aspects, including novel approaches to fault-tolerance and deadlock avoidance. Biography Steve Furber CBE FRS FREng is ICL Professor of Computer Engineering in the School of Computer Science at the University of Manchester, UK. He obtained a BA in Maths in 1974 and a PhD in Aerodynamics in 1980, both from the University of Cambridge, and was Rolls Royce Research Fellow at Emmanuel College, Cambridge, from 1978-81. He then joined Acorn Computers where he was a principal designer of the BBC Micro, which introduced computing into most UK schools, and the ARM microprocessor, which today powers most of the world’s consumer electronics, over 50 billion having been shipped by ARM Ltd’s semiconductor partners. In 1990 he moved to the ICL Chair at Manchester, where his research interests include Systems-on-Chip, low-power and asynchronous logic design, and neural systems engineering - building a massively-parallel computer for real-time brain modelling. "
Achieving balanced buffer utilization with a proper co-design of flow control and routing algorithm.,"Buffer resource minimization plays an important role to achieve power-efficient NoC designs. At the same time, advanced switching mechanisms like virtual cut-through (VCT) are appealing due to their inherited benefits (less network contention, higher throughput, and simpler broadcast implementations). Moreover, adaptive routing algorithms exploit the inherited bandwidth of the network providing higher throughput. In this paper, we propose a novel flow control mechanism, referred to as type-based flow control (TBFC), and a new adaptive routing algorithm for NoCs. First, the reduced flow control strategy allows using minimum buffer resources, while still allowing VCT. Then, on top of TBFC we implement the safe/unsafe routing algorithm (SUR). This algorithm allows higher performance than previous proposals as it achieves a proper balanced utilization of input port buffers. Results show the same performance of fully adaptive routing algorithms but using less resources. When resources are matched, SUR achieves up to 20% throughput improvement.","Achieving Balanced Buffer Utilization with a Proper Co-Design of Flow Control and Routing Algorithm Miguel Gorgues1 , Dong Xiang2 , Jos ´e Flich1 , Zhigang Yu2 , and Jos ´e Duato1 1Uni. Polit `ecnica de Val `encia, Spain, Email: migoral@disca.upv.es 2 School of Software, Tsinghua University, China, Email:dxiang@tsinghua.edu.cn Abstract—Buffer resource minimization plays an important role to achieve power-efﬁcient NoC designs. At the same time, advanced switching mechanisms like virtual cut-through (VCT) are appealing due to their inherited beneﬁts (less network contention, higher throughput, and simpler broadcast implementations). Moreover, adaptive routing algorithms exploit the inherited bandwidth of the network providing higher throughput. In this paper, we propose a novel ﬂow control mechanism, referred to as type-based ﬂow control (TBFC), and a new adaptive routing algorithm for NoCs. First, the reduced ﬂow control strategy allows using minimum buffer resources, while still allowing VCT. Then, on top of TBFC we implement the safe/unsafe routing algorithm (SUR). This algorithm allows higher performance than previous proposals as it achieves a proper balanced utilization of input port buffers. Results show the same performance of fully adaptive routing algorithms but using less resources. When resources are matched, SUR achieves up to 20% throughput improvement. IN TRODUC T ION I . Nowadays, there is no doubt that network-on-chip (NoC) [1] has moved from concept to technology. NoCs are the natural way to allow efﬁcient communication inside a chip in terms of performance, area, and power. NoCs replace complex allto-all communication solutions, or simple solutions as busses or crossbars which do not scale. The NoC concept was inherited from the off-chip domain, where high-performance interconnects were designed to build large HPC installations or datacenter systems. This shift in the environment (from HPC/datacenters to chip) makes NoC design a challenge, since the engineer must face very limiting constraints in the chip design environment. Area constraints impose optimized designs trying to use as less resources as possible. This forces the engineer to very optimized resource designs. However, more important is to provide a powerefﬁcient design. The chip power budget is highly limited and this imposes severe constraints in the resources used within the chip. Power-hungry components (mainly buffers) must be minimized if not removed at all. However, the engineer faces the problem of complying with those constraints while still delivering the expected performance. One clear example of this problem is the fact that engineers usually rely on wormhole switching where blocked packets keep in the network along their paths, keeping buffers in different routers. This allows buffers to be reduced in size, smaller than the packet size. However, this switching mechanism imposes large performance impact. Indeed, by blocking several routers, a packet may introduce severe congestion problems. 0 This work was supported by the Spanish Ministerio de Econom´ıa y Competitividad (MINECO) and by FEDER funds under Grant TIN201238341-C04-01 and by the National Science Foundation of China under grants 60910003, 61170063, and 61373021 and the research grant from Education Ministry under grant 20111081042. In addition, wormhole switching imposes more architectural constraints. For instance, tree-based broadcast/multicast operations [2] can not be implemented unless more buffers are allocated. This is needed to avoid deadlock situations. In addition, the way buffer components are managed may lead to power waste. Indeed, their use should be as balanced as possible in order to economize energy. Typically, routing algorithms rely on different virtual channels, specially in wormhole switching. Also, several virtual channels are implemented to cope with protocol-level deadlocks induced by dependencies between messages generated by higher-level coherence protocols. This leads to a large number of buffers and, thus, to a waste of resources if they are not equally balanced, which is the typical case. In this paper we address the problem of balanced buffer utilization. In order to address this challenge we ﬁrst propose a novel ﬂow control strategy, referred to as Type-Based Flow-Control (TBFC). This mechanism is tailored to buffer resources with minimum capacity but still allowing virtual cut-through switching (thus enabling its beneﬁts). In addition, TBFC is prepared for a new type of routing algorithms which, depending on the type of a packet may take different routing decisions. Indeed, we apply a novel adaptive routing algorithm on top of TBFC. The algorithm, referred to as Safe/Unsafe routing (SUR), classiﬁes packets as safe of unsafe depending on the chances of packets to induce deadlock. When combined, TBFC and SUR achieve a perfect balanced utilization of resources thus achieving an optimal use. Performance results show a boost in performance when the algorithm is used in 2D torus networks. Also, performance is kept maximum in 2D meshes while using less resources. The rest of the paper is organized as follows. In Section II we describe the TBFC mechanism. Then, in Section III we describe the SUR algorithm working on top of TBFC. In Section IV, we provide the performance evaluation and its analysis. The related work is described in Section V, and the paper is concluded in Section VI. I I . TBFC D E SCR I P T ION Before describing TBFC, we need to differentiate between two different crossbar switching strategies that may be implemented inside the router. The ﬁrst one is termed ﬂit-level switching and consists of improving buffer utilization by allowing the router to multiplex ﬂits of different packets through the same output port but mapped in different virtual channels. The second one is termed packet-level switching and consists in preventing the router to multiplex ﬂits from different packets to the same output port. In this approach, when a packet header gets access to the crossbar, the remaining data of the packet will keep the crossbar connection and follow without interruption. Flit-level switching is conceived for wormhole switching while packet-level switching is conceived for virtual cut-through. However, both approaches can be used for any switching mechanism. Nevertheless, taking ﬂit-level or packetlevel switching into account is important since it affects how ﬂow control can be implemented. Next, we describe our ﬂow control method for both crossbar switching strategies. A. TBFC with Flit-Level Crossbar Switching Figure 1 shows a traditional credit-based ﬂow control implementation for a pair of output-input ports. Flit-level crossbar switching is assumed. At each output port, the router needs some control information. Indeed, for each VC we need: one ﬁeld for the number of credits available (CRED), one ﬁeld to determine whether the VC is being used or not (USED), and the input port and VC that has this VC granted (stablishes a link between the input port and the granted VC). Fig. 1. Traditional credit-based ﬂow control with ﬂit-level crossbar switching. When a packet header is routed, the router sends a request to the target output port. At that port, the virtual channel allocator (VA) checks whether there is any free VC that has enough credits at the next router for the whole packet (we assume virtual cut-through switching). Then, the router arbitrates (in round-robin fashion) among all the requests and assigns the VC to the winning request. It stores the winning input port and virtual channel in the control info structure associated to the VC. It also decrements the available credits in the control info associated to the VC. At Switch Allocation (SA) stage, the arbiter selects the input port that will send a ﬂit through the output port the next cycle. SA selects this port between the input ports assigned to this output port by the VA stage. The arbiter rotates the priorities whenever an input wins the access, thus implementing ﬂit-level crossbar switching. The router sends the ﬂit together with the VC ID to the next router. The next router uses the VC ID to demultiplex and allocate the ﬂit into the correct VC. When a tail ﬂit is forwarded the VC is freed and can be assigned again to a new packet header. At the input port, when one ﬂit is forwarded, the Flow Control Logic (FCLogic) sends a credit back to the upstream router. To do this, the router needs at least log2 (V)+1 wires to indicate the VC that will receive the credit (signals VC), where V is the number of virtual channels at each input port. It also sends the control signal CRED. Upon reception, the credit counter associated with the VC is incremented. Figure 2 shows TBFC when applied to ﬂit-level crossbar switching. The ﬁrst difference between the traditional ﬂow control and TBFC is the ﬂow control information structure. TBFC adds two new ﬁelds per output port: FREE, which accounts for the number of available VCs, and TYPE, which accounts for the number of packets stored at the input port labeled with a particular type (we will later describe the type usage in the routing algorithm). Then, for each VC, the control info keeps the CRED counter and the associated info for the assigned input port and VC. The USED ﬁeld is removed. Fig. 2. TBFC ﬂow control with ﬂit-level crossbar switching. Contrary to the baseline ﬂow control, the rules (at VA stage) to assign a VC to an incoming request are different. In our design, the VA stage checks only the number of free VCs and the number of labelled packets (TYPE ﬁeld) (more details described in the next section). When one request wins the output VC, the input port of this request is assigned to the output VC. The winning input port and input VC are associated to the control info for the VC. The number of FREE VCs is decremented by one and, if the packet sent downstream is labelled, then the TYPE ﬁeld is incremented by one. At SA stage, the router selects the input port to pass through the output port and forwards the ﬂit to the next router. At crossbar stage, the router does not send the VC selected. Instead, it performs a packet → ID mapping (ML block) to assign one identiﬁer to the packet. When a head ﬂit is sent, the router sends also the type of the packet. The identiﬁer, the packet type and the ﬂit are sent through the link to the next router. All the ﬂits of the same packet will use the same identiﬁer and only the packet header will contain the type ﬁeld. When the downstream router receives the head ﬂit, the packet type and the identiﬁer, a new mapping is performed (ML block). In this case, an ID → V C mapping is performed, thus allocating the new packet in one free VC. After the head ﬂit, all ﬂits that arrive with the same identiﬁer are kept in the same VC through the mapping logic. Each input VC has one bit associated, referred to as Last token (LT). When one head ﬂit arrives and is allocated in one VC, this VC sets its LT bit to one and the LT bit of the other VC is reset to 0.1 This ﬁeld is used to guarantee in-order delivery of packets. Indeed, if two VCs at the same input port have a header packet with the same destination, then the oldest one (the one with LT bit set to zero) is the one to access the VA stage. Otherwise, both packets may access the VA stage. Notice that if the routing algorithm allows out-of-order delivery, then the LT bit and its associated logic can be removed. This is the case of SUR. In addition, each input VC has a TYPE bit which indicates whether the packet allocated on that VC is labelled or not. This bit is updated with the type information received when a header ﬂit arrives. Whenever a head ﬂit is sent downstream, the TYPE bit is transmitted upstream. Upon reception, the upstream switch decreases the TYPE counter. In any case, the FREE ﬁeld is increased by one. Notice also that the CRED ﬁeld is still used in TBFC. This is needed as we are assuming ﬂit-level crossbar switching, which may induce different reception and transmission rates at the input ports. B. TBFC with Packet-Level Crossbar Switching Now, we focus on the TBFC mechanism when packet-level crossbar switching is enabled. Notice that in this case packets will not be mixed in the crossbar. This fact, together with the 1 If the router has more than 2 VCs, the LT ﬁeld will need log2 (V) bits and will be updated following an algorithm similar to the ones used in caches with Least Recently Used (LRU) replacement policies. use of VCT, will guarantee that reception and transmission rates of packets at the input ports will be equal. This means that whenever a packet header wins the access to the crossbar, the whole packet can be transmitted and will not stop its transmission until reception at the downstream router. This fact simpliﬁes greatly the TBFC mechanism, as we will see. Figure 3 shows TBFC with packet-level crossbar switching. The ﬁrst thing to notice is the simpliﬁcation of the control structures. Now, we do not need credits anymore and we only need to keep which input port and VC got access to the VCs downstream through an output port. In particular, the FREE and TYPE ﬁelds are still used. Also, the mapping logic blocks are removed. Indeed, when a packet gets access to the output port will be transmitted uninterruptedly. Fig. 3. TBFC ﬂow control with packet-level crossbar switching. The VA stage is not modiﬁed as it takes into account only the number of free VCs (FREE ﬁeld) and number of packets labelled at the downstream router (TYPE ﬁeld). The SA stage is also simpliﬁed since there is not ﬂit multiplexing at the output. The SA stage needs only to arbitrate among competing packets but must keep the token priority ﬁxed until the packet’s tail leaves the router. This guarantees no crossbar multiplexing. At the downstream input port the logic is also simpliﬁed. There is no ML logic and the FCLogic only sends back upstream the type of the packet that just started to leave the input port. LT bits are still used if in-order delivery needs to be guaranteed and the type ﬁeld per VC is needed to remember whether the packet in the VC is labeled or not. I I I . SA FE /UN SA FE ROUT ING A LGOR I THM In this section we present the new routing algorithm adapted to the TBFC strategy. Each input port contains two VCs, while each VC is assigned a buffer to keep the whole packet. The SUR algorithm is fully adaptive and relies on an escape path to prevent deadlocks. The underline routing algorithm to implement this escape path is XY. The algorithm can work either on routers using ﬂit-level crossbar switching or packet-level crossbar switching. SUR works on n-dimensional meshes and n-dimensional tori. TBFC enables packet labeling and exposes this information to the routing stage. In our case, SUR labels packets as safe or unsafe. Packets are labeled when they are sent to a downstream router as follows: In an n-dimensional mesh a packet is delivered and kept in the next router as a safe packet if the next hop conforms to the baseline routing algorithm. Otherwise, the packet is labeled as unsafe. In an n-dimensional torus a packet is delivered and labeled in the next router as safe if one of the following conditions is met: The next hop of the packet is to traverse a wraparound link along dimension d, and the • ◦ • ◦ packet does not need to traverse a wraparound link with a lower dimension than d. The packet does not need to traverse any wraparound link from the current router to the destination and the next hop conforms to the baseline routing. If any of these two conditions is not met, then, the packet is delivered and labeled as unsafe packet. With this classiﬁcation, the routing algorithm will decide which outputs ports are eligible for packets. In detail, output ports along the minimal paths to destination will be eligible. Safe packets will be routed without any restriction and unsafe packets will be routed only in some particular conditions. To assist this routing algorithm we deﬁne a check port function suitable for meshes and tori. Algorithm 1 shows the function check-port(f , s). This function avoids ﬁlling any input port with only unsafe packets. It checks, for a given input port, the number of free VCs (f ) and safe packet(s) as follows: f > 1, the packet can be delivered as there is more than one free VC in the input port at the next router. f = 1 and s > 0, the packet can be delivered because there is at least one safe packet in the next router. f = 1 and s = 0, the packet can be delivered only if the packet is safe at the next router; otherwise, the packet is blocked or takes another output port. f = 0, the packet is blocked or takes another port. • • • • Algorithm 1 check-port(f,s) The number of free VCs in the downstream node, f ; The number of safe packets in the downstream node, s; Whether the packet can route to the downstream node; Input: Output: 1: if f > 1 then return true; 3: end if 2: 4: if f = 1 and s ≥ 1 then 5: return true; 6: end if 7: if f = 1 and s = 0 then 8: and the packet will be delivered and labeled as a safe packet in the next router 9: return true; 10: end if 11: return false; The proposed fully adaptive routing algorithm in 2-D mesh is shown in Alg. 2, where fi+, si+ represent the number of free VCs and safe packets in the input port in the neighbor the positive direction, respectively. Similarly, fi− and si− router attached to the current node C along dimension i in represent the number of free VCs and safe packets in the input port along dimension i in the negative direction, respectively. The algorithm takes as inputs the coordinates of the current and destination nodes, number of free slot and number of safe packets of all neighboring input ports. The available channel set and the selected output channel are initialized to ∅ and null, respectively. If the current node is the destination, the internal channel is selected to consume the packet. Otherwise, if the offset along dimension i (dimensions 1 and 2 in Alg. 2) is greater than 0 and check-port(fi+,si+) returns a true value, then the packet can be delivered along a ci+ channel. If the offset along dimension i is less than 0 and check-port(fi -,si ) returns a true value, the packet can be delivered via a ci− Algorithm 2 safe-unsafe-2D-meshes Input: Output: coordinates of the current node C : (c1 , c2 ), coordinates of the destination D : (d1 , d2 ), free buffers: (f1−,f1 +, f2−, f2 +), safe packets: (s1−, s1 +, s2−, s2 +); selected output channel; 1: S=0;ch=null; 2: if C == D1 then 3: ch=internal; return true; if di − ci > 0 and check − port(fi+, si+) then S =← S ∪ {ci+}; if 0 > di − ci and check-port(fi−,si−) then S ← S ∪ {ci -}. 4: end if 5: for i == 1 to 2 do 6: 7: 8: 9: 10: 11: end if end if 12: end for 13: if S (cid:54)= ∅ then 15: end if 16: if if S = ∅ then 18: end if ch = null; ch = select(S ); 14: 17: Algorithm 3 safe-unsafe-2D-torus Input: Output: coordinates of the current node C : (c1 , c2 ), coordinates of the destination D : (d1 , d2 ), free buffers: (f1−,f1 +, f2−, f2 +), safe packets: (s1−, s1 +, s2−, s2 +); selected output channel; 1: S=0;ch=null; 2: if C == D1 then 3: ch=internal; return true; 4: end if 5: for i == 1 to 2 do 6: if 0 < di −ci ≤ k/2 or di −ci < −k/2 and check−port(fi+, si+) S =← S ∪ {ci+}; then if di − ci > k/2 or −k/2 ≤ di − ci < 0 and check-port(fi−,si−) S ← S ∪ {ci -}. then end if 7: 8: 9: 10: 11: 14: end if 12: end for 13: if S (cid:54)= ∅ then 15: end if 16: if if S = ∅ then 18: end if ch = null; 17: ch = select(S ); channel. The check-port(fi ,si ) function adds the channel ci+ or ci− to S if the packet can advance along dimension i. Alg. 3 presents the fully adaptive routing in 2-D torus. The difference lays in the computation of the direction to take in each dimension. Also, the check-port function must take into account the additional rule to deﬁne a packet as unsafe based on the crossing of wraparound links (see previous conditions). Finally, the proposed routing algorithm randomly selects an output channel from S if it is not null. Otherwise, the packet is blocked and routed in the next cycle. A. Deadlock-freedom property In this subsection we demonstrate SUR is deadlock-free by using a contradiction approach. We ﬁrst focus on 2-D meshes and then extended it to 2-D torus. Fig. 4. Example of deadlock-freedomness in a 1D ring network. Let us assume we have a cycle in a 2D mesh. Such a cycle will have dependencies between x → y and y → x channels. x → y dependencies are allowed by the underlying routing algorithms but y → x dependencies are not allowed. Packets stored in an Y input port will be labeled as unsafe as they are requesting an X output port. In order to create deadlock, packets inside a cycle should not advance. This means either all the buffers are full in the cycle or the routing restrictions do not allow packets to advance. The ﬁrst condition does not hold since it would mean that in the Y input port both VCs are storing unsafe packets. This can not happen since unsafe packets can be forwarded only if both VCs are available, or one VC is available and the other VC is holding a safe packet. The second condition (the routing restrictions do not allow packets to advance) does not apply neither. Indeed, if one packet is at a Y input port requesting an X output port the associated X input port will store either one, or two safe packets, or will be completely empty. In the case of storing one safe packet or being empty the unsafe packet at input Y can advance, thus no deadlock. In case of storing two safe packets, both can advance since they will always have in front of them safe packets, which potentially will move as they are using acyclic paths (conformed by safe packets using the underlying deadlock-free baseline routing algorithm). Therefore, not blocking packets in the cycle. In other words, safe packets, stored through deadlock free paths have always a reserved VC, always advancing. Unsafe packets can cross cycles but never ﬁlling up input buffer, thus avoiding deadlocks. Therefore, for any potential cycle, unsafe packets will never take all resources in an input buffer. They, in turn, will advance when both VCs are available. For the n-D torus case we follow a similar approach. In this case wraparound links take also an important role. If the packet does not need to traverse any wraparound link, the packet follows the behaviour described above. So, the packet will not create a deadlock. If the packet is stored in a router connected to a wraparound link, then if the packet requests the wraparound link with the lowest dimension that the packet needs to traverse, the packet is following the baseline routing. This means that the packet will be delivered and labeled as safe. On the other hand, if the packet requests an output port connected to a wraparound link and the dimension of this wraparound link is not the lowest dimension that the packet needs to cross, then the packet will be delivered and labeled as unsafe. As we have shown before this happens only if the next input port is empty or has one free VC and the packet stored in the other is safe. So, input buffers will never ﬁll with unsafe packets. In other words, in the case of 2D torus, all the input buffers will always allow safe packets to advance. Let us expose an example in Figure 4. In this 1-D torus all routers send messages to a router located at two hops to their right. R0 keeps in the input port two packets from R4 with destination node R1. These two packets are safe because they arrived from R4 crossing the wraparound link with the lowest dimension required. R1 contains two packets with destination R2 that came from R0. These two packets are safe because they do not require to cross any wraparound link and the packets (a) TBFC+SUR walkthrough example (b) Example Model Fig. 5. TBFC+SUR Example follow the baseline routing. R2 and R3 have the same situation as R1. R4 has one packet with destination to R0, this packet is unsafe because the packet comes from a router not connected to the wraparound link and needs to traverse the wraparound link to reach its destination node. Therefore, R3 will not send another unsafe packet to R4. Then, all the packets can advance as packets at R3 can be forwarded and consumed in R4. B. TBFC+SUR Example Figure 5(a) shows a walkthrough example of TBFC+SUR. Assume two routers, X and Y , connected through a 1-cycle delay link, one above the other in a 2D mesh. Also packetlevel crossbar switching is used. The buffers at the north input port of Y are empty, so the values stored in the ﬁelds FREE and SAFE at the south output port of X are 2 and 0, respectively. At time t3 the header ﬂit of packet p1 with destination south, competes for VA and SA stages. p1 wins SA, thus is forwarded in the next cycle through the south output port. As p1 follows the baseline routing, it is considered as a safe packet. Accordingly, FREE is decremented by 1 and SAFE is incremented by 1. At t4 , router X send the packet type (safe) and the ﬂit to router Y . At t4 , p2 arrives to VA and SA stages in router X . As packet-level crossbar switching is implemented, p2 should wait until p1 is completely transmitted. At t6 , p1 frees the crossbar and the header of this packet arrives to router Y . p2 tries to win the VA stage. In this case the packet’s destination is located south-east, thus potentially crossing an illegal turn in XY algorithm. The check-port() function is called by VA. By using the south port, p2 will be delivered as unsafe as it is not following the baseline routing algorithm. As SAFE is 1 (the south input port has one safe packet stored), the SA arbiter can select p2 to be forwarded through the south output port. The output port control is updated: FREE is decremented by one and SAFE is kept without change. The packet header leaves router X at t7 . At t7 , p3 arrives to VA stage and it is blocked because the output port is being used. For the same reason, p4 is also blocked at t8 . In this cycle, p1 wins the VA/SA stage at router Y , then the router Y sends back the signal FREE and TYPE to router X . At t9 , router X receives the ﬂow control signals and increases FREE by one and, as type is equal to safe, decreases TYPE by one. So at this moment the ﬂow control values are FREE equal to 1 and TYPE equal to 0. Then, only a SAFE packet could win the output port, as we explain above. Then, both p3 and p4 try to win the VA stage. The check-port() function allows packet p3 to win the resources at the south output port, because the packet follows the baseline routing algorithm. However, p4 is not allowed to win any resource at the south output port. This happens because the destination of p4 is located south-east, thus the packet would be delivered as UNSAFE as it is not following the baseline routing algorithm. The VA arbiter selects p4 to be forwarded through the south output port. So, p3 wins the VA/SA stage and will be forwarded in the next cycle. At t11 packet header of p2 wins the VA/SA stage at router Y , then the router send back the ﬂow control signals. In the next cycle, t12, the ﬂow control signals arrive to router X and FREE is increased by 1, and, as a type is unsafe, the SAFE ﬁeld is not modiﬁed. Then, p4 accesses the VA/SA stage and as SAFE is equal to 1, the packet can be forwarded through the south output port as unsafe packet, decreasing the ﬂow control variable FREE by 1. IV. P ER FORMANC E EVALUAT ION Now, in this section, we perform an evaluation and analysis of our proposal. In particular, we ﬁrst describe the analysis tools and simulation parameters. Then, we show the performance results for TBFC and SUR. We analyze two scenarios: a 2D mesh with 64 routers and a 2D Torus with 64 routers. A. Analysis Tools and Parameters The tool we use for this analysis is an event-driven cycleaccurate simulator coded in C++. The simulator allows to model any network topology and router architecture. We modeled a 4-stage pipelined router with VCs and ﬂit-level crossbar switching. Table I shows the simulation parameters used for the 2D mesh scenario. Transient and permanent messages relate to the number of messages processed until the simulator enters the permanent state and ﬁnishes the simulation, respectively. In this scenario, we analyze three routing algorithms: deterministic routing (XY), fully adaptive routing (FA) using the typical credit-based ﬂow control and SUR routing (SUR) with TBFC. Notice that evaluating TBFC in isolation makes no sense since it needs a routing algorithm that takes into account the type-based approach. Parameter Network topology VCs at each input port Message size, Flit size Queue size Fly link Transient, Permanent msgs FA, SUR 2VC, XY 4x4 mesh 2 80 bytes, 4 bytes 20 ﬂits 1 cycle 10000, 10000 TABLE I. PARAM E TER S AND VA LU E S U S ED FOR 2D M E SH . For the torus scenario, the same parameters were used except for the number of VCs and queue size at each input port. These parameters depend on the routing algorithm and ﬂow control scheme used. Table II shows the values in the torus scenario. In torus scenario we analyze ﬁve routing algorithms: deterministic routing (XY), fully adaptive with one adaptive channel and two escape channels (FA), fully adaptive with the bubble ﬂow control [3] (FA bubble) using one adaptive channel and one escape channel with double size (to implement the bubble). Also, we analyze SUR with two and three virtual channels (SUR 2VC and SUR 3VC). These conﬁgurations (except SUR 3VC) are the ones with minimum buffer needs to become deadlock-free and to guarantee VCT. Routing FA Bubble FA, SUR 3VC XY, SUR 2VC VCs 2 3 2 Queue Size 20 ﬂits adap, 40 ﬂits esc 20 ﬂits 20 ﬂits TABLE II. PARAM E TER S AND VA LU E S U S ED FOR 2D TORU S . We evaluate six trafﬁc distributions: bit-complement, bitreversal, transpose, perfect shufﬂe, uniform and hotspot. For the sake of space we only show four of them: bit-reversal, transpose, uniform, and hotspot.2 In bit-reversal trafﬁc, the node with binary value an−1 , an−2 ,..., a1 , a0 communicates with node a0 , a1 ,..., an−2 , an−1 . For transpose trafﬁc, node with binary value an−1 , an−2 ,..., a1 , a0 sends packets to node an/2−1 ,... a0 , an−1 , ....an/2 . Finally, in hotspot trafﬁc, ten randomly chosen nodes send 20% of their trafﬁc to an speciﬁc node and the rest of trafﬁc to any other node with equal probability. The rest of nodes keep injecting using a random uniform distribution. B. Performance Result. Figure 6 presents results for 2D mesh. Figures 6(a) and 6(e) show the performance results for the bit-reversal trafﬁc. In this scenario, our method reaches similar results on throughput than the ones achieved by FA but improves latency close to saturation. In any case, both adaptive algorithms (SUR and FA) outperform XY. With transpose trafﬁc, Figures 6(b) and 6(f), SUR outperforms FA by about 10% in network throughput. Latency is also improved by SUR when working close to saturation. For the other trafﬁc distributions (uniform and hotspot; rest of Figure 6) we see similar results for the three routing algorithms. SUR, FA, and XY achieve similar throughput. However, SUR slightly improves latency. In 2D Torus, differences are much more signiﬁcant. Figures 7(a) and 7(e) show the performance for bit-reversal trafﬁc. SUR 2VC improves network latency achieved by FA and FA bubble. This is achieved by using less buffer resources (2VCs each with 20 slots, instead of either 3 VCs each with 20 slots or 2 VCs one with 20 slots and the other with 40 slots). 2We achieved similar conclusions with the not-shown trafﬁc distributions. Moreover, for the same number of resources, SUR 3VC works much better than FA and FA bubble on both, network latency and throughput (9% better). Also, in transpose trafﬁc, Figures 7(b) and 7(f), SUR performs much better than FA and FA bubble. In this case, both versions of SUR achieve a boost in throughput of 20% when compared to FA. Also both SUR versions perform better on network ﬂit latency. Finally, improvements are also achieved in uniform and hotspot. Figures 7(c) and 7(g) present the performance comparison for uniform trafﬁc. SUR 2VC and FA perform similar on latency and throughput. SUR 3VC has the best performance on network ﬂit latency and throughput (14% better than FA). With hotspot trafﬁc, Figures 7(d) and 7(h) illustrate as SUR 3VC works better than FA and FA bubble on network ﬂit latency, and all routing algorithms have similar throughput. Fig. 8. 2D Mesh Example. As we have seen, SUR (together with the TBFC strategy) improves network throughput and latency over FA. In Figure 8 we show an example that highlights why we are achieving such improvement over FA. The ﬁgure represents a 2× 2 mesh. Assume that R0 wants to send a packet to R3. In FA, R0 can send the packet to R1 or R2. In case of R1 it can send the packet adaptively or through the escape channel (conforming to XY routing), and in case of R2 can send the packet only via the adaptive channel. Therefore, it can allocate this packet only in two VCs at R1 or in one VC in R2. The default fully routing algorithm (which promotes adaptive VCs over escape VCs) would then use only two possible VCs (one in each router). In case of an optimized FA algorithm (which gives the same priorities to adaptive and escape VCs), three VCs can be used (two in R1 and one in R2). However, in SUR algorithm safe packets can be allocated in any of the four VCs. Even unsafe packets can use any of the four VCs (taking into account there is an empty VC in the input port router). So, SUR has more options to allocate the packet, allowing SUR to improve performance obtained by FA. (a) SUR Scalability (b) VC utilization Fig. 9. Scalability and VC utilization Figure 9(a) shows how the beneﬁts of TBFC+SUR scale with the number of VCs. TBFC+SUR with one VC less than FA achieves the same behaviour on throughput. As can be appreciated TBFC+SUR with 3VC achieves the same maximum throughput as FA with 4 VC, and the same for TBFC+SUR with 4 VC compared with FA 5VC. Figure 9(b) shows the VC (a) latency bit reversal (b) latency transpose (c) latency uniform (d) latency hotspot (e) throughput bit reversal (f) throughput transpose Fig. 6. Performance evaluation in 8 × 8 mesh networks. (g) throughput uniform (h) throughput hotspot (a) latency bit reversal (b) latency transpose (c) latency uniform (d) latency hotspot (e) throughput bit reversal (f) throughput transpose Fig. 7. Performance evaluation in 8 × 8 torus networks. utilization at the mesh scenario, with uniform trafﬁc. SUR with low trafﬁc achieves a balanced use of the resources. However FA use mainly the VC0, the adaptive channel. V. R ELAT ED WORK In this section we describe relevant previous work about ﬂow control and adaptive routing algorithms in NoCs. In the recent years, a lot of papers about ﬂow control and routing for NoCs have been presented. Here, we introduce some of them. Tang in [4] proposes a ﬂow control for meshes in which they limit injection rate dynamically. Nousias in [5] proposed an adaptive rate control strategy in wormhole switching with virtual channels. When the contention changes, the destination node sends a signal to the source node to regulate the injection rate accordingly. Avasare in [6] proposed a centralized end-toend ﬂow control for packet switching. This ﬂow control requires two networks, the control network and the transmission data network. In [7], ﬂit reservation control is presented. In this ﬂow control strategy, the ﬂit control traverses the network (g) throughput uniform (h) throughput hotspot in advance of data ﬂits reserving the virtual channels. After that the packet is sent to the destination node. In all the previous works, either congestion is addressed or end-to-end ﬂow control. Also, additional structures (parallel networks) are needed. Dally and Aoki [11] described the dynamic misrouting algorithm by tagging packets based on how many misroutes they have incurred and allow any packet to request any VC as long as its not waiting for a packet with a lower dimensional reversal number. Glass and Ni in [8] proposed turn model for designing partially adaptive deadlock-free algorithms in a mesh. The west-ﬁrst routing algorithm in a 2D mesh traverses the west hops ﬁrst, if necessary, and then adaptively south, north and east. The negative-ﬁrst routing (NFR) algorithm in a 2D mesh routes a packet ﬁrst adaptively west and south, and then adaptively east and north. Chiu [9] proposed the improved partially adaptive routing algorithm odd-even turn model by constraining turns, that can introduce deadlocks, to occur in the same row or column. Wu [10] proposed a fault-tolerant odd-even turn model for 2D meshes. Dally and Seitz in [11] presented the sufﬁcient and necessary condition for deadlock-free routing in an interconnection network. Several routing algorithms were proposed for meshes and tori [9], [12], [13]. Load-balanced, non-minimal adaptive routing algorithms for tori were proposed by Singh, et al. [13], [14] with three virtual channels. The method in [16] presented an adaptive minimal deadlock-free routing algorithm for 2D tori. However, the number of virtual channels required by the method was not well-controlled in [16]. Duato [17] proposed a necessary and sufﬁcient condition for deadlock-free adaptive routing in WH-switched networks. Methodologies for design of deadlock-free adaptive routing algorithms are also presented in [18]. The adaptive bubble router [3] for VCT-switched torus is based on Duato’s protocol. It requires an escape channel applied dimension-order routing (DOR) and an adaptive channel. A ﬂow control function is added to the escape channel in order to avoid deadlocks. In NoCs, Marculescu in [19] proposed a new routing technique (DyDA) which switches between deterministic and adaptive routing based on the network’s congestion conditions. When the network is not congested DyDA router works with deterministic routing. When the network becomes congested, then DyDA router works with adaptive routing. Ebrahimi in [20] proposed a new fully routing algorithm (DyXYZ) for 3D NoCs. In this new routing the congestion information is used as a congestion metric to select the best output port. Ma, et al. in [22] proposed a hybrid wormhole/VCT switching technique to reduce buffering while improving the performance of fully adaptive routing. Ma, et al. [21] proposed a ﬂit bubble ﬂow control scheme by reﬁning the baseline bubble ﬂow control scheme. Chen, et al. [23] proposed worm bubble ﬂow control (WBFC), which reduces the buffer requirements and improves buffer utilization in torus networks. However, the methods in [21]–[23] still need to partition virtual channels into adaptive and escape channels. Kumar, et al. [24] present a new router architecture which performs aggressive VA allocation after performing the SA arbitration. To do this, a bundle signal is sent prior to the ﬂit. Also, some arbitration strategies are included (e.g. per ﬂow arbitration). Our work differs in the sense that we aim for routing optimization and ﬂow control adaptation. The previous ﬂow control methods need more information control than our proposal. Some of them use circuit switching [6], so they need two different networks. Therefore, these ﬂow control strategies use more resources than our proposal. Also, our proposal does not limit the injection rate, as happens in [4]. Finally, the SUR algorithm with only two virtual channels can provide fully adaptive routing for TBFC. Two virtual channels are not classiﬁed into escape and adaptive channels. Therefore, the buffer resources can be used more equally. Indeed, none of previous proposals focused on balancing buffer resources by co-designing the ﬂow control and routing algorithm. V I . CONCLU S ION This paper presents a novel ﬂow control Type-Based FlowControl (TBFC) with Safe/Unsafe routing algorithm (SUR) which allows an optimized balanced buffer utilization. This is achieved because our proposal does not differentiate between VCs and does not divide the virtual channels into adaptive and escape channels. The combination of TBFC and SUR allows us to reduce the number of VCs required to implement fully adaptive routing algorithms. Sufﬁcient simulation results were presented by comparison with the previous methods. The results showed that the proposed TBFC with SUR algorithm outperform better than the previous methods under different communication patterns. [5] [17] [18] [10] "
QuT - A low-power optical Network-on-Chip.,"To enable the adoption of optical Networks-on-Chip (NoCs) and allow them to scale to large systems, they must be designed to consume less power and energy. Therefore, optical NoCs must use a small number of wavelengths, avoid excessive insertion loss and reduce the number of microring resonators. We propose the Quartern Topology (QuT), a novel low-power all-optical NoC. We also propose a deterministic wavelength routing algorithm based on Wavelength Division Multiplexing that allows us to reduce the number of wavelengths and microring resonators in optical routers. The key advantages of QuT network are simplicity and lower power consumption. We compare QuT against three alternative all-optical NoCs: optical Spidergon, λ-router and Corona under different synthetic traffic patterns. QuT demonstrates good scalability with significantly lower power and competitive latency. Our optical topology reduces power by 23%, 86.3% and 52.7% compared with 128-node optical Spidergon, λ-router and Corona, respectively.","QuT: A Low-Power Optical Network-on-Chip Parisa Khadem Hamedani∗ , Natalie Enright Jerger∗ and Shaahin Hessabi† ∗The Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada †Department of Computer Engineering, Sharif University of Technology, Tehran, Iran Email: {parisa, enright}@eecg.toronto.edu, hessabi@sharif.edu Abstract—To enable the adoption of optical Networks-on-Chip (NoCs) and allow them to scale to large systems, they must be designed to consume less power and energy. Therefore, optical NoCs must use a small number of wavelengths, avoid excessive insertion loss and reduce the number of microring resonators. We propose the Quartern Topology (QuT), a novel low-power all-optical NoC. We also propose a deterministic wavelength routing algorithm based on Wavelength Division Multiplexing that allows us to reduce the number of wavelengths and microring resonators in optical routers. The key advantages of QuT network are simplicity and lower power consumption. We compare QuT against three alternative all-optical NoCs: optical Spidergon, λrouter and Corona under different synthetic trafﬁc patterns. QuT demonstrates good scalability with signiﬁcantly lower power and competitive latency. Our optical topology reduces power by 23%, 86.3% and 52.7% compared with 128-node optical Spidergon, λrouter and Corona, respectively. IN TRODUC T ION I . As we usher in the many-core era, increasing attention is being paid to communication as a potential power and performance bottleneck. Electrical NoCs are an effective solution for current and near-future on-die communication requirements. However, as future many-core systems require higher throughput and operate under increasingly tight power budgets, the continued scalability of electrical NoCs is questionable [1][2]. Integrated silicon-compatible photonic technology is an alternative approach to communication, which has the potential to support large-scale networks with low power, low latency and high bandwidth [3][4]. Photonics reduce network power consumption by omitting electrical buffering and switching. Electrical NoCs. Power and latency constraints in future CMPs severely limit the scalability of electrical NoCs. For an electrical mesh with 168-bit ﬂits and 4-ﬂit buffers per input port, the energy to transmit one ﬂit across a 1 mm link and subsequent router (energy-per-ﬂit-per-hop) is 197 pJ [2][5]. The total energy consumed in a clock cycle is proportional to the average number of ﬂits traversing links per clock cycle [2]. Therefore, the power consumption of 128-node mesh is 82 W1 , in 32nm at 3 GHz under uniform trafﬁc, with average link utilization of 0.3 and XY routing. This power consumption is unacceptably high. Evaluation of a 256-node 2D mesh in 22nm shows that the network channel power and buffering power of an electrical NoC exceeds the allocated network power budget by an order of magnitude [1]. In terms of performance, assuming 3 clock cycles per router and a single cycle per link, the average zero-load latency under uniform random trafﬁc is 32 and 43 clock cycles in 128- and 256-nodes mesh NoC, respectively. Zero-load latency does not consider router congestion which has a negative effect This research was supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto. on latency. As the network size grows, the latency in an electrical NoC quickly increases [6]. Since future CMPs need a low power and low latency communication fabric, studying alternatives such as Optical NoCs which can overcome these limitations is valuable. Optical NoCs. Although nano-photonics promises lowpower, high-throughput communication, it presents a number of design challenges. Despite the fact that research in nanophotonic devices and materials is a hot area, there are some technical hurdles that could limit the deployment of optical networks. To overcome these impediments, scalable optical networks should carefully consider power, the number of microrings and the number of wavelengths. • Insertion Loss: To fully realize the potential of nanophotonics, the network must be carefully designed with low power in mind. Laser power is one of the main factors in the power consumption of an optical NoC. The laser power is calculated based on the maximum insertion loss (IL) in an optical system. Therefore, to reduce optical power, we must decrease IL. • Number of Microrings: In terms of reliability, microrings are a major source of faults in an optical system, since they are sensitive to process and temperature variation. An optical NoC should have simple switches to reduce the number of microrings in the path of the optical streams. • Number of Wavelengths: The number of wavelengths used to modulate the data in an optical system, should be reduced to have a low power and practical optical NoC. ◦ Wavelength-division multiplexing (WDM) can substantially improve the optical network bandwidth. WDM allows the transmission of different data streams through a single optical waveguide [3]. Although, existing work relies heavily on Dense WDM, the number of wavelengths that can be transmitted in a single waveguide is limited. Current research estimates that less than 100 wavelengths can be supported per waveguide [8]. ◦ The laser must produce sufﬁcient output power to propagate an optical stream with low bit error rate. The output power of today’s multi-wavelength lasers is low [9][10]. Low-power means there is not enough power to modulate data correctly across many wavelengths. As a result, a single-wavelength laser must be used; however, a large number of single-wavelength lasers cannot be attached to the chip. Therefore, the number of wavelengths should be small to reduce the number of lasers. ◦ Laser power is proportional to the number of wavelengths in an optical system. Supporting a large number of wavelengths increases the required laser power. To design a low power optical architecture, we must address all of these challenges. Focusing on one or two of them is not 1 Power consumption is estimated using the method by Eisley and Peh [7]. (cid:26) (xi + 1) mod N (cid:26) (xi + N/4) mod N (cid:26) (xi + 1) mod N (xi + 3N /4) mod N QuT is based on the following connectivity formulation: • Ring links (bidirectional) xi is connected to (xi − 1) mod N • Cross links (bidirectional) if i is even, xi is connected to • Bypass links (unidirectional) if i is odd, xi is connected to (xi − 1) mod N Where switch xi is connected to node i and N is the number of nodes in QuT. Although bypass links appear to have the same connectivity as ring links, there are two important differences. First, they are unidirectional: they only emanate from odd nodes. Second, their connection within the switch is different from ring links. We use bypass links, instead of additional cross links in odd switches, to prevent the data from being absorbed by the wrong node as we will see in Section II-B2. B. Router Microarchitecture In this section, we describe the assignment of wavelengths to nodes and the design of the optical switches. 1) Wavelength Assignment: Each node in the QuT data network has a speciﬁc, dedicated but not unique wavelength onto which other nodes will modulate their data to communicate with this node. We have designed QuT, its deterministic wavelength routing and its optical switches to use a small number of wavelengths. In an N -node QuT, we use N/4 distinct wavelength sets to modulate all data in network. A wavelength set of λ(i mod N/4) is assigned to node i (Figure 1). Each wavelength set can include one or more wavelengths based on the required network bandwidth. 2) All-Optical Switches: QuT requires two different switches, one for even (Figure 2a) and one for odd (Figure 2b) nodes. Employing passive microring resonators in QuT switches eliminates the path reservation phase required in prior work that used electro-optic broadband ring resonators [12]. A source chooses one of the four injection channels in its optical switch, based on the destination’s index. Choosing among four injection channels helps reduce the total number of wavelengths required by the network. Microring resonators in the even and the odd switches are classiﬁed in four groups: (a) Add Microring Resonators (AµR) that multiplex optical streams from injection channels or cross links to ring links (b) Bypass Microring Resonators (BµR) in the even switches that switch optical streams from bypass links to cross links (c) Cross Microring Resonators (CµR) in odd switches which turn optical streams from ring links to bypass links (d) Drop Microring Resonators (DµR) that send the data, modulated on the wavelength of corresponding destination node, from ring links into the switch’s detector. AµR, BµR, CµR and DµR are arrays of microrings, where each microring is sensitive to a speciﬁc wavelength. By assuming that λ(i mod N/4) is devoted to ith node, DµR in the ith optical switch is only sensitive to λ(i mod N/4) . AµR is used to prepare suitable turns for all optical data streams. Hence, AµR is sensitive to all wavelengths in QuT. BµR is used to send data on one of the cross links connected to the even node’s switch. Therefore, BµR is sensitive to all wavelengths in QuT. Even nodes beneﬁt from CµRs in odd switches when they want to send data to the destination that is a distance Figure 1: 16-node QuT structure and wavelength assignment sufﬁcient to have a low power optical NoC. We propose the Quartern Topology (QuT), a novel low-power all-optical NoC targeting key optical challenges. We use passive microring resonators [11] to route optical streams based on their wavelength. QuT is a ring-like topology with strategically placed extra links to reduce the diameter and number of wavelengths required. Our proposed deterministic routing algorithm avoids collisions between streams that need to transmit on the same wavelength. Contributions. We make the following key contributions: • A new all-optical architecture: QuT takes steps towards addressing the optical challenges including IL, number of wavelengths and microrings. Therefore, QuT consumes less power compared with state-of-the-art proposals; • A new deterministic wavelength routing that provides contention-free network traversal while reducing the number of wavelengths required by optimizing the optical switches; • 128-node QuT achieves power reductions of up to 23%, 86.3% and 52.7% over Spidergon, λ-router and Corona. I I . QUART ERN ARCH I T EC TUR E In this section, we present our low-power all-optical NoC that features a small number of wavelengths and simple switches that leads to low resource requirements. These features are essential to realizing optical networks in practice and scaling them to large systems. QuT is a contention-free topology; data streams intended for different destinations do not block each other. However, a given destination can only accept trafﬁc from a single source at a time; as a result, QuT requires a control network to serialize access by multiple sources to a particular destination. Both the control and data networks are optical. We ﬁrst explain the data network and optical switches. Then, we explain the WDM routing and clarify the details of QuT through an example. Finally, we explain the control network. A. Data Network QuT uses passive microring resonators, which route optical streams based on their wavelengths. Figure 1 shows a 16-node QuT network. Extending this structure to larger numbers of nodes is straightforward; the connectivity for an arbitrary N nodes is given below. QuT includes two different types of switches and three different connections between nodes which enable a WDM routing approach to communicate between nodes. The link for sending data is chosen based on the distance between the current node and the destination. Data traverses Ring links when the distance between the current node and the destination is less than N/4. Otherwise, Cross or Bypass links are used for sending data streams. Utilizing these different links reduces the network diameter to N/4 + 1. (a) Even Switch (b) Odd Switch Figure 2: Structure of QuT Switches Figure 3: Routing Example: N4 to N12, N2 to N8 of N/2 from them. In QuT, nodes with distance N/4 or N/2 from a source, have the same wavelength because we use N/4 different wavelengths. As a result, if a source wants to send data to the destination that is a distance of N/2, it should use a path that will bypass the node that is N/4 from it to prevent the data stream from being absorbed by the wrong node. Employing CµRs in odd switches prepares suitable paths for that purpose. Hence, CµRL (CµRR) in the ith optical switch is sensitive to the wavelength assigned to the even switch to the left (right) of the ith optical switch. CµRL is sensitive to λ((i+1) mod N/4) and CµRR is sensitive to λ((i−1) mod N/4) . C. QuT WDM Routing We develop a novel deterministic optical routing algorithm tailored to the QuT architecture and based on wavelength routing. The wavelength assignment coupled with the routing algorithm allows us to realize our contention-free architecture. No two distinct source-destination pairs using the same wavelength will collide in the network; if they use the same wavelength, they will take different routes. Also, using dedicated deterministic optical routing helps optimize the optical switches and reduces the number of microrings compared with non-deterministic optical routing algorithm. First, the source node chooses an injection channel (designated I#) based on the destination. Then, data is modulated onto the destination wavelengths. The optical data stream is transmitted through optical switches and links until it is ejected at the destination. There are several scenarios depending on the source and destination, listed as follows: • If the source is even and the distance between the source and the destination is: ◦ less than N/4, I2 (I3) is chosen if the destination is to the left (right) of the source. Data is sent on the ring link to the left (right) of the current switch. ◦ equal to N/4, I1 (I4) is chosen if the destination is to the left (right) of the source. Data is sent over the cross link to the left (right) of the current switch. When the distance between two nodes is divisible by N/4, these nodes have the same dedicated wavelength. ◦ equal to N/2, the source and the destination have the same dedicated wavelength. Therefore, I2 is chosen to send data over the ring link to the left of the current switch. The odd switch to the left of current switch will send the data stream over its bypass link, to bypass the node that is a distance of N/4 from the source and prevent the data from being absorbed by wrong node. Figure 4: Control Network Structure ◦ greater than N/4 and not equal to N/2, I1 (I4) is chosen if the destination is to the left (right) of the source. The cross link to the left (right) of the current switch is used. • If the source is odd and the distance between the source and the destination is: ◦ less than or equal to N/4, I2 (I3) is chosen if the destination is to the left (right) of the source. Data is sent on the ring link to the left (right) of the current switch. ◦ greater than N/4, I1 (I4) is chosen if the destination is to the left (right) of the source. Data is sent over the bypass link to the right (left) of the current switch. D. Example Figure 3 gives an example of routing in QuT. A common wavelength (λ0 ) is used for both data streams from N 4 to N 12 and N 2 to N 8; data transmission through different paths prevents optical collision. If N 2 wants to send data to N 8, it chooses injection channel I1. Therefore, data is transferred over a cross link connected to N 6. Then, the data stream is turned onto the ring link by AµR between the cross and ring links in N 6. Data is transferred through ring link without changing direction in N 7. Finally, in N 8, data turns into the ejection channel through DµR. If N 4 wants to send data to N 12, it chooses injection channel I2. Therefore, data is transferred over a ring link connected to N 5 by AµR in N 4. Then the bypass link transmits the data through CµR in N 5. In N 6, data is transferred through the cross link by BµR to N 10. In N 10, data is sent over the ring link by AµR. Data is transferred through ring link without changing direction in N 11. Finally, in N 12, data is ejected by DµR. E. Control Network QuT checks the status of the destination’s ejection channel to avoid optical stream collisions from multiple sources. To check if the corresponding ejection channel is free, we use a separate optical control network (CN). Our CN is based on a Multiple-Writer Single-Reader optical bus [13]. The CN uses three kinds of control packets: request, acknowledgment (ACK) and negative acknowledgment (NACK). In the CN, each source node has a dedicated wavelength; a node modulates a control packet on its dedicated wavelength. Therefore, a destination can accept multiple optical streams simultaneously as each will be on a different wavelength. Each source ﬁrst sends a request packet over the CN waveguide connected to the desired destination. The source must wait for a response from the destination. An ACK is sent by the destination through the CN if the requested ejection channel is available (not being used by another source in the data network). Otherwise a NACK will be sent. Once the source receives the ACK, it begins sending data through the data network. If the destination is busy, a subsequent request packet will be sent after a back-off period which is determined by the average time required for sending a packet. The CN is implemented as several waveguides (Figure 4). 16 nodes can accept data from each waveguide by utilizing splitters, which split an optical stream across different paths [14]. However, all nodes can send request packets on each waveguide. This structure helps reduce the waveguide crossings between laser power waveguide and CN waveguides. Hence, CN structure in an N -node QuT has N/16 waveguides and N wavelengths. By using more splitters on a waveguide, we can further reduce waveguide crossings. However, this increases the CN energy consumption, as a request packet is detected by more nodes per waveguide. As an example, if node 15 wants to send data to node N −2, it will modulate its control packet onto waveguide N/16 − 1. Each control packet has 6 bits: 4 bits for addressing 16 nodes and 2 bits for encoding the packet type. The receiver must have a (N − 1) × 6 bit buffer to hold simultaneous requests from all other nodes. I I I . M ETHODO LOGY We compare a QuT with 64 and 128 nodes, in terms of latency and energy efﬁciency, against three alternative optical NoCs: λ-router [15], all-optical Spidergon [16] and Corona[17]. We use PhoenixSim, an event-driven simulator implementing optical NoCs [18]. It models the performance of electrical, optical and hybrid NoCs. PhoenixSim is based on OMNet++ [19], providing a component-based C++ library and discrete-event simulation framework. λ-router [15] is an all-optical contention-free NoC. It uses passive microring resonators and routes optical streams in accordance with their wavelengths. We use an ideal λ-router which has inﬁnite receiver-side buffering to eliminate the need for an arbitration mechanism. In practice, λ-router’s performance will be limited when a more realistic buffer size is chosen. λ-router’s scalability is limited by the number of wavelengths and switches. Since an N-node λ-router uses N/2 × (N − 1) switches, the number of switches increases quadratically with the number of nodes in network. λ-router with N nodes needs N distinct wavelength sets. A non-ideal λ-router would require a control network just as QuT does to avoid collisions at the destination due to limited receiver-side buffering. We choose to compare against an idealized version which allows us to assess the cost and performance degradation that occurs with the addition of a control network. Optical Spidergon [16] uses passive optical components but has an electrical control network. We replace its electrical control network with the optical CN described in Section II-E to achieve better power and latency. In an all-optical Spidergon, each node has a dedicated but not unique wavelength, used by other nodes to modulate data. An all-optical Spidergon needs N/2 wavelength sets as each node is connected to its neighbors Table I: Insertion Loss Parameters Parameter CF toW Le Waveguide propagation loss Passing through ring loss Passing by ring loss Waveguide bending loss Waveguide crossing loss Splitter loss Receiver Sensitivity Value 1 dB [21] 5 dB [21] 1 dB/cm [22] 0.5 dB [23] 0.01 dB [23] 0.005 dB [23] 0.12 dB [24] 0.1 dB [14] -17 dBm [25] in a ring and to the node that is N/2 nodes away. We choose Spidergon to compare to QuT since it is also a ring-based topology and uses cross links to reduce the NoC’s diameter. Corona [17] uses an optical crossbar with optical tokenring arbitration to permit a node to send data. It uses a Multiple-Writer-Single-Reader optical bus; when the number of nodes increases, both the waiting time for receiving a token and the network diameter increase. Therefore, we choose the best proposed token-ring arbitration scheme, slot-tokenring arbitration, as a control network of Corona [14]. The original Corona implementation uses a lot of wavelengths (64) to modulate data packets to improve the latency; however, this requires an impractically large number of microrings (one million) and high power consumption which rapidly increases with network size. To enable a consistent evaluation, we keep a constant ﬂit-width of 1 byte across all networks. In Corona, the number of wavelengths equals the ﬂit-width. Hence, we reduce the total number of wavelengths in Corona to eight distinct wavelengths. Therefore, our implementation improves Corona’s power consumption and area by reducing the total number of microrings and waveguides, but its latency is increased. We choose Corona to compare with QuT since unlike QuT, its number of wavelengths is independent from the number of nodes, it does not need optical switches and it has a very simple architecture to implement the optical crossbar. We believe these three design points cover a range of interesting optical topologies that have been proposed in the literature. We evaluate these NoCs with 64 and 128-nodes to compare their scalability. We assume the size of the die is the same for 64 and 128 nodes (225 mm2 ). We keep the optical bandwidth constant for all-optical NoCs; we assign eight distinct wavelengths to each node. Therefore, each optical data stream is modulated on eight wavelengths assigned to the corresponding destination. However, each control packet is modulated on one wavelength assigned to the corresponding source. Since the control packets are small, the CN does not require large bandwidth. We assume a data packet size of 256 bits and 10Gb/s modulator and detector. We evaluate the topologies under the following synthetic trafﬁc patterns: random, bitreverse, neighbor, tornado and hotspot in which a random node receives 30% of all requests and the remaining 70% is uniformly distributed. We evaluate these patterns with various offered loads. Offered load (α) is Tmd/(Tmd + Tp ), where Tmd is the time that a packet takes to be transmitted through the network and Tp is the exponentially distributed inter-message gap [20]. IV. EVALUAT ION A. Delay Evaluation The total latency in an optical path includes: delay of the optical switches in the path, delay in bends and waveguide crossings, propagation delay in waveguides (11.4 µs/µm), (a) Hotspot Trafﬁc (b) Other Trafﬁc Patterns at α = 0.5 Figure 5: Average Packet Latency for 64 nodes (a) Hotspot Trafﬁc (b) Other Trafﬁc Patterns at α = 0.5 Figure 6: Average Packet Latency for 128 nodes modulator delay (23.8 ps) and detector delay (4.2 ps) [16]. We use aggressive delay parameters in 22nm. However, relative trends of our results will still hold with more conservative parameters. Also, improving modulator bit rate, e.g. from 10 to 40 Gb/s, can improve the latency, since modulating data to an optical stream is one of the major factors in latency. The total delay per packet includes the path delay through the control and data networks and the time the packet waits in the processor’s output buffer to be transfered into network. Figures 5 and 6 show the average packet latency at α = 0.5 for different trafﬁc patterns for 64- and 128-node networks, respectively. Latency is reported in processor cycles. We assume a 5 GHz clock for the processors. QuT and Spidergon must check the status of the destination to avoid contention in the ejection channel. This destination checking and the ability to only accept one packet at a destination leads to an increase in latency for QuT and Spidergon, especially in high contention trafﬁc such as hotspot. In hotspot trafﬁc, 64- and 128-node QuTs saturate at α = 0.44 and α = 0.38, respectively because a destination can only receive from one source at a time. The latency in 64- and 128-node QuT is up to 32% and 24% higher compared to 64- and 128-node Corona, respectively. Corona uses slot-token-ring arbitration to reserve a destination, which has better performance than QuT’s control network. However, it consumes more power and energy. The latency in a 128-node Corona is 11.5% higher than that of a 64-node Corona. In Corona, token waiting time and network diameter both increase as the number of nodes increases. Waiting time in a processor’s output buffer coupled with the delay associated with modulating the packet, primarily contribute to the latency in QuT. When the network size grows from 64 to 128 nodes, the average optical path latency only increases by two cycles. Altogether, QuT’s latency does not rapidly increase for 128 nodes compared to 64 nodes, assuming the packet size, the number of wavelengths used to modulate a packet and the network offered load are the same. λ-router has better latency compared to QuT since we assume that λ-router can accept data from all of the sources at a time. Buffering for one packet per source is added to each destination. Hence, each node has 4064 bytes of buffering for 128-node λ-router and only 32 bytes for 128-node QuT, Spidergon and Corona. If the buffer size in λ-router is reduced, the network latency will increase sharply. The buffering overhead is not accounted in power and energy evaluations. calculated by laserpower(λ) = −17 + I L + Le + CF toW , B. Power Evaluation Total power consumption in an optical NoC includes off-chip laser power, on-chip microring heating power and electrical to optical (E/O) and optical to electrical (O/E) circuit power. • Laser Power: Off-chip laser power is constant and independent of network trafﬁc. This power is computed from the maximum optical insertion loss in the NoC. The insertion loss is the summation of: passing through the microring loss, passing by microring loss, waveguide crossing and bending loss and waveguide propagation loss. The laser power per λ, before entering to the chip, is where −17 is the receiver sensitivity and Le and CF toW represent laser efﬁciency and coupling coefﬁcient, respectively. Finally, the total laser power is a multiplication of laser power per λ and total wavelengths in the NoC. To perform a fair comparison, we use the same optical parameters for all of the optical topologies, listed in Table I. Again, we consider aggressive parameters to show the full opportunity for power savings across these networks. More conservative parameters can increase the power consumption of optical NoCs, especially Corona. • Microring Heating Power: We use thermally-tunable microrings. Extra power helps these microrings maintain their resonance wavelength when they experience temperature variation on chip. We use 20 µW/ring, under typical conditions, when the rings in the system would experience a • Total Power: In QuT, Spidergon and Corona, the power of temperature range of 20K [26]. the CN must be factored in. Total power is the summation of total power consumption in the data and control networks. Table II: Power dissipation for optical networks with 64 Nodes Max. Optical Loss (dB) Laser Power (λ) (mW) Laser Power (mW) Ring Heating (mW) Total Power (W) QuT & CN QuT CN 16.36 17 3.44 4 440.32 256 901.12 85.76 1.69 Spidergon & CN Spidergon CN 13.78 17 1.9 4 486.4 256 1320.96 85.76 2.15 Corona & CN Corona CN 22.66 21.8 14.66 12.02 117.28 769.28 655.36 245.76 1.8 λ-Router 20.1814 8.282 4240.384 1955.84 6.196 Table III: Power dissipation for optical networks with 128 Nodes Max. Optical Loss (dB) Laser Power (λ) (mW) Laser Power (W) Ring Heating (W) Total Power (W) QuT & CN QuT CN 24.11 21 20.46 10 5.24 1.28 3.44 0.346 10.31 Spidergon & CN Spidergon CN 22.04 21 12.7 10 6.5 1.28 5.26 0.346 13.39 Corona & CN Corona CN 34.24 32.1 210.86 128.82 1.69 16.49 2.62 0.983 21.78 λ-Router 29.18 65.76 67.34 7.844 75.18 Table IV: Characteristics of Optical Topologies 64-node 128-node Num. of Wavelengths Num. of Microrings Num. of Wavelengths Num. of Microrings QuT 128 45056 256 172000 Spidergon 256 66048 512 263000 CN(QuT& Spidergon) 64 4288 128 17300 λ-router 512 97792 1024 392192 Corona 8 32768 8 131072 CN(Corona) 64 12288 128 49152 The power dissipation for various optical networks with 64 and 128-nodes are shown in Tables II and III. Although QuT has higher optical loss compared with Spidergon, this is compensated by fewer wavelengths. QuT uses 256 wavelengths, while Spidergon needs 512 wavelengths for a 128-node network (Table IV). The required wavelengths are determined such that each topology has an 8-bit ﬂit size. Microrings are sensitive to process and temperature variation. Therefore, they are a major source of faults in an optical NoC. An optical NoC should reduce the number of microrings to improve its reliability. QuT requires fewer microrings by 29.8%, 49.5%, 32.4% and 51.7% compared to a 64-node Spidergon and λ-router, and a 128-node Spidergon and λ-router, respectively (Table IV). Power consumption increases for the 128-node optical NoCs due to increases in IL, the number of microrings and the total number of wavelengths. Reducing the number of wavelengths required to modulate data can reduce the power consumption. There is a trade-off between bandwidth and power consumption in an optical NoC. Future innovation in detector sensitivity will have a positive effect on the laser power [21]. For example by using a receiver that requires minimum power of -20 dBm, laser power of 128-node QuT is 2.63W, half of the laser power with a receiver requiring minimum power of -17 dBm (Table III). Among these optical NoCs, Corona’s total number of wavelengths remains constant when the network size grows. However, Corona and its CN have the largest IL for 64 nodes among the other optical NoCs as shown in Table II. Therefore, increasing its IL, its CN IL and the number of wavelengths for 128 nodes, dramatically increases Corona’s power consumption. The IL increases because of longer waveguide length and more waveguide crossings. 64-node QuT consumes up to 21%, 73% and 6% less power than Spidergon, λ-router and Corona, respectively. However, 128-node QuT achieves power reductions of up to 23%, 86.3% and 52.7% over Spidergon, λ-router and Corona, respectively. C. Energy Evaluation Energy consumption in an optical NoC includes the energy dissipation in the laser, micro-ring heating and back-end circuitry, E/O and O/E conversion. Based on the model in Section IV-B, Figures 7a and 7b show the average energy-perbit for different optical NoCs at α = 0.5 for different trafﬁc patterns. E/O and O/E consume 100 fJ/b [21]. In QuT and Spidergon, the average energy-per-bit includes the energy consumption in the data network and in the CN for multiple request packets per data packet if necessary. Energy consumption in Corona includes the energy consumption in both the data and control network. Since QuT has lower power dissipation and a smaller average optical path delay, it has lower energy-per-bit compared to Corona and λ-router. QuT achieves an energy-per-bit reduction of up to 69%, 92% over 128-node λ-router and Corona, respectively. In Tornado trafﬁc, a large number of packets are transferred between nodes with N/2 distance from each other; for a 128-node network, the average optical path delay increases. Thus, a data stream is sent through more optical switches and waveguides in QuT. Conversely, Spidergon beneﬁts from this trafﬁc pattern, since a data stream passes through less optical switches and waveguides; it has 34% reduction in energy-perbit compared with QuT. However, QuT has fewer wavelengths and microrings and lower power consumption. For hotspot trafﬁc in QuT and Spidergon, a large number of requests are sent through the CN for a speciﬁc node. Due to destination contention, this results in a sharp increase in energy-per-bit (Figures 7a and 7b). At the saturation point in QuT and Spidergon, a small fraction of energy-per-bit is related to data network, e.g. the data network consumes 9.11% and 7.2% of the energy-per-bit in the 64-node QuT and Spidergon, respectively. Corona’s CN always consumes energy even when the data network is idle, as Corona has a slot-tokenring arbitration. Also, Corona’s data network has an order of magnitude larger energy consumption compared with the CN. Thus, the average energy-per-bit of Corona at the saturation point in hotspot trafﬁc does not increase signiﬁcantly. D. Throughput Evaluation Effectively, QuT trades off throughput for lower cost and lower power consumption. Figure 8 shows the normalized average throughput-per-watt for optical NoCs. 64-node QuT has (a) 64-node networks (b) 128-node networks Figure 7: Average Energy-per-bit. Trafﬁc Patterns (excluding hotspot) use α = 0.5. (a) 64-node networks (b) 128-node networks Figure 8: Average Throughput-per-watt normalized to QuT. Trafﬁc Patterns (excluding hotspot) use α = 0.5 lower throughput-per-watt than Corona. However, since QuT consumes less power, 128-node QuT achieves throughput-perwatt improvements of up to 43%, 28% and 85% over 128-node Corona, Spidergon and λ-router, respectively, which indicates better scalability for QuT. E. Area Evaluation Area overhead in an optical NoC includes the area required for microrings, waveguides and detectors. Table V shows the normalized area for optical NoCs considered in this work. 128node Spidergon and λ-router require 44% and 154% more optical area than QuT, since they have more microrings. λrouter uses more detectors to allow each node to accept data from different sources simultaneously. Although Corona has fewer microrings, it needs more waveguides and detectors compared to QuT. Therefore, its area is almost equal to QuT. F. Discussion QuT outperforms Spidergon, λ-router and Corona in terms of power dissipation and achieves better energy-per-bit consumption compared with λ-router and Corona. Although Spidergon slightly outperforms QuT in energy-per-bit, QuT has lower power comsumption, fewer wavelengths (reduced by a factor of 2) and microrings. Due to lower resource requirements, QuT is easier to implement. QuT also uses fewer microrings and wavelengths compared with λ-router. To have a low power optical NoC, all of the optical factors mentioned in Section I should be considered. Having a topology only with smaller IL or fewer wavelengths does not lead to the best design choice in terms of power consumption. Although QuT scales better compared to other optical NoCs, its power consumption does not scale well with node count. Therefore, without improving detector sensitivity or reducing optical bandwidth to compensate for the extra IL with increasing network size, QuT is not suitable for networks larger than 128 cores. However, if we use electrical or optical clustering, we can use QuT for systems larger than 128 Table V: Area overhead normalized to QuT 64-node 128-node QuT & CN 1 1 Spidergon & CN 1.38 1.44 λ-router 2.42 2.54 Corona & CN 1.06 1.03 cores. Electrical clustering: several cores can be clustered to connect to one optical node; e.g. a 128-node QuT with cluster size of 4 can be used to implement a 512-core system. Optical clustering: a single large optical NoC can be divided into several smaller parallel optical NoCs by using 3D stacking [21]; e.g. in 512-core system, instead of one 512-node QuT, we can use six 128-node QuTs to connect all of the cores. These optical NoCs are implemented in different optical layers to reduce the optical losses. Also, by using the electrical and optical clustering, we can connect 1024 cores with 6 parallel 128-node QuTs with electrical clustering size of 4. V. R E LAT ED WORK Silicon photonics has generated signiﬁcant interest among the research community as a solution to future interconnect obstacles. Several photonic NoCs [2][3][27][28][29][30] are implemented based on electro-optic broadband ring resonators. These optical NoCs use a circuit-switching approach and establish a path through the network before data is injected. A hybrid optical NoC combines an optical circuit-switched network for bulk message transmission with an electrical packetswitched NoC for control and short message exchange [2]. Each optical data transmission needs path setup and teardown, controlled by an electrical NoC. Thus, the performance of the electrical network and electro-optic broadband ring resonators and the use of electrical path-reservation affect the latency and power of the NoC. Large data messages can mitigate the cost of path reservation [3]; however, common packet sizes in manycore architectures are small (64-128 bytes to transmit a single cache line). Since QuT uses passive microring resonators, it has a simpler control network. The optical destination checking in QuT has signiﬁcantly lower delay than electrical path reservation resulting in superior performance compared to circuit-switching approaches. Other optical networks [13][15][16][17][21][23][26] use passive microring resonators to route data based on wavelength. These optical NoCs eliminate the path reservation phase. Passive microrings consume less power than electro-optic broadband ring resonators. Fireﬂy [13] is a hybrid hierarchical network architecture that employs a conventional electrical network for short/local data transmission and optical signaling for long/global communication. The crossbar-like optical NoC, is implemented through a Single-Writer-Multiple-Reader bus. The source node needs to broadcast a head ﬂit to notify the destination node. R-3PO [21] utilizes an optical crossbar based on a Multi-Write-Single-Read optical bus. R-3PO uses token slot as a control network. Crossbar-like optical NoCs need more optical resources such as microrings, compared with QuT, when the network size or optical bandwidth is increased. As the network size increases, the power of the network increases rapidly. A photonic Clos [26] uses pointto-point channels to consume less power. The Clos uses more optical resources such as waveguides, compared to QuT. The number of waveguides in the Clos increases quadratically with the number of nodes which may limit its scalability. LumiNOC [31] is a lower power multi-stage design whose primary stage is optical. However, unlike QuT, it is not an alloptical NoC, since it uses electrical routers in its intermediate stage. At high injection rate, its performance is limited by electrical routers. Its arbitration policy is based on optical collision detection and dynamic channel scheduling techniques, which efﬁciently reuses optical resources for power efﬁciency. V I . CONC LU S ION S Integration of on-die photonics continues to pose challenges for both technology and architecture. NoC architects must identify clever solutions to overcome these technological limitations so that the promise of low-power, high-bandwidth on-die optical communication can be realized. To address challenges such as power, the number of microrings and wavelengths, we present QuT, an all-optical NoC using passive microrings. We propose a novel deterministic wavelength routing algorithm to optimize optical switches, reduce the number of wavelengths and microrings and lower the IL. QuT consumes less power and energy which allows it to scale better than state-of-art proposals. When the network size is increased, QuT is able to achieve lower power compared to Corona, λrouter and Spidergon. ACKNOW L EDG EM EN T S The authors thank Joyce Poon, the members of the Enright Jerger research group and the anonymous reviewers for their valuable feedback and constructive suggestions to improve this work. [1] "
Bubble sharing - Area and energy efficient adaptive routers using centralized buffers.,"Edge buffers along with multiple virtual channels have traditionally been used to provide deadlock freedom guarantees in on-chip networks. The problem with such schemes is their high buffer space requirement which consumes significant power and area. In this work, we propose bubble sharing flow control to provide deadlock freedom with small, shared central buffers, eliminating edge buffers, improving buffer utilization, and decreasing router buffer requirements. The key insight involves sharing of the flit-size bubbles (free buffers) among cyclic network paths via central buffers in the router, reducing the overall router buffering space requirement. This technique effectively reconciles the trade-off between high radix and buffer space, encouraging the use of low hop count, high-radix topologies, with both deterministic and adaptive routing. Comparisons show improvement in average packet latency by 31% as compared to traditional 2VC edge buffer routers with 33% reduction in area for an 8×8 generalized hypercube topology.","Bubble Sharing: Area and Energy Efﬁcient Adaptive Routers using Centralized Buffers Syed Minhaj Hassan Sudhakar Yalamanchili School of Electrical and Computer Engineering Georgia Institute of Technology Email: minhaj@gatech.edu Abstract—Edge buffers along with multiple virtual channels have traditionally been used to provide deadlock freedom guarantees in on-chip networks. The problem with such schemes is their high buffer space requirement which consumes signiﬁcant power and area. In this work, we propose bubble sharing ﬂow control to provide deadlock freedom with small, shared central buffers, eliminating edge buffers, improving buffer utilization, and decreasing router buffer requirements. The key insight involves sharing of the ﬂit-size bubbles (free buffers) among cyclic network paths via central buffers in the router, reducing the overall router buffering space requirement. This technique effectively reconciles the trade-off between high radix and buffer space, encouraging the use of low hop count, high-radix topologies, with both deterministic and adaptive routing. Comparisons show improvement in average packet latency by 31% as compared to traditional 2VC edge buffer routers with 33% reduction in area for an 8x8 generalized hypercube topology. I . IN TRODUC T ION The state of the practice for baseline network-on-chip (NoC) routers has been the use of edge buffers, whose buffer capacity requirements are proportional to the router radix and link length (to fully utilize the link in the presence of ﬂow control delays). These buffer requirements are commonly increased through the use of virtual channels (VCs) to ensure deadlockfree routing, and further increased multiplicatively with the number of message classes, to avoid protocol deadlock [3] [7]. This results in substantial area and power devoted to buffers, up to several hundred KBs of storage for a 32-64 node NoC. These overheads mitigate the advantages of NoCs, specially with high radix routers (which have low hop count and utilize the increased wiring density of NoCs more effectively). We argue that the desirable design point for NoCs is the one with high radix, low buffer space, and supports both adaptive and deterministic routing. This paper proposes a router architecture that effectively reconciles this trade-off between radix and buffer space, using a novel combination of ﬂow control and buffering strategies. The key idea is to use shared central buffers coupled with novel wormhole-based extensions to bubble ﬂow control. Techniques for ensuring deadlock freedom have evolved from early channel dependency-based techniques to the more recent buffer management approaches, wherein progress is ensured by guaranteeing the availability of free buffer space (bubbles) in every cyclic packet path. The key insight in this paper involves the sharing of the ﬂit-size bubbles, (free buffers), among cyclic network paths via central buffers in the router, hence called bubble sharing ﬂow control. These extensions minimize the amount of buffer space that has to be reserved to ensure deadlock freedom, permitting the use of small, low-cost School of Electrical and Computer Engineering Georgia Institute of Technology Email: sudha@ece.gatech.edu central buffers without the need for edge buffers, increasing area and energy efﬁciency. The buffer size is independent of radix, but grows slowly with the number of escape paths. The result is a large reduction in the buffer space for adaptive or deterministically routed high radix NoC routers, harnessing the beneﬁts of high radix, while minimizing traditional high buffer space overheads. For example, in an 8x8 generalized hypercube, our adaptive bubble shared router decreases the average packet latency by 31% over a traditional 2VC router with 33% reduction in area. This paper makes the following contributions. • Introduces Bubble Sharing, a ﬂow control technique that extends the worm-bubble ﬂow-control [1] scheme to centralized buffer routers, reducing its buffer space. • Proposes Adaptive Bubble Sharing that enable adaptive routing with bubble sharing ﬂow control for wormhole switched networks. • Evaluates a centralized buffer router architecture that implements the bubble sharing scheme, and compares its power and performance against competing approaches. The remainder of the paper is organized as follows. Section II explains the importance of reduction in the buffer space and the recent proposals that enable it. Section III describes the bubble sharing scheme and its adaptive version. Finally, section IV compares the power and performance of our scheme with different state of the art low latency routers. I I . BACKGROUND & MOT IVAT ION A. Need for Buffer Space Reduction Traditional virtual channel based routers use edge buffers for deadlock freedom and performance optimization. This puts increased pressure on buffering requirement of an on-chip network. The problem with large buffer space is high area and static power that dominates all other components in traditional networks. Buffer storage in these networks increases with an increase in the number of ports, the number of virtual channels, the number of message classes, and the link width. Furthermore, to keep the links fully utilized, each buffer has to be large enough to hide the credit round trip delay, which becomes higher with longer links. Thus, a 5 ported router with 2VCs, used in a 2 dimensional torus, with 16 byte wide links, and 5-ﬂit input and 2-ﬂit output buffer requires (16 ∗ 5 ∗ (5 ∗ 2 + 2) = 960) ∼ 1K bytes of storage per message class. With 3 message classes (as required in MEOSI directory protocol) and an 8x8 network, the buffer space required by the network approaches 1 ∗ 3 ∗ 64 = 192KB . In addition, packet sized buffering is required in the network interface for both injection and ejection that further increases the buffer area. Fig. 1. Area distribution with different ports & VCs Figure 1 gives the breakdown of buffer vs. non-buffer area, (calculated from Orion2.0 [16]), with different ports and VCs. The four bars on the left gives the area by changing the number of ports. Input buffer size, output buffer size, and link width is 5 ﬂits, 1 ﬂit, and 128 bits, respectively. VCs and message classes are ﬁxed at 2 and 1 respectively. With small number of ports, e.g., in 2D torus, buffers constitute more than 60% of the router area. By increasing the number of ports to 10, (as required by a 4x4x4 ﬂattened butterﬂy), the buffer area increases by 2x. Note that this does not account for extra buffering required to keep the pipeline busy with longer links. The bars on the right sets the number of ports to 5 but increases the virtual channel count. Again, even with 4 VCs, buffers constitute almost 55% of the total router area. It is evident that reduction in buffer space with minimum cost in performance is desirable. In this work, we propose using shared central buffers coupled with novel variations of bubble ﬂow control to achieve this goal. B. Related Work Many recent works have addressed the buffer space reduction problem [15] [12]. The main focus has been to either share the buffers among the VCs, or reduce the number of entries per input buffer. The extreme case includes various versions of buffer-less ﬂow control that removes the input buffers altogether, by the use of deﬂection routing [10] [6]. The problem with buffer-less routers is their low saturation throughput and out of order delivery of packets. Out-of-order delivery requires greater buffering at the network interface, negating much of the advantages. Some other techniques include ﬂit-reservation ﬂow control [14] and whole packet forwarding [8] that focus on the efﬁcient utilization of the input buffers. Recently, the use of shared central buffers have been proposed to keep the buffering space largely independent of radix [5]. We have extended the ideas used in their scheme to ﬂit level, for wormhole networks, using bubble ﬂow control [11]. We will next discuss the key ideas behind centralized buffer routers and bubble ﬂow control, that target reduction in the number of input buffers and VCs, respectively. 1) Bubble Flow Control and its Variant: Bubble Flow Control (BFC) has been proposed [11] to avoid deadlocks in a packet based ring without the use of VCs. The basic idea is to ensure that at least one packet sized bubble (empty buffer space) is kept in the ring all the time, even after a new packet is injected into the ring. This is ensured locally by allowing injection in any ring only when an empty space of 2 packets is available at the input port. The problem with this local bubble ﬂow control (LBFC) is that it keeps too many empty packets in the ring. Critical Bubble Scheme (CBS) [2] was proposed to ensure that only one global empty packet is required in each ring, by marking one packet sized bubble in the ring as critical. Injection into the critical bubble is prohibited, keeping it preserved all the time. Both LBFC and CBS, however, only works for packet based networks. A wormhole-based version of CBS called worm-bubble ﬂow control (WBFC) was presented recently [1]. The idea is to mark ﬂit size bubbles in the input buffer as critical before injection, and keep a count of the marked bubbles. Once an injection port has marked enough bubbles to hold a complete packet, a new packet is allowed to enter the ring. In this way, the original critical bubbles of the ring, (inserted at initialization), will always be maintained. We extended the WBFC scheme to routers with shared central buffers, called Bubble Sharing. Both WBFC & Bubble Sharing schemes are further explained with an example in Section III-A. Fig. 2. Bubble Coloring Scheme (BCS) [17] presents bubble coloring scheme (BCS), a method to perform adaptive routing using bubble ﬂow control in packet based networks. The basic idea is to maintain a virtual ring with a critical bubble that connects all the routers of the network. This ring will always be kept deadlock free, and can be used as an escape path for adaptive routing. Consider the example of a mesh shown in Fig 2. The dotted line represents a fully connected virtual ring utilizing some channels of the network. A critical bubble is maintained in the ring using the injection/ejection rules of CBS. This bubble will allow packets in the ring to always make forward progress. Packets that are not in the ring can always contest for injection into the ring. e.g. in Fig 2, four packets are waiting on each other in a cycle. However, packet 0 is also contesting for the north port, which is part of the virtual ring. The critical bubble present in the ring will move backwards, allowing packet 0 to escape the cycle. This scheme was proposed for packet based edge buffer networks.We have used the basic idea of providing an escape path using a virtual ring to design our adaptive bubble sharing scheme for wormhole based centralized buffer routers. Section III-B provides the details. 2) Centralized Buffer Routers for High Radix Networks: [5] proposes centralized buffer routers (CBR). The basic idea is to remove the edge buffers per port, and use a central buffer to be shared among all ports. The router micro-architecture, with modiﬁcations for the current scheme presented as shaded regions, is given in Fig 5. The major components of the router are the central buffer (CB) and the crossbar, with single ﬂit input and output staging buffers. The central buffer is bypassed in case there is no conﬂict at the output port. Packets that enter the central buffer are prioritized over packets at the input buffer that arrive later. This is done by three different allocation stages that work in parallel (i.e., IBSA, CBSA, and CBA). The authors argue that because of the bypass path, only the packets that conﬂict at the output ports will enter the central buffer. This means that the probability of conﬂicts at the input port of the central buffer is low. However, if multiple packets do collide, their entry into the central buffer will be serialized. They note that the waiting time of this serial entry is very small (equal to pkt size). This allows them to keep the number of input ports of the central buffer as one, keeping its area small. This holds true for the output port of the central buffer as well. The scheme uses elastic buffer links [9] instead of credit based ﬂow control. This allows downstream packets to not wait on the credit round trip latency, avoiding holes among subsequent ﬂits, even with single ﬂit staging buffers at the input. Moreover, the control and data information in the links are split to make the bypass path latency single cycle. The design uses a variant of localized bubble ﬂow control. The idea is to inject in a ring only if there is enough space in the central buffer to drain the complete packet, and still leaves at least one ﬂit-sized bubble in the ring. This scheme was typically proposed for large radix routers, which require substantial buffering at the inputs. The shared central buffer reduces this requirement proportional to packet size and network dimension, i.e., 2*dim*PktLength+1 [5]. Our Bubble Sharing scheme took the basic CBR design and extended the WBFC idea to be used with central buffers. This reduces the buffering requirement to ﬂit or worm-bubble level. I I I . BUBBL E SHAR ING F LOW CONTROL A. Bubble Sharing with Central Buffers This section describes our bubble sharing scheme. We ﬁrst explain the key ideas of WBFC using an example and points out the modiﬁcations required to adapt it to CBRs. We organized the central buffer as dynamically allocated multiqueue (DAMQ) [15] with shared pool of small worm-bubble sized slots. Each slot has a space of 2-3 ﬂits and assigned completely to an output port; that is, once a slot is assigned to output x, all entries will be consumed by packets going to output x, until the slot is unassigned. Each slot act as a worm bubble, multiple of which can be taken by each ring. Fig. 3. Worm Bubble Flow Control (WBFC) Black & White Bubbles: Consider the example in Figure 3 with edge buffer routers. Suppose each bubble indicates an empty input buffer in the downstream router and is denoted by worm-bubble or simply bubble. Black means a marked worm-bubble and white means an unmarked worm-bubble. Let PktS WB = M/x denote packet size in terms of worm-bubbles, where M = packet-size and x = worm-bubble size. Injection: Before insertion into a ring x, a new packet ﬁrst marks the worm-bubble of the corresponding ring as black. This is shown in Fig 3a), where a packet at R4 is trying to get injected. It marks the corresponding bubble as black, and also maintains a count of the marked bubbles, shown by CntI = 1. Forward movement of ﬂits displaces the bubble along with their color backwards. Hence the black bubble at R4 will be pushed backwards to R3, leading to the reappearance of a white bubble at R4. The packet trying to get injected can again mark this bubble as black, further incrementing its count. Now consider the case when PktS WB1 worm-bubbles have already been marked for the ring, (i.e., C ntI ≥ P ktS W B − 1), and it encounters an empty white bubble. If the packet is injected now, it will occupy the space of bubbles marked by itself and the current white bubble. Hence, guaranteeing that any black worm-bubbles injected in the ring during initialization, will remain intact. This is the key idea in WBFC that has been used in our Bubble Sharing scheme. Central buffers, as explained earlier, provide a shared pool of these bubbles, instead of having 1 bubble for each ring, per router. Hence, instead of reserving a black bubble every cycle, for a particular ring, multiple black bubbles can be allocated simultaneously. A count called W hiteBubbleC nt, is maintained to keep track of the unoccupied white bubbles. Hence, if W hiteBubbleC nt + C ntI ≥ P ktS W B and there is an empty white bubble, injection can happen directly. It should be noted that during injection, C ntI is passed to the head ﬂit of the packet as done in the original WBFC. The rules for injection are given by label 4 & 6 in algorithm 1. Transit: In the original WBFC, the marked bubbles are unmarked when the packet is moving through the ring, decrementing C ntH . In case of Bubble Sharing, multiple black bubbles for a particular ring are unmarked simultaneously at a particular router. This quickly reduces the amount of black bubbles in the ring, leading to signiﬁcant reduction in injection delays as compared to WBFC. The rule is given by label 11 & 12 in algorithm 1. Ejection: If the packet does not encounter an equal amount of marked bubbles, the remaining count is passed to the ejecting router, which means that this router has already injected few black bubbles into the corresponding ring, This rule, (label 10 in algorithm 1), remains the same in both WBFC and Bubble Sharing. Grey Bubble: If multiple ports are marking bubbles in the ring simultaneously, it is possible that all the bubbles in the ring are marked without any packet being injected. This can lead to starvation. A grey bubble was introduced in original WBFC that allows packets to be injected even if the input port has not marked enough bubbles. An example of this is shown in Fig 3b), which illustrates a case when all bubbles are either occupied, or have been marked as black except for one grey bubble. Since the packet encounters a grey wormbubble, it will be injected. The details of why grey bubbles work can be found in [1]. Bubble Sharing scheme uses the grey Fig. 4. (a,b,c) Avoiding 1 ring to take all bubbles. (d) Deadlock in wormhole networks with BCS bubble rules without any modiﬁcations (label 5, 7, 8). It should be noted that the grey bubble rules require P ktS W B black and one grey bubble at initialization (label 1, algorithm 1). Furthermore, progressive movement of grey worm-bubble is necessary to ensure progress. Details can be found in [1]. Init: for each ring do Insert P ktS W B BLACK & 1 GREY bubble; For each router, assign 1 bubble as BLUE; WhiteBubbleCnt = non-assigned worm-bubbles; 1 2 3 4 if W hiteBubbleC nt > 0 and C ntI + W hiteBubbleC nt ≥ P ktS W B then assignColor(BLACK); // till CntI == PktS WB-1; HF.CntH = CntI; CntI = 0; 5 else if isC olor(GREY ) and C ntI ≥ 0 then HF.color = GREY; color = WHITE; HF.CntH = CntI; CntI = 0; 6 else if W hiteBubbleC nt > 0 and C ntI < P ktS W B − 1 assignColor(BLACK); // till CntI == PktS WB-1; don’t Inject; end Injection: then end Ejection: 9 else end end Transit: 7 if H F .color == GREY then 8 if W hiteBubbleC nt > 0 then assignColor(GREY); turnBlueToGrey; 10 CntI=HF.CntH; HF.CntH = 0; HF.color=WHITE; 11 if isC olor(BLACK ) and H F .C ntH > 0 then removeColor(BLACK); // as much as possible 12 else if isC olor(BLACK ) and H F .C ntH == 0 then bkwdDispl(BLACK); end CntI Logic: 13 if C ntI > P ktS W B − 1 then end bkwdDispl(CNTI); CntI–; Algorithm 1: Bubble Sharing Flow Control Blue Bubbles: It is possible that one ring takes all the white bubbles and starves others. Moreover, the movement of this ring can be dependent on movement of other rings, for example in xy routing, movement of rings in the x-dimension are dependent upon successful movement of rings in the ydimension. If the ring x takes all bubbles at a particular router, ring y may not be able to move through that router, which will stop ring x from moving as well, causing deadlock. The example in Fig 4a) explains the situation. A packet in R2 in ringY wants to move through R6, but all bubbles of R6 are taken by ringX. Thus ringY cannot move, although it has sufﬁcient bubbles in R10. Since ringY cannot move, a packet at R5 in ringX cannot make progress as well. The problem can be solved by introducing a blue bubble for each router in each ring. This bubble acts as a normal white bubble assigned to that ring, but as a black bubble assigned to a different ring for all other rings. This ensures that at least 1 worm-bubble slot is kept in each router for each ring, thus allowing that ring to always make progress. In the previous example, Fig 4b) shows blue bubbles for both rings. The blue bubble of ringY will allow ﬂits waiting at R2 to move forward into R6 consuming the blue bubble (Fig 4c). However, as soon as the ﬂit leaves the router, the blue bubble for that ring is reclaimed, ensuring forward movement all the time. It should be noted that to guarantee progressive movement of a grey bubble, the blue bubble should be converted to grey bubble for that ring, in case no other white or black bubble for that ring is left in that router. This also holds true in the case of ejection. The ejection rule is slightly changed to encounter the blue bubble as shown by label 9 in algorithm 1. Starvation Concern: As explained earlier, if a packet does not encounter enough black bubbles during transit, it passes the remaining count of its marked black bubbles (i.e., C ntH ) to the ejection port. However, for trafﬁc patterns with 11 communication, it is very likely that this counter keeps incrementing at a particular ejection point. This means that one router has injected most of the black bubbles in that ring. This condition can lead to starvation of other routers in the ring, which may never inject new black bubbles. We solved this problem by having a separate backward displacement signal for CntI. Every time CntI becomes greater than the packet size in terms of worm bubbles, CntI is displaced backwards, evening out the black bubbles injected by different routers in the ring (rule 13). Backward displacement of CntI means that a router, which has injected too many black bubbles in the ring, is giving those bubbles to its neighboring routers to decrease their injection delay. B. Adaptive Bubble Sharing Deadlock avoidance with adaptive routing requires three things to happen: 1) There must always be an escape path from any source to any destination. 2) Packets ejecting the escape path must be consumed. 3) All packets are guaranteed to contest for injection into the escape path. Satisfying Condition 1: The ﬁrst condition is satisﬁed by having a virtual ring with guaranteed bubbles, as used in BCS. For networks with centralized buffers, bubble sharing can be used instead of a packet-based critical bubble. A problem, however, is that the virtual ring in BCS allows the use of 180 degree turns. With wormhole networks, this can lead to deadlocks within a packet, that is, a packet going towards its minimal direction takes a 180 degree turn to enter the escape ring, and then takes another 180 degree turn towards its minimal direction, deadlocking itself. We avoided this by having two separate virtual rings going in opposite directions, and prohibit 180 degree turns. Since both escape paths are deadlock free, prohibiting one does not break deadlock avoidance guarantees provided by the other. Satisfying Condition 2: The second condition is similar to what is generally called the consumption assumption. However, since packets can leave the virtual ring without reaching their destinations, it is possible that the head ﬂit leaves the ring and gets stuck in the non-ring channels of the network, leaving body and tail ﬂits in the virtual ring. This situation is explained in Fig 4d) for edge buffer wormhole networks, in which the head H1 of a packet P1 is contesting for the escape path in the virtual ring. However, another packet P3 has occupied it, but cannot make forward progress because the tail T1 of packet P1 is stuck in its path in the virtual ring. The bubbles present somewhere else in the virtual ring cannot be used to remove the deadlock. This condition does not occur in original BCS because, in packet-based networks, when a packet leaves the ring, it always drains, i.e., there are no body and tail ﬂits to get stuck. With central buffers, we can utilize the above mentioned fact by checking a space of PktS WB in the central buffer before ejecting a packet from the ring, ensuring that it will drain completely. Hence, ejection out of the virtual ring as W hiteBubbleC nt+Y ellowBubbleC nt > P ktS W B − 1. This condition is enough for drainage of packets ejecting the ring as well (condition 2), since they are also injecting into the non-ring channels. The introduction of yellow bubbles will allow packets to take minimal non-ring channels more often, is only allowed if W hiteBubbleC nt > P ktS W B − 1 Satisfying Condition 3: The third condition is satisﬁed in edge buffer routers by allowing head ﬂits to leave only when the input buffer of the downstream router is empty (credit=input buffer size). This cannot be used with centralized buffer routers due to the presence of EB links with no credit based ﬂow control. This means that it is possible that all head ﬂits wait behind the tail ﬂits in a cycle, and are not even allowed to contest for the escape path. The solution requires a guarantee that once a head ﬂit traveling outside the virtual ring leaves the allocation stage, it will reach the head of the downstream input buffer, contesting for the escape path (if required). A simple way to guarantee this is to allow movement only when there is a space of one packet left among the white slots in the central buffer. This will ensure that the current packet drains completely, and the subsequent packet’s head reaches the top of the input buffer. However, this will put a signiﬁcant injection bottleneck in the non-ring channels, especially because the virtual ring will be eager to occupy any available white bubbles. We reserve a pool of bubbles, (let’s call them yellow bubbles), speciﬁcally for the non-ring channels, and prohibit the virtual ring to take these bubbles. Channels not in the ring are allowed to occupy from the pool of yellow and white bubbles, while channels within the ring can only take white or their own black, blue or grey bubbles. Injection in the non-ring channels is allowed as long having a signiﬁcant impact on low load latency. C. Modiﬁcations to Centralized Buffer Router Fig. 5. Centralized Buffer Router [5] The modiﬁcations to CBRs required to implement both deterministic and adaptive bubble sharing is given as shaded regions in Fig 5. As explained earlier, the DAMQ-style central buffer is organized as small worm-bubble sized slots. A color and a portno ﬁeld is added to each slot to determine the color and the ring assigned to the slot. A logic of few gates is added during the allocation and deallocation of each slot, to determine whether a slot can be assigned a speciﬁc color and ring, based on its current color and status. IBSA stage is modiﬁed to implement algorithm 1, and rules of adaptive bubble sharing. This, however, works in parallel to the allocation logic, having minimal impact in its critical path. Other modiﬁcations like CntI for each ring, CntH in each head ﬂit, progressive movement, and backward displacement signals are added similar to the original WBFC and BCS schemes. Backward displacement hardware is slightly modiﬁed to accommodate exchange of CntI signals. Overall, the modiﬁcations added a few gates, ﬂip ﬂops, and control signals, with almost negligible impact on the critical path of the router. D. Worm Bubble Coloring (WBCS) The idea of adaptive routing with single-VC wormhole networks can also be applied to edge buffer routers. We only need to satisfy the three conditions given above. Condition 1 and 3 can be satisﬁed by WBFC, and by allowing head ﬂits to leave only when credit=input buffer size. The problem of drainage from the virtual ring can be solved by providing a small packet size - worm bubble size entry central buffer. Ejection from the virtual ring is only allowed when this buffer is empty, and there is no other packet leaving any of the virtual ring. If a packet leaving the ring is detected to be stuck, it is moved to the central buffer to allow subsequent packets to contest for the virtual ring. We call this scheme as WormBubble-Coloring and used it for evaluation purposes. IV. R E SU LT S A. Simulation Setup The proposed schemes are evaluated with an in-house router micro-architectural simulator. We also developed 3 edge buffer router models for comparison: First using Worm Bubble Coloring, second using WBFC, and third using standard Duato’s protocol [4] to avoid deadlocks. Any extra VCs in all edge buffer routers use minimal adaptive routing. We used worm size of 2 ﬂits for central buffers in all our simulations. For Fig. 6. Delivered Throughput (Retired Flits/Node/Cycle) vs Average Latency (Cycles) for 4x4 Torus with different trafﬁc patterns adaptive bubble shared routers, the routing logic provides minimal adaptive paths along with the non-minimal escape rings. We prioritize minimal paths over non-minimal ones during allocation. We assume that backward displacement or progressive movement of bubbles can take place in a single cycle regardless of link delays. This can be done by having a separate network for control signals. Parametric conﬁgurations of each of the routers is given in Table I. The table also shows the abbreviations used in the results section for each of the router. Here VCx means edge buffer router with x number of VCs. Similarly, Cx y represents bubble sharing routers with x entries reserved for white bubbles, and y entries reserved for yellow bubbles. Note that the reserved slots for blue bubbles (1 per ring) are additional from the ones given by Cx y. We have assumed 1-message class and single ﬂit output buffers, with 128 bit wide links for all the routers. Injection & ejection queue size is kept to be 20 ﬂits for all cases. Router Baseline Worm Bubble Flow Control Adaptivity with Edge Buffers Bubble Sharing Adaptive Bubble Sharing Abbreviation Base VCx WBFC VCx Worm BCS VCx Bubble Share Cx Adp Bubble Cx y InBuff CBuff 2*x 2*x 2*x 1 1 0 0 4 x+8 x+y+4 SY ST EM CON FIGURAT ION S O F VAR IOU S ROU TER S TABLE I We model 4 different topologies, namely 4x4 and 8x8 torus, and 4x4 and 8x8 generalized hyper-cube (GHC), representing various degrees of high and low radix routers. The torus networks have single cycle link delays between subsequent routers. The GHC models multi-cycle links, equal to the number of routers between the source and destination, that is, the link delay for node 2 and node 3 from node 0 in xdimension will be two and three cycles, respectively. Four different synthetic trafﬁc patterns, (random, tornado, bit-reversal, and bit-rotation), are used. The packet size for synthetic trafﬁc is kept to be 6 ﬂits. All simulations are done for 10 million cycles. Application traces are taken by running 64 threaded version of PARSEC and SPLASH benchmarks with 64 cores, 16 MC conﬁguration, using an in-house simulator, with DRAMSim2 [13] as the main memory model. The traces are generated at the back side of the L1 cache, and messages are classiﬁed into read/write/coherence type requests. A reply of 6 ﬂits is generated from the destination for all read requests. Read requests and coherent messages consists of 2 ﬂits, and write messages are 6 ﬂits. This allows us to test our scheme for variable size packets as well. For area & power modeling, Orion 2.0 [16] is used. We modiﬁed Orion to accurately model centralized buffer routers (CBRs). As a conservative estimate, EB links are modeled to take 3x more leakage power and 3x more area in routing logic than non-EB links. Since CBRs have 3 allocation stages, they take 3x more area and power in arbiters. Furthermore, they have an additional crossbar output port going to the central buffer, increasing their area. The extra injection & ejection logic used for WBFC takes few gates and is ignored. Similarly, power taken by extra backward displacement signals, and extra injection and ejection logic are assumed to be negligible. The network is modeled to be running at 2GHz with Vdd = 1.0V and 45nm technology. Activity for different components such as crossbar, input and, output buffers, etc., are taken directly from performance simulations, and fed as activity of different components to Orion. B. Performance with Synthetic Trafﬁc 2D Torus Topologies: Figure 6 compares throughput and average packet latency of various schemes in a 4x4 torus network. The performance of single-VC WBFC and single-VC Worm-BCS is low. This is because of the injection limitations required to enter a ring. However, with 2VCs, the performance increase signiﬁcantly, more than the baseline 2VC router for most trafﬁc patterns. The reason lies in the availability of more non-ring channels that does not suffer the injection limitation. Bubble Sharing gives the best result both in terms of low load latency and saturation throughput. Reduction in lowload latency is attributed to elastic links and single cycle pipeline delay in the router. Furthermore, having multiple bubbles per ring reduces the injection delay, which results in improved throughput as compared to WBFC. Adaptive bubble sharing suffers performance loss, both in terms of no-load latency and saturation throughput, because of the ejection limitations discussed in section III-B. Since, there are a few number of non-ring channels, ejection limitation does not allow packets that have already entered the ring to leave it, limiting the throughput equal to the throughput of the virtual ring. However, the minimum number of buffers required is extremely low, even with respect to bubble shared router. This is explained in section IV-C. 8x8 Torus topology shows similar results (not shown due to space constraints). 2D GHC Topologies: Figure 7 shows the same result with a 4x4 GHC. In this case, the ejection limitation is reduced because of the high number of non-ring channels available. Furthermore, prioritizing minimal hops reduces the low-load latency of the adaptive bubble sharing router, as well as its saturation throughput. Both edge buffer routers suffer due to the presence of credit base ﬂow control. Figure 8 compares throughput and average packet latency with an 8x8 GHC. It can be seen that the adaptive bubble scheme surpasses others by a large margin. Again, this is because of the high number of Fig. 7. Delivered Throughput (Retired Flits/Node/Cycle) vs Average Latency (Cycles) for 4x4 GHC Fig. 8. Delivered Throughput (Retired Flits/Node/Cycle) vs Average Latency (Cycles) for 8x8 GHC non-ring channels available along with EB links. Furthermore, the buffering requirement per router did not change, even with high number of ports. Extremely low no-load latency is also achieved due to the presence of EB links, priority for minimal hops, and low hop count. We conclude this section by making the following remarks. For low radix networks, adaptivity using bubbles does not help. In such a case, deterministic bubble sharing scheme performs the best with reduced buffer space due to sharing. However, with high radix topologies that have many routes to destination, adaptivity provided with bubbles signiﬁcantly improves performance without increasing the buffer area. C. Buffer Space Analysis and Impact on Area Edge buffer routers requires at least 1 worm-sized entry per input port per VC. Worm BCS also requires a packet size - worm size central buffer for minimum operation. WBFC requires PktS WB Black and 1 Grey bubble to be injected per ring at initialization. For a 6 ﬂit packet with bubble size of 2, this means four bubbles per ring. The total number of rings in a 4x4 torus are 16, four in each direction. Thus, 4x16=64 bubbles will be injected at initialization. Edge buffer routers use input buffers to provide these bubbles. With central buffers, minimum number of entries required by each router is 64/16=4 bubbles. Assuming that we have at least 1 white bubble per router at initialization, the number of entries required at each router = 5 worm-bubbles or 10 ﬂits. Furthermore, each router requires 1 blue bubble per ring making the minimum central buffer size to be 18 entries for routers with bubble sharing. It should be noted that with an 8x8 torus, this will be reduced to 12 entries per router. In the adaptive bubble sharing case, the 2 virtual rings require 4*2=8 bubbles anywhere in the network, in addition to the 2 blue bubbles per router. This makes the minimum requirement with adaptive bubble sharing to be 6-8 entries per router. Table II gives the total buffer space requirements of different routers with different conﬁgurations. The formula for calculating the buffer space is [P ∗ (I ∗ V C + O) + C ] ∗ L, where L is the link width, P is the number of ports, I , O , C is number of ﬂits in input, output and central buffers, respectively. For 2Dring topologies, the 2VC baseline and WBFC routers require 400 bytes per router. Worm BCS requires slightly more with a small central buffer. Since, worm size is only 2 ﬂits, and we have 1 ﬂit staging buffer in the centralized buffer routers, the total buffer space in bubble sharing router with central buffer entries of 20 is high. However, the buffer size increases very slowly with increase in the number of ports and gets extremely low, e.g., for 8x8 GHC. The adaptive bubble sharing router will always have smaller buffering requirement than others because of its small entry central buffer. Furthermore, its throughput with large number of adaptive channels is high, making it an ideal candidate for high-radix routers. RowNo 1 2 3 4 5 6 7 Parameter Baseline VC2 WBFC VC2 Worm BCS VC2 Bubble Share C10 Bubble Share C12 Adp Bubble C4 2 Adp Bubble C4 6 2D-Torus 4x4-GHC 8x8-GHC 400 400 464 448 480 320 384 560 560 624 512 544 384 480 1200 1200 1264 768 800 640 704 BU FFER S PAC E P ER ROU TER (BYT E S ) W I TH D I FFEREN T CON FIGURAT ION S TABLE II Fig 9a) gives the area distribution of a 4x4 Torus and an 8x8 GHC router with different conﬁgurations. Baseline has a large area in the input buffer for both 4x4 and 8x8 routers even with a single message class. The input buffer area for bubble sharing routers is low. However, central buffer takes a signiﬁcant amount of area as well. In 2D-Torus, this area dominates over other parts making bubble shared routers 3% more expensive than the baseline 2VC router. However, with adaptive bubble sharing and small buffers, the area decreases by 27% making it the cheapest. Furthermore, with high radix, such as in GHC, the increase in central buffer area is very low, as compared to area requirements of multi-VC input buffers and crossbars, thus reducing the area of adaptive bubble router by 46% and 52% compared to the Base VC2 and WormBCS VC2 routers, respectively. Note that the crossbars are conﬁgured as input channels X output ports, that makes their area larger depending upon the number of VCs. Since centralized buffer routers do not have VCs, their crossbar area does not increase as signiﬁcantly as edge buffer routers. D. Power Analysis Figure 9b) compares the static and dynamic power dissipation of different routers at low loads conﬁgured in a 4x4 torus and 8x8 GHC topology. At low loads, most of the router power Fig. 9. Router Area & Power with different conﬁgurations a) Area Distribution b) Low Load Power Distribution c) High Load Power Distribution Fig. 10. Real Application Results. (a) Normalized average packet latency for 8x8GHC. Throughput per unit power for (b) 8x8GHC (c) 8x8Torus is static, with very small dynamic power in the buffers. The static power of the bubble shared router is 24% lower than the baseline in 4x4 torus topology. This is due to the presence of a smaller crossbar (single-VC) in it. Adaptive bubble shared routers further reduces it to 32% & 41% for 4x4 torus and 8x8 GHC, respectively, because of their smaller central buffers. At high loads, as shown in Figure 9c), buffers take a signiﬁcant portion of the overall dynamic power, with reduced distribution in links & crossbars. The highest for torus topology is taken by bubble sharing routers because of the largest buffer size. However, the adaptive bubble sharing case, as can be seen with the GHC topology, requires the least amount of dynamic buffer power even with the highest throughput. E. Results with Real Benchmarks Fig 10a) gives the average packet latency of various schemes with an 8x8 GHC topology normalized with respect to the baseline 2VC router. As can be seen, adaptive bubble sharing consistently gives lower latency across a range of benchmarks. The percentage improvement on average is 31%. Similar results can be seen in Fig 10b) showing throughput per unit power. Average percentage improvement in this case is 41%. With 8x8 torus (Fig 10c), although adaptive bubble sharing did not perform as good, our bubble sharing scheme outperforms all other schemes with an average improvement in throughput per unit power by 13% and 25% compared to Base VC2 and WBFC VC2, respectively. Average packet latency results for an 8x8 torus (not shown) are similar. V. CONC LU S ION S This paper addresses the buffer space reduction problem in on-chip networks by proposing to use variants of bubble ﬂow control in centralized buffer environment, specially for high radix networks. A wormhole-based version of the bubble coloring scheme is also presented to provide adaptivity without the use of VCs in centralized buffer routers. The routers presented use less buffering, with lower power and improved throughput, as compared to traditional multi-VC edge-buffered routers. Our results indicate an average improvement of 41% in throughput per unit power for PARSEC and SPLASH benchmarks, conﬁgured in an 8x8 GHC topology. Future work includes the design of better virtual rings to reduce the latency of non-minimal escape paths and allowing separation of ﬂows for different message classes within the rings. ACKNOW L EDG EM EN T S This research was supported in part by the National Science Foundation under grant CNS 0855110 and Sandia National Laboratories. We also acknowledge the detailed and constructive comments of the reviewers. "
Hermes - Architecting a top-performing fault-tolerant routing algorithm for Networks-on-Chips.,"Networks-on-Chips (NoCs) are experiencing escalating susceptibility to wear-out and reduced reliability, with the risk of becoming the key point of failure in an entire multicore chip. In this paper we propose Hermes, a highly-robust, distributed fault-tolerant routing algorithm, whose performance degrades gracefully with increasing faulty NoC link counts. Hermes is a deadlock-free hybrid routing algorithm, utilizing load-balanced routing on fault-free paths, while providing pre-reconfigured escape routes in the vicinity of faults. An initial experimental evaluation shows that Hermes improves network throughput by up to 2.2× when compared against the existing state-of-the-art.","Hermes: Architecting a Top-Performing Fault-Tolerant Routing Algorithm for Networks-on-Chips Costas Iordanou† , Vassos Soteriou† , Konstantinos Aisopos‡ , Elena Kakoulli† †Department of Electrical Eng., Computer Eng. and Informatics ‡ Microsoft Corporation Cyprus University of Technology WA 98052, USA vassos.soteriou@cut.ac.cy kaisopos@microstoft.com Abstract—Networks-on-Chips (NoCs) are experiencing escalating susceptibility to wear-out and reduced reliability, with the risk of becoming the key point of failure in an entire multicore chip. In this paper we propose Hermes, a highly-robust, distributed fault-tolerant routing algorithm, whose performance degrades gracefully with increasing faulty NoC link counts. Hermes is a deadlock-free hybrid routing algorithm, utilizing load-balanced routing on fault-free paths, while providing prereconﬁgured escape routes in the vicinity of faults. An initial experimental evaluation shows that Hermes improves network throughput by up to 2.2× when compared against the existing state-of-the-art. I . IN TRODUC T ION At deep sub-micron CMOS scales on-chip components become increasingly susceptible to permanent faults [5]. Networks-on-Chips (NoCs), employed as the communication medium in Chip MultiProcessors (CMPs) [7], are no exception to this norm. In NoCs even an isolated intra-router link failure, as a consequence of the harmful physical effects of electro-migration [2], can turn a static regular topology into an irregular one. Hence, either physical connectivity among routers may not exist at all, and/or the associated routing protocol may not be able to advance packets to their destinations due to protocol-level breakdown; back-pressure then causes accumulated congestion, possible trafﬁc deadlocks, and even the entire multicore system to stall indeﬁnitely rendering it inoperable. Hence an NoC failure, can become the entire multicore chip’s single fatal failure. To achieve seamless NoC operation in the presence of faulty links we propose Hermes, a top-performing and highly robust distributed Fault-Tolerant (FT) routing algorithm that bypasses faulty network paths/regions/areas, at a gracefully degrading mode with increasing faulty link numbers. Hermes employs a dual-strategy routing function that is aware of the current topological region’s state: (1) with the NoC topology being entirely healthy, or in the region(s) where only healthy links exist, by default, Hermes utilizes either DimensionOrder Routing (DOR-XY) or load-balancing O1TURN routing [6] to sustain high throughput, while (2) it provides pre-calculated escape path selection in the vicinity of faulty links, with routing information distributed in tables at each router. To achieve the latter, it employs up*/down* routing, a topology-agnostic strategy that can be tailored to be deadlock-free, ideal for achieving fault tolerance in NoCs. Hermes is geared towards 2D mesh-interconnected NoCs. With new faulty link(s) appearing in the topology, any existing up*/down*based routes are no longer valid and new ones have to be calculated to replace them. As such, the network is frozen, and using topologyscanning ﬂags that spread synchronously from the new faulty point, analogous to Ariadne’s elegant re-conﬁguration scheme [1], Hermes discovers the new NoC inter-connectivity and updates each table with new route information. Once completed, the NoC resumes operation until a new faulty link(s) appears, where the process is repeated. A key and distinct contribution, is that operating routing functions that Hermes encompasses enable it to achieve signiﬁcantly higher and linearly-degrading performance with the two synergistically increasing fault counts vs Ariadne which merely uses up*/down* routing in any unhealthy topology. Further, Hermes establishes that Virtual Channel (VC) classiﬁcation with regard to the underlying health state of the routing path offers far-improved performance against non-classiﬁed use of VCs, i.e., VCs being used at will by a single routing scheme whether traversing faulty or healthy links, as demonstrated in the experimental evaluation results of Section IV. Hermes possesses the following attributes, ideally found in a well-designed FT routing protocol [3], [8]: (1) it establishes faulttolerance bypassing a high number of faulty links that can form any fault region with no faulty link spatial placement restrictions and/or healthy link victimization. Given a realistic minimally-connected path, Hermes maintains feasibility in packet delivery given any physical connectivity. Hence, Hermes adapts to the state of the topology, under any spatial permutation of healthy and faulty links; (2) it is deadlock-free, where no deadlocked situation exits to halt the ﬂow of packets, while establishing short routes, devoid of livelocks; (3) it is distributed, where each node individually directs its packets towards their next-hop router; (4) it offers load-balancing in faultfree regions to extend throughput; (5) it is lightweight requiring small area and power overheads; (6) it can handle run-time faults. I I . H ERM E S ROU T ING A LGOR I THM With any NoC link becoming faulty, Hermes’ reconﬁguration process is initiated to discover and mark all faulty links in the newly formed irregular topology, and then update routing tables distributed at each node, with stored information used to steer trafﬁc. Following Ariadne’s reconﬁguration scheme [1], route discovery depends on atomic ﬂag broadcasts conforming to up*/down* rules. Up*/down*, originally used in topology-irregular NoWs [4], is hereby adapted to mesh NoCs [7]. It was chosen as it is topology-agnostic and ensures acyclic path formations devoid of deadlocks. Once the tables are updated, NoC operation resumes until a new faulty link(s) appears. A. Routing in a Non-Faulty Topology Region Hermes employs a dual routing strategy. In a healthy topology or fault-free region encountered at packet injection, Hermes uses either DOR-XY, dubbed as H-XY, or partially adaptive O1TURN [6] routing, named H-O1T, where upon NoC ingress a packet is granted a 50% probability in utilizing either XY or YX routing as a means of balancing network load. At minimum, H-XY uses 2 VCs, while HO1T uses 3 VCs. Upon encountering a faulty link, routing switches to pre-conﬁgured up*/down* rules exclusively, outlined next. B. Routing in a Faulty Topology Region: Reconﬁguration Algorithm A packet switches to up*/down* routing (from XY or O1TURN) and occupies its associated VC, only when it encounters a faulty link in its path, and keeps following its rules until its ejection to ensure deadlock-freedom. The reconﬁguration process begins upon the detection of a new faulty link(s), at which point the router to which this link is connected becomes the root node (initiator). It begins broadcasting Direction-Recoding Flags (DRF) to all of its output ports connected to a healthy link that lead to associated nexthop routers to discover the topology’s connectivity. The concept is that, upon reception of these DRF ﬂags, the next-hop neighboring routers record into their routing tables the healthy input port through which the localized DRF had arrived so as to designate the direction (port) that leads them back to the root (broadcasting) node. DRF ﬂags are spread using a 1-bit overlay control network to ensure allto-all node ﬂag transmission/reception. Once a root node completes reconﬁguration, the remaining nodes individually become root nodes, so that all nodes will eventually discover how to reach each other. Each node carries a 4-stage reconﬁguration process, requiring control information bookkeeping and handling of reconﬁguration orchestration among routers, analogous to that of Ariadne [1]): (action 1) entering recovery, where a node invalidates existing routing tables and freezes the ﬂow of trafﬁc, (action 2) tagging link directions, where the up*/down* scheme marks all adjacent links of a node as either “up” or “down”, (action 3) routing table update, where information is recorded regarding which port can be used to reach the current broadcasting node, and (action 4) reconﬁguration ﬂag forwarding, where a DRF(s) is forwarded to the neighboring node(s). Synchronization is achieved as each node deterministically takes N clock cycles to broadcast its ﬂags in an N -node NoC, where the order of broadcasting nodes is based on simple modulo arithmetic which correlates each node’s topological coordinates to the NoC’s global clock [1]. The entire process completes in exactly N 2 cycles. I I I . H ERM E S M ICRO -ARCH I T EC TUR E Hermes requires minimal hardware augmentations to a base NoC router. Its routing logic unit is divided into three blocks: up*/down*, DOR-XY, and DOR-YX, each utilizing its own independent VC. HXY utilizes the ﬁrst two blocks, while H-O1T utilizes all blocks. The up*/down* block is directly connected to Hermes’ logic unit which provides the table-contained routing information, and is utilized when the current packet being routed has encountered a faulty link along its path. These routing tables are updated every time a new faulty link(s) appears, dictated by the reconﬁguration process (see Section II-B). The VC allocator is responsible in assigning a packet either to the VC governed by up*/down* when a packet is being/has routed in a faulty region; otherwise XY’s VC is assigned under H-XY, or either of XY or YX are assigned in H-O1T, when routing in a healthy region. Next, Hermes requires a 1-bit Status Register (SR) to hold the current state of the network: either “recovering” to indicate that the network is currently stalled and that the routing tables are being reconﬁgured, or “normal” to signify regular network operation or a resumption of regular operation with routing table reconﬁguration completion. Additional requirements include four 1-bit registers, one at each port, to designate “up” or “down” marking during up*/down* rules-driven reconﬁguration, the logic to update the SR accordingly (action 2), to ﬁll up the routing tables (action 3), and to forward the DRF ﬂags (action 4). Next, eight 1-bit width wires serve ﬂag reception and forwarding at each router. Finally, in any N -node NoC, the hardware which identiﬁes the current broadcasting node utilizes a set of 2l og2 (N ) bits, with the N ﬁrst least signiﬁcant bits used to sync the current broadcasting cycle of each node to the NoC’s global clock, while the next l og2 (N ) higher bits are used to identify the current ﬂag-broadcasting router node. IV. EX P ER IM EN TA L S E TU P, EVA LUAT ION AND R E SU LT S Our cycle-accurate simulation framework supports 2D meshes with classic four-stage pipelined routers [3], each with one to three 6-ﬂit 0 0.05 0.1 0.15 0.2 Injec t ion rate (flits /node/cyc le) 0.25 40 60 80 100 1 faulty link A e v r e g a e n t w o r k l a t y c n e ( s e c y c l ) 0.05 0.1 0.15 Injec t ion rate (flits /node/cyc le) 40 60 80 100 3.12% faulty links A e v r e g a e n t w o r k l a t y c n e ( s e c y c l ) 0.05 0.1 Injec t ion rate (flits /node/cyc le) 0.15 40 60 80 100 5.36% faulty links A e v r e g a e n t w o r k l a t y c n e ( s e c y c l ) 0 0.02 0.04 0.06 0.08 0.1 Injec t ion rate (flits /node/cyc le) 40 60 80 100 10.27% faulty links A e v r e g a e n t w o r k l a t y c n e ( s e c y c l ) ARIADNE (1 VC) ARIADNE (2 VCs) ARIADNE (3 VCs) H-XY (2 VCs) H-XY (3 VCs) H-O1T (3 VCs) Fig. 1. Latency-throughput curves under uniform random trafﬁc. buffered VCs per input port. All link faults are randomly assigned, while maintaining full topological connectivity, with 50 experiments, each run for a million cycles, repeated at each point to even-out idiosyncratic ﬂuctuations, with results averaged. A. Results With Synthetic Uniform Random (UR) Trafﬁc We utilize UR trafﬁc with six-ﬂit 128-bit packets. Under all four faulty link scenarios and respective VC usage, presented in Fig. 1, both Hermes variants, H-XY and H-O1T, consistently outperform Ariadne, in terms of higher sustainable throughput and lower latency as well, as Ariadne’s healthy links victimization in pursue of deadlock-freedom following the up*/down* rules causes longer path traversals in the NoC topology. H-O1T constantly outperforms HXY due to its load-balancing capability. With all algorithms utilizing 3 VCs per port and under a single faulty link, H-O1T offers 7.7% and 225% higher throughput versus H-XY and Ariadne, respectively. Under the densest faulty link population, i.e., at 10.27% or 23 faulty links in the NoC, Hermes’ advantage over Ariadne is limited as it also begins to be dominated by up*/down* routing as in the case of Ariadne, causing longer packet path traversals. Hermes’ performance, notably degrades gracefully at higher faulty link intensities. V. CONC LU S ION S We presented Hermes, a distributed and deadlock-free FT routing algorithm with graceful performance degradation. Hermes is a hybrid approach: it balances trafﬁc onto fault-free paths, while it provides pre-conﬁgured escape path selection in the vicinity of faults. "
Extending bufferless on-chip networks to high-throughput workloads.,"Bufferless networks-on-chip (NoC) has been proposed to reduce network cost by removing router input buffers and improve energy-efficiency. However, bufferless NoC has some limitations that include lower network throughput caused by deflection routing at high load. In addition, the longer router critical path impacts the router frequency, which reduces the amount of bandwidth provided by the network router. These limitations reduce any benefit of bufferless NoC - especially for high-throughput workloads such as GPGPU. In this work, we first provide a simple analysis into how the benefit of bufferless NoC can be reduced for high throughput workloads, especially in terms of energy-efficiency. We then propose clumsy flow control (CFC) - a congestion control mechanism that can reduce the amount of deflection and improve the efficiency of bufferless NoC. The clumsy flow control enables the allocation to be simplified and we propose a novel switch allocation (randomized-deterministic allocation) which significantly reduces the router critical path. The combination of these two techniques result in our bufferless NoC to exceed the system performance of buffered network by approximately 7% (up to 22%) while reducing network area by 53% and energy by 52%.","Extending Bufferless On-Chip Networks to High-Throughput Workloads Hanjoon Kim, Changhyun Kim, Miri Kim, Kanghee Won and John Kim Dept. of Computer Science, KAIST Daejeon, Korea {hanj, nangigs, mrkim11, snowstorm, jjk12}@kaist.ac.kr Abstract—Bufferless networks-on-chip (NoC) has been proposed to reduce network cost by removing router input buffers and improve energyefﬁciency. However, bufferless NoC has some limitations that include lower network throughput caused by deﬂection routing at high load. In addition, the longer router critical path impacts the router frequency, which reduces the amount of bandwidth provided by the network router. These limitations reduce any beneﬁt of bufferless NoC – especially for high-throughput workloads such as GPGPU. In this work, we ﬁrst provide a simple analysis into how the beneﬁt of bufferless NoC can be reduced for high throughput workloads, especially in terms of energy-efﬁciency. We then propose clumsy ﬂow control (CFC) – a congestion control mechanism that can reduce the amount of deﬂection and improve the efﬁciency of bufferless NoC. The clumsy ﬂow control enables the allocation to be simpliﬁed and we propose a novel switch allocation (randomizeddeterministic allocation) which signiﬁcantly reduces the router critical path. The combination of these two techniques result in our bufferless NoC to exceed the system performance of buffered network by approximately 7% (up to 22%) while reducing network area by 53% and energy by 52%. IN TRODUC T ION I . With the increasing number of cores, networks-on-chips (NoC) are an important aspect of future manycore processor architecture. In this work, we leverage bufferless networks [3] and explore how it can be leveraged in manycore accelerator architectures such as GPGPUs which can inject signiﬁcantly large number of requests and require a high-throughput network. While bufferless NoC can provide beneﬁts such as lower cost because of no input buffers, one challenge with bufferless NoC is the performance degradation as the network load increases. The additional amount of deﬂection can actually reduce the energy beneﬁt of bufferless router, compared with buffered router. To overcome this, we propose clumsy ﬂow control and a new allocation algorithm (randomized-deterministic allocation) for bufferless NoC such that it can scale to high-throughput workloads. Flow control in interconnection networks deﬁnes how the shared network resources are utilized, especially when contention occurs for shared resources [8]. A commonly used ﬂow control in NoC is buffered ﬂow control, as buffers decouple the allocation of channel bandwidth resources. For example, if two packets contend for the same output port in the same cycle, one packet is granted access to the output port channel bandwidth while the other packet is temporarily buffered. Buffered ﬂow control needs to communicate the availability of buffers in neighboring routers and credit-based ﬂow control is commonly used in buffered ﬂow control. Recently, bufferless ﬂow control and deﬂection routing [3] has been extended to NoC to reduce the cost of the network [16], [13], [11] including area and power. Bufferless ﬂow control removes on-chip router buffers, but additional mechanisms are needed when contentions occur. Packets can either be misrouted or be dropped and retransmitted. However, it has been previously shown that a bufferless network results in lower network throughput without input buffers and is not suitable when the network load increases. To overcome the limitations, we propose to introduce ﬂow control into bufferless networks that we refer to as clumsy ﬂow control (CFC) [14]. CFC is based on conventional creditbased ﬂow control but since there is no router input buffers, CFC is destination-based because the credit information is based on the packet’s destination and not on the intermediate per-hop routers. CFC is also an approximate or clumsy ﬂow control because exact buffer information is not necessarily required. By throttling network trafﬁc, CFC minimizes network congestion and improves overall performance while improving the efﬁciency of the bufferless NoC. We extend static CFC by introducing dynamic CFC where the amount of credit assigned to each core is not static but varies during the execution of the program. Another challenge with bufferless routing is maintaining low critical path in bufferless router. GPGPU workloads require high throughput (i.e., high network bandwidth). Since the network bandwidth is proportional to w × f where w is the channel width and f is the network frequency, signiﬁcantly wider channel would be required for bufferless router with high critical path to provide high network bandwidth. In this work, we propose randomized-deterministic allocation (RDA) that signiﬁcantly reduces the router allocation complexity and while also enabling a single-cycle bufferless router. In summary, the contributions of this work include the following. • We show how the performance of bufferless network can be signiﬁcantly degraded for high throughput workloads. We provide an analysis on the impact of deﬂection routing and show how the energy beneﬁts of bufferless can be signiﬁcantly degraded with high amount of deﬂection. • We propose a clumsy ﬂow control (CFC) – a credit-based ﬂow control that reduces network congestion and reduce the amount of additional hop count from bufferless routing. • We extend baseline static CFC to dynamic CFC to dynamically vary the amount of credits that each core is assigned and further increase performance. • By leveraging CFC, complex allocation in the bufferless router is not necessary (with minimized amount of deﬂection). We exploit this to propose a novel randomized-deterministic allocation (RDA) for the switch that signiﬁcantly reduces the router critical path with minimal loss in performance. I I . BU FFER LE S S ROUT ING L IM I TAT ION S A. Evaluation Methodology We evaluate bufferless NoC for manycore accelerator architectures such as GPGPU, a massively multi-threaded, throughput-oriented architecture. The NoC for such architectures needs to support high throughput. The GPGPU architecture that we assume has two separate trafﬁc classes – request and reply trafﬁc. We use two separate networks in parallel, with one network for request trafﬁc and another network for reply trafﬁc, in order to avoid protocol deadlock. The baseline bufferless router is a deﬂection-based bufferless router microarchitecture, similar to what was proposed earlier [16]. For synthetic workload, we use a cycle-accurate network simulator [8]; for the different GPGPU workloads, we use the GPGPU-Sim [2] simulator. The conﬁguration for GPGPU-sim is listed in Table II and other parameters are similar to what was used in [1] – we use a 6×6 Shader Cores Max Threads / Core L1 Cache / Core NoC Topology Flit Size (B) Virtual Channels(buffered NoC) Virtual Channel Buffer Size(Flits) DRAM Chips Memory Controllers DRAM Chips / MC DRAM Queue Size Memory Scheduler 28 1024 16KB 2D Mesh 8 4 8 16 8 2 32 FR-FCFS TABLE II. M ICROARCH I T EC TUR E PARAM ET ER S (a) (b) Fig. 1. (a) Latency-throughput curve comparing buffered and bufferless network with uniform random trafﬁc and (b) latency breakdown with bufferless. Benchmark Label Fast Walsh Transform Scalar Product Speckle Reducing Anisotropic Diffusion Breadth-First Search Nearest Neighbor Back Propagation Weather Prediction fwt sp srad bfs nn bp wp MUMmerGPU mum lib blk LIBOR Monte Carlo BlackScholes Sparse-matrix /dense-vector multiplication Fast Fourier Transform Suite CUDA SDK [17] CUDA SDK [17] Rodinia [7] Rodinia [7] Rodinia [7] Rodinia [7] 3rd Party [2] 3rd Party [2] 3rd Party [2] 3rd Party [2] Input problem size 8M data points 256 vertor, 4K vector length 2048 × 2048 data points 1M input nodes 10K data points 64K input nodes 3 × 4K metrix 1.6M queries, 76M citeerences 4K path 2M options 49 × 146K metrix 4K non-zero entries in each row 256 vector, 1K vector length spmv Parboil [18] fft Parboil [18] TABLE I. A P P L ICAT ION S U S ED 2-D mesh network with staggered memory controller placement and 8 bytes channel size for each network. For the buffered baseline, we assume 8 buffer entries and 4 VCs per port and XY routing. DSENT [21] was modiﬁed to measure the energy consumption of both buffered and bufferless NoC. Applications that we used are listed in Table I. We selected applications with different characteristics – including memory bandwidth requirements, and ratio of read and write requests – in order to show the impact of bufferless architecture across a wide range of applications. For long packets consisting of multiple ﬂits, they are truncated into single-ﬂit packets and a reassembly buffer is needed at the endpoints to re-assemble the packet, because ﬂits can arrive out-of-order. There are two types of long packets – write requests and read reply packets. Similar to [10], we use the MSHR (Miss-Status Handling Registers) structure as reassembly buffers for read reply packets, while the memory queues at the memory controllers are used for the write request packets. B. Bufferless NoC Limitations The network-only latency-throughput curves of bufferless and buffered NoC of a 2D mesh network are shown in Figure 1(a) for uniform random trafﬁc. The throughput of the bufferless NoC is limited compared with that of buffered routers, similar to what has been previously observed [16]. The performance of the buffered router will vary depending on the number of virtual channels and the amount of input buffers; however, for bufferless, the throughput is limited as load increases because of the increase in the amount of deﬂection. The bufferless network latency is broken down into the source queuing latency – i.e., the queuing latency at the source node – and the network latency in Figure 1(b). The network latency increases as load increases but near saturation, the source queuing latency signiﬁcantly increases (a) (b) Fig. 2. (a) Performance comparison of GPGPU workloads between buffered and bufferless and (b) amount of deﬂection with bufferless routing. – i.e., packets cannot be injected into the network as deﬂected packets in the network cause congestion. The impact of bufferless routers across different GPGPU workloads is shown in Figure 2(a). For a few applications, bufferless NoC actually improves overall performance by a few percent as the deﬂection routing introduces adaptivity and load-balances the network. However, for other workloads, it can result in up to 50% reduction in performance. On average, the bufferless NoC resulted in a 19.2% reduction in performance compared with that of baseline buffered NoC. Figure 2(b) shows the additional hop count that results from bufferless network. On average, the increase in the hop count is approximately 5× and can be as high as 18 for some workload. C. Energy Trade-off of Bufferless NoC In order to achieve energy efﬁciency with bufferless routing especially at high load, the amount of deﬂection needs to be signiﬁcantly reduced. The energy consumed in a buffered network (Eb ) and bufferless network (Ebl ) can be approximated with the following equations, Eb = Havg b (Er b + Ec ) Ebl = Havg bl (Er bl + Ec ) (1) where Havg b (Havg bl ) Average hop count in buffered NoC (bufferless NoC) Er b (Er bl ) Per-hop router energy in buffered NoC (bufferless NoC) Ec Channel (or link) energy per hop These values are for a single ﬂit and Havg bl includes additional hop count from deﬂection routing. The different components that consume energy within a router (e.g., buffer, crossbar, etc) are lumped into Er b (Er bl ). In order for bufferless NoC to be more energy efﬁcient than buffered NoC, the following relationship needs to hold. Eb Ebl > 1 (2) Our evaluation of the GPGPU workloads show that the γ can reach a value as high as approximately 16, and on average, is approximately 5 (Figure 2(b)). This analysis is also consistent with prior work [15], which showed that bufferless networks are not necessarily as energyefﬁcient buffered network. In order to achieve energy efﬁciency with bufferless routing especially at high load, the amount of deﬂection needs to be signiﬁcantly reduced. In the following section, we describe the clumsy ﬂow control that minimizes network congestion and enable bufferless NoC to be used with minimal impact on overall performance. I I I . C LUM SY F LOW CON TROL (CFC ) A. CFC Overview In credit-based ﬂow control, each upstream router maintains a credit that represents the number of unoccupied or free downstream buffer entries. As data ﬂows in one direction, credits ﬂow in the opposite direction. A packet is partitioned into one or more ﬂits [8] and before a ﬂit is transmitted to downstream, the appropriate credit count is decremented. Once the ﬂit departs the downstream router, a credit is returned back upstream. This common ﬂow control is implemented at per-hop granularity as credit is transferred between neighboring routers. However, in a bufferless on-chip network without any router input buffer, such per-hop credit-based ﬂow control is not needed as arriving ﬂits are guaranteed to make progress – they either move towards their destinations if there is no contention or are deﬂected if there is contention for the same output. In this work, we propose a ﬂow control for bufferless routers that we refer to as clumsy ﬂow control (CFC) – an approximate, destination-based credit-based ﬂow control that throttles network trafﬁc to avoid network congestion and minimize deﬂections in bufferless NoC. CFC is based on credit-based ﬂow control but instead of per-hop ﬂow control, CFC is destination-based as the credits represent buffer availability at the packet destination. 1 The credits in CFC are only maintained by the cores and the credits do not represent local router buffer occupancy but represent the buffer availability at the destination. With the trafﬁc from the cores to the memory controllers (MC), the destination buffers that we leverage are the memory request queues at the MCs. Thus, each credit represents the ability for each core to inject another request into the network based on the memory queue occupancy. By throttling the memory requests through CFC, the number of in-ﬂight requests is minimized and thus, minimizes network congestion. Once requests are injected into the network, the credit count is decremented and when a reply is received from the MCs, the corresponding credit is incremented. Although credits are used to represent available buffer entries, one key difference with conventional credit ﬂow control is the accuracy of the credits. In credit-based ﬂow control, credits are an accurate representation of the buffer space; this guarantees that packets are not dropped in the network. However, in CFC, the credits do not necessarily need to be an exact representation of the memory queue buffer space but can be an approximation – hence clumsy ﬂow control. If more packets are sent compared to the amount of buffer space available, the extra packets will be deﬂected – following normal behavior of the bufferless router. In addition, credits in credit-based ﬂow control are explicitly returned to upstream routers, either through dedicated wires or by piggybacking. In comparison, CFC does not require explicit credits to be returned but credits are implicitly returned when response packets return from the MC back to the cores. 1 In this work, we assume a GPGPU architecture and thus, the communication occur only between the shader cores and the memory controllers (MCs). Fig. 3. Comparison of energy consumed for bufferless and buffered NoC for (a) α = 0 and (b) α = 0.25. The x-axis is β and the lines represent different values of γ . If we substitute Equation 1 into Equation 2, the relationship can be re-written as ( 1 γ ) 1 + α β + α > 1 (3) where α = Ec /Er b = the ratio of the per hop channel energy compared with per hop router energy in buffered NoC, β = Er bl /Er b = the ratio of per hop router energy between bufferless and buffered router, and γ = Havg bl /Havg b = the ratio of additional hop count that results from deﬂection routing. The different values of these three parameters (α, β , γ ) determine how much beneﬁt, if any, can come from bufferless NoC and deﬂection routing. These three parameters are deﬁned as they represent important design parameters in determining the beneﬁt of bufferless NoC. For example, β is the efﬁciency of a bufferless router, compared with the buffered router, γ shows the additional cost of deﬂection routing, and α incorporates the impact of channel energy. In general, β ≤ 1 as Er bl will be lower than Er b per hop since the input buffers are removed while γ ≥ 1 since the average hop count will likely increase with deﬂection routing in bufferless NoC. Figure 3 plots the left part of Equation 3 for α = 0 and α = 0.25 with the x-axis representing β parameter and the different lines representing different values of γ . If the plotted value is higher than one, it represents a point where bufferless NoC is more energy efﬁcient than buffered NoC. The γ = 1 line represents a bound on the improvement of bufferless NoC over buffered NoC. As β is decreased (i.e., bufferless routers are made more energy efﬁcient compared with buffered NoC), the energy improvements of bufferless NoC increases. Some prior work [13] in bufferless routing have ignored the impact of the channel energy (i.e., α = 0). Based on DSENT [21] for 45nm technology, we estimate the value of α = 0.25. As shown in the results, the beneﬁts of bufferless are signiﬁcantly higher with α = 0 since the additional energy consumed from the channels through the increased hop count is not included, compared with α = 0.25. With α = 0 and if we assume γ = 1 (i.e., if no packets are deﬂected and there is no increase in the average hop count), the energy beneﬁt would be proportional to 1/β or the energy reduction from each router. As γ increases, the energy reduction would decrease proportionally. With α = 0.25, we show that if γ > 4 (i.e., the average hop count is increased by a factor of 4 with bufferless NoC), regardless of how efﬁcient a bufferless router can be made compared with buffered router by removing the input buffers (i.e., for any value of β ), bufferless NoC cannot be as energy-efﬁcient as buffered NoC. Fig. 4. Performance of using CFC with different credit numbers. Fig. 6. Average number of deﬂections. but use w = 1 to avoid write deadlock and vary the value of r . Figure 4 and Figure 5 show the performance and energy results for CFC(r ,1). As mentioned earlier, a baseline bufferless NoC results in signiﬁcant loss of performance by up 50%, and an approximately 20% reduction on average. Among the different CFC(r ,1) that we evaluated, CFC(2,1) resulted in the best overall performance, on average, and performance decreases when r is either increased or decreased. As r increases, it introduces more trafﬁc and causes more congestion, which degrades overall performance, while for a smaller value of r , throttling limited overall performance. On average, CFC(2,1) resulted in only a performance loss of only 1.8%; in several workloads, CFC(2,1) actually exceeded the performance of the baseline buffered network. CFC(2,1) also improves the performance of the baseline bufferless by 22%. The energy consumed in the baseline bufferless can exceed that of the baseline buffered by up to 38% (Figure 5) and on average, only results in 5.3% reduction in energy. However, by using CFC(r ,1), the energy is signiﬁcantly reduced – CFC(2,1) reduces energy by 39% and 36% compared with the baseline buffered and baseline bufferless NoC, respectively. To understand the results with CFC(r,w), we ﬁrst plot the average number of deﬂections per ﬂit in Figure 6. As r decreases, the amount of deﬂection also decreases as the number of ﬂits in-ﬂight are reduced. Compared with the baseline bufferless, CFC(2,1) results in an approximately 92% reduction in average number of deﬂections per ﬂit. The impact of both CFC ﬂow control and reduction in deﬂection improves the average memory access latency (AMAT), as shown in Figure 7(a). The AMAT that we measure includes the request network latency from the core to the MC, the memory access latency, and the reply network latency from the MC back to the core. For the buffered NoC, the latency includes the queuing latency within the network, while for the bufferless and CFC(r,1), the additional source latency caused by throttling is also included; thus, AMAT is measured from when the request is generated till when the data arrives. The reordering latency (i.e., the amount of time it takes for all of the ﬂits to arrive) for multi-ﬂit packets is also included in the bufferless implementations. While CFC(8,1) increases AMAT by 33% compared with the baseline buffered, CFC(2,1) results in minimal change (an increase of only 4.7%). For workloads such as SP, CFC(2,1) does increase AMAT by 83%, which results in a reduced overall performance but does not necessarily translate into an 83% reduction in performance since the large number of threads is able to tolerate some memory access latency. We also partition AMAT into read AMAT (Figure 7(b)) and write AMAT (Figure 7(c)). As r increases, the read AMAT in general decreases since the throttling is reduced resulting in improved read AMAT, except for CFC(1,1). However, for the write AMAT, all except for CFC(1,1) results in an increase of the write request latency as r increases. Although r is related to the read requests, it also impacts the write request latency since the amount of trafﬁc from the read requests impact the write trafﬁc. Fig. 5. Energy comparison of using CFC with different credit numbers. B. Details of CFC Each core (i) maintains two credit counts for each memory controller (j ), which we represent as rij and wij for read and write requests. Initially, rij = r and wij = w ∀i, j , where r and w are the initial credits allocated to each core for read and write requests. For a read request from core i destined to memory controller j , if rij > 0, the core injects the request into the network and decrements rij . If rij = 0, the requests are throttled until credit becomes available. Once a reply returns from memory controller j back to core i, the appropriate rij is incremented. For write requests, the corresponding wij value is used. We denote different implementations of CFC as CFC(r, w). The values of r and w can be any value greater than 0. For smaller values, more throttling is done to minimize the amount of deﬂection, but smaller values can also limit overall performance. For larger values, there is less throttling but also results in more deﬂection in the network. As r and w values approach inﬁnity, CFC corresponds to the baseline bufferless router without any ﬂow control. The impacts of different initial values of credits are evaluated in Section III-D. C. Deadlock in CFC Protocol deadlock is avoided by having separate networks for request and reply (Section II-A) but another type of deadlock can still occur because of multi-ﬂit packets and the destination re-assembly buffer. If each entry in the destination re-order buffer is partially occupied (i.e., only some of the ﬂits of the packets have arrived), the remaining write request ﬂits need to arrive in order for the system to make progress. However, if the network is full and the remaining write request ﬂits cannot be injected, deadlock will occur. This deadlock is identical to the problem identiﬁed earlier as reassembly buffer overﬂow [10] and occurs because of a lack of ﬂow control in the bufferless NoC. In this work, we leverage CFC in order to avoid such deadlock. With CFC(r,w), if the sum of w values across all the cores is smaller than the write memory queue size, write request packets are guaranteed space at the destination. In this work, we apply CFC to the forward path (i.e., from the cores to the MCs) but not on the return path from the MCs to the cores. No ﬂow control is explicitly needed on the return path since the forward path ﬂow control also limits the replies from the MCs back to the cores. D. Results We evaluate the performance and energy impact of CFC, using the conﬁguration described earlier in Section II-A. We evaluate CFC(r,w) (a) (b) (c) Fig. 7. (a) average memory access latency (AMAT) latency per packet. The AMAT are partitioned into (b) read and (c) write requests. All results are normalized to baseline buffered NoC. Fig. 8. Impact of throttling by reducing the amount of MSHR in each core. The baseline bufferless in our evaluation is MSHR 64. E. Comparison to Alternative Static Throttling Another simple approach to throttling is to reduce the number of MSHR in the cores. If the MSHR entries are full, the core is stalled and thus, additional requests cannot be injected into the network as requests are throttled. However, this approach is limited since it can stall the core when network utilization is not necessarily high based only on the source trafﬁc. In our evaluation, we assumed a baseline MSHR with 64 entries in each core and we reduced the number of MSHR to evaluate the impact of throttling on performance (Figure 8). This approach has the same goal as static CFC since the network trafﬁc (or requests) is throttled by limiting hardware resources (e.g., MSHRs). However, in CFC, we do not restrict the number of MSHR entries and thus, throttling requests does not necessarily stall the core 2 – while with reduced number of MSHR, throttling requests can stall the core and signiﬁcantly impact performance. As a result, for many of the workloads (e.g., bp, mum, nn), reducing the number of MSHR results in continued decrease in performance – resulting in lower performance than the baseline bufferless networks. For some of the workload (e.g., bfs,fwt), throttling by reducing the amount of MSHR does improve performance – for example, by signiﬁcantly reducing the MSHR size to only 4 entries, performance of bfs improves by 39%. However, this form of static source throttling is not beneﬁcial across all workloads. On average, reducing the amount of MSHR only reduces the overall performance as shown in Figure 8. IV. DYNAM IC C LUM SY F LOW CON TRO L In the previous section, CFC was static since the maximum amount of credit was pre-determined for each workload and this maximum value did not change. However, as results from Figure 4 and Figure 5 show, the most optimal point, in terms of performance, is not necessarily identical for all of the workload. For example, for fwt, the static CFC that resulted in the best performance was (2,1) while for different workloads (such as sp), (4,1) results in the best performance. In addition, the behavior of program can change during execution such that the constant value of (r, 1) is not necessarily optimal. (a) (b) Fig. 9. (a) Block diagram of the buffer partition and (b) state diagram of how the credit count is adjusted. To provide higher ﬂexibility, we extend CFC to dynamic CFC (dCFC) where the amount of credit is dynamically modiﬁed. Similar to static CFC(r,w), dCFC(r,w) notation is used where r and w represent the initial amount of credits for read and write request packets, respectively. However, the amount of maximum credits available at each core is modiﬁed, based on the congestion or the queue occupancy at the destination. For the destination buffer, there are a lower and an upper threshold occupancy value which partitions the buffer into three regions – high, medium, and low occupancy region (Figure 9(a)). If the buffer occupancy moves from the medium to the high region, the credit count is decreased since congestion is expected. Similarly, if the buffer occupancy moves from the medium to the low region, the credit count is increased. The threshold values were determined empirically to ensure that the switches between the different regions do not occur frequently and the values used in our evaluation was thresholdlower = 0.25max size and thresholdupper = 0.75max size where max size is the buffer depth of the destination buffer (i.e., the memory queues). A state diagram that illustrates how the credit is modiﬁed is shown in (Figure 9(b)). This approach is similar to how on-off ﬂow control [8] operates for buffered ﬂow control between neighboring routers. When an upper threshold is reached, an “off ” signal is sent to ensure no more packets are sent while when a lower threshold is reached, an “on” signal is sent to notify more packets can be sent. This ﬂow control is used to minimize the signaling between neighboring nodes in a buffered network. However, as shown in Figure 9(b), dCFC is not used for buffer management as on-off ﬂow control but used for congestion management. As a result, similar to static CFC, the amount of credits that each core has does not need to be accurate but are an approximate estimation since they are used to avoid network congestion. Thus, the “on” and “off ” signal that we use in this dCFC do not need to be conservative, as is the case in on/off ﬂow control. 2 In CFC, the core will stall if the MSHR is full; however, the main difference is that throttling of requests with CFC does not necessarily stall the core – whereas by restricting the number of MSHR entries, throttling that results from the limited MSHR always throttles the core. One design parameter is determining the initial values of the credit value. Similar to static CFC, we use w = 1 to avoid deadlock but for the read credit r , our evaluation show that different initial values Fig. 10. Average read credit count and how it changes over time for lib workload for dCFC(2,1) and dCFC(16,1). (a) (b) Fig. 13. Two examples of deﬂection decision during arbitration. For simplicity, productively routed ﬂits from the ﬁrst stage of the allocation are not drawn in this ﬁgure. Fig. 12. High-level block diagram of the proposed randomizeddeterministic allocation logic. For simplicity, a 3-port allocation logic is shown. Fig. 11. Performance comparison of dynamic CFC with static CFC. of r has minimal impact. Figure 10 plots how the read credit count changes over time for dCFC(2,1) and dCFC(16,1) for a particular workload (lib). Because of the different starting value for r , the read credit values are initially different – however, the credit count eventually converges and there is little difference in the read credit count used in the workload and minimal impact on performance. In the dynamic CFC evaluation, we assume dCFC(16,1) as the initial values for the read credits and use this for the rest of this work. The results of dCFC(16,1) are shown in Figure 11 along with static CFC(4,1) which provided the best average results among the different static CFC conﬁgurations. On average, dCFC(16,1) performance is nearly identical to the baseline buffered NoC while exceeding the performance of static CFC(4,1) by 5%. For some of the workload such as sp, dCFC(16,1) exceeds the performance of CFC(4,1) by 12%. As a result, dynamic CFC is able to better adapt to the requirements of the different workloads and provide better results. V. RANDOM I Z ED -D ET ERM IN I S T IC A LLOCAT ION (RDA ) In the previous sections and evaluation, we assumed the router frequency for buffered and bufferless was identical. However, as we will show in Section V-C, baseline bufferless router implementation can signiﬁcantly increase router frequency and impact overall performance. The critical path of deﬂection-based bufferless router is dominated by the routing allocation – which consists of the priority sort and the allocation of the output ports [10]. In this section, we propose randomized-deterministic allocation (RDA) which provides high performance while taking advantage of randomization to simplify the logic. The allocation is also deterministic since the deﬂection port is predetermined. The combination of these two component signiﬁcantly reduces the critical path of a deﬂection-based bufferless router. The key insight into RDA is avoiding the serialized allocation through a pre-determined allocation and enables parallel allocation. A. Two-stage Allocation Two-stage allocation or separable-allocation [8] is commonly used in buffered routers – e.g., ﬁrst stage consists of arbitration at the inputs (among the VCs) and the second stage consists of arbitration at the outputs among requests from different inputs [8]. We also use a 2-stage arbiter for deﬂection routing in RDA as shown in Figure 12. Since there is no virtual channel in bufferless router, the ﬁrst stage consists of an output arbiter, similar to the second stage of buffered router allocation. In the second stage, we implement a novel deterministic arbiter to avoid the serialization and enable each port to be deﬂected in parallel. First-stage Arbiter: The requests (rij ) from the input port (i) to output port (j ) are used in the ﬁrst stage arbiter. The ﬁrst stage arbitration is an output arbitration since requests from each input are sent to each output for arbitration. The injection port of the router does not request allocation if there are requests from all other four ports – to ensure correctness of deﬂection routing and all packets can be deﬂected appropriately [16]. The input request bit vector indicates the minimal output ports for each packet. With minimal routing, at most two bits can be set – e.g., r12 = r13 = 1 while for other output port j , r1j = 0 if a packet from input port 1 can send its packet to either output 2 or 3 with minimal routing. By having multiple bits set in the request vector, minimal adaptive routing can be done and increase performance. However, this also increases complexity since a single input port can have up to two outputs granted after the ﬁrst stage. It would require another level of logic to ensure that the packet is only granted one output port and likely increase the critical path. To minimize the impact on critical path while still providing adaptivity, we use randomization to create a 1-hot request vector – i.e., only one request from each input is used in the ﬁrst stage arbiter. If there are multiple minimal output ports, we randomly select one of the output ports, similar to how O1Turn [20] routing algorithm randomly determines whether to route either using XY or YX routing. Our simulation results show that this randomized 1-bit randomized request vector nearly matches the performance of minimal adaptive allocation where both minimal paths are considered. Second-stage Deﬂection: After the ﬁrst stage, all of the inputs ports that were not granted need to be deﬂected. The serialization in the allocation of the output ports is needed in this stage since it is unknown which output ports will be used for deﬂection routing by other input ports. – i.e., two inputs cannot be deﬂected to the same output. To avoid this serialization, we deﬁne a ﬁxed rule that each port follows for deﬂection that is based on the outcome of the ﬁrst stage. The simple rule that we enforce is that each packet is deﬂected to the ﬁrst output port available in the clockwise direction from the input (a) (b) Fig. 15. Impact of different allocation and different router pipeline frequency. Fig. 14. (a) Critical path and (b) Area comparison of RDA and alternatives. port. 3 In some cases, the ﬁrst available output port might be the same output port for two inputs (e.g., Figure 13(b)). In this case, the input closest to the output port has higher priority. However, this contention does not require serialization – i.e., the allocators in the second stage of Figure 12 do not need to be connected serially – since this priority can be determined based on the output of the ﬁrst stage. Each port that needs to deﬂect a packet in the second stage needs to count the number of input ports in the clockwise direction that also needs to be deﬂected before an available output for deﬂection is achieved. To provide support for RDA, two additional bit vectors are needed in the second stage – input deﬂection (ID) bit vector and output availability (OA) bit vector. ID bit vector is used to identify which inputs need to be deﬂected and the OA bit vector determines which output is available for deﬂection after the ﬁrst-stage arbitration. Both of these bit vectors can be generated from the request vectors rij and the intermediate grants yij as shown in Figure 12. B. Allocation Example Figure 13 illustrates an example of how the deﬂection routing works with RDA. In this section, we assume the ﬁrst stage (or the output arbitration) is already done and highlight the second-stage of the RDA allocation. In Figure 13(a), we assume packets from the N and the S ports need to be deﬂected and the available output ports are W and E ports. Since each port ﬁnds the closest available output port in the clockwise direction, the packet from the S port is deﬂected towards the W port while the packet from the N port is deﬂected towards the E port and there is no contention in the deﬂection routing. In Figure 13(b), packets from the N and the E port need to be deﬂected while N, S, and W ports are available. The closest clockwise port available for deﬂection is the S port for both packets. However, the priority is given to the closer port (i.e., E input port) and the packet from N port is deﬂected towards the W port. This does not require serialization since the N port, within the RDA deﬂection allocation, will know that E port also needs to be deﬂected (based on the ID bit vector) and thus, not assign the ﬁrst available clockwise port but assign the second closest available clockwise output port. C. Results In this section, we ﬁrst show the impact of RDA on the critical path and area. We use the Verilog router model [4] as the baseline and modiﬁed it to implement RDA with bufferless router, and used Synopsys Design Compiler with TSMC 45nm technology to evaluate the area and critical path. We compare the critical path and area using RDA with baseline buffered and state-of-the-art bufferless implementation, CHIPPER [10] and our baseline bufferless. The critical path results of RDA are shown in Figure 14(a). RDA is able to reduce clock period by 13% and 14%, compared with CHIPPER 3Our design is one example of pre-determined rules for deﬂection routing. For example, our RDA can be implemented using a counter-clockwise rule. Fig. 16. Cumulative distribution of deﬂection count with various deﬂection probability. and baseline buffered. In addition to the improvement in the clock frequency, the per-hop router latency is lower since RDA enables a single-cycle bufferless router, whereas both the baseline buffered and CHIPPER requires a two-stage router. Compared with the baseline bufferless, RDA is able to reduce critical path by over 45% as RDA avoids the serialization. The router area comparison is shown in Figure 14(b) – compared with CHIPPER, RDA is able to reduce area by approximately 25%. This additional reduction in area comes from the simpliﬁcation of the allocation as well as removal of extra pipeline stages required in CHIPPER. 4 In Figure 15, we compare the performance of baseline buffered, dCFC(16,1) which includes a realistic clock frequency of bufferless router, dCFC(16,1)+RDA which assume the frequency of bufferless is identical to baseline buffered but used RDA allocation, and dCFC(16,1)+RDA f req which leverages the higher clock frequency from RDA allocation. Results with a baseline bufferless router microarchitecture and dCFC(16,1) can signiﬁcant reduce performance – by approximately 35% compared with the baseline. The results of dCFC(16,1)+RDA is similar to baseline buffered and shows RDA allocation has minimal impact on performance, compared with agebased arbitration in the baseline bufferless NoC. With the increased clock frequency from RDA, dCFC(16,1)+RDA f req is able to exceed the performance of baseline buffered NoC by approximately 7% (up to 22% for some workloads). D. Livelock Discussion In the proposed RDA allocation, livelock can be an issue since there is no way to prevent a packet from being continuously deﬂected. In Figure 16, we provide an analytical study through simulations on the cumulative distribution of the number of deﬂection count for a packet as the deﬂection probability is varied from 0.05 to 0.5. The analysis shows that if the probability of deﬂection is under 0.2, 99% of the packets are deﬂected less than 10 times. At high load in a bufferless NoC, the probability of deﬂection can be very high; however, bufferless NoC with CFC signiﬁcantly reduces the deﬂection 4 Performance results of CHIPPER is not shown but CHIPPER is an improvement of router microarchitecture for bufferless NoC but does not provide any congestion management. As a result, the performance of CHIPPER on the high-throughput workload is similar to the baseline bufferless. probability. In our evaluation with CFC, the deﬂection probability is decreased to under 0.1 in all of the workloads that we evaluated. In addition, probabilistic livelock avoidance can be guaranteed as long as there is a “guarantee of non-zero chance of a packet moving towards its destination at each hop” [8]. In our CFC implementation, the ﬁrst stage arbiter of RDA was based on a round-robin arbiter but if a simple random arbiter is used in the ﬁrst stage, then there is a nonzero chance of a packet moving towards its destination at each hop and thus, probabilistic livelock avoidance can be guaranteed. V I . R E LATED WORK Congestion in interconnection networks has been estimated through different metrics [12], [19] and has been used for adaptive routing. However, prior work on congestion and adaptive routing are not necessarily applicable to the bufferless network that we consider in this work. Different bufferless NoC routers have been recently proposed [16], [13], [11]. In this work, we focused on deﬂectionbased bufferless NoC but the proposed CFC is applicable to dropand-retransmit bufferless architecture. To overcome the complexity in the control logic of the bufferless NoC, CHIPPER [10] was proposed to minimize the complexity with minimal loss in performance. However, these prior NoC architectures do not necessarily provide high performance at high load when packets continue to be deﬂected or need to be retransmitted multiple times. MinBD [5] was proposed, which adds an intermediate buffer to CHIPPER in order to minimize deﬂection. However, adding intermediate buffers reduces the beneﬁts of the bufferless router and at high-load or near saturation, deﬂection will continue to occur. G ´omez et al [11] reduced the amount of congestion or packets being dropped by combining dropping packets and misrouting and adding additional physical channels. In this work, we attempt to reduce network congestion without additional complexity to the network. [15] showed the limitations of bufferless NoC because of the deﬂection routing and the energy consumed in the channel. We also provide similar observation through analysis in Section II and we overcome these limitations of the bufferless NoC through CFC ﬂow control. Network throttling approaches have been previous proposed to reduce network congestion, such as self-tuned congestion control mechanism [22]. This approach focuses on network congestion and using global information, detects when congestion in the network occurs and then, throttles the source. In our work, we estimate congestion using destination buffer and can minimize network congestion from occurring. As a result, our implementation of [22] resulted in very little impact on overall performance for the GPGPU workloads that we considered in this work. Heterogeneous adaptive throttling (HAT) [6] also propose source throttling but their work mainly focuses on multiprogrammed workload and is based on application-aware throttling and analyzing MPKI of the different workloads. They also introduce network-load-aware throttling where network congestion, based on channel or buffer utilization, is used to throttle the source. For the high-throughput workloads that we evaluated, such approach does not reduce the destination congestion that CFC avoids. As a result, results from HAT evaluation [6] showed signiﬁcant performance improvement on multiprogrammed workloads, but had minimal improvement for multithreaded workloads. Tilera on-chip network [9] also used a similar end-to-end ﬂow control to guarantee storage at DRAM memory controller. However, the details of how the ﬂow control are implemented is not clear and their goal is not to minimize deﬂection but to avoid deadlock. V I I . CONC LU S ION In this work, we address some of the challenge in scaling bufferless NoC to high-throughput GPGPU workloads. We ﬁrst show how the large amount of deﬂection for workload with high network load can signiﬁcantly reduce the beneﬁts of bufferless NoC. We show how the amount of deﬂection in bufferless NoC needs to be reduced for GPGPU workloads and propose clumsy ﬂow control (CFC) to reduce the amount of congestion in the network with minimal overhead. CFC provides an approximate estimate of the congestion at the destination and by minimizing network congestion, minimizes the number of deﬂection in the network. We also extend CFC to dynamic CFC where we adjust the amount of maximum credits at each node dynamically. Another challenge in bufferless NoC is reducing the critical path since deﬂection routing can increase the router delay. To overcome this, we propose randomized-deterministic allocation (RDA) which enables a single-cycle bufferless router with minimal loss in performance. Through the combination of CFC and RDA, we show how bufferless NoC can be scaled to workloads which have high load while improving energy efﬁciency. "
An energy-efficient millimeter-wave wireless NoC with congestion-aware routing and DVFS.,"Traditional multicore designs, based on the Network-on-Chip (NoC) paradigm, suffer from high latency, significant power consumption and temperature hotspots as the system size scales up due to the inherent multi-hop nature of the communication fabric. NoCs have been shown to achieve increased performance by inserting long-range wired links following the principles of small-world graphs [1]. Design and optimization of multi- and many-core systems on chip (SoCs) that exploit small-world effects have already been demonstrated [1]. Small-world graphs are characterized by many short-distance links between neighboring nodes as well as a few relatively long-distance direct shortcuts. These networks can be further improved by replacing the long-distance shortcuts with single-hop, energy-efficient, wireless links. Wireless NoCs (WiNoCs) with millimeter (mm)-wave wireless links working at 10-100 GHz is envisioned as an enabling technology to design low-power and high-bandwidth massive multicore architectures [2]. These mm-wave small-world NoC (mSWNoC) networks are used as the interconnection backbone in this work.","An Energy-Efficient Millimeter-Wave Wireless  NoC with Congestion-Aware Routing and DVFS  Ryan Kim, Jacob Murray, Paul Wettin, Partha Pratim Pande, Behrooz Shirazi  School of Electrical Engineering and Computer Science  Washington State University, Pullman, WA, USA  {rkim, jmurray, pwettin, pande, shirazi}@eecs.wsu.edu  Traditional multicore designs, based on the Network-on[4]. ALASH builds upon the layered shortest path (LASH)  Chip (NoC) paradigm, suffer from high latency, significant  routing algorithm by allowing each message to adaptively  power consumption and temperature hotspots as the system  switch paths. Each message chooses its own route at every  size scales up due to the inherent multi-hop nature of the  intermediate switch to increase flexibility. ALASH uses the  communication fabric. NoCs have been shown to achieve  knowledge of the traffic congestion in the network to help  increased performance by inserting long-range wired links  balance the load across the network. This type of routing is  following the principles of small-world graphs [1]. Design and  inherently beneficial to improve the thermal profile of the  optimization of multi- and many-core systems  on chip (SoCs)  mSWNoC.   that  exploit  small-world  effects have  already been  To further improve the thermal profile of the mSWNoC we  demonstrated [1]. Small-world graphs are characterized by  propose to combine our DTM methodology with that of  many short-distance links between neighboring nodes as well  Dynamic Voltage and Frequency Scaling (DVFS). DVFS is a  as a few relatively long-distance direct shortcuts. These  popular methodology to optimize the power usage/heat  networks can be further improved by replacing the longdissipation of electronic systems without significantly  distance shortcuts with single-hop, energy-efficient, wireless  compromising overall system performance. Most of the  links. Wireless NoCs (WiNoCs) with millimeter (mm)-wave  existing works principally address power and  thermal  wireless links working at 10-100 GHz is envisioned as an  management strategies for the processing cores only. The  enabling technology to design low-power and high-bandwidth  opportunity for performing DVFS on an NoC depends on the  massive multicore architectures [2]. These mm-wave smallarchitecture’s link utilization characteristics. A histogram of  world NoC  (mSWNoC) networks are used as  the  the link-level traffic utilizations for conventional mesh and  interconnection backbone  in this work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  mSWNoC architectures is shown in Fig. 1. From this, it is  As hundreds of cores are integrated in a single chip,  evident that in the mesh architecture, a significant amount of  designing effective  packages for dissipating maximum heat is  links has more than 90% utilization for the SPLASH-2 and  infeasible. Moreover, technology scaling is pushing the limits  PARSEC benchmarks considered in this work, and hence, the  of affordable cooling,  thereby requiring suitable design  ability to perform link-level DVFS is low. In this case, there is  techniques to reduce peak temperatures. Thus, addressing  not significant room for improvement with DVFS as the  thermal concerns at different design stages is critical for the  voltage and frequency cannot be tuned often on the links with  success of future generation systems. In this context, Dynamic  high utilization without encountering excessive  latency  Thermal Management (DTM) appears as a solution to avoid  penalties. On the other hand, the mSWNoC reduces load on  high spatial and temporal temperature variations among NoC  wireline links by carrying significant amount of traffic through  switches, and thereby mitigate local network hotspots. Recent  the wireless shortcuts creating significant opportunities for  works on DTM [3] for multicore architectures principally focus  implementing DVFS, which can also be seen clearly in Fig. 1.  on optimizing the performance of the cores only and explores  Because of this, there is more room for energy savings in  the design space in the presence of thermal constraints.  mSWNoC in presence of DVFS compared to mesh.  However, the performance and thermal profiles of a multicore  Figs. 2 and 3 show the latency and network energy  chip is also heavily influenced by its overall communication  dissipation for the mesh, mSWNoC, and the small-world NoC  infrastructure, which is predominantly a NoC. In this work, we  (SWNoC), which is basically the mSWNoC without wireless  propose evaluating the performance of an adaptive congestionaware routing-based DTM followed by DVFS to avoid  creation of network temperature hotspots in WiNoCs.   In the mSWNoC topology, each core is connected to a  switch and the switches are interconnected using both wireline  and wireless links. The topology of the mSWNoC is a smallworld network where  the  links between switches are  established following a power -law model. The power-law  based mSWNoC principally has an  irregular network  topology. To ensure deadlock-free routing in mSWNoC, we  implement an adaptive layered shortest-path routing (ALASH)  .9 < U <= 1 .8 < U <= .9 .7 < U <= .8 .6 < U <= .7 .5 < U <= .6 U <=.5 Figure 1. Link Utilizations with various traffic patterns  100 80 60 40 20 0 CANNEAL BODYTRACK RADIX FFT e b W S k n i l W S W S W S W S a t o t % ( f o f o m LU C o u N s e M s e M s e M s e M C o h s e M C o N C o N C o N N m N m h h h h ) l m m m s r This work was supported in part by the US National Science Foundation  (NSF) grants CCF-0845504, CNS-1059289, and CCF-1162202, and Army  Research Office grant W911NF-12-1-0373.                  100 90 80 70 60 ) s e l c y C ( y c n e t a L FFT RADIX LU CANNEAL BODYTRACK FFT RADIX LU CANNEAL BODYTRACK Base DTM DTM + DVFS DTM DTM + DVFS DTM DTM + DVFS Mesh SWNoC mSWNoC y g r e n E k r o w t e N l a t o T 25 20 15 10 5 0 ) J μ ( Base DTM DTM + DVFS DTM DTM + DVFS DTM DTM + DVFS Mesh SWNoC mSWNoC Figure 2. Average network latency with various traffic patterns.  Figure 3. Total network energy with various traffic patterns.  links. It can be observed from Fig. 2 that for all of the  benchmarks considered, the latencies of the small-world  architectures are lower than that of the mesh architecture. This  is the result of the small-world architecture’s lower average  hop-count than that of mesh. It can also be seen that the  latency of the mSWNoC is lower than that of the SWNoC.   This is a result of the even lower average hop-count with the  addition of direct long-range, one-hop wireless shortcuts.   It can be observed from Fig. 3 that in each benchmark, the  network energy  is much  lower  for  the small-world  architectures compared to the mesh architecture. In the smallworld architectures, the hop-count decreases significantly, and  hence, on average, packets have to traverse through less  number of switches and  links. Between SWNoC and  mSWNoC, a significant amount of traffic traverses through  the energy-efficient wireless channels  in mSWNoC;  consequently allowing the interconnect energy dissipation of  mSWNoC to further decrease compared to the SWNoC. With  the addition of DVFS, the total network energy can be further  reduced. With the heavy reduction of traffic traversing through  the wireline links in the small-world architectures, seen in Fig.  1, combined with their lower latencies compared to mesh, the  opportunity for implementing DVFS is significant. This is  evident by the significant energy reductions seen in Fig. 3 by  implementing DVFS on the small-world architectures.   The goal of this work is to improve the overall thermal  profile of mSWNoC by reducing the temperature of localized  hotspot switches without compromising the average execution  time of the benchmark running on the cores . From Fig. 3, we  can see that the difference in energy dissipation between  small-world architectures and mesh is significant and hence, it  is natural that their switches are cooler. For the same reasons,  the switches are cooler on the mSWNoC when compared to  the SWNoC.                        (a)                                   (b)                                  (c)  Figure 4. Network-level hotspots in (a) mesh and (b) SWNoC and (c)  mSWNoC for the BODYTRACK benchmark  In the case of SWNoC, as the shortcuts are imp lemented  through multi-hop wireline links, moving traffic through these  wireline links takes more time and energy than the wireless  links which correlates with  less  temperature reduction.  Conversely, between SWNoC and mSWNoC, the mSWNoC  achieves a higher link hotspot temperature reduction. This is  due to the wireless links detouring significant amounts of  traffic away from the wireline links.    The  inherent advantage  in  the SWNoC/mSWNoC  architectures is not only the fact that it reduces hop -count. As  neighbors in these architectures are not necessarily physically  near each other, rerouting traffic to a neighbor may send data  to an entirely different portion of the chip. Fig. 4 shows such  an example of the hotspot issue in the BODYTRACK  benchmark. Here, it can be clearly seen that attempting to  perform temperature-aware rerouting in the hotspot region of  the mesh architecture will be unsuccessful as suboptimum path  selection will lead to longer routes within the same hotspot  area for the rerouted traffic as shown in Fig. 4(a). The relative  temperature hotspot region in SWNoC shrinks compared to  that of mesh as seen in Fig. 4(b). Moreover, due to efficient  wireless links mSWNoC reduces the relative hotspot region to  only a few switches. The DTM mechanism performs much  better for localized network hotspots, like the example for the  mSWNoC architecture in Fig. 4(c). By performing DVFS on  top of the DTM, the full thermal profile can be improved.    The proposed methodology is general, in the sense that it is  not bound to the specific DTM and DVFS mechanisms that we  consider. Instead, we focus on demonstrating how to enhance  the power and thermal profiles of WiNoC-enabled multicore  chips by complementing the advantages introduced by the  interconnect architecture.   "
A loosely synchronizing asynchronous router for TDM-scheduled NOCs.,"This paper presents an asynchronous router design for use in time-division-multiplexed (TDM) networks-on-chip. Unlike existing synchronous, mesochronous and asynchronous router designs with similar functionality, the router is able to silently skip over cycles/TDM-slots where no traffic is scheduled and hence avoid all switching activity in the idle links and router ports. In this way switching activity is reduced to the minimum possible amount. The fact that this relaxed synchronization is sufficient to implement TDM scheduling represents a contribution at the conceptual level. The idea can only be implemented using asynchronous circuit techniques. To this end, the paper explores the use of “click-element” templates. Click-element templates use only flip-flops and conventional gates, and this greatly simplifies the design process when using conventional EDA tools and standard cell libraries. Few papers, if any, have explored this.","A Loosely Synchronizing Asynchronous Router for TDM-Scheduled NOCs I. Kotleas∗ , D. Humphreys∗ , R.B. Sørensen∗ , E. Kasapaki∗ , F. Brandner† and J. Sparsø∗ ∗Department of Applied Mathematics and Computer Science, Technical University of Denmark Email: {ikotleas,dean.humphreys}@gmail.com, {rboso,evka,jspa}@dtu.dk † Computer Science and System Engineering Department, ENSTA ParisTech, France Email: ﬂorian.brandner@ensta-paristech.fr Abstract—This paper presents an asynchronous router design for use in time-division-multiplexed (TDM) networks-on-chip. Unlike existing synchronous, mesochronous and asynchronous router designs with similar functionality, the router is able to silently skip over cycles/TDM-slots where no trafﬁc is scheduled and hence avoid all switching activity in the idle links and router ports. In this way switching activity is reduced to the minimum possible amount. The fact that this relaxed synchronization is sufﬁcient to implement TDM scheduling represents a contribution at the conceptual level. The idea can only be implemented using asynchronous circuit techniques. To this end, the paper explores the use of “click-element” templates. Click-element templates use only ﬂipﬂops and conventional gates, and this greatly simpliﬁes the design process when using conventional EDA tools and standard cell libraries. Few papers, if any, have explored this. I . IN TRODUC T ION The communication fabric in today’s multi-core platforms typically consists of one or more packet-switched networkson-chip (NOC), and it is widely accepted that a globallyasynchronous, locally-synchronous (GALS) organization of these platforms is required [1], [2]. This implies that the NOC must use mesochronous1 or asynchronous routers and links. Asynchronous circuits are more challenging to implement, but they obviate the need for the clock-synchronization circuitry that is otherwise required in a clocked mesochronous design, making asynchronous circuits the most obvious choice (at least conceptually). Asynchronous circuits make extensive use of C-elements and enable-latches and typically have numerous combinational circuit loops. In combination this represents a challenge when using conventional EDA tools and standard cell libraries. Considerable designer intervention is needed to deﬁne timing constraints and obtain timing closure. The click-element templates developed by Philips Incubators and NXP [3] only use D ﬂip-ﬂops and conventional gates and are therefore a better “ﬁt” to existing EDA tools and standard cell libraries. After the publication of [3], Philips Incubators stopped its activities in the ﬁeld of asynchronous design, and we are not aware of any subsequent publications on the use of click-element templates. 1A single clock-source; unknown, bounded, and possibly varying skew In this paper we explore the use of click-element templates to implement routers of an asynchronous NOC, and, as part of this, we design some new click-element components. The NOC is intended for use in a (application independent) standard platform supporting hard real-time applications. Instead of using a single network for all types of trafﬁc – realtime as well as best-effort – we prefer a simpler and more predictable solution with several networks, each optimized for its individual purpose. The NOC that we consider supports asynchronous message passing across virtual end-to-end channels and it implements this using time-division-multiplexing (TDM). The platform uses a GALS timing organization based on asynchronous routers and links. TDM requires some form of global synchrony and all published TDM-routers – synchronous, mesochronous or asynchronous – are based on a notion of clock cycles or handshake cycles that deﬁne a TDM time-slot. As such, they all share the same fundamental functionality: A router synchronizes all input ports and propagates either a scheduled “valid-phit” or a “void phit” on every port in every cycle. In this paper we present a novel asynchronous TDM router that completely avoids switching activity in gates and wires on links and router ports where no phits are scheduled, while still offering sufﬁcient synchronization to implement TDM scheduling. This results in reduced power consumption. The design explores one of the key features of asynchronous circuits; that they are very efﬁcient in being idle and waiting for some event. A synchronous implementation of the idea is not possible. The use of distributed routing enables a design in which the router silently waits for the next active TDM time-slot in which phits are scheduled to be transported, before becoming active again. This conceptual idea and a click-element-based asynchronous implementation of such a router is the main contribution of this paper. The paper is organized as follows: Section II reviews related work. Section III presents some background on TDM-based NOCs. Section IV presents the architecture of the new loosely synchronizing router. Section V addresses its correctness. Section VI presents the click-element-based implementation of the router and Section VII presents results for area, speed, and energy. Finally, Section VIII concludes the paper. I I . BACKGROUND AND R E LATED WORK An NOC for a real-time platform has to guarantee bandwidth and latency constraints for individual point-to-point transactions. This requires some form of end-to-end (virtual) circuit connection. In a packet-switched NOC, this can be implemented in at least two fundamentally different ways: (i) by some form of time-division multiplexing, as used in [2], [4]–[6], or (ii) by using non-blocking routers combined with rate control, [7] as in [8]. In a TDM-based NOC, packets are injected into the network of routers and links according to a predetermined static schedule. The schedule determines when a packet for a given destination can be injected into the packet-switched NOC, and once injected the packet travels along a pipelined path from source to destination. Our scheduler is presented in [9]. The schedule guarantees absence of deadlocks, and the fact that it avoids arbitration, buffering and ﬂow control results in small and efﬁcient circuit implementations. In [2], [10] it is observed that an aelite router [11] is ten-times smaller than a MANGO router [8]. Published, TDM-based NOCs have used both source routing [11], [12] and distributed routing [2], [13]. TDM is often criticized for not being work-conserving. For real-time trafﬁc, where worst-case guarantees are of interest, this is not an issue. In systems with mixed-criticality trafﬁc, we believe that using several NOCs (at least one for guaranteedservice trafﬁc and one for best-effort trafﬁc) represents a better choice than implementing a complex “one size ﬁts all” NOC. On the basis of the above observations, we decided to use a TDM-based NOC to support the real-time communication in the platform we are currently developing [14]. For completeness, note that we have designed a network interface where the TDM scheme is extended to include the network interfaces themselves [10]. This results in bufferless and unarbitrated end-to-end transfer of data from the sending processor’s local memory to the local memory of the receiving processor. This NI can be used with the new routers presented in this paper. The next issue is then how to satisfy the requirement for a GALS organization. TDM is obviously a good ﬁt for a clocked design, and the use of mesochronous routers is mentioned in [11]. However, it is also observed that the explicit synchronization required at every router input port roughly doubles the area of the clocked TDM router. The possibility of using asynchronous routers in a TDMbased NOC was ﬁrst proposed in [15], [16]. The ﬁrst published design of such an asynchronous router is presented in [12] and its timing behavior is analyzed in [17]. A plethora of different asynchronous circuit implementation styles exists – bundled-data or delay-insensitive, two-phase or four-phase handshaking, etc. At the conceptual level, a twophase, bundled-data implementation seems attractive due to its high speed. However, even with this choice, there is still a range of different handshake controllers to choose from. Due to its high speed and small size, the most attractive is probably the Mousetrap controller [18]. It has been used in a number of asynchronous NOC routers supporting best-effort and realtime trafﬁc [17], [19], [20]. We have found that the intensive use of transparent latches in a Mousetrap-based design represents a challenge when using conventional EDA tools and standard cell libraries – in our case Synopsys, Cadence and the STM 65 nm CMOS standard cell library. One problem is that the process of obtaining timing closure requires signiﬁcant intervention by the designer. Another problem is that of reset/initialization: The cell library contains very few latches, and some circuitry has to be added around the latches that are to be initialized to hold a logic one. This adds to the timing-closure problem and erodes the simplicity and elegance of the Mousetrap controller. For this reason, we decided to explore the click-element template [3] that uses only edge-triggered ﬂip-ﬂops and conventional gates. I I I . BA SE L IN E OB SERVAT ION S ON TDM ROU TER S All published TDM-router designs, including [4], [5], [11]– [13], use a valid signal in the links to indicate whether or not the link is carrying a scheduled phit in the given slot (i.e., clock cycle or handshake cycle). We ﬁnd it useful to think of this as if the links in the NOC transmit a valid phit (valid=1) or a void/idle phit (valid=0) in every cycle, as illustrated in Figure 1. With this understanding, all the above-mentioned TDM-based routers – regardless of whether they are synchronous, mesochronous or asynchronous, and regardless of whether they use source routing or distributed routing – are based on the same fundamental idea: In every clock cycle (or handshake cycle) a router consumes a phit at every input port and produces a phit at every output port. Figure 1 illustrates this for a 5-ported router using a 3phit packet format (which is similar to that used in [11], [12]). The ﬁgure shows a possible input stream of phits to the router, assuming the most general situation where the pipeline depth of the routers and links is different from the packet size, and where packets may therefore arrive unaligned. For the following discussion of the new router design, we introduce a more reﬁned view of void/idle phits: A void-phit is transmitted in slots that have been reserved, but where the source processor/NI has no data to transmit, and an idle-phit is sent in time slots where no data transmission has been scheduled. A void phit and an idle phit may be represented by a single valid signal, indicating that the link does not carry a valid phit in the current cycle, but it nevertheless requires a clock tick and some circuit switching to receive and react to this. The best one can do in order to save power is to gate off the clock tick to the datapath part of the individual pipeline registers. The asynchronous routers in [12], [17] implement the same clock-gating technique in the datapath, but still handshake on all ports in every cycle (in a sense mimicking the distribution of a global clock). When mapping applications onto a given NOC-based platform, it is very likely that some routers and links carry very little scheduled trafﬁc and the propagation of idle phits can result in signiﬁcant power consumption. Fig. 1. Example packet format and principle of operation of a TDM router. IV. TH E NEW ROU TER – ARCH I T EC TUR E Inspired by Figure 1, the following question comes to mind: “Would it be possible to design a router that avoids all switching activity in ports that are idle in a given cycle, and that silently skips over slots where all ports are idle""? Our new router architecture achieves exactly this. To achieve this, we observe that the individual routers must know which input ports are active in which TDM slots. With this requirement it is natural to also distribute the routing information into the routers. A synchronous implementation of an NOC using such routers would require that a free-running clock signal be distributed to all routers and, hence, it would not be possible to gate off all switching activity inside the router when it is idle. An asynchronous implementation, on the other hand, is possible by implementing a mechanism that waits for and synchronizes (i.e., joins) exactly those input ports that are known to be active in the next non-idle TDM slot of a router. The architecture of the new router is shown in Figure 2. All arrows represent asynchronous handshake channels. The inputto-output datapath consists of handshake registers in all ports and a switch block. This implements a two-stage pipeline: Link traversal and router traversal. The control part of the router consists of a routing control block and a conditional fork. The operation of the router is best explained through an example. Consider, for instance, slot i + 2 in Figure 1. In this cycle the router propagates a phit from input port N to some output. After this slot has completed, the routing control block initiates a new handshake cycle in which it sends a join-mask (cid:104)N , E , S, W, L(cid:105) = (cid:104)0, 1, 1, 1, 0(cid:105) along with the information used to control the router in the next slot in which it is active. In this example, this is TDM-slot i + 5, i.e., the router silently skips over TDM slots i + 3 and i + 4. Controlled by the join-mask, the conditional fork handshakes with the channels of the input ports E , S and W on the switch block. The input and the control are joined and the incoming valid phits are forwarded to the output port, as indicated by the control channel. On port S , the router receives a void phit and clock gating will suppress switching activity in the data path of that port. Input ports N and L are ignored and are silent. The only synchronization is performed by the conditional fork that does not allow the routing control block to advance into the next slot before all phits scheduled in the Fig. 2. Architecture of the new router. current slot have been propagated. This is indicated by the acknowledge signal in the channel connecting the conditional fork and the routing control block. In Section VI we present the router and its implementation in more detail. A GALS organization of a platform using the asynchronous routers presented in this paper can be implemented in at least three different ways, depending on how much one relaxes the notion of time, see Figure 3, the details of which are beyond the scope of this paper. Here, we limit our discussion to the implications for realizing a TDM schedule. Figure 3 shows a fragment of an NOC and illustrates how an NI and its IP core are connected to a router. The clock domain crossing between the IP core and the NI is largely handled by the dual-ported scratchpad memory. This architecture is adopted from [10]. The most rigid scenario (Case A in Figure 3) is to use a single TDM-clock signal that is used to clock all the NIs. Here, a TDM period is a sequence of slots with a quantiﬁed ﬁxed time duration and all NIs operate in synchrony with that TDM clock. The use of asynchronous routers and links provides some time elasticity, which allows the NIs to operate mesochronously. This design requires that the structure of routers and links is capable of operating (i.e., handshaking) faster than the TDM-clock period. This scenario is used in [10]. A more relaxed scenario abandons the quantiﬁable global time and only guarantees an ordering of the delivery of phits as prescribed by the TDM schedule. This scenario can be implemented by also applying the asynchronous handshaking phits while other routers process several TDM slots. A natural question is then: Are the routers sufﬁciently synchronized to guarantee the correct delivery of all phits? In order to answer this question, we use a time-expanded model of the network and the TDM schedule. A network topology can be represented as a graph G = (V , E ), where nodes in V represent routers/NIs and edges in E ⊆ V × V represent links between routers/NIs. The time-expanded network GT = (VT , ET ) then consists of multiple copies of this graph, where T represents the length of the TDM schedule. For each v ∈ V copies vi ∈ VT , i ∈ [0, T ] are introduced. Similarly, for each edge (u, v) ∈ E exists an edge (ui , vi+1 ) ∈ ET , i ∈ [0, T −1]. A TDM schedule is a sub-graph GS = (VS , ES ), where VS ⊆ VT and ES ⊆ ET . Constraints on the shape of the sub-graph ensure that the TDM schedule is valid. Note that for a router node v ∈ V of the network topology, the routing decision at time instant t is represented by a copy vt ∈ VS , if such a copy exists. The incoming edges of vt then indicate at which input ports phits are arriving. The copies vt ∈ VS of v ordered by their time index t thus give rise to the table entries of the routing control block. The TDM schedule describes a partial ordering of the delivery of phits throughout the entire NOC. An edge (ut , vt+1 ) represents the delivery of a phit p in TDM slot t + 1 at node v . Traditionally, TDM-based NOCs rely on a global time base, which ensures that any phit p(cid:48) that is supposed to be delivered before p has indeed been delivered. Correctness is trivially guaranteed by standard NOC implementations. We can show for the new router design proposed here that the ordering of the TDM schedule is respected when asynchronous handshaking between NIs and routers is used (cf. Case B from Section IV). For the following proof, we assume that phits sent by an NI are ordered, i.e. the phit sent by node vs is sent before that of vt iff s < t (even when the edges of the TDM schedule do not express this ordering explicitly). Theorem 1: The asynchronous click routers with their routing control block ensure a correct ordering of phits. Induction Base t = 0 → t = 1: Any Proof (sketch): node n0 ∈ VS active at TDM slot 0 has to be an NI, as only NIs can inject new phits into the network. The NI trivially determines that TDM slot 0 has been reached and offers its phit on its link. For any NI n0 , a corresponding router r1 has to be active at TDM slot 1 (and vice-verse), i.e. (n0 , r1 ) ∈ ES . The router waits precisely for the phit of its NI, the handshaking between the two succeeds, and the phit is forwarded to the router’s output port. Both nodes agree that TDM slot 0 has passed and TDM slot 1 is active. Any outgoing phit of a router at TDM slot 1 is thus guaranteed to be delivered after the respective incoming phit of TDM slot 0 has been delivered. The same trivially applies to an NI that injects its ﬁrst phit at TDM slot 1, i.e. that was not active at TDM slot 0. Consequently, the delivery of phits in TDM slot 0 and TDM slot 1 respects the ordering of the TDM schedule. Induction Step t → t + 1: Under the hypothesis that all predecessors (if any exist) of a node in the TDM schedule Fig. 3. Possible implementations of a GALS-style platform. to the NIs. In a context where asynchronous routers and links are already used, this design is perhaps more natural. Because of the handshaking, the duration of a TDM slot will be dictated by the slowest router or NI – in practice a “master” NI in which the duration of the TDM slot can be controlled. The NIs can be designed to synchronize reception and transmission in the same TDM slot (Case B in Figure 3). Alternatively, the NIs can be designed as two separate entities and understood as eager producers and eager consumers which simply transmit and receive phits in the order prescribed by the schedule respectively (Case C in Figure 3). If all signals on all interfaces between routers and NIs are projected onto the same time axis, then the set of possible traces observable for Case A will be a subset of the possible traces for Case B, which again will be a subset of the possible traces for Case C. A ﬁnal and important issue is initialization. In an asynchronous circuit it is typically necessary to initialize some handshake registers to hold data tokens and others to be empty. One way of determining the initial state is to think of it as a snapshot taken during normal operation. A related challenge is how the circuit behaves in the case where the reset signal is de-asserted with some skew. It is attractive if the initial state is a stable state in which the circuit is waiting for some input from its environment. Since the TDM schedule is programmed into the NIs and routers after reset, and since the schedule tables can be changed as part of the normal operation (a mode change), the initial state cannot be related to a particular “snapshot”. On the basis of these observations, we decided on an initial state where all handshake registers in all routers are empty (i.e. not holding data tokens). This means that the NOC must reach the same all-empty state between every repetition of the TDM schedule. As a result, the link utilization may be low in a few slots at the beginning and at the end of the schedule period. The scheduler we have implemented for our NOC implements this, [9], and we anticipate that most other schedulers will do the same because they do not consider overlapping the end of a schedule period with the beginning of the next. This decision also simpliﬁes mode changes as it creates points in time where the network of routers and links are empty. V. CORREC TN E S S Recall from Section III that, even in asynchronous designs, void/idle phits fully synchronize all routers. This is not the case anymore in our design, since routers may wait for incoming GS delivered their phits in order, we show that all subsequent phits are ordered correctly. We have to distinguish two cases for a node vt+1 ∈ Vs : (a) the node has at least one incoming edge and (b) the node has no incoming edge. (a) Any incoming edge of vt+1 can only originate from a predecessor ut in the TDM schedule. As all the preceding phits were delivered in order, all predecessors have to agree that TDM slot t is active and thus offers a phit on their output ports. The node vt+1 precisely waits for these phits, the handshaking succeeds and the involved nodes agree that TDM slot t has passed and TDM slot t + 1 is now active. All outgoing phits are thus delivered according to the partial order of the TDM schedule. (b) As the node has no incoming edges, it has to be an NI, injecting a new phit into the network. The TDM schedule does not specify any particular delivery order of this phit in relation to other phits in the network, except for phits sent by another copy vr (r < t) of the NI v . This constraint is trivially satisﬁed by the local counter advancing through the table of the routing control block. The delivery of phits in TDM slot t + 1 consequently respects the ordering of the TDM schedule. The proof sketch above only refers to the local ordering of phits expected or sent by routers and NIs and thus also covers an implementation relying on mesochronously synchronized NIs (cf. Case A from Section IV). With minor modiﬁcations of the deﬁnition of a time-expanded network, where NIs are split into receiving and sending nodes respectively, the proof also applies to a more relaxed ordering deﬁnition (cf. Case C). Note that the proof only refers to the ordering of phits relative to the partial ordering prescribed by the TDM schedule. Consequently, no guarantees can be given with respect to the delivery of phits that are not in an ordering relation in the schedule. This may have an impact on the observed timing when the schedule is processed by the NOC. In particular, the delivery of phits within unconnected sub-graphs of the TDM schedule is not guaranteed to be in a speciﬁc order. This may reduce the observed length of a schedule period for nodes of such a sub-graph, where the period depends on the length of the longest path through that sub-graph. V I . TH E N EW ROUT ER – D E S IGN AND IM P LEM EN TAT ION A. Design We have implemented the new router using static data-ﬂow components [21, Chapter 3] as shown in Figure 4. A path from an input port to an output port contains a handshake register, a demultiplexer, a merge, and another handshake register. Together, the demultiplexer and merge components implement a crossbar switch. The demultiplexers are controlled by the conditional fork as described in Section IV. The merge component assumes mutually exclusive inputs and simply propagates an input to the output. The data-ﬂow components can be implemented using a variety of handshake protocols and implementation templates. Below we present implementations that use two-phase, bundled-data, click-element templates. Fig. 4. Design of the new loosely synchronizing router using static data-ﬂow components (register, demultiplexer, merge, and register). Fig. 5. A handshake register. (a) Schematic. (b) Implementation. B. Implementation using Click Elements Click elements [3] are an implementation technique for data-driven asynchronous circuits that use a two-phase, bundled-data handshaking protocol. Instead of using Celements and latches to store data (as is common in asynchronous design), the click-element template uses ﬂip-ﬂops for both control and datapath. The use of ﬂip-ﬂops makes the timing paths in circuits easier to analyze. Below, we ﬁrst review a key component from [3], the handshake register. Following this we present a number of new click-element components that we have developed while working on the new router design. 1) Handshake Register: The data part is simply a set of clocked ﬂip-ﬂops. The control uses a toggle ﬂip-ﬂop (signal state) and a small combinational circuit that implements the logic function: (prev .req (cid:54)= state) ∧ (next.ack = state). The result is that a clock-pulse is generated when new data from the predecessor is available and the successor has acknowledged reception of the data that the register is currently holding. 2) Non-Arbitrated Buffered Merge: In [3], an arbitrated merge is described. In our router design a simpler and more conventional merge [21, Chapter 3] that assumes mutually exclusive inputs can be used. Further simpliﬁcation is possible by combining the merge and the handshake register in the router output port, cf. Figure 4. The component symbol and implementation are shown in Figure 6. Since the input requests are assumed to be mutually exclusive and only one can trigger at a time, we implement the click Fig. 7. The unbuffered demultiplexer. (a) Schematic. (b) Implementation. Fig. 6. The non-arbitrated buffered merge. (a) Schematic. (b) Implementation. function in a different way that suits our purposes better. The click-pulse is produced when (prev(1).req ⊕ prev(2).req ⊕ prev(3).req ⊕ prev(4).req) (cid:54)= state) ∧ (next.ack = state). This click pulse is used to trigger a request to the next channel. The acknowledgment to the previous channel that triggered the click is given by clock-gating independent toggle ﬂip-ﬂops – one for every input channel. The sel(i) signal that is used for the gating is produced by an XOR gate with input signals prev(i).req and state(i). 3) Unbuffered Demultiplexer: We have modiﬁed the demultiplexer component described in [3] by adding a separate control channel (as in [21, Chapter 3]) and extended the number of output channels to four, Figure 7. The component joins the previous and control ports and passes the previous port’s handshake to the selected next port. The click-function is deﬁned as (control.req (cid:54)= state) ∧ (prev .req (cid:54)= state) ∧ ((next(1).ack ⊕ next(2).ack ⊕ next(3).ack ⊕ next(4).ack) = state). The one-hot decoded selection bits of the control channel are used to clock-gate the independent toggle ﬂipﬂops that drive the output requests. Since only one handshake at the next channels is triggered at a time, only one is expected to acknowledge. Therefore, the combined acknowledgement is produced by the XOR gate. Because the component is unbuffered, the data and the combined acknowledgement are simply propagated through it. 4) Buffered Conditional Fork: The last component, whose functionality goes beyond the typical set of data-ﬂow components [21, Chapter 3], is the buffered conditional fork, Figure 8. It was introduced in Section IV and its functionality is key for the new loosely synchronizing router. It handshakes on a subset of its output channels according to a mask, which it receives on its input channel. Since the fork functionality is most efﬁciently integrated into the output port of a clickelement handshake register, we implement a buffered version of the conditional fork. As with the previous components, the conditional fork incorporates independent toggle ﬂip-ﬂops to drive the output Fig. 8. The buffered conditional fork. (a) Schematic. (b) Implementation. requests. Unlike before, where only one of these ﬂip-ﬂops was expected to toggle in every handshaking cycle, an arbitrary number (between one and ﬁve) may be triggered in a cycle. Due to this, a combined 2-phase acknowledgement from the next channels cannot be deﬁned. Instead, an independent Boolean acknowledgement (without phase) for every output port is detected with XNOR gates. The output of the 5-input AND gate indicates that the active outputs have completed handshaking (acknowledgement completion), thus enabling the next handshake on the Prev input channel. The click function is similar to the one of the handshake register in Section VI-B1 and the click pulse is used to generate the backward acknowledgement. The input token carries an onehot decoded mask signifying which output ports to trigger. The bits of this mask are used to clock-gate the independent toggle ﬂip-ﬂops that drive the output requests. There is a race condition between the Boolean acknowledgement completion going low and a Prev.ack triggering a Prev.req. This race is dealt with using a matched delay. This delay may be placed either on the Prev.req or the Prev.ack path. 5) The Routing Control Block: Finally, we show the implementation of the routing control block, Figure 9. Each entry in the routing table contains a 5-bit join mask (one bit for each router input port) and an indication of which router output port the input must be routed to. This is represented using a 4-bit one-hot code (four because a phit cannot enter and exit the EN ERGY CON SUM PT ION FOR D I FFEREN T U SAGE SC ENAR IO S TABLE I Scenarios Leakage Scenario 1 Scenario 2 Scenario 3 Full Utilization Scenario 4 Scenario 5 Scenario 6 Router Util. (%) 0 20 40 60 80 100 100 100 100 100 100 100 100 100 100 100 75 60 Link Util. (%) 0 100 100 100 100 20 40 60 80 100 100 100 100 100 100 60 80 100 Slot Util. (%) 0 100 100 100 100 100 100 100 100 1 20 40 60 80 100 50 50 50 Combined Util. (%) 0 20 40 60 80 20 40 60 80 1 20 40 60 80 100 30 30 30 Energy per Slot (pJ) 0,0398 3,3911 6,3506 9,5975 12,4278 3,5728 6,5632 9,0666 12,405 7,6408 9,3708 10,8671 12,5026 13,8285 15,3582 7.0723 6,8454 6,9913 Fig. 10. Energy consumption per slot for different utilization percentages. and clock gating reduces power consumption in ports that propagate void phits. Table I shows the energy consumption of a single router (excluding the routing control block) for a range of scenarios. The measure is energy per TDM slot and this corresponds to energy per clock cycle for a clocked circuit. Scenarios 1 through 3 explore the effect of varying the three parameters in isolation, and in Figure 10 we have plotted the results as a function of a single combined utilization percentage. The upper line (Scenario 3) is where clock gating is the only mechanism that saves power, and here the energy per slot does not reach zero. This corresponds to previously published TDM routers [2], [12]. The lower lines (Scenarios 1 and 2) capture the new router’s ability to save power when the entire router, or some of its ports, are idle. Hence, the graphs represent upper and lower bounds on the energy consumption. Any actual trafﬁc mix will result in a point in-between, as seen for Scenarios 4, 5, and 6 at 30 % utilization. The above demonstrates the new router’s ability to save power when the entire router is idle and when ports are idle. It is also interesting to compare this with alternative router designs. A fair comparison would require the implementation Fig. 9. The routing control block. (a) Schematic. (b) Implementation. router using the same port). In addition, each entry contains a 1-bit end-of-schedule ﬂag that is set if the current entry is the last in the schedule period. The routing control block is implemented as a click-element component and it consists of a RAM block, a counter and a multiplexer, Figure 9(b). A delay element is used to match the propagation delay (i.e. access time) of the RAM. In handshake cycle where the router is active, the counter is incremented and a RAM-access is performed. If the end-of-schedule ﬂag is set, the multiplexer resets the counter. Both the counter and the RAM are silent in TDM slots where the router is idle. V I I . R E SU LT S To verify the functionality of the new router design, we have synthesized a small bi-torus NOC targeting the STM 65 nm CMOS CORE65 cell library and successfully simulated its operation using trafﬁc generators mimicking the NIs. The following CAD tools were used: Synopsys Design Vision, Mentor Graphics Modelsim and Synopsys Prime Time. The design was optimized for speed with a global constraint. Due to the click-element template, all paths start and end at a ﬂip-ﬂop and no logic loops exist in the design. The matched delays were introduced as timing constraints similar to conventional bundled-data asynchronous components. The cell area of a router is 36870 µm2 – 28890 µm2 for the routing control block using a ﬂip-ﬂop-based implementation of a 60×26 bit table and 7980 µm2 for the router itself. The minimum handshake cycle time is 1.15 ns (corresponding to a speed of 870 M phits/sec per port). We have performed an exhaustive evaluation of the new router’s ability to avoid idle switching, by measuring its energy consumption (excluding the routing control block) for a range of scenarios characterized by the following three parameters. The router utilization is the percentage of the slots in a schedule period in which the router is not completely idle. When the router is completely idle, e.g. in Figure 1 slots i + 3 and i + 4, its dynamic power consumption is zero. In slots where the router is utilized, the link utilization is the percentage of links that are active (calculated as an average across those slots in a schedule period where the router is active). Here the router control block and the active router ports consume energy. Finally, when a link is scheduled to communicate data, the slot utilization is the percentage of reserved slots for which valid phits are transmitted. In the remaining reserved slots, void phits are sent. Here the router control block and the active router ports consume energy of several complete NOCs (consisting of routers, links and network interfaces) and simulation of these using realistic application benchmarks. This is very difﬁcult to do in a meaningful way, and it is far beyond the scope of this paper. For a single router we offer the following data for comparison: In [17] a 65 nm CMOS implementation of a two-phase bundled-data asynchronous TDM router using source routing is reported to have: cell area 7715 µm2 , speed 1.13 GH z and energy consumption 1.78-8.24 pJ per handshake cycle (TDM slot), depending on the mix of valid and void phits. In [20], a 40 nm CMOS implementation of an asynchronous router using Mousetrap latch controllers [18] is reported to have: cell area 4691 µm2 , minimum handshake cycle time 915 ps and 2.3 pJ per phit. The router is a best-effort router, while ours is a guaranteed service router and the 2.3 pJ is per phit, while our energy ﬁgure is per slot in which a router can route up to 5 phits. These ﬁgures show that our router is indeed a very low power design. Furthermore, when the energy consumption of links is taken into consideration, the ability to remain completely silent is even more important. V I I I . CONCLU S ION The paper presented a novel, loosely synchronizing router architecture for TDM-based NOCs. While all existing TDM routers require the transmission of a phit (valid or void) across every link in every clock cycle, our new router avoids any switching in ports that are idle. When no trafﬁc has been scheduled, it is capable of silently skipping over TDM slots. Depending on the particular application executing on the platform, this can lead to signiﬁcant power savings. This makes the new router design particularly attractive in standard platforms, where the NOC topology is ﬁxed. A second contribution of the paper is an asynchronous implementation of this new router using click-element implementation templates. Here, the paper contributed a design of the new router as well as the implementation of a set of new handshake data-ﬂow components: A demultiplexer, an outputbuffered merge (assuming mutually exclusive inputs), and an output-buffered conditional fork, which is key to implementing the router’s functionality. Area, speed and power consumption (when routing phits) are comparable to or better than previously published designs. When the router is idle, its power consumption is zero and when ports are not used there is no switching activity in these ports. This is in contrast to all previously published TDM routers as these exhibit switching activity in all ports in every clock or handshake cycle. ACKNOW L EDGM ENT This work was partially funded by the European Union’s 7th Framework Programme under grant agreement no. 288008: Time-predictable Multi-Core Architecture for Embedded Systems (T-CREST), and by the Danish Council for Independent Research | Technology and Production Sciences, Project no. 12-127600: Hard Real-Time Embedded Multiprocessor Platform (RTEMP). "
Single-cycle collective communication over a shared network fabric.,"In the multicore era, on-chip network latency and throughput have a direct impact on system performance. A highly important class of communication flows traversing the network is collective, i.e., one-to-many and many-to-one. Scalable coherence protocols often leverage imprecise tracking to lower the overhead of directory storage, in turn leading to more collective communications on-chip. Routers with support for message forking/aggregation have been previously demonstrated, supporting such protocols. However, even with the fastest possible designs today (1-cycle routers), collective flows on a k×k mesh still incur delays proportional to k since all communication is across the entire chip. As k increases across technology generations, the latency of these flows will also go up. However, the pure wire delay to cross the chip is just 1-2 cycles today, and is expected to remain roughly invariant. The dependence of message delays on k arises due to the requirement to latch messages at every router. In this work, we remove this requirement.We design a network fabric that enables messages to (1) dynamically create virtual 1-to-Many (multicast) and Many-to-1 (reduction) tree routes over a physical mesh, (2) get forked/aggregated at nodes on the tree, and (3) traverse the tree - all within a single-cycle across each dimension. For synthetic 1-to-Many/Many-to-1 flows, we demonstrate 76/82% reduction in latency, and 1.6/2X improvement in throughput over a state-of-the-art NoC with 1-cycle routers and support for collective communication. Across a suite of SPLASH-2 and PARSEC benchmarks, full-system runtime and energy is reduced by 14% and 50% for a limited-directory protocol.","Single-Cycle Collective Communication Over A Shared Network Fabric Tushar Krishna Intel Corporation, VSSAD Hudson, MA 01749. USA tushar.krishna@intel.com Li-Shiuan Peh Department of EECS. MIT Cambridge, MA 02139, USA peh@csail.mit.edu Abstract—In the multicore era, on-chip network latency and throughput have a direct impact on system performance. A highly important class of communication ﬂows traversing the network is collective, i.e., one-to-many and many-to-one. Scalable coherence protocols often leverage imprecise tracking to lower the overhead of directory storage, in turn leading to more collective communications on-chip. Routers with support for message forking/aggregation have been previously demonstrated, designs today (1-cycle routers), collective ﬂows on a k×k mesh supporting such protocols. However, even with the fastest possible still incur delays proportional to k since all communication is across the entire chip. As k increases across technology generations, the latency of these ﬂows will also go up. However, the pure wire delay to cross the chip is just 12 cycles today, and is expected to remain roughly invariant. The dependence of message delays on k arises due to the requirement to latch messages at every router. In this work, we remove this requirement. We design a network fabric that enables messages to (1) dynamically create virtual 1-to-Many (multicast) and Many-to-1 (reduction) tree routes over a physical mesh, (2) get forked/aggregated at nodes on the tree, and (3) traverse the tree - all within a single-cycle across each dimension. For synthetic 1-to-Many/Many-to-1 ﬂows, we demonstrate 76/82% reduction in latency, and 1.6/2X improvement in throughput over a state-of-the-art NoC with 1-cycle routers and support for collective communication. Across a suite of SPLASH-2 and PARSEC benchmarks, full-system runtime and energy is reduced by 14% and 50% for a limited-directory protocol. I . IN TRODUC T ION In a multicore system, there occur scenarios when all cores need to participate to either service a request or signal the end of a transaction. This collective communication can be further classiﬁed into one-to-many (multicast) and many-to-one (reduction) ﬂows. In the message passing domain, examples include routines like M P I B cast and M P I Reduce/M P I Barrier respectively. In the shared memory domain, these occur in cache coherence protocols that use imprecise sharer information [1], [2] at the directory, for area/power/scalability, and instead resort to broadcasting requests and collecting ACKs to maintain coherence. Without any network support to handle collective communication, a root sending/receiving broadcast/ACKs to/from M cores sends/receives M separate messages, which adds serialization delay and throttles throughput, by at least M , leading to system slowdown. This observation has spawned a lot of recent research in NoCs that support message forking [3], [4], [5], [6], [7], [8], [9] and aggregation [8], [9] at routers. We call this body of work Baseline+Collective in this paper. There has also been work in dedicated reduction networks for barriers [10], [11], [12]. While message forking and aggregation within the NoC lowers the bandwidth demands of these ﬂows, latency is still a concern, since by design the root almost always has to communicate with the furthest cores on-chip1 . This means that even if we can design routers with forking/aggregation support at only 1-cycle delay at every hop [8], the network delay will grow proportional to k in a k-node ring or a k×k mesh. As core count scaling increases k , this starts becoming a concern, potentially requiring expensive on-chip directory structures to minimize long distance communication. Global wire delay is in fact not the problem when it comes to on-chip latencies. Repeated wires have been shown to transmit up to 13-16mm within 1 ns (i.e., ∼62 ps/mm)2 [13], [14], [15], [16]. Given maximum chip sizes of ∼20mm×20mm today, repeated wires can thus enable cross-chip communication within 1-2 cycles at 1 GHz. Absolute wire delay is not going down with technology scaling [13], [17]. But the trend of fairly constant clock frequencies (due to the power wall) and chip dimensions (due to yield) means that the delay in cycles to get from one end of the chip to the other is expected to remain 1-2. Creating a fully-connected topology is however not a feasible solution beyond a few cores, and we need routers to multiplex ﬂows on a shared set of links. These routers lead to the dependence of latency on the number of hops traversed. A highly promising approach to remove this dependence has been to create virtual, reconﬁgurable single-cycle multihop paths over a regular mesh [15], [16]. The idea is to replace the clocked drivers at every router by clockless repeaters, and drive signals across multiple hops within a cycle before they get latched at the destination router. The maximum number of hops (tile to tile distance) that can be traversed in a cycle, or HPC max , depends on the underlying technology. At 45 nm, HPC max is 16 for a pure repeated wire with 1mm tiles at 1GHz, and drops to 11 for a full data-path with a repeater and crossbar (mux) at every hop. Dynamically creating singlecycle multi-hop paths on demand has been demonstrated in SMART [15], [16] and found to be a better solution than both high-radix topology solutions with dedicated wires between a subset of nodes, and meshes with 1-cycle routers. We compare the performance of Baseline+Collective - with 1-cycle-routers, and SMART, against a Baseline mesh (with 1-cycle routers and no collective communication support) and a Fully-connected NoC with 1-cycle dedicated links between all NICs (impractical due to area/power reasons). In Figure 1, we plot the average runtime of a 64-core system running a The authors acknowledge the support of DARPA UHPC, SMART-LEES and MARCO C-FAR. This work was performed while Tushar Krishna was a PhD student at MIT. 1 This is mitigated if the furthest cores are not in the destination set. 2 Experimental Parameters: repeater spacing = 1mm, wire spacing = 3X the minimum to nullify the coupling capacitance.  	  /  #&             *%- *%, *%+ * )%. )%- )%, )%+ )     "" !& !&   % !% ""+-,  Fig. 1: Impact of NoC designs on runtime for HT [1]. limited-state directory protocol, modeled similar to AMD’s HyperTransport [1] protocol3 across a suite of SPLASH-2 and PARSEC benchmarks. Baseline+Collective and SMART give 18.7% and 14.0% performance improvement, respectively, over the Baseline. However, both these designs leave 26-30% performance on the table, compared to a Fully-Connected topology. The goal of this work is to bridge this gap, allowing collective communication ﬂows to achieve the performance of a fully-connected topology on a shared mesh. The key challenge is that all the Baseline+Collective designs require ﬂits to stop at every router, to get forked/aggregated. The SMART approach, on the other hand, is to allow ﬂits to bypass routers completely and only get latched at the destination. These are conﬂicting targets since the former requires the use of intelligent logic within the router, while the latter tries to bypass all logic within the router. This work presents a technique to simultaneously achieve both goals. We present two techniques, SMART-FanOut (SFO) and SMART-FanIn (SFI) that overlay broadcast and reduction trees, with dynamically changing roots (i.e., the source/destination of the broadcast/reduction) over a shared network fabric, such as a mesh, and traverse these trees within a cycle leveraging the single-cycle multi-hop capability of repeated wires. Across synthetic benchmarks, SFO/SFI demonstrate 76/82% reduction in latency, and 1.6/2X improvement in throughput over Baseline+Collective. Full-system simulations over a limited-directory protocol show a runtime reduction of 14% with SFO+SFI, which is only 12% away from the runtime of the Fully-connected topology. The paper is organized as follows. Section II describes relevant background on SMART. Section III and IV present SFO and SFI. Section V presents the evaluations. Section VI contrasts against prior art and Section VII concludes. I I . BACKGROUND : SMART NOC Single-cycle Multi-hop Asynchronous Repeated Traversal (SMART) [15] enables traversals across multiple-hops within one cycle. The key idea is to replace clocked drivers within every router’s crossbar by clock-less/asynchronous repeaters, thus dynamically creating a multi-hop repeated link which can drive signals across 13-16 mm within a GHz [13], [15], [16]. These repeaters are simultaneously setup a cycle in advance through a ﬂow control mechanism to allow multiple ﬂows to create virtual single-cycle multi-hop paths cycle-by-cycle. In conventional NoCs, ﬂits at every router arbitrate among themselves to gain access to the output ports during Switch 3 The distributed directory serves as an ordering point, but does not have any state. Instead it broadcasts all requests, and collects all ACKs. Allocation Local (SA-L). The winner of SA-L traverses the crossbar and output link to the next router, stops, arbitrates for the next link, and so on. In a SMART NoC, ﬂits arbitrate for multiple links and the buffer at the end point, all within the same cycle. Each output port winner from SAL ﬁrst broadcasts a SMART-hop setup request (SSR) up to H P Cmax -hops from that output port. These SSRs - dedicated repeated wires that connect every router to a neighborhood of up to the H P Cmax - help preset the intermediate routers for a multi-hop bypass path. SSRs are log2 (1 + H P Cmax ) bits wide, and carry the number of hops the ﬂit wishes to traverse. Following the initial SSR broadcast, every router performs a second round of arbitration - Switch Allocation Global (SAG) - to arbitrate among the SSRs they have received from the routers in their H P Cmax neighborhood and setup three controls signals: BWena , BMsel and X Bsel . BWena decides whether to latch the incoming ﬂit or not; BMsel at the input of the crossbar switch chooses between the incoming (bypass) ﬂit on the link and a buffered (local) ﬂit, and X Bsel connects an input port to an output port. In the next cycle, the ﬂit performs a multi-hop switch and link traversal till it is stopped at a router with BWena = 1. Figure 2 provides an illustration. Router R0 creates a 3-hop path that can be traversed within a cycle, and the control signals at each router are shown. Competing SSRs. The SA-G arbiters guarantee that only one ﬂit will be allowed access to any particular input/output port of a router crossbar; any conﬂicting ﬂits will be stopped by pulling BWena high. To decide which ﬂit gets to go and which has to stop, every router prioritizes SSR requests according to a ﬁxed priority based on ﬂit distance. For e.g., the Prio=Local scheme gives highest priority to the local ﬂit, followed by the ﬂit from the neighboring router, followed by the ﬂit from the router two hops away, and so on; If a router receives an SSR requesting a bypass, but also has its own ﬂit to send out, it prioritizes the latter by raising BWena to 1, and setting BMsel to local. Single-cycle multi-hop paths are thus opportunistic, not guaranteed. An alternate priority, Prio=Bypass, prioritizes ﬂits from the furthest router over the ﬂits from the nearer ones. All routers need to collectively agree, in a distributed manner and during the same cycle, on which ﬂit is performing a particular multi-hop traversal. This is required to make sure that the ﬂit is latched at its correct destination, and not misrouted beyond the allowed H P Cmax hops. This is guaranteed by enforcing the same relative priority between SSRs at each router - i.e., all routers need to enforce Prio=Local or Prio=Bypass. For bypassing routers at turns, a second-level priority based on direction is also required. Other details about the design (VC allocation, ordering etc) are not relevant to understand the rest of this work. In this work, we use the SMART 1D design, where ﬂits can create single-cycle multihop paths along one dimension at a time, stopping at the turning routers. I I I . SMART-FANOU T: S ING L E -CYC L E BROADCA S T The goal of SMART-FanOut is to accomplish a broadcast within a single-cycle per-dimension. For e.g., in a 8×8 mesh,   	    	 Cin Win [BWena, BMsel, XBsel ]: [0, 0, Cin->Eout] R0 bypass BMsel 0 XBsel local R1 [0, bypass, Win->Eout] BWena R2 R3 [0, bypass, Win->Eout] [1, 0, X] Fig. 2: Example of a Single-cycle Multi-hop Path from R0 to R3 using SMART. The control signals shown are setup one cycle before the actual traversal using separate control wires called SSRs. If R2 has to send a ﬂit out in the same cycle, it can set its BWena to 1 and BMsel to local, prematurely stopping the ﬂit from R0. [15] it should take 2 cycles to reach all of the 64 routers. A. Routing over Virtual Broadcast Trees Broadcasts are routed by creating virtual trees over the physical mesh, such as a XY-tree as shown in Figure 3(a). We call this a Shared Virtual Tree (SVT). Each physical link of the mesh still needs to be arbitrated for, and the forks within the route (i.e., sending copies out of multiple ports) are performed by the router, within a cycle [6], [7], [8]. Broadcasts from multiple sources leads to the overlay of multiple SVTs arbitrating for the same set of links, resulting in either shorter bypass paths in SMART due to premature stopping (if Prio=Local is used) or longer wait times to get the longer bypass path (if Prio=Bypass is used). We present a novel broadcast tree called Private Virtual Tree (PVT) that eliminates contention between broadcasts completely, enhancing the chances of successfully arbitrating for all links along the dimension. PVTs originate from the four corner routers (CR), and use entirely different links to broadcast. All nodes that wish to broadcast ﬁrst send the ﬂit as a unicast to the closest CR, dynamically arbitrating for SMART paths like other unicasts. Since SMART is the underlying fabric, this redirection is relatively inexpensive4 . Figure 3(b) shows an example. Suppose Router 22 wants to send a broadcast. In Step 0, it sends a unicast to CR 40. In Step 1, CR 40 broadcasts the message out of its North output port. We call this the straight dimension. In Step 2, all routers along the right edge (including Router 40) broadcast the message along the West direction. We call this the turn dimension. A copy of the message is retained at all routers as it proceeds along the multihop path. In Step 3, the broadcast ﬂits are delivered to the NIC. Steps 0 and 3 are unicasts (to CR and NIC respectively) and use regular SMART arbitration to resolve contention. Steps 1 and 2 are broadcasts, and PVT ensures no contention for links among the broadcasts ﬂits, as shown in Figures 3(c) and (d). CR 00 uses E-N links, 40 uses N-W links, 44 uses W-S links, and 04 uses S-E links. But there can be contention for links between a broadcast and unicasts, for which we explore two arbitration strategies, Complete and Greedy. B. SMART-FanOut Complete (SFO Complete) In a complete fanout, a multicast ﬂit at a router proceeds only if it has access to all links along the straight/turn dimension during the same cycle. This is ensured by presetting all routers along the straight and turn dimensions into a full bypass path (i.e., BMsel=bypass in Figure 2), one cycle after 4Moreover, in many limited directory protocols, broadcasts only emanate from the corner routers, since these house memory controllers connected to the directory/LLC, removing the need for this redirection. the other. These are called Broadcast Slot Straight (BSS) and Broadcast Slot Turn (BST). BWena is also set to 1 during both the broadcast slots to retain a local copy of the bypassing ﬂit, to send it to the NIC. The time interval between two BSSs is called Broadcast Interval (BI), and is a microarchitectural parameter. For instance, if BI equals 4, then cycle 0, 4, 8,... correspond to BSS, and cycles 1, 5, 9,... correspond to BST5 . During BSS, W→E, S→N, E→W and N→S bypass paths are preset in routers along the bottom, right, top and left edges respectively, as shown in Figure 3(c). During BST, W→E, S→N, E→W and N→S bypass paths are preset at all routers, as shown in Figure 3(d). Steps 1 and 2 occur during BSS and BST respectively, guaranteeing a 2-cycle chip-wide broadcast. No unicast ﬂits are allowed to use the reserved paths in these cycles. This is achieved by blocking SA-G for the appropriate ports one-cycle before BSS and BST. A subtle point to note is that arbitration for the NIC (i.e, Step 3) by buffered ﬂits within the routers (unicast or broadcast) also needs to be blocked for the broadcast slots. This is because the input bandwidth of all crossbars at the participating routers is allocated to the bypassing ﬂits on single-cycle multi-hop paths, so locally buffered ﬂits at the same input ports cannot use this bandwidth during this cycle. If the mesh were to be designed exclusively for broadcasts, the theoretically minimum value of BI is 3: two cycles for BSS and BST, and one for Step 3 and/or 0. 1) Deadlocks. Figure 3(b) shows that PVT allows X to Y, Y to X, and u-turns (e.g., the N port at CR 40). To avoid deadlocks, we divide the VCs in the broadcast virtual network into 3 classes: VCto CR , VCbef ore turn , VCaf ter turn . The ﬁrst is used by “broadcast” ﬂits to reach the CR via unicast. The ﬂits then switch to VCbef ore turn and traverse the ﬁrst dimension (X or Y depending on the particular CR). At the turn, they switch to VCaf ter turn . There is no cyclic dependency. 2) Buffer Management. The VCto CR is used by unicasts and sends a on-off signal only to its nearest neighbors [15]6 . The VCbef ore turn and VCaf ter turn classes send one-bit on-off signals each up to H P Cmax hops via repeated wires (which are inherently multidrop). Step 1 from the CR occurs only if all routers along that dimension have a free buffer to hold the broadcast ﬂit in the VCbef ore turn class. If not, it waits till its next time slot to 5 If H P Cmax is less than the number of nodes in a dimension, the leaves that lie within H P Cmax to 2×H P Cmax , receive the broadcast one cycle later by statically shifting BSS and BST across the routers, and so on. 6A ﬂit is conservatively pre-emptively stopped at a router if its neighbor does not have free VCs, since bypass at the neighbor is not guaranteed. 02 01 03 00 04 12 11 13 10 14 22 21 23 20 24 32 31 33 30 34 42 41 43 40 44 02 01 03 00 04 12 11 13 10 14 22 21 23 20 24 32 31 33 30 34 42 41 43 40 44 02 01 03 00 04 12 11 13 10 14 22 21 23 20 24 32 31 33 30 34 42 41 43 40 44 02 01 03 00 04 12 11 13 10 14 22 21 23 20 24 32 31 33 30 34 42 41 43 40 44 -1 0 1 2 3 1 1 1 1 2 2 2 2 (c) Broadcast over straight dimension (d) Broadcast over turn dimension (b) Private Virtual Tree (PVT) (a) Shared Virtual XY Tree (SVT) CR:  Corner Router Fig. 3: SMART-FanOut (SFO). send. Similarly for Step 2. SFO Complete does not send ﬂits to only a subset of nodes along the dimension. C. SMART-FanOut Greedy (SFO Greedy) In a greedy fanout, the broadcast ﬂit traverses its route (PVT or SVT) opportunistically, trying to reach as many of its leaf routers as possible within a cycle. While it has a higher chance of reserving all links along the dimension in a PVT than in a SVT, competition with unicast ﬂits, and/or insufﬁcient buffers can lead to shorter express paths, and re-arbitration during subsequent cycles for the rest of the path. Broadcast ﬂits arbitrate for multiple output ports (depending on their route). The winner at each output port sends out an SSR along that direction, requesting a bypass path till the end of the dimension, with an additional broadcast bit. During SA-G, in addition to the appropriate setup of the BMsel and X Bsel signals, BWena is also made high to retain a copy of the incoming ﬂit at this router. In the next cycle, the ﬂit performs a multi-hop switch and link traversal along the dimension. The retained copies arbitrate for the ejection port, and the turn dimension. A broadcast’s bypass could get terminated (BMsel = 0 or local) at a router R before traversing the entire dimension for the following reasons: (a) it loses SA-G to some locally buffered ﬂit at R if Prio=Local were used, (b) the neighbor of R does not have free buffers7 . D. Multicasts Multicasts are delivered exactly like broadcasts in both schemes, with copies being retained at every router. This copy is dropped if the current NIC is not part of the destination set8 . IV. SMART-FAN IN : S ING L E -CYC L E R EDUC T ION We target the scenario where M nodes send an “ACK” each to the same destination, where it is ultimately combined via some reduction operator (ADD, OR, MIN, MAX etc). Examples include acknowledgments in coherence protocols, barrier synchronization, M P I reduce routine in MPI, and so on. M separate ACKs cause latency, throughput and energy overheads. The goal of SMART-FanIn is to perform a distributed reduction within the network on a single-cycle (per-dimension) multi-hop path, so that the destination receives only one ACK with the result of the reduction - a count of M if the operator is ADD. We assume an ADD operator, though any associative and commutative operator works as well. 7We do not allow a non-continuous subset of routers to receive the broadcast to avoid tracking which routers the broadcast was not delivered to. 8Our design is optimized for broadcasts and dense multicasts. If the destination set is very sparse, it will be more efﬁcient to send it as separate unicasts with SMART paths rather than create the broadcast tree. A. SMART-FanIn Complete (SFI Complete) In a complete fanin, the destination NIC receives exactly one ACK representing an aggregate of all ACKs. The waiting cache/directory controller can proceed as soon as this ACK is received. Conceptually, all injected ACKs wait indeﬁnitely at routers, before their ACK count is added into the last ACK that is injected into the system. But we show an optimized implementation where most ACKs can be dropped immediately upon injection, and no explicit addition is required. 1) Microarchitecture. We add 2 extra ﬁelds to the SSRs: 1-bit is ACK , and k-bit ACK id. ACK id is used to identify ACKs from the same ﬂow. We add a central table called ACK Reduction Table (ART) with 2k entries, one for each ACK id. Each entry is only 3-bits: 1-bit reserved, and 2-bit num dir - to hold the count of the number of directions from where to aggregate ACKs. An ART entry is reserved by the preceding broadcast at its root/source, and used by the response ACKs. For each ACK id, exactly num dir separate ACKs enter each router. Till num dir is greater than 1, incoming ACKs are stopped (BWena = 1) and dropped; num dir is decremented by 1. If num dir equals 1, the incoming ACK is the last ACK this router is waiting for, and is allowed to bypass or get buffered, decrementing num dir to 0. The buffered ACK arbitrates for the switch to proceed further. 2) Walk-through Example. We present a walk-through example of how SFI Complete works, using Figure 4. The example is a simple one where there is only one M -to-1 (M =24) ﬂow in the system, with all nodes sending ACKs to the root NIC at router 40. Step #-1 (Broadcast Slot): Suppose the broadcast is sent out from Router 40 in a YX manner, as shown earlier in Figure 3(b). The broadcast sets up the control circuitry for the ACK. At the root CR 40, ART [0] is reserved for the ACKs by setting reserved to 1, and ACK id 0 is embedded into the broadcast ﬂit, and will be sent out with the ACKs. The num dir count is appropriately set at all routers, for the XY routing by ACKs in this example. At Router 40, it is set to 2 to account for ACKs coming in from W and N9 . At Router 00, it is set to 1 as an ACK from only the NIC will enter this router. At Router 41, it is set to 3 to account for ACKs from the W, N and NIC10 . 9 There is no ACK from NIC 40 as it is the source of the broadcast. 10 If a multicast, not a full broadcast, was sent out, the num dir would not count ACKs from those NICs that are not in the destination set. 0 1 0 02 0 1 0 01 0 1 0 03 0 1 0 00 0 1 0 04 0 2 1 12 0 2 1 11 0 2 1 13 0 2 1 10 0 2 1 14 0 2 1 22 0 2 1 21 0 2 1 23 0 2 1 20 0 2 1 24 0 2 1 32 0 2 1 31 0 2 1 33 0 2 1 30 0 2 1 34 0 3 2 42 0 3 2 41 0 3 2 43 2 40 44 (c) (d)  (b) (a) 0 0 2 1 0 02 0 01 0 03 0 00 0 04 0 1 0 12 0 1 0 11 0 1 0 13 0 1 0 10 0 1 0 14 0 1 0 22 0 1 0 21 0 1 0 23 0 1 0 20 0 1 0 24 0 1 0 32 0 1 0 31 0 1 0 33 0 1 0 30 0 1 0 34 0 2 1 42 0 2 1 41 0 2 1 43 0 2 1 40 0 1 0 44 0 0 0 0 0 0 1 0 02 0 1 0 01 0 1 0 03 0 1 0 00 0 04 0 12 0 11 0 13 0 10 0 14 0 22 0 21 0 23 0 20 0 24 0 32 0 31 0 33 0 30 0 34 0 1 0 42 15 0 1 0 41 20 0 1 0 43 10 0 1 0 40 0 44 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 5 24 ACK_id Row 0 num_dir 0 02 0 01 0 03 0 00 0 04 1 0 1 0 12 0 1 0 11 0 1 0 13 0 10 0 14 1 22 0 1 0 21 0 1 0 23 0 20 0 24 1 32 0 1 0 31 0 2 1 33 0 30 0 34 2 42 0 2 1 41 0 2 1 43 1 40 0 44 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 1 2 3 4 0 1 2 X X X X X X X X X X X X X X X X X X X X X X X X X X Row 1 Row 2 Row 3 ACK Reduction Table  (ART) ACK  Count ACK dropped Row 4 Row 0 Row 1 Row 2 Row 3 Row 4 Fig. 4: SMART-FanIn (SFI). Step #0 (ACK injection): All NICs inject ACKs into their routers. All NICs might not (and probably will not if there is contention at the NIC/cache) inject in the same cycle. We assume the same cycle just for the sake of this example. This is shown in Figure 4(a). The ACKs at Routers 00, 01, 02, 03 and 04 have ART [0].num dir = 1, and get buffered to arbitrate for the output port (i.e., SA-L) competing with other buffered ﬂits. All other ACKs have num dir > 1 and are dropped. ART [0].num dir is decremented by one. Step #1 (ACK X traversal): The ACKs that win SA-L (assumed to be all ready ACKs - 00, 01, 02, 03 and 04), send out SSRs along the X (East) direction, with is ACK set to 1, and ACK id set to 0. Let us look at the Row 0 in Figure 4(b). Routers 10, 20, and 30 have ART [0].num dir as 1, and hence setup a single-cycle multi-hop bypass path for Router 00 till Router 40. In the next cycle, Router 00 sends the ACK in a cycle to 40, and frees ART [0]. At 10, 20, and 30, ART [0].num dir is decremented to 0, and freed. The bypassing ACK has effectively aggregated the ACKs at the routers it bypassed. The same holds for other rows. At Routers 43, 42, 41, and 40, ART [0].num dir is 2, and so these incoming ACKs from West are dropped, decrementing num dir to 1. At Router 44, ART [0].num dir is 1, and the incoming ACK from West is buffered, becoming the only active ACK for this M-to-1 ﬂow. Step #2 (ACK Y Traversal): This aggregating traversal is repeated along the Y direction. Figure 4(c) shows that Router 44 has ART [0].num dir = 0, and tries to setup a single-cycle path to Router 40. Routers 43, 42, 41 and 40 setup a multihop bypass path since num dir is 1. The ACK from Router 44 bypasses all these routers, leaving one ACK representing a count of 24 at Router 40. This ACK is sent up to the NIC. We have achieved perfect aggregation. 3) Handling arbitrary delays. If num dir is not 1, BWena is pulled high during SAG, and the incoming ACK next cycle is latched. We allow only the ﬁnal ACK to bypass through a router to aggregate all other ACKs. Figure 4(d) demonstrates a snapshot of the network at a certain cycle, demonstrating some scenarios that introduce arbitrary delays across the ACKs. Row 0: We can infer from num dir=0 at 00, 10, 20 and 30 that their ACKs have already been delivered to 40 as an aggregated ACK, and then dropped since num dir at 40 is 1. Row 1: The ACK from 01 is aggregating ACKs at 11, 21, 31 and 41. Row 2: Router 02 has num dir equal to 0, so its ACK has been sent to 12 and dropped. The NIC at 12 is injecting an ACK this cycle that will decrement num dir to zero and get buffered. It will then be able to perform a multi-hop aggregating traversal to Router 42. Row 3: The ACK from Router 03 aggregates ACKs at 13 and 23, but stops at 33, since num dir was 2 as the ACK from the NIC has not been received yet. Row 4: The aggregated ACK from 44 tries to perform a multi-hop traversal to 40, but is dropped at 43, as num dir is 2. Once the ACK from NIC 33 is injected, it can reach 43, making num dir 0. This ACK would then be able to go to 40, bypassing (i.e., aggregating) 42, 41, and 40. We are able to achieve complete aggregation in this scenario as well, though not all ACKs were able to create long SMART paths, increasing the latency of the ﬁnal received ACK. 4) Route. The destination of the ACKs (the requester) could be different from the source of the broadcast (a CR - if PVT is used - or a memory controller). The requester id ﬁeld from the broadcast ﬂit is used to compute num dir for the ACKs for a pre-decided XY or YX routing. 5) ART entries. All unique broadcast roots/sources need to choose different ART entries, thus making sure that ACKs from two separate active ﬂows do not get assigned the same ACK id. The minimum number of entries is 4 for PVT and N (number of cores) for SVT. We use a 64-entry ART (i.e., one per requesting core), derived empirically. Using an ART reduces ACK trafﬁc, allowing us to remove a 128-bit buffer from the response VCs at each input port and use those to build the ART, thus adding no area or energy overhead [18]. A root is allowed to reassign an ART entry only after its ACKs have been received at the destination. If the ACK destination is the same as the root, this is easy to enforce. But if the ACK destination is different from the root, the root should not free the ART entry even after it reaches num dir = 0 (the other routers should). The destination upon receiving the aggregated ACK sends a separate message to the root to free the entry. We piggy back this message on the regular coherence unblock messages to the directory at the CR, thus adding no overhead. If there are no free ART entries for a broadcast to reserve, it marks its ACK id as invalid. The corresponding ACKs do not try and aggregate, and are instead delivered as separate unicasts, which is functionally correct. Reducer 1 + 0 1 OR header operand SSR.ACK_id operand 1 id num_dir 1 0 BWena !=1 0 1 Fig. 5: Reducer (OR) on datapath. 6) Deadlocks. ART entries conceptually represent indeﬁnitely waiting ACKs. We avoid deadlocks since each ACK ﬂow has a reserved ART entry at every router, and does not block ACKs from another ﬂow. 7) Implementing other Reduce operations. SFI can implement other Reduce operations such as OR, MAX, MIN, etc, by adding a Reducer block on the datapath, as shown in Figure 5. The protocol remains exactly the same, except that the ART has an extra ﬁeld containing the operand value of the dropped ACK. The Reducer operates on the operands from the bypassing ACK and the one from the ART, and the result is sent out. 8) Timing. On the control path, we added a check to set BWena to 1 ifART [ACK id].num dir is not 1. This lookup did not affect the critical path (RTL implementation of SA-G) which was dominated by setting up the crossbar signals. On the datapath, if the Reducer is used, its gate delay could affect H P Cmax which dropped from 11 to 9 for a 2-input OR (Spice simulations at 45nm). B. SMART-FanIn Greedy (SFI Greedy) In a greedy fanin, ACKs are opportunistically aggregated with no explicit waiting. This means that the root could receive one or more partially aggregated ACKs. The ART is not used. The unique ACK id of each ﬂow is identiﬁed as (mshr idx, dest id) where mshr idx is the index of the MSHR at the root node from where the broadcast was injected. Buffered ACKs poll incoming SSRs, while arbitrating for the switch, and aggregate any ACKs with the same ACK id stopping at the router. For Prio=Bypass, we add an adder as the Reducer in Figure 5 to aggregate ACKs bypassing this router, if an ACK id match was found during the SSR stage. V. EVALUAT ION We implement SFO and SFI in the cycle-accurate NoC simulator Garnet [19], available within the GEMS [20] infrastructure.We model a 8×8 mesh for all our runs. Baseline+Collective, described in Section I, uses the SVT. For SFO and SFI, we assume H P Cmax=8 [15].11 A. Synthetic 1-to-Many Trafﬁc We start by evaluating the ﬂavors of SFO with synthetic broadcast trafﬁc. The metric of performance is 1-to-M latency, which we deﬁne as the time taken to deliver the multicast to all its destinations. We also plot the Ideal in each graph, assuming 1-cycle contention-less traversal, and theoretical throughput for that particular trafﬁc over a mesh. Broadcast from CRs. Figure 6a plots the 1-to-M latency as a function of injection rate, when only the 4 CRs inject. SFO 11A smaller H P Cmax , say 4, which we do not present in the interest of space, increases per-dimension latency to 2 cycles. Broadcast lowers broadcast delivery time by 73-86% over the baseline. We ﬁrst analyze SFO Complete. In this design, the complete broadcast takes exactly two cycles. The best low-load latency is 3.8 cycles, offered by Broadcast Interval (BI) = 4. With BI=3, there is only one non-blocked time slot for the ﬂits going up to the NIC (Step 3 in Figure 3(b)), adding penalties to ﬂits that just missed the time slot, increasing low-load latency to 5.4 cycles. For BI=6, 8 and 10, the wait cycles for the broadcast slots increases low-load latency to 5-7.5 cycles. The best throughput is offered by BI=6, which is 1.36-2.14X higher than other BIs. This is because 6 cycles is exactly enough to cover the 2 broadcast slots, and 4 cycles to send broadcast ﬂits from each CR serially up to the NIC. A BI lower than this throttles throughput, while a BI higher than this adds wait times. The greedy scheme offers 83% reduction in low-load latency, and similar throughput as baseline with Prio=Local12 . from all nodes. When all nodes inject, as shown in Figure 6b, the role of PVT and SVT come into play. SFO Complete with BI=6 over the PVT has 62% lower latency and similar throughput as the baseline which is over a SVT. SFO Greedy over SVT (with no redirection to the CR) provides the best latency of 5.6 cycles, and a throughput that is 1.22X better than baseline and 30% away from the ideal. SFO Greedy with Prio=Local over the PVT shows 1.55X higher throughput than the baseline, and is only 10% away from the theoretical ideal on a mesh, demonstrating the throughput beneﬁts of the PVT. The 3-4 cycle redirection latency to get to the nearest CR increases its low-load latency. This points to the interesting design space tradeoffs enabled by SMART where a low-latency redirection enabled a large throughput enhancement. Multicast from all nodes. Figure 6c shows the performance for multicast trafﬁc, with every node injecting to a randomly chosen set of destinations. Here SFO Complete with BI=4 gives better throughput than BI=6 since fewer ﬂits need to go the NIC. SFO Greedy provides 75% reduction in lowload latency, and 11% higher throughput than the baseline. The SFO PVT is optimized for broadcasts, and does not offer signiﬁcant throughput advantages to multicasts. In summary, of all ﬂavors of SFO, the greedy scheme over SVT provides the lowest latency, and the greedy scheme over PVT provides the highest throughput. B. Synthetic Many-to-1 Trafﬁc Synthetic many-to-1 trafﬁc represents ACK trafﬁc in coherence protocols or the MPI reduce primitive in MPI. In this pattern, all nodes (except the destination, so 63 in this case) inject “ACKs” - with the same ACK id - directed to a randomly chosen destination, at a speciﬁed injection rate. We do not inject a broadcast to reserve ART entries. Instead, we reserve them magically prior to the ACK injection, and free them according to the SFI scheme described earlier. In 12 Prio=Local offers better throughput than Prio=Bypass [15]. This is because Prio=Bypass can reserve output links for bypass ﬂits, at the cost of locally buffered ﬂits, yet no ﬂit may actually show up due to some SSR interaction at the previous routers it is unaware about. Prio=Local would have sent the local ﬂits on the output links instead.    ! "" # $  $    $ ! !$                          	   (a) Broadcasts from 4 corner routers. '	  (  (	 ("" (# (% (& (   # "" !      ! $ %   #                          	   (b) Broadcast from all nodes. &  	'  	'$  	'  	'""  		 		 	 # "" !      #  #    # !                          	   (c) Multicast (random # dest) from all nodes. %  	&  	&$  	&  	&""  Fig. 6: SMART-FanOut (SFO) with synthetic 1-to-many trafﬁc. "" !# !  #   #  #     "" $  %                     		 	   &     	'   	'    (a) Latency for receiving all 63 ACKs. ""  !% !$ !# !"" !  % $ # ""      ""  #  $  % !                   			 (b) ACKs received / M-to-1 Flows Injected. &	  '	  '   	 	  	  ""    !  ""        ! ""                  	   	 	#		 		 $ 		 (c) Avg ART entries occupied. Fig. 7: SMART-FanIn (SFI) with synthetic many-to-1 trafﬁc -/&. -/&/ -/&- -/&1 -/&, -/&- -0&. -.&. -/&- -/&- -2&+ -/&0 4&4 ,,&. ,,&. ,,&1 4&4 4&3 2&3 1&. 3&- 1&. 1&, 4&+ ,-&1 ,/&- ,.&0 ,.&0 ,-&4 ,-&. 4&/ 4&, ,+&2 4&. 2&, ,,&. , , , , , , , , , , , , .+ -0 -+ ,0 ,+ 0 + 5! ,&-  $ , +&3 +&1 +&/ +&- +            #  ""   '    ""   '          &               & ""    & # / 1                                        	'($ ,''$ 	' ($ (a) Token Coherence (TC) Protocol Performance. -.&- --&3 -.&. -.&1 --&- --&0 4&3 4&/ -,&. -,&2 4&- -,&2 2&- 2&- 2&. 2&- 1&5 2&, 1&/ 1&1 1&3 1&5 1&/ 1&4 - - - - - - - - - - - - - - - - - - - - - - - - -.&1 -, 3&1 1 .&1 , - ,&4 ,&2 ,&0 ,&. ,            #  ""   (    ""   (          &               & ""    & # 0 2 .                                   6!  (()(  (()(  $( !'	((-""  	 (b) HyperTransport (HT) Protocol Performance. Fig. 8: Full-system impact of SFO and SFI for a 8×8 CMP. Figure 7a, the x-axis plots the injection rate for each 63to-1 ﬂow, and the y-axis plots the M-to-1 latency - cycles it took to receive one (or more) ACKs with a combined count of 63. For the baseline, the average M-to-1 latency is ∼25 cycles, till the network saturates at an injection rate of 0.44 M-to-1 ﬂows/cycle. SFI Greedy (Prio=Local) works almost identical to the baseline since all ACKs are forced to stop at every router, to prioritize its locally buffered ACKs over the bypassing ones to use the links. With SFI Greedy (Prio=Bypass), the very same locally buffered ACKs now get aggregated into the higher-priority bypassing ACK, as explained earlier in Section IV-B, reducing the latency to 5.7-8.9 cycles, before the network saturates at an injection rate of 0.8. With SFI Complete, the latency goes down to 4.7-7.1 cycles, and the network does not saturate even at an injection of 1 M-to-1 ﬂow every cycle. SFI Complete reduces the effective injection rate by M enhancing throughput. Figure 7b plots the aggregation ratio, i.e., number of ACKs received per M-to-1 ﬂow. The SFI Complete meets the ideal by delivering only one ACK with a count of 63 for each ﬂow. SFI Greedy (Prio=Bypass) delivers 4 ACKs on average - this number goes up with arbitrary delays between ACK injection which will be explored with full-system trafﬁc later in Section V-C. The baseline and SFI Greedy (Prio=Local) wait for 1 cycle on average at every router (during which time they opportunistically aggregate), and deliver 12-18 ACKs per ﬂow before saturation. Once they saturate, ACKs are forced to wait for 4.5 cycles on average, increasing aggregation ratio to the ideal 1 ACK per ﬂow. Figure 7c plots the number of distinct ACKs13 at each router, on average. This represents the total number of active M-to-1 ﬂows in the system at any time. For the baseline, the number of ACKs goes up to 9 before saturation (at which point it shoots up to 20). For SFI, the number of ACKs (i.e., ART occupancy) is about 4. In summary, the complete SFI scheme provides the lowest latency, and highest throughput for reduction trafﬁc. C. Full-system Performance We run the parallel sections of SPLASH-2 [21] and PARSEC [22] through Wind River Simics [23] with 64 in-order SPARC cores, connected to the GEMS [20] timing model. We model 32kB Private I&D L1, and 1M Private L2 caches per tile. We run two coherence protocols: (1) Token Coherence (TC) [2], which has 52% broadcast ﬂows, and (2) HyperTransport (HT) [1], which has 14% broadcast and 14% ACK ﬂows, on average, across these workloads. SFO on TC. Figure 8a plots the runtime and 1-to-M latency of SFO Greedy with SVT and PVT, compared to the baseline and fully-connected NoCs, all running TC. SFO provides 16% runtime reduction over the baseline. There is no signiﬁcant difference between the runtime in SVT and PVT implementations since most workloads are not network bandwidth limited. 13 Two ACKs from the same ﬂow cannot be simultaneously buffered at a router in both Base+Coll and SFI, as one would be aggregated into the other.                          	 	          	                             	                	             The exception is water-nsq where SVT Greedy provides 16% runtime improvement while PVT Greedy provides 35%. The average 1-to-M latency over PVT is 3 cycles more than over SVT due to the redirection. SFO+SFI on HT. Figure 8b plots the runtime and ACKs received per M-to-1 ﬂow for the SFI Greedy (Prio=Bypass) and Complete schemes, with the SFO Greedy scheme, compared to the baseline and fully-connected, all running HT. SFI Greedy provides 11% runtime reduction on average, with SFI Complete reducing it by a further 3% on average. The baseline, which has an optimized aggregator at each router, delivers 8-12 ACKs per M-to-1 ﬂow. SFI Greedy reduces this to 5-6, and SFI Complete reduces it to exactly 1. While SFI Greedy and SFI Complete have similar performance, the latter consumes 50% less energy, and the former 29%, than the baseline, due to the fewer buffer writes/reads and link traversals. This is despite the crossbar energy per access in SFI being 1.58 times higher than in the baseline to account for the bypass muxes and repeaters driving wires multi-hop [15]. One ART entry per core sufﬁces for all this performance gain and energy savings, and the average ART occupancy was 20 for SFI Complete. V I . R E LAT ED WORK Networks with Multicast Support. On-chip routers with message forking support have been proposed recently in works like VCTM [3], bLBDR [5], MRR [4], RPM [6], FANOUT [8]. Our baseline is derived from these, and assumes a highly-optimized 1-cycle forking delay [6], [8]. Tree-based multicast routing algorithms like Whirl [8] and BAM [9] (optimized over RPM [6]) try to utilize the network bandwidth and buffers more efﬁciently. Our PVT has a similar goal, and succeeds in completely eliminating contention between multicasts, at the cost of redirection to a CR. SFO is the ﬁrst work to demonstrate single-cycle traversals across multiple nodes of a multicast tree. Networks with Reduction Support. In the off-chip domain, dedicated reduction networks for barrier synchronization have been used in supercomputers like NYU Ultracomputer [24] and IBM Blue Gene/L [25]. Aggregation of memory requests was done in IBM RP3 [26] and NYU Ultracomputer [24]. In the on-chip domain, barrier synchronization is performed over dedicated global broadcast wires [11], [12], [27] or transmission lines [10]. Com [9] and FANIN [8] perform in-network reduction without adding a dedicated reduction network, and form our baseline. In FANIN the ﬁrst ACK for a ﬂow to enter a router opportunistically aggregates other ACKs of the same ﬂow for a heuristically deﬁned wait time, before proceeding further. Com uses a central CAM at every router to perform an addition of ACK counts, and requires ACKs to return to the broadcast root. Our ART avoids a CAM lookup by ACK id indexing, can handle ACKs ﬂowing to a different node than the root, and can also handle any reduction operation. SFI is the ﬁrst work to demonstrate reduction across multiple routers within the same cycle. V I I . CONC LU S ION In this work, we present SMART-FanOut and SMARTFanIn that enable forking and reduction respectively across multiple nodes in a dimension within a single-cycle, leveraging clockless repeated wires on the datapath. We explore greedy and complete forking/aggregation approaches, and conclude that a greedy forking and complete aggregation strategy provides the best performance at the lowest energy. Going forward, single-cycle collective communication enabled by SFO and SFI, without the overhead of dedicated networks, can provide scalability to limited-directory protocols and MPI as we scale core counts. "
A novel non-minimal/minimal turn model for highly adaptive routing in 2D NoCs.,"Networks-on-Chip (NoCs) are emerging as a promising communication paradigm to overcome bottleneck of traditional bus-based interconnects for current micro-architectures (MCSoC and CMP). One of the current issues in NoC routing is the use of acyclic Channel Dependency Graph (CDG) for deadlock freedom. This requirement forces certain routing turns to be prohibited, thus, reducing the degree of adaptiveness. In this paper, we propose a novel non-minimal turn model which allows cycles in CDG provided that Extended Channel Dependency Graph (ECDG) remains acyclic. The proposed turn model reduces number of restrictions on routing turns, hence able to provide path diversity through additional minimal and non-minimal routes between source and destination.","A Novel Non-minimal/Minimal Turn Model for Highly Adaptive Routing in 2D NoCs Manoj Kumar1 , 3 , Vijay Laxmi1 , Manoj Singh Gaur1 , Masoud Daneshtalab2 , Pankaj1 , Seok-Bum Ko3 and Mark Zwolinski4 1Malaviya National Institute of Technology, Jaipur, India bohra.manoj1980@gmail.com, vlaxmi@mnit.ac.in, gaurms@mnit.ac.in, pankaj.cs.009@gmail.com 2University of Turku, Turku, Finland masdan@utu.ﬁ 3University of Saskatchewan, Saskatoon, Canada seokbum.ko@usask.ca 4University of Southampton, Southampton, United Kingdom mz@ecs.soton.ac.uk Abstract—Networks-on-Chip (NoCs) are emerging as a promising communication paradigm to overcome bottleneck of traditional bus-based interconnects for current microarchitectures (MCSoC and CMP). One of the current issues in NoC routing is the use of acyclic Channel Dependency Graph (CDG) for deadlock freedom. This requirement forces certain routing turns to be prohibited, thus, reducing the degree of adaptiveness. In this paper, we propose a novel non-minimal turn model which allows cycles in CDG provided that Extended Channel Dependency Graph (ECDG) remains acyclic. The proposed turn model reduces number of restrictions on routing turns, hence able to provide path diversity through additional minimal and non-minimal routes between source and destination. Keywords—Networks-on-Chip, deadlock freedom, routing, nonminimal paths, degree of adaptiveness. I . EX TEND ED AB STRAC T The turn model representation is an easy and effective way to describe routing algorithms and their routing restrictions. Figures 1a and 1b show turn model representation of Mady [1] and LEAR [2] routing algorithms, respectively. In order to achieve deadlock freedom, Mad-y imposes following constraints on routing turns: 1) 2) 3) It prohibits four 90-degree turns (N 2-W , S 2-W , E N 1 and E -S 1) as shown in Fig. 1a(i) and 1a(ii). It also prohibits two 0-degree turns (S 2-S 1 and N 2N 1) as shown in Fig. 1a(iii). Since it is a minimal routing algorithm, it does not permit any 180-degree turn. The LEAR turn model has same turn constraints as in the Mad-y (Fig. 1b(i), 1b(ii), and 1b(iii) ) except 180-degree turns. The Mad-y model prohibits all 180-degree turns while LEAR model allows some 180-degree turns as shown in Fig. 1b(iv). A. Proposed Turn Model An acyclic CDG requirement to avoid deadlocks places unnecessary restrictions on the routing turns in a routing algorithm. Both Mad-y and LEAR routing methods are proved deadlock-free using acyclic CDG [3]. Thus, their routing functions cannot use all qualiﬁed turns to forward packets through less congested areas. The proposed model imposes substantially fewer restrictions on routing turns (specially on 90-degree) using [4], thus it provides additional minimal and non-minimal paths between source and destination nodes than Mad-y and LEAR. Figure 1c shows turn model representation of the proposed method. A packet is permitted to use ﬁrst virtual channel (N 1 or S 1) at any time as shown in Fig. 1c(i). It can use second virtual channel (N 2 or S 2) only if it has already routed to negative direction of X dimension (west). This is because, the packet cannot take west turn after using (N 2 or S 2) as shown in Fig. 1c(ii). It can take only two 180degree turns from west to east (W -E ) and south to north (S 2N 2) as shown in Fig. 1c(iv) only if it has completed routing in west and south directions, respectively. In short, in order to avoid deadlocks, the proposed method imposes following constraints on routing turns: 1) 2) 3) It prohibits two 90-degree turns (S 2-W and N 2-W ). It allows 0-degree turns (S 1-S 1, N 1-N 1, S 2-S 2 and N 2-N 2) as shown in Fig. 1c(iii). It allows 0-degree turns (S 1-S 2, N 1-N 2, S 2-S 1 and N 2-N 1), as shown in Fig. 1c(iii), with some restrictions. It allows these restricted turns only when packet does not need to be forwarded further west. It permits some 180-degree turns (W -E and S 2-N 2). The proposed method provides more minimal and nonminimal paths between source and destination by allowing some routing turns which were prohibited in Mad-y and LEAR. For example, E -N 1 (i.e. a packet moving from east to north-1 virtual channel) and E -S 1 (i.e. a packet moving from east to south-1 virtual channel) are permitted by the proposed method. However, these permitted routing turns create cycles in CDG, but we have shown that proposed routing method is deadlock free using Duato’s well known theorem [4]. Duato has proved that cycles can be allowed in CDG provided that ECDG is acyclic. B. Deadlock and Livelock Freedom With deterministic routing, packets can be routed over single output channel at each node. Thus, it is mandatory to remove all cyclic dependencies between network channels in order to achieve deadlock freedom. In adaptive routing, packets (a) Mad-y (b) LEAR (c) Proposed Turn Model Fig. 1: Turn models (solid lines for permitted turns and dash lines for prohibited turns) often have several options for routing at each node. Thus, it is not mandatory to eliminate all cyclic dependencies between channels, provided that every packet can be forwarded on a route whose channels are not involved in cyclic dependencies. The channels involved in these acyclic routes are considered as escape channels from deadlocks (cycles). The deadlock-freedom of the proposed method is assured by using Duato’s theory [4] stated as follows: Theorem 1: (Duato’s Theorem) For a given interconnection network I , a connected and adaptive routing function R is deadlock-free if there exists a routing subfunction R1 ⊆ R, that is connected and has acyclic extended channel dependency graph. Following Duato’s terminology, the route computation function of the proposed method is denoted by R and the set of channels used by R is denoted by C . To assure deadlock freedom of the proposed method, we ﬁrst identify the subset of channels C1 ⊆ C , that deﬁnes routing subfunction R1 ⊆ R that is connected and has an ECDG with no cycles arising from direct, indirect, direct-cross and indirect-cross dependencies. For the proposed method, C1 has all virtual channels except N 1 and S 1. Lemma 1: The routing subfunction R1 is connected. Proof: R1 routing subfunction with channel set C1 is non-minimal version of west-ﬁrst [5] routing method. Since non-minimal west-ﬁrst routing function is connected, so R1 is connected. Lemma 2: Extended channel dependency graph of channel set C1 with additional channel (N 1 and S 1) introduced by R is acyclic. Proof: There is no direct-cross dependency in ECDG of C1 as routing function R does not add any new routing option between channels of C1 directly. However, additional channels introduced by routing function R add new routing options between channels of C1 indirectly, but it does not produce any indirect-cross dependency. Additional channels introduced by R can cause only indirect dependencies between west channels as a packet can use west channel and later can use west channel of different row and column. But this indirect dependency does not introduce any cycle in ECDG of C1 . The ECDG for C1 has no dependencies from a channel in north, east, or south directions to a channel in the west direction, so the west channels are always used before all other channels in C1 . Hence, these indirect dependencies introduce new dependencies between only the west virtual channels and create no cycles using only the west virtual channels. Since there are no indirect and direct dependencies, which produce cycle in ECDG. Therefore ECDG of C1 is acyclic. Theorem 2: The proposed routing algorithm is deadlockfree. Proof: It can be concluded from Lemma 1 & Lemma 2 and using Theorem 1 that the proposed routing algorithm is deadlock-free. Non-minimal routing algorithms are susceptible to livelock. The proposed routing algorithm is proved to be livelock free using following theorem. Theorem 3: The proposed routing algorithm is livelockfree. Proof: We can observe from the proposed method (Fig. 1c) that whenever a packet is routed in the east direction, it is not allowed to route the packet back in the west direction. So, in the worst scenario, the packet may reach to the west most column then it starts to move towards destination column. Similarly, whenever a packet is routed in the north direction, it is not allowed to route back in south. In each dimension, only one 180-degree turn is allowed. Therefore, after a limited number of hops, the packet reaches to its destination node. Thus, proposed routing algorithm is livelock free. ACKNOW L EDGM ENT This research is partially supported by UK India Education and Research Initiative grant for the collaborative project on HiPER NIRGAM (2011-2014). "
Towards stochastic delay bound analysis for Network-on-Chip.,"We propose stochastic performance analysis in order to provide probabilistic quality-of-service guarantees in on-chip packet-switching networks. In contrast to deterministic analysis which gives per-flow absolute delay bound, stochastic analysis derives per-flow probabilistic delay bounding function, which can be used to avoid over-dimensioning network resources. Based on stochastic network calculus, we build a basic analytic model for an on-chip router, propose and exemplify a stochastic performance analysis flow. In experiments, we show the correctness and accuracy of our analysis, and exhibit its potential in enhancing network utilization with a relaxed delay requirement. Moreover, the benefits of such relaxation is demonstrated through a video playback application.","Towards Stochastic Delay Bound Analysis for Network-on-Chip Zhonghai Lu∗ , Yuan Yao∗ , Yuming Jiang† ∗KTH Royal Institute of Technology, Sweden †Norwegian University of Science and Technology (NTNU), Norway Abstract—We propose stochastic performance analysis in order to provide probabilistic quality-of-service guarantees in on-chip packet-switching networks. In contrast to deterministic analysis which gives per-ﬂow absolute delay bound, stochastic analysis derives per-ﬂow probabilistic delay bounding function, which can be used to avoid over-dimensioning network resources. Based on stochastic network calculus, we build a basic analytic model for an on-chip router, propose and exemplify a stochastic performance analysis ﬂow. In experiments, we show the correctness and accuracy of our analysis, and exhibit its potential in enhancing network utilization with a relaxed delay requirement. Moreover, the beneﬁts of such relaxation is demonstrated through a video playback application. I . IN TRODUC T ION Network-on-Chip (NoC) is becoming an essential architecture as CMPs and SoCs advance from multi-core to manycore era. Compared with bus or crossbar based interconnects, NoCs can be scalable in bandwidth and power consumption. Yet, NoC design adds magnitudes of complexity to system architects because of distributed contention over distributed resources (buffers and link bandwidth), likely causing unpredictable network performance and consequently unpredictable application performance. When a unicast packet stream called a ﬂow is sent from a source to a destination, it often passes a tandem of routers where it shares the channels and buffers of the routers with other ﬂows. As a result of ﬂow interference, the packet delivery behavior becomes nondeterministic. The packet delivery time is non-uniform, largely varying from the minimal noncontentional latency to a maximal worst-case latency. Also the maximal packet latency is usually hard to obtain and much larger than the average packet latency, so called the long-tail phenomenon. Therefore ﬁnding per-ﬂow deterministic or stochastic packet delay bound has been a critical yet hard problem in order to satisfy real-time applications with performance guarantees. To tackle this problem, there are mainly two approaches: simulation based and formalism based. Simulation-based approaches, which are often good and can be accurate for average-case analysis [1], are generally unscalable for worst-case analysis because of the difﬁculty in ﬁnding ”worst cases” in time-consuming simulations for large systems. Also, simulated results might not expose adequate insights about how the performance depends on architectural parameters and application characteristics. Built on abstract models, assumptions and mathematical foundations, formalism based studies can overcome some fundamental limitations of the simulation based approaches. Because of their pros and cons, both approaches are often needed together for performance evaluation, for example, using simulations as an empirical means to validate analytical results. In the paper we take a formal approach to address stochastic delay bound analysis in NoCs based on stochastic network calculus [2][3][4], a stochastic version of network calculus [5][6][7]. In network calculus, a ﬂow is abstracted using an arrival curve to upper-bound the amount of trafﬁc within any time interval. A network element is modeled as a service curve, which provides a lower bound on the offered service. With the upper-bounded arrival curve and lower-bounded service curve, packet delay and buffer backlog bounds can be reasoned about. Previous research has applied deterministic network calculus for on-chip per-ﬂow delay bound analysis [8][9][10]. However, deterministic delay bounds are absolute upper delay limits, which are only desirable for hard realtime communication tasks. For soft real-time communication tasks, ensuring deterministic delay bounds implies system over-dimensioning [11], since an absolute guarantee is not always necessary and occasionally surpassing such a delay threshold is acceptable. For example, multimedia tasks may allow occasionally long delayed frames according to different quality standards [11]. Therefore, for such kinds of applications, deriving stochastic delay bounds is needed to satisfy acceptable quality-of-service (QoS) with effective resource utilization. Our contributions can be summarized as follows: • We propose a router service analysis model to perform per-ﬂow stochastic delay bound analysis, considering buffer sharing and link sharing. • We suggest a step-by-step procedure to analyze per-ﬂow end-to-end stochastic delay bounding function. Given certain assumptions, we are able to derive closed-form results. • In experiments, we show the accuracy of our stochastic analysis against simulation, deﬁning metrics to study the tightness of delay bounding function. We also expose the signiﬁcance and possible application of such stochastic performance analysis in enhancing network utilization. The rest of the paper is organized as follows. In Section II, we discuss related work, followed by introducing the basic concepts of stochastic network calculus in Section III. After presenting the router service analysis model in Section IV, we give and exemplify the stochastic analysis ﬂow in Section V. In Section VI, we present experiments and results. Finally, we conclude in Section VII. I I . R E LAT ED WORK As surveyed in [12], data ﬂow analysis, schedulability analysis and network calculus are three common formal approaches tackling the realtime communication performance analysis problem in NoCs. Data ﬂow analysis is rooted in an abstract process model using a unidirectional FIFO channel for inter-process communication [13]. This is an untimed concurrent communicating process model designed to reﬂect process ﬁring conditions and ﬁring outcomes through token (data element) consumption and production. The data ﬂow model has been very successfully used, particulary for realtime stream processing applications, to construct static repeatable process schedules (avoiding run-time scheduling overhead), analyze system throughput, and dimension buffer sizes by knowing a prior the static or cyclo-static token consumption/production relations of all processes [14]. Extending the untimed model by associating each process with a worst-case execution time, one can assess the timing properties such as throughput and latency of a NoC-based system [15]. In [16], Hansson et al. showed how to apply data ﬂow analysis to verify application-level temporal behavior and compute buffer sizes. Data ﬂow analysis can naturally capture multiple data rates, packet dependency and network back-pressure. Due to the static or cyclo-static ﬁring rules, it is however difﬁcult to handle dynamic relations. Schedulability analysis traditionally investigates if a set of tasks can be scheduled on one or multiple shared computation (CPU)/communication (bus) resources without violating their timing constraints under a given scheduling policy [17]. The tasks are typically prioritized and often assumed to be periodic or sporadic. When applied to on-chip networks, it deals with packets and the timing requirement concerns the end-toend packet delivery time. Along the packet’s traversal path, all possible direct and indirect contentions can be counted. The classic worst-case response time analysis theory for preemptive or non-preemptive (to include possible priority inversion) tasks [18] can be utilized to reason about the maximum blocking time of prioritized trafﬁc classes [19]. Nevertheless, schedulability analysis is somewhat restrictive since it is difﬁcult to count the effect of back pressure and thus often has to assume routers with large enough buffers. Grounded in min-plus and max-plus algebra, network calculus deals with ﬂows and can be used to derive per-ﬂow worst-case bounds on maximum latency, backlog and minimum throughput. Network calculus was pioneered by Cruz [5] and Chang [6]. It has been very successful when applied to various networks, such as ATM, Internet with differentiated and integrated services [7], Intranet, and to realtime embedded systems [20]. Recently the stochastic extension was systematically formalized in [3]. In [21], network calculus was used to calculate the buffer backlog and thus dimension buffer sizes in the network interfaces. In [8], network calculus was applied to compute per-ﬂow delay bound in best-effort packet switching networks. Recently network calculus based per-ﬂow delay bound analysis was linked to a formalized micro-architecture modeling framework [10]. All the above works use deterministic network calculus and thus derive deterministic bounds. Stochastic network calculus was utilized in [22] to experimentally study the stochastic delay and backlog in a mesh NoC. However, it establishes neither a router analysis model nor a stochastic analysis methodology. In [23], an analytical model based on stochastic network calculus was proposed for the edge node in a chip-scale optical network. The edge node transforming electrical packets into optical bursts has a simple two-stage queuing (burst assembly queue and burst transmission queue) structure. A stochastic service curve model is built for the twostage queuing and used to estimate the probabilistic queue size and packet delay. Since this work focuses on the electricaloptical interface, the stochastic analysis does neither consider the common ﬂow interference due to sharing buffers nor the multi-node end-to-end analysis under various resource-sharing scenarios. The necessity of performing stochastic analysis for soft realtime applications has been observed by other research works. Stochastic behavior of trafﬁc was observed in onchip networks [24] and applications [25]. An empiricallyderived NoC trafﬁc model exhibits strongly statistical spatiotemporal characteristics in homogeneous NoCs [26]. With a multimedia playout buffer study, Raman et al. proposed a framework to represent loss in quality and mathematically quantify it using probabilistic parameters [11]. They showed that using reduced delay requirement can enormously save on-chip memory requirement. I I I . PR E L IM INAR I E S A. Basic Concepts and Notations In network calculus, two fundamental abstract concepts are arrival curve and service curve, which are built on arrival process A(t ), service process S(t ), and departure process A∗ (t ), where t ≥ 0 is time. Following [3], we present their deﬁnitions and basic performance results of network calculus. We shall use the following notations. F: the set of nonnegative wide-sense increasing functions; ¯F: the set of nonnegative wide-sense decreasing functions. ⊗ is the min-plus f ⊗ g(t ) =in f 0≤s≤t [ f (s) + g(t − s)], where in f convolution, is ”inﬁnimum” or ”minimum” whenever applicable. Symbol ∧ denotes minimum, i.e., x ∧ y = min(x, y). Notation (x)+ = x, x ≥ 0; (x)+ = 0, otherwise. To symbolize aggregate and equivalent services, S p denotes the aggregate service to all passing ﬂows at a service point (with a subscript i ∈ N0 ) an equivalent service p while S p provided by service point p to a speciﬁc trafﬁc ﬂow Ai or Fi . B. Deterministic and Stochastic Arrival Curves deterministic arrival curve α ∈ F if its arrival process A(t ) satisﬁes, ∀0 ≤ s ≤ t , A(s, t ) ≤ α(t − s), where A(s, t ) = A(t ) − A(s) represents the cumulative trafﬁc volume in bits from time s to t . backlog-centric) stochastic arrival curve α ∈ F with bounding Stochastic Arrival Curve: A ﬂow A conforms to a (virtualfunction f ∈ ¯F , if∀ t ≥ 0 and ∀x ≥ 0, Deterministic Arrival Curve: A ﬂow A conforms to a i P{su p0≤s≤t {A(s, t ) − α(t − s)} > x} ≤ f (x), where su p is ”supremum” or ”maximum” whenever applicable. The ﬂow is denoted by A ∼< f , α >. If A ∼< 0, α >, meaning that A(t ) is always upper-bounded by f = 0, α(t ), ∀0 ≤ s ≤ t , A(s, t ) ≤ α(t − s). In other words, A follows a deterministic arrival curve α. One well studied stochastic arrival curve is the Exponentially Bounded Burstiness (EBB) model. With the EBB model, α(t ) =ρ t and f (x) =ae −bx , where ρ is the sustainable average rate, and a, b ≥ 0. For example, consider a Poisson process with density λ, and the packet lengths are independent and identically distributed, following a negative exponential distribution with mean 1/μ, it is shown in [4] that the Poisson process has a stochastic arrival curve α(t ) = λ μ−θ t with bounding function f (x) =e −θx , ∀θ > 0. As studied in [24], the arrival process of the header ﬂits in NoCs follows a Poisson process. In [27], Guz et al. employed this assumption to derive a link delay model for trafﬁc ﬂows in wormhole NoCs, and the analytic model was then used by an optimization algorithm to minimize link capacities without sacriﬁcing perﬂow performance assurance. C. Deterministic and Stochastic Service Curves Deterministic Service Curve: Consider a system S with input arrival process A(t) and output departure process A∗ (t ). β(t ) ∈ F if ∀t ≥ 0, A∗ (t ) ≥ A ⊗ β(t ). The system provides to the input a deterministic service curve The latency-rate server [28] is one most popular service abstraction which can model a range of scheduling disciplines such as weighted round robin (WRR), deﬁcit round robin, virtual clock, weighted fair queueing (WFQ), start-time fair queuing, FIFO, static priority etc. (Section 2.2 in [3]). Such shared resource scheduling policies were proposed not only for macro-networks but also for micro-networks to achieve QoS. For example, a variant of virtual clock was proposed in comparison with WFQ [29] for NoCs in CMPs and SoCs, and WRR was suggested for the NoC-based SoCs [30]. Mathematically the latency-rate service function has form βR,T (t ) =R( t − T )+ , where R bounds the minimum service rate and T the maximum processing latency. A workconserving constant rate (C) server counting the effect of packetization (in contrast to the ﬂuid model) can be expressed using βC,τ [4], where τ = L C with L being the maximum data unit size. With the ﬂuid model in a synchronous network, the work-conserving constant rate server can be modeled by βC,0 . stochastic service curve, β ∈ F with bounding function g ∈ ¯F , Stochastic Service Curve: A server S provides a (weak) denoted by S ∼< g, β > if ∀t ≥ 0 and ∀x ≥ 0, P{A ⊗ β(t ) − A ∗ (t ) > x} ≤g(x ). When g = 0, the server provides a deterministic service curve. In other words, a deterministic service curve β is a special case of a stochastic service curve, S ∼< 0, β >. D. Deterministic and Stochastic Delay Bounds has a stochastic arrival curve A ∼< f , α > , and S a stochastic Consider that node S serves an input ﬂow A. Suppose A service curve S ∼< g, β >. Then ∀t ≥ 0 and x ≥ 0, the delay D(t ) of ﬂow A is probabilistically bounded by P{D(t ) > h(α + x, β)} ≤ f ⊗ g(x) (1) where h(α + x, β) represents the maximum horizontal distance between the two curves, α + x and β. With the special case of deterministic arrival curve and deterministic service curve, we have a deterministic upper bound, P{D(t ) > h(α, β)} = 0, i.e. D(t ) ≤ h(α, β). IV. ROU T ER S ERV IC E MOD E L ING Our aim is to ﬁnd per-ﬂow end-to-end stochastic delay bound. We ﬁrst build a router service model, and then derive per-ﬂow Equivalent Stochastic Service Curve (ESSC) at each router so as to apply the node concatenation theorem to derive per-ﬂow end-to-end delay bounding function. A. On-chip Router and Flow Interference 0F 1B 1B 2B 1F Router 1N 2B 2F Router 2N Fig. 1. Flows pass routers contending for buffers and links Figure 1 draws an example of two connected routers with internal data path details. For brevity, we only show the relevant data path, omitting the control path of the router. The router is a typical input-queuing packet-switching router. Different from macro-networks, the NoC operates synchronously by a single clock. At each inport, there are two FIFO queues. With this ﬁgure, we also show that three ﬂows, F0 , F1 and F2 , pass through the routers. The data unit of ﬂows is packet. We assume (1) Deterministic routing, i.e., one ﬂow has one deterministic path. Though lack of adaptivity, deterministic routing has been used in many NoCs because of its simplicity in analysis and implementation. (2) Virtual channels are statically allocated to ﬂows. (3) The FIFO queues are bounded with sufﬁcient sizes to avoid buffer overrun. To determine per-ﬂow router service, the key is to analyze the interferences the ﬂow may experience during delivery. When a ﬂow passes a router, it receives the routing and switching services. The routing service ﬁnds the right outport, and the switching service delivers the ﬂow to the right outport and thus the right output link. The routing service always succeeds through exclusive routing computation. But the switching service may temporarily fail as other ﬂows may contend for the same outport simultaneously. Whenever such contention occurs, only one ﬂow can be served and the others have to wait in their respective buffers. This is a kind of interference called link sharing. As the queues are shared, a queue may contain packets of multiple ﬂows, causing headof-line blocking. This kind of interference is called buffer sharing. Essentially we have to build analytical models to deal with both the buffer sharing and link sharing interferences. B. Service Analysis Model Figure 1 illustrates a typical scenario for sharing buffers and links. Let’s take a close look at the scenario. At router N1 , ﬂow F0 shares the eastern link with ﬂow F1 after passing respective buffer B1 and B2 . Both F0 and F1 join the input queue B1 of the downstream router N2 , resulting in buffer sharing interference. Flow F0 and F2 contend for the eastern output link of router N2 , resulting in link sharing interference. When looking into exactly where the two kinds of interferences are handled, we can see that the buffer sharing interference is handled at the crossbar inport through de-multiplexing and the link sharing interference at the crossbar outport through multiplexing. From the switching service perspective, these can be viewed as crossbar input service denoted by Sci and crossbar output service denoted by Sco , respectively. 0F 1B 1N 1,N ci S 1,N co S 2 ,N ci S 2 ,N co S 1B 2N 2B 1F 2B 2F 0 SNi,ci one aggregate ﬂow, and then use the concept of leftover service to deduct remaining service from the aggregate service after serving other interference ﬂows. To obtain , we apply Theorem 5.45 Leftover Stochastic Service Curve in [3], which says that: For an aggregate ﬂow of F0 and F1 , F0,1 , served by a system offering a stochastic service curve β ∈ F with bounding function g ∈ ¯F , i.e. S ∼< g, β >, if F1 conforms to a stochastic arrival curve α1 , F1 ∼< f1 , α1 >, then F0 receives a stochastic service curve β − α1 with bounding function f1 ⊗ g, i.e., S0 ∼< f1 ⊗ g, β − α1 >. • Finally, resolve node concatenation to compute the composition of the two ESSCs, SNi,ci . and SNi,co 0 0 0F S Ni ci , 0 S Ni co , 0 0F S Ni cio , 0 a) Crossbar input and output services to  0F b) Router’s ESC to  0F Fig. 3. The ESSC of router Ni offered to F0 0 ∼< g1 , βNi,ci > and SNi,co 0 Figure 3(a) illustrates the equivalent crossbar input and output services provided to ﬂow F0 . Let SNi,ci ∼< g2 , βNi,co >. We can observe that the two servers are concatenated. According to Theorem 5.36 in [3] for node concatenation of stochastic servers, they provide to F0 an ESSC SNi,cio ∼< g1 ⊗ g2 , βNi,ci ⊗ βNi,co >. This is illustrated in , and Figure 3(b). Hence we obtain router Ni ’s ESSC to F0 , . ⊗ SNi ,co 0 = SNi ,ci 0 = SNi,cio SNi,cio 0 0 SNi 0 0 Fig. 2. A service analysis model for all ﬂows V. S TOCHA S T IC ANA LY S I S T ECHN IQU E From the above analysis, we can draw a service analysis model for all ﬂows passing the two routers, as shown in Figure service at router Ni (i ∈ N) is separated into a crossbar input 2. In this graph, buffers are visible and the crossbar switching service SNi,ci and a crossbar output service SNi,co . Both SNi,ci and SNi,co are work conserving servers with an aggregate capacity. In N1 , F0 exclusively uses service SN1,ci and then shares with F1 the crossbar output service SN1,co that models the link sharing. In N2 , F0 and F1 share the crossbar input service SN2,ci which models the buffer sharing, and then F0 and F2 share the crossbar output service SN2,co . C. Per-Router ESSC for Flow Assume that trafﬁc ﬂows conform to a stochastic arrival curve. To compute the per-router ESSC for F0 at router Ni , we take the following three steps: • First, resolve link sharing to compute the ESSC provided by the crossbar output service (SNi,co ) toF 0 , SNi,co . The exact form of SNi,co depends on the link characteristics (capacity, work conserving or not etc.) and arbitration policy. • Second, resolve buffer sharing to compute the ESSC provided by the crossbar input service (SNi,ci ) to F0 , . We treat the multiple ﬂows entering one buffer as SNi,ci 0 0 0 i i We present and exemplify the analysis ﬂow followed by brief analysis of other interference patterns. A. General Analysis Procedure N1 , N2 , · · · , NN . The analysis steps to derive Fi ’s end-to-end Suppose that Fi passes through a series of routers from stochastic delay bounding function are as follows: 1) Derive per-router ESSC for all routers visited by Fi . At router N j ( j ∈ [1, N ]), the per-router ESSC SN j is the service traversing the crossbar SN j ,cio , which is a concatenation of crossbar input service (SN j ,ci ) and crossbar output service (SN j ,co ), i.e., SN j . Step 1 is iterative so as to ﬁnd Fi ’s ESSC for all passing , · · · , SNN routers, SN1 , SN2 . 2) Deduct per-path (end-to-end) ESSC for Fi . The principle for calculating the per-path ESSC follows the node concatenation model of stochastic servers. Hence, we have Fi ’s end-to-end ESSC as SN1,2,···,N ⊗ SN2 . 3) Compute end-to-end stochastic delay bounding function. The last step is straightforward once the per-ﬂow end-toend ESSC is derived. We apply Formula (1) in Section III-D to compute the stochastic delay bounding function for P{D(t ) > x}. i = SN j ,cio i ⊗ · · · SNN ⊗ SN j ,co i = SN j ,ci i SN1 i = i i i i i i i i Note that, if multiple ﬂows share the same path, we could consider them as one aggregated ﬂow ﬁrst. After deriving the ESSC for the aggregate ﬂow, we can then derive the ESSC for a particular ﬂow using the left-over service. It turns out that the analysis procedure is similar to that proposed for deterministic analysis in [9]. Both procedures ﬁrst analyze per-node service, and then compute per-ﬂow end-toend equivalent service using the node concatenation property, and ﬁnally calculate the delay bound. This is a good news because a uniform analysis procedure can be integrated for both deterministic and stochastic analysis. The difference is that, while the technique in [9] computes per-ﬂow absolute upper bound, this one computes per-ﬂow delay bounding function. B. An Example We exemplify the general analysis procedure with the running example in Figure 1. We derive the stochastic delay bound for ﬂow F0 and treat F1 and F2 as interference ﬂows. We give certain assumptions which allow us to derive closedform formulas to compute the stochastic delay bound. Flow Fi (i ∈ N0 ) conforms to a stochastic arrival curve αi (t ) = ρit with bounding function f i , i.e., Fi ∼< f i , ρit >. Each link or physical channel is work-conserving and has a deterministic capacity of C. Let Tho p be per-hop propagation (transmission) time. The channel arbitration policy allows to model the channel’s equivalent service to a passing ﬂow by a latency-rate server (see Section III-C). To be concrete, we shall exemplify with the WRR arbitration policy. 1) Deduct Per-Router ESSC for F0 : In Figure 1, ﬂow F0 traverses two routers, N1 and N2 . We ﬁrst compute the ESSC it receives from N1 , SN1 0 , and from N2 , SN2 0 . At router N1 , both SN1,ci and SN1,co are work-conserving constant rate servers. We have SN1,ci = SN1,co = βC,0 . With WRR, the interference from other ﬂows is isolated in the sense wiC (i = 0, 1, 2), where wi ≤ 1 is the service weight to Fi , that each ﬂow receives a guaranteed deterministic service rate w0 + w1 or w2 = 1, and the maximum processing delay Tw0 is the maximum time needed to serve the other ﬂow per service round. Hence we have SN1,co , we haveS N1 At router N2 , the aggregate ﬂow F0,1 is de-multiplexed and served by two separate servers. We also have SN2,ci = SN2,co = βC,0 . F1 is sunk to the southern port without blocking in an ideal way. But due to head-of-line blocking, ﬂow splitting is interfered with each other. To derive SN2,ci using the left-over service principle, we cannot only consider SN2,ci . Rather we have to consider the rate dependence of the next service SN2,co together. Irrespective of F2 ’s behavior due to WRR, SN2,co 0, βw0C,Two >, but the delay Two shall not propagate back to SN2,ci as the connection from SN2,ci to SN2,co is ideal, and their separation can be considered logical. Hence, we get SN2,ci ∼< 0, βw0C,Tw0 >. Since ∼< 0, βw0C,Two >. = (SN2,ci ⊗ βw0C,o − α1 )+ . Therefore SN2,ci ∼< f1 , βw0C−ρ1 ,0 >. ∼< f1 , βw0C−ρ1 ,Tw0 >. Finally, using the node concatenation property, we can derive per-router ESSC to F0 by N2 as SN2 = SN1,ci ⊗ SN1,co ∼< SN1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (cid:10) ), = SN1 +Tho p ) > ∼< f1 , βw0C−ρ1 ,2(Tw0 ⊗ SN2 2) Compute End-to-End ESSC for F0 : As Ssys 0 , we have Ssys ∼< f1 , βw0C−ρ1 ,2Tw0 >. Taking into account the propagation delay 2Tho p (refer to Section 1.6.3 in [7]), we further have Ssys 3) Derive Delay Bounding Function for F0 : After obtaining the end-to-end ESSC for F0 , together with the stochastic arrival curve of F0 , we can apply Formula (1) (Refer also to Theorem 7 in [2]) to compute the stochastic packet delay bound for F0 . With a few derivation steps (omitted due to space limitation), we obtain, ∀t ≥ 0, P{D(t ) > x} ≤ f0 ⊗ f1 (x (2) where x(cid:10) = (x − 2(Tw0 + Tho p )) · (w0C − ρ1 ). Function G(x) = P{D(t ) > x} is called Complementary Cumulative Distribution Function (CCDF) in probability theory. independent, and Fi (i ∈ N0 ) follows the compound Poisson The derived result is general. Suppose that the ﬂows are process with intensity λi and packet length mean 1/μi . Thus t >, where θi > 0. According to Lemma 6.1 and Example 6.2 in [3], we obtain a closed-form result: P{D(t ) > x} ≤(1 + θx (cid:10) )e where θ = θ0 ∧ θ1 with θi = μi − λi (i = 0, 1). C. Generalization of the Example Consider ﬂow F0 passing a series of N routers with the same interference pattern (in total N + 1 ﬂows) as the previous example in Section V-B, we can generalize the delay bound result. The derivation procedure is similar. Hence without elaborating details, we give the formula for the general probabilistic delay bound of F0 as follows: Fi ∼< e−θi x , μi−θi −θx(cid:10) (3) λi , P{D(t ) > x} ≤ f0 ⊗ fsys (xsys ), (4) + Tho p )) · where fsys = f1 ⊗ f2 · · · ⊗ fN−1 , and xsys = (x − N (Tw0 Csys with Csys = (w0C − ρ1 ) ∧ (w0C − ρ2 ) · · · ∧ (w0C − ρN−1 ) meaning the bottleneck service bandwidth for ﬂow F0 . Again, if the ﬂows are independent and conform to the compound Poisson distribution, we obtain the following closed-form result: P{D(t ) > x} ≤(1 + θxsys )e (5) where θ = θ0 ∧ θ1 ∧ θ2 · · · ∧ θN−1 with θi = μi − λi (i ∈ N0 ). D. Analyze Interference Patterns −θxsys , F1 F2 1S F3 2S F1 F2 1S F3 2S 3S a) Parallel pattern (F2 and F3 are parallel) b) Nested pattern (F3 is nested in F2) F1 F2 1S F3 2S 3S F1 F2 1S 2S 3S F3 F2o c) Cross pattern (F2 crosses F3) d) Cross pattern is cut into a parallel and a nested pattern Fig. 4. Interference patterns due to buffer sharing According to [9], ﬂow interference upon buffer sharing can be classiﬁed into three kinds of patterns: parallel pattern, nested pattern, and cross pattern, as shown in Figure 4(a), 4(b) and 4(c), respectively. The detailed example in Section V-B and Section V-C actually analyzes only the parallel pattern. Nevertheless our technique can also be used to analyze the other interference patterns. The key step is still to obtain perﬂow end-to-end ESSC, which we shall show for ﬂow F1 . For the nested pattern in Figure 4(b), we ﬁrst consider ﬂows, F1 and F2 , as one aggregate ﬂow F1,2 . We obtain the ESSC provided by S2 to F1,2 , S2 1,2 , by removing the interference of F3 using the left-over service principle. We ∼< f3 , (S2 − α3 )+ >. Then we compute F1,2 ’s endhave S2 to-end ESSC, S1,2 ∼< f3 , S1 ⊗ (S2 − α3 )+ ⊗ S3 >. Finally we obtain the end-to-end ESSC for F1 as S1 ∼< f3 ⊗ f2 , ((S1 ⊗ 1,2 (S2 − α3 )+ ⊗ S3 ) − α2 )+ >. As shown in [9], the cross pattern can be analyzed through a divide-and-conquer approach by transforming it into a parallel pattern plus a nested pattern. This is done by ”cutting” a cross ﬂow, for example, F2 , as illustrated in Figure 4(c) and 4(d). The additional analysis is to deduce the output characterization of F2 at the cut point, F2o , which can be obtained using Theorem 5.8 in [3]. V I . EX P ER IM EN T S AND R E SU LT S We present experiments and results to validate the accuracy, study the scalability, and show the signiﬁcance of the stochastic analysis together with a video playback application. A. Experimental Setting We construct a NoC simulator to simulate the packet delivery behavior. We consider the interference pattern as shown in Figure 1, 2. The packet-switching router is inputqueuing (see Figure 1). To show clear delay results, it is designed as a single-cycle router, meaning that delivering one packet in one cycle. Each channel has a deterministic capacity of C = 1 packet/cycle. The channel arbitration is round-robin, meaning a fair service to all ﬂows. Flows are independent. For each ﬂow Fi , packets of uniform size are generated following the compound Poisson process with a pair of parameters (λi , μi ). Indeed εi = λi/μi in packet/cycle represents Fi ’s injection rate. To achieve stability and analyzability, the trafﬁc injection never saturates the network, ∀i ∈ [1, N ], ε0 + εi < 1. Hence, we can also use ε = λ/μ as a measure of the actual network utilization. Since we have already developed closed-form results for stochastic packet delay of F0 (See Formulas (3) and (5) in Section V), we compare F0 ’s theoretical results against the simulated packet delay results. By the experimental setup, we have w0 = 0.5 and Tw0 = 1 cycle in Formulas (3) and (5). To distance, Δmax , between the bounding curve P{D(t ) > x} and quantify the bounding tightness, we use the maximum vertical the envelop of the actual delay probabilities from simulation P{Ds (t ) > x} (Ds (t ) is simulated delay), i.e., Δmax = maxx≥0 (P{D(t ) > x} − P{Ds (t ) > x}) and Δmax ∈ [0, 1]. Since Δmax is an extreme, we also measure and give the average tightness Δavg for the basic two-node case. A lower value implies a tighter bounding function. B. Accuracy of the Stochastic Analysis To investigate the accuracy, we ﬁrst consider the simplest case, 2 nodes 3 ﬂows (F0 , F1 and F2 , see Figure 1). F1 and F2 are set to the same parameters. ) x > D ( P 1 0.8 0.6 0.4 0.2 0 0 ε0 , λ0 , μ0 : 0.35, 0.1, 2/7 ε0 , λ0 , μ0 : 0.5, 0.1, 0.2 Δmax = 0.37 Δmax = 0.43 1 0.8 0.6 0.4 0.2 ) x > D ( P ε1 , λ1 , μ1: 0.2, 0.1, 0.5 ε1 , λ1 , μ1: 0.35, 0.1, 2/7 Δmax = 0.24 Δmax = 0.37 50 100 150 Packet delay x (cycles) 200 0 0 50 100 150 Packet delay x (cycles) 200 (a) ε1 = 0.2 (λ1 = 0.1, μ1 = 0.5) (b) ε0 = 0.5 (λ0 = 0.1, μ0 = 0.2) Fig. 5. The basic two-node case Figure 5 shows F0 ’s packet delay results for the simplest case. The curves are calculated bounding functions. In all cases, the theoretical bounding functions constrain the simulated results, implying the correctness of the stochastic analysis. The two ﬁgures also show the trend of tightness, with Δmax marked but not Δavg . Figure 5(a) shows the tightness increases when F0 ’s injection rate increases under given interference ﬂows’ characteristics. Speciﬁcally, when ε1 = ε2 = 0.2, increasing F0 ’s injection rate ε0 from 0.35 to 0.5 reduces the Δmax from 43% to 37%, and Δavg from 13.2% to 10.6%. Figure 5(b) shows the tightness increases when F1 ’s injection rate increases under given F0 ’s characteristics. Speciﬁcally, when ε0 = 0.5, increasing interference ﬂows’ injection rate ε1 (ε2 ) from 0.2 to 0.35 reduces the Δmax from 37% to 24% and Δavg from 10.6% to 2.1%. Both phenomena indicate that, while the bounding function computes the CCDF under the consideration that interference always exists, higher likelihood of ﬂow interference due to higher network utilization (indicated by larger ε) results in better tightness. C. Scalability of the Stochastic Analysis 0F 1N Buf. 1F 2N Buf. 2F 3N Buf. 1NF (cid:16) NN Buf. NF Fig. 6. Interfered ﬂow traversal over a tandem of nodes ) N ( x a m Δ 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 ε = 0.2 ε = 0.3 0 24 6 8 10 12 14 Number of interfering ﬂows N ) x > D ( P 1 0.8 0.6 0.4 0.2 0 Simulation B.F. Δmax = 0.046 0 20 40 60 80 100 120 140 Packet delay x (cycles) (a) Interference vs. tightness (b) N = 14, ε = 0.2 Fig. 7. The scalability result Then we extend the simplest case to a scalable one, N nodes N + 1 ﬂows. For convenience, we illustrate this scenario in Figure 6. With the scalable case, we evaluate how the accuracy of the theoretical results would vary when the number of interference ﬂows increases. We set the same ε for all ﬂows, i.e., ε = ε0 = ε1 = · · · εN , and ε < 0.5. Thus all ﬂows receive equal service from the round-robin arbitration. In this way, we can eliminate the impact of service over-provisioning and under-provisioning (due to uneven trafﬁc rates) on the analytic tightness. Figure 7(a) gives the results for ε = 0.2, 0.3 when λ = 0.1. We can see again that the analysis gives tighter results when the network utilization (indicated by ε) is higher. More interestingly, when F0 experiences more contention from more ﬂows, the discrepancy Δmax drops quickly. When N = 14 and ε = 0.2, Δmax (14) =0.046 (4.6%), as depicted in Figure 7(b) where ”B.F.” denotes Bounding Function. When N = 14 and ε = 0.3, Δmax (14) → 0. This implies that the bounding function can be inﬁnitely approached and is thus strictly tight. D. Signiﬁcance of the Stochastic Analysis requirement to P{D(t ) > 150} ≤ 0.02 (2%), ε is improved from 0.2 to 0.33, i.e, 65% in percentage. Further relaxing this requirement improves the network utilization further but with lower acceleration. The maximum network utilization is saturated till 0.38 when P{D(t ) > 150} ≤0.1 (10%). E. Video Playback Application 50 40 30 20 10 ) s p b M , e t a r t i b ( b r 30 20 10 rp rp rb 0 0 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 ε (network utilization) ) s p f , e t a r k c a b y a l p ( p r 10% 8% 6% 4% 2% e g a t n e c r e P 0 0 ε : 0.2, rb : 20, rp : 30 ε : 0.375, rb : 44, rp : 28.6 30 60 90 120 150 180 Packet delay x (cycles) ) X > D ( P Δmax = 0.058 ε Sim. (λ(cid:2) , μ(cid:2) ) B.F. (λ(cid:2) , μ(cid:2) ) B.F. (λ, μ) 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02 0 70 90 110 130 150 170 190 Packet delay x (cycles) 0.4 0.35 0.3 0.25 0.2 0 Theory Simulation 0.02 0.04 0.06 0.08 0.1 0.12 δ(150) (a) 2% tolerance at x0 = 150 (b) Delay tolerance at x0 = 150 Fig. 8. Relaxing delay requirement raises network utilization g(x0 ,δ) (x, λ(cid:10) We investigate the signiﬁcance of the stochastic analysis by exploring the potential of the theoretical results and matching them with simulated ones. We show that, given a maximum delay required by an application, how much can the network utilization (ε) be enhanced if relaxing this exact delay constraint to a probabilistic requirement. Given ﬂow parameters (λ, μ), we ﬁrst compute its bounding function for P{D(t ) > x} as g(x, λ, μ) using Formula (5). Then given a speciﬁc delay requirement x0 , we add a constant tolerance δ(x0 ) of violation probability to the bounding function at x0 , i.e., g(x0 , λ, μ) ⇐ g(x0 , λ, μ) +δ(x 0 ). With the new bound a new value pair (λ(cid:10) and using the reverse function of Formula (5), we compute , μ(cid:10) ). Then we use this value pair for all ﬂows to (1) get a new bounding function for P{D(t ) > x} as , μ(cid:10) ) and (2) control the trafﬁc injection, and ﬁnally we record the actual probabilities P{Ds (t ) > x} in simulation. With the scalable network when N = 14 and ε = 0.2, Figure 8(a) draws two theoretical curves, one without tolerance g(x, λ, μ) and the other with δ(150) =2% (2% tolerance at x0 = 150 cycles), i.e., g(150,2%) (x, λ(cid:10) , μ(cid:10) ), and the actual simulated probability for P{Ds (t ) > x}. We can see that the theoretical estimation gives good tightness, with the maximum discrepancy Δmax = 0.058 (5.8%). Figure 8(b) shows its network utilization when allowing the bounding tolerance δ(150) =2% in both theory and in simulation. The line marked by ”(cid:13)” records the actual probabilities from simulations, which validates the correctness and accuracy of the projected probabilities. We can observe that, if we require that P{D(t ) > 150} →0, ε = 0.2; If we can relax this (a) Utilization vs. Video quality (b) Delay histograms Fig. 9. Delivering higher bit rates with marginal quality loss In our analysis, we have assumed that ﬂows follow a Poisson distribution. While this assumption may be questioned for its general applicability, we can employ a stochastic ﬂow shaping technique to ensure that the ﬂow conforms to its speciﬁc distribution. Flow shaping has been an effective technique in trafﬁc engineering of communication systems [3] and realtime systems [31]. As demonstrated in [32], it can be used not only for trafﬁc speciﬁcation conformance but also for buffer optimization in NoCs. Through an on-chip MPEG2 playback application, we experiment ﬂow shaping, and further demonstrate the insightful tradeoffs between network utilization and QoS exposed in Section VI-D. In this experiment, Fi (i = 0, 1, 2, · · · 14) consists of video streams read from Tektronix MPEG2 source ﬁles [33] and sent over the 14-node network to a video decoder at the destination. The stream reading is controlled by a parameterized Poisson process. As a result, when the ﬂows enter the network, they follow their respective Poisson distribution as speciﬁed in Section VI-D. The video streams are encoded at 30 fps with different bit rates in Mbps. One packet contains constant bytes of data. As in Section VI-D, the packet delay threshold is set to 150 cycles. A received packet is stored for playback if its delay is less than this threshold, and discarded otherwise. We record the number of played packets, which is converted to playback rate in fps. Figure 9(a) shows the bit rate (rb in Mbps) and the playback rate (r p in fps) versus the network utilization (ε). A higher network utilization means that more data can be transmitted within the same time interval and thus permits a higher bit rate. This is a linear relation. However, as analyzed previously, at higher bit rates, some packets may experience a delay larger than 150 cycles, resulting in reduced playback rate. Nevertheless, we can observe that the network, which is designed for transferring a bit rate of 20 Mbps without the delay violation (ε = 0.2), can actually transfer a bit rate of 44 Mbps (ε = 0.375) with only 4.67% reduction in video quality (from 30 to 28.6 fps). The results agree with Figure 8(b). Figure 9(b) shows the delay histograms when ε = 0.2, 0.375. With ε = 0.2, no packet has a delay larger than 150 cycles. While ε = 0.375, 4.67% of packets have a delay larger than 150 cycles, as the longer tail indicates. V I I . CONC LU S ION Based on stochastic network calculus, we have presented a stochastic delay bound analysis technique for on-chip networks. With a running example, we have applied the technique to derive closed-form stochastic delay bound formulas, and validated the accuracy in the experiments. More interestingly, we have also shown a great potential in raising network utilization or reducing service capacity requirement when a probabilistic delay bound is allowed. Such insights might be hard to obtain from simulations. Speciﬁcally in the example, when relaxing a speciﬁc delay requirement, 150 cycles, within only 2% of violation probability, the network utilization can be increased by 65%. A further relaxation can gain further improvement but with lower acceleration until saturation. A video playback application shows that the network can indeed transmit MPEG2 streams with much higher bit rates at the expense of negligible quality loss. In NoCs, a ﬂow control scheme between neighboring routers or end-to-end network interfaces, e.g., credit-based, on-off, or ack/nack based techniques [1], is typically introduced to coordinate the use of buffers without overﬂow. In our stochastic analysis, we have not included ﬂow control. The reason is that the ﬂow control introduces a feedback loop in the analysis model. Currently in stochastic network calculus, this remains an open challenge (see Appendix A.2 on Feedback Analysis in [3]). We consider this an urgent task, which shall be tackled in the near future. "
"Introduction to the special session on ""Interconnect enhances architecture - Evolution of wireless NoC from planar to 3D"".","Continuing progress and unprecedented integration levels in current silicon technologies make possible complete end-user systems consisting of an extremely high number of cores integrated on a single chip for embedded or high-performance computing. However, without developing new paradigms for energy- and thermally-efficient design, meeting the computing, storage, and communication demands of the emerging applications is highly unlikely. Moreover, in order to sustain the predicted growth of number of embedded cores on a single die, it is extremely important to have a scalable, low power, and high bandwidth on-chip communication infrastructure. Towards this end, wireless Network-on-Chip (WiNoC) represents an emerging paradigm to design a low power yet high bandwidth interconnect infrastructure for multicore chips.","Introduction to the Special Session on “Interconnect Enhances Architecture: Evolution of Wireless NoC from Planar to 3D” Radu Marculescu1 , Partha Pratim Pande2 , Deukhyoun Heo2 , and Hiroki Matsutani3 1 Carnegie Mellon University, United States — radum@ece.cmu.edu 2 Washington State University, United States — {pande,dheo}@eecs.wsu.edu 3 Keio University, Japan — matutani@arc.ics.keio.ac.jp Continuing progress and unprecedented integration levels in current silicon technologies make possible complete end-user systems consisting of an extremely high number of cores integrated on a single chip for embedded or high-performance computing. However, without developing new paradigms for energy- and thermally-eﬃcient design, meeting the computing, storage, and communication demands of the emerging applications is highly unlikely. Moreover, in order to sustain the predicted growth of number of embedded cores on a single die, it is extremely important to have a scalable, low power, and high bandwidth on-chip communication infrastructure. Towards this end, wireless Network-on-Chip (WiNoC) represents an emerging paradigm to design a low power yet high bandwidth interconnect infrastructure for multicore chips. The emerging ﬁeld of WiNoC is actively being pursued by a number of researchers worldwide, from a variety of diﬀerent perspectives, ranging from very high levels of abstraction (e.g., system architecture) all the way down to very low levels (e.g., on-chip antenna and transceiver design). Successful solutions will likely encompass elements from all or at least several levels of abstraction and rely on interdisciplinary concepts from multi-core architectures, millimeter wave integrated circuits, 3D ICs, digital communications, complex networks, and optimization techniques. Recent investigations have demonstrated that WiNoCs can be designed using conventional CMOS-compatible design techniques and it is capable of simultaneously addressing the latency, power, and interconnect routing problems of traditional NoCs. Starting from these overarching ideas, this special session considers a holistic approach to the WiNoC paradigm and identiﬁes a few critical issues related to the theoretical basis, essential properties and performance metrics of designing and characterizing the WiNoC architectures as a communication backbone of future multi-core systems. Towards this end, this session consists of three forward-looking talks addressing these fundamental challenges and opportunities. The ﬁrst talk by Radu Marculescu (Foundations of On-chip Communication: Performance and Power Management in 2D and 3D Multicore Platforms) discusses the fundamentals of network-based communication for 2D and 3D multicore systems and advanced design techniques for multi-domain clock and power management for embedded and high-performance processors, using real examples of multicore platforms. Finally, this talk looks into fundamental limitations of performance improvement, as well as scalable control algorithms for power and thermal management in 2D- and 3D on-chip networks. The second talk by Partha Pande and Deukhyoun Heo (Planar WiNoC Architectures) addresses various design challenges of WiNoC architectures using the traditional 2D IC technology. First, this talk presents the design of on-chip, highly eﬃcient, wideband miniaturized antennas and millimeter (mm)-wave wireless transceivers operating in the range of a few tens to one hundred GHz for high throughput on-chip data transfer. After this, the discussion focuses on how high bandwidth and low power WiNoC architectures can be designed by incorporating the small-world architecture. However, a small-world network architecture relies on an irregular topology; routing in irregular networks is more complex than in mesh topologies, because routing methods need to be topology agnostic. Hence, it is necessary to investigate new routing mechanisms for small-world networks. Moreover, in order to utilize the wireless channels eﬃciently, one needs to design suitable wireless medium access control (MAC) schemes. The last part of this talk presents detailed performance evaluation and necessary design trade-oﬀs for the small-world WiNoCs with respect to their conventional wireline counterparts. Finally, this part of the special session concludes with a discussion on thermal and power management policies suitable for WiNoCs. The last presentation by Hiroki Matsutani complements the above discussions regarding planar WiNoCs by introducing the wireless 3D NoCs that use inductive coupling though-chip interfaces (TCIs) to connect stacked chips by using square or hexagon coils as data transmitters. The inductive-coupling TCI is a low-cost 3D integration solution, as these are implemented with metal layers of common CMOS process. Towards this end, the third talk will focus on two topics: 1) design and implementation of the wireless Cube-1 system that consists of four cores interconnected with a wireless 3D NoC and results from its prototyping; 2) novel interconnection techniques for wireless 3D NoCs that cover the 3D topology, routing and ﬂow control schemes, and optimization methods using small world eﬀects. In summary, this session will provide a timely and insightful journey into various challenges and emerging solutions regarding the design of WiNoC architectures. By scope and contents, this special session represents an engaging proposition to the NOCS attendees from both academia and industry. "
Variable-width datapath for on-chip network static power reduction.,"With the tight power budgets in modern large-scale chips and the unpredictability of application traffic, on-chip network designers are faced with the dilemma of designing for worst-case traffic loads and incurring high static power overheads, or designing for average traffic and risk degrading performance. This paper proposes adaptive bandwidth networks (ABNs) which divide channels and switches into lanes such that the network provides just the bandwidth necessary in each hop. ABNs also activate virtual channels (VCs) individually and take advantage of drowsy SRAM cells to eliminate false VC activations. In addition, ABNs readily tolerate silicon defects with just the extra cost of detection. For application benchmarks, ABNs reduce total power consumption by up to 45% with comparable performance compared to single-lane power-gated networks, and up to 33% compared to multi-network designs.","Variable-Width Datapath for On-Chip Network Static Power Reduction George Michelogiannakis, John Shalf Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720 Email: {mihelog, jshalf}@lbl.gov Abstract—With the tight power budgets in modern largescale chips and the unpredictability of application trafﬁc, onchip network designers are faced with the dilemma of designing for worst-case trafﬁc loads and incurring high static power overheads, or designing for average trafﬁc and risk degrading performance. This paper proposes adaptive bandwidth networks (ABNs) which divide channels and switches into lanes such that the network provides just the bandwidth necessary in each hop. ABNs also activate virtual channels (VCs) individually and take advantage of drowsy SRAM cells to eliminate false VC activations. In addition, ABNs readily tolerate silicon defects with just the extra cost of detection. For application benchmarks, ABNs reduce total power consumption by up to 45% with comparable performance compared to single-lane power-gated networks, and up to 33% compared to multi-network designs. I . IN TRODUC T ION Current and future large-scale chips are increasingly constrained by power [15]. Modern on-chip networks contribute signiﬁcantly to the chip’s power, area, and performance characteristics [6], [21], [1], [13]. A challenge in reducing network power is designing the network independently of the system and applications, given that communication demands can vary substantially across different applications. Also, applications tend to load the network unevenly in both space and time [35], [9], [19], [3]. For example, channel utilization ranges from near zero to 43% in PARSEC benchmarks [19], [3], [5]. Designing the on-chip network to handle worst-case loads increases both area and static power compared to designing for average trafﬁc. Static power is mainly composed of leakage power and the power to toggle clocking inputs, with leakage typically dominating [30]. Leakage power can constitute 90% of network power with light-trafﬁc applications, or 30% to 50% with heavy-trafﬁc benchmarks [13], [18], [24]. To make matters worse, leakage power is projected to increase in future near threshold voltage technologies even up to 90% of total power under higher loads [6], [23], [9]. In this paper, we propose adaptive bandwidth networks (ABNs) which continuously adapt to trafﬁc load by activating the proper amount of bandwidth individually at each channel, and the proper number of virtual channels (VCs) in each input port. ABNs accomplish this by dividing channels into lanes. Lanes are activated individually according to local trafﬁc demands. Inactive lanes are power gated, consuming near zero static power. ABNs also power gate individual This work was supported by the Director, Ofﬁce of Science, of the U.S. Department of Energy under Contract No. DE- AC02-05CH11231. VCs [30]. However, unlike past work, ABN use drowsy SRAM cells which enable ABNs to make activation decisions in the upstream router’s VC allocator, thus avoiding mispredictions which can cause more VC activations than necessary [30]. ABNs also power gate router switches by adding multiple lanes for every input and output [18]. ABNs hide activation delays using a single look-ahead signal per ﬂit for both VC and lane activations [28], [29]. Finally, ABNs readily apply to fault tolerance by deactivating only lanes that contain defects, instead of whole channels. In our experiments, ABNs reduce total power by 15% for uniform random (UR) trafﬁc and up to 45% by average across application benchmarks with comparable performance, compared to single-lane power-gated networks [28], [29], [35], [9]. Compared to state-of-the-art multi-network designs [7], [13], ABNs reduce total power by up to 33% for application benchmarks and increase throughput by 8% for UR trafﬁc, due to the ﬂexibility ﬂits have to switch lanes at each hop, instead of only at injection time. ABNs also provide more ﬁne-grain fault tolerance than multi-network designs. I I . BACKGROUND AND R ELAT ED WORK Power gating techniques typically disconnect cells from power or ground lines in a coarse- or ﬁne-grain manner using high threshold voltage (low leakage) connector transistors [29], [37], [9]. Such work activates channels or routers in time for ﬂit traversal, or enables detours around inactive resources and guarantees full connectivity [29], [35], [9], [18]. Power gating of input buffers is possible at the granularity of entire buffers [29], [18], VCs [30], or individual entries [24], [32]. Power gating has also been applied to switches and allocators [18], [38]. Other related work scales the voltage or clock frequency of channels and routers [31], [27], [1]. To hide the latency of waking up resources, past work uses look-ahead signals [28], [30]. However, look-ahead signals can cause false activations if they are eligible to activate multiple resources, such as one of multiple VCs [30]. This can occur when a packet A requests an output port that another packet B already has reserved a VC in. In this case, the router may conservatively activate a second VC in anticipation of packet A’s arrival, because packet B’s completion time is unknown. To eliminate false VC activations, ABNs adopt drowsy SRAMs [16]. Drowsy SRAMs can be activated in a single cycle and still hold data when drowsy. However, when deactivated, drowsy cells consume more leakage power than powerFig. 1: With ABNs, packets A and B can be placed in the same channel lanes in low-trafﬁc regions. gated cells, and require more energy to be activated. Drowsy SRAMs were brieﬂy investigated in on-chip networks [10]. Past work, related to ABNs, also adjusts channel bandwidth dynamically but does so by using channels in a bidirectional manner [19], [11], [26]. Further work provides duplicate physical channels between routers instead of VCs, where each channel can reverse direction or be disabled individually [14]. Further related work reduces static power dynamically by using multiple subnetworks [7], [13]. In those designs, trafﬁc sources either inject to an active network or activate a powergated network using oblivious [7] or adaptive [13] metrics. Compared to multi-network approaches, ABNs provide ﬂits the ﬂexibility to switch lanes in each hop, instead of just at injection time. This reduces the number of lane activations and the number of cycles channel wires are active for, especially under uneven network load. As an example, consider the case where packets need to be placed in separate subnetworks such that congestion is avoided in a high-trafﬁc region. This placement, however, is not optimal for low-trafﬁc regions the packets may traverse, resulting in more channel activations in the low-trafﬁc regions. This is illustrated in Fig. 1. Packet placement in subnetworks affects performance in addition to energy because packets encountering congestion are unable to utilize another subnetwork’s bandwidth. Also, packet injection decisions are inherently less accurate than per-hop decisions because global and accurate knowledge of current and future network state is impossible at injection time. ABNs also more readily apply to fault tolerance since a defect in a single channel wire shuts down only the affected lane of that channel. In multi-networks, a single fault would disable an entire subnetwork, without the complexity to enable packet detours [36]. Finally, the radix of the network interface at each endpoint increases with the number of subnetworks. On the other hand, an ABN with two lanes and the same bisection bandwidth as a multi-network design with two subnetworks has half the number of switches but of twice the radix each. Due to the quadratic cost of switches with radix, this results in half the switch area and energy for multi-networks compared to ABNs, and simpler switch allocators. Mechanisms to dynamically detect silicon defects have been proposed [17]. Faulty channels can be disabled which forces packets to route around faults [34]. Alternatively, channels can include spare wires to replace faulty wires [12], [39], ﬂits Fig. 2: ABNs divide channels into lanes. Each lane is an independent power gating domain. can be serialized through the functioning wires [33], [8], or channels can be used in a bidirectional manner [36], [14]. ABNs advance the state of the art by using the low activation delay of drowsy SRAMs to activate VCs without false activations, adjusting the bandwidth in every hop to match trafﬁc demands without the drawbacks of multi-network approaches, and using the same lane and VC activation mechanism for channel fault tolerance in addition to reducing static power. I I I . ADA P T IV E BANDW IDTH N E TWORK S A. Multi-lane Channels Fig. 2 illustrates how ABNs divide channels into lanes without affecting the bisection bandwidth (which is an orthogonal option). Each lane is an independent power gating domain and is activated according to local trafﬁc demands. We use channel power gating as described in [29], [10], which disconnects cells from ground using high voltage threshold (low leakage) connector transistors. We use a 65nm technology library and modify the models of [2] to estimate area, power, and wakeup latencies. Using those models we pessimistically estimate 3ns for the channel activation latency (LaneActLat ), which matches the upper bound reported by [29], [10] for another 65nm process. Power-gated channel bits consume 0.5% of their leakage power (Laneinact ), due to the connector transistors. Channel lanes are activated by the router that is driving data on them. Therefore, each lane requires an extra wire to control the high voltage threshold connector transistors and therefore the lane’s status. Finally, the activation energy penalty is the equivalent of eight clock cycles of leakage power at 1GHz (LaneActP en ). This covers the activation penalty, the propagation of the control bit, as well as the gradual increase and decrease of leakage power. We restrict packets to using one lane per hop per cycle. In other words, multiple ﬂits of the same packet may not be transmitted in the same cycle using different lanes. This decision was made in the interest of static power and resembles the operation of alternative techniques such as multi-networks [7], [13] which assign packets to a single subnetwork. Without this restriction, a packet could activate all channel lanes, which defeats the purpose of channel lanes. However, this restriction increases serialization latency similar to multi-networks. Still, execution time never increases more than 1% in two-lane ABNs for our application benchmarks. B. Router Datapath For input buffers, we model drowsy SRAM cells [10]. Each VC can be activated independently from other VCs and in a single cycle (V CActLat ). When inactive, drowsy SRAM cells consume 15% of their active leakage power (V Cinact ). We VC 0 VC 1 VC 2 VC 3 VC 3 Reg Reg Reg Reg Reg Reg Reg Reg Fig. 3: One input and two outputs are shown. The switch connects to each VC and each output lane. These connections are power gated according to the state of VCs and output lanes. VC 0 VC 1 VC 2 VC 3 Fig. 4: A multiplexer for each VC is required because ﬂits from any lane may be destined to any VC. Look-ahead signals minimize the timing impact of these multiplexers. pessimistically estimate the energy penalty for activating a VC to equal sixteen cycles of leakage power at 1GHz (V CActP en ). This covers both the activation energy and the leakage power during the cycle that a VC is activating or deactivating. Even though channels may deliver one ﬂit per lane per cycle, those ﬂits will be destined to different VCs since packets are restricted to send only one of their ﬂits per cycle. Therefore, VCs can be placed in separate SRAMs to make managing VCs as independent power gating domains more straightforward. To avoid making the input side of router switches a bottleneck, router switches need to connect to every input VC with separate switch lanes. At the output side, switches connect to each lane of each output channel. Therefore, switches have (I nputP orts × V C s) inputs and (OutputP orts × C hannelLanes) outputs. This way, routers can transmit a ﬂit to each lane of each output port in every cycle, and each input VC can transmit a ﬂit independently of other input VCs. In a single-lane network with the same bisection bandwidth switches have I nputP orts inputs and OutputP orts outputs, but the width of each of these switch input and output ports equals the width of C hannelLanes switch ports in ABN switches. Therefore, the output side of ABN switches has the same number of bits as a single-lane network with the same bisection bandwidth, whereas the input side is no wider as long as C hannelLanes ≤ V C s. In addition to VCs and channel lanes, ABNs also apply power gating to switches [18]. At the input side, the switch connection to a VC is only active when the VC is active. At the output side, switch lanes are only active when the corresponding channel lanes are active. This is shown in Fig. 3. Because ﬂits in any channel lane may be destined to any input VC, input buffers require a multiplexer for each input VC, as shown in Fig. 4. To mitigate the timing overhead to the last pipeline stage of the channel, the multiplexer’s control inputs for each input VC arrive one cycle before the ﬂit, using the look-ahead signal for that ﬂit (explained in Section III-D). Still, with a large number of channel lanes, this multiplexer may pose a noticeable timing overhead even with preset control inputs. We can simplify this multiplexer by mapping a subset of VCs to each lane such that choosing an output VC restricts the allowed output channel lanes. While this will result in more channel lane activations due to choice restriction, it also simpliﬁes switch allocation by reducing the possible input VC–output port combinations. Finally, if the number of VCs equals the number of lanes, reserving an output VC essentially also reserves a lane in the output channel. Therefore, VC allocation is no longer required. We call this option ABN simple and quantify its efﬁciency in Section V. Since packets choose a VC before lanes, ABNs use VCs similarly to networks without lanes for deadlock avoidance. C. ABN Complexity ABNs increase switch allocator complexity because multiple grants may be generated for each output (one for each lane), and each input VC may be granted independently of other VCs of the same input. With ABNs, switch allocators perform an (I nputP orts × V C s) × (OutputP orts × C hannelLanes) allocation, where the same input can receive multiple grants to different VCs in the same cycle. However, in the typical case where C hannelLanes ≤ V C s, the switch allocator is no more complex than the VC allocator, which performs an (I nputP orts × V C s) × (OutputP orts × V C s) allocation where the same input can also receive multiple grants to different output VCs. Past work reports that in a typical mesh with 2 VCs, extending the radix of the switch allocator to become equivalent to that of the VC allocator extends the switch allocator’s minimum timing path by 10% for separable allocators [4]. Even in a high radix ﬂattened butterﬂy (FBFly) topology, the switch allocator’s path is only extended by 15% [4]. However, given than VC and switch allocation are typically performed in separate pipeline stages and the switch allocator is no more complex that the VC allocator if C hannelLanes ≤ V C s, this timing overhead is unlikely to extend the router critical path. In addition, increasing the switch allocator radix in a mesh with 2 VCs to match that of the VC allocator would increase area by approximately 30% and power by 35% for separable allocators [4]. However, the VC allocator occupies approximately 5000 µm2 and consumes 2 to 10 mW, both of which are very small percentages of the router [4], [21]. For example, the Intel Teraﬂop chip consumes 7% of the network’s power for allocation and all other router logic [21]. As stated in Section III-B, ABNs do not increase router switch radix compared to a single-lane network with the same bisection bandwidth as long as C hannelLanes ≤ V C s. Moreover, ABNs have the same overhead for power gating compared to past work because ABNs use the same models of power-gated transistors [18], [29], [7], [13]. Finally, past work also uses look-ahead signals for wakeup. ABNs reduce the overhead of look-ahead signals by using drowsy SRAMs which allows a single look-ahead signal per ﬂit. D. Router Pipeline and Control The router pipeline is illustrated in Fig. 5. For each ﬂit receiving a switch allocator grant, a look-ahead signal is sent one cycle before the ﬂit enters the output channel that: Router IB: Flits are written into input buffers VA: VC allocation. SA: switch  allocation. ST: switch traversal. Routers use look-ahead routing. 1 2 IB VA SA ST B Link trav. A IB VA SA ST Fig. 5: The pipeline for two consecutive routers. A look-ahead signal is created for each ﬂit winning switch allocation. • A: Alerts the downstream router that a ﬂit will be arriving for a certain output. The router can then activate more switch and output channel lanes. • B: Alerts the input buffer of the downstream router of the VC the ﬂit will be arriving for. If that VC is inactive, it will be activated. The multiplexer at the input side of that input VC will be set at the beginning of the next cycle. Look-ahead signals are generated for each ﬂit in a packet. That is necessary because switch allocation may delay body ﬂits long enough for resources to power down. Each lookahead signal contains the input VC and output port the ﬂit will use in the downstream router, obtained in the upstream router using look-ahead routing [28]. Therefore, look-ahead signals are log2 Outputports + log2 I nputV C s + V alidB it bits wide. One look-ahead signal is required per channel lane. Channel and switch lanes are activated to match the number of ﬂits destined to each output port. Routers maintain a counter per output port. Look-ahead signals increment the counter for the output port they are requesting. Flits receiving a grant in switch allocation decrement the counter. Routers activate as many channel lanes and corresponding switch output lanes as the counter’s value, but with a delay of LaneActW ait cycles. Speciﬁcally, lane X is activated if the counter’s value for that output has been at least X continuously for the last LaneActW ait cycles. This ensures that ABNs do not overreact to short-lived congestion. ABNs can use more complex policies that consider past or neighboring state [1], [14], [19], [11], but this is left as future work. Routers deactivate lanes if they are inactive for LaneDeactW ait cycles. In our four-stage routers, look-ahead signals hide three cycles of activation latency for switch and channel lanes. That is because look-ahead signals arrive one cycle in advance of their corresponding ﬂit, and ﬂits need to go through VA and SA before traversing the switch. In our 65nm technology library and power gating models [29], [10], this is enough to hide the lane activation delay in full. In addition, since switch traversal precedes link traversal by a cycle, channel lanes need only be activated one clock cycle after the corresponding switch lanes. Because of the proactive nature of lane activation, false lane activations are possible. A false lane activation is an activation without a subsequent ﬂit traversal. Consider the case where packets A and B arrive for the same output port, but A is stalled waiting for credits until after B departs the router. In this example, two lanes are activated to guarantee that A will not stall waiting for a lane if a credit arrives. However, one active lane would sufﬁce. We quantify the frequency of false lane activations in Section V-B. Look-ahead signals arrive only one clock cycle in advance of the corresponding ﬂits. One cycle sufﬁces to hide the singlecycle activation delay of drowsy SRAMs. Power gated (nondrowsy) SRAMs which require more cycles (such as three cycles in [30]) would require an additional look-ahead signal before VC allocation in the upstream router’s pipeline. The single-cycle activation delay enables the use of the upstream router’s VC allocator to activate downstream VCs. This eliminates false VC activations [30] because only VCs that ﬂits are actually assigned to are activated. However, downstream VCs are not activated until after the ﬂit wins switch allocation. In addition, to reduce unnecessary VC activations, VC allocators prioritize active output VCs. Input VCs are deactivated if empty and have either been idle for V CDeactW ait cycles or all channel lanes to that input are inactive. E. Silicon Defect Tolerance While the primary purpose and novelty of ABNs lie in static power consumption, ABNs also readily apply to isolating channel defects with only the additional cost for detecting faults. With ABNs, faulty VCs as well as channel or switch lanes can simply be disabled, but the remaining resources are still usable. Therefore, a single fault does not necessitate disabling entire input ports or channels and resort to extra complexity to enable detours [25], [34]. If the fault probability for a single channel bit is P (ranging from 0 to 1), channel width W, number of lanes L, the probability for a channel to fully fail is: ( W L × P )L L = 1 represents the baseline single-lane network assuming channel bit failures are independent events. Lanes are considered failed if they contain at least one faulty bit. As the number of lanes increases, the probability that all lanes contain a fault decreases. Therefore, ABNs are more likely to maintain network connectivity with an equal number of faults compared to the baseline single-lane network. Multinetwork designs will fail if a single channel in any subnetwork fails, without extra VCs and complexity to enable detours [36], [25], [34]. That is because ﬂits already injected to the faulty subnetwork cannot switch subnetworks, and there is propagation delay to alert all trafﬁc sources of the fault. IV. M E THODO LOGY For our evaluations, we use a modiﬁed version of Booksim [22]. We present results for synthetic trafﬁc and PARSEC benchmarks collected with Netrace traces [5], [20], which respect packet dependencies and therefore reﬂect the impact of the network to application execution time. For synthetic trafﬁc we use a read-reply communication protocol. The trafﬁc pattern decides the destination of read and write requests. Each request generates a reply. Read requests and write replies are 128 bits. Write requests and read replies are 640 bits. For our synthetic trafﬁc we vary the injection rate of request packets. We use the Netrace traces provided in the project’s website. These traces were collected for a 64-core cache-coherent chip multiprocessor (CMP) with in-order ALPHA cores. L1 data and instruction caches are 32KBs each, 4-way set associative, and use MESI cache coherency. L2 caches are fully shared SNUCA with 64 banks and 16MB, eight-way set associativity and 8-cycle bank access time. Finally, the memory has a 150-cycle access time and 8 on-chip memory controllers. We simulate 200,000 packets of the parallel region of seven PARSEC benchmarks using their medium size input sets. Longer simulations produce comparable results. We compare the following networks: • Baseline network without power gating (baseline): This is a single network without power gating. • Single-lane power gating network (single-lane): This network represents the state of the art in single-network power gating [28], [30]. In order to isolate the gains from channel and switch lanes, in this network we still use drowsy SRAM cells [30], [10]. • Flexible adaptive bandwidth network (ABN ﬂexible): We keep bisection bandwidth constant compared to other networks. Therefore, with two lanes, ﬂits are half the width compared to the networks above, and each input port has twice the VCs. Flits can choose any output VC. • Simple adaptive bandwidth network (ABN simple): Same as above, but we map lanes to only allow delivery to a subset of VCs. With four VCs and two lanes, each lane delivers to one request VC and one reply VC. • Multi-network designs (multinets): This represents the state of the art in static power reduction [7], [13]. Sources inject ﬂits to the ﬁrst subnetwork unless the count of available buffer space in all input buffers of the injection router is less than half of total buffer size [13]. In that case, sources consider the next subnetwork, and so on. If all subnetworks are congested, sources choose one at random. Bisection bandwidth and ﬂit width equal ABNs for an equal number of lanes and subnetworks. We use an 8×8 2D mesh with dimension-order routing (DOR), 2mm channels, and the router pipeline of Fig. 5. The baseline network has 128-bit channels and two VCs per input, chosen as a good trade-off between performance and cost. VCs are equally divided among requests and replies. To keep total buffer size constant, we increase the number of VCs for networks with more than one lane, because such networks have narrower ﬂits. Making VCs deeper instead typically does not justify the increased cost as long as the credit round-trip delay is covered. Therefore, two-lane ABNs have four VCs per input. The increased number of VCs in ABNs may affect router clock frequency if the VC allocator is in the critical path and the network needs to be clocked at maximum frequency. In that case, ABNs can use fewer VCs which will sacriﬁce performance, but also reduce cost. Increasing the radix of VC allocators also increases their power and area costs [4], but all router allocation and control logic is just 7% of network power in the Intel Teraﬂop processor [21]. Multinets also have more TABLE I: Network and model parameters. Parameter Value LaneActLat V CActLat Laneinact V Cinact LaneActP en V CActP en LaneActW ait LaneDeactW ait V CDeactW ait AreaOverhead 3 cycles 1 cycle 0.5% of full leakage 15% of full leakage 8 cycles worth of leakage 16 cycles worth of leakage 15 cycles 3 cycles 6 cycles 7% VCs in total but the VCs are distributed among subnetworks. For cost estimation, we use a 65nm technology library and a 1GHz clock frequency. We modify the area and power models of [2] to include power gating [29], [10], [18], drowsy SRAMs, and additional overhead such as the extra wires for the look-ahead signals. From these models, we pessimistically estimate the power gating area overhead to be 7% for buffers, switches, and channels (AreaOver ). Drowsy SRAMs can be activated in a single cycle [16], whereas for channel and switch lanes the activation latency is 3ns [29]. Since ABNs hide three cycles of lane activation delay, ABNs fully hide the lane activation delay in this technology process. We derive the parameters shown in TABLE I based on our models and preliminary evaluations. While LaneActW ait , LaneDeactW ait , and V CDeactW ait depend on the probability of ﬂits reusing lanes or VCs, which depends on the trafﬁc pattern, we choose one set of numbers for all trafﬁc patterns for simplicity. We report static power which is predominantly composed of leakage power but also includes the power to toggle the capacitance of the clock input of cells and SRAMs. Static power also includes energy penalties from activating resources. Dynamic power includes look-ahead and wakeup signals. V. EVALUAT ION A. Application Trafﬁc For our PARSEC simulations, we ﬁrst replay the traces and respect packet dependencies. This produces an approximately 0.2% ﬂit injection rate across benchmarks. We call this the low load testcase, which also evaluates application execution time. We then relax packet dependencies and increase the average ﬂit injection rate to 2% (medium load) and 3.5% (high load) across benchmarks. Injection rates higher than 3.5% are susceptible to causing tree saturation in some benchmarks due to load imbalance. Medium and high loads are not used to measure execution time, but rather to load the network in a manner closer to an application with higher loads than our PARSEC benchmarks. The PARSEC benchmarks we choose exhibit a variety of communication patterns. Speciﬁcally, in blackscholes and ﬂuidanimate cores transmit equally to each of two nearby cores, in canneal trafﬁc resembles UR by average, and in the rest of the benchmarks cores transmit different amounts of data to a subset of nearby and distant cores [3]. Two ABN lanes and two mulKnets subnetworks  ) % ( n o 0 c u d e r r e w o p l a t o t e g a t n e c r e P 40  35  30  25  20  15  10  5  0  ) % ( n o 0 c u d e r r e w o p l a t o t e g a t n e c r e P 30  25  20  15  10  5  0  Four ABN lanes and four mulKnets subnetworks  Blackscholes  Canneal  Dedup  Ferret  Fluidanimate  VIPS  X264  Average  Blackscholes  Canneal  Dedup  Ferret  Fluidanimate  VIPS  X264  Average  Low  Medium  High  Low  Medium  High  Fig. 6: Total power reduction of ABN simple with two lanes compared to multinets with two subnetworks. Fig. 7: Total power reduction of ABN simple with four lanes compared to multinets with four subnetworks. Fig. 6 presents the percentage of total power reduction of ABN simple with two lanes compared to multinets with two subnetworks. We observe an approximately 33% average total power reduction for ABN simple for low loads. Application trafﬁc is often bursty and produces unbalanced loads [35], [9], [19], [3]. Hotspots exacerbate the impact of the lack of ﬂexibility ﬂits have in choosing subnetworks after injection, as shown in Fig. 1. Bursty trafﬁc also creates temporary congestion in routers which causes ﬂits injected to that router to be sent to another subnetwork. Those ﬂits may not switch subnetworks after the injection router, and therefore may not share active resources with other low trafﬁc. This results in 43% to 55% more active channel and switch lane cycles by average across benchmarks for multinets compared to ABN simple. Compared to single-lane power-gated networks, ABN simple reduces total power by an average of 45%. Both ABNs and multinets cause an average slowdown of just 0.95%, with the maximum being 1.05% in the case of blackscholes. Static power reductions decrease with an increase in injection rate due to fewer power gating opportunities with more trafﬁc. B. Synthetic Trafﬁc We use synthetic trafﬁc to gain more insight and to evaluate the worst-case trafﬁc for ABNs since UR trafﬁc is perfectly load balanced (load imbalance favors ABNs compared to multinets). Results are shown in Fig. 8. ABN simple saturates at an 8% higher injection rate than multinets because in multinets ﬂits cannot escape transient congestion encountered in a subnetwork even with UR trafﬁc, and perfect injection decisions are unrealistic. This also causes multinets to have a 34% higher average latency close to saturation (40% request packet injection rate). Baseline and single-lane each provide an 8% lower throughput than ABN simple because ABN simple has twice as many VCs. However, due to serialization latency, baseline and single-lane have a 10% lower average zero-load packet latency compared to ABNs and multinets. ABN ﬂexible and multinets have comparable power consumption across injection rates. They each have a 15% lower total power consumption than the single-lane network, and 24% compared to the baseline. In addition, multinets have 7% lower dynamic power compared to each other network, because multinets use twice the number of switches of half the radix, which therefore incur one quarter of the cost each. However, because ﬂits pick a subnetwork at injection time with imperfect knowledge and are not able to switch subnetworks later, multinets have a 9% higher static power compared to ABN ﬂexible due to a 36% higher channel static power and 48% higher buffer static power. This offsets the gains in dynamic power for multinets. ABN simple has no false lane activations because once a packet chooses an output VC, all ﬂits have to use the lane that output VC is assigned to. ABN ﬂexible has a 19% lower activation power overhead than multinets because it experiences 17% fewer activations, since it is free to make maximum use of already active lanes. In the baseline single-lane network, channel static power is 51% of overall static power, while buffer static power is 20% and switch 23%. Static power is 62% of overall power under a 2% request packet injection rate. ABN ﬂexible reduces channel static power including activation penalties by 53%, buffer static power by 73%, and switch static power by 56%. Comparing ABN simple and ﬂexible, ABN simple saturates at a 21% higher injection rate. This is because in ABN ﬂexible ﬂits may request any output lane. This increases the allocation problem and intensiﬁes the inefﬁciencies of our separable single-iteration VC and switch allocators. In summary, for UR trafﬁc multinets have power consumption comparable to ABNs, but lower performance. UR trafﬁc is the worst-case for ABN because trafﬁc patterns with imbalance in time or space favor ABNs due to the ﬂexibility in ABNs in choosing lanes in each hop. As discussed in Section V-A, ABNs lower the total power consumption by up to 33% compared to multinets for application trafﬁc. C. Increasing the Number of Lanes In this Section, we divide the same bisection bandwidth to four lanes for ABNs and four subnetworks for multinets. As shown in Fig. 7, ABN simple reduces total power by 21% under low loads, 8% under medium loads, and 13% under high loads, compared to multinets. Power reductions are smaller compared to Section V-A because further subdividing into more subnetworks makes router switches in multinets more energy efﬁcient due to their quadratic cost with radix. In addition, power gains under high loads are larger for ABN simple because the lack of ﬂexibility of ﬂits in multinets becomes                     0 0 20 40 60 Injection rate (request packets/cycle * 1000) 50 100 150 200 A e v r e g a l a t y c n e ( k c o c l s e c y c l ) 8x8 mesh. DOR. UR traffic Baseline Single lane ABN flexible ABN simple MultiNets 0 0 10 20 30 40 50 Injection rate (request packets/cycle * 1000) 2 4 6 8 10 12 o T t a l o p w e r ( W ) 8x8 mesh. DOR. UR traffic Baseline Single lane ABN flexible ABN simple MultiNets 0 0 20 40 60 Injection rate (request packets/cycle * 1000) 1 2 3 4 5 S t a i t c o p w e r ( W ) 8x8 mesh. DOR. UR traffic Baseline Single lane ABN flexible ABN simple MultiNets Fig. 8: ABNs have two lanes and multinets consist of two subnetworks. Static power includes activation penalty power. 0 0 10 20 30 40 50 Injection rate (request packets/cycle * 1000) 50 100 150 200 8x8 mesh. DOR. UR traffic. ABN simple 250 A e v r e g a l a t y c n e ( k c o c l s e c y c l ) Single lane ABN two lanes ABN four lanes Two multinets Four multinets Fig. 9: Scaling of ABNs and multinets with UR trafﬁc. more pronounced compared to having two subnetworks. ABNs also have a marginal (1%) beneﬁt in execution time under low loads in ﬁve benchmarks compared to multinets. Fig. 9 presents latency under UR trafﬁc. As shown, multinets with four subnetworks saturate at a 5% lower injection rate and have a 45% higher average latency compared to ABN simple with four lanes. This is because the effect of ﬂits being unable to switch to idling subnetworks is more pronounced with four subnetworks compared to two. Finally, ABNs with four lanes and multinets with four subnetworks have a 19% higher average zero-load latency than ABNs with two lanes. D. Design Space Exploration To motivate use of drowsy SRAM cells, we compare ABN ﬂexible with drowsy SRAMs and power-gated (non-drowsy) SRAMs [30], [16] under UR trafﬁc. At low loads, we observe an average 43% false VC activations with power-gated SRAMs (there are no false activations with drowsy SRAMs), and 15% more active VC cycles for power-gated SRAMs. However, due to the different energy overheads, non-drowsy SRAMs consume 22% less activation power, but 32% more static power due to the extra active cycles, resulting in 11% higher static power (including activation overhead) overall. At high loads, static power without activation overheads is comparable, but non-drowsy SRAMs incur 12% more activation power due to the 35% false VC activations. While these numbers depend on the trafﬁc pattern, they show the beneﬁts of drowsy SRAMs, while also simplifying the router pipeline since one wakeup signal sufﬁces for both VCs and channel lanes. We also evaluate a baseline network with four VCs and ABNs with eight VCs to test the sensitivity of our results to the number of VCs. With UR trafﬁc, ABN simple has 0 0 20 40 60 Injection rate (request packets/cycle * 1000) 2000 4000 6000 8000 10000 C s e c y l n u l i t e n n a h c l f a l i u r e 8x8 mesh. DOR. UR traffic Single lane / Multinets ABN two lane ABN four lane Fig. 10: Time until a packet chooses an output port leading to a failed channel, for single-lane, multinets, and ABN ﬂexible. comparable (1% higher) performance than multinets with separable allocators, and 3% higher with wavefront allocators. However, with realistic application trafﬁc, there is insigniﬁcant impact because the network is not close to saturation. Finally, our results depend on the relative contributions of channels and switches. Topologies with higher-radix switches, such as the FBFly, favor multinets because router switches have a higher radix and therefore the beneﬁt of reducing their radix in half in multinets is relatively larger. In contrast, topologies with longer channels, such as a mesh with longer channels than our mesh, favor ABNs because ABNs reduce channel leakage power compared to multinets. E. Silicon Defect Tolerance To measure the improved resiliency of ABNs, we simulate UR trafﬁc and assign a 5× 10−4 probability that any one channel bit line will fail in each cycle. We assume that channels have two spare bit lines [12], [39]. We report the time that a packet chooses an output port that leads to a failed channel (without detours). In the case of ABN ﬂexible, this means all lanes have failed. Essentially, this is the time period that the network is no longer able to function correctly. Multinets have comparable probability as the single-lane network because when a channel in any subnetwork fails, ﬂits already injected may not switch subnetworks to avoid the faulty channel, and there is propagation delay to alert sources. Fig. 10 shows the increased resiliency of ABNs (2× higher for two lanes compared to single-lane and multinets). The variance in our results, especially for four lanes, stems from the many random choices in each experiment. ABN simple performs in between ABN ﬂexible and multinets.                                               V I . D I SCU S S ION Our results illustrate the advantage of allowing ﬂits to switch lanes in ABNs with local per-hop decisions to accommodate regions with different trafﬁc conditions, compared to multinets where the decision is made once at injection time where perfect knowledge of current and future global state is impossible. This translates to performance and power beneﬁts, especially with unbalanced trafﬁc. However, dividing a single network into subnetworks with multinets makes router switches more energy and area efﬁcient. Comparing ABN simple and ﬂexible, ABN simple provides better performance but ABN ﬂexible provides slightly lower static power. Routers with shallow pipelines would activate VCs in a similar manner because of the single-cycle activation delay of drowsy SRAMs. However, routers with shallow pipelines or higher clock frequencies (which increase wakeup latencies in terms of clock cycles) may not be able to fully hide channel and switch lane wakeup latencies with look-ahead signals. Future technologies may also affect wakeup latencies. For example, past work reports 5.1ns for a 32nm technology library [13] and 4ns for 45nm [9]. If wakeup latencies cannot be fully hidden, networks can either use predictors similar to nondrowsy SRAMs [30], or leave some lanes constantly activated. In addition, ABNs have similar power gating granularity and require similar power distribution networks or power gating transistors than multinets or other past work [24], [32], [13]. Look-ahead signals do cause extra bits to be transported, but they are only a small fraction of the ﬂit width and enable signiﬁcant savings in leakage power which will be critical in future technologies [6], [23], [9]. Finally, upcoming technologies such as FINFETs may reduce the contribution of leakage power, but that is still projected to increase and remain important in future technologies [6], [23]. V I I . CONC LU S ION This paper proposes ABNs. ABNs divide channels and switches into lanes each of which can be activated individually to match trafﬁc demands. Unlike power-gating approaches with multiple subnetworks, ﬂits are free to choose a different lane at each hop instead of committing to a set of lanes at injection time. At the input buffer side, ABNs take advantage of drowsy SRAM cells to activate VCs individually without the possibility of false activations. ABNs also readily apply to silicon defect tolerance. For application trafﬁc, ABNs reduce total power consumption by up to 33% compared to multinetwork designs and up to 45% compared to single-lane networks, with comparable or superior performance. "
CLAP - a crosstalk and loss analysis platform for optical interconnects.,"Basic photonic devices in inter- and intra-chip optical networks suffer from inevitable power loss and crosstalk noise. Incoherent crosstalk introduces quick power fluctuations, while coherent crosstalk varies the optical power of the optical signal in optical interconnection networks (OINs). As a result, the accumulative crosstalk in large scale OINs considerably hurts the signal-to-noise ratio (SNR) and imposes high power penalties. In this work, we aim at studying the worst-case incoherent and coherent crosstalk in OINs at the system level. The proposed analytical models are integrated into a newly developed crosstalk and loss analysis platform, called CLAP, to facilitate the SNR analyses in arbitrary OINs.","CLAP: a Crosstalk and Loss Analysis Platform for Optical Interconnects Mahdi Nikdast1 , Luan H. K. Duong1 , Jiang Xu1 , S ´ebastien Le Beux2 , Xiaowen Wu1 , Zhehui Wang1 , Peng Yang1 , and Yaoyao Ye1 1Hong Kong University of Science and Technology 2Lyon Institute of Nanotechnology, University of Lyon, France Email: mnikdast@connect.ust.hk,jiang.xu@ust.hk,Sebastien.Le-Beux@ec-lyon.fr Abstract—Basic photonic devices in inter- and intra-chip optical networks suffer from inevitable power loss and crosstalk noise. Incoherent crosstalk introduces quick power ﬂuctuations, while coherent crosstalk varies the optical power of the optical signal in optical interconnection networks (OINs). As a result, the accumulative crosstalk in large scale OINs considerably hurts the signal-to-noise ratio (SNR) and imposes high power penalties. In this work, we aim at studying the worst-case incoherent and coherent crosstalk in OINs at the system level. The proposed analytical models are integrated into a newly developed crosstalk and loss analysis platform, called CLAP, to facilitate the SNR analyses in arbitrary OINs. As the technology advances and allows the integration of a large number of processing cores on a single die, metallic interconnects cannot efﬁciently address the on-chip communication requirements in multiprocessor systems-on-chip (MPSoCs) due to their high power dissipation, high latency, and low bandwidth. Optical interconnects using wavelength-dimension multiplexing (WDM), on the other hand, can bring ultra-high bandwidth and very low power consumption as well as low latency to the on-chip communication in MPSoCs. However, the imperfect structure of photonic devices in optical interconnection networks (OINs) causes inevitable crosstalk noise and power loss in such devices. The crosstalk noise and power loss are very small at the device level, but can accumulate in large scale OINs and cause performance degradation. The crosstalk in OINs can be classiﬁed as intrachannel and interchannel crosstalk. Intrachannel crosstalk is when the crosstalk signal is at the same wavelength as that of the desired optical signal. In contrast, interchannel crosstalk is when the crosstalk signal is at a wavelength sufﬁciently different from the desired signal’s wavelength. The intrachannel crosstalk noise is of critical concern because it cannot be removed by ﬁltering, and hence we target to study this type of crosstalk noise in OINs. The intrachannel crosstalk noise can be either incoherent, whose phase is uncorrelated with the desired optical signal, or coherent, whose phase is correlated with the desired optical signal. The incoherent crosstalk noise may also be a coherent combination of crosstalk contributors and then cause much higher noise power [1]. When the optical propagation delay differences exceed the coherent time of the laser, then the crosstalk noise contributors are incoherent with the desired optical signal and, in the worst-case, their powers can be accumulated and ultimately hurt the SNR at the receiver. On the other hand, when the propagation delay This work is partially supported by GRF620911, GRF620512, and DAG11EG05S. (a) Drop Add Drop Add (cid:79)i (cid:79)i (cid:79)i+1 (cid:79)i Input Through (cid:79)i+1 Input Through (b) S2 (cid:79)i out2 Drop (cid:79)i S1 Input out1 out3 (cid:79)1 S1 Input (cid:79)1 (cid:79)2 ... (cid:79)n Add S2 (cid:79)1 Through Fig. 1. Crosstalk and power loss in basic optical devices: (a) coherent crosstalk and (b) incoherent crosstalk. differences are smaller than the coherent time of the laser, then the crosstalk contributors will combine coherently to form a composite crosstalk. The coherent crosstalk causes power ﬂuctuations when the optical propagation delays’ differences are much less than the time duration of one bit, as shown in [1]. In the same work, it was proved that the coherent crosstalk also causes noise if the mentioned condition is not satisﬁed. The major contribution of this work is to extend the newly developed crosstalk and loss analysis platform (CLAP) to include the coherent crosstalk analysis in arbitrary OINs [2]. Relying on our previous works [2] and [3], we present a formal analytical approach that enables worst-case incoherent and coherent crosstalk analyses in different OIN architectures. The presented hierarchical formal analytical approach models the power loss and crosstalk in basic photonic devices and utilizes those analyses to study the worst-case crosstalk and SNR at the router and network levels in OINs. Fig. 1 indicates the incoherent and coherent crosstalk in a waveguide crossing and parallel switching element (PSE) as an example. Considering Fig. 1a, a portion of the optical signal, which has experienced a different delay compared with the desired optical signal, adds to the desired optical signal as coherent crosstalk. As can be seen from Fig. 1b, when the optical signals S1 and S2 , which are from different power sources but have the same wavelength, pass a common waveguide crossing or PSE, a portion of the optical signal S1 (S2 ) mixes with the optical signal S2 (S1 ) as incoherent crosstalk noise. Also, power loss will be imposed on an optical signal when it passes a waveguide crossing or microresonator (MR) as well as when it couples into an MR. With regard to the basic photonic device characteristics, we CLAP analyzer MR MR Device library Optical router analyzer Network library M×N Mesh M×N Folded-Torus k-core Fat-Tree ... Optical router analyzer results Signal power models SNR models Crosstalk noise models Inputs Optical router configuration Photonic device parameters Optical router structure Network configuration Outputs Signal power SNR Crosstalk noise power Analytical equations Fig. 2. CLAP’s overview. develop an analytical approach to analyse the power loss and incoherent and coherent crosstalk in such devices. Considering the PSE shown in Fig. 1, (1) indicates the signal power on the Through port after it passes the MRs, and (2) calculates the signal power on the Drop port by considering the drop loss in the MR. P T = L P D = L λi n λi i−1 in p P (1) (2) λi p LdP λi in In these equations, Lp and Ld are the passing loss and drop loss of the MR, respectively. Also, λi indicates the optical wavelength under consideration, and n indicates the total number of wavelengths in the OIN. Pin is the optical signal power at the input port. Considering the coherent crosstalk in the PSE shown in Fig. 1a, we consider the worst-case scenario, in which the crosstalk signal and the desired optical signal are out of phase and when the state of polarization (SOP) of the crosstalk signal is the same as the SOP of the desired optical signal. As a result, the coherent crosstalk diminishes the desired optical signal power at the receiver, given that the optical propagation delays’ differences are within the time duration of one bit. The introduced power losses to the desired optical signal can be calculated based on the received signal power and the total coherent crosstalk power at the receiver [4]. The coherent crosstalk power in the PSEs shown in Fig. 1a can be analyzed using (5) and considering the passing loss calculation in (1). According to Fig. 1b, the power loss imposed on the optical signal while passing a waveguide crossing is calculated in (3), while (4) calculates the incoherent crosstalk noise at the other two output ports. P λi λi out1 = LcP in λi λi λi out2 = P out3 = XcP in (3) (4) P In these equations, Lc is the crossing loss in a waveguide crossing and Xc is the crosstalk coefﬁcient deﬁned for the waveguide crossing. The incoherent crosstalk noise in the PSE shown in Fig. 1b can be calculated based on the Lorentzian power transfer function of the MR. For the optical signal carried on the wavelength λi , the optical power that is transferred to the drop port can be expressed as (5) [5]. P λi drop λi in P = δ2 (λi − λM Rm )2 + δ2 (5) In this equation, 2δ is the 3 dB bandwidth of the MR and it can be calculated based on Q = λM R 2δ , in which Q is the Waveguide Optical terminator (cid:48)(cid:53) Microresonator (cid:51)(cid:39) Photodetector Core . . . . . . . . . . . . . . . . . . . . . . . . ... . . . . . .CA cluster CC0 Cluster agent i Optical switching box . . . ... MR MR ... MR MR ... PD PD ... PD PD . . . ... PD PD Interface to cluster agent ... MR MR ... MR MR ... PD PD VCSELs VCSELs VCSELs CA7 CA0 CA1 ... CA63 CA56 CA57 CA i CC1 CC2 CC63 CC62 CC3 CC i Core cluster i C o n n e c e d t t o C A Optical transceiver ... Optical transceiver Optical receiver ... PD PD Optical receiver Data channel 0 Data channel i Data channel i+1 Data channel M-1 Fig. 3. The sectioned undirectional optical ring (SUOR) structure [7]. quality factor of the MR. λM R m is the resonant wavelength of the m-th MR. The analytical models at the basic device level can be used to study the power loss, crosstalk noise, and SNR at the optical router and network levels. The newly developed CLAP is extended to realize the coherent crosstalk analysis in arbitrary OINs. Fig. 2 illustrates the internal structure of CLAP. CLAP is publicly released and it is available online with documentation [2], [6]. CLAP has a complete library of basic photonic devices, which allows the design of different optical router and network architectures. Using different considered input ﬁles, the user can easily deﬁne the optical router structure and conﬁguration, the optical network and its communication pattern, the loss and crosstalk noise values in the basic devices as well as their dimensions, the FSR, the MR’s Q-factor, the number of wavelengths in the network, the laser input power, etc. CLAP analyses the power loss, incoherent and coherent crosstalk noise, and SNR at the destination of an optical link that the user has deﬁned. Furthermore, CLAP is capable of providing the worst-case and the average results. By way of example, we employed CLAP to compare the worst-case SNR and power loss in two ring-based optical networks-onchip (ONoCs), Corona and sectioned undirectional optical ring (SUOR) networks, both of which consisted of 256 processor cores [3], [7]. Fig. 3 indicates the architectural overview of SUOR. The analyses results indicate that the crosstalk and power loss from the basic optical devices considerably hurt the SNR, and especially impose severe power loss in the ringbased ONoCs: for example, the worst-case SNR and power loss on the Corona’s data channel are equal to 13 dB and -69 dB, respectively. "
Transient queuing models for input-buffered routers in Network-on-Chip.,"Analytical modeling of Network-on-Chip mainly focuses on steady-state conditions although traffic patterns and the behavior of applications are frequently non-stationary. Hence, in such scenarios, it is likely that a system seldom reaches stationarity, and steady-state models are inapplicable. In this work, we propose queuing-theoretic models for the transient analysis of output contention in Network-on-Chip. Contention occurs when multiple input queues intend to forward to the same output, and it is one of the main reasons for increased latencies and blocking probabilities in input-buffered routers. In Network-on-Chip, the end-to-end latency of a single packet can be determined by adding the latencies along its path. Therefore, understanding interactions within a single router are beneficial for system and parameter design of Network-on-Chip. Furthermore, we validate the models proposed by numerical evaluations which confirm the accuracy and practicality of the queuing models.","Transient Queuing Models for Input-Buffered Routers in Network-on-Chip David ¨Ohmann, Erik Fischer, and Gerhard Fettweis Vodafone Chair Mobile Communications Systems Technische Universit ¨at Dresden, Germany Email: {david.oehmann, erik.ﬁscher, fettweis}@tu-dresden.de Abstract—Analytical modeling of Network-on-Chip mainly focuses on steady-state conditions although trafﬁc patterns and the behavior of applications are frequently non-stationary. Hence, in such scenarios, it is likely that a system seldom reaches stationarity, and steady-state models are inapplicable. In this work, we propose queuing-theoretic models for the transient analysis of output contention in Network-on-Chip. Contention occurs when multiple input queues intend to forward to the same output, and it is one of the main reasons for increased latencies and blocking probabilities in input-buffered routers. In Network-on-Chip, the end-to-end latency of a single packet can be determined by adding the latencies along its path. Therefore, understanding interactions within a single router are beneﬁcial for system and parameter design of Network-on-Chip. Furthermore, we validate the models proposed by numerical evaluations which conﬁrm the accuracy and practicality of the queuing models. I . IN TRODUC T ION Network-on-Chip (NoC) is a widely used technology for interconnection in multiprocessor System-on-Chip (MPSoC) [1]. Instead of buses and crossbar switches, processing elements are interconnected by a packet-switched network of routers similar to off-chip networks. In case of input-buffered routers, a common problem is head-of-line blocking where input queues are stalled due to output contention, see e.g., [2] and [3]. Hence, high contention scenarios cause increases in latencies and blocking probabilities of packets routed through a network. Therefore, a reasonable and sturdy design of network topologies and parameters, in order to provide quality of service guarantees (e.g., for high performance or real time applications), is of high importance, [4]. Although other techniques to prevent head-of-line blocking, such as virtual output queues [3], exist, achieving a deeper understanding of output contention by the techniques presented in this work is worthwhile for system design and analysis. Chip designers can examine their designs by experiments or extensive cycle-accurate simulations, but these methods are time- and resource-consuming. Another way to gain insights into the performance of NoC is by utilizing analytical models. Especially for early design phases, when many possible designs have to be evaluated and compared, easily computable and simple analytical models are an interesting and promising option. Another argument for the use of analytical approaches is the dramatic increase in the number of processing and routing elements in MPSoC [1]. It is expected that future systems consist of hundreds or thousands of processing elements and routers. Therefore, performing extensive simulations becomes cumbersome or even impractical, and abstractions are needed. Analytical models offer a fast and efﬁcient solution to review a system’s performance. In literature, there are basically two types of models. First, network calculus is a common toolset to analyze performance guarantees in terms of quality of service and has also been applied to NoC [5]. Although network calculus enables capturing time-dependent and dynamic behavior of a system, it is often limited to worst-case analysis and to an evaluation of performance bounds. Second, stochastic modeling based on queuing theory (see [6] for more details) is a prevalent technique to assess the average performance of NoC, [7]-[10]. It is important to note that most of the related works focus on the analysis of stationary trafﬁc conditions neglecting that NoC frequently experiences ﬂuctuating trafﬁc intensities and time-varying statistics. For instance, in [11], Zhang et al. propose a queuing model for output contention and ﬂow control feedback between adjacent routers, but they target the steady-state performance of the system. Further work on the modeling of output contention can be found in [12]. Nevertheless, a few works on the analytical characterization of non-stationary trafﬁc and its consequences on the system performance exist. In [13] and [14], Bogdan et al. introduce a statistical physics inspired model for characterizing the fractal nature of dynamic NoC trafﬁc. Additionally, Qian et al. propose in [15] a ﬂexible reconﬁgurable NoC architecture in combination with a dynamic routing algorithm, which enables a NoC to adapt to dynamic trafﬁc scenarios. In this work, we utilize queuing theory to investigate an aspect which has, to the best knowledge of the authors, not been addressed so far. More precisely, we propose queuingtheoretic models for the transient behavior of input-buffered routers. The transient behavior describes the behavior of a system over time and is of special interest when dealing with highly dynamic systems (for instance, those with timevarying trafﬁc statistics), where stationarity is scarcely reached and steady-state models might be inapplicable. Dynamic task mapping or workload adaptive routing algorithms are only two examples of such applications [16]. In addition, recent publications such as [14] illustrate that steady-state analysis is often not sufﬁcient for heterogeneous and dynamic systems like NoC. Transient models provide a deeper insight into the system’s behavior, and include the steady-state as well; therefore, transient analysis represents a more general solution. Furthermore, while related work focuses mainly on mean values only, our models are capable of providing complete distributions of statistics such as queue lengths, latencies, and blocking probabilities, which enables more detailed analyses. Note that although the models presented in this paper are primarily designed for NoC, the principles and techniques are valid for systems with input-buffered queues in general. The paper is structured as follows. After introducing the system model in Sec. II, we present a simple model for routers with two competing input queues and with an arbitration scheme based on ﬁxed priorities in Sec. III-A. This examination also helps to understand the phenomenon of output contention and illustrates that even in a simple conﬁguration, detailed modeling of system behavior is challenging. In Sec. III-B, we extend the queuing models to consider three or more queues, and validate the models by numerical evaluations. Finally, we conclude the paper in Sec. IV. I I . SY ST EM MODE L In a NoC, processing elements such as computing cores and memory entities are connected to routers which form an interconnection network comparable to standard computer networks. One modeling idea is to distinguish between network and router level, as done in [10]. A network model characterizes the trafﬁc streams between the routers in terms of trafﬁc intensities and interarrival time distributions. At another level, the behavior of the individual routers in terms of latency and blocking probabilities is described by a model for output contention. In this work, we focus on the transient behavior of the basic elements, the routers. It is left for future work to connect multiple instances of the transient router model to a network. For the router model, we assume one input-buffered queue per input port. The number of outputs must not necessarily be equal to the number of inputs I , but for simplicity, we only deal with the same number of inputs and outputs. Additionally, the queuing models proposed in this work describe systems with limited queuing space. Throughout the paper, we use a maximum queue size N of 10 ﬂits (ﬂow control digits) for numerical evaluation. If a queue is already ﬁlled, additional ﬂows that arrive are blocked and discarded. Data transmissions in the NoC are described based on the notion of ﬂits, and we consider a synchronous system, i.e., a common clock is used for the NoC. Therefore, arrival and service of ﬂits happen only at discrete time instances, which leads to discrete-time queuing models. Additionally, we assume a delay between arrival and service. A ﬂit which arrives at an input port is stored in its corresponding queue. After at least one cycle in the queue (if there are no other ﬂits to be served), the ﬂit gets into service where, if possible, it is forwarded to an output. If two or more queues try to forward a ﬂit to the same output, a contention resolution carried out by an arbitration unit is necessary. There are different techniques for carrying out contention resolution, e.g., by ﬁxed priorities or roundrobin arbitration. Depending upon the arbitration scheme, the arbitration unit allows exactly one input queue to forward its ﬂit. The other queues are not allowed to send and have to wait for the next cycle during which they can retry forwarding. This short introduction already indicates why output contention causes increased queue lengths and latencies. Since ﬂits are destined for different destinations, the forwarding probabilities of ﬂits at a speciﬁc queue need not be uniform. We deﬁne a matrix F = [fi,j ] where fi,j describes the probability of a ﬂit in queue i being forwarded to output j . Later on, we utilize these probabilities to characterize the contention scenario in more detail. We assume that the forwarding probabilities as well as the arrival rates λi ∈ [0, 1] (describing the trafﬁc intensities (in ﬂits per cycle) which enter the router) are known, e.g., given by a network model such as in [10]. Since λi gives the probability of an arrival event, the probability of no arrival is 1 − λi and, additionally, the arrival process is characterized by a geometric distribution. I I I . TRAN S I EN T ROUT ER MODE L S In this section, we put forward queuing-theoretic models characterizing the transient behavior of NoC routers. A. Simple NoC router with two inputs and ﬁxed priorities First, we start with a simple router consisting of two inputs and two outputs. In this example, the structure of F containing the forwarding probabilities is (cid:20)f1,1 f2,1 (cid:21) f1,2 f2,2 F = . (1) The scenario and the forwarding probabilities are depicted in Fig. 1, where, for instance, f2,1 describes the probability that a ﬂit served in input queue 2 is destined for output 1. Fig. 1. Schematic diagram of a router with two inputs. The arrival rates, service rates, and forwarding probabilities are annotated. a) Problem Description: In order to keep the problem as simple as possible, we apply an arbitration scheme based on ﬁxed priorities. For output 1, the order of priority is {1, 2}, and for output 2, it is {2, 1}, which means that if both servers process ﬂits with the same destination i, queue i is prioritized and allowed to forward its ﬂit. The other queue is stalled, i.e., the server waits for the next cycle to retry transmission. Note that there is a mutual dependency between the queues. The queue size of queue 1 depends on the activity of queue 2 and vice versa. In the remainder of the paper, we use the term blocking when a queue is stalled due to output contention and should Third, for computing service rates, we also deﬁne the probability of being in an active, non-blocking state (the corresponding states are marked in Fig. 2) as ηi,NB (t) := πi (x, t). (3) (cid:88) Fig. 2. A coupled queue within a router with ﬁxed priorities is characterized by a state diagram consisting of 2(N + 2) states. not be confused with blocking that occurs when ﬂits arrive at queues that are already full. b) Queuing model: The behavior of each input queue is described by a queuing model deﬁned as follows. For the special case of ﬁxed priorities, we model output contention by utilizing a state model consisting of 2(N +2) states as depicted in Fig. 2.1 The lower row of states describes the system in a non-blocking state, i.e., there was no stalling in the last cycle. On the contrary, if the corresponding queue has been stalled, the system transitions into one of the blocking states described by the upper row. Furthermore, the columns of the state model denote the number of active ﬂits in the queuing system, e.g., state 0 and 1 describe an idle system, state 2 and 3 represent a system with one ﬂit in service and an empty queue, and so forth. The meaning of the columns is indicated in Fig. 2 as well. Furthermore, the transition rates among the states are denoted by black arrows. Note that some horizontal transitions are not considered, e.g., a direct transition from state 2 to 4 is not possible. This can be reasoned as follows. In the state model, the system can only change to a state on the right if there was an arrival and no service event in the last cycle.2 However, the latter implies that the system goes to a blocking state; therefore, a direct transition from 2 to 4 is not possible. Moreover, note that different service rates µi,B and µi,NB are used in the upper and lower states which is explained hereinafter. c) Estimation of service rates: Before determining the service rates, we introduce a few other metrics required in the Furthermore, it is required that (cid:80) remainder of the paper. First, the probability that a queuing system i is in state x at time t is described by πi (x, t). x πi (x, t) = 1. Second, the state probabilities πi (x, t) are utilized to describe the activity of a queue by ηi (t) := 1 − πi (0, t). (2) x>0 x even The service rates in the blocking and non-blocking states of the state model presented are different which can be reasoned as follows. If a ﬂit is served by queue i and is destined for output i, the probability of being stalled is zero due to the ﬁxed priority scheme applied (hereafter referred to as scenario ‘a’). On the contrary, if a ﬂit is destined for output j ((cid:54)= i), the probability of being stalled is greater than zero since there is another queue which has a higher priority with respect to this output and might aim to send a ﬂit as well (hereafter referred to as scenario ‘b’). A newly arriving ﬂit can experience both scenarios, ‘a’ and ‘b’, and therefore, the probability of being stalled is between the individual probabilities of both scenarios mentioned. The latter case, a mixture of scenarios ‘a’ and ‘b’, is applied for computing service rates of nonblocking states. On the contrary, a ﬂit that is already stalled will deﬁnitely experience scenario ‘b’ which implies a higher stalling probability and, therefore, a decreased service rate in the blocking states. By considering the former explanations and examining all possible combinations of contention, the service rate of a queue in a non-blocking state can be computed as µi,NB (t) = fi,i + fi,j [ηj,NB (t)(fj,j · 0 + fj,i ) + (1 − ηj,NB (t))] . (4) Queue i can forward a ﬂit in all cases except the one, where the ﬂit is destined for output j , queue j is active, and queue j also intends to send to output j .3 In this case, queue j has a higher priority and the ﬂit in queue i is stalled. We simplify the expression to µ1,NB (t) = 1 − η2,NB (t)f1,2 f2,2 µ2,NB (t) = 1 − η1,NB (t)f2,1 f1,1 . (5) The service rates of blocking states are a bit different. Since it is known that the ﬂit in service is destined for the other output, the forwarding probability fi,j is 1, and the service rates for the blocking states change to µ1,B (t) = 1 − η2,NB (t)f2,2 µ2,B (t) = 1 − η1,NB (t)f1,1 . (6) In (5) and (6), we utilize ηj,NB instead of ηj since queue j can prevent queue i from transmitting only if queue j is in a non-blocking state. If queue j is in a blocking state, it is ensured that queue i is allowed to send in the subsequent cycle. 1 The number of states can be reasoned as follows. There are two rows with N + 2 states each (N queuing slots plus idle state plus server busy state). 2 For simplicity, we assume that a queue which is not affected by contention serves one ﬂit per cycle, i.e., the standard service rate is one ﬂit per cycle. 3 The activity of queue i does not matter since having a service rate implies an active queue. (a) System startup with constant arrival rates. (b) Time-varying arrival rates according to (7). No steady-state exists. Fig. 3. The average number of ﬂits in the queues are depicted over time. For the evaluation, the system model of Fig. 1 with two inputs and two outputs is used. Heterogeneous arrival rates and forwarding probabilities are applied (F = [0.3, 0.7; 0.6, 0.4] and λ = [0.7, 0.9]), and initially, the system starts with zero activity. d) Computation of Transient Behavior: The transient tion (cid:80) behavior is computed iteratively. Initially, for each queue i, we start with arbitrary state probabilities fulﬁlling the condix πi (x, t) = 1. These state probabilities are utilized to compute the activities ηi,NB (t = 0) which are used as inputs for the ﬁrst iteration step. For instance, for an inactive system, the state probabilities are [1, 0, 0, ...], and the activities ηi,NB (t = 0) are zero. Then, for each queue and cycle, an iteration is performed which basically consists of four parts: i) using the current activity values, new service rates are computed by applying (5) and (6), ii) the arrival and service rates are utilized for generating transition matrices Ti (t) (for more details, see standard techniques for discrete-time queueing systems, e.g., [6]) according to the state model in Fig. 2, iii) multiplying the transition matrix by the current state probabilities results in new state probabilities, i.e., πi (t + 1) = Ti (t)πi (t), where πi (t) is a vector summarizing all state probabilities of queue i at time t, iv) as a last step, the state probabilities are utilized to compute the activity values for the next iteration step. So, the coupling between the queues is modeled by updating the activity values after each cycle. The development of the state probabilities πi (x, t) over time is the main result which can be used to derive sophisticated performance metrics, such as queue lengths, latencies, and blocking probabilities. For a detailed description of the relationship between πi (x, t) and the metrics aforementioned, we refer to [17]. In the remainder of this work, we compute the average queue sizes, but more advanced statistics can be derived as well. e) Evaluation: In order to demonstrate the accuracy of the queuing model proposed, we present some numerical results. The forwarding probabilities are chosen to be F = [0.3, 0.7; 0.6, 0.4]. Provisionally, we assume the arrival rates to be λ = [0.7, 0.9]. The system startup is depicted in Fig. 3(a), where the mean queue size is shown with respect to time in cycles and where the solid lines describe the results of the model proposed. In order to validate the model, a selfimplemented, discrete event simulation (DES) is performed which simulates the system event by event. A Monte Carlo simulation with 30000 runs is executed to ﬁnd the average behavior of the system. The results of the DES are depicted as dashed lines in Fig. 3(a). Comparing the analytical queuing model to the results of the DES reveals a high accuracy of the model. Only slight deviations can be noticed. Additionally, the ﬁgure shows that the system at hand needs between 200 and 400 cycles to approach stationarity, which is also an important insight into the transient behavior of the system. Moreover, the proposed model is quite ﬂexible. Besides system startup with constant arrival rates, it can be used also in scenarios in which rates and properties change continuously. Supplementary results are shown in Fig. 3(b), where we apply time-varying arrival rates according to λ1 (t) = 0.7 + sin(t/30)/9, λ2 (t) = 0.9 + 2 sin(t/70)/9. (7) Now, the previously constant arrival rates are overlaid by sine functions of different types. The sine function for λ2 has a longer period and a higher amplitude. For illustration purposes, we choose sine functions, but certainly, other arbitrary functions can be applied as well. The mean queue sizes of the two input queues closely mimic the sinusoidal behavior of the arrival rates used. Furthermore, as seen in Fig. 3(a), the queuing models match the results of the DES accurately. In summary, this example with unequal forwarding probabilities and arrival rates shows that the model proposed is capable of closely modeling any kind of parameter constellation. Moreover, the model supports scenarios with varying parameters where stationarity is rarely reached and steady-state models are inapplicable. Additionally, the results in Fig. 3(b) illustrate the coupling between input queues which is caused by output contention. Observing the transient behavior of the mean queue size of queue 1, reveals some noticeable events. In the time period around t = 825, the peak of the mean queue size reaches a value of 3.5 ﬂits which is far below other peaks (e.g., 5 ﬂits at t = 640 or t = 1020). This behavior is due to coupling between the queues. In the time period around t = 825, the other queue (queue 2) experiences a phase with low queue size. Therefore, there are fewer contention events and queue 1 does not reach the peak value expected. Similar mutual interactions can be noticed at t = 1300, where a low queue size in queue 2 signiﬁcantly inﬂuences the average queue size in queue 1. B. Models for routers with more than two inputs Now, we extend the queuing models for routers with more than two input queues. Additionally, we drop the assumption of ﬁxed priorities and include a more advanced arbitration scheme. A widely used scheme is round-robin arbitration, where one pointer per output indicates which input queue is prioritized. After utilizing a pointer, its value is incremented and it points to the next input in order to balance priorities among the inputs. In this work, we use an abstraction of round-robin arbitration, and assume that the input is chosen randomly in contention scenarios. This contention resolution also implies fairness among the input queues and seems to be easier to integrate in queuing models. In the evaluation part, we compare the queuing models to simulations with round-robin and random contention resolution. a) Queuing model and service rates: We adopt the model presented in Sec. III-A to routers with more than two input queues. The fundamental techniques are retained: i) the transient behavior of a router is computed iteratively considering changing activities and service rates due to coupling effects among input queues, ii) the arrival rates λ are known, and iii) the mutually coupled service rates are approximated by evaluating forwarding and activity probabilities over time. In contrast to Sec. III-A, we use a simpler queuing model for each queue and no longer differentiate between blocking and non-blocking states. In case of two queues and ﬁxed priorities, direct inferences about what happens between the queues can be made, e.g., if one queue is in blocking state, the other queue is certainly allowed to send in the next cycle (see Sec. III-A). Since it is difﬁcult to ﬁnd and describe such relations for more than two queues, we utilize a simpler, one-dimensional queuing model with only one row of states. Before we introduce a general expression for the computation of service rates, we deﬁne a vector o = [o1 , o2 , ..., oI ] ∈ O ⊆ RI that describes the destinations of the ﬂits at the input queues, i.e., the ﬂit processed in the server of queue 1 is destined for o1 and so forth. Later on, we iterate over all  ηj fj,oj 1 fj,oj possible variants of this vector. Additionally, we specify the scenario probability for oi = oj ∧ i (cid:54)= j, for i = j, (8) s(i, j, o) = otherwise, which gives the probability that queue j is active and intends to forward to the same output queue i aims to send to. If i = j , the activity ηj is not considered since queue i is deﬁnitely active. If queues i and j try to send to different outputs, the result is one which is a neutral element in the product applied hereinafter. The function s(·) is utilized to derive the overall service rate of queue i as 1 − (cid:88) o∈O  (cid:32) I(cid:89) (cid:124) j=1 µi = µ (cid:33) (cid:125) (cid:32) (cid:124) s(i, j, o) · 1 − (cid:123)(cid:122) scenario prob 1(cid:80)I j=1 (cid:123)(cid:122) stalling prob 1(oi=oj )  ,  (cid:33) (cid:125) (9) where µ is the normal service rate of a stand-alone queue without output contention, and 1(oi=oj ) gives one if oi = oj . The outer sum in (9) sums over all possible destination vectors o ∈ O . The term marked by scenario prob describes the probability of a certain scenario, where queue 1 sends to o1 , queue 2 sends to o2 , and so on. The probability of a scenario occurring is multiplied by the probability of stalling within this scenario, which is determined by one minus the probability of being allowed to forward a ﬂit. To summarize, the outer sum gives the total probability of the queue i being stalled. Computing one minus this probability results in the probability of being allowed to forward a ﬂit, which is the service rate. b) Iterative computation: The transient behavior is computed similar to point ‘d)’ in Sec. III-A. For each input queue and cycle, the service rate is determined, and the product of the transition and previous state probability matrices is computed. c) Alternative method of computation and complexity analysis: There is another way to compute the transient behavior. A multi-dimensional state space can be constructed where dimension i describes the number of ﬂits in queue i. By considering all possible transitions among the states and constructing a large transition matrix, the transient behavior can be computed. Unfortunately, the size of the transition matrix grows with N I , where N and I denote the maximum number of states per queue and the number of coupled queues, respectively. For large N and I , the computation becomes intractable. Therefore, we have proposed a different technique in paragraph ‘a)’ in Sec. III-B, which utilizes one transition matrix per queue and timestep. This model grows linearly both with N and I . In the numerical evaluation, we call this technique Avg. Contention. Remark: To reduce computational complexity even further, another interesting idea is applicable to scenarios where the queues are equally or similarly loaded. The performance of only one queue i is computed and then, the results are used for all queues j . In this case, the queue is coupled with itself, i.e., its own activity ηi is utilized for all other activities ηj in Fig. 4. Transient behavior of one router computed for uniform arrival rates λ and by using different approximation techniques. Fig. 5. System startup of one router with three input/output pairs. The arrival rates are chosen to be heterogeneous: λ = [0.6, 0.7, 0.8]. (8), and the complexity is approximately constant with respect to I . Subsequently, in the numerical evaluation, we call this approach One Matrix. d) Numerical examples: In the following, a numerical study of a router with three input/output pairs is given. The matrix F is of size 3 × 3. For the sake of simplicity, we assume that a ﬂit is not allowed to be forwarded to the same input/output pair, i.e., fi,i = 0, and use homogeneous forwarding probabilities, i.e., F = [0, 0.5, 0.5; 0.5, 0, 0.5; 0.5, 0.5, 0]. Therefore, the input queues all experience the same contention scenario. In this scenario, the general formula of the service rates in equation (9) (with µ = 1) reduces to 2 µ1 = 1 − f1,2 f3,2 η3 µ2 = 1 − f2,1 f3,1 η3 µ3 = 1 − f3,1 f2,1 η2 2 2 2 − f1,3 f2,3 η2 − f2,3 f1,3 η1 − f3,2 f1,2 η1 2 2 , , . In Fig. 4, the mean queue sizes are depicted over time. The transient behavior after system startup is computed by using the approaches Avg. Contention and One Matrix considering various homogeneous arrival rates λ. In addition, the results of two DES serve as references. The curves DES Rand are generated by using a random arbitration scheme, whereas a round-robin scheme is used in DES RR. The results illustrate the accuracy of the modeling techniques. First of all, it is noticeable that the two techniques Avg. Contention and One Matrix produce exactly the same results and can be used interchangeably in this scenario. This can be justiﬁed by the homogeneous forwarding probabilities and queue workloads. Moreover, the results of all approaches show that the mean queue sizes are small for low arrival rates since contention events seldom occur. For higher arrival rates, contention occurrences are more likely and queue sizes increase. As stated earlier, the queuing models assume a random arbitration scheme, which is conﬁrmed by the simulation demonstrating a match between the queuing results and DES Rand. The DES RR shows similar and only slightly different results. This illustrates that for homogeneous trafﬁc and arrival scenarios, round-robin schemes can be modeled by random arbitration techniques. After investigating the initial behavior of the system, we analyze the accuracy of the models in a toy scenario with heterogeneous conditions. We change the arrival rates λ to λ = [0.6, 0.7, 0.8]. The results in Fig. 5 show that there is no longer a perfect match between the model and the simulations. Nevertheless, the basic behavior is well represented by the models. The approach One Matrix is left out since the activities of the queues are markedly varied and the simpliﬁcation of coupling a queue with itself is not applicable. To summarize, numerical evaluation shows that the queuing models enable a detailed analysis while requiring low computational resources. In heterogeneous scenarios, the accuracy decreases but is still sufﬁcient for a rough assessment of performance. IV. CONCLU S ION S In this work, we present queuing-theoretic models for the transient analysis of output contention in Network-on-Chip routers. For the exemplary case of two competing input queues and ﬁxed priorities, we propose a state space model with blocking and non-blocking states describing different contention scenarios. Additionally, we introduce a generalized model for routers with more than two input queues. In both cases, numerical results and comparisons to discrete event simulations illustrate the accuracy of the queuing models. Furthermore, we make use of an abstraction of round-robin arbitration by selecting input queues randomly in the event of contention. Results reveal that, in case of homogeneous trafﬁc and load distributions, the round-robin arbitration can be represented by a random scheme. For heterogeneous conditions, the accuracy of the approximation decreases but still gives a valuable insight into the general performance and behavior. The queuing models presented are capable of analyzing transient behavior (even in scenarios with highly dynamic time-varying trafﬁc statistics) and are a ﬁrst step in a new research direction towards non-stationary analysis of Networkon-Chip. So far, the models have been applied only to single routers. In our future work, we will extend the router model to a network of routers enabling a transient analysis of a complete NoC, and we will analyze the models presented in more advanced and complicated scenarios incorporating realistic trafﬁc patterns. Additionally, ﬂow control mechanisms between adjacent routers are not considered yet. The models proposed should be extended by ﬂow control feedback in order to capture backpressure effects more precisely. ACKNOW L EDGM ENT This work was partly sponsored by the European Social Fund and the Free State of Saxony within the project Secure Remote EXecution (nr 100111037). "
Design trade-offs in energy efficient NoC architectures.,"This paper studies design trade-offs in energy efficient Networks-on-Chip by evaluating every network architecture that derives when we apply all possible variations of design-configuration parameters on a baseline 2D mesh. Network separation (P), concentration (C), express channels (X), flit widths (W), and virtual channels (V). Our comperative analysis selects the network architecture configuration that gives the best energy delay product (EDP) while allowing a maximum area margin of 15% over the most energy efficient configuration of the baseline.","Design Trade-offs in Energy Efﬁcient NoC Architectures Antonis Psathakis, Vassilis Papaefstathiou‡ , Manolis Katevenis∗ , and Dionisios Pnevmatikatos† FORTH-ICS - Heraklion, Crete, Greece {psathas,papaef,kateveni,pnevmati}@ics.forth.gr Abstract— This paper studies design trade-offs in energy efﬁcient Networks-on-Chip by evaluating every network architecture that derives when we apply all possible variations of design-conﬁguration parameters on a baseline 2D mesh. Network separation (P), concentration (C), express channels (X), ﬂit widths (W), and virtual channels (V). Our comperative analysis selects the network architecture conﬁguration that gives the best energy delay product (EDP) while allowing a maximum area margin of 15% over the most energy efﬁcient conﬁguration of the baseline. I . IN TRODUCT ION Existing NoC schemes address the energy inefﬁciency at network architecture level and present important ﬁndings. However, they may appear to have conﬂicting results and limited design space (P, C, X) coverage. Psathakis et al. [1] address the later issue, and explore the architectural choices (P, C, X) by employing speciﬁc ﬂit widths and virtual channel counts (W, V) to equalize the bisection bandwidth and input buffer storage. Although a ﬁxed distribution of network resources implies a fairness among the evaluated NoCs, previous studies have also shown that W, V parameters may impose signiﬁcant energy overheads when they are not properly optimized. To this end, this work examines the design space through different perspective. It combines both the architectural variations (P, C, X) and conﬁgurations (W, V) on a baseline 2D mesh and selects the network architecture conﬁguration that gives the best energy delay product (EDP) while allowing a maximum area margin of 15% over the most energy efﬁcient conﬁguration of the baseline. We assume 64 processing elements (PEs) and use synthetic workloads that exhibit diverse communication behaviors with two corner cases of control to data packet ratios. We consider a homogeneous (HOM) and two forms of heterogeneous (HET1 and HET2) schemes1 for network separation (P), concentration degree equal to 4 (C), 2-hop and 4-hop intervals for express physical channels (X), 4 ﬂit widths (W), and 4 virtual channel counts (V). I I . PRE L IM INAR I E S A typical system employs 3 message classes: request messages (read/write requests), intervention messages (forward∗Also with the CS Department, Univ. of Crete, Heraklion, Greece. †Also with the ECE Department, Tech. Univ. of Crete, Chania, Greece. ‡Currently with the CSE Department, Chalmers Univ. of Technology, Gothenburg, Sweden. Email: vaspap@chalmers.se 1 Section II describes these schemes in detail. Fig. 1: Experimental methodology for 64 PEs. ing/invalidation requests), and response messages (read/write replies). Thus, we can separate a single physical network (SPN) by: (i) two subnetworks where each carries all message classes (HOM), (ii) two subnetworks where each carries a subset of the message classes (HET1), and (iii) as many subnetworks as the total message classes (HET2). Regarding the X parameter, we pick the EPN2 scheme due to its scalability characteristics. In particular, the EPN maintains the maximum input/output router port growth constant (equal to 2) and independent of node count (O(1)). Moreover, bisection wires grow independently of node count (O(1)) and grow only by ⌊ exp 2 ⌋+ 1 times, resulting in O(exp) complexity. We use the following convention in order to refer to any of the NoC architectures: We ﬁrst place the C symbol if concentration applies, followed by the X2 or X4 symbol, if express physical channels apply, and ﬁnally HOM, HET1 or HET2, if separation applies, otherwise we place SPN. I I I . M E THODO LOGY We consider tiled chip multiprocessors (CMP) with 64 tiles and map each tile on a single PE. We assume a 150mm2 die area, 45nm ITRS Low Standby Power device technology, an on-chip voltage of 1.1 V, and 2GHz clock frequency. For router components, we assume a 3-stage input-buffered speculative router, equipped with credit based ﬂow control, a 2-port register FIFO for buffer accesses, a DOR X-Y 2 EPN [2] connects non-adjacent routers with express physical channels bypassing 3 routers using a 4-hop express interval (exp=4) on X or Y dimension. Our work also explores the exp=2 case.  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  1.1  1.2 N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H r=3 r=0.33 CX2 C X4 X2 (a) Zero-load packet latency (cycles).  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H CX2 C X4 X2 (b) Bandwidth (bits per cycle).  1.5  1.4  1.3  1.2  1.1  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0 N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H CX2 C X4 X2 (c) Energy (energy per bit).  2.6  2.4  2.2  2  1.8  1.6  1.4  1.2  1  0.8  0.6  0.4  0.2  0 N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H N P S M O H 1 T E H 2 T E H CX2 C X4 X2 (d) EDP (energy per bit × cycles per bit). Fig. 2: Performance and energy for 64 PEs. algorithm for route computation, and a tree-based multiplexor for crossbar traversals. Considering the links, we correlate the link length (l) with the side of die area (X ), the express interval (exp), the number of P E s, the concentration degree (C ), and the router side (r): l = (X × exp/pP E s/C ) − r . Whenever the l parameter exceeds a threshold, we insert a pipeline stage. We assume this threshold equal to 1.5mm, because its delay3 (∼ 330ps) meets the 2 GHz clock period (500ps). We distinguish packet types as control (128 bits) and data (640 bits), and employ the rc\d parameter to illustrate the ratio of control to data packet count. Control packets carry read/intervention requests and write replies. Data packets carry read replies and write requests. We set the rc\d parameter according to trafﬁc characterization results of previous efforts considering two corner cases: 3 data packets per control packet (rc\d=0.33) and 3 control packets per data packet (rc\d=3). We use the cycle accurate simulator Booksim2 [4] and get the required statistics for performance and energy evaluation. We estimate the energy by feeding the Orion3 [5] and our NoC model with the Booksim2 statistics. We implement HET1, HET2, and EPN schemes in Booksim2 and extend Orion3 with our custom4 link pipelining model. Regarding the fact that a NoC often operates under low loads, we derive the simulator statistics when the NoC operates at twice its zero-load latency, similar to previous efforts’ methodology [6, 7]. Our synthetic workload mix consists of four trafﬁc patterns: uniform, neighbor, bit complement, and transpose. We picked this trafﬁc mix to ensure diverse communication behaviors and avoid biasing the workload in favor of one topology. When our methodology procedure begins, we ﬁrst set the baseline area reference point by evaluating all 16 conﬁgurations (W,V) and select the one with the best EDP. Detailed steps of our methodology are depicted in Fig. 1. 3 45nm ITRS-LSTP with 30% wire delay overhead (CACTI [3]) 4Derived empirically through post-PnR simulations of piped/non-piped wires for 65, 90, 130, and 180 nm, and then projecting for 45nm. IV. R E SU LT S Our results (Fig. 2) indicate that for 64 PEs, the baseline provides the highest energy efﬁciency. Although the C-{HOM, HET2} and CX2-HET2 show comparable EDP, the baseline is still sufﬁcient. Considering the energy savings, the HET2 separation scheme combined with either concentration or express physical links of an interval equal to 2, offers 16% energy savings and the exclusive use of HET2 scheme results in 20% energy savings. We also observe the load imbalance (as control to data packet ratio decreases) of heterogeneous architectures, worsen the energy efﬁciency by ∼30%. In terms of performance, concentration combined with express physical channels of an interval equal to 2, using either the HET1 scheme or not, improves 35% the latency whereas, the baseline is sufﬁcient in order to maintain high bandwidth at low loads. ACKNOW L EDGM EN T This work was supported by the GreenVM project (#1643), co-funded by the ESF and Greek National Resources. "
FMEA-based analysis of a Network-on-Chip for mixed-critical systems.,"Network-on-Chip-based multi- and many-core architectures show high potential for use in safety-critical real-time applications, such as Flight Management Systems, considering their superior efficiency. For such use however, safety standards require proof that the architecture meets the specified security goals. This usually involves a Failure Mode and Effects Analysis (FMEA) to reveal the effects of all potential failures. Moreover, the Network-on-Chip (NoC) is a shared component and plays central role in a mixed-critical system, which must guarantee the isolation between tasks, that may have distinct criticality levels. We present an FMEA-based system-level analysis for NoCs designed for mixed-critical systems. It comprises FMEA, error effects classification regarding duration and isolation violation, and technology independent probability assessment. The analysis gives effective insight into fault-related weaknesses of the NoC, and enables considerable improvements to the NoC's resilience with minimal overhead. We apply it to a typical packet-switched NoC and present the results. Although developed for safety critical applications, the approach can be applied to improve the robustness of general systems.","FMEA-Based Analysis of a Network-on-Chip for Mixed-Critical Systems Eberle A. Rambo, Alexander Tschiene, Jonas Diemer, Leonie Ahrendts, and Rolf Ernst Institute of Computer and Network Engineering TU Braunschweig, Germany {rambo|tschiene|diemer|ernst}@ida.ing.tu-bs.de l.ahrendts@tu-bs.de Abstract—Network-on-Chip-based multi- and many-core architectures show high potential for use in safety-critical real-time applications, such as Flight Management Systems, considering their superior efﬁciency. For such use however, safety standards require proof that the architecture meets the speciﬁed security goals. This usually involves a Failure Mode and Effects Analysis (FMEA) to reveal the effects of all potential failures. Moreover, the Network-on-Chip (NoC) is a shared component and plays central role in a mixed-critical system, which must guarantee the isolation between tasks, that may have distinct criticality levels. We present an FMEA-based system-level analysis for NoCs designed for mixed-critical systems. It comprises FMEA, error effects classiﬁcation regarding duration and isolation violation, and technology independent probability assessment. The analysis gives effective insight into fault-related weaknesses of the NoC, and enables considerable improvements to the NoC’s resilience with minimal overhead. We apply it to a typical packet-switched NoC and present the results. Although developed for safety critical applications, the approach can be applied to improve the robustness of general systems. I . IN TRODUC T ION Multi- and many-core processors have been extensively adopted by the server and consumer electronics markets. Not only do they present better performance and are more efﬁcient with respect to power, area, and cost, than single-cores, but also allow the integration of multiple applications in a single chip. Now, such processors are being evaluated for embedded markets, as real-time safety-critical systems [1]. Safety-critical systems are required to be certiﬁed, proving that its processing architecture is capable of meeting the speciﬁed safety goals. The certiﬁcation processes and goals are deﬁned in standards such as the IEC 61508 [2] and the domain-speciﬁc counter-parts ISO 26262, for automotive electric/electronic systems [3], and DO-254, for airborne electronic hardware [4]. Usually, a proof requires a Failure Mode and Effects Analysis (FMEA) [5], which systematically captures all potential faults and their effects. Those must then be appropriately addressed with fault-tolerance mechanisms. A central and heavily shared component of these architectures is the interconnect, which will increasingly be implemented as a Network-on-Chip (NoC). Since now multiple applications co-exist in a safety-critical system, maintaining the isolation between critical tasks is a requirement and must be kept even in the presence of (uncorrelated) errors. The propagation of an error from one application to another is considered a failure. This work has been partially funded by the FP7 project Certiﬁcation of Real Time Applications Designed for Mixed Criticality (CERTAINTY), agreement no. 288175, and by German Research Foundation (DFG) as part of the priority program ”Dependable Embedded Systems” (SPP 1500 – spp1500.itec.kit.edu). Fault-tolerance for NoCs has been extensively researched in the last 10 years [6]. Researches have been addressing however only speciﬁc errors and, in consequence, failure modes. A holistic approach meeting certiﬁcation requirements is yet to be presented. It grants a deeper comprehension of the errors in the NoC, their occurrence, impact in the short and long terms, and enables ﬁne-tuned, fault-tolerant solutions. For instance, it is utterly conservative to consider that all unmasked errors lead to failure, as it is conventionally done, given that there are (transient) effects that can be safely tolerated. Once the NoC has been analyzed, its reliability assessed and improved with or without using fault-tolerance mechanisms, the designer can consider the need for heavier mechanisms depending on the tasks’ criticality. For instance, the loss of a value from sensor readings may be tolerated and the task just waits for the next value, while a command message to an actuator must be delivered and would require the use of retransmission protocols, e.g. end-to-end. Notice that both tasks would be executing in the same system, but using different levels of fault-tolerance. However, retransmission protocols will not work if transient errors have static effects and therefore lead to permanent failure of the network. This may happen due to Single Events in conﬁguration registers of network components such as switches. It is imperative to investigate the effect durations of Single Events as they can make a major difference in the severity of an error. On the other hand, a holistic analysis can be challenging due to the complexity of many current NoCs with respect to features (e.g. quality-of-service) and implementation (e.g. buffering). This complexity results in a very large state space which needs to be covered, and, in the case of an FMEA, where each state requires a dedicated evaluation, a large state space increases the analysis costs tremendously. Certiﬁcation costs are also an important factor in the realtime industry. Every new change in the design triggers the analysis of the impacts on the whole system again, which is very costly. Besides, one NoC architecture can fulﬁll the interconnect needs in several projects, and should not need to have each instance certiﬁed. If the NoC is analyzed and certiﬁed as a subsystem, its certiﬁcation can be reused in different instantiations across distinct projects. This holds as long as the NoC does not change and thus requires deﬁning its boundaries appropriately. The contribution of this paper is an FMEA-based systemlevel analysis for NoCs designed for real-time mixed-critical systems. The analysis provides effective insight into faultrelated weaknesses of the NoC. As a result, it enables considerable improvements to the NoC’s resilience with minimal overhead. Besides reducing the complexity of the FMEA to an acceptable level, while still capturing all potential errors, the analysis also comprehends a classiﬁcation of the uncovered errors’ effects with respect to duration and ability to compromise task isolation, and an assessment of technology independent probability of failure modes occurrence. We present a practical example for a typical packet-switched NoC designed for realtime mixed-critical systems. With the analysis, we were able to considerably mitigate the effects of errors with a slight change in the design, and to compare with the original design. Although it was developed for safety-critical applications, the approach can be used to improve the robustness of general systems. I I . R ELAT ED WORK In [6], the authors reviewed and summarized failure mechanisms, faults models, diagnose techniques, and fault-tolerance methods. They present the approaches for three communication layers: data link, network, and transport layers. Despite the number of mechanisms developed these years, they point out the need for holistic, goal oriented approaches that are better linked to the physical causes. [7] gives an overview about outstanding research problems in NoC design and also list fault-tolerance and reliability as key problems. The motivations given in [6], [7] and many related works are however the increased transient faults caused by transistor scaling and not the certiﬁcation for safety-critical systems. For off-chip networks, there is an even longer history of studies on reliability including FMEA, e.g. [8]. Some ideas can be applied to on-chip networks, but not always, since there are limiting factors, such as area, power, and timing constraints. Also, in off-chip networks, the objective of FMEA studies is usually to increase the availability of the network. We, on the other hand, aim at creating a base for a faulttolerant NoC which is compliant with safety standards, with minimal overhead. A system-level fault model was proposed in [9]. Functional effect of faults such as misrouting, ﬂit loss, and ﬂit duplication, are modeled. However, the fault model is intended to detect only circuit-level timing violations, caused by process and temperature variations, and does not cover Single Event Upsets (SEUs). An FMEA method for SoC-level design compliant with IEC 61508 was proposed in [10]. Sensible zones (e.g. memory elements such as registers, primary inputs and outputs, and critical nets such as clocks) are extracted from the RTL description and used as a base for the FMEA. The proposed methodology was applied to design memory subsystems for microcontrollers. [11] presents a SoC-level risk assessment using a SystemC TLM model. Although it seems sufﬁcient for risk assessment, it does not yield sufﬁcient insight into error propagation. Numerous studies have addressed transient faults [12]– [16]. Other works have put effort into comparing different fault-tolerant schemes. In [17] the reliability-energy tradeoff and the performance-energy trade off for different error correction schemes are studied. Speciﬁcally, these schemes are error detection with retransmission “Automatic Repeat Request” (ARQ) and “Hybrid ARQ” (HARQ), in which ARQ is combined with forward error correction. [18] and [19] concentrate on the triple performance-reliability-energy consumption trade-off, the latter focusing on router architectures. Regarding permanent faults, [20] proposes a highly resilient NoC architecture. The redundancy in the network and routers (e.g. buffers, and decoders) is used as a mean to provide robustness though reconﬁguration without the overhead inherent in N-modular redundancy-based solutions. [21] presents a fault-tolerant scheme exploring bidirectional channels between routers. Instead of detouring packets as in traditional schemes, a bidirectional channel is used in half-duplex mode when faulty channels are present. Faults have also been treated at a higher level by means of an adaptive checkpointing scheme [22]. It periodically saves each applications’ state so that a fault-free state can be loaded in case of an error. Although extensive research has been carried out on NoC fault tolerance, only certain mechanisms are presented or evaluated. Therefore one can not prove that all Single Events are covered, as required by safety standards, since a systematic assessment of all potential failure modes is required. Furthermore, task isolation is essential for mixed-critical real-time systems and reliability of the isolation provided by the NoC must be addressed. I I I . M ETHODO LOGY As a ﬁrst step, we point out that the objective of the analysis is to investigate the effects up to the network level. The effects of an error in the NoC at layers above the network, e.g. application, are not considered, since it is highly dependent on the application and the semantics of its messages transiting in the NoC. The analysis is based on the FMEA, where, basically, each system component is scrutinized independently for its effects on the system behavior, when faulty. Such an approach is justiﬁed under the assumption that component failures occur uncorrelated and seldom. A. The FMEA In the FMEA, each component instance is analyzed, for each component type [5]. Each possible failure mode of the component instance is then examined for every possible system state, evaluating local and global effects of the failure mode. A failure mode is the speciﬁc way by which a failure occurs in terms of failure of the component function under investigation. Local effects concern how the functionalities of individual sub-components or the local switch are affected. Global effects concern how the functionality of the NoC as a system is affected, i.e. error propagation. This description may be translated into an equation which allows to compute the number of cases N that have to be treated in a FMEA without further simpliﬁcations: (cid:88) j N = I (j ) · F M (j ) · S (1) j where : identiﬁcation number for each component type, 1 ≤ j ≤ number of component types : number of instances of a component type j I (j ) F M (j ) : number of failure modes of a component type j : number of system states S If applied directly, this approach leads to an explosion of cases to be evaluated, exceeding reasonable limits. To avoid this, the following techniques are applied. • Error abstraction: Instead of evaluating single-bit errors individually, the signals are grouped logically by their function which dramatically decreases the number of cases that have to be investigated in the process. This simpliﬁcation leads to conservative results because we assume that every single-bit error has an immediate effect on each output signal of the component. In the results section we will present a comparison with fault injection tests to show that this abstraction is justiﬁed. Furthermore, errors are considered equivalent if their effect on a ﬂit transmission is identical (e.g. all errors in the packet’s payload are considered equivalent). Also, only errors at the block level are considered, assuming that errors occurring within a block will either show up at its interface or are masked and hence are irrelevant. Notice, however, that we do look inside the blocks (e.g. internal state) to determine the effect of each error. • Symmetry exploitation: Each switch has multiple inputs and outputs, which behave identically (naturally, they are distinguished during packet routing). Hence, it’s not necessary to analyze each input and output path of each switch separately, the analysis of one such path is sufﬁcient. Likewise, a mesh network shows similar symmetry between switches for different paths through the network. Thus, we don’t need to consider a large network, but can construct a minimal network conﬁguration which still shows all effects of possible path segments in larger networks. For instance, considering all possible routes using XY routing in a mesh topology, a packet is forwarded at each switch in three possible ways: 1) Straight: The packet continues in the same direction as on the previous switch, i.e. a “run”. 2) Turn: The packet turns either right or left. 3) Exit: The packet goes to one of the up-links connected to the NIs. All behaviors are already shown in a mesh containing four switches, which can thus be used as a minimal conﬁguration. When the packet leaves the NI and traverses the ﬁrst switch, it can be considered as a straight forwarding since it shows all effects of a switch to switch straight forwarding. Notice that a smaller system, with only one or two switches, cannot exhibit all behaviors listed above (i.e. no turns) and is hence not suitable for the analysis. On the other hand, a larger system with longer routes will present repeated behaviors (e.g. a run through more than one switch), and will only result in more effort to produce the same results. • Worst-case effect on a test packet: A single error in the state of a switch may have a multitude of consequences depending on its current state. The state of a switch is deﬁned by the packets being transferred and the respective state of these transfers. And it would result in a tremendous state space to consider all possible combinations of transfers and their states. To avoid this, the transmission of one test packet along a selected path is considered (which is sufﬁcient due to symmetries), using different ﬂit compositions. For that packet, all possible error effects on the packet itself and on any potential background load are evaluated. For the background transfers, only the worst-case transfers and states are considered, i.e. those that actually interact with the test packet. Notice that the background trafﬁc and the test trafﬁc are symmetric, thus uncovered errors affecting the test trafﬁc do not need to be evaluated again for the background load. Another important decision is drawing the system boundary between the NoC switches and the network interfaces that are used to connect the IP blocks (e.g. processors, memory). This decision aims at modularity, since there may be different network interfaces depending on the type of IP. Analyzing the NoC as a subsystem without the NIs (instead of including it in the analysis of the complete multi-core) allows reuse of certiﬁcation [4] for different NoC instantiations and hence saves certiﬁcation costs. Regarding the network interfaces, we only: • Assume that the packets are injected fault-free in the NoC. In case they are injected faulty, the errors are a subset of the ones found in a packet transmission between switches. • Include in the analysis the state machine that handles ﬂits received from the link, in order to reveal the effects of an error in a switch (up to the network layer). This does not affect modularity since this state machine does not change between different NIs in the same NoC. They only depend on the packet formats supported by the network and are essentially the same as the state machine in the switch. The analysis is performed on the block level, therefore errors are assumed on all connections between blocks and between adjacent switches. The errors are evaluated individually for each logical signal, such as the actual ﬂit data or the synchronization between modules (e.g. valid signals). B. Effects characterization Once the possible errors are known, their impact can be analyzed and ranked. Therefore we classify the errors’ effects by their duration on the NoC, based on the error classiﬁcation presented in [23]. At ﬁrst, we started with two classes of effects, the conventional ones: Transient and Static. Later on, with modiﬁcations in the NoC, other classes of effects were detected, such as Degrading and Intermittent. • Transient: the error vanishes with the affected packet, e.g. packet payload corruption. • Degrading: the error degrades performance. It accumulates due to new occurrences and eventually manifests itself, e.g. credit counter fails does not update and credits are erroneously lower. • Intermittent: the error remains but can eventually vanish, e.g. a virtual channel was not released by a corrupt packet and will only be released by a subsequent healthy packet traversing the switch in the same direction. • Static: the error remains in a switch affecting the subsequent transmissions, e.g. virtual channel reservation is never released. Since we are analyzing a NoC for mixed-critical systems, we also want to understand how task isolation can be violated. Task isolation means that the misbehavior of a task will not impair other tasks in the system. Thus studying the effects that a faulty transmission belonging to one task has on other tasks in the NoC is crucial in order to prevent them properly. That resulted in another class of errors called Isolation Violation. • Isolation violation: the error, independent of its duration on the NoC, affects the transmission of other streams, e.g. by corrupting or losing a packet. This is not an exhaustive list of classes, as there may be speciﬁc cases in a particular architecture. Nonetheless, it serves as a starting point and guideline for the more general cases. C. Effects probabilities After classifying the errors’ effects, the designer already has a good idea about the weaknesses of the NoC’s architecture when exposed to faults. There are however failure modes that are very unlikely and others very likely to occur. One way to consider this aspect is to assess their probability of occurrence. Despite being common in other areas, such an approach has not been applied to NoCs yet. In the FMEA, we conservatively consider that any fault in a block leads to an error. As a result, the occurrence probability of an error on an output signal of a component, i.e. a failure mode, is calculated as the probability that a fault happens in that component. In this work, we avoid being bounded to a speciﬁc technology node in order to show which effects are more probable than others in a system without going into details of the underlying technology. Instead we derive the fraction of chip area that is consumed by a component from metrics that are provided by mapping reports of Xilinx FPGA synthesis tools. An approach that focuses on SEUs is to assess probabilities relative to the number of registers used by each component in the switch. Another possibility which respects Single Event Transients (SETs) was also considered: relative to the number of Look-up Tables (LUTs) of each component. IV. ANA LY S I S S ETU P The proposed methodology is applied to a typical packet switched NoC [1] presenting the common 2D mesh topology. Each switch can be connected to up to four neighbor switches and can also be connected to up to four IP blocks through network interfaces. A. The NoC The minimal network conﬁguration fulﬁlling the methodology requirements used in the analysis is shown in Figure 1. It contains 4 switches (S) connected in a 2D mesh fashion with the respective links (L). Each switch is also connected to the network interface (NI) of one tile (IP). In this context, the contents of the tile are not relevant except for the fact that it contains a NI. The test packet traverses the NoC from tile IP1 to tile IP3 through the path indicated by the arrows. Fig. 1. Minimal network conﬁguration. Fig. 2. The switch architecture Figure 2 shows a block diagram of the switch architecture, which is based on [24]. Data packets received over the links are stored in one of the input buffers, which represent distinct virtual channels (VCs). Inside the input buffers, the packets are stored in FIFOs. A credit counter manages the space in the buffers on the granularity of Flow Control Units (ﬂits), preventing over- and underﬂows. Packets are forwarded as soon as the ﬁrst ﬂit is available, and multiple ﬂits of a single packet may span multiple switches (wormhole switching). The packets are composed of one Head Flit (HF), zero or more Body Flits (BF), and one Tail Flit (TF). Packets comprised of only a HF are also allowed and are called Single Flits (SF). The HF of a packet contains the complete routing information, which is determined at the source following e.g. an XY routing scheme. The route is encoded as a series of “runs” with each run containing a direction (north, east, west, south) and the number of hops that are traversed in this direction. At each hop, the route ﬁeld is updated so that the current output port can always be found at the same bit location for fast routing decisions. The routing information in the HF is read and the routing port is identiﬁed as soon as a new packet arrives in a switch. In the sequence, a Virtual Channel Access Controller reserves access to the VC in the next downstream switch. The request may be refused if a packet from another input has already reserved the VC. When granted, the reservation is maintained until the last ﬂit of the packet (TF) has been transferred. The L I S T O F S IGNA L S BE TW EEN B LOCK S IN TH E SW I TCH TABLE I TABLE II L I ST O F S IGNA L S BE TW EEN SW I TCHE S Block Signal Description Signal Description Input Buffer VC Acc. Ctrl Credit Counter Switch Arbiter Switch Fabric Register Bank port request port request BE port request GT port accept ﬂit type request vc status update data out data out valid vc access credit available back suction input port valid input port select port grant output data output data valid slv data out mst data out priority ds thresh int thresh Request access to output port Request type (best-effort or guaranteed-service) Accept grant from switch arbiter Request or release a VC to VC Access Controller Forward data to switch fabric Grant access to VC Indicate availability of credit Indicate low buffer occupancy Control switch fabric Grant port request to input buffer Send data to downstream switch Read data to register bank port Priority of each VC Conﬁguration of QoS switch arbiter grants access to the individual VCs requesting access to an output port via the switch fabric. To avoid input buffer overﬂow, neighboring switches exchange credit points to indicate free buffer space. They are registered in each switch’s credit counter (credit based ﬂow control). The switch can handle two different trafﬁc classes: BestEffort and Guaranteed-Service, which is bandwidth. The switch arbiter manages Quality-of-Service (QoS) guarantees, using a scheme called Back Suction [25]. The conﬁguration is done via a register bank, that stores parameters such as priority for each VC and is accessed by the network interface via a dedicated port. The isolation is provided by mapping streams to different virtual channels. If necessary, streams of critical applications may be transmitted through non-overlapping routes in order to avoid contention. Once the sender NI has handed over the message in form of a (valid) network packet to the NoC, the NoC is supposed to deliver it to the recipient. The NoC’s function can be speciﬁed with the following tasks: • transfer all messages to the recipient, • maintain packet integrity, • maintain isolation (avoid corruption of other parallel data transmissions), and • ensure Quality-of-Service guarantees. The implementation of this NoC architecture presents 21 groups of signals between blocks in the switch. The signals between switches, the link, comprehend 7 groups. The signal groups, which will be used in the FMEA, and their description are presented in Tables I and II. B. Effort estimation The cost of the analysis can now be estimated using Equation 1. Notice that the techniques previously described in Section III were used to reduce the analysis complexity. There are two components: the switch (j = 1) and the link (j = 2). There are 3 instances of the component switch (I (1)) and 4 instances of the component link (I (2)). t output credit output valid a :VC :FT :Route :Supervisor :Payload u o a d t u p t Indicates new available credit (from Input Buffer) Indicates new ﬂit available (from Switch Fabric) Indicates the virtual channel (all ﬂits) Indicates the ﬂit type (all ﬂits) Route of the packet (HF/SF) Indicates whether sender is the supervisor (HF/SF) Payload of the ﬂit (all ﬂits) Within the switch, there are 21 signal groups to be analyzed (F M (1)). And between switches (link), there are 7 signal groups (F M (2)). Also, there are 3 different types of packet that have to be considered: SF , H F +T F , H F +n.BF +T F . N = [I (1) · F M (1) + I (2) · F M (2)] · S = [3 · 21 + 4 · 7] · 3 = 273 Note that N should be understood as an upper bound of the number of cases to be expected in the FMEA, as a certain reduction is still to be expected, mostly due to the regular NoC architecture and the similar behavior of instances of one system component. N also successfully serves to estimate the analysis effort for a speciﬁc system simpliﬁcation approach. V. R E SU LT S The FMEA uncovered 107 failure modes within the switch and 54 failure modes in the link between switches. Since the analysis output is an extensive textual description, and due to space limitations, we present here a small but essential extract of the FMEA. The full FMEA output can be found in [26]. Based on those results, the errors’s effects were then characterized and their probabilities calculated. A. Local effects Each error case can present one or more local effects. For instance, a ﬂit is sent to the wrong output port and the ﬂit that was supposed to be sent to that port is therefore lost. The local effects, when the error is not masked, were found to be of 5 different types, as follows: • Error masked. The subsequent block ignores it. It happens when the faulty signal is only evaluated when other signals have a speciﬁc value. • Flit corruption. The ﬂit content changes. Depending on where this error occurs inside the ﬂit, there may be different global consequences. • Flit loss. It may happen if there is an error in the communication between blocks, e.g. the input module sends a ﬂit to the switch fabric but it doesn’t get through, or the corruption of a ﬂit’s VC ID and the ﬂit is hence stored in an incorrect VC buffer. • Flit sent to wrong output port. It may be caused by an error in the control information, e.g. input port select, or an error in the communication between blocks, e.g. switch arbiter and switch fabric. • Flit transmission delayed. It may be caused by an error in the control information, e.g. VC priority, or an error in the communication between blocks, e.g. register bank and switch arbiter. Arbitration errors usually do not lead to a complete loss of functionality as the switch re-arbitrates every cycle. • VC buffer blockage. It may happen if the switch makes an incorrect decision regarding VC reservation. An allocation error is usually permanent, as a VC buffer (e.g. one that had already been allocated) is only released if a TF is processed, which will never happen as the reservation error prevents progress. B. Global effects Despite the high number of failure modes (161 in total), only 5 global effects, i.e. system failures, were identiﬁed. The global effects depend on the type and location of the local errors. One error can present one or more global effect as its consequence. For instance, an error can cause the corruption of a packet and at the same time cause the loss of a packet of the background trafﬁc. The following global effects were found: 1) Quality-of-Service violation. The VC QoS guarantee is violated. This may happen temporarily, e.g. due to an incorrect switch decision or a signal glitch, or permanently, e.g. due to corruption of the VC priority, or due to VC buffer blockage. 2) Packet loss. The packet is lost in the network. This may be caused by the loss of HF or TF, route or VC ﬁeld corruption, an incorrect decision in a switch, or VC buffer blockage. Also, the packet may be delivered to a wrong recipient. 3) Packet corruption. The packet arrives at the correct destination but is corrupt. This may happen e.g. due to a bit ﬂip in the payload of a ﬂit or the loss of a BF. 4) Return route corruption. The route ﬁeld gets corrupted without immediate effect, e.g. because it affected only the part of the route that was already traversed, causing the NI to be unable to reconstruct the return route e.g. for an acknowledgment. 5) VC buffer blockage. This may be caused by the loss of a HF or TF, or incorrect switch decision caused by corrupt control data, e.g. credit counter and VC access control. This failure often happens together with packet loss, although they may happen independently. Due to wormhole switching, this error may propagate to the downstream switches. For instance, if a packet’s TF is lost, all switches downstream will not receive a TF and hence never release the corresponding VC reservation. The effects of the corruption of the ﬂit type and virtual channel ﬁelds can also be considered as a ﬂit loss. The corruption alters the ﬂit semantics and, in the perspective of its aggregating packet, the ﬂit is lost. Nonetheless, the ﬂit still exists and it will affect the transmission of other packets. C. Effects characterization Studying the effects that a faulty transmission belonging to one stream has on other streams is crucial in order to prevent them properly. The metric used here to show the results is the percentage of cases where a global effect (1–5) presents a given characteristic. We focus initially on the conventional ones, transient and static. Keep in mind that an effect is Transient when the effect goes with the affected packet, or is 2 B1 I B I B I B I B I 3 4 5 0 20 40 60 80 100 60 40 20 0 relative number (%) rel. number (%) Static Degrading Intermittent Transient Isolation Fig. 3. Global effects characteristics: relative number. See Section V-C Static when the effect remains in the NoC affecting subsequent transmissions. Let us consider the values for baseline (B) on the left-hand side of Figure 3. Improved (I) is addressed in Section V-F. Global effect 1 usually presents transient effect duration, whereas effects 3 and 4 are always transient. The most problematic cases are the global effects 2 and 5. Effect 2 remains in the NoC in more than 57% fo the cases, and effect 5 remains in all cases. Global effects 2 and 5 are caused by the error’s local effects that effectively corrupt the state of the switch. The state of the switch is deﬁned by the packets being transferred and the state of these transfers. In the occurrence of the local effects ﬂit corruption (VC data), ﬂit loss, ﬂit sent to wrong output port, and VC buffer blockage, the state of one or more transfers don’t reﬂect the reality anymore. The right-hand side of Figure 3 shows the relative number of cases where a failure mode violates the isolation property. Notice that this characteristic is independent from the other ones, transient and static. The isolation is violated when a ﬂit deviates from its route and affects other transmissions in the NoC either by migrating to another VC or by changing its route. The local effects ﬂit corruption (VC and route data) and ﬂit sent to wrong output port are responsible for the violation. D. Effects probabilities We now assess the relative probability that a failure mode occurs and presents a global effect with a certain duration and whether it violates isolation. The probabilities assume a Single Event Model, and were calculated relative to the number of registers of each component in a switch in the NoC to avoid being bounded to a speciﬁc technology node. In an alternative calculation we derived all probabilities based on the number of LUTs as an estimate for consumed chip area. The results showed a difference smaller than 0.21% between the two approaches. The numbers were obtained using synthesis results for a Xilinx Virtex-6 FPGA. The assessed probability accounts for the direct effects of an error. The indirect effects, i.e. the probability that one switch is affected by effects propagated from errors in a different switch, are object of future work. Let us consider in Figure 4 the values for baseline (B). Notice that this Figure uses a different scale. Among the set of global effects, effects 2, 3 and 5 are the most likely to happen. This can be explained by the impact of the input buffer’s size, which uses approx. 85% of the switch registers, on the occurrence probability of an error that causes a ﬂit to 2 B1 I B I B I B I B I 3 4 5 0 5 10 15 20 25 30 8 6 4 2 0 relative probability (%) rel. probability (%) Static Degrading Intermittent Transient Isolation Fig. 4. Relative occurrence probabilities of global effects. See Section V-D get corrupted or lost. Effects 1 and 4 present only transient duration and their occurrence probability is relatively small. E. Comparison with fault injection FMEA is an analytic method and signiﬁcantly differs from simulation or test procedures. To make this approach feasible we had to ﬁnd simpliﬁcations like error abstraction which are described in Section III. By running fault injection tests on some of the routers’ components, we can quantify the conservatism of our approach. For this purpose we selected the credit counter which keeps track of the ﬂits that were sent in a guaranteed service channel, compares this number to a given threshold and signals credit level and transmission requests to subsequent routers via two output signals. This rather small unit consumes 24 register cells which, in combination with external input signals, yield a total number of 1,536 states. We discovered that an error directly effects the output signals in 47% of the cases. Masking effects inside the component were not identiﬁed. The other cases represent latent faults, which means that the error will manifest itself eventually. We also performed fault injection tests for the switch fabric component and obtained similar results. Therefore, we conclude that assuming errors in logical signals instead of each single bit register is not utterly conservative and this reasonable abstraction can be used to reduce the effort by the FMEA. F. Analysis-enabled resilience improvement The analysis revealed a large number of static effects, resulting in a very high probability of occurrence. As is, transient faults are not only causing transient effects but also static effects, which can not be avoided by using end-toend mechanisms. A detection and retransmission mechanism clearly would not be effective in the presence of a blocked VC buffer. Such cases can only be prevented without unnecessary overhead in the NoC design with knowledge from a more holistic analysis. We identiﬁed the source of static effects, caused by transient errors, as the inability of the switch to handle receiving unexpected ﬂits, i.e. to handle unexpected input. For instance, the switch does not allow any other packet to be transmitted to a reserved VC in a given output port when that combination is reserved for the remaining ﬂits of an incomplete packet. When a packet loses its TF, the VC used will not be released and the next packets will be blocked in the input buffer (IB). The other case is when a HF is lost and a BF or TF is the ﬁrst ﬂit of the packet to arrive at an IB. The switch will try to forward the ﬂit but the ﬂit does not contain routing data and a VC reservation cannot be found. As a result, the ﬂit will remain in the input buffer. The state machine responsible for the VC reservation is shown in Figure 5. The state transitions of the baseline version is represented by the solid and dashed edges. This state machine controls the second stage of the switch arbitration, where an IB requests access to one VC in a given output port. The ﬁrst stage arbitration controls the access to an output port. In the Figure, all the requests in the conditional transitions belong to the same IB, except for the rightmost transition from reserved to deny access. The ﬁrst case happens when the VC is in the state released and an IB requests access to transmit a BF or TF (bold cycle on the left-hand side). The second case happens when the VC is in the state reserved and the IB requests permission to transmit a HF or SF (bold cycle on the right-hand side). Currently, the access will always be denied in both cases and the VC becomes blocked. In order to transform those effects into non-static ones, we made two modiﬁcations to the switch’s policy. It was changed as follows: • Drop ﬂits that cannot be forwarded (i.e. when the switch receives a BF or TF in the input port where a HF or SF is expected); • Transfer the VC-reservation to the next packet entering the switch through the same input port as the packet that has the VC-reservation when the switch detects it was not properly released (i.e. when the switch receives a SF or HF in the input port where a TF or BF is expected). To be consistent, the same policy changes were also applied to the NI, instead of simply assuming the ﬁrst ﬂit received to be a HF/SF. The changes are also depicted in Figure 5, where the state transitions represented by the dashed edges were removed and the dotted ones were added. Considering the ﬁrst case now, when the VC is in the released state and, therefore, the access is denied to BFs and TFs, the respective ﬂit will also be dropped, allowing the next one in the IB to be served. In the second case, when a VC in an output port is already reserved for an IB, when a request for a HF/SF from that same IB arrives, the new packet takes over the reservation. The case where the VC is reserved and a different IB sends a request remains unchanged: the access is always denied, since no safe assumption can be made. The improved NoC was also analyzed to evaluate the impact of those changes. First, let us consider the percentage of cases where a global effect presents a given characteristic. The numbers for the improved version of the NoC (I) are shown on Figure 3. There is a major improvement, since now all effects present transient error duration, except for effects 1 and 5. The changes introduced two new classes of effect: degrading and intermittent. Nonetheless, both effects are less compromising than Static. Degrading describes an effect that degrades the performance (e.g. input buffer space is reduced), accumulates due to new occurrences and then manifests itself (e.g. violating Quality-of-Service guarantees). Intermittent means that the effect can go away (i.e. not static) but it takes longer (or is less likely to happen) than a transient one. Regarding isolation, the deny access drop ﬂit Request (HF/SF) Request (BF/TF) ) F T F / B ( t s e u q e R deny access R I e q u n e p s u t t f r B o u m f f e o r t h e r start released Request (HF/SF) allow access HF/BF reserved SF/TF Request (HF/SF) Both Baseline Improved Fig. 5. The VC Access Control state machine. percentage of occurrences decreases in all cases. Considering all cases, it decreased by 14.77 percentage points. Now let us consider the changes in the occurrence probabilities of the global effects on Figure 4. For improved (I), the reduction of the probabilities of 2, 4 and 5 increased the ones for 1 and 3. This can be explained by the fact that, since the static VC blockage (5) is not occurring anymore, fewer packets are lost (2) due to corruption and, as a result, these packets are delivered despite being corrupted (3). They also may corrupt other packets and consequently isolation violation increases. Return route corruption (4) decreases due to the improved policy in the NI’s input. And ﬁnally, Quality-ofService (1) increased both because this effect is not being masked by VC blockage anymore and also because the values are relative: since 2, 4 and 5 decreased, 1 and 3 will increase to compensate. V I . CONC LU S ION We have presented an FMEA-based analysis for Networkon-Chips designed for real-time mixed-critical systems. The methodology reduces the analysis effort to an acceptable level by exploiting various abstractions and symmetries while being conservative enough and ensuring that all potential errors are still captured. As a result, the analysis provides a full coverage of potential single event faults, as required by safety regulations standards. The analysis also yields deep insight into the failure modes and their impacts, which enables signiﬁcant improvements to the NoC’s resilience. As a consequence, the outcome of this work forms a solid starting point for the selection, design, and implementation of effective lightweight fault-tolerance solutions with lower performance overhead. In particular, we were able to identify and effectively mitigate the impact of static effects, which disable retry protocols, with a slight change to the NoC’s design. Although the analysis focuses on one microarchitecture, the methodology is not bound to it and can be used to improve NoCs’ robustness both in real-time and general systems. "
Towards compelling cases for the viability of silicon-nanophotonic technology in future manycore systems.,"Many crossbenchmarking results reported in the open literature provide optimistic expectations on the use of optical networks-on-chip (ONoCs) for high-performance and low-power on-chip communication in future manycore systems. The goal of this paper is to highlight key methodological steps for a realistic assessment of the emerging nanophotonic technology. Building on this methodology, the paper provides an accurate energy efficiency comparison between an ONoC and an ENoC counterpart both at the level of the system interconnect and of the system as a whole. As a result, the paper points out the most promising directions for the development of the technology for the sake of practical relevance, and confirms that the technology has potential based on a characterization methodology with uncommon cross-layer visibility.","Towards Compelling Cases for the Viability of Silicon-Nanophotonic Technology in Future Manycore Systems Luca Ramini1 , Herv ´e Tatenguem Fankem 1 , Alberto Ghiribaldi1 Paolo Grani 2 , Marta Ort´ın-Ob ´on 3 , Anja Boos 4 , Sandro Bartolini 2 1 University of Ferrara, Italy, 2 University of Siena, Italy 3 University of Zaragoza, Spain, 4 University of Munich, Germany 1 {luca.ramini, herve.tatenguemfankem, alberto.ghiribaldi}@unife.it,2 {grani, bartolini}@dii.unisi.it 3 ortin.marta@unizar.es, 4 anja.boos@tum.de Abstract—Many crossbenchmarking results reported in the open literature provide optimistic expectations on the use of optical networks-on-chip (ONoCs) for high-performance and lowpower on-chip communication in future manycore systems. The goal of this paper is to highlight key methodological steps for a realistic assessment of the emerging nanophotonic technology. Building on this methodology, the paper provides an accurate energy efﬁciency comparison between an ONoC and an ENoC counterpart both at the level of the system interconnect and of the system as a whole. As a result, the paper points out the most promising directions for the development of the technology for the sake of practical relevance, and conﬁrms that the technology has potential based on a characterization methodology with uncommon cross-layer visibility. IN TRODUC T ION & MOT IVAT ION S I . Optics could solve many physical problems of on-chip interconnect fabrics, including precise clock distribution, system synchronization, bandwidth and density of long interconnections, and reduction of power dissipation [1]. However, despite the arguments in favor of optics for interconnects on the silicon chip, the technology has currently industrial relevance only for chip-to-chip communication. The main reason is that most of the crossbenchmarking frameworks between ONoCs and ENoCs reported in the literature ultimately fail to make compelling cases for the viability of silicon-nanophotonic technology. In practice, they tend to deliver overly optimistic results for ONoCs while not supporting them with a corresponding depth of analysis. In fact, while the use of abstract simulation models, of optimistic technology assumptions, and of pencil-and-paper ﬂoorplanning considerations have been of help to researchers to prove the potential beneﬁts of nanophotonic interconnects, they cannot foster the next step, that is, an industrial uptake. For this, other requirements come into play, such as a true cross-layer design methodology, a more thorough optimization of electronic architectures and digital systems to take beneﬁt of a photonic medium. This paper intends to lay the foundation for an evaluation methodology of practical relevance of the emerging optical interconnect technology. The need for Golden Rules for the trustworthy crossbenchmarking of optical NoCs vs. their electrical counterparts was originally pointed out by [2]. The distinctive contribution of this paper is twofold. First, those rules are augmented based on the experience of the authors of this paper on layout- and system-aware design of optical communication fabrics. Second, the resulting evaluation methodology has been followed to come up with a realistic energy efﬁciency comparison between ONoC and ENoC at the level of both the system interconnect and the system as a whole. While pros and cons of silicon nanophotonic networks clearly come to the forefront with a more insightful analysis framework, the paper keeps painting a promising picture for the applicability of this technology. Fig. 1: Optical Network Interface Architecture. NoC concept into a viable technology of practical relevance implies that the gap between these two communities is bridged, hence leading to a cross-layer design methodology. This latter should validate abstract models with depth of analysis, and should in turn re-architect systems around higher-order photonic switching structures. In this direction, we draft an initial list of Golden Rules to come up with an accurate and relevant crossbenchmarking of optical interconnect fabrics, by extending those ﬁrstly reported in [2]. * RULE#.1: Do not provide an hardwired speciﬁcation of the logic topology with its physical implementation. * RULE#.2: Account for the Place&Route effects, which introduce unexpected waveguide crossings. * RULE#.3: Account for the optical power distribution network, and for its layout effects. * RULE#.4: Explore the space of mapping options to nanophotonic devices. * RULE#.5: Provide the design of the full network interface architecture, not just of the domain conversion circuitry. * RULE#.6: Consider an aggressive electrical baseline, not a naive one. * RULE#.7: Assume a broad range of device parameters to reﬂect a fast evolving technology. * RULE#.8: Carefully consider static power overhead, which optical technology are quite sensitive to. * RULE#.9: Keep the optical NoC simple to minimize the adoption risk. I I . FOUNDAT ION S FOR AN A S S E S SM EN T M E THODO LOGY There are two traditional communities working on different aspects of optical NoCs: technology experts, with expertise on silicon photonic devices and/or electromagnetic ﬁeld propagation, and system designers, with expertise on multi- and many-core parallel hardware platforms. Evolving the optical We now follow the methodology to gain well-grounded insights into the maturity of optical interconnection networks. I I I . TARGE T 3 -D ARCH I T EC TUR E We consider a 3D architecture with an array fabric of 16 processing tiles on the electronic plane, similar to [3]. Fig. 2: Normalized performance comparison at the system level. Fig. 4: Normalized energy comparison for the systems as a whole. proves more energy efﬁcient than the ONoC by roughly 11.6%. The major source of overhead is given by all the NIs, which consume around 35% more energy than the optical fabric itself. However, we should at this point observe that an interconnection network is typically only a small contributor to the total system energy. At the same time, the ONoC is capable of speeding up the system execution time, hence potentially leading to power-hungry systems burning power for a lower amount of time. Fig.4 shows in fact a comparison of both solutions when focusing on the energy of the system as a whole. A 15W electronic tile-based architecture has been assumed, as in [3]. The best case is achieved by ONoCaugmented systems with 4 bit parallelism, which can save more than 20% energy even with a conservative underlying optical technology. V. CONCLU S ION This paper lays the foundation for a methodology to realistically assess the maturity of optical interconnect technology. The superior performance properties of photonic switching fabrics, when properly exploited at the architectural level, enable the system as a whole to burn power for a lower amount of time. Therefore, system energy savings are within reach even with a conservative technology, in the order of 20% with respect to systems revolving around ENoCs aggressively optimized for power. The higher level of accuracy pursued by this work sets more realistic expectations for siliconnanophotonic technology in future manycore systems, while conﬁrming its potential viability. ACKNOW LEDGM ENT S This work was supported by the PHOTONICA project (RBFR08LE6V) under the FIRB 2008 program, funded by the italian government. "
STORM - A Simple Traffic-Optimized Router Microarchitecture for Networks-on-Chip.,"Networks-on-Chip (NoCs) offer a scalable means of on-chip communication for future many-core chips. This work explores NoC router microarchitectures which leverage traffic pattern biases and imbalances to reduce latency and improve throughput. It introduces STORM, a new, low-latency, fair, highth-roughput NoC router design, customized for the traffic seen in a two-dimensional mesh network employing dimension-order routing. Compared to a baseline NoC router with equivalent buffer resources, STORM offers single cycle operation and reduced cycle time (17% less than the baseline on 45nm CMOS). This design yields a higher overall network saturation throughput (13% higher than the baseline) in an 8x8 2D mesh network for uniform random traffic. STORM also reduces packet latencies under realistic workloads by 36% on average.","STORM: A Simple Trafﬁc-Optimized Router Microarchitecture for Networks-on-Chip Shalimar Rasheed, Paul V. Gratz, Srinivas Shakkottai, Jiang Hu Department of Electrical and Computer Engineering, Texas A&M University {rshalimar,pgratz,sshakkot,hujiang}@ece.tamu.edu Abstract—Networks-on-Chip (NoCs) offer a scalable means of on-chip communication for future many-core chips. This work explores NoC router microarchitectures which leverage trafﬁc pattern biases and imbalances to reduce latency and improve throughput. It introduces STORM, a new, low-latency, fair, highthroughput NoC router design, customized for the trafﬁc seen in a two-dimensional mesh network employing dimension-order routing. Compared to a baseline NoC router with equivalent buffer resources, STORM offers single cycle operation and reduced cycle time (17% less than the baseline on 45nm CMOS). This design yields a higher overall network saturation throughput (13% higher than the baseline) in an 8x8 2D mesh network for uniform random trafﬁc. STORM also reduces packet latencies under realistic workloads by 36% on average. I . IN TRODUC T ION Networks-on-Chip (NoCs) employ routers at each node to direct trafﬁc. A typical, baseline router pipeline consists of two stages [2]. The ﬁrst stage involves lookahead routing, virtual channel allocation, and speculative switch allocation, to compute a ﬂit’s output port, allocate a downstream virtual channel, and a crossbar output port, respectively. These three operations are performed in parallel. Switch allocation in the baseline NoC router involves twolevel, separable, round-robin arbitration. This scheme, while simple to implement, can result in matching inefﬁciencies that limit network throughput [2]. Other allocation schemes such as wavefront allocation [8] and its fairer variant [1], which we term wavefront+ , aim to improve network throughput, though usually at the expense of higher implementation complexity. Under realistic workloads, trafﬁc patterns in typical NoC designs are known to be both biased and highly imbalanced from node-to-node within the network. In this work, we propose STORM - a Simple Trafﬁc-Optimized Router Microarchitecture - to leverage these per-node imbalances and trafﬁc biases to lower latency and improve throughput. This work is targeted towards a 2D mesh topology using dimension-order routing, arguably among the most prevalent of NoC architectures due to its low complexity and modularity. I I . TH E STORM ROUT ER D E S IGN A primary feature of STORM is destination-biased VC partitioning to facilitate simple, yet efﬁcient matching. The concept of VC partitioning has been explored previously for instance, partitioning has been based on X-Y dimensions [5] [4]. These works, however, primarily focus on low-overhead, low-throughput networks, or employ allocation techniques which require greater complexity than the baseline router, impacting cycle time. STORM leverages trafﬁc pattern This work was supported in part by Intel Corporation. Fig. 1: Sample Microarchitecture of STORM. biases to partition VCs based on output ports; VC partitions in STORM are non-uniform across network locations and customized for location-speciﬁc trafﬁc patterns. Let V be the number of VCs per input port. We assume that V is the same for all input ports in all routers across the network. For a router at a particular node, we identify the following parameters: • i - input port index • Pi - set of output ports that can be requested by input i • p - output port index, p  Pi • Np - number of nodes that can be reached via output p • dip - number of VCs at i which request output port p Our goal is to ﬁnd dip for all i and p, for the router at a speciﬁc node in the network. We assume that all possible ﬁnal destination nodes for any ﬂit at any input port are equally likely - a characteristic of a uniformly random trafﬁc pattern. Thus, splitting V input VCs at each input port is estimated by (cid:33) (cid:25) ∗ V , ∀ p  Pi , ∀ i (1) (cid:24)(cid:32) pPi Np(cid:80) Np dip ≥ 1, ∀ p  Pi , ∀ i dip = V , ∀ i (cid:88) dip = subject to and pPi Figure 1 shows a sample microarchitecture of our proposed STORM router, for 5 VCs per port, with biased VC partitioning. The set of VCs across all input ports that are assigned to the same output port form a path-set. These per-output pathsets result in simpliﬁed, efﬁcient VC and switch allocation. VC allocation uses two-level round-robin arbitration, with the ﬁrst level picking one VC out of each path-set, and the second Fig. 2: Timing-level load-latency, 6VCs per port. Fig. 3: Scaling of throughput with increasing VC count. level picking one downstream VC for the winner from the ﬁrst level. Switch allocation involves a single level of roundrobin arbitration for each path-set. Since each path-set has a unique, independent arbiter, there is no contention between different path-sets for the same output port, and thus network throughput is enhanced. I I I . EVA LUAT ION Network-level simulations and RTL synthesis are performed to evaluate the baseline router, the wavefront+ router and STORM designs. We examine three variants of STORM: • STORM-2: a 2-stage pipeline with a dedicated switch traversal stage. • STORM-1: a single stage router pipeline. • STORM-1S: STORM-1 utilizing a single STORM router microarchitecture used across the entire network. STORM-1S is a simpler implementation of STORM, in which we perform trafﬁc-optimal VC splitting for a single central node in the network, and use this same microarchitecture at all the other nodes. Simulations are carried out on an 8x8 2D mesh network with dimension-order (XY) routing, using the network simulator Ocin Tsim [7]. We simulate the designs with 6 4-ﬂit deep VCs per port. For uniform random trafﬁc, after a warm-up phase of 10000 cycles, the runs last until 1 million 4-ﬂit packets have been routed. For RTL synthesis, we use an open-source, parameterized Verilog RTL model for the baseline router [1], which also includes the description of a wavefront+ switch allocator. This RTL was modiﬁed to evaluate the STORM design. We synthesize the baseline, wavefront+ and STORM designs using TSMC’s TCBN45GS 45nm CMOS library with an operating voltage of 0.9 V. For routers using 6 VCs per port, the minimum clock periods for the baseline, wavefront, STORM-2 and STORM-1 designs are 0.69, 0.80, 0.50 and 0.57 nanoseconds, respectively. Figure 2 shows load-latency curves for the router designs for uniform-random trafﬁc; in the case of STORM, it is assumed that the entire network is clocked at the speed of its slowest router. STORM-1 offers a 41% reduction in zero-load latency and 13% increase in saturation throughput, relative to the baseline. We also analyze the variation in saturation throughput for the router designs at the cycle-level, as the number of VCs per port varies from 4 to 16 (Figure 3). To serve as performance bounds, a router using maximum bipartite matching for switch allocation, and an unrestricted router [6] are also simulated. We see that STORM-1 consistently outperforms the baseline Fig. 4: Average ﬂit latency for PARSEC traces on 8x8 mesh and wavefront+ designs in terms of throughput for VC counts of 5 and greater. STORM-1S, where the router microarchitecture is held constant over the entire network, while not the most optimal design in terms of throughput, still betters the performance of both the baseline and wavefront+ routers. We execute PARSEC benchmark traces using the Netrace [3] library integrated with Ocin Tsim to evaluate the average ﬂit latency offered by the baseline, wavefront+ and proposed designs on these realistic workloads. The PARSEC traces were simulated with six 5-ﬂit deep VCs per input port. Figure 4 shows the average ﬂit latency in nanoseconds for PARSEC traces while employing the different router designs. STORM-1 provides signiﬁcant improvement in terms of latency, being, on average, 36% faster than the baseline router. "
Effective abstraction for response proof of communication fabrics.,We present a satisfiability backbone-based formulation for ranking structure discovery and thereby present an alternative scalable proof technique for the response properties. Our algorithm offers enhanced automation by reducing the need for user supplied input information compared to the known technique for ranking structure discovery [1]. We demonstrate that backbone based response verification algorithm scales up or attains comparable scalability without user supplied safety invariants.,"Effective Abstraction for Response Proof of Communication Fabrics Sayak Ray Dept. of Electrical Engineering Princeton University, USA sayakr@princeton.edu Sharad Malik Dept. of Electrical Engineering Princeton University, USA sharad@princeton.edu I . IN TRODUC T ION Formal veriﬁcation of liveness properties is a critical step in the design ﬂow of communication fabrics as these microarchitectural modules are prone to liveness bugs like deadlock and livelock. Liveness properties, however, are challenging to prove for automated techniques such as model checking. This makes system-speciﬁc veriﬁcation heuristics and designers’ insights about systems critical for scaling liveness veriﬁcation algorithms to industrially relevant large designs. Recently, one such novel insight about the operations of communication fabrics, viz. ranking structures, has been proposed [1]. It has been shown that correct operation of a communication fabric can be arranged in a monotone sequence of ‘fairness events’ which gives rise to a sequence of ‘ranked regions’ in its state space and this can lead to a scalable proof of the fabric’s ‘responsiveness’. However, efﬁcient algorithmic discovery of the ranking structure from a structural description of a communication fabric is still an open question. Towards answering this question, we present an algorithm based on iterative discovery of satisﬁability backbones that efﬁciently mines the ranking structure from a bit-level netlist representation of a communication fabric. To the best of our knowledge, our work is the ﬁrst in applying satisﬁability backbone analysis technique in ranking structure abstraction as well as in liveness veriﬁcation. In this extended abstract, we present the core notion of backbone-based formulation of ranking structure and our experimental observations. In our experiments, we used our algorithm to prove response property of a collection of Executable Micro-Architecture Speciﬁcation (XMAS) [2] benchmarks. Both the XMAS formalism and the response property are described brieﬂy below. Section II then presents our backbone-based formulation and the resulting mining algorithm. Further details are available in a longer version of the paper [3]. Communication fabrics are typically constructed using structural primitives like ﬁnite FIFO buffers, sources and sinks of ﬂits 1 , function blocks acting on ﬂits, decision primitives like switches and arbiters, and synchronization primitives like forks and joins. XMAS is an effective formalism for modeling communication fabrics and it provides precise formal deﬁnitions for a selected set of such structural primitives [2]. A suitable compiler can translate a high-level description of an XMAS fabric into a synchronous sequential Boolean circuit that works under single global clock. The experiments reported in Table I are performed on a collection of such XMAS fabrics taken from the literature [2], [5], [6]. Our benchmark suite consists of a basic virtual channel (V C ), virtual channels with channel buffers (V CB2 , V CB4 ) and a virtual channel with ordering logic (V CO). Informally, the response property speciﬁes that any data transfer request will be eventually granted by a fabric under appropriate fairness assumptions. Suppose our target XMAS fabric has channels 2 a, i1 , . . . , in . Our objective is to prove that channel a satisﬁes the response property under the strong fairness assumptions [1] on channels i1 , . . . , in . Formally, this property is described by the following linear logic (LTL) formula: f airness ⇒ response(a) temporal n(cid:94) where f airness := GF(ik .gnt) and response(a) := k=0 G(a.req ⇒ F(a.gnt)). G and F are standard LTL operators representing temporal modalities ‘always’ and ‘eventually’ respectively. Response property is a well-known liveness property and is among the most-frequently model-checked liveness properties for communication fabrics. It is directly related to deadlock freedom of the underlying communication fabric. While in theory we can invoke any LTL liveness model checker to prove this property, but as argued in [1], reasoning with ranking structures can expedite this proof process substantially. The technique presented in this paper is a proof technique. It manages to discover a ranking structure if one exists in the given fabric. If the fabric has any response bug, it cannot have a ranking structure in its state space. In that case, our algorithm will not be able to predict the absence of ranking structure and will potentially go into an inﬁnite loop. We note that there are scalable techniques, mostly based on bounded model checking (BMC), to ﬁnd response bugs in incorrect designs. While BMC offers scalability for discovering bugs, it cannot prove a property. In order to develop a complete veriﬁcation procedure, our proof technique can be interleaved 1A ﬂit (ﬂow-control unit) is a unit of data transfer in communication fabrics, see [4] for details. 2Here channel refers to a bundle of signals that XMAS formalism uses to connect two structural primitives. with any such BMC based bug discovery technique, as done in other state-of-the-art tools [7]. I I . RANK ING S TRUC TUR E THROUGH BACKBON E S For a design satisfying the response property, its state space can be divided into regions that form a directed acyclic graph. These regions are called ranking regions. In general, a ranking region is deﬁned as a Boolean formula σ over a set of predicates deﬁned over design signals. As observed in [1], deﬁnition of a ranking region may involve determining predicates that are not present in the design netlist. However, our close scrutiny of the XMAS library reveals that some XMAS primitive signals present in the synthesized fabrics can often be used (directly) to deﬁne ranking regions instead of using externally introduced predicates. Therefore, while [1] argued that user should add additional predicates to the design as the basis for disjunctive stabilizing constraint discovery, we claim that an adequate basis set of signals is already present in most of the fabrics and the ranking regions can be represented as conjunctions of some of them. Toward this goal, we observe that if a ranking region σ is represented as ∧k i=1 li where l1 , . . . , lk are fabric signals with appropriate polarity, ∧k essentially behaves as a satisﬁability backbone if evaluated on any concrete state in σ . 3 In order to discover backbone-based descriptions of the predicates σi ’s, we invoke safety model checking and backbone analysis in an iterative and interleaved manner. A model checker is invoked to sample an adequate number of candidate concrete states from a region σi for i ≥ 1. Then backbone analysis is performed on those concrete states and the fabric signals to produce a backbone-based description for σi . With the description of σ1 , σ2 , . . . , σi thus learned, we repeat the rounds of model checking and backbone analysis to learn σi+1 . We stop when we exhaustively partition the state space into a ﬁnite number of σi ’s. This termination criterion makes our algorithm complete modulo the existence of a linear sequence of backbone-based predicates σi ’s. We leverage the dual capability of contemporary safety model checkers like IC3 [8] that they can generate counter-examples to a safety property if the property is violated as well as can prove that no counter-example exists if the property is satisﬁed. We use the counter-example generation capability to sample candidate concrete states from σi ’s and the proof capability to establish that some region σi has been sampled exhaustively before exploring σi+1 . i=1 li I I I . EX PER IM EN TA L R E SU LT S ABC [9] is used as the veriﬁcation platform in our experiments. Our backbone-based algorithm is implemented in ABC as command bbd and its performance is compared against ABC’s implementation of response veriﬁcation algorithm based on ranking structure discovery through disjunctive 3A satisﬁability backbone, which is a well-known concept in the realm of Boolean satisﬁability, is deﬁned as follows: given a satisﬁable formula F over a set of Boolean variables V , a variable xi ∈ V is a backbone variable of F if either A(xi ) = 1 for all A |= F or A(xi ) = 0 for all A |= F , i.e., if either F ⇒ xi or F ⇒ ¯xi holds. stabilizing constraint mining (available as command kcs). Our main experimental result is summarized in Table I. The experiments are performed on a laptop with 4 Intel(R) Core(TM) i5-3320M 2.6GHz cores (each core having 3MB cache) and 3.6GB RAM that runs the Ubuntu 12.04LTS operating system. Run-times are reported in seconds. Since the correct depth of disjunction in command kcs is not known a priori, experiments are performed with depth 2 (column 4) and depth 3 (column 5). Runtime of command bbd is broken down into two parts: backbone mining (column 6) and reachability analysis (column 7). As evident from the overall runtime of bbd (Column 8), bbd outperforms kcs in most of the cases. D V C V CB2 V CB4 V CO PI 18 18 18 20 f/f 52 68 92 69 kcs -C (2) 0.43 132.78 TO 5.96 kcs -C (3) 1.35 1421.25 TO 8.72 TABLE I bb mining 0.18 11.91 8.83 8.51 bbd verif 0.02 1.02 43.23 2.65 total 0.2 12.93 52.06 11.16 COM PAR I SON O F RUN - T IM E BE TW EEN TH E PRO PO S ED A LGOR I THM AND TH E BA SE - L INE A LGOR I THM IV. CONC LU S ION We present a satisﬁability backbone-based formulation for ranking structure discovery and thereby present an alternative scalable proof technique for the response properties. Our algorithm offers enhanced automation by reducing the need for user supplied input information compared to the known technique for ranking structure discovery [1]. We demonstrate that backbone based response veriﬁcation algorithm scales up or attains comparable scalability without user supplied safety invariants. V. ACKNOW L EDG EM EN T This work was supported in part by C-FAR, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA. Sayak Ray thanks Robert K. Brayton and Alan Mishchenko for many helpful discussions. "
Using packet information for efficient communication in NoCs.,"Multithreaded workloads like SPLASH2 and PARSEC generate heavy network traffic. This traffic consists of different packets injected by various nodes at various points of time. A packet consists of three essential components viz., source, destination and data. In this work, we study the opportunity to increase the efficiency of the underlying network. We come up with novel methods to share various components of the packets present in a router at any time. We provide an analysis of the performance gains over contemporary optimization techniques. We conduct experiments on a 64 node setup as well as a 512 node setup and record the energy consumption, IPC gains, network latency and throughputs. We show that our technique outperforms the contemporary Hamiltonian routing by 8.43% and VCTM routing by 7.7% on an average, in terms of IPC speedup.","Using Packet Information for Efﬁcient Communication in NoCs Prasanna Venkatesh Rengasamy PACE Lab, IIT Madras, Chennai, India 600036 Email: pras@cse.iitm.ac.in Madhu Mutyam PACE Lab, IIT Madras, Chennai, India 600036 Email: madhu@cse.iitm.ac.in Abstract—Multithreaded workloads like SPLASH2 and PARSEC generate heavy network trafﬁc. This trafﬁc consists of different packets injected by various nodes at various points of time. A packet consists of three essential components viz., source, destination and data. In this work, we study the opportunity to increase the efﬁciency of the underlying network. We come up with novel methods to share various components of the packets present in a router at any time. We provide an analysis of the performance gains over contemporary optimization techniques. We conduct experiments on a 64 node setup as well as a 512 node setup and record the energy consumption, IPC gains, network latency and throughputs. We show that our technique outperforms the contemporary Hamiltonian routing by 8.43% and VCTM routing by 7.7% on an average, in terms of IPC speedup. IN TRODUC T ION I . Chip multiprocessor (CMP) designs support parallel execution of more than one application. As these applications try to co-exist in a chip during their execution time, performance limitation occurs from places where parallelism gets limited. This is studied by Amdahl’ s law [1]. In a CMP, there are a lot of inherent sharing. For example, independent processes share resources like shared cache blocks, network connecting distributed cache banks with the cores and the memory system. These shared resources hinder the performance of parallel applications. Also, sharing of data at the cache level causes a lot of coherence trafﬁc to get produced. We can see in Sections III and VI that when the number of cores in a CMP becomes large, performance of a parallel application becomes inﬂuenced by the efﬁciency of shared resources. We provide key optimization at the network-on-chip (NoC) level to gain better performance over existing techniques. We start by listing the different parts of an NoC packet, that we will use in this paper for optimizing trafﬁc ﬂow. • • • Source, Data: Multicast optimization via a dynamic common path. Destination: Packet concatenation for saving routecomputation (RC) time and virtual channel (VC) space. Address1 : Arresting a read request and replying if the router VC already has the response data. Our methods read one or more of the items above from the packets in router’s buffers to achieve better network performance. 1 This is a special case of data sharing I I . BACKGROUND The past works in literature are examined in three categories of NoCs in this section. NoCs [2] manage communication between various components in a chip. Congestion control algorithms aim at providing better bandwidth in heavytrafﬁc scenarios. Most of such works treat a packet as a black box. Our work aims to provide better bandwidth by understanding what a packet transports. So, combining both congestion control and packet-peeping would result in better network performance. Some initial works on congestion control include Odd Even routing [3] and West-ﬁrst routing [4]. These techniques get the framework ready for exploring optimized congestion control methods in NoCs. Both these methods give the idea of choosing one out of many available productive paths for a packet. Adaptive techniques learn and choose the best path for each packet from the available list of paths. DyAd routing [5] switches between static and dynamic route computation methods based on the occupation levels of input queues. Thus, this method marks the low and high injection phases for a network. But, it does not do much good when it has to decide on options between the available set of adaptive routes. RCA [6] uses both crossbar demand and VC occupation levels for its congestion metric to choose a link from a set of adaptive routes. It then communicates the congestion metric with other routers in every region. It has good performance gains with some multi-threaded benchmarks too. DAR [7] is an advancement over RCA where each router estimates and correct its routes dynamically. Each router maintains some book-keeping to update the recent paths taken by ﬂits in its buffers. This router design is complex and it scales better with network size when compared to RCA-QUAD, the best performing RCA technique. BARP [8] uses scout ﬂits to warn sources of a congested link. It has no turn restrictions and so has better path redundancy. It avoids deadlocks by bifurcating the network into XY+ and XY- networks. Thus this network model is much simpler when it comes to route computation. The only overhead for this technique is the scout ﬂit traversals. It suffers a set back in performance when the network congestion blocks the scouts from reaching the sources to take timely decisions. HPRA [9] uses dedicated nodes in every region of a CMP for predicting the trafﬁc ﬂow. It uses Artiﬁcial Neural Networks (ANNs) in each region to compute ”hotspots” and warn the nodes in the region. Thus the effectiveness of this technique is dependent on the versatility of the ofﬂine learning for ANNs. GCA [10] optimizes the communication of congestion values by packing them up into the empty bit ﬁelds in the data packets. Unlike the previous techniques, this technique does not need extra communication. It stores the communicated congestion values in every router using minimal storage bits. This is the latest technique for congestion control and it outperforms RCA. A. Congestion Control in Three Dimensional NoCs Path redundancy improves with improvement in router degree. Thus 3D NoCs introduced better path redundancy in networks from the advent of Through Silicon Vias [11] and a TDMA bus on Z-direction [12] to full ﬂedged routers like the MIRA router [13], DimDe router [14] and Roce-Bush [15]. These network implementations enabled a variety of routing and optimization techniques to achieve better performance. AFRA [16] achieves a fault tolerant deadlock free routing in 3D NoCs. Every router stores the nearest Z-link and a backup Z-link for fault tolerance. It follows dimension order routing when the destination is in the same XY plane of the current router. Elevator First [17] routes the trafﬁc through a deadlock free algorithm on a partially connected Z network in a 3D NoC. This is similar to AFRA but it achieves Z-traversals by masking the header with a new header to go to the nearest node linking the destination XY plane. There are recent techniques to reduce congestion by handling multicasts efﬁciently too. B. Multicast Routing In any typical CMP, multiple private caches are attached to a (group of) processor core(s). They have the possibility of holding a copy of the same block of data at any given time. These copies get validated for correctness by communicating updates through the interconnection network. Such update messages are often seen as a critical factor for performance. As the number of sharers increases for a given data block, every update to the shared data may cause a multicast to the sharers in the network. Also, there are directory based protocols that generalize these multicasts as broadcasts for simplicity. Network takes these multicasts/broadcasts as multiple single packet injections of the same data. So, techniques like EVCT [18], VCTM [19] and Hamiltonian [20] provide a static path construction technique. The aim of this path construction is to make a single injection into the network. This single packet moves along a common path viable for all the multicast destinations. When the packet reaches a point where it is not possible anymore to hop to a common router, the packet forks. This forking again is optimized to happen during link traversals. Thus, the above techniques avoid multiple copies from ﬂooding the network. This whole process of a single packet propagating to all destinations create a tree-like path. This tree gets ﬁxed for every group of multicast destinations. Thus this deterministic nature provides less path diversity in case of congestion. All these trees are maintained in small CAM tables at every router. These techniques save route computations by having static lookup tables for destination sets. Adaptive routing for multicast tree management have been proposed in XHiNoC [21]. It also uses a lookup table like VCTM to store routes computed. These techniques optimize route computations using lookup tables. But to ﬁll the tables, a control ﬂit is sent apriori for each multicast packet destination (a) Mean and Max Sharers Per Multicast (b) Existence of Max Sharers Fig. 1: Motivation: Sharing in 32 Threaded Applications for route discovery. Thus for bigger networks as well as large destination sets based multicasts (refer Fig. 1a), it becomes expensive to maintain bigger tables and to send scout ﬂits. C. Cache Coherence at NoC In the previous section, we studied techniques that treated multicasts differently from unicasts. To know whether a request is unicast or multicast, they peek into the packet for its designated bit. We could also peek into the packet data further to gain better knowledge on the system behavior. Thus there are techniques that use cache coherence information to optimize network performance. INSO [22] provides snoop ordering from NoC level to extend snoop-based protocol for topologies other than bus. This reduces broadcasts and so utilizes the available bandwidth efﬁciently for transmitting useful information. Helia [23] provides better power management by exploiting NoC-cache combination to reduce tag-mismatches. There are further areas available for exploration at the data sharing level, since multicast optimization motivates us to exploit sharing. Thus in Sections III and IV, we will see novel usage of packet data to improve performance. I I I . MOT IVAT ION A. General Congestion Control We study and come up with the following arguments to emphasize on the reasons behind the optimizations proposed. Communication in SPLASH2 [24] and PARSEC [25] has been analysed in depth in [26] . This analysis forms the basis for establishing the motivation of our optimizations. The following observations are from the analysis. 3) 1) Multithreaded applications communicate shared data. 2) The authors analyze only the producer-consumer type sharing as shared communication in the paper. Shared writes constitute up to 55% of the total writes and shared reads constitute to 9% of the total reads. Temporal locality among these producer-consumer patterns show that communications occur in heavy bursts during synchronization phases, but light and random otherwise. 4) Also, we study the sharers for any packet in its lifetime in the network as conﬁgured in Section V and get the pattern as shown in Fig. 1. Fig. 1a shows the mean number of sharers in multicast messages sent by the high injection benchmarks. The mean ranges from 2.2 for Barnes and 3.4 for Radix. From point number 3 in the above observation, we can see that up to 9% of the total reads constitute shared read. Thus if we are using techniques like EVC-T, VCTM or Hamiltonian, static route assignment for speciﬁc multicast destination sets may under-utilize the network. In addition, Fig. 1a indicate that there can be up to 28 sharers in a 32 threaded application (87.5%). Thus, sending scout ﬂits for route-discovery to assign destination sets with output ports like VCTM and XHiNoC will also become undesirable. The only way for the network would be to pick the least congested port for every destination in a multicast at each router. This means that there may be up to 0.875 × N umT hreads route computations at every router. This will make the route computation a bottleneck. VCTM and Hamiltonian techniques route a multicast packet along a ﬁxed path. We show adaptive route for multicast packets perform better than them. But, doing so may create a bottleneck at the RC phase of the pipeline. So, to reduce the delay of computing routes for all multicast destinations, we propose redundant RC units. Fig 1a shows that the number of sharers can reach up to 87.5% [28 out of 32] of the number of threads. We study the percentage of occurrence of such multicasts in the execution phase and populate the results in Fig. 1b. It shows that such maximums occur only for 3.2% on an average across the benchmarks under consideration. To decide on the number of RC units to use, we use the mean sharers per multicast packet. This value is 2.3, and so it is enough to have two RC units in place of one. This reduces most of the stalls caused by computing route for each destination at every hop. This approach of dynamic RC for every destination in multicasts may still starve other packets in the router. Our aim is to reuse the RC output of a ﬂit to other ﬂits going to the same destination. This technique is a variation from Packet Chaining [27] where switch allocator (SA) memoizes the output for faster allocation. Here, we propose a destination lookup circuitry, that concatenates packets with same destination. These concatenated packets traverse as a single ”SuperPacket” till the destination. Thus we save some RC time on unicast packets by allowing greater ﬂexibility on multicast packets path. Such packet concatenations are usually done for the request packets. They are just one ﬂit long and a VC of more than one buffers is allocated for storing the request. The reasoning behind the coincidental arrival can be better explained by the point number 4 in the above study. We see that a producer-consumer pattern occur during synchronization phases. At these phases the cores make a lot of requests to the same block of data. These requests arrive at the same L2-bank. This causes the L2-bank to fetch the block and multicast it back to the requesting nodes. These phases provide three different optimization opportunities for us. • • There are different single-ﬂit request packets traversing towards the same destination. So, if there is a bottleneck due to an occupied RC phase as described above, packet concatenation can help. Assume that a request for a block is on its way to reach the L2 bank at time ’t’. And the L2 bank has sent the same block of data to some other node at time t - ∆t. Now, if the request and response happen (a) VC as Cache (b) Packet Concatenation Fig. 2: Modiﬁcations to Router Microarchitecture • to meet in any router on their ways, we can arrest the request and send the reply back. We should note that the request still has to reach the home L2 bank for coherence. But L2 need not respond with a reply unless there is an update from its previous response. Synchronization phases with high-injection to the network tend to wane out and occur in bursts of time. So, there is little to gain on top a request-limited network (phases with few injections). In these phases, we send the requested word along the header of the ﬁve-ﬂit response packet. This will enhance the response from the receiving core to get the further requests inject quicker. This improves our opportunity to get better network utilization. IV. A P PROACH Though we propose three optimization techniques, we do not modify much at the base router microarchitecture level. First, we classify a packet as unicast or multicast packet using a separate control bit in the header. A packet may be a request to read or write a cache block, a cache block itself or a coherence message. For simplicity, we consider them to be a single ﬂit packet or a ﬁve ﬂit packet at the router level. The components of a packet at the network layer are header, body and tail. A. Addressing: Bit ﬁeld and Subnet mapping For a single ﬂit packet, the single ﬂit itself will serve as all the components. But, a ﬁve ﬂit packet has 64 bytes of data and another 16 bytes of header. As mentioned in GCA, this 16 bytes in header is used for communicating the source node-id, destination node-id(s), the memory address in transmission, along with 80 bits of optional control information. We use these 80 bits and the 4 bits for destination-id to denote multicast destination address. Thus this scheme supports till 84 nodes in a network. We also use the same bit ﬁeld based destination encoding to denote a group of nodes in a network with size more than 84 too. In this paper, we consider one bit to denote one 2 × 2 × 2 cube for a 512 node network. We can optimize the addressing further to associate the bit ﬁelds to various groups of nodes like the groups discovered in VCTM and XHiNoC. This is left as an extension in the future to this work. B. Critical Word First In this optimization, for all unicast packets, we put the word (8 bytes) requested in the header ﬂit itself. We send the word to the requesting core once the header ﬂit arrives at the destination to speedup the execution process. This is a simple rearrangement of the block data, and we assume it to happen at the cache-controller before injection. C. VC As Cache For this technique we have a tag+source-comparator at each input as shown in Fig. 2a. It checks the tag and destination for an incoming read request with those of every multicast responses staying in the VCs. If there is a match, it marks a special bit on the read request to ask the receiver to not send any reply back along with the current timestamp. This does not prevent us from forwarding the request till the destination because of the need of the home L2 bank to know the current sharers. It also sets the requestors bit to one in the multicast address of the response, to make the request get a quicker response. This also enhances the network utilization as discussed in Section VI. This step occurs when such a request is waiting in its VC for RC, SA and link traversal (LT) stages of the pipeline. Thus it does not consume any critical path delay. A read request can also miss this optimization if there are a lot of read requests in the VCs. We simulated to check only till 5 responses per request. This is to meet the clock cycle demands as discussed in Section VI-B. D. Dynamic Multicast Tree A multicast packet contains the destinations encoded in the header as bit ﬁelds. To build a multicast tree, we compute minimal odd-even route [3] for each destination and decide to fork a packet or not. In a 3D NoC, a minimal route computation at any router will give utmost three productive output ports. So, ﬁnding the common output port for all destinations is a simple task of mapping each destination to an output port. But, it takes longer time to compute routes for each destination. We propose two solutions to handle this bottleneck. 1) 2) Add redundant RC units. Though this appears to be an expensive solution, the odd even algorithm has a lot of slack time and is simple in comparison to the implementation of the allocators. We are able to accommodate till three RC units without incurring any visible cycle delay. The next solution can be used in combination with redundant RC units too. This technique is “Packet Concatenation” and is explained below. E. Packet Concatenation We start concatenating packets when we have a request packet to inject into a router and there is no free VCs in it. In that case we compare the destination of the request to inject with the requests in the VCs. If there is a match, and if the VC has a free buffer we concatenate the packets. For example, consider the following scenario. The RC units of a cache bank are busy with a multicast-reply packet and there are no free VCs available at an input channel. To handle the cache miss, the cache bank has to access the appropriate memory controller with a read request. Let us assume that one of the VCs already holds a memory read request to the memory controller. If there are more such read requests, we can concatenate them to form a longer chain of ﬂits in the same VC. As the size of a request is one ﬂit, this case is possible. This implementation continues as an extension to the VC as cache method we saw earlier. The controller at the input level compares the source address bits for request packets in the previous optimization. We add a similar controller to match destination [Refer Fig. 2b]. Similar to the VC as cache comparator, we limit only 5 comparisons for maintaining cycle time limit. Thus by utilizing the otherwise idle buffers, we also reduce the number of route computations required by unicast packets going to the same destination. V. EX PER IM EN TA L S ETU P The simulation setup consists of the following phases: A. Benchmark Execution We use Multi2sim4.0.1 [28] simulator for benchmark execution. We use Booksim2.0 [29] for our NoC level simulations. We modify Multi2sim to let an instance of Booksim to mange its network modelling. Booksim services the network requests and intimates the Multi2sim once the service is complete. We also modify the FastForward setup in Multi2sim to warm up the cache during the FastForward. This is to start sharing of the same data by different cores. We then executed the multithreaded benchmarks from SPLASH2 and PARSEC workloads using this setup. We conﬁgure the simulators with the parameters given in TABLE I. 1) Architecture for Executing Benchmark: We take a 64 − node system-on-chip (SoC) consisting of 32 − cores with private L1 − caches and 32 L2 − cache banks. Then, we arrange it on a 3D NoC as a chess board like structure [30]. This paper discusses in detail about the thermal beneﬁts with such an arrangement. We use this to demonstrate our technique. We ﬁx this arrangement for all the techniques compared. We could also adopt any other topology or NoC conﬁguration as per the need and implement our optimizations on top of them without any modiﬁcations to them. As we target multicast trafﬁc, we use L1 - L2 - Memory as our cache hierarchy. The advantage we get is the density of trafﬁc that ﬂows through this network is higher when compared to more levels of cache. This hierarchy is still practical as we have contemporary examples of this hierarchy in SPARC Processor [31] and Intel Xeon Phi Coprocessor. Vantage [32] validates cache partitioning with banked L2 cache. McPAT [33] calls this banked L2 cache shared among CPU cores as ”future high throughput computing”. We used the cycle accurate full system simulator to run all the SPLASH2 and PARSEC workloads for 100K cycles after an initial warm-up phase for removing cold cache misses. This warm-up phase is decided empirically after observing the TABLE I: Conﬁguration used in our simulations. Parameter Number of Cores Architecture simulation Value Description 32/256 We use 256 nodes to demonstrate scaling 2.5GHz Clock Speed Number of L2 Banks - Core Properties Cache Coherence Protocol MOESI Cache Replacement Policy LRU Number of L1 Caches 32/256 One per core - not shared, - L1 Properties 2 way set associative, 128 Sets, 64B block size, 2 cycle latency, 2 ports. 32/256 Shared by all L1 caches, split address space, 16 way set associative, 1024 Sets, 64B block size, 20 cycle latency, 4 ports. Shared by all L2 caches, split address space The 8 corners of 3D NoC Number of Memory Banks - L2 Properties 8 Parameter General Router Properties - Number of VC Classes - Flits per Cache Requests - Flits per Cache Block transfer - Flits per Coherence Data - Clock Frequency (GHz) - Number of buffers per VC - Number of RC units - Flit Size - Link Width - Pipeline Width Simulation Abbreviations Network simulation Value Description Handles all trafﬁc. 1 1 5 1 2.5 5 2 128+6 bits 5 extra bits as 128+6 bits speciﬁed by [10] and 1 for denoting unicast and multicast 2 D C V P Dynamic Multicast Tree Critical Word First VC as Cache Packet Concatenation cache population pattern of the benchmarks using a perfect network. This perfect network handles any request to get delivered in the next cycle to the destination. We observe that most of the PARSEC workloads have low injection rate. So, they pose no real challenge for the interconnection network. For the 512 node simulation setup, we found most of the multithreaded benchmarks running a maximum of only 64 threads. so, we used combinations of different benchmarks (multiprograms) to run on all cores. For the 512 node system experiment, we have 256 cores and 256 L2 banks. We run 32 threads of each of Barnes, Cholesky, Radiosity, Radix and Ocean benchmarks. We also run two 32 threaded Fmm instances, making the total to 7 × 32 + 32 = 256. We use this conﬁguration for all the algorithms. B. Network Simulation Booksim2.0 [29] has built-in C++ models for two stage pipelined router, virtual channels, switch allocators and other routing components. As mentioned in Section V-A, we send network injections from Multi2sim4.0.1 to a Booksim2.0 instance. So, the simulators interact with a common cycle accurate clock. With this setup they can send and receive request/response and acknowledgments in real time. As a full system in operation, the trafﬁc does not handle message deadlines. It is essentially a network talking to the architecture. When the response from the architecture gets delayed, the network does nothing about it. Booksim2.0 processes the injections and respond back to Multi2sim4.0.1 using a common clock. This communication is possible by linking the clock increments to a TCP socket. This socket synchronizes both the simulators and network injections are sent through this channel. Thus cycle accuracy is maintained. The injections and responses to/from the network are marshalled and unmarshalled as ﬁxed length strings. Thus, there is no data loss with this model. We also built a new network topology to support z − direction jumps, with a speciﬁc router model for supporting the jumps. These two components are integrated to form a base network model for our simulations. C. Hardware Modelling We model a comparator in Verilog for both source+tag comparison of read requests and compare the responses present in the VC buffers for its match. This comparator is 38 bits long and is implemented as two stages. The ﬁrst stage XORs two numbers and the second stage checks for equality of the two numbers by OR-ing all the 38 bits and checks for zero. We also model a similar 6 bit destination id comparator for packet concatenation. We develop a Verilog module for a crossbar, a basic routing algorithm module, virtual channels, inputoutput ports and the comparators along each input channel buffers. The buffers are made using a matrix of T ﬂip-ﬂops to the required dimensions. We then used Synopsys-Synthesis tool with 65nm technology. We also encode the necessary parameters for the router design into Orion 2.0 [34] and integrate the model to our simulator stack. We did this by using a MySQL table as a buffer between booksim and orion executables. Booksim dumps per cycle per router vc injections into the table. A daemon checks and executes orion for each of the dumped injection value and collects stats on various energy parameters reported by orion back into the table. This way, we will able to get accurate cycle level energy consumption details for every router in the network. The results from this modelling are discussed in Section VI. We do not develop hardware level designs for all the techniques compared in this paper. We compare our 2D designs area, power and energy consumption with the base router models 2D only. We do not introduce any change to the 3D router design at all. It can be implemented using TSVs or using a full ﬂedged DimDe like router too. V I . R E SU LT S We compare our optimizations on the base router with two a 4×4×4 network as explained in Table I. The results obtained contemporary techniques, namely VCTM and Hamiltonian for are discussed below. routing for any multicast chain. A. Real Trafﬁc from a Full System Simulator The results from full system simulations can be seen from Fig. 3. The full-system performance is obtained from the committed instructions per cycle for each simulation run. System speedup graph populated in Fig. 3b shows variations due to the proposed optimizations over the base odd-even routing. We also compare the performance speedup with state (a) Latency (b) System Speedup (c) Network Throughput Fig. 3: Real Trafﬁc Results[Please refer to Simulation Abbreviations in TABLE I] of the art VCTM and Hamiltonian methods. Though the GeoMean shows step by step speedup improvement from the base, there are certain individual benchmarks where it does not report such performance gains. Barnes posts most gains from dynamic multicasting. With other optimizations over dynamic multicasting, it brings down the performance achieved by the dynamic multicasting. This is because, Barnes has a highcommunication phase prevalent for most of the time in its execution. Thus by doing critical word ﬁrst, we tend to make the destination node act on the received data before the body and tail ﬂits retire. Since it is a heavy injecting phase, this causes the destination node to inject a new packet quicker than the case without enabling critical word ﬁrst. Dynamic multicasting gains up to 9% of IPC because the average hop length of the multicast packets is 2.1 for Barnes. Also, from Fig. 1, we can see that the maximum sharers for Barnes is 10 and it occurs for 4.7% of the multicasts. The other 95.3% of the multicasts are around the mean with less than four sharers. VC as cache saves the damage done by critical word ﬁrst to cause the performance gain to improve by 1%. Though Barnes cannot recover from critical word ﬁrst to add to the performance gain by dynamic multicast, it improves the network performance by reducing the number of ﬂits transferred by the network. This is shown in the ﬂits transferred bar for Barnes in Fig. 3c. It saves up to 1200 ﬂits in the network and gains 1% on top of the performance by critical word ﬁrst. The last optimization we have is Packet Concatenation on top of the rest. Though it improves the number of ﬂits transferred, it does to not gain much performance when compared to VC as cache. This is because, though it increases the number of requests submitted to L2 banks, it does not imply a higher multicast percentage. Though the requests are traveling toward the same destination, they are actually for different blocks. Thus the L2 bank becomes the bottleneck and stops this optimization from gaining anything at all. If we examine Cholesky, we see a similar behavior. Though there is an overall gain over the base with all the optimizations, VCTM still outperforms our optimization. In Fmm Critical Word First adds on to the overall speedup. Since Fmm has relatively low number of multicasts (Figs. 1a and 1b) the network could bear more injections as the critical words reach faster and gets processed. This can be seen in the raise in the number of ﬂits transferred in Fig. 3c. VC as cache adds further by making the responses reach quicker. But Packet Concatenation lowers the performance. This is because of the existence of short bursts of multicasts make the network congested for short periods of time. But, by the time the network starts having free VCs, there are various unicast packets that get concatenated already. Since we consider a once concatenated super-packet to never split again, we tend to make the packets’ efﬁciency to reach to get limited by the number of concatenations. But, this is not always the case. For example, in both Radix and Swaptions, Packet Concatenation alone improves IPC to 60% and 20%, respectively. Radix gains because of its dense multicasts. As we can see from Fig. 1a, the number of sharers peak to 28. So, it could potentially mean that we could gain performance from any of our optimizations. There is not much to gain with VC as cache in here because, the multicast data get no extra additional requests later. Swaptions loose performance when using dynamic broadcast. As Fig. 1a explains, Swaptions has a few number of sharers. Also, the workload is sensitive to the order of requests. Since with dynamic multicast, we tend to fork at each common point, we delay some injection. This stalls some threads and causes the loss in performance. Though the network is able to transfer more ﬂits in the execution at a lower average latency, the workload’s sensitivity on the data becomes crucial. Thus, it starts recovering from the damage using Critical Word First. When there is a stall in injecting into a router, VC as cache becomes a natural option for the workload to show huge gains. IPC Speedup jumps by 12% and it also beats VCTM. With Packet concatenation further reduces injection stalls and gains up to 20% in terms of IPC Speedup. Hamiltonian and VCTM also gain with VC as cache on an average. We could also apply any of the aforementioned optimization on top of these multicast techniques. But the analysis on the impact of performance of these techniques is not done for brevity. B. Chip Area, Clock Delay and Control Lines As mentioned in Section V-C, all the components of the router are modelled in Verilog. We do not ﬁnd any area overhead by adding one extra RC unit. But by adding two extra RC units we were able to get a slight increase in the area (a) Barnes Energy Across Time (b) Ocean Energy Across Time (c) Overall Per Flit Energy Footprint Fig. 4: Energy Analysis[Please refer to Simulation Abbreviations in TABLE I] of 0.013%. The area overhead for ﬁtting in two comparators for VC as Cache and Packet Concatenation techniques were found to be 1% of the total area of the router. We need one extra line in the link to indicate multicast and unicast packets. We do not impose additional delays to the router pipeline. The comparators take 0.063ns for one full comparison. Since our clock delay is 4ns, we could ﬁt 6.33 comparisons per cycle. But we limit our comparisons to ﬁve to provide some slack. C. Energy Analysis As mentioned in Section V, we use Orion 2.0 to interact with our full system simulator to gain ﬁne grained dynamic energy at every cycle. The crossbars and the pipeline units consume less than 1% of the total energy. Clock consumes 0.3%, while the pipeline stages consume around 0.02% of the total router energy. The remaining 99.5% of the total energy is controlled by the buffers. Since we use the idle buffers in the baseline model to our advantage by adding more ﬂits to them, our model utilizes the buffers more. Thus we show the per-ﬂit energy consumption on an average at each router in Fig. 3. As we can see from the ﬂits transferred by the network in Fig. 3c, our techniques tend to utilize the network better by transferring larger number of ﬂits most of the time. Thus the dynamic energy consumption of our technique is slightly greater than the baseline router on an average. Our Base+CVDP consumes 5.4nJ on an average per cycle while the base design consumes 4.75nJ. But for a fair analysis and point out the advantage of our optimizations correctly, we take a look at the energy spent on a ﬂit by a router. A multicast packet is still treated as a single packet with ﬁve ﬂits only. Fig. 4c shows that our optimizations reduce the energy spent by a router on a ﬂit in most cases. This reﬂects our philosophy behind using the idle buffers. But in cases like Cholesky, ocean or Radix, our overall optimization shoots up the energy per ﬂit by a router. It shows that Packet Concatenation is a bad idea when congestion seem to alleviate quicker than the average packet latency. To understand this data, we take two cases - Barnes and ocean. In Barnes (refer to Fig. 4a), we gain up to 22.85% with respect to the energy spent on a ﬂit by the base technique. Thus the optimizations we do beneﬁt the system on the whole. Another thing to see is the erratic behavior of the energy line by Dynamic Multicast. Since Barnes is a tree access algorithm, the threads accessing a particular block of data tend to be close to each other in the beginning. In those places, our forking take place almost near the end of the multicast tree. This is very efﬁcient and shows up to 17.34% energy savings. But as the algorithm progresses, the threads accessing the same block tend to be located at opposite corners of the mesh, making the packet forking to occur prematurely. Thus the multicast transfers start congesting the network and other optimizations start kicking in. Unlike multicast, the other optimizations occur only when there is a congestion. So, we see a smooth gain in energy with CD, CVD and CVDP. On the other hand, Fig. 4b shows ocean to behave in the exact opposite way. This makes Packet Concatenation loose to the base by about 1.8%. This again boils down to the trafﬁc behavior. The energy gain in multicast trafﬁc is almost similar to that of Barnes. But the congestion alleviation happens so rapidly that doing Packet Concatenation drops the energy usage to 1.79% higher than the base. Also, VC as cache consumes higher energy than plain multicasts at many places. This is mainly because of the futile matches made by the router. Though there is a good request - response meeting at the intermediate routers, the tags requested are different most of the time. So, the comparisons cause the overheads in degrading energy per ﬂit when compared to dynamic multicast. But we do post better geometric mean than the base technique. We save 1.66% of energy spent per ﬂit by a router with respect to that of base. Our energy savings peak to 17.7% when running Radiosity. D. Scaling to 512 Nodes As we discuss sharing, we also evaluated our model in a 512 node setup and the results are shown in Figs. 5 and 6. The speedups against base in the 64-node by CVDP, Hamiltonian and VCTM are 18.4%, 10.01% and 10.6%, respectively. Now in 512 nodes, the speedups are 7%, 5.8% and 5%, respectively. This shows the scalability of our technique when compared to Hamiltonian and VCTM. Also CVD gains 9% in this 512 node setup while it gained 11.6% in the 64 node setup. This steady behavior is from the fact that concatenated packets travel longer distances with increase in network size. Thus there is a degradation in performance from CVD to CVDP by 2%. VCTM+V gives 5.5% and that is 10% improvement coming from VC as cache. Hamiltonian+V on the other hand gains only 4%. This loss of 31.08% is because, the responses take a longer path to move along the Hamiltonian route. V I I . CONCLU S ION We proposed a scalable way of handling multicasts and demonstrated it to outperform the best techniques available today in two different network sizes using and real multithreaded benchmarks. We analysed the other possible optimizations to support this implementation and we discussed the way to incorporate it with the existing techniques. Thus, we could derive much better performance of the whole system by utilizing the knowledge of data transmitted in an NoC of any CMP. It is Fig. 5: Scaling to 512 Nodes: Latency Fig. 6: Scaling to 512 Nodes: System Speedup also cautioned that Packet Concatenation can be beneﬁcial only when there is no alternative path to take. Though this would occur often in case of heavy workloads like the experiments we conducted with, it may not be the case all the time. In such cases, path diversity will be better than getting stuck behind a queue of packets. ACKNOW L EDGM ENT The authors would like to thank VIRGO team, IIT Madras for their kind support. This work is funded by DST, Govt. of India. "
ElastiNoC - A self-testable distributed VC-based Network-on-Chip architecture.,"Network-on-Chip (NoC) design tries to keep a balance between network performance and physical implementation flexibility. The adoption of Virtual Channels (VC) holds promise for scalable NoC design. VCs allow for traffic separation and isolation, enable deadlock avoidance and improve network performance. In this paper, we present ElastiNoC, a novel distributed VC-based router architecture that enjoys all the benefits offered by VCs and leads to efficient silicon-aware implementations. The proposed architecture utilizes an efficient buffering strategy and allows for modular pipelined organizations that increase the clock frequency. Moreover, it offers maximum freedom in terms of physical placement, by allowing the NoC components to be physically spread throughout the chip, irrespective of the network topology. The combined effect of all supported features enables significant delay reductions under equal performance, when compared to state-of-the-art VC-based NoC implementations. Moreover, the careful addition of self-test structures allows ElastiNoC to enjoy fully distributed Built-In Self Testability (BIST), where testing unfolds in phases and reaches high fault coverage with small test application time.","ElastiNoC: A Self-Testable Distributed VC-based Network-on-Chip Architecture I. Seitanidis(cid:3) , A. Psarras(cid:3) , E. Kalligerosy , C. Nicopoulosz and G. Dimitrakopoulos(cid:3) (cid:3)Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece y Information & Communication Systems Engineering, University of the Aegean, Samos, Greece zElectrical and Computer Engineering, University of Cyprus, Nicosia, Cyprus Abstract—Network-on-Chip (NoC) design tries to keep a balance between network performance and physical implementation ﬂexibility. The adoption of Virtual Channels (VC) holds promise for scalable NoC design. VCs allow for trafﬁc separation and isolation, enable deadlock avoidance and improve network performance. In this paper, we present ElastiNoC, a novel distributed VC-based router architecture that enjoys all the beneﬁts offered by VCs and leads to efﬁcient silicon-aware implementations. The proposed architecture utilizes an efﬁcient buffering strategy and allows for modular pipelined organizations that increase the clock frequency. Moreover, it offers maximum freedom in terms of physical placement, by allowing the NoC components to be physically spread throughout the chip, irrespective of the network topology. The combined effect of all supported features enables signiﬁcant delay reductions under equal performance, when compared to state-of-the-art VC-based NoC implementations. Moreover, the careful addition of self-test structures allows ElastiNoC to enjoy fully distributed Built-In Self Testability (BIST), where testing unfolds in phases and reaches high fault coverage with small test application time. I . INTRODUC T ION The design of scalable Network-on-Chip (NoC) architectures calls for new implementations that achieve high throughput and low-latency operation, without exceeding the stringent area-energy constraints of modern Systems-on-Chip (SoC) [1]. In terms of network functionality, Virtual Channels (VCs) – which allow a physical channel to be used in a timemultiplexed manner by different trafﬁc ﬂows, provided that each ﬂow owns a separate buffer space – are an already proven solution [2]. Architectures supporting the use of VCs (1) enable trafﬁc separation and isolation by assigning different trafﬁc classes to different VCs, (2) improve performance, and (3) reduce on-chip physical routing congestion, by trading off physical channel width with a number of VCs, thereby creating a more layout-ﬂexible SoC architecture. VCs are also instrumental for the correct operation of higher-level mechanisms. For instance, protocol-level restrictions in Chip Multi-Processors (CMP) employing directorybased cache coherence necessitate the use of VCs. Coherence protocols require isolation between the various message classes to avoid protocol-level deadlocks. For example, the MOESI directory-based cache coherence protocol requires at least three virtual networks to prevent protocol-level deadlocks. A virtual network comprises of one VC (or a group of VCs) that handles a speciﬁc message class of the protocol [3]. Traditional VC-based NoC architectures focus mostly on microarchitectural improvements to the router’s internal organization and pipeline structure [4], [5]. Prior research has explored salient router attributes, such as appropriate allocation policies [6], as well as the optimization of the associated VC buffering structures [7], [8], [9], concentrating mostly on buffer sharing and related ﬂow control strategies. A complete overview of routers’ microarchitecture can be found in [10]. In this paper, we revisit ﬁrst the pipelined conﬁgurations of baseline routers with the goal of identifying – via a simple intuitive analytical model – the amount of pipelining needed to achieve optimal network latency under arbitrary topologies, packet sizes, and routing algorithms. Our analysis aims to shed more light on previous design trends that targeted primarily the reduction of the intra-router pipeline stages. We clearly show that router pipelining (and its associated clock frequency beneﬁt) will always be beneﬁcial, even for simple NoC designs, when applied with care, so as to avoid overpipelining and its associated diminishing returns. Motivated by this analysis, we introduce a novel distributed VC-based router architecture, which enables ﬁnegrained pipelining and provides maximum ﬂexibility in terms of NoC physical placement. The proposed structures are also enhanced with novel self-testability features and a scalable testing mechanism that achieves high fault coverage with small test application time. While the concepts of distributed router design and ﬁnegrained network pipelining have been explored in the past, the focus has been on applying the said attributes to designs that do not support VCs [11], [12], [13], [14], [15]. Supporting VCs in that context needs multiple parallel networks of such distributed routers. Obviously, multiple networks do not constitute the most resource-efﬁcient solution, due to inevitable resource duplication. Hence, the need for an architecture that efﬁciently combines all these beneﬁts with support for VCs is imperative. To the best of our knowledge, the design proposed in this paper is the ﬁrst distributed VC-based router implementation that supports this form of modularity. In summary, the main contributions of this work are: (cid:15) The introduction of a new architectural paradigm for VCbased NoC routers, called ElastiNoC, which enables the modular construction of pipelined routers. The resulting design, presented in Section III, yields highly-scalable NoC implementations. (cid:15) A shift in the design philosophy of VC-based NoC routers from centralized and monolithic structures to modular and distributed components that can be “stitched” together to form a larger entity, while still providing full VC support and extensive ﬂexibility during physical placement. (cid:15) The augmentation of ElastiNoC with self-testability caVC allocation, whereby packets cannot change VC. All delays are normalized to the delay of the wormhole router, which does not support VCs. In all cases, the routing computation is performed in series with the remaining tasks.The VC-based routers have 4 VCs per input port with 4 buffers per VC. The wormhole router has 4 buffer slots per input port. The delay of each single-cycle router is the sum of several sub-tasks, such as Buffer Read (BR), Routing Computation (RC), VC Allocation (VA), Switch Arbitration (SA), Handling of returning Grants (GH), and Switch Traversal (ST), which also includes the delay of credit updates and VC state reallocation (in the case of a tail ﬂit leaving an output port). Note that in speculative routers that do VA and SA in parallel, the critical path passes through the SA unit. Even though the evaluated routers have completely different behavior in terms of throughput-versus-load performance, they represent almost all design options available for the design of monolithic NoC routers. In any case, the minimum clock cycle that a singlecycle router can operate at is TC Y C (cid:21) D + c, where D represents the worst-case delay of the router’s internal paths, and c is the clocking overhead (sum of the register clock-to-Q delay and the setup time; depicted as CO in Figure 1). Router pipelining is expected to reduce the clock period. However, every pipelining decision stops across the borders of the traditional basic blocks within each router, e.g., VC allocation, switch arbitration, and switch traversal. The fact that such blocks do not have an evenly balanced delay proﬁle – as shown in Figure 1 – makes pipelining even harder, since the achieved clock frequency is limited by the delay of the critical path. For the optimal case, we can assume that it is possible to break the router’s critical path into k equal-delay stages. Then, the router’s clock period can drop to TC Y C (cid:21) D k + c (1) By increasing the pipeline stages of a router, its clock frequency can increase, thereby leading to faster implementations. At the same time, however, each ﬂit spends more cycles inside each router, before moving to the next one. Therefore, the depth of the pipeline cannot be decided in isolation; the decision should also take into account other network parameters, such as the number of hops each packet needs to make between source and destination pairs, and the average packet size, assuming that a mix of packets of different sizes may traverse the network. The hop count is determined by many factors, such as the network topology and size, the employed routing algorithm, and the statistics of the trafﬁc patterns. To keep our model simple, we incorporate the contributions of all these factors within one variable, i.e., the average hop count H , which averages the contribution of each feature. Thus, the zero-load latency (in cycles) of a packet is equal to T = H (k + 1) + P (cid:0) 1 (2) Each ﬂit spends k cycles in each k-pipelined router and 1 cycle to cross the link between two routers. Variable P = L=W represents the serialization latency of a packet with a size of L bits traveling over W -bit wide physical links1 . 1 The minus one removes the contribution of the head ﬂit, which is included in the ﬁrst term of the equation. Fig. 1. The delay of representative single-cycle 5-port NoC routers with 64-bit wide ports and 4 VCs per input in the case of VC-based routers. The results are normalized to the delay of a wormhole-based router (i.e., no VC support). “No VC change” means that packets do not change VC; their VC is decided upon injection and remains the same until they reach their destination. pabilities, as presented in Section IV. The routers are able to conduct testing sessions in a modular manner over multiple phases, that achieve high fault coverage (in excess of 99%). Self testing imbues ElastiNoC with a valuable (albeit often ignored) asset, which greatly enhances its viability to much larger future designs. The experimental results presented in Section V – based on both network simulations and standard-cell-based hardware synthesis implementations – validate the efﬁcacy and efﬁciency of ElastiNoC. The combined effect of all introduced features enables the design of highly scalable VC-based NoC architectures, which offer high operating frequencies and provide equal (or even better) networking performance, as compared to state-of-the-art VC-based implementations. I I . MOD EL ING LOW- LATENCY ON -CH I P N ETWORK S In this section, we develop a simple intuitive analytical model that connects the network latency with the routers’ operating clock frequency and their internal pipeline organization. The goal is to construct a model that enables the designer to derive a ﬁrst-order approximation to an optimal conﬁguration, given certain parametrical constraints. The presented model, although based on several simplifying assumptions, provides valuable intuition on when router pipelining is needed, and which pipeline depth makes sense to implement. First, assume that the NoC’s topology and size are ﬁxed, and the possible use of concentration has already been decided. Moreover, assume that the link bit-widths have also been decided. Such decisions ﬁx the radix of the routers and their port sizes, which are critical factors in determining the overall delay. Still, even for ﬁxed-radix routers, their delay can vary signiﬁcantly, depending on the microarchitecture of the routers (e.g., support for VCs, allocation organization etc.) and other implementation constraints. Figure 1 shows the normalized minimum delay of several single-cycle routers with 5 input ports and 64-bit wide channels when synthesized in 45nm technology. The comparison includes (a) a simple wormhole router, (b) a VC-based router with baseline VC allocation, whereby packets can change VC in-ﬂight, (c) a VC-based router with baseline VC allocation, whereby packets are not allowed to change VC, (d) a VCbased router with speculative VC allocation, whereby packets can change VC, and (e) a VC-based router with speculative In addition to the low-radix scenario examined above, we also experimented with high-radix routers (e.g., those found in a ﬂattened butterﬂy topology [16]) to explore optimality in networks with lower hop counts, but higher router latencies (due to the complexities associated with high-radix designs). Our evaluation results – omitted for brevity – indicate that optimal pipelining in those scenarios is achieved with 4 or 5 stages, depending on the various salient parameters. Using the simple analytical model leads to two interesting conclusions. The ﬁrst one is that the decision of pipelining the router cannot be made solely based on its delay, but the process should also take into account the environment in which the router will operate. The second one (and perhaps non-intuitive) is that the designer should not only opt for microarchitectural optimizations that decrease the router’s delay by parallelizing its tasks (e.g., with speculation), but, instead, should embrace a combined approach that utilizes optimal pipelining. This realization serves as the primary motivation and fundamental driver of the work presented in this paper. Unfortunately, in state-of-the-art monolithic router structures, pipelining decisions stop across the boundaries of the traditional basic blocks, which have been widely viewed as “atomic” (i.e., indivisible). Furthermore, the delay of these blocks is not evenly balanced. Therefore, even if 3-stage pipelines (or 4- and 5-stage pipelines in high-radix environments) are still possible with this coarse separation, the achievable clock frequency would be sub-optimal, since the speed of the router would be limited by the worst-case delay of the most delay-critical block. Additionally, most existing router designs are inherently centralized in terms of their physical layout. This is attributed to certain monolithic components within each router; the crossbar switch, the allocators, and the buffering structures signiﬁcantly limit the possible ﬂexibility in the physical placement of the overall router design. Consequently, current architectures are not spatially scalable, i.e., they cannot be efﬁciently distributed in space. This limitation may also have adverse effects on the router’s delay. These limitations of traditional VC-based router architectures are addressed by the ElastiNoC architecture proposed in this work. The new design philosophy: (a) enables modular pipeline implementations, (b) yields high operating frequencies, and (c) allows for efﬁcient spatially distributed hardware implementations. The latter characteristic provides the ﬂoorplanning and placement tools with more freedom in generating optimal layout conﬁgurations. I I I . E LA S T INOC : MODULAR VC -BA S ED ARCH I T EC TUR E Any network topology, from single-stage crossbars to arbitrary cubes, meshes, or butterﬂy-based structures can be implemented by decomposing the switch operation to primitive merge and split functions. In this paper, we design, for the ﬁrst time, novel merge primitives (and the associated simpliﬁed split structures) that support VCs and offer the same degree of ﬂexibility – in terms of network performance and functionality – as monolithic VC-based routers, but with higher-operating speed, and distributed physical placement capabilities. A. Modular router construction The fundamental primitive of ElastiNoC, called the Merge Unit (MU), consists of two inputs and one output. Its goal is (a) (b) Fig. 2. The average zero-load packet latency (in absolute time) computed directly from eq. (3) and the associated router area overhead, as the number of pipeline stages are varied. Results are shown for (a) a baseline, and (b) a speculative VC-based router assuming 4 VCs per input port. Since each k-stage pipelined router works with a clock period of TC Y C 2 , the zero-load latency of each packet in absolute time is the product of the latency in cycles and the minimum clock period of a pipelined router: ( ) TABS (k) = T (cid:2)TC Y C = (H (k + 1) + P (cid:0) 1) D k + c (3) It should be noted here that even if the use of the packet’s zero-load latency alone is not sufﬁcient to fully capture a NoC design’s behavior, the resulting conﬁgurations will still hold for the majority of possible network loading conditions that are not close to the saturation throughput. To explore the interesting interplay between packet latency and pipeline depth, we ﬁx the average hop count to H = 6:25, an 8(cid:2)8 2D mesh, assuming uniform random trafﬁc and an which roughly corresponds to deterministic XY routing in average packet size of P = 3 (50% 1-ﬂit request packets and 50% 5-ﬂit reply packets). For this conﬁguration, the zero-load latencies TABS (as a function of k) of (a) a baseline and (b) a speculative single-cycle VC-based router that allows for inﬂight VC changes, are shown in Figure 2. When k moves from 1 to 2 (k = 1 corresponds to the un-pipelined single-cycle solution), the latency savings are signiﬁcant and are above 20%. The addition of more pipeline stages reduces packet latency, but with diminishing returns. For example, moving from 3 to 4 pipeline stages offers less than 1% savings in packet latency, without justifying the additional cost in control logic and buffering resources. In a VC-based router, the number of buffers should be equal to the minimum required to cover the ﬂow-control round-trip latency; else, throughput is severely compromised. Pipelining increases the round-trip delay, which, in turn, increases the mimimum buffering requirements of the entire router. Therefore, any pipeline decision should also take into account the buffering cost that this option incurs. Figure 2 depicts the area cost required for each pipelined alternative. Straight lines are actual measurements after synthesis while dashed lines correspond to calculations that add the area of extra buffering. Inspecting packet latency and buffering cost together leads to the conclusion that pipelining is indeed a useful design choice that ends its useful contribution at around 3 pipeline stages. Above that point, the investment in extra area due to more pipeline stages is not compensated by the (diminishing) reductions in packet latency. 2 For simplicity, we assume that all NoC components (e.g., routers and links) belong to a single clock domain. Fig. 3. The modular construction of an example ElastiNoC 4(cid:2)4 VC-based router using the proposed MU primitive that supports 2 VCs. to switch and buffer locally the ﬂits of two inputs that belong to different VCs. Buffering is done via ElastiStore units [17], which follow an elastic protocol and are able to simultaneously store the data of many VCs using the minimum amount of buffering. Each ElastiStore module comprises one single-ﬂit register per VC, plus one other single-ﬂit register that can be dynamically allocated to the ﬁrst stalled VC. By using MUs and splitting the data arriving at each input port, one can design an arbitrary VC-based router. An example is shown in Figure 3, which depicts an ElastiNoC router with 4 inputs and 4 outputs. Upon arrival at the input of the router, each packet has already computed its destined output port via Look-ahead Routing Computation (LRC). Subsequently – depending on buffer availability, output VC availability, and the allocation steps involved in each MU – the ﬂits of the packet are forwarded to the MU of the appropriate output. Integration of MU and ElastiStore primitives is straightforward, since they all operate under the same ready(i)/valid(i) handshake protocol. All router paths from input to output see a pipeline of MUs of log2 N stages. Moving to the next router involves one extra cycle on the link that is just a one-to-one connection between two ElastiStores. The ﬂow control on the links does not allow packets to change VC and its operation needs only an arbiter and a multiplexer for selecting a ﬂit to send to the next router. The fact that all input-to-output paths experience log2 N stages of MUs is extremely important. This attribute aligns ElastiNoC with the optimal pipelining conclusions extracted in Section II for both low- and high-radix routers. For lowradix routers (with 5-8 input ports), optimal pipelining calls for 2-3 stages, while the 4-5 pipeline stages required for high-radix routers (with more than 12 input ports) are also in agreement with the logarithmic number of stages of the proposed architecture. Thus, ElastiNoC allows for sufﬁciently ﬁne-grained modularity, which can yield optimally pipelined designs over a wide spectrum of router radices. Due to the distributed nature of ElastiNoC, the split connections can be customized to reﬂect the turns allowed by the routing algorithm. For example, in a 5-port router for a 2D mesh employing XY dimensioned-ordered routing, splitting from the Y+ input to the X+ output is not necessary since this turn is prohibited. Several other deterministic and partiallyadaptive routing algorithms can be deﬁned via turn prohibits as shown in [18]. When such customization is utilized, signiﬁcant area savings are expected, due to the removal of both buffering Fig. 4. The fundamental ElastiNoC primitive, the Merge Unit (MU). The diagram depicts the per-input and per-output multiplexers together with the combined allocation logic (SA1, SA2) that runs in parallel to VA1. and logic resources. On the contrary, when such optimizations are performed in traditional VC-based routers, only parts of the crossbar and switch allocation logic are reduced, while the VC allocation logic and buffering, which are responsible fot the majority of the router’s area, are not affected. This modular router construction enables packet ﬂow to be pipelined in a ﬁne-grained manner, implementing all necessary steps of buffering, VC and port allocation, and multiplexing in a distributed way inside each MU, or across MUs. Also, the placement of MUs does not need to follow the ﬂoor-plan of the chosen NoC topology. Instead, MUs can be freely placed in space, provided that they are appropriately connected. B. The Merge Unit (MU) Each MU is responsible for switching one output between 2 inputs assuming the existence of per-input and per-output VCs, as shown in Figure 4. Since switching is achieved by connecting several MUs in series (as illustrated in Figure 3), the buffers presented at the input of Figure 4 are actually the output buffers of the previous MUs. 1) Allocation and Switching Logic: Packets arriving at the two inputs of each MU must compete for a single output. Since the output can carry ﬂits that belong to different VCs, each packet has to ﬁrst allocate a VC at the output of the MU (known as an “output VC”), before leaving the input VC. Allowing packets to change VC in-ﬂight, within each MU, is possible when the routing algorithm does not impose any VC restrictions (e.g., XY routing does not even require the presence of VCs). However, if the routing algorithm and/or the upper-layer protocol (e.g., cache coherence) place speciﬁc restrictions on the use of VCs, arbitrary VC changes are prohibited, because they may lead to deadlocks. Any restrictions are enforced by the allocator of the MU. Our goal is to make the MU as fast as possible without sacriﬁcing throughput. Therefore, we follow a combined allocation approach [19], customized and optimized to the characteristics of our design by allowing packets to change VC in ﬂight at the granularity of a single MU. Each input VC holds two state variables showing (a) if the VC has valid data, and (b) if it has been allocated to an output VC. Each output VC also holds two state variables: (a) variable “available” indicates whether it is currently allocated (“locked”) by an input VC, and (b) variable “ready” indicates if there exists free buffer space, which, in our case, is received by the output ElastiStore’s ready signals. When a head ﬂit arrives at an input VC it simultaneously tries to get matched to an output VC, and also to gain access to the output port of the MU. Both actions should be successful for the head ﬂit to reach the output of the MU. Before issuing any request to the allocation logic, the head ﬂit checks if there is at least one available and ready output VC (readiness corresponds to buffer availability). If this is true, the head ﬂit issues a request to SA1 that promotes one ﬂit from each input. Next, the two input ports (i.e., the SA1 winner of each port) arbitrate in global SA (SA2) to advance to the output port via the data multiplexers driven by the grant signals of the SA1 and SA2 round-robin arbiters. In parallel to SA1 and SA2, the head ﬂit has to select one available output VC. This is done independently per input VC using one V:1 round robin arbiter (VA1), where V denotes the number of supported VCs. Thus, when a head ﬂit wins SA2, it is allocated to the output VC selected in parallel by VA, and it updates its per-input state variable. On the contrary, if a head ﬂit loses in SA2, it will not refresh its VC state and retry in the next cycle, repeating the whole process. The parallel operation of VA1 and SA1-SA2 does not involve any speculation, since SA1 requests are considered valid only when there is at least one available and ready output VC. The stored input VC state is inherited by the packet’s following body and tail ﬂits, which use it (a) to generate an SA1 request (after checking with the output VC’s readiness), and (b) to reach the same output VC, after winning in SA2, as well. The tail it is also responsible for releasing both the per-input and the per-output state variables, allowing the output VC to be allocated to another packet. 2) Buffering: ElastiStore allocates the minimum of just one register (i.e., holding a single ﬂit) per VC, plus one additional register that is shared dynamically between VCs, and enables full throughput of elastic operation using one ready/valid handshake signal per VC. The static allocation of a single buffer to each VC guarantees forward progress for all VCs and avoids possible protocol-level deadlocks. Since each ElastiStore acts both as an input VC buffer for downstream connections and as an output VC buffer for upstrem connections, it keeps all the necessary variables per VC, as shown in Figure 5. When an input VC is ready to accept a new ﬂit it asserts Fig. 5. The ElastiStore-based buffer architecture. ElastiStore consists of merely one single-ﬂit register per VC, plus one additional single-ﬂit register that is shared dynamically between VCs. Fig. 6. BIST enhancements of a merge unit by incorporating the logic of LSFR/MISR within that of the shared buffer at each ElastiStore. the ready(i) signal. An in valid(i) signal means that valid data for the ith VC has arrived at the ElastiStore buffer. A transfer occurs for VC#i, when both signals are asserted. Flits arriving at each ElastiStore are stored in the main register of each VC. The shared register is used only to host any in-ﬂight arriving ﬂit for a stalled VC. This may happen, since back-pressure signals are ﬁrst registered, before being sent upstream, to guarantee maximum scalability in terms of delay. Filling the main register with data does not render the corresponding VC unavailable, since there is still extra space at the shared register. A VC stops being ready when its main and the shared buffer are full. When the allocation logic of MU dequeues a ﬂit from an input VC, the main register of this VC is reﬁlled either with new arriving data from the input, or with data possibly stored in the shared buffer. This automatic data movement from the shared to main buffer avoids any bubbles in the ﬂow of ﬂits across the MUs and achieves maximum throughput. IV. E LA ST INOC S EL F T E S TAB I L I TY As technology continues to scale and chips continue to grow, system reliability and scalable Built-In Self-Test (BIST) architectures are gaining signiﬁcant importance. NoC testing has evolved over the recent years providing topology-agnostic and modular self-testing methodologies [20], [21]. The distributed structure of ElastiNoC does not match well with traditional core-level BIST architectures [22]. Therefore, we targeted the design of a new distributed BIST architecture that (a) reuses as much as possible the hardware of ElastiNoC, (b) achieves high fault coverage and fault localization at the MU level (detects which MU contains a faulty node) and (c) completes NoC testing within a small number of test cycles. The last feature is critical when the NoC that reaches all IP cores of the system is used as a test access mechanism for those cores. In such cases, the sooner the NoC is tested, the sooner the testing of the rest of the system can begin. The self-testability features of ElastiNoC are applied at the MU level. Our target is to test the two input ElastiStores of each MU, along with the associated multiplexing and allocation logic, and capture the responses in the shared buffer of the output ElastiStore. To achieve this, the shared buffers of the input ElastiStores should function as Test Pattern Generators (TPGs) during testing, and speciﬁcally as Linear Feedback Shift Registers (LFSRs), so as to provide pseudorandom patterns to the tested circuit. Furthermore, the shared buffer of the output ElastiStore of the MU should act as a Multiple Input Signature Register (MISR), in order to compact the responses. This organization, shown in Figure 6, allows us to reuse the ﬂip-ﬂops of the shared register of each Elastistore and transform it into a Built-In Logic Block Observer (BILBO) with small hardware overhead (a BILBO register combines the operation of an LFSR, an MISR, and a shift register). Testing of a router’s MUs that belong to the same switching level constitutes an independent test phase. In the next test phase, where the previous and the next MU levels are tested, the functionality of the shared buffers as LFSRs/MISRs is inverted, since the output ElastiStores of the current level are the inputs of the next, whereas the inputs of the current level are the outputs of the previous one. A test phase can be applied simultaneously to all NoC routers. Allowing the shared buffers of the input and output Elastistores to act as TPGs and response compactors respectively, requires some additional test isolation logic that is enabled only during testing. The Elastistores under test (input ElastiStores) are isolated using 2-to-1 multiplexers that multiplex their data/control inputs with the outputs of the shared buffers that act as LFSRs, as depicted in Figure 6. During testing (TestEnable=1) the original bypass multiplexers of Elastistores get the same value, which guarantees that the outputs of the LFSRs propagate in the MU irrespective of the value on the select lines of the bypass multiplexers. Additionally, every other testing logic added should pay off in terms of fault coverage. Data registers are easily testable since they are directly accessible. The combinational logic of the MUs can be easily tested as well. Testing gets complicated for the input/output VC state registers and the priority state of each round-robin arbiter that can be accessed and observed only implicitly. Our preliminary sequential ATPG and faultsimulation experiments indicate that long test sequences with top-off deterministic patterns cannot achieve anything more than just moderate fault coverage. For that reason, we have chosen to adopt a partial-scan approach, where the internal state registers shown in Figure 6 are put in scan chains (the tested circuit, as a whole, remains sequential). This choice allowed for very high fault coverage with very short, strictly pseudorandom, test sequences, without incurring signiﬁcant overhead, since the aforementioned scannable ﬂip-ﬂops are only a small portion of the total ﬂip-ﬂops involved in a MU (the majority are data registers). To drive the scan chains and compact the captured responses, we augmented the shared buffer with some additional ﬂip-ﬂops, as shown in Figure 6. During each test phase, multiple MUs are independently tested in parallel. For example, the 4 (cid:2) 4 router depicted in Figure 3 would have been tested in three phases. The ﬁrst phase utilizes the shared buffers of the input Elastistores as TPGs and the shared buffers at the outputs of the ﬁrst-level of MUs as response compactors. Before testing starts, all ﬂipﬂops of the tested circuits are reset. Due to the partial scan chain architecture, the generation of a new pseudo-random test vector requires a certain number of cycles (equal to the scanchain length), so as to be shifted in the scan chains. Then, 1 clock cycle is needed to put the MU in normal mode and allow the circuit responses to be captured in the scan chains. In the same clock cycle, normal MU outputs are fed and compacted in the output MISR. Finally, the response captured in the scan chains is shifted-out and compacted to the MISR as well. This last operation is overlapped with the scan shift-in of the next test vector. Thus, the total number of clock cycles for generating, applying and compacting a test vector is equal to scan-chain length + 1 (the “+1” term is for the capture cycle). At the end of each test phase, a signature for all responses is stored in each MISR. To verify the results of the test session, this signature needs to be compared with the golden fault-free signature (computed off-line) of the applied test vectors and produce a ﬁnal error bit. This comparison can be made serially, bit-by-bit, with a locally stored golden signature. In the following two test phases, the intermediate ElastiStore shared buffers change role from MISR to LFSR and test the last MUs and their associated LRC logic, using exactly the same test sequence (the responses are captured at the output ElastiStores). The last test phase tests the output links that are connected to the inputs of the next routers via the two ElastiStores present at their endpoints. Since these three test phases can be applied simultaneously to all NoC routers, testing can ﬁnish in a few thousand cycles, as shown in Section V. Gathering the error signals of each MU can be either done at the router level via a small local test controller, or they could be sent via 1-bit pipelined links to a centralized test controller. Depending on the number of pipeline stages per router, and based on the fact that the even-numbered stages can be tested independently from the odd-numbered ones, testing of ElastiNoC requires a constant number of 2 or 3 test phases overall, which is independent of the size of the network. V. EX P ER IM EN TAL R E SU LT S In this section, we compare ElastiNoC with conventional VC-based architectures, both in terms of network performance and hardware complexity. We also report the fault coverage achieved by the proposed distributed BIST architecture and the required test application time, and we quantify the area/delay overhead of the self-testability features. A. Hardware complexity The proposed ElastiNoC routers (using lookahead RC) were implemented in VHDL and mapped (synthesized) to an industrial low-power 45 nm standard-cell library under worstcase conditions (0.8 V, 125 (cid:14) C), using the Cadence digital implementation ﬂow. The generic router models have been conﬁgured to 5 input-output ports, as needed by a 2D mesh network, and to 2 and 4 VCs per port, while the ﬂit width was set to 64 bits. Arbitration in all routers follow the fast arbiter design of [23]. The area/delay curves obtained for all designs - after constraining the logic-synthesis and back-end tools, and the extraction of physical layout information (each output is loaded with a wire of 2 mm) - are shown in Figure 7. The routers under comparison for 2 and 4 VCs per port include an ElastiNoC design with 2 MUs in series per router, a speculative 1-cycle design that corresponds to the fastest monolithic design, as well as 2-stage and 3-stage pipelined baseline router implementations. In both cases (2 and 4 VCs), the proposed ElastiNoC design achieves the highest delay savings of 20% and 15% for 2 and 4 VCs, respectively, as compared to the fastest 3-stage pipelined baseline router. Please keep in mind that the delay numbers reported correspond T E S T COV ERAG E AND T E S T A P P L ICAT ION R E SULT S FOR A MU . TABLE I VCs Scan FFs Scan chains Stuck-at FC Test patterns Test cycles Aliasing 2 24 6 99.93% 302 1510 0% 4 78 13 100% 1642 11494 0% (a) (b) Fig. 7. Hardware implementation results of various router designs with (a) 2 VCs, and (b) 4 VCs per port. to 0.8V operation, which increases signiﬁcantly the delay of the circuits. For example, a close inspection of the clock frequency of ultra-fast 3-stage commercial routers optimized at the transistor level [24], [25], which offers additional beneﬁts versus standard-cell-based design, reveals that their frequency marginally passes 1 GHz when operated at 0.8V. For all cases regarding state-of-the-art routers, we assumed the minimum buffering requirement needed to cover the roundtrip time imposed by their internal pipeline organization. The round-trip latency of a single-cycle router is three cycles, translating to three buffers per VC, since each ﬂit spends one cycle inside the router, one additional cycle on the link in the forward direction, while the back-pressure signals (such as credit updates) need one cycle on the link to return. Thus, the pipelined routers with two and three stages increase the round-trip latency by one and two, respectively – unless, direct combinational ﬂow-control update paths are employed across routers, which limit the beneﬁts of pipelining. As a result, the 2- and 3-stage pipelined routers need a minimum of four and ﬁve buffers per VC. The amount of ElastiNoC buffering is between the buffers required for a 2- and a 3-stage router. While ElastiNoC requires larger area than monolithic routers for the case of 2 VCs, this trend changes in the case of 4 VCs. In this case, under equal delay, the proposed routers and especially the one that is optimized to the routing logic, depicted as “ElastiNoCOptimized” save signiﬁcant amount of area when compared to the 2- and the 3-stage routers, since it allows for both buffer and logic removal. The power consumption of the routers under comparison follows a similar trend. Finally, we measured the area-delay performance of ElastiNoC-Optimized and assuming that the packets entering the network are not allowed to change VC as done in [26]. This simpliﬁcation saves more than 3% and 10% of the delay of ElastiNoC for 2 and 4 VCs per port, respectively, and lowers its area footprint by 12% and 15%. The expected drawbacks of such optimization are: (a) a reduction in throughput by increasing head-of-line blocking per static VC, and (b) complications in implementing adaptive routing. (a) (b) Fig. 8. Latency vs. load curves for (a) uniform random trafﬁc and (b) nonuniform localized trafﬁc. Network trafﬁc from real applications is estimated to lie in-between these two synthetic trafﬁc patterns. Note that the exhibited FC has been calculated over all testable stuck-at faults of a circuit. A small amount of the total faults of each examined MU (approximately 1-1.5%) have been reported as untestable by the Strategate sequential ATPG tool [28], and, as a result, they are not included in FC calculation. No deterministic test patterns have been used though for obtaining the reported results. The only purpose of ATPG was to determine the untestable faults. As can be seen, the proposed BIST approach achieves very high FC (complete in the case of 4 VCs) with quite short test sequences. The 3 (test phases) (cid:2) 1510 = 4530 clock cycles for 2 VCs, and total NoC test time is network-size independent and equal to 3 (cid:2) 11494 = 34482 cycles for 4 VCs. In these ﬁgures, a few extra cycles should be added for signatures comparison and error signal gathering. Our experiments showed that there is no FC penalty when modifying the scan chains volume; more and shorter scan chains can be used, when possible, for reducing test application time. Also, as expected with such wide MISRs (64 bits + the volume of ready/valid output signals), no aliasing was observed between the golden and the faulty circuits signatures. After including the needed testability structures described in Section IV in ElastiNoC and resynthezing the resulting total area for a 5(cid:2)5 router. The impact on the delay is a slight designs, we end up with 22% increase, on average, in the 7% increase, as compared to an ElastiNoC without any selftestability features. This area/delay overhead should be treated as an investment that pays off its purpose by offering ﬁnegrained testability, and fault isolation at the MU level. Its cost can be amortized by increasing the ﬂit width and the number of VCs. This happens since the extra cost involves mostly the logic of the shared buffer at each ElastiStore, which is constant irrespective of the number of VCs. B. Fault coverage and test application time The synthesized MU netlists for 2 and 4 VCs were utilized for obtaining the self-testability results. The Hope sequential fault-simulator [27] was employed to compute the fault coverage (FC), while the LFSR TPG and the MISR compaction operations were simulated using a custom tool. The results of the proposed MU BIST approach are shown in Table I. C. Network performance Network-performance comparisons were performed using a cycle-accurate SystemC network simulator that models all 8(cid:2)8 2D mesh network with XY dimension-ordered routing. micro-architectural components of a NoC router, assuming an The evaluation involves two synthetic trafﬁc patterns: Uniform Random (UR) and non-uniform Localized Trafﬁc (LT). We estimate that network trafﬁc from real applications would lie in-between these two synthetic trafﬁc patterns. For LT trafﬁc, we assume that 75% of the overall trafﬁc is local (i.e., the destination is one hop away from the source), while the remaining 25% of the overall trafﬁc is uniform-randomly distributed to the non-local nodes. We experimented with other distributions as well, but they all showed similar results. The injected trafﬁc consists of two types of packets to mimic realistic system scenarios: 1-ﬂit short packets (just like request packets in a CMP), and longer 5-ﬂit packets (just like response packets carrying a cache line). For the latency-throughput analysis, we assume a bimodal distribution of packets with 50% of the packets being short, 1-ﬂit packets, and the rest being long, 5-ﬂit packets, in accordance to recent studies [29]. Figure 8 shows the latency-throughput curves as functions of the node injection rate, for the two aforementioned synthetic trafﬁc patterns, and the same router conﬁgurations (in terms of numbers of supported VCs and their pipeline structure) used in the hardware complexity analysis. In all cases, the performance of the ElastiNoC routers is indistinguishable from the equivalent baseline routers, both at low and at high loads, while in some cases the performance of ElastiNoC is, in fact, better. The latency of the 3-stage pipelined router is higher, since it costs more cycles to traverse each router of the network. For the 2-stage pipelined solutions that include ElastiNoC and the baseline router, keep in mind that even if the reported latency in cycles is equal, in reality it corresponds to different clock frequencies; ElastiNoC is at least 15% faster. Multiple parallel physical elastic-buffer-based networks of simpler wormhole routers [30] (with each network mapped to one VC) would enjoy slightly higher clock frequencies, due to the complete removal of any VC allocation step. However, when compared with ElastiNoC routers under equal network bisection bandwidth, multiple networks would suffer in performance, as veriﬁed by our experiments (omitted due to space limitations), because of the high serialization latency imposed by the narrower channels in each physical network. V I . CONC LU S ION S Complex MPSoCs built for embedded systems, as well as modern CMPs, have adopted NoC technology for their internal connectivity. This approach was originally utilized to tackle the physical integration and veriﬁcation complexity of large systems. Future NoCs go beyond requirements related to physical implementation and actively participate in delivering high performance, quality of service, and dynamic adaptivity at a minimum area overhead. Virtual channels within NoC routers are quickly becoming a necessary ingredient of modern NoCs, and are viewed as instrumental in enhancing performance and offering several network- and system-wide services. In this paper, we introduce the ElastiNoC architecture as the ﬁrst NoC design that offers: (a) distributed implementation for VCs, including buffering, allocation, and necessary switching; (b) modular pipelined organization; (c) same (or even better) network performance, as compared to baseline monolithic VCbased architectures; and (d) a scalable self-testing mechanism that enables ﬁne-grained fault localization (at the MU level) with small test application time. "
"Technology assessment of silicon interposers for manycore SoCs - Active, passive, or optical?","In this paper, the influence of the possible silicon interposer 2.5D stacking strategies on the micro-architecture of an interconnect topology is studied. We present case studies at different chip scales, based on active CMOS interposers using synchronous or asynchronous NoCs or point to point links, passive metal interposers with DC lines or RF microstrip lines, and optical interposers using multipoint links. We show that a single physical link energy per bit is not a sufficient metric to benchmark these strategies, as the choice leads to strong implications on various parts of the system, including clock distribution, data synchronization between blocks, arbitration for shared resources in the network or at network boundaries, tuning of the optical devices on laser wavelengths, and thermal management at different granularity levels, from photonic devices to chip. Based on a system-level integration analysis, we identify trends on the best candidate depending on bandwidth requirements and number of stacked dies.","Technology Assessment of Silicon Interposers for  Manycore SoCs: Active, Passive, or Optical?  Yvain Thonnart, Mounir Zid  Univ. Grenoble Alpes, F-38000 Grenoble, France  CEA, LETI, MINATEC Campus, F-38054 Grenoble, France  Abstract—In this paper, the influence of the possible silicon  interposer 2.5D stacking strategies on the micro-architecture of  an interconnect topology is studied. We present case studies at  different chip scales, based on active CMOS interposers using  synchronous or asynchronous NoCs or point to point links,  passive metal interposers with DC lines or RF microstrip lines,  and optical interposers using multipoint links. We show that a  single physical link energy per bit is not a sufficient metric to  benchmark these strategies, as the choice leads to strong  implications on various parts of the system, including clock  distribution, data synchronization between blocks, arbitration  for shared resources in the network or at network boundaries,  tuning of the optical devices on laser wavelengths, and thermal  management at different granularity levels, from photonic  devices to chip. Based on a system-level integration analysis, we  identify trends on the best candidate depending on bandwidth  requirements and number of stacked dies.  Keywords—Networks-on-Chip, Interposers, Silicon Photonics   I. INTRODUCTION  The field of on-chip optical communications is getting a  strong interest from the international academic community and  industrial labs as a promising candidate to break through the  data bandwidth and power walls in future VLSI designs.  Thereby, several research groups have focused on  the  development of new integrated optical devices on silicon,  while others have pushed the Network-on-Chip paradigm to  new optical flavors, leading to new optical NoC architectures  and systems [1]. Nevertheless, the breakeven point more  conventional metal or CMOS interconnects is still unclear, and  companies still hesitate to bring this technology to market.  II. CASE STUDY  A. Technological and Architectural Options  1) CMOS  interposer with synchronous point-to-point  links: A complete graph is considered between all chiplet  interfaces. CMOS is used for wire buffering (with inverters),  link pipelining (FIFO logic with flip-flops and flow control),  and routing/arbitration (traffic is split on the interposer  towards long links according to routing, and arbitrated on the  interposer before reaching the chiplets). Hence, interfaces  present a single channel, with clock-domain crossings.   2) CMOS interposer with quasi-delay-insensitive (QDI)  asynchronous point-to-point links: This option is similar to the  previous one, but fully implemented in asynchronous logic  3) CMOS  interposer with  synchronous NoC:  the  interposer integrates a 2D-mesh topology using low-cost  synchronous NoC routers with source routing (SNoC). Clockdomain crossing is considered at 3D interfaces.  4) CMOS  interposer with QDI asynchronous NoC:  Similar to the preivous one using the ANoC technology [5].  5) Passive interposer with high-speed transmission lines:  the complete graph between chiplets is done using high -speed  serial links made of microstrip lines with low impedance, and  associated drivers and receivers.  6) Silicon Photonics interposer with shared lines per  receiver: of various optical NoC technologies, the simple  multipoint optical links are the earliest ones that could be  adopted insofar as they do not require in-network routing. We  considered a multiple-writer single-reader (MWSR) optical  topology (a line per receiver, shared between transmitters),  following principles of the Corona optical NoC [2].  Passive interposer with DC lines: This option offers less  connectivity than other solutions (either active or passive), but  is more natural for early neighbor to neighbor 2.5D connection.  It compares to the synchronous NoC for sizing and power, but  with routers inside chiplets, and hence with 4 times the  interface area, for all router ports (solution not considered).  B. Complexity scale  1) Single point-to-point link: The first considered structure  is a bidirectional link with a transmitter and a receiver at each  end. This structure connects only two chiplets.  2) Small-scale architecture: connectivity attainable with a  low-radix crossbar, or a few NoC routers, for 3-6 chiplets.  3) Larger-scale architecture: connectivity requires either  a high-radix crossbar or an advanced NoC topology. The  baseline for NoC is 2D-mesh, starting with 7-8 chiplets.  C. Assumptions  Our analysis assumes advanced technologies in chiplets  (28nm FDSOI), and more mature technology (65nm) CMOS  from STMicroelectronics for the active interposer. In all  technologies, a pitch of 40µm between copper pillars is  considered for 3D connections, as in [5]. Line widths and  pitches from internal data, pitch 20um for transmission lines, 4  cu-pillars per optical signal. Synchronous and asynchronous  interconnect designs come from CEA-Leti’s NoC technology.  in 2D and 3D from [5] and internal studies. Performance for  DC lines from [4], for transmission lines from [3].        The benchmark was done for general purpose parallel  computing applications, considering quad-core chiplets  running at 2GHz, with distributed L2 cache traffic between  chiplets (512 bit per cache line+1/8 protocol overhead). All  architectures are sized so as to preserve a chiplet bandwidth of  1 Byte/flop. Power analysis is done including all dynamic and  static costs in the interposer and the chiplets, and also thermal  tuning of resonant optical devices and laser source costs, with a  wall-plug efficiency of 20%.   III. RESULTS  1) Area: worst of logic area in chiplets, CuPi area, logic  area on interposer, reported to chiplet or inteproser area (Fig.  1). Point-to-point lines on CMOS interposer suffer from the  quadratic explosion in the number of links compared to the  NoCs. High-speed links, electrical or optical, have a much  lower footrprint, and challenge the NoC in terms of area,  staying around 10% of chiplet size.  2) Critical height: worst of 3D interface height reported to  chiplet height, and cumulative ligne height at interposer  bissection reported to interposer height (Fig. 2). As before, the  CMOS point-to-point lines, synchronous or asynchronous,  require the complete interposer height to connect all chiplets.  As for the high-speed transmission lines, they suffer from the  chiplet interface on the perimeter, due to pitch constraints.  3) Power consumption: total power consumption is the  sum of leakage and idle power (dynamic power of clock  distribution for CMOS, laser and thermal tuning for optics),  and useful switching power, reported to computing power  (Fig. 3). The optical NoC starts with a high static cost  compared to the other solutions, but as the number of chiplets  increase, the electrical NoC is sized accordingly to avoid  contention, at the cost of higher power consumption.  4) System  latency:  communication  set-up  latency  (pipeline, signaling…) plus packet size×throughput,  in  processor cycles (Fig. 4). Synchronous solutions have a huge  latency (up to 100 cycles) compared to other solutions due to  low-frequency pipelining on the interposer. Asynchronous  solutions have a latency similar to DC buffering across chip,  which is still twice as high as high-speed serial solutions.  IV. CONCLUSION  This analysis shows that the metal interposer is best for a  single chip-to-chip connection, such as a processor to highdensity memory connection as in the Hybrid Memory Cube,  but is not relevant for larger-scale integration. In that case, the  active interposer appears as the best solution up to 10-12  chiplets, but then suffers from NoC area and latency. The  optical interposer, even in a simple MWSR/SWMR solution,  offers better scalability, and allows for denser integration at a  similar power cost, for multiprocessors of 16 chiplets or more.  "
An OFDMA based RF interconnect for massive multi-core processors.,"A paradigm shift is apparent in Chip Multiprocessor (CMP) design, as the new performance bottleneck is becoming communication rather than computation. It is widely provisioned that number of cores on a single chip will reach thousands in a decade. Thus, new high rate interconnects such as optical or RF have been proposed by various researchers. However, these interconnect structures fail to provide essential requirements of heterogeneous on-chip traffic; bandwidth reconfigurability and broadcast support with a low complex design. In this paper we investigate the feasibility of a new Orthogonal Frequency Division Multiple Access (OFDMA) RF interconnect for the first time to the best of our knowledge. In addition we provide a novel dynamic bandwidth arbitration and modulation order selection policy, that is designed regarding the bimodal on-chip packets. The proposed approach decreases the average latency up to 3.5 times compared to conventional static approach.","An OFDMA Based RF Interconnect for Massive Multi-core Processors Eren Unlu∗ , Mohamad Hamieh† , Christophe Moy∗ , Myriam Ariaudo† , Yves Louet∗ , Frederic Drillet† , Alexandre Briere‡ , Lounis Zerioul† , Julien Denoulet‡ , Andrea Pinna‡ , Bertrand Granado‡ , Franc¸ ois P ˆecheux‡ , Cedric Duperrier† , Sebastien Quintanel† , Olivier Romain† and Emmanuelle Bourdel† ∗SUPELEC/IETR F-35576 Cesson-Sevigne Cedex, France Email: eren.unlu@supelec.fr †ETIS/ENSEA/UCP/UMR8051 ‡Sorbonne Universites, UPMC Univ Paris 06, UMR 7606, LIP6 F-95000, Cergy-Pontoise, France F-75005, Paris, France Abstract—A paradigm shift is apparent in Chip Multiprocessor (CMP) design, as the new performance bottleneck is becoming communication rather than computation. It is widely provisioned that number of cores on a single chip will reach thousands in a decade. Thus, new high rate interconnects such as optical or RF have been proposed by various researchers. However, these interconnect structures fail to provide essential requirements of heterogeneous on-chip trafﬁc; bandwidth reconﬁgurability and broadcast support with a low complex design. In this paper we investigate the feasibility of a new Orthogonal Frequency Division Multiple Access (OFDMA) RF interconnect for the ﬁrst time to the best of our knowledge. In addition we provide a novel dynamic bandwidth arbitration and modulation order selection policy, that is designed regarding the bimodal on-chip packets. The proposed approach decreases the average latency up to 3.5 times compared to conventional static approach. I . IN TRODUC T ION Global on-chip communication problem represents the main challenge for the future 1000-core CMPs. Various technologies have been already investigated to solve intra-chip communication problem which usually involve a hierarchical structure, where multiple cores are grouped to constitute clusters, a global serpentine waveguide for RF or optical inter-cluster communication and a local electrical network for the intracluster communication [1]. Current RF interconnects combine the advantages of providing low latency and compatibility with silicon technologies, however these interconnects require large number of redundant components for implementing orthogonal channels, e.g. local oscillators and ﬁlters etc. As a node’s RF front-end has to include these components, a reconﬁgurable multiband communication is not scalable for these conventional architectures [2]. On-chip trafﬁc is particularly spatio-temporally heterogeneous, which requires rapid dynamic bandwidth allocation. In order to overcome all these challenges mentioned above, we evaluate the feasibility of Orthogonal Frequency Division Multiple Access (OFDMA) as a new modulation scheme for on-chip RF-interconnects. This work is supported by French ANR under No. ANR-GUI-AAP-05 I I . ARCH I T EC TUR E O F TH E CMP AND OFDMA RF - IN TERCONNEC T We present a 3-level hierarchical 2048-core CMP architecture as in Fig. 1. 4 cores (each containing local caches), along with a local RAM and a Direct Memory Access (DMA) controller constitute a tile. In a tile, cores communicate via a crossbar and share the same address space. 16 tiles form a cluster, where intra-cluster communication is performed by a standard electric mesh network. Our OFDMA based RF interconnect serves as the inter-cluster communication system for 32 clusters. Each cluster has an Orthogonal Frequency Division Multiplexing (OFDM) transceiver block, which is connected to a certain node in mesh network with a router. OFDM is a modulation technique that encodes information on frequency domain by creating spectrally overlapped, lowerrate orthogonal channels, i.e. subcarriers, over one symbol period. Taking an Inverse Fast Fourier Transform (IFFT) of the constellation mapped data and serializing, produces an OFDM symbol, where information is carried on multiple orthogonal channels on the frequency domain. In reception part, the inverse of this procedure is applied to the signal: Fast Fourier Transform (FFT) to extract the information from frequency domain. On the other hand, OFDMA is an OFDM based multiple access scheme, where each user gets certain part of the bandwidth. This is performed by simply padding zero on the non-used subcarriers, before IFFT phase. Therefore, dynamic bandwidth arbitration between nodes in OFDMA is a non-complex and rapid procedure. Besides, as each node has to decode entire symbol, this communication scheme has an intrinsic broadcast nature, which is essential for on-chip trafﬁc. We propose to use a 20 GHz bandwidth RF front-end, made of a direct I/Q conversion with a ﬁxed 30 GHz local oscillator for each cluster. In order to exploit the rapid ﬂexibility and broadcast capability of our OFDMA interconnect, we develop a novel dynamic bandwidth allocation algorithm which concerns the bimodal lengths of cache coherency messages, while requiring zero control overhead. It was shown that between 70-80% of all packets are short control packets while the rest are cache line carrying long packets [3]. Therefore, providing a larger bandwidth for long packets is a convenient method to decrease average latency. We fragment the packets into 32 bits of ﬂowunits (ﬂits) as described in [4]. Each packet contains a header ﬂit, which contains the communication information such as source ID and packet type (short/long). We choose to have 64 bit short packets and 64 byte cache-line carrying long packets with 32 bits of header and tailer (2 and 18 ﬂits). A cluster, that decodes a header ﬂit of a long packet, adds the ID of that cluster into its register. In order to avoid starvation of any cluster, we only allow at most one cluster to transmit its 16 ﬂits payload on one symbol. We refer these 512 subcarriers as the payload channel. Also, to provide better fairness, we alternate the payload channel every symbol. Our protocol (simpliﬁed version) is illustrated in Fig.2 for a 4 clusters and 2-ﬂit payload example. In this ﬁgure, on the ﬁrst symbol, Cluster-0 (C-0) and Cluster-3 (C-3) send a short packet header (H(S)). There is no header for a long packet (H(L)), thus the register stack is empty. Hence the payload channel is inactive for the second symbol and each cluster can use its home channel. On the second symbol, C-0 and C-3 send the tailer of the short packets (T(S)) and C-1 sends a long packet header. Hence, it wins the ﬁrst place in the register stack. On the third symbol, each cluster checks their register and the position of the payload channel, which is on the left of the bandwidth, composed of C-0 and C-1’s home channels. At the end, C-0 aborts transmission on its home channel, in order to allow C-1 to send its 2-ﬂit payload. I I I . R E SU LT S In order to test our protocols, we have used OMNeT++, a discrete event simulator [5]. We assume Poisson distribution for a cluster’s packet generation at each symbol. Fig.3 shows our novel arbitration protocol’s up to 3.5x lower average latency with increasing injection rate compared to static allocation, equal share. IV. CONCLU S ION In this paper, we evaluate the feasibility of an OFDMA based RF-interconnect for CMPs, for the ﬁrst time to the best of our knowledge. OFDMA supports broadcasting and provide high spectral density, also with a rapid reconﬁgurable bandwidth thanks to its communication nature, without requiring intensive circuitry compared to conventional RF interconnects. In addition, we developed an arbitration policy for this new OFDMA interconnect, which does not waste bandwidth with extra communication overhead, while providing up to x3.5 lower average latency compared to static scheme. "
"Introduction to the special session on ""Silicon photonic interconnects - an illusion or a realistic solution?"".","The performance of a multiprocessor system-on-chip (MPSoC) is determined not only by the performance of its processing cores and memories, but also by how efficiently they collaborate with one another. It is the MPSoCs communication architecture which determines the collaboration efficiency. The migration towards MPSoCs is propelled by the shrinking feature sizes in each generation of process technology. On the one hand, smaller transistors allow for more processor cores and memories on a single chip and result in more on-chip computations as well as communications. On the other hand, reducing feature sizes makes on-chip communication more difficult. The International Roadmap for Semiconductors (ITRS) shows that the latency of metallic interconnects increases exponentially as feature sizes decrease. On-chip communication using metallic interconnects will need more than one clock cycle to send information from sources to destinations. Moreover, metallic interconnects consume a significant amount of power. Studies shows that global metallic interconnects could consume kilowatts of power to achieve required communication bandwidth by 2020 [1].","Introduction to the Special Session on “Silicon Photonic Interconnects: an Illusion or a Realistic Solution?” Jiang Xu1 , S´ebastien Le Beux2 , and Yvain Thonnart3 1 Hong Kong University of Science and Technology, Hong Kong — jiang.xu@ust.hk 2 Lyon Institute of Nanotechnology, Ecole Centrale de Lyon, France — sebastien.le-beux@ec-lyon.fr 3 CEA LETI Minatec, Grenoble, France — yvain.thonnart@cea.fr The performance of a multiprocessor system-on-chip (MPSoC) is determined not only by the performance of its processing cores and memories, but also by how eﬃciently they collaborate with one another. It is the MPSoCs communication architecture which determines the collaboration eﬃciency. The migration towards MPSoCs is propelled by the shrinking feature sizes in each generation of process technology. On the one hand, smaller transistors allow for more processor cores and memories on a single chip and result in more on-chip computations as well as communications. On the other hand, reducing feature sizes makes on-chip communication more diﬃcult. The International Roadmap for Semiconductors (ITRS) shows that the latency of metallic interconnects increases exponentially as feature sizes decrease. On-chip communication using metallic interconnects will need more than one clock cycle to send information from sources to destinations. Moreover, metallic interconnects consume a signiﬁcant amount of power. Studies shows that global metallic interconnects could consume kilowatts of power to achieve required communication bandwidth by 2020 [1]. Photonic technology brings low power, high bandwidth, and low latency options to communications. It has made enormous advances in recent years. Photonic technology has come to dominate communications in wide area, metropolitan area, and local area networks, and is making impressive progress in inter-rack, inter-board, and on-board communications. High-performance silicon photonic devices, such as optical waveguides and microresonators, have been demonstrated in CMOS-compatible fabrication processes, making them promising candidates to build high-performance low-power optical interconnect network. Realizing optical interconnects as a communication infrastructure for MPSoCs, it is necessary to investigate: 1) the feasibility and performance of optical interconnects compared to other possible technologies in addressing the communications in MPSoCs; 2) the guidelines and design requirements to realize optical interconnects, and 3) the performance and eﬃciency of optical interconnects in MPSoCs. This special session is devoted to discuss the most recent advances in optical interconnects, starting from the physical level all the way up to the system level. In particular, the ﬁrst paper will present comparisons among case studies at diﬀerent chip scales, based on active CMOS interposers using synchronous or asynchronous Networks-on-Chip (NoCs) or point to point links, passive metal interposers with DC lines or RF microstrip lines, and optical interposers using multipoint links. As a result, the authors identify trends on the best candidate depending on bandwidth requirements and the number of stacked dies. The second paper presents a cross-layer design methodology of the optical transport medium, ranging from the consideration of the predictability gap between optical NoC (ONoC) logic schemes and their physical implementations, up to architecture-level design issues. The paper provides an energy eﬃciency comparison between optical and electronic NoCs. The third paper presents a systematic study on crosstalk noise issues in optical interconnect networks. It considered both coherent and incoherent crosstalks and provide an open-source tool, called CLAP, to analyze optical crosstalk noise, lose, and signal-to-noise ratio (SNR) in optical interconnect networks. "
An efficient Network-on-Chip (NoC) based multicore platform for hierarchical parallel genetic algorithms.,"In this work, we propose a new Network-on-Chip (NoC) architecture for implementing the hierarchical parallel genetic algorithm (HPGA) on a multi-core System-on-Chip (SoC) platform. We first derive the speedup metric of an NoC architecture which directly maps the HPGA onto NoC in order to identify the main sources of performance bottlenecks. Specifically, it is observed that the speedup is mostly affected by the fixed bandwidth that a master processor can use and the low utilization of slave processor cores. Motivated by the theoretical analysis, we propose a new architecture with two multiplexing schemes, namely dynamic injection bandwidth multiplexing (DIBM) and time-division based island multiplexing (TDIM), to improve the speedup and reduce the hardware requirements. Moreover, a task-aware adaptive routing algorithm is designed for the proposed architecture, which can take advantage of the proposed multiplexing schemes to further reduce the hardware overhead. We demonstrate the benefits of our approach using the problem of protein folding prediction, which is a process of importance in biology. Our experimental results show that the proposed NoC architecture achieves up to 240X speedup compared to a single island design. The hardware cost is also reduced by 50% compared to a direct NoC-based HPGA implementation.","An Efﬁcient Network-on-Chip (NoC) based Multicore Platform for Hierarchical Parallel Genetic Algorithms Yuankun Xue(cid:3) , Zhiliang Qiany , Guopeng Weiz , Paul Bogdan(cid:3) , Chi-Ying Tsuiy , Radu Marculescuz (cid:3)University of Southern California, Los Angeles, CA, USA, {yuankunx,pbogdan}@usc.edu yHKUST, Hong Kong, China, {qianzl,eetsui}@ust.hk zCarnegie Mellon University, Pittsburgh, PA, USA, {danielwei,radum}@cmu.edu Abstract—In this work, we propose a new Network-on-Chip (NoC) architecture for implementing the hierarchical parallel genetic algorithm (HPGA) on a multi-core System-on-Chip (SoC) platform. We ﬁrst derive the speedup metric of an NoC architecture which directly maps the HPGA onto NoC in order to identify the main sources of performance bottlenecks. Speciﬁcally, it is observed that the speedup is mostly affected by the ﬁxed bandwidth that a master processor can use and the low utilization of slave processor cores. Motivated by the theoretical analysis, we propose a new architecture with two multiplexing schemes, namely dynamic injection bandwidth multiplexing (DIBM) and time-division based island multiplexing (TDIM), to improve the speedup and reduce the hardware requirements. Moreover, a task-aware adaptive routing algorithm is designed for the proposed architecture, which can take advantage of the proposed multiplexing schemes to further reduce the hardware overhead. We demonstrate the beneﬁts of our approach using the problem of protein folding prediction, which is a process of importance in biology. Our experimental results show that the proposed NoC architecture achieves up to 240X speedup compared to a single island design. The hardware cost is also reduced by 50% compared to a direct NoC-based HPGA implementation. INTRODUCT ION I . Evolutionary algorithms (EA) mimic the biological evolution and selection processes in nature and can solve complex optimization problems efﬁciently [4]. As a particular type of EA, the Genetic Algorithms (GA) are widely applied to search for an optimized solution in applications such as wireless communication, physics and bioinformatics [7]. In GA, the potential candidate solutions (i.e., feasible individuals) are encoded into arrays of bits and are evaluated based on a speciﬁc ﬁtness function [7]. Then, genetic operations like mutation and crossover are applied to the generation of individual populations [4]. The newly generated individuals, together with the stochastically selected elites from current population, form a new generation [13]. This evolution process repeats until the best ever ﬁtness score converges to an optimal value or a certain termination criteria is met [13]. In general, GAs can solve optimization problems over small spaces within an affordable time [3]. However, for many more complicated optimization problems, such as the protein folding prediction which requires to ﬁnd out the best protein conformation from numerous potential solutions [1], the computation time increases dramatically. For these problems, parallel Genetic Algorithms (PGA) have been proposed in order to take the advantage of multiple computing resources available on the system. The most straightforward implementation of PGA is to use massively parallel processors to calculate the ﬁtness value of each individual in a single population (i.e., global singlepopulation master-slave GAs) [3]. As shown in Fig. 1-a, in a master-slave based GA platform [3], the master processor distributes the individuals among the slave processors. Although simple in nature, in this architecture, the master processor becomes the performance bottleneck for the entire system. Moreover, the overall speedup performance is limited as only the master needs to collect the ﬁtness values in the current generation and mount genetic operations to produce a new generation. Consequently, a more efﬁcient PGA implementation consists of multiple master islands that can perform evolution separately and migrate the individuals occasionally among the islands (i.e., distributed PGA) [3]. In Fig.1-b, we show the architecture of a typical hierarchical distributed PGA (HPGA). In the upper level, multiple islands are used to do computations for different populations. The migrations also occur between any two islands to avoid the best ever ﬁtness score getting stuck in a local optimum trap. On the other hand, within each island, the master-slave level parallelization is implemented [3]. In this way, the HPGA can take the advantage of the existing parallel computing platforms such as the NoC-based Multi-processor System-on-Chip (MPSoC)[9] to perform the computations separately and accelerate the overall optimization process across the design space. In this work, we tackle the problem of efﬁcient implementation of the PGA on an NoC-based MPSoC. We ﬁrst propose a formal method to analyze the speedup gain of directly mapping the HPGA on NoC. Then, we identify two speedup bottlenecks of this architecture and propose two multiplexing schemes to mitigate these limitations. More speciﬁcally, a dynamic injection bandwidth multiplexing (DIBM) scheme is proposed to increase the effective channel resources that can be used by the master processor. A time-division based island multiplexing (TDMI) scheme is designed to map and share several islands of the proposed NoC platform dynamically; This improves the Fig. 1: a) The master-slave based parallelization of GA [3] b) The hierarchical parallel algorithm combing the multiplepopoluation coarse-grained GAs in the upper level and masterslave parallelization within each island [3] processors utilization and reduces the hardware requirements. Based on the newly proposed NoC architecture, a task-aware adaptive routing algorithm is used to further increase the resource utilization and reduce the latency. Finally, we use the protein folding prediction problem [14] as a case study and evaluate the performance of the proposed HPGA platform. Our experimental results shows a up to 240X speedup in protein folding prediction compared to a single-master-single-slave implementation. Moreover, the proposed multiplexing schemes signiﬁcantly reduces hardware overhead compared to a direct HPGA mapping. The reminder of the paper is organized as follows. In section II, we review related works. In section III, we analyze the speedup performance of the NoC-based HPGA platform and discuss the bottlenecks of the architecture. We then present the proposed multiplexing based NoC platform as well as the dynamic routing algorithm. The experimental results are discussed in section IV. Finally, section V concludes this work by numbering our main contributions. I I . R ELATED WORK For PGA implementation, most of the proposed platforms are based on GPU or computer-clusters. In [1], a computer cluster-system is used for a single population master-slave based PGA. However, as there is only one master processor in the system, the speedup tends to saturate when the number of processors increases. In [10], an island-based genetic algorithm is implemented on NVIDIA GPU-based architecture. In order to be compatible with the GPU architecture, a simpliﬁed unidirectional ring migration is used in CUDA software model; This compromises the speed of migration among populations. Recently, motivated by the high scalability of NoC, the PGAs can also be implemented efﬁciently on a single chip for many emerging applications in embedded computing. For instance in [5], a NoC-based MPSoC platform is developed for an islandbased parallel genetic algorithm. In this architecture, each processor corresponds to an island and evolve independently. However, their adopted topology allows only for migrations between neighboring processors in NoC. For each population, the evolution process occurs within a single processor. Hence, the computation efﬁciency can be further improved by using several slave processors for each population, i.e., master-slave level parellelization within each island. Fig. 2: The island-based NoC architecture for HPGA: a) algorithm b) An example which maps the island onto a 4 × 4 the island associated with master and slave processes in the region in NoC In NoC-based multicore systems, several time-divisionmultiplexing (TDM) schemes have been proposed in hard realtime systems. For instance in [8], TDM virtual circuits (VCs) have been proposed to meet the real time requirements of a certain application. In order to reduce the maximum delay, the TDM scheme allows two or more connections to share the usage of buffers and links in dedicated time slots. In [12], a TDM based circuit-switched NoC is presented. In order to achieve predictable worst case execution time, the authors propose an algorithm to determine the schedule for setting up all-to-all virtual circuits on top of NoC topologies. In contrast to these previous efforts on improving the quality-of-service (QoS) of speciﬁc ﬂows, we borrow the idea of TDM and propose two multiplexing schemes on NoC-based HPGA platform which target higher bandwidth and processor utilization. In this work, instead of running the HPGA programs in GPUs or CPU-based clusters, we propose an NoC-based customized platform for embedded applications. The speedup performance is enhanced by improving the bandwidth utilization through two new multiplexing schemes. A task-aware routing algorithm is also designed based on the proposed architecture. Towards this end, we make the following contributions: 1) We provide a quantitative analysis of the speedup performance of mapping the PGA onto NoC. This analysis identiﬁes two bottlenecks on overall performance. 2) Based on our analysis, we propose a new NoC architecture with two multiplexing schemes to improve the speedup and reduce the hardware requirements. 3) A task-aware routing algorithm is designed for efﬁcient communication among the processors under the proposed NoC architecture. 4) We demonstrate the beneﬁts of our approach via the protein folding problem and show that our solution achieves a signiﬁcant speedup while reducing the hardware overhead. I I I . PRO PO SED NOC -BA SED HPGA PLAT FORM In this section, we present the proposed HPGA platform based on a mesh NoC. The mesh topology is chosen due to its simplicity, layout and electrical efﬁciency. We start with a direct island-based architecture. Based on the analysis of achievable speedup gain, we then propose a new architecture with two multiplexing schemes to enhance the processor and bandwidth utilization. Finally, a task-aware routing algorithm is presented to further reduce the delivery latency between the processors at run time. A. Island-based HPGA-NoC and motivation for the proposed new NoC architecture The HPGA presented in Fig. 1-b can be implemented directly by mapping each population island onto a mesh NoC consisting of a master and a set of slave processors. As shown in Fig. 2, within each island, the messages exchanged among the master and slave processors (i.e., the distribution ﬂow and ﬁtness return ﬂow) are sent through the routers. In the upper level, the islands are interconnected as in Fig. 1-b to address the communication required among the master processors of different islands (i.e., the population migrations in HPGA). However, due to network trafﬁc conditions [2] this brute-force architecture has two drawbacks: 1) Limited injection bandwidth of the master processor: In HPGA, only one master can perform genetic operations on the entire population. So the workloads and trafﬁc ﬂows between the master and slave processors are quite uneven. Most of the time, master processor needs to produce a new generation, divide the individuals and distribute them to the slaves. For the island-based architecture presented in Fig. 2, the injection/ejection bandwidth is ﬁxed between the processors and the network (e.g., one ﬂit width per cycle). The master processor could only send one ﬂit to a speciﬁc slave processor every cycle. The rest of slaves need to wait for a certain interval Twait before they receive the chromosome information. So increasing the number of slave processors in the island, on one hand, reduces the overall ﬁtness calculation time in the population. On the other hand, it also signiﬁcantly increases the average Twait and reduces the slave processor utilization as a result. To explore the tradeoffs between the computation and waiting time, we deﬁne the network capacity C as the maximal number of slave processors that can be used in a single island, which is given by: C = arg max{∆G(N )=∆N > 0}; N ∈ N (1) where N is the number of slave processors; G(N ) is the overall speedup compared to the one master and one slave based design. From Eqn. 1, the speedup performance is saturated beyond C slave processors as there is no further improvement in ∆G(N ) = G(N + 1) − G(N ). Experimental results show that the island tends to be saturated if the average waiting time required to receive two individuals in a slave processor (Twait ) becomes comparable to Tsdelay . Here, Tsdelay refers to the average time seen by a master processor from sending out an individual to receive its ﬁtness. For a network with n slave processors, Twait can be represented as n ∗ Tinterval , where Tinterval is the time of transferring an individual in master processor and is given by: (2) In Eqn. 2, Tinterval consists of two parts: Tturnaround is the average inter-arrival time of two chromosomes which is determined by the master processor, while Lchm is the length of encoded chromosome packet in terms of ﬂits. Based on Eqn. 2, the network capacity C is derived as: Tinterval = Tturnaround + Lchm ; Fig. 3: The ratio of time that an island stays in the DIS and GA phases C ≈ n = Tsdelay =Tinterval (3) Next, we consider the effects of network size on the ﬁtness distribution time Tdis . Here, Tdis is the time from sending the whole population out to receive their ﬁtness values from slave processors. 1=Tdis represents the speed in the ﬁtness calculation process. We further assume Tcom represents the average communication time between a master and a slave processor and Tcalc is the average time required to calculate the ﬁtness value in a slave processor. Based on the size of population N , the network capacity C and the number of available slave processors NC in the network, Tdis can be computed as Eqns. 4-6, respectively. If N < NC , Tdis is calculated as: Tdis = N ∗ (Lchm + Tturnaround ) + 2 ∗ Tcom +Tcalc − Tturnaround (4) If NC < C and N ≥ NC , the calculation time in slaves dominates the distribution process and Tdis equals: Tdis = (N=NC + 1) ∗ Tcalc + (N mod(NC )) ∗ Tinterval (5) Otherwise, if the network size NC is larger than the capacity C , Tdis is determined by the transfer time of the master processor: Tdis = (N=NC ) ∗ NC ∗ Tinterval +2 ∗ Tcom − Tturnaround + Tcalc (6) The population size N in this is set to be large enough. For the single master and single slave design, NC equals to 1 and Eqn. 5 is used to obtain Tdis . On the other hand, the maximum distribution speed of the architecture in Fig. 2 is achieved if NC is larger than the network capacity C (Eqn. 6). We assume Tcom is much smaller than Tcalc and consequently Tsdelay = Tcalc + 2 ∗ Tcom ≈ Tcalc . Then the maximum speedup gain over the single slave architecture is the ratio of the two Tdis values: GN ≈ Tcalc =Tinterval ≈ C (7) From Eqn. 7, GN is inﬂuenced by the network capacity C . In the direct island-based HPGA platform, Tcalc and Tinterval are ﬁxed. As Tinterval is dictated by the bandwidth between the processors and network, the maximum speedup gain is also limited for this architecture. 2) Low slave processor utilization: In HPGA, the overall algorithm can be divided into two phases. During ﬁrst phase, Fig. 4: a) Proposed DIBM Router b) Concentration Degree the master distributes the task to slaves and waits for the returned ﬁtness values (named as DIS phase). After all the ﬁtness values are returned to the master, in the second step, genetic operations are performed in the master to select individuals for the next generation (named as GA phase). Our proﬁling results show that most of the slave processors remain idle during the second stage. Fig.3 shows the ratio between the time that an island stays in these two phases versus the number of slave processors. It is observed when the number of slaves approaches the network capacity, the time distribution of DIS and GA phases becomes comparable, which means most slave processors are seldom used at 50% of time. B. Dynamic Injection Bandwidth Multiplexing (DIBM) In an island-based NoC architecture, the speedup gain is mainly determined by the injection channel bandwidth of the given topology. Under a ﬁxed channel bandwidth between the master and the network, the maximum packet injection rate is always smaller than the reciprocal of Tinterval packet/cycle, which results in an increased Twait and limits the network capacity. In order to address this bottleneck, we propose to expand the injection bandwidth at run time. Fig. 4-a shows the proposed router architecture that enables the multiplexing of injection channel bandwidth. The DIBM scheme is motivated by the observation that, in HPGA, the utilization of injection channel and ejection channel is greatly unbalanced. In DIS stage, the master injects the chromosome packet at a much higher rate than the slaves can send back the ﬁtness ﬂits. This is because Tinterval is much smaller than the ﬁtness calculation time in the slave processor. Moreover, the one-ﬂit ﬁtness packet from the slave processor is much shorter than a chromosome packet sent by a master. Compared to the injection channel used by the master processor, most of the time, the injection channels of the slaves remain idle. Thus, it is possible to take advantage of the injection channels and increase the effective injection bandwidth in a multiplexed manner. Fig. 4-a shows the proposed router architecture to support the DIBM scheme. An input queue is used to maintain the information of the multiplexing ﬂow from the master. It is controlled by a local ﬁnite state machine (FSM). A built-in latch is used to store the injection data temporarily if the current injection channel is used by the packets from the other processors. After the occupied channel is released, the data stored in the latch will be pushed back into the queue. At run time, when the FIFO queue is not empty, the injection channel Fig. 5: Example of the TDIM schedule is exclusively used by the input queue. Otherwise, this channel is released to the local injection ﬂow. In order to evaluate the effects on bandwidth multiplexing, we use a parameter P to represent the concentration degree of the NoC architecture; This equals to the number of adjacent routers whose injection channels are multiplexed with the current node. Fig. 4-b shows how the routers are organized under different P values. Similar to the Tdis analysis in an island-based NoC, when N < NC , Tdis is calculated as: Tdis = N P ∗ (Lchm + Tturnaround ) + 2 ∗ Tcom +Tcalc − Tturnaround (8) On the other hand, if N > NC and NC < Cdim , Tdis is still dominated by the slave processors. Hence, Tdis can be calculated as in Eqn. 5. Different from Eqn. 1, in the derivation of the network capacity Cdim for DIBM, the waiting time Twait dim need to account for a group of P slave processors rather than a single one. Thus, Cdim can be approximated as: (9) Finally, when NC ≥ Cdim , Tdis is calculated as: Cdim ≈ n ∗ P = P ∗ Tcalc =Tinterval N P Tdis = ∗ Tinterval + 2 ∗ Tcom − Tturnaround + Tcalc (10) Gf it ≈ P ∗ Tcalc =Tinterval ≈ P ∗ C Similar to (7), the overall speedup could be calculated as: (11) Based on Eqn. 11 and 9, by multiplexing the injection bandwidth among the processors with a degree of P , the achievable speedup gain for ﬁtness calculation also increases by P times. C. Time-division Island Multiplexing (TIDM) The TIDM scheme is proposed to address the low slave processor utilization observed in the island-based NoC architecture. In particular, since the slave processors remain idle during a large portion of time, these free time slots can be used by other islands. We employ a time-division based task mapping algorithm which allows multiple islands to share the same resources and perform independently without interrupting each other. Multiple masters are located next to each other. As shown in Fig. 5, they are scheduled to take turns to enter the DIS phase. In Fig. 5, three islands are mapped onto the same network where the ratio between Tdis and Tproc is assumed to be 1=2. Here, Tproc refers to the time needed to produce a new generation during the GA phase. In this case, after each island leaves the DIS stage, the hardware resources are released to the scheduled next neighboring island. In this way, the idle time of slaves in the island-based architecture is shared by the other two islands at run time. Compared to a 3-island-based NoC design, this requires only 1/3 hardware overhead. In addition, since every time when an island enters the DIS phase, the slave processors are occupied exclusively, no extra control or computation overhead is introduced due to the mutual interruption. In the TIDM scheme, we can derive the maximum number of multiplexed islands as a function of the population size N , the concentration degree P and the network size NC as: K (N ; P ; NC ) = ⌊ Tproc (N ) Tdis (N ; P ; NC ) ⌋ + 1; In order to evaluate the hardware requirements of TDIM, we deﬁne Ni as the total number of islands that are required in the HPGA. Ntdim and Nmul represent the number of slave processors per island for the proposed TDIM and island-based NoC architectures, respectively. We evaluate hardware cost by comparing the total number of slave processors Nall in these different architectures. The hardware reduction factor RH representing the ratio of Nall in the TDIM architecture and the island-based architecture, which is given by: (12) RH (N ; P ; NC ) = 1 K (N ; P ; NC ) ∗ Ntdim Nmul ; (13) Here we distinguish two cases: i) If Ntdim = Nmul , then Eqn. 13 yields: RH = 1 K (N ; P ; NC ) (14) ii) If Ntdim = M ∗ Nmul = M ∗ C = M ∗ P which means the total number processor cores required in proposed TDIM scheme is K (N ; P ; NC ) times smaller. where 1 < M ≤ P , then from Eqn. 11, the proposed TDIM combined DIBM scheme can achieve M times speedup in the ﬁtness calculation. The RH under this case is given by: (cid:0)1 ∗ Cdim , M K (N ; P ; NC ) RH = (15) Eqn. 15 indicates that as long as M is smaller than K , the proposed TDIM scheme can achieve both performance improvement and area reduction compared to the island-based baseline design. Our simulation results in Section IV consistently demonstrate these beneﬁts by combining the TDIM and DIBM schemes together. D. Task-Aware Adaptive Routing In the proposed TDIM scheme, in order to further improve the utilization of slave processors, the DIS phase of two different islands can be overlapped such that more islands can share same physical resources. This overlapped time is represented as ∆tovp . Then, Eqn. 12 can be re-written as: K = ⌊ Tproc Tdis (∆tovp ) − ∆tovp ⌋ + 1; (16) Fig. 6: Rounting-Interception and Diversion To avoid the extra delays introduced when two multiplexed masters distribute their individuals simultaneously, a taskaware routing algorithm is proposed to support packets (individuals) to change their predeﬁned destination dynamically. In the routing, a chromosome packet generated from the master processor is ﬁrst assigned with a destination. Then, the packet is sent under a deadlock-free XY routing algorithm. Upon arriving at each router, the header ﬂit checks the router status to determine whether the slave processor associated with current router is available. If it is free, the header ﬂit sends a forwarding request to the router. Because there may be several packets contending for the same slave processor, an arbiter is used to resolve the conﬂicts. After being granted the access to the current slave processor, the packet enters the current slave directly by ﬂushing its original destination. In the router, an occupation ﬂag is asserted to indicate the status changed from ”available” to ”busy”. On the other hand, if the current slave processor is not free, the packet will proceed towards its original destination. As shown in Fig. 6, the dynamic dropping policy may bring the case that a head ﬂit arrives at its original destination whereas this slave processor has already been occupied by another individual. Under this situation, a deﬂection routing decision is made based on the evaluation of the current slave status as well as the network trafﬁc conditions (Eqn. 17): jXdim(cid:0)sj∑ k=1 (1 − P<(s;t);j> ) + P<(s;t);k> (18) Pk ∗ k (17) k(cid:0)1∏ Twait(s;t) > 2 ∗ E (ls;t ) = Pk = P {n = k} = j=1 In Eqn. 17, Twait(s;t) is the time needed to wait for the current slave at (s; t) to be available and indicated by a counter in each router. E (ls;t ) is the average distance to ﬁnd a free slave from (s; t) along X/Y dimension. Pk is probability the current individual move to a node k-hops from the current node which has a free slave processor. In Eqn. 18, P<(s;t);j> is probability for a ﬂit at (s; t) to ﬁnd a free slave at a node which is j -hops away. In this work, we use longest Manhattan distance within the mesh to estimate E (ls;t ) empirically. If Eqn. 17 is met, the destination of this ﬂit will be changed to an end-node alongside a preferred deﬂected dimension. Otherwise, it will repeatly checking Eqn. 17 and requesting the current router. In short, compared to other taskaware algorithms like [11] with unchangeable destination, the proposed routing algorithm could adaptively access to available resources based on current network status. Algorithm 1 Task-aware Routing Algorithm Input: Current node cur; Destination node dst; Set of candidate output directions Pc ; Occupation status for current slave Scur ; Diverted times Tdivert ; Predeﬁned maximal diverted times Tmax div ;Remaining calculation time, Trm calc ;Calculation time for a single individual, Tcalc ; Metric function: M (cur; Trm calc ): Boolean decision for diversion. Preferred diversion dimension, D ; Output direction Po ∈ Pc ; Updated destination dst Output: 1: if Scur == 0 then 2: Po = S elect out port(cur; cur); 3: Scur = 1; 4: Trm calc+ = Tcalc 5: else 7: end if 10: else 6: Ptemp = S elect out port(cur; dst); 8: if Ptemp !=S elect out port(cur; cur) then 9: Po = Ptemp ; then if M (cur; Trm calc ) == true ∧ (Tdivert < Tmax div ) dst=C redit based select node(cur; D); Po = S elect out port(cur; dst) Tdivert + +; D = (D == 0 ? 1 : 0) else Po = S elect out port(cur; cur); 11: 12: 13: 14: 15: 16: 17: 18: end if 19: end if 20: return Po , dst IV. S IMU LAT ION AND R E SU LT S A. Simulation Setup We implement a cycle-accurate NoC simulator in C++ and apply the proposed platform to solve the protein folding prediction problem based on 3D Hydrophobic-polar side-chain (HPSC) based protein model [1]. We use a ﬁtness function similar to [1] to evaluate the energy of each individual’s conformation. In the GA simulations for the protein folding, 2000 generations are run with a population size of 2400. The crossover and mutation rate are 80% and 10%, respectively. Moreover, 10% elites are kept directly to the next generation. The migration among the master processors happen every 40 generations. We compare the proposed NoC platform with the island-based HPGA architecture described in Section III.A (named as naive mesh NoC) under different size of the network ranging from 2 × 2 to 16 × 16 . Throughout the experiments, we assume that each input channel has a buffer depth of 4 ﬂits and 4 virtual channels. B. Comparison of network capacity extension and speedup In this experiment, we compare the speedup gain, the network capacity and the speedup efﬁciency of both convenFig. 7: Comparisons of speedup in ﬁtness calculation tional NoC [14] and the proposed architecture using DIBM. The concentration degree changes from P = 3 to P = 9 to evaluate the effect of multiplexing degree on the overall performance. The prediction results obtained from our analysis (in Section III) are compared against experimental ones in Fig. 7. From Eqn. 9, it is expected that the capacity of the proposed architecture with DIBM is P times higher than a naive mesh design. Moreover, in the proposed architecture, the speedup saturates in a network with P times larger size. Fig. 7 shows the simulation results of the ﬁtness calculation speedup. The black line indicates the linear speedup with increasing number of slave processors. The dashed lines are the predicted upper bounds of the speedup gain in ﬁtness calculation. From Fig. 7, it can be observed the saturation appears in a island-based mesh NoC when the number of slave processor exceeds 25. On the other hand, for the NoC with DIBM, the maximum number of slave cores increases to 75, 120, and 225, respectively, which yield a ceiling speedup at 75X , 107X and 206X . The comparison of core efﬁciency is shown in Fig. 9. The green line with triangle markers shows the efﬁciency in terms of ﬁtness calculation. For conventional architecture, the efﬁciency drops dramatically after 25 cores. On the other hand, the efﬁciency for DIBM with P = 3; 5; 9 remain almost 1 before their network capacity of 75, 120 and 225 is reached. Moreover, to evaluate the realistic hardware speedup, we have prototyped the proposed DIBM-based platform in fully synthesizable Verilog using P=3 as an example. Simulations are done with Xilinx Virtex-6 LX760 FPGA simulator under different network sizes ranging from 2x2 to 7x7 due to hardware limitations. Results shown in Fig. 8 are consistent with the experiments done with a NoC simulator whereas performance degradation could be observed due to latency introduced by extra control. C. Evaluation of time-division island multiplexing Next, the hardware overhead reduction is evaluated. We ﬁrst measure the maximal multiplexing degree K on the network. Fig. 10 shows the maximal number of islands (i.e., K ) under different mesh and population size. As shown in Fig. 8: Comparisons of hardware speedup Fig. 11: Speedup for different numbers of multiplexed islands Fig. 9: Comparisons of the slave core utilization the ﬁgure, it can be observed that with the increasing of the concentration degree P , the maximal number of islands also increases for different slave and population sizes. We also evaluate the hardware reduction when building a 24-island HPGA on NoC. We normalize the hardware overhead of the proposed architecture to that of the naive mesh NoC architecture described in section III.A. As shown in Fig. 13, the area cost of the conventional mesh NoC grows linearly Fig. 10: Maximal number of islands that can be multiplexed under TDIM Fig. 12: Comparisons of ﬁtness calculation delay because each island occupies intra-island resources exclusively. In contrast, if TDIM is adopted, the blue line shows a 50% hardware reduction. Moreover, the TDIM scheme can be combined with DIBM, which allows more logic islands to be mapped on the same physical network. Finally, the possible speedup degradation due to the introduction of TDIM is evaluated. To illustrate the degradation, we adopt an 8 × 8 mesh NoC example. The concentration degree P is set to be 5. TDIM is employed in both designs. The number of islands mapped to this 64-core NoC varies from 2 to 7. Fig. 11 shows that the speedup degrades as more islands are mapped to the same physical network. For the naive island-based mesh NoC, the speedup drops ealier since it offers less ﬁtness calculation speedup gain compared to the proposed architecture with DIBM. D. Routing Evaluation We compare the proposed routing algorithm with dimension-ordered XY and minimal adaptive routing in [6] under different task overlap ratios. The overlap ratio is the percentage of time that the DIS phases of two logic islands overlap with each other because they are mapped onto the same physical region. As shown in Fig. 12, the adaptive routing achieves better delay performance compared to the static routing algorithms. The proposed task-aware routing implement HPGA for protein folding analysis. Simulation results show our proposed architecture, together with the multiplexing scheme and routing algorithm, achieve a ﬁtness calculation speedup of 206X , an overall speedup of 240X and improved quality of solution. Compared to the direct islandbased HPGA implementation, over 50% of hardware overhead is reduced due to the proposed multiplexing schemes. V I . ACKNOW LEDG EM EN T The authors are thankful to anonymous reviewers for their valuable feedback. RM acknowledges the support for this work by US National Science Foundation (NSF) under Grant CNS-1128624. PB acknowledges the support by US National Science Foundation (NSF) under Grant 1331610. "
Design of a low power NoC router using Marching Memory Through type.,"Power consumption of Network-on-Chip (NoC) is becoming more important in many core processors. Input buffers utilized in routers consume a significant part of the total power of NoCs. In order to reduce this power consumption, a novel power efficient memory called Marching Memory Through type (MMTH) is introduced. By connecting transparent latches in tandem, MMTH achieves high speed operation with a low power consumption. MMTH, however, requires a certain overhead at read operation, and hence we propose a latency reduction scheme based on the look-ahead routing. The proposed router was designed in Renesas's 40nm process and compared with a standard router using conventional register-based FIFOs in terms of the network performance, application performance, and power consumption. The result of evaluation shows that the proposed router reduces the power consumption by 42.4% on average at 2GHz and the expense of only 0.5-2.0% performance overhead.","Design of a Low Power NoC Router using Marching Memory Through type Ryota Yasudo† , Takahiro Kagami† , Hideharu Amano† , Yasunobu Nakase, Masashi Watanabe‡ , Tsukasa Oishi‡ , Toru Shimizu‡ , and Tadao Nakamura† †Keio University ‡ Renesas Electronics Yokohama, Japan 223-8522 Tokyo, Japan 100-0004 E-mail: marching@am.ics.keio.ac.jp Abstract—Power consumption of Network-on-Chip (NoC) is becoming more important in many core processors. Input buffers utilized in routers consume a signiﬁcant part of the total power of NoCs. In order to reduce this power consumption, a novel power efﬁcient memory called Marching Memory Through type (MMTH) is introduced. By connecting transparent latches in tandem, MMTH achieves high speed operation with a low power consumption. MMTH, however, requires a certain overhead at read operation, and hence we propose a latency reduction scheme based on the look-ahead routing. The proposed router was designed in Renesas’s 40nm process and compared with a standard router using conventional register-based FIFOs in terms of the network performance, application performance, and power consumption. The result of evaluation shows that the proposed router reduces the power consumption by 42.4% on average at 2GHz and the expense of only 0.5-2.0% performance overhead. I . INTRODUC T ION NoC (Network-on-Chip) [1], [2], [3] is a key component of recent multi-core and many-core CMPs (Chip Multiprocessors) as well as heterogeneous SoCs (Systems-on-a-Chip) [4], and its design is a crucial factor for system performance and cost of the chip. Unlike the traditional bus connected systems, many IP (Intellectual Property) cores on a single chip can be connected through routers which transfer packets. It provides high communication bandwidth, parallelism, and scalability. Since these networks are facing tight delay requirements, prior designs and architecture studies are heavily performancedriven, aiming at lowering network latency. Since the operational frequency of NoCs reaches a few GHz to achieve the low latency data transfer, the power consumption of NoC sometimes occupies a considerable part of the system; for example, in MIT 16-core RAW CMP [5], Intel 80core Tera FLOPS processor [6] and Intel 48-core SCC [7], it occupies 36%, 28% and 10% of each total power, respectively. This compels us to review network microarchitecture from a power-driven perspective. Since input buffers provided in the router are the dominant part of the power consumption, introducing a low power FIFO concept is efﬁcient to reduce the total power of the router. Here, a novel power efﬁcient “through type” of the marching memory [8] is proposed and utilized in the router of an NoC. A novel power efﬁcient buffer memory called Marching Memory Through type (MMTH) is proposed for buffers in routers of NoCs. Marching Memory is a memory with high speed marching of data/information stored in the memory [8]. MMTH consists of transparent latches connected in tandem, and works as a FIFO with high operational frequency yet low consuming power. The problem of MMTH is that it requires some time delay of signals as a read latency. In order to reduce it, a new mechanism based on the look-ahead technique is proposed. A network latency overhead is reduced to only one clock cycle per packet. The remaining part of this paper is organized as follows: Section II shows the motivation to this work and reviews related work brieﬂy. Section III describes Marching Memory Through type, which is utilized in the input buffers. Section IV describes our proposed router microarchitecture. In Section V, evaluation results about power consumption and performance are reported. Finally, Section VI concludes this paper. I I . MOT IVAT ION AND R ELATED WORK A. Input buffers in routers Compared with off-chip networks, on-chip networks are cost sensitive, and hence buffers reduction is important [2]. As shown in Figure 1, common router provides several input ports each of which provides packet buffers. Since recent CMPs require several virtual channels for types of packets, a few set of buffers are required for each input port. Buffers are implemented with a set of registers or small 2-port memory. In order to follow recent CMPs operating with a few GHz clock, NoCs are also required to work with the same clock signal. Thus, high speed buffer with registers are utilized rather than 2-port memory. It is reported that approximately 46% of the power and 15% of area of a router are occupied by the input buffers [9]. Moreover, a leakage power modeling [10] shows that the largest leakage power consumer in a router is input buffers, and the analysis of power consumption [11] clariﬁes dynamic power of the buffer is also high, and it increases rapidly as the trafﬁc of packets increases. From the above, our design goal is to reduce the power consumption of buffers as well as maximizing the performance of network. STT-MRAM is required in this design. In fact, an incoming ﬂit is ﬁrst written into the SRAM buffer quickly and subsequently migrated to STT-MRAM slowly. Besides, this scheme needs to trigger the migration of a ﬂit on the basis of the estimated network load per VC in the router so as to reduce wasteful power at low loads when STT-MRAM is unnecessary. This method needs both a hybrid design and a migration scheme as well as new process technology, while they are unnecessary for our proposal. I I I . MARCH ING M EMORY THROUGH TY P E A. Marching Memory Marching Memory (MM) [8] is invented as a novel memory device that integrates all memory including cache memory and register ﬁles into a single unit and can avoid the memory bottleneck [20] by accessing with the same clock cycle of the CPU. MM uses DRAM based memory cell technology but reorganizes the structure that consists of columns and rows of the DRAM. The basic idea is to create a memory structure wherein the data is scheduled to arrive at a ﬁxed physical memory port for immediate use by the processor’s functional units. Data are shifted to the CPU synchronized with the clock and come to the processor rather than the CPU searching randomly for the data. Since only data marched to the output port are accessed, high speed access without the energy for accessing bit lines of the DRAM using precharge and sensing can be done. B. Concept of MMTH MMTH (Marching Memory Through type) is a novel buffer memory based on an idea from MM that data march along the circuit. It is mainly designed for storing streaming data for media processing and buffers for communication including NoC instead of main memory or cache memory. The structure and target of MMTH are completely different from the original MM, however, it is an application example of MM concept. Although MM is an epoch-making invention, it needs a new DRAM based design technology, and can not be used in CMPs immediately. On the other hand, MMTH is designed so that it can be implemented with the current common CMOS technology unlike the original MM. The clear distinction of MMTH is that a kind of asynchronous circuit is used. This circuit, which we assume a black box for the present, is sandwiched between input/output ports as shown in Figure 2. By going through the circuit, the written data is moved from the input port to the output port. After we write data to the input port, the data goes through the asynchronous circuit and we can read the data from the output port in order. MMTH is appropriate for input buffers in an NoC router because it is a memory of small capacity with low power at high speed whose data structure is a FIFO structure. C. Behavior of MMTH The detailed behavior of MMTH including the operating of an asynchronous circuit is described here. Basically, the Fig. 1: Router microarchitecture B. Related Work Reduction of the power consumed in buffers is tried from various aspects. Buffer-less deﬂection routers which remove input buffers completely [12] are proposed to reduce router power. In buffer-less schemes, conﬂicting packets or ﬂits are retransmitted by deﬂecting them to a free output port. By controlling which ﬂits are deﬂected, a buffer-less deﬂection router can ensure that every trafﬁc is eventually delivered. Its routing scheme is based on “hot-potato” routing [13], which is originally proposed for off-chip networks. Large power savings are reported compared with conventional buffered networks. For example, BLESS [12] and CHIPPER [14] reduce power by 39% and 55%, respectively. At high network load, however, deﬂection routing degrades performance because of frequent deﬂections caused by many conﬂicting packets. More realistic optimization is to decrease the number of input buffers rather than removing them. A centralized buffer router [15] with elastic buffers on the link [16] is proposed to decouple the required buffer space per router. At heavy loads, the centralized buffer is used, and at light loads, it is power gated and bypassed. ViChaR [17] and Reconﬁgurable routers [18], where the buffer slots are dynamically allocated, are proposed to increase the buffer efﬁciency in a router. The depth of each buffer word can be reconﬁgured at run time according to the trafﬁc pattern. Above sophisticated buffer management methods achieves efﬁcient usage of the total buffer based on the observation of a trafﬁc load and the current situation of the router. However, they introduce complexity to both the structure and control of buffers as well as a certain performance degradation. Our approach is to reduce the power consumption of the buffer by introducing novel buffer memory without using complicated buffer structure and its management. A similar approach is taken by using MRAM technologies. A hybrid buffer design with STT-MRAM (Spin Torque Transfer Magnetic RAM) [19] is proposed to reduce the bottleneck through increasing throughput. Since STT-MRAM is a high-density memory, area budget can be used efﬁciently. This memory, however, requires long latency and high power consumption in write operations. Therefore a hybrid design of input buffers using both SRAM and STT-MRAM is proposed to hide the long write latency. A migration scheme between SRAM and Fig. 2: Overview of Marching Memory Through type (a) Write (b) Read (c) Write & Read Fig. 3: Behavior of Marching Memory Through type asynchronous circuit consists of tandem connected transparent latches. A set of latches whose size correspond to the size of a ﬂit or word composes a column. Figure 3 illustrates the operation of MMTH. Each rectangle represents a column. Wpointer and R-pointer control the movement of the data and both pointers initially indicate the right most position. For write operation, the written data are directly transferred to the position where W-pointer indicates with a clock cycle as shown in Figure 3a, and W-pointer moves one to the left at the next clock cycle. In other words, W-pointer points the terminal column of the transition and the data are written from the input port to the column pointed by W-pointer. For read operation, the data in the column pointed by R-pointer is transferred to the output port as shown in Figure 3b, and then R-pointer moves one to the left. That is to say, R-pointer indicates the starting point of the transition. Both write and read operation can be done in the same clock cycle as shown in Figure 3c. In this way, a FIFO concept is achieved by using an asynchronous circuit. The positions of the two pointers also determine the state of MMTH. If W-pointer moves to the left most position, the memory becomes full and the data cannot be written anymore. The data pointed out by R-pointer can only be read out. On the contrary, when R-pointer indicates the same position as W-pointer, it becomes empty. When both W-pointer and RFig. 4: Memory cell of MMTH pointer reach the left most position, the MMTH is reset by an external signal and both pointers return to the right most position. However, the read operation requires some delay for transferring data to the output port. The delay depends on the operational clock frequency and size of the memory. For example, when eight-depth MMTH works at 2GHz as we assume, one clock cycle is needed to read the data. D. Structure of MMTH The memory cell of MMTH is composed of a transparent latch with a transmission gate as shown in Figure 4, where T and TB are a control signal and its inverted one, respectively. If T is asserted, the data goes through the memory cell, otherwise the data is stored in the cell. This memory cell structure reduces power and area, since a simple transparent latch is used and a local clock signal is unnecessary. W-pointer and R-pointer are provided for each cell so as to control T. Figure 5 shows pointer generation circuits. The circuit shown in Figure 5a generates W-pointer. The diagram of Flip-ﬂop used in the circuits is shown in Figure 5b. When WP<i> is asserted, column<i> lets an input data through. Initially, all WPs are asserted by a reset signal WRST, since a written data goes through all columns at ﬁrst. DL, a delay element, makes a column-by-column wiring delay. When WENIN, a write enable signal, is asserted, WCLK moves at the same speed as the written data. First, when WCLK reaches the right most Flip-ﬂop, WP<0> is negated. When the next data are written, thus, the WENIN is asserted, the WP<1> is negated after a certain delay as the data goes through. The WPs are negated from right to left with the same manner as shown in Figure 5c, and ﬁnally when WP<7> is negated, the buffer becomes full. Almost the same structure is utilized for R-Pointer except the 1s and 0s are inverted as shown in Figure 5c. When WP<7> is negated and RP<7> is asserted, the reset signals: WRST and RRST are asserted to initialize the MMTH again. Note that both W-pointer and R-pointer require a considerable amount of hardware, they are shared by a buffer memory column with a certain bit-width, for example 64bits, here. E. Power Consumption related to Bit Change Rate If the frequency of read/write operations is the same, a standard FIFO consumes almost the constant power regardless of input data pattern, however MMTH has the unique characteristic that a power consumption depends also on the input data contents. This is caused by the behavior of MMTH (a) W-Pointer generation circuit (b) Flip-ﬂop Fig. 5: Pointer generation circuits (c) Transition of pointer mentioned above. Because the data is written to a number of columns from the input port to the column that W-pointer indicates, it is written on the top of preceding data necessarily. If the same bit is continuously written, almost no power except the controllers for W-pointer and R-pointer is required. Here the probability of bit change is called BCR (Bit Change Rate). The power consumption of MMTH linearly changes in response to BCR. Note that the position of pointers when data are written does not affect the power consumption because all data go through the same number of cells in total. IV. THE ROU TER U S ING MMTH A. Baseline Router using register-based FIFOs Figure 1 in Section I sketches a standard input-buffered virtual cut-through router for the 2-dimensional mesh network using virtual-channel ﬂow control [21]. Here, this standard router structure is adopted as the baseline for our design. It provides ﬁve input and output physical channels (four for neighboring routers and one for the processor core), a 5 × 5 crossbar switch, and a round-robin arbiter that allocates a pair of output virtual and physical channels for each incoming packet. A crossbar switch consists of ﬁve 5-to-1 multiplexers, each of which is controlled by a select signal from the arbiter. At each input physical channel, two input buffers are organized as separate FIFO queues for each virtual channel. For these FIFO queues, the baseline router and the proposed router use the standard circular buffers composed of a bunch of ﬂip-ﬂops and MMTH respectively. Besides, an input physical channel has a routing computation unit and a multiplexer that selects only a single output from two virtual channels. Generally, ﬁve steps, Routing Computation (RC), Virtual channel Allocation (VA), Switch Allocation (SA), Switch Traversal (ST) and Link Traversal (LT) are required to transfer a packet through a router. The RC and VA stages are required only for the header ﬂit. The simplest implementation is making the pipeline which processes each step in a clock cycle with a dedicated stage. In NoCs, the basic ﬁve-stage pipeline is rarely used since it requires too large latency. A speculative technique [22] and a look-ahead routing scheme [23] are introduced in order to reduce the number of pipeline stages in a router. Using the speculative technique, VA and SA are performed in parallel. Provided that the VA fails, SA will be ignored even if it succeeds. The look-ahead routing employs Next Routing Computation (NRC) instead of RC. In NRC stage, where the output port of a packet is computed, one hop in advance, and consequently, NRC and VA/SA can be executed in parallel. The ﬁrst routing is computed beforehand by a source node. Thus, a low latency router with a 3-stage pipeline shown in Figure 6a, which is illustrated from a router’s viewpoint, can be designed if standard register-based FIFO is used for the input buffer. B. Proposed Router using MMTH A router using MMTH, whose architecture is almost the same as the above baseline router, is designed in a similar fashion. However, since MMTH needs an extra clock for a read operation, Buffer Read (BR) stage is necessary for the (a) A traditional router (b) A router using MMTH Fig. 6: Pipeline structures TABLE I: A type of a ﬂit Type Value None Header Body Pre-header 00 01 10 11 router with MMTH as shown in Figure 6b, and the latency of the packet transfer is stretched if we design naively a router with MMTH. Since the latency of an NoC is directly related to the pipeline depth in a router, it is a serious problem. Thus, a new router design inspired by Flit-Reservation Flow Control [24] for MMTH is proposed in order to avoid the extra clock delay. Figure 7 illustrates the stage reduction by applying the look-ahead routing scheme from a header ﬂit’s viewpoint. The result of NRC is ﬁlled in a header ﬂit in the baseline router. In the proposed router, however, the routing information for the next router computed in the NRC stage is ﬁlled in an additional temporary ﬂit to transmit only the routing information including the result of NRC and the destination node. This ﬂit is called pre-header ﬂit in the sense that it is antecedent to a header ﬂit. It is a proxy for header ﬂit stored in the buffer with read latency rather than the reservation. As a result, a type of a ﬂit, which is speciﬁed in the most signiﬁcant two bits, is assumed as shown in Table I. The pre-header ﬂit bypasses input buffers and is directly forwarded to the next router as shown in Figure 8. Since bypassing ﬂit does not need BR stage, a clock earlier the pre-header ﬂit arrives at the next router, and consequently, the next router can start VA, SA and NRC during the LT of the ﬁrst header ﬂit of the packet. By using this design, the overhead becomes only one clock cycle in the destination router. Their respective latencies are formulated as follows: Lbaseline=3H Lnaive=4H Lproposed=3H + 1, (1) (2) (3) where H is the number of hops from source to destination. When H is 3 as shown in Figure 7, the latency is 9 cycles, 12 cycles and 10 cycles, respectively. It is important to note that the overhead of the proposed router (i.e. Lproposed −Lbaseline ) is always one clock cycle regardless of the number of hops. Fig. 7: The latency reduction applying the look-ahead technique Fig. 8: The latency reduction scheme This means, the inﬂuence of performance overhead is constant even if a number of hops increases. NoCs with 1024 nodes will become realistic around year 2020 [25], and if trafﬁc between distant nodes are increased in such routers, the inﬂuence of the overhead may become small. We will see the inﬂuence on real applications in Section V. Several additional external signals are also needed to control MMTH. A reset signal is important to use MMTH since the buffer reset is required when a packet is stored in the buffer. Since the whole ﬂit of a packet is transmitted consecutively in a virtual cut-through router, a reset signal should just be asserted after ﬁnishing transmitting a packet. This does not affect performance because the duration of reset is only one clock cycle. In BR stage, an invalid signal is required to invalidate data, since the output during BR stage is not used. This signal is asserted in response to a stage of each virtual channel. V. EVALUAT ION To understand the impact of our proposal, we evaluate the baseline router and our proposed router in terms of the performance and power consumption by using a full system simulator and RTL models. A. Performance overhead We develop three different router models in a platform, GEM5 full system simulator [26], to investigate the performance degradation by using MMTH in the router. The baseline router, the naive router with BR stage, and the proposed router TABLE II: CMP System Conﬁguration TABLE III: NoC System Conﬁguration in RTL models System Parameters Details Processor # of processors # of directories # of L2 caches L1 I/D cache size L2 caches size Coherence protocol X86-64 4 4 16 32KB 256KB MOESI directory are compared. The baseline router and the naive router have a three-stage pipeline and a four-stage pipeline respectively. The proposed router is based on a three-stage pipeline. However, an extra stage is created in the destination router as described in Section IV-B. We assume a many-core processor with 16 cores which are connected with 4 × 4 mesh by the above described router. Nine benchmark programs from NAS Parallel Benchmark (NPB) [27] are simulated. These programs are designed to help evaluate the performance of parallel supercomputers. Table II lists the detailed CMP conﬁguration we use to run benchmarks. First, network performance is evaluated. Figure 9 shows the average latency versus injected trafﬁc under the three different synthetic trafﬁc patterns (the uniform random trafﬁc, bitcomplement trafﬁc and tornado trafﬁc). These traces represent a mixture of benign and adversarial trafﬁc patterns. All the simulations are performed for 10000 cycles. Although the naive router design with the BR extra stage stretches the latency of about 20%, the overhead in the latency of the improved design is only approximately 5%. In the case of the tornado trafﬁc pattern, where each node sends packets (⌈k/2⌉ − 1) mod k hops (k represents the size of the network in a dimension) to the right in the X dimension, the overhead of the naive router is even modest. This is because the number of hops in the tornado trafﬁc tends to be small. When k is four as we assume, the number of hops is mostly only one. The saturation throughput which shows the bandwidth of the network is almost the same in all the three designs. Secondly, the full system simulation are implemented. Figure 10 shows the execution result of full simulation. This graph also shows that the performance degradation of the improved version router is only 0.5% - 2.0%. A remarkable difference is observed in the case of CG (Conjugate Gradient), which consists of irregular memory accesses and communications. In the program, the overhead of the naive router and the proposed router is 10% and 2%, respectively. B. Power consumption We design the baseline router and the proposed router with Renesas’s 40nm CMOS design technology to evaluate the power consumption of the router with MMTH. The router architecture is the same as shown in Figure 1. The width of a link is set to be 64bits, and four 16-bit width 8-depth FIFO units are used for a virtual channel. Table III speciﬁes the detailed conﬁguration of RTL models. We adopt the commonly System Parameters Clock frequency Topology # of cores # of VCs per input port Buffer size Routing Arbiter type Flit size Packet size Trafﬁc pattern Details 2GHz 2D-Mesh 4 2 8ﬂits XY Routing Round-robin 64bit 1 header ﬂit + 6 body ﬂits Uniform used XY routing where packets are ﬁrst routed in the Xdimension followed by the Y-dimension. Firstly, we evaluate the power consumption using Apache’s PowerArtist [28] on the basis of the RTL and Synopsys’s Liberty library format [29]. Since PowerArtist does not take BCR into consideration, the result corresponds to the maximum power consumption (i.e. BCR = 100%). The second bar in Figure 11 shows the maximum power of the proposed router when it works at 2GHz. For the comparison, we also evaluate the baseline router with the traditional register-based FIFO and the ﬁrst bar in Figure 11 shows the result. Since the standard cells are designed for low energy consumption rather than high speed operation, the baseline router can only work at 800MHz. The dynamic power of the baseline router is scaled assuming that it works at 2GHz. The scaling is needed for fair comparison because MMTH is designed for high speed operation. From the ﬁgure, it appears that the router using MMTH improves the power consumption by 28.8% in the aggregate. Although the power except for input buffers shown as “The others” in the ﬁgure increases by 13.3% owing to additional control signals and logic, the power of input buffers shown as “Input buffers” decreases by 46.5%. It indicates that the proposed router can reduce the power even if BCR is 100%. Subsequently, we take BCR into consideration in application programs. To compute the BCR of benchmark programs, GEM5 full system simulator and NPB are used again. Specifically, the change between current bit and preceding bit is classiﬁed into four patterns (0 to 0, 0 to 1, 1 to 0, and 1 to 1) whenever a ﬂit comes to a router, and each rate is computed. Figure 12 shows this result. A sum total of “0 to 1” and “1 to 0” shows BCR. From the ﬁgure, BCR is only 25.0% on average and “0 to 0” is especially frequent. It is expected that high-order bits include zeros plentifully. Considering BCR, the real power consumption is computed as follows: Px = Pmin + x 100 (Pmax − Pmin ), (4) where Px , Pmin and Pmax are powers when BCR is x%, 0% and 100%, respectively. Pmin contains only the power consumption for W-Pointer and R-Pointer, and are evaluated by the special tool for MMTH.  0  10  20  30  40  50  60  0  0.05  0.1  0.15  0.2 Injected Traffic [packets/node/cycle]  0.25 A e v r F e g a t i l a L t y c n e [ e c y c l ] Baseline MM(naive) MM(proposed) (a) Uniform random trafﬁc  0  10  20  30  40  50  0  0.05  0.1  0.15 Injected Traffic [packets/node/cycle]  0.2 A e v r F e g a t i l a L t y c n e [ e c y c l ] Baseline MM(naive) MM(proposed) (b) Bit-complement trafﬁc  0  10  20  30  40  50  60  0  0.05  0.1  0.15  0.2  0.25 Injected Traffic [packets/node/cycle]  0.3 A e v r F e g a t i l a L t y c n e [ e c y c l ] Baseline MM(naive) MM(proposed) (c) Tornado trafﬁc Fig. 9: The average ﬂit latency vs. injected trafﬁc  0.8  0.85  0.9  0.95  1  1.05  1.1  1.15  1.2 IS MG CG FT BT LU EP SP UA Ave. Benchmark programs E u c e x i t n o i t m e ( N o r m a i l d e z ) Baseline MM(naive) MM(proposed) Fig. 10: Execution Time of Full System Simulation By using Pmin and Pmax , we compute the real power consumption with the BCR in NPB. The results are shown in the last bars in Figure 11. As shown in the ﬁgure, the reduction ratio of power consumption is further increased to 42.4% on average. As for input buffers, the reduction ratio runs up to 68.4%. Note that this reduction is achieved only by utilizing MMTH. Provided that our approach can be combined with other power reduction technique, more signiﬁcant reduction can be done. V I . CONC LU S ION S In this paper, a router using MMTH, which is a novel power efﬁcient buffer memory, has been presented. It is an efﬁcient approach for power reduction without reducing the number of buffers or using complicated scheme. We have compared a baseline router using traditional register-based FIFOs and our proposed router. The present study has demonstrated that the power consumption is associated with the bit change rate of the input data, and when NPB work on NoC, it is reduced by 42.4% on average at 2GHz compared with a traditional FIFO implementation. The performance degradation caused by the delay of the reading time can be mostly saved by the new scheme based on the look-ahead technique in the router. Our results show the execution time of full system simulations increases only 0.5%-2.0% and the saturation throughput is not degraded. ACKNOW LEDGM EN T This work was supported by New Energy and Industrial Technology Development Organization (NEDO) under the Ministry of Economy Trade and Industry of Japan. "
