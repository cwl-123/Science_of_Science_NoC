year,title,abstract,full_text
2007,Interconnect-Centric Computing.,"Summary form only given. As we enter the many-core era, the interconnection networks of a computer system, rather than the processor or memory modules, will dominate its performance. Several recent developments in interconnection network architecture including global adaptive routing, high-radix routers, and technology-matched topologies offer large improvements in the performance and efficiency of this critical component. The implementation of a portion of several interconnection networks on multi-core chips also raises new opportunities and challenges for network design. This talk explores the role of interconnection networks in modern computer systems, recent developments in network architecture and design, and the challenges of on-chip interconnection networks. Examples will be drawn from several systems including the Cray BlackWidow","Keynote I  Interconnect-Centric Computing  William J. Dally  Bell Professor and Chairman of Computer Science  Stanford University  Abstract  As we enter the many-core era, the interconnection networks of a computer system, rather than  the processor or memory modules, will dominate its performance.  Several recent developments in  interconnection network architecture including global adaptive routing, high-radix routers, and  technology-matched topologies offer large improvements in the performance and efficiency of  this critical component.  The implementation of a portion of several interconnection networks on  multi-core chips also raises new opportunities and challenges for network design.  This talk  explores the role of interconnection networks in modern computer systems, recent developments  in network architecture and design, and the challenges of on-chip interconnection networks.  Examples will be drawn from several systems including the Cray BlackWidow.  About the Speaker  Bill Dally is the Willard R. and Inez Kerr Bell Professor of Engineering and the Chairman of  the Department of Computer Science at Stanford University.  Bill and his group have developed  system architecture, network architecture, signaling, routing, and synchronization technology that  can be found in most large parallel computers today. While at Bell Labs Bill contributed to the  BELLMAC32 microprocessor and designed the MARS hardware accelerator. At Caltech he  designed the MOSSIM Simulation Engine and the Torus Routing Chip which pioneered  wormhole routing and virtual-channel flow control. While a Professor at MIT, his group built the  J-Machine and the M-Machine, experimental parallel computer systems that pioneered the  separation of mechanisms from programming models and demonstrated very low overhead  synchronization and communication mechanisms.  At Stanford University his group has  developed the Imagine processor, which introduced the concepts of stream processing and  partitioned register organizations.  Bill has worked with Cray Research and Intel to incorporate  many of these innovations in commercial parallel computers, with Avici Systems to incorporate  this technology into Internet routers, co-founded Velio Communications to commercialize highspeed signaling technology, and co-founded Stream Processors, Inc. to commercialize stream  processor technology.  He is a Fellow of the IEEE, a Fellow of the ACM and has received  numerous honors including the IEEE Seymour Cray Award and the ACM Maurice Wilkes award.   He is chairman of Stream Processors and on the board of directors of Portal Player.  He currently  leads projects on computer architecture, network architecture, and programming systems. He has  published over 170 papers in these areas and is an author of the textbooks, Digital Systems  Engineering and Principles and Practices of Interconnection Networks.  1-4244-0805-9/07/$25.00 ©2007 IEEE 1                     "
2007,A Domain-Specific On-Chip Network Design for Large Scale Cache Systems.,"As circuit integration technology advances, the design of efficient interconnects has become critical. On-chip networks have been adopted to overcome scalability and the poor resource sharing problems of shared buses or dedicated wires. However, using a general on-chip network for a specific domain may cause underutilization of the network resources and huge network delays because the interconnects are not optimized for the domain. Addressing these two issues is challenging because in-depth knowledges of interconnects and the specific domain are required. Non-uniform cache architectures (NUCAs) use wormhole-routed 2D mesh networks to improve the performance of on-chip L2 caches. We observe that network resources in NUCAs are underutilized and occupy considerable chip area (52% of cache area). Also the network delay is significantly large (63% of cache access time). Motivated by our observations, we investigate how to optimize cache operations and and design the network in large scale cache systems. We propose a single-cycle router architecture that can efficiently support multicasting in on-chip caches. Next, we present fast-LRU replacement, where cache replacement overlaps with data request delivery. Finally we propose a deadlock-free XYX routing algorithm and a new halo network topology to minimize the number of links in the network. Simulation results show that our networked cache system improves the average IPC by 38% over the mesh network design with multicast promotion replacement while using only 23% of the interconnection area. Specifically, multicast fast-LRU replacement improves the average IPC by 20% compared with multicast promotion replacement. A halo topology design additionally improves the average IPC by 18% over a mesh topology","A Domain-Speciﬁc On-Chip Network Design for Large Scale Cache Systems Yuho Jin Eun Jung Kim Department of Computer Science Texas A&M University {yuho,ejkim}@cs.tamu.edu Ki Hwan Yum Department of Computer Science University of Texas, San Antonio yum@cs.utsa.edu Abstract As circuit integration technology advances, the design of efﬁcient interconnects has become critical. On-chip networks have been adopted to overcome scalability and the poor resource sharing problems of shared buses or dedicated wires. However, using a general on-chip network for a speciﬁc domain may cause underutilization of the network resources and huge network delays because the interconnects are not optimized for the domain. Addressing these two issues is challenging because in-depth knowledges of interconnects and the speciﬁc domain are required. Recently proposed Non-Uniform Cache Architectures (NUCAs) use wormhole-routed 2D mesh networks to improve the performance of on-chip L2 caches. We observe that network resources in NUCAs are underutilized and occupy considerable chip area (52% of cache area). Also the network delay is signiﬁcantly large (63% of cache access time). Motivated by our observations, we investigate how to optimize cache operations and and design the network in large scale cache systems. We propose a single-cycle router architecture that can efﬁciently support multicasting in on-chip caches. Next, we present Fast-LRU replacement, where cache replacement overlaps with data request delivery. Finally we propose a deadlock-free XYX routing algorithm and a new halo network topology to minimize the number of links in the network. Simulation results show that our networked cache system improves the average IPC by 38% over the mesh network design with Multicast Promotion replacement while using only 23% of the interconnection area. Speciﬁcally, Multicast FastLRU replacement improves the average IPC by 20% compared with Multicast Promotion replacement. A halo topology design additionally improves the average IPC by 18% over a mesh topology. 1. Introduction With the current rate of technology advancement, increasing wire delays in modern microprocessor designs [1, 12] lead to various technologies to minimize the impact of slow This work was supported in part by NSF grants CCF-0541360 and CCF0541384. on-chip communication. Typically, on-chip communication has been conducted via shared buses or dedicated wires. Dedicated wire networks can provide the best customization to applications. However, predicting the delays until the latestage in the design cycle is difﬁcult [18]. These interconnects are inﬂuenced by various parasitic capacitances and crosstalks from adjacent wires, which cannot be predicted until the actual layout and routing are performed. For shared bus networks, the whole bus is occupied by a single communication even if multiple communications could operate simultaneously on different parts of the bus. Using global buses is not an effective scalable solution because the bus bandwidth may become a major bottleneck as the number of components on the chip increases. Another way to design an on-chip communication is with a switched network. When using a switched on-chip interconnection network, all the components are connected to the network that routes packets among them, which has the advantages of structure, performance, and modularity [7, 4, 29, 9]. There has been much research on the architectures of future chip multiprocessor (CMP) designs [28, 24, 20] using switched networks for better scalability and resource sharing. Furthermore, these networks have been adopted to overcome wire delay in speciﬁc domains such as large scale cache designs [17, 3]. The regular topologies, such as meshes and tori, have been used in on-chip network designs. However, a generalpurpose network with regularly distributed network resources can cause problems in the following two cases; underprovision and overprovision of network resources. Underprovision of network resources causes poor performance. On the other hand, when network resources are overprovided, underutilization of the network resources occurs and large network delays are caused by the increased network size. Furthermore, the overprovision of network resources results in the waste of the chip area. Therefore, it is critical to design an optimal network for a speciﬁc domain by breaking the regularity of the interconnection network. It is also important to exploit the potential parallelism of the interconnection networks in the problem domain. Achieving these two goals requires profound knowledge of those areas; interconnects and the speciﬁc domain. In some large scale cache designs [17, 3], 2D mesh net1-4244-0805-9/07/$25.00 ©2007 IEEE 318 works have been adopted to interconnect small cache banks to overcome wire delays. For example, in Non-Uniform Cache Architectures (NUCAs) [17], the cache is broken into multiple banks that can be accessed at different latencies through an on-chip network. D-NUCA (Dynamic NUCA) allows cache blocks to migrate among cache banks in such a way that recently accessed cache blocks can move towards the core, which helps reduce the average cache access time. However, the network delay is still a dominant portion of the cache access time. A 16MB D-NUCA conﬁguration using a 16×16 mesh network demonstrated an average access time of 17 cycles with SPEC2000 benchmarks without network contention while the bank access time is only 3 cycles. The worst case is when the requested cache block is not found in the L2 cache. In this case, all the cache banks in the bank set 1 must be checked sequentially and then the memory is accessed. Although D-NUCA can use partial tag comparisons to detect cache miss early, additional memory in the cache controller is required to store the partial tags. Moreover, 20% of the links in a mesh network are never used, while the network occupies 52% of the total area. While on-chip networks are relatively well understood for multiprocessor systems [13, 28], there is no prior research considering the detailed design of interconnection networks for such large scale cache designs. Thus, the main purpose of this paper is to investigate the design space of the interconnection framework and, particularly, how it interacts with the rest of the multi-bank cache architecture. The research proceeds as follows: First, we propose a single-cycle wormhole router architecture that supports multicasting efﬁciently. Because multicasting can signiﬁcantly reduce the large network delay, its reduces the cache access time dramatically. Unlike the existing multicast routers proposed before, the router takes only one cycle in each hop and does not require any extra storage. It becomes the basic building block of the interconnection network design for the proposed large scale cache systems. Next, we present Fast-LRU replacement, where cache replacement overlaps with tag-matching in the system. We investigate detailed operations in the network including tagmatching, replacement, and placement for a cache hit and miss. Fast-LRU can be further improved with the multicasting support of the underlying network. The proposed networked cache system shows the best performance when it uses Multicast Fast-LRU replacement. Finally, we propose a deadlock-free XYX routing algorithm and a new halo network topology to reduce the cache access time and minimize the number of links in the cache system. We also discuss the layout of the L2 cache on the processor die to help reduce the cache access time. In the XYX routing algorithm, the normal XY routing is used for delivering cache requests from the core to the banks while the replies from each bank to the core are transfered in the Y direction ﬁrst. Compared with XY routing, the XYX routing algorithm saves most horizontal links in a mesh network. 1 To implement a set associative cache in a networked cache system, a set is distributed across multiple banks. This is named a bank set [17]. The removal of horizontal links leads to the removal of all the associated input buffers and the simpliﬁcation of both the arbiter and the crossbar designs. This simpliﬁed router also brings latency reduction as well as reduction in the total chip area. With the halo topology, all the closest banks of each column in a mesh network are placed in the same one-hop distance from the core. Simulation results with SPEC2000 benchmarks show that the networked cache system with all the proposed techniques improves the IPC by an average of 38% and uses only 38% of the interconnection area of the D-NUCA system with Multicast Promotion replacement. Speciﬁcally, the average IPC is improved by 20% with the Multicast Fast-LRU replacement in a mesh topology and by 18% with the halo topology. This paper is organized as follows: Section 2 describes the related work. Section 3 explains a single-cycle wormhole router architecture with multicasting support and Fast-LRU replacement policy. In Section 4, we develop a new deadlockfree XYX routing algorithm and propose a halo topology for large scale cache systems. Section 5 and Section 6 discuss the simulation platforms and the experimental results, followed by the concluding remarks in Section 7. 2. Related Work Several works explore the large on-chip cache designs to overcome the wire delay problem. NUCA [17] shows that the traditional large cache based on partitioned subbanking is ineffective because its access latency is determined by the slowest (i.e. farthest from the core) subbank. One of the proposed designs, Static NUCA (S-NUCA), uses dedicated wires to each bank and incurs signiﬁcant area overhead. In another design (D-NUCA) that uses a switched network, they exploited cache block migration, partial-tag search for early miss detection, and multicast for fast bank access. NuRAPID [5] leverages sequential tag-data access and decouples tag and data placement to save power. It divides the cache data array into several large distance groups and places frequently accessed data to the fast group. However, it has overhead to maintain pointer structures in the existing bank architecture. The communication medium in TLC [2] is a fast transmission line having short inductive-capacitance (LC) delays. Although transmission line can provide express channels, its application is limited due to the high area requirement. Recently, all three designs were examined for cache sharing in a CMP architecture [3, 6, 16]. An on-chip network that enables low-latency for high bandwidth communication is used in partitioned architectures such as CMPs [28, 24] and Network-on-Chips (NoCs) [7, 4, 29, 9]. A few research showed that the network design must be tailored to the communication behavior of applications running on the network to save resources and achieve performance improvement. A mesh network was optimized to each parallel application by analyzing its requirement of both links and routers [13]. Instead of the uniform assignment of buffering resources in a 2D mesh network, the allocation of different buffering size in each channel reduces 319 the implementation costs and increases performance in the NoC design of media applications [15]. Moreover, the router design directly impacts the performance of the network. In a pipelined router architecture, speculative switch allocation reduces the latency by doing virtual channel allocation and switch allocation at the same cycle [23]. Pre-computation of arbitration decisions also reduces the latency by separating the arbitation logic from the critical path [21]. To support ”one-to-many” communication primitives, a multicast router has been designed especially for large networks such as multistage networks and 2D mesh/hypercube topologies [27, 19]. Due to the nature of multicasting, we need to provide extra storage to hold packet and a deadlock prevention mechanism. In a chip-to-chip domain, the centralbuffer-based design has better performance over the inputbuffer-based design, because the queuing capability of the central buffer is superior for unicast packets [27]. However, the additional storage requirement is not desirable in the onchip domain where area budget is very tight. There are two ways to prevent deadlock; deadlock avoidance/recovery and complete packet buffering. While deadlock avoidance routing causes resource underutilization and deadlock recovery mechanism is highly complex, complete buffering requires the large buffer storage. Therefore, the main challenge of multicast support in an on-chip network is to prevent deadlock without extra storage requirements. 3. System Architecture In this section we describe a single-cycle multicasting wormhole router that is the basic building block of the interconnection network that connects the core, the banks, and the off-chip memory. Then, we analyze the communication patterns for LRU replacement in a cache network and propose Fast-LRU replacement. 3.1. Single-Cycle Multicasting Router In this research, we use wormhole routers due to small buffer requirement and high throughput. Figure 1 shows the major components of a wormhole router. It has 5 Physical Channels (PCs) that connect four neighbors and one injection/ejection unit, and each PC is divided into multiple Virtual Channels (VCs). VCs from the same PC share one crossbar switch port to reduce the complexity of the switch. While middle/tail ﬂits require 3 operations (input buffering, switch allocation, and switch traversal), head ﬂits require additional two operations (routing and VC allocation). Even though an aggressive design can merge adjacent operations into a single-cycle operation, this dependency still exists. Moreover, recent results show that the operating clock cycle for the router is 12 fanout-of-four (FO4) delays [21], which is close to the optimal pipeline delay of the modern superscalar processor [14]. To minimize and break this serial dependency, we use lookahead routing [10], buffer bypassing, speculative switch allocation [23], and arbitration precomputation [21]. All these techniques work well in a lightly loaded network since they can ﬁnd the possible cases credits out to downstream router VC buffer bypass . . . replicator PC id VC id X+ replicator X− Y+ Y− inject routing precomputation VC allocator credits in precomputation precomputation speculative SA SA X+ X− Y+ Y− eject data path control path crossbar Figure 1. Single-Cycle Multicasting Router frequently. This router design enables a ﬂit to route in a single clock cycle by reducing the critical path in a traditional pipelined router. We recognize that multicasting plays a vital role in deciding a cache hit/miss in the networked cache system because it helps access the multiple banks concurrently. Multicasting requires the replication of ﬂits inside the router to forward them to multiple destinations. Synchronous replication copies ﬂits after reserving all destination ports in a lock-step, which can result in deadlock. Asynchronous replication allows the router to forward ﬂits to a subset of the destination ports. While synchronous replication does not require extra buffers, asynchronous scheme needs additional buffers in order to hold ﬂits until all copies are transmitted. We cannot apply both replication schemes in the router design directly for the following reasons: With the tight chip area constraint, asynchronous replication is not a desirable option; however, synchronous replication in wormhole switching is susceptible to deadlocks. We aim to design a replication scheme without extra buffers and to handle each replica separately in asynchronous manner. Communication patterns in the networked cache system show that PCs in some routers within the network are not fully utilized. (Detailed analysis is given in Section 4.) This property makes us choose a hybrid scheme that exploits the underutilized input buffer space to store replicated ﬂits. The router shown in Figure 1 copies the original ﬂit to one VC of a different PC, when a multicasting packet needs replication. When a replicator selects a PC that has at least one free VC, PCs that are less frequently used are preferred. Then, one free VC is chosen. A free VC of another PC can be easily obtained by checking the status of the input buffer. If there is no available VC for replication, any ﬂit forwarding is blocked. We observe that blocking rarely happens in the cache systems. This hybrid scheme minimizes the overhead of multicasting support in a low latency router since the changes in the existing VC/switch allocators is not required, and only replication logics are needed to ﬁnd a free VC of other PCs and connect320 ing wires to their input buffers. 3.2. Fast-LRU Replacement In this section, we propose Fast-LRU replacement with multicasting to reduce the long latency of the LRU replacement scheme after examining optimization of the LRU replacement policy with unicasting. We start with the brief discussion of cache operations in D-NUCA [17]. As shown in Figure 2 (a), the cache is broken into multiple banks that can be accessed through a mesh network. One column of the mesh represents one set of a set-associative cache, which is statically determined by the low-order bits of the block address. Cache blocks in a set are spread across multiple banks in the column. Each set distributed in a column is called a bank set. Thus, the cache system searches for a block by ﬁrst selecting the column, selecting the set within the column using direct-mapping, and ﬁnally performing a tag-match on distributed blocks. Note that each way has a different network latency depending on the distance from the core. Although a bank set can be distributed on every column to give approximately equal access time across all bank sets, we do not consider this conﬁguration due to the larger hop count. (1) (2) (3) (4) forward hit block (closest, MRU) Bank 1 (6) (7) (8) (9)             miss Bank 2                   miss Bank 3             miss Bank 4       hit (5) (10) notify completion . . . (farthest, LRU) Bank 16 tag match             miss (1) (2) tag match/ move             notify completion empty (6)                          (3)                   miss                                                          . . . (4)             miss move                                                   hit (5)               . . . . . . (a) LRU (b) Fast-LRU Figure 2. Two Unicast LRU Replacement Schemes in a Networked Cache System To reduce the average access time, we should place frequently used data in the banks closer to the core, which can be achieved with LRU replacement. The LRU generates 14% higher cache hit rate than Promotion [17], which swaps the hit block with a block in the bank that is next closest to the core. Therefore, it makes the ﬁrst way and the last way be placed on the closest (MRU) bank and the farthest (LRU) bank, respectively. However, maintaining the LRU order in a bank set requires many swaps of blocks between banks. Figure 2 (a) shows the overhead of the LRU replacement policy by depicting the required communications among banks. Assuming that a data request is a hit in Bank 4, the 321 request traverses from Bank 1 to Bank 4 ((1) ∼ (4)). Then a hit block is sent to Bank 1 ((5)), resulting in the corresponding blocks in Bank 1, 2, and 3 being moved to Bank 2, 3, and 4 ((7) ∼ (9)), respectively. In this example, the total communication time is 21 hops including the notiﬁcation of completion; the initial tag-match time to ﬁnd a hit bank is 7 hops and the remaining part is 14 hops. It is clear that the cache hit latency can be decomposed into two parts— tagmatch and move (replace) operations. Therefore, the total communication time for block movement after ﬁnding a hit can easily exceed the initial tag-match time. A cache miss needs tag-match along all banks and a new block placement to the MRU bank incurs multiple block movements to arrange corresponding blocks in the LRU order. The proposed Fast-LRU replacement allows the tag-match operation to overlap with the replace operation as shown in Figure 2 (b). If there is a miss in a bank, the corresponding block in the bank is evicted and immediately transferred to the next bank with the data request ((1) ∼ (4)). Unless the request is a hit in the MRU bank, the corresponding block in the bank is pushed to the next bank consecutively until the ﬁnal LRU bank. Once there is a hit in a bank, that block is transferred to the MRU bank where its corresponding frame is already empty ((5)). If all the banks generate misses, the request is forwarded to the off-chip memory. Then this new block is stored in the MRU bank and sent to the core. Since the invariant property of LRU is that all the banks ahead of the hit bank generate misses, the total communication time of the Fast-LRU replacement scheme is 12 hops in Figure 2 (b). In addition, this scheme almost halves the number of bank accesses since both tag-match and replacement are performed simultaneously. Next, we further investigate how to reduce the cache access time by exploiting the multicast router proposed in Section 3.1. Even though Fast-LRU replacement reduces the hit latency, a hit on the LRU bank or a miss on a cache (i.e. all banks produce misses.) still suffer from the long latency, which is the sum of the bank access time over all banks in a bank set and the network latency. Multicasting relieves this problem by allowing concurrent accesses of multiple banks for tag-match. Figure 3 (a) illustrates a cache hit with Multicast Fast-LRU replacement. Time diagram of each operation is illustrated in Figure 3 (c). When the multicast router attached to the MRU bank (Bank 1) receives a data request, it forwards the request to two destinations, the MRU bank and the second MRU bank (Bank 2), at the same time. The router attached to the second MRU bank (Bank 2) also forwards the request to the attached bank and the next bank (Bank 3), and so on. If the requested block is found in the MRU bank, no block replacement is required and the core is notiﬁed of a cache hit. Otherwise, the MRU bank initiates Fast-LRU replacement by sending its evicted block to the second MRU bank. Each nonMRU bank that experienced a cache miss waits for the evicted cache block from the previous bank. As soon as it receives the evicted block, it also sends its evicted block to the next bank. This operation stops at the bank where the requested block is bank 3                bank 2     (right after miss detection) move notify completion . . . . . .     notify hit tag match bank 2 bank 3             miss bank 4     notify miss bank 1                bank 1 bank 4 empty                  hit                                                miss miss newly fetched block tag match move           bank 2 notify miss bank 1 . . . . . . . . . if the victim is dirty replacement done write back bank 16 (LRU)     core memory memory core           core                                    miss miss miss (a) Cache Hit (b) Cache Miss bank1 net net bank2 net net bank3 net net bank4 net bank2 net bank3 net bank4 miss notification hit decision replacement replacement tag match bank1 hit completion net net net net net bank1 net net bank2 net net bank3 net memory ... ... victim to memory replacement miss completion new block placement bank15 net net bank16 tag match miss decision net bank1 net memory net net core net net net bank16 net bank2 net bank3 net bank4 net (c) Hit Case Time Diagram (d) Miss Case Time Diagram Figure 3. Fast-LRU Replacement with Multicasting Support found. Note that the replacement packet (including an evicted block) can never catch the data request packet and that the data request packet always reaches the LRU bank. Each nonMRU bank should not initiate its evicted block transfer to the next bank unless it receives an evicted block from the previous bank. A cache miss occurs when all the banks in the bank set produce misses as depicted in Figure 3 (b) and (d). It incurs the off-chip memory access and the write-back of the evicted block from the LRU bank (Bank 16) to the memory, if it is dirty. The core waits for all the banks to report misses, and then invokes the memory access. When the memory sends a new block to the MRU bank, the MRU bank sends this newly incoming block to the core. The block movement is similar to that of a cache hit, except the LRU bank notiﬁes the core of the replacement completion and writes back the victim to the memory if the block is dirty. 4. Network Topology and Layout In this section, we analyze the utilization of the interconnection network for large scale cache systems and explore a few alternatives for simpliﬁcation and optimization. Figure 4 (a) shows all possible XY routing communication patterns of a large scale L2 cache system on an 8 × 8 mesh network. The core is attached to the fourth router of the top row and the memory is attached to the ﬁfth router of the bottom row. Forwarding of a data request to the appropriate bank set needs the traversal of the ﬁrst row (A) and the traversal of banks within the column (B) until there is a hit in the bank. When a bank sends the requested cache block to the core, the communication path is D or E from the hit bank. The data movement between two banks occurs only in a one column of the mesh (B or C). While a cache miss has the same communication patterns (A’ and B’) as a cache hit, it requires the delivery of the new data block from memory to the MRU bank (F). After a new memory block placement to the MRU bank, the evicted block is replaced with the block in the next farther bank (B’) or is written back to the memory (G), if dirty. hit miss core A D A’ F G E B’ C B memory core memory (a) Communication Patterns (b) Minimal Link Requirement Figure 4. An 8 × 8 Mesh Network for a Large Scale Cache System One observation of these patterns is that the unidirectional horizontal link is sufﬁcient except the ﬁrst row, the last row, and links between the core-attached column and the memoryattached column. Figure 4 (b) shows the minimum set of 322 links after removing all unnecessary links. In general, we can remove (n − 2)2 links among the total 4(n − 1)2 links of the n × n mesh, which makes the link area reduced by 25%. Another observation is that the horizontal links except the ﬁrst row are infrequently used, because they are used only when a bank needs to communicate with the core (i.e. the cache controller) or the memory. Considering the locality behavior in the cache, utilization of the horizontal links of the last row is low because those links are only used for the memory accesses. The number of these underutilized links2 is n2 − 2. These underutilized links can be eliminated at the expense of small bandwidth loss. This simpliﬁed mesh achieves an additional 25% savings in link area. However, it causes the change of the routing scheme since communications from the banks to the core/memory start in the Y direction ﬁrst, which violates the existing XY routing. Thus, we propose a new routing scheme called XYX routing to overcome this problem. XYX routing is deadlock-free because we can enforce the total order of channels in the mesh network. Figure 5 (a) shows the XYX routing algorithm and Figure 5 (b) shows the channel enumeration in a 3 × 3 mesh network for XYX routing. Any path in XYX routing follows increasingly numbered channels such as two paths, (14, 19) and (5, 6, 13). Inputs: coordinates of source bank(Xsrc , Ysrc ) coordinates of destination bank(Xdest , Ydest ) Outputs: Selected output C hannel Procedure: Xof f set := Xdest − Xsrc ; Yof f set := Ydest − Ysrc ; if Yof f set ≥ 0 then if Xof f set > 0 then C hannel := X+; else if Xof f set < 0 then C hannel := X−; else if Yof f set = 0 then C hannel := I nternal; else C hannel := Y +; endif else C hannel := Y −; endif (0, 0) 2 19 1 20 (a) Routing Algorithm 7 14 9 16 11 18 4 21 3 22 8 13 10 15 12 17 Y− 6 23 X− X+ Y+ 14, 19 5 24 (2, 2) 5, 6, 13 (b) Channel Enumeration in a 3 × 3 Mesh Figure 5. XYX Routing In the view of the router design, expunging horizontal links leads to the elimination of all the input buffers associated with those links and to the simpliﬁcation of both the 2 In a mesh network requiring the minimal number of links, rows from the second to the last second have n vertical links. The last row has 2(n − 1) vertical links. Therefore, the total number of these links is n × (n − 2) + 2(n − 1). 323 arbiter and the crossbar. This simpliﬁed router also creates latency reduction as well as saves area. core core memory memory (a) Mesh (b) Simpliﬁed Mesh ... ... . . . ... core memory ... . . . ... ... core memory (c) Halo Constructed with Uniform Size Banks (d) Halo Constructed with Non-uniform Size Banks Figure 6. Domain-Speciﬁc Development of Network Designs One of the disadvantages in a mesh network is uneven network latencies among MRU banks depending on the distance from the core. Since there is only one path between the core and one special MRU bank, the leftmost or rightmost MRU banks cannot avoid suffering from the long network latency (Figures 6 (a) and (b)). So it is crucial to provide a direct communication path between the core and each MRU bank. For this purpose, we choose a topology in which the core is located in the same distance from all the MRU banks. We call this topology a halo network in Figure 6 (c). The core plays a role of a hub to control the departure and the arrival of cache requests. In this design, we assume that the cache controller can support multiple ports/interfaces to the networked cache3 . A bank set is distributed over multiple banks on a spike branched from a hub, which bidirectionally connects all the banks in the order of the way. However, if the size of all banks that build a spike is identical, the banks positioned at the end of the spike cannot ﬁll the increased area entirely. Although we can curve a spike, the spiral spike layout incurs the longer wire delay than the straight spike layout. If the bank size increases along the spike, we can reduce the unused area and draw a compact design by tightly integrating banks on a chip die as shown in Figure 6 (d). As a bank is 3 To issue multiple cache requests simultaneously, we put a small queue (2 entries) for each spike like a multiple issue queue. Thus, a cache request is ﬁrst stored to each spike queue to be subsequently forwarded to the L2 cache. located farther from the core, its size becomes larger and its access time increases due to the increased capacity. Therefore, capacity-increased banks have more than one way. A halo network incorporated with non-uniform size banks has a topological beneﬁt by giving the same access time to all the MRU banks, and it enjoys a better area utilization over a halo network with the uniform size banks. Note that the memory controller is in the center of the cache. To access the off-chip memory from the memory controller, its wire delay in the halo is longer than in the mesh. 5. Methodology We use sim-alpha simulator [8] that models an Alpha 21264 core to generate L2 cache accesses. The clock frequency of the core is scaled to 5 GHz. To measure the contention effect of the banks and the interconnection networks in details, a separate L2 cache simulator with an interconnection network was developed. Sim-alpha directly sends a chunk of L2 accesses to the cache simulator. We models the latency of the bank from Cacti 3.0 [25], and that of the global level wire from the ﬁrst order RC model [22] under optimal repeater insertion at 65nm technology. We obtain the resistance and capacitance of the unit-length wire from [26], and the wire length is determined by the bank size. Each component of the pipelined router takes one cycle. The base conﬁguration is a 16MB L2 cache by interconnecting 256 64KB banks with a 16×16 mesh network. The core and the memory are attached at the center of the top row and the bottom row, respectively, to evenly distribute trafﬁc. Main parameters are summarized in Table 1. Table 1. System Parameters Memory Block size 64B Memory latency (pipelined) 130 cycles + 4 cycles per 8B Router Flit buffer size Number of VCs per PC Flit size Latency of one stage 4 ﬂits 4 128 bits 1 cycle 64KB 1 cycle bank size 64KB 128KB 256KB 512KB 512KB 3 cycles Wire Delay 128KB 256KB 2 cycles 2 cycles Bank Access Latency tag matching only tag matching+replacement 2 cycles 3 cycles 4 cycles 4 cycles 4 cycles 5 cycles 5 cycles 6 cycles To measure various design impacts, we use SPEC2000 benchmarks. We fastforward 2 billion instructions, warm up the L2 cache for the next 100 million instructions, and measure the performance of each architecture for remaining instructions. Table 2 shows the perfect L2 IPC and L2 cache access behavior of each benchmark. A 32-bit address is divided into 4 ﬁelds: tag (12 bits), index (10 bits), bank-column (4 bits), and offset (6 bits). The 324 Table 2. Benchmarks Used for Experiments benchmark name applu(FP) apsi(FP) art(FP) galgel(FP) lucas(FP) mesa(FP) bzip2(INT) gcc(INT) mcf(INT) parser(INT) twolf(INT) vpr(INT) instr. exec. 500M 1B 500M 2B 1B 2B 2B 500M 250M 2B 1B 1B perfect L2 IPC 0.43 0.40 0.40 0.43 0.44 0.40 0.39 0.29 0.34 0.38 0.38 0.41 L2 L2 read write 9.444M 4.428M 12.375M 8.204M 63.877M 13.578M 19.415M 4.137M 19.506M 13.226M 2.907M 2.656M 16.301M 4.233M 26.201M 14.827M 29.500M 15.755M 18.257M 6.915M 20.283M 7.653M 12.459M 5.024M L2 access per instr. 0.028 0.021 0.155 0.012 0.033 0.003 0.010 0.082 0.181 0.013 0.028 0.017 bank-column is used to select one of 16 columns of the network while the index identiﬁes one of the entries in each bank in the column. With uniform size 64KB banks, each bank is a direct-mapped cache, and the cache blocks in the banks form a 16-way bank set. Since a cache network delivers packetized data, the onchip network does not need the separate address bus and data bus used in the traditional cache. In wormhole switching, ﬂitization requires the overhead data [7] in a ﬂit such as type (2 bits for specifying head/middle/tail), size (7 bits for ﬂit size), routing (8 bits for source and destination), and communication type (1 bit for unicasting/multicasting). Because the width of link is 16B, a read request packet or a notiﬁcation packet that consists of only the address ﬁts in one ﬂit even with the overhead data. When a packet includes a block data for write request, replacement, memory access, or hit data forwarding, one packet consists of 32-bit address, 64B data and overhead data, which are divided into ﬁve ﬂits. 6. Experimental Evaluation We perform experiments to inspect the efﬁciency of communication support of Fast-LRU replacement in Section 6.1. Different interconnects for the networked cache system are examined in Section 6.2. The area of router, wire, and cache in each design is analyzed in Section 6.3. 6.1. Performance of Fast-LRU Replacement Figure 7 shows how the total average cache access latency is divided into bank access, network traversal, and memory access for benchmarks in 16MB L2 cache with uniform size banks. A signiﬁcant portion of the total average latency is network access (65% on the average) while bank access (25%) and memory access (10%) are relatively small. LRU is a better policy than Promotion since more requests are hit in the MRU (fastest) banks. Speciﬁcally, LRU shows a hit increase by 5%-19% at the MRU banks. In Figure 8, we compare performance results from the Multicast Fast-LRU method with those of two existing Promotion schemes [17] 4 and Unicast LRU/Fast-LRU methods. 4 In our implementation of cache miss of Promotion, the incoming block  0  20  40  60  80  100 a a p p plu si a rt g alg el lu c a s m e s a b zip 2 g c c m cf p a c e r r t olf w v p r a v g e p r e g a n e c t ( % ) bank network memory Figure 7. Latency Distributions of L2 Cache Access in the Unicast LRU Environment Figure 8 (a) illustrates the average access latency while hit and miss latencies are depicted in Figures 8 (b) and (c) 5 . In the unicast environment, LRU naturally increases the average access latency by 4.4% over Promotion, but Fast-LRU reduces it by 30.2%. Recall that the number of bank accesses in Fast-LRU is almost the same as Promotion, but it concentrates hits to the MRU(closest) banks. All the results of Multicast Fast-LRU show reduction of the average access latency by 46% over Unicast LRU and 27% over Unicast Fast-LRU. Also Multicast Fast-LRU reduces the average hit and miss latency of Unicast LRU by 48% and 32%, respectively. Its latency improvement over Multicast Promotion is 37%, and the IPC is improved by 20%. 6.2. Performance Comparison of Different Network Designs We examine the performance of the L2 cache with varying network size, network topology, bank size, and the position of the core and the memory. We evaluate six designs summarized in Table 3. All conﬁgurations have the same capacity (16MB) and use efﬁcient Multicast Fast-LRU replacement. Design A, the baseline conﬁguration, uses a 16×16 mesh network to connect 256 64KB banks. Design B uses the same size simpliﬁed network by removing most horizontal links and moving the memory controller next to the core shown in Figure 6 (b). A small 16×4 mesh network with large banks is incorporated in Design C. Design D still uses a mesh network, but non-uniform size banks are used. Non-uniformity of the size still maintains the same associativity but has different wire delays between tiles. One bank set is constructed by 5 banks: two 1-way 64KB banks, one 2-way 128KB bank, one 4-way 256KB bank, and one 8-way 512KB bank in the order of the distance from the core. In 16 × 5 mesh, we set the same delay (3 cycles) in the horizontal direction as for 512KB from the memory evicts the data in the closest bank and causes recursive replacement. In [17], if the incoming block replaces the data in the cache and the victim is moved to the memory (zero-copy) or the lower-priority bank (one-copy), the miss latency can be reduced. However, it can evict the important data from the cache. 5We omit art results in Figure 8 (c) since there is no cache miss except compulsory misses during our simulation.  0  50  100  150  200  250  300 a p plu a p si a rt g alg el lu c a s m e s a b zip 2 g c c m cf p a s e r r t olf w v p r l a t y c n e ( s e c y c l ) unicast+promotion unicast+LRU unicast+fastLRU multicast+promotion multicast+fastLRU (a) Average Access Latency  0  20  40  60  80  100  120 a p plu a p si a rt g alg el lu c a s m e s a b zip 2 g c c m cf p a s e r r t olf w v p r l a t y c n e ( s e c y c l ) (b) Average Hit Latency  100  200  300  400  500 a p plu a p si g alg el lu c a s m e s a b zip 2 g c c m cf p a s e r r t olf w v p r l a t y c n e ( s e c y c l ) (c) Average Miss Latency Figure 8. L2 Cache Access Latency Comparisons bank while the delay in the vertical direction increases as the bank size increases. Designs E and F use the halo topology such as Figures 6 (c) and (d). Since the memory controller is located in the center of the cache system, we need to consider the longer wire delay for the off-chip memory access. Wire delays are 16 and 9 cycles in Designs E and F, respectively. Table 3. Different Network Designs Design A B C D E F Interconnection Network Bank Size uniform (64KB) uniform (64KB) uniform (256KB) non-uniform uniform (64KB) non-uniform 16 × 16 mesh 16 × 16 simpliﬁed mesh 16 × 4 simpliﬁed mesh 16 × 5 simpliﬁed mesh 16-spike halo (length of spike=16) 16-spike halo (length of spike=5) Figure 9 shows the relative IPC normalized to the Design A. The simpliﬁed mesh network in Design B achieves almost the same performance results as Design A despite the decreased bandwidth. Even low hit rate benchmarks, applu and lucas, show the IPC increase by almost 7% and 10%. 325         16x16 mesh (64KB bank) 16x16 simpl. mesh (64KB bank) 16x4 simpl. mesh (256KB bank) 16x5 simpl. mesh (non-uniform bank) 16-spike halo (64KB bank) 5-spike halo (non-uniform bank) C P I d e z i l a m r o n  1.4  1.2  1  0.8  0.6  0.4 applu apsi art galgel lucas mesa bzip2 gcc mcf parser twolf vpr Figure 9. Performance Comparison in Different Interconnection Networks The main reason of the performance enhancement is the miss latency reduction. Designs C and D show the average performance degradation by 14% and 12% respectively, due to the high wire latency to traverse the larger bank. The halo topology in Designs E and F gives performance improvement by 12% and 13%. Since non-uniform size banks can reduce the wire delay to the memory, Design F shows slightly better performance than Design E, especially in applu, apsi, and lucas. However, art having no misses in our simulation shows performance degradation due to increased wire delay for large size banks. Design F achieves 1.13 times the IPC increase over Design A. We can observe this improvement in both high and low hit rate benchmarks (1.33 times in art and 1.19 times in lucas). Compared with NUCA’s Multicast Promotion, the halo topology coupled with Multicast Fast-LRU improves 1.38 times of the IPC. 6.3. Area Comparison of Different Network Designs We estimate the required area on the banks, routers, and links of a 16MB L2 cache system. The bank area is extracted from Cacti model [25]. In the router area, we account for ﬂit buffers and the crossbar switch, where each is analytically driven by the feature size [11]. We estimate the link area by computing its width and length. Assuming the wire pitch is 1μm, a bidirectional link that transmits 128-bit ﬂit consists of 256 wires, which has 256μm width. To estimate the link length to cover one tile, we use the sum of both router and bank areas. We assume that there is no additional area for repeaters and latches in a wire because wires are not routed over banks. Table 4. Area Analysis of Network Designs Design A B E F bank (%) 47.8 58.4 67.5 78.7 router (%) 20.8 13.0 14.1 5.7 link (%) 31.4 28.6 18.4 15.7 L2 area (mm2 ) 567.70 464.60 402.30 312.19 chip area (mm2 ) 567.70 521.99 1602.22 517.61 Table 4 describes the area consumption of each component for four designs discussed in Section 6.2. The last column is the size of the minimal rectangular chip that includes the L2 cache. Design A (16×16 mesh) uses almost 52% of the cache area for the network. Design B (16 × 16 simpliﬁed mesh) consumes the 18% smaller area than Design A by removing almost half of the links and incorporating the simple 3-port router that takes up only 48% of the normal router area. For halo networks (E, F), we assume that a 4mm × 4mm core is placed in the center of a L2 cache. Design E (a halo network connecting uniform size banks) reduces the 13% of the L2 cache area over Design B, but its L2 cache uses only about a quarter of the total die. Applying the non-uniform size banks (Design F) not only reduces 6.3 times the unused area in a die over Design E, but also consumes the only 23% of the L2 area of Design A. The main reason for its compact layout is the small size of network that requires fewer routers and links. Area estimation for Design F is based on the conﬁguration shown in Figure 10.                                                         Figure 10. 16-Spike Halo Network Design for L2 Cache 7. Concluding Remarks We have presented in this paper a domain-speciﬁc on-chip network design for large scale L2 cache systems. This research was motivated by the discernment that the network latency is signiﬁcant, while the network is occupying considerable chip area in networked cache systems. The detailed design proposed in this paper includes: (i) a singlecycle router architecture with multicasting support as the basic building block of the interconnection networks; (ii) FastLRU replacement that can reduce the network latency; (iii) appropriate deadlock-free XYX routing algorithm that requires no horizontal links in a mesh except the ﬁrst row so as to save area and power; (iv) a new network topology, called a halo network, where the MRU banks are of the same distance from the core; and (v) a halo network with non-uniform 326   sized banks, thus reducing the wasted area on the processor die. Simulation results performed on SPEC2000 benchmarks show that the proposed cache system with all the proposed techniques achieves signiﬁcant performance improvement and area efﬁciency. The important conclusions of this work are the following: First, domain-speciﬁcally designed networks for large cache systems can provide better performance than general interconnection networks once we understand the communication patterns well. Next, the average cache latency of the networked cache systems is comprised of three factors: cache bank access latency, network latency, and memory latency. Among them, the network latency occupies the largest portion; therefore, reducing the average cache latency requires a reduction in the network latency. Finally, domain-speciﬁc designs cannot only improve the performance, but also can reduce the area claimed by the cache systems. We are planning to expand the study presented in this paper to include CMP environments by ﬁrst analyzing the trafﬁc patterns and ﬁnding suitable interconnects for those systems. Another direction for future work is energy consumption analysis of the networked cache systems. We are developing an on-demand power control scheme that can dynamically turn on/off a subset of cache systems. The proposed multicast router cannot be directly adopted in a network for general workloads, because we cannot guarantee the availability of input buffers for replication. The multicast router design in a general on-chip network should be revisited. "
2008,CMP network-on-chip overlaid with multi-band RF-interconnect.,"In this paper, we explore the use of multi-band radio frequency interconnect (or RF-I) with signal propagation at the speed of light to provide shortcuts in a many core network-on-chip (NoC) mesh topology. We investigate the costs associated with this technology, and examine the latency and bandwidth benefits that it can provide. Assuming a 400 mm <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>  die, we demonstrate that in exchange for 0.13% of area overhead on the active layer, RF-I can provide an average 13% (max 18%) boost in application performance, corresponding to an average 22% (max 24%) reduction in packet latency. We observe that RF access points may become traffic bottlenecks when many packets try to use the RF at once, and conclude by proposing strategies that adapt RF-I utilization at runtime to actively combat this congestion.","CMP Network-on-Chip Overlaid With Multi-Band RF-Interconnect  M. Frank Chang‡    Jason Cong    Adam Kaplan    Mishali Naik    Glenn Reinman    Eran Socher‡     Sai-Wang Tam‡  ‡ UCLA Electrical Engineering Department  UCLA Computer Science Department  {mfchang,socher,roccotam}@ee.ucla.edu  {cong,kaplan,mishali,reinman}@cs.ucla.edu Abstract  In this paper, we explore the use of multi-band  radio frequency interconnect (or RF-I) with signal  propagation at the speed of light to provide shortcuts  in a many core network-on-chip (NoC) mesh  topology. We investigate the costs associated with  this  technology, and examine  the  latency and  bandwidth benefits that it can provide. Assuming a  400mm2 die, we demonstrate that in exchange for  0.13% of area overhead on the active layer, RF-I can  provide an average 13% (max 18%) boost in  application performance, corresponding  to an  average 22% (max 24%) reduction in packet latency.  We observe that RF access points may become traffic  bottlenecks when many packets try to use the RF at  once, and conclude by proposing strategies that  adapt RF-I utilization at runtime to actively combat  this congestion.   1. Introduction  The age of nanometer design has brought power  and thermal considerations into sharp focus, making  them high-priority architectural design metrics.  Additionally,  long wires  (such as  those  that  communicate dependency information) have become  problematic, as  their delay and  the power  consumption of repeaters is increasing relative to that  of the transistors that drive them [9]. Thus, highfrequency single-core processors have become less  attractive in the nanometer age, as their performance  gains are achieved at a tremendous energy expense.  Processor manufacturers are increasingly relying on  Chip Multi-processor (CMP) designs, where silicon  resources are partitioned among a number of  processor cores. These cores can be connected  together with a network on-chip (NoC) interconnect   that can also include the shared on-chip (but off-core)  cache hierarchy.   In order to scale future CMPs to 100's or even  1000's of cores, sophisticated interconnect topologies  will be essential in enabling low-latency application  communication and efficient cache utilization. We  find that RF interconnects have tremendous promise  in providing higher bandwidth between such a large  number of  interacting components, as well as  reducing the number of cycles required for cross-chip  communication, via signal propagation at the speed  of light. However, RF bandwidth comes at an area  cost, and cannot completely replace conventional RC  wired interconnect. Therefore we propose a twolayer hybrid NoC scheme called MORFIC (Mesh  Overlaid with RF InterConnect), where the RC wires  are analogous to city streets accommodating local  traffic, and the RF is like a superhighway, connecting  distant points on the chip. We consider the circuit  challenges remaining in bringing RF technology into  CMOS design. The contributions of this work are as  follows:  • Using real-world designs and ITRS projections as  well as a physical implementation in 90 nm IBM  process, we investigate the costs associated with onchip RF interconnect, and demonstrate a physical  roadmap for this promising technology (Section 2).   • We present the MORFIC architecture (Section 3),  and discuss the architectural decisions a designer  must make when implementing a two-layer NoC  topology (Section 4).  • We demonstrate the performance/area tradeoff of  augmenting a mesh  interconnect with various  amounts of RF-shortcut, providing an average  performance improvement of 13% (up to 18%) for an  area cost of roughly 0.13% on the active silicon  layer. This corresponds to an average 22% reduction  in the average latency experienced by each packet in  the network (Sections 5.1 and 5.3).   • We study the deadlock problem that can occur  when routing using general shortcuts, and evaluate  978-1-4244-2070-4/08/$25.00 ©2008 IEEE 191                 the application performance impact of two types of  deadlock solutions: a  turn-model based  route  restriction as well as a progressive deadlock detection  and recovery scheme (Section 5.2).   • We observe that RF access points can become  bottlenecks when many packets try to access the RF  at once, and show that this congestion can be  alleviated by statically restricting RF shortcut usage  (Sections 5.3.1 and 5.3.2).   • Finally, we propose strategies which dynamically  detect congestion at RF-I shortcuts and throttle RF-I  usage accordingly (Section 5.3.3), and conclude this  work in Section 6.  2. On-Chip RF Interconnect   RF interconnect was proposed in [4] as a high  aggregate bandwidth, low latency alternative to  traditional interconnect. Its concept and benefits of  enhancing it with FDMA and CDMA were also  demonstrated, mainly  for  off-chip  on-board  applications [5][12]. Its benefits to CMP design and  performance were not previously analyzed. On chip  RF interconnect is a new interconnect concept  proposed here for CMPs. It again offers  the  advantages of a very high aggregate bandwidth and  low latency for direct across chip communication to  improve the CMP processing speed. It also offers  lower power consumption compared with traditional  global interconnect, while using the standard digital  CMOS  technology without  additions  or  modifications. Taking full advantage of CMOS  transistors speed, RF-I performance benefits from the  CMOS technology scaling trend. It offers even  further performance benefits by statically or even  dynamically allocating the aggregate chip bandwidth  to different users by assigning the available RF  bands. In this section we propose the concept of RF  interconnect for CMPs, review prior work on RF-I,  and compare this interconnect technology to prior  proposed alternatives.  2.1. Limits of traditional on-chip interconnect  Traditional global  interconnects based on   repeater bus wires suffer from two main limitations  when considering the future needs of CMPs. The first  of these is poor latency scaling – the ITRS [16]  projects that the repeated wire delay will remain  fairly constant for future technology nodes and may  even increase. Moreover, for a global interconnect  across the chip, the required energy-per-bit of a  repeated bus does not scale well either, since the  capacitance and supply voltage scale poorly.  However, the amount of on-chip interconnect grows  rapidly with each technology generation, causing the  total power consumption of the on-chip interconnect  to rise at an alarming rate. The result is that  traditional repeated bus based global interconnects  are major power consumers with limited data rates  that do not take full advantage of the available superscaled transistor bandwidth. For example, in 90nm  CMOS technology, the typical repeater signal is  running at 4Gbit/s which requires it only occupy  about 4GHz of bandwidth. As compared with the fT  (frequency of unity current gain) of 90nm CMOS  transistors, which is about 120GHz, the traditional  buffer utilizes less than one-tenth of the total  available bandwidth.  2.2. RF Interconnect and its benefits  The concept of RF interconnect is based on  transmission of waves, rather than voltage signaling.  When using voltage signaling, the entire length of the  wire has to be charged and discharged to signify  either ‘1’ or ‘0’. In the RF approach, an electromagnetic (EM) wave is continuously sent along the  wire (treated as a  transmission  line). Data  is  modulated onto that carrier wave using amplitude  and/or phase changes. A simple and popular  modulation scheme for this application is binaryphase-shift-keying (BPSK) where the binary data  changes the phase of the wave between 00 and 1800.   Figure 1 demonstrates an example of a ten carrier  RF-I. This design uses  ten different carrier  frequencies ranging from 20GHz up to 200GHz,  where each carrier (or band) transmits a 10Gbit/s  data stream. Therefore, the total aggregate data rate  per wire in this example is 10Gbit/s per carrier × 10  carriers = 100Gbit/s per transmission line. In the  frequency domain, BPSK data modulation at a rate of  R, takes about R of bandwidth, but it requires a 2R  carrier  frequency  spacing  to decrease data  interference between channels. As a result, a total  available bandwidth of BW can be used to transmit  an aggregate data rate of BW/2. In the transmitter,  each data stream is first up-converted with individual  carrier frequency. After that, these ten up-converted  signals are then combined and coupled into the onchip  transmission  line.  In  the  receiver, each  individual channel signal is down-converted by a  selective mixer, and ten different data streams are  recovered following their respective low-pass filters.  The bottom of figure 1 shows the signal on the  transmission line in the frequency domain, with data  bandwidth centered on different carrier frequencies,   192             Table 1: CMOS sw itching speed scaling  RF CMOS vs. Tech Node  (ITRS)  fT (GHz)  fmax (GHz)  Max RF carrier frequency  (GHz)  Max Aggregate Data Rate with  RF-I (Gb/s/wire)  90nm  65nm  45nm  32nm  22nm  16nm  120  170  200  270  324 [10] 432  160  216  240  370  592  296  320  480  768  384  400  590  944  472  490  710  1136  568  utilizing the total available bandwidth much more  efficiently.  RF-I available data rates are inherently limited by  the switching speed of conventional CMOS circuits.  Faster switching devices enable faster modulation of  the signal and also increase the number of available  channels that we can exploit. There are two metrics  to describe how fast the CMOS can be switched: fT,  which is the frequency of unity current gain and fmax,  which is the frequency of unity power gain and also  referred to as the maximum oscillation frequency  achievable using  that CMOS  technology.  In  mainstream 90nm CMOS, both fT and fmax already  exceed 100GHz for NMOS devices. ITRS predicts  that in 22nm and 16nm CMOS, both fT and fmax will  be higher  than 500GHz. Recently, we have  demonstrated a 324GHz voltage controlled oscillator  (VCO) in standard 90nm CMOS technology [10],  breaking the assumed oscillation frequency barrier of  fmax. A reasonable rule of thumb to estimate and  project the aggregate data rate of RF-I in future  technology nodes would be half the maximum carrier  frequency possible in that technology. Using this rule  in Table 1, we can project a maximum aggregate data  rate as high as 568Gbit/s per wire in 16nm CMOS  technology.   We have designed a single-band RF-I in 90nm  CMOS technology, and have achieved a signal data  rate of 5Gbit/s with the carrier frequency centered at  20GHz. Table 2 summarizes the area and power  overhead for the Tx and Rx in our design at 90nm.   Table 3 demonstrates characteristics of our  implementation of a multi-band RF-I at 90nm CMOS  technology. A behavioral model simulation shows  that 10GHz channel spacing is sufficient to carry  5Gbit/s data with a low BER. The suggested channels  at the 90nm node are 10GHz, 20GHz, 30GHz,  40GHz, 50GHz and 60GHz. Therefore, the total  aggregate data rate is 30Gb/s per wire, which is at  least six times larger than the data rate of a single  traditional repeater bus.  As shown above, FDMA (frequency division  multiple access) can be used with RF-I to increase  the data rate between two users. It can also be used to  allow multiple users to connect to the same shared  transmission line and communicate concurrently  using different frequency bands. Each user has a  transmitter, a receiver or both, each of them selecting  a specific frequency band using its up-converting or  down- converting mixers. Therefore, N channels can  support up to 2N different users simultaneously  communicating with each other. Multiple channels  can even be assigned to a particular communicating  pair to increase the amount of bandwidth available  for communication.  2.3. RF-I scaling   Passive devices, such as inductors, consume the  dominant portion of the transceiver area. Since the  size of a passive device is inversely proportional to  the operational frequency, as the frequency of the  signal increases, the size of the passive device can be  scaled down (Figure 2a). At 20GHz, the size of the  inductor is approximately 50µm×50µm. However,  due to frequency scaling, the size of the inductor at  400GHz can be as small as 12µm×12µm, about a 20x  reduction in area. As long as the carrier frequency  can increase at each new generation of technology,  the transceiver area will also scale down. According  to ITRS, the fT of the NMOS transistor in 22nm  CMOS  technology will be around 400GHz.  Switching as fast as 400GHz in future generations of  CMOS will allow us to have a large number of high  frequency channels for an RF-I. In each new  technology generation, the number of channels  available on a single transmission line can be  expected to grow thanks to the faster transistors  available (shown in Figure 2b). It is assumed that the  average power consumption per transceiver channel  is expected to stay constant at about 6mW. The logic  behind the assumption is that although RF circuits at  higher carrier frequencies require more power, this  additional power is compensated by the power saved  at the lower carrier frequencies due to higher fT  transistors available with scaling. In addition to  increased number of channels, the modulation speed  of each carrier would also increase, allowing a higher  data rate per channel. As a result, the aggregate data  rate is expected to increase by about 40% every  technology node, as shown in Table 3. In addition,  the cost of the data rate, in terms of area/Gbps and  the energy consumption per transmitted bit are  expected to scale down.  2.4. Comparison with other types of on-chip  interconnect 193         e e r Data1 r Data1 w w o o P P l l a a n n g g i i S S Mixer Mixer Output Buffer Output Buffer Mixer Mixer LPF LPF r r e e w w o o P P Data1 Data1 frequency frequency f1 f1 X10  X10  TX TX Transmission  Transmission  Line Line e e r Data10 r Data10 w w o o P P l l a a n n g g i i S S frequency frequency f10 f10 l l a a n n g g i i S S frequency frequency X10 RX X10 RX e e r Data10 r Data10 w w o o P P l l a a n n g g i i S S frequency frequency f1 f1 f10 f10 Figure 1: A ten carrier RF-Interconnect and corresponding waveform at the transmission line  We compare our performance estimations to that of  the parallel bus based on ITRS [16] and to that of  optical interconnects proposed in [11] and extrapolate  to future technology nodes up to 22nm. We assume a  usage of differential transmission line with 12µm pitch  for the RF-I. The traditional bus that uses optimal  repeated wires exhibits latency of 800ps for a distance  of 2cm at 90nm technology, which gets even worse  with scaling to almost 1500ps at 22nm. RF-I and  optical interconnect maintain a fairly low and constant  latency of about 200ps that is mostly limited by wave  propagation time. The energy consumption of the bus  is expected to improve from 21pJ/bit at 90nm to  13pJ/bit at 22nm, but RF-I and optical-I are expected  to achieve an order of magnitude reduction in energy.  As opposed to optical-I energy consumption which  does not benefit from scaling, RF-I energy does  (mainly due to higher modulation rates). The data rate  density of the bus is slowly increasing with scaling  from 2Gbps/µm at 90nm to 8Gbps/µm at 22nm but  would require more buffers. In RF-I, this effect is more  dramatic (up to 12Gbps/µm at 22nm) due to addition  of carrier frequencies and increased modulation rate,  both contributed by scaling of transistors. Optical-I  also expects an increase in data rate density due to  technology, but uncorrelated to CMOS scaling and  therefore smaller. RF-I has the advantage of using the  standard digital CMOS technology, while optical-I  requires integration with on-chip and off-chip nonCMOS devices adding to package complexity and cost.  These devices are also highly temperature sensitive,  which raises even more power related issues.  3. The MORFIC architecture  In this section we propose the MORFIC (Mesh  Overlaid with RF  InterConnect) architecture: a  conventional mesh topology augmented with RF-I  enabled shortcuts to reduce communication latency. As  demonstrated in Section 2, RF interconnects can  provide scalable, low-latency communication with  conventional CMOS  technology. RF-I  is also  extremely flexible, as different frequencies on different  transmission lines can be allocated to the NoC. The  flexibility and extreme low-latency of RF interconnect  argues for the use of a shared pool of transmission  lines that can physically span the NoC. Different  points on the NoC can then access this shared  waveguide pool for rapid access to other points on the  NoC. Collectively, the RF-I can be thought of as a  “super-highway,” sending packets long distances with  very low latency. By contrast, the standard 2-D mesh  links can be thought of as the “city streets” of the chip.  RF access points (or “freeway onramps”) in the mesh  allow packets to enter the faster RF-I.   The baseline topology we consider in this paper is  shown in Figure 3a. It is comprised of a 10x10 mesh of  5-port routers, each with a local port attached to either  a processor core, an L2 cache bank, or a DRAM  interface (pictured as a circle, diamond, or plus  respectively). This design uses 64 cores, 32 cache  banks, and 4 memory interfaces, with a 4GHz system  clock. The interconnect operates at 2GHz. Each router  in the mesh (represented as a square) has a 5-cycle  pipelined latency, and routes packets using an XY/YX  scheme. In XY/YX routing, half of the packets are  routed in the X dimension first, then along the Y axis  to their destination. The other half are routed in the Y  dimension first, then along the X axis. The baseline  mesh links are 16 byte wide, single-cycle buses  194             Table 2: Power and area of single-carrier RF-I  w ith 20GHz carrier and 5Gbit/s in 90nm CMOS  TX  Mixer  PA  Total TX  RX  Mixer  Baseband  Total RX  Power (mW)  0.5  1.5  2  Power (mW)  2  2  4  Active Area  5um x 5um  10um x 10um  125um2  Passive Area  50um x 50um  50um x 50um  5000um2  Active Area  10um x 10um  20um x 20um  500um2  connecting each router to its immediate neighbors, as  well as its local attached node. We have implemented  full virtual channel support, and have given each  buffer a capacity of 8 entries. We select a 2D mesh as  our reference topology, as mesh networks allow for  regular implementation in silicon, and are simple to lay  out. Comparison against other topologies is beyond the  scope of this work, but the techniques we describe  hence could be employed on a number of designs of  this scale.   We have chosen this overall topology to reduce  long-distance communication bottlenecks on the chip.  The largest messages being sent in the network are  DRAM responses, which each carry 128-byte L2 cache  blocks from a main memory interface to a fetching L2  bank. By surrounding the DRAM interfaces with L2  cache banks, we reduce the distance that the largest  messages must travel, and reduce their spatial overlap  with traffic between cores and L2 caches. This  computation/storage spatial hierarchy, with a cluster of  L2 cache banks at the center of the chip, surrounded by  the processor cores they service, has been explored in  other designs, namely Beckmann and Wood’s CMPSNUCA [2], which surrounded a mesh-interconnected  bank-cluster with eight CPU cores.   Our shared L2 cache is a statically addresspartitioned NUCA with a directory-based MSI  coherence protocol. Our coherence protocol has been  optimized to reduce message injection via silent  evictions and reply-forwarding [6]. Furthermore, our  protocol  is  robust enough  to  tolerate network  reordering of all coherence messages, including silent  evictions and coherence acknowledgements.    As we will demonstrate, the MORFIC architecture:  • Provides scalable, low-latency performance for a  forward-thinking many-core mesh NoC topology.  • Avoids costly arbitration for RF-I frequencies  across the mesh topology.  • Allows simultaneous communication on different  frequency bands for improved bandwidth.  3.1. Related Work   Beckmann and Wood [1] introduced the use of  transmission lines for mitigating the impact of the  communication latency between L2 cache banks and  the cache controllers. They have outlined CMP  floorplans optimized for less complex circuitry, where  the cache banks reside near the edges of the chip and  cache controllers are located in the center of the chip.  Transmission lines provide a low latency shortcut  between two components distantly located from each  other. However, in future CMPs with a large number  of cores and cache banks on the die, it is essential to  extend such schemes for improving the latency of both  core-to-core as well as core-to-cache communication.  And while transmission lines provide low-latency  shortcuts in a mesh topology, they do not take  advantage of frequency divided communication.  Ogras and Marculescu  [13] explored  the  enhancement of a standard mesh network via addition  of application-specific long-range links between pairs  of frequently communicating routers. The goal of their  work is to maximize the amount of traffic that could be  injected  into  the network before saturation was  reached. Using a profile of an application’s network  traffic, their algorithm searches through all possible  shortcut permutations and then estimates the effect of  these shortcuts on the critical traffic load. In order to  avoid deadlock in routes constructed using these links,  they employ a turn-model route restriction called  South-Last (which we implement and evaluate in  Section 5.2). Unlike  the single-cycle shortcuts  employed in the MORFIC architecture, Ogras and  Marculescu implement their long-range links using a  higher-latency point-to-point pipelined bus. The  application-specific nature of these long-range links  makes them unsuitable for use in a general-purpose  architecture, and no algorithms are presented to adapt  their use to changing communication conditions.   Kirman et al.  [11] have employed optical  technology to design a low-latency, high-bandwidth  shared bus. While their design does take advantage of  low-latency and high bandwidth via simultaneous  transmission on different wavelengths, they examine  optical interconnect to augment a bus topology instead  of a more scalable mesh topology. Moreover, none of  these studies have considered dynamically adapting  shortcut utilization during an application  run.  Applications exhibit a wide range of communication  patterns, and for future CMPs with large number of  cores on chip overuse of shortcut access points can  lead to substantial congestion, as we will demonstrate.  195               (a)     (b)      (c)  Figure 2: RF-I (a) inductor scalability, (b) available channels, (c) transceiver area per Gbps  Table 3: Scaling trend of RF-I  Technology  # of  Carriers  Data  rate per  band  (Gb/s) Total  data rate  per wire  (Gb/s) Power  (mW)  Energy per  bit(pJ)  Area  (Tx+Rx)  mm2  Area/Gbps  (µm2/Gbps)  90nm 6 5 30 36 1.2 0.0107 357 65nm 8 6 48 48 1 0.0112 233 45nm 10 7 70 60 0.85 0.0115 164 32nm 12 8 96 72 0.75 0.0119 124 22nm 14 10 140 84 0.6 0.0123 88 R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R C C C C C C C C C R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R R R R R R R R R $ $ $ $ $ $ $ $ $ R RR R RR R RR R RR R RR R RR R RR R RR R RR R RR R RR R RR A A A F F F A A A C C C C C C B B B B B B D D D D D D E E E E E E G G G G G G H H H H H H F F F          (a)   (b)  Figure 3: Mesh topologies – baseline (a) and w ith RF shortcuts (b)  4. MORFIC architectural decisions  4.1. RF link  placement   Additional hardware is required at each access  point to interface with the RF transmission line (as  described in Section 2). Therefore we must decide  where to place the access points to the RF transmission  line, using the 10x10 mesh topology in Figure 3a as a  starting point.   The MORFIC architecture is realized via a rounded  Z-shaped RF transmission line connecting three routers  at each corner of the mesh to four routers in the center  of the mesh, as demonstrated in Figure 3b. The Zshaped waveguide is not just a single shared bus – it is  capable of supporting multiple simultaneous point-topoint transmissions over each RF band. The Z-shaped  topology is intended to illustrate the connection of the  transmission line to the set of transmitters and  receivers. The actual wire layout would use mainly  Manhattan geometry with smooth corners.   The placement of the RF access points in the mesh  was designed to minimize the number of cycles  between distant endpoints of the mesh. We logically  divided the mesh into five sectors – the four corner  sectors for core accesses and the center sector for L2  accesses. Using this topology, every sector can reach  every other sector through the RF transmission line.  Each lettered router in Figure 3b is equipped with an  196                                               additional sixth port, over which it can transmit and  receive data to/from the RF-I.  4.2. Packet routing   One design concern is how to determine when a  packet should make use of RF-enabled shortcuts –  standard XY/YX routing is not sufficient in these  cases. Instead, we add a routing table to each router in  our topology. The table has one entry for each  destination in the mesh (in our case 100 entries) and  has three bits per entry. Each entry indicates what  direction a packet should  travel  to get  to  the  destination – one of either five or six possible  directions depending on the radix of the router. For the  six radix routers (i.e. the routers with RF-I access), one  possible direction is to use the RF shortcut. These  routers are statically configured based on the current  topology of the mesh using an all-pairs shortest path  algorithm.   It is possible that RF-I access points could become  an NoC bottleneck if too many packets want to use the  shortcuts. This is akin to too many cars trying to get on  to a freeway at a single on-ramp. In such cases, we  would like to have some packets use XY/YX routing  instead of using RF-I – analogous to having some  drivers use surface streets when highway conditions  are congested. We have a single bit per packet which is  used to determine whether the packet will use the  routing table or XY/YX routing. This bit is set when  the packet enters the network. We will explore  different policies for setting this bit in Section 5.3.  4.3. The perils of deadlock  The RF shortcuts in our topology create cyclic  dependencies in the mesh that can lead to deadlock.  For example if router X wants to transmit a flit to  router Y, but Y’s incoming buffer for that port is full,  X will have to wait. Eventually other routers waiting to  transmit flits to X will also block, one of which could  be Y. This condition of circular dependence may lead  to a state of deadlock, where every router involved in  the cycle is waiting for an output buffer to become free  on an upstream router.  There are several techniques that can be employed  to deal with deadlock conditions. The turn-model [8]  completely avoids deadlock by making sure that the set  of allowable turns made by packets in the network  cannot form a cycle. Thus, by selecting a subset of  legal turns which a packet can make along its route,  deadlock can be avoided. We consider one turn-model  approach called South-Last, which was used by Ogras  and Marculescu in their Small World design [13].  South-Last  imposes  two  restrictions on packets  entering a router. If a packet is traveling south, it  should continue traveling southward (either SouthWest, South-East, or directly South). Also, if a packet  enters a router traveling west (entering on its east port),  it cannot be routed such that it makes a U-turn, and  travels back east. These restrictions apply to outbound  long-range shortcut links as well as to links in the  baseline mesh. With these types of restrictions in  place, circular buffer dependences cannot occur.  However, these same restrictions can potentially limit  the achievable performance of RF shortcuts, by  disallowing a packet to use them under certain  conditions.  A less restrictive option is to allow turns that form a  cycle, but to detect potential cases of deadlock and  recover from them. This strategy of deadlock detection  and recovery is based on theory presented by Duato  and Pinkston [7], which states that deadlock-free  routing can be achieved as long as a connected channel  subset is deadlock-free. In other words, there is no  need to restrict the possible turns made in the network,  as long as we are able to detect potential deadlock  conditions and react by routing packets on a reserved  emergency channel, which itself can never deadlock.  Our deadlock detection scheme works as follows. If  a source router S tries to transmit a flit of data to a  receiving router T, and T’s inbound queue is full on  the port connecting S to T, then S must block and  retransmit the flit later. In this case, S will use the same  channel to transmit a waiting-list to T. The waiting-list  is a bit-vector with one bit per router in the NoC mesh  (in this case size 100). Every bit set in the waiting-list  identifies a router that is waiting on the recipient.  When S sends a list to T, at minimum the bit in the  waiting-list corresponding to router S is set, so that T  knows that S is waiting on T. S will also set bits in the  waiting-list corresponding to any routers that are  waiting on it (as it may have received some waiting  lists as well). In this manner, each router accumulates a  list of what other routers are waiting on it. In cases of  circular dependences that lead to deadlock, a router  will eventually detect that it is waiting on itself – it  simply need detect when the bit corresponding to itself  is set in its own waiting-list. This condition raises  deadlock. Note  that  this  is a conservative and  imprecise detection mechanism, as we are really  detecting circular buffer dependence, which is a  necessary condition of deadlock.  A router sends a waiting-list whenever a message  cannot be sent due to a full buffer on the receiving  router. This waiting-list message does not interfere  with other communication because the communication   197           Table 4: Simulation parameters  Number of Cores  Fetch/Issue/Retire Width  ROB/IQ Size  Branch Misprediction  Branch Predictor  P l L1 Instruction Cache  L1 Data Cache  Core Parameters  64  6/3/3  128 / 12  6 cycles  Hybrid, 16k-entry  8 KB, 4 way, 32 byte block size, 2 ports  8 KB, 4 way, 32 byte block size, 1 port  L2 Cache Parameters  Number of Banks  32  Each bank: 256KB, 8 way, 128 byte blocks, 1 port  link would have been idle otherwise (as the receiving  router buffer is full). When T’s incoming queue  becomes un-blocked, S will send T a one time waitinglist-clear message, which contains the same contents as  the original waiting-list message. Using this, T will  know that the routers corresponding to bits set in that  message are no longer waiting on it. If some other  router U is also trying to send to T, and has transmitted  its own waiting-list to T, then bits common between S  and U’s waiting lists will naturally be set on the next  attempted send from U (assuming the clog has not  been relieved).  When deadlock occurs, all queued packets in the  network become XY-routed packets which can no  longer use shortcuts. These packets will be routed on  an emergency virtual channel, a spare virtual channel  which is only used when this condition occurs. As XYrouting does not allow turns which would form a cycle,  this spare virtual channel will not deadlock, and by  Duato and Pinkston’s theory [7] the network will  remain deadlock-free. As soon as all packets are  converted to XY routed packets which can only use the  spare VC, the deadlock condition is lowered. Only  packets injected into the network after this point will  have an opportunity to use shortcuts, unless and until  deadlock occurs again.   We implemented both South-Last turn restrictions  as well as the progressive deadlock detection and  recovery scheme described above. In Section 5.2 we  discuss the performance impact of these approaches.  4.4. Frequency band allocation  Another issue is how to allocate frequency bands in  the shared transmission-line pool. One approach would  be  to dynamically assign  frequency bands  to  communicating pairs on the fly – this would ideally  provide maximal allocation flexibility and bandwidth  utilization. It requires some arbitration mechanism to  assign frequency bands to both the sender and receiver  of data. The latency of this operation includes sending  a communication request to the arbiter, the actual  arbitration, sending the corresponding frequency to  both sender and receiver, the actual communication on  that frequency, and the signal to release the assigned  frequency. Rather than add this latency, we consider a  topology where the frequencies are distributed between  communicating pairs in the mesh topology at a coarser  granularity, amortizing  the cost of  frequency  assignment over a larger number of cycles. Frequencyband assignment could potentially be done by the  hardware or software (i.e. application or OS).   In this paper, we logically organize the shared  waveguide as eight bidirectional shortcut links. Each  lettered router in Figure 3b represents an end-point for  these bidirectional links. For example, the router  labeled A in the upper left sector of the mesh can  transmit directly to the router labeled A in the center  sector of the mesh, and vice versa, in a single clock  cycle. However, neither router labeled A is able to  transmit data directly to any other RF-enabled router  with a different letter: the A in the upper left cannot  send directly to E in the center via the RF-I. However,  a packet can be sent from A in the upper left to A in  the center via the RF-I, and then be routed west to E  over a standard mesh link.  This distribution of shortcuts allows messages in a  sector to easily hop to neighboring sectors directly via  the RF transmission-line pool. For example, at the  upper left sector, a message can either take its Clabeled shortcut router down to the lower-left sector,  its B labeled router to the upper-right sector, or its Alabeled router to the center of the mesh. In the case of  the four corner sectors, diagonal communication (i.e.  from the upper left to the lower right sector) will take  two trips on the RF transmission line (i.e. one to the  center, and then one to the destination sector).   For this paper, we assume that the available  transmission-line pool bandwidth is evenly allocated to  all eight bidirectional shortcuts for the duration of  program execution –  therefore  the  frequency  assignment is done only once for the entire application.  However, future work will consider heterogeneous  allocation of bandwidth based on run-time network  congestion. This is a natural fit for this frequency  allocation strategy: periodic coarse-grain assignment as  a means of NoC adaptation.  5. Experimental Results  We have adapted the SESC [14] framework for this  study, completely rewriting  the on-chip network  topology and L2 cache code. Our core and cache bank  simulation parameters are summarized in Table 4.  198           256B 50% usage 256B 100% usage t a L t e k c a P g v A d e z i l a 0.95 0.90 0.85 0.80 0.75 0.70 0.65 m r o N t f f i x d a r p s r e t a w 2 ^ n r e t a w u l n a e c o s e n r a b Figure 4: Latency reduction w ith 256B RF-I  A five-cycle fully pipelined router delay was  assumed, with 1-cycle delays for each stage. The  stages are routing, virtual channel arbitration, switch  allocation, switch traversal, and output link traversal.  Our simulator models network routers and their queues  with a behavioral model of store-and-forward packet  switching. A packet stalls within a router for one of  two reasons: either the outbound link is occupied, or a  buffer at the destination router is full.  We use  seven  shared-memory multithreaded  applications from the Stanford SPLASH suite [15] to  evaluate RF shortcuts: Barnes, FFT, LU, Ocean,  Radix, Water-Nsquared, and Water-Spatial. These  applications were configured to use 64 threads with  their standard input set, and each of these threads was  mapped to one of the MORFIC cores. We simulated  each benchmark by fast-forwarding to its parallel  section, and then running to completion.  5.1. Application performance benefits of RF  Interconnect  An allocation of 256B of RF bandwidth, evenly  divided between each of the eight point-to-point  shortcuts described in Section 4.4, would match the  baseline mesh bandwidth for each of these links. Put  differently, between each endpoint of an RF shortcut,  16B of link bandwidth would be available each cycle,  the same as between neighboring nodes on the baseline   mesh. In a 32nm design, this RF provision would  consume roughly 0.51 mm2 on the active silicon layer.  On a 400 mm2 die, this would be an area overhead of  0.13%.   In Figure 5a, we demonstrate the performance gain  achieved by enhancing our mesh with an overlay of  256B of RF bandwidth. In each column, the left bar  (256B 50% usage) represents the following policy:  half of the packets injected into the network use  standard XY/YX routing, and the other half proceed  along their shortest path, using the routing table  described in Section 4.2. The right bar (256B 100%  usage) represents a policy allowing all packets to  proceed along their shortest path, requiring a routing  199 table lookup at each intermediate router. Hence, we  refer to this latter policy as opportunistic shortcut  usage, as every packet is given the opportunity to  exploit RF shortcuts for latency savings. These bars  are normalized to the runtime of each application on  the baseline mesh topology, with no RF-shortcuts and  only XY/YX routing.   At an allocation of 256B RF bandwidth, higher  performance is achieved by sending more packets  along their shortest-path, as the congestion at the RF  access points is outweighed by the latency-savings  experienced for cross-chip traversal. The opportunistic  use of RF shortcuts in the 100% usage case leads to an  average performance gain of 13% on  these  benchmarks, with the highest gains experienced by  FFT and Barnes at 18%.   These gains are consistent with our investigation of  the latency and bandwidth sensitivity of these Splash-2  benchmarks on the baseline 10x10 mesh. We found  that doubling the pipelined router latency from 5cycles to 10-cycles had a drastic effect on these  benchmarks  (an average of 44% performance  degradation), whereas halving link bandwidth from  16B to 8B only degraded performance by an average  7%. RF shortcuts reduce the number of cycles required  to traverse long distances on chip, and this latency  savings translates directly into increased application  performance.  Corresponding to this performance increase, we  notice a reduction in the average latency experienced  by each packet en route from its source to its  destination. Figure 4 shows the average packet latency  for opportunistic as well as 50% usage of 256B of RFinterconnect, normalized to the average packet latency  on the baseline mesh. The latency savings vary little by  benchmark, and average 11% for 50% usage, and 22%  for opportunistic usage. Barnes experiences the largest  average latency savings, at 24%.  5.2. Performance impact of deadlock strategy  In Section 4.3 we described two strategies for  dealing with the inevitability of deadlock in a shortcutenhanced mesh network: the South-Last turn model  [13] and a progressive deadlock-detection and  recovery (DDR) scheme using an emergency virtualchannel [7]. In Figure 5b, we demonstrate how each of  these deadlock strategies performs with an allocation  of 256B of RF bandwidth. As the previous section  indicated, opportunistic (100%) usage of RF-I leads to  higher performance at 256B of allocation, therefore in  this section we continue to use RF opportunistically.  As in Figure 5a, these results are normalized to the                 1.00 0.95 0.90 0.85 0.80 0.75 0.70 t f f r x d a i w a t e r p s w a t e r n ^ 2 l u n a e c o a b r s e n N o r m a i l d e z R n u C c y l s e 256B 50% usage 256B 100% usage 1.00 0.95 0.90 0.85 0.80 0.75 0.70 t f f r x d a i w a t e r p s w a t e r n ^ 2 l u n a e c o a b r s e n N o r m a i l d e z R n u C s e c y l DDR 256B South-Las t 256B            (a)  (b)  Figure 5: (a) Performance w ith 256B RF-I bandw idth and (b) DDR vs South-Last approaches  performance of each application on the baseline mesh,  with no RF shortcuts. (Note that the results of Section  5.1 used the DDR scheme, and that the 256B 100%  usage results in Figure 5a are identical to the DDR  256B results presented in Figure 5b.)  As indicated by the results, deadlock-detection and  recovery always outperforms the South-Last turnmodel. At this bandwidth allocation, the South-Last  rules are too restrictive, and certain routes which  would have decreased packet latencies are disallowed  due to their potential to cause deadlock. As a result, the  average performance gain experienced using  the  South-Last policy is only 7%, around half of that  experienced when using a DDR strategy. Although the  DDR strategy allows deadlock to occur, the recovery is  not time-consuming, as all packets in flight will switch  to XY-routing. Additionally, deadlock is detected very  infrequently (tens to hundreds of times per application,  over many millions of cycles). However, DDR requires  additional resources to achieve its performance gains,  in the form of an extra buffer on each physical router  channel, as well as the transmission of waiting-list and  waiting-list-clear messages to blocked routers. For the  rest of this paper, we will use the DDR strategy.  5.3. Static bandwidth allocation  5.3.1. Opportunistic  shortcut  usage. As  demonstrated in Section 5.1, the opportunistic usage of  shortcuts at 256B of allocation leads to a significant  performance increase. However, such an RF allocation  may be too costly for designers. In Figure 6, we  explore several allocations of aggregate RF bandwidth,  ranging from 16B to 256B, and normalize their  runtime on each application to the baseline case (no RF  shortcuts, represented by the horizontal line at 1).  When less RF bandwidth is available, opportunistic  shortcut usage presents a problem, as application  performance can degrade by more than 400% (for  Barnes) for small RF allocations. For instance, in the  case of 16B of RF, each shortcut is only allocated a  single-byte of bandwidth in each direction. In this case,  packets naively routed on their shortest path will  congest the shortcut access points, causing queues to  fill up at shortcut entrances. This can result in massive  network congestion. At this design point, the latencysaving potential of RF is dwarfed by its negative  impact on congestion.  5.3.2. Statically restricted shortcut usage. As  opportunistic usage of RF-I can create bottlenecks at  small RF allocations, we attempt to alleviate this  congestion by allowing some, but not all, packets to  use the RF. As mentioned in Section 4.2, a single bit in  each packet can be used to determine whether the  packet will use its shortest path (which may include the  use of RF-I shortcuts), or whether the packet will use  XY/YX routing, avoiding RF entirely.  In Figure 7a, we show the performance obtained on  an aggregate 32B of RF allocation, where each  shortcut is given 2B in each direction. The bars  indicate the runtime of each application (normalized to  no RF interconnect), where 25%, 50%, 75%, and  100% of the packets entering the network are sent on  their shortest path. Figure 7b presents these same  configurations for 96B of RF allocation, where each  shortcut is given 6B in each direction. At 32B of total  RF bandwidth, performance increases as shortcuts are  used less. However at 96B, this relationship is more  complex. For some applications, such as WaterNsquared, LU, and Barnes, performance improves as  shortcuts are used more. This is due to the fact that the  performance lost to shortcut bottlenecks is outweighed   5 4 3 2 1 0 t f f r x d a i w a t e r p s w a t e r n ^ 2 l u n a e c o a b r s e n N o r m a i l d e z R n u C s e c y l 16B 32B 96B 256B Figure 6: Performance obtained by varying  RF-I bandw idth   200                                          2.5 2 1.5 1 0.5 0 t f f l s e c y C n u R d e z i l a m r o N 25% 50% 75% 100% l s e c y C n u R d e z i l a i x d a r p s r e t a w 2 ^ n r e t a w u l n a e c o s e n r a b m r o N 1.2 1 0.8 0.6 0.4 0.2 0 t f f 25% 50% 75% 100% i x d a r p s r e t a w 2 ^ n r e t a w u l n a e c o s e n r a b (a)   (b)  Figure 7: Reduced RF-I utilization for (a) 32B bandw idth and (b) 96B bandw idth     (a)     (b)     (c)       (d)  Figure 8: Number of flits sent, normalized to maximum across (a) no RF-I, (b) 256B RF-I 100%  usage, (c) 32B RF-I 100% usage, and (d) 32B RF-I 25% usage     (a)     (b)     (c)       (d)  Figure 9: Number of router stalls, normalized to maximum across (a) no RF-I, (b) 256B RF-I  100% usage, (c) 32B RF-I 100% usage, and (d) 32B RF-I 25% usage  by  the performance gain from shortcut  latency  reduction. For FFT,  the best performance  is  experienced when half of the packets use their shortest  path, and the other half use XY/YX routing.  Figure 8 and Figure 9 are color-charts depicting the  activity of each router in the 10x10 mesh, aligned as  presented in Figure 3. In Figure 8a and Figure 9a, the  routers attached to memory-interfaces are labeled M,  and in Figure 8 and Figure 9 b,c, and d, the routers  attached to RF-I access points are labeled as in Figure  3b. Figure 8 depicts the number of flits sent at each  router, and Figure 9 depicts the number of cycles that a  router must stall while waiting for an occupied  outbound link. These values are reported for the  benchmark Water-Nsquared, for no RF-I (in a), 256B  RF-I used opportunistically (in b), 32B of RF-I used  opportunistically (in c), and 32B of RF-I where 25% of  the packets are routed along their shortest path. The  square representing each router is shaded relative to  the maximum value across each configuration (where a  lighter shade represents more activity). Comparing the  two figures, it is clear that when no RF-I is used, the  routers attached to L2 cache banks and memory  interfaces experience the most stalls, and send the most  flits. When shortcuts are used opportunistically, they  send the most flits, but also experience a great number  of stalls. At 32B of opportunistic usage, where each  shortcut is only allocated 2B in each direction, it is  clear that stalls are concentrated around the shortcut  access points. However, when shortcut usage is  restricted to 25%, the concentration of traffic becomes  more spread out, as does the pattern of stalled routers.  5.3.3. Dynamic restricted shortcut usage.  Based on  the results from the previous section, we note that no  shortcut-usage restriction fits all the applications for a  given amount of RF bandwidth. Some applications  may experience better performance when sending  201                                                                   more packets along their shortest path route, whereas  others may experience more congestion, and require  further RF restriction.  We have explored two simple strategies to try and  locate the optimal shortcut restriction at runtime: one  which searches for the optimal RF-I shortcut utilization  one time at the beginning of the parallel section of  each application, and another which continuously  adapts shortcut utilization  throughout application  execution.  The performance impact of each of these  strategies is presented in an extended version of this  work [3]. We find that the ability to adapt to changing  network conditions  leads  to better application  performance.  6. Summary  In this work, we have motivated the use of multiband RF-interconnect as a low-latency alternative to  traditional on-chip interconnect in CMP architectures.  Starting with a physical implementation in 90 nm  process, we have applied ITRS projections to show   how RF-I will scale to future process technologies, and  evaluate its potential to boost the performance of  shared-memory multithreaded applications on a CMP.  Assuming a 400mm2 die in 32 nm process, we have  demonstrated that in exchange for 0.13% of area  overhead on the active layer, RF-I can provide an  average 13%  (max 18%) boost  in application  performance, corresponding to an average 22% (max  24%) reduction in packet latency. We have also  evaluated two different approaches to deadlock, and  found  that deadlock detection  and  recovery  outperforms a restrictive deadlock avoidance strategy  on this topology. We have also noticed that RF access  points may attract too much traffic if strict shortestpath routing is used, and have proposed to avoid these  bottlenecks by detecting and reacting to network  congestion.  Acknowledgments  This research was supported in part by NSF grant CCF0133997, Semiconductor Research Corporation grant 2005TJ-1317, and DARPA grant SA5430-79952. The authors  acknowledge the support of the GSRC Focus Center, one of  five research centers funded under  the Focus Center  Research Program. We wish to thank our reviewers for  useful feedback.  "
2008,Regional congestion awareness for load balance in networks-on-chip.,"Interconnection networks-on-chip (NOCs) are rapidly replacing other forms of interconnect in chip multiprocessors and system-on-chip designs. Existing interconnection networks use either oblivious or adaptive routing algorithms to determine the route taken by a packet to its destination. Despite somewhat higher implementation complexity, adaptive routing enjoys better fault tolerance characteristics, increases network throughput, and decreases latency compared to oblivious policies when faced with non-uniform or bursty traffic. However, adaptive routing can hurt performance by disturbing any inherent global load balance through greedy local decisions. To improve load balance in adapting routing, we propose Regional Congestion Awareness (RCA), a lightweight technique to improve global network balance. Instead of relying solely on local congestion information, RCA informs the routing policy of congestion in parts of the network beyond adjacent routers. Our experiments show that RCA matches or exceeds the performance of conventional adaptive routing across all workloads examined, with a 16% average and 71% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. Compared to a baseline adaptive router, RCA incurs a negligible logic and modest wiring overhead.","Regional Congestion Awareness for Load Balance in Networks-on-Chip Paul Gratz† Boris Grot § Stephen W. Keckler § † Department of Electrical and Computer Engineering The University of Texas at Austin pgratz@cs.utexas.edu §Department of Computer Sciences The University of Texas at Austin {bgrot, skeckler}@cs.utexas.edu Abstract Interconnection networks-on-chip (NOCs) are rapidly replacing other forms of interconnect in chip multiprocessors and system-on-chip designs. Existing interconnection networks use either oblivious or adaptive routing algorithms to determine the route taken by a packet to its destination. Despite somewhat higher implementation complexity, adaptive routing enjoys better fault tolerance characteristics, increases network throughput, and decreases latency compared to oblivious policies when faced with nonuniform or bursty trafﬁc. However, adaptive routing can hurt performance by disturbing any inherent global load balance through greedy local decisions. To improve load balance in adapting routing, we propose Regional Congestion Awareness (RCA), a lightweight technique to improve global network balance. Instead of relying solely on local congestion information, RCA informs the routing policy of congestion in parts of the network beyond adjacent routers. Our experiments show that RCA matches or exceeds the performance of conventional adaptive routing across all workloads examined, with a 16% average and 71% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. Compared to a baseline adaptive router, RCA incurs a negligible logic and modest wiring overhead. 1 Introduction Moore’s law has steadily increased on-chip transistor densities and enabled the integration of dozens of components on a single die. These components include regular arrays of processors and cache banks in tiled chip multiprocessors (CMPs) and heterogeneous resources in systemon-chip (SoC) designs. One outcome of greater integration is that interconnection networks have started to replace shared buses and other forms of communication featuring long, global wires. Networks-on-chip (NOCs) scale better than traditional forms of on-chip interconnect, and enjoy superior performance and fault tolerance characteristics [6]. ∗Mr. Gratz and Mr. Grot contributed equally to this paper; their names are placed alphabetically in the author list. NOCs can be constructed using nearest-neighbor pointto-point links with large bit-width, facilitating naturallypipelined, high-bandwidth communication [14, 15]. To date, most NOCs have employed simple topologies such as two-dimensional meshes [35, 24, 34] and rings [23], in part because both designs are good matches to planar silicon manufacturing processes and support short link lengths. Minimizing router overheads, NOCs tend to use simple router designs with limited virtual channels, shallow ﬂit buffers per virtual channel, short router pipeline stages, and messages with a limited number of ﬂits. While the abundance of on-chip wires enables wider physical channels, wormhole ﬂow control will likely dominate due to the shallow virtual channel buffers. NOCs tend to employ simple oblivious routing algorithms, such as dimension order routing (DOR). While such oblivious routing algorithms are easy to implement in hardware, they often do a poor job of balancing the load across the links. Adaptive routing has been employed in multichip interconnection networks as a means to improve network performance and to tolerate network link or router failures. Despite additional implementation complexity, adaptive routing is appealing for emerging NOCs with an increasing number of connected elements. Performance can improve by routing around pockets of congestion and ﬂattening the distribution of trafﬁc among the links. In both cases, the improvement is realized through increased load balance, which smoothes out non-uniformities in the original trafﬁc pattern. However, adaptive routing requires network path diversity between source and destination nodes to facilitate load balance. The availability of network path diversity depends on the topology of the network, the trafﬁc pattern, and whether non-minimal routes are allowed. The key inhibitor to performance in existing adaptive routers is an ignorance of global network state, leading to router output port selection based only on locally-available congestion estimates. Such short-sighted routing decisions tend to upset global load balance in many trafﬁc patterns. In this paper, we introduce Regional Congestion Awareness (RCA), an approach that propagates congestion information across the network in a scalable manner, improving the ability of adaptive routers to spread network load. RCA 978-1-4244-2070-4/08/$25.00 ©2008 IEEE 203 aggregates locally computed congestion metrics with those propagated from neighbors before transmitting them to upstream routers. The aggregation process naturally weighs contention information by distance from the current node so that nearby congestion inﬂuences routing more than distant congestion. We present three variants of RCA that simplify design by considering only relevant slices and regions of the network when aggregating congestion metrics. RCA matches or exceeds the performance of conventional adaptive routing across all workloads examined, with a 16% average and 71% maximum latency reduction on SPLASH-2 benchmarks running on a 49-core CMP. RCA has a negligible impact on router area and no impact on its critical path, compared to a conventional adaptive router. Section 2 summarizes relevant related work in adaptive routing for both on-chip and inter-chip interconnection networks. Section 3 outlines the design of our baseline router as a point of reference and describes the new elements required for capturing congestion metrics. Section 4 describes the RCA algorithms and variants that capture different degrees of network congestion. Section 5 presents performance results of RCA along with several sensitivity studies and Section 6 concludes. 2 Background and Prior Work A paramount concern for any routing scheme, oblivious or otherwise, is its ability to balance network loads. Much research has gone into designing oblivious routing algorithms with provable worst- and average-case behavior [33, 20, 32, 26]. While these analyses typically assume a healthy network and a static load, interconnection networks frequently have non-uniform (bursty) injection rates and time-varying communication patterns [14], leading to temporary pockets of congestion known as hotspots. Schemes that have some ﬂexibility with respect to route choice, provide advantages over oblivious approaches that are not able to adapt to the communication pattern and network state. Adaptive routing is a technique for fault tolerance and congestion avoidance, successfully used in commercial multiprocessors from IBM [1], Cray [25], and Compaq [18]. Non-minimal adaptive routing has the potential to improve load balance beyond the limits of minimal routing [5, 27], but at the cost of greater implementation complexity and potentially higher per-packet latency and energy. Thus, we restrict our evaluation to minimal routing, but the general principles presented here could be applied to non-minimally routed networks as well. 2.1 Routing Policies The routing policy determines the dynamic path taken by a given packet through an adaptively-routed network.    	      ,69>?@ 1-=4 0?C:EF9 1-;4 A 8BCD6B 124      	    +E: NE: OEHEH 7H9B:P 12Q4 GFHH LEMMHF J?E9: 1-34 GFHH 5I J?E9: 1K4       ' ("")!%% $ !%& !  !""#   + -. /,0 1234 567 869: 1;;4 ,+<< 12=4 *+, Figure 1. Taxonomy of routing policies with respect to congestion avoidance. Figure 1 presents a taxonomy of routing policies. Adaptive routing policies can be classiﬁed as either congestionoblivious or congestion-aware, based on whether they take output link demand into account. Given a set of free and legal output ports, random [10] and zigzag [2] routing policies respectively choose an output direction randomly or based on the remaining hop count in each dimension, while noturn [13] seeks to avoid unnecessary turns by following a dimension until it is either exhausted or blocked. Congestion-oblivious routers are inherently unable to balance the load on many important trafﬁc patterns, because they do not consider the congestion status of available ports. Congestion-aware routing policies seek to address this shortcoming. Dally and Aoki proposed to use the number of free virtual channels at an output port as a contention metric, with the routing algorithm favoring the port with the largest number of available VCs [5]. Their evaluation compared this approach to congestion-oblivious zigzag and no-turn routing and showed that congestion awareness yields lower latency and competitive throughput. More recently, Kim et al. examined buffer availability at adjacent routers as a congestion metric [16], while Singh et al. used the output queue length for the same purpose [27, 28]. Congestion-aware routing policies can be further classiﬁed based on whether they rely on purely local congestion information or take into account congestion status at other points in the network. In this context, local information is deﬁned as information readily available at a given node, representing the status of that node or its immediate neighbors. For instance, GOAL [27] uses the queue length at each output port as its local congestion indicator during the routing phase, while GAL [28] uses the same metric for both quadrant selection and routing. A count of available virtual channels or buffers on the other end of a physical link is also local information, since it is al204 ready maintained for ﬂow control. We deﬁne non-local information as originating beyond a node’s immediate neighbors. To the best of our knowledge, existing evaluations of adaptively-routed interconnection networks are either congestion-oblivious or only consider local congestion indicators in their output port selection. Regional Congestion Awareness (RCA) is the ﬁrst work to present a comprehensive evaluation of the utility of non-local information for improving the dynamic load-balancing properties of fullyadaptive minimally-routed networks. 2.2 Congestion Management Some researchers proposed combining oblivious routing with various congestion management strategies to improve network performance. RECN dynamically allocates separate queues for ﬂows implicated in causing congestion upstream, thus avoiding head-of-line blocking due to these ﬂows [9]. Distributed routing balance (DRB) seeks to distribute obliviously-routed trafﬁc by choosing one of several possible paths for each packet based on the expected latency of each route [11]. Both of these approaches depend on each packet injected into the network to follow a predetermined path – a limitation that adaptive routing does not have. Finally, injection throttling aims improve the throughput of a network under high load by limiting injection of new packets [4, 31, 21]. Similar to congestion-aware adaptive routing, injection throttling requires knowledge of network state; however, the type of information and the way it is used is different from RCA. indicating the start of a new packet, it proceeds to the routing stage, which determines the output port that the packet will use. In the following cycle, the header ﬂit attempts to acquire a virtual channel for the next hop. Upon successful VC allocation, the header ﬂit enters the switch arbitration stage, where it competes for the output port with other ﬂits from the router. Once crossbar passage is granted, the ﬂit traverses the switch and enters the channel. Subsequent ﬂits belonging to the same packet can proceed directly to switch allocation, skipping the RT and VA stages. To reduce the impact of router pipeline delay, researchers have developed route look-ahead, which performs routing one hop in advance and reduces the required number of stages from four to three [12]. Another latency-hiding approach is speculation, which allows switch allocation to be overlapped with VC allocation [22]. If both allocation requests are granted, the latency of switch arbitration is hidden. When coupled with route look-ahead, speculation reduces the pipeline length to two cycles in the best case. Mullins et al. demonstrated that additional speculation reduces router latency to a single cycle if crossbar traversal is optimistically initiated in parallel with VC and switch allocation [19]. The speculation is successful only at low loads; mis-speculation incurs a one-cycle penalty. In this paper, we use a 2-cycle adaptive router design based on preselection. While we expect that the mechanisms for adaptivity are compatible with a single-cycle router, a proof is orthogonal to this work. With a one-cycle channel delay, the zero-load latency of the design is three cycles per hop for the baseline DOR router. 3 Network-on-Chip Routers 3.2 Adaptive Router Microarchitecture This section details the microarchitecture of a conventional network-on-chip router and describes the modiﬁcations necessary to support adaptivity. While we restrict the discussion to a 2D mesh, this topology is not an inherent limitation of the design. 3.1 DOR Router Microarchitecture The canonical NOC virtual channel router was ﬁrst described by Peh and Dally [22]. The router is input-queued and has ﬁve ports, of which four are network ports and one is an injection port. Key architectural elements of the router include the virtual channel FIFOs, route computation unit, VC allocation logic, crossbar allocation logic and the crossbar itself. The pipeline consists of four stages: route computation (RT), VC allocation (VA), switch allocation (XA), and crossbar traversal (XB). In this architecture, a ﬂit enters the router through one of the network ports and is stored in a VC FIFO, which has been reserved at the upstream node. If the ﬂit is a header, Given NOC’s extreme sensitivity to latency, any modiﬁcations to the router microarchitecture must minimally affect router pipeline delay. Thus, adaptive routing is attractive only if it does not increase the per-hop latency. A key difference between an adaptive router and an oblivious one is that more than one legal port may be produced by the route computation unit; therefore, port selection must precede VC allocation. Two challenges complicate this process. (1) With route look-ahead, a newly arrived packet proceeds directly to VC allocation, leaving no opportunity to hide the latency of port selection prior to the VA stage. (2) VC allocation is typically on the critical path, so any major impact to the latency of this stage is undesirable. Kim et al. proposed an elegant solution which relies on precomputation to select the preferred output direction for each packet a cycle in advance [16]. This strategy takes advantage of the fact that in a minimally routed 2D mesh, every packet travels in one of four quadrants: NE, NW, SE, and SW, with each quadrant having exactly two possible output directions, excluding the local port. The output port 205 Z[ RSTU VWX YWVU (a) RCA 1D (b) RCA Fanin (c) RCA Quadrant Figure 3. Regional Congestion Awareness overcome the limitations of conventional adaptive routers, which we term locally adaptive. RCA is a family of scalable light-weight mechanisms for integrating congestion information from different points in the network into the port selection process. RCA does not require centralized tables, all-to-all communication, or in-band signaling that contributes to congestion. Instead, RCA uses a low-bandwidth monitoring network to propagate congestion information among adjacent routers. At each network hop, the router aggregates its local congestion estimate with that of neighboring nodes. The new congestion estimate is used for port preselection and is propagated upstream. The aggregation step weighs contention information based on distance from the current node, reducing the negative effects of staleness and avoiding interference from non-minimal paths. The proposed scheme can be trivially integrated into the pipeline of a conventional locally-adaptive router, with negligible impact on area and no effect on its critical path. 4.1 RCA Variants We examine three promising RCA variants with different cost-performance characteristics. RCA 1D: This simple design aggregates and propagates congestion information along each dimension independently. RCA 1D offers excellent visibility along the axes bounding a packet’s routing quadrant, but provides no direct knowledge of network status from the middle of the quadrant. Figure 3(a) shows how RCA 1D propagates congestion status in the West direction. While offering only limited visibility into the network, this approach has the lowest implementation complexity in the RCA design space. RCA Fanin: The goal of RCA Fanin is to provide more information about network state than RCA 1D at minimal logic overhead. RCA Fanin provides a coarse view of regional congestion by aggregating congestion estimates along the axis of propagation with those from orthogonal directions as shown in Figure 3(b). While RCA Fanin encompasses signiﬁcantly larger regions of the network than RCA 1D’s uni-directional congestion vectors, it also introduces noise into its estimates by combining information from mutually exclusive routing quadrants. RCA Quadrant: Depicted in Figure 3(c), RCA Quadrant aims to maximize the accuracy of congestion estimates by maintaining separate congestion values for each network quadrant. Doing so reduces the noise caused by combining information from mutually exclusive routing regions that exist in RCA Fanin while maximizing the coverage as compared to RCA 1D. Since each port belongs to two different quadrants, two separate congestion values must be received, updated and propagated at each network interface, incurring twice the overhead in logic and wiring complexity as either RCA 1D or RCA Fanin. 4.2 RCA Microarchitecture We modify only the conventional locally adaptive router’s port preselection logic in RCA’s implementation, maintaining its simplicity and low latency. As discussed in Section 3.2, port preselection has low logic complexity, permitting integration of additional functionality with no impact on cycle time. Figure 4 shows the modiﬁcations to the 2-stage adaptive router for RCA. The two new modules we add are congestion status Aggregation and Propagation. Aggregation: In a conventional adaptive router, local congestion estimates serve as inputs to the port preselect logic. With RCA, the port preselect logic remains unmodiﬁed, but its inputs are generated by the aggregation module, which combines local and non-local congestion estimates. An aggregation module resides at each network interface in all RCA variants, although RCA Quadrant has two such modules per port. Figure 5(a) shows the aggregation module in detail. Inputs to the aggregation module come from downstream routers and the local CVRs, reﬂecting the local congestion estimate. Aggregation logic combines the two congestion values, potentially weighting one value differently than the other, and feeds the result to the port preselect logic and the propagation module. The exact weighting of local and non-local congestion estimates determines the dynamic behavior of the routing 207     |{ {z}  {{z  ¡ ¢ £¤Figure 4. RCA Adaptive Router.   y{y xyz{ |}~ }|{    y{y  { y{ ~~~{ |y} y y}~ { | x y~{ |y}y}~{ |y} {{z  }{ yz{ y}~{ |y} {{z  y  x {y  }{ yz{ policy. Placing more emphasis on local congestion information moves a design toward the locally-adaptive end of the spectrum. Too much weight on the non-local data increases the risk of making decisions based on remote parts of the network that may be unreachable with minimal routing. We performed a detailed empirical evaluation to determine the proper weighting of local versus non-local information, and found that the simplest assignment of weights, 50-50, is the most consistent performer across a wide set of benchmarks. Thus, aggregation is a simple matter of ﬁnding the arithmetic mean of local and non-local values, efﬁciently computed via an add and a right-shift. The 50-50 weight assignment makes sense, since information from nearby nodes is emphasized more than information from farther downstream in potentially unreachable network regions. Propagation: Transmission of congestion information to adjacent nodes is performed by the propagation module, which combines congestion values computed by the router’s aggregation units to reﬂect conditions along a given dimension, quadrant, or any other set of ports. The exact function of the propagation module differentiates the RCA variants from one another. Figure 5(b) details the propagation module for RCA Fanin. At a high level, a packet arriving at a given input port can leave toward one of two quadrants. The straightline path from a given input to an output lies in both of those quadrants, while a turn corresponds to just one of the quadrants. For instance, a packet arriving at the East input may route to either the NW or SW quadrant, so the probability of the West port being a legal output is higher than either the North or the South. The propagation module for RCA Fanin accounts for this effect by assigning 50% of the weight to the straight-line path and 25% to each of the other possible outputs. RCA Fanin’s propagation logic consists of two adders and two ﬁxed shifters. The ﬁrst adder-shifter pair averages the congestion estimates from the orthogonal directions, while the second combines this average with the µ§¶° µ¶¬·¬ ª¬¨° µ¶§¸©®©° ­§² ËÌº° ¸º° ÊÊ « ¬ ­®¯°¥¥±§²³ ª§¨© ª ¥¥¦§¨© ª ¹ã ä ¹§²®¬·° ­§² ¦§¨© ª ¿®® ¶¬®©° ­§² ´´ « ¬ ­®¯° ½§« ²·° ¶¬©¼ ¹§²®¬·° ­§² ·°©° º· » ¶§¼ ¾¬° ¶­¨ ²§½¬ ÏÐÑ ÒÓÔ ÔÕÕÖ×ÕÐØ ÙÚÛ ÜÚÝÞß× ÊÊ ÍË É ± ÀÁÁÂÃÁÄÅ ÆÇÈ ¹§²®¬·° ­§² ¦§¨© ª µ¶§¸©®©° ­§² ÏàÑ ÒÓÔ áÖÚâÐÕÐØ ÙÚÛ ÜÚÝÞß× ¨§²°¬²° ­§² Î© ªº¬· ±§²³ ª§¨© ª ¼¬° ¶­¨· Figure 5. Generic RCA aggregation module (a) and RCA Fanin propagation module (b). straight-line congestion value, creating the desired weight distribution. The propagation module for RCA Quadrant is simpler than RCA Fanin’s, as it requires only one adder and a shifter to average the aggregated congestion estimates for a given quadrant. For RCA 1D, the aggregated congestion values from each port are forwarded upstream unmodiﬁed, eliminating the need for a propagation unit. 4.3 Status Network Design All RCA variants must satisfy two conﬂicting goals: low network bandwidth and high congestion status resolution. The latter is key to early congestion detection. The averaging step in each router’s aggregation unit limits the bitwidth of a congestion estimate, but also leads to information loss, as one bit of congestion data is discarded per hop. With N bits of precision in a congestion estimate, a newly-aggregated value is completely discarded in N hops. To ensure that congestion information is not phased out too rapidly, the router normalizes local values by left-shifting them prior to aggregation. Normalization can be accomplished by folding the additional shift distance into the local weight adjustment in the aggregation module shown in Figure 5(a). The shift amount determines the minimum number of hops that a given congestion value will be “live.” Empirically, we established that a shift distance of ﬁve seems to work well for our baseline 8x8 mesh. While we do not tune this parameter for any of the benchmarks, different mesh sizes and packet length distributions could likely beneﬁt from some amount of tuning. 208 Characteristic Topology Routing Router uArch Per-hop latency Virtual channels/Port Flit buffers/VC Packet length (ﬂits) Trafﬁc workload Simulation warmup (cycles) Analyzed packets Baseline 8x8 2D Mesh Minimal, fully-adaptive, reserved VC deadlock avoidance [8] Two-stage speculative 3 cycles: 2 cycles in router, 1 cycle to cross channel 8 5 1–6 (uniformly distributed) transpose, bit-complement, uniform random, self-similar 10,000 100,000 Variations 4x4 Mesh, 16x16 Mesh – – – 2; 4 – 1; 1–15 Permutations; SPLASH-2 traces – 200,000; whole trace Table 1. Baseline network conﬁguration and variations. Assuming that a congestion metric can be summarized in three bits, plus ﬁve additional bits for normalization, both RCA 1D and RCA Fanin require eight bits per link; RCA Quadrant doubles this number to 16 bits. Given that current NOC designs feature channel widths on the order of 128 bits [14], RCA wire overhead represents just 6% for 1D and Fanin and 12% for Quadrant. While NOCs are not generally wire limited, it may sometimes be necessary to reduce this overhead. One way to lower RCA bandwidth requirements serializes congestion updates. We experimented with a monitoring network that reduces RCA’s bandwidth demand at the cost of lower update frequency. Across all of our benchmarks, results show that even bit-serial status networks (one bit per channel for RCA 1D and RCA Fanin, two bits for RCA Quadrant) do not cause noticeable performance degradations compared to a full-width RCA design. Thus, low-bandwidth RCA can be deployed in wire- or pinconstrained environments, provided trafﬁc patterns are stable enough to tolerate reduced update frequency. 5 Evaluation We evaluated the three RCA variants using both synthetic and real workloads, comparing them to oblivious and local adaptive routing techniques. We also examined RCA’s sensitivity to a variety of network parameters. 5.1 Methodology We use a cycle-accurate network simulator that models the two-cycle router microarchitecture from Section 3. The router model is instrumented to collect the congestion metrics proposed in Section 3.3 and supports all RCA variants. We measure the performance of three baseline architectures: (1) DOR, a dimension-ordered oblivious router; (2) Local, a locally adaptive router that uses the vc congestion metric; and (3) Local Best, which is an adaptive router that uses our xb vc combined congestion metric. RCA 1D, RCA Fanin, and RCA Quadrant also use the xb vc congestion metric. Table 1 details the baseline network conﬁguration, along with the variations used in the sensitivity studies. 5.2 Workload We evaluate regional congestion awareness using four standard synthetic trafﬁc patterns: transpose, bitcomplement, uniform random and self-similar. These workloads provide insight into the relative strengths and weaknesses of the different congestion metrics and aggregation techniques. They represent adversarial, friendly, and nominal workloads for adaptive routing algorithms. Except for self-similar, all synthetic trafﬁc patterns use a uniform random injection process. The self-similar trafﬁc pattern uses a randomly generated fractional Gaussian noise distribution with a Hurst constant value of 0.8 for both the injection process and the source/destination node generation [7]. Permutation patterns, in which clusters of nodes communicate among themselves for extended intervals, are common in multiprocessor applications. We evaluate RCA on 100 randomly generated directed communication graphs at 30% injection bandwidth using the methodology similar to that of Singh and Dally [27]. Finally, we evaluate RCA on trace driven trafﬁc generated from SPLASH-2 benchmarks [29], representing a typical CMP scientiﬁc workload. The traces were obtained from a forty-nine node, shared memory CMP system simulator, arranged in a 7x7 2-D mesh topology [17]. We conﬁgured our network simulator to match the environment in which the traces were captured. 5.3 Evaluation of Regional Congestion Awareness Metrics Standard Synthetic Loads: Figure 6 contains a set of load-latency graphs for the RCA variants compared to DOR and Local across each synthetic trafﬁc pattern. Saturation bandwidth is measured as the point at which the average packet latency is three times the zero load latency. As expected, Local provides an improvement in throughput over 209 (a) Transpose Trafﬁc (b) Bit Complement Trafﬁc (c) Uniform Random Trafﬁc (d) Self-Similar Trafﬁc Figure 6. Load-latency graphs comparing RCA, locally adaptive, and oblivious routing. DOR on transpose and self-similar trafﬁc. Load imbalances caused by DOR with these trafﬁc patterns are egregious enough that Local metrics can detect and compensate for them. Also as expected, DOR outperforms Local on bit-complement and uniform random trafﬁc. These trafﬁc patterns are uniformly distributed with DOR, and Local’s greedy behavior causes a signiﬁcant throughput reduction. Local Best performs marginally better than Local across all trafﬁc patterns, although the variance is under 5%. The RCA variants show very little difference across the synthetic workloads, although typically RCA Quadrant performs best, followed closely by RCA Fanin and RCA 1D. The one exception is with bit-complement, in which RCA 1D outperforms both RCA Fanin and RCA Quadrant. With bit-complement trafﬁc, load is a direct function of the distance from the bisection of the network. RCA 1D only considers uni-directional congestion vectors, enabling it to keep trafﬁc ﬂowing in lanes, similar to DOR. The RCA schemes, as compared to Local, show improvement in throughput across all trafﬁc patterns with no sacriﬁce in latency. The largest gain is observed on bitcomplement, where RCA shows a 23% throughput improvement over Local, although it remains 8% shy of DOR. RCA is unable to match DOR’s throughput because bitcomplement trafﬁc is ideally balanced under DOR routing. On all other synthetic trafﬁc patterns, including the statistically balanced uniform random, RCA outperforms both DOR and Local by detecting transient load imbalances from afar and adjusting its routing decisions accordingly. Permutation Trafﬁc: Figure 7 shows the packet latency averaged across 100 random permutations at 30% injection bandwidth. All adaptive approaches outperform DOR by dynamically adjusting routing decisions in response to each pattern’s characteristics. RCA schemes do a better job of globally balancing the load than Local methods, yielding lower average latencies as a result. Among adaptive schemes, RCA Quadrant performs best, followed in order by RCA Fanin, RCA 1D, Local Best, and Local. Although the absolute latencies are not meaningful due to the arbitrary choice of injection bandwidth, the results show the relative 210 Figure 7. Average latency for 100 permutations of random pair trafﬁc at 30% injection bandwidth. Error bars show the 95% conﬁdence interval of the mean. performance of the different approaches on this workload. SPLASH-2 Benchmark Trafﬁc: Figure 8 shows the average packet latency across eight SPLASH-2 benchmark traces, normalized to DOR, grouped into uncontended and contended categories. In uncontended benchmarks (barnes, ocean, radix, and raytrace) contention forms less than 15% of the total packet latency. Contention is the cause of signiﬁcant packet latency in fft, lu, water-nsquared, and waterspatial; thus adaptive routing has an opportunity to improve performance. The ﬁnal two clusters of bars in Figure 8 show the geometric mean across all benchmarks and across the contended benchmarks. Although RCA variants provide equal or lower latency than Local schemes, RCA shows the greatest beneﬁt on water-spatial, with a 71% reduction in latency. This application’s trafﬁc contains a single, localized hotspot which RCA detects, allowing it to route packets around it before they encounter congestion. On average, RCA provides a latency reduction of 16% across all benchmarks, and 27% across contended benchmarks versus Local. All three RCA variants show similar performance on these benchmarks. 5.4 Sensitivity to Network Design Point Individual network implementations are likely to vary from the baseline designs of the previous section, depending on the needs of the system. Here we present variations that provide insight into the performance of RCA metrics in different environments. We show results for only the bitcomplement trafﬁc pattern, which we choose because, as an adversarial trafﬁc pattern for adaptive routing, it can give us better insight into RCA’s relative performance against both Local and DOR. The graphs in this subsection may be compared against the baseline conﬁguration with bitcomplement trafﬁc in Figure 6(b). In our experiments with this trafﬁc pattern, the variance between RCA schemes is under 5%, so only RCA 1D is shown in subsequent ﬁgures. Figure 8. Average latency across SPLASH-2 benchmarks normalized to latency of DOR. Network Dimension: On-chip networks are likely to exhibit a great deal of variation in size from design to design. Figure 9 shows load-latency graphs for two different network sizes: 4x4 and 16x16. The results for the 4x4 mesh, in Figure 9(a), show that RCA performs very well, achieving 25% better throughput than Local and slightly exceeding that of DOR. On smaller networks, RCA provides excellent visibility into the congestion state of the network, allowing it to capitalize on the transient hotspots caused by the random injection process. Figure 9(b) shows the results for the 16x16 network. On this trafﬁc pattern, adaptive approaches do not perform as well versus DOR. The performance loss of RCA relative to DOR is caused by a reduced visibility horizon and increased noise in congestion estimates due to a large network diameter. Network size has a stronger effect on Local than RCA, allowing RCA to maintain a lead of approximately 25%. Packet Length: Figure 10 shows load-latency graphs for very short (1 ﬂit) and longer (1-15 ﬂits) packets. Short packets, shown in Figure 10(a), represent an NOC where many small values are transfered, such as in a scalar operand network [30]. Compared to the baseline bitcomplement results in Figure 6(b), the gap between the adaptive approaches and DOR is somewhat larger. RCA continues to perform well relative to Local, showing a 15% improvement in its saturation bandwidth. Single-ﬂit packets cause highly transient network congestion which is difﬁcult for adaptive routing to exploit, increasing the gap between all adaptive routers and DOR. The larger distribution of packet lengths (from 1 to 15 ﬂits) in the experiment shown in Figure 10(b) are more representative of packet sizes found in networks for memory trafﬁc. The average packet latencies for both the adaptive and DOR routers are signiﬁcantly higher for long packets than for short, even discounting the latency due to packet length. The increased latency is a known effect of wormhole routing with long packets, where imbalances in resource utilization arise because packets hold resources over multiple routers. RCA capitalizes on this phenomenon to provide an 211 (a) 4x4 Mesh (b) 16x16 Mesh Figure 9. Load-latency graphs for 4x4 and 16x16 meshes with bit-complement trafﬁc. (a) Short Packets (1 ﬂit) (b) Long Packets (1-15 ﬂits) Figure 10. Load-latency graphs for with short and long packets with bit-complement trafﬁc. accurate picture of network utilization and improve routing decisions, almost matching the performance of DOR. Virtual Channel Count: Figure 11 shows a loadlatency graph for a modiﬁed baseline conﬁguration with the virtual channel count reduced to four. RCA continues to perform signiﬁcantly better than Local, delivering an improvement of 18% in throughput, although the performance gap is reduced. Fewer virtual channels, and by extension fewer ﬂit buffers, reduce the resolution of various contention metrics and cause diminished performance in RCA. Another issue is the imbalance in virtual channel utilization caused by the presence of the escape VCs in the Y direction. The escape VCs are reserved for packets on the last leg of their network traversal and cannot otherwise be used. Our contention metrics do not account for the special status of these VCs, and end up providing a misleading picture of resource availability. The attenuating effect of reserved VC’s on the accuracy of congestion estimates is ampliﬁed as the number of VCs is reduced, a trend conﬁrmed with experiments simulating two VCs per physical channel. 5.5 Evaluation Summary Across a wide range of synthetic and trace-based workloads, the RCA variants match or outperform current Local routers. RCA performs particularly well when the trafﬁc pattern is highly asymmetric as in the water-spatial SPLASH-2 benchmark. RCA also performs well on workloads where greedy, local decisions can hurt global load balance, such as bit-complement trafﬁc. RCA’s impact is reduced when the network diameter is large, or when congestion is highly transient. A large network diameter reduces the effectiveness of RCA designs because, with a 50-50 weighting of local and propagated contention metrics, small ﬂuctuations in local metrics can outweigh strong distant trends. To improve performance of RCA in large meshes, one might consider tuning local versus non-local weights, increasing RCA bit-width for greater visibility, or using concentration to reduce network diameter [3]. Highly transient trafﬁc patterns also complicate adaptive routing’s ability to get an accurate picture of 212 contention. The area overhead of RCA is minimal and its logic lies off the router’s critical path. Overall, the RCA variants we examine reduce latency and improve throughput substantially over traditional local congestion metrics. Such improvements to adaptive routing can greatly enhance application performance or reduce cost at a given performance level. Furthermore, our method of aggregating and transmitting non-local congestion measurements can be applied to other minimally-adaptive routing algorithms. While we have focused on meshes, our approach is applicable to other network topologies. For example, tori are interesting topologies as they are amenable to simple non-minimal dimension-order adaptive routing algorithms. Such routing algorithms often include a phase in which packets are routed minimally within a given quadrant of the network, a phase to which RCA can be adapted directly. We also expect that RCA can be extended to non-minimal adaptive routing by simultaneously considering non-local contention and hop-count toward the destination in each dimension. We will examine the details of integration of RCA with non-minimal adaptive routing in our future work. Acknowledgments We thank Dr. Li-Shiuan Peh and Amit Kumar for their contribution of the SPLASH-2 benchmark traces. This research is supported by the Defense Advanced Research Projects Agency under contract F33615-01-C-4106 and by NSF CISE Research Infrastructure grant EIA-0303609. "
2008,Performance and power optimization through data compression in Network-on-Chip architectures.,"The trend towards integrating multiple cores on the same die has accentuated the need for larger on-chip caches. Such large caches are constructed as a multitude of smaller cache banks interconnected through a packet-based network-on-chip (NoC) communication fabric. Thus, the NoC plays a critical role in optimizing the performance and power consumption of such non-uniform cache-based multicore architectures. While almost all prior NoC studies have focused on the design of router microarchitectures for achieving this goal, in this paper, we explore the role of data compression on NoC performance and energy behavior. In this context, we examine two different configurations that explore combinations of storage and communication compression: (1) Cache compression (CC) and (2) Compression in the NIC (NC). We also address techniques to hide the decompression latency by overlapping with NoC communication latency. Our simulation results with a diverse set of scientific and commercial benchmark traces reveal that CC can provide up to 33% reduction in network latency and up to 23% power savings. Even in the case of NC - where the data is compressed only when passing through the NoC fabric of the NUCA architecture and stored uncompressed - performance and power savings of up to 32% and 21%, respectively, can be obtained. These performance benefits in the interconnect translate up to 17% reduction in CPI. These benefits are orthogonal to any router architecture and make a strong case for utilizing compression for optimizing the performance and power envelope of NoC architectures. In addition, the study demonstrates the criticality of designing faster routers in shaping the performance behavior.","Performance and Power Optimization through Data Compression in Network-on-Chip Architectures ∗∗ Reetuparna Das† Asit K. Mishra† Chrysostomos Nicopoulos† Dongkook Park† Vijaykrishnan Narayanan† Ravishankar Iyer‡ Mazin S. Yousif‡ Chita R. Das† †Dept. of CSE, The Pennsylvania State University ‡Corporate Technology Group, Intel Corporation University Park, PA 16801 Hillsboro, OR 97124 {rdas,amishra,nicopoul,dpark,vijay,das}@cse.psu.edu {ravishankar.iyer,mazin.s.yousif}@intel.com Abstract The trend towards integrating multiple cores on the same die has accentuated the need for larger on-chip caches. Such large caches are constructed as a multitude of smaller cache banks interconnected through a packet-based Network-onChip (NoC) communication fabric. Thus, the NoC plays a critical role in optimizing the performance and power consumption of such non-uniform cache-based multicore architectures. While almost all prior NoC studies have focused on the design of router microarchitectures for achieving this goal, in this paper, we explore the role of data compression on NoC performance and energy behavior. In this context, we examine two different conﬁgurations that explore combinations of storage and communication compression: (1) Cache Compression (CC) and (2) Compression in the NIC (NC). We also address techniques to hide the decompression latency by overlapping with NoC communication latency. Our simulation results with a diverse set of scientiﬁc and commercial benchmark traces reveal that CC can provide up to 33% reduction in network latency and up to 23% power savings. Even in the case of NC – where the data is compressed only when passing through the NoC fabric of the NUCA architecture and stored uncompressed – performance and power savings of up to 32% and 21%, respectively, can be obtained. These performance beneﬁts in the interconnect translate up to 17% reduction in CPI. These beneﬁts are orthogonal to any router architecture and make a strong case for utilizing compression for optimizing the performance and power envelope of NoC architectures. In addition, the study demonstrates the criticality of designing faster routers in shaping the performance behavior. 1. Introduction The last few years have witnessed a major upheaval in microprocessor architectures. The relentless increases in clock frequencies that shaped the scene in the late 1990s and early 2000s have almost come to a standstill. Architects hit what has come to be known as the power wall; higher frequencies ∗ This research is supported in part by NSF grant 0702617, and grants from Intel Corporation. lead to higher power consumption, which, in turn, spawned cooling and reliability issues. Furthermore, the exploitation of Instruction-Level Parallelism (ILP) through deep pipelining and complicated out-of-order superscalar cores seems to have reached a plateau: the inevitable and anti-climactic law of diminishing returns has set in. Architects have to look elsewhere to jump-start the quest for higher performance and frugal power envelopes. The solution to thinning ILP gains has come in the form of the multi-core/CMP revolution. The enabler for this promising design paradigm is the steadfast shrinkage of technology feature sizes into the nanometer regime. By utilizing a number of simpler, narrower cores on a single die, architects can now provide Thread-Level Parallelism (TLP) at much lower frequencies. The presence of several processing units on the same die necessitates oversized L2 and, where applicable, L3 caches to accommodate the needs of all cores. Burgeoning cache sizes are easily facilitated by the aforementioned explosion in on-chip transistor counts. However, the implementation of such large cache memories could be impeded by excessive interconnect delays. While smaller technology nodes are associated with shorter gate delays, the former are also responsible for increasing global interconnect delays [1]. Therefore, the traditional assumption that each level in the memory hierarchy has a single, uniform access time is no longer valid. Cache access times are transformed into variable latencies based on the distance traversed along the chip, and has spurred the Non-Uniform Cache Architecture (NUCA) [8] concept. A large, monolithic L2 cache is divided into multiple independent banks, which are interconnected through an on-chip interconnection network. What can clearly be surmised from the NUCA mechanism is the enormous strain placed on the interconnection backbone. The network fabric undertakes the fundamental task of providing seamless communication between the processing cores and the scattered cache banks. Although its intricacies are often ignored or underestimated, the on-chip interconnect in large NUCA-based CMPs is of utmost importance. Most prior work in CMP NUCA designs uses a packet-based Network-on-Chip (NoC) [11] architecture. Widely viewed as the de facto solution for integrating the many-core architectures, NoCs are be978-1-4244-2070-4/08/$25.00 ©2008 IEEE 215 coming increasingly popular because of their well-controlled and highly predictable electrical properties and their scalability. However, research in NoC architectures clearly points to some alarming trends; the chip area and power budgets in distributed, communication-centric systems are progressively being dominated by the interconnection network [35]. The ramiﬁcations of this NoC dominant design space have led to the development of sophisticated router architectures with performance enhancements [19, 26], area-constrained methodologies [21], power-efﬁcient and thermal-aware designs [31], and fault-tolerant mechanisms [29]. Cache Compression Scheme (CC) NIC Compression Scheme (NC) CPU Node CPU L1 Comp/ Decomp Compressed Data NIC NUCA L2 Network Compressed Data & Compressed Traffic Cache Bank Node L2 NIC CPU Node CPU L1 Uncompressed Data NUCA L2 Network Uncompressed Data but Compressed Traffic Cache Bank Node L2 Uncompressed Data Comp/ Decomp in NIC Comp/ Decomp in NIC R Compressed Data R R Compressed Traffic R Figure 1. Cache Compression (CC) and NIC Compression (NC) schematic While all prior studies have exploited the router microarchitecture design for optimizing the NoC performance and power envelope, the essence of this paper revolves around the idea of employing data compression [5, 6, 33, 17] for additional gain in performance and power. The effect of compression on the interconnection network of NUCA-based CMPs is currently not clear. Toward that extent, this work analyzes the impact of compression on the underlying NoC. The ramiﬁcations on network performance, router complexity, buffer resources, and power consumption are investigated in-depth by conducting a series of experiments involving the major architectural components of a typical NoC router. Intuitively, data compression would reduce network load, and, in effect, lower average network latency and power consumption, leading to application level performance improvement. The motivation of this paper is to quantify this gain for critical design decisions. In this context, we explore two data compression techniques for NUCA-based CMP systems, as illustrated abstractly in Figure 1. The two techniques explore combinations of storage and communication compression. The ﬁrst scheme, known as Cache Compression (henceforth abbreviated to CC), compresses data after the L1 cache, prior to sending it to a shared L2 cache bank (i.e. compression beneﬁts both storage and network communication). Compressing data in the cache is a technique aimed at improving memory and, ultimately, system performance by squeezing more information into a ﬁxed-size cache structure. The second scheme, called Compression in the NIC (abbreviated to NC), employs compression at the Network Interface Controller (NIC), prior into injection in the network. NC implies the use of uncompressed storage data in the cache banks, with only the network communication trafﬁc being compressed. A simple compressor/decompressor module can be integrated in the NIC to facilitate this technique. The NC scheme has the important advantage that it does not require any modiﬁcation in the L2 cache banks. This attribute affords NC plug-and-play capability, which is crucial in modern Systemon-Chip (SoC) implementations that comprise several Intellectual Property (IP) blocks from various vendors. In many cases, these IP blocks may not be modiﬁed (known as hard IP cores); CC would, therefore, not be applicable in such environments. Moreover, the inclusion of NC in this study aims to characterize and quantify the effect of compression purely in the NoC of a NUCA CMP. On the downside, NC requires a compressor/decompressor unit in the NIC of every node in the network, as can be seen in Figure 1. This is in contrast to the CC scheme, which requires such units only in the CPU nodes. It will be shown that the additional area and power overhead is minimal, since the compressor/decompressor unit is very lightweight. However, performance is also affected because the NC mechanism involves compression and decompression operations for both read and write requests, while CC involves only one of the two operations for a given memory request (decompression for reads, compression for writes). To mitigate the performance overhead of decompression, we overlap the decompression phase with network traversal. This mechanism hides some of the decompression latency overhead by commencing progressive decompression as soon as the header ﬂit of a packet reaches its destination NIC. To facilitate a complete system evaluation, including processor, memory and NoC systems, we have developed an integrated NoC/cache simulator for CMP architectures. We have used a diverse set of benchmarks, ranging from scientiﬁc applications to commercial workloads, for in-depth performance analysis on 8- and 16-core systems with varying number of L2 banks, connected through a 2-D mesh. Overall, the major contributions of this paper are the following: • First, to the best of our knowledge, this is the ﬁrst work to comprehensively characterize and quantify in detail the effect of data compression on the on-chip network of a NUCA-based CMP. The effects of both CC and NC schemes on the network behavior are analyzed in terms of average network latency, power consumption, and buffer utilization. We also extend the analysis to estimate the beneﬁts at the application level. • Overall, the CC scheme provides on an average 21% reduction in network latency with maximum savings of 33%. Similarly, power consumption is reduced by an average of 7% (23% maximum). At the application level, we observe, on an average 7% reduction in CPI compared to the generic case. • The NC scheme on an average provides network latency reduction of 20%, (32% maximum), while power consumption reduces on an average by 10% (21% maximum). The novelty of this scheme is that it does not need any 216 Zero    16Bit    8BitRepeated 4Bit    16BitPadded Uncompressed 8Bit    16BitHalfSE 100% 80% 60% 40% 20% 0% Figure 2. Data Pattern Breakdown modiﬁcations to the cache architecture and can be adopted in generic SoC/multi-core platforms. • Finally, a direct corollary of this analysis is the very interesting conclusion that existing NoC architectures are so elaborate and over-designed that they do not adequately match the speed of the processing cores. This mismatch in speed is detrimental to the overall performance. Instead, lean and lightweight on-chip routers are preferred in terms of operating frequency and they can over-compensate for any lack of sophistication. The rest of the paper is organized as follows: Section 2 presents the two compression schemes to be investigated in this paper; the experimental platform and workload used in this study are presented in Section 3; Section 4 is devoted to performance evaluation and, summary of related work is presented in Section 5. Finally, the concluding remarks are drawn in Section 6. 2. Compression Techniques 2.1. Cache Compression Cache compression is a well-studied technique and researchers have used various compression techniques over time [37, 5, 23]. The current state-of-the-art compression design uses frequent-pattern compression [4], which is a signiﬁcancebased compression scheme. It compresses some frequent patterns such as consecutive 0s and 1s in data streams. In our investigation of the workloads, we observed a few patterns occurring with very high frequencies, as illustrated in Figure 2, corroborating the effectiveness of the aforementioned compression technique. For example, as shown in Figure 2, applications like apache, ocean and zeus have more than 60% of consecutive 0s, while on the other hand, applications such as mgrid and swim have very little frequent patterns, rendering them mostly uncompressible. The advantage of this frequentpattern compression method is that high compression ratios can be achieved. The disadvantage, however, is that the scheme is a variable-length compression method. This implies that decompression cannot be done in parallel, and the cache set structure is complicated due to the variable length cache lines. Cache compression allows to store more data in the same cache size. This can be achieved by using higher associativity for the same physical set size and by additional hardware to handle variable cache line sizes in every set. A cache architecture to handle variable length cache lines is depicted in Figure 3(a). In this design, the traditional hardware granularity of one cache line is now reduced to a segment within a set. As in [5], one segment consists of 4 compressed words with one set containing 32 segments. Each cache line may consist of one to eight segments, depending on its compressibility. The tag is thus modiﬁed to accommodate a 3-bit length value (i.e. up to 8 cache lines). We use a 1-bit compression status ﬂag indicating whether the line is compressed or not. Although in this work we do not use the adaptive scheme of [5], we still use this bit to optimize the on-chip interconnect throughput by sending the uncompressed packet over the interconnect without the tag, if none of the words were compressible in a particular cache line. The additional hardware modules required to enable the use of the cache compression scheme are shaded in Figure 3(a). The critical path for a cache lookup is also shown in detail in the adjacent ﬂow graph (the additional modules in Figure 3(b) are shaded in grey). Every cache line starts at a variable segment offset from the start of the set, depending on the cumulative length of all previous valid cache lines. Therefore, the start address of each cache line needs to be computed. A parallel preﬁx adder is employed for this purpose. The adder can operate in parallel with the tag comparators. Since the granularity of a cache line has been reduced to a variable number of segments, a cache line can now potentially start from any segment offset in the set (each set has 32 segments) and, similarly, end at any segment. Hence, the straightforward output from the tag comparators can no longer be used to drive the output driver multiplexer circuits [32]. Instead, the monolithic output driver circuits must be broken into many smaller drivers to account for the increased segment granularity in each cache line. These drivers are controlled by a 32-bit controlling mask, with each bit representing one segment in the entire set. A bit-mask generator is, thus, added to the circuitry, which takes the cumulative length and current length as inputs to generate a bit-mask that will activate only those segments belonging to the valid matched cache line. The bit-mask is subsequently fed to the data output driver circuits. In order to support variable-line size, the tag array access and data array access components need to be decoupled, as in [15, 34]. Further, contiguous storage invariant needs to be maintained at all times. This is required to avoid any internal fragmentation within the set, and, more importantly, to simplify the data-array lookup. If holes of invalid segments are allowed to permeate the valid cache lines, then the system would need to keep track of the starting addresses of each valid cache line in addition to the line length. To maintain a contiguous storage space, segments within the cache set must be moved when certain events occur. We call the operation of moving 217  BIT LINES ADDRESS INPUT BIT LINES  WORD LINES TAG ARRAY ADDRESS DECODER DATA ARRAY  WORD LINES COLUMN MUXES PARALLEL PREFIX ADDER START LEN. SEL. MUX 8 x 5 START SEG. 5 BITS OUTPUT DRIVER SENSE AMPS SENSE AMPS COMPARATORS 8 x 3 CSIZE CSIZE SELECT MUX 3 BITS BIT MASK GEN. (32 BITS) 32 BITS VALID OUTPUT DATA OUTPUT (a) Cache Model SEGMENT COLUMN MUXES OUTPUT DRIVERS START ADDRESS DECODE, WORD LINE SELECT, BIT LINE SELECT AND SENSE AMPLIFIER COMPARE TAGS PARALLEL PREFIX ADDER MUX DRIVER SEGMENT BIT MASK GENERATOR DATA OUTPUT DRIVER COMPACTION DETECTION NO YES LOCK SET COMPACT UNLOCK SET EXIT (b) Cache Lookup Algorithm Figure 3. Cache Conﬁguration To Support Variable Line Size segments within the set compaction. Compaction is needed under two circumstances: (1) On a write hit, and when the data to be written back is larger than the original compressed line (fat write) and (2) On a replacement, and when the line chosen to be replaced is not the exact same size as that of the fresh line being brought into the set. Note that more than one line may need to be replaced to accommodate a new line. This process may require more than one compaction operation. Before the compaction operation commences, the set needs to be locked and cannot service further requests. Compaction is not on the critical delay path, as the processor is not stalled for a write, and, in the case of a miss, the fetched data is sent back immediately while the set is simultaneously locked to prevent further access. However, the locking of the set may cause performance degradation since the set is no longer available until compaction is complete. Fortunately, our experiments indicate that the number of compactions required is relatively low, and the time period between consecutive compactions is large enough to accommodate the compaction time. Our experiments show that the average number of cycles between two consecutive compactions to a bank is around six thousand cycles with a minimum of around two hundred cycles. Compaction needs two additional hardware modules(not shown in Figure 3(a)): a Compaction Detection Module, and a Compaction Buffer of the size of one set. A single compaction operation will ﬁrst require a read from the set, followed by two writes to the set. To limit the area overhead imposed by the compaction scheme, only one set buffer per cache bank is used; thus, the buffer needs to be shared by all sets in the bank. In order to read/write data from the set during a compaction operation, the column decoder logic needs to be coordinated by the Compaction Detection Module to enable the right segments for read and write-back to the set. The area overhead for compression in a cache bank has been calculated in [5] to be approximately 7%, but without taking into account the compressor/decompressor circuit. In our analysis, we have accounted for cache compaction and additions to the basic cache structure in order to investigate the impact on access time. The additional compaction buffer per bank CYCLES 0 1 2 3 4 5 6 7 8 9 TO L1/ PROCESSOR DECOMPRESSION CYCLES FLITS CYCLES 0 1 2 3 4 5 6 TO L1/ PROCESSOR FLITS PRECOMPUTATION (STAGES 1,2 & 3) STAGES 4 AND 5 SAVINGS PER MSG. Figure 4. An Example Time Line for a Five Flit Message (size of one set), adds a minimal 0.1% overhead. To calculate the timing overhead, the baseline cache timing parameters were taken from CACTI 4.2 [32] and the additional modules including compressor/decompressor circuit were implemented in Verilog, synthesized in 90 nm and scaled to 45 nm [9]. The additional modules add 0.41 ns to the critical path. This requires two additional clock cycles for each cache access to support a variable line, segmented cache for a 4-GHz processor. This result is signiﬁcant, because it shows that cache compression does, indeed, affect the cache access time. The area overhead and dynamic power consumption of the compressor/decompressor circuit from our implementation were found to be 0.183 mm 2 and 0.273 W, respectively. 2.2. Compression in NIC For architectures that do not employ cache compression, we can use compression in the NIC prior to injecting trafﬁc into the network. This scheme needs a compressor and decompressor in each NIC, and thus, adds to the area and timing overheads. In this paper, we show that some of the compression and decompression latency can be hidden by utilizing a pipelined router design. Frequent-pattern compression can be realized in ﬁve cycles [4]. In the ﬁrst cycle, the length of each word is decoded using the preﬁx tags. In the next two cycles, the starting bit address of each word is computed by cumulatively adding the length bits of each word. In the fourth and the ﬁfth cycles, a parallel pattern decoder decodes the contents of the compressed 218 Baseline Simulation Conﬁguration 3.1. Baseline CMP Architecture Processor Cores Private L1 Cache Eight Processors at 4GHz clock Split I and D cache, each 32KB,4-way set associative, 64B line,3-cycle access time Shared L2 Cache Uniﬁed 4MB with 16 256KB bank, each bank 4 cycle access time uncompressed 4+2(access time overhead)+5(decompressor overhead)cycles for compressed 4GB DRAM,400 cycle access,each processor up to 16 outstanding memory requests 2-stage,wormhole routing, 2 VCs per PC,80-bit ﬂits, 4 ﬂits buffer depth Router Pipeline Memory Table 1. Simulation Model 3 , ' 3 3 3 , ' , ' , ' , ' , ' , ' , ' 3 3 3 3 Figure 5. Baseline CMP Layout data bits into uncompressed words using their respective preﬁx bits. This overhead is on the critical path of the L2 hit latency. Prior work has tried to reduce this latency by an adaptive scheme based on the LRU stack depth in the cache [5]. Here, we present an optimization based on the observation that in a packet-based network, it is possible to overlap the decompression with the network latency. The decompression pipeline can be designed such that we need not wait for the entire cache line to arrive before we can start the decompression phase. Since at the ejection port we can get at most one ﬂit per cycle in a wormhole switched network, decompression may commence as soon as the header ﬂit of the compressed cache line arrives. This ﬂit contains the preﬁx tags and all the necessary information for the ﬁrst three stages of the decompression pipeline. After the tail ﬂit arrives at a NIC, we can complete the ﬁnal two steps of the decompression stage and commit the data to the L1 cache and/or processor. This technique will save three clock cycles per decompression operation, effectively reducing the overhead from 5 cycles to 2 cycles. Figure 4 demonstrates the three cycle saving that can be accrued from the proposed scheme. 3. Experimental Platform In this section, we describe our complete CMP platform including the processor core and the underlying NoC architecture, the router model and the workloads used for the performance analysis. 219 Without loss of generality we assume an 8-core CMP at 45 nm technology size as our baseline architecture. Each core has split private L1 cache (instruction and data cache each 32 KB in size). All the cores share the 4 MB last level cache (L2). The L2 cache is split into 16 banks, each of size 256 KB. A 6x4 mesh network( as shown in Figure 5) connects these banks. Each L2 cache bank has a router associated with it. Each router connects to four adjacent routers in the four cardinal directions. The detailed conﬁguration is given in Table 1. We implemented a detailed trace-driven cycle-accurate hybrid NoC/ cache simulator for CMP architectures. The memory hierarchy implemented is governed by a two-level directory cache coherence protocol. Each core has a private write-back L1 cache. The L2 cache is shared among all cores and split into banks. Our coherence model includes a MESI-based protocol with distributed directories, with each L2 bank maintaining its own local directory. The simulated memory hierarchy mimics SNUCA [8]. The sets are statically placed in the banks depending on the low order bits of the address tags. The network timing model simulates all kinds of messages: invalidates, requests, replies, write-backs, and acknowledgments. The interconnect model implements a state of art low-latency packet-based NoC router architecture. The NoC router adopts the deterministic X-Y routing algorithm, ﬁnite input buffering, and wormhole ﬂow control. The power estimates extracted from the synthesized implementations are also fed to the simulator to compute overall power consumption.This platform is used in conjunction with the Simics full system simulator [25], which is used as memory reference generator. All the additional hardware logic mentioned in this paper was implemented in structural RegisterTransfer Level (RTL) Verilog and then synthesized in Synopsys Design Compiler using a TSMC 90 nm standard cell library. The library utilized appropriate wire-load approximation models to reasonably capture wire loading effects. The area and power numbers then obtained were scaled down to 45 nm technology [9]. 3.2. Baseline Router Architecture A generic NoC router architecture is illustrated in Figure 6. The router has P input and P output channels/ports. In most implementations, P=5; four inputs from the north, east, south and west directions, and one from the local Processing Element (PE). The Routing Computation unit, RC, is responsible for directing the header ﬂit of an incoming packet to the appropriate output Physical Channel/port (PC) and dictating valid Virtual Channels (VC) within the selected PC. The routing is done based on destination information present in each header ﬂit, and can be deterministic or adaptive. The Virtual channel Allocation unit (VA) arbitrates amongst all packets requesting access to the same VCs and decides on winners. The Switch Allocation unit (SA) arbitrates amongst all VCs requesting access to the crossbar and grants permission to the winning ﬂits. The winners are then able to traverse the crossbar and are placed on the output links. Simple router implementations require TPC-W: We use an implementation of the TPC-W benchmark [2] from New York University consisting of two tiers - a JBoss tier that implements the application logic and interacts with the clients and a Mysql-based database tier that stores information about items for sale and client information - under conditions of high consolidation. It models a online book store with 129600 transactions and 14400 customers. Java ServerWorkload: SPECjbb. SPECjbb2000 is a Java based benchmark that models a 3-tier system. We use eight warehouses for eight processors, we start measurements 30 seconds after rampup time. Static Web Serving: Apache. We use Apache 2.0.43 for SPARC/Solaris 10 with default conﬁguration. We use SURGE [7] to generate web requests. We use a repository of 20,000 ﬁles (totalling 500 MB). We simulate 400 clients, each with 25 ms think time between requests. Static Web Serving: Zeus. Zeus is the second web server workload we used. It has an event-driven server model. We use a repository of 20,000 ﬁles of 500 MB total size, 400 clients, 25 ms think time. SPEComp: We used SPEomp2001 as another representative workload. We present the results from apsi, art, mgrid and swim due to space constraints. SPLASH 2: SPLASH is a suite of parallel scientiﬁc workloads. Each benchmark executed sixteen parallel threads running on a eight way CMP. Mediabench (mm). We use eight benchmarks (cjpeg, djpeg, jpeg2000enc, jpeg2000dec, h263enc, h263dec, mpeg2enc and mpeg2dec) from the Mediabench II [3] suite to cover a wide range of multimedia workloads. We used the sample image and video ﬁles that came with the benchmark. For throughput computing we simultaneously executed all benchmarks on each individual core for 30 back-to-back runs. Table 2. Workloads a clock cycle for each component within the router. Lowerlatency router architectures parallelize the VA and SA using speculative allocation [30], which predicts the winner of the VA stage and performs SA based on that. Further, look-ahead routing [14] can also be employed to perform routing of node i+1 at node i. These two modiﬁcations have led to two-stage, and even single-stage [26], routers, which parallelize the various stages of operation. In this work, we assume the use of a two-stage router. Furthermore, since compression would result in variable size packets, the router architecture is modiﬁed accordingly. In our experiments, the ﬂit size was chosen to be 80 bits (64bit data payload and 16-bit network overhead). At any given time, both data and control packets ﬂow in the network. Data packets are typically much larger than control packets. Our exploration indicated that control packets range in size from 39 to 64 bits. Thus to avoid internal fragmentation in control packets, we choose a ﬂit size that could accommodate at least a 64-bit payload. The contemporary state-of-art routers, with wormhole routing and several VCs per physical channel, have achieved maximum operating frequencies well below 1 GHz [27, 12, 19] at 220 VC 1 VC 2 VC v VC Identifier Input Port 1 Input Port P Input Channel 1 Credit out Input Channel P Credit out Routing Unit (RT) VC Allocator (VA) Switch Allocator (SA) Crossbar (P x P) Credits in Output Channel 1 Output Channel P Figure 6. A Generic Router Architecture larger technology nodes. Even after scaling to 45 nm technology [9], the single-stage router of [27] can reach only about 1 GHz. The router in [12] can reach 2 GHz, but it is deeply pipelined (5-stage router). However, various CMP designs [8, 6] demonstrate a processor clock speed within 5 to 10 GHz. We analyzed the router pipeline to discover that the arbitration stage is the bottleneck of the pipeline, and this stage determines the clock frequency. We also observed from our circuit-level implementation and timing analysis that reducing the number of VCs increases router frequency considerably. Thus, routers can achieve almost 4 GHz at 45nm technology by cutting down the VCs to two per physical channel. Another important conclusion from our experiments is that cutting down the number of VCs by half marginally affected network latency in all workloads. This can be attributed to low injection rates seen in the interconnect. Thus, we gain in router clock speed with a low penalty in performance. 3.3. Workloads For the performance analysis, we use a diverse set of workloads comprising of scientiﬁc and commercial applications. The workload details are summarized in Table 2. 4. Performance Evaluation It is intuitively clear that compression can give us diverse beneﬁts, depending on the on-chip/off-chip bandwidth characteristics and the compressibility of the workloads. In this section, we quantify these beneﬁts in terms of network centric parameters such as network latency, memory latency, power consumption, and buffer utilization, and the application centric CPI parameter. The results are discussed under four subsections. First, we conduct a co-evaluation of CMP cores and NoC router architectures to understand the impact of router clock cycles on overall system performance. This study indicates that a router-to processor clock ratio of 1:1 is essential for performance optimization, and is used in the rest of the study. Second, we report the beneﬁts of compression on network performance, followed by a scalability analysis of the two cases(CC and NC). Finally, we conduct a application level CPI analysis. We, thus, compare three cases; a generic NoC without any compression (gen), compression in the cache (CC), and compression in the NIC (NC). For clarity of discussion, most of the queuelat blocklat netlat clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 art tpcw wns zeus (a)Network Latency(Cycles) in-net off-chip others s e c y l C 200 160 120 80 40 0 s e c y l C 25 20 15 10 5 0 clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 clk  1:1 clk  1:2 clk  1:4 art tpcw wns zeus (b)Memory Response Time Breakdown Figure 7. Performance Sensitivity as a Function of Router-to-Processor Clock Ratio latency results are normalized with respect to the generic case. 4.1. Network and Processor Co-evaluation Except for a few studies [12, 24], almost all prior work in NoCs has focused only on the network design aspects without considering a holistic approach involving the processor and memory systems. Similarly, the processor/memory designs for CMPs [6] have neglected the details of the NoC architecture, and have usually assumed a ﬁxed network latency. A consequence of this disjoint approach is that while some of the recent papers have assumed high processor clock rates (4-10 GHz), contemporary NoCs are yet to approach the processor clock rate [27, 12, 19]. This is primarily because of the sophisticated router architectures. This mismatch between processor and NoC design could have signiﬁcant performance implications, since one network clock cycle could be equivalent to multiple processor cycles. The ramiﬁcation of this mismatch needs quantiﬁcation for providing useful design guidelines. In [28], the authors have analyzed the impact of using NoC in a NUCA-based CMP. In contrast, our work focuses on compression and studies its effect on the interconnect. In this section, we analyze the impact of the NoC design on overall system performance by varying the ratio of processor and router operating frequencies. Figure 7 demonstrates how interconnect scales with the ratio of router clock to processor clock for four representative workloads in an 8-core system with 16 cache banks, connected as a 2-D mesh. Figure 7(a) illustrates how the network latency grows fast as the router to processor clock ratio changes from (1:1) to (1:4). Since the L1 to L2 cache communication follows a request-response communication model, the memory response time includes the round trip time in the interconnect. Thus, we see that at a clock ratio mismatch of 1:4, the L2 access time can increase as much as by 170 cycles, making on-chip latency comparable to off-chip latency. in-net off-chip other zeus wns tpcw swim sjbb ray ocean mm mgrid barnes art apsi apache 0% 20% 40% 60% 80% 100% Figure 8. Average Memory Response Time Breakdown Figure 7(b) breaks down memory latency into three categories. First, is the component of average memory response time spent over the on-chip interconnect (in-net). The transactions which use the interconnect consist of L1 cache ﬁlls, L1 write-backs, and the coherence messages. Second, is the component of average memory response time spent off-chip (offchip). This is the time required to access the off-chip DRAM. The component of average memory response time spent in L1 accesses, L2 accesses, and the queuing time of the cache controllers are all clubbed together in the third category (other). Figure 7(b) shows that as the clock mismatch grows, the time spent in the interconnect becomes disproportionately larger than time spent off-chip or time spent within the cache controllers. These results indicate that it is essential to designing faster routers to cope with the processor speed. Otherwise, the NoC will be a major performance bottleneck. Thus, in the rest of the study, we use a router-to-processor clock ratio of (1:1) to analyze the impact of compression without any bias in the relative speed. From an interconnect perspective, it is useful to segregate applications with respect to their on-chip (interconnect bound) and off-chip (memory bound) bandwidth demands. Applications which have high L1 miss rates will inevitably consume signiﬁcant L2 on-chip bandwidth, and interconnect latency will dominate the overall memory latency, thus they will have high in-net component. For applications with high L2 miss rates the off-chip component will dominate the average memory latency. For application which are not memory intensive and get satisﬁed with L1 itself (i.e. low L1 miss rates) the other component will dominate the average memory latency. Figure 8 shows the percentage of time an application spends 221 in-network, off-chip and others. Here we see that applications art, ray, tpcw have high in-net component, thus they are likely to beneﬁt more from NC than CC. 4.2. Network-Centric Evaluation gen cc nc 1.2 1 0.8 0.6 0.4 0.2 0 (a) Normalized Interconnect Latency per Message gen cc nc 1.2 1 0.8 0.6 0.4 0.2 0 10.16  3.21  6.75  7.18  21.59  9.36  10.99  10.67  15.83  38.92 11.14  3.96  7.29 (b) Normalized Memory Response Time Figure 9. Normalized Performance Figure 9(a) depicts the impact of compression on network latency per message and Figure 10 shows the actual latency numbers per ﬂit in clock cycles. The network latency is broken down into queuing latency, blocking latency and actual transfer time (trans latency). We ﬁnd, on an average, 21% improvement going up to a maximum of 33% with the CC scheme, while the NC scheme provides equally competitive 20% and 32% average and maximum latency reduction, respectively. The queuing latency and blocking latency per ﬂit do consistently decrease across all benchmarks (except for swim and mgrid since these applications have little room for compression as depicted in Figure 2), as we consume lower network bandwidth due to compression. The actual network transfer time (translat) does not decrease considerably because the ﬂits still need to traverse the same number of hops. However, the message latency decreases as number of ﬂits per message (Figure 11) reduces due to compression. Figure 11 indicates that the average ﬂit size for some of the highly compressible applications such as apache and zeus reduces by more than 50% due to compression. It must be noted from Fig. 9(a) that two or three cycles are saved for data packets per message in addition to savings in blocking and queuing latency per ﬂit. However, only around 42% of the total number of packets in our applications are data packets, so we do not see a signiﬁcant difference in per ﬂit latency savings and per message latency savings. We also see that network bound applications (Section 4.1) art, ray, tpcw perform better for NC than CC scheme. This can be attributed to the fact that CC always has a 2 cycle overhead for ever L2 cache access due to additional complexity in the cache set structure(Section 2.1). Thus, if the application has a small off-chip component gen cc nc e g a s s e m r e p s t i l F 6 5 4 3 2 1 0 Figure 11. Average Flit Size Reduction per Message and high in-net component, the CC scheme does give beneﬁt in terms of additional cache capacity, but incurs the cache access time overhead. Figure 9(b) shows the impact of compression on memory response time. This time is measured in cycles from the issue of a memory request from the core until the response is sent back after going through the memory hierarchy. The actual memory latency (in cycles) for each application with respect to the generic case is given at the bottom of the ﬁgure. We ﬁnd, on an average, 10% and 9% improvements with the CC and NC schemes, respectively, with the exception of mgrid and swim applications. The cache compression actually increases the L2 cache miss rate for mgrid from 65% to 68% due to conﬂicts. NC still shows marginal improvement for mgrid as we do not affect the cache behavior in NC. swim has very low compressibility, thus it does not provide beneﬁts in any scheme. Figure 12(a) shows the improvements in power conservation to be on an average 10% for NC and 7% for CC. One interesting observation is that, although the NC scheme needs a compressor and decompressor in each NIC (thus increasing the area and power overhead), the overall power consumption is still better than the generic router due to reduction in network latency and buffer space. Figure 12(b) also shows average buffer utilization reduction by 47% for both CC and NC schemes respectively. Lower buffer utilization implies that we can reduce the buffer size without affecting performance. This in turn would lead to substantial power saving in the network. 4.3. Scalability Study The previous experiments have been conducted with an 8core CMP with 16 L2 cache banks. In this section, we investigate the scalability of the compression schemes using a 16core system with varying number of cache banks but keeping the last level level cache size constant at 8MB. The results are plotted in Figure 13 (a). We choose SPECjbb as a representative commercial workload for these experiments. We scale the workload accordingly for the 16-core system. The graphs show average network latency reduction (in % with respect to the generic case) for 16, 32, 64 node cache banks, conﬁgured as 2-D mesh networks. Results for the 8-core CMP are also plotted for comparison. The results indicate that as the network size increases from 8 to 16 cores, compression results in equivalent latency reduction for the same processor to cache 222     s e l c y C 40 30 20 10 0 1.2 1 0.8 0.6 0.4 0.2 0 translat   blocklat queuelat c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g c c c n n e g apache apsi art barnes mgrid mm ocean ray sjbb swim tpcw wns zeus Figure 10. Interconnect Latency Breakdown per Flit gen cc nc gen cc nc 1.2 1 0.8 0.6 0.4 0.2 0 (a) Normalized Network Power Reduction (b) Normalized Router Buffer Utilization Figure 12. Interconnect Savings bank size ratio. Hence, data compression in larger multi-core systems is expected to provide similar performance and power optimization. It can also been be seen that improvements with both the CC and NC schemes scale down with network size. This is intuitive since by increasing the network size for a constant L2 cache size, the probability of blocking and queuing in the network will be lower, thus reducing beneﬁts. However, the slope is not steep, thus improvements scale down gracefully. Also, if the technology allows for a larger cache in a multi-core chip, data compression would be more attractive. Another interesting scalability result would be to see the effect of wider channels/links on achieved performance improvements. We plot these results in Figure 13 (b). We vary the ﬂit/link size from 4 bytes to 32 bytes. As can be seen, the improvements scale down for both NC and CC. The reason for this degradation is two fold. On one hand, the injection ratio into the network reduces, causing reduction in blocking and queuing in the network. This makes compression less effective. On the other hand, since number of ﬂits per message reduces with wider ﬂits, the overlapping of decompression latency with communication latency is not always possible. However, as the number of cores and banks increase with technology scaling, the available bandwidth per channel will also scale down, which in turn will limit the width of links and hence the ﬂit size. 4.4. Application-Centric Evaluation In this section, we analyze how the savings in network latency due to compression translates to the application level. We use the Simics[25] full system simulator instantiated with the g-cache model to measure the CPI for different applications. The last level (L2) cache latencies ( Figure 10) are obtained from our NUCA-based cache simulator, as discussed earlier, and we back-annotate these latency values to simulate the 8core CMP system. We use an in-order core model. Figure 14 shows the normalized CPI reduction for each application using CC and NC respectively. We ﬁnd an overall CPI reduction of 5%. It should be noted that, the overhead of cache access latency or the beneﬁt of cache capacity increase was not included, speciﬁcally, to quantify the network effect. 5. Related Work In this section, we provide a brief summary of prior work overlapping the techniques presented in this paper. Compression has been explored not only at various levels of the memory hierarchy [5, 6, 33, 20, 13, 23, 36] to increase effective storage size, but also in macro-networks, off/on-chip buses to improve bandwidth, reduce crosstalk, and improve bus scalability [18, 10]. Recently, researchers [5, 6] have analyzed the impact 223 cc-8p cc-16p nc-8p nc-16p 6. Conclusion 30% 20% 10% n a i G e c n a m r o f r e P 0% Data compression is a well-known technique for enhancing the performance of communication architectures. Compression has also been applied in designing the memory hierarchy to increase the effective storage size in uni-processor and multicore systems. In this paper, we examine how compression can be be utilized in deriving additional performance and power beneﬁts in on-chip interconnects within the context of multicore/CMP systems. In particular, the objective of this study is to quantify these beneﬁts through a holistic evaluation of the CMP architecture. In this context, we explore two data compression techniques, called Cache Compression (CC) and Compression in the NIC (NC), for NoC architectures. We also analyze the design details of the cache architecture and the NIC compression scheme to estimate the area, power, and performance overheads. Our performance results indicate that the CC scheme can reduce the network latency by up to 33% (average 23%), and the power consumption by up to 24% (average 7%). In terms of application-level performance, we observe up to 17% decrease in CPI compared to the generic case. Similarly, the NC scheme can provide competitive latency, power and CPI reductions of up to 32%, 26%, and 13%, respectively. These beneﬁts are orthogonal to, and additional to, the inherent NoC architecture, and are signiﬁcant considering the overall NoC design objectives. Another useful insight of this integrated NoC and memory system evaluation is the criticality of designing faster NoC routers to cope with the processor speed. Otherwise, the NoC will become the major bottleneck in overall system performance. Thus, in contrast to sophisticated but rather slow router designs that employ complex arbitration and ﬂow control mechanisms, the study indicates that NoC routers should rely on simple architectures to increase the router speed. In summary, all these results indicate that compression would beneﬁt the NoC design in terms of lower network latency, lower power consumption and improved application performance. In particular, the CC scheme will have the dual beneﬁt of helping the cache performance and the NoC performance in a CMP. The NC scheme does seem to be as effective as the CC scheme, and thus is a viable option since we can squeeze performance and power beneﬁts without modifying the cache architecture. "
2009,Optimizing communication and capacity in a 3D stacked reconfigurable cache hierarchy.,"Cache hierarchies in future many-core processors are expected to grow in size and contribute a large fraction of overall processor power and performance. In this paper, we postulate a 3D chip design that stacks SRAM and DRAM upon processing cores and employs OS-based page coloring to minimize horizontal communication of cache data. We then propose a heterogeneous reconfigurable cache design that takes advantage of the high density of DRAM and the superior power/delay characteristics of SRAM to efficiently meet the working set demands of each individual core. Finally, we analyze the communication patterns for such a processor and show that a tree topology is an ideal fit that significantly reduces the power and latency requirements of the on-chip network. The above proposals are synergistic: each proposal is made more compelling because of its combination with the other innovations described in this paper. The proposed reconfigurable cache model improves performance by up to 19% along with 48% savings in network power.","Optimizing Communication and Capacity in a 3D Stacked Recon(cid:2)gurable Cache Hierarchy (cid:3) Niti Madany , Li Zhaoz , Naveen Muralimanohary , Aniruddha Udipiy , Rajeev Balasubramoniany , Ravishankar Iyerz , Srihari Makineniz , Donald Newellz y School of Computing, University of Utah z System Technology Lab, Intel Corporation Abstract Cache hierarchies in future many-core processors are expected to grow in size and contribute a large fraction of overall processor power and performance. In this paper, we postulate a 3D chip design that stacks SRAM and DRAM upon processing cores and employs OS-based page coloring to minimize horizontal communication of cache data. We then propose a heterogeneous recon(cid:2)gurable cache design that takes advantage of the high density of DRAM and the superior power/delay characteristics of SRAM to ef(cid:2)ciently meet the working set demands of each individual core. Finally, we analyze the communication patterns for such a processor and show that a tree topology is an ideal (cid:2)t that signi(cid:2)cantly reduces the power and latency requirements of the on-chip network. The above proposals are synergistic: each proposal is made more compelling because of its combination with the other innovations described in this paper. The proposed recon(cid:2)gurable cache model improves performance by up to 19% along with 48% savings in network power. Keywords: multi-core processors, cache and memory hierarchy, non-uniform cache architecture (NUCA), page coloring, on-chip networks, SRAM/DRAM cache recon(cid:2)guration. 1. Introduction The design of cache hierarchies for multi-core chips has received much attention in recent years (for example, [5, 8, 10, 18, 38, 48]). As process technologies continue to shrink, a single chip will accommodate many mega-bytes of cache storage and numerous processing cores. It is well known that the interconnects that exchange data between caches and cores represent a major bottleneck with regard to both power and performance. Modern Intel chips already accommodate up to 27 MB of cache space [29]; interconnects have been attributed as much as 50% of total chip dynamic power [28]; on-chip networks for large tiled chips have been shown to consume as much as 36% of total chip power [23, 44]; long on-chip wires and router pipeline (cid:3)This work was supported in parts by NSF grants CCF-0430063, CCF0811249, NSF CAREER award CCF-0545959, Intel, and the University of Utah. delays can lead to cache access times of many tens of cycles [12, 36]. Not only will we require intelligent mechanisms to allocate cache space among cores, we will also have to optimize the interconnect that exchanges data between caches and cores. This paper makes an attempt at addressing both of these issues. 3D stacking of dies has been demonstrated as a feasible technology [46] and is already being commercially employed in some embedded domains [35, 39]. In most commercial examples, 3D is employed to stack DRAM memory upon CPU cores [35, 39]. This is especially compelling because future multi-cores will make higher memory bandwidth demands and the inter-die interconnect in a 3D chip can support large data bandwidths. Early projections for Intel’s Polaris 80-core prototype allude to the use of such 3D stacking of DRAM to feed data to the 80 cores [42]. Given the commercial viability of this technology, a few research groups have already begun to explore the architectural rami(cid:2)cations of being able to stack storage upon CPUs [21, 24(cid:150)27]. In this paper, we (cid:2)rst postulate a physical processor design that is consistent with the above trends. We then take advantage of the following three key innovations to architect a cache hierarchy that greatly reduces latency and power: (i) we employ page coloring to localize data and computation, (ii) we propose the use of cache recon(cid:2)guration to accommodate large working sets for some cores, (iii) we identify a network topology that best matches the needs of data traf(cid:2)c and incurs low delay and power overheads. The proposed processor employs three dies stacked upon each other (see Figure 1). The lowest die contains the processing cores (along with the corresponding L1 caches). The second die is composed entirely of SRAM cache banks (forming a large shared L2 cache) and employs an on-chip network so that requests from the CPU can be routed to the correct bank. The third die is composed of DRAM banks that serve to augment the L2 cache space provided by the second SRAM die. It is also possible to stack many more DRAM dies upon these three dies to implement main memory [26], but we regard this as an orthogonal design choice and do not consider it further in this paper. The L2 cache banks are organized as a non-uniform 978-1-4244-2932-5/08/$25.00 ©2008 IEEE 262 cache architecture (NUCA) [22]. The request from the processing core is transmitted to the SRAM die through inter-die vias. From here, the request is propagated to the appropriate bank through the on-chip network and the latency for the access is a function of the proximity of this bank to the processing core. Many recent papers have explored various mechanisms to reduce average access times in a NUCA cache [5, 7, 9, 10, 18, 48]. Most dynamic (DNUCA) mechanisms can cause data to be placed anywhere on the chip, requiring search mechanisms to locate the data. We dis-regard these algorithms because of the complexity/cost of search mechanisms and resort to a static-NUCA (S-NUCA) organization, where a given physical address maps to a unique bank in the cache (the physical address maps to a unique bank and set within that bank; the ways of the set may be distributed over multiple subarrays within that bank). To improve the proximity of storage and computation, we employ page coloring to ensure that data is placed in (cid:147)optimal(cid:148) banks. The idea of employing page coloring to dictate data placement in a NUCA cache was proposed in a recent paper by Cho and Jin [11]. Ideally, a low-overhead run-time mechanism would be required to estimate the usage of a page so that pages can be dynamically migrated to their optimal locations [1]. The design of such mechanisms is a non-trivial problem in itself and beyond the scope of this paper. For this work, we carry out an off-line analysis to identify pages that are private to each core and that are shared by multiple cores. Private pages are placed in the bank directly above the core and shared pages are placed in one of four central banks. With the above page coloring mechanism in place, we expect high data locality and most cores will end up (cid:2)nding their data in the L2 cache bank directly above. In a multi-programmed workload, each core may place different demands on the L2 cache bank directly above. In a multi-threaded workload, the centrally located cache banks will experience higher pressure because they must accommodate shared data in addition to the data that is private to the corresponding cores. These varied demands on each cache bank can perhaps be handled by allowing a core to spill some of its pages into the adjacent banks. However, this not only increases the average latency to access that page, it also places a higher bandwidth demand on the inter-bank network, a trait we are striving to avoid (more on this in the next paragraph). Hence, we instead spill additional pages into the third dimension (cid:150) to the DRAM bank directly above the SRAM cache bank. Note that the DRAM bank and SRAM bank form a single large vertical slice in the same L2 cache level. When the DRAM space is not employed, each SRAM cache bank accommodates 1 MB of cache space. If this space is exceeded, the DRAM bank directly above is activated. Since DRAM density is eight times SRAM density, this allows the cache space to increase to roughly 9 MB. While DRAM has poorer latency and power characteristics than SRAM, its higher density allows a dramatic increase in cache space without the need for many more stacked dies (that in turn can worsen temperature characteristics). Further, we architect the combined SRAM-DRAM cache space in a manner that allows non-uniform latencies and attempts to service more requests from the faster SRAM die. DRAM bank access itself has much lower cost than traditional DRAM main memory access because the DRAM die is partitioned into many small banks and a single small DRAM bank is looked up at a time without traversing long wires on the DRAM die. This results in a heterogeneous recon(cid:2)gurable cache space, an artifact made possible by 3D die stacking. A reasonable alternative would be the implementation of a 3-level cache hierarchy with the top DRAM die serving as an independent L3 cache. A static design choice like that would possibly complicate cache coherence implementations, incur the latency to go through three levels for many accesses, and reduce the effective capacity of the L2 because of the need for L3 tags. We believe that the overall design is made simpler and faster by growing the size of the L2 cache bank on a need basis. Finally, we examine the traf(cid:2)c patterns generated by the cache hierarchy described above. Most requests are serviced by the local SRAM and DRAM cache banks and do not require long traversals on horizontal wires. Requests to shared pages are directed towards the centrally located banks. Requests are rarely sent to non-local non-central banks. Such a traf(cid:2)c pattern is an excellent (cid:2)t for a tree network (illustrated in Figure 1). A tree network employs much fewer routers and links than the grid network typically employed in such settings. Routers and links are cumbersome structures and are known to consume large amounts of power and area [23, 44] (cid:150) hence, a reduction in routers and links has many favorable implications. Tree networks perform very poorly with random traf(cid:2)c patterns, but the use of intelligent page coloring ensures that the traf(cid:2)c pattern is not random and best (cid:2)ts the network topology. A tree network will likely work very poorly for previously proposed D-NUCA mechanisms that can place data in one of many possible banks and that require search mechanisms. A tree network will also work poorly if highly pressured cache banks spill data into neighboring banks, making such a topology especially apt for the proposed design that spills data into the third dimension. The contributions of the paper are: (cid:149) A synergistic combination of page coloring, cache recon(cid:2)guration, and on-chip network design that improves performance by up to 62%. (cid:149) The design of a heterogeneous recon(cid:2)gurable cache and policies to switch between con(cid:2)gurations. The paper is organized as follows. Section 2 provides basic background on recent innovations in multi-core cache design and related work. Section 3 describes our proposed cache architecture. Results are discussed in Section 4 and we draw conclusions in Section 5. 263 2. Background and Related Work 3. Proposed Ideas Most future large caches are expected to have NUCA architectures [22]. A large shared L2 or L3 cache can either be placed in a contiguous region or split into slices and associated with each core (tile). Early designs split the ways of a set across multiple banks, allowing a given block to have multiple possible residences. Policies were proposed to allow a block to gravitate towards a way/bank that minimized access time (D-NUCA [22]). However, this led to a non-trivial search problem: a request had to look in multiple banks to eventually locate data. A static-NUCA (S-NUCA) design instead places all ways of a set in a single bank and distributes sets across banks. Given a block address, the request is sent to a unique bank, that may or may not be in close proximity. In a recent paper, Cho and Jin [11] show that intelligent page coloring can in(cid:3)uence address index bits so that the block is mapped to a set and bank that optimizes access time. The work in this paper is built upon the page coloring concept to improve access times and eliminate search. Other papers that attempt to improve data placement with a D-NUCA approach include [5, 7, 9, 10, 48]. A number of papers also attempt to improve multi-core cache organizations by managing data cooperatively within a collection of private caches [8, 15, 38]. In these papers, if a core’s private cache cannot provide the necessary capacity, blocks are spilled into the private caches of neighboring cores. Recent papers have also proposed innovative networks for large caches. Jin et al. [20] propose a halo network that best meets the needs of a single-core D-NUCA cache, where requests begin at a cache controller and radiate away as they perform the search. Beckmann and Wood [6] propose the use of transmission lines to support low-latency access to distant cache banks. Muralimanohar and Balasubramonian [31] propose a hybrid network with different wiring and topologies for the address and data networks to improve access times. Guz et al. [17] propose the Nahalal organization to better manage shared and private data between cores. A number of recent papers have proposed cache hierarchy organizations in 3D. Li et al. [24] describe the network structures required for the ef(cid:2)cient layout of a collection of cores and NUCA cache banks in 3D. Some bodies of work implement entire SRAM cache structures on separate dies [21, 25, 27]. Loh [26] proposes the changes required to the memory controller and DRAM architecture if several DRAM dies are stacked upon the CPU die to implement main memory. Ours is the (cid:2)rst body of work that proposes recon(cid:2)guration across dies and combines heterogeneous technologies within a single level of cache. Prior work on recon(cid:2)gurable caches has been restricted to a single 2D die and to relatively small caches [3, 34, 47]. Some prior work [19, 33, 43] logically splits large cache capacity across cores at run-time and can be viewed as a form of recon(cid:2)guration. 3.1. Proposed NUCA Organization 2 We (cid:2)rst describe the basic model for access to the L2 cache in our proposed implementation. As shown in Figure 1, the bottom die contains 16 cores. The proposed ideas, including the tree topology for the on-chip network, will apply to larger systems as well. We assume 3.5 mm area for each core at 32 nm technology, based on a scaled version of Sun’s Rock core [41]. Each die is assumed to have an area of around 60 mm 2 . Each core has its own private L1 data and instruction caches. An L1 miss causes a request to be sent to the L2 cache implemented on the dies above. The SRAM die placed directly upon the processing die is partitioned into 16 banks (we will later describe the role of the DRAM die). Based on estimates from CACTI 6.0 [32], a 1 MB SRAM cache bank and its associated router/controller have an area roughly equal to the area of one core. Each bank may itself be partitioned into multiple subarrays (as estimated by CACTI 6.0) to reduce latency and power. Each bank is associated with a small cache controller unit and a routing unit. On an L1 miss, the core sends the request to the cache controller unit directly above through an inter-die via pillar. Studies [16, ?] have shown that high bandwidth vias can be implemented and these vias have pitch values as low as 4 (cid:22)m [16]. The L2 cache is organized as a static-NUCA. Four bits of the physical address are used to map a data block to one of the 16 banks. As a result, no search mechanism is required (cid:150) the physical address directly speci(cid:2)es the bank that the request must be routed to. Once an L2 cache controller receives a request from the core directly below, it examines these four bits and places the request on the onchip network if destined for a remote bank. Once the request arrives at the destination bank, the cache subarrays in that bank are accessed (more details shortly) and data is returned to the requesting core by following the same path in the opposite direction. The L2 tag maintains a directory to ensure coherence among L1 caches and this coherencerelated traf(cid:2)c is also sent on the on-chip network. It must be noted that the baseline model described so far is very similar to tiled multi-core architectures that are commonly assumed in many papers (for example, [48]). These are typically referred to as logically shared, but physically distributed L2 caches, where a slice of L2 cache is included in each tile. The key difference in our model is that this slice of L2 is separated into a second SRAM die. 3.2. Page Coloring An S-NUCA organization by itself does not guarantee low access times for L2 requests. Low access times can be obtained by ensuring that the data requested by a core is often placed in the cache bank directly above. Page coloring is a well-established OS technique that exercises greater 264 Inter-die via pillar to access portion of L2 in DRAM (not shown: one pillar per sector) Inter-die via pillar to send request from core to L2 SRAM (not shown: one pillar for each core)  Die containing 16 DRAM sectors  5x5 router links Die containing 16 SRAM sectors  On-chip tree network implemented on the SRAM die to connect the 16 sectors  Die containing 16 cores Figure 1. Stacked 3D processor layout. The bottom die contains the 16 cores, the second die contains 16 SRAM banks, and the top die contains 16 DRAM banks. Inter-die via pillars (one for each bank) are used to implement vertical connections between cores and DRAM/SRAM banks. Horizontal communication happens on the on-chip tree network (shown on the right) implemented on the second die. There are no horizontal connections between cores (banks) on the bottom (top) die. control on the values assigned to bits of the physical address. Traditionally, page coloring has been employed to eliminate the aliasing problem in large virtually indexed caches. In the context of an S-NUCA cache, page coloring can be employed to in(cid:3)uence the four bits of the physical address that determine the bank within the NUCA cache. If the entire L2 cache size is 16 MB (each bank is 1 MB) and is 4-way set-associative with 64 byte line size, a 64-bit physical address has the following components: 6 bits of offset, 16 bits of set index, and 42 bits of tag. If the page size is 4 KB, the 12 least signi(cid:2)cant bits are the page offset and the 52 most signi(cid:2)cant bits are the physical page number. The four most signi(cid:2)cant bits of the set index are also part of the physical page number. These four bits can be used to determine the bank number. Since the OS has control over the physical page number, it also has control over the bank number. For pages that are accessed by only a single core, it is straightforward to color that page such that it maps to the bank directly above the core. For pages that are accessed by multiple cores, the page must ideally be placed in a bank that represents the center-of-gravity of all requests for that page. Creating such a mapping may require page migration mechanisms and hardware counters to monitor a page’s access pattern and cache bank pressure. These are non-trivial policies that are beyond the scope of this work. For now, we assume that mechanisms can be developed to closely match the page allocation as computed by an off-line oracle analysis. In other words, the optimal cache bank for a page is pre-computed based on the center-of-gravity of requests for that page from various cores. As a result, shared data in multi-threaded workloads tend to be placed in the four banks in the center of the chip. We also devise policies to map a shared instruction (code) page either in the central shared bank or as replicated read-only pages in each bank. We evaluate the following three (oracle) page coloring schemes (also shown in Figure 2). Note that in all these schemes, private pages are always placed in the bank directly above; the differences are in how shared data and instruction pages are handled. (cid:149) Share4:D+I : In this scheme, we employ a policy that assigns both shared data and instruction pages to the central four banks. The shared bank is selected based on the proximity to the core that has maximum accesses to that page. If the program has a high degree of sharing, then we expect the central four banks to have increased pressure. This may cause the central banks to enable their additional DRAM cache space. (cid:149) Rp:I + Share4:D : In this scheme, we color all shared data pages so they are mapped to central banks. We replicate shared instruction pages and assign them to each accessing core. This causes the bank pressure to increase slightly for all private banks as the working set size of instruction pages is typically very small. This improves performance greatly as code pages are frequently accessed in the last-level cache (LLC) in commercial workloads. (cid:149) Share16:D+I : In order to uniformly utilize the available cache capacity, we color all shared pages (data and code) and distribute them evenly across all 16 banks. This page coloring scheme does not optimize 265 Figure 2. Page Coloring Schemes for latency for shared pages, but attempts to maximize SRAM bank hit rates. Note that the above schemes may require minimal OS and hardware support. These schemes rely upon detection of shared versus private pages and code versus data pages. 3.3. Recon(cid:2)gurable SRAM/DRAM Cache As described, the L2 cache is composed of 16 banks and organized as an S-NUCA. Each bank in the SRAM die has a capacity of 1 MB. If the threads in a core have large working sets, a capacity of 1 MB may result in a large miss rate. Further, centrally located banks must not only service the local needs of their corresponding cores, they must also accommodate shared pages. Therefore, the pressures on each individual bank may be very different. One possibility is to monitor bank miss rates and re-color some pages so they spill into neighboring banks and release the pressure in (cid:147)popular(cid:148) banks. Instead, we propose a recon(cid:2)gurable cache that takes advantage of 3D stacking to seamlessly grow the size of a bank in the vertical dimension. This allows the size of each bank to grow independently without impacting the capacity or access time of neighboring banks. Many proposals of 2D recon(cid:2)gurable caches already exist in the literature: they allow low access times for small cache sizes but provide the (cid:3)exibility to incorporate larger capacities at longer access times. The use of 3D and NUCA makes the design of a recon(cid:2)gurable cache especially attractive: (i) the spare capacity on the third die does not intrude with the layout of the second die, nor does it steal capacity from other neighboring caches (as is commonly done in 2D recon(cid:2)gurable caches [3, 47]), (ii) since the cache is already partitioned into NUCA banks, the introduction of additional banks and delays does not greatly complicate the control logic, (iii) the use of a third dimension allows access time to grow less than linearly with capacity (another disadvantage of a 2D recon(cid:2)gurable cache). While growing the size of a bank, the third die can implement SRAM subarrays identical to the second die, thus allowing the bank size to grow by a factor of two. But recon(cid:2)guration over homogeneous banks does not make much sense: the added capacity comes at a trivial latency cost, so there is little motivation to employ the smaller cache con(cid:2)guration. Recon(cid:2)guration does make sense when employing heterogeneous technologies. In this case, we advocate the use of a third die that implements DRAM banks, thus allowing the bank size to grow by up to a factor of nine1 . By stacking a single DRAM die (instead of eight SRAM dies), we provide high capacity while limiting the growth in temperature and cost, and improving yield. Since DRAM accesses are less ef(cid:2)cient than SRAM accesses, dynamic recon(cid:2)guration allows us to avoid DRAM accesses when dealing with small working sets. Organizing Tags and Data. The SRAM bank is organized into three memory arrays: a tag array, a data array, and an adaptive array that can act as both tag and data arrays. As with most large L2s, the tags are (cid:2)rst looked up and after the appropriate way is identi(cid:2)ed, the data subarray is accessed. Assuming that each block is 64 bytes and has a 32 bit tag (including directory state and assuming a 36-bit physical address space), the corresponding tag storage is 64 KB. The use of DRAM enables the total bank data storage to grow to 9 MB, requiring a total 576 KB of tag storage. Since tags are accessed before data and since SRAM accesses are cheaper than DRAM accesses, it is bene(cid:2)cial to implement the tags in SRAM. As a result, 512 KB of SRAM storage that previously served as data subarray must now serve as tag subarray. Compared to a typical data array, a tag array has additional logic to perform tag comparison. The data sub-array has more H-tree bandwidth for data transfer compared to its tag counterpart. To keep the recon(cid:2)guration simple, the proposed adaptive array encompasses the functionalities of both tag and data array. Growing Associativity, Sets, Block-Size. The increased capacity provided by DRAM can manifest in three forms (and combinations thereof): (i) increased associativity, (ii) increased number of sets, and (iii) increased block size. 1 Published reports claim a factor of eight difference in the densities of SRAM and DRAM [40]. 266 Con(cid:2)guration Baseline SRAM 1 MB bank Recon(cid:2)gurable cache (ways) DRAM Recon(cid:2)gurable cache (sets) DRAM Recon(cid:2)gurable cache (block size) DRAM Access time (ns) 3.13 6.71 6.52 5.43 Total Energy per access(nJ) 0.699 1.4 1.36 51.19 2 ) Area (mm 2.07 3.23 2.76 1.44 Table 1. Access time, energy, and area for various cache con(cid:2)gurations at a 4 GHz clock derived from [32]. The (cid:2)rst form of recon(cid:2)guration allows the bank to go from 4-way to 34-way set associative. 32 data ways are implemented on the DRAM die and two of the original four data ways remain on the SRAM die after half the data subarrays are converted to tag subarrays. Such an approach has the following advantages: (i) dirty lines in the two ways in the SRAM die need not be (cid:3)ushed upon every recon(cid:2)guration, (ii) every set in the bank can have two of its ways in relatively close proximity in SRAM. The primary disadvantage is the power and complexity overhead of implementing 34-way set-associativity. We can optimize the cache lookup further by moving the MRU block into the two SRAM ways on every access in the hope that this will reduce average access times. With this policy, a hit in DRAM space will require an SRAM and DRAM read and write. In our evaluations, we employ recon(cid:2)guration of the number of ways, but do not attempt the optimization where MRU blocks are moved into SRAM. The second form of recon(cid:2)guration causes an increase in the number of sets from 2K to 16K (we will restrict ourselves to power-of-two number of sets, possibly leading to extra white space on the top die). When cache size is increased, nearly every dirty cache line will have to be (cid:3)ushed. When cache size is decreased, lines residing in the SRAM data subarrays need not be (cid:3)ushed. The large cache organization has a more favorable access time/power for a fraction of the sets that map to SRAM data subarrays. The page coloring mechanism could attempt to color critical pages so they reside in the sets that map to SRAM. The third form of recon(cid:2)guration increases the block size from 64 bytes to 512 bytes (again, possibly resulting in white space). Note that this approach does not increase the tag space requirement, so 1 MB of data can be placed in SRAM, while 7 MB is placed in DRAM. This has the obvious disadvantage of placing higher pressure on the bus to main memory and also higher energy consumption for accesses. While re-con(cid:2)guring the number of sets or block size, care must be taken to not change the address bits used to determine the bank number for an address. Thus, there are multiple mechanisms to recon(cid:2)gure the cache. The differences are expected to be minor unless the application is highly sensitive to capacity (8 MB versus 8.5 MB) and memory bandwidth. While some mechanisms can escape (cid:3)ushing the entire cache, these savings are relatively minor if cache recon(cid:2)guration is performed infrequently. In our evaluations, we only focus on the (cid:2)rst recon(cid:2)guration approach that changes the number of ways. It is worth noting that banks in the DRAM die are laid out very similar to banks in the SRAM die. Our estimates for DRAM delay, power, and area are based on CACTI6.0 and discussions with industrial teams. Table 1 summarizes the delay, power, and area of the considered organizations. The DRAM banks can also be statically employed as a level-3 cache. However, this would signi(cid:2)cantly reduce the size of the SRAM L2 cache as 0.5 MB space on the second die would have to be designated as L3 tags. This may have a negative impact for several applications with moderate working-set sizes (not evaluated in this paper). It will also increase latencies for L3 and memory accesses because tags in multiple levels have to be sequentially navigated. Having described the speci(cid:2)c implementation, the following additional advantages over prior 2D designs [3, 47] are made clear: (i) a dramatic 8x increase in capacity is possible at a minor delay overhead, (ii) only two con(cid:2)gurations are possible, enabling a simpler recon(cid:2)guration policy, (iii) each cache bank can recon(cid:2)gure independently, thus avoiding a (cid:3)ush of the entire L2 all at one time. Recon(cid:2)guration Policies. We next examine the design of a recon(cid:2)guration policy. The frequency of recon(cid:2)guration is a function of the overheads of a cache (cid:3)ush and cache warm-up. Up to 16K cache lines will have to be (cid:3)ushed or brought in upon every recon(cid:2)guration. While the fetch of new lines can be overlapped with execution, cache (cid:3)ush will likely have to stall all requests to that bank. A state-of-the-art memory system can handle a peak throughput of 10 GB/s [36]. A complete (cid:3)ush will require a stall of roughly 100 K cycles. For this overhead to be minor, a recon(cid:2)guration is considered once every 10 M cycles. Recon(cid:2)guration policies are wellstudied (for example, [2, 4, 13, 14]). We design two simple policies that are heavily in(cid:3)uenced by this prior work. Every 10 M cycles, we examine the following two metrics for each cache bank: bank miss rate and usage. A high bank miss rate indicates the need to grow cache size, while low usage indicates the need to shrink cache size. It is important to pick a low enough threshold for usage, else the con(cid:2)gurations can oscillate. Usage can be determined by maintaining a bit per cache block that is set upon access and re-set at the start of every 10 M cycle interval (this is assumed in our simulations). There also exist other complexity-effective mechanisms to estimate usage [33]. In an alternative policy, various statistics can be maintained per bank to determine if the application has moved to a new phase (a part of application execution with different behavior and characteristics). If a substantial change 267 in these statistics is detected over successive large time intervals (epochs), a phase chase is signaled. Upon a phase change, we simply implement each of the two con(cid:2)guration choices and measure instruction throughput to determine which con(cid:2)guration is better (referred to as exploration). Each exploration step has to be long enough to amortize cache warm-up effects. The optimal organization is employed until the next phase change is signaled. If phase changes are frequent, the epoch length is doubled in an attempt to capture application behavior at a coarser granularity and minimize the overheads of exploration. Since these policies are strongly based on prior work (most notably [4]), and have been shown to be effective in other domains, we don’t focus our efforts on evaluating the relative merits of each of these policies. Because of the large sizes of the caches and epochs, extremely long simulations will be required to observe any interesting artifacts with regard to program phases. Our simulations model the (cid:2)rst recon(cid:2)guration policy that chooses to grow or shrink cache size based on miss rate and usage, respectively. In practice, we believe that the second recon(cid:2)guration policy may be more effective because it avoids the overheads of having to keep track of cache line usage. 3.4. Interconnect Design Most papers on NUCA designs or tiled multi-cores have employed grid topologies for the inter-bank network. Grid topologies provide high performance under heavy load and random traf(cid:2)c patterns. This was indeed the case for prior D-NUCA proposals where data could be placed in one of many possible banks and complex search mechanisms were required to locate data. However, with the 3D recon(cid:2)gurable cache hierarchy and the use of S-NUCA combined with page coloring, the traf(cid:2)c patterns are typically very predictable. For multi-programmed workloads, most requests will be serviced without long horizontal transfers and for multi-threaded workloads, a number of requests will also be serviced by the four central banks. There will be almost no requests made to non-local and non-central banks. With such a traf(cid:2)c pattern, a grid topology is clearly overkill. Many studies have shown that on-chip routers are bulky units. They not only consume a non-trivial amount of area and power [23], commercial implementations also incur delay overheads of four [23] to eight [30] cycles. Hence, it is necessary that we (cid:2)nd a minimal topology that can support the required traf(cid:2)c demands. Given the nature of the traf(cid:2)c pattern, where most horizontal transfers radiate in/out of the central banks, we propose the use of a tree topology, as shown in Figure 1. This allows the use of four 5x5 routers in the central banks and one 4x4 router as the root. In addition, the 12 leaf banks need buffers for incoming (cid:3)its and some control logic to handle (cid:3)ow control for the outgoing (cid:3)its. Note that this network is only employed on the second die (cid:150) there are no horizontal links between banks on the (cid:2)rst and third dies. 4. Results 4.1. Methodology We use a trace-driven platform simulator ManySim [49] for our performance simulations. ManySim simulates the platform resources with high accuracy, but abstracts the core to optimize for speed. The core is represented by a sequence of compute events (collected from a cycle-accurate core simulator) separated by memory accesses that are injected into the platform model. ManySim contains a detailed cache hierarchy model, a detailed coherence protocol implementation, an on-die interconnect model and a memory model that simulates the maximum sustainable bandwidth speci(cid:2)ed in the con(cid:2)guration. All our simulation parameters are shown in Table 2. CACTI-6.0 [32] is employed for estimating cache area, access latency, and power. We assume a 32nm process for our work. We derive network router power and area overheads from Orion [45]. Each router pipeline is assumed to three stages and link latency is estimated to be three cycles for the grid and tree topologies. We also incorporate a detailed network model into the ManySim infrastructure. As an evaluation workload, we chose four key multithreaded commercial server workloads: TPC-C, TPC-E, SPECjbb, and SAP. The bus traces for these workloads were collected on a Xeon MP platform where 8 threads were running simultaneously with the last level cache disabled. To simulate our 16-core system, we duplicate the 8-thread workload to run on 16 cores. This results in true application sharing only between each set of 8 cores. We do offset the address space of each 8-thread trace such that there is no address duplication. Thus, our network latencies for shared pages in the baseline are lower as we do not access the most distant bank, and we will likely see better improvements for a true 16-thread workload. We run these workloads for approximately 145 million memory references. Since we assume an oracle page-coloring mechanism, we annotate our traces off-line with the page color and append the page color bits to the address. 4.2. Baseline Organizations We consider three different baseline organizations with varying number of dies: Base-No-PC: A chip with two dies: the bottom die has 16 cores and the second die has 16 1 MB L2 SRAM cache banks organized as S-NUCA but with no explicit page coloring policy. All the banks have a roughly equal probability of servicing a core’s request. A grid topology is assumed for the inter-bank network. Base-2x-No-PC: A chip with three dies: the bottom die has 16 cores and the second and third dies contain SRAM L2 cache banks organized as S-NUCA (no explicit page coloring policy). This organization simply offers twice the cache capacity as the previous baseline at the expense of an additional die. 268 16 Private MLC Cache banks SRAM Active power 16 DRAM sector Core/Bank Area Process node Each 128KB 4-way 5-cyc 0.3W Each 8MB 32-way 30-cyc 3:5mm 2 32nm 16 LLC SRAM NUCA Cache Banks Page Size DRAM Active Power Chip Footprint Frequency 1MB 4-way, 13-cyc 4KB 0.42W 56mm 2 4 GHz Table 2. Simulation Parameters 2MB, Share4:D+I and Rp:I+Share4:D perform worse than Share16:D+I and no page-coloring case. Since Share4:D+I and Rp:I+Share4:D map all shared pages to the central four banks, these banks suffer from high pressure and high miss rates due to less cache capacity. However, since Share16:D+I distributes shared pages across all banks, it does not suffer from bank pressure. We also observe that Share16:D+I performs slightly better (7%) than the base case (no page coloring) as it is able to optimize access latency for private pages for small cache banks. For a larger cache bank (8MB), Rp:I+Share4:D performs the best compared to all the other schemes. The overall performance improvement in Share4:D compared to no page-coloring (1MB) baseline is 50%. Rp:I+Share4:D has higher performance as it has lower access latency for all the code pages and does not suffer from high bank pressure in spite of code replication overheads due to available cache capacity. We observe Share4:D+I and Share16:D+I to have comparable performance. We notice that performance of Share16:D+I does not scale with cache capacity as much. Thus, when cache capacity is not a constraint, Rp:I+Share4:D delivers the best performance compared to other schemes and makes an ideal candidate for SRAMDRAM cache. With regards to workloads, SPECjbb always performs better than the baseline (with no page-coloring) irrespective of the page coloring scheme employed. Since SPECjbb has low degree of application sharing, it does not suffer from pressure on shared banks and performs better due to communication optimization enabled by page-coloring. However, SAP and TPC-E have poor performance due to high bank pressure for small sized banks as they have high degree of sharing. Figure 5 further af(cid:2)rms our observations and shows miss ratios for various schemes. Clearly, SPECjbb has low miss ratio compared to other workloads. As expected for most cases, Rp:I+Share4:D has higher miss-ratio due to additional con(cid:3)icts introduced by code replication. 4.5. Recon(cid:2)gurable SRAM›DRAM Cache We next evaluate our SRAM-DRAM recon(cid:2)gurable cache. If an SRAM bank encounters high bank pressure, it enables the DRAM bank directly above. The total available cache capacity in each combined SRAM-DRAM bank is 8.5 MB with 0.5 MB tag space. However, we assume the total per-bank capacity to be 8MB to ease performance modeling. We compare our recon(cid:2)gurable cache against Base-No-PC and Base-2x-No-PC. Figure 6 shows IPC for various con(cid:2)gurations and baselines. When the recon(cid:2)guFigure 3. Workload characterization: sharing trend in server workloads Base-3-level: The DRAM die is used as an L3 UCA cache. The tags for the L3 cache are implemented on the SRAM die, forcing the SRAM L2 cache size to shrink to 0.5 MB. No page coloring is employed for the L2. 4.3. Workload Characterization We (cid:2)rst characterize the server workloads to understand their implications on our proposed techniques. Figure 3 shows the percentage of shared pages in all the workloads. All workloads exhibit high degree of code page sharing. SPECjbb has poor data sharing characteristics and TPCE and SAP have high data sharing charateristics. We also observed that when threads share code pages, the degree of sharing is usually high. This implies that the (cid:147)Rp:I + Share4:D(cid:148) model would have to replicate code pages in most cache banks. However, the working set size of code pages is very small compared to data pages (0.6% on average), but the relative access count of code pages is much higher (57% on average). 4.4. Evaluation of Page Coloring Schemes In order to isolate the performance impact of pagecoloring schemes, we study the effect of these schemes as a function of cache capacity. We use IPC and miss-ratio as the performance metrics. We assume a tree-network topology and vary each cache bank’s capacity. All numbers are normalized against the 1 MB baseline bank that employs no page coloring (Base-No-PC), i.e., even private pages may be placed in remote banks. Figure 4 shows the performance effect of various page-coloring schemes and Figure 5 shows the corresponding miss rates. For cache banks smaller than 269 [16] S. Gupta, M. Hilbert, S. Hong, and R. Patti. Techniques for Producing 3D ICs with High-Density Interconnect. In Proceedings of International VLSI Multilevel Interconnection, 2004. [17] Z. Guz, I. Keidar, A. Kolodny, and U. Weiser. Nahalal: Memory Organization for Chip Multiprocessors. IEEE Computer Architecture Letters, vol.6(1), May 2007. [18] J. Huh, C. Kim, H. Sha(cid:2), L. Zhang, D. Burger, and S. Keckler. A NUCA Substrate for Flexible CMP Cache Sharing. In Proceedings of ICS-19, June 2005. [19] R. Iyer, L. Zhao, F. Guo, R. Illikkal, D. Newell, Y. Solihin, L. Hsu, and S. Reinhardt. QoS Policies and Architecture for Cache/Memory in CMP Platforms. In Proceedings of SIGMETRICS, June 2007. [20] Y. Jin, E. J. Kim, and K. H. Yum. A Domain-Speci(cid:2)c OnChip Network Design for Large Scale Cache Systems. In Proceedings of HPCA-13, February 2007. [21] T. Kgil, S. D’Souza, A. Saidi, N. Binkert, R. Dreslinski, S. Reinhardt, K. Flautner, and T. Mudge. PicoServer: Using 3D Stacking Technology to Enable a Compact Energy Ef(cid:2)cient Chip Multiprocessor. In Proceedings of ASPLOSXII, October 2006. [22] C. Kim, D. Burger, and S. Keckler. An Adaptive, NonUniform Cache Structure for Wire-Dominated On-Chip Caches. In Proceedings of ASPLOS-X, October 2002. [23] P. Kundu. On-Die Interconnects for Next Generation CMPs. In Workshop on On- and Off-Chip Interconnection Networks for Multicore Systems (OCIN), December 2006. [24] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, N. Vijaykrishnan, and M. Kandemir. Design and Management of 3D Chip Multiprocessors Using Network-in-Memory. In Proceedings of ISCA-33, June 2006. [25] C. C. Liu, I. Ganusov, M. Burtscher, and S. Tiwari. Bridging the Processor-Memory Performance Gap with 3D IC Technology. IEEE Design and Test of Computers, 22:556(cid:150) 564, November 2005. [26] G. Loh. 3D-Stacked Memory Architectures for Multi-Core Processors. In Proceedings of ISCA-35, June 2008. [27] G. Loi, B. Agrawal, N. Srivastava, S. Lin, T. Sherwood, and K. Banerjee. A Thermally-Aware Performance Analysis of Vertically Integrated (3-D) Processor-Memory Hierarchy. In Proceedings of DAC-43, June 2006. [28] N. Magen, A. Kolodny, U. Weiser, and N. Shamir. Interconnect Power Dissipation in a Microprocessor. In Proceedings of System Level Interconnect Prediction, February 2004. [29] C. McNairy and R. Bhatia. Montecito: A Dual-Core, DualThread Itanium Processor. IEEE Micro, 25(2), March/April 2005. [30] S. Mukherjee, P. Bannon, S. Lang, A. Spink, and D. Webb. The Alpha 21364 Network Architecture. In IEEE Micro, volume 22, 2002. [31] N. Muralimanohar and R. Balasubramonian. Interconnect Design Considerations for Large NUCA Caches. In Proceedings of the 34th International Symposium on Computer Architecture (ISCA-34), June 2007. [32] N. Muralimanohar, R. Balasubramonian, and N. Jouppi. Optimizing NUCA Organizations and Wiring Alternatives for Large Caches with CACTI 6.0. In Proceedings of the 40th International Symposium on Microarchitecture (MICRO-40), December 2007. [33] M. Qureshi and Y. Patt. Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches. In Proceedings of MICRO-39, December 2006. [34] P. Ranganathan, S. Adve, and N. Jouppi. Recon(cid:2)gurable caches and their application to media processing. Proceedings of ISCA-27, pages 214(cid:150)224, June 2000. [35] Samsung Electronics Corporation. Samsung Electronics Develops World’s First Eight-Die Multi-Chip Package for Multimedia Cell Phones, 2005. (Press release from http://www.samsung.com). [36] Semiconductor Industry Association. International Technology Roadmap for Semiconductors 2005. http://www.itrs.net/Links/2005ITRS/ Home2005.htm. [37] K. Skadron, M. R. Stan, W. Huang, S. Velusamy, and K. Sankaranarayanan. Temperature-Aware Microarchitecture. In Proceedings of ISCA-30, pages 2(cid:150)13, 2003. [38] E. Speight, H. Sha(cid:2), L. Zhang, and R. Rajamony. Adaptive Mechanisms and Policies for Managing Cache Hierarchies in Chip Multiprocessors. In Proceedings of ISCA-32, June 2005. [39] Tezzaron Semiconductor. www.tezzaron.com. [40] S. Thoziyoor, J. H. Ahn, M. Monchiero, J. B. Brockman, and N. P. Jouppi. A Comprehensive Memory Modeling Tool and its Application to the Design and Analysis of Future Memory Hierarchies. In Proceedings of ISCA-35, June 2008. [41] M. Tremblay and S. Chaudhry. A Third-Generation 65nm 16-Core 32-Thread Plus 32-Scout-Thread CMT. In Proceedings of ISSCC, Febuary 2008. [42] S. Vangal, J. Howard, G. Ruhl, S. Dighe, H. Wilson, J. Tschanz, D. Finan, P. Iyer, A. Singh, T. Jacob, S. Jain, S. Venkataraman, Y. Hoskote, and N. Borkar. An 80-Tile 1.28TFLOPS Network-on-Chip in 65nm CMOS. In Proceedings of ISSCC, February 2007. [43] K. Varadarajan, S. Nandy, V. Sharda, A. Bharadwaj, R. Iyer, S. Makineni, and D. Newell. Molecular Caches: A Caching Structure for Dynamic Creation of Application-Speci(cid:2)c Heterogeneous Cache Regions. In Proceedings of MICRO39, December 2006. [44] H. S. Wang, L. S. Peh, and S. Malik. A Power Model for Routers: Modeling Alpha 21364 and In(cid:2)niBand Routers. In IEEE Micro, Vol 24, No 1, January 2003. [45] H.-S. Wang, X. Zhu, L.-S. Peh, and S. Malik. Orion: A Power-Performance Simulator for Interconnection Networks. In Proceedings of MICRO-35, November 2002. [46] Y. Xie, G. Loh, B. Black, and K. Bernstein. Design Space Exploration for 3D Architectures. ACM Journal of Emerging Technologies in Computing Systems, 2(2):65(cid:150)103, April 2006. [47] C. Zhang, F. Vahid, and W. Najjar. A Highly Con(cid:2)gurable Cache Architecture for Embedded Systems. In Proceedings of ISCA-30, June 2003. [48] M. Zhang and K. Asanovic. Victim Replication: Maximizing Capacity while Hiding Wire Delay in Tiled Chip Multiprocessors. In Proceedings of ISCA-32, June 2005. [49] L. Zhao, R. Iyer, J. Moses, R. Illikkal, S. Makineni, and D. Newell. Exploring Large-Scale CMP Architectures Using ManySim. IEEE Micro, 27(4):21(cid:150)33, 2007. 273 "
2009,Prediction router - Yet another low latency on-chip router architecture.,"Network-on-Chips (NoCs) are quite latency sensitive, since their communication latency strongly affects the application performance on recent many-core architectures. To reduce the communication latency, we propose a low-latency router architecture that predicts an output channel being used by the next packet transfer and speculatively completes the switch arbitration. In the prediction routers, incoming packets are transferred without waiting the routing computation and switch arbitration if the prediction hits. Thus, the primary concern for reducing the communication latency is the hit rates of prediction algorithms, which vary from the network environments, such as the network topology, routing algorithm, and traffic pattern. Although typical low-latency routers that speculatively skip one or more pipeline stages use a bypass datapath for specific packet transfers (e.g., packets moving on the same dimension), our prediction router predictively forwards packets based on a prediction algorithm selected from several candidates in response to the network environments. In this paper, we analyze the prediction hit rates of six prediction algorithms on meshes, tori, and fat trees. Then we provide three case studies, each of which assumes different many-core architecture. We have implemented a prediction router for each case study by using a 65 nm CMOS process, and evaluated them in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. The results show that although the area and energy are increased by 6.4-15.9% and 8.0-9.5% respectively, up to 89.8% of the prediction hit rate is achieved in real applications, which provide favorable trade-offs between the modest hardware/energy overheads and the latency saving.","Prediction Router: Yet Another Low Latency On-Chip Router Architecture ∗ Hiroki Matsutani1 , Michihiro Koibuchi2 , Hideharu Amano1 , and Tsutomu Yoshinaga3 1Keio University 2National Institute of Informatics 3-14-1, Hiyoshi, Kohoku-ku, Yokohama, 2-1-2, Hitotsubashi, Chiyoda-ku, Tokyo, {matutani,hunga}@am.ics.keio.ac.jp JAPAN 223-8522 JAPAN 101-8430 koibuchi@nii.ac.jp 3The University of Electro-Communications 1-5-1, Chofugaoka, Chofu-shi, Tokyo, JAPAN 182-8585 yosinaga@is.uec.ac.jp Abstract 1 Introduction Network-on-Chips (NoCs) are quite latency sensitive, since their communication latency strongly affects the application performance on recent many-core architectures. To reduce the communication latency, we propose a lowlatency router architecture that predicts an output channel being used by the next packet transfer and speculatively completes the switch arbitration. In the prediction routers, incoming packets are transferred without waiting the routing computation and switch arbitration if the prediction hits. Thus, the primary concern for reducing the communication latency is the hit rates of prediction algorithms, which vary from the network environments, such as the network topology, routing algorithm, and trafﬁc pattern. Although typical low-latency routers that speculatively skip one or more pipeline stages use a bypass datapath for speciﬁc packet transfers (e.g., packets moving on the same dimension), our prediction router predictively forwards packets based on a prediction algorithm selected from several candidates in response to the network environments. In this paper, we analyze the prediction hit rates of six prediction algorithms on meshes, tori, and fat trees. Then we provide three case studies, each of which assumes different many-core architecture. We have implemented a prediction router for each case study by using a 65nm CMOS process, and evaluated them in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. The results show that although the area and energy are increased by 6.4-15.9% and 8.0-9.5% respectively, up to 89.8% of the prediction hit rate is achieved in real applications, which provide favorable trade-offs between the modest hardware/energy overheads and the latency saving. ∗ This work is supported by VLSI Design and Education Center (VDEC), the University of Tokyo in collaboration with STARC, e-Shuttle, Inc., and Fujitsu Ltd. As semiconductor technology improves, the number of processing cores integrated on a single chip has continually increased. More recently, commercial or prototype chips that have 64 or more processing cores have already been produced [5, 19, 21]. To connect such many cores, Network-on-Chips (NoCs) [6, 2, 3] that introduce a packetswitched network structure have been widely employed instead of traditional bus-based on-chip interconnects. NoCs are quite latency sensitive, since their communication latency determines the application performance on the many-core architectures, and it is becoming more dominant factor when the number of cores on a chip increases. Unfortunately, the communication latency in packet-switched networks tends to be large compared with that in bus-based systems, since packets are forwarded via many routers, each of which performs complex operations including the routing computation, switch allocation, and switch traversal for every packet or ﬂit, as shown in Figure 1. To reduce the communication latency on interconnection networks, various router architectures have been recently developed. A speculative router speculatively performs different pipeline stages of a packet transfer in parallel [18, 7, 11]. The look-ahead routing technique removes the control dependency between the routing computation and switch allocation in order to perform them in parallel [7, 15, 16]. In addition, aggressive low-latency router architectures that bypass one or more pipeline stages for speciﬁc packet transfers have been proposed [13, 8, 14, 17, 12]. As yet another low-latency router architecture, we propose the prediction router that predicts an output channel being used by the next packet transfer and speculatively completes the switch arbitration. In the prediction routers, incoming packets are transferred without waiting the routing computation and switch arbitration if the prediction hits. Otherwise, they are transferred through the original pipeline stages without any additional latency overheads. Thus, the 978-1-4244-2932-5/08/$25.00 ©2008 IEEE 367 primary concern for reducing the communication latency is the hit rates of prediction algorithms, which vary from the network environments, such as the network topology, routing algorithm, and trafﬁc pattern. For example, a predictor that exploits the path regularity of dimension-order routing on meshes or tori [8] would work well for packet transfers that span multiple hops, but it rarely hit when a given trafﬁc pattern contains a lot of neighboring communications. Although existing low-latency routers that speculatively skip one or more pipeline stages use a bypass datapath for speciﬁc packet transfers [13, 8, 14, 17, 12], our prediction router predictively forwards packets based on a prediction algorithm selected from several candidates in response to the network environments. Although only the concept of predictive switching was proposed in [22] and [23] for interconnection networks of massively parallel computers, they neither showed their onchip router architecture nor evaluated its performance with various prediction algorithms, cost, and energy consumption based on the detailed router design. In this paper, ﬁrst, we propose the prediction router architecture that can select a prediction algorithm from several candidates in response to the network environments. Second, we present formulas that estimate the prediction hit rates of various algorithms on meshes, tori, and fat trees. Third, we provide three case studies, each of which assumes different many-core architecture. We have implemented a prediction router for each case study by using a 65nm CMOS process, and evaluated them in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. Our claim is that the low-latency routers should support multiple prediction algorithms (not a single algorithm!) in order to deal with various trafﬁc patterns, and this paper would be a ﬁrst guide to ﬁnd which prediction algorithms should be supported for a given network topology, routing algorithm, and trafﬁc pattern. The rest of this paper is organized as follows. Section 2 introduces typical low-latency router architectures. Section 3 explains the prediction router architecture. Section 4 analyzes the hit rates of various prediction algorithms on meshes, tori, and fat trees. Then, Section 5 evaluates the prediction router through the three case studies. Section 6 concludes this paper. 2 Related Work We ﬁrst show a pipeline structure of a baseline router, and introduce two conventional low-latency techniques, called the speculative and look-ahead transfers, to shorten the router pipeline stages. Then we describe the other aggressive low-latency router architectures that speculatively skip one or more pipeline stages. 2.1 Conventional Router A 4-cycle router quoted from [7] is assumed as a baseline router in this paper. In the router, a header ﬂit is transferred through four pipeline stages that consist of the routing computation (RC) stage, virtual channel allocation (VA) HEAD RC VSA ST RC VSA ST RC VSA ST DATA_0 DATA_1 DATA_2 SA ST SA ST SA SA ST SA ST ST SA ST SA ST SA ST SA ST 1 2 3 4 5 6 7 8 9 10 11 12 ELAPSED TIME [CYCLE] ROUTER A ROUTER B ROUTER C Figure 1. Router pipeline structure (3-cycle) HEAD VSA NRC ST VSA NRC ST VSA NRC ST DATA_0 SA ST SA ST SA ST DATA_1 SA ST SA ST SA ST 1 2 3 4 5 6 7 8 9 ELAPSED TIME [CYCLE] ROUTER A ROUTER B ROUTER C Figure 2. Router pipeline structure (2-cycle) stage for output channels, switch allocation (SA) stage for allocating the time-slot of the crossbar switch to the output channel, and switch traversal (ST) stage for transferring ﬂits through the crossbar. 2.2 Speculative Router A well-known technique to reduce the number of pipeline stages in a router is the speculative transfer that performs different pipeline stages in parallel [18, 7, 11]. Figure 1 shows an example of the speculative router that performs the VA and SA in parallel. These operations are merged into a single stage, called the virtual channel and switch allocation (VSA) stage. Notice that when the VA operation in the VSA stage cannot be completed due to the conﬂicts with other packets, the SA operation also fails regardless of its result and must be re-executed. For further reducing the pipeline stages, the double speculation that performs RC, VA, and SA operations in parallel is possible, but it would degrade the performance due to the frequent miss speculations and retries. 2.3 Look-Ahead Router The look-ahead routing technique removes the control dependency between the routing computation and switch allocation in order to perform them in parallel, by selecting the output channel of the i-th hop router in the (i − 1)-th hop router [7]. That is, each router performs the routing computation for the next hop (denoted as NRC), as shown of the (i − 1)-th hop router is used in the i-th hop router, the in Figure 2. Since the computational result at the NRC stage result does not affect the following VA and SA operations 2 368 reserve VSA Arbiter kill Predictor request & grant configure [4:0] VC0  [63:0] [63:0] p0 p4 p0 p4 Input channels Crossbar (5x5) Output channels Figure 3. Prediction router architecture in the (i − 1)-th hop router; therefore the NRC and VSA operations can be performed in parallel (Figure 2). However, the NRC stage should be completed before the ST stage in the same router, because the hint bits in a packet header, which are the results of the NRC, must be updated for the NRC/VSA stage of the next router before the packet header is sent out. Thus, the control dependency between the NRC and ST stages in a router still remains difﬁculty for shortening to a single cycle router without harming the frequency, although some aggressive attempts using this approach have been done [7, 15, 16]. 2.4 Bypassing Router This section introduces existing aggressive low-latency router architectures that bypass one or more pipeline stages for the speciﬁc packet transfers, such as paths frequently used so far [13, 17], packets continually moving along the same dimension [8], and paths pre-speciﬁed [14, 12]. Express virtual channels (VCs) have been proposed to reduce the communication latency by bypassing pipeline stages in intermediate routers between frequently used source-destination pairs [13]. This method is efﬁcient for reducing the communication latency of long-haul packet transfers that span multiple intermediate routers; however, it does not work well for communications between neighboring routers. Dynamic fast path architecture also reduces the communication latency of frequently used paths by sending a switch arbitration request to the next node before ﬂits in the fast path actually enter the next node [17]. Mad-postman switching exploits the path regularity of dimension-order routing on meshes for reducing the communication latency on off-chip networks that use bit-serial physical channels [8]. In Mad-postman switching, a router forwards an incoming packet to the output channel in the same dimension as soon as it receives the packet. Thus, it also has a problem with neighboring communications that do not go straight. Preferred path employs a bypass datapath that connects input and output channels without using the crossbar switch in a router, in addition to the original datapath that goes through the crossbar [14]. The bypass datapath can be customized so as to reduce the communication latency between the speciﬁc source-destination pairs; however, it cannot adaptively select the bypass (or original) datapath for each packet in response to the recent trafﬁc pattern. Also, ORIGINAL RC VSA ST RC VSA ST RC VSA ST MISS HIT HIT PREDICTION RC VSA ST PST PST 1 2 3 4 5 6 7 8 9 10 ELAPSED TIME [CYCLE] ROUTER A ROUTER B ROUTER C Figure 4. Prediction router pipeline Default-backup path (DBP) mechanism provides a lowlatency unicursal ring network that spans all routers on a chip, though it has been originally proposed for on-chip fault-tolerance [12]. Only the packets that move along the DBP can be transferred in a single cycle. All techniques listed above can bypass some pipeline stages only for the speciﬁc packet transfers. However, the network environments (e.g., routing algorithm and trafﬁc pattern) will be easily changed, since multiple applications are usually running on an NoC. A technique that accelerates only a speciﬁc trafﬁc may not work well for another one. The low-latency routers should thus support multiple prediction algorithms in order to deal with more than one application. In addition, we do need a guide to ﬁnd which prediction algorithms should be supported for a given network topology, routing algorithm, and trafﬁc pattern. 3 Prediction Router In this section, we propose the prediction router architecture, which predictively forwards packets without waiting the RC, VA, and SA operations, based on the prediction algorithm selected from several candidates in response to the network environments, such as the network topology, routing algorithm, and trafﬁc pattern. The prediction router architecture and its pipeline structure are described in Section 3.1 and Section 3.2, respectively. Six prediction algorithms designed for the prediction router are introduced in Section 3.4. 3.1 Router Components For purposes of illustration, we ﬁrst introduce the baseline router architecture, and then we show the changes required for the prediction router. As a baseline router, we assume a dimension-ordered wormhole router that has ﬁve physical channels, each of which has two virtual channels. It thus has ﬁve input channels, ﬁve output channels, a 5 × 5 crossbar switch, and an arbitration unit, as shown in Figure 3. When an input channel receives a packet, it computes an output channel to be used based on the destination address stored in the header ﬂit. It asserts the request signal to the arbiter in order to allocate a time-slot of the crossbar for the requested output channel. The channel access is granted by the arbiter when the request wins the arbitration. Then it will be released after the corresponding tailer ﬂit goes through the crossbar. 3 369 Algorithm 1 State transitions of an input channel (idata: an input ﬂit, odata: an output ﬂit) 2: 3: 4: 5: 6: 7: 8: 1: if current state == RC && idata is a packet header then port ← routing computation(idata); current state ← VSA; if predicted port is free then odata ← idata; {predictive switch traversal (PST)} if predicted port == port then current state ← ST; {the prediction hits} else kill ← predicted port; {remove the mis-routed ﬂit at the output channel} end if end if 11: else if current state == VSA then normal VSA operation; 13: else if current state == ST then normal ST operation; 15: else if current state == RC && no packet arrives then predicted port ← prediction(); {an output channel likely to be used is selected and reserved} 17: end if 9: 10: 12: 14: 16: Figure 3 illustrates the prediction router architecture changed from the original one mentioned above. The changes required for the prediction router are as follows: 1) adding a predictor for each input channel, 2) changing the arbitration unit so that it can handle the tentative reservations from predictors, and 3) adding the kill signal for each output channel in order to remove mis-routed ﬂits when the prediction fails. The predictor in an input channel forecasts which output channel will be used by the next packet transfer while the input channel is idle. Then it asserts the reserve signal to the arbiter in order to tentatively reserve a time-slot of the crossbar for the predicted output channel, as shown in Figure 3. The details about the prediction algorithms are described in Section 3.4. The arbiter handles the request and reserve signals from each input channel (Figure 3). Obviously, the former has higher priority than the latter; thus an output channel that has been reserved by an input channel will be preempted by another input channel that requests the output channel in the next cycle. Although the predictive switching drastically reduces the packet delay at a router if the prediction hits, it generates mis-routed ﬂits (or dead ﬂits) if the prediction fails. To remove such dead ﬂits inside a router, the input channel asserts the kill signal to the mis-predicted output channel if it predictively forwards a packet to the wrong output channel. The output channel masks all incoming data if the kill signal for it is asserted; thus the dead ﬂits never propagate to the outside of the router. 3.2 Pipeline Structure Figure 4 shows a timing diagram of a packet transfer from router (a) to router (c) in the cases of the 3-cycle speculative router (denoted as original) and the prediction router. In the prediction router, the prediction hits in router (b) and router (c), while it fails in router (a). Since the prediction router completes the RC, VA, and SA operations prior to packet arrivals, it performs the predictive switch traversal (PST) as soon as a new packet arrives; thus the header ﬂit transfer is completed in a single cycle without waiting the RC and VSA stages if the prediction hits. Otherwise, although dead ﬂit(s) are forwarded to the wrong output channel, their propagation is stopped inside a router and their copy is forwarded to the correct output channel through the original pipeline stages (i.e., RC, VSA, and ST stages) without any additional latency overheads. Such state transitions of an input channel in the prediction router are summarized in Algorithm 1. Notice that the predictive switching can be applied to various router architectures, such as the look-ahead 2-cycle router mentioned in the previous section. We will apply this concept to three case studies in Section 5. 3.3 Predictor Selection Each input channel can select a single prediction algorithm from all algorithms supported in the channel. The reconﬁguration of the predictor is classiﬁed into two schemes: static and adaptive. A single algorithm is statically selected at the beginning of each application in the static scheme, while it is dynamically switched in the adaptive scheme. In this paper, we assume the static scheme, since it is efﬁcient for many on-chip applications whose trafﬁc pattern can be pre-analyzed at the design time by system-level simulations. For unknown applications, here we introduce the adaptive scheme. Assume that an input channel supports n algorithms. All n algorithms predict their preferred output channel when the input channel receives a packet. Each algorithm increments its own counter when its prediction hits. For every m packet transfers, a prediction algorithm with the largest counter value is selected as the next algorithm. The value m determines the frequency of the reconﬁgurations. The adaptive scheme consumes more energy, since every algorithm predicts for each packet, though it can adaptively adjust the predictor even with unknown trafﬁcs. 3.4 Prediction Algorithms Since some pipeline stages are skipped only when the prediction hits, the primary concern for reducing the communication latency is the prediction algorithm to be used. Fortunately, sophisticated value predictors have been developed for the speculative execution in microprocessors [4] and the universal information theory [9]. The prediction router can use some of them. Here we brieﬂy describe the ones applicable to our lightweight prediction routers. • Random: The simplest prediction algorithm is Random, which randomly predicts an output channel that will be used by the next incoming packet. In the case of dimension-order routing on 3-D torus, for example, a packet in an input channel on the y -dimension is transferred to an output channel on the y - orz -dimension, or local core. Thus, it randomly selects one of them. 4 370 1 2 3 0 1 2 3 0 Core Router 0 Router Core 0 1 2 3 1 2 3 0 3 1 2 0 3 1 2 0 1 2 3 (a) 2-D mesh (b) 2-D torus rank-2 p=2 rank-1 q=4 # of ranks (r) = 2 (c) Fat tree (2,4,2) 0 1 2 3 (d) Fat tree (4,4,2) Figure 5. Target network topologies Since it exploits neither the topological regularity nor the trafﬁc locality, its prediction hit rate tends to be quite low. • Static Straight (SS): A simple and practical prediction algorithm optimized for dimension-order routing on meshes or tori is SS, which assumes that all incoming packets are continuing along the same dimension, as in Mad-postman switching [8]. In the case of dimension-order routing on 2-D mesh, for example, the SS predictor fails at most two times per a ﬂight, since a packet may turn from the x-dimension to y dimension in addition to the destination core. Therefore, packets that travel a long distance increase the prediction hit rate, whereas the communication locality negatively affects the SS predictor. It does not require any history tables (i.e., it is stateless); hence its implementation cost is low. • Custom: The Custom strategy allows users to deﬁne which output channel each input channel should predict, as in Preferred path [14]. In the case of a router with ﬁve physical channels, for example, a 3-bit register to store the preferred output channel is required for each input channel. • Latest Port Matching (LP): The LP strategy predicts that the next incoming packet will be forwarded to the same output channel as that of the previous packet. The LP predictor requires only a single history record in each input channel. The killer applications of LP are the trafﬁc patterns that have strong access regularity, such as the straight-after-straight communications, and the repeated communications between two neighboring nodes. • Finite Context Method (FCM): The nth-order FCM strategy predicts the most frequently used value after the last n-context sequence [4]. As n increases, the two-dimensional history table consisting of the ncontext sequence and next value frequency gets bigger, while the prediction accuracy improves. For the sake of simplicity, in this paper, we have implemented and evaluated the 0th-order FCM predictor, which just selects the most frequently used output channel. • Sampled Pattern Matching (SPM): The SPM algorithm was originally proposed as a universal predictor [9]. It selects a value which has the highest probability after a sufﬁx sequence, called a marker, in a given data. The predicted value is calculated by applying a majority rule to all values appearing at positions just after the markers in the data. We can use it to predict an output-channel number by ﬁnding a value of the most frequently occurring number after the (longest) sufﬁx sequence of communication history. A preferred output channel is statically ﬁxed in SS and Custom, while the other algorithms dynamically predict it according to the trafﬁc pattern or their history table. Each algorithm requires different predictor circuit and provides different hit rate for a given trafﬁc pattern. The following sections illustrate these trade-offs. In the next section, we analyze the prediction hit rates of these algorithms on meshes, tori, and fat trees. Their area and energy overheads are evaluated in Section 5 through three case studies. 4 Hit Rate Analysis The reliable analytical model is essential to expect the prediction hit rate and communication latency of various prediction algorithms on a given network environment (e.g., topology, routing, and trafﬁc). In this section, we present formulas that estimate the prediction hit rates of the proposed prediction algorithms on tori, meshes, and fat trees, respectively (see Figure 5). Using the analytical model, we can select or dynamically update the prediction algorithm in response to the environment. The accuracy of the model will be conﬁrmed through the simulations of three case studies in Section 5. Such conﬁrmation is also important to guarantee the value and reliability of the analytical model. 4.1 Torus (k-ary n-cube) To analyze the prediction hit rate on uniform trafﬁc, we ﬁrst count the number of paths passing through each channel on a given k-ary n-cube, assuming that the dimensionorder routing is used on it. Then we derive the formulas that calculate the prediction hit rates of SS, FCM, LP, SPM, and Random algorithms. 5 371 0 2 3 1 4 6 7 5 # of paths crossing the dateline    = 1+2+3+4 = 10 # of straight paths over the dateline    = 1+2+3 = 6 (solid lines show the straight paths) (0 to 4) (1 to 4) (1 to 5) (2 to 4) (2 to 5) (2 to 6) (3 to 4) (3 to 5) (3 to 6) (3 to 7) a dateline Figure 6. Path distribution on a k-ary 1-cube 4.1.1 Number of Paths Here we assume that each node consists of a processing core and a router. In a k-ary 1-cube, the number of paths that go through a channel between neighboring two nodes, T1d , and the number of paths that go straight at the next node, T1dss , are calculated as follows (see Figure 6). T1d = 1 + 2 + . . . + ( k 2 − 1 2 ) = k 2 − 1 2(cid:2) i=1 i (1) T1dss = 0 + 1 + . . . + ( k 2 − 3 2 ) = k 2 − 1 2(cid:2) i=1 (i − 1) = k 2 − 3 2(cid:2) i=1 i (2) We extend them to k-ary n-cubes, since most NoCs have employed two-dimensional topologies, and more recently, three-dimensional NoCs are also studied [10]. Assuming that k is an odd number, the number of paths on a channel, T , and the number of paths that go straight at the next node, Tss , are calculated as follows. T = kn−1 k 2 − 1 2(cid:2) i=1 i (3) Tss = kn−1 k 2 − 3 2(cid:2) i=1 i (4) If k is an even number, on the other hand, there are two ways to reach a destination k 2 hops away in a dimension: one uses the wrap-around channel and the other does not. Assuming that both paths are equally used, the number of paths that go through a channel can be estimated as follows. T = kn−1 k 2(cid:2) i=1 (i − 1 2 ) = kn−1 ( k 2(cid:2) i=1 i − k 4 ) (5) Tss = kn−1 k 2 −1(cid:2) (i − 1 2 i=1 ) = kn−1 ( k 2 −1(cid:2) i=1 i − k 4 + 1 2 ) (6)  0  20  40  60  80  100  4  6  8  10  12  14 Network size (k-ary 2-cube)   16 P r c d e i i t n o h t i r a t e [ % ] 75.8% 22.5% SS, FCM Random LP, SPM Figure 7. Prediction hit rates on k-ary 2-cube In the case of dimension-order routing, a packet at an i-dimensional is transferred to a j input channel dimensional output channel when it completes its movements on 1, 2, . . . , (j − 1) dimensional channels, where i < j . The number of paths from an i-dimensional input channel to a j -dimensional output channel, T (j ), and the number of paths from an i-dimensional input channel to a local processing core, TP E (i), are calculated as follows. T (j ) = ( k 2 − 1 2 ) 2 kn+i−j−1 (7) TP E (i) = ( k 2 − 1 2 )k i−1 (8) 4.1.2 Prediction Hit Rate The prediction hit rate of SS algorithm is calculated as a percentage of the number of paths from the input channel to the predicted output channel, in the total number of paths that go through the input channel. Using the T and Tss , the prediction hit rate of SS algorithm at an i-dimensional input channel is calculated as follows. Pss = Tss T (9) The SS predictor is equivalent to the 0th-order FCM predictor in the case of uniform trafﬁc, because, for each input channel, the straight output channel in the same dimension is the most frequently used one; thus Pss = Pf cm . The LP predictor succeeds when two consecutive packets are transferred between the same pair of input and output channels in a router. Thus, the prediction hit rate of LP algorithm at an i-dimensional input channel is calculated as follows. Plp = ( Tss T )2 + ( TP E (i) T )2 + 2 n(cid:2) j=i+1 ( T (j ) T )2 (10) The SPM predictor uses a history table that records the output channels used by packets. In the case of uniform trafﬁc, since each core independently injects packets to a random destination, the hit rate of the SPM predictor is the same as that of the LP predictor; thus Plp = Pspm . 6 372       0 2 3 1 k-1 k-2 1(k-1) 2(k-2) 3(k-3) (k-1)1 (k-2)2 4(k-4) (k-1)1 (k-2)2 (k-3)3 (k-4)4 2(k-2) 1(k-1) # of paths crossing each link Figure 8. Path distribution on a k-ary 1-mesh  0  20  40  60  80  100  4  6  8  10  12  14 Network size (k-ary 2-mesh)   16 P r c d e i i t n o h t i r a t e [ % ] 82.4% 41.8% SS, FCM Random LP, SPM Figure 9. Prediction hit rates on k-ary 2-mesh For the purpose of comparison, we calculate the hit rate of the Random predictor. In this case, a packet at an idimensional channel is transferred to one of j -dimensional channels or a local processing core, where i ≤ j . Thus, the hit rate of the Random predictor at an i-dimensional input channel is calculated as follows. Prandom = 1 n n(cid:2) j=i 1 2(n − j + 1) (11) Finally, the prediction hit rate of each algorithm at a local input channel is calculated as follows. PssP E = Pf cmP E = TP E (n) n j=1 TP E (j )2 kn − 1 (12) PlpP E = PspmP E = 2 (cid:3) (kn − 1)2 (13) Based on the formulas listed above, we have estimated dom predictors on k-ary 2-cubes, where 4 ≤ k ≤ 17. As the prediction hit rates of the SS, FCM, LP, SPM, and Ranshown in Figure 7, the hit rates of SS, FCM, LP, and SPM increase as the number of nodes (i.e., k) increases, while that of Random is constant. We have also conﬁrmed these results by using a cycle-accurate network simulator. 4.2 Mesh (k-ary n-mesh) We derive the formulas that estimate the prediction hit rate of mesh by using the similar approach to the torus. The hit rate of each predictor at an i-dimensional input channel on a k-ary n-mesh is calculated as follows. T = kn−1 k−1(cid:2) j=1 j (k − j ) (14) Pss = Pf cm = kn−1 (cid:3) k−1 j=1 j (k − j − 1) T (15) Plp = Pspm = P 2 ss + k2(n−1) (cid:3) j=1 j k T 2 k2(n+i−d−1) ((k − j )2 + (j − 1)2 ) T 2 + (16) n(cid:2) d=i k(cid:2) j=1 We here omit the formulas that estimate the prediction hit rate at a local input channel on mesh, since they are quite similar to those on torus. Figure 9 shows the prediction hit rates of the prediction algorithms on k-ary 2-meshes, where 4 ≤ k ≤ 17. Since edge or corner nodes on mesh have fewer links than the others, the prediction hit rate of a mesh tends to be higher than that of the same-sized torus. We have conﬁrmed these results in Section 5.1.3. 4.3 Fat Tree (p, q , r) A fat tree is expressed with a triple (p, q , r), where p is the number of upward links, q is the number of downward links, and r is the number of ranks in the fat tree (see Figure 5). A router located in i-hop distance from a core is called a rank-i router, and a core is regarded as a rank-0 router, for the sake of convenience. The adaptive up*/down* routing algorithm is employed in fat trees. That is, a fat tree provides multiple upward paths from a source core to the least common ancestor (LCA) of the source and destination cores, while it provides only a single deterministic path from the LCA to the destination core. In the upper transfers, the prediction succeeds if one of output channels towards the root has been selected as a predicted output channel; thus the prediction frequently hits in lower input channels, while it is difﬁcult to predict a correct output channel in upper input channels, especially for uniform random trafﬁc. As in the cases of the torus and mesh, the prediction hit rate of each algorithm on fat tree is calculated based on the path distribution. Figure 10 illustrates the number of paths from a single source to several destinations. The number of paths at an upper output channel in a rank-i router, Tup (i), is calculated as follows. Tup (i) = ( q p )i (qr − q i ) = q i+r − q2i pi (17) In a rank-i router, the number of paths from a lower input channel to a lower output channel, Tlo (i), is calculated by using the regularity of fat tree. Tlo (i) = q i−1 pi−1 (18) 7 373       0 1 2 3 4 5 6 7 # of path=1 # of path=1 # of path=1 0 1 2 3 1 # of paths=q  -1  r # of paths=(1/p)(q  -q) r 2 1 1 1 # of paths=q/p 3 # of paths=q/p 2 1 1 1 1 # of paths=(1/p)  (q  -q  ) 2 r 2 # of paths=q  / p  2 2 Figure 10. Path distribution on fat tree (1,4,3) SS,FCM (p=any) LP,SPM (p=any) Random (p=1) Random (p=2) Random (p=4) 48.7%  100 ] % [ e t a r t i h n o i t i c d e r P  80  60  40  20 56.2%  0 42 62 82 102 122 142 Network size (number of cores)  162 Figure 11. Prediction hit rates on fat tree (p,4,r) The prediction hit rate of a lower input channel in a ranki router is calculated as follows. Pss = Pf cm = pTup (i) Plp = Pspm = P 2 ss Tup (i − 1) + (q − 1)( Tlo (i) Tup (i − 1) (19) (20) )2 Again, in the upper transfers, the prediction succeeds if one of output channels towards the root has been selected as a predicted output channel. Notice that the SS predictor is equivalent to the 0th-order FCM predictor in uniform trafﬁc, because the lower input channels frequently forward incoming packets to the upper output channels. The prediction hit rate of an upper input channel is 1 q , and that of a lower input channel at a rank-r router is q−1 , since the trafﬁc is uniformly distributed to lower channels. Figure 11 shows the prediction hit rates of SS, FCM, LP, SPM, and Random(p) on fat trees (p, 4, r), where 2 ≤ r ≤ 4. One of case studies in the next section uses a fat tree to conﬁrm these results (see Section 5.3). 1 5 Evaluations In this section, we apply the prediction router architecture discussed in the previous sections to three case studies listed in Table 1. For each case study, we design a prediction router and compare it with an original router in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. 5.1 Case Study 1: 256-Core Fine-Grained Operand Network The ﬁrst case study assumes a ﬁne-grained operand mesh network that connects 256 ALUs with simple routers. 5.1.1 Router Architecture Original Router The dimension-order routing and wormhole switching with no virtual channel are employed as the low-cost routing and switching techniques for the ﬁne-grained operand mesh network. The router has a 4-ﬂit input buffer for each channel. It has three pipeline stages that consist of the RC, SA, and ST stages, as shown in Figure 4. The distance between neighboring cores is typically short in ﬁne-grained ALU networks; thus the link traversal (LT) is lumped into the ST stage in this case study. Prediction Router We selected SS as a simple and practical predictor for the dimension-ordered routers on mesh. All channels except a local input channel in the router employ the SS predictor, while the local channel uses the LP predictor since SS does not work well in the injection channel. The prediction router transfers a header ﬂit in a single cycle when the prediction hits. Otherwise, it takes at least three cycles depending on the packet conﬂicts. Dead ﬂits are removed safely inside the router by the kill mechanism. 5.1.2 Simulated Throughput First, we show the performance impact of the router delay in order to illustrate the advantages of low-latency routers. We used a cycle-accurate ﬂit-level network simulator to measure the throughputs of ﬁve networks, each of which employs the 4-cycle, 3-cycle, 2-cycle, 1-cycle routers, and the SS-based prediction router (Pred(SS)), respectively. The other simulation parameters are listed in Table 1. Figure 12(a) shows the simulation results. The Pred(SS) router network achieves 30.4% higher throughput compared with the 4-cycle network. The prediction router is regarded as a 1.4 cycle router when the prediction hit rate is 80%. In this case, its throughput is between the 1-cycle and 2-cycle. 5.1.3 Prediction Hit Rate We estimated the prediction hit rate of the SS-based prediction router by using the cycle-accurate network simulator in order to conﬁrm the accuracy of our formulas derived in Section 4. Figure 12(b) shows the simulation results. The curve of SS(sim) in the graph is very similar to that in the analysis results (Figure 9). For reference, we estimated the simulated hit rates of the FCM and LP predictors. We also conﬁrmed that their values are similar to those in Figure 9, though the FCM(sim) gives higher hit rates in small networks. 8 374       Table 1. Network speciﬁcation of three case studies Topology # of cores Core distance Trafﬁc Routing Switching Piplie structure Packet size Input buffer Predictor(s) Case study 1 16×16 cores 2-D mesh 0.75mm Uniform Case study 2 2-D mesh 8×8 cores 1.50mm 7 NPB programs + 4 synthetic patterns Dimension-order Dimension-order (deterministic) (deterministic) Wormhole; no VC Wormhole; 2 VCs [RC][SA][ST] [RC][VSA][ST][LT] 4-ﬂit (1-ﬂit=64-bit) 4-ﬂit (1-ﬂit=64-bit) 4-ﬂit FIFO 4-ﬂit FIFO SS SS + LP + FCM Case study 3 Fat tree (4,4,4) 256 cores 0.75mm Uniform up*/down* (adaptive) Wormhole; no VC [RC][SA][ST] 4-ﬂit (1-ﬂit=64-bit) 4-ﬂit FIFO SS(LRU) + LP 5.1.4 Zero Load Latency The zero load latency is the ﬂight time of a single packet from source to destination with no packet conﬂicts. Based on the prediction hit rates obtained previously, we compare the original router and the prediction router in terms of their zero load latency. Assuming a packet that consists of L ﬂits including a single header ﬂit goes through h wormhole routers, its zero load latency is calculated as follows. 0 = Tlt (h − 1) + (Trc + Tvsa + Tst)h + L/BW, (21) T orig where Trc , Tvsa , Tst , and Tlt are the latencies for the RC, VSA, ST, and LT stages, respectively. The prediction router can skip the RC and VSA stages only when the prediction hits; thus its zero load latency is calculated as follows. T pred 0 = Tlt (h − 1) + (Tpst )hPhit + (Trc + Tvsa + Tst )h(1 − Phit ) + L/BW, (22) where Tpst is the latency for the predictive switch traversal (PST), and Phit is the prediction hit rate. Figure 12(c) shows the zero load latencies of the original router (Orig), the SS-based prediction router (Pred(SS)), and that with perfect (or oracle) prediction (Ideal). The prediction router reduces 48.2% zero load latency compared with the original one, in the case of the 256-core mesh. 5.1.5 Hardware Amount To estimate the gate counts of the original and the SS-based prediction routers, we synthesized their RTL designs with a 65nm standard cell library by using the Synopsys Design Compiler version Y-2006.06-SP2. The behavior of the synthesized NoC designs was conﬁrmed through gate-level simulations at an operating frequency of 500MHz. Figure 12(d) shows the gate counts of the original router (Orig) and the prediction router (Pred(SS)). Although the prediction router requires the predictors, kill signals, and a modiﬁed arbiter, its area overhead is only 10.1% compared with the original one that requires at least three cycles to forward a header ﬂit. 5.1.6 Router Critical Path Figure 12(e) shows the maximum delay of each pipeline stage of the original and prediction routers. As shown, the critical paths of both routers are on their VSA stage, and their differences are quite small (i.e., 6.2%). Note that the prediction router forwards a ﬂit from its RC stage if the prediction hits (see line 4 of Algorithm 1). This is the reason why the delay of RC stage in the prediction router is much larger than that in the original one. 5.1.7 Energy Consumption The average energy consumption to transmit a single ﬂit from source to destination can be estimated as [20] Ef lit = wHave (Esw + Elink ), (23) where w is the ﬂit-width, Have is the average hop count, Esw is the average energy to switch the 1-bit data inside a router, and Elink is the 1-bit energy consumed in a link. Here we compare the prediction router with the original one in respect to Esw , since their Elink values are the same. We used the Synopsys Power Compiler in order to extract the Esw of these routers. They were synthesized, placed, and routed with the 65nm standard cell library. The switching activities of the running routers were captured through the gate-level simulations operating at 500MHz with a 1.2V core voltage. Figure 12(f) shows the ﬂit switching energy Esw [pJ/bit] extracted from the switching activities. Pred(hit) and Pred(miss) show the energy of the prediction router when the prediction hits and misses, respectively. Regardless of hit or miss, the prediction router requires slightly more energy due to the prediction. In addition, more energy is consumed when the prediction misses, since the dead ﬂits are removed and a copy of them is transferred to the correct output channel as well as the original router. Pred(hit) and Pred(miss) respectively require 6.4% and 13.6% more energy compared with the original value. Although the energy overhead of Pred(miss) is costly, approximately 80% of the predictions hit in a 16 × 16 mesh, as mentioned in Section 5.1.3. Thus, the expected Esw assuming that the 80% of the predictions hit (denoted as Pred(80)) is close to Pred(hit), 9 375  0  0.05  200  400  600  800  1000  1200  0.075  0.1  0.125 Accepted traffic [flit/cycle/core]   0.15 a L t y c n e [ e c y c l ] +30.4% 4-cycle 3-cycle 2-cycle Pred(SS) 1-cycle (a) Simulated throughput  0  20  40  60  80  100  4  6  8  10  12  14 Network size (k-ary 2-mesh)   16 P r c d e i i t n o h t i r a t e [ % ] 81.3% 43.7% SS (sim) FCM(sim) LP (sim) (b) Prediction hit rate  0  5  10  15  20  25  30  35  40  4  6  8  10  12  14 Network size (k-ary 2-mesh)   16 P e k c a t l a t y c n e [ s e c y c l ] -48.2% Orig Pred(SS) Ideal (c) Zero load latency  0  5  10  15  20 Pred(SS) Orig R u o t e r a r a e [ K l i o a g t s e ] +10.1% (d) Hardware amount  50  45  40  35  30  25  20  15  10  5  0 Pred(SS) Orig S t e g a y a e d l [ F O s 4 ] Critical path (+6.2%) RC stage VSA stage ST stage (e) Router critical path  0  0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  0.18 Pred(miss) Pred(hit) Pred(80) Orig F t i l s w t i g n h c i e n e r y g [ J p / b ] t i +8.02% +6.42% +13.62% (f) Flit switching energy Esw Figure 12. Evaluation results of case study 1 and its overhead is only 8.0%. Note since both routers consume the same Elink for each ﬂit, the total energy overhead for transmitting a ﬂit is less than 8.0%, depending on the link length. Throughout this case study, the evaluation results of the prediction router showed that although the area and energy are increased by 10.1% and 8.0% respectively, the communication latency is reduced by 48.2%; thus the prediction router presents favorable trade-offs between these modest overheads and the latency saving. 5.2 Case Study 2: 64-Core Chip Multiprocessor Network The second case study assumes a chip multiprocessor network that connects 64 processors with virtual channel routers. Multiple prediction algorithms are evaluated on the network with seven parallel applications taken from the NAS parallel benchmark (NPB) [1] programs, in addition to typical synthetic trafﬁc patterns, in order to clearly show their strong and weak points. 5.2.1 Router Architecture Original Router Dimension-order routing and wormhole switching with two virtual channels are employed as a routing and a switching technique, respectively. The router has a 4-ﬂit input buffer for each virtual channel. Since we use one cycle for the link traversal (LT), a header ﬂit is transferred to the next router or core in at least four cycles. Prediction Router We need multiple prediction algorithms, since various parallel applications are running on the network. Here we selected SS, LP, and FCM as simple and practical predictors. We implemented them on two prediction routers. Pred(SS+LP) supports SS and LP, while Pred(SS+LP+FCM) supports FCM in addition to SS and LP. They can dynamically change their prediction algorithm in a single cycle in response to a given trafﬁc pattern. They forward a header ﬂit in two cycles when the prediction hits. Otherwise, they use at least four cycles. 5.2.2 Prediction Hit Rate Figure 13(a) shows the prediction hit rates of SS, LP, and FCM algorithms on seven NPB programs (BT, SP, LU, CG, MG, EP, and IS) and four synthetic patterns (bitcomp, bitrev, transpose, and uniform [7]). The prediction hit rates for the synthetic trafﬁcs are better than those for the application trafﬁcs, since the destination distributions of synthetic ones are simpler than those of real applications. Although the SS predictor achieves more than 80% of hit rates in large networks (e.g., the 256-core mesh in case study 1), its prediction hit rates are not so high in this 64core mesh. Especially, its hit rate is extremely low (12.0%) in LU trafﬁc that contains a lot of 1-hop communications the SS never hits. In most applications, the LP and FCM predictors provide higher hit rates than the SS does. Particularly, LP achieves 89.8% of hit rate in SP trafﬁc that contains a lot of repeated short-distance communications. Each prediction algorithm has strengths and weaknesses. Our prediction routers that support multiple algorithms would be able to accelerate wider range of applications. 5.2.3 Hardware Amount The original router (Orig), the prediction router with SS and LP (Pred(SS+LP)), and the prediction router with SS, LP, and FCM (Pred(SS+LP+FCM)) were synthesized with the 65nm CMOS standard cell library in the same way as in 10 376                                 ] % [ e t a r t i h n o i t i c d e r P  100  80  60  40  20  0 57.7% 89.8% 12.0% 70.7% Hit(SS) [%] Hit(LP) [%] Hit(FCM)[%]  45  40 ] +6.40% +15.93%  35  30  25 s e t a g o l i K [ a e r a r e t u o R  20  15  10 BT SP LU CG MG EP IS Bitcomp Bitrev Transpose Uniform (a) Prediction hit rate  5  0 Orig Pred(SS+LP) Pred(SS+LP+FCM) (b) Hardware amount Figure 13. Evaluation results of case study 2 case study 1. As shown in Figure 13(b), the area overheads of Pred(SS+LP) and Pred(SS+LP+FCM) are 6.4% and 15.9%, respectively. The latter one needs more gate counts, since it supports the FCM predictor, which requires a history table (e.g., 4-bit binary counter) in order to record the number of references to each output channel. Though, it gives high and stable hit rates in most trafﬁc patterns, as shown in Figure 13(a). 5.2.4 Energy Consumption Here we estimate the ﬂit switching energy Esw [pJ/bit] of Orig and Pred(SS+LP+FCM) routers. As shown in Section 5.1.7, the energy overhead of prediction routers is depending on their prediction hit rate. In this case study, the maximum hit rate on each application is higher than 70% except MG and EP trafﬁcs (Figure 13(a)). Assuming that the 70% of the predictions hit, the expected energy overhead of the prediction router is estimated as 9.5%. Throughout this case study, we showed that the hit rates of the three prediction algorithms range from 12.0% to 89.8% on the seven NPB programs. This means that the prediction routers that support multiple algorithms can accelerate wider range of applications, though their area overheads range from 6.4% to 15.9%, depending on the prediction algorithms supported. We have illustrated these tradeoffs through this case study. 5.3 Case Study 3: 256-Core Adaptive Tree Network The third case study assumes a ﬁne-grained operand network that connects 256 ALUs as in case study 1, but its connection topology is a fat tree, which uses the adaptive up*/down* routing. 5.3.1 Router Architecture Original Router To connect 256 ALUs, we use fat tree (4,4,4), in which each router except rank-4 routers has four physical channels for upper and lower connections, respectively. For packets moving toward the root of the tree (i.e., upper transfers), each router adaptively forwards an incoming packet to one of four upper channels, while it selects a single lower channel in the case of lower packet transfers. The router has a 4-ﬂit input buffer for each input channel, and its pipeline structure is the same as in case study 1. Prediction Router Here we use the modiﬁed version of SS algorithm called LRU for lower channels. The LRUbased predictor speculatively forwards an incoming packet to the least recently used upper channel, in order to distribute the congestion over multiple upper channels. In the case of upper channels, on the other hand, it is difﬁcult to predict a single correct output channel among four candidates. We implemented two prediction routers. Pred(LRU) employs LRU for lower channels but no predictor for upper ones, while Pred(LRU+LP) employs LRU and LP for lower and upper channels respectively. Pred(LRU+LP) can reduce more latency by the aggressive prediction, although it incurs more energy overhead due to more dead ﬂits. 5.3.2 Prediction Hit Rate / Zero Load Latency By using the cycle-accurate network simulator, we estimated the prediction hit rates of the two prediction routers on 16-core, 64-core, and 256-core fat trees with uniform random trafﬁc. Then the same tendency can be seen in both the simulation results and the analysis results (Figure 11). Figure 14(a) shows the zero load latencies of the original router (Orig), Pred(LRU), Pred(LRU+LP), and that with perfect prediction (Ideal). The results show that Pred(LRU+LP) reduces 30.7% zero load latency compared with the original one, in the case of the 256-core fat tree. 5.3.3 Hardware Amount / Energy Consumption The original router (Orig) and Pred(LRU+LP) were synthesized with the 65nm standard cell library in the same way as in case study 1. Figure 14(b) shows that the area overhead of the prediction router is only 7.8%. As for the energy consumption, we estimated the ﬂit switching energy Esw of Pred(LRU+LP) router, as in Section 5.1.7. Assuming that the 55% of the predictions hit, the expected energy overhead of the prediction router is 9.0%. Throughout this case study, we conﬁrmed that our prediction router architecture can be applied to the adaptive up*/down* routing on fat trees, in addition to the deterministic routing on meshes and tori. 6 Conclusions We proposed the prediction router architecture, which predictively forwards packets without waiting the RC, VA, 11 377              25  20  15 ] l s e c y c [ y c n e t a l t  10 e k c a P  5  0 Orig Pred(LRU) Pred(LRU+LP) Ideal -30.7%  35 +7.81%  30  25 ] s e t a g o l i K [  20 a e r a  15 r e t u o  10 42 62 82 102 122 142 Network size (number of cores)  (a) Zero load latency 162 R  5  0 Orig Pred(LRU+LP) (b) Hardware amount Figure 14. Evaluation results of case study 3 and SA operations, based on the prediction algorithm selected from several candidates in response to the network environments, such as the network topology, routing algorithm, and trafﬁc pattern. Then, we presented the formulas that estimate the hit rates of ﬁve prediction algorithms on meshes, tori, and fat trees. In the three case studies, we implemented prediction routers by using a 65nm CMOS process, and evaluated them in terms of the prediction hit rate, zero load latency, hardware amount, and energy consumption. Case study 1 assumed a 256-core ﬁne-grained mesh network. We conﬁrmed the accuracy of the formulas by comparing the prediction hit rates derived from the formulas with the simulation results. The evaluation results of the SS-based prediction router showed that although the area and energy are increased by 10.1% and 8.0% respectively, the communication latency is reduced by 48.2% thanks to its high hit rate. Thus, the prediction router presents favorable trade-offs between these modest overheads and the latency saving. Case study 2 assumed a 64-core chip multiprocessor network. Multiple prediction algorithms were evaluated on the network with seven NPB programs. Their hit rates range from 12.0% to 89.8% depending on the trafﬁc pattern. Thus, the prediction routers that support multiple algorithms can accelerate wider range of applications. Though, their area overheads range from 6.4% to 15.9%, depending on the prediction algorithms each router supports. In case study 3, we also applied the prediction router to fat tree networks and showed its feasibility. Throughout these case studies, we showed that the prediction router architecture is versatile and applicable to various network environments. Possible improvement of this study is to investigate the adaptive predictor-selection mechanism that dynamically optimizes the predictor in response to the current trafﬁc pattern. We are designing some adaptive selection policies and will evaluate them as future work. "
2009,A low-radix and low-diameter 3D interconnection network design.,"Interconnection plays an important role in performance and power of CMP designs using deep sub-micron technology. The network-on-chip (NoCs) has been proposed as a scalable and high-bandwidth fabric for interconnect design. The advent of the 3D technology has provided further opportunity to reduce on-chip communication delay. However, the design of the 3D NoC topologies has important distinctions from 2D NoCs or off-chip interconnection networks. First, current 3D stacking technology allows only vertical inter-layer links. Hence, there cannot be direct connections between arbitrary nodes in different layers - the vertical connection topology are essentially fixed. Second, the 3D NoC is highly constrained by the complexity and power of routers and links. Hence, low-radix routers are preferred over high-radix routers for lower power and better heat dissipation. This implies long network latency due to high hop counts in network paths. In this paper, we design a low-diameter 3D network using low-radix routers. Our topology leverages long wires to connect remote intra-layer nodes. We take advantage of the start-of-the-art one-hop vertical communication design and utilize lateral long wires to shorten network paths. Effectively, we implement a small-to-medium sized clique network in different layers of a 3D chip. The resulting topology generates a diameter of 3-hop only network, using routers of the same radix as 3D mesh routers. The proposed network shows up to 29% of network latency reduction, up to 10% throughput improvement, and up to 24% energy reduction, when compared to a 3D mesh network.","A Low-Radix and Low-Diameter 3D Interconnection Network Design ∗ Yi Xu† , Yu Du‡ , Bo Zhao† , Xiuyi Zhou† , Youtao Zhang‡ , Jun Yang† † Dept. of Electrical and Computer Engineering ‡ Dept. of Computer Science †{yix13, boz6, xiz44, juy9}@pitt.edu, ‡{ﬁsherdu,zhangyt}@cs.pitt.edu University of Pittsburgh, Pittsburgh, PA 15621 Abstract Interconnection plays an important role in performance and power of CMP designs using deep sub-micron technology. The network-on-chip (NoCs) has been proposed as a scalable and high-bandwidth fabric for interconnect design. The advent of the 3D technology has provided further opportunity to reduce on-chip communication delay. However, the design of the 3D NoC topologies has important distinctions from 2D NoCs or off-chip interconnection networks. First, current 3D stacking technology allows only vertical inter-layer links. Hence, there cannot be direct connections between arbitrary nodes in different layers — the vertical connection topology are essentially ﬁxed. Second, the 3D NoC is highly constrained by the complexity and power of routers and links. Hence, low-radix routers are preferred over high-radix routers for lower power and better heat dissipation. This implies long network latency due to high hop counts in network paths. In this paper, we design a low-diameter 3D network using low-radix routers. Our topology leverages long wires to connect remote intra-layer nodes. We take advantage of the start-of-the-art one-hop vertical communication design and utilize lateral long wires to shorten network paths. Effectively, we implement a small-to-medium sized clique network in different layers of a 3D chip. The resulting topology generates a diameter of 3-hop only network, using routers of the same radix as 3D mesh routers. The proposed network shows up to 29% of network latency reduction, up to 10% throughput improvement, and up to 24% energy reduction, when compared to a 3D mesh network. 1. Introduction The technology driven integration of many cores into a single chip is facing critical challenges such as high power dissipation, resource management etc. In particular, the interconnection network starts to play a more and more important role in determining the performance and power of the entire chip [24]. In packet-switched on-chip network, ∗ This work is supported in part by NSF 0747242, 0641177, 0720595, 0734339 and Intel. which is the predominant network-on-chip (NoC) for tiled multicore processors, communication among cores at distance experiences long latencies because packets need to compete for resources on a hop-by-hop basis. To provide low latency and high bandwidth communication in NoCs, many researches have been carried to optimize the network in various approaches such as developing fast routers [19, 20, 23, 28] and designing new network topologies [9, 18, 30]. The emerging three-dimensional (3D) stacking technology has provided a new horizon for NoC designs. 3D stacking is a technology that stacks multiple active silicon layers on top of each other, and connect them through wafer bonding. It reduces interconnect delay via much shorter vertical wires between the dies. It is estimated that 3D architectures reduce wiring length by a factor of the square root of the number of layers used [16]. The main beneﬁts of 3D stacking over traditional 2D designs are higher performance and lower interconnect power due to reduced wire length. Such an advantage enables higher transistor packing density, which is particularly suitable for chip multiprocessor (CMP) designs, and has created unique opportunities and challenges to design low latency, low power and high bandwidth interconnection network in 3D. Lately, there has been an increasing interest in interconnection network designs for 3D stacked CMPs [21, 25, 31]. Since the major difference between 3D and 2D NoC is the presence of vertical links that connect different layers, existing researches have focused mainly on optimizing the vertical communication including choosing good vertical link architecture [25], designing more efﬁcient routers to reduce vertical hop count [21], and reducing power consumption of routers via a multi-layered 3D technology [31]. However, despite the previous efforts in minimizing the vertical communication time, the 3D NoC of a CMP still incurs long network latency mainly due to the the large network diameter, much like in a 2D NoC. The purpose of this paper is to develop a 3D NoC topology of low latency. This is achieved through a low-diameter network using long wires and low-overhead routers. There have been a myriad of research decades ago on off-chip interconnection network topology designs, especially in the parallel processing community. However, there are essen978-1-4244-2932-5/08/$25.00 ©2008 IEEE 30 Figure 1. A sample 4×4×5 3D chip architecture. Each node represents either a core or a cache memory bank. tial differences between on-chip interconnection networks and their off-chip counterparts. The distinctions between this paper and the past efforts mainly come from the physical constraints in 3D chips. Speciﬁcally, • Nodes in 3D interconnect represent either cores or cache banks which are typically tiled in 2D and stacked in 3D regularly, as depicted in Figure 1. The links between different layers, at current fabrication stage, are only vertical links connecting nodes directly atop or below. It is not feasible to directly connect nodes on different layers with an angle. This restriction eliminates a large portion of topologies containing angled links such as trees. In fact, the only freedom in placing links lies within each layer. Therefore, it appears that our problem can be reduced to using a low-diameter 2D topology, which has also been extensively studied in the past. However, this brings another distinction between this paper and the prior art. • A low diameter 2D topology such as the ﬂattened butterﬂy [18], or any other topology that can be ﬂattened, entails high-radix routers. For example, a fully connected 2D network (diameter=1) for each layer in Figure 1 requires radix-18 routers (15 level + 2 vertical + 1 local port). Using ﬂattened butterﬂy of diameter-2 requires radix-9 routers (without concentration). Lower radix routers, such as those in Express Virtual Channel [23], varietal tori or hypercubes, will continue to increase the diameter of the network. Hence, to keep the diameter in each layer very low, e.g., 1 or 2, one must incorporate high-radix routers. Unfortunately, high-radix routers accompanied with long wires imposes great concerns in their area and power overhead in both routers and wires, which are the ﬁrst-order constraints in a 3D stacked chip. Simply replicating those 2D designs in the layers of our 3D network would generate prohibitive area overhead and power surge. • Finally, even though we have freedom in placing links in each layer, long wires occupy 4× to 8× the width of short wires [8]. Therefore, we may not be able to place all desired long wires in one metal layer. This is a critical restriction in designing our topology. Therefore, we need to seek an alternative that does not compromise the network diameter. In this paper, we develop a methodology for designing a low-diameter 3D NoC using low-radix routers to achieve low network latency. Our design is suitable for a prevalent 3D CMP architecture where all cores are placed in the layer closest to the heat sink (for best heat dissipation), and the cache memories are stacked in the remaining layers [4, 17, 26]. Our topology adopts the one-hop router design in vertical vias [21], and replaces the level 2D mesh with a network of long links connecting nodes that are at least m mesh-hops away, where m is a design parameter. The mesh for the core layer is preserved for short distance communication less than m hops. In such a topology, communication that requires more than m horizontal hops will leverage the long physical wire and vertical links to reach destination, achieving low total hop count. Longrange links have been used on-chip for improving the performance of critical paths [29]. Long links have also been inserted into an application-speciﬁc 2D mesh to reduce its average packet hop count [30]. Although the main challenges in using long links are 1) they may limit the clock frequency of the network; and 2) they may consume higher power than shorter links, we demonstrate through our experiments that we still obtain positive gains. To use low-radix routers while incorporating long wires as many as possible, we leverage the great advantage of 3D stacking to distribute long wires onto different layers, reducing the radix pressure on each router. Intra-layer long distance communication may utilize the point-to-point long wire at a different layer via vertical hops. We develop a mechanism to automatically generate such a network topolwe can distribute a fully connected 4×4 2D network onto ogy using Integer Linear Programming. Using this method, 5 layers forming a diameter-3 3D network using routers with same degree as a 3D mesh router. That is, any pointto-point communication requires at most 3 hops. We also present methods to scale our design with different layer and core numbers. Our experimental results on synthetic trafﬁc, SPLASH2, OpenMP, and SpecJbb 2005 benchmark traces show up to 29% reduction in zero-load packet latency, reducing the network energy by up to 24%, as compared to the 3D mesh network. The remainder of this paper is organized as follows. Section 2 gives an illustrative example of our proposed topology and its advantages over a conventional 3D mesh. Section 3 discusses the details of our design methodology. Section 4 explains the wire models. Section 5 shows the experimental results. Section 6 explains the scalability of our proposed network. Section 7 discusses related works. Finally, Section 8 concludes this paper. 2. An Illustrative Example In this section, we highlight the advantages of our proposed topology over a conventional 3D mesh network using an illustrative example. First, we introduce the 3D CMP architecture that our network is developed upon. A 3D CMP can be built in a number of ways. The ﬁrst design is to place cores and caches in alternating locations, both horizontally and vertically forming a staggered layout [1, 25]. This design avoids direct contact between active cores and interleaves cool cache banks evenly with hot active cores. The second scheme redesigns the entire core and 31 Core layer Cache layers B E B B’ A D C A (a) 3D chip architecture (b) Routing path in a 3D mesh network (c) Routing path in our proposed network Figure 2. A motivating example. other logic into a 3D circuit to span them across all layers of the chip [4, 31, 32]. This design reduces the wire latency within logic to improve performance. The third design places cores in one layer, closest to the heat sink, and cache memories in all remaining layers [4, 17, 26], as illustrated in Figure 2(a). This design is the best in terms of heat dissipation and scalability in number of layers. Our network topology will be developed based on this layout, but the principle is equally applicable to other architectures. The mostly adopted interconnection network in 3D chip is a regular 3D mesh network [21, 25, 31], as shown in Figure 2 (b). Here we omit some layers and the vertical links for clarity. Each node represents a router associated with a core or a cache bank. In such a network, packets are typically routed using dimension-order routing (DOR) such as x-y-z. For example, when node A sends a packet to node B in Figure 2(b), the routing path will follow the arrows shown in the graph, traversing through 8 routers, or in 7 hops. Every hop involves certain delay in the router, creating long latencies between point-to-point communication. Previous contributions have optimized the vertical link and router architecture such that data transfer between any two layers can complete in a single hop [21, 25]. Therefore, to further reduce the network latency, optimizations must be carried in each layer. Our proposed topology aims to place long links between remote nodes in a layer. For example, in Figure 2(c), when node C sends a packet to B, it can directly go through the long link CB’, and then B’B, taking 2 hops altogether which is a signiﬁcant reduction from 7. We will call CB’ a 6hop link since C and B’ are 6 hops away in the original mesh with DOR. However, due to the wire area and node degree restrictions (a node cannot take too many long links), similar long links may not exist in all layers. For example, when node A communicates with B, the similar 6-hop link may not be available in A’s layer. Hence, A will borrow link CB’ via a vertical hop, leading to a 3-hop path to B. This is still shorter than the 7-hop path in a mesh. In an extreme case where an m-hop (m > 1) link cannot be found in any cache layers, we direct the request to the core layer using the mesh that is preserved for core-to-core communication. is easy to see that when m ≤ 2, the routing distance in the This is illustrated in Figure 2(c) for trafﬁc from D to E. It long-link topology is identical to that in a 3D mesh. 3 Design of the Proposed Topology Knowing the advantages of long links, we now elaborate the design of a network for a 3D architecture using long wires. First of all, the core layer has many intra-layer performance critical trafﬁc such as the cache coherence messages. We preserve the mesh for the core layer because the one hop trafﬁc are most efﬁciently handled here. Taking them down to the cache layers would triple their hop count and introduce more trafﬁc in the entire network. Our focus is to design a mechanism to connect the routers in the cache layers with long wires. Adding long wires in the cache layers reduces the non-uniformity of the shared cache accesses, which implies that we can avoid complex cache management techniques such as data migration since all cache banks are of approximately the same distance to the requesting core. Therefore, we only need to consider the inter-layer trafﬁc between the cores and the cache nodes. 3.1 The Rationale The ideal diameter of such a 3D network is 2, meaning that every pair of core-cache nodes are at most 2 hops away: 1 horizontal plus 1 vertical hop. This would require the intra-layer routing distance between every pair of nodes be 1, indicating a clique (fully connected) network per layer. This is clearly too expensive. Therefore, we relax the network diameter to 3, allowing one more hop between any pair of core-cache nodes to exist either horizontally or vertically. If this hop is horizontal, then the routing distance between any intra-layer pair of nodes is ≤ 2. This is still very expensive in total link count and router ports required per layer. For example, the 4×4 ﬂattened butterﬂy topology has a diameter 2, but requires routers of radix 9 which is quite high especially in a 3D chip. Hence, we are left with the choice of letting the extra hop be a vertical one. This implies that the only horizontal hop in a 3-hop path is still a long jump. However, it does not have to exist in every layer. As long as it is in some layer, we can always use a vertical hop to reach that layer, and then ﬁnish the route in 2 hops. An intuitive example was given in Figure 2(c) for the path between node A and B. This seems a feasible approach as it does not require a long link to exist in every layer. To achieve a true network diameter of 3, we need to ensure that every pair of core-cache nodes are within 3 hops away, i.e., 2 vertical hops and 1 horizontal hop at most. That is, for every node pair (i, j ) in a layer, there must be a link between them either in this layer, or in a different layer with end nodes that are vertically aligned with i and j . This means that we are implementing a clique, but the links are in different layers. Essentially, we are slicing a clique onto different layers with the top layer being a mesh, and each of the remaining layers being a subgraph with smaller node degrees and fewer links. Such a topology will be practical for implementation in 3D. Note that we may not need intralayer 2-hop links as their 3D path length is 3, which would not generate any hop reduction. Therefore, the subgraphs can be even thinner without the 2-hop links. However, we will leave this as an option, as having them in the subgraphs can help to distribute the network trafﬁc more evenly. The reason we are able to achieve a low-diameter and 32 low-radix topology lies in the great advantage of the vertical links, i.e., pillars that connect nodes on different dies. We are taking advantage of the single hop in vertical direction between any layers, but the connection among the vertical routers does not form a clique, leveraging the recent contributions in 3D router designs [21, 31]. Every router uses the same number of vertical ports as in a 3D mesh, but interlayer links can be connected through a “connection box” of only a few transistors (see Figure 5) that can be dynamically turned on and off in every cycle to connect and disconnect inter-layer links. This enables a single hop between any layers using only one link between adjacent layers. Such a technology leverage the short distance in the vertical direction of a 3D chip, so it is difﬁcult to implement for off-chip interconnection networks. 3.2 The Design Space Our topology design is subject to the radix of the routers (or the number of links) allowed in each layer. In this section, we examine the design space for embedding a clique into a 3D topology. Let L be the number of cache layers, N be the node count per layer, and R be the router’s port count for non-local horizontal links. Then every layer can host RN/2 links maximally, and the total number of horitotal number of links in an N -node clique is N (N − 1)/2. zontal links accumulating all cache layers is LRN/2. The Removing the 2(N − √ N ) mesh links, the total links we should distribute to the L layers are N (N − 1)/2 − 2(N − N ). Therefore, to accommodate all the links, we must have: √ LRN 2 ≥ N (N − 1) 2 LR ≥ N + √ − 2(N − − 5 4√ N N ) (1) (2) or Also, our topology should have the same number of links per layer as in a mesh network to keep the same network bandwidth. This gives: = 2(N − RN 2 N ) or R = 4 − 4√ N (3) Hence, 3 ≤ R ≤ 4, meaning that some routers use 4 ports and the rest use 3 ports. Combining equation (2) and (3), we can obtain a set of practical solutions to our design space, as listed in Table 1. The L column shows the number of layers N 16 25 36 LR ≥ R L ≥ 4 3 3.2 ≥ 7 3.3 ≥ 10 √ 12 21 32 Table 1. Solutions space for a 3D N -node clique. required to include all the necessary links. If the layer count is greater than the given number, links can be replicated in different layers, which could potentially ofﬂoad some trafﬁc from the single long link design with minimum layer count. The stacking depth is also limited by the current and future technology in 3D integration. The ITRS projected that by year 2011, the number of dies that can be stacked will reach 11 [37]. Hence, for N ≤ 36, the L’s fall within a practical region for implementation. If the layer count is smaller than the number given in Table 1, we can only incorporate a subset of the links, which will be discussed in the next section. When N > 36, the layer count or the ports per router required become too large. We will discuss the method for scaling the topology for larger networks in Section 6. 3.3 Subgraphing a Clique Using ILP The design space analysis shows that in theory, our topology can be developed given the router port and link count constraints. In practice, there is another important constraint that must be considered for on-chip interconnection network design. This is the area overhead and wiring complexity of those long links. If the space for global wires is limited, we may not be able to place enough long links in the cache layer, and would have to push the trafﬁc to the top mesh layer. This would reduce the attainable average hop reduction, and impose the concerns on imbalanced trafﬁc distribution. R0 R4 R8 R1 R5 R9 R2 R6 R3 R7 R10 R11 R12 R13 R14 R15 Figure 3. The Boolean variables for wire routing. Therefore, our goal is to select the wires that satisfy the port, link, and area constraints while achieving the largest reduction in hop count. This selection process can be carried systematically through Integer Linear Programming (ILP) which is a powerful method for maximizing (minimizing) certain objectives through determining a set of decision variables, subject to some constraints. We will now discuss these three main ILP components: variables, objective function, and constraints. 3.4 Decision variables Our decision variables are all boolean variables representing whether a wire connecting node i and j on layer k should be selected. However, we need to take into account how the wire will be routed on-chip as their layout will affect the area and wiring density around the routers. Since wires are laid out in horizontal and vertical directions only, we use two directional variables Xi,j,k and Yi,j,k to indicate whether the wire is routed ﬁrst in the x or y direction. For example, in Figure 3, the wire between node 0 and 15 on layer 1 is represented by X0,15,1 , indicating that the wire is ﬁrst routed in horizontal direction, and Y0,15,1 , indicating that it is ﬁrst routed vertically. As a wire can be only routed in one direction ﬁrst, we have Xi,j,k + Yi,j,k ≤ 1, 0 ≤ Xi,j,k , Yi,j,k , Xi,j,k = Xj,i,k , Yi,j,k = Yj,i,k (4) 3.5 Objective function Our objective is to maximize the latency reduction in the network. Different wires have different latencies. As 33 we will discuss in Section 4, we will pipeline long wires if higher network frequency is required. We take this into consideration for calculating the latency. Let us ﬁrst consider the hop count between a core node with coordinate (x, y , 0), and a cache node with coordinate (u, v , w). Let i be this cache node, and j be another cache node on the same layer with coordinate (x, y , w). As we can see, in a 3D mesh, the hop count between (x, y , 0) and (u, v , w), denoted as Hmesh (i, j ) is the Manhattan distance between i and j plus 1: In our long wire topology, the hop count between (x, y , 0) and (u, v , w) is 2 if the wire between i and j is in layer w , and 3 otherwise. We assume that the trafﬁc is uniform random since our study is general purpose. Thus, there are 1/L (there are L layers of cache) chances that the hop count is 2. Hence, the hop count between (x, y , 0) and (u, v , w) in our long wire topology is: Hmesh (i, j ) = M anhatten(i, j ) + 1 (5) Hlong (i, j ) = 2 × 1 L + 3 × L − 1 L = 3 − 1 L The latency for packet as [23]: L = D/v + L/b + H × Trouter + Tcontention , transmission can be expressed where D is average Manhattan distance, v is signal propagation velocity, L is packet size, b is channel bandwidth, H is hop count, Trouter is router delay, and Tcontention is the delay due to network contention. We assume uncontended network to formulate our latency calculation (i.e. Tcontention = 0). For example, the 6-hop long wire with pipelined design requires 3 cycles under 3GHz frequency (see Section 4). The value D is the total wire length of a path (1 level 6-hop wire and up to 2 vertical pillars). The vertical pillars are signiﬁcantly shorter than the long wire, and thus can be neglected for simplicity. Thus, D/v only depends on the clock frequency of the long wire, e.g. 1ns for the 6-hop long wire and 1 3 ns for a 1-hop mesh link under 3GHz. The value b is 1 ﬂit per cycle. The value H can be obtained from (5) for mesh and (6) for long link network. L and Trouter are both constants. The packet latency can be then computed by summing up the above values. Let i,j be the packet latency in mesh network and long-link network respectively. The objective function can be expressed as: (6) Lm i,j and Ll L−1(cid:3) (cid:2) M AX N −1(cid:3) N −1(cid:3) (Xi,j,k + Yi,j,k )(Lm i,j − Ll i,j ) (7) (cid:4) k=0 i=0,i(cid:3)=j j=0 3.6 Constraints We ﬁrst discuss the area constraints for wiring long links, and then summarize mathematically the rest constraints that we discussed earlier. Wiring Area. The number of wires that can be routed in a interconnection network is limited by the size of the router as well as the area taken by the wires. The wiring density for interconnection network is referred as the maximum number of tile-to-tile wires routable across a tile edge [15]. Therefore, given the size of a router, the number of global wires that can be routed through the router is ﬁxed. Studies show that global wires can consume 4 to 8 times the area (width+spacing) of short local wires [8]. The link in a regular mesh spanning one tile is a short wire. In our topology, √ N − 1 hops. The 2 and we have wires spanning from 2 to 3-hop wires are considered as short and medium wires, and rest are considered as long wires. Let Wi,j denote the area taken by the wire from node i to j , Amax be the wiring density in unit of the area for one short wire. Let s denote a one hop segment between two neighboring routers, and S denote the union of them. Then the area constraint for wiring can be expressed as: Wi,j ×(Xi,j,k+Yi,j,k ) ≤ Amax , ∀k , 0 < k < L, ∀s ∈ S all wires(cid:3) passing thru. s (8) The sum can be expanded by examining the routing paths of all wires. The constant Wi,j and Amax are determined as follows. If the Manhattan distance between i and j is larger than 3, Wi,j is 4× the area of a short wire. Otherwise it is 1×. For Amax , we deﬁne that the total pitch (width+spacing) of wires in one hop does not exceed the edge of a 3D mesh router. Note that the routers in our topology are no larger than that of a 3D mesh router because of the port constraint we deﬁned. Therefore, our area overhead does not create pressure in routing the introduced long wires. In our experimental setting, the wire bandwidth is 130 bits (data + control). Every such bundle is doubled to support bidirectional communication. According to the ITRS prediction in 2006 [37], the semi-global wire width is 180nm. Previous studies on 3D mesh routers Hence, we have 130×2×x×180nm≤√ of 5 horizontal ports show that its area is ∼0.37mm2 [25]. 0.37mm, where x indicates how much area, in multiple of the semi-global wire width, is allowable per 3D mesh router’s edge. Therefore, Amax = x ≤ 12. That is, we can arrange up to 12× the area of a semi-global wire per hop. This can accommodate, for example, 3 long wires, or 2 long wires plus 4 short wires etc., depending on the global gain calculated from our objective function deﬁned in equation (7). Router Port. We have discussed earlier that we use only low-radix routers in our network. Hence, we deﬁne that the number of ports per router should not exceed the maximal ports per mesh router, denoted as Pmax , which is typically 7 (4 intra-layer, 1 local, and 2 vertical) unless more pillars are used. Hence, (Xi,j,k +Yi,j,k ) ≤ Pmax , ∀i, 0 ≤ i < N , ∀k , 0 ≤ k < L 0≤j<N(cid:3) j (cid:3)=i Total Links. We have discuss in equation (3) that we keep which is 2(N − √ the total number of links in the same amount as in a mesh, N ), to provide the same network bandwidth. This can be deﬁned mathematically as: (9) (X (i, j, k) + Y (i, j, k)) ≤ 2(N − N ) (10) √ N −1(cid:3) N −1(cid:3) i=0 j (cid:3)=i 3.7 Summary and ILP efﬁciency Putting everything together, we have deﬁned the boolean variables of our ILP problem as X (i, j, k) and Y (i, j, k) to indicate the wires and their routing direction. The results of these variables are determined by evaluating the objective 34 function speciﬁed in equation (7) subject to the constraints deﬁned in equation (4), (8), (9), and (10). The constraints and the objective functions were formulated using the front-end AMPL language [10]. The system was solved using the state-of-the-art ILP solver lpsolve [3]. Due to the presence of large number of decision variables and complex constraints, directly solving the entire system of equations requires several days to get the ﬁnal results on a 2.4GHz dual-core Intel Xeon workstation. To improve the efﬁciency, we divided the decision variables into two groups – one for short wires, and the other for long wires. Since long wires are more effective in reducing hop counts, they are given higher priority. Therefore we ﬁrst use ILP solver to map long wires. Considering the wire area constraints, the number of long wires between any singlehop node pair is no more than 3. Other constraints on link and port count remain unchanged. After obtaining the routing of long wires, we then apply the solver again to ﬁnd the routing for short wires. With this two-phase optimization, we can generate result within 1 minute for a 4 × 4 × (4 or 5) topology. Note that even though our network size, objective function, variables and constraints are all ﬁxed, changing the ILP solving procedure will result in different topologies. Our experimental results show that the difference in the produced topologies before and after the optimization does not have impact on the average network latency. 3.8 A sample topology generated We now present a sample 3D topology generated using ILP. A 4 × 4 × 5 (16 cores and 4 cache layers) 3D chip’s network was solved. The resulting topologies for all cache layers are shown in Figure 4. In the ﬁgures, the bold lines stand for long wires that are thicker than short wires. The constraints we used here are in line with the discussion in Section 3.2. Apart from the vertical ports and local port, each router is restricted to a maximum of 4 intra-layer ports. For each 16-node layer, the total number of wires allowed is 24, which is equal to that of a mesh. For a 16 (nodes)×4(layer) network, there are 96 links that are equal to or longer than 2 hops. The ILP solver was able to place all those links into the network under the port, link and area constraints. As we can see from the resulting topologies, the wire densities in the center of the topology tend to be higher than those along the edges. However, all segments between neighboring routers are limited by Amax (12). We will show in our experiments later that our total network energy is less than that of a mesh because of the hop count we saved. 3.9 Routing Algorithm We choose the deterministic routing algorithm in our topology as it produces the minimal hop count. Once the topology is generated, our routing algorithm is also determined: when a core with coordinate (x, y , 0) generates a request to a cache bank (u, v , w) on the wth layer, the router of the core checks whether the long link between (x, y ) and (u, v ) exists in any layer l. If so, the routing path is computed as (x, y , 0)→(x, y , l)→(u, v , l) →(u, v , w) with 3 hops. Note that if l = w , the route is completed in 2 hops. When the data bank responds to the core’s request, the same path in a reversed order is used. If the long link between (x, y ) and (u, v ) does not exist. The request will follow the DOR algorithm such as XYZ or YXZ. The cache-to-core trafﬁc will also follow the same path, but in a reversed order. Also, in addition to deterministic routing algorithm, we used 3 VC per port to avoid deadlocks in our network. Our current deterministic routing has generated encouraging improvement in network latency (see Section 5). We will use an adaptive routing in the future to balance the trafﬁc load and further improve the performance. 3.10 Routing Table We use routing tables to implement our routing algorithm. Each router with coordinate (x, y , z ) has a lookup long link from router (x, y , ∗) to any router (u, v , ∗), where table that contains the information for the location of the ∗ represents any layer, and u, v represent coordinates other than x, y . This is because a packet arriving at a router will ﬁrst search for the long link that reaches the destination. If it is in the current layer, the packet will be routed across. Otherwise, the packet will be ﬁrst brought to the layer that has the long link, and then routed across. Therefore, the routing tables for a column of routers are identical. For our 4×4×4 network, there are 15 entries in each table. Each entry of long link from (x, y , ∗) to (u, v , ∗): layer ID and output port the table (in a router (x, y , z )) contains the location of the ID which require 2 bits for each. Hence, the routing table size for the 4×4×4 network is 4 × 15 = 60 bits. 3.11 Discussion Although our design results in different interconnections in different layers, the maximum radix of routers of all layers and the total number of links in every layer are the same as in a mesh. The only difference among layers is the wiring. Fortunately, our proposed ILP methodology can help to automate the process of generating the topology and wire layout. Also, additional constraints can be added to satisfy manufacture and technology requirements. When the topology is modiﬁed, the only change to the network is the routing table contents. The routing algorithm remains the same. 4 Modeling Long Wires In section 3.6, we discussed the handling of global wire area. In this section, we focus on its delay and energy consumption. Global wires are placed in the top metal layers, e.g. metal 7 or metal 8 layer. They are thicker and wider than local wires that are placed near the device layer. Because of their size, global wires take longer time and more energy to carry signals than local (short) wires. If our single-hop long link takes only one clock cycle to complete, the delay on the longest link in our topology will have an impact on the clock frequency of the network. For this reason, we will model the long wires with two different clock frequencies: one that can allow the longest link to transmit signals in one clock cycle, and another that requires pipelined wire designs for higher clock frequencies. Their impact on the overall network latency and energy consumption will be shown in the experimental section. 35 (a) ﬁrst cache layer (b) second cache layer (c) third cache layer Figure 4. A 3D topology for a 4 × 5 × 4 3D chip (1 core layer and 3 cache layers) generated using ILP. (d)forth cache layer For all types of wires (long or short, pipelined or none), we use HSPICE with 45nm BSIM4 technology model from PTM [35]. The supply voltage Vdd is 0.8V and the operating temperature is assumed to be 70◦C. Single-cycle long wires. There has been an incessant effort in minimizing the delay of global wires, especially with the shrinkage of process dimension. Successful researches have been carried to lower the resistance of electrical wires and provide high speed (near velocity-of-light) and high bandwidth [5, 7, 14] global wires. New technologies such as optical [22] and radio frequency [6] signaling have also been proposed for on-chip communication. These innovations can be leveraged for designs utilizing global wires [18, 29, 30]. However, we will use the conventional global wires with repeaters to illustrate the applicability of our design even in the current technology. We modeled the delay for both long and short wires. For long wires, we obtain a delay-optimized design through carefully inserting the wire repeaters. We ﬁrst refer to ITRS 2007 to obtain the latest dimension of wires, e.g. 1.5mm per hop, which are then used to calculate the wire parasitics (capacitance and resistance) in the PTM interconnection model [36]. Using these parameters, we calculate distances between the inverters and the size of them for a delay-optimized wire of certain length. Since there should be even number of inverters on every wire, we round the delay-optimized repeater number up to the nearest even integer. Then we shrink the size of each inverter to reduce its power without affecting the delay. To model the wire power, we used the bus coupling model in [13]. To simulate the cross coupling effect between wires, we connected every adjacent wire pair with coupling capacitor using the PTM interconnect model [36]. We simulated the wires under the worst case scenario where every wire has an inverse voltage level change with its two neighbors. The resulting delay and energy for wires of 1-6 hops are listed in the “sng” columns of Table 2. Note that the 1hop wire is simply the short links in a mesh network. The slowest long wire is the 6-hop wire with a delay of 957ps. This is sufﬁcient to sustain a 1GHz network such that every link requires only 1 clock cycle to transmit a signal. We consider this frequency reasonable because 3D chips have high constraint in heat dissipation and high clock frequency may not be preferred. Furthermore, the power and energy of n-hop long wires are less than n times the power and energy for a 1-hop wire. Our later results will show that using single-cycle long wires will achieve energy reductions in routers and wires, in addition to latency reduction. layeri layeri ctrl ctrl ctrl layeri+1 Original connection box layeri+1 Revised connection box Figure 5. Connection box transistor logic. Pipelined long wires. The single-cycle long wires can sustain a network frequency of no more than 1GHz. To obtain higher clock frequencies, the long wires must be segmented and pipelined to accommodate smaller cycle time. We implemented pipelined long wires that can sustain a 3GHz network. The long wires are divided into N segments by N − 1 ﬂip-ﬂops (FF). For example, the N for the longest 6-hop wire is 3. Note that we still need repeaters to drive the wire load. The repeater distance and sizes are designed in the same way as in single-cycle long wires. However, the FFs are more expensive in both power and area than the repeaters. Therefore, we decreased their size for lower power. This shrinkage will positively impact the wire delay. Hence, we increased the repeater size to offset the delay increase while making sure we still save power. The results for repeaters, FF, delay and power/energy are given in the “ppl” columns of Table 2. As we can see, the total energy for transmitting one bit has increased noticeably for all long wires. We will show later that such increase does have an impact on the overall network energy for some workloads. Modeling pillars. For vertical pillars, we followed the scheme proposed in [21], paying special attention to the connection box between adjacent wire segments. In our setting, the nMOS-only design with an inter-layer wire length of 50μm shows a decrease in output when passing logic 1. After several stacked nMOS stages, the output reduces to nearly the Vth (0.3V in our 45nm transistor model), which is difﬁcult to be recognized by the next stage logic. Therefore, we revised the connection box with a pair of nMOS and pMOS —transmission gate, as illustrated in Figure 5. Simulation results show that this design solves the voltage drop problem. However, using transmission gate requires the complementary control signal, which increases the control wire numbers. We report the results based on this revised model. We obtained the energy for transmitting one bit on a pillar that goes through 2 layers (50μm), 3 and 4 layers are 111, 211, and 293fJ respectively. 36 -hop repeaters ﬂip ﬂops delay(ps) clock cycles Dynamic P (mW) Static P (μW) Total E (pJ) link ppl sng ppl sng ppl sng ppl sng ppl sng ppl sng ppl sng 6 9 7 2 0 958 957 3 1 4.91 1.09 11.87 5.37 1.641 1.094 5 7 5 1 0 644 951 2 1 6.51 0.94 9.68 2.91 2.182 0.945 4 6 4 1 0 636 959 2 1 4.32 0.82 8.58 1.71 1.449 0.823 3 4 3 1 0 629 826 2 1 2.24 0.35 3.56 0.91 0.750 0.349 2 3 2 0 0 318 505 1 1 0.75 0.32 3.23 0.56 0.256 0.324 1 1 1 0 0 221 221 1 1 0.70 0.70 2.89 2.89 0.238 0.238 Table 2. Modeling results of single-cycle and pipelined long wires. “ppl” stands for “pipelined”. “sng” stands transmitting one bit on one wire. The total E is calculated as (dynamic + static power)×clock cycles×cycle for “single-cycle”. “P”, “E” stands for power and energy respectively. The power and energy results are for time. 5 Performance Evaluation In this section, we present simulation-based performance evaluation of our proposed 3D topologies, and the 3D mesh with the state-of-the-art router designs developed in previous researches [21]. 5.1 Simulation Infrastructure To model and compare different network designs, we extended a cycle-accurate 2D NoC simulator Noxim [41] developed in SystemC into a 3D network. The simulator models all major components of the NoC: routers, wires, and pillars down to the level of details such as a signal or a switch. We also augmented the energy model in the original Noxim to characterize the 3D behavior. The technology and energy parameters were obtained from Orion [34] for routers. The power model for wires and vertical pillars were from Section 4. Other essential parameters used in our simulator are listed in Table 3. We used both synthetic and real workload traces to test different networks. For the 3D mesh, we evaluated DOR routing algorithms. The routing for our topology is deterministic as explained earlier. The synthetic trafﬁc uses 1 ﬂit for request messages and 5 ﬂits for data messages. We tested Uniform Random trafﬁc (each node uniformly injects packets into the network with random destinations) and HotSpot trafﬁc (different processors generate requests to the same region of cache banks with high probability). The real workload trafﬁc traces include SPLASH-2 [40], OpenMP [38] and Specjbb 2005 [39]. They are gathered from the full-system simulator Simics [27] conﬁgured into a multicore processor with large shared last level cache to mimic our 3D chip. Each workload was simulated for 50M instructions per core. This generated > 500K packets per workload. The detailed processor conﬁgurations are listed in Table 4. The router microarchitecture has typical components as in a state-of-the-art NoC router. We have described the connection box in building vertical pillars. The intra-layer components are input buffers, a VC allocator, a routing unit, a switch allocator and a crossbar. Each router has 5 intralayer ports, and each port of the router has 3 VCs. The buffer depth of each VC is 5 ﬂits, the size of a packet. Packets of different message types are assigned to corresponding VCs to avoid message deadlock. The arbitration scheme of switch allocator is round-robin. We use determined routing 5.2 Network Latency Reduction algorithm to avoid routing deadlock, since packet could access the horizontal destination in one-hop with long wires. Both the mesh and long-link network use the same router architecture except for their port numbers. Figure 6 plots the average ﬂit latencies for a 4 × 4 × 4 3D chip (3 cache layers) using two different trafﬁc. The curves labeled with “long” and “long pipeline” are the results from our topology with single-cycle long wires (1GHz) and pipelined long wires (3GHz) respectively. The rest curves are results for a 3D mesh using different routing algorithms: ZXY, ZXY-XYZ, and XYZ. The ZXY-XYZ means that the communication initiated by the core is ﬁrst routed down, and then across the destination layer. Among the three algorithms, our experiments show that XYZ outperforms the other two most of the time. Therefore, we only show the XYZ and pipelined long wire results for the 3GHz network. The results show that both long-link based topologies outperform the 3D mesh in terms of latency and throughput (for HotSpot trafﬁc) under almost all injection rates. For the HotSpot trafﬁc, the cores may generate high volume of trafﬁc to the same region of cache banks (4 in our setting) in one layer. As we can see, the no-load latency of the 3D mesh using XYZ routing is improved by 25.7% and 20.2% for single-cycle and pipelined long link topology respectively. Also, the network saturation point of our topology is 10.8% later than the XYZ-mesh, resulting a noticeable throughput increase under our topology. The reason for the improvements is that the long-link network distributes congested intra-layer hotspot trafﬁc onto different layers, because the long links to the hotspot are not placed in the same layer. This effectively balances the trafﬁc congestion in the entire network, proving the advantages of our design. For the Uniform Random trafﬁc, the zero-load latency sees a 25.9% and 20.4% improvement for 1GHz and 3GHz respectively. However, the network saturation point is 3.5% earlier than the 3D mesh. This is mainly due to the imbalanced trafﬁc distribution in the topology under high uniform injection rates. Recall that the network topology of our modeled 3D chip was shown in Figure 4. The 3-layer cache design cannot accommodate all the links in a clique excluding the 1-hop links. Therefore, for those missing links, all those trafﬁc are routed to the top core layer, creating contention when the trafﬁc injection rate is high. In fact, the 4 clique has 120 links in total. They can be subgraphed into 5 37 Characteristic Technology Vdd Network size Routing Router delay virtual channels/port ﬂit size number of pillars frequency simulation warmup(cycles) Analyzed packets Parameters 45nm 0.8v 4×4×(4 or 5) DOR / Determined Routing 2 cycles 3 128 bits 2∼4 per node 1GHz 20,000 100,000 Table 3. Baseline network conﬁguration. Number of Processors Issue Width ISA L1 conf. 16 4 SPARC 32K-I/D, 4 way, 64B/line, 2 cycles 512KB/bank, 48 banks, 16 way, 64B/line, 10 cycles Cache coherence protocol MESI SCMP Memory DDR2-800, 55ns (220 cycles) Processor frequency 4GHz L2 size Table 4. Architecture parameters for trace generation. (a) 1GHz (b) 3Ghz Figure 6. Average network latency for a 4×4×4 network with 1GHz and 3GHz frequencies. (c) 1GHz (d) 3GHz layers, each having 24 links. The 3 cache layer design discarded 24 links, and distributed the rest (24×4) links onto 4 layers including the mesh layer. Hence the mesh layer is loaded with double the trafﬁc of any cache layer because it absorbs both one-hop and the trafﬁc that cannot be routed in the cache layers. If we increase the the cache layer number to 4, and allow the 4th layer to also use 24 links, then the trafﬁc will be perfectly distributed onto 5 layers, resulting a more balanced design. The latency results using 4 cache layers are plotted in Figure 7. The results show noticeable improvement in both network latency and throughput. The no-load latency for the Uniform Random trafﬁc is improved by 29.6% (1GHz) and 23.9% (3GHz) for long wire designs. The latencies for HotSpot trafﬁc are also increased by 29.5% and 23.9% for 1GHz and 3GHz respectively. The saturation point of both long wire designs are 3.5% (Uniform Random) and 10% (HotSpot) later than the XYZ routing in mesh, indicating a throughput improvement as well. At this time, the latency of long wires has 86% improvement over the mesh. Also the latency gap between the two topologies does not narrow as quickly as the 3-layer design. We will use the 4-layer cache design to present the subsequent results. Impact of pillar number. It has been noted by previous researches that the vertical communication in 3D chip is critical to the network performance [21]. This is also the case in our design, as we use vertical hops to reduce the complexity of routers. Previous works have shown that ThroughSilicon-Vias (TSV) have pitches of 4∼10μm [12, 21]. With this dimension, the area consumed by a bundle of 161 wires (128 bit data + control signals) is around 0.01mm2 . With this area, the state-of-the-art routers can have more than 30 [25] to 100 [11] pillars. Hence, increasing the vertical pillar seems feasible and will likely not be a limiting factor for several generations [21, 26]. Figure 8 shows the latency 120 100 ) l s e c y c ( y c n e t a L e g a r e v A 2 3 4 5 6 80 60 40 20 0 0.01 0.06 0.11 0.16 0.21 Injection rate 0.26 0.31 0.36 Figure 8. Latency reduction with number of pillars. reduction as we increase the pillar count from 2 to 6, with increasing ﬂit injection rate. As we can see, although the pillar density is not a concern, increasing the pillars gives diminishing returns. From 2 pillars to 3 pillars, the latency improvements are clearly observed; but beyond 4, the gains are negligible. Therefore, we choose 4 as our pillar count for each router. 5.3 Energy Reduction We have introduced our wire energy and delay model in Section 4. The router energy is modeled in Orion [34]. We then measured the energy reduction of our topology compared to the 3D mesh using synthetic trafﬁc of the same number of packets, at an injection rate close to their network saturation points to fully activate all components of the networks. Figure 9 shows the measured energy reductions in 38     (a) 1GHz Figure 7. Latency improvement for a 4×4×5 network of 1GHz and 3GHz. (b) 3GHz (c) 1GHz (d) 3GHz router, wires and their sum for the single-cycle long wire design. As we can see, the router energy reduction is from 4655% mainly because the signiﬁcant hop count reductions achieved in our topology. The wires also have some energy reduction, ranging from 14-15%. This is because: 1) We applied a leakage reduction technique, described in section 5.4, for links that are idle. We observed more leakage reduction than dynamic energy reduction from the mesh. 2) The energy of n-hop wires is less than n times the energy of 1-hop wires, for single-cycle long wires. Hence, it pays off to use long wires to reduce hop count. The total energy reduction range from 46-55% because the dominant energy is dissipated in routers. Figure 9. Energy reduction compared with 3D mesh using XYZ routing. Results are collected near the network saturation point. 5.4 Results for Real Workload Traces Figure 10 shows the average network latencies and energy consumption of the real workload traces we collected for the 4-layer cache conﬁguration. The single-cycle and pipelined wire results are normalized to the 3D mesh using XYZ routing with 1GHz and 3GHz clock frequency respectively. We can see that no matter which wire frequency is used, the long link based topology consistently reduces the latency across all tested benchmarks. They show a fairly steady reduction amount: 8.8%∼24.2% (single-cycle) and 6.7%∼19.9% (pipelined long wire), with the largest seen in benchmark raytrace and water-spatial (SPLASH-2), and average of 20.5% (single-cycle) and 15.9% reduction (pipelined long wire). This is mainly because these real workload traces present near Uniform Random characteristic with relatively low ﬂit injection rate. The energy reductions from these benchmarks however results are 1.6%∼16.8% and -1.7%∼9.6% for single-cycle are not as signiﬁcant as those with synthetic trafﬁc. The and pipelined long wires respectively. The pipelined wire design, unsurprisingly, increased the total energy for fft and volrend. Other benchmarks show a mild reduction. We observed that the leakage energy becomes dominant in the total energy consumption because the ﬂit injection rates of these benchmarks are much lower than the network saturation point. Hence, many components are idle, consuming only leakage energy. We developed a leakage reduction method that can turn off the long links while they are idle. We turn off the all the links of a router when all its internal buffers are empty. We turn the links on whenever there is a ﬂit enters the router. This is conservative, but simple to implement. A more aggressive router with techniques such as pre-routing can further lower the leakage by accurately turning on and of the long links. On average, the singlecycle and pipelined long wires achieve 8.5% and 4% energy reduction over a baseline 3D mesh network. 5.5 Hardware Overhead Comparison In this section, we compare the hardware savings in routers of our topology, and the overhead introduced by long wires. The area of a router, denoted as SR , can be modeled as SR=H1 n+H2n2 . Here H1 and H2 are area coefﬁcients related to dimensions of a channel, input and output buffers, crossbar etc. [2]. n is the radix of a router. Since both our topology and the 3D mesh use the same ports for vertical links, we will only count the In a 4×4×4 3D mesh, each lateral ports for clarity. routers. Hence, SR (mesh)=3×(4×5+4×3+8×4)H1+3× layer has 4 5-port routers, 4 3-port routers, and 8 4-port (4×52+4×32+8×42 )H2=192H1+792H2 . In our 3-cache layer long link based topology, every router has 4 ports except for the last layer, 2 routers there have only 3 ports due to the area constraints. We can come up with a similar equation and get SR (long)=190H1+754H2 . As we In the 4×4×5 network, can see, SR (long)¡SR (mesh). SR (mesh) = 256H1 +1056H2 , and SR (long) = 256H1 + 1032H2 . As we can see SR (long) < SR (mesh) still holds. Hence, the router area in our long-link based topology is less than that of a 3D mesh network. The overhead we pay in this design is the longer and wider wires. Here we quantify such an increase. In a 4×4×4 3D mesh, the total number of links is Swtotal (mesh)=24×3=72Sw without the top mesh layer since it is the same for both networks. The Swtotal denotes the total wire area, and Sw denotes the area of a 1-hop link. With our long wire design, adding the hop counts of all our wires and considering the 4× the area for 3-6 hop links, we get Swtotal (long)=640Sw . same method for the 4×4×5 network, we can obtain Using the Swtotal (long)=7.2Swtotal (mesh). We remark that alHence, Swtotal (long)=8.9Swtotal (mesh). 39 though the area of the long wires is 7∼9× of the mesh topology, global wires are placed on top metal layers and do not take the space of the device layer. Hence, such an area “increase” is not a burden to the chip real estate. Also, as we have deﬁned the routing and area constraints in developing the topology, the increased wire area will be unlikely to impact the layout of the rest of the chip. 6 Scalability Our proposed topology using long links can be scaled design is workable for core count up to 36 (6 × 6), with curin different ways. We have shown in Section 3.2 that our rent and near future 3D stacking technology. Beyond this number, we have to increase either the radix of the router or stacking depth in order to ﬁt a clique into all layers in 3D. One method of scaling the topology is to use the Concentrated mesh [2] (CMesh) as our baseline for improvement. The CMesh network reduces a regular mesh to a radix-4 mesh in which each router services four processors. For example, a 64-node mesh can be concentrated into a 16-node CMesh with the same topology. Although a single CMesh’s router is larger compared to a single router of a regular mesh, it is not four times larger. Hence, the total router area of a CMesh is smaller than that of a regular mesh. Concentration can also reduce the hop count of the network. It was studied that concentration improves both area efﬁciency and energy efﬁciency of a network [2]. Using concentration, our design on a 4×4×L network (L is number of layers) can be expanded to an 8×8×L network, with each router servicing 4 cores or cache banks. Similarly, a 6×6×L can be expanded to 12×12×L, which is already on the high-end of multicore designs. Naturally, when a wire is overly long, we can apply the pipelined design as described in Section 4. Such a technique has also been investigated recently in topologies using long wires [18]. Another way of scaling is to relax the diameter further. node network (N≤36), where a node can be either a regUp till now we have been focusing on a clique for an N ular mesh router or a concentrated router. We can apply the same principle to subgraphing a 2-hop diameter network such as the ﬂattened butterﬂy [18] onto different layers with still low-radix routers. For example, for a 7×7 network, the radix of each router is 6×2=12 (not counting the local port). Then the total number of links in the ﬂattened butterﬂy network is 49×12/2=294. If we use a radix-4 router in our 3D topology, each layer can host 4×49/2=98 links. Then we only need to stack 294/98=3 layers of cache to incorporate all links. Similarly, subgraphing the ﬂattened butterﬂy of a 10×10-node network requires only 5 layers of 3D stacking Figure 10. Latency and energy reduction for SPLASH2, OpenMP, and SpecJbb 2005 workloads. with radix-4 routers (not counting the local port). Note that we can continue to scale using concentration here. 7 Related Work There have been many researches lately on improving the network performance. They can be broadly categorized into topology optimization and router enhancement. For 2D network topology, the “Small-World” design [30] inserts extra long links into a 2D mesh to exploit the application-speciﬁc bandwidth requirement. Our design is general purpose and the trafﬁc was assumed to be rather uniform across the entire network. Also, inserting links into a mesh increases the complexity of the router. The ﬂattened butterﬂy topology uses high-radix routers with concentration to achieve a diameter of 2 NoC for a 64-node network. Again, one of our goals is to use low-radix routers in a 3D network to meet the power and area constraint. Express cubes [9], using physical express channels to connect distant nodes, loses performance when a number of neighboring nodes compete for the same express channel. Also it takes more than one hop for data to traverse from local router to interchange units, which means that the diameter of express cubes is larger than our proposed topology. The Express Virtual Channel (EVC) [23] design for a planar mesh uses a novel ﬂow control mechanism and router microarchitecture renovation to allow packets to virtually bypass intermediate routers along their path. However, the diameter of the 2D network is strictly larger than 1. Linear programming has also been used in custom 2D NoC designs for SoCs where the bandwidth requirement are known ahead of time, but the number of routers and the power consumption of the network need to be minimized, subject to LP solving time [33]. We are targeting a more general purpose network rather than an application speciﬁc custom network. A 3D dimensionally-decomposed router was designed to provide a good tradeoff between circuit complexity and performance beneﬁts. With this design, the communication between any two layers requires only a single hop, and nonoverlapping vertical communication can carry in parallel – a signiﬁcant improvement over the bus architecture [25]. We are already using the properties of this router in our topology design. 8 Conclusion We presented a new network topology using long-range links for 3D CMPs. The new topology has a low diameter, but requires only low-radix routers to implement. We modeled the long-range links and showed that they have la40 tency advantages even when we increase the network frequency and pipeline the wires. Also, their power/energy increase is sub-linear to their increase in length. We experimented a 4×4×4 and a 4×4×5 3D network. Our experiments show that a 1GHz network frequency is more suitable for 3D chips because it achieves more latency and energy reductions. A higher clock frequency for a 3D chip brings the concern of high heat dissipation, and therefore is not recommended for implementation (though its latency and energy results are still positive). Our topology can be scaled to larger networks using techniques such as network concentration. "
2010,FlexiShare - Channel sharing for an energy-efficient nanophotonic crossbar.,"On-chip network is becoming critical to the scalability of future many-core architectures. Recently, nanophotonics has been proposed for on-chip networks because of its low latency and high bandwidth. However, nanophotonics has relatively high static power consumption, which can lead to inefficient architectures. In this work, we propose FlexiShare - a nanophotonic crossbar architecture that minimizes static power consumption by fully sharing a reduced number of channels across the network. To enable efficient global sharing, we decouple the allocation of the channels and the buffers, and introduce novel photonic token-stream mechanism for channel arbitration and credit distribution The flexibility of FlexiShare introduces additional router complexity and electrical power consumption. However, with the reduced number of optical channels, the overall power consumption is reduced without loss in performance. Our evaluation shows that the proposed token-stream arbitration applied to a conventional crossbar design improves network throughput by 5.5× under permutation traffic. In addition, FlexiShare achieves similar performance as a token-stream arbitrated conventional crossbar using only half the amount of channels under balanced, distributed traffic. With the extracted trace traffic from MineBench and SPLASH-2, FlexiShare can further reduce the amount of channels by up to 87.5%, while still providing better performance - resulting in up to 72% reduction in power consumption compared to the best alternative.","FlexiShare: Channel Sharing for an Energy-Efﬁcient Nanophotonic Crossbar Yan Pan, John Kim† , Gokhan Memik Northwestern University 2145 Sheridan Road, Evanston, IL {panyan,g-memik}@northwestern.edu †KAIST Daejeon, Korea jjk12@kaist.edu Abstract On-chip network is becoming critical to the scalability of future many-core architectures. Recently, nanophotonics has been proposed for on-chip networks because of its low latency and high bandwidth. However, nanophotonics has relatively high static power consumption, which can lead to inefﬁcient architectures. In this work, we propose FlexiShare – a nanophotonic crossbar architecture that minimizes static power consumption by fully sharing a reduced number of channels across the network. To enable efﬁcient global sharing, we decouple the allocation of the channels and the buffers, and introduce novel photonic token-stream mechanism for channel arbitration and credit distribution. The ﬂexibility of FlexiShare introduces additional router complexity and electrical power consumption. However, with the reduced number of optical channels, the overall power consumption is reduced without loss in performance. Our evaluation shows that the proposed token-stream arbitration applied to a conventional crossbar design improves network throughput by 5.5× under permutation trafﬁc. In addition, FlexiShare achieves similar performance as a token-stream arbitrated conventional crossbar using only half the amount of channels under balanced, distributed trafﬁc. With the extracted trace trafﬁc from MineBench and SPLASH-2, FlexiShare can further reduce the amount of channels by up to 87.5%, while still providing better performance – resulting in up to 72% reduction in power consumption compared to the best alternative. 1. Introduction With the prospect of many core processors [12, 6, 22] on the horizon, the energy efﬁciency of on-chip networks is becoming increasingly important. As on-chip network size continues to increase, high bandwidth and low latency are required to achieve high performance. Hence, architects have recently explored nanophotonics [23, 14, 13, 18] as an alternative to conventional electrical signaling. However, nanophotonics pose many new challenges [11]. For example, unlike conventional electrical signaling, static power consumption constitutes a major portion of the total power [5]. The laser that carries the signal faces various losses along the path and the total loss can be signiﬁcant; and the energy conversion efﬁciency of laser sources is around 30% [5], further aggravating the total power consumption. In addition, the ring resonators used in nanophotonics require thermal tuning, incurring signiﬁcant heating power. Such static power consumption makes nanophotonic channels an expensive onchip resource. Thus, future nanophotonic on-chip networks need to fully utilize their provisioned channels and avoid overprovisioning channel bandwidth. Since nanophotonics is appropriate for global signaling in on-chip networks without requiring repeaters, they are suitable for implementing global crossbars [14, 23, 18]. However, in conventional nanophotonic crossbar designs, channels are dedicated to each node in the network – thus, as network size increases, the number of channels required increase proportionally. When global bandwidth is not heavily utilized, such bandwidth provisioning can be excessive. In Section 2, we will present Network-on-Chip (NoC) traces showing that some nodes are inactive for extended periods of time during the execution. To address these problems, we propose the FlexiShare nanophotonic crossbar architecture, where global channels are detached from the routers and shared globally among all the routers in the network – implementing global concentration. To support efﬁcient channel utilization with limited number of channels, we propose photonic token-stream arbitration and provide multiple tokens to increase network throughput. The token-stream concept is also extended to buffer management: token streams are used to manage the shared buffers and hence the allocation of the channels and buffers are decoupled. FlexiShare introduces additional complexity to support the ﬂexibility. However, through evaluation, we show how the number of channels and their associated static power consumption can be signiﬁcantly reduced while maintaining similar performance as conventional designs – thus achieveing higher overall power efﬁciency. We compare FlexiShare against alternative architectures using both evenly distributed synthetic trafﬁc patterns and traces from SPLASH-2 [25] and MineBench [17] benchmarks. In summary, the contributions of this paper include: • A ﬂexible crossbar topology that allows channel provisioning according to average trafﬁc load and create global concentration that exploits unbalanced bandwidth requirements. • A distributed token stream arbitration which provides multiple tokens for a given channel and enables high channel utilization. • Two-pass token-stream arbitration which provides lower bound on network fairness. • A global buffer manangement scheme that decouples buffer resources from channels. 987-1-4244-5659-8/09/$25.00 ©2009 IEEE We also analyze the total network requests from each nodes in a 64-node CMP running 9 benchmarks from SPLASH-2 and MineBench, as shown in Figure 2. For some benchmarks, there is a small set of nodes that generate a large portion of the total trafﬁc. Thus, if we could share a smaller number of channels across all the nodes, instead of dedicating a set of channels to each node, the channel utilization and the overall efﬁciency of the network can be improved. Electrical   Signal Photo Detector Filters Laser  Source Modulated Laser of  different wavelengths R ing  Heating Electrical   Signal Waveguide Resonant  Modulators Figure 3. Nanophotonic Devices 2.2. Power Consumption of Nanophotonics Silicon nanophotonics is an emerging technology that promises low latency and high bandwidth density. The basic components of nanophotonic signaling are shown in Figure 3. An off-chip laser source generates laser beam with multiple wavelengths, which is coupled to the on-chip waveguide. The resonant ring modulators modulates a speciﬁc wavelength with the electrical signal. At the receiver end, ring resonators ﬁlters out that speciﬁc wavelength and photodetectors convert the optical signal back into electrical signal. A common building block here is ring resonator, which needs to be thermally tuned to align its resonant wavelength. Figure 1. Network request rate for 64-core CMP running radix (SPLASH-2). ϬйϮϬйϰϬйϲϬйϴϬйϭϬϬй ŬŵĞĂŶƐ ůƵ ƌĂĚŝǆ ƐĐĂůƉĂƌĐ ǁĂƚĞƌ Figure 2. Load distribution across 64 nodes for the selected benchmarks. Different shades indicate different nodes. ĂƉƌŝŽƌ ŝ ďĂƌŶĞƐ ĐŚŽůĞƐŬǇ ŚŽƉ The remainder of this paper is organized as follows. In Section 2, we present experiment results motivating the design of FlexiShare. Section 3 describes the details of the FlexiShare architecture and the token stream arbitration schemes for channels and buffers. We evaluate the performance and power efﬁciency of FlexiShare and compare it against alternative crossbar designs in Section 4. After reviewing the related work in Section 5, we conclude the paper in Section 6. 2. Motivation In FlexiShare, we advocate reducing the number of of nanophotonic channels and share them globally among all the nodes. This is based on two observations: (a) unbalanced trafﬁc load in NoC architectures and (b) dominant static power consumption in nanophotonic channels. 2.1. Unbalanced Traﬃc Although different synthetic trafﬁc patterns are often used in evaluation of on-chip networks, they typically assume equal trafﬁc is injected from each node in the network, which is not necessarily representative of network trafﬁc in CMPs. Figure 1 shows the average network request rate of each core when running radix (SPLASH-2 kernel) in a 64-core CMP under Simics/GEMS simulation environment assuming a pointto-point network topology. The horizontal axis is time, segmented in 400K-cycle frames. The trafﬁc load is shown for all the 64 nodes in parallel. It can be seen that while some hot nodes (like node 0 and 1) have high loads, a large number of nodes have low bandwidth requirement, creating opportunity for bandwidth sharing among the nodes. Figure 4. Energy breakdown in a conventional radix-32 nanophotonic crossbar For conventional electrical network designs, the buffers and switches dominate the total power consumption [24] and with the abundant on-chip wire resources available, there is little motivation to provide sharing of the channels. In addition, much of the power consumption is dynamic power, which is proportional to the activity of the channels. However, for nanophotonic on-chip networks, the power consumption scenario is different. The laser power and the ring tuning power, which are activity-independent, dominate the total power consumption. For example, prior work Corona [23] estimated a static power of 26W for the optical crossbar, regardless of the trafﬁc in the application. The nanophotonic Clos network [13] adopts point-to-point nanophotonic links and based on their power model, a 33W laser power budget is assumed, while the in . . . R0 in . . . R1 CH0 CH1 …... out R0 . . . out R1 . . . in R0 . . . in R1 . . . CH0 CH1 …... out R0 . . . out R1 . . . in R0 . . . in R1 . . . in Rk-1 . . . CHK -1 out Rk-1 . . . in Rk-1 . . . CHK-1 out Rk-1 . . . in Rk-1 . . . CH0 CH1 …... CHM-1 out R0 . . . out R1 . . . out Rk-1 . . . (a) SWMR (b) MWSR (c) FlexiShare  Figure 5. Radix-k nanophotonic crossbar implementations routers and electrical local links together consume less than 8W for a high bandwidth design. Based on the power model in Section 4.7, we analyzed the energy consumption breakdown in a conventional radix-32 single-write-multiple read crossbar [18], as shown in Figure 4. It shows how static power dominates the overall power consumption. With such high static power cost in using nanophotonic links, it is important to improve the utilization of the channels and try to reduce the number of channels without losing performance. Through ﬂexible channel sharing, we show how this is achieved in FlexiShare in the following sections. 3. FlexiShare Architecture In this section, we describe the FlexiShare nanophotonic crossbar architecture which includes a novel token-stream based arbitration scheme, a distributed credit-based ﬂow control scheme, and the router microarchitecture description. 3.1. Overview : Shared Channel Design i High-level architecture diagrams of two conventional nanophotonic crossbar implementations are shown in Figure 5, in comparison with FlexiShare. Concentration is assumed as each router is connected to multiple nodes. To illustrate the logical organization of the nanophotonic data channels (CHi ), we separate each router (Ri ) into an input router (Rin i ) or the sending router and an output router (Rout ) or the receiving router. However, they would be physically implemented as a single router. In the single-write-multiple-read (SWMR) design, each router has a dedicated sending channel, and all the routers read on the other routers’ channels to implement the logical crossbar at the receivers’ end. This only requires local arbitration at the receiver side and has been proposed by Kirman et al. [14] and in Fireﬂy [18]. Multiple-write-single-read (MWSR) crossbars, on the other hand, provision each router with a dedicated receiving channel, and all the routers transmit data on the receivers’ channels, forming a crossbar at the senders’ end. This design requires global arbitration [23, 18] to resolve channel contention. For both designs of conventional nanophotonic crossbar, the number of channels required in the design is proportional to the radix (k) of the crossbar to provide full connectivity. If the network trafﬁc is unbalanced (e.g., only a few nodes are actively exchanging data) or if the network bandwidth usage is low, not all of these channels will be fully utilized. In FlexiShare, we propose to detach the channel resources from the routers and provision the channel bandwidth independently, as shown in Figure 5(c). In this design, on the sender side, the architecture is similar to MWSR, where each sender can transmit its packets on any available channel, forming a logical crossbar. On the receiver side, it is similar to SWMR, where all the receivers are equipped to read from all of the channels, forming a second logical crossbar. To support the two back-to-back crossbars, FlexiShare requires additional router complexity and approximately twice the amount of optical hardware (ring resonators) as SWMR or MWSR if the number of channels in the crossbars is held constant. However, the added ﬂexibility allows FlexiShare to be provisioned with any number of channels (M ), independent of the crossbar radix (k). Such ﬂexibility improves utilization of the nanophotonic data channels and hence results in higher power efﬁciency, especially under unbalanced trafﬁc load. R0 R1 R2 R3 D owns tream U ps tream R7 R6 R5 R4 R0 R1 R2 R3 R7 R6 R5 R4 (a) (b) Figure 6. Nanophotonic data channel designs: (a) two-round and (b) singl-round data channels 3.2. Nanophotonic Data Channel Design There are two ways to implement a nanophotonic data channel [13]. The waveguide layout of both are shown in Figure 6, with the path of data from R5 to R1 highlighted. In Figure 6(a), a two-round data channel employs a single set of wavelengths to traverse each node twice – allowing senders to modulate in the ﬁrst round and the receivers to detect and demodulate in the second round. An alternative is the single-round data channel, as shown in Figure 6(b), which uses two sets of wavelengths in opposite directions. Here, we call the direction of increasign router number as “downstream”, and the opposite direction as “upstream”. Even though single-round data channel requires extra wavelengths (and waveguides), it reduces the waveguide length and minimizes optical loss, and hence is adopted in FlexiShare. In the following sections, we call the data channel in one direction as a sub-channel and our discussion only focuses D0 0 1 2 3 Cycle 4 5 R0 R1 R2 R3 T0 T0 T1 T1 T2 T4 T2 T4 T1 T2 T3 T4 T1 T2 T3 T4 T0 T0 T3 T3 R 0 R 1 R 2 R 3 … . . .Token  Inject ion 0 1 2 3 4 Cycle 5 R0 R1 R2 R3 T0 T1 T2 T4 T4 T2 T3 T4 T2 T1 T3 T4 6 T0 T3 T3 D1 D2 D4 D0 D3 D2 D4 D0 D3 D2 D1 D4 D3 D2 D4 D3 R0 T0 T0 D1 T2 T1 R 0 R 1 R 2 R 3 … … . . . Token  Inject ion . . .Data  Slots 1 T 0 T D 1 D 0 1 T 0 T (b) (c ) 0 1 2 3 Cycle 4 5 R0 R1 R2 R3 T T T T R 0 R 1 R 2 R 3 (a) T T T T T T T T T T1 D0 D1 ... ... ... ... ... ... ... ... R 0 R 0 R 1 R 1 R 1 R 0 Figure 7. Token based arbitration schemes on a 4-node network. (a) Token-ring scheme; (b) Token-stream scheme; (c) Token-stream scheme applied to data channel arbitration, with requests from R0 and R1 in Cycle 0, R2 in Cycle 1 and R1 in Cycle 2. on the downstream sub-channel if not otherwise indicated. 3.3. Token Stream Channel Arbitration One challenge faced by FlexiShare crossbar is to make sure the data of different senders are not overlapped on a shared channel – i.e., to resolve contention for the shared channel resources properly while maintaining high channel utilization. Such arbitration is needed in the ﬁrst logical crossbar which is identical to the global arbitration needed in an MWSR crossbar. Prior work [18, 23] on MWSR crossbars have used token-based global arbitration, which leverages the low latency of nanophotonics.1 A single photonic token, which represents the right to transmit data on a channel, circulates around the network and passes all nodes. An example is shown on the left of Figure 7(a), where a token circulates around 4 routers in the clockwise direction. If a node wants to transmit data on a data channel, it has to ﬁrst grab the corresponding token by coupling the energy of the token off the waveguide. However, with such token-ring arbitration, the round-trip latency of the token can limit the throughput of the network and result in poor channel utilization. For the previous example of Figure 7(a), if we assume a token round-trip latency of 2 cycles, the resulting timing diagram of the token circulation is shown on the right in Figure 7 (a). Each node can only grab the token every other cycle – thus, resulting in a maximum of 50% throughput on trafﬁc patterns such as permutation trafﬁc. In general, if the token round-trip latency is r cycles, the network throughput can be limited to 1/r on adversarial trafﬁc patterns. 3.3.1 Single-Pass Token-Stream Arbitration In FlexiShare, we overcome this throughput bottleneck by introducing a novel token-stream arbitration scheme, as shown in Figure 7(b). Similar to token-ring arbitration, photonic tokens are used to arbitrate the channels. However, multiple tokens are continuously injected to create a “stream” of tokens that traverses all the nodes, in parallel to its corresponding data channel. The low latency of nanophotonics allows a token to traverse multiple nodes in a single cycle (two nodes in the example in Figure 7(b)), allowing, e.g., either R0 or R1 to grab T 0 in cycle 0. Each cycle, a new token is injected at the origin 1We will refer to this arbitration as token-ring arbitration since the token traverses the waveguide in a circular, ring structure. of the stream, and a token, if not grabbed by any router, will be eliminated at the end of the stream. As opposed to the token-ring arbitration, a token in the token stream represents the right to modulate on the corresponding data channel in the next data slot. For example, in Figure 7(c), the timing of the tokens (T 0, T 1, · · ·) are shown together with their respective data slots (D0, D1, · · ·). The superscript of a data slot indicates the router that has modulated on that slot. Tokens that are already grabbed by an upstream router are greyed out and ﬁlled with white color, to illustrate that other routers cannot grab these tokens. In this example, R0 and R1 both request a token in cycle 0, but only R0 will be able to couple the energy of token T 0 off the waveguide as it is upstream to R1 . R0 will occupy data slot D0 in the next cycle (cycle 1), and the modulated data in D0 will arrive at its destination R2 in cycle 2 to be detected and demodulated. R1 does not get a valid T 0 and will retry in cycle 1 for T 1, which will subsequently allow it to occupy D1 and transmit to R3 . As there is a one-to-one correspondence between the tokens and the data slots, data channel conﬂicts are resolved (e.g., R1 and R2 will never overwrite R0 ’s data in slot D0), and a daisy-chain-like priority scheme is implemented. This token stream arbitration scheme leverages the passive writing nature of nanophotonics, where the modulation is done by imprinting message onto a laser beam rather than driving a whole channel. Thus, the key for arbitration is not to avoid two writers on a same channel at any moment, but rather to avoid the overwriting on the same slot by two senders. By arbitrating on a ﬁne-grain data slot basis, the channel utilization can be signiﬁcantly improved, compared to the token-ring scheme. However, the token-stream arbitration also has its limitations. For example, it is not possible to hold on to a channel and send multiple ﬂits in sequence, as compared to token-ring arbitration where a node can delay the re-injection of the token to occupy the channel for more than 1 cycle. However, given the high bandwidth density of nanophotonics, the channels in an on-chip nanophotonic crossbar are often wide-enough such that a large packet (e.g., a cache line) can ﬁt in a single ﬂit – thus, interleaving ﬂits in the FlexiShare architecture is not a serious problem. Another drawback of this single-pass token stream arbitration is its daisy-chain-like ﬁxed priority – upstream routers C1 C2 C4 C3 C0 C1 C2 C4 C3 C0 C1 C2 C4 C3 C0 0 1 2 3 4 Cycle 5 R0 R1 R2 R3 T0 T1 1 T T2 T4 T4 T1 T3 T4 T1 T2 T3 T4 6 T0 T3 T3 T0 T0 T2 T2 R 0 R 1 R 2 R 3 … . . . Token  Inject ion T1 T2 T4 T3 T0 T1 T2 T4 T3 T0 T1 T2 T4 T3 0 1 2 3 4 Cycle 5 R0 R1 R2 R3 T0 T2 T4 T4 T1 T3 T4 T1 T2 T3 T4 6 T0 T3 T0 T0 T2 T2 R 0 R 1 R 2 R 3 … . . . Token  Inject ion D1 D2 D4 D0 D2 D4 D0 D3 7 D2 D4 D0 D1 D2 D4 D0 D3 T1 T2 T4 D3 R0 T3 T0 T2 D1R1 T4 T3 T0 T1 T2 D1 R1 T4 D3R0 T3 T0 T1 T2 T4 T3 T0 7 8 T1 T1 T3 T1 0 T 1 T 0 T D 1 D 0 … . . . Data  Slots R 1 R0 T1 0 1 2 3 4 5 Cycle R0 R1 R2 R3 C 1 6 R 0 R 1 R 2 R 3 … . . . Credit Distribut ion 7 C 0 C1 C3 C0 C1 C2 C3 C0 C3 C1 C3 C0 C1 C0 C2 C3 C0 C1 C3 C0 C1 C2 C3 C1 C2 C0 C1 C3 8 9 D 0 (a) (b) (c) T0 T0 T1 T2 T3 T4 1st pass 2nd pass 1st pass 2n d pass ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... C4 C2 C4 C2 C3 C4 C4 C4 C4 C1 C2 C0 C2 C2 C0 C4 C4 C4 Credit recollection Sending credits Figure 8. (a) Two-pass token stream arbitration scheme; (b) Two-pass token stream arbitration scheme with requests from R0 and R1 in Cycle 3; (c) Two-pass credit distribution scheme where R1 is distributing credits to the other routers. always have advantage over downstream routers. In the example shown in Figure 7(c), if R0 continuously have requests for a data channel, it will always obtain the token and occupy the data slot while downstream routers will be starved. To overcome this limitation, we propose a two-pass token stream arbitration in the next subsection. 3.3.2 Two-Pass Token Stream Arbitration In two-pass token stream arbitration, the token stream runs past each router twice2 , as shown by the two solid lines on the left of Figure 8(a). In the ﬁrst pass, each token is dedicated to a different router. During the second pass, all the routers are free to grab any token if the router does not have a dedicated token on the ﬁrst pass for that cycle. For example, in Figure 8(a), only routers R0 , R1 and R2 transmit on the downstream data sub-channel. In the ﬁrst pass T 0 is dedicated to R0 , T 1 to R1 , T 2 to R2 , and T 3 back to R0 again. R3 does not need any token for this downstream subchannel as all the trafﬁc it sends will be on the upstream channel. In the timing diagram, tokens that cannot be grabbed by a router because of lack of priority are greyed out. In general, for a radix-k crossbar, token T ((k − 1) i + j ), (i = 0, 1, · · ·) is dedicated to router Rj in the ﬁrst pass. Such dedication is implemented by a state-machine at each node – thus each token stream is still 1-bit wide. When a token comes back to R0 in the second pass, any router can grab it, just like in a singlepass token stream. An example is shown in Figure 8(b), where data slots D0, D1 · · · are shown together with the two pass token stream T 0, T 1, · · ·. In cycle 2, R1 can request for T 0 in the second pass even though T0 was dedicated to R0 in the ﬁrst pass. However, R2 is not allowed to grab T0 in cycle 3, because in that cycle, R2 has a dedicated token T 2 in the ﬁrst pass and it has to use the dedicated token for its data transfer. Upon grabbing a token, the router is granted the right to modulate on the associated data slot, which comes after the token passes the node the second time. Note that the data channels still only run a single round across each node. The two-pass token stream arbitration scheme puts a lower bound on the fairness of the sharing of a single data channel 2 The wrap-around of the stream (dashed lines in Figure 8) can be easily implemented by routing the waveguide as shown in Figure 12. as each node are guaranteed (in the ﬁrst pass) its own share of the channel while unused dedicated data slots can be recycled in the second pass – combining the beneﬁts of both dedicated time slot scheme and daisy-chain priority scheme. 3.4. Receiver-side Arbitration In FlexiShare, once a channel is granted by the token stream arbitration, the router will essentially act like a SWMR router to send the data packet to its destination through the temporarily dedicated channel for the sender. We adopt the reservationassisted scheme in Fireﬂy[18], where the sender will activate its receiver through reservation channels before sending over the data on the data sub-channel. The overhead of latency and power of the reservation channels are fully modeled in our evaluation. 3.5. Credit Stream Flow Control Buffered ﬂow control is employed in FlexiShare and proper buffer management is required to ensure packets are not dropped. Conventional credit-based ﬂow control maintains a credit count at the sender side for the available downstream buffers. Thus, each credit (and the corresponding buffer space) is implicitly associated with a channel – i.e., obtaining a credit also implies a particular channel to be used for transmission. However, in FlexiShare, because of the global sharing of the channels, channels are detached from the routers and hence the downstream buffers. The conventional credit-based ﬂow control could still be used but would be expensive because of the O(k2 ) broadcasting of the credits as well as properly maintaining the credits across all the routers. Hence, we treat the input buffers as a resource shared globally among all the sending routers, just like the channels, and we extend the token-stream mechanism to manage the credits. The scheme is shown in Figure 8(c). Unlike conventional credit-based ﬂow control where sending routers maintain credits for receiving buffers, each router keeps its own credit count for the number of available input buffers. As long as there is enough buffer, the router (e.g., R1 in the ﬁgure) will send out optical credit tokens (C 0, C 1,. . . ) in a credit stream that passes all the other routers twice. Similar to the two-pass token stream described previously, the two passes are needed to provide fairness – credits in the ﬁrst pass are dedicated to the different in U P D N CHi CHi Router R i UP CH0 … … UP CHi - 1 CHi+1 DN … … D N CHK- 1  out (a) Router R i in U P Router R i CHi CHi D N in UP U P CH0  … … U P CHM -1  DN CH0  D N CHM- 1  … … DN CH0 … DN CHi-1 CH i+1 U P … U P CHK-1  out … … … … (b) CH0  …… U P CHM -1  D N CH0  … D N CHM- 1  . . . … … … out Shared  Buffer (c) Figure 9. Microarchitecture of router Ri in a radix-k nanophotonic crossbar with concentration C = 1 using (a) MWSR, (b) SWMR, and (c) FlexiShare with M channels routers, and any remaining credits ﬂowing into the second pass can be used by any router. Different from the token stream for channels, the credit streams have to originate from each router. This requires the laser to traverse slightly longer distance to cover all the node twice after it ﬁrst passes the credit distributor. When the the tokens come back to its distributor (R1 ) after the two passes, they will be re-collected by the distributor to increment its credit count since the credit (or the buffer space) was not utilized. Another difference from token stream is that a single stream is enough for each router and it can run in either upstream or downstream direction, whichever yields the shorter waveguide length. For example, in Figure 8(c), the two passes are indicated by different colors of the solid lines on the left. R1 injects credits C 0, C 1 and C 2 but stops afterwards because no more buffer is available. C 0 is dedicated to R2 but is grabbed in the second pass by R3 , in cycle 3; while R0 grabs its dedicated C 2 in its ﬁrst pass in cycle 4. C 1 is not grabbed by any router, and is recollected by R1 , in cycle 5. 3.6. Router Microarchitecture With the token stream channel arbitration and credit stream ﬂow control, each packet in FlexiShare experiences several pipeline stages. Upon entering the sending router, it generates a credit request for its destination router’s input buffer. When the credit is acquired, the packet generates a channel request. Based on the relative location of the sender and receiver, this could be a request for either the upstream or downstream data channel. Any channel in the proper direction is equally feasible. Upon grabbing a token for the data channel, the packet will be distributed to the modulators of the data channel, a reservation will be sent on the corresponding reservation channel to activate the reciever detectors, and the packet will be modulated onto the assigned data slot. On the receiver end, the packets will be stored in a shared buffer before being sent to the ejection ports of the output router which are connected to the network terminal nodes. With a shared buffer, a single credit count is used to represent the availability of the buffer, as described earlier in Section 3.5. The shared buffer is implemented by using a switch organization similar to a Load Balanced Birkoff-von Neumann Switch [7] (Figure 9(c)), where the ﬁrst switch load-balances the 2(M − 1) incoming channels onto 2(M − 1) intermediate queues (the shared buffer) and a second switch3 connects these queues to the ejection ports. The load-balancing across the intermediate queues enable the use of a single credit count as the queue lengths are balanced. Switch speedup [9] in the ﬁrst switch here can be implemented to reduce the number of intermediate queues as needed but we do not assume any switch speedup. The comparison of alternative router microarchitectures is shown in Figure 9, assuming single-round data channels with an upstream and a downstream sub-channel. For simplicity, a concentration of C = 1 is shown. Note that in MWSR implementation (Figure 9(a)), the direction of the data channel (either upstream or downstream) is decided by the relative location of the sender and receiver, and the receiver has to buffer the packets from both directions of the data channel. The opposite situation applies to SWMR, where the direction of the receiving sub-channels are decided by the relative location. Because of this restriction of channel usage, each MWSR or SWMR router has access (read or write) to only half the number of data channels. However, for FlexiShare, full access to all of the data channels are allowed, as shown in Figure 9(c). This corresponds to the fact that FlexiShare can match the performance of MWSR or SWMR with half the amount of channels, as discussed in the evaluation section. 3.7. Realistic Token-Stream Latency For simplicity, the timing diagrams shown in previous sections ignored various latencies for signal conversion, token traversal, switch traversal, arbitration logic, etc. In reality, a more detailed timing diagram for a single-pass token stream would be as shown in Figure 10. In this example, there is a skew for each token to arrive at each router, based on the router location. R0 indicates its request for a token at the beginning of cycle 0, and will only get the grant at the beginning of cycle 2 due to latency in signal conversion. And it takes another cycle for R0 to send the data packet to the appropriate modulators to send in cycle 3 on data slot D0. However, these skews of latencies are constant for each router and does not impact the arbitration mechanism. The evaluation in the next section accounts for all these realistic latencies. 3 In Figure 9(c), this switch is reduced to a single (M − 1)-to-1 multiplexer because concentration C = 1 is assumed. 0 1 2 3 4 5 Cycle R0 R1 R2 R3 T0 T1 T1 T2 T4 T2 T4 T3 T4 R 0 R1 Req R 1R4  Req R0 Grant R1 R4 Grant 6 7 8 D1 D2 D3 9 10 11 R4 R5 R6 R7 T0 T0 T3 D0 D4 D4 Token  Stream Data  Channel R0 R1 R2 R3 R4 R5 R6 R7 Data Channel Token Stream T0 T1 T2 . . . D0 D1 D2 . . . Traff ic Example: Cycle 0: R0  Cycle 3: R4 R5, R1  R6 R4  D1 D2 D4 T1 T2 D1 D2 T0 T3 T4 D4 T1 T2 D1 D2 T0 T2 T4 T2 T4 T3 T4 D2 D4 D4 D2 D4 T2 D2 T0 T3 T4 D4 T2 D2 T0 T1 D3 D3 D3 T3 T3 D1 D0 D0 T1 T1 T1 T0 Data  Recept ion Data  Modulation Token  Request R0 D0 T3 D0 R0 R0 D0 R0 D0 D1 R4 R0 D1 R4 D0 R0 D1 R4 D3 R1 D3 R1 D3 R1 D3 R1 Figure 10. Sample single-pass token stream timing diagram with realistic latencies. R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R (a) (b) (c ) R R R R R R R R Figure 11. Waveguide layout for varied crossbar radix (k) and concentration (C ): (a) k = 8, C = 8, (b) k = 16, C = 4 and (c) k = 32, C = 2. 3.8. Waveguide Layout and Integration We assume 3-D integration to support FlexiShare such that a separate optical die is stacked on top of a processor die. Such stacking enables specialized fabrication process for the optical die and more freedom for the waveguide routing. 3-D stacking also makes power a more important constraint compared to area for the on-chip network [13]. The same layout of the waveguide for a 64-tile processor is assumed for SWMR, MWSR and FlexiShare, as shown in Figure 11. Each tile has a single interface to the on-chip network and various degrees of concentration can be implemented. We assume Dense Wavelength Division Multiplexing (DWDM) where up to 64 wavelengths are transmitted in a single waveguide (in both directions). The waveguides run in parallel to avoid crossing. Figure 12(a,b) show the waveguide layout for the token streams (Section 3.3.1) and credit streams (Section 3.5). The token stream waveguide passes each router twice to enable the two-pass token-stream arbitration (Section 3.3.2); while for the credit streams, the laser has to be routed to the router distributing the credits ﬁrst (R2 in the ﬁgure), and then traverse all the routers twice, as highlighted with different colors. The different types of channels required in a radix-k FlexiShare network with M channels is shown in Table 1. Because of the need for upstream and downstream sub-channels, two R0 R1 R2 R3 R4 R5 R6 R7 C redit Stream R0 R1 R2 R3 R4 R5 R6 R7 Token Stream (a) (b) R0 R1 R2 Data /  R3 R eservation R4 R5 R6 R7 (c) Token C redit Figure 12. (a) Token stream waveguide. (b) Credit stream waveguide. (c) Waveguides for all the 3 types of channels. Channel Data Reservation Token Credit # of λ Waveguide 2M × w 1-round, bi-dir 1-round, bi-dir 2-round, bi-dir 2.5-round, uni-dir Comments w-bit datapath broadcast 2k log k 2k k Table 1. Channels in FlexiShare sets of wavelengths are needed for data, reservation and token channels in opposite directions. Reservation channels need higher laser energy to broadcast the destination information for a packet. Figure 12(c) demonstrates how to layout the waveguides of all these channels without crossing each other. Note that only downstream data/reservation and token waveguides are shown. The upstream counterparts are simply mirrored by y-axis. Here, credit streams of R0 · · · R3 are downstream and are shown, while R4 · · · R7 credit streams are not shown as they are upstream. 4. Evaluation In this section, we ﬁrst describe our system setup for evaluation. Then, the performance of FlexiShare with varied channel provision is analyzed, followed by comparison with alternative designs. Based on the nanophotonic power model in [13], we also estimate the potential of power reduction in FlexiShare. 4.1. System Setup A cycle accurate network simulator is developed based on the booksim simulator [9, 3] and modiﬁed to represent the various topologies. The evaluated alternative topologies with their features are listed in Table 2. We generalize conventional nanophotonic crossbar designs into two categories: MWSR (e.g., Corona [23]) and SWMR (e.g., the optical bus proposed by Kirman et al. [14]). Note TR-MWSR adopts the two-round data channel design (Figure 6(a)). The proposed token-stream arbitration scheme is applied to an MWSR architecture (TSMWSR) to demostrate the beneﬁt of this arbitration scheme itself. We assume a refractive index of n = 3.5 for the waveguide and a conservative 2-cycle latency each for processing an optical token request. The clock frequency is targeted at 5GHz. We focus on network size of 64 (N = 64) and each data packet contains a single ﬂit of 512-bits. 4.2. FlexiShare Channel Provision Figure 13 shows the load latency curves for a radix-8, 64node FlexiShare with varied amount of channels (M ). Note Code Name Channel Arbitration Credit Control Data Channel Type Comments TR-MWSR Token Ring Inﬁnite Credit Two-round (Figure 6 (a)) TS-MWSR 2-pass Token Stream Inﬁnite Credit Single-round (Figure 6 (b)) R-SWMR 2-pass Credit Stream Single-round (Figure 6 (b)) Reservation-assisted FlexiShare 2-pass Token Stream 2-pass Credit Stream Single-round (Figure 6 (b)) Reservation-assisted Table 2. Evaluated networks Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ Ϭ Ϭ͘Ϯ Ϭ͘ϰ Ϭ͘ϲ Ϭ͘ϴ  Ğ ǀ ƌ Ă > Ğ Ő Ă  ƚ Ŷ Ğ Ǉ Đ ;  Đ Ǉ Đ ů Ğ Ɛ Ϳ /ŶũĞĐƚŝŽŶ ZĂƚĞ Dсϰ Dсϲ Dсϴ Dсϭϲ DсϯϮ ;ĂͿ ϯϬ Ϯϱ ϮϬ ϭϱ ϭϬ ϱ Ϭ Ϭ Ϭ͘Ϯ Ϭ͘ϰ Ϭ͘ϲ /ŶũĞĐƚŝŽŶZĂƚĞ Ϭ͘ϴ  Ğ ǀ ƌ Ă > Ğ Ő Ă  ƚ Ŷ Ğ Ǉ Đ ;  Đ Ǉ Đ ů Ğ Ɛ Ϳ Dсϰ Dсϲ Dсϴ Dсϭϲ DсϯϮ ;ďͿ Figure 13. FlexiShare (C = 8, N = 64, k = 8) with varied M under (a) uniform random and (b) bitcomp trafﬁc Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ Ϭ Ϭ͘ϭ Ϭ͘Ϯ Ϭ͘ϯ Ϭ͘ϰ Ϭ͘ϱ /ŶũĞĐƚŝŽŶZĂƚĞ Ϭ͘ϲ  Ğ ǀ ƌ Ă > Ğ Ő Ă  ƚ Ŷ Ğ Ǉ Đ ;  Đ Ǉ  ů Ğ Ɛ Ϳ ŬсϯϮ͕сϮ Ŭсϭϲ͕сϰ Ŭсϴ͕сϴ Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ Ϭ Ϭ͘Ϯ Ϭ͘ϰ Ϭ͘ϲ Ϭ͘ϴ EŽƌŵĂůŝǌĞĚ/ŶũĞĐƚŝŽŶZĂƚĞ ϭ  Ğ ǀ ƌ Ă > Ğ Ő Ă  ƚ Ŷ Ğ Ǉ Đ ;  Đ Ǉ Đ ů Ğ Ɛ Ϳ Dсϰ Dсϴ Dсϭϲ DсϯϮ ;ĂͿ ;ďͿ Figure 14. (a) Flexishare (M = 16, N = 64) with varied k and C under uniform random trafﬁc. (b) Channel utilization of FlexiShare (M = 16, N = 64) under bitcomp trafﬁc each data channel has two sub-channels running in opposite directions. By varying the number of channels (M ) provisioned, the network throughput can be tuned almost linearly – increasing M increases the throughput as the amount of network bandwidth increases with M . This ﬂexibility is important as the amount of provisioned channels directly corresponds to the designed optical power budget. FlexiShare with the 2-pass tokenstream arbitration is also insensitive to trafﬁc patterns, showing minimal performance loss with permutation trafﬁc such as bitcomp (Figure 13(b)). 4.3. Channel Utilization Figure 14(a) shows the performance of FlexiShare with the same number of channels (M = 16) for a N = 64 network, but with different crossbar radix and concentration. It can be seen that higher throughput is achieved by lower radix. The reason is that in the token stream arbitration scheme, each cycle a Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ Ϭ Ϭ͘ϭ dZͲDt^Z;DсϭϲͿ d^ͲDt^Z;DсϭϲͿ ZͲ^tDZ;DсϭϲͿ  &ůĞǆŝ^ŚĂƌĞ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ;DсϴͿ Ϭ͘Ϯ Ϭ͘ϯ Ϭ͘ϰ Ϭ͘ϱ /ŶũĞĐƚŝŽŶZĂƚĞ Ϭ͘ϲ  Ğ ǀ ƌ Ğ Ő Ă ů  Ă ƚ Ŷ Ğ Ǉ Đ  ;  Đ Ǉ  ů Ğ Ɛ Ϳ Ϭ ϱ ϭϬ ϭϱ ϮϬ Ϯϱ Ϭ Ϭ͘ϭ dZͲDt^Z;DсϭϲͿ d^ͲDt^Z;DсϭϲͿ ZͲ^tDZ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ;DсϴͿ Ϭ͘Ϯ Ϭ͘ϯ Ϭ͘ϰ /ŶũĞĐƚŝŽŶZĂƚĞ Ϭ͘ϱ  Ğ ǀ ƌ Ğ Ő Ă ů  Ă ƚ Ŷ Ğ Ǉ Đ  ;  Đ Ǉ  ů Ğ Ɛ Ϳ ;ĂͿ ;ďͿ Figure 15. TR-MWSR, TS-MWSR, R-SWMR and FlexiShare (k = 16, N = 64) under (a) uniform random and (b) bitcomp trafﬁc. router speculatively sends a request for one of the channels for each packet to be sent. If the request is not granted, it will retry each channel in a round-robin fashion in the following cycles. Thus, if there are a large number of routers requesting on the same stream, the success rate for downstream routers diminishes, which corresponds to degraded throughput. Figure 14(a) shows that a throughput reduction of 18% for a radix-32 FlexiShare compared to a radix-8 FlexiShare. Channel utilization directly corresponds to the efﬁciency of the network. In Figure 14(b), we normalize the injection rate by the amount of nanophotonic channels provisioned (x-axis). Thus a normalized injection rate of 1.0 means ideal utilization of the nanophotonic channels. It can be seen that the throughput is around 0.95 if the amount of channels is far lower than the number of tiles (e.g., 8 channels for a 64-tile network). However, as the number of channels increases, this efﬁciency tends to decrease. This is because, if there are not enough channels, there will be multiple routers trying to grab the tokens of each channel, and thus it is less likely for a token to go un-grabbed – resulting in high channel utilization. When there are more channels, there will be fewer routers trying to grab each token, and if a router makes a wrong speculation on which channel to try, it might be blocked while letting an available token on a different channel go unused – resulting in lower channel utilization. However, even for the full provision of FlexiShare with 32 channels for 64 tiles, the throughput is still above 0.7. 4.4. Comparison with Alternatives We compare the performance of FlexiShare to the alternative nanophotonic crossbar implementations in Figure 15. The token-ring arbitrated TR-MWSR shows the worst throughput, as the channel is not fully utilized because of the token roundϮ ϰ Ϯ ϰ͘ϰ ϭ͘ϱ ϭ Ϭ͘ϱ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ &ůĞǆŝ^ŚĂƌĞ;DсϰͿ &ůĞǆŝ^ŚĂƌĞ;DсϴͿ ZͲ^tDZ;DсϴͿ d^ͲDt^Z;DсϴͿ dZͲDt^Z;DсϴͿ hŶŝĨŽƌŵ ϭ͘ϱ ϭ Ϭ͘ϱ EŽƌŵĂůŝǌĞĚǆĞĐƵƚŝŽŶdŝŵĞ &ůĞǆŝ^ŚĂƌĞ ;DсϴͿ &ůĞǆŝ^ŚĂƌĞ ;DсϭϲͿ ZͲ^tDZ ;DсϭϲͿ d^ͲDt^Z ;DсϭϲͿ dZ ͲDt^Z ;DсϭϲͿ hŶŝĨŽƌŵ Ϭ ŝƚĐŽŵƉ ;ĂͿ Ϭ ŝƚĐŽŵƉ ;ďͿ Figure 16. Normalized execution time (with respect to FlexiShare). (a) k = 8 (b) k = 16 trip latency. By applying our proposed token-stream arbitration to a MWSR network (TS-MWSR), the throughput is improved by 5.5 times for bitcomp trafﬁc. R-SWMR and TS-MWSR show similar throughput, while the differences can be attributed to conﬂict scenarios at the sender and receiver ends. A striking difference here is that with the same amount of channels (M = 16), FlexiShare is able to provide almost twice the throughput as TS-MWSR or R-SWMR. This is because FlexiShare fully accesses both the upstream and downstream data channels, while the dedicated channel TS-MWSR or RSWMR can only utilize half of the data channels, as shown in Figure 9. For example, for a radix-8 network under bitcomp trafﬁc, TS-MWSR only utilizes the downstream sub-channel to send to node 4,5,6,7 and the upstream sub-channels to send to 0,1,2,3, leaving the upstream sub-channels to 4,5,6,7 and the downstream sub-channels to 0,1,2,3 idle. Note that these idle data channels are necessary to guarantee full connectivity of the network. Thus, FlexiShare is able to provide similar performance as TS-MWSR or R-SWMR with half the amount of channels, as shown by the dashed lines (FlexiShare (M = 8)) in Figure 15. However, with the token-stream arbitration, the delayed data slot increases the zero-load latency of the packets and results in approximately 30% increase in zero-load latency, compared to token-ring arbitration. 4.5. Synthetic Workload In addition to the load latency curves, we composed a synthetic workload where each tile has a ﬁxed amount of network requests (100K), whose destination observe a certain trafﬁc pattern (bitcomp or uniform). Upon receiving a request, the tile generates a reply packet to the source – in an attempt to mimic cache coherence messages. Each tile is allowed to have at most 4 outstanding requests, at which point it will be blocked from sending more request. The results are shown in Figure 16. Similar to previous ﬁndings, token stream arbitration reduces the total execution time on MWSR crossbars by at least a factor of 3.5 on bitcomp trafﬁc as compared to the token ring arbitrated TR-MWSR. In addition, FlexiShare with half the amount of channels achieves similar performance as TS-MWSR or RSWMR. It is no surprise to ﬁnd again that lower radix FlexiShare achieves higher performance. 4.6. Trace-based Workload In the synthetic trafﬁc patterns, we have shown performance results of the various networks under balanced trafﬁc loads – that is, each node has similar injection rate. However, in reality, the trafﬁc load distribution on a CMP may not be always balanced. There will be busy nodes and largely idling nodes, as demonstrated in Section 2. Thus, we create trace-based workloads where we use Simics/GEMS [16] to generate the network requests of several SPLASH-2 [25] and MineBench [17] applications. These traces contain time-stamped source/destination information for each request. As a compromise, we calculate the total number of requests for each node. Then, we assume the highest trafﬁc node has an injection rate of 1.0, while the other nodes all have an injection rate proportional to its total number of requests. Each node will send reply packets ahead of its own requests, and will be allowed a maximum of 4 outstanding requests before being blocked. We measure the total execution time of the traces as the performance metric. This maintains the unbalanced nature of the trafﬁc load, and in general stress the network more than the time-stamped trace as an injection rate of 1.0 is assumed for the busiest node. Thus it constitutes a pessimistic and conservative evaluation of FlexiShare, as in reality the trafﬁc load might be even lower – requiring even fewer channels in FlexiShare. Figure 17 shows the performance of a 64-node Radix-16 FlexiShare with varied number of channels. It is clear that for benchmarks barnes, cholesky, lu and water, only 2 channels (M = 2) are enough to support the trafﬁc load (in contrast to M = 16 for conventional nanophotonic crossbars); while apriori, hop and radix need more channels to be provisioned. This shows that FlexiShare can be provisioned according to the average trafﬁc load. Comparing FlexiShare with R-SWMR and TS-MWSR with double the amount of channels, we ﬁnd that similar performance can be achieved for low trafﬁc load benchmarks like lu and water, while FlexiShare performs signiﬁcantly better in hop and radix. This can be attributed to the global channel sharing in FlexiShare. Note the topologies here implement a local concentration (C = 4) so that channel resources are shared among neighboring nodes; but it is obvious that the global sharing in FlexiShare is more effective. Component Coupler Splitter Non-linear Modulator-  Insertion Loss 1 dB 0.2 dB 1 dB 0.001 dB Component Waveguide Loss Waveguide Crossing Ring Through Loss Filter Drop Photo Detector Loss 1 dB/cm 0.05 dB 0.001 dB/ring 1.5 dB 0.1 dB Table 3. Optical loss [13] 4.7. Power Model To show how the reduction in the number of channels affect the power consumption in the nanophotonic crossbars, we adopt the nanophotonic power model by Joshi et al. [13]. In terms of laser power, we model various losses along the path as listed in Table 3. Modulator insertion and non-resonant rings all pose energy losses to the laser beam, together with the length-dependent loss in the waveguide. Varied values have been assumed for the sensitivity of photodetectors in on-chip networks, from 80µW [11] to 1µW [26]. In our model, we EŽƌŵĂůŝǌĞĚ ǆĞĐƵƚŝŽŶdŝŵĞ ϬϭϮϯϰ EŽƌŵĂůŝǌĞĚ ǆĞĐƵƚŝŽŶdŝŵĞ ϬϬ͘ϱϭϭ͘ϱϮϮ͘ϱϯ ϳ͘ϳ ĂƉƌŝŽƌŝ ĂƉƌŝŽƌŝ dŽŬĞŶ ^ƚƌĞĂŵ ĂƚĂŚĂŶŶĞůƐ ƌĞĚŝƚ^ƚƌĞĂŵ ZĞƐĞƌǀĂƚŝŽŶŚĂŶŶĞůƐ Ϯϰ Ϳ t ;  ƌ Ğ ǁ Ž W  ƌ Ğ Ɛ Ă >  ů Ă Đ ŝ ƌ ƚ Đ Ğ ů ϭϬ ϵ ϴ ϳ ϲ ϱ ϰ ϯ Ϯ ϭ Ϭ ϲ͘ϳ ϱ͘ϱ ŚŽƉ ŬŵĞĂŶƐ ŬŵĞĂŶƐ ůƵ dŽŬĞŶ ^ƚƌĞĂŵ ĂƚĂŚĂŶŶĞůƐ ďĂƌŶĞƐ ĐŚŽůĞƐŬǇ ďĂƌŶĞƐ ĐŚŽůĞƐŬǇ ŚŽƉ ƌĞĚŝƚ^ƚƌĞĂŵ ZĞƐĞƌǀĂƚŝŽŶŚĂŶŶĞůƐ ϭϭ ƌ ƚ Đ Ğ Ž W  t Ɛ Ă >  ů Ă Đ Ğ ;  ƌ Ğ ǁ ϰ ϯ Ϯ ϭ ŝ ů Ϳ ƌ dZͲDt^Z ;DсϯϮͿ d^ͲDt^Z ;DсϯϮͿ ;ĂͿ ZͲ^tDZ ;DсϯϮͿ &ůĞǆŝ^ŚĂƌĞ ;DсϭϲͿ Ϭ dZͲDt^Z ;DсϭϲͿ d^ͲDt^Z ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ ;DсϴͿ ZͲ^tDZ ;DсϭϲͿ ;ďͿ Figure 19. Electrical Laser Power Breakdown for various crossbars (a) k = 32 (b) k = 16 assume it to be 10µW , in accordance with values adopted by Joshi et al. [13]. Based on these parameters, we layout the waveguides and estimate the per-wavelength laser power needed to activate the farthest detector. As for ring heating, we also assume 1µW heating power per ring per Kelvin, and 20K tuning range [13]. We estimate the electrical switches in the routers are the dominant part in electrical power, and apply a power model [24] to account for the sizes of the switches. Targeting a 22nm technology node based on ITRS [1], we calculate the baseline energy for a 512bit packet to traverse a 5 × 5 electrical switch to be 32pJ . 4.7.1 Laser Power Comparison As the added ﬂexibility enables FlexiShare to at least match the performance of the alternatives with twice the amount of channels, we compare the power consumption of FlexiShare with half the channels as the alternative designs in Figure 19. It can be seen that TR-MWSR consumes much higher power, because its twice long waveguides incur more optical loss. The credit streams and token streams consume minimal laser power, due to their respective narrow width compared to the data channel and fewer number of through rings on the path. The broadcasting reservation channels constitutes a major overhead for R-SWMR and FlexiShare. This is especially the case Figure 17. Normalized execution time for FlexiShare (N = 64, k = 16) with varied M . Figure 18. Normalized execution time for various crossbars (N = 64, k = 16). ϲ͘ϰ ϱ͘ϱ ƌĂĚŝǆ ƐĐĂůƉĂƌĐ ƐĐĂůƉĂƌĐ ǁĂƚĞƌ ůƵ ƌĂĚŝǆ DсϭDсϮDсϯDсϰDсϲDсϴDсϭϲDсϯϮ ǁĂƚĞƌ &ůĞǆŝ^ŚĂƌĞ;Dсϴ Ϳ ZͲ^tDZ;Dс ϭϲͿ d^ͲDt^Z;Dс ϭϲͿ dZͲDt^Z;D сϭϲͿ when the crossbar radix is higher. This signiﬁes that a moderate radix is more efﬁcient for FlexiShare and R-SWMR crossbars. Even with all these overheads, FlexiShare with half the amount of channels reduce the total laser power by at least 18% and 35%, for k=32 and k=16, respectively, compared to the best alternatives. 4.7.2 Total Power Comparison Assuming a uniform average trafﬁc load of 0.1pkt/cycle, we compare the total energy consumption of different architectures. The results, presented in Figure 20, show that ring heating and laser power are dominant in TR-MWSR, TS-MWSR and R-SWMR, as expected. The added complexity in the electrical routers of FlexiShare results in higher electrical overhead. However, this is incurred for signiﬁcant power reduction in the dominant laser and ring heating power. The advantage of FlexiShare is especially clear when much fewer channels are provisioned according to the trafﬁc. For example, according to Figure 17 and 18, merely 2 channels (M = 2) are enough for a radix-16 FlexiShare to achieve the same performance as the alternatives for lu application; and FlexiShare with M = 4 performs better for radix application. In these cases, radix-16 FlexiShare reduces the total power consumption by 41% and 27%, respectively. The reduction could be up to 72% (when M = 2) against a best alternative if radix-32 designs are compared. 4.7.3 Device Requirement According to Figure 17 and Figure 18, FlexiShare with M = 4 outperforms most of the alternative networks. We align their performance and show the device requirements FlexiShare (M = 4), R-SWMR (M = 16), TS-MWSR (M = 16) and TR-MWSR (M = 16) would impose under different power budgets, as shown in Figure 21. In this ﬁgure, the contour lines are electrical power budgets in watt for generating the laser. By reducing the number of channels provisioned, FlexiShare can   ϬϱϭϬϭϱϮϬϮϱϯϬϯϱϰϬϰϱ dZͲDt^Z   ;DсϯϮͿ d^ͲDt^Z ;DсϯϮͿ ZͲ^tZD ;DсϯϮͿ ůĞĐ͘>ĂƐĞƌ ZŝŶŐ,ĞĂƚŝŶŐ KͬKŽŶǀ͘ >ŽĐĂů>ŝŶŬ WŽǁĞƌ ZŽƵƚĞƌ &ůĞǆŝ^ŚĂƌĞ  ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ  ;DсϴͿ &ůĞǆŝ^ŚĂƌĞ  ;DсϰͿ &ůĞǆŝ^ŚĂƌĞ  ;DсϮͿ dŽƚĂůWŽǁĞƌ;tͿ dŽƚĂůWŽǁĞƌ;tͿ ϬϱϭϬϭϱϮϬϮϱ dZͲDt^Z  ;DсϭϲͿ d^ͲDt^Z   ;DсϭϲͿ ZͲ^tZD   ;DсϭϲͿ &ůĞǆŝ^ŚĂƌĞ  ;DсϴͿ &ůĞǆŝ^ŚĂƌĞ ;DсϲͿ ůĞĐ͘>ĂƐĞƌ ZŝŶŐ,ĞĂƚŝŶŐ KͬKŽŶǀ͘ >ŽĐĂů>ŝŶŬ WŽǁĞƌ ZŽƵƚĞƌ &ůĞǆŝ^ŚĂƌĞ  ;DсϰͿ &ůĞǆŝ^ŚĂƌĞ  ;DсϮͿ ;ĂͿ ;ďͿ Figure 20. Total Power Breakdown. (a) k = 32 (b) k = 16. 1e-4 1e-3 6e-4 3e-4 1e-2 6e-3 3e-3 1e-1 5e-2 3e-2 h T r u s s o L ( d B r / i g n ) 0 .5 1 1.5 W aveguide Loss (dB/cm) 2 2.5 1e-4 1e-3 6e-4 3e-4 1e-2 6e-3 3e-3 1e-1 5e-2 3e-2 h T r u s s o L ( d B r / i g n ) 0 .5 1 1.5 W aveguide Loss (dB/cm) 2 2.5 1e-4 1e-3 6e-4 3e-4 1e-2 6e-3 3e-3 1e-1 5e-2 3e-2 h T r u s s o L ( d B r / i g n ) 0 .5 1 1.5 Waveguide Loss (dB/cm) 2 2.5 3 6 18 12 24 30 3 6 12 18 24 30 3 6 12 18 30 24 (a) (b) (c) Figure 21. Electrical laser power (W) for (a) TR-MWSR (k = 16, C = 4, M = 16), (b) TS-MWSR (k = 16, C = 4, M = 16) and (c) FlexiShare (k = 16, C = 4, M = 4). meet an electrical laser power budget as low as 3W with ring through loss of up to 0.011 and waveguide loss of 1.7dB/cm. 5. Related Work Recent advances in optical signaling [2, 4, 19, 21] have made the use of on-chip optical signaling a possibility. Different on-chip network architectures have been proposed to exploit silicon nanophotonics. Kirman et al. [14] proposed a 64node CMP architecture which takes advantage of nanophotonics to create an on-chip bus and results in a hierarchical, multibus interconnect. Shacham et al. [20] proposed using an electrical layer for control signals while the channels and the switches for data transmission use optical signaling. The Corona [23] architecture implements a monolithic crossbar topology to support on-chip as well as off-chip communication. Another crossbar structure was proposed by Batten et al. [5] to connect a many-core processor to the DRAM memory using monolithic silicon. Recently, Joshi et al. [13] proposed to implement a nanophotonic clos network that provides uniform latency and throughput while consuming low power. Nanophotonic switching is explored in the Phastlane [8] work. The hierarchical Fireﬂy architecture [18] was proposed to use partitioned nanophotonic crossbars to connect clusters of electrically connected mesh networks, so as to improve power efﬁciency. Zheng et al. [26] implemented a hybrid nanophotonic-electric network where latency-critical messages are broadcast in a planar optical waveguide using a unique optical antenna structure. The FlexiShare architecture proposed in this paper is an efﬁcient implementation of a nanophotonic crossbar, that can be leveraged by existing topologies to improve energy efﬁciency. Resource sharing is a common technique that has been used in interconnection network and different aspects of sharing has been proposed for on-chip networks. XShare [10] switch architecture was proposed to combine multiple short packets and send them together on a wide channel. For on-chip networks, concentrated mesh [3] topology was proposed to reduce the network diameter and improve channel utilization by sharing routers among multiple nodes. Kumar et al. [15] studied different implementations of concentration and its the impact on the network efﬁciency. In this work, we extend the concept of concentration to provide global concentration such that the resources (channels and buffers) are shared across the network. Load-balanced Birkhoff-von Neumann switches were proposed by Cheng et al. [7], where back-to-back crossbars are used with the ﬁrst crossbar for load-balancing and a buffer stage inserted in between. Flexishare has a similar logical structure but our main objective is reducing the amount of global bandwidth (i.e., optical channels) to improve the power efﬁciency of nanophotonics. In addition, the arbitration of the crossbar in FlexiShare is not a centralized scheduler but distributed and leverages nanophotonics with novel token-stream arbitration schemes. 6. Conclusion Recent advances in nanophotonics have presented opportunities for architects to exploit the beneﬁts of nanophotonics for       on-chip communication in future many-core processors. However, nanophotonics poses different constraints compared to conventional, electrical on-chip network as the channels become an expensive resource because of the static power overhead. Motivated by this constraint and the unbalanced trafﬁc observed in different applications, we propose FlexiShare – an energy-efﬁcient crossbar design which provides the ﬂexibility to provision the amount of channel resource independent of the network size. By sharing the channels among all of the nodes in the network – creating global concentration, the number of channels required can be reduced to mitigate the static power overhead of nanophotonics. FlexiShare decouples the allocation of buffers from the channels to enable global sharing of the buffers as well. A novel token-stream mechanism is used for channel arbitration to better utilize the channels as well as distributed credit information for efﬁcient buffer management. By applying this arbitration scheme to a conventional MWSR crossbar, the throughput is improved by up to 5.5× compared to a token-ring arbitrated network. Flexishare introduces additional complexity to support the global sharing and results in moderate increase in electrical power consumption compared to a conventional nanophotonic crossbar. However, because of the signiﬁcant reduction of the amount of channels we can achieve an overall power reduction of at least 27% (up to 72%) while providing similar or better performance than the conventional nanophotonic crossbars under unbalanced trace trafﬁc loads. Acknowledgements This work is supported by NSF grants CCF-0916746 and CCF-0747201. Special thanks to Yu Zhang for generating the NoC trafﬁc traces in our evaluation and Dr. Ajay Joshi and Dr. Christopher Batten for their help with the nanophotonic power models. We would also like to thank all the anonymous referees for their helpful comments. "
2010,Simple virtual channel allocation for high throughput and high frequency on-chip routers.,"Technology scaling has led to the integration of many cores into a single chip. As a result, on-chip interconnection networks start to play a more and more important role in determining the performance and power of the entire chip. Packet-switched network-on-chip (NoC) has provided a scalable solution to the communications for tiled multi-core processors. However the virtual-channel (VC) buffers in the NoC consume significant dynamic and leakage power of the system. To improve the energy efficiency of the router design, it is advantageous to use small buffer sizes while still maintaining throughput of the network. This paper proposes two new virtual channel allocation (VA) mechanisms, termed Fixed VC Assignment with Dynamic VC Allocation (FVADA) and Adjustable VC Assignment with Dynamic VC Allocation (AVADA). The idea is that VCs are assigned based on the designated output port of a packet to reduce the Head-of-Line (HoL) blocking. Also, the number of VCs allocated for each output port can be adjusted dynamically. Unlike previous buffer-pool based designs, we only use a small number of VCs to keep the arbitration latency low. Simulation results show that FVADA and AVADA can improve the network throughput by 41% on average, compared to a baseline design with the same buffer size. AVADA can still outperform the baseline even when our buffer size is halved. Moreover, we are able to achieve comparable or better throughput than a previous dynamic VC allocator while reducing its critical path delay by 60%. Our results prove that the proposed VA mechanisms are suitable for low-power, high-throughput, and high-frequency on-chip network designs.","Simple Virtual Channel Allocation for High Throughput and High Frequency On-Chip Routers ∗ Yi Xu† , Bo Zhao† , Youtao Zhang‡ , Jun Yang† † Dept. of Electrical and Computer Engineering ‡ Dept. of Computer Science University of Pittsburgh, Pittsburgh, PA 15621 †{yix13, boz6, juy9}@pitt.edu, ‡{zhangyt}@cs.pitt.edu Abstract Technology scaling has led to the integration of many cores into a single chip. As a result, on-chip interconnection networks start to play a more and more important role in determining the performance and power of the entire chip. Packet-switched network-on-chip (NoC) has provided a scalable solution to the communications for tiled multicore processors. However the virtual-channel (VC) buffers in the NoC consume signiﬁcant dynamic and leakage power of the system. To improve the energy efﬁciency of the router design, it is advantageous to use small buffer sizes while still maintaining throughput of the network. This paper proposes two new virtual channel allocation (VA) mechanisms, termed Fixed VC Assignment with Dynamic VC Allocation (FVADA) and Adjustable VC Assignment with Dynamic VC Allocation (AVADA). The idea is that VCs are assigned based on the designated output port of a packet to reduce the Head-of-Line (HoL) blocking. Also, the number of VCs allocated for each output port can be adjusted dynamically. Unlike previous buffer-pool based designs, we only use a small number of VCs to keep the arbitration latency low. Simulation results show that FVADA and AVADA can improve the network throughput by 41% on average, compared to a baseline design with the same buffer size. AVADA can still outperform the baseline even when our buffer size is halved. Moreover, we are able to achieve comparable or better throughput than a previous dynamic VC allocator while reducing its critical path delay by 60%. Our results prove that the proposed VA mechanisms are suitable for low-power, high-throughput, and high-frequency on-chip network designs. 1. Introduction Technology scaling has enabled the integration of billions of transistors on a single chip. Chip multiprocessors (CMP) have emerged as an effective design for utilizing ∗ This work is supported in part by NSF grants 0747242, 0641177, 0720595, and 0734339. on-chip transistors to continue the growth of chip performance with integration density [9, 24]. With the proliferation of CMPs, on-chip interconnection networks start to play a more and more important role in determining the performance and power of the entire chip [14]. Among various network on-chip (NoC) designs, packet-switched network nication in 10’s∼100’s-core CMPs. is recognized as a scalable and ﬂexible solution for commuAs with designing a general microprocessor, one of the major challenges in packet-switched NoC is how to limit its power consumption. Studies have shown that on-chip network can consume 30%∼40% of the chip power [10, 32], which is a major limitation to future NoC development. Hence, many techniques such as packet bypassing [16, 17], router concentration [1], crossbar and buffer optimization [32], bufferless routing [19] etc. have been proposed to save dynamic power or make the NoC more energy efﬁcient. On the other hand, there has been an incessant effort in pursuing aggressive high-throughput and high-frequency router designs. Examples include the 5GHz router design in Teraﬂops chip [10, 31] and the 4 GHz 256Gbits/s per node router design [30]. Extremely high frequency NoCs may require long pipeline stages [10, 30] in the router, which may be of a concern under low trafﬁc load where packet latency is more important. High-frequency routers are typically of high power too. Hence, to achieve high-frequency, lowlatency and high-throughput under power constraint, one must adopt simple yet still highly efﬁcient router design. One of the most power hungry components in a router is its buffer [15, 32]. A buffer is used as a set of virtual channels (VCs) [3, 4] to store incoming or outgoing packets. How buffers are used impacts both network throughput and power. They are mainly determined by VC number, VC sizes and how they are allocated. The size of a VC queue can be either static or dynamic. A static queue always has a ﬁxed size while a dynamic queue can grow and shrink. Although a dynamic queue size helps to improve the buffer utilization efﬁciency such that more ﬂits can be stored in the buffer, it does however increase the complexity of the related control logic. For example, the DAMQ design [2, 28] used hardware to implement a linked list to 987-1-4244-5659-8/09/$25.00 ©2009 IEEE manage dynamic queue size, resulting large delay in every ﬂit arrival/departure. Later designs avoided using linked list, but still incur high cost in control logic. For example, The ViChaR design [23] can support a VC size anywhere from one ﬂit to a whole packet, generating a maximum VC count as the size of the whole buffer in terms of ﬂits (VC count = buffer size / VC size). This requires f :1 arbiters, where f is the buffer size, in both VC allocation (VA) and switch allocation (SA) stage. Such cardinality of an arbiter may introduce latency bottleneck in the critical path of a router, which can limit the frequency of the network. Hence, a static VC queue design is preferred in a high-frequency router design unless high-complexity logic is employed for short cycle time. From VC allocation point of view, a VC can be statically assigned to trafﬁc in one direction, or dynamically assigned to any incoming/outgoing trafﬁc on-demand. Static allocation, though not used as much nowadays, has its advantage in that trafﬁc to/from all directions have equal opportunities to compete for the switch. Dynamic allocation, on the other hand, can better tolerate burst trafﬁc from one direction, or uneven trafﬁc distribution among all directions. As we can see, both allocation schemes have their own advantages. A design that integrates features in both schemes would be most beneﬁcial to a high-throughput router. In this paper, we propose simple VC allocation schemes to achieve high-frequency and high-throughput router design. We use a small maximum number of VCs with ﬁxed VC size to keep the arbiter size small and fast. To integrate the advantages of both static allocation and dynamic allocation schemes, we designate each VC for one output port (direction) but allow such assignment to vary according to trafﬁc ﬂuctuation. Since our total VC count is small, we give body and tail ﬂits higher priorities than a header ﬂit during VA to recycle a VC quickly. Our changes required to a baseline router are extremely simple, compared to previous designs which improve network performance at the cost of major changes in the router. Simulation results show that our proposed scheme performs equally well compared to the baseline router design with double the buffer size. When buffer sizes are identical, our schemes achieves 41% better throughput than the baseline. When compared to ViChaR, the most aggressive dynamic VC allocation scheme that best utilizes buffer space, our results are similar or even better for some trafﬁc patterns. However, we reduced its critical path delay by 60%, which allows us to clock the network at 1.5× higher frequency. The remainder of this paper is organized as follows. Section 2 discusses related works on buffer designs. Section 3 discusses the details of proposed virtual channel allocation mechanisms as well as router architecture and adaptive routing design. Section 4 shows the experimental results. Finally, Section 5 concludes this paper. 2 Related Work Due to the increasing stringent power constraint, there have been many work on reducing the buffer power dissipation in a router. One approach is to directly reduce the buffer size for lower power. The “iDEAL” [13] design utilizes the channel buffers to store ﬂits during trafﬁc congestion. Through this way, the buffer size inside the router can be decreased to save both dynamic and static power. This method has also been pushed to one extreme to eliminate entirely the buffers inside a router such as the Elastic Buffer [18] and bufferless routing [19] to achieve simple router design and improve energy efﬁciency. Flits are immediately passed to an output port upon arriving at an input port [19]. This is suitable for low trafﬁc load where buffers are mostly under-utilized. When buffer size is reduced, fewer ﬂits can be stored in the router, and the network throughput is penalized. Therefore, many works have focused on improving the utilization of buffers to achieve high-throughput with small buffer sizes. ViChaR [23] is such a design which makes full use of every ﬂit slot in a buffer. Whenever a ﬂit comes in, it is stored in the buffer as long as there is a free slot. Flits within a packet may be distributed anywhere in the buffer, but their locations are bookkept in a control table. A VC is reserved when a header ﬂit comes in, and released when its tail ﬂit leaves the router. Hence, the number of VCs inside the router can range anywhere from f /p to f , where f if the buffer size in ﬂits, and p is the number of ﬂits per packet. As a result, the arbiters in the VA and SA stages need to be as wide as f :1. For example, if a buffer can store 4 complete packets, and each packet contains 5 ﬂits, then the arbiters have a cardinality of 20:1. Our design in this paper reduces it to 4:1 without losing the throughput of the network. The buffer pool design [15] is similar to ViChaR except that each VC can store multiple packets, and its VC count is ﬁxed. Though the buffer pool design uses smaller arbiters, its VC allocation scheme is dynamic while ours is between static and dynamic allocation, as explained next. The DAMQ [28] and its improvements [22, 25] use ﬁxed number of VCs, each dedicated to an output port with an extensible queue length. Hence, trafﬁc heading to one direction is stored in one queue. The beneﬁt of static VC allocation is that trafﬁc from all directions have equal opportunities to compete for the crossbar, resulting good arbitration efﬁciency. Such philosophy can also be seen in the RowColumn(RoCo) Decoupled Router [12] design where VCs are grouped for trafﬁc in x-direction and y-direction separately. The contention in SA arbitration is effectively reduced as shown in that work. The dynamic queue length in DAMQ is helpful to handle burst trafﬁc from one direction. However, managing the dynamic queue is very costly in the original design. Even if the control table used in ViChaR was adopted, the cost would still be high as the table has to account for the longest queue length for each VC, resulting inefﬁcient use of control table space. Therefore, our design ﬁxes the VC queue size, and starts with a static VC allocation for good arbitration efﬁciency. When trafﬁc becomes imbalanced, we start dynamic VC allocation to compensate for the inﬂexibility of the ﬁxed queue size. 3 Proposed Virtual Channel Allocation In this section, we will describe the state-of-the-art router architecture designs, followed by the introduction of our proposed router design with the simple buffer design. 3.1 Background: A Generic Router Architecture A generic router in mesh topology has 5 input/output ports for four cardinal directions (North, South, West, East ), and one local processing element (CPU or Cache Bank). When a header ﬂit arrives at the router, it is ﬁrst buffered in the determined VC in the buffer writing (BW) pipeline stage. Then, the routing computation(RC) unit determines the output port based on the destination information in the header ﬂit and the routing algorithm. Next, the virtual channel allocation(VA) unit performs arbitration among all ﬂits requesting for the same output VC. If the ﬂit is able to obtain a free VC, it proceeds to the switch allocation(SA) stage where it arbitrates for the switch input and output ports. Once the ﬂit is granted to use the output port, it proceeds to crossbar traversal(ST) stage where the ﬂit traverses the crossbar, followed by the link traversal(LT) to the next router or PE [26]. Body and tail ﬂits do not need to go through RC and VA stage, but SA is still necessary and it is done on per ﬂit basis. Once the tail ﬂit leaves the router, it deallocates the VC reserved for the packet. Each pipeline stage takes one cycle to execute. To shorten the pipeline depth, many prior works have proposed techniques such as look-ahead routing(LA) [7], speculative allocation [26], pipeline bypassing [15, 17, 21], aggressive speculation [21] to remove dependencies and parallelize pipeline stages. We use two-stage router design similar to [11] in this paper. VC ID 0 OP EAST RP WP 0 1 Status SA Pre-route OVC EAST 0 Table 1. A sample entry of VC control table. 3.2 VC Buﬀer Organization and VC Control Table The components we revise over the baseline two-stage router are in the VA stage. First, the buffers are implemented as SRAM (Static Random Access Memory) arrays. Each input port has a buffer which is organized as 4 VCs, each having 5 ﬂits, the size of a packet. We will defer the allocation schemes of the VCs to later sections. The management of the VCs is carried through a control table as shown in Table 1. The table maintains the state of each VC. The “OP” is the output port produced by the RC of the last router to denote the required output port of the current node. The “RP” is the read pointer used as address to read ﬂit from the SRAM and pass it on to the crossbar. The “WP” is the write pointer which indicates the write address for the next incoming ﬂit. If the read pointer and write pointer are the same after a write operation, the VC buffer is full. If they are the same after a read operation, the buffer is empty. The “Status” ﬁeld indicates the status or in which stage the packet in this VC is in — idle, RC, VA, SA, ST, and others. The “Pre-route” produced by RC at local node is the output port index for the next node downstream. The “OVC” is the VC ID for the ﬂit to switch to in the downstream router. We use the credit-based ﬂow control in this work, where a credit is the number of available buffer slots for each VC of the downstream node. It decrements whenever a ﬂit is read out of the buffer and leaves the current router, and increments when it receives the one-bit control signal from downstream router to denote that a ﬂit has left. If a ﬂit obtained an output VC successfully, won the SA stage and the credit of the chosen VC was larger than zero, it can proceed to the ST stage and then traverse to the next node. 3.3 Arbiter The arbitration stages in VA select a winner per output VC among all ﬂits at the head of the VCs. This process VA together with SA are the bottleneck stages of the router pipeline [23]. since they account for most of the router control logic. Hence, their latencies determine the clock cycle time of the router. The VA and SA latencies are dependent on the cardinality of the arbiters, or the worst-case delay on the critical path. For our buffer size of 20 ﬂits, an aggressive dynamic allocation scheme such as ViChaR requires 20:1 arbiters. This is a signiﬁcant delay, area and power budget in the router, even when the actual number of requests are smaller than 20. Table 2 compares essential metrics for a 20:1 and 4:1 arbiter, the latter being used in our VA and SA. Both arbiters are implemented in HSPICE with 45nm PTM [33] device models at 1.1V and a temperature of 90◦C. As we can see, the 20:1 arbiter is more than 4× slower, consumes more than 2× dynamic power and 7× static power, and takes more than 4× the area than a 4:1 arbiter. We also varied the number of inputs to each arbiter and observed that the dynamic power consumption of the arbiter is independent of the number of inputs. This is shown in Table 3. The reason behind it is that there is a priority vector inside the arbiter, and this vector changes according to some algorithm, e.g. Round Robin, on every arbitration. Hence, larger arbiters constantly consume more power than small arbiters. Based on the above observations, we conclude that simple arbiter design is more advantageous. Arbiter Design 4:1 arbiter 20:1 arbiter Delay (ps) 140 600 Dynamic Power (mW ) 1.07 2.49 Static Power (µW ) 1.59 11.2 Area (µm2 ) 120 530 Table 2. Latency, area and power of an arbiter. Arbiter Design 4:1 arbiter 20:1 arbiter Request Type 0 request 1 request 4 requests random requests 0 request 1 request 20 requests random requests Power (mW ) 1.049 1.059 1.037 1.079 2.478 2.459 2.455 2.521 Table 3. Power of an arbiter with different types of inputs. 3.4 Arbitration Priority Following our study, we use small arbiters in VA and SA for low latency and low power. We use 4 VC/port for (FVADA) scheme, and 2∼5 VC/port for Adjustable VC Asour Fixed VC Assignment with Dynamic VC Allocation signment with Dynamic VC Allocation (AVADA) scheme respectively. FVADA and AVADA will be introduced in next sections. The choice of VC count will also follow naturally there. Since the VC resources in our design is not as ample as other dynamic VC allocation designs, we do not want a packet to hold many VCs across the intermediate router nodes. In current router design, once a portion of a packet occupies a VC queue, other packets are not allowed to use the rest free VC ﬂit slots until the tail ﬂit of the current packet comes, so as to avoid interleaving of ﬂits from different packets. As a result, this scheme implies a poor buffer resource allocation since the vacant VC resources are wasted if the rest of the packet is blocked in some router upstream. To alleviate this problem, especially for small VC count design, the arbitration in our design gives higher priority to the body and tail ﬂits over a new header ﬂit. This way the body or tail ﬂit can release their VC number sooner than otherwise. This scheme was used in [15] to improve SA efﬁciency. We use it here for fast recycling of VCs. If, however, the body or tail ﬂit is not in the router yet, the arbiter still selects a header ﬂit for VC allocation. Note that such a priority rule will not incur starvation because at any time, there are only limited number of VCs in the middle of transferring a packet, and each packet has only limited number of ﬂits. Hence, a header ﬂit can always become a SA winner within ﬁnite number of arbitrations. Such a priority rule is not useful for other buffer management schemes with large number of VCs because VC resources are abundant. However, too many VCs could cause a packet to spread across many routers, which increases its latency in the network [5]. This will be discussed further in the performance evaluation section later. In summary, therefore, there are two distinct characters in our design. The ﬁrst one is that we use small number of VCs to keep the latency of every major component of the router low so that the router is able to accommodate high clock frequency. The second one is that the body/tail ﬂits are given higher priority to mitigate inefﬁcient buffer resources allocation. Next, we will describe our VC allocation mechanisms. 3.5 FVADA Alleviating HoL. Since our VC number is small, using existing dynamic allocation would create many Head-ofLine (HoL) blocking, meaning that the ﬂit at the head of a VC could not move forward due to output port being busy or no free buffer slot in the downstream router, then all other ﬂits following this ﬂit in that VC are blocked even if their required port and buffer resources are available. In order to reduce the HoL blocking and increase the network throughput, FVADA maps each VC to an output port. For example, the buffer for the East input port has 4 VCs, corresponding to West, South, North, and Local respectively. The packets destined to a particular output port will be queued into the corresponding VC. This is supported by the pre-routing outcome in the upstream router. Such a mapping is ﬁxed and pre-deﬁned. Figure 1 illustrates how this allocation can alleviate the HoL blocking. Assume there are four incoming packets P1∼P4, with designated output ports EAST, EAST, WEST and WEST respectively in the current router. Consider a condition that the EAST output port is busy now, or, the downstream router does not have free buffer resources so that the ﬂits heading to EAST have to stay in the current buffer. In Figure 1(a), it shows the scenario of the baseline router design: P1 and P2 at the head of VC0 and VC1 cannot move forward, so they block subsequent header ﬂits of P3 and P4 that are routed to the WEST port. While in FVADA design, shown in Figure 1(b), the packets routed to the same directions are stacked in the same queue. The ﬂits in VC1 routed to the WEST port are now separated from ﬂits in VC0 since they go to different directions. They are not blocked from being transmitted to their next node. Figure 1. VC Allocation in FVADA. Four incoming packets P1∼P4: P1 to East, P2 to East, P3 to West, P4 to West. Flits heading to EAST cannot move forward now as EAST port is busy or the downstream router does not have free buffer resources. Better Arbitration Efﬁciency. In addition to reducing the HoL blocking, FVADA can also improve the efﬁciency of the two-level SA arbitration. The ﬁrst stage of SA selects a local winner from 4 VCs, the second stage selects from local winners those that can be forwarded to the next routers at the same time through the switch. With FVADA, more local winners may be selected in the second stage of SA, and so more ﬂits can be passed on to their next routers, improving the overall network throughput. This is mainly attributed to the ﬁxed mapping between the VCs and the output ports. We use an example to illustrate this beneﬁt. Figure 2 compares the arbitration results of a baseline router and FVADA for two cycles: t and t + 1. In a baseline router, there could be more number of VCs than in FVADA. Hence, packets heading to the same output port can be allocated to different VCs, such as the V0 and V1 in Figure 2(a). Both are routed to the EAST port. Assume that V0 and V1 are the local winner at t and t + 1 respectively. Let us consider two scenarios where they might fail the global arbitration in the baseline router but not in FVADA. In the ﬁrst scenario, one fourth of the total available buffer space. This pressure can quickly backﬁres to the sender where packets to other directions can be blocked simply because the local queue is ﬁlled. To avoid performance degradation, FVADA employs dynamic VC allocation instead of static partitioning. During VC allocation, the “home” VC was ﬁrst considered. A “home” VC is the one corresponding to the ﬂit’s output port. If this home VC is full or already reserved, the allocation logic will consider other free VCs with vacant buffer slots. If there is no such VC, the home VC will still be assigned to this packet. We term this process “VC Selection”, a preparation step for “VC Assignment”, illustrated in Figure 3. The selection logic can be implemented using very simple circuit, as one depicted in Algorithm 1 that produces the corresponding selection decision. This simple circuit can operate in parallel with the SA stage. The latency is only 90ps in 45nm node, compared to the 280ps two-level arbiter latency in the baseline router. Since FVADA can also allocate VCs dynamically, packets routed to different ports may still mingle together, though not as much as in the baseline dynamic allocated VCs. For example, we observed that the chances of a packet being mingled as such in the BitComplement Trafﬁc at its saturation point is only 3%. We remark that FVADA has two major advantages over the DAMQ design and its improvements. First, FVADA is capable of adjusting VC allocation based on trafﬁc variation, which reduces more blocking than in DAMQ. The VC allocation of DAMQ is always ﬁxed, which is inferior when the trafﬁc distribution is uneven. For example, if the packet at the head of the VC for EAST port is blocked in some upstream router, then the entire VC is simply occupied and later packets cannot go into this VC even if the EAST port and the link are idle, wasting network resources. While FVADA takes a more ﬂexible approach by starting with ﬁxed allocation, and adjusting to varying VC occupancy. As a result, one blocking packet does not block all the packets to the same direction. Second, the overhead in managing the variable VC queue was overly expensive in DAMQ. Though later improvements could reduce the hardware implemented linked list, other overhead introduced turned out to be also prohibitive [22]. As we discussed earlier, even if we use a VC control table like the ones in [15, 23], its size must accommodate the longest queue length which is very inefﬁcient in both area and power consumption. FVADA on the other hand has a very small control table. And the VA selection logic only requires several 2:1 multiplexers. Also it can be operated in parallel with the SA stage. 3.6 AVADA In FVADA, the mapping between a VC and an output port index is ﬁxed, meaning that VC0 is always mapped to EAST, for example. An alternative design is to use an adjustable mapping between VCs and the output ports. The goal is to adapt the number of VCs to the varying trafﬁc loads in all directions. For example, VC0 can be mapped to EAST and later WEST output port. Also multiple VCs can be mapped to the same output port, depending on the trafﬁc conditions. This can provide more ﬂexibility in using Figure 2. An example of FVADA Arbitration. Scenario A: Local input port has competitors for EAST output port. Scenarios B: The buffers of downstream router at EAST side are full. suppose there is a Vx in another input port requesting also for the EAST port. In the baseline router, even if V0 wins the EAST port at t, V1 cannot win again at t + 1 because the global arbitration will grant the request to Vx to ensure fairness. While in FVADA (Figure 2(b)), V1 may still win together with Vx because V1 stores ﬂits heading to the North port. In the second scenario, suppose the EAST port downstream router is temporarily out of buffer resources. In the baseline router, as shown in Figure 2(c), both V0 and V1 will fail. Whereas in FVADA, V0 fails but V1 may still win at t + 1 because the North port downstream router may have free buffer slots. As we can see, FVADA design is able to improve the global arbitration efﬁciency by improving the fairness among the output ports instead of the VCs. This can better utilize the available resources such as idle output ports and the vacant buffer slots in downstream routers. Dynamic VC Allocation. When trafﬁc is fairly evenly distributed across the network, FVADA performs well and delivers good throughput. However, if the distribution is not always even and packets traveling in certain directions dominate, the buffer utilization efﬁciency of FVADA will be signiﬁcantly impaired because each direction has only VCs. We term this mechanism Adjustable Number of VCs with Dynamic VC Allocation, or AVADA. AVADA does not require 4 VCs per input port so that the number of VCs does not depend on the number of output ports. This can vary, as shown later in simulation results where VC number per input port varies from 2 to 4. The mapping between VCs and ports can be stored in a small table that is content addressable (CAM). Each entry corresponds to a VC number in the input port of the downstream router, and stores the mapped output port in that router, as indicated in the VC mapping table of Figure 3. During the VC selection stage, the outcome of the pre-routing will be used to lookup for an available VC from the mapping table. If found, then the VC (home VC) will be assigned to the ﬂit if it has an available slot. Otherwise, an empty VC is identiﬁed and assigned to the ﬂit. The mapping table is updated accordingly. If no empty VC can be found, then the selection logic will pick any VC that has an available slot. Here an available slot means a vacant one that follows a tail ﬂit. The mapping table is extremely small — only 12 bits are necessary for a 4-VC buffer organization since 3 bits are necessary to index each output port and empty status. 3.7 FVADA and AVADA Implementation In this section, we introduce the implementation of our proposed VC allocation mechanisms: FVADA and AVADA. We adopted the non-speculatively VA design [15] in our router. In a generic router designs, the routing unit returns an output VC for each packet. Then, the VA stage arbitrates among conﬂicting requests holding the same VC of the same input port in the next router. Our baseline VA stage is similar to ViChaR where VC assignment is performed after the arbitration stage, when the global winners are clear. This design results in simpler VA logic since it only needs to arbitrate for each output port, not each VC as in the generic case, and then assigns each global winner a VC pulled from a free-VC pool using some priority rules such as Random or FIFO. Algorithm 1 Control Logic for VC Selection in FVADA VD: home VC VF: VC with free buffer slots VO: Output VCID if (VD does not have free buffer slots and there is VF) then VO = VF; else VO = VD; end if In this paper, we adopt the concept of assigning VCs to the SA winners since there are at most 4 of them. The pipeline stages are shown in Figure 4, along with the baseline pipeline for comparison. The architecture details of the VC allocation is shown in Figure 3(b). Our router has only two stages. The ﬁrst stage includes parallelized Lookahead routing (LA), Buffer Writing (BW), Switch AlloAlgorithm 2 Control Logic for VC Selection in AVADA VD: home VC VE: VC which is currently empty VF: VC with free buffer slots VFIFO: VCID at the head of available VC queue VO: Output VCID if (there is VD and VD has empty buffer slots or there is no VF) then VO = VD; else if there is VE and VD has no free buffer slots or there is no VD then VO = VE; if there is VF and there is no VE and (no VD or VD has no free buffer slots) then VO = VF; end if else end if else VO = VFIFO; end if cation and VC Allocation (SA+VA). The second stage includes Buffer Read (BR) and Switch Transmission (ST). In the ﬁrst stage, we split VA into “VC Selection” and “VC Assignment”, and parallelize “VC Selection” with LA and SA. The SA performs typical two-stage arbitration to produce local and global winners for the output ports. The LA also performs typical pre-routing for the next router using either DOR or adaptive routing. The “VC Selection” logic, being in parallel with SA and LA does not have the knowledge of the global winners and their output ports in the next router. However, what it can do is to assume that each output port will have a winner, and prepare the VC candidates for that winner with all possible output directions it will go in the next router. For example, the “VC Selection” will use either the FVADA or AVADA algorithm to compute 4 VCs for the ﬂit that will traverse through the EAST output to the next router, assuming the ﬂit will be pre-routed to the EAST, NORTH, SOUTH and LOCAL port in the next router. Hence, once the outcomes of the SA and LA are available, the “SA Assignment” can simply choose from the VC candidates for the true winner (indicated by SA) and its true output direction (indicated by LA). Continuing the example, if there is a global winner for the EAST output port, and this ﬂit will go to the SOUTH output port in the next router, then the VC prepared for that path is used in the assignment stage. As we can see, the only dependencies are from LA and SA to “VC Assignment”. There are no dependencies among LA, SA and “VC Selection”, and hence they can be parallelized. The algorithms of the “VC Selection” logic for FVADA and AVADA are shown in Algorithm 1 and 2 respectively. The “Home VC”, or VD is determined by the ﬁxed mapFigure 4. FVADA/AVADA router pipeline design. Figure 3. Proposed VC allocation architecture. ping in FVADA, or the CAM mapping in AVADA. “VF” is derived from the buffer credit. The “VE” used in AVADA is also provided by the CAM mapping table. The “VFIFO” is the head of free-VC queue, similar to that in the baseline and ViChaR. Finally, “VO” is decided based on both the buffer fullness and VC status. The former is indicated by the credit-based ﬂow control, and the latter is provided by the AVADA’s CAM table. We now discuss the timing of different pipeline stages in Figure 4. The shaded boxes highlight the differences between the baseline and FVADA/AVADA. If DeterminedOrdered Routing( DOR) is used, then the LA circuit requires only 70ps, measured from our circuit implementation using 45nm technology. The SA stage in baseline and our design requires 280ps, versus the 740ps required by ViChaR. This is because ViChaR uses large arbiters in this stage. The control logic of VC selection is very simple, and the sizes of VC mapping table and VC queues are small. The selection logic requires 105ps and 225ps for FVADA and AVADA respectively. The logic for AVADA includes a 12-bit CAM, which is slower than FVADA’s logic but still tolerable. From those latencies, we can see that parallelizing LA, SA and VC Selection stages does not increase the stage time since SA is the bottleneck. The VC Assignment stage is different from the baseline and ViChaR, but it is only a 4:1 MUX (select one winner from four candidates). The total latency of (SA+VA) stage is 315ps, also listed in Table 4, which is much shorter than the delay in ViChaR. 3.8 Adaptive Routing We also designed our buffer management to support adaptive routing, which is used to balance the link loads and improve the network performance, or to tolerate network failures. To avoid deadlocks, we adapt a previous deadlock-free design [6] to our FVADA/AVADA. In that scheme, the VCs are divided into two classes: one employs minimum adaptive routing(AR) and the other employs DOR routing. Once packets are stored in the VC of DOR class, they can only use the VCs in the same class. For packets in the AR class, they can also pick VCs in the DOR class if the routing decision generates the same direction as DOR routing. The VCs of DOR class are used as escape VCs to break the deadlock, so at least one VC is reserved for this type of VC class. For FVADA, one more VC is necessary to apply deadlock avoidance since the other four VCs are mapped to four output ports. AVADA has more ﬂexibility in VC mapping so deadlock-free design does not require additional VCs. Previous dynamically allocated buffer designs [13, 23] use an existing VC as one of the escape VCs to break deadlock when a pre-speciﬁed time threshold is exceeded. Since FVADA/AVADA uses a small number of VCs, using a dedicated VC for escape is inefﬁcient as it is not used frequently. Our objective here is to ensure deadlock-free without sacriﬁcing the throughput. Therefore, The DOR VCs set in our design is not dedicated for escape. It can also be allocated to the packets when all the VCs of AR class are not available or the router is out of buffer resources. Once the packet is steered into the DOR class VCs, it uses DOR routing and stays in this class. This helps greatly in improving the VC and buffer utilization for small number of VCs design. The efﬁciency will be shown in section 4. We remark that DAMQ would not work well with adaptive routing algorithm unless it doubles its VC count. This is because every VC is dedicated to one direction. If only one VC is added as escape VC, it would be ﬁlled with packets going to all directions, creating many HoL blocking which can severely hurt the network performance. 4 Performance Evaluation In this section, we present simulation-based performance evaluation for our proposed FVADA/AVADA. We compare the results with the state-of-the-art generic router and dynamic VC Regulator (ViChaR) designs developed in previous researches [23]. Router Pipeline BW Stages SA(baseline) SA(FVADA/AVADA) SA (ViChaR) BR+ST Critical path delay (ps) 145 280 315 740 275 Table 4. Critical path delay FVADA/AVADA and ViChaR designs. for Component (one input port) Buffer( 20 ﬂits) SA+VA logic (Baseline) SA+VA logic(FVADA) SA+VA logic(AVADA) SA+VA logic (ViChaR) Crossbar Area (µm2 ) 1500 173 213 635 666 36335 Dynamic Power (mW ) 37.39 1.23 1.27 2.03 2.79 101.06 Static Power (µW ) 1030 6.99 7.12 33.66 24.77 326.1 Table 5. Power and area for FVADA/AVADA and ViChaR designs. 4.1 Simulation Infrastructure To model and compare different network designs, we use a cycle-accurate 2D NoC simulator Noxim [34] developed in SystemC. The simulator models all major components of the NoC. An 8×8 mesh topology with 2-stage pipelined routers are modeled in this paper. Each router has 5 ports, and each port has 4 VCs in the baseline design. Each VC is 5-ﬂit deep. The size of each ﬂit and the link bandwidth is 128 bits. The baseline, ViChaR and FVADA/AVADA are compared with equal size of buffers (5 × 4 = 20 buffer slots for each input port). Each packet consists of one head ﬂit, three body ﬂits and one tail ﬂit. The baseline router has ﬁxed number of VCs with ﬁxed VC size, but uses dynamic VC allocation. The ViChaR design used dynamically allocation buffer regulator to achieve variable number of VCs and adjustable VC depth. We tested (1) Uniform Random trafﬁc type where each node uniformly injects packets into the network with random destinations and (2) Permutation trafﬁc type, where each node has dedicated destination node. We evaluated Bit-Complement, Transpose, Tornado, Butterﬂy, Bitreversal and Shufﬂe trafﬁc. We used both DOR and deadlockfree minimum adaptive routing to measure the average network latency and saturation throughput of baseline, ViChaR and FVADA/AVADA designs. 4.2 Latency, Power and Area Estimation for the Router The main components of the router were implemented to analyze the power, area and latency overhead. An SRAM based buffer in 45nm technology node with one read/write port is modeled using CACTI [29]. There are 20 entries in the buffer for each input port, with 128 bits in each entry. A 4-input arbiter, a 20-input arbiter and a crossbar are built and simulated in HSPICE with the 45nm PTM [33] device model. For the arbiters, we utilize a typical arbiter design described in [5], which updates the priority in a round-robin manner. The 5-port, 128 bit wide crossbar is built as a matrix crossbar. The control logics of VC allocation mechanisms are also simulated in HSPICE using 45nm Technology. All circuits are simulated with 1.1V Vdd and a temperature of 90◦C. We use the πRC model for modeling the wires, and we assume a 25% activity factor in the simulation. The critical path delay of each pipeline stage of our design and ViChaR design is shown in Table 4. From Table 4, we can see the critical path delay of FVADA/AVADA is only 42.6% of ViChaR design. It is mainly because large number of VCs used by ViChaR requires complicated control logics in SA stage. The singlecycle of SA stage design for ViChaR can sustain a network frequency of no more than 1.35GHz. To obtain higher clock frequencies, the SA stage must be divided to multiple pipeline stages to accommodate smaller cycle time, such as the SA design in [20] which consists of 2 pipeline stages. We implemented a 3-stage ViChaR router that can sustain a higher frequency, e.g. 2.5GHz network. Since the critical delay of FVADA and AVADA is less than 400ps, the SA stage can complete in one cycle at 2.5GHz. We model both low frequency(1GHz) router and high frequency (2.5GHz) routers, and compare the performance among three designs. Table 5 shows the power and area estimation of major router components. As FVADA/AVADA adds the VC selection logic and use different VC assignment from baseline, they consume higher power and area than the baseline design. However, the increase is still trivial. Power increases by 0.58%/1.1% of the total router power and area increases by 0.35%/4.1% for FVADA and AVADA respectively. The VC selection of AVADA also costs a little more than FVADA. Both schemes provide dynamic power and area savings in control logic compared to ViChaR, whose overhead mainly comes from the large arbiters. 4.3 Simulation Results and Discussion For DOR, baseline and FVADA/AVADA have 4VC/port, each VC can hold 5 ﬂits, while ViChaR has equal-sized buffer slots (20 slots per port). Figure 5 plots the average ﬂit latencies for a 8 × 8 mesh topology using DOR (a-g) and Adaptive Routing, AR, (h-i) with both types of trafﬁc. Figure 5(a) and 5(b) show the average latency for uniform random (UR) and bit-complement trafﬁc (BC). Both perform well with DOR but not AR [8] since the trafﬁc by itself is fairly balanced. Local congestion information could be a disturbance and create global imbalance. For other trafﬁc types in (c)∼(g), imbalanced loads cannot be solved by DOR, and their saturation occurs earlier than in UR. As we can see, FVADA/AVADA consistently outperform the baseline in throughput. The improvement is 41% on average and 66.7% maximally, even though both of them employs statically VC number and size. It proves that FVADA/AVADA can work well either in uniform or nonuniform trafﬁc types. FVADA/AVADA have comparable performance in almost all trafﬁc patterns. FVADA achieves better throughput than ViChaR in BC by 10.5% and in Shufﬂe Trafﬁc Pattern by 4.7%. The reasons are two-fold: (1) FVADA improves the SA efﬁciency as we discussed in Sec(a) Uniform Random (b) Bit-Complement (c) Transpose (d) Tornado (e) Bit-Reversal (f) Shufﬂe (g) Butterﬂy (h) Adaptive routing for Transpose (i) Adaptive routing for Butterﬂy Figure 5. Average network latency for a 8×8 network at 1GHz under DOR and Adaptive Routing. tion 3.5. Ideally, we hope to produce winners in the ﬁrst ﬁxed VC and output port mapping. To have a fair comparistage of SA that are heading to as many output ports as son, we also increase the number of VCs in the baseline and possible to reduce the contention in the second stage. For ViChaR. FVADA/AVADA still outperforms the baseline in example, if the 4 ﬁrst-stage winners go to 4 different diterms of throughput by 52.5%/84.2%. AVADA has better rections, they are then all winners in the second stage beperformance than FVADA in adaptive routing because it has cause they can share the switch in one cycle. We collected more ﬂexible mapping. Multiple VCs can be assigned to this data for BC in Figure 6. It shows the number of outDOR class to alleviate the HoL blockings without increasput ports requested by the ﬁrst stage local winners, relative ing the total number of VCs. AVADA with 4VC/port has to the ViChaR results. We can see that FVADA wins over 2.2% less throughput than VichaR with 4VC/port due to less ViChaR in 4 output ports by 20%. This is a strong proof of VC resources. The scenario was discussed in section 3.8. the SA efﬁciency in FVADA design. Again, this is mainly For example, the buffer credits of the downstream router because VA assigns the VC based on their designated output may show that the packet should pick a direction using AR port. (2)The average path length in BC is longer than other over DOR. However, there might not be available VCs of trafﬁc types. ViChaR has a large number of VCs, which inclass AR at current node. Then, VA cannot allocate a VC troduces higher number of interleaving packets, especially for the packet and it has to wait for several cycles. If the when the path is long. The packets occupy VCs resources port select logic picks the DOR direction based on the VC throughout their routing path, increasing the average packet resources information, then it has chosen a busier path for latency. AVADA, on the other hand, cannot match the perthe packet. And once the packet enters the DOR class, it formance of FVADA in BC and Shufﬂe trafﬁc due to the dycannot come out and return to AR class VCs, which weaknamical VC mapping. Multiple VCs can be mapped to the ens the advantage of adaptive routing. For ViChaR design, same output ports, generating contention in SA. However VCs number is not a constraint factor so it does not limit AVADA has its advantages in adaptive routing as shown in the adaptive routing performance, resulting a slightly better Figure 5(h) and (i). throughput than AVADA. Figure 5(h) and (i) show the network latency under AR Figure 8 shows the average latency for higher frequency for Transpose and Butterﬂy trafﬁc. “AVADA 5”means that router design. The no-load latency of FVADA/AVADA is it has 5 VC/port etc. As we mentioned before, FVADA uses reduced by 33% on average compared to ViChaR design. It Figure 6. Number of designated output ports for local winners in FVADA and ViChaR at saturation injection rate. is mainly because the critical path delay of ViChaR is too long to ﬁt in one cycle. FVADA/AVADA still have comparable saturation throughput with ViChaR design, and even better throughput in BC and the Shufﬂe trafﬁc. Figure 8(h) and (i) show the network latency using adaptive routing. The saturation point of AVADA is able to match the ViChaR design for butterﬂy trafﬁc. Because ViChaR has a deeper pipeline under high frequency, the delay of the credit loop also increases. The latency of the backward credit path impairs the utilization of buffer resources in a router because it increases buffer idle time between uses. Increasing the buffer turnaround time hurts the network throughput [27]. Therefore, the throughput of ViChaR is now the same as the AVADA design. Finally, Figure 7 plots the latency curves for AVADA with various buffer sizes compared to ViChaR using equalsized buffers, and the baseline with ﬁxed buffer size under UR. This ﬁgure proves that our AVADA design can outperform the baseline even with half of the buffer size. We are always comparable to ViChaR under different sizes. Reducing buffer size by 50% could produce sigiﬁcant savings in both area and power, which alleviates the limitation of tight power budget in network design. 5 Conclusion We propose two new VA mechanisms, termed Fixed VC Assignment with Dynamic VC Allocation (FVADA) and Adjustable VC Assignment with Dynamic VC Allocation (AVADA) to improve the buffer utilization. VCs are assigned based on the designated output port of a packet to reduce the HoL blocking. Also, the number of VCs allocated for each output port can be adjusted dynamically according to the trafﬁc condition. A small number of VCs is used to keep the arbitration latency low. Simulation results show that for either uniform or non-uniform trafﬁc, FVADA can improve the network throughput by 41% on average, compared to a baseline design with the same buffer size using DOR. Compare to FVADA, AVADA is more suitable for adaptive routing due to its dynamic VC mapping. AVADA outperforms the baseline even when our buffer size is halved. Therefore by using small buffer sizes, we could save signiﬁcant power and area overhead and the throughput of the networks does not degrade. Moreover, we are able to achieve comparable or better throughput than a previous work ViChaR while reducing its critical path delay Figure 7. Average latency for different buffer sizes. by 57.4%. Our results prove that the proposed simple VA mechanisms are suitable for low-power, high-throughput, and high-frequency on-chip network designs. "
2011,CHIPPER - A low-complexity bufferless deflection router.,"As Chip Multiprocessors (CMPs) scale to tens or hundreds of nodes, the interconnect becomes a significant factor in cost, energy consumption and performance. Recent work has explored many design tradeoffs for networks-on-chip (NoCs) with novel router architectures to reduce hardware cost. In particular, recent work proposes bufferless deflection routing to eliminate router buffers. The high cost of buffers makes this choice potentially appealing, especially for low-to-medium network loads. However, current bufferless designs usually add complexity to control logic. Deflection routing introduces a sequential dependence in port allocation, yielding a slow critical path. Explicit mechanisms are required for livelock freedom due to the non-minimal nature of deflection. Finally, deflection routing can fragment packets, and the reassembly buffers require large worst-case sizing to avoid deadlock, due to the lack of network backpressure. The complexity that arises out of these three problems has discouraged practical adoption of bufferless routing. To counter this, we propose CHIPPER (Cheap-Interconnect Partially Permuting Router), a simplified router microarchitecture that eliminates in-router buffers and the crossbar. We introduce three key insights: first, that deflection routing port allocation maps naturally to a permutation network within the router; second, that livelock freedom requires only an implicit token-passing scheme, eliminating expensive age-based priorities; and finally, that flow control can provide correctness in the absence of network backpressure, avoiding deadlock and allowing cache miss buffers (MSHRs) to be used as reassembly buffers. Using multiprogrammed SPEC CPU2006, server, and desktop application workloads and SPLASH-2 multithreaded workloads, we achieve an average 54.9% network power reduction for 13.6% average performance degradation (multipro-grammed) and 73.4% power reduction for 1.9% slowdown (multithreaded), with minimal degradation and large power savings at low-to-medium load. Finally, we show 36.2% router area reduction relative to buffered routing, with comparable timing.","CHIPPER: A Low-complexity Bufferless Deﬂection Router Chris Fallin cfallin@cmu.edu Chris Craik craik@cmu.edu Onur Mutlu onur@cmu.edu Computer Architecture Lab (CALCM) Carnegie Mellon University Abstract As Chip Multiprocessors (CMPs) scale to tens or hundreds of nodes, the interconnect becomes a signiﬁcant factor in cost, energy consumption and performance. Recent work has explored many design tradeoffs for networks-on-chip (NoCs) with novel router architectures to reduce hardware cost. In particular, recent work proposes bufferless deﬂection routing to eliminate router buffers. The high cost of buffers makes this choice potentially appealing, especially for lowto-medium network loads. However, current bufferless designs usually add complexity to control logic. Deﬂection routing introduces a sequential dependence in port allocation, yielding a slow critical path. Explicit mechanisms are required for livelock freedom due to the non-minimal nature of deﬂection. Finally, deﬂection routing can fragment packets, and the reassembly buffers require large worst-case sizing to avoid deadlock, due to the lack of network backpressure. The complexity that arises out of these three problems has discouraged practical adoption of bufferless routing. To counter this, we propose CHIPPER (Cheap-Interconnect Partially Permuting Router), a simpliﬁed router microarchitecture that eliminates in-router buffers and the crossbar. We introduce three key insights: ﬁrst, that deﬂection routing port allocation maps naturally to a permutation network within the router; second, that livelock freedom requires only an implicit token-passing scheme, eliminating expensive age-based priorities; and ﬁnally, that ﬂow control can provide correctness in the absence of network backpressure, avoiding deadlock and allowing cache miss buffers (MSHRs) to be used as reassembly buffers. Using multiprogrammed SPEC CPU2006, server, and desktop application workloads and SPLASH-2 multithreaded workloads, we achieve an average 54.9% network power reduction for 13.6% average performance degradation (multiprogrammed) and 73.4% power reduction for 1.9% slowdown (multithreaded), with minimal degradation and large power savings at low-to-medium load. Finally, we show 36.2% router area reduction relative to buffered routing, with comparable timing. 1. Introduction In recent years, NoCs have become a focus of intense interest in computer architecture. Moore’s Law compels us toward larger multicore processors. As tiled CMPs [41, 4, 27, 2] are adopted, on-chip interconnect becomes critically important. Future tiled CMPs will likely contain hundreds of cores [42, 22, 6], and one current chip already contains 100 cores [51]. At this density, a commonly proposed on-chip interconnect is the 2D mesh: it maps naturally to the tiled CMP architecture [2] and allows for simple routing algorithms and low-radix router architectures. Traditionally, interconnection network designs have been motivated by and tuned for large, high performance multiprocessors [30, 9]. As interconnects migrate to the on-chip environment, constraints and tradeoffs shift. Power, die area and design complexity become more important, and link latencies become smaller, making the effects of router latency more pronounced. As a consequence, any competitive router design should have a short critical path, and should simultaneously minimize logic and buffer footprint. Low-cost NoC designs have thus become a strong focus. In particular, one line of recent work has investigated how to eliminate in-router buffers altogether [38, 19, 16], or minimize them with alternative designs [25, 26, 39]. The completely bufferless designs either drop [19, 16] or misroute (deﬂect) [38] ﬂits when contention occurs. Eliminating buffers is desirable: buffers draw a signiﬁcant fraction of NoC power [21] and area [17], and can increase router latency. Moscibroda and Mutlu [38] report 40% network energy reduction with minimal performance impact at low-to-medium network load. For a design point where interconnect is not highly utilized, bufferless routers can yield large savings. Bufferless deﬂection routing thus appears to be promising. However, that work, and subsequent evaluations [36, 19], note several unaddressed problems and complexities that signiﬁcantly discourage adoption of bufferless designs. First, a long critical path in port allocation arises because every ﬂit must leave the router at the end of the pipeline, and because deﬂection is accomplished by considering ﬂits sequentially. Second, livelock freedom requires a priority scheme that is often more complex than in buffered designs: for example, in Oldest-First arbitration, every packet carries a timestamp, and a router must sort ﬂits by timestamps. Finally, packet fragmentation requires reassembly buffers, and without additional mechanisms, worst-case sizing is necessary to avoid deadlock [36]. In this paper, we propose a new bufferless router architecture, CHIPPER, that solves these problems through three key insights. First, we eliminate the expensive port allocator and the crossbar, and replace both with a permutation network; as we argue, deﬂection routing maps naturally to this arrangement, reducing critical path length and power/area cost. Second, we provide a strong livelock guarantee through an implicit token passing scheme, eliminating the cost of a traditional priority scheme. Finally, we propose a simple ﬂow control mechanism for correctness with reasonable reassembly buffer sizes, and propose using cache miss buffers (MSHRs [29]) as reassembly buffers. We show that at low-tomedium load, our reduced-complexity design performs competitively to a traditional buffered router (in terms of application performance and operational frequency) with significantly reduced network power, and very close to baseline bufferless (BLESS [38]) with a reduced critical path. • Cheap deﬂection routing by replacing the allocator and crossbar with a partial permutation network. This design parallelizes port allocation and reduces hardware cost signiﬁcantly. • A strong livelock guarantee through implicit token passing, called Golden Packet (GP). By replacing the OldestFirst scheme for livelock freedom [38], GP preserves livelock freedom while eliminating the need to assign and compare timestamps. • A ﬂow-control scheme, Retransmit-Once, that avoids worst-case reassembly buffer sizing otherwise necessary to avoid deadlock. Use of MSHRs as reassembly buffers, allowing packet fragmentation due to deﬂection routing without incurring additional buffering cost. • Evaluation over multiprogrammed SPEC CPU2006 [49] and assorted desktop and server (web search, TPC-C) applications, and multithreaded SPLASH-2 [57] workloads, showing minimal performance degradation at lowto-medium network load with signiﬁcant power, area and complexity savings. Our contributions are: 978-1-4244-9435-4/11/$26.00 ©2011 IEEE  144   2. Bufferless Deﬂection Routing 2.1. Why Bufferless? (and When?) Bufferless1 NoC design has recently been evaluated as an alternative to traditional virtual-channel buffered routers [38, 19, 16, 31, 52]. It is appealing mainly for two reasons: reduced hardware cost, and simplicity in design. As core count in modern CMPs continues to increase, the interconnect becomes a more signiﬁcant component of system hardware cost. Several prototype manycore systems point toward this trend: in MIT RAW, interconnect consumes ∼40% of system power; in the Intel Terascale chip, 30%. Buffers consume a signiﬁcant portion of this power. A recent work [38] reduced network energy by 40% by eliminating buffers. Furthermore, the complexity reduction of the design at the high level could be substantial: a bufferless router requires only pipeline registers, a crossbar, and arbitration logic. This can translate into reduced system design and veriﬁcation cost. Bufferless NoCs present a tradeoff: by eliminating buffers, the peak network throughput is reduced, potentially degrading performance. However, network power is often signiﬁcantly reduced. For this tradeoff to be effective, the power reduction must outweigh the slowdown’s effect on total energy. Moscibroda and Mutlu [38] reported minimal performance reduction with bufferless when NoC is lightly loaded, which constitutes many of the applications they evaluated. Bufferless NoC design thus represents a compelling design point for many systems with low-to-medium network load, eliminating unnecessary capacity for signiﬁcant savings. 2.2. BLESS: Baseline Bufferless Deﬂection Routing Here we brieﬂy introduce bufferless deﬂection routing in the context of BLESS [38]. BLESS routes ﬂits, the minimal routable units of packets, between nodes in a mesh interconnect. Each ﬂit in a packet contains header bits and can travel independently, although in the best case, all of a packet’s ﬂits remain contiguous in the network. Each node contains an injection buffer and a reassembly buffer; there are no buffers within the network, aside from the router pipeline itself. Every cycle, ﬂits that arrive at the input ports contend for the output ports. When two ﬂits contend for one output port, BLESS avoids the need to buffer by misrouting one ﬂit to another port. The ﬂits continue through the network until ejected at their destinations, possibly out of order, where they are reassembled into packets and delivered. Deﬂection routing is not new: it was ﬁrst proposed in [3], and is used in optical networks because of the cost of optical buffering [8]. It works because a router has as many output links as input links (in a 2D mesh, 4 for neighbors and 1 for local access). Thus, the ﬂits that arrive in a given cycle can always leave exactly N cycles later, for an N -stage router pipeline. If all ﬂits request unique output links, then a deﬂection router can grant every ﬂit’s requested output. However, if more than one ﬂit contends for the same output, all but one must be deﬂected to another output that is free. 2.2.1. Livelock Freedom Whenever a ﬂit is deﬂected, it moves further from its destination. If a ﬂit is deﬂected continually, it may never reach its destination. Thus, a routing algorithm must explicitly avoid livelock. It is possible to 1More precisely, a “bufferless” NoC has no in-router (e.g., virtual channel) buffers, only pipeline latches. Baseline bufferless designs, such as BLESS [38], still require reassembly buffers and injection queues. As we describe in § 4.3, we eliminate these buffers as well. probabilistically bound network latency in a deﬂection network [28, 7]. However, a deterministic bound is more desirable. BLESS [38] uses an Oldest-First prioritization rule to give a deterministic bound on network latency. Flits arbitrate based on packet timestamps. Prioritizing the oldest trafﬁc creates a consistent total order and allows this trafﬁc to make forward progress. Once the oldest packet arrives, another packet becomes oldest. Thus livelock freedom is guaranteed inductively. However, this age-based priority mechanism is expensive [36, 19] both in header information and in arbitration critical path. Alternatively, some bufferless routing proposals do not provide or explicitly show deterministic livelock-freedom guarantees [19, 16, 52]. This can lead to faster arbitration if it allows for simpler priority schemes. However, a provable guarantee of livelock freedom is necessary to show system correctness in all cases. 2.2.2. Injection BLESS guarantees that all ﬂits entering a router can leave it, because there are at least as many output links as input links. However, this does not guarantee that new trafﬁc from the local node (e.g., core or shared cache) can always enter the network. A BLESS router can inject a ﬂit whenever an input link is empty in a given cycle. In other words, BLESS requires a “free slot” in the network in order to insert new trafﬁc. When a ﬂit is injected, it contends for output ports with the other ﬂits in that cycle. Note that the injection decision is purely local: that is, a router can decide whether to inject without coordinating with other routers. 2.2.3. Ejection and Packet Reassembly A BLESS router can eject one ﬂit per cycle when that ﬂit reaches its destination. In any bufferless deﬂection network, ﬂits can arrive in random order; therefore, a packet reassembly buffer is necessary. Once all ﬂits in a packet arrive, the packet can be delivered to the local node. Importantly, this buffer must be managed so that it does not overﬂow, and in such a way that maintains correctness. The work in [38] does not consider this problem in detail. Instead, it assumes an inﬁnite reassembly buffer, and reports maximum occupancies for the evaluated workloads. We will return to this point in § 3.3. 3. Deﬂection Routing Complexities While bufferless deﬂection routing is conceptually and algorithmically simple, a straightforward hardware implementation leads to numerous complexities. In particular, two types of problem plague baseline bufferless deﬂection routing: high hardware cost, and unaddressed correctness issues. The hardware cost of a direct implementation of bufferless deﬂection routing is nontrivial, due to expensive control logic. Just as importantly, correctness issues arise in the reassembly buffers when they have practical (non-worst-case) sizes, and this fundamental problem is unaddressed by current work. Here, we describe the major difﬁculties: output port allocation, expensive priority arbitration, and reassembly buffer cost and correctness. Prior work cites these weaknesses [36, 19]. These problems directly lead to the three key insights in CHIPPER. We will describe in each in turn. 3.1. Output Port Allocation Deﬂection-routing consists of mapping a set of input ﬂits, each with a preferred output and some priority, to outputs such that every ﬂit obtains an output. This computation is fundamentally difﬁcult for two reasons: (i) every non-ejected input ﬂit must take some output, since ﬂits are never buffered 145 	  	 			 	                       		  	   	 	 	   	 	 	 	 	     (a) Buffered router port allocator (b) Bufferless deﬂection router port allocator Figure 1: Port allocator structures: deﬂection routing requires more complex logic with a longer critical path. or dropped; and (ii) a higher-priority ﬂit might take a lowerpriority ﬂit’s preferred output, so the routing for a given ﬂit involves an inherently sequential dependence on all higherpriority ﬂits’ routing decisions (as noted in [36] and [19]). In other words, the routing decision depends on the earlier port allocations; furthermore, the notion of “earlier” depends on the sorted ordering of the inputs. Thus, ﬂits must be sorted by priority before port allocation. A carry-select-like parallel optimization [19] can reduce the critical path by precomputing deﬂections (e.g., for all possible permutations of ﬂit priorities), but the sequential dependence for ﬁnal port allocation remains a problem, and the area and power cost of the redundant parallel computations is very high with this scheme. Fig. 1 shows a high-level comparison of the buffered and bufferless port allocation problems. In a traditional buffered router, each output port can make its arbitration decision independently: multiple ﬂits request that output port, the port arbiter chooses one winner, and the remaining ﬂits stay in their queues. In contrast, port allocation is inherently more difﬁcult in a bufferless deﬂection router than in a buffered router, because the decision is global over all outputs. The algorithm requires that we obey priority order, and so ﬂits must pass through a sort network before allocating ports. Then, port allocation occurs sequentially for each ﬂit in priority order. Because ﬂits that lose arbitration deﬂect to other ports, lower-priority ﬂits cannot claim outputs until the deﬂection is resolved. Thus, the port allocator for each ﬂit must wait for the previous port allocator. The sequential dependence creates a long critical path; the worst case, in which all ﬂits contend for one port, limits router speed. Finding a full permutation with deﬂections, in a bufferless router, has inherently less parallelism, and more computation, than port allocation in a buffered router. 3.2. Priority Arbitration The priority arbitration problem – computing a priority order on incoming ﬂits – also becomes more costly in a bufferless deﬂection network. In particular, the network must explicitly avoid livelock through careful design of its priority scheme. One option (used in [38]) is an Oldest-First priority scheme to guarantee ﬂit delivery: the oldest ﬂit will always make forward progress, and once it is delivered, another ﬂit becomes the oldest. However, this scheme requires an age ﬁeld in every packet header, and the ﬁeld must be wide enough to cover the largest possible in-ﬂight window. The arbiter then needs to sort ﬂits by priorities in every cycle. A bitonic sort network [5] can achieve this sort in three stages for 4 ﬂits. Unfortunately, this is a long critical path in a high-speed router, especially when combined with the port allocator; alternately, pipelining the computation yields a longer router latency, impacting performance signiﬁcantly. 3.3. Reassembly Buffers A bufferless deﬂection network must provide buffer space at network entry and exit: injection and reassembly buffers, respectively. Injection buffers are necessary because injection can only occur when there is an empty slot in the network, so new trafﬁc must wait its turn; reassembly buffers are needed because deﬂection routing may fragment packets in transit. Injection buffers pose relatively little implementation difﬁculty: a node (e.g., a core or a shared cache) can stall when its injection FIFO ﬁlls, and can generate data on demand (e.g., from the cache, in the case of a cache-miss request). However, reassembly buffers lead to correctness issues that, without a more complex solution, yield large worst-case space requirements to avoid deadlock. Worst-case sizing is impractical for any reasonable design; therefore, this is a fundamental problem with bufferless deﬂection networks that must be solved at the algorithmic level. Despite the fundamental nature of this problem in deﬂection routing, management of reassembly buffer space has not yet been considered in existing deﬂection-routed NoCs. BLESS [38] assumes inﬁnite buffers, and then gives data for maximum reassembly buffer occupancy. In a real system, buffers are ﬁnite, and overﬂow must be handled. Michelogiannakis et al. [36] correctly note that in the worst case, a reassembly buffer must be sized to reassemble all packets in the system simultaneously. To see why this is the case, observe the example in Fig. 2. Assume a simple reassembly-slot allocation algorithm that assigns space as ﬂits arrive, and frees a packet’s slot when reassembly is completed. The key observation is that a bufferless deﬂection network has no ﬂow control: whereas in a buffered network, credits ﬂow upstream to indicate free downstream buffer space (for both in-network buffers and ﬁnal endpoint reception), nodes in a bufferless network are free to inject whenever there is a free outbound link at the local node. Thus, a reassembly buffer-full condition is not transmitted to potential senders, and it is possible that many packets are sent to one destination simultaneously. When all packets are sent to this single node (e.g., Node 0), the ﬁrst several ﬂits to arrive will allocate reassembly slots for their packets. Once all slots are taken, ﬂits from other packets must remain in the network and deﬂect until the ﬁrst packets are reassembled. Eventually, this deﬂecting trafﬁc will ﬁll the network, and block further injections. If some ﬂits constituting the partially reassembled 146 &)	%	 	  				*		 	"" ')		*	* 						 					 	"" !)	+(	*	 						 	""	  				,	(		 					 .)	-		 				*/0  				*	1)-)	%2 					 %	 	   	"" 	  %  %"" % %!  ""  &  ' "" ' "" & ' ! 	 	  		  	&  	'  	! # $ ! # $ ! # $ %& 	 	 %&		 	(	 	""  		 	 & "" & ' "" '  		     Figure 2: Deadlock due to reassembly-buffer overﬂow. packets ﬂits have not been injected yet (e.g., packet A), deadlock results. We have observed such deadlock for reasonable reassembly buffer sizes (up to 4 packet slots) in realistic workloads of network-intensive applications. Fundamentally, this deadlock occurs because of a lack of backpressure (i.e., buffer credits) in the network. In other words, reassembly buffers have no way to communicate to senders that they are full, and so the only way to avoid oversubscription is worst-case provisioning. A bufferless network provides backpressure only in local injection decisions [23] – i.e., when the network is locally busy, a node cannot inject – which is not sufﬁcient to prevent deadlock, as we just argued. To build an effective bufferless deﬂection NoC, we must guarantee correctness with a reasonable reassembly buffer size. As argued above, the na¨ıve locally-controlled buffer allocation leads to worst-case sizing, which can signiﬁcantly reduce the area and energy beneﬁts of bufferless routing. Because reassembly buffers are a necessary mechanism for deﬂection routing, and because the lack of ﬂow control might allow deadlock unless buffers are unreasonably large, we consider the reassembly-buffer problem to be fundamentally critical to correctness, just as efﬁcient port allocation and priority arbitration are fundamental to practicality. These three complexities directly motivate the key insights and mechanisms in our new router, CHIPPER. 4. CHIPPER: Mechanisms As we have seen, bufferless deﬂection routing introduces several complexities that are not present in traditional buffered networks. Here, we introduce CHIPPER (Cheap-Interconnect Partially Permuting Router), a new router microarchitecture based on the key insight that these complexities are artifacts of a particular formulation of deﬂection routing, rather than fundamental limitations. By introducing a new architecture based on a permutation network, and two key algorithms, Golden Packet and Retransmit-Once, we provide a feasible implementation of bufferless deﬂection routing. More details are available in a technical report [15]. 4.1. Permutation Network Deﬂection Routing Section 3.1 describes how deﬂection routing can lead to inefﬁcient port allocation, because of the sequential dependence that deﬂection implies. We observe that sequential port allocation is not necessary for ensuring mutual exclusion on output ports. Instead, the deﬂection-routing problem can map to a permutation network. A network composed of 2x2 arbiter blocks that either pass or swap their arguments will implicitly give a 1-to-1 mapping of inputs to outputs. In other words, if we assign the outputs of a permutation network to the output ports of a router, mutual exclusion naturally arises when ﬂits contend for an output port, because at the ﬁnal stage, only one ﬂit can take the output. This idea leads to a completely new router organization. Fig. 3 shows the high-level CHIPPER router architecture. The pipeline contains two stages: eject/inject (parts (a), (b), (c), described in § 4.1.1 below) and permute. As shown, the permutation network replaces the control and data paths in the router: there is no crossbar, as each ﬂit’s data payload travels with the header bits through the permutation network. This leads to a more area-efﬁcient design. A permutation network directs deﬂected ﬂits to free ports in an efﬁciently parallelized way. However, obeying priority order in port allocation still requires a sequential allocator. To eliminate this problem, we relax the problem constraints: we require only that the highest-priority ﬂit obtains its request. As we will argue below (in § 4.2), this constraint is sufﬁcient for livelock freedom. This also allows the permutation network to have a simpler design (with fewer stages) that gives only partial permutability.2 The design is fully connected: if there is only one input ﬂit, it can route from any input to any output. However, the single crossover limits the possible turn conﬁgurations. Note that the assignments of input ports relative to output ports is “twisted”: N and E are paired on input, while N and S are paired on output. This arrangement is due to the observation that straight-through router traversals (N ⇐⇒ S, or E ⇐⇒ W ) are more common than turns.3 Our second insight is that the priority sort and port allocation steps can be combined in the permutation network. (However, note that the permutation network does not need to perform a full sort, because we only need to determine the highest-priority ﬂit.) The key to this arrangement is in the steering function of each 2x2 arbiter block: ﬁrst, a priority comparison determines a winning ﬂit; then, the winning ﬂit 2While this increases deﬂection rate, we show in our evaluations in § 5 that the impact in the common case is minimal. The critical-path and simplicity savings thus outweigh this cost. 3 [25] also makes use of this observation to obtain a cheap microarchitecture in a different way. 147       	  	   	  	 	   	   		        !		"" #$""""%	""& """"""""		&""' ($""""%	"" """"""""	""	 )$""""	"" """"""""	*""	 Figure 3: CHIPPER architecture: a permutation network replaces the traditional arbitration logic and crossbar. picks the output that leads to its desired port. The losing ﬂit, if present, takes the other output. This design preserves priority enforcement at least for the highest-priority ﬂit, since this ﬂit will always be a winning ﬂit. In the highest-contention case, when all four ﬂits request the same output, the arbiter becomes a combining tree. In the case where every ﬂit requests a different output, the number of correct assignments depends only on the permutability of the arbiter. 4.1.1. Injection and Ejection We must now consider injection and ejection in this arbiter. So far, we have assumed four input ports and four output ports, without regard to the ﬁfth, local, router port. We could extend the permutation network to a ﬁfth input and output. However, this has two disadvantages: it is not a power-of-two size, and so is less efﬁcient in hardware cost; and more importantly, the local port has slightly different behavior. Speciﬁcally, the ejection port can only accept a ﬂit destined for the local node, and injection can only occur when there is a free slot (either because of an empty input link or because of an ejection). We instead handle ejection and injection as a separate stage prior to the arbitration network, as shown in Fig. 3. We insert two units, the ejector and the injector, in the datapath. This allows the stages to insert and remove ﬂits before the set of four input ﬂits reaches the arbiter. The ejector recognizes locally-destined ﬂits, and picks at most one through the ejector tree (part (a) in the ﬁgure). The ejector tree must respect the priority scheme, but as we will argue in the next section, our Golden Packet scheme is very cheap. When a ﬂit is chosen for ejection, the tree directs it to the local ejection port, and the ejector kill logic (part (b)) removes it from the pipeline. Finally, when a ﬂit is queued for injection, the injector ﬁnds an empty input link (picking one arbitrarily if multiple are available, not shown in the ﬁgure for simplicity) and directs the local injection port to this link via the injector muxes (part (c) in the ﬁgure). The resulting ﬂits then progress down the pipeline into the permute stage. As we note in § 5.7, the router’s critical path is through the permutation network; thus, the eject/inject stage does does not impact the critical path. 4.2. Golden Packet: Cheap Priorities So far, we have addressed the port allocation problem. An efﬁcient priority scheme forms the second half of a cheap router. In our design, each 2x2 arbiter block must take two ﬂits and decide the winner. The Oldest-First priority scheme used by BLESS [38] decides this with an age comparison (breaking ties with other ﬁelds). However, this is expensive, because it requires a wide age ﬁeld in the packet header, and large comparators in the arbiter. We wish to avoid this expense, even if it may sacriﬁce a little performance. We start with the explicit goal of preserving livelock freedom, while stripping away anything unnecessary for that property. We observe that it is sufﬁcient to pick a single packet, and prioritize that packet globally above all other packets for long enough that its delivery is ensured. If every packet in the system eventually receives this special status, then every packet will eventually be delivered. This constitutes livelock freedom. We call this scheme, which prioritizes a single packet in the system, Golden Packet. We will introduce Golden Packet, or GP, in two pieces. First, GP provides a set of prioritization rules that assume the golden packet is already known. Then, GP deﬁnes an implicit function of time that rotates through all possible packets to deﬁne which is golden. Ruleset 1 Golden Packet Prioritization Rules Golden Tie: If two ﬂits are golden, the lower-numbered ﬂit (ﬁrst in the golden packet) wins. Golden Dominance: If one ﬂit is golden, it wins over any non-golden ﬂit. Common Case: Contests between two non-golden ﬂits are decided pseudo-randomly. 4.2.1. Prioritization Rules The GP prioritization rules are given in Ruleset 1. These rules are designed to be very simple. If a ﬂit header already contains a bit indicating golden status, then a GP arbiter requires only a comparator as wide as the ﬂit sequence number within a packet – typically 2 or 3 bits – and some simple combinational logic to handle the two-golden-ﬂit4 , one-golden-ﬂit and the most common nogolden-ﬂit cases. These rules guarantee delivery of the golden packet: the golden packet always wins against other trafﬁc, and in the rare case when two ﬂits of the golden packet contend, the Golden Tie rule prioritizes the earlier ﬂit using its in-packet sequence number. However, since most packets are not golden, the Common Case (random winner) rule will be invoked most often. Thus, Golden Packet reduces critical path by requiring a smaller comparator, and reduces dynamic power by using that comparator only for the golden packet. 4 The two-golden-ﬂit case is only possible when two ﬂits from the single Golden Packet contend, which happens only if some ﬂits in the packet had been deﬂected before becoming golden while in ﬂight: once the packet is golden, no other trafﬁc can cause its ﬂits to deﬂect. 148          4.2.2. Golden Sequencing We must specify which packet in the system is golden. All arbiters must have this knowledge, and must agree, for the delivery guarantee to work. This can be accomplished by global coordination, or by an implicit function of time computed at each router. We use the latter approach for simplicity. We deﬁne a golden epoch to be L cycles long, where L is at least as large as the maximum latency for a golden packet, to ensure delivery. (This upper bound is precisely the maximum Manhattan distance times the hop latency for the ﬁrst ﬂit, and one more hop latency for each following ﬂit, since the Golden Packet will never be misrouted and thus will take a minimal path.) Every router tracks golden epochs independently. In each golden epoch, either zero or one packet is golden. The golden packet ID rotates every epoch. We identify packets by (sender, transaction ID) tuples (in practice, the transaction ID might be a sender MSHR number). We assume that packets are uniquely identiﬁable by some such tuple. We rotate through all possible tuples in a static sequence known to all routers, regardless of packets actually in the network. This sequence nevertheless ensures that every packet is eventually golden, if it remains in the network long enough, thereby ensuring its delivery. The golden sequence is given in Algorithm 2 as a set of nested loops. In practice, if all loop counts are powers of two, a router can locally determine the currently-chosen golden packet by examining bitﬁelds in a free-running internal counter. In our design, routers determine the golden-status of a packet in parallel with route computation. This check is lightweight: it is only an equality test on packet ID. Note that packets must be checked at least once per epoch while in transit, because they may become golden after injection. However, the check can be done off the critical path, if necessary, by checking at one router and forwarding the result in a header bit to the next router. Algorithm 2 Golden Epoch Sequencing (implicit algorithm at each router) for t in Nt xn id s do for n in Nnod es do packet from transaction id t sent from node n is golden for L cycles while true do end for end for end while 4.3. Retransmit-Once: Flow Control for In-MSHR Buffering As we motivated in § 3, reassembly buffers pose significant problems for bufferless deﬂection networks. In particular, because there is no feedback (backpressure [23]) to senders, correct operation requires that the buffers are sized for the worst case, which is impractical. However, a separate mechanism that avoids buffer overﬂow can enable the use of a much smaller reassembly space. Along with this insight, we observe that existing memory systems already have buffer space that can be used for reassembly: the cache miss buffers (MSHRs [29] and shared cache request queues/buffers). In fact, the cache protocol must already allocate from a ﬁxed pool of request buffers at shared cache nodes and handle the buffers-full case; thus, our ﬂow control solution uniﬁes this protocol mechanism with network-level packet reassembly. 4.3.1. Integration with Caches: Request Buffers In a typical cache hierarchy, buffering exists already in order to sup149 port cache requests. At private (L1) caches, MSHRs [29] (miss-status handling registers) track request status and buffer data as it is read from or written to the cache data array. This data buffer is ordinarily accessible at the bus-width granularity in order to transfer data to and from the next level of hierarchy. Similarly, at shared (L2) cache banks, an array of buffers tracks in-progress requests. These buffers hold request state, and also contain buffering for the corresponding cache blocks, for the same reasons as above. Because both L1 and L2-level buffers are essentially the same for ﬂow control purposes, we refer to both as “request buffers” in this discussion. We observe that because these request buffers already exist and are accessible at single-ﬂit granularity, they can be used for reassembly and injection buffering at the NoC level. By considering the cache hierarchy and NoC designs together, we can eliminate the redundancy inherent in separate NoC reassembly and injection buffers. In particular, an injection queue can be constructed simply by chaining MSHRs together in injection order. Similarly, a reassembly mechanism can be implemented by using existing data-steering logic in the MSHRs to write arriving ﬂits to their corresponding locations, thereby reassembling packets (cache blocks) in-place. By implementing separate injection and reassembly buffers in this way, we can truly call the network bufferless. 4.3.2. Flow Control: Retransmit-Once The lack of ﬂow control in a bufferless deﬂection network can lead to deadlock in worst-case situations. We showed in § 3.3 that deadlock occurs when reassembly buffers (or request buffers) are all allocated and additional trafﬁc requires more buffers. § 3.3 shows that a simple mechanism to handle overﬂow based on local router decisions can lead to deadlock. Therefore, an explicit ﬂow control mechanism is the most straightforward solution to allow for non-worst-case buffering. The design space for managing request buffers is characterized by two design extremes. First, a ﬂow control scheme could require a sender to obtain a buffer reservation at a receiver before sending a packet that requires a request buffer. This scheme can be implemented cheaply as a set of counters that track reservation tokens. However, reservation requests are now on the critical path for every request. Alternately, a ﬂow control scheme could operate opportunistically: it could assume that a buffer will always be available without a reservation, and recover in the uncommon case when this assumption fails. For example, a receiver might be allowed to drop a request or data packet when it does not have an available buffer. The system can then recover either by implementing retransmit timeouts in senders or by sending a retransmit request from receiver to sender (either immediately or when the space becomes available). This scheme has no impact on the critical path when a request buffer is available. However, recovery imposes additional requirements. In particular, senders must buffer data for possible retransmission, and possibly wait for worst-case timeouts. Instead, we propose a hybrid of these two schemes, shown in Fig. 4, called Retransmit-Once. The key idea is that the opportunistic approach can be used to establish a reservation on a request buffer, because the sender can usually regenerate the initial request packet easily from its own state. The remainder of the transaction then holds this reservation, removing the need to retransmit any data. This combination attains the main advantage of the opportunistic scheme – zero critical-path overhead in the common case – while also removing retransmit-buffering overhead in most cases. In other words, there is no explicit retransmit buffer, because only the initial request packet can be dropped and the contents of this packet is implicitly held by the sender.     ( + % 	 	   	   # 	 			 &  	  	 ' ) 		 	         	   ! 	  	         ""# $		    		 	  * ,         Figure 4: Retransmit-Once ﬂow control scheme. We will examine the operation of Retransmit-Once in the context of a simple transaction between a private L1 cache (the requester) and a shared L2 cache slice (the receiver). The L1 requests a cache block from the L2; the L2 sends the data, and then, after performing a replacement, the L1 sends a dirty writeback to the L2 in another data packet. Request buffers are needed at both nodes. However, the requester (L1) initiates the request, and so it can implicitly allocate a request buffer at its own node (and stall if no buffer is available). Thus, we limit our discussion to the request buffer at the receiver (L2). Note that while we discuss only a simple threepacket transaction, any exchange that begins with a single-ﬂit request packet can follow this scheme5 . In the common case, a request buffer is available and the opportunistic assumptions hold, and Retransmit-Once has no protocol overhead. The scheme affects operation only when a buffer is unavailable. Such an example is shown in Fig. 4. The L1 (sender) initially sends a single-ﬂit request packet to the L2 (receiver), at 1 . The packet has a special ﬂag, called the start-bit, set to indicate that the transaction requires a new request buffer (this can also be implicit in the packet type). In this example, the L2 has no request buffers available, and so is forced to drop the packet at 2 . It records this drop in a retransmit-request queue: it must remember the sender’s ID and the request ID (e.g., MSHR index at the sender) in order to initiate a retransmit of the proper request. (In practice, this queue can be a bitﬁeld with one bit per private MSHR per sender, since ordering is not necessary for correctness.) Some time later, a buffer becomes available, because another request completes at 3 . The receiver (L2) then ﬁnds the next retransmit in its retransmit-request queue, and sends a packet to initiate the retransmit. It also reserves the available buffer resource for the sender (L1). This implies that only one retransmit is necessary, because the request buffer is now guaranteed to be reserved. The L1 retransmits its original request packet from its request state at 4 . The L2’s request buffer is reserved for the duration of the request at 5 , and the transaction continues normally: the L2 processes the request and sends a data response packet. Finally, in this example, the L1 sends a dirty writeback packet. This last packet has a special end-bit set that releases the request buffer reservation. Importantly, during the remainder of the sequence, the L1 never needs to retransmit, because the L2 has reserved a request buffer with reassembly space. Thus, no retransmit buffering is necessary. Accordingly, when the L1 sends its dirty writeback, it can free all resources associated with the transaction at 6 , because of this guarantee. Algorithms 3 and 4 specify ﬂow-control behavior at the receiver for the ﬁrst and last packets in a transaction. A counter tracks available buffers. 4.3.3. Interaction with Golden Packet Finally, we note that Retransmit-Once and Golden Packet coexist and mutually maintain correctness because they operate at different protocol levels. Golden Packet ensures correct ﬂit delivery without livelock. Retransmit-Once takes ﬂits that are delivered, and provides deadlock-free packet reassembly and request buffer management, regardless of how ﬂit delivery operates and despite the lack of backpressure. In particular, Golden Packet always dictates priorities at the network router level: a packet that has a reserved buffer slot destination is no different from any other packet from the router’s point of view. In fact, golden ﬂits may contend with ﬂits destined to reserved buffer spaces, and cause them to be deﬂected or to not be ejected in some cycle. However, correctness is not violated, because the deﬂected ﬂits will eventually be delivered (as guaranteed by Golden Packet) and then reassembled (by Retransmit-Once). Algorithm 3 Receiving a packet with the start-bit if sl ot s > 0 then sl ot s ⇐ sl ot s − 1 allocate buffer slot and return else end if set retransmit bit for sender (nod e, t ransact ionID) drop packet Algorithm 4 Receiving a packet with the end-bit if pending retransmits then send retransmit request indicated by next set retransmit bit sl ot s ⇐ sl ot s + 1 else end if 5. Evaluation Our goal is to build a cheap, simple bufferless deﬂection router while minimally impacting performance for our target, low-to-medium-load applications. We evaluate two basic metrics: performance (application-level, network-level, and operational frequency), and hardware cost (network power and area). We compare CHIPPER to a traditional buffered NoC, as well as a baseline bufferless NoC, BLESS [38]. We will show performance results from simulation, and hardware cost results (including per-workload power) from RTL synthesis of BLESS and CHIPPER models, as well as ORION [55]. 5.1. Methodology 5 For more complex protocols that may send a data packet to a third party (e.g., more complex cache mapping schemes where writebacks may go to different L2 banks/slices than where the replacement data came from), a separate control packet can make a reservation at the additional node(s) in parallel to the critical path of the request. We evaluate our proposed design using an in-house cycleaccurate simulator that runs both multiprogrammed and multithreaded workloads. For multiprogrammed runs, we collect instruction traces from SPEC CPU2006 [49] applications, 150 	   Parameter System topology Core model Private L1 cache Shared L2 cache Coherence protocol Interconnect Links Baseline buffered router Baseline BLESS router Setting 8x8 mesh, dense conﬁguration (core + shared cache at every node); 4x4 for multithreaded Out-of-order x86, 128-entry instruction window, 16 MSHRs 64 KB, 4-way associative, 64-byte block size perfect (always hits), distributed (S-NUCA [24]), 16 request buffers (reassembly/inject buffers) per slice Simple directory-based, based on SGI Origin [30], perfect directory 1-cycle latency, 128-bit ﬂit width (4 ﬂits per cache block) 2-cycle latency, 4 VCs/channel, 8 ﬂits/VC 2-cycle latency, FLIT-BLESS [38] Table 1: System parameters used in our evaluation. as well as several real-world desktop and server applications (including two commercial web-search traces). We use PinPoints [43] to select representative phases from each application, and then collect instruction traces using a custom Pintool [32]. For multithreaded workloads, we collect instruction traces from SPLASH-2 [57], annotated with lock and barrier information to retain proper thread synchronization. Power, area and timing cost results come from hardware models, described in § 5.7. Power results in this section are based on cycle-accurate statistics from workload simulations and represent total network power, including links. Each multiprogrammed simulation includes a 40M cycle warmup, and then runs until every core has retired 10M instructions. Applications freeze statistics after 10M instructions but continue to run to exert pressure on the system. We found that warmup counters on caches indicate that caches are completely warm after 40M cycles for our workloads, and 10M instructions is long enough for the interconnect to reach a steady-state. Each multithreaded simulation is run until a certain number of barriers (e.g., main loop iterations). 5.2. System Design and Parameters We model an 8x8-mesh CMP for our multiprogrammed evaluations and a 4x4-mesh CMP for our multithreaded evaluations. Detailed cache, core and network parameters are given in Table 1. The system is a shared-cache hierarchy with a distributed shared cache. Each node contains a compute core, a private cache, and a slice of shared cache. Addresses are mapped to cache slices with the S-NUCA scheme [24]: the lowest-order bits of the cache block number determine the home node. The system uses a directory-based coherence protocol based on the SGI Origin [30]. We also evaluate sensitivity to cache mapping with a locality-aware scheme. Importantly, we model a perfect shared cache in order to stress the interconnect: every access to a shared cache slice is a hit, so that no requests go to memory. This isolates the interconnect to provide an upper bound for our performance degradation – in other words, to report conservative results. 5.3. Workloads Multiprogrammed: We run 49 multiprogrammed workloads, each consisting of 64 independent programs. 39 of these workloads are homogeneous, consisting of 64 copies of one application. The remaining 10 are randomly-chosen mixes from our set of 39 applications. Our application set consists of 26 SPEC CPU2006 benchmarks (including two traces of mcf), three SPEC CPU2000 benchmarks (vpr, art, crafty), and 10 other server and desktop traces: health (from the Olden benchmarks [45]), matlab [33], sharepoint.1, sharepoint.2 [37], stream [34], tpcc [1], xml (an XML-parsing application), search-1, search-2 (web-search traces from a commercial search engine). Multithreaded: We run ﬁve applications from the SPLASH2 [57] suite: fft, luc, lun, radix and cholesky. As described in § 5.1, we delineate run lengths by barrier counts: in particular, we run cholesky for 3 barriers, fft for 5, luc for 20, lun for 10, and radix for 10 barriers. 5.4. Application-Level Performance In multiprogrammed workloads, we measure applicationlevel performance using the weighted speedup metric [48]: W S = N∑ i=1 IPCshared IPCal one (1) We compute weighted speedup in all workloads using a buffered-network system as the baseline (IPCal one values). This allows direct comparison of the networks. In multithreaded workloads, we compare execution times directly by normalizing runtimes to the buffered-network baseline. Overall results: For our set of multiprogrammed workloads, CHIPPER degrades weighted speedup by 13.6% on average (49.8% max in one workload) from the buffered network, and 9.6% on average (29.9% max) from BLESS. For our set of multithreaded workloads, CHIPPER degrades performance (increases execution time) by 1.8% on average (3.7% max). As described above, these results are pessimistic, obtained with perfect shared cache in order to stress the interconnect. Additionally, for the least intensive third of multiprogrammed workloads, and for the multithreaded workloads we evaluate, performance impact is completely negligible. Per-workload results: However, average performance degradation does not tell the whole story. Examining degradation by workload intensity yields more insight. Fig. 5 shows weighted speedup (for multiprogrammed) and normalized runtime (for multithreaded), as well as network power, for a representative subset of all multiprogrammed workloads (for space reasons) and all multithreaded workloads. Behavior can be classiﬁed into two general trends. First, for workloads that are not network-intensive, CHIPPER experiences very little degradation relative to both buffered and BLESS networks. This is the best case for a cheap interconnect, because the application load is low, requiring much less than the peak capacity of the baseline buffered network. As workloads begin to become more network-intensive, moving to the right in Fig. 5, both bufferless networks (BLESS and CHIPPER) generally degrade relative to the buffered baseline. We note in particular that the SPLASH-2 multithreaded workloads experience very little degradation because of low network trafﬁc. As described in [38], bufferless routing is a compelling option for low-to-medium load cases. We conclude that at low load, CHIPPER is effective at preserving performance while signiﬁcantly reducing NoC power. 151  0  10  20  30  40  50  60 W e i h g t p u d e e p S d e Buffered BLESS CHIPPER  0  5  10  15 p erlb c d s c g sje alc e h a c alII are nto ctu c ulix e p s n c h oint.1 to A D M h n p g v a 2 o p 6 v o n g 4 d ref m r ra y m a c s tp r s e c arc c h.1 M M I I .5 .2 X X b zip 2 M M M I I I .8 .0 .6 X X X o m n etp p M s I .3 X p hin x 3 m ilc G s o e ple a s x m F D T D stre lb m m h e art alth m cf A V G N e t w o r o P k w e r ( W )  0  2  0.2  0.4  0.6  0.8  1  1.2 p u d e e p S ( o n r m a i l d e z ) lu c h c ole dix s k y ra fft lu n A V G  0  0.5  1  1.5 N e t w o r o P k w e r ( W ) Figure 5: Application performance and network power comparisons. 5.5. Power and Energy Efﬁciency Power: Figure 5 shows average network power for each evaluated workload. These results demonstrate the advantage of bufferless routing at low-to-medium load. Both CHIPPER and BLESS have a lower power ceiling than the buffered router, due to the lack of buffers. Thus, in every case, these router designs consume less power than a buffered router. In multiprogrammed workloads, CHIPPER consumes 54.9% less power on average than buffered and 8.8% less than BLESS; with multithreaded workloads, CHIPPER consumes 73.4% less than buffered and 10.6% less than BLESS. System energy efﬁciency: The discussion above evaluates efﬁciency only within the context of network power. We note that when full-system power is considered, slowdowns due to interconnect bottlenecks can have signiﬁcant negative effects on total energy. A full evaluation of this tradeoff is outside the scope of this work. However, the optimal point depends entirely on the fraction of total system power consumed by the NoC. If this fraction is sufﬁciently large, the energy tradeoffs shown here apply. For low-to-medium intensity loads, minimal performance loss coupled with signiﬁcant router power, area and complexity reduction make CHIPPER a favorable design tradeoff regardless of the fraction of system power consumed by the NoC. 5.6. Network-Level Performance We present latency and deﬂection as functions of injection rate for uniform random trafﬁc in Figures 6a and 6b respectively. We show in Fig. 6a that CHIPPER saturates more quickly than BLESS, which in turn saturates more quickly than a buffered interconnect. Furthermore, CHIPPER clearly has a higher deﬂection rate for a given network load, which follows from the less exhaustive port allocator. Sensitivity: We evaluate sensitivity to two network parameters: golden epoch length, and reassembly buffer size. For the former, we observe that as epoch length sweeps from 8 (less than the minimum required value for a livelock freedom guarantee) to 8192, and synthetic injection rate sweeps from 0 to network saturation, IPC varies by 0.89% maximum. This small effect is expected because golden ﬂits comprise 0.37% on average (0.41% max) of router traversals over these sweeps. The epoch length is thus unimportant for throughput. The reassembly buffer size can have a signiﬁcant effect if it is too small for the presented load. When reassembly buffers are too small, they become the interconnect bottleneck: the opportunistic assumption of available receiver space fails, and most requests require retransmits. With a 25 MPKI synthetic workload and only one buffer per node, the retransmit rate is 72.4%, and IPC drops by 56.7% from the inﬁnite-buffer case. However, IPC reaches its ideal peak with 8 buffers per node at this workload intensity, and is ﬂat beyond that; for a lessintensive 10 MPKI synthetic workload, performance reaches ideal at 5 buffers per node. In the application workloads, with 16 buffers per node, the retransmit rate is 0.0016% on average (0.021% max). Thus, we conclude that when buffers are sized realistically (16 buffers per node) the overhead of RetransmitOnce is negligible. Effect of Locality-Aware Data Mapping: Finally, we evaluate the effects of data locality on network load, and thus, the opportunity for a cheaper interconnect design. We approximate a locality-aware cache mapping scheme by splitting the 8x8-mesh into sixteen 2x2 neighborhoods or four 4x4 neighborhoods: for each node, its cache blocks are striped statically across only its neighborhood. This is a midpoint between one extreme, in which every node has its entire working set in its local shared-cache slice (and thus has zero network trafﬁc) and the other extreme, S-NUCA over the whole mesh, implemented in our evaluations above. We ﬁnd that for the set of 10 random-mix workloads on an 8x8-mesh (11.7% weighted speedup degradation from buffered to CHIPPER), modifying cache mapping to use 4x4 neighborhoods reduces weighted speedup degradation to 6.8%, and using 2x2 neighborhoods reduces degradation to 1.1%. This result indicates that mechanisms that increase locality in NoC trafﬁc can signiﬁcantly reduce network load, and provide further motivation for cheap interconnects such as CHIPPER. 152             s t s e u q e R m o d n a R m r o f i n U , y c n e t a L t e k c a P ) e l c y c r e p t i l f r e p ( e t a R n o i t c e l f e D  60  55  50  45  40  35  30  25  20  15 Buffered BLESS CHIPPER  0  0.1  0.2  0.3  0.4 Injection Rate (per node per cycle) (a) Total latency  0.5  0.5  0.4  0.3  0.2  0.1  0  0 BLESS CHIPPER  0.1  0.2  0.3  0.4 Injection Rate (per node per cycle) (b) Deﬂection rate  0.5 Figure 6: Network-level evaluations: latency and deﬂection with synthetic trafﬁc. 5.7. Hardware Complexity In order to obtain area and timing results, and provide power estimates for workload evaluations, we use RTL (Verilog) models of CHIPPER and BLESS, synthesized with the Synopsys toolchain using a commercial 65nm process. We model the buffered baseline timing with a publicly available buffered NoC model from Stanford [50]. However, because of an inadequate wire model, we were not able to obtain adequate area/power estimates for the ﬂit datapath; for this reason, we used ORION [55] to obtain estimates for the buffered baseline area/power. For both bufferless routers, we synthesized control logic, and then added crossbar area and power estimates from ORION. CHIPPER is also conservatively modeled by including crossbar area/power, and shrinking the permutation network to only the control-path width; further gains should be possible with a realistic layout that routes the datapath through the permutation network. For both bufferless models, we synthesize a single router, with parameters set for an 8x8-mesh. Finally, for all three networks, we model link power with ORION assuming 2.5mm links (likely conservative for an 8x8-mesh). We do not model reassembly buffers, since we use MSHRs for this purpose. Table 2 shows area and timing results for CHIPPER, BLESS and traditional buffered routers. The reduction in area from buffered to either of the bufferless designs is signiﬁcant; this gap is dominated by buffers (35.3% of the buffered router’s area). The additional reduction from BLESS to 153 CHIPPER is due to simpler control logic. Altogether, CHIPPER has 36.2% less area than the buffered baseline. Additionally, the critical path delays are comparable for both designs: CHIPPER’s critical path, which is through the sort network, is only 1.1% longer than the critical path in the buffered model. We conclude that CHIPPER can attain nearly the same operating frequency as a buffered router while reducing area, power (as shown in § 5.5) and complexity signiﬁcantly. 6. Related Work Deﬂection routing: Deﬂection routing was ﬁrst introduced as hot-potato routing in [3]. It has found use in optical networks [8], where deﬂection is cheaper than buffering. Recently, bufferless routing has received renewed interest in interconnect networks. BLESS [38] motivates bufferless deﬂection routing in on-chip interconnect for cost reasons. However, it does not consider arbitration hardware costs, and it does not solve the reassembly-buffer problem. The Chaos router [28] is an earlier example of deﬂection routing. The router is not bufferless; rather, it uses a separate deﬂection queue to handle contention. The HEP multiprocessor [47] and the Connection Machine [20] used deﬂection networks. Finally, [31, 52] evaluate deﬂection routing in several NoC topologies and with several deﬂection priority schemes. However, [31] does not evaluate application-level performance or model hardware complexity, while [52] does not show livelock freedom nor does it consider hardware cost of the deﬂection router control logic. Neither work examines the reassembly-buffer problem that we solve. Drop-based bufferless routing: BLESS [38] is bufferless as well as deﬂection-based. However, several networks eliminate buffers without deﬂection. BPS [16] proposes bufferless routing that drops packets under contention. SCARAB [19] builds on BPS by adding a dedicated circuit-switched NACK network to trigger retransmits. This work evaluates hardware cost with detailed Verilog models. However, neither BPS nor SCARAB rigorously prove livelock freedom. Furthermore, the separate NACK network increases link width and requires a separate circuit-switching crossbar. Other bufferless alternatives: Ring-based interconnects [46, 44] are particularly well-suited for bufferless operation, because no routing is required once a ﬂit enters the ring: it simply travels until it reaches its destination. Rings have low complexity and cost, but scale worse than meshes, tori and other topologies beyond tens of nodes. Hierarchical bus topologies [53] offer another alternative, especially compelling when trafﬁc exhibits locality. Both of these non-mesh topologies are outside the scope of this work, however. Reducing cost and complexity in buffered routers: Elastic Buffer Flow Control [35] makes use of the buffer space inherent in pipelined channels to reduce buffer cost. The iDEAL router [26] reduces buffering by using dual-function links that can act as buffer space when necessary. The ViChaR router [39] dynamically sizes VCs to make more efﬁcient use of a buffer budget, allowing reduced buffer space for equivalent performance. In all these cases, the cost of VC buffers is reduced, but buffers are not completely eliminated as in bufferless deﬂection routing. Going further, Kim [25] eliminates VC buffers while still requiring intermediate buffers (for injection and for turning). The work shares our goal of simple microarchitecture. Routing logic is simpler in [25] than in our design, because of buffering; however, [25] does not use adaptive routing, and requires ﬂow control on a ﬁner grain than Retransmit-Once to control injection fairness. [56] pro                  Area Timing (crit path) Buffered 480174 μ m2 1.88ns BLESS 311059 μ m2 2.68 ns CHIPPER % Δ Buffered → CHIPPER % Δ BLESS → CHIPPER 306165 μ m2 36.2% reduction 1.6% reduction 1.90 ns 1.1% increase 29.1% reduction Table 2: Hardware cost comparisons for a single router in a 65nm process. poses buffer bypassing to reduce dynamic power and latency in a buffered router, and [36] evaluates such a router against BLESS. The paper’s evaluation shows that with a custom buffer layout (described in [2]), an aggressive buffered design can have slightly less area and power cost than a bufferless deﬂection router, due to the overhead of BLESS arbitration and port allocation. However, our goal is speciﬁcally to reduce these very costs in bufferless deﬂection routing; we believe that by addressing these problems, we show bufferless deﬂection routing to be a practical alternative. Improving performance and efﬁciency of bufferless NoCs: Several works improve on a baseline bufferless design for better performance, energy efﬁciency, or both. Jafri et al. in [23] propose a hybrid NoC that switches between bufferless deﬂection routing and buffered operation depending on load. Nychis et al. in [40] investigate congestion control for bufferless NoCs that improves performance under heavy load. Both mechanisms are orthogonal to our work, and CHIPPER could be combined with either or both techniques to improve performance under heavy load. Permutation network: Our permutation network is a 2-ary 2-ﬂy Butterﬂy network [12]. The ability of indirect networks to perform permutations is well-studied: [54] shows a lower bound on the number of cells required to conﬁgure any permutation. (For our 4-input problem, this bound is 5, thus our design is only partially permutable.) Rather, the new contribution of the CHIPPER deﬂection-routing permutation network is the realization that the deﬂection-routing problem maps naturally to an indirect network, with the key difference that contention is resolved at each 2x2 cell by misrouting rather than blocking. CHIPPER embeds these permutation networks within each node of the overall mesh network. To our knowledge, no other deﬂection router has made this design choice. Livelock: Livelock freedom guarantees can be classiﬁed into two categories: probabilistic and deterministic. BLESS [38] proposes Oldest-First (as discussed in § 2.2.1), which yields an inductive argument for deterministic livelock freedom. Busch et al. [7] offer a routing algorithm with a probabilistic livelock guarantee, in which packets transition between a small set of priorities with certain probabilities. [28] also provides a probabilistic guarantee. Golden Packet provides a deterministic guarantee, but its key difference from [7, 28] is its end goal: it is designed to be as simple as possible, with hardware overhead in mind. Deadlock: Deadlock in buffered networks is well-known [11] and usually solved by using virtual channels [10]. However, our reassembly-buffer deadlock is a distinct issue. Hansson et al. [18] observe a related problem due to inter-packet (request-response) dependencies in which deadlock can occur even when the interconnect itself is deadlock-free. Like the reassembly-buffer deadlock problem described in this paper, this occurs due to ejection backpressure: responses to previous requests cannot be injected, and so new requests cannot be ejected. However, our problem differs because it exists independently of inter-packet dependencies (i.e., could happen with only one packet class), and happens at a lower level (packet reassembly). [18] proposes end-to-end ﬂow control with token passing as a solution to message-dependent deadlock, but assumes a window-based buffering scheme. Our ﬂow-control scheme is distinguished by its opportunistic common-case, lack of explicit token passing, and lack of an explicit retransmit window due to integration into MSHRs. 7. Other Applications and Future Work While CHIPPER’s design point is appealing for its simplicity, there is a large design space that spans the gap between large, traditional buffered routers and simple deﬂection routers. Several directions are possible for future work. First, the mechanisms that comprise CHIPPER are not limited to the speciﬁc design shown here, nor are they mutually dependent, and extensions of these techniques to other networks might allow for hardware cost reduction at other design points. Golden Packet can be extended to any nonminimal adaptive interconnect in order to provide livelock freedom. Likewise, the basic permutation-network structure can be used with other priority schemes, such as Oldest-First or an application-aware scheme [14, 13], by modifying the comparators in each arbiter block. Finally, Retransmit-Once offers deadlock freedom in any deﬂection network that requires reassembly buffers. In fact, it can also be extended to provide ﬂow control for other purposes, such as congestion control; in general, it allows receivers to throttle senders when necessary, in a way that is integrated with the basic functionality of the network. Additionally, we have shown only one permutation network topology. A more detailed study of the effect of partial permutability on network-level and applicationlevel performance would allow for optimizations that take advantage of properties of the presented trafﬁc load. In particular, heterogeneity in the permutation network with regard to the more likely ﬂit permutations (at center, edge and corner routers) might increase efﬁciency. 8. Conclusions We presented CHIPPER, a router design for bufferless deﬂection networks that drastically reduces network power and hardware cost with minimal performance degradation for systems with low-to-medium network load. CHIPPER (i) replaces the router core with a partial permutation network; (ii) employs Golden Packet, an implicit token-passing scheme for cheap livelock freedom; and (iii) introduces RetransmitOnce, a ﬂow-control scheme that solves the reassembly-buffer backpressure problem and allows use of MSHRs for packet reassembly, making the network truly bufferless. Our techniques reduce router area by 36.2% from a traditional buffered design and reduce network power by 54.9% (73.4%) on average, in exchange for 13.6% (1.9%) slowdown, with multiprogrammed (multithreaded) workloads. In particular, slowdown is minimal and savings are signiﬁcant at low-to-medium load. We thus present a cheap and practical design for a bufferless interconnect – an appealing design point for vastly reduced cost. It is our hope that this will inspire more ideas and further work on cheap interconnect design. Acknowledgments We thank the anonymous reviewers for their feedback. We gratefully acknowledge members of the SAFARI research 154 group, CALCM, and Thomas Moscibroda at Microsoft Research for many insightful discussions on this and related work. Chris Fallin was supported by a PhD fellowship from SRC, and subsequently NSF, while conducting this work. We acknowledge the support of Intel, AMD, and Gigascale Systems Research Center. This research was partially supported by an NSF CAREER Award CCF-0953246. "
2011,A new server I/O architecture for high speed networks.,"Traditional architectural designs are normally focused on CPUs and have been often decoupled from I/O considerations. They are inefficient for high-speed network processing with a bandwidth of 10Gbps and beyond. Long latency I/O interconnects on mainstream servers also substantially complicate the NIC designs. In this paper, we start with fine-grained driver and OS instrumentation to fully understand the network processing overhead over 10GbE on mainstream servers. We obtain several new findings: 1) besides data copy identified by previous works, the driver and buffer release are two unexpected major overheads (up to 54%); 2) the major source of the overheads is memory stalls and data relating to socket buffer (SKB) and page data structures are mainly responsible for the stalls; 3) prevailing platform optimizations like Direct Cache Access (DCA) are insufficient for addressing the network processing bottlenecks. Motivated by the studies, we propose a new server I/O architecture where DMA descriptor management is shifted from NICs to an on-chip network engine (NEngine), and descriptors are extended with information about data incurring memory stalls. NEngine relies on data lookups and preloads data to eliminate the stalls during network processing. Moreover, NEngine implements efficient packet movement inside caches to address the remaining issues in data copy. The new architecture allows DMA engine to have very fast access to descriptors and keeps packets in CPU caches instead of NIC buffers, significantly simplifying NICs. Experimental results demonstrate that the new server I/O architecture improves the network processing efficiency by 47% and web server throughput by 14%, while substantially reducing the NIC hardware complexity.","A New Server I/O Architecture for High Speed Networks  Guangdeng Liao+, Xia Zhu±, Laxmi Bhuyan+  +University of California, Riverside  ±Intel Labs  {gliao, bhuyan}@cs.ucr.edu, xia.zhu@intel.com  Abstract  Traditional architectural designs are normally  focused on CPUs and have been often decoupled from  I/O considerations. They are inefficient for high-speed  network processing with a bandwidth of 10Gbps and  beyond. Long latency I/O interconnects on mainstream  servers also substantially complicate the NIC designs.  In this paper, we start with fine-grained driver and OS  instrumentation  to  fully understand  the network  processing overhead over 10GbE on mainstream  servers. We obtain several new findings: 1) besides  data copy identified by previous works, the driver and  buffer release are two unexpected major overheads (up  to 54%); 2) the major source of the overheads is  memory stalls and data relating to socket buffer (SKB)  and page data structures are mainly responsible for  the stalls; 3) prevailing platform optimizations like  Direct Cache Access (DCA) are  insufficient  for  addressing the network processing bottlenecks.   Motivated by the studies, we propose a new server  I/O architecture where DMA descriptor management is  shifted from NICs to an on-chip network engine  (NEngine), and descriptors are extended with  information about data  incurring memory stalls.  NEngine relies on data lookups and preloads data to  eliminate  the stalls during network processing.  Moreover, NEngine  implements efficient packet  movement inside caches to address the remaining  issues in data copy. The new architecture allows DMA  engine to have very fast access to descriptors and  keeps packets in CPU caches instead of NIC buffers,  significantly simplifying NICs. Experimental results  demonstrate that the new server I/O architecture  improves the network processing efficiency by 47%  and web server throughput by 14%, while substantially  reducing the NIC hardware complexity.  1. Introduction      Ethernet continues to be the most widely used  network architecture today for its low cost and  backward compatibility with the existing Ethernet  infrastructure. It dominates in data centers and is  replacing specialized fabrics such as InfiniBand [12],  Quadrics [32], Myrinet [4] and Fiber Channel [6] in  high performance computers. Driven by increasing  networking demands such as Internet search, web  hosting, video on demand etc, network speed is rapidly  migrating from 1Gbps to 10Gbps and beyond [7]. High  speed networks require servers to provide efficient  network processing with a low design complexity of  network interfaces (NIC).       Traditional architectural designs of processors,  cache hierarchies and system interconnect focus on  CPU/memory-intensive applications, and are usually  decoupled  from  I/O considerations. They are  inefficient for network processing. It was reported that  network processing in the receive side over 10Gbps  Ethernet network (10GbE) easily saturates two cores of  an Intel Xeon Quad-Core processor [19, 22]. Assuming  ideal  scalability over multiple cores, network  processing over 40GbE and 100GbE will saturate 8  and 20 cores, respectively. In addition  to  the  processing inefficiency, the increasing network speed  also poses a big challenge to NIC designs. DMA  descriptor fetches over long latency PCI-E bus heavily  stress the DMA engine in NICs and need larger NIC  buffers  to  temporarily  keep  packets. These  requirements significantly  increase NIC's design  complexity and price [37]. For instance, the price of a  10GbE NIC can be up to $1.4K but a 1GbE NIC costs  less than $40 [13, 14]. Our aim is to understand  network processing efficiency over high speed  networks and design a new server I/O architecture to  tackle those challenges.    In the past decade, a wide spectrum of research has  been done in network processing to understand and  optimize the processing efficiency [1-3, 9, 16-22, 25,  27-28, 38].  Zhao et al. [38] used a cache simulator to  study cache behavior of the TCP/IP protocol. They  showed that packets show no temporal locality and  proposed a copy engine to move packets in memory.  To eliminate memory stalls to packets, Intel proposed  DCA to route incoming network data to caches [9, 1819]. Furthermore, Binkert et al. [2-3] integrated a  simplified NIC into CPUs to naturally implement DCA  and used Program I/O (PIO) to move data from NICs  in software. We also did extensive performance  evaluation of an Integrated NIC (INIC) architecture  [20] and along with Intel researchers, suggested some  978-1-4244-9435-4/11/$26.00 ©2011 IEEE  255         techniques  to  improve  the network processing  performance [21-22]. Recently, Intel proposed on-die  message engines to move data between I/O devices and  CPUs to reduce the PCI-E traffic [17]. However, all  these works did not study overheads at the operating  system (OS) level (e.g. NIC driver, buffer management  etc) and address those bottlenecks.   In this paper, we begin with per-packet processing  overhead breakdown by running a network benchmark  Iperf [10] over 10GbE on Intel Xeon Quad-Core  processor based servers. We find that besides data  copy, the driver and buffer release, unexpectedly take  46% of processing time for large I/O sizes and even  54% for small I/O sizes. To understand the overheads,  we instrument the driver and OS kernel using hardware  performance counters [11]. Unlike existing profiling  tools attributing CPU cost, such as retired cycles or  cache misses,  to  function  level  [29],  our  instrumentation is at very fine granularity and can  pinpoint data incurring the cost. Through the above  studies, we obtain several new findings: 1) the major  network processing bottlenecks lie  in the driver  (>26%), data copy (up to 34% depending on I/O sizes)  and buffer release (>20%), rather than the TCP/IP  protocol itself; 2) in contrast to the generally accepted  notion that long latency NIC register access results in  the driver overhead [2-3], our results show that the  overhead comes from memory stalls to network buffer  data structures; 3) releasing network buffers in OS  results in memory stalls to in-kernel page data  structures, contributing to the buffer release overhead ;  4) besides memory stalls to packets, data copy  implemented as a series of load/store instructions, also  has significant time on L1 cache misses and instruction  execution. Prevailing optimizations for data copy like  DCA are insufficient to address the copy issue.   Based on the studies, we discuss several intuitive  solutions and find that a holistic I/O solution is needed  for high speed networks. We propose a new server I/O  architecture, where the responsibility for managing  DMA descriptors is moved to an on-chip network  engine (NEngine). The on-chip descriptor management  exposes plenty of optimization opportunities like  extending descriptors. We add information about data  incurring memory stalls during network processing  into descriptors. When the NIC receives a packet, it  directly pushes the packet into NEngine without  waiting for long latency descriptors fetches. NEngine  reads extended descriptors to obtain packet destination  location and information about data incurring memory  stalls. Then, it moves the packet into the destination  memory location and checks whether data incurring the  stalls resides in caches. If not, NEngine sends data  address to the hardware prefetching logic for loading  the data. To address the data copy issue, NEngine  256 moves payload inside last level cache (LLC) and  invalidates source cache lines after the movement. The  new I/O architecture allows DMA engine to have fast  access to descriptors and keeps packets in CPU caches  rather than in NIC buffers. These designs substantially  reduce the burden on the DMA engine and avoid  extensive NIC buffers in high speed networks. While  NICs are decoupled from DMA engine, they still  maintain other hardware features such as Receive Side  Scaling (RSS) [33], and Interrupt Coalescing [7].  Different from previous research aiming at data copy,  the new server I/O architecture ameliorates all major  performance bottlenecks of network processing and  simplifies NIC designs, making general purpose  platforms well suited for high speed networks.        To evaluate our designs, we enhanced the full  system simulator Simics [24] with detailed timing  models and implemented the new I/O architecture in  the simulator. We developed a 10GbE NIC as a device  module of the simulator and a corresponding driver  with the support of Large Receive Offload (LRO) [8]  in Linux. In  the experiments, both  the microbenchmark Iperf and the macro-benchmark SPECWeb  [34] are used. Experimental results demonstrate that  the new I/O architecture  improves  the network  processing efficiency by 47% and web server  throughput by 14% while substantially reducing the  NIC hardware complexity.      The remainder of this paper is organized as follows.  We revisit the conventional I/O architecture in Section  2 and then present a detailed overhead analysis over  10GbE in Section 3. Section 4 elaborates the new  server  I/O architecture  in detail,  followed by  performance evaluation in Section 5. Finally we  discuss related work and conclude our paper in Section  6 and 7, respectively.   2. Revisiting I/O Architecture  2.1 Network Processing  Unlike CPU-intensive  applications,  network  processing  is I/O-intensive and  involves several  hardware components (e.g. NIC, PCI-E, I/O Hub,  memory, CPU) and system components (e.g. NIC  driver, TCP/IP). Network processing in the receive side  has significant processing overheads, consuming  thousands of CPU cycles for each packet. In this  subsection, we revisit the network receiving process.   In the receive side, an incoming packet starts with  the NIC/driver  interaction. The RX descriptors  (typically 16 bytes each), organized in circular rings,  are used as a communication channel between the  driver and the NIC. The driver tells the NIC through  these descriptors, where in the memory to copy the  incoming packets. To be able to receive a packet, a    NIC Driver 6 Read Interrupt Cause 9 Write Enable Mask (UC) 5 Interrupt CPU 1 Read RX Descriptors NIC 2 Receive Data 7 8 Skb conversion Allocate new skb  buffers Rx Descriptor Ring skb X buf TCP/IP Layer 4 Write Status 3 Write Received Data skb ……. buf Skb Buffer skb buf Figure 1. NIC/Driver interaction descriptor should be in “ready” state, which means it  from memory before it writes packets into memory (or  has been initialized and pre-allocated with an empty  reads packets from memory in the transmit side).  packet buffer (SKB buffer in Linux) accessible by the  Although PCI-E bus bandwidth has improved, its  NIC [36].  The SKB buffer is the in-kernel network  latency has worsened by up to 25X over earlier PCI-X  buffer to hold any packet up to MTU (1.5 KB).  It  incarnations. The round-trip traversal over PCI-E bus  contains an SKB data structure of 240 bytes carrying  can take up to ~2200 ns, mostly due to complex PCI-E  packet metadata used by the TCP/IP protocol and a  transaction layer protocol implementation [26]. The  DMA buffer of 2 KB holding the packet itself.   long  latency  traversal substantially  increases  the      The detailed interaction is illustrated in Figure 1.  processing overhead of DMA engine. As network  Before transferring the received packets, the NIC needs  traffic becomes intense, the DMA engine is heavily  to read ready descriptors from memory over PCI-E bus  stressed [37]. Long latency descriptor fetches also  to know the DMA buffer address (step 1). After  require large NIC buffers to temporarily store packets.  receiving the Ethernet frames from the network (step  Moreover, in order to leverage multiple cores in CMPs  2), it transfers the received packets into those buffers  for packet processing, modern NICs typically introduce  (denoted as buf in Fig.1) using DMA engine (step 3).  a large number of receive/transmit (RX/TX) queues  Once the data is placed in memory, the NIC updates  and allow each core to have a dedicated RX/TX queue.  descriptors with packet length and marks them as used  For instance, an Intel 82599 10GbE NIC has 128  (step 4). Then, the NIC generates an interrupt to kick  RX/TX queues per port, corresponding to 512KB and  off network processing in CPUs (step 5). Note that  160KB buffers [14]. All of these complicate NIC  several interrupts can be coalesced into one interrupt to  designs and pose big challenges.   reduce overheads. In the CPU side, the interrupt  handler in the driver reads the NIC register to check  the cause of the interrupt (in step 6). If legal, the driver  reads descriptors to obtain packet’s address and length,  and then maps the packet into SKB data structures  (step 7). After the driver delivers SKB buffers to the  protocol stack,  it  reinitializes and  refills used  descriptors with new allocated SKB buffers for future  incoming packets (step 8). Finally, the driver reenables the interrupt by setting the NIC register (step  9). After the driver completes its operations, SKB  buffers are delivered up to the protocol stack. Once the  protocol stack finishes processing, applications are  scheduled to move packets to user buffers. Finally, the  SKB buffers are reclaimed into OS [5, 36].        We conducted extensive experiments to understand  network processing overheads over 10GbE across a  range of I/O sizes. Both SUT (System under Test) and  stress machines are Intel servers, which contain two  Quad-Core Intel Xeon 5355 processors [11]. Each core  is running at 2.66GHz frequency and each processor  has 2 LLC of 4MB each shared by 2 cores. The servers  are connected by two Intel 10Gbps 82598 server  adapters sitting in PCI-E X8 slots [13]. They ran Linux  kernel 2.6.21 and Intel 10GbE NIC driver IXGBE  version 1.3.31. We retain default settings of the Linux  network subsystem and the NIC driver, unless stated  otherwise. Note that LRO [8], a technique to amortize  the per-packet processing overhead by combining  multiple in-order packets into one single packet, is  enabled in the NIC driver. Popular stream hardware  3. Understanding Processing Overheads   2.2 High Speed NIC Challenges  As shown in Fig.1 (step 1), the NIC must initiate  DMA transfers over PCI-E bus to fetch descriptors  257     s e l c y C U P C 64B 256B 1KB 4KB 16KB 2000 1500 1000 500 0 Driver Buffer Release IP TCP Data Copy System Call Iperf others Figure 2. Per-packet processing overhead breakdown  prefetcher employing a memory access stride based  unable to identify data or macro incurring the cost, so  predictive algorithm is configured in the servers [11].  we had to manually instrument inside functions. Table  In our experiments, the micro-benchmark Iperf with 8  1 shows one such example of instrumentation in the  TCP connections is run to generate network traffic  driver. We first measured the function's cost and then  between servers (SUT is a receiver). We find that one  did fine-grained  instrumentation for every code  core with 4MB LLC achieves ~5.6Gbps throughput  segment if the function has high cost. We continue to  and two cores with 8MB LLC are saturated in order to  instrument each code segment until we locate the  obtain line rate throughput. The high processing  bottlenecks in all functions along the processing path.  overhead motivates us to further breakdown the perMost events are collected including CPU cycles,  packet processing overhead.   instruction and data cache misses, LLC misses, ITLB  misses and DTLB misses etc. Since large I/Os include  all three major overheads, this subsection presents the  detailed analysis only for the 16KB I/O.   Table1. Fine-grained instrumentation  3.1. Packet Processing Overhead Breakdown       We used Oprofile [29] to collect system-wide  function overheads while Iperf is running over 10GbE.  We group all functions into components along the  network processing path: the driver, IP, TCP, data  copy, buffer release, system call and Iperf.  All other  supportive kernel functions such as scheduling, context  switches etc. are categorized as others. Per-packet  processing time breakdown under various I/O sizes is  calculated and illustrated in Figure 2. Note that I/O size  is used by TCP/IP stack and I/Os larger than MTU are  segmented into several Ethernet packets (<=MTU).       We observe the following from Fig.2: 1) the  overhead in data copy increases as the I/O size grows  and becomes a major bottleneck with large I/Os  (>=256 bytes); 2) the driver and buffer release  consume ~1200 cycles and ~1100 cycles per packet,  respectively, regardless of I/O sizes. They correspond  to ~26% and 20% of processing time for large I/Os and  even higher for small I/Os; 3) the TCP/IP protocol  processing overhead is substantially reduced because  LRO coalesces multiple packets into one large packet  to amortize the processing overhead. Thus Fig.2  reveals that, besides data copy, high speed network  processing over mainstream servers has another two  unexpected major bottlenecks: the driver and buffer  release.   3.2. Fine-Grained Instrumentation   Oprofile in Subsection 3.1 does profiling at the  coarse-grained level and attributes CPU cost, such as  retired cycles and cache misses, to functions. It is  258  3.2.1. Driver  The driver comprises of three main components:  NIC register access (step 6 and 9), SKB conversion  (step 7) and SKB buffer allocation (step 8), as shown  in Fig.1. Existing studies [2-3] claimed that NIC  register access contributes to the driver overhead due  to long latency traversal over PCI-E bus, and then  proposed NIC integration to reduce the overhead. In  this subsection, we architecturally breakdown the  driver overhead for each packet and present results in  Figure 3. In contrast to the generally accepted notion  that the long latency NIC register access results in the  overhead [3], the breakdown reveals that the overhead  comes from SKB conversion and buffer allocation.  Although NIC register access takes ~2500 CPU cycles  on mainstream servers, ~60 packets are processed per  interrupt over 10GbE (~7 packets/interrupt over 1GbE)  substantially amortizing the overhead. In addition,  Fig.3 also reveals that L2 cache misses mainly result in        the SKB conversion overhead, and long instruction  path is the largest contributor of the SKB buffer  allocation overhead.  s e l c y C 800 600 400 200 0 100% 50% 0% L2 DTLB ITLB Dcache  Icache inst Step 6&9 Step 7 Step 8 Figure 3. Architectural breakdown  Other Descriptor Header SKB Memory stalls Figure 4. L2 cache misses breakdown for step 7     Since L2 cache misses in the SKB conversion  constitute ~50% of the driver overhead, we did further  instrumentation to identify the data incurring those  misses. We group data in the driver into various data  types (SKB, descriptors, packet headers and other local  variables) and measure their percentage of misses. The  result, presented in Figure 4, reveals that SKB is the  major source of  the memory stalls  (~1.5 L2  misses/packet on SKB). Different from the results in  prior studies [2-3], we find that the memory stalls to  packet headers are hidden and overlapped with  computation. It is because the new drivers use software  prefetch instructions to preload headers before being  accessed. Unfortunately, SKB access occurs at the very  beginning of  the driver and software prefetch  instructions cannot help. Although DMA invalidates  descriptors to maintain cache coherence, the memory  stalls  to descriptors are negligible  (~0.04 L2  misses/packet). That is because each 64 bytes cache  line can host 4 descriptors of 16 bytes each, and the  hardware prefetchers preload several consecutive  descriptors with a cache miss. To understand the SKB  misses, we also instrumented the kernel to study its  reuse distance over 10GbE. It is observed that SKB has  long reuse distance (~240K L2 access), which explains  the miss behavior.   3.2.2. Data Copy  After protocol processing, user applications are  scheduled to copy packets from SKB buffers to user  buffers. Data copy incurs mandatory cache misses on  payload because DMA triggers cache invalidation to  maintain cache coherence. We study the architectural  overhead breakdown of data copy and show the results  in Figure 5. 16KB I/O is segmented into small packets  of MTU each in the sender and they are sent to the  receiver. Fig.5 shows that L2 cache misses are the  major overhead (~50%, ~3.5 L2 misses/packet),  followed by data cache misses  (~27%, ~50  misses/packet) and  instruction execution (~20%).   Although DCA, implemented in recent Intel platforms  avoids L2 cache misses, it is unable to reduce  overheads of L1 cache misses and a series of load/store  instruction executions (total ~47%).  Also, routing  network data into L1 caches would pollute caches and  degrade performance because of small L1 cache size  [19, 35]. Since packets become dead after data copy  [36], loading them into L1 caches may evict other  valuable data. Hence, more optimizations are needed to  fully address the data copy issue.  100% 50% 0% data copy L2 DTLB ITLB Dcache  Icache inst Figure 5. Data copy overhead breakdown  100% 50% 0% Buffer release L2 DTLB ITLB Dcache  Icache inst Figure 6. Buffer release overhead breakdown  100% 95% 90% others page structure Memory stalls Figure 7.  L2 cache misses breakdown  3.2.3. Buffer Release  SKB buffers need to be reclaimed after packets are  copied to user applications. SKB buffer allocation and  release are managed by slab allocator [5]. The basis for  this allocator is retaining an allocated memory that  used to contain a data object of certain type and  reusing that memory for the next allocations for  another object of the same type. Buffer release consists  of two phases: looking up an object buffer controller in  OS and releasing the object into the controller. In the  implementation of slab allocator,  the page data  structure is used to keep buffer controller information  and read during the object lookup. This technique is  widely used by mainstream OS such as FreeBSD,  Solaris and Linux etc.  Figure 6  shows  the architectural overhead  breakdown of buffer release. We observe from Fig.6  259           that L2 cache misses are the single largest contributor  to the overhead (~1.6 L2 cache misses/ packet).   Similarly, we analyze data sources of L2 cache misses  and present results in Figure 7. The figure reveals that  L2 cache misses are from 128 bytes in-kernel page data  structures. The structure reuse distance analysis shows  that it is reused after ~255K L2 cache access, which  results in the large cache misses.  The above studies reveal that besides memory stalls  to itself, each packet incurs several cache misses on  corresponding data  (skb buffer and page data  structures) and has considerable data copy overhead.  Some intuitive solutions like having larger LLC  (>8MB for 10GbE) or extending the optimization DCA  might help to some extent. Our simulation results show  that, without  considering  application memory  footprint, 16MB LLC is needed to avoid those cache  misses for packet processing over 10GbE. When  network jumps to 40GbE and beyond, increasing LLC  becomes an ineffective solution.  More importantly, it  is unable to address NIC challenges and the data copy  issue. Extending DCA to deliver both packets and  those missed data from NICs into caches is more  efficient in avoiding memory stalls. Unfortunately, it  stresses NICs more heavily and degrades PCI-E  efficiency of packet transfers [30-31], and does not  consider the data copy issue as well. In order to attack  all challenges from increasing network speed, a new  holistic I/O solution is needed.   4. Proposed Server I/O Architecture  In this section, we propose a new server I/O  architecture for high speed networks. The overview of  architecture is illustrated in Figure 8. Essentially, we  move the DMA descriptor management from NICs to  an added on-chip network engine (NEngine) close to  LLC. The on-chip descriptor management enables us  to easily extend descriptors with information about  data incurring memory stalls. Similar to the memory  controller, the NEngine connects to I/O Hub (IOH) for  parsing PCI-E transactions. It communicates with  faster  cache  hierarchy  for DMA  descriptor  fetches/writes and packet movement, alleviating the  processing burden on the DMA engine. The NEngine  has low communication cost with LLC due to its close  proximity.  When NEngine  receives a packet,  it  reads  descriptors from cache hierarchy. Then it moves the  packet into corresponding cache location and preloads  those data  incurring memory stalls. The new  architecture exploits LLC to keep packets instead of  multiple RX/TX queues in NICs. Modern high speed  NICs have one dedicated RX/TX queue for each core,  thus increasing their cost and impeding scalability over  260 a large number of cores. Moreover, NEngine also  implements efficient payload movement inside LLC  and proactively purges dead packet data after data copy  is finished to address the data copy issue. The new I/O  architecture fundamentally reduces all three major  performance bottlenecks of network processing while  effectively simplifying NICs. The proposed designs are  elaborated in the following subsections.   Figure 8. New server I/O architecture  4.1. NEngine  During network processing, CPUs and NICs  communicate through DMA descriptors, as described  in detail in Section 2. Descriptors are organized as a  circular ring. Each descriptor is 16 bytes long and  includes packet metadata such as packet length,  memory address and status etc.  In the contemporary  I/O architecture, NICs fetch or write descriptors via  PCI-E bus before or after packet movement. The  descriptor fetches/writes have long latency stressing  DMA engine [37] and also waste a large number of  PCI-E transactions degrading PCI-E payload efficiency  [30-31]. Our on-chip descriptors management scheme  avoids these issues and more importantly, enables us to  easily  extend  the descriptors due  to  faster  communication with the cache hierarchy. By exploiting  this design, we extend RX descriptors with information  about data incurring memory stalls: SKB and page data  structures, as pinpointed  in Subsection 3.2. The  extended descriptors are  illustrated  in Figure 9.   Besides original 16 bytes, each new descriptor includes  4 bytes physical address of SKB and page data  structures each. Two hardware registers in NEngine are  dedicated to storing data structure length in terms of  the number of cache lines. For example in Linux, SKB  is 240 bytes and page structure  is 128 bytes,  corresponding to four and two cache lines of 64 bytes  each. For a typical ring buffer size of 1024 entries in  10GbE NICs, the new ring buffer size only increases  by 8KB.         Figure 9.  Extended descriptors  The block diagram of NEngine is illustrated in  Figure 10 with the new descriptors. Besides major  components shown in Fig. 10, NEngine also offers  dedicated registers to keep ring buffer base address and  ring pointer information as traditional NICs do. When  a packet arrives at the NIC, without fetching DMA  descriptors to know memory location for the packet,  the NIC calculates core ID for packet processing using  RSS hardware unit (RSS hardware distributes packets  among cores by hashing packet's 4-tuple) and sends the  packet with core ID into a small buffer in NEngine.  Fetch descriptor unit identifies the corresponding  descriptor address according to the ring base address of  the core ID and ring buffer pointers, and then sends a  cache read request to get the descriptor. Sec .3 shows  that mainstream severs exhibit extremely high  descriptor cache hit ratios even with DMA invalidation  (96%). The on-chip descriptor management avoids  DMA invalidation and has a higher descriptor cache hit  ratio. Thus, the fetch descriptor unit can access the  descriptors very fast and is much simpler than the  original DMA engine. With the knowledge of memory  location and data incurring memory stalls, the write  packet unit moves the packet into caches. Meanwhile,  the lookup/load unit lookups and preloads those data.  To facilitate the unit, we extend the cache architecture  with a new cache operation: lookup. The new operation  lookup returns whether data is in caches, other than  data themselves. The lookup/load unit sends lookup  operations to lookup those data and generates prefetch  commands to the hardware prefetching logic if they are  not in caches. After the packet is moved into cache  hierarchy, NEngine updates the descriptor status field  and ring buffer pointers similar to the traditional NICs.    In addition, NEngine is capable of moving payload  inside LLC. Since the source data becomes dead after  data copy [36], NEngine invalidates source cache lines  to purge the data. To support efficient movement, we  extend the cache architecture with a new cache  operation: read_invalidate, which reads cache lines  and then does cache invalidation. During data copy,  TCP/IP protocol breaks discontinuous physical address  Figure 10.  Basic blocks of NEngine  ranges into a set of consecutive physical ranges and  programs NEngine via three hardware registers: src,  dst, len. Then, NEngine breaks continuous physical  address ranges into a set of chunks at the cache line  granularity  and generates new  read_invalidate  operations to read and invalidate cache lines. Finally, it  writes those data into destination cache lines. Our  payload movement differs from prior copy engines [1,  16, 38] as follows: 1) payload movement is done inside  caches as opposed to memory in previous cases, and  payload in caches is invalidated after movement. The  invalidation avoids unnecessary memory write-backs  of dirty data, reducing memory traffic and improving  performance [15, 23]; 2) the virtual-to-physical address  translation overhead is negligible because data copy is  done in the OS context. In Linux, less than 10 cycles  are needed for the address translation.   When we come to the transmit side, NEngine reads  transmitted packets from cache hierarchy and transfers  them into the NIC over PCI-E bus. Once the NIC  receives the transmitted packets from NEngine, the  MAC processing units automatically sends them over  Ethernet links. Besides efficient network processing,  our designs simplify NIC designs in terms of buffer  resource and DMA engine and also reduce PCI-E  traffic used for descriptor fetches/writes.   4.2. NIC      In the new architecture, NICs are simplified with  less hardware resource. Figure 11  illustrates a  traditional NIC in the left box and the new NIC in the  right box.  In the traditional NIC, the MAC processing  unit receives packets from Ethernet and does RSS to  load balance incoming packets among cores at the  connection  level. The packets are  stored  in  corresponding RX queues. DMA engine uses PCI-E  transactions to fetch descriptors from memory and to  move data from RX queues to memory. Interrupt  coalescing unit will send interrupts to cores when the  number of transferred packets reach up to a threshold  set by the driver or a preprogrammed timer expires.  Similarly, in order to transmit packets, the NIC fetches  261 TX descriptors to know packet memory location and  moves packets into corresponding TX queues. Then,  packets are sent over Ethernet and interrupts are sent to  cores. In the new NIC, we remove large multiple  hardware queues and DMA engine marked as grey in  the left box. When RSS receives a packet from the  MAC processing unit, it calculates the core assigned to  packet processing. Then, the NIC directly sends the  packet with core ID to NEngine. Similar to the receive  side, when the NIC receives a transmitted packet, the  MAC processing unit directly takes over the packet for  transmission. RSS and Interrupt coalescing units  behave the same as the traditional NICs.     Figure 11.  Simplification of the NIC     4.3. Software Support     The new server  I/O architecture  inherits  the  descriptor-based software/hardware interface, and only  needs some modest support from the device driver and  the data copy components. When new SKB buffers are  allocated to refill RX descriptors, the driver sets  starting address of SKB and page data structures in  descriptors in addition to DMA buffer address. When  packets finish protocol processing, the data copy  component programs NEngine to move payload and  waits until NEngine finishes the movement. There is  no need to modify TCP/IP protocol stack, system call  and user applications.     5. Evaluation      We choose the full system simulator Simics to  evaluate our designs by enhancing it with detailed  cache, I/O timing models and effects of network DMA.  We extend the DEC 21140A Ethernet device with the  support of interrupt coalescing using Device Modeling  language to simulate a 10GbE Ethernet NIC. The  device itself is connected to a lossless, full-duplex link  of configurable bandwidth. The latency of a packet  traversing the link is simply fixed to 1 us. Two systems  (client and server) running Linux 2.6.16 are simulated  and interconnected with 10GbE. Since the stream  hardware prefetcher is the most popular prefetcher in  262 servers, we employ it in the simulator to speed up the  memory access of network data.      We implemented the new I/O architecture and  developed a NIC driver  in Linux. LRO was  implemented in the driver. To understand performance  impacts of our designs on network processing, we first  used the micro-benchmark Iperf. Then, we study how  much benefit web servers achieve by running the  SPECWeb benchmark. In each case, only one system  is of interest, while the other merely serves as a  stressor. SUT is configured with detailed timing  models and the stressor runs with the fast functional  mode and is not a bottleneck. The parameters we used  in modeling the configuration are listed in Table 2. We  are more interested in the relative behavior of these  systems than their absolute performance, so some of  these parameters are approximate.  Table 2. System configurations  Quad-Core, 3GHz, two-issue, in-order  32KB 2-way, 3 cycles hit, 64 bytes cache line  8M, 16-way, 14 cycles hit, 64 bytes cache  line, shared by all cores  400 cycles  Stream prefetch with degree 4  1600 cycles  64 packets per interrupt  10 cycles to L2 cache  1024 entries/ring  Processor  ICache/DCache  L2 Cache  Main memory  Prefetcher  I/O Register  Interrupt rate  NEngine  Ring buffer   5.1. Network Performance      First, we looked at network performance in the  receive  side by  running  Iperf under various  the original system (orig), DCA  configurations:  routing data to L1 caches (DCA-L1), DCA routing data  into L2 caches (DCA-L2), and the new server I/O  architecture  (new). LRO  is  included  in  all  configurations. Since large I/Os have all three major  overheads, we present  large I/O results  in  this  subsection.       Figure 12 illustrates network throughput achieved by  various configurations. We also present corresponding  core utilization and utilization breakdown in Figure 13.  As shown in the figures, orig can achieve only ~8  Gbps throughput by consuming ~225% core utilization  in the SUT with four cores. Memory subsystem is the  potential bottleneck of achieving line rate throughput  and an increase in CPU performance could not further  improve throughput. We observe from Fig. 13 that data  copy, the NIC driver and buffer release are three major  overheads. By injecting network data into L1 caches,  DCA-L1 eliminates the memory stalls to packets and  obtains  line rate  throughput using ~200% core  utilization. Utilization breakdown reveals that the  higher  network  processing  efficiency  or  throughput/core is from CPU cycle savings in data      copy. Instead of L1 caches, DCA-L2 routes network  data into a larger L2 cache. It achieves line rate  throughput and consumes fewer CPU cycles than  DCA-L1. That is because DCA-L1 delivers ~64 packets  or ~96KB data for each interrupt into small L1 caches  of 32 KB each, incurring cache pollution. With high  speed networks like 10GbE and beyond, DCA-L2 is a  more practical approach.    10 t ) s u p h p b u o G g r ( h T 5 0 configurations achieve similar hit ratios in L1 cache  except DCA-L1 and new. Due to small cache sizes,  DCA-L1 results in L1 cache pollution and decreases  the L1 cache hit ratio. New bypasses L1 caches during  data copy and has a higher L1 cache hit ratio. We do  not present results for  the sender side because  performance is not significantly improved.    100% 95% 90% 85% L1 Cache L2 Cache s o i t a r t i h e h c a C orig DCA-L1 DCA-L2 new Figure 12. Network throughput  t orig DCA-L1 DCA-L2 new Figure 14. Cache hit ratios  4 2 0 ) s u p h p b u o G g r ( h T orig DCA-L1 DCA-L2 new Figure 15. Web server throughput  TCP/IP  procesisng users 100% 50% 0% orig DCA-L1 DCA-L2 new Figure 16.  Utilization breakdown   5.2. Web Server Performance      We also studied web server performance by running  the web server benchmark SPECweb99 over 10GbE.  The same configurations as subsection 5.1 were used.  Web server throughput with various configurations is  illustrated in Figure 15, where the server achieves  ~2.8Gbps, ~3.1Gbps and ~3.3Gbps throughput in orig,  DCA-L1 and DCA-L2. CPU utilization breakdown in  Figure 16 reveals that throughput increases are from  the CPU cycle savings in network processing. In the  new architecture, the network processing overhead is  further reduced due to the elimination of the memory  stalls and more efficient data copy. The improved  network processing translates up to ~3.8Gbps server  throughput, 14% better than DCA-L2.      5.3. NIC Design Benefits      Besides having efficient network processing, the  new server I/O architecture also simplifies NIC  hardware designs by lessening pressure on DMA  engine and avoiding extensive NIC buffers. We  measure round-trip time over PCI-E bus on mainstream  servers and assume  that each PCI-E  transaction  (typically, 256 bytes transaction size) transfers 16  n o i t a z i l i t U e r o C 250% 200% 150% 100% 50% 0% Others Iperf System Call Data Copy TCP/IP Buffer Release NIC Driver orig DCA-L1 DCA-L2 new Figure 13. Utilization breakdown      Although DCA is able to reduce the data copy  overhead, it is unable to resolve the performance issues  in other components such as the driver and buffer  release. The new I/O architecture not only avoids  memory stalls in the driver and buffer release, but also  further improves data copy performance. Fig. 12 and  Fig.13 show that it obtains line rate throughput but  substantially reduces core utilization to ~125%. The  utilization breakdown confirms that the reduction is  from  the driver, buffer release and da ta copy.  Compared to DCA-L2 which is employed in recent  commercial servers, the new I/O architecture reduces  core utilization by 33%, corresponding to 47% network  processing efficiency improvement. The reduced core  utilization or higher processing efficiency means that  the cores can be better used for application processing  instead of network processing.      Additionally, we also investigate cache behavior of  high  speed network processing under various  configurations in Figure 14. We observe that orig only  achieves a 92% L2 cache hit ratio. By avoiding the  memory stalls to packets, both DCA-L1 and DCA-L2  increase L2 cache hit ratios to 96%. The new  architecture almost avoids memory stalls during  network processing and escalates the L2 cache hit ratio  to 99%. The higher L2 cache hit ratio explains the  benefits of core utilization shown in Fig.13. All  263                   descriptors. We obtain average per packet time for  descriptor read/write by amortizing the round -trip time  over the number of descriptors per transfer.  Packets  themselves can be transferred in a pipelined way and  do not stress DMA engine.  Assuming DMA engine  runs at 200MHz, time of a MTU packet spent on DMA  engine is illustrated in Figure 17. It shows that the new  architecture substantially ameliorates DMA engine  pressure. Although results for DCA configurations are  not shown, they do not avoid long latency descriptor  fetches/writes and behave the same as orig. In addition  to the benefits from DMA engine, the new I/O  architecture also reduces NIC buffers. Our experiment  results show that it only needs 8KB buffer (4KB buffer  in the NEngine and 4KB buffer in the NIC) for the  10Gbps network, but more than 512KB NIC buffer is  needed in traditional I/O architectures. With 40Gbps  and 100Gbps networks, the new I/O architecture will  achieve much higher benefits. In the new architecture,  NEngine essentially behaves similarly to DMA engine  but simplifies designs of DMA engine and reduces  NIC buffers. Therefore, it saves overall hardware cost  (CPU+NIC) and offers a promising I/O solution for  high speed networks.   ) s n ( e m i t 150 100 50 0 descriptor packet orig new Figure 17.  Per packet time on DMA Engine  6. Related Work      A wide spectrum of research has been done to  understand the network processing overhead [18-22,  25, 28, 38]. Nahum et al. [28] used a cache simulator  to study cache behavior of the TCP/IP protocol and  showed that instruction cache has the greatest effect on  network performance. Similarly, Zhao et al. [38]  revealed that packets and DMA descriptors exhibit no  temporal locality. Makineni et al. [25] conducted  architectural characterization of TCP/IP processing on  Pentium M microprocessors and concluded that the  receive side is more memory-intensive than the send  side. Unfortunately, they did not conduct a systemwide architectural analysis for high speed network  processing on mainstream servers. Moreover, due to a  lack of fine-grained instrumentation, none of them  located the performance bottlenecks.      In addition, researchers have proposed several  architectural schemes  to optimize  the processing  efficiency [1-3, 9, 27, 35, 38]. Most of them aimed to  264 reducing the data copy overhead. Muker jee et al. [27]  put a NIC in coherent memory to improve the  performance by facilitating transfers of whole cache  blocks and reducing control overheads. Zhao et al. [38]  designed an off-chip DMA engine close to memory to  move data inside memory. The similar idea has been  implemented in the Intel I/OAT technique [1], but has  little performance improvement because memory stalls  to packets are still incurred. To eliminate the memory  stalls, Intel proposed DCA to route network data into  caches [9], and implemented it in Intel 10 GbE NICs  and server chipsets. Its performance evaluation on real  servers has demonstrated overhead reduction in data  copy [18-19]. Recently, Tang et al. [35] claimed that  DCA might incur cache pollution on small LLC and  introduced two cache designs (a dedicated DMA cache  or limited ways of LLC) to keep packets. Similar to  our work, Binkert et al. [2-3] integrated a redesigned  NIC  to  reduce  the processing overhead by  implementing zero-copy and reducing access latency to  NIC registers. The major difference between the  integrated NIC and our designs lies in as follows: 1)  our architecture only  integrates DMA descriptor  management onto CPU rather than the whole NIC,  leveraging existing NIC designs and reducing CPU die  area; 2) the integrated NIC targets at data copy and  uses PIO to move data from NICs lacking scalability  over a large number of cores. Our architecture  enhances the legacy DMA mechanism and uses  efficient on-chip data movement to attack all three  major performance challenges; 3) instead of multiple  queues in NICs, our designs leverage caches for  keeping packets, thus further saving NIC cost and  achieving better NIC scalability over cores.  7. Conclusion  As network speed continues to grow, it becomes  critical  to understand and address challenges of  network processing in servers. In this paper, we first  studied the per-packet processing overhead on servers  with 10GbE and pinpointed three bottlenecks: data  copy,  the driver and buffer release. Then, we  instrumented the driver and OS to do a system-wide  architectural analysis. Unlike existing tools attributing  CPU cost at the function level, our instrumentation was  done with fine granularity to reveal exact bottlenecks.  Motivated by the studies, we proposed a new server  I/O architecture that addresses all three performance  challenges by using  extended on-chip DMA  descriptors and efficient payload movement. It allows  DMA engine to have very fast access to descriptors  alleviating burden on the DMA engine and keep  packets in CPU caches avoiding extensive NIC buffers.  Evaluation results show that the new architecture           significantly improves network processing efficiency  and achieves better web server performance while  reducing the NIC hardware complexity. Given the  trend towards rapid evolution of network speed in  future, we view the new architecture as a promising  I/O solution.   Acknowledgements  The research was supported by NSF grants CCF0811834, CSR-0912850, and a grant from Intel  Corporation.   "
2011,HAsim - FPGA-based high-detail multicore simulation using time-division multiplexing.,"In this paper we present the HAsim FPGA-accelerated simulator. HAsim is able to model a shared-memory multicore system including detailed core pipelines, cache hierarchy, and on-chip network, using a single FPGA. We describe the scaling techniques that make this possible, including novel uses of time-multiplexing in the core pipeline and on-chip network. We compare our time-multiplexed approach to a direct implementation, and present a case study that motivates why high-detail simulations should continue to play a role in the architectural exploration process.","HAsim: FPGA-Based High-Detail Multicore Simulation Using Time-Division Multiplexing Michael Pellauer∗ , Michael Adler† , Michel Kinsy∗ , Angshuman Parashar† , Joel Emer∗ † ∗Computation Structures Group Computer Science and A.I. Lab Massachusetts Institute of Technology {pellauer, mkinsy, emer}@csail.mit.edu †VSSAD Group Intel Corporation {michael.adler, angshuman.parashar, joel.emer}@intel.com Abstract—In this paper we present the HAsim FPGAaccelerated simulator. HAsim is able to model a sharedmemory multicore system including detailed core pipelines, cache hierarchy, and on-chip network, using a single FPGA. We describe the scaling techniques that make this possible, including novel uses of time-multiplexing in the core pipeline and on-chip network. We compare our timemultiplexed approach to a direct implementation, and present a case study that motivates why high-detail simulations should continue to play a role in the architectural exploration process. Index Terms—Simulation, Modeling, On-Chip Networks, Field-Programmable Gate Arrays, FPGA added to a general-purpose computer via a fast link such as PCIe [4], HyperTransport [5], or Intel Front-Side Bus [6]. On an FPGA, adding detail to a model does not necessarily degrade performance. For example, adding a complex reorder buffer (ROB) to an existing core uses more of the FPGA’s resources, but the ROB and the rest of the core will be simulated simultaneously during a tick of the FPGA’s clock. Similarly, communication within an FPGA is fast, so there is great incentive to ﬁt interacting structures like cores, caches, and OCN routers onto the same FPGA. I . IN TRODUC T ION Gaining micro-architectural insight relies on the architect’s ability to simulate the target system with a high degree of accuracy. Unfortunately, accuracy comes at the cost of simulator performance—the simulator must emulate more detailed hardware structures on every cycle, thus simulated cycles-per-second decreases. Naturally, there is a temptation to reduce the detail of the model in order to facilitate efﬁcient simulation. Typical simulator abstractions include ignoring wrong-path instructions, or replacing core pipelines with abstract models. While such low-ﬁdelity models can help greatly with initial pathﬁnding, the best way for computer architects to convince skeptical colleagues remains a cycle-by-cycle simulation of a realistic core pipeline, cache hierarchy, and on-chip network (OCN). While parallelizing the simulator can recover some performance, parallel simulators have found their performance limited by communication between the cores on the OCN, and have been forced to reduce ﬁdelity in the OCN in order to achieve reasonable parallelism [1], [2], [3]. In this paper we advocate an alternative approach— hosting the simulator on a reconﬁgurable logic platform. This is facilitated by an emerging class of products that allow a Field Programmable Gate Array (FPGA) to be In this paper we present HAsim, a novel FPGAaccelerated simulator that is able to simulate a multicore with a high-detail pipeline, cache hierarchy, and detailed on-chip network using a single FPGA. HAsim is able to accomplish this via several contributions to efﬁcient scaling that are detailed in this paper. First, we present a ﬁne-grained time-multiplexing scheme that allows a single physical pipeline to act as a detailed timing-model for a multicore. Second, we extend the ﬁne-grained multiplexing scheme to the on-chip network via a novel use of permutations. We generalize our technique to any possible OCN topology, including heterogeneous networks. We compare HAsim’s time-multiplexing approach to a direct implementation on an FPGA. Finally, we use HAsim to study the degree that realism in the core model can affect OCN simulation results in a shared-memory multi-core, an argument for the continued value of highdetail simulation in the architectural exploration process. This paper only considers a single FPGA accelerator. A complementary technique for scaling simulations is to partition the model across many FPGAs. However we do not consider this a limitation, as in order to maximize capacity of the multi-FPGA scenario we must ﬁrst maximize utilization of an individual FPGA. 978-1-4244-9435-4/11/$26.00 ©2011 IEEE  406   Fig. 1. (A) CAM Target (B) Simulating the CAM with a RAM and FSM over multiple FPGA cycles. Fig. 2. (A) Large Cache Target (B) Simulating the cache using a memory hierarchy. I I . T ECHN IQU E S FOR SCA L ING FPGA -ACC E L ERAT ED S IMU LAT ION A. Background: FPGAs as Simulation Accelerators Using FPGAs to accelerate processor simulation revolves around the realization that one tick of the FPGA clock does not have to correspond to one tick of the simulated machine’s clock. The goal is not to conﬁgure the FPGA into the target hardware, but into a performance model that accurately tracks how many model clock cycles the operations in the target are supposed to take. This allows the model to simulate FPGA-inefﬁcient structures using FPGA-efﬁcient components, while using a separate mechanism to ensure their simulated timings match the target circuit. For example, a Content-Addressable Memory (CAM) would be inefﬁcient to implement directly on an FPGA, resulting in high area and a long critical path. However we can simulate a CAM using a single-ported Block RAM and an FSM that sequentially searches the RAM, as shown in Figure 1. The FSM may take more or fewer FPGA cycles to search the RAM, depending on occupation. However the model clock cycle is not incremented until the search is complete. Taking more or fewer FPGA cycles affects the rate of simulation, but does not affect the results. Thus the simulator architect is able to trade increased time for decreased utilization—if this tradeoff improves the FPGA clock rate and the FPGA-cycle-toModel-cycle Ratio (FMR) remains favorable then this tradeoff is worth making. Detailed discussions of these techniques are given in [7], as well as Chiou [8],[7], Tan [9], and Chung [10]. Separating the model clock from the FPGA clock also allows the simulator to leverage the large amount of system memory in the host platform, even though the sizes and latencies may be radically different than those being simulated. In Figure 2 the simulator is run on a platform that has three levels of memory: on-FPGA Block RAM, on-board SRAM, and DRAM managed by the OS running on the host processor. The simulator wishes to use this hierarchy to simulate a 5 MB last-level cache. It can accomplish this by allocating space in the Block RAM, the SRAM, and host DRAM—essentially using 3 caches in place of a single large cache. To simulate an access of the target cache the FPGA ﬁrst checks if the line is resident in the Block RAM. If it is, the simulator can quickly determine if the access hit or missed. Otherwise, it must access the SRAM or DRAM, and possibly add the response to the BRAM. In this case, in the rate of simulation will be slower, dependent on the distance of the memory where the line resides. But note that the level of physical memory accessed affects only the rate of simulation, and is orthogonal to whether or not the simulated 5MB cache hit or missed. To facilitate interfacing the simulator with the host system, HAsim uses the LEAP virtual platform [11], [12]. An FPGA-accelerated simulator is composed of many parallel modules, each of which can take an arbitrary number of FPGA cycles to simulate a model cycle. The problem now becomes connecting them together to form a consistent notion of model time. In HAsim this is done by representing the model using a portbased speciﬁcation [7], as shown in Figure 3A. In such a speciﬁcation the model is represented as a directed graph of modules connected by ports. In order to simulate a model cycle each module reads all of its input ports, computes local updates, and writes all of its output ports. If a module does not wish to transmit a message then it sends a special NoMessage value. Since each port has a message on it for every model cycle, the messages themselves can be thought of as tokens that enumerate the passage of model time. Port-based speciﬁcations predate FPGA implementation [13], but are a natural ﬁt as they allow individual modules to make a local decision about whether to simulate the next cycle, without the need for global synchronization. 407 Fig. 3. (A) Port-based model of a processors’ PC Resolve stage. (B) Time-multiplexing between 4 virtual instances. Liberty [14] ProtoFlex [10] UT-FAST [8], [15] RAMP Gold [16] HAsim Functional Model FPGA Software N/A FPGA FPGA Timing Model Software FPGA FPGA FPGA Time Multiplexed No Yes No Yes Yes Num Cores 16 16 16 64 16 Core Detail * * Yes No Yes OCN No No No No Yes Comments *Uses hard PowerPCs on FPGA. *SMARTS-style functional/timing split. Software feeds trace to FPGA, which adds timing and may rollback software. Focuses on efﬁcient simulation of cache models with abstract cores and no network. Model generalized cores, including out-of-order, superscalar. Fig. 4. Comparison of FPGA-based processor simulators. HAsim also employs a timing-directed approach, whereby the simulator is partitioned into a functional and timing model [17]. As in a traditional software simulator, the functional model is responsible for correct ISA-level execution, while the timing model adds micro-architecture speciﬁc timings such as branch predictions [18]. This technique is also employed by FPGAaccelerated simulators Protoﬂex [10], UT-FAST [8], and RAMP Gold [9]. In each case, the details of the partitioning schemes are different, as shown in Figure 4. The goal of the partitioning is to reduce the development effort associated with FPGAs: the functional model is written once, veriﬁed, optimized, and used across many different timing models. B. Fine-Grained Time-Multiplexed Simulation Separating the model clock from the FPGA clock can help with scaling speciﬁc structures within a target circuit, but experience has shown that it does not save enough space to allow duplicating high-detail cores, caches, and routers into a multicore conﬁguration on a single FPGA. Given this, time-division multiplexing is a technique that can help enable scaling our models to larger multicores. In such a scheme a single physical core is used to sequentially simulate several virtual instances that ﬂow through the pipeline in sequence. Internal core state such as the program counter (PC) or register ﬁle (RF) is duplicated, but the combinational logic used to simulate each pipeline stage is not.1 The disadvantage to timemultiplexing is that it can reduce simulation rate, as a single physical pipeline is being used sequentially to do the work of many. The time-multiplexing approach was ﬁrst used in the Protoﬂex simulator [10]. Protoﬂex multiplexes a functional model between 16 threads, but does not support any timing model on the FPGA. RAMP Gold [16] is another FPGA-accelerated simulator that uses a coarsegrained approach whereby a scheduler chooses a virtual instance to simulate, and performs the functional emulation of that instance without adding any timing model of the core. RAMP Gold does support timing models of caches, but does not currently support simulations of on-chip networks. A contribution of HAsim is to extend previous multiplexing schemes to detailed timing models of core pipelines, while simultaneously minimizing any performance reduction from sequential time-multiplexing. HAsim accomplishes this by using the ports between modules to implement time-multiplexing: at simulator startup the ports are initialized with message tokens from each virtual instance, as shown in Figure 3B. 1 This kind of multiplexing bears a resemblance to multi-threading in real microprocessors, but it is important to distinguish that this is a simulator technique, not a technique in the target architecture. The cores being multiplexed may or may not support multi-threading. 408 Fig. 5. (A) Target multicore with uni-directional ring network. (B) Multiplexed core connected to ring-network via sequential de-multiplexing. In this scheme each stage of the core pipeline can be simulating a separate virtual core. For instance, the Fetch stage may be simulating Core 3 while the Decode stage simulates Core 2. Furthermore, modules that are implemented using multiple FPGA cycles per model cycle may themselves be pipelined. This ﬁne-grained time multiplexing minimizes impact on simulation rate by improving the utilization of the physical execution units. For example, if the CAM from Figure 1 were connected to the cache from Figure 2 then we would expect the rate of simulation to be limited by off-chip accesses in the cache, and during this time the CAM would mostly be idle. If this simulator were time-multiplexed, then the CAM module will not go idle until it has simulated all of the other virtual instances. Thus in many instances N -way time-multiplexing of a module does not slow the module’s simulation rate by N . In fact, the simulation rate will not be affected at all until N grows beyond the current rate-limiting step. A study detailing the sub-linear slowdown of HAsim’s scaling is presented in Section V-C. Note that time-multiplexing scheme is possible only because the state of the different cores being simulated is independent. That is, the Register File of Core 0 cannot directly affect the Register File of Core 1. Only by going through the OCN can the various cores affect each other’s simulation results. Because of this crossinstance communication, traditional time-multiplexing is insufﬁcient for modeling the OCN—different techniques are needed that can take the interaction into account while still exploiting ﬁne-grained parallelism. I I I . T IM E -MU LT I P L EX ED S IMU LAT ION O F ON -CH I P N E TWORK S V IA P ERMU TAT ION S A. First Approach: De-multiplexing The previous section established that timemultiplexing the core works well because it improves both scaling and utilization. Now, the problem becomes attaching a single physical (time-multiplexed) core to an on-chip network. Consider the ring network shown in Figure 5A. Each router has 4 ports that communicate with the core/cache: msgIn, creditIn, msgOut, and creditOut. Additionally each router has 4 more ports that communicate with adjacent routers: msgToNext, creditFromNext, msgFromPrev, creditToPrev. A baseline approach to simulating this target is to fully replicate the routers, and synthesize an on-chip network directly. The messages from the cores are then sequentially de-multiplexed and sent to the appropriate router. Each router can now simulate its next model cycle when data arrives. Responses are re-multiplexed and returned to the cores. This situation is shown in Figure 5B. In this ﬁgure and throughout the paper we represent sequential de-multiplexing by augmenting a de-multiplexor with a sequence denoting where each sequential arrival is to be sent. In this case the ﬁrst arrival is sent to router 0, the second to router 1, and so on. While this scheme is functionally correct, it presents many practical challenges. Most signiﬁcantly, the physical core is now no longer adjacent to any particular router. Thus the FPGA synthesis tools are presented with the difﬁcult problem of routing the de-multiplexed signals to the individual routers. Second, the routers themselves are under-utilized: at any given FPGA cycle only a small subset of routers are actively simulating the next model cycle—most are waiting for their corresponding virtual core to the produce data for a given model cycle. HAsim solves these problems by extending the time-multiplexing to the OCN routers themselves via a novel use of permutations. 409 Fig. 6. Time-Multiplexing the ring is complicated by the cross-router ports/dependencies. Fig. 7. Connecting the credit ports to each other, and the message ports to each other, and applying permutations to the messages. Fig. 8. Simulating a model cycle for ring network via permutations. B. Time-Multiplexed Ring Network via Permutation If we wish to time-multiplex the ring, observe that the simulation of router n is complicated by the communication from routers n − 1 and n + 1. As shown in Figure 6, it is the ports that cross between routers that present a challenge to time-multiplexing, as they express the fact that the differing virtual instances’ behaviors are not independent. How can we ensure that each crossvirtual instance port’s messages are transmitted to the correct destination? The key insight, as shown in Figure 7, is that we can connect these ports to themselves. That is, the output from msgToNext is fed into msgFromPrev, and creditFromNext produces creditToPrev. This makes sense intuitively: messages leaving one router are the input to the next router. However, simply making the connection is not sufﬁcient: router n produces the message for router n + 1, not for router n. One way to solve this would be to store cross-router communication in a RAM. The index of the RAM to be read and written by each virtual index would be calculated by accessing an indirection table. This approach is similar to the way a single-threaded software simulator simulates an on-chip network. The disadvantage is that a random-access memory is overkill, as the accesses are actually following a static pattern determined by the topology. HAsim’s insight is that the communication pattern can be represented by a small permutation. For the msg port the output from router 0 is the input for router 1 (on the next model cycle), 1 is for 2, and so on to N − 1, which is for 0. For the credit port 0 goes to N − 1, 1 to 0, 2 to 1, and so on. The advantage of this approach is that these permutations can be represented using two queues: a main queue and a side buffer. A small FSM determines which queue will be enqueued to, and which queue will be dequeued from. Formally, given N cores the permutation σ for the xth input of each port is as follows: • σmsg (x) = x + 1 mod N • σcredit (x) = x − 1 mod N In this paper we will express the permutations as shown in Figure 7: a concrete table showing that the output for core 0 is sent to core 1, and so on, until core 5’s output is sent to core 0. This table is then supplemented with a generalized formula that scales the permutation to any number of routers. Given these permutations, Figure 8 shows a complete example of simulating a model cycle in the ring network. In 8A the messages are in their initial conﬁguration. The router simulates the next model cycle, consuming 410 Fig. 9. Time-multiplexing a torus network. Cores/caches are not pictured. Credit ports are omitted as they use the same permutations. Fig. 10. Simulating a grid network using the same permutations as the torus and sending NoMessage messages on non-existent edges. N inputs and producing N new outputs, resulting in the state shown in 8B. After the permutation is applied we can conﬁrm that the resulting conﬁguration in 8C is correct: on the next model cycle router 0 will receive the message from router 3, and the credit from 1. Router 1 will receive the message from router 0, and credit from router 2, and so on. Although we present this execution as happening in three separate phases, on the FPGA we can overlap the execution. C. Time-Multiplexed Torus/Grid Let us extend the permutation technique to another topology, the 2D torus shown in Figure 9A. Here each router has ports going to/from 4 directions: msgFromNorth, msgFromEast, msgFromSouth, msgFromWest and so on, as well as ports/to from the local core. As shown in Figure 9B the msgToEast port is connected to the msgFromWest port and so on, as expected. However, compared to the ring network the permutation is different to reﬂect the width of the torus. In order to simulate the cores in numeric order, the permutation for the East/West ports for a network of width w is: • σmsgF romEast (x) = x + 1 mod w • σmsgF romW est (x) = x − 1 mod w Similarly the permutation for the North/South port must take into account the width of the network (not the height): • σmsgF romN orth (x) = x + w mod N • σmsgF romSouth (x) = x − w mod N Similarly router 0 will receive messages from Note output that these permutations mean from router 0 will be sent that the to routers σmsgF romN orth (0) = 3, σmsgF romEast (0) = 1, σmsgF romSouth (0) = 6, and σmsgF romW est (0) = 2. σmsgF romN orth (6) = 0, σmsgF romEast (2) = 0, σmsgF romSouth (3) = 0, σmsgF romW est (1) = 0, corresponding exactly to the original target. Once we have a torus model it is straightforward to alter this model to simulate a grid topology such as the one shown in Figure 10. We will not do this by altering the permutations or physical ports of our network, but rather by just altering the routing tables to send NoMessage (Section II-A) along the links that do not exist in the grid network. For instance, router 0, in the Northwest corner, will only send NoMessage West or North. If other routers obey similar rules then it will only receive NoMessage from those directions as well. 411 Fig. 11. Building permutations for an arbitrary network. The permutations given in this section assume that the ﬁrst processor that should be simulated (core 0) is located in the upper left-hand corner of the topology. If the architect desired a different simulation ordering they could accomplish this by changing the permutation — analogous to a sequential software simulator of a grid changing the order of indexing in a for-loop. IV. G EN ERA L I Z ING TH E P ERMU TAT ION T ECHN IQU E The permutations described earlier correspond to picking the simulation order of the routers in the network and properly routing the data between them, similar to how a sequential software simulator cycles through nodes in sequence. It is always possible to create a sequential simulator for any valid OCN topology. In this section we demonstrate that it is similarly always possible to construct a set of permutations to allow any valid topology to be time-multiplexed. A. Permutations for Arbitrary Topologies Assume that the target OCN has been expressed as a port-based model: a digraph G = (M , P ) where M is the modules in the system and P is the ports connecting them. Label the modules M with a valid simulation ordering [0, 1, .., n] such that 0 is the ﬁrst node simulated and n is the last. Note that if the graph contains zerolatency ports then not all simulation orderings will be valid. However if the graph represents valid hardware then there is guaranteed to exist at least one valid simulation ordering. Once the simulation ordering is picked we must combine the ports into as few time-multiplexed ports as possible. To do this we divide the edges P into the minimum number of sets P0 , P1 ..Pm such that each set Pm obeys the following properties: • ∀{s, d} ∈ Pm , ¬∃{s(cid:2) , d(cid:2) } ∈ Pm .s = s(cid:2) • ∀{s, d} ∈ Pm , ¬∃{s(cid:2) , d(cid:2) } ∈ Pm .d = d(cid:2) In other words, no two ports in any given set can share the same source, or share the same destination. Each set Pm corresponds to a permutation that we must construct in our time-multiplexed model. Ensuring that no source or destination appears twice ensures that we will construct a valid permutation. We construct permutations σ0..n : M → M using the following rule: • ∀{s, d} ∈ Pm , σm (s) = d The remaining range of σm represent “don’t-care” values and so may be chosen in any way that creates a valid permutation. (It is possible that certain permutations will be cheaper to implement on an FPGA than others.) Finally, each permutation should be associated with a port of the physical module. This module can be timemultiplexed using standard techniques (Section II-B), with one additional restriction: the time-multiplexed module should ensure that NoMessages are sent on port m for undeﬁned values in the range of σm . This represents the fact that these output ports do not exist for a particular virtual instance. The torus/grid discussion in Section III-C is an example of this phenomenon. Figure 11 shows an example applying this process to an arbitrary, irregular topology. First a desired simulation order is selected (11A). The ports are arranged into three sets (11B), the fewest possible for this example. These sets then form the basis of permutations (11C). The don’t-care values of the permutations can be can be resolved in any way that creates a legal permutation. The router is time-multiplexed across 6 virtual instances, and the virtual instances are arranged to send NoMessage values on non-existent ports. For example, instance 0 will send NoMessages on two of the output ports, as the original router 0 only had one output port. The meaning of undeﬁned values in the permutations can clearly be seen when we apply the technique to a star network topology (Figure 12). The resulting timemultiplexed network has the same number of physical ports as the grid network, but the permutations themselves are different. Each leaf node only contains a subset of nodes of the hub, and thus will send NoMessage on ports that do not exist for them. Given this, the undeﬁned values in the permutations can be ﬁlled in using straightforward modular arithmetic. 412 Fig. 12. Multiplexing a star topology results in many undeﬁned values representing non-existent ports. Fig. 13. A heterogeneous grid, where routers connect to different types of nodes. Fig. 14. Time-multiplexing the heterogeneous network via interleaving. B. Heterogeneous Network Topologies V. A S S E S SM EN T Thus far we have presented OCNs where all of the routers are connected to homogeneous cores. This has kept the examples pedagogically clear, but is unrealistic. Architects often wish to study multicores such as those shown in Figure 13, a 3x3 grid that contains a memory controller, 2 last-level caches, and 6 cores. The cores and caches will be simulated using time-multiplexing. How then can we connect them to our permutationbased grid? The answer is to sequentially multiplex the streams together, pass them to the time-multiplexed OCN, and de-multiplex the responses (Figure 14). Unlike the original de-multiplexing approach presented in Section III-A this imposes no difﬁcult routing problem on the synthesis tools, as the modules being connected are time-multiplexed physical cores. A key advantage of this technique is that it requires no changes to the individual modules—they can be time-multiplexed independently using established techniques. This same technique allows for efﬁcient timemultiplexing of indirect network topologies such as butterﬂies, omitted for space considerations. A. Time-Multiplexing versus Direct Implementation In this section we compare HAsim’s time-multiplexed approach to Heracles [19], a traditional direct implementation of a shared-memory multicore processor on an FPGA. Heracles aims to enable research into routing schemes by allowing realistic on-chip-network routers to be paired with caches and cores, and arranged into arbitrary topologies. Heracles emphasizes parameterization in an effort to ﬁt in many different existing FPGA platforms. A comparison of a typical Heracles implementation and a typical HAsim model is shown in Figure 15. We synthesized both conﬁgurations using Xilinx ISE 11.5, targeting a Nallatech ACP accelerator [6], which connects a Xilinx Virtex 5 LX330T FPGA to a hostcomputer via Intel’s Front-Side Bus protocol. The resulting FPGA characteristics are shown in Figure 16. Heracles is speciﬁcally made for efﬁciency, but the FPGA synthesis tools still have a problem scaling a complete system with core, cache, and router. This is because duplicating Heracles’ caches exceeds the FPGA’s Block 413 Core ISA Multiply/Divide Floating Point Pipeline Stages Bypassing Branch policy Outstanding memory requests Address Translation Store Buffer Level 1 I/D Caches Associativity Size Outstanding Misses Level 2 Cache Size Associativity Outstanding Misses On-Chip Network Topology Routing Policy Virtual Channels Buffers per channel Heracles HAsim 32-Bit MIPS Software Software 7 Full Stall 64-Bit Alpha Hardware Hardware 9 Full Predict/Rollback 1 None None Direct 16KB 1 None None None Grid X-Y DO Wormhole 2 4 16 Translation Buffers 4-entry Direct 16 KB 16 256 KB 4-way 16 Grid X-Y DO Wormhole 2 4 Fig. 15. Component features of Heracles and HAsim. Registers Heracles 2x2 44,512 (21%) 3x3 65,602 (31%) 4x4 DNF HAsim (16-way multiplexed) 4x4 120,213 (57%) Lookup Tables BlockRAM 33,555 (16%) 59,394 (28%) DNF 328 (101%) 738 (227%) DNF 165,454 (79%) 88 (27%) Fig. 16. approach. Scaling a direct implementation versus the multiplexing RAM capacity. The synthesis tool was able to complete even in the presence of overmapping for the 2x2 and 3x3 conﬁgurations, but ran out of memory for the 4x4 case. We estimate that cache sizes would have to be reduced by a factor of 16 in order to successfully ﬁt onto this FPGA. In contrast, despite HAsim’s signiﬁcantly increased level of detail, we are easily able to ﬁt a 4x4 multicore with L1 and L2 caches onto the same FPGA. This is due to four factors discussed earlier: First, separating the model clock from the FPGA clock allows efﬁcient use of FPGA resources (Section II-A). Second, use of off-chip memory allows large memory structures like caches to be modeled using few on-FPGA Block RAM (Section II-A). Third, using a partitioned simulator allows HAsim to reduce the detail necessary in the timing model (Section II-A): it is well-known that timing models of caches need to store tags and status bits, but not the actual data. Most signiﬁcantly, the HAsim 4x4 model is actually a single physical core, single cache, and single router that has been time-multiplexed 16 ways (Section III). HAsim is an example of a space-time tradeoff. These techniques allow us to ﬁt much more detail onto a single FPGA, paying for scaling by reducing simulation rate. Since at most one virtual instance can complete the physical pipeline per FPGA cycle, it takes a minimum of 16 FPGA cycles to simulate one model cycle. As the FPGA is clocked at 50 MHz, this gives HAsim a peak performance of 50/16 = 3.125 MHz, multiple orders of magnitude faster than software-only industry models that are comparable levels of detail [8], [13]. B. Case Study: Effect of Core Detail on OCN Simulation It is not uncommon for architects who wish to study an OCN topology to reduce the level of detail in the core pipeline for the sake of efﬁcient simulation. In such a situation the architect is hoping that the ability to run an increased variety of benchmarks will offset the increased margin of error of each run. It our hope that FPGA-accelerated simulators will present an alternative to reducing ﬁdelity. This idea is particularly appealing if the FPGA means that the extra detail has minimal impact on simulation rate. In order to evaluate the impact core ﬁdelity can have on both simulation results and simulation rate, we modeled 2 multicore systems that differed only in the core pipelines. The ﬁrst is a 1-IPC “magic” core running Alpha ISA that stalls on cache misses, similar to an architectural model. The magic core will never have more than one instruction in ﬂight, and thus never produce more than one simultaneous cache miss. The second is the 9-stage pipeline described in Figure 15. This core does not reﬂect any particular existing architecture, but rather is representative of the general result of adding a higher-level of detail to the simulator. Each core was then connected to the cache hierarchy described in Figure 15 and arranged into 4 different grid conﬁgurations: 1x1, 2x2, 3x3, and 4x4. In each case one of the nodes was occupied by the memory controller, so the 4x4 conﬁguration consisted of 15 core/cache pairs and 1 memory controller. It is well-known that adding more cores to a sharedmemory multicore can degrade the average IPC of each individual core, as contention on the OCN increases. This phenomenon represents a typical concern that an architect would like to characterize for a proposed OCN topology. We used HAsim to characterize the reported IPC of the individual cores running a variety of integer benchmarks, ranging from microkernels like Towers of Hanoi and vector-vector multiplication, to SPEC 2000 benchmarks gzip, mcf, and bzip2. 414 0 0 0 0 0 0 8 8 8 8 8 8 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 5 5 5 5 5 1 1 1 1 1 . . . . . 0 0 0 0 0 0 0 0 0 0 . . . . . M M M M M R R R R R E E E E E T T T T T C C C C C M M M M M L L L L L 0 0 0 0 0 0 0 0 0 0 0 0 . . . . . . . . ( a ) 1 x 1 M R R E T T C C M L L 1 . 0 0 0 . 3 6 0 . 3 6 0 . 0 5 0 . 8 5 0 . 0 0 ( b ) 2 x 2 M R E T C M L 0 0 1 . 0 . 2 6 0 . 6 6 0 . 4 6 8 6 0 . 4 6 0 . 0 . 0 5 5 5 0 . 0 0 5 5 0 0 0 . 6 5 0 . 0 0 . ( c ) 3 x 3 0 0 8 8 5 5 0 . 4 7 M E M 1 . 0 0 0 . 2 3 0 0 1 1 4 4 . 0 . 1 6 9 5 0 . 0 . 9 6 0 . 6 6 0 . 0 5 0 . 4 3 3 3 0 . 0 0 0 0 3 3 . 0 . 8 3 0 . 6 3 0 . 3 5 0 . 0 0 . 0 . 8 2 ( d ) 4 x 4 g F i . 7 1 . P e r C o r e I P C : M g a i c C o r e G i r d s 0 0 0 0 1 1 0 . 0 5 . 0 5 0 . M R E T C M L 0 . 0 0 ( a ) 1 x 1 1 . 0 0 M R E T C M L 0 . 0 5 0 . 4 4 0 . 2 4 0 . 4 4 0 . 0 0 ( b ) 2 x 2 M R E T C M 0 0 1 . 0 0 4 4 2 2 0 . 9 3 L 0 . 0 5 0 . 9 2 4 4 2 2 0 0 . 0 . 4 3 0 . 3 3 9 3 0 . 0 . 0 0 . 0 . 4 2 ( c ) 3 x 3 1 . 0 0 0 0 1 1 1 1 0 0 4 4 2 2 0 . 8 3 0 . 6 3 M E M 0 . 0 5 1 1 0 . 0 . 6 0 2 1 0 . 0 0 0 0 1 1 . . 1 2 0 . 0 . 1 2 5 3 0 . 0 . 0 0 7 0 0 . 0 . 6 0 0 . 6 0 . 0 . 0 1 ( d ) 4 x 4 g F i . 8 1 . P e r C o r e I P C : D e t a l i d e C o r e G i r d s 1 1 0 0 0 0 0 . 0 5 . 0 . 0 4 M R E T C M L 0 0 0 . ( a ) 1 x 1 1 . 0 0 0 . 0 5 9 1 0 . 0 . 9 1 0 . 0 0 0 . 6 1 ( b ) 2 x 2 0 0 1 . 0 . 2 4 0 0 9 9 2 2 0 . 0 5 0 . 3 3 0 . 1 3 0 . 0 3 0 0 3 3 2 2 . 0 . 5 2 0 . 0 0 0 . 5 2 . ( c ) 3 x 3 1 . 0 0 0 0 7 7 2 2 0 . 8 4 0 . 7 3 0 . 4 3 0 . 0 5 0 . 3 2 5 2 0 . . 9 2 0 . 0 . 8 2 0 0 6 6 2 2 8 3 0 . 0 . 2 3 1 3 0 . 0 . 0 0 0 . 4 2 0 . 2 2 . ( d ) 4 x 4 g F i . 9 1 . A b s o l u t e D f f i e r e c n e i n R o p e t r d e I P C 415 The results are given in Figures 17-19. They demonstrate that the reported IPC of a particular core varies 0.16-0.48 between the two models. The most variation was shown by core (1,0) in the 4x4 model—the core directly south of the memory controller. This is because in the detailed model the cores south and east of this core generate more OCN trafﬁc, due to simultaneous outstanding misses. The dimension-order routing scheme overwhelms core (1,0)’s ability to serve its local trafﬁc. In the undetailed model the reduced contention allows (1,0) to sufﬁciently warm up its caches to run without network accesses. An architect studying the detailed model might conclude to move the memory controller, or institute a different routing policy—insights that might be missed when using the magic core. All in all, these results indicate that high-detail simulation will remain a useful tool in the computer architect’s toolbox. C. Scaling of Simulation Rate Now let us examine how HAsim’s simulation rate scales as we add cores to the system. The timemultiplexing scheme means that simulating N processors has a best-case overall FPGA-cyle-to-Model-cycle Ratio (FMR) of N , with a best-case per-core FMR of 1. As a baseline, a single-core model of our processor takes an average of 19.7 FPGA cycles to simulate a model cycle across a range of SPEC benchmarks. At ﬁrst glance this seems to indicate that simulating N cores will reduce the FMR to N ∗ 19.7. (FMR would scale linearly with the number of cores.) However, as noted in Section II-B, HAsim’s ﬁne-grained multiplexing at the port granularity means that the modules themselves are implemented in a pipelined fashion. This pipelining can lower the impact of time-multiplexing. In the best-case scenario the FMR of 19.7 would mean that we could simulate 19 virtual cores without impacting FMR at all, as we could ﬁnish the simulation of a core per FPGA cycle. Unfortunately the situation is not so simple. Adding more virtual cores to the system impacts the per-core FMR of individual cores. This is because: • Virtual cores increase cache pressure on the on-chip BRAM used to model the caches (Section II-A). This can reduce the FMR of the cores (though note that it has no impact on the simulation results themselves). • The round-robin nature of the multiplexing scheme described in Section II-B means that when a particular virtual instance stalls for an off-chip access, the amount of work the rest of the system can perform FPGAͲCycleModelͲCycleRatio(FMR) 300 250 200 150 100 100 50 0 1x1 1x1 2x2 3x3 3x3 4x4 Linear slowͲ down Fig. 20. Impact on FMR of scaling inorder core to multicore. The diamonds represent linear slowdown compared to the FMR of a single core. Min FMR Overall Per-Core Simulation Rate Overall 160 KHz Per-Core 1.84 MHz 16 5 Max 218 27 Average 80 11 3.2 MHz 9.5 MHz 625 KHz 4.54 MHz Fig. 21. Comparing overall simulation rate to per-core rates. is limited. For example, if we are simulating a 4core system and Core 0 has an off-chip access then we can only simulate Core 1, 2, and 3 before we are back to 0 and cannot proceed. Thus in the worst-case simulation rate could actually scale worse than linearly with the number of cores. To test this phenomenon we used the time-multiplexed inorder core scaling between 1x1 and 4x4, as described in the previous section. The results of this scaling are shown in Figure 20. There are several interesting features of this graph that are worth exploring. First, note that when we scale from 1x1 to 2x2, the performance impact is quite minimal. In fact, in the case of the wupwise benchmark HAsim actually achieves the best-case scenario of not reducing FMR at all. This is because wupwise has a small working set that exerts very little cache pressure. On average the additional cache pressure slows the 2x2 simulation by 46% over the baseline. On average, this is signiﬁcantly better than linear a slowdown of 300%, which is indicated by the diamonds on the graph. The ﬁne-grained pipelining offsets the increased cache pressure, but not completely. As we scale to 8 and 16 cores the increased cache pressure begins to have a greater impact. Although on aggregate we are still scaling better than linear slow416 down, the difference is clearly reduced. One interesting case is wupwise, which goes from having the best 2x2 simulator performance to having the worst at 4x4. It seems that once this benchmark’s working set no longer ﬁts in the on-chip cache the impact is quite extreme. A breakdown of per-core FMR and simulation rate is given in Figure 21. It demonstrates that although the fastest simulator runs at 3.2 MHz, the average is 625 KHz. However, this rate is because we are simulating so many cores. The per-core simulation rate averages 4.54 MHz, peaking at 9.5 MHz in the best case. As simulation rates are almost entirely limited by offchip accesses, current research is focused on improving hit rates in the host memory hierarchy, either by an improved cache algorithms, or using a hardware platform with larger on-board DRAMs, or providing faster access to host memory. An alternative approach would be to loosen the round-robin multiplexing in order to keep the FPGA busy longer when off-chip accesses occur. Currently, no scheme is known that results in better performance at an acceptable hardware cost. V I . CONC LU S ION Time-multiplexed simulation of detailed multicores using FPGAs represents a new tool in the architect’s toolchest of simulation techniques. By trading spacesavings for sequentialized simulation, it allows the possibility to free up substantial FPGA area. This critically limited resource can then be utilized to increase ﬁdelity without negatively impacting simulation rate. Alternatively, a natural extension of the techniques presented in this paper is to store the state of the virtual instances off-chip. Careful orchestration of memory accesses should be able to bury much of this latency and keep the physical pipeline busy. Currently we are aiming to use the techniques discussed here to model a thousand-node on-chip network using only a single timemultiplexed FPGA. ACKNOW L EDGM EN T S The authors would like to acknowledge the help and feedback of our collaborators in the RAMP project: Arvind, Derek Chiou, James Hoe, Krste Asanovic, John Wawrzynek. Other people who have contributed code to HAsim include Muralidaran Vijayaraghavan, Kermin E Fleming, Nirav Dave, Martha Mercaldi, Nikhil Patil, Abhishek Bhattacharjee, Guanyi Sun, and Tao Wang. "
2012,Supporting efficient collective communication in NoCs.,"Across many architectures and parallel programming paradigms, collective communication plays a key role in performance and correctness. Hardware support is necessary to prevent important collective communication from becoming a system bottleneck. Support for multicast communication in Networks-on-Chip (NoCs) has achieved substantial throughput improvements and power savings. In this paper, we explore support for reduction or many-to-one communication operations. As a case study, we focus on acknowledgement messages (ACK) that must be collected in a directory protocol before a cache line may be upgraded to or installed in the modified state. This paper makes two primary contributions: an efficient framework to support the reduction of ACK packets and a novel Balanced, Adaptive Multicast (BAM) routing algorithm. The proposed message combination framework complements several multicast algorithms. By combining ACK packets during transmission, this framework not only reduces packet latency by 14.1% for low-to-medium network loads, but also improves the network saturation throughput by 9.6% with little overhead. The balanced buffer resource configuration of BAM improves the saturation throughput by an additional 13.8%. For the PARSEC benchmarks, our design offers an average speedup of 12.7% and a maximal speedup of 16.8%.","Supporting Efﬁcient Collective Communication in NoCs Sheng Ma† ‡ , Natalie Enright Jerger‡ , Zhiying Wang† †School of Computer, National University of Defense Technology, Changsha, China ‡Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada masheng@nudt.edu.cn, enright@eecg.toronto.edu, zywang@nudt.edu.cn Abstract Across many architectures and parallel programming paradigms, collective communication plays a key role in performance and correctness. Hardware support is necessary to prevent important collective communication from becoming a system bottleneck. Support for multicast communication in Networks-on-Chip (NoCs) has achieved substantial throughput improvements and power savings. In this paper, we explore support for reduction or many-to-one communication operations. As a case study, we focus on acknowledgement messages (ACK) that must be collected in a directory protocol before a cache line may be upgraded to or installed in the modiﬁed state. This paper makes two primary contributions: an efﬁcient framework to support the reduction of ACK packets and a novel Balanced, Adaptive Multicast (BAM) routing algorithm. The proposed message combination framework complements several multicast algorithms. By combining ACK packets during transmission, this framework not only reduces packet latency by 14.1% for low-to-medium network loads, but also improves the network saturation throughput by 9.6% with little overhead. The balanced buffer resource conﬁguration of BAM improves the saturation throughput by an additional 13.8%. For the PARSEC benchmarks, our design offers an average speedup of 12.7% and a maximal speedup of 16.8%.1 1 Introduction Efﬁcient and scalable on-chip communication will be required to realize the performance potential of many-core architectures. To harness this performance, it is imperative that NoCs be designed to efﬁciently handle a variety of communication primitives. Collective communication lies on the critical path for many applications; the criticality of such communication is evident in the dedicated collective and barrier networks employed in several supercomputers, such as NYU Ultracomputer [15], CM-5 [24], 1 This research was carried out while Sheng Ma was a visiting international student at the University of Toronto supported by a CSC scholarship. 978-1-4673-0826-7/12/$26.00 ©2011 IEEE s t n u o c n o i t a n i t s e D 10 9 8 7 6 5 4 3 2 1 0 Figure 1. Average destinations per multicast for the PARSEC benchmarks. Cray T3D [6], Blue Gene/L [2] and TH-1A [43]. Likewise, many-core architectures will beneﬁt from hardware support for collective communications but may not be able to afford separate, dedicated networks due to rigid power and area budgets [34]; this paper explores integrating collective communication support directly into the existing NoC. Various parallel applications and programming paradigms require collective communication such as broadcast, multicast and reduction. For example, a directory-based coherence protocol relies heavily on multicasts to invalidate shared data spread across multiple caches [19] and Token Coherence uses multicasts to collect tokens [29]. Reductions and multicasts are used for barrier synchronization [35, 42]. These collective communications can have a signiﬁcant effect on many-core system performance. Without any special hardware mechanisms, even if 1% of injected packets are multicast, there is a sharp drop in saturation throughput [12]. Recent work proposes efﬁcient multicast routing support to improve NoC performance [1, 12, 20, 26, 37, 38, 39, 41]. Often a multicast will trigger an operation, such as invalidating a cache line [19] or counting available tokens [29]. To notify the source of the completion of these operations, the multicast destination nodes send out responses. The resulting many-to-one communication operation is called a reduction [9]. Figure 1 shows that a cache line invalidation message triggers on average 7.44 acknowledgement mes  T ransac t ion Mult ic as t ) l s e c y c ( y c n e a L t 200 150 100 50 0 0.0 0.1 0.2 0.3 0.4 Injec tion rate ( flits /node/cyc le) Figure 2. Latency of multicast-reduction transaction. (Multicast: latency of last arriving multicast replica; Transaction: latency of last arriving ACK; the network is routed by BAM+NonCom as described in Section 5.) sages for the PARSEC benchmarks [3] in a 16-core system2 . Prior NoC multicast proposals [1, 12, 20, 26, 37, 38, 39, 41] implicitly assume several unicast packets will deliver these messages to a single destination; this can lead to redundant network traversals and create transient hotspots in the network. To provide high performance, scalable NoCs should handle trafﬁc in an intelligent fashion by eliminating these redundant messages. Furthermore, the multicast-reduction transaction cannot complete until all responses are received [4, 10, 19, 22, 25, 29]. As a result, the transmission of reduction messages lies on the critical path of a multicast-reduction transaction. These multicast-reduction operations are often associated with stores; for out-of-order cores, stores do not lie on the critical path. However, these stores can delay subsequent loads to hotly contended cache lines. For CMPs that employ simple, in-order cores, stores will lie on the critical path and can signiﬁcantly impact performance. Figure 2 shows the completion latency of multicast-reduction transactions in a 4×4 mesh running uniform random trafﬁc. The transmission of reduction packets accounts for ∼40% of the total transaction latency. We propose a novel packet reduction mechanism to improve performance of the full multicastreduction transaction. A noteworthy property of coherence-based reduction messages is that they carry similar information in a simple format. For example, invalidation acknowledgement messages only carry the acknowledgement (ACK) for each node and token count replies merely carry the count of available tokens at each node. Therefore, these response messages can be combined without loss of information. Combining these messages eliminates redundant network traversals and optimizes performance. We propose an efﬁcient message combination framework with little overhead. To simplify discussion, we focus on invalidation ACK packets in a directory-based coherence protocol. Our design can be easily extended to other types of reductions such as those used in Token Coherence [29]. Our proposed message combination framework complements several multicast routing algorithms. Sending a multicast packet constructs a logical tree in the network. The framework steers each ACK packet to traverse the same logical tree back to the root (source) of the multicast. In each router, a small message combination table (MCT) records total and received ACK counts for active multicast transactions. When an ACK packet arrives at the router, the MCT is checked. If the router has not received all expected ACKs, the table is updated and the incoming ACK is discarded. If the router has received all expected ACKs, the incoming ACK packet is updated and forwarded to the next node in the logical tree. Dropping in-ﬂight ACK packets reduces network load and power consumption. Our goal is to improve overall network performance in the presence of both unicast and multicast-reduction trafﬁc. The recently proposed Recursive Partitioning Multicast (RPM) routing algorithm utilizes two virtual networks (VNs) to avoid deadlock for multicasts [39]. However, the division of these two VNs results in unbalanced buffer resources between vertical and horizontal dimensions, which negatively affects performance. Therefore, we propose a novel multicast routing algorithm, Balanced, Adaptive Multicast (BAM), which does not need two VNs to avoid multicast deadlock. BAM balances the buffer resources between different dimensions, and achieves efﬁcient bandwidth utilization by computing an output port based on all the multicast destination positions. To summarize, our main contributions are the following: • An efﬁcient message combination framework that reduces latency by 14.1% and energy-delay product (EDP) by 20-40% for low-to-medium network loads and improves saturation throughput by 9.6%. • A novel multicast routing algorithm which balances of buffer resources across different dimensions and improves network throughput by an additional 13.8%. 2 Message Combination Framework In this section, we describe the proposed message combination framework. We use a multicast-reduction example to illustrate the framework. One multicast packet with destinations 0, 7 and 15 is injected by node 9. A logical multicast tree [13] is built as shown in Figure 3(a); grey nodes indicate destinations while white nodes are branches that are only traversed by the packet. Each multicast destination responds with an ACK message to the root node 9. Without combination, the ACKs are transmitted as unicast packets back to node 9 (Figure 3(b))3 . These ACK packets travel some common channels; merging them can reduce the network load. Figure 3(c) shows the logical ACK tree with message combination, which is the same as the logical 2 See Section 5 for detailed experimental conﬁguration. 3We assume the ACKs sent out by nodes 0 and 7 both traverse node 5.   9 9 9 5 15 5 15 5 15 V src ID 1 bit 4 bits 3 bits pre_rep _router 4 bits incoming _port 3 bits expected _count 4 bits cur_ACK _count 4 bits 0 7 0 7 0 7 (a) Logical Multicast tree. (b) Logical ACK tree. (c) Logical ACK tree. (w/o combination) (with combination) Figure 3. Message combination framework overview. multicast tree except with the opposite transmission direction. In this example, routers 5 and 9 serve as fork routers which are responsible for gathering and forwarding ACKs. Next, we address two important issues associated with our message combination framework: ensuring that ACK packets traverse the same logical tree as the multicast packet and ensuring that the fork routers are aware of the expected total and currently received ACKs. In an n × n network, a log(n × n)-bit ﬁeld in the multicast header is reserved to identify the router where the last replication happened (pre rep router). This ﬁeld is initially set to be the source node and is updated when a multicast packet replicates during transmission. A 3-bit ID ﬁeld is used to differentiate multicast packets injected by the same source node. This ﬁeld increments when a new multicast packet is injected. The src ﬁeld encodes the source node. A small message combination table (MCT) is added to each router. A multicast allocates an MCT entry upon replication. This entry records the identity of the router where the last multicast replication occurred and the total expected ACK count. The transmission of a multicast packet establishes a logical tree. Each branch in the logical tree has an MCT entry pointing to the previous fork router. Each multicast destination responds with an ACK packet. A log(n × n)-bit ﬁeld (cur dest) in the ACK header serves to identify the intermediate destination. Its value is set to the pre rep router ﬁeld in the triggering multicast packet. Each ACK packet has two ﬁelds named multicast src and multicast ID which correspond to src and ID of al log(n × n)-bit ﬁeld (ACK count) is used to record the the triggering multicast packet, respectively. An additioncarried ACK response count of the combined packet. When an ACK packet arrives at its current destination, it accesses the MCT. If the router has not yet received all expected ACKs, the incoming packet is discarded and the entry’s received ACK count is incremented. If the router has received all expected ACKs, the incoming ACK packet updates its cur dest ﬁeld to be the next replication router. It will be routed to the fork router at the next level; thus, ACK packets traverse the same logical tree as multicast in the opposite direction. 2.1 MCT Format Figure 4 illustrates the format of an MCT entry. The V ﬁeld is the valid bit for the entry. The src, ID and Figure 4. MCT entry format. (Port encoding: E: 0, W: 1, S: 2, N: 3, local: 4. Assuming 16 nodes.) pre rep router ﬁelds are the same as the corresponding ﬁelds in the multicast packet initializing this entry. The MCT is a content-addressable memory (CAM); the src and ID ﬁelds work together as the tag. The incoming port ﬁeld records the incoming port of the multicast packet. The expected count ﬁeld indicates the total expected ACK count, which is equal to the number of destinations at this branch of the multicast tree. The value of cur ACK count ﬁeld tracks the current received ACK count. As we will show later, recording the total expected ACK count instead of simply counting the number of direct successors is needed for handling full MCTs. 2.2 Message Combination Example Figure 5(a) gives a multicast example within a 4×4 mesh network which is the same as Figure 3(a). Figure 5(b) shows the multicast header values. Although our framework is independent of the multicast packet format, we assume bit string encoding [5] for the destination addresses in the destinations ﬁeld of the header for clarity. Ma is the injected packet; its destinations ﬁeld contains the three destinations. Ma replicates into two packets, Mb and Mc at router 9. An MCT entry is created. The src, ID and pre rep router ﬁelds of this entry are fetched from Ma . The incoming port ﬁeld is set to 4 to indicate that Ma comes from the local input port. The expected count ﬁeld is set to the total destination count of Ma : 3. The cur ACK count ﬁeld is set to 0. At router 5, Mb replicates into two packets: Md and Me . An MCT entry is created with an expected count of 2. The pre rep router ﬁelds of both Md and Me are updated to 5 since the last replication occurred at router 5. After a destination node receives a multicast, it responds with an ACK packet. Figure 6(a) shows the transmission of ACK packets corresponding to the multicast shown in Figure 5(a). Figure 6(b) gives the ACK header values. Ac , Ad and Ae packets are triggered by the Mc , Md and Me multicast packets, respectively. The multicast src, multicast ID and cur dest ﬁelds of the ACK packets are equal to the src, ID and pre rep router ﬁelds of the triggering multicast packet, respectively. The ACK count ﬁelds of these three ACK packets are set to 1 since they all carry an ACK response from only one node. The cur dest ﬁeld deﬁnes the current destination of ACK packet. As shown in Figure 6(a), Ae can be routed along a different path than Me to reach its intermediate destination at router 5. ACK packets only need to follow the same logical tree as the multicast packet giving our design signiﬁcant 1 Me 0 Me Me 4 5 Mb Ma 9 Mc 13 8 12 2 6 Md 3 Md 7 Md 1 9 2 9 2 2 MCT entry 0 10 11 1 9 2 9 4 3 0 MCT entry Mc 14 Mc 15 Mc Ma Mb Mc Md Me IDsrc 29 destinations 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 pre_rep _router 9 29 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 29 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 29 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 29 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 9 5 5 Ae Ae 0 4 8 Aa 1 Ae 5 Ab 9 2 6 3 Ad 7 Ad Ad 1 9 2 9 2 2 MCT entry 0 Ac 10 MCT entry 1 9 2 9 4 3 0 12 13 14 Ac 11 Ac 15 Ac multicast _src 9 multicast _ID 2 cur_dest ACK_count 9 3 9 9 9 9 2 2 2 2 9 9 5 5 2 1 1 1 Aa Ab Ac Ad Ae (a) Transmission procedure. (b) Header value. Figure 5. A multicast packet transmission example. (a) Transmission procedure. (b) Header value. Figure 6. ACK packets transmission example. ﬂexibility. A similar scenario can be seen for Ac . Ad and Ae both set their cur dest ﬁelds as router 5; at router 5, they will be merged. Analysis shows that the possibility of multiple simultaneous MCT accesses is quite low (≤ 0.1%) as ACK packets will experience different congestion. A small arbiter is used to serialize concurrent accesses. Assuming Ad arrives earlier than Ae ; its multicast src and multicast ID are used together as the tag to search the MCT. The sum of cur ACK count ﬁeld of the matched entry and the carried ACK count of Ad is 1, which is smaller than the expected count of 2 in that entry. Therefore, Ad is discarded and the cur ACK count ﬁeld is incremented by 1. When Ae arrives at router 5, it accesses the MCT. Since router 5 has received all expected ACKs, Ae will remain in the network. Its cur dest ﬁeld is updated to the pre rep router ﬁeld of the matched entry and its ACK count ﬁeld is updated to 2 since it now carries the ACK responses of nodes 0 and 7 (see Ab ). Ab uses the incoming port ﬁeld of the matched entry as the output port. The combined ACK packet is required to use the same multicast path for one hop to avoid an additional routing computation stage which would add an additional cycle of latency. Now that router 5 has received all expected ACKs, the corresponding MCT entry is freed. Finally, node 9 receives Ab and Ac and combines them into Aa . The multicast-reduction transaction is complete. 2.3 Insuﬃcient MCT Entries So far, we have assumed that there is always an available MCT entry when a multicast packet replicates. However, since the table size is ﬁnite, we must be able to handle a full MCT. If there are no free MCT entries, the replicated multicast packet will not update its pre rep router ﬁeld. In the previous example, if there is no available MCT entry at router 5 when Mb replicates, Md and Me will keep their pre rep router ﬁeld as 9. When Ad and Ae are injected, their cur dest ﬁelds are set to 9; they will combine in router 9 instead of router 5. Both Ad and Ae must travel to router 9. In this case, router 9 will receive two ACK packets for the north-bound replication branch; this is why we record the expected total count of ACKs in MCT instead of recording the number of direct successors in the logic multicast tree. In our design, insufﬁcient MCT entries may affect performance, but do not pose any correctness issues. We evaluate the effect of the MCT size in Section 5.3. 3 Balanced, Adaptive Multicast Routing In this section, we describe our Balanced, Adaptive Multicast (BAM) routing algorithm. To achieve efﬁcient bandwidth utilization, a multicast routing algorithm must compute the output port based on all destination positions in a network [39]. A simple and efﬁcient routing algorithm, RPM, was recently proposed to deliver high performance [39]. As shown in Figure 7, RPM partitions the network into at most 8 parts based on the current position, and applies several priority rules to avoid redundant replication for destinations located in different parts; the goal is to deliver a multicast packet along a common path as far as possible, then replicate and forward each copy on a different channel bound for a unique destination subset. We observe that although RPM provides efﬁcient bandwidth utilization, it suffers from unbalanced buffer resources between different dimensions which negatively affects network performance. To avoid deadlock for multicast routing, RPM divides the physical network into two virtual networks (VNs): VN0 is for upward packets and VN1 is for downward ones. The horizontal VC buffers must be split into two disjoint subsets for the two VNs, while the vertical ones can be exclusively used by one VN [9, 39]. When a packet is routed in each VN, there are 2x more available vertical buffers than horizontal ones. This unbalanced buffer conﬁguration negatively affect both unicast and multicast routing, since the more limited horizontal VCs become a performance bottleneck. Conﬁguring different VC counts for different dimensions may mitigate this effect. However, it requires different control logic for each input port as the size of the arbiters in VC and switch allocators is related to the VC count; a heterogeneous router requires extra design effort [31]. Also, the critical path may increase since it is determined by the largest arbiter. Therefore, we assume a homogeneous NoC router architecture in this paper. Based on these observations, we propose a novel adaptive multicast routing algorithm: Balanced, Adaptive Multicast (BAM). The deadlock freedom of BAM is achieved by utilizing Duato’s unicast deadlock-avoidance theory [8] for multicast packets, rather than leveraging multiple VNs. The 0 1 2 3 Part2 4 Part1 5 6 7 Part3 8 9 Part7 10 11 Part4 12 Part5 13 Part6 14 15 Part0 Figure 7. Network partitioning. (The current position of a packet is Router 9.) multicast packets in NoCs are generally short as they carry only control packets for the coherence protocol; these are most likely single-ﬂit packets [12], making the routing of each multicast branch independent. Thus, Duato’s unicast theory can be applied to multicasts by regarding the routing of each multicast branch as an independent unicast. In Duato’s theory, VCs are classiﬁed into escape and adaptive VCs. When a packet resides in an adaptive VC, it can be forwarded to any permissible output port. This property enables BAM to select the best output port based on all destination positions. An additional advantage is that this design is compatible with an adaptive unicast routing algorithm. Figure 7 shows the partitioning of destinations into 8 parts. For P art1 , P art3 , P art5 or P art7 , there is only one admissible output port. For P art0 , P art2 , P art4 or P art6 , there are two alternative output ports. If a multicast packet has some destinations located in P art1 , P art3 , P art5 and P art7 , the corresponding N, W, S and E output port must be used; these ports are called obligatory output ports for this multicast packet. To achieve efﬁcient bandwidth utilization, we design a heuristic output port selection scheme for destinations located in P art0 , P art2 , P art4 and P art6 : (i) If only one of the two alternative output ports is an obligatory output port for the multicast packet, the router will use this output port to reach the destination; (ii) If the two alternative output ports both are obligatory or not obligatory output ports, the router will adaptively select the one with less congestion. This scheme maximally reuses the obligatory output ports to efﬁciently utilize bandwidth. Figure 8 shows the output port calculation logic for BAM. The one bit Pi indicates whether there is a destination in P arti . Take P art0 as an example: Np0 and Ep0 indicate that the router uses the north or east port to reach the destinations located in P art0 . Nne and Ene signals indicate whether the north or east output has less relative congestion in the northeast (ne) quadrant. These signals are provided by the routing computation module. E scapen , E scapew , E scapes and E scapee indicate whether the multicast packet can use the escape VC for the N, W, S and E output ports, respectively. If a multicast packet uses the north output port to reach nodes in P art0 or P art2 , it is not allowed to use the north escape VC since this packet will make a turn forbidden by DOR. A similar rule is applied to the south escape VC. The east and west 0 2 1 p p N P N N W P W W E P E E S P S S         2 4 3 p p 0 6 7 p p 4 6 5 p p 0 1 7 1 7 1 7 p ne ne N E N W S W S W P P P P N P P N P P P P E P P E P P P P N P P N P P P P W P P W P P P P S P P S P P P P W P P W P P P P S P P S P P P P E P P E                                               0 1 7 1 7 1 7 p ne ne  2 1 3 1 3 1 3 p nw nw    2 1 3 1 3 1 7 p nw nw 4 5 3 5 3 5 3 p sw sw 4 5 3 5 3 5 3 p sw sw   6 5 7 5 7 5 7 p se se     6 5 7 5 7 5 7 p se se 0 2 1 n p p Escape N N P Escape W  Escape S Escape E    w 4 6 5 s p p S P   e Figure 8. BAM routing computation logic. escape VCs are always available for routing. If a multicast packet resides in an escape VC, it will replicate according to DOR, similar to VCTM [12]. Once a multicast packet enters an escape VC, it can be forwarded to the destinations using only escape VCs; there is no deadlock among escape VCs. Any multicast packet residing in an adaptive VC has an opportunity to use an escape VC. This design is deadlock free [8]. Compared with RPM, BAM does not need to partition the physical network into two virtual networks to avoid multicast deadlock; it achieves balanced buffer resources across vertical and horizontal dimensions. Moreover, BAM achieves efﬁcient bandwidth utilization as well. 4 Router Pipeline and Microarchitecture Our baseline is a speculative VC router [7, 11, 36]. Look-ahead signals transmit the unicast routing information one cycle ahead of the ﬂit traversal to overlap the routing computation (RC) and link traversal (LT) stages [17, 23]. We use a technique to pre-select the preferred output for adaptive routing; the optimal output port for each quadrant is selected one cycle ahead based on network status [16, 21, 27]. The pipeline for unicast packets is two cycles plus one cycle for LT, as shown in Figure 9(a). Including multicast routing information in the lookahead signals requires too many bits; therefore, we assume a 3-cycle router pipeline for multicasts, as shown in Figure 9(b). A multicast packet replicates inside the router if multiple output ports are needed to reach the multicast destinations. We use asynchronous replication to eliminate lock-step traversal among several branches; the multicast packet is handled as multiple independent unicast packets in the virtual channel allocation (VA) and switch allocation (SA) stage, except that a ﬂit is not removed from the input VC until all requested output ports are satisﬁed [20, 39]. ACK packets are handled differently from other unicast packets. When an ACK packet arrives, its cur dest ﬁeld is checked. If this ﬁeld does not match the current router, the ACK packet is handled like a normal unicast packet (Figure 9(a)). If they match, the ACK packet accesses the MCT instead of performing the routing computation. The MCT access is overlapped with the RC stage. As we show in Section 5.3, this operation can ﬁt within a single pipeline stage; it does not add additional latency to the critical path. Fig                            Router i RC VA SA ST LT Router i+1 RC VA SA ST LT ST LT RC VA SA Router i:  replication Router i+1:  no replication RC VA SA ST LT (a) Normal unicast packet. (b) Multicast packet. MCT access Router i: cur_dest=i Router i+1: cur_destŬi+1 VA SA ST LT RC VA SA ST LT Router i:  cur_dest=i MCT access Drop packet (c) Updated&forwarded ACK. (d) Dropped ACK. Figure 9. Router pipeline. MCT V src ID 1 9 2 pre_rep _router 9 incom ing _por t 2 expected _count 2 cur_ACK _count 0 (cid:258) VA SA multicast src multicast ID s t r o P t u p n I VC0 VC0 VCx VC0 VCx VCx (cid:258) (cid:258) (cid:258) Routing Unit e n E e n N w n N w n W w s S w s W e s S e s E Port Pre-se lection s t r o P t u p u t O Figure 10. Router microarchitecture. ures 9(c) and 9(d) illustrate the ACK packet pipeline. Figure 10 illustrates the proposed router microarchitecture. If a multicast packet needs multiple output ports after the routing computation, an entry is allocated in the MCT. This operation is overlapped with the VA/SA operations and does not add delay to the critical path. The Port Pre-selection module provides eight signals indicating the optimal output port for each quadrant [16, 21, 27]. These signals are used by both unicasts and multicasts to avoid network congestion. 5 Evaluation We evaluate our message combination framework with RPM and BAM using synthetic trafﬁc and real application workloads. We modify the cycle-accurate Booksim simulator [7] to model the router pipelines and microarchitecture described in Section 4. For synthetic trafﬁc, we conﬁgure two VNs to avoid protocol-level deadlock [7]: one for multicasts and one for ACKs. RPM further divides the multicast VN into two sub-VNs: one for upward packets and one for downward ones. BAM does not need to sub-divide the multicast VN. Normal unicast packets can use any VN. However, once injected into the network, a packet’s VN is ﬁxed and cannot change during transmission. Unicast packets are routed by an adaptive routing algorithm. For BAM’s multicast, BAM’s ACK and RPM’s ACK VNs, the algorithm is designed based on Duato’s theory [8] and uses one VC as the escape VC. The two sub-VNs of RPM’s multicast VN enable adaptive routing without requiring escape VCs. We use a local selection strategy for adaptive routing: when there are two available output ports, the selection strategy chooses the one with more free buffers. Table 1. Simulation conﬁguration and variations. Characteristic Baseline Variations 4×4 8×8, 16×16 4 ﬂits/VC, 8 VCs/port 4 & 6 VCs/port Normal: 1 & 5 (bi-modal) ACK: 1; multicast: 1 uniform random, transpose, bit rotation, hot spot 10% Topology (mesh) VC conﬁguration Packet length (ﬂits) Unicast trafﬁc pattern Multicast ratio 5%, 15%, 20% Multicast dest. 2 - 4, 4 - 14, count 10 - 14, 15 ACK resp. cycles 1 - 4 (uniformly dist.) MCT entries 64 0, 1, 4, 16 Warmup & total 10000 & 100000 cycles Table 2. Full system simulation conﬁguration. # of cores 16 L1 cache (D & I) private, 4-way, 32KB each L2 cache private, 8-way, 512KB each Cache coherence MOESI distributed directory 4×4 2D-Mesh Topology 2 - 10 (uniformly dist.) Multicasts and ACKs are single-ﬂit packets, while the normal unicasts are bimodally distributed, consisting of 5ﬂit packets (50%) and 1-ﬂit packets (50%). We use several synthetic unicast trafﬁc patterns [7], including uniform random, transpose, bit rotation and hot spot, to stress the network for detailed insights. We control the percentage of multicast packets relative to whole injected packets. For multicasts, the destination counts and positions are uniformly distributed. A cache’s ACK packet response delay is uniformly distributed between 1 and 4 cycles. We assume a 64-entry MCT; in Section 5.3, we explore the impact of MCT size. Table 1 summarizes the baseline conﬁguration and variations used in the sensitivity studies. To measure full-system performance, we leverage two existing simulation frameworks: FeS2 [33] for x86 simulation and BookSim for NoC simulation. FeS2 is a timingﬁrst, multiprocessor x86 simulator, implemented as a module for Virtutech Simics [28]. We run the PARSEC benchmarks [3] with 16 threads on a 16-core CMP, consisting of Intel Pentium 4-like CPU. We assume cores optimized for clock frequency; they are clocked 5× faster than the network. We use a distributed, directory-based MOESI coherence protocol that needs 4 VNs for protocol-level deadlock freedom. The cache line invalidation packets (multicasts) are routed in VN1, while the acknowledge packets are routed in VN2. The VCs/VN, VC depth and MCT size are the same as the baseline (Table 1). Cache lines are 64 bytes wide and the network ﬂit width is 16 bytes. All benchmarks use the simsmall input sets to reduce simulation time. The total runtime is used as the metric for full-system performance. Table 2 gives the system conﬁguration. 5.1 Performance We evaluate four scenarios: RPM without message combination (RPM+NonCom), RPM with message combination (RPM+Com), BAM without message combina    RPM+N onC om RPM+C om BAM+N onCom BAM+C om ) l s e c y c ( y c n e a L t 50 40 30 20 10 0 RPM+N onCom RPM+C om BAM+NonC om BAM+Com ) l s e c y c ( y c n e a L t 50 40 30 20 10 0 RPM+N onCom RPM+C om BAM+NonC om BAM+Com ) l s e c y c ( y c n e a L t 50 40 30 20 10 0 RPM+N onCom RPM+C om BAM +NonC om BAM +Com ) l s e c y c ( y c n e a L t 50 40 30 20 10 0 0.0  0.1  0.2  0.3  0.4  Injec tion rate (flits /node/c yc le) 0.5  0.0  0.1  0.2  0.3  Injec tion rate (flits /node/cy c le) 0.4  0.0  0.1  0.2  0.3  0.4  Injec tion rate (flits /node/cy c le) 0.5  0.0  0.1  0.2  0.3  Injec tion rate (flits /node/c yc le) 0.4  (a) Uniform random. (b) Transpose. (c) Bit rotation. Figure 11. Overall network performance. (10% multicast, average 6 destinations.) (d) Hot spot. tion (BAM+NonCom) and BAM with message combination (BAM+Com). Overall Network Performance. Figure 11 illustrates overall network performance. Both RPM+Com and BAM+Com see performance improvements compared to RPM+NonCom and BAM+NonCom respectively; not only are the network latencies reduced, but the saturation throughputs4 are improved. Detailed analysis reveals that the combination framework reduces the average channel traversal count of ACK packets from 4.7 to 2.5, reducing approximately 45% of the network operations for ACKs. The network latency for RPM+Com is reduced by 10%20% under low-to-medium injection rates5 compared to RPM+NonCom (average 14.1%); larger improvements are seen at high injection rates. Similar latency reductions are seen for BAM+Com vs. BAM+NonCom. Packet dropping shortens the average hop distance bringing latency reductions at both low and high network loads. The mitigation of ejection-side congestion is also beneﬁcial to latency reduction. Saturation throughput improvements resulting from the framework (RPM+Com vs. RPM+NonCom) range from 8.5% to 11.2% (average 9.6%). BAM+Com sees similar throughput improvements over BAM+NonCom. Discarding in-ﬂight ACKs reduces the network load, which is helpful to improve the saturation throughput. Across the four trafﬁc patterns, both BAM+NonCom and BAM+Com improve the saturation throughput over RPM+NonCom and RPM+Com, respectively. For transpose, bit rotation and hot spot patterns, although BAM+NonCom has larger latencies than RPM+Com under low loads, its saturation throughputs are higher. The balanced buffer conﬁguration between vertical and horizontal dimensions helps BAM to improve the saturation throughput. BAM+Com improves the saturation throughput by 14.2%, 27.6%, 26.4% and 25.1% (average 23.4%) over RPM+NonCom for the four trafﬁc patterns. Both the ACK packet dropping and balanced buffer conﬁguration contribute to this performance improvement. As 4 The saturation point is measured as the injection rate at which the average latency is 3× the zero load latency. 5 The low-to-medium injection rate is the injection rate at which the average latency is less than 2× the zero load latency. shown in Figure 11, the trend between BAM+NonCom and RPM+NonCom is similar to the trend between BAM+Com and RPM+Com; thus, we omit BAM+NonCom in the following sections for brevity. Multicast Transaction Performance. To clearly understand the effects of message combination on multicast transactions, we measure the multicast-reduction transaction latency. Figure 12 shows the results for ﬁve injection rates. The injection rate for the last group of bars exceeds the saturation point of RPM+NonCom. Under all injection rates, RPM+Com and BAM+Com have lower transaction latencies than RPM+NonCom; dropping ACK packets reduces network congestion and accelerates multicast-reduction transactions. ACK packet acceleration contributes more to the latency reduction than multicasts. A multicast needs to send out multiple replicas; releasing its current VC depends on the worst congestion each replica may face. Thus, the multicast is not as sensitive as the ACK to the network load reduction. For example, with low-to-medium injection rates (≤ 0.30) under uniform random trafﬁc (Figure 12(a)), the average multicast delay is reduced by 9.5% for BAM+Com versus RPM+NonCom. Yet, ACK packet delay is reduced by 17.6%. These two factors result in an average 13.4% transaction latency reduction. The transaction acceleration increases with higher network load. Merging ACKs reduces the number of packets the source needs to wait for to ﬁnish a transaction. Waiting for only one ACK instead of multiple ACKs can improve performance since multiple packets may encounter signiﬁcantly more congestion than a single packet, especially under high network load. For uniform random trafﬁc with a high injection rate (0.40), RPM+Com and BAM+Com reduce the transaction latency by 46.2% and 57.6% compared to RPM+NonCom, respectively. The message combination framework accelerates the total transaction by almost a factor of 2. As injection rate increases, BAM+Com outperforms RPM+Com by a signiﬁcant margin. Real Application Performance. Figure 13 shows the speedups over RPM+NonCom for the PARSEC benchmarks. Although the message combination framework mainly optimizes the performance for one of the four VNs (the ACK VN) used in full-system simulation, collective         0 50 100 150 200 1 3 5 7 9 11 13 15 17 y c n e a L t ( s e c y c l ) ACK Mult ic as t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.40 Injection rate (flits/node/cycle) 0.45 (a) Uniform random. (b) Transpose. (c) Bit rotation. (d) Hot spot. Figure 12. Multicast-reduction transaction latency. (10% multicast, average 6 destinations.) 0 50 100 150 200 1 3 5 7 9 11 13 15 17 y c n e a L t ( s e c y c l ) ACK Mult ic as t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.35 Injection rate (flits/node/cycle) 0.40 0 50 100 150 200 1 3 5 7 9 11 13 15 17 y c n e a L t ( s e c y c l ) ACK Mult ic as t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.40 Injection rate (flits/node/cycle) 0.45 0 50 100 150 200 1 3 5 7 9 11 13 15 17 y c n e a L t ( s e c y c l ) ACK M ult ic as t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.25 0.30 Injection rate (flits/node/cycle) 0.35 1 1.05 1. 1 1.15 k s a T r n u i t m p u d e e p s e RPM+C om BAM+C om Figure 13. System speedups against RPM+NonCom for the PARSEC benchmarks. communication often lies on the critical path for applications. Also, dropping ACK packets in one VN reduces switch allocation contention and improves the performance of other VNs. These factors result in RPM+Com achieving speedups over RPM+NonCom ranging from 5.3% to 8.3% for all applications. The efﬁciency of message combination depends on the multicast destination count. The multicast destination counts of body track , f acesim and ray trace are the lowest; the speedups of RPM+Com for these 3 applications are lower than the remaining 7 applications. The balanced buffer conﬁguration utilized in BAM+Com supports higher saturation throughput than the unbalanced one. BAM+Com improves the performance of applications which have high network loads and signiﬁcant bursty communication. For blackscholes, f luidanimate, ray trace and swaptions, the additional performance gain due to the balanced buffer conﬁguration (BAM+Com vs. RPM+Com) ranges from 2.3% to 3.7%. The network loads of these applications are low and do not stress the network. However, BAM+Com achieves additional speedups ranging from 5.5% to 8.6% over RPM+Com for body track , canneal, f acesim, f erret, streamcluster and v ips. These applications have more bursty communication and higher injection rates. For all ten applications, BAM+Com achieves an average speedup of 12.7% over RPM+NonCom. The maximal speedup is 16.8% for canneal. 5.2 Comparing BAM’s and RPM’s Multicast VN Conﬁguration In this section, we delve into the effect of the unbalanced buffer conﬁguration used in the multicast VN of RPM on both unicast and multicast packet routing. Since the ACK VNs of RPM and BAM are the same, we assume the network only has one VN: the multicast VN. Four VCs are conﬁgured in this VN, and RPM further divides this VN into two sub-VNs: the horizontal VC count of each sub-VN is two, while the vertical count is four. Unicast Performance. Figure 14 compares the performance of BAM’s and RPM’s multicast VN conﬁguration using only unicast packets. To extensively understand the effect of the unbalanced buffer conﬁguration, we evaluate the performance of XY, YX and a locally adaptive routing algorithm (Adaptive) in RPM’s multicast VN. XY efﬁciently distributes uniform random trafﬁc and achieves the highest performance for this pattern. For the other three patterns, Adaptive has the highest performance; adaptively choosing the output port mitigates the negative effect of unbalanced buffer resources. Therefore, this work uses locally adaptive routing for unicast packets. Although Adaptive has better performance than XY and YX, its performance is still limited by the unbalanced buffer resources used in each of RPM’s multicast sub-VNs; the horizontal dimension has half the buffer resources of the vertical one. However, in BAM’s VN, the number of buffers of different dimensions are equal. Adaptive routing in BAM’s VN shows substantial performance improvement over Adaptive routing in RPM’s VN. Transpose has the largest performance gain with a 73.2% saturation point improvement. Across these four trafﬁc patterns, BAM’s VN-Adaptive achieves an average saturation throughput improvement of 35.3% over RPM’s VN-Adaptive. Multicast Performance. Figure 15 shows the performance using 100% multicast packets. The Adaptive curve shows the performance of the adaptive multicast routing algorithm without our heuristic replication scheme; multicast replicas adaptively choose the output ports without considering the reuse of obligatory ports. This negatively affects bandwidth utilization. BAM has 8.7% higher saturation throughput than Adaptive. BAM achieves 47.1% higher saturation throughput over RPM. The effect of the unbalanced buffer resources is greater on multicasts than unicasts. A multicast packet is removed from its current VC only after all its replicas are sent out; the horizontal VC bottleneck affects             0 10 20 30 40 50 0.0 0.1 0.2 0.3 0.4 0.5 Injec tion rate ( flits /node/cyc le) 0.6 y c n e a L t ( s e c y c l ) RPM 's VN - Adapt iv e RPM 's VN - XY RPM 's VN - YX BAM 's VN - Adapt iv e (a) Uniform random. 0 10 20 30 40 50 0.0 0.1 0.2 0.3 0.4 Injec tion rate ( flits /node/cy c le) 0.5 y c n e a L t ( s e c y c l ) RPM 's VN - Adapt iv e RPM 's VN - XY RPM 's VN - YX BAM 's VN - Adapt iv e (b) Transpose. (c) Bit rotation. Figure 14. Unicast trafﬁc performance for RPM’s and BAM’s multicast VN. 0 10 20 30 40 50 0.0 0.1 0.2 0.3 0.4 0.5 Injec tion rate (flits /node/c yc le) 0.6 y c n e a L t ( s e c y c l ) RPM 's VN - Adapt iv e RPM 's VN - XY RPM 's VN - YX BAM 's VN - Adapt iv e 0 10 20 30 40 50 0.0 0.1 0.2 0.3 Injec tion rate (flits /node/cy c le) 0.4 y c n e a L t ( s e c y c l ) RPM 's VN - Adapt iv e RPM 's VN - XY RPM 's VN - YX BAM 's VN - Adapt iv e (d) Hot spot. 0 10 20 30 40 50 0.00 0.02 0.04 0.06 Injec tion rate ( flits /node/cy c le) 0.08 y c n e a L t ( s e c y c l ) RPM Adapt iv e BAM Figure 15. Performance of 100% multicast trafﬁc. (average 6 destinations.) multicast performance more strongly than unicast performance. Average switch traversal counts are 8.6, 8.8 and 8.4 for RPM, Adaptive and BAM, respectively, which further demonstrates that our applied heuristic replication scheme achieves efﬁcient bandwidth utilization. 5.3 MCT Size As described in Section 2.3, the MCT size affects network performance but not correctness. Too few MCT entries will hamper ACK packet combination and force the ACK packets to travel more hops than with combination. To determine the appropriate size, we simulate an inﬁnite MCT using the baseline conﬁguration (Table 1) and uniform random trafﬁc. Multicast packets are routed using BAM. Figure 16(a) presents the maximum and average concurrently valid MCT entries. For low-to-medium injection rates (< 0.39), the maximum number of concurrently valid entries is less than 10 and the average number is less than 1.5. Even when the network is at saturation (0.52 injection rate), the maximum number of concurrently valid entries is 49 and the average is 10.15. These experimental results indicate that a small MCT can provide good performance. Figure 16(b) shows the performance for different table sizes. The 0-entry curve performs no ACK combination; all ACK packets are injected into the network with their destination set to the source node of the triggering multicast packet. Even with only one entry per router, ACK combination reduces the average network latency by 10%. More entries reduce the latency further, especially for high injection rates. Saturation throughput improvements range from 3.3% to 12.1% for 1 to 64 entries. 0 10 20 30 40 50 0.0 0.1 0.2 0.3 0.4 Injec tion rate ( flits /node/cy c le) 0.5 C u c n o r r n e t a v i l n e d r t i s e Max im um Av erage (a) Concurrent valid entries. (b) Performance. Figure 16. Evaluation of MCT size. (10% multicast, average 6 destinations.) 0 10 20 30 40 0.0 0.1 0.2 0.3 0.4 Injec tion rate (flits /node/c yc le) 0.5 y c n e a L t ( s e c y c l ) 0-ent ry 1-ent ry 4-ent ries 16-ent ries 64-ent ries Table 3. MCT overhead. Area (mm2 ) Energy (nJ ) Time (ns) 0.0011 0.0017 0.0031 Entries Bytes 16 32 64 0.0008 0.0013 0.0026 0.138 0.146 0.153 48 96 192 Cacti [32] is used to calculate the power consumption, area and access latency for the MCT in a 32nm technology process. Table 3 shows the results. Assuming a 1 GHz clock frequency, a 64-entry table can be accessed in one cycle. This size provides good performance for nearly all injection rates for various trafﬁc patterns. In the full system evaluation, we observe that the maximal concurrently valid entries for the PARSEC benchmarks are less than 25 entries. For area-constrained designs, fewer entries still provide latency and throughput improvements. 5.4 Sensitivity to Network Design To further understand the scalability and impact of our design, we vary the VC count, multicast ratio, destination count per multicast packet and network size. Figure 17 presents the average performance improvement across the four synthetic unicast trafﬁc patterns. In each group of bars, the ﬁrst two bars show the saturation throughput improvement, and the second two bars show the latency reduction under low-to-medium loads. VC Count. Figure 17(a) shows the performance improvement with 8, 6, and 4 VCs per physical channel. One interesting trend is observed: For smaller VC counts, the gain due to combination framework increases (RPM+Com vs. RPM+NonCom), while the improvement due to balanced buffer resources declines (BAM+Com vs. RPM+Com).                 20% 15% RPM+Com vs . RPM+NonCom( throughput improv ement) BAM+Com v s . RPM+NonCom( throughput improvement) RPM+Com vs . RPM+NonCom( latenc y reduc tion) BAM+Com v s . RPM+NonCom( latency reduc tion) 25% 30% 30% 30% 25% 25% 25% 20% 20% 20% 15% 15% 15% 10% 10% 10% 5% 5% 5% 0% 0% 0% n a g e c n a a g e c n a i n a g e c n a 10% 5% e P m r m r e P e P o f r o f r i m r o f r i n a g e c n a m r o f r e P 0% n i 8 VCs 6 VCs 4 VCs 5% 10% 15% 20% (a) VC counts. (b) Multicast ratios. 3 6 9 12 15 (c) Multicast destinations. 4x4 (6) 8x 8 (12) 16x 16 (24) (d) Network sizes. Figure 17. Performance gains of RPM+Com and BAM+Com over RPM+NonCom for sensitivity study. The reasons for this trend are two-fold. First, fewer VCs per port makes the VCs a more precious resource; dropping ACK packets improves the reuse of this resource. For example, RPM+Com has a 14.8% higher saturation throughput than RPM+NonCom with 4 VCs per physical channel. As the number of VCs increases, this resource is not as precious; its effect on performance declines. However, even with 8 VCs per physical channel, dropping ACK packets still improves the saturation throughput by 9.6%. Second, BAM+Com uses escape VCs to avoid deadlock. The horizontal escape VC can always be used, while the vertical one can only be used by DOR; there is some imbalance in the utilization of escape VCs and this imbalance increases with fewer VCs. However, the situation is worse for RPM+Com. In RPM’s multicast VN conﬁguration, the vertical dimension always has twice as many VCs as the horizontal one. Even with 4 VCs/port, the saturation point improvement of BAM+Com is still larger than RPM+Com’s improvement by about 9.0%. With fewer VCs, RPM+Com’s latency reduction increases; it achieves a 18.5% reduction with 4 VCs/port. BAM+Com further reduces latency due to the balanced buffer conﬁguration among different dimensions. The latency difference between BAM+Com and RPM+Com is not as signiﬁcant as the saturation throughput improvement. Adaptive routing mainly accelerates packet transmission at high injection rates by avoiding network congestion. Multicast Ratio. Figure 17(b) presents the performance improvement for several multicast ratios: 5%, 10%, 15% and 20%. Increasing the multicast portion leads to greater throughput improvements due to message combination. The improvement contributed by the balanced buffer conﬁguration remains almost constant (BAM+Com vs. RPM+Com). A higher multicast packet ratio triggers more ACK packets; ACK combination has more opportunity to reduce network load. The framework becomes more effective. BAM+Com achieves a 27.2% saturation throughput improvement with 20% multicast ratio. A higher multicast ratio also results in larger latency reductions. The VC count per physical channel is kept constant (8 VCs/port) in this experiment, so the gap between BAM+Com and RPM+Com remains almost the same. Destinations per Multicast. Figure 17(c) illustrates the performance gain for different average numbers of multicast destinations: 3, 6, 9, 12, and 15 (broadcast). The trend is similar to varying the multicast ratio. Although the multicast ratio remains constant (10%), more destinations per multicast trigger more ACK packets. The framework combines more of these ACKs during transmission. As the destination count varies from 3 to 15, BAM+Com improves the saturation throughput by 17% to 28%. RPM+Com reduces latency by 10% to 25%; BAM+Com’s latency reduction ranges from 13% to 27%. Network Size. Figure 17(d) shows the performance improvement for different network sizes: 4×4, 8×8 and 16×16 mesh networks6 . Since 8×8 and 16×16 networks have more nodes, we increase the average destinations per multicast to 12 and 24, respectively. The message combination framework is more efﬁcient at larger network sizes since packets traverse more hops on average. As a result, tions. For the 16×16 network, the message combination combining ACK packets eliminates more network operaframework improves the saturation point by 16.8%. Similarly, larger network sizes show greater latency reductions. RPM+Com and BAM+Com achieve 25% and 27% latency reductions for a 16×16 mesh, respectively. The efﬁciency of the balanced buffer conﬁguration utilized by BAM+Com remains constant with different network sizes. Throughout the sensitivity studies, the MCT size is 64 entries. One may think that with more multicast destinations, a higher multicast ratio or a larger network size that more entries will be required. Yet, analysis reveals changing these aspects reduces the injection rate at which the network becomes saturated. Thus, the maximal concurrently active multicast transactions supported by the network is reduced. As a result, a 64-entry MCT is able to achieve high performance for these different network design points. 6 Power Analysis A NoC power model [30] is leveraged to determine overall network power consumption; network power consumption is contributed by three main components: channels, input buffers and router control logic. The activity of these components is obtained from Booksim. Leakage power is included for buffers and channels. The power consump6Bit string encoding is used in all experiments to encode the destination set; this method is impractical in large networks. However, further exploration of this issue is orthogonal to the message combination framework.         0 1 2 3 4 1 3 5 7 9 11 13 15 17 a o T t l o p w e r ( W ) MCT access Static Dynamic 0.40 P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.35 Injection rate (flits/node/cycle) (a) Power (10% multicast). 0 1 2 3 4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 a o T t l o p w e r ( W ) MCT acc ess Static Dynamic P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.35 Injection rate (flits/node/cycle) (b) Power (20% multicast). (c) EDP (10% multicast). Figure 18. Power consumption and EDP results. (average 6 destinations.) 600 500 400 300 200 100 0 1 3 5 7 9 11 13 15 17 P D E ( s e c y c x J n l ) Energy -delay produc t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.35 0.40 Injection rate (flits/node/cycle) 600 500 400 300 200 100 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 P D E ( s e c y c x J n l ) Energy -delay produc t P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + N n o C o m P R M + C o m A B M + C o m P R M + C o m A B M + C o m 0.10 0.20 0.30 0.35 Injection rate (flits/node/cycle) (d) EDP (20% multicast). tion of a MCT access is also integrated into the power model. We assume a 128-bit ﬂit width. The technology process is 32nm and the network clock is 1 GHz based on a conservative assumption of router frequency. Figures 18(a) and 18(b) show the power consumption using transpose trafﬁc for unicast messages. We measure the MCT access power and the static and dynamic network power for two multicast ratios: 10% and 20%. MCT access power comprises only a small portion of the total power. The reason is two-fold. MCT is very small; each entry is only 3 bytes. With 64 entries, the size of MCT is 192 bytes, which is only 7.5% of the size of ﬂit buffers. Second and more importantly, the MCT access activity is very low. Even when the network is saturated, only 7.2% of all cycles have an MCT access; if the network is not saturated, the activity is even lower. RPM+Com reduces power consumption compared to RPM+NonCom for all injection rates due to the dropping of ACK packets. BAM+Com further reduces the power consumption due to more balanced buffer utilization among different dimensions. As the injection rate increases, the reduction in power consumption becomes more obvious. For 0.35 injection rate with a 10% multicast ratio, RPM+Com and BAM+Com achieve a 7.6% and a 10.8% power reductions over RPM+NonCom, respectively. With larger multicast ratios, the combination framework is able to reduce more power for both RPM+Com and BAM+Com. The energy-delay product (EDP) [14] of the whole network further highlights the energy efﬁciency of our design, as shown in Figures 18(c) and 18(d). Dropping ACK packets during transmission not only results in fewer network operations, but also reduces network latency. For low-to-medium injection rates with a 10% multicast ratio, RPM+Com and BAM+Com show about 20%-40% EDP reductions. At a high injection rate (0.35), this reduction can be as much as 60%-75%. Higher multicast ratios result in greater EDP reduction. 7 Related Work In this section, we review related work for message combination and NoC multicast routing. Barrier synchronization is an Message Combination. important class of collective communication, in which a reduction operation is executed ﬁrst followed by a broadcast. Gathering and broadcasting worms have been proposed [35]. Many supercomputers including the NYU Ultracomputer [15], CM-5 [24], Cray T3D [6], Blue Gene/L [2] and TH-1A [43] utilize dedicated or optimized networks to support the combination of barrier information. Oh et al. [34] observe that the using a dedicated network in many-core platform is unfavorable and propose the use of on-chip transmission lines to support multiple fast barriers. Our work focuses on collective communication that is used in cache coherence protocols where the multicast is sent ﬁrst followed by the collection of acknowledgements. Bolotin et al. [4] acknowledge that ACK combination might be useful, but do not give a detailed design or evaluation. Krishna et al. [22] propose efﬁcient support for collective communications in coherence trafﬁc. Our message combination mechanism is quite different from their design. We utilize a CAM to record the ACK arrival information, while they keep the earlier arriving ACK in network VCs to merge with later arriving ones [22]. NoC Multicast Routing. Recent work explores various NoC multicast routing algorithms. Lu et al. [26] use pathbased multicast routing, which requires path setup and acknowledgement messages resulting in a long latency overhead. Tree-based multicast mechanisms in NoCs avoid this latency overhead. VCTM [12] is based on the concept of virtual multicast trees. bLBDR [37] uses broadcasting in a small region to implement multicasting. RPM [39] focuses on achieving bandwidth efﬁcient multicasting. Based on the isolation mechanism proposed in bLBDR, Wang et al. [41] extend RPM for irregular regions. MRR [1] is an adaptive multicast routing algorithm based on the rotary router. Whirl [22] provides efﬁcient support for broadcasts and dense multicasts. The deadlock avoidance mechanisms of Whirl and our proposed BAM are similar; both are based on Duato’s theory [8]. Both BAM and RPM use bit string encoding for multicast packet destinations; this method is not scalable for large networks. Some compression methods [40] can improve the scalability of this encoding scheme. In addition, coarse bit vectors [18], similar to what has been proposed for directories, are another possible ap                    proach to reduce the size of the destination set encodings. This type of encoding will increase the number of destinations per multicast and receive greater beneﬁts from our proposal. Delving into this issue is left as future work. 8 Conclusions Scalable NoCs must handle trafﬁc in an intelligent fashion; to improve performance and reduce power they must eliminate unnecessary or redundant messages. The proposed message combination framework does just that by combining in-ﬂight ACK responses to multicast requests. A small 64-entry CAM is added to each router to coordinate combination. In addition to the framework, we propose a novel multicast routing algorithm that balances buffer resources between different dimensions to improve performance. Simulation results show that our message combination framework not only reduces latency by 14.1% for lowto-medium network loads, but also improves the saturation point by 9.6%. The framework is more efﬁcient for fewer VCs, larger network size, a higher multicast ratio or more destinations per multicast. The balanced buffer conﬁguration achieves an additional 13.8% saturation throughput improvement. Our proposed message combination framework can be easily extended to support the reduction operations in Token Coherence and other parallel architectures. Acknowledgments We thank the reviewers for their constructive suggestions and members of Prof. Enright Jerger’s group for feedback on this work. We also thank Daniel Becker of Stanford for his detailed and valuable comments. This work is supported in part by the University of Toronto, NSERC of Canada, the Connaught Fund, 863 Program of China (2012AA010302), NSFC (61070037, 61025009, 60903039, 61103016), China Edu. Fund. (20094307120012), Hunan Prov. Innov. Fund. For PostGrad. (CX2010B032). "
2012,Whole packet forwarding - Efficient design of fully adaptive routing algorithms for networks-on-chip.,"Routing algorithms for networks-on-chip (NoCs) typically only have a small number of virtual channels (VCs) at their disposal. Limited VCs pose several challenges to the design of fully adaptive routing algorithms. First, fully adaptive routing algorithms based on previous deadlock-avoidance theories require a conservative VC re-allocation scheme: a VC can only be re-allocated when it is empty, which limits performance. We propose a novel VC re-allocation scheme, whole packet forwarding (WPF), which allows a non-empty VC to be re-allocated. WPF leverages the observation that the majority of packets in NoCs are short. We prove that WPF does not induce deadlock if the routing algorithm is deadlock-free using conservative VC re-allocation. WPF is an important extension of previous deadlock-avoidance theories. Second, to efficiently utilize WPF in VC-limited networks, we design a novel fully adaptive routing algorithm which maintains packet adaptivity without significant hardware cost. Compared with conservative VC re-allocation, WPF achieves an average 88.9% saturation throughput improvement in synthetic traffic patterns and an average 21.3% and maximal 37.8% speedup for PARSEC applications with heavy network loads. Our design also offers higher performance than several partially adaptive and deterministic routing algorithms.^1","Whole Packet Forwarding: Efﬁcient Design of Fully Adaptive Routing Algorithms for Networks-on-Chip Sheng Ma† ‡ , Natalie Enright Jerger‡ , Zhiying Wang† †School of Computer, National University of Defense Technology, Changsha, China ‡Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada masheng@nudt.edu.cn, enright@eecg.toronto.edu, zywang@nudt.edu.cn Abstract Routing algorithms for networks-on-chip (NoCs) typically only have a small number of virtual channels (VCs) at their disposal. Limited VCs pose several challenges to the design of fully adaptive routing algorithms. First, fully adaptive routing algorithms based on previous deadlockavoidance theories require a conservative VC re-allocation scheme: a VC can only be re-allocated when it is empty, which limits performance. We propose a novel VC reallocation scheme, whole packet forwarding (WPF), which allows a non-empty VC to be re-allocated. WPF leverages the observation that the majority of packets in NoCs are short. We prove that WPF does not induce deadlock if the routing algorithm is deadlock-free using conservative VC re-allocation. WPF is an important extension of previous deadlock-avoidance theories. Second, to efﬁciently utilize WPF in VC-limited networks, we design a novel fully adaptive routing algorithm which maintains packet adaptivity without signiﬁcant hardware cost. Compared with conservative VC re-allocation, WPF achieves an average 88.9% saturation throughput improvement in synthetic trafﬁc patterns and an average 21.3% and maximal 37.8% speedup for PARSEC applications with heavy network loads. Our design also offers higher performance than several partially adaptive and deterministic routing algorithms. 1 1 Introduction Networks-on-chip (NoCs) have been proposed to meet the communication requirements of many-core computing platforms [7]. NoC performance is sensitive to the choice of routing algorithm, as the routing algorithm deﬁnes not only the packet transmission latency, but also the saturation throughput a NoC can sustain. Many novel routing algorithms have been proposed to deliver high performance in NoCs [19, 21, 24, 28, 31, 43, 50]. 1 This research was carried out while Sheng Ma was a visiting international student at the University of Toronto supported by a CSC scholarship. 978-1-4673-0826-7/12/$26.00 ©2011 IEEE o i t a r t e k c a p t i l f l e g n i S 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Figure 1. Single-ﬂit packet ratio for the PARSEC benchmarks. (Flit width is 16 bytes.) In addition to performance considerations, the routing algorithm has correctness implications for the network. Since deadlock is unacceptable, any proposed routing algorithm must be deadlock free, at both the network- and protocollevel. The guarantee of network-level deadlock freedom for a routing algorithm is generally based on deadlockavoidance theories. There are many theories for deadlockfree fully adaptive [12, 13, 18, 29, 41, 48] and partially adaptive routing algorithm design [4, 6, 19, 20]. Although most theories were originally proposed for off-chip networks, they are widely used in today’s NoCs [19, 21, 24, 28, 31, 43, 50]. However, the characteristics of packets in NoCs are quite different than those in off-chip networks. Abundant wiring resources lead to wider ﬂits which decreases the number of ﬂits per packet; short packets dominate trafﬁc in NoCs. In contrast, the wiring resources in off-chip networks are limited by the pin count. For example, the ﬂit width of a typical off-chip router is on the order of 32 bits (e.g. the Alpha 21364 router [35]), while the ﬂit width of a NoC is typically between 128 [22] and 256 bits [10]. With such wide ﬂits, coherence messages carrying a memory address and control information but no data can be encoded as single-ﬂit packets in NoCs. Figure 1 shows that on average 78.7% of packets are single ﬂits for PARSEC benchmarks [3]; the remaining packets are 5 ﬂits long and contain a full 64B cache line.     Another noteworthy difference is that the buffer resources in NoCs are more precious than in off-chip networks due to the tight area and power budgets [17, 23], thus NoCs generally use ﬂit-based wormhole ﬂow control [9]. Although buffer resources are limited, several separate physical or virtual networks are leveraged for delivering different types of messages to avoid protocol-level deadlock. Table 1 lists the number of separate physical and virtual networks deployed in some industrial designs of offchip and on-chip networks. We also show the number of required virtual networks for some cache coherence protocols in the GEMS simulator [34]. Typically, four or ﬁve virtual networks are needed to avoid protocol-level deadlock. Considering the expense of buffers in NoCs, each virtual network will be conﬁgured with a small number of VCs [5] since more VCs require more buffers and a larger allocator. For example, TILE64 [49] and TRIPS [22] have only one VC per virtual network. Thus, a NoC routing algorithm is generally running with a limited number of VCs. In a VC-limited network with short packets dominating trafﬁc, the design of fully adaptive routing algorithms faces several new challenges. In a wormhole network, fully adaptive routing algorithms based on existing theories require a conservative VC re-allocation scheme: a VC can only be re-allocated to a new packet when it is empty [12, 13, 18, 29, 41, 48]. This conservative scheme prevents network-level deadlock, but it is very restrictive resulting in bandwidth and performance loss in the presence of many back-to-back short packets [19]. Figure 2 illustrates the performance of three algorithms when each virtual network is conﬁgured with two VCs2 . Despite its ﬂexibility and load-balancing capability, the fully adaptive routing algorithm has even poorer performance than the deterministic and partially adaptive ones, since both the deterministic and partially adaptive algorithms can apply an aggressive VC re-allocation scheme. It is imperative to improve deadlock-avoidance theories to enhance the performance of fully adaptive routing algorithms in NoCs. We propose a novel VC re-allocation scheme: whole packet forwarding (WPF), for fully adaptive routing algorithms. This scheme is summarized as follows: if a nonempty VC has enough buffer slots to hold the whole packet, then this VC can be re-allocated to the new packet even though it is not empty. WPF can be viewed as applying packet-based ﬂow control in a wormhole network. This hybrid ﬂow control mechanism solves the shortcoming of conservative VC re-allocation by allowing a VC to be reallocated before it is empty, which greatly improves the saturation throughput of fully adaptive routing algorithms. We prove that a fully adaptive routing algorithm using WPF is deadlock-free if this routing algorithm is deadlock-free with 2 See Section 5 for detailed experimental conﬁguration and description of the routing algorithms. PSF DOR Odd-even ) l s e c y c ( y c n e a L t 80 60 40 20 0 0% 10% 20% 30% 40% 50% Injec tion bandwidth ( flits /node/cy c le) Figure 2. Routing algorithms performance with bit reverse trafﬁc. (PSF: fully adaptive routing, DOR: deterministic routing; Odd-even: partially adaptive routing.) conservative VC re-allocation. WPF is an important extension to previous deadlock-avoidance theories. WPF enables the design of a fully adaptive routing algorithm with superior performance in a VC-limited network. Our novel routing algorithm achieves high VC utilization and maximal routing ﬂexibility. Compared with conservative VC re-allocation, WPF provides an average saturation throughput improvement of 88.9% for synthetic trafﬁc, and achieves an average 21.3% and maximal 37.8% speedup for PARSEC applications that heavily load the network. Our design also offers higher performance than several partially adaptive and deterministic routing algorithms. In summary, this paper makes the following primary contributions: • Proposes WPF, which greatly improves the performance of fully adaptive routing algorithms, especially with limited VC resources. • Proves WPF can be used by most previous deadlockfree fully adaptive routing algorithms; it is an important extension to existing deadlock-avoidance theories. • Demonstrates that in a VC-limited network, maintaining packet adaptivity is very important and proposes an efﬁcient fully adaptive routing algorithm that takes advantage of WPF. 2 Background In this section, we discuss related work in deadlockavoidance theories and design methodologies for fully adaptive routing algorithms. 2.1 Deadlock Avoidance Theories Since NoCs typically use wormhole ﬂow control [9] to reduce buffering requirements [7, 33, 39], we focus on theories for wormhole networks. Dally and Seitz proposed a seminal deadlock avoidance theory [6] which can be used to design partially adaptive and deterministic routing algorithms. Duato introduced the concept of a routing subfunction, and gave an efﬁcient design methodology [12, 13]. Lin et al. [29] leveraged the message ﬂow model, and Schwiebert and Jayasimha [41] utilized the channel waiting   Table 1. Number of physical/virtual networks. (PN: physical network; VN: virtual network) Industrial products Cache coherence protocols in GEMS simulator [34] TILE64 [49] TRIPS [22] MESI directory MOESI directory MOESI token 5 PNs (1 VN/PN) 2 PNs (4 VNs for OCN, 1 VN for OPN) 5 VNs 4 VNs 4 VNs Alpha 21364 [35] 1 PN (7 VNs) graph to analyze deadlock properties. Recently, Verbeek and Schmaltz [47, 48] proposed a necessary and sufﬁcient condition for deadlock-free routing based on static conditions. These theories [12, 13, 18, 29, 41, 48] can be used to design fully adaptive routing algorithms. A limitation of these theories for fully adaptive routing is that they all require that a VC be re-allocated only when it is empty [12, 13, 18, 29, 41, 48]. This requirement guarantees that all blocked packets can reach the head of a VC to gain access to the ‘deadlock-free’ path at every router. However, considering the large fraction of short packets in NoCs, strict adherence to this requirement strongly limits performance, especially when the number of VCs is small. To address this issue, some deadlock-recovery based designs [1] or theories [15] are proposed, which remove the constraint of conservative VC re-allocation. They allow the formation of deadlocks, and then apply some recovery mechanism [1, 15]. In contrast, WPF extends existing deadlock-avoidance theories which prohibit the formation of deadlock. To the best of our knowledge, WPF is the ﬁrst proposal which allows multiple packets to reside in a VC concurrently for routing algorithms based on these previous deadlock-avoidance theories [12, 13, 18, 29, 41, 48]. Several partially adaptive algorithms based on the turn model have been proposed: negative-ﬁrst, north-last, westﬁrst [20] and odd-even [4]. The Abacus turn model is a dynamically reconﬁgurable routing algorithm [19]. These partially adaptive algorithms allow aggressive VC reallocation: a VC can be re-allocated as soon as the tail ﬂit of the last packet arrives [8]. This property can be directly deduced from Dally and Seitz’s theory [6] since the channel dependency graphs of these algorithms are acyclic. However, they all suffer from limited adaptivity: packets cannot use all minimal paths between the source and destination, while fully adaptive ones can use all minimal paths. 2.2 Fully Adaptive Routing Algorithms Duato’s theory [12, 13] is widely used in the design of fully adaptive routing algorithms. In this theory, VCs are classiﬁed into two types: escape and adaptive. In the event of deadlock among the adaptive VCs, packets must have the opportunity to ‘escape’ to a deadlock-free set of VCs, known as escape VCs. Escape VCs are kept deadlock free by applying a more restrictive routing algorithm; dimension order routing (DOR) is typically used. An escape VC can only be used by a packet whose output port selection corresponds to a DOR path. Many algorithms based on Duato’s theory [21, 31, 35, 50] are composed of two parts: the routing function and selection strategy. If the selection strategy selects one output port, the packet can only request VCs that belong to the chosen output port. This requirement imposes a limitation on these algorithms: once a packet enters an escape VC, it can only use escape VCs until it is delivered. Otherwise, the escape VC may be involved in deadlock. In a VC-limited network, such as a cache-coherent NoC, this limitation easily results in adaptivity loss. However, Duato’s theory supports the design of algorithms which can use an adaptive VC after using an escape VC if packets are always guaranteed to be able to use escape VCs [12, 13]. Based on these observations, we propose a design which maintains high adaptivity for packet routing, works well in a VC-limited environment and has low hardware overhead. 3 Motivation In this section, we analyze the requirements of fully adaptive routing algorithms. We also illustrate how these requirements negatively affect performance. 3.1 VC Re-allocation Scheme One limitation of fully adaptive routing algorithms is that at any time, a VC can hold at most one packet; a VC can only be re-allocated when it is empty. This is a reasonable requirement since fully adaptive routing algorithms put no limitation on the routing of some VCs and allow a cycle to form among VCs. For example, in routing algorithms based on Duato’s theory, the adaptive VCs can be arbitrarily used [12, 13]. If multiple packets are allowed to reside in the same VC, a deadlock conﬁguration is easily formed. Figure 3 illustrates a deadlock conﬁguration [15]. Here each VN is conﬁgured with two VCs: an adaptive VC (AVC) and an escape VC (EVC). Conﬁguring more VCs cannot eliminate this deadlock scenario since cycles are allowed to existed among adaptive VCs. Eight packets are involved in this deadlock: P0 - P7 . The head ﬂit of P0 is behind the tail ﬂit of P1 in AV C1 . The same is true for P1 , P2 , P4 , P5 and P6 . Although the head ﬂits of P3 and P7 are at the head of AV C3 and AV C6 , they cannot move forward as the two valid output VCs, AV C0 and EV C0 , are both occupied by other packets. No packet can move forward. This deadlock is due to that some head ﬂits are not at the VC heads, resulting some packets unable to gain access to the ‘deadlock-free’ path. Also, the tail ﬂits of these packets resides in other VCs, prohibiting following packets to reach the head of these VCs or even utilize these VCs. The following packets then may cyclically block aforementioned packets. For example, the tail ﬂit of P0 resides in AV C0 , blocking P3 from utilizing this VC, which cyclically blocks the routing of P0 . A V C ) 2 E V C 2 R0 R1 R3 R2 P 2 ( B P 2 ( T ) P 1 ( H ) P 1 ( B ) P2(B ) P2(H) P3(T) P3(H) AVC3 EVC3 Dest(P0)=R0 C V A 0 C V E 0 P ( 0 B ) P ( 0 B ) Dest(P2)=R3 D AVC1 P1(B) P1(T) P0(H) P0(B) EVC1 R5 R4 Dest(P4)=R4 AVC6 EVC6 P7(H) P7(T) P6(H) P6(B) A V C 5 E V C 5 P4(B) P4(H) P5(T) P5(B) AVC4 EVC4 P ( 0 B ) P ( 0 T ) P ( 4 B ) P ( 4 B ) P ( 4 B ) P ( 4 T ) P 5 ( H ) P 5 ( B ) P 6 ( B ) P 6 ( T ) D e s t ( P 1 ) = R 1 e s t ( P 3 ) = R 2 D e s t ( P 5 ) = R 5 Dest(P6)=R3 D e s t ( P 7 ) = R 2 Figure 3. Deadlock in a fully adaptive routing algorithm if multiple packets are allowed to reside in one VC. (AVC: adaptive VC; EVC: escape VC; Pi (H), Pi (B) and Pi (T): the header, body and tail ﬂit of Packet Pi , respectively; Dest(Pi ): the destination of Packet Pi .) A V C 2 E V C 2 These slots can not be used by P1 according  to conservative VC re-allocation scheme. E V C 1 R0 R1 R2 P 3 ( T ) P 2 ( H ) P 2 ( T ) A V C 1 P 1 ( H ) P 1 ( T ) Figure 4. A VC underutilization scenario with conservative VC re-allocation. (AVC: adaptive VC; EVC: escape VC; Pi (H) and Pi (T): the head and tail ﬂit of Packet Pi .) If the packet length is greater than the VC depth, allowing multiple packets to reside in one VC easily results in deadlock. When a long packet enters a non-empty VC, its header ﬂit is not at the head of a VC, while its tail ﬂit blocks the head of another VC. However, due to the abundant wiring on chip, NoC trafﬁc is dominated by short packets. With many short or single-ﬂit packets, strictly adhering to the requirement that a VC can only hold one packet reduces throughput and results in VC underutilization. In Figure 4, neither EV C2 nor AV C2 are available for reallocation; P1 must wait in AV C1 until either EV C2 of AV C2 becomes empty. However, since P1 consists of only two ﬂits, and both EV C2 and AV C2 have enough slots to hold this packet, forwarding P1 into these VCs will not prevent following packets from getting to the head of AV C1 , as was the case in Figure 3. This is an opportunity for performance optimization. We will prove that P1 can be forwarded into EV C2 or AV C2 without leading to deadlock. 3.2 Packet Adaptivity In this section, we focus on maintaining packet adaptivity in VC-limited environments. Many fully adaptive routing algorithms based on Duato’s theory are composed of V:1  Arbiter input VC0 at input port 0 VC status which VCs are free for  Reg0 re-allocated at the  optimal output port? V optimal  output port VC  requests E D M X U 1 V PV .. to second stage arbiters, and each bit corresponds to a second stage arbiter PV M X U 1 (a) Port-selection-ﬁrst design. (b) Naive design. Figure 5. The structure of a ﬁrst stage arbiter in a VC allocator for one VN. (Each VN is conﬁgured with V VCs and P input/output ports.) 2V:1  Arbiter input VC0 at input port 0 VC status which VCs are free for  Reg0 re-allocated at the two  available output ports? 2V two available  output ports VC  requests E D M X U 1 2V PV .. to second stage arbiters, and each bit corresponds to a second stage arbiter PV M X U 1 A V C 1 E V C 1 R1 R2 R6 R5 P3(T) P3(H) AVC2 EVC2 Dest(P4)=R0 C V A 3 C V E 3 Dest(P1)=R7 D AVC0 P0(H) P0(T) EVC0 P ( 4 H ) P ( 4 T ) P ( 5 H ) P ( 5 B ) P ( 5 T ) P 1 ( H ) P 1 ( B ) P 2 ( H ) P 2 ( T ) P 1 ( T ) Dest(P5)=R0 Dest(P2)=R7 e s t ( P 3 ) = R 5 D e s t ( P 0 ) = R 2 R3 R7 R0 R4 The selection strategy selects the north port for P4 and P5. The selection strategy selects the south port for P1 and P2. Figure 6. A deadlock conﬁguration if packets in EVCs can apply for AVCs in port-selection-ﬁrst algorithms. two parts: routing function and selection strategy [21, 31, 35, 50]. The routing function computes all available output ports, then the selection strategy selects one of them. Once the selection strategy makes a choice, the packet can only request VCs for this particular port. This type of routing algorithm is called port-selection-ﬁrst. Assuming a separable VC allocator consisting of two stages of arbiters [2, 36, 40], a port-selection-ﬁrst algorithm only requires V : 1 arbiters in the ﬁrst stage as shown in Figure 5(a). A limitation of these algorithms is that once a packet enters an escape VC, it must continue to use escape VCs; the packet will lose adaptivity in subsequent hops. Violating this limitation results in deadlock as shown in Figure 6. In this example, both south and east ports are available for P1 and P2 . If the selection strategy chooses the south port, P1 and P2 can only apply for AV C2 . They cannot request EV C2 because escape VCs can be only used when the chosen port adheres to DOR. Similarly, the selection strategy chooses the north port for P4 and P5 ; they can only apply for AV C0 . No packet can move forward. Thus, the limitation that once a packet enters an escape VC, it can only use escape VCs until delivered is necessary for port-selectionﬁrst algorithms. However, this requirement results in significant adaptivity loss with limited VCs, since packets have a high probability of going into escape VCs. Duato’s theory supports the design of algorithms which allows a packet to use adaptive VCs after using escape VCs, if it satisﬁes the following condition: a packet must be able to request an escape VC at any time. Once satisfy this condition, packets can always ﬁnd a path whose VCs are not involved into cyclic dependencies since the extended channel dependency graph of escape VCs is acyclic [12, 13]. To achieve this target, a packet could be allowed to request all available output VCs, since at least one output port must adhere to DOR and the packet can use the escape VC of this port [12, 13]. However, this naive design results in additional hardware overhead. As shown in Figure 5(b), the VC allocator must use 2V : 1 arbiters in the ﬁrst stage to cover the at most two available output ports for minimal routing algorithms. Based on these observations, we propose a novel design which maintains signiﬁcant packet adaptivity with only minor additional hardware. 4 Whole Packet Forwarding and Fully Adaptive Routing In this section, we present our whole packet forwarding scheme and prove it is deadlock-free. Next, we design a routing algorithm which maintains packet adaptivity without signiﬁcant hardware costs. Finally, we describe the hardware design and overhead. 4.1 Whole Packet Forwarding As described in Section 3.1, existing fully adaptive routing algorithms use conservative VC re-allocation to prevent deadlock. However, this results in poor VC utilization. Therefore, we propose a novel VC re-allocation scheme: whole packet forwarding, which greatly improves VC utilization while not inducing deadlock. Suppose a packet Pk with length of leng th(Pk ) currently resides in V Ci , and V Cj is downstream of V Ci . Assume that the routing algorithm allows packet Pk to use V Cj . With conservative VC re-allocation, V Cj can be re-allocated to Pk only if the tail ﬂit of its most recently allocated packet has been sent out, i.e., it is currently empty [8]. For our proposed VC re-allocation scheme, V Cj can be re-allocated if it already holds the tail ﬂit of the most recently allocated packet, and the current free buffer count (f ree slots(V Cj )) is greater than or equal to leng th(Pk ). If f ree slots(V Cj ) ≥ leng th(Pk ), then all ﬂits of Pk are guaranteed to be sent to V Cj after a limited time3 . We call this VC re-allocation scheme whole packet forwarding (WPF). Figure 7 shows a WPF example. Here, the routing algorithm allows P1 to use V C2 . V C2 has already received the tail ﬂit of P2 and its free buffer count is two; this space is sufﬁcient to hold the whole packet P1 which consists of two ﬂits. As a result, if we use WPF to re-allocate V C2 to P1 , all ﬂits of P1 will be sent to V C2 in a limited time. WPF forwards a packet into a non-empty VC 3 The time to send all ﬂits of Pk to V Cj will be determined by the congestion and switch allocation but all ﬂits of Pk are guaranteed to advance. R0 R0 R1 P 1 ( T ) P 1 ( H ) V C 1 R2 P 2 ( T ) P 2 ( H ) V C 2 VC2 is re-allocated  to P1 R1 V C 1 P 1 ( T ) P 1 ( H ) P 2 ( T ) P 2 ( H ) V C 2 R2 Figure 7. An example of whole packet forwarding. Whole packet forwarding if it has enough free buffers to hold the whole packet. In this case, WPF works similarly to packet-based ﬂow controls such as store-and-forward (SAF) [16] and virtual cutthrough (VCT) [26]. However, if the downstream VC is empty, we still use wormhole ﬂow control, which does not require the empty VC to have enough slots to hold the whole packet; this reduces the buffering requirements compared to SAF and VCT. WPF can be viewed as applying packetbased ﬂow control in a wormhole network. This hybrid ﬂow control mechanism solves the shortcoming of conservative VC re-allocation for fully adaptive routing algorithms. Our contention is that if the routing algorithm with conservative VC re-allocation is deadlock-free, then applying WPF to forward packets into non-empty VCs will not lead to deadlock. If the VC depth is larger than the maximum packet length, and the network applies packet-based ﬂow controls, multiple packets are allowed to reside in one VC for fully adaptive routing algorithms [14]. However, for the wormhole network, a blocking packet may reside in multiple VCs, introducing two additional dependencies, indirect and cross indirect dependency, between non-neighboring channels [12, 13, 14]. With these additional dependencies, it is difﬁcult to prove the deadlock-free property of WPF based on existing theories. We ﬁrst give a qualitative proof for algorithms based on Duato’s theory: using WPF will never allow a packet to get stuck ‘mid-way’ between two routers, as packets will always either be able to be fully transmitted to non-empty VCs or otherwise they will be able to use the escape VCs. However, the ‘escape VC’ is only deﬁned in Duato’s theory, and other theories may not have this deﬁnition. Thus, we provide a general proof. For convention, we label the routing algorithm with conservative VC re-allocation as Alg ; Alg + W P F is Alg with WPF applied to allow forwarding of entire packets to non-empty VCs. Theorem 1: If Alg is deadlock-free, then Alg + W P F is also deadlock-free. Informal Description: Our proof is by contradiction. We prove that if there is a deadlock conﬁguration for Alg + W P F, then there is a deadlock conﬁguration for Alg as well. Using the deadlock conﬁguration C onf ig0 shown in Figure 8 as an example, we remove these packets whose head ﬂits are not at the heads of VCs, and get a new conﬁguration C onf ig1 . We prove that Alg can achieve C onf ig1 , and C onf ig1 is a deadlock conﬁguration. However, Alg is R0 P 0 ( T ) P 0 ( H ) P 1 ( T ) P 1 ( H ) V C 1 VC0 P7(H) P7(T) P6(H) P6(T) R3 3 C V ) H ( 5 P ) T ( 5 P ) H ( 4 P ) T ( 4 P Config0 Removing P0,  P2, P4 and P6 from Config0 R1 P2(T) P2(H) P3(T) P3(H) VC2 R2 R0 VC0 P7(H) P7(T) R3 R1 P3(T) P3(H) VC2 R2 P 1 ( T ) P 1 ( H ) V C 1 3 C V ) H ( 5 P ) T ( 5 P Config1 Figure 8. The construction of a new conﬁguration based on C onf ig0 of Alg + W P F . deadlock-free, thus there is no such conﬁguration. Proof : By contradiction. If Alg + W P F is not deadlockfree, then there is a deadlock conﬁguration (C onf ig0 ) in which a set of packets, Pset0 are waiting on VCs held by other packets in Pset0 . We prove that a deadlock conﬁguration also exists for Alg . Our proof consists of three steps. Step 1: We build a new conﬁguration based on C onf ig0 . Consider each packet Pi in Pset0 . If Pi is a packet whose header ﬂit is not at the head of a VC, then this VC was allocated to Pi using WPF; therefore, all ﬂits of Pi must reside in this VC in C onf ig0 . We remove Pi from the network and label these removed packets as Psubset0 . We label the new conﬁguration as C onf ig1 , and the set of packets remaining in this conﬁguration as Psubset1 . Step 2: We prove that when the network is routed by Alg , all packets in Psubset1 can be forwarded into their current VCs in C onf ig1 . We consider each packet Pj in Psubset1 . We further consider each hop hopk of Pj when the network is routed by Alg + W P F. Without loss of generality, we assume the head ﬂit of Pj is forwarded from V Ck to V Ck+1 during hopk . There are two situations for V Ck+1 . 2.1) V Ck+1 is empty when the head ﬂit of Pj is forwarded into it; therefore, V Ck+1 is allocated to Pj using conservative VC re-allocation. Thus, if the network is routed by Alg , Pj can use V Ck+1 . 2.2) V Ck+1 is not empty when the head ﬂit of Pj is forwarded into it, thus V Ck+1 is allocated to Pj using WPF. Since Pj can be forwarded into V Ck+1 , the routing algorithm allows Pj to use V Ck+1 . However, if the network is routed by Alg , Pj cannot be forwarded into V Ck+1 until it is empty. Since Alg is deadlock-free, the packet currently residing in V Ck+1 must be sent out in a limited time. Then V Ck+1 can be re-allocated to Pj using conservative VC reallocation. Thus, if the network is routed by Alg , Pj can use V Ck+1 . Considering 2.1) and 2.2) together, for each hop, if a VC is used by Pj when the network is routed by Alg + W P F, this VC can be also used by Pj when the network is routed by Alg . Thus, Pj can be routed to its current VC(s) in C onf ig1 by Alg . Step 3: We prove that C onf ig1 is a deadlock conﬁguration for Alg . For each Pi in the removed packet set Psubset0 , all ﬂits of Pi reside in one VC but the head ﬂit of Pi is not at the head of its VC. Thus, removing Pi from the network does not create an empty VC; each VC now holds ﬂits of only one packet. Alg utilizes conservative VC re-allocation which only allows empty VCs to be re-allocated. Therefore all packets in the remaining packet set Psubset1 still wait for VCs held by other packets in Psubset1 . Thus, C onf ig1 is a deadlock conﬁguration for Alg , but Alg is deadlock-free, so there is no such deadlock conﬁguration. Thus, Alg +W P F is deadlock-free as well. Note that our proof does not make any assumption about the routing algorithm; WPF can be utilized with any fully adaptive routing algorithm if it is deadlock-free using conservative VC re-allocation. WPF removes the constraints of conservative VC re-allocation. Thus, it is an important extension of these theories. 4.2 Fully Adaptive Routing Algorithm As demonstrated in Section 1, fully adaptive routing algorithms can yield worse performance than deterministic and partially adaptive ones in VC-limited networks. To combat this problem, we leverage WPF to design a novel fully adaptive routing algorithm with superior performance. Our design is based on Duato’s theory [12, 13]. In a VClimited NoC, the routing algorithm should maintain maximum routing ﬂexibility; it should allow the use of adaptive VCs after using escape VCs. Otherwise, once a packet goes into escape VCs, it loses adaptivity in subsequent routing. The design must guarantee that at any time a packet is able to request an escape VC [12, 13]. We make a simple modiﬁcation. In port-selection-ﬁrst algorithms, the only time a packet cannot use an escape VC is when the selection strategy chooses an port that violates DOR. Our design allows the packet to violate the selection in this case; the packet can apply for the escape VC of the other port that was not selected in addition to adaptive VCs of the selected one. Using P1 in Figure 6 as an example, if the selection strategy chooses the south port, our algorithm allows P1 to request the escape VC of the east output port as well. If there is only one available port, this port must adhere to DOR, and the packet can request its escape VC. Our design guarantees that a packet always has an opportunity to use an escape VC. Thus, it allows a packet to move back into an adaptive VC after using an escape VC. It only needs V : 1 arbiters in the ﬁrst stage of the VC allocator. Large arbiters result in more hardware overhead and introduce additional delay on the critical path. 4.3 Router Microarchitecture The pipeline of a canonical NoC Router [8, 16, 36, 40] consists of four stages: routing computation (RC), VC allocation (VA), switch allocation (SA) and switch traversal (ST). Several optimizations are applied to achieve high baseline performance. The speculative switch allocation is used to parallelize VA and SA [40]. Look-ahead routing removes RC from the critical path; the adaptive routing algorithm calculates at most two available output ports one hop ahead and applies a selection strategy to choose an optimal one [21, 27, 31]. The delay of the baseline router is 2 cycles plus an additional cycle for link traversal. Both WPF and our routing algorithm only require simple modiﬁcations to the baseline VC allocator. They can be used with any type of VC allocator; we assume a separable VC allocator which is widely used due to its low complexity and high frequency [2, 36, 40]. In a separable VC allocator, each input VC determines which output VC of the selected output port to bid on in the ﬁrst stage. The winning requests from the ﬁrst stage then arbitrate for an output VC in the second stage. We modify the ﬁrst stage arbiters. First, we need to monitor whether a downstream VC is free to be re-allocated with WPF. The criterion is that the downstream VC holds the tail ﬂit of its most recently allocated packet and still has enough free slots to hold the entire new packet. Calculating whether there are enough free buffer slots for a new packet introduces some hardware overhead. However, considering that cache coherence packets exhibit a bimodal distribution and long packets are generally longer that the VC depth, we focus on applying WPF to single-ﬂit packets. Thus, if a downstream VC receives the tail ﬂit of the most recently allocated packet, and it still has free slots, it can be re-allocated to a single-ﬂit packet. Figure 9 depicts our proposed VC allocator. Reg0 records if a downstream VC is free to be re-allocated with conservative re-allocation; the downstream VC is currently empty. An additional register Reg1 is needed to record whether a downstream VC is free to be re-allocated with WPF. Based on the incoming packet type, Reg0 or Reg1 is chosen as the input to MUX0. If the incoming packet is a single-ﬂit packet, we apply WPF, choosing the contents of Reg1 as the input for MUX1. Otherwise, the contents of Reg0 are sent to MUX0. Updates to Reg0 and Reg1 are off the critical path since a router monitors the status of downstream VCs using credits [8]. The only increase in delay for WPF is an additional 2-input multiplexer: MUX0. To support our new fully adaptive routing algorithm, we modify MUX1 and DEMUX1, as shown in Figure 9. MUX1 needs two additional input signals: DOR and the other output port. The DOR signal indicates if the chosen optimal output port obeys DOR or not. The other output port signal records the available output port that was not chosen. The routing computation logic produces these two signals. If the DOR signal is ‘0’, then the selected output port violates DOR path. In this case, the status of the escape VC for the other output port rather than the chosen optimal one will be sent to the V : 1 arbiter. This is accomplished with a 2-input multiplexer whose select signal is DOR. DEMUX1 also needs these two additional signals. If the DOR signal is ‘0’, the result of V : 1 arbiter is de-multiplexed to the second stage arbiter for the escape VC of the other first stage arbiter for input VC0 at input port 0 the other output port DOR single-flit packet PV Reg0 0 1 which VCs are free  for re-allocated? V .. V V:1  Arbiter 1 X U M E D DOR the other output port s u a t t s C V X U M X U PV PV M Reg0 optimal  output port VC  requests second stage arbiters PV .. from other  input VCs PV .. PV .. PV:1 Arbiter 0 .. PV:1 Arbiter PV-1 .. PV to other  input VCs PV .. .. PV to input VC0 at  input port 0 Figure 9. The proposed VC allocator for one VN. Table 2. The critical path delay and area results. Design Delay (ns) Area (μm2 ) 49437.4 56045.4 49512.6 Port-selection-ﬁrst (Figure 5(a)) Naive design (Figure 5(b)) Proposed design (Figure 9) 1.78 1.92 1.79 output port instead of the chosen port. A 2-input demultiplexer implements this function. The increased delay for our new fully adaptive routing algorithm is an additional 2input multiplexer and demultiplexer. To analyze the hardware overhead, we implement the three VC allocators (Figures 5 and 9) in RTL Verilog for an open-source NoC router [2] and synthesize in Synopsys Design Compiler with a TSMC 65nm standard cell library. The designs operate at 500 MHz under normal conditions (1.2V, 25◦C). We use simple round-robin arbiters [8]. This router has 5 ports (P = 5) and supports 4 VNs; each VN has 2 VCs (V = 2). Table 2 presents the area and critical path delay estimates. The naive design uses 4:1 arbiters in the ﬁrst stage, resulting in a 7.9% longer critical path and 13.4% more area than the port-selection-ﬁrst design. Our design uses 2:1 arbiters in the ﬁrst stage and only increases the critical path by 0.5% and area by 0.2%. An allocator’s power consumption is largely decided by the arbiter size [2, 50]; given the small arbiters in our design, there should be negligible power overhead compared with the port-selection-ﬁrst design. However, we omit a detailed power evaluation as it depends on the activity factor of each signal. 5 Evaluation We modify the cycle-accurate Booksim simulator [8] to model the microarchitecture discussed in Section 4. We compare the performance of our proposed fully adaptive routing algorithm with conservative VC re-allocation (FULLY) and with WPF (FULLY+WPF) against several routing algorithms. We implement a port-selection-ﬁrst fully adaptive routing algorithm with conservative VC re-allocation (PSF) and with WPF (PSF+WPF). The deterministic routing algorithm is DOR. West-ﬁrst, negative-ﬁrst and oddeven represent partially adaptive algorithms. Since the design of selection strategy is orthogonal to this paper, we use a local selection strategy for all adaptive algorithms; when there are two available output ports, the selection strategy   Table 3. Baseline conﬁguration and variations. Characteristic Baseline Variations 4×4 8×8 Topology (mesh) VCs/VN 2 4 Flit buffers/VC 4 3, 2 Packet length (ﬂits) long: 5, short: 1 SFP ratio 80% 60%, 40% Warmup cycles, Total Cycles 10000, 100000 Table 4. Full system simulation conﬁguration. # of cores 16 L1 cache (D & I) private, 4-way, 32KB each L2 cache private, 8-way, 512KB each Cache coherence MOESI distributed directory 4×4 Mesh Topology chooses the port with more free buffers. We evaluate both synthetic trafﬁc and real applications. is independent. Our baseline conﬁguration uses a 4×4 mesh For synthetic trafﬁc patterns, we use one VN since each VN with 2 VCs that are each 4 ﬂits deep. The packet lengths exhibit a bimodal distribution; there are single-ﬂit and ﬁve-ﬂit packets. The baseline single-ﬂit packet (SFP) ratio is 80%. Table 3 summarizes the baseline network conﬁguration and the variations used in the sensitivity studies. To measure full-system performance, we leverage two simulation frameworks: FeS2 [37] for x86 simulation and BookSim for NoC simulation. FeS2 is a timing-ﬁrst, multiprocessor, x86 simulator, implemented as a module for Virtutech Simics [32]. We run PARSEC benchmarks [3] with 16 threads on a 16-core CMP. We assume cores optimized for clock frequency; they are clocked at a frequency 5× higher than the network. Prior research shows the frequency of simple cores in many-core platform can be optimized to 5∼10 GHZ, while the frequency of NoC router are limited by the allocator speed with a large number of VCs [11]. As we consider several VNs, more VCs are needed. Thus, it is reasonable to assume cores will be clocked faster than the network. Each core is connected to private, inclusive L1 and L2 caches. Cache lines are 64 bytes; long packets are 5 ﬂits wide with a 16-byte ﬂit width. We use a distributed, directory-based MOESI coherence protocol which needs 4 VNs for protocol-level deadlock freedom. Each VN has 2 VCs; each VC is 4 ﬂits deep. All benchmarks use the simsmall input sets to reduce simulation time. The total runtime is used as the metric for full-system performance. Table 4 presents the system conﬁguration. 5.1 Performance of synthetic workloads Figure 10 illustrates the performance of several routing algorithms in our baseline conﬁguration using four synthetic trafﬁc patterns: bit reverse, hotspot and 2 transpose patterns. Across these four patterns, the fully adaptive routing algorithms (PSF and FULLY) show the poorest performance. Although PSF and FULLY offers adaptiveness for all trafﬁc, conservative VC re-allocation signiﬁcantly limits their performance. In contrast, DOR and partially adaptive routing algorithms use aggressive VC re-allocation. For all four patterns, PSF’s performance is worse than FULLY. PSF’s performance is further limited by its poor ﬂexibility: once a packet enters an escape VC, the packet can only be routed by DOR using escape VCs in subsequent hops. {s3 , s2 , s1 , s0 } sends trafﬁc to destination {s0 , s1 , s2 , s3 }. For bit reverse trafﬁc, a source node with bit address 62.5% of this trafﬁc is between the north-east and southwest quadrants; negative-ﬁrst offers adaptiveness for this trafﬁc. Only 37.5% of the trafﬁc is eastbound; west-ﬁrst offers adaptiveness for this trafﬁc, which leads to poorer performance than negative-ﬁrst. The adaptiveness offered by odd-even is lower than negative-ﬁrst, thus its performance is worse than negative-ﬁrst. Although WPF improves the VC utilization for PSF+WPF, its saturation throughput is lower than odd-even and negative-ﬁrst. PSF+WPF is still limited by its poor ﬂexibility. FULLY+WPF provides high VC utilization and signiﬁcant routing ﬂexibility leading to the highest saturation throughput4 . For transpose-1, a node (i, j ) sends messages to node (3 − j, 3 − i). Negative-ﬁrst deteriorates to DOR for this pattern. West-ﬁrst still offer adaptiveness for 37.5% of the trafﬁc, thus it has better performance than negative-ﬁrst. Odd-even offers greater adaptiveness than the other two partially adaptive algorithms and has higher performance. FULLY+WPF offers adaptiveness for all trafﬁc, achieving 15.7% higher saturation throughput than odd-even. Transpose-2 is a favorable pattern for negative-ﬁrst; a node (i, j ) sends messages to node (j, i). Negative-ﬁrst offers adaptiveness for all trafﬁc in this pattern and has the highest performance. Although FULLY+WPF offers adaptiveness for all packets as well, its performance is limited by the restriction on usage of the escape VCs: only if the output port adheres to DOR, can the escape VC be used. The performance of FULLY+WPF and odd-even with transpose-2 are very close to their performance with transpose-1 since the two transpose patterns are symmetric and these two algorithms offer the same adaptiveness for them. With hotspot trafﬁc, four nodes are chosen as hot spots and receive an extra 20% trafﬁc in addition to the uniform random trafﬁc. This pattern mimics memory controllers receiving a disproportionate amount of trafﬁc. FULLY+WPF and odd-even algorithms show higher performance than negative-ﬁrst and west-ﬁrst ones, because they can offer greater adaptiveness. Due to the more limited adaptiveness offered by odd-even, its performance is worse than FULLY+WPF. DOR has better performance than negative-ﬁrst and west-ﬁrst, since DOR more evenly distributes uniform trafﬁc which is used as the background in this pattern. In summary, with conservative VC re-allocation, the fully adaptive algorithm has the worst performance. Negative4 The saturation point is measured as the injection rate at which the average latency is 3 times the zero load latency. DOR Wes t -f irs t N egat ive-f irs t Odd-ev en PSF PSF+WPF FU LLY FU LLY+W PF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR Wes t -f irs t N egat iv e-f irs t Odd-even PSF PSF+WPF FU LLY FU LLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR Wes t -f irs t N egat ive-f irs t Odd-ev en PSF PSF+WPF FU LLY FU LLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR Wes t -f irs t N egat ive- f irs t Odd-ev en PSF PSF+WPF FU LLY FU LLY+W PF ) l s e c y c ( y c n e a L t 80 60 40 20 0 0% 10% 20% 30% 40% 50% 60% Injec tion bandw idth ( flits /node/cyc le) 0% 10% 20% 30% 40% 50% 60% Injec tion bandw idth (flits /node/cyc le) 0% 10% 20% 30% 40% 50% 60% Injec tion bandw idth ( flits /node/cyc le) 0% 10% 20% 30% 40% 50% Injec tion bandw idth ( flits /node/cyc le) (a) Bit reverse. (b) Transpose-1. (c) Transpose-2. Figure 10. Routing algorithm performance for the baseline conﬁguration. Table 5. Average saturation throughput improvement. Algorithm Improvement Algorithm Improvement (d) Hotspot. FULLY DOR West-ﬁrst Negative-ﬁrst 88.9% 64.5% 58.6% 26.6% Odd-even PSF PSF+WPF 16.3% 130.9% 31.3% ﬁrst and west-ﬁrst offer uneven adaptiveness for different patterns. For example, although negative-ﬁrst achieves high performance for transpose-2, it deteriorates to DOR for transpose-1. As the trafﬁc in NoCs changes throughout runtime and different VNs may run different trafﬁc patterns, these partially adaptive routing algorithms are unsuitable. Odd-even offers limited adaptiveness for all trafﬁc patterns. WPF improves VC utilization for fully adaptive routing algorithms. In a VC-limited environment, routing ﬂexibility must also be considered to fully leverage WPF. PSF+WPF does not provide enough routing ﬂexibility resulting in lower performance than some partially adaptive algorithms. FULLY+WPF provides high VC utilization and routing ﬂexibility, leading to the best performance. Table 5 gives the average saturation throughput improvement of FULLY+WPF over the other algorithms across all four patterns. The 88.9% saturation throughput gap between FULLY+WPF and FULLY reﬂects the effect of WPF. The gap between FULLY+WPF and PSF+WPF represents of the effect of routing ﬂexibility; sufﬁcient ﬂexibility leads to an average saturation throughput improvement of 31.3%. 5.2 Performance of PARSEC benchmarks Figure 11 shows the speedups relative to PSF for the PARSEC workloads. We divide the 10 applications into 2 classes. For blackscholes, f luidanimate, ray trace and swaptions, different routing algorithms have similar performance. The working sets of these applications ﬁt into the caches leading to a lightly loaded network. Their system performance is unaffected by techniques that improve throughput, such as sophisticated routing algorithms. However, the routing algorithm inﬂuences the performance of the remaining 6 applications; they exhibit bursty communication and have heavier loads. Their system performance is sensitive to network optimizations; routing algorithms with higher saturation throughputs improve performance. For example, FULLY+WPF has 48.5% and 43.0% speedup over PSF for f acesim and streamcluster . Westﬁrst has the best performance for v ips because most of its bursty communication is eastbound. For f acesim and streamcluster , negative-ﬁrst offers higher adaptiveness than odd-even for bursty communication, thus achieving higher performance. For all heavy load applications except v ips, FULLY+WPF has the best performance. Across these applications, FULLY+WPF achieves an average of 21.3% and maximum 37.8% speedup over FULLY. With sufﬁcient ﬂexibility, FULLY+WPF has an average 12.1% speedup over PSF+WPF. The average speedups of FULLY+WPF are 29.3%, 15.0%, 10.1%, 9.9% and 10.4% over PSF, DOR, west-ﬁrst, negative-ﬁrst and odd-even, respectively. Our design supports higher saturation throughput; if higher saturation throughput is not required, WPF can beneﬁt the NoC by reducing network resources such as reducing the buffer resources and reducing the channel width. 6 Sensitivity to Network Design Single-ﬂit packet ratios. Individual network implementations are likely to vary from our baseline conﬁguration, depending on the needs of the system. We explore variations for further insight. Except for the analyzed parameter, the other parameters are the same as the baseline (Table 3). Single-ﬂit packet (SFP) ratios depend on the running application, the cache hierarchy and the coherence protocol. To test the robustness of WPF, we evaluate 60% and 40% SFP ratios for transpose-1 trafﬁc. As illustrated in Figure 12, DOR, west-ﬁrst, negative-ﬁrst and odd-even exhibit nearly identical performance for different SFP ratios. Since they apply aggressive VC re-allocation, their performance is not sensitive to packet length. However, the performance of PSF and FULLY improves as the SFP ratio shrinks. The conservative VC re-allocation used by PSF and FULLY favors long packets since they utilize buffers more efﬁciently than short ones. As the SFP ratio decreases so does the possibility of applying WPF. Thus, the performance gap between FULLY+WPF and FULLY (or PSF+WPF and PSF) decreases. However, even with a 40% SFP ratio, FULLY+WPF achieves a 53.1% saturation throughput improvement over FULLY. For these different SFP ratios, FULLY+WPF has the best performance. VC depth. Different NoCs may use different VC depths.         p u d e e p s e m i t n u r k s a T 1.6 1.5 1.4 1.3 1.2 1.1 1 0.9 0.8 DOR Wes t-f irs t N egat iv e-f i rs t Odd-ev en PSF PSF+WPF FULLY FULLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 Light load applications Heavy load applications blackscholes fluidanimate PSF DOR raytrace W est- fi rst swaptions bodytrack canneal Negative- fi rst Odd-even facesim PSF+WPF fer ret FULLY streamcluster FULLY+WPF vips Figure 11. System speedup for PARSEC benchmarks. DOR Wes t -f irs t N egat iv e-f irs t Odd-ev en PSF PSF+WPF FU LLY FU LLY+W PF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR West -f i rs t N egat iv e-f irs t Odd-ev en PSF PSF +WPF FU LLY FU LLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR Wes t-f irs t N egat iv e-f i rs t Odd-ev en PSF PSF+WPF FULLY FULLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 0% 10% 20% 30% 40% 50% Injec tion bandwidth (flits /node/c yc le) 0% 10% 20% 30% 40% 50% Injec tion bandwidth (flits /node/c yc le) 0% 10% 20% 30% 40% 50% Injec tion bandwidth ( flits /node/cyc le) 0% 10% 20% 30% 40% Injec tion bandwidth (flits /node/c yc le) (a) 60% SFP ratio (b) 40% SFP ratio. Figure 12. The performance with different SFP ratios. (a) 3-ﬂit deep VCs. (b) 2-ﬂit deep VCs. Figure 13. The performance with different VC depths. To test the ﬂexibility of WPF, we further evaluate 3- and 2ﬂit deep VCs. Comparing 4 ﬂits/VC (Figure 10(a)) and 3 ﬂits/VC (Figure 13(a)), the performance of DOR and westﬁrst remain almost constant, while FULLY and PSF exhibit minor performance degradation. DOR and west-ﬁrst offer no or very limited adaptiveness which is a major factor in their performance. Thus, reducing the VC depth from 4 to 3 has little effect. The bottleneck of FULLY and PSF is conservative VC re-allocation. Considering the majority of single-ﬂit packets, reducing the VC depth from 4 to 3 only affects performance slightly. However, the performance of FULLY+WPF, PSF+WPF, odd-even and negative-ﬁrst declines with shallower VCs since the VC depth is a bottleneck for them. Shallow VCs increase the number of hops that a blocked packet spans, which increases the effect of chained blocking [46]. Comparing 3 and 2 ﬂits/VC, performance drops for all algorithms. FULLY has better performance than DOR and west-ﬁrst with 2 ﬂits/VC. As VC depth decreases, the difference between aggressive and conservative VC re-allocation declines. FULLY has a relative performance improvement. However, even with only 2 ﬂits/VC, WPF still optimizes the performance since short packets dominate trafﬁc. In Figure 13(b), FULLY+WPF has a 46.2% saturation throughput improvement over FULLY. Applying WPF on fully adaptive routing algorithms leads to superior performance even with half of the buffer resources; enabling the design of a very low-cost NoC. With 2 ﬂits/VC (Figure 13(b)), FULLY+WPF achieves a saturation throughput of 40.3%, while the saturation throughput of FULLY is 32.3% with 4 ﬂits/VC (Figure 10(a)). The same is true for PSF+WPF with 2 ﬂits/VC and PSF with 4 ﬂits/VC. As semiconductor scaling continues, a VN may be conﬁgured with more VCs. Coherence protocols VC count. may be optimized to reduce the number of required VNs allowing more VCs per VN. Comparing Figure 14(a) (4 VCs/VN) and Figure 10(a) (2 VCs/VN), the performance of DOR, west-ﬁrst and odd-even is almost the same. These algorithms offer limited adaptiveness; although additional experimental results show increasing the VC count from 1 to 2 improves performance, increasing the VC count from 2 to 4 cannot reduce the congestion for physical paths and does not further improve performance. Negative-ﬁrst has a modest performance improvement. In contrast, PSF, FULLY, PSF+WPF and FULLY+WPF all have signiﬁcant improvement; more VCs mitigate the negative effects of conservative VC re-allocation. The performance difference between PSF and FULLY (or PSF+WPF and FULLY+WPF) decreases with more VCs; more VCs reduce the possibility of using escape VCs in PSF which limits the packets that lose adaptivity. Figure 14(b) shows the performance of transpose-2, which is a favorable pattern for negative-ﬁrst. FULLY+WPF achieves almost the same performance as negative-ﬁrst; with more VCs, the effect of restricting the use of escape VCs in FULLY+WPF declines. With more VCs, the gap between FULLY and FULLY+WPF (or PSF and PSF+WPF) diminishes. More VCs improves the possibility of a packet being forwarded into an empty VC, thus improving the performance of FULLY (or PSF). Furthermore, using WPF to forward a new packet into a non-empty VC may result in head-of-line congestion [8] and degrade the performance of FULLY+WPF (or PSF+WPF). Nevertheless, FULLY+WPF still shows an average 19.8% saturation throughput improvement over FULLY for these two patterns with 4 VCs; providing high VC utilization strongly outweighs the negative effect of HoL blocking in a VClimited environment. Yet, additional experimental results             DOR Wes t -f irs t N egat iv e-f irs t Odd-even PSF PSF +W PF F U LLY F U LLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 DOR W es t -f irs t N egat iv e-f irs t Odd-ev en PSF PSF +WPF F U LLY F U LLY+WPF ) l s e c y c ( y c n e t a L 80 60 40 20 0 DOR Wes t-f irs t N egat iv e-f i rs t Odd-ev en PSF PSF+WPF FU LLY FU LLY+WPF ) l s e c y c ( y c n e t a L 80 60 40 20 0 DOR West -f irs t N egat iv e-f irs t Odd-ev en PSF PSF +WPF FU LLY FU LLY+WPF ) l s e c y c ( y c n e a L t 80 60 40 20 0 0% 10% 20% 30% 40% 50% 60% 70% Injec tion bandwidth (flits /node/cyc le) 0% 10% 20% 30% 40% 50% 60% 70% Injec tion bandwidth (flits /node/cyc le) 0% 5% 10% 15% 20% 25% Injec tion bandwidth (flits /node/c yc le) 0% 5% 10% 15% 20% 25% Injec tion bandwidth (flits /node/cy c le) (a) Bit reverse. (b) Transpose-2. Figure 14. The performance with 4 VCs/VN. Figure 15. The performance for an 8×8 mesh network. (a) Bit reverse. (b) Transpose-1. Network size. show that WPF results in minor performance degradation with 8 VCs/VN for some trafﬁc patterns. Similarly, aggressive VC re-allocation slightly degrades the performance of deterministic and partially adaptive algorithms compred to conservative VC re-allocation with 8 VCs/VN for some patterns. With abundant VCs, the HoL blocking outweighs the positive effect of high VC utilization. Similar to VC depth, with only 2 VCs, FULLY+WPF offers near or even better performance (Figures 10(a) and 10(b)) than FULLY with 4 VCs (Figure 14) across these two patterns; WPF provides similar or higher performance with half as many VCs. design for an 8×8 mesh. The trends across different alFigure 15 explores the scalability of our gorithms are the same as with the 4×4 mesh (Figure 10). Communication is determined by the trafﬁc pattern not the network size. Since larger networks lead to higher average hop counts [31], a larger network puts more pressure on VCs than a smaller one. WPF achieves greater performance improvement in a larger network. The average saturation LY+WPF over FULLY is 108.2%, while in a 4×4 mesh, throughput improvement for these two patterns of FULit is 93.1%. As packets must travel more hops in a larger network, the possibility of entering an escape VC increases. For PSF and PSF+WPF, once the packet enters an escape VC, it loses adaptivity in subsequent hops. Therefore the performance gap between FULLY+WPF and PSF+WPF (or FULLY and PSF) increases with a larger network; providing routing ﬂexibility becomes more important with a larger network. In summary, although the effect of WPF decreases with smaller SFP ratios, shallower VC depths or more VCs, applying WPF with fully adaptive routing still improves performance signiﬁcantly. The effect of WPF as well as routing ﬂexibility increases with a larger network. Applying WPF to fully adaptive routing algorithms provides similar or even better performance with half of the buffer resources or VCs as a network that does not employ WPF. 7 Further Discussion and Future Work Packet length and VC depth. Packet lengths for cache coherence trafﬁc typically have a bimodal distribution. However, optimizations such as cache line compression [11, 25] create packet distributions that are not bimodal; the packet length may be distributed between a single ﬂit and the maximum ﬂits per packet supported by the architecture. To apply WPF on such NoCs, more downstream VC status registers are needed for the ﬁrst-stage arbiters shown in Figure 9. An important consideration is how many different packet lengths to apply WPF to. The longest packet length that can use WPF is one ﬂit shorter than the VC depth. Designers can ignore long packets, since they have few opportunities to apply WPF. This tradeoff depends on the packet length distribution, VC depth, hardware overhead and the expected performance gain. Delving into this tradeoff is left for future work. In this paper, we assume the VC depth is shorter than the maximum packet length. If the VC depth exceeds the maximum packet length, conservative VC re-allocation results in poorer VC utilization for wormhole than VCT, and applying WPF in this case behaves the same as VCT. The contribution of WPF is that it allows multiple packets to reside in one VC while allowing the VC depth to be shorter than the maximum packet length, thus giving designers more ﬂexibility. DAMQ and hybrid ﬂow controls. Previous research proposed the dynamically allocated multi-queue (DAMQ) designs for both off-chip [45] and on-chip networks [38, 50] to improve the VC utilization. Even with DAMQ, allowing multiple packets to reside in one VC may lead to a deadlock conﬁguration similar to Figure 3 for fully adaptive routing algorithms in a wormhole network. WPF is complimentary to DAMQ as it ensures deadlock-freedom and improves the design ﬂexibility. WPF can viewed as a hybrid mechanism combining wormhole and VCT. There are some previous hybrid ﬂow controls [42, 44, 30]. Hybrid switching [42] and buffered wormhole [44] remove a blocked wormhole packet to release held physical channels by utilizing either the processing node memory [42] or a central buffer [44]. Layered switching divides the long wormhole packets into several groups and tries to keep the switch allocation grants for a whole group [30]. The purpose of WPF is quite different; we focus on improving the performance for fully adaptive routing algorithms in wormhole networks.         8 Conclusion Whole packet forwarding is a novel VC re-allocation scheme for fully adaptive routing algorithms in wormhole networks. This scheme allows multiple packets to reside in one VC concurrently; it greatly improves VC utilization in VC-limited networks where short packets dominate trafﬁc. We prove that WPF does not lead to deadlock if the algorithm is deadlock-free with conservative VC reallocation. Thus, WPF is an important extension to existing deadlock-avoidance theories. We further propose a novel fully adaptive routing algorithm that exploits WPF and provides routing ﬂexibility with modest hardware overhead. Compared with conservative VC re-allocation, WPF improves the saturation throughput by 88.9% on average in synthetic trafﬁc patterns and achieves up to 37.8% (21.3% average) full-system speedup for network-intensive PARSEC benchmarks, and offers similar or even better performance with half of the buffer resources or VCs. Acknowledgments We thank the anonymous reviewers for their helpful suggestions and members of Prof. Enright Jerger’s group for feedback. We also thank Daniel Becker of Stanford for his explanation on the open-source router implementation. This work is supported by the University of Toronto, NSERC of Canada, the Connaught Fund, 863 Program of China (2012AA010302), NSFC (61070037, 61025009, 60903039, 61103016), China Edu. Fund. (20094307120012), Hunan Prov. Innov. Fund. For PostGrad. (CX2010B032). "
2013,Runnemede - An architecture for Ubiquitous High-Performance Computing.,"DARPA's Ubiquitous High-Performance Computing (UHPC) program asked researchers to develop computing systems capable of achieving energy efficiencies of 50 GOPS/Watt, assuming 2018-era fabrication technologies. This paper describes Runnemede, the research architecture developed by the Intel-led UHPC team. Runnemede is being developed through a co-design process that considers the hardware, the runtime/OS, and applications simultaneously. Near-threshold voltage operation, fine-grained power and clock management, and separate execution units for runtime and application code are used to reduce energy consumption. Memory energy is minimized through application-managed on-chip memory and direct physical addressing. A hierarchical on-chip network reduces communication energy, and a codelet-based execution model supports extreme parallelism and fine-grained tasks. We present an initial evaluation of Runnemede that shows the design process for our on-chip network, demonstrates 2–4x improvements in memory energy from explicit control of on-chip memory, and illustrates the impact of hardware-software co-design on the energy consumption of a synthetic aperture radar algorithm on our architecture.","Runnemede: An Architecture for Ubiquitous High-Performance Computing Nicholas P. Carter1,4 , Aditya Agrawal1,2 , Shekhar Borkar1 , Romain Cledat1 , Howard David1 , Dave Dunning1 , Joshua Fryman1 , Ivan Ganev1 , Roger A. Golliver1 , Rob Knauerhase1 , Richard Lethin3 , Benoit Meister3 , Asit K. Mishra1 , Wilfred R. Pinfold1 , Justin Teller1 , Josep Torrellas2 , Nicolas Vasilache3 , Ganesh Venkatesh1 , and Jianping Xu1 1 Intel Labs, Hillsboro, Oregon 2University of Illinois at Urbana-Champaign, Champaign, Illinois 3Reservoir Labs, New York, New York 4Contact email: nicholas.p.carter@intel.com Abstract DARPA’s Ubiquitous High-Performance Computing (UHPC) program asked researchers to develop computing systems capable of achieving energy efﬁciencies of 50 GOPS/Watt, assuming 2018-era fabrication technologies. This paper describes Runnemede, the research architecture developed by the Intel-led UHPC team. Runnemede is being developed through a co-design process that considers the hardware, the runtime/OS, and applications simultaneously. Near-threshold voltage operation, ﬁne-grained power and clock management, and separate execution units for runtime and application code are used to reduce energy consumption. Memory energy is minimized through application-managed on-chip memory and direct physical addressing. A hierarchical on-chip network reduces communication energy, and a codelet-based execution model supports extreme parallelism and ﬁne-grained tasks. We present an initial evaluation of Runnemede that shows the design process for our on-chip network, demonstrates 2-4x improvements in memory energy from explicit control of on-chip memory, and illustrates the impact of hardware-software co-design on the energy consumption of a synthetic aperture radar algorithm on our architecture. 1. Introduction DARPA’s Ubiquitous High-Performance Computing (UHPC) program challenged researchers to develop hardware and software techniques for Extreme-Scale systems: computing systems that deliver 100–1,000x higher performance than current systems of the same physical footprint and power consumption [34]. Extreme-scale systems should deliver the energy-efﬁciency (50 GOPS/Watt), reliability, and scalability required to construct practical exaOP supercomputers (machines that execute 1018 operations/second) in the 2018-2020 timeframe. In this paper, we describe Runnemede, the research architecture developed by the Intel-led UHPC team. Runnemede’s goal is to explore the upper limits of energy efﬁciency without the constraints imposed by backward compatibility and the need to support conventional programming models. Its hardware, OS/runtime, and applications are being developed through a co-design process to produce a system in which hardware and software work together to maximize performance and minimize energy consumption. We begin this paper by outlining our technical approach. We then present Runnemede’s architecture, focusing on the hardware but describing the software stack where relevant. This is followed by a preliminary evaluation of our network design, memory system, and the impact of our co-design process. Finally, we present related work and conclude. 2. Technical Approach Runnemede1 is heavily inﬂuenced by several predictions about 2018-2020 fabrication technology. The power consumed by logic is expected to scale well as feature sizes 1 Following Intel tradition of naming projects after geographic locations in the US or Canada, the Runnemede project was named after Runnemede, NJ, inspired by Runneymede, England, where the Magna Carta was signed. shrink, but not as well as transistor density, leading to the design of overprovisioned, energy-limited systems that contain more hardware than they can operate simultaneously. Signaling power is expected to scale much less well than logic power, making on-chip and off-chip communication a larger fraction of overall power. SRAM power is also expected to scale less well than logic power, due to the difﬁculty of designing SRAM circuits that can operate at low supply voltages. We expect the power consumed by DDR DRAMs to decrease relatively slowly over time, although stacked DRAMs with improved interfaces will become available in the not-too-distant future [28], significantly reducing DRAM power consumption. However, technology will limit the number of DRAM die per stack, and the need to provide I/O pins for each stack will limit the total DRAM capacity of systems that use stacked DRAMs, potentially leading to systems that combine stacked and DDR DRAMs into two-level DRAM hierarchies. These technology trends lead to several predictions about extreme-scale computer systems. Extreme-scale computer systems will be energy-limited and overprovisioned. Therefore, to achieve maximum performance, they should be designed such that their key subsystems (cores, memory, and networks) can each consume a disproportionate share of the system’s full power budget when that subsystem is the limiting factor on performance. This, in turn, requires that software and hardware actively manage their power consumption to ensure that the system stays below budget. The computer industry is already seeing early examples of overprovisioned designs, such as Intel’s Turbo Boost [17] technology, which deﬁnes a base clock rate for each chip that meets the chip’s power budget when all cores are active, and increases the clock rate when some cores are idle to provide high performance on both serial and parallel codes/regions. Because extreme-scale systems will be energy-limited, they must be designed to operate at their most-efﬁcient supply voltages and clock rates (low, near-threshold voltage (NTV) and modest frequency), although the ability to increase supply voltage and clock rate on serial sections of code will also be of great beneﬁt. Operating at low clock rates implies that extreme-scale systems will require more parallelism than current systems to deliver a given amount of performance, leading to interest in execution models that maximize parallelism and minimize synchronization. Energy-limited systems are expected to be heterogeneous, because there is no performance beneﬁt to building more of a given unit, core, or module than can be operated simultaneously (possibly plus a few spares for reliability), which is sometimes described as the “dark silicon problem/opportunity” [6]. Being energy-limited also encourages specialization, or the design of hardware that may be used infrequently, as long as it can be powered off when not in use. A small number of custom functions/instructions can signiﬁcantly improve an architecture’s efﬁciency and performance on applications that make use of those functions [15] [35], and including such functions does not decrease the performance of applications that do not use them if the system is limited by energy instead of area. Finally, the technology scaling trends described above imply that energy-limited systems must minimize data movement in order to achieve maximum performance. While stacked DRAM is expected to require much less energy to access than DDR DRAM, on-chip wires and SRAMs are expected to scale less well than logic, making data movement increasingly expensive relative to computation. This argues that extreme-scale memory systems and applications should focus on bandwidth efﬁciency by eliminating unnecessary data transfers. 2.1. Energy-Eﬃciency from the Ground Up The Runnemede architecture is built from the ground up for energy efﬁciency. All of the layers of the computing stack are co-designed to consume the minimum possible energy, accepting the cost of limited compatibility with previous operating systems and applications. The processor is intended to operate at near-threshold supply voltages. At such voltages, within-die parameter variations are expected to be signiﬁcant. Consequently, much thought has been put into understanding the likely parameter variations [19], and on designing circuits and organizations to tolerate them in an energy-efﬁcient manner. In addition, Runnemede has widespread clock and power gating in processors, memory modules, and networks. To provide the parallelism required to achieve extremescale performance at the low clock rates that near-threshold voltages allow, Runnemede’s processor chip includes a large number of relatively-simple cores. The initial design described in Section 4 takes this philosophy to the extreme of single-issue, in-order cores, although future work will explore the trade-offs involved in different core architectures. One likely future design is a system containing a small number of large cores optimized for ILP and a large number of simple cores, in order to provide both good performance on sequential sections of code and high parallelism on parallel regions. To exploit locality, cores are organized into groups, where each group contains a set of processors and local memories connected by an energy-optimized network. Runnemede’s memory system is designed to maximize software’s ability to control data placement and movement, in the belief that this will minimize energy consumption at the cost of placing additional responsibility on the software. We provide a single, shared, address space across an entire Runnemede machine. Instead of a hardware-coherent cache hierarchy, our on-chip memory consists of a hierarchy of scratchpads and software-managed incoherent caches, and we provide a set of block transfers to minimize the cost of data movement. Our off-chip memory is implemented using stacked DRAMs with an energy-optimized interface, significantly reducing the energy consumed per bit transferred. The on-chip network is designed with wide links to reduce latencies, but its components are power-gated when unused. In addition to the data network, we provide a network for barriers and reductions/broadcasts. This network reduces the latency and energy cost of synchronization and collective operations, both through specialized hardware and by making it easier for cores to clock- or power-gate themselves while waiting for a synchronization or a collective operation to complete. The software system is co-designed with the hardware, and provides a number of programming models with different trade-offs between simplicity and control of the underlying hardware. Our higher-level models include Hierarchically-Tiled Arrays (HTAs) [13], which expresses computations as blocks or tiles in successive, hierarchical levels, and Concurrent Collections (CnC) [7], which describes computations in a high-level dataﬂow-like manner. The R-Stream R(cid:13) compiler can automatically generate parallelized and locally-optimized code for sequential loop nests. Finally, for programmers who want a lower-level interface to the hardware, it is possible to code directly to our runtime’s codelet model, which is described in the next section. Runnemede also includes mechanisms for energyefﬁcient resilience. In particular, we envision an incremental in-memory checkpointing system [2], which can be adapted to take advantage of the structure of CnC programs 2.2. Hardware-Software Co-Design Runnemede is a co-designed hardware/software effort, in which the hardware, execution model, OS/runtime, and applications are being developed simultaneously by a team that combines computer architects, system software experts, compiler developers, and application experts. This approach is made easier by the fact that the UHPC program deﬁnes ﬁve “challenge problems” that represent a signiﬁcant fraction of the anticipated extreme-scale applications. Given the extreme amounts of parallelism required to achieve exaOP performance at near-threshold supply voltages, we are designing Runnemede’s runtime system around a dataﬂow-inspired [30] execution model. In contrast to pthread-style execution models, which implement parallel programs using long-running communicating threads, dataﬂow-inspired execution models represent programs as graphs of (typically short-running) tasks, where edges represent dependencies between tasks. This is similar to the way dataﬂow processors represent programs as graphs of instructions, with edges that represent dependencies. Further, in our execution model, the tasks in a program graph are codelets [14] — self-contained units of computation with clearly-deﬁned inputs and outputs. Codelets are assumed to run to completion once they begin executing, although the operating system may intervene to halt a codelet that has entered an inﬁnite loop or otherwise exceeded the “acceptable” codelet execution time. Dataﬂow-inspired execution models have a number of characteristics that make them well-suited to extreme-scale systems. First, they make it easy for each phase of an application to exploit all of the parallelism available to it instead of encouraging a static division of an application into threads. Second, in dataﬂow execution models, only the producer and the consumer(s) of an item need to synchronize, potentially reducing synchronization costs. Third, the non-blocking “complete or fail” nature of codelets allows us to avoid much of the context-switching overhead of traditional OSes. Finally, a dataﬂow-inspired execution model makes it easy to identify a computation’s inputs and outputs, and thus to schedule code close to its data, to marshal input data at the core that will perform a computation, and to distribute results from producers to consumers. This wellspeciﬁed data movement both motivates and supports our decision to use software-managed on-chip memories. Just as hardware issues affect our choice of execution model, software concerns inﬂuence hardware design. Having an execution model in which tasks (codelets) have welldeﬁned inputs encourages the design of cores with separate power and clock gating for memories and execution units. This allows the runtime to turn on a core’s memory(ies) in order to marshal a codelet’s inputs and only turn on the execution units when input data is available and the codelet is ready to execute, thus avoiding the energy that would be wasted by idle execution units waiting for inputs to arrive. Our execution model also inﬂuences our hardware design by encouraging the design of two types of cores: general-purpose Control Engines (CEs), which execute OS/runtime code, and energy-optimized Execution Engines (XEs), which execute codelets from user applications. With a space-separated (rather than time-separated) division between system code and user code [24], hardware to enforce protection rings (e.g., user/kernel mode) can be safely omitted from the CEs. Additionally, the non-blocking property of codelets means that I/O operations can only occur on inter-codelet boundaries, allowing a model in which the XEs do not contain I/O hardware. Instead, an I/O operation is represented as a dataﬂow dependence between two codelets. When the producer codelet reaches the I/O operation, it terminates with a request that a CE perform the I/O operation. When the I/O operation completes, the runtime notes that the consumer codelet’s data dependence has been satisﬁed, and schedules the consumer for execution. Co-design also affects our resilience and powerefﬁciency schemes. Our hardware provides the runtime 16 MB  L4  Memory Unit Third-level Data Network Thermal Monitors Power Monitors Circuit Monitors CPU Block Block Block Block 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block 2 e v e l B a r r . N w k . Block Block Block Block 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block 2 e v e l B a r r . N w k . Block Block Block Block 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block 2 e v e l B a r r . N w k . Block Block Block Block 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block 2 e v e l B a r r . N w k . 2 n n d d l l e v e l D a t a N w k . Block Block Block Block Block Block Block Block 2 e v e l B a r r . N w k . 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block Block Block Block Block 2 e v e l B a r r . N w k . 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block Block Block Block Block 2 e v e l B a r r . N w k . 8MB L3 Memory 2 n n d d l l e v e l D a t a N w k . Block Block Block Block Block Block Block Block 2 e v e l B a r r . N w k . 8MB L3 Memory Third-level Barrier Network Offchip  Mem.  Cntl. Offchip  Nwk.  Int. Figure 1. Runnemede chip architecture. with information about the status, temperature and power usage of different regions on the chip. This allows the runtime to allocate work to avoid overheating, reducing hardware complexity. Similarly, the runtime is able to react to hardware failures, for example by assigning less-parallel tasks to a region that contains a failed core, or by increasing the region’s clock rate to compensate if latency is important. Finally, having a well-designed set of target applications allows us to co-design the hardware and the applications. We implement a number of instructions, such as the sincos instruction described later, that have a signiﬁcant impact on one or more of the challenge applications. Application characteristics also inﬂuence our network design, including its barrier hardware and support for collective operations, and our synchronization primitives. In turn, understanding the hardware design allows our application experts to tune their algorithms to the strengths of the hardware. 3. Runnemede Architecture As illustrated in Figure 1, the Runnemede architecture is modular and hierarchical, which allows applications to take advantage of locality and makes it easy to scale the architecture to a wide range of performance, price, and chipsize points. The basic module of Runnemede is the block (shown in Figure 2), which contains several cores, the ﬁrstlevel networks, and an L2 scratchpad memory. The next level of the hierarchy is the unit, which contains multiple blocks, an L3 scratchpad, and the second-level networks. A full Runnemede chip would contain multiple units and an L4 scratchpad, which would be connected by the third-level networks, allowing hundreds of cores to be integrated onto a chip. An off-chip network port allows multiple Runnemede chips to be integrated into a single-board system, with larger systems consisting of multiple boards. First-level Data Network L1 Barrier Network XE XE XE XE CE XE XE XE XE 2.5 MB L2  Memory Block Ex. Unit Reg. File 32K  Incoherent  Cache Control Engine (CE) Very Large  Reg. File 32K  Incoherent  Cache 64K L1  Memory Ex. Unit Performance  Monitors Vdd and Clock  Gating Execution Engine (XE) Figure 2. Contents of a block. 3.1. Architecture of a Block Each block is a heterogeneous system that contains one Control Engine (CE), which executes operating system and runtime routines, and multiple Execution Engines (XEs), which execute tasks from application programs. The 8 XEs shown in the ﬁgure are an initial estimate of the number of XEs that one CE will be able to support without the CE becoming the performance bottleneck; the number of XEs/block in an actual design will depend on the amount of CE support each codelet requires. Similarly, the 8 blocks per unit shown in Figure 1 is an early estimate that will be revised as we gather data about how much L3 memory is required to support a block of cores and the amount of locality in extreme-scale applications. This heterogeneity is expected to increases energyefﬁciency by allowing us to optimize each type of core for the work it does. XEs can be optimized for performance/watt on parallel computations, while CEs are optimized for more latency-sensitive OS operations. Infrequently-used hardware, such as I/O, can be placed in the CE to improve XE efﬁciency on computation kernels. Finally, separating the XE and the CE simpliﬁes the design of systems in which the XEs in different blocks are optimized for different types of computations, since blocks with different types of XE would present the same interface to the OS and hardware outside the block. CEs in Runnemede are typically general-purpose processor cores. XEs are typically custom architectures, containing one or more execution pipelines, a large (512-1024 entry) register ﬁle, a software-managed L1 scratchpad and an incoherent cache. XE instructions may be stored either in the cache or the scratchpad, as selected by a mode bit. Each of the scratchpads and register ﬁles in a Runnemede system maps onto a unique range of addresses from a single shared address space that is described in more detail in Section 3.2.                                                                 Each XE also contains performance monitors and registers that control power and clock gating, which are also mapped into the address space. To reduce XE-CE communication overhead, each XE contains a set of memory-mapped registers for fast XE-CE communication. Writes into these registers inform the CE that the XE needs attention and pass information about what the XE needs the CE to do. If the CE requires additional information to handle the XE’s request, it can read that information directly from the XE’s memory or registers. 3.2. Memory Hierarchy and Address Space Runnemede’s on-chip memory hierarchy does not have hardware-coherent caches. While coherent caches simplify programming, their ﬁxed line lengths and replacement policies can make them energy-inefﬁcient if an application’s access patterns do not match the cache’s assumptions. Instead, most of Runnemede’s on-chip memories are software-managed scratchpads (blocks of SRAM that are mapped onto distinct regions of the address space so that software, rather than hardware, selects which data is kept in each scratchpad). This approach can signiﬁcantly increase the energy-efﬁciency of some codes by eliminating transfers of unused data, false sharing, set conﬂicts, and collisions between streaming data and high-locality data — albeit at a potentially non-trivial cost in programming effort. To simplify the use of the scratchpads, Runnemede provides a set of DMA-like block transfer operations. These operations also reduce energy by performing cache-line-wide accesses to DRAM, which are much more energy-efﬁcient than single-word accesses. Runnemede also includes incoherent, softwaremanaged, caches in each core, which are accessed via The software-managed caches are intended to provide an intermediate efﬁciency/programming effort point between scratchpads and hardware-coherent caches. In our software-managed caches, hardware manages fetching and writeback of lines from/to whichever scratchpad, register ﬁle, or DRAM the address being referenced maps onto, but software is responsible for maintaining consistency when multiple caches may contain copies of the same location. To assist in this, Runnemede provides several cache-management instructions that prefetch lines, invalidate them (remove them from the cache without writing back dirty data), update them from the backing store, or evict them without writing back dirty data. Runnemede provides a single, 64-bit, address space that is shared by all of the software running on the machine. Each scratchpad, DRAM, and register ﬁle in the system, as well as all of the performance monitors and control registers, appears in the address map. Runnemede implements a load.cache and store.cache instructions. physical address space, with no virtual memory. This eliminates both the energy costs of address translation and the limits that TLB capacity places on the amount of memory a program can access efﬁciently (without requiring additional memory accesses for page-table walks). Using a physical address space eliminates the energy cost of translation, but also eliminates the protection and relocation beneﬁts of virtual memory. In addition, a physical address space has the potential to require the use of 64-bit addresses everywhere in the memory system. We solve the latter problem with hardware that determines the number of address bits required for each request based on the distance to the memory being referenced and only sends that many bits over the network, greatly reducing the number of address bits transmitted in programs with high locality. To provide protection without translation, each block in a Runnemede architecture incorporates a “gate” unit that can be conﬁgured to allow or deny requests by a given set of cores to given ranges of the address space. Moreover, our runtime provides relocation by allocating memory as “data blocks”, each of which has a unique identiﬁer. Before a codelet’s ﬁrst use of a data block, it must call a translation routine that returns the physical address of the start of the data block. This allows the runtime to relocate data blocks as long as no codelet is currently referencing them, and limits translation costs to one translation per codelet per data block, as opposed to one translation per memory reference. Finally, our future research will investigate address translation mechanisms that impose less overhead than traditional virtual memory [12][33], in order to make it easier to map large data structures across multiple physical memories. 3.3. Networks A Runnemede processor contains two independent hierarchical networks: a data network and a barrier/reduction network. Each block contains its ﬁrst-level data and barrier networks, which provide low-energy communication within the block. Each block’s ﬁrst-level networks also interface with the second-level networks that connect the blocks in a unit, which in turn interface with the third-level networks that connect the units on a chip. This hierarchical network design allows Runnemede to provide tapered bandwidth, such that the amount of bandwidth between two points is inversely proportional to the distance between them and thus to the energy per bit of messages. The data network handles the trafﬁc generated by ordinary memory references. When a core references data from a memory located outside of the core, the hardware creates a request message that is transmitted to the destination memory over the data network, instructing it to perform the memory access and return the result. In contrast, the barrier/reduction network provides a mechanism for fast barriers and reductions (operations that combine inputs from many cores into one output), and can also be used to perform broadcast/multicast operations. Performing a barrier or a reduction using this network is a two-phase process that separates arriving at a barrier or at the start of a reduction from waiting for the barrier/reduction to complete. This allows a task to signal other tasks that they can proceed past the barrier/reduction at the earliest possible point, perform any independent work that it may have, and then only wait for other tasks to reach the barrier/reduction when it becomes absolutely necessary, reducing wait times. 3.4. Power Management Since an overprovisioned system incorporates more hardware than it can simultaneously operate, it must also contain mechanisms that allow it to dynamically allocate power to different portions of the system in order to remain within its power budget. Runnemede’s power management mechanisms provide ﬁne-grained control over clock rates, supply voltages, and power/clock gating. They are integrated into our address map to make them easy to access. 3.4.1. Dynamic Voltage/Frequency Scaling The Runnemede architecture divides each CPU into several independently controllable clock/power domains, each containing one or more blocks, depending on the size of the chip and the number of voltage controllers it is feasible to fabricate, with an additional domain for the on-chip network. This provides ﬁne-grained control over voltage and frequency, and also allows the network to operate at a voltage/frequency point that minimizes errors while the cores operate at the most energy-efﬁcient voltage/frequency. 3.4.2. Power and Clock Gating Runnemede provides clock-gating (clock is disabled but power supply is on, so that the unit retains its state) and power-gating (power supply and clock are disabled, which destroys the state of the unit) at multiple levels of granularity. Individual cores, memory modules, and network components can be clock-gated or power-gated independently. In addition, portions of a core may be power- or clockgated independently, allowing software to disable units that it knows it will not use, such as the ﬂoating-point unit during an integer computation. This also allows software to power on a core’s memory and ALUs at different times, to minimize power consumption while waiting for input data. 3.4.3. Power Management Interface Runnemede’s power management mechanisms are controlled through a set of memory-mapped registers that appear in the global address space. Reading these registers returns information about the state of the appropriate unit, while writing them changes that state. To prevent malware or buggy code from selecting power states that exceed the chip’s thermal budget or interfere with other applications, we use a combination of our gate-based memory protection mechanisms and hardware-enforced limitations on which registers user code may write to. Looking forward, one of the challenges in our future work will be developing runtime systems and hardwaresoftware interfaces that use our power management interface to conﬁgure the chip to deliver the best performance per Watt for each application. For example, applications that achieve good parallel speedup will maximize performance per Watt by running on many cores but at low clock rates and supply voltages, while applications with poor parallel speedup will prefer a small number of cores at high clock rates. To achieve optimal efﬁciency, extreme-scale systems will need mechanisms to determine which category each application or phase of an application falls into, how to allocate their power budget across the applications running on a system, and how to conﬁgure the resources available to each application to best make use of its power budget. 3.5. Resilience and Reliability Resilience is a major challenge in extreme-scale systems. Errors, variation, and failure rates are expected to increase in future fabrication technologies, particularly when operated at near-threshold voltages. Moreover, the large scale of exaOP supercomputers will lead to high error and failure rates per system. Finally, the drive for energy-efﬁciency may lead to systems that operate with smaller guard bands than today’s systems, increasing transient error rates. Extreme-scale systems must tolerate these non-ideal behaviors, and can only afford to devote a small amount of energy to doing so, which prohibits the use of many of the redundancy-based reliability mechanisms that have been used in the past. Runnemede takes a cross-layer [8] approach to reliability that combines hardware-based error detection with software-based recovery and adaptation mechanisms, minimizing reliability overheads during the common case of correct operation. However, we also incorporate hardware-based recovery techniques, such as ECC memory, where they are energy- and complexity-effective. We use a scalable checkpointing approach based on Rebound [2] to protect state against unrecoverable errors. We envision that a supercomputer-scale Runnemede system will use multiple levels of checkpointing, including checkpointing to DRAM, NVRAM, and hard disks, to reduce checkpoint and recovery overheads for common, localized failures, while still protecting itself against uncommon system-wide failures. 4. Sunshine: an Initial Design Early in the Runnemede project, we began the design of a test chip, code-named “Sunshine.” Intended for fabrication in 22nm technology, Sunshine would have been a demonstration of our architecture and a platform for our software team’s work. While the Sunshine test chip was never actually built, the process of designing a test chip contributed signiﬁcantly to the ideas that went into Runnemede. To strike a balance between utility and implementation effort, Sunshine incorporated most of the programmervisible features in the Runnemede architecture, such as the scratchpad memories, software-managed caches, registerbased power management, and the XE-CE communication interface. However, other aspects of the design, in particular the architecture and microarchitecture of the cores, were chosen to minimize implementation effort. Sunshine’s CEs were based on the Siskiyou Peak [32] synthesizable core, with a custom interface to the on-chip networks and memory hierarchy. The XEs were singleissue in-order cores with a custom RISC ISA that incorporated instructions for software-managed caches, synchronization, network collectives, and block memory transfers. Several Sunshine implementations were considered, including a multi-block chip that supported multi-chip systems. One area where the Sunshine work heavily inﬂuenced the larger Runnemede effort was the sizing of the scratchpads and caches shown in Figure 1. Sunshine’s target clock rate was 500 MHz – 1 GHz, and our analysis suggested that 64KB was the largest scratchpad that would ﬁt in that clock cycle when implemented in energy-efﬁcient SRAM. Similarly, the software-managed caches were sized at 32KB because of the latency incurred by tag lookup and hit/miss checks. We selected 2.5MB as the L2 scratchpad size in order to keep the ﬁrst-level (within a block) network latency under one cycle. The L3 and L4 scratchpad sizes shown in the ﬁgure were not directly driven by the Sunshine design. set of libraries that allows programmers to write scratchpadstyle programs for Linux workstations. Our energy estimates were generated using an internal power model that estimates the energy of each functional block of a design (wires, memories, pipeline stages, etc.) based on representative circuits from existing designs. The unit of energy in our results is the amount of energy required to perform a double-precision ﬂoating-point multiply (FM64), which we selected to allow a comparison between different simulations while still being free of fabrication process details. 5.1. HW-SW Co-Design Case Study One of the ﬁve challenge problems used to benchmark UHPC architectures is a streaming sensor application based on synthetic aperture radar (SAR). SAR’s input is a set of vectors, each of whose values represent the returns from a given radar pulse as a function of time. Given this set of vectors and the location of the radar at the time each pulse was emitted, SAR generates an output image that shows how much energy was reﬂected from each point in the image. We implement our SAR algorithm using the codelet execution model and run it on our simulator, modeling a fourblock Sunshine architecture with a total of 32 XEs and 4 CEs. Figure 3 shows how different co-design optimizations reduce the energy consumption of this application. At the top of the ﬁgure, the Base SAR bar shows the energy consumed by our ﬁrst implementation of the algorithm, which is dominated by the energy consumed in computation. In particular, SAR performs a large number of sin and cos operations, and our baseline implementation spends the majority of its time in math library routines. Compute Memory Network Base SAR +ISAOpt +TrigOpt +Blocking +CompilerOpt 5. Initial Evaluation 0.00E+00 2.00E+10 4.00E+10 6.00E+10 8.00E+10 Energy Consumed (FM64)  As an initial evaluation of Runnemede, we present three results: a case study showing how hardware-software codesign improves the energy efﬁciency of an application, an analysis of the energy/bandwidth trade-offs in different network topologies, and a comparison of the energy costs of scratchpad-based and cache-based memory hierarchies. The co-design results were generated using a functional simulator of the Sunshine architecture, while the network results were generated using an analytic model that accounts for wire length, switch size, and switch energy. Our memory analysis was done with a trace-driven simulator, using traces generated with a custom PIN [26] tool and a Figure 3. Co-design optimizations for SAR To address this problem, we add a sincos instruction to the ISA that computes both the sin and the cos of its input (since SAR typically needs both values). The +ISAOpt bar on the graph shows the energy consumed with this improvement, which reduces compute energy by 86%. We also implement an algorithmic change that replaces the original single-precision computation of each output pixel’s value with a double-precision computation of a subset of the pixels and a less-expensive interpolative computation of the remaining pixels. This reduces compute energy by an additional 45%, as shown in the +TrigOpt bar. The +Blocking bar shows the energy consumed after we modify SAR so that each codelet copies the portions of the input array that it will use into the XE’s L1 scratchpad rather than fetching values from DRAM each time they are used. Since pixels that are close together in the output image depend on similar regions of the input pulses, this substantially reduces the number of DRAM references SAR makes. Finally, our compiler does not perform some address calculation and strength reduction optimizations that a moremature compiler would perform. Hand-implementing these optimizations reduces computation energy by 47% over the value shown in the +Blocking bar, yielding the results shown in the +CompilerOpt bar. In total, our co-design optimizations reduce computation energy by 97% and total energy by 75%, showing the value of jointly optimizing hardware, applications, and development tools. 5.1.1. Eﬀect of Technology Scaling The results presented in Figure 3 assumed a 45nm implementation of Runnemede. Figure 4 shows predictions of how the energy consumed scales with fabrication process (based on internal predictions about process scaling), while Figure 5 shows how the fraction of energy consumed by computation, memory, and the on-chip network scales. As expected, computation energy scales well, decreasing by 77% as we move from 45nm to 10nm, while network energy only decreases by 51%. Memory energy also decreases drastically over time, driven by the energy per byte improvements from the use of stacked DRAM. As a result, SAR’s energy consumption is relatively balanced between computation, memory, and network in the 10nm node. Compute Memory (DRAM + SRAM) Network n o i t a c i r b a F s s e c o r P 45nm 22nm 10nm 0.00 0.20 0.40 0.60 Relative Energy Consumed  0.80 1.00 Figure 4. SAR energy scaling Compute Memory (DRAM + SRAM) Network 0% 20% 40% 60% Energy Distribution  80% 100% Figure 5. SAR energy distribution 45nm 22nm 10nm s s e c o r P n o i t a c i r b a F 5.2. Network Analysis Our network analysis focuses on minimizing the amount of energy required to send each message (or packet) in the network. Extreme-scale applications need signiﬁcant communication locality in order to meet their energy goals, encouraging the design of networks that minimize the cost of short-distance communication even if that somewhat increases the cost of long-distance communication. However, we also have to ensure that the on-chip network provides enough global bandwidth to not limit the performance of application phases that require global communication, under the assumption that such phases power-down other portions of the chip in order to free up power for communication. Given these guidelines, we examine a number of treebased networks, as they provide high bandwidth with a small number of switch crossings for local messages. In addition, recent work [23, 22] suggests that designs based on relatively high-radix switches reduce network energy. Tree-based networks can also provide differing amounts of bandwidth at each level in the tree to tune the ratio of local to global bandwidth. Figure 6 illustrates this effect. In a pruned tree, each link in the network has the same bandwidth. As a result, the total bandwidth at each level in the network decreases by the radix of the switches used. At the other extreme, the bandwidth of the links in a fat tree scales up by the switch radix at each level, keeping the total bandwidth per level constant. In between these extremes, the bandwidth per link of a hybrid tree increases as one moves up the tree, but at a rate smaller than the switch radix, so the global bandwidth at each level decreases more slowly than the global bandwidth of the pruned tree. In our experiments, we consider hybrid tree networks in which the total bandwidth at each level is a factor of two lower than the total bandwidth of the level below it, regardless of switch radix. We evaluate our networks by calculating the energy per bit of messages in two access patterns: uniform-random and a localized pattern. In the latter, each message’s destination is selected randomly from the nodes at a distance H from the source, where the probability that a message travels H hops is N selected such that (cid:88) KH H , with KH being the number of nodes at distance H from the source node, and N being a normalizing constant = 1. In our analysis, we model the number of hops each message traverses, the wire length of each hop, and the switch energy of each conﬁguration. Figure 7 shows the results of our localized-trafﬁc analysis. The fat tree and pruned tree topologies show energy minima between 4-ary and 16-ary trees, depending on the size of the network. These minima are less evident in the hybrid tree topologies, whose energy curves are closer to monotonically-increasing with switch radix. A Runnemede system fabricated in 2018-2020 would be N KH H ∀H           4""ary""fat(tree"" w w w w 4w"" 4w"" 4w"" 4w"" 4""ary""hybrid(tree"" w w w w 2w"" 2w"" 2w"" 2w"" 4""ary""pruned(tree"" w w w w ""w"" """"w"" """"""w"" """"""w"" Figure 6. Schematic of a fat tree, a hybrid tree and a pruned tree Hybrid-tree  Pruned-tree  0  0.1  0.2  0.3  0.4  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  Fat-tree  0  0.1  0.2  0.3  0.4  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  0  0.1  0.2  0.3  0.4  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  256  512  1024  4096  16384  #Nodes"" Figure 7. Energy per bit for localized trafﬁc as a function of switch radix Fat-tree  Hybrid-tree  Pruned-tree  3.2  2.8  2.4  2  1.6  1.2  0.8  0.4  0  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  3.2  2.8  2.4  2  1.6  1.2  0.8  0.4  0  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  #Nodes"" 3.2  2.8  2.4  2  1.6  1.2  0.8  0.4  0  1  4  16  64  256 1024 4096 16384  E e n r y g e p r b t i ( F M 4 6 ) K of K-ary switches  256  512  1024  4096  16384  Figure 8. Energy per bit for uniform random trafﬁc as a function of switch radix expected to have a 512- to 1,024-node on-chip network. Focusing on the 1,024-node network, the curves show that the best conﬁgurations of the pruned tree and hybrid tree topologies require very similar amounts of energy to send a message, and that the curve is relatively ﬂat near the optimal point. The 512-node network also sees similar message energy in the hybrid tree and pruned tree topologies, although its slope is distorted because there are only a small number of valid switch radices for 512-node trees if all levels in the tree are required to have the same radix. The fat tree network shows signiﬁcantly higher message energies for both the 512-node and 1,024-node networks, making it unattractive for Runnemede. On uniform-random trafﬁc, as shown in Figure 8, the different networks have very different energy trends. In the fat tree network, message energy decreases signiﬁcantly as ary, and thus switch size, increase, while message energy increases monotonically with switch radix in the hybrid tree network. Finally, the pruned tree network sees high message energies for both very small and very large switch sizes, with minima in between. These curves can be explained by considering the size, and thus the energy cost, of the switches in each network. Fat trees use wider channels in higher levels of the tree, making switches in the top levels large and energyexpensive. This encourages the use of high-radix switches that increase the number of nodes a given node can reach via the lower levels of the tree. In pruned trees, switches at all levels of the network are the same size, creating a trade-off between switch size and the number of switches traversed by an average message and leading to sub-optimal energy at either extreme of radix size. Hybrid trees lie between these two extremes: their link widths increase at higher levels in the tree, but at a much slower rate than fat trees. This causes them to have signiﬁcantly lower message energies than fat trees, approaching the message energy of pruned trees.                                                 Table 1. Runnemede network parameters Network First-Level Network Second-Level Network Third-Level Network Link Width N 2N 4N Radix (Modules Connected) 11 (1 CE, 8 XEs, 1 L2 Memory, 1 port to Second-Level Network) 10 (8 First-Level Network Ports, 1 L3 Memory, 1 Third-Level Network Port) 11 (8 Second-Level Network Ports, 1 L4 Memory, 1 Off-Chip Memory, 1 Off-Chip Network) Based on these results, we envision a hybrid tree network for Runnemede, but one whose link bandwidth increases more slowly with level in the tree than the hybrid tree networks evaluated here. Pruned tree networks had the lowest message energy in our studies, but provide very little bisection bandwidth, making them a performance bottleneck on applications with limited communication locality. Fat tree networks have high bisection bandwidth, but are too energyexpensive. Our results also suggest that an 8-ary or 16-ary tree is close to energy-optimal, leading to a three-level tree for 512- to 1,024-node networks. Based on this analysis, Table 5.2 shows the parameters for the Runnemede network. 5.3. Evaluating Scratchpad Memories To evaluate the energy-efﬁciency of scratchpad memories, we simulate the execution of a 1,024x1,024 Givens QR decomposition and a 2,048x2,048 matrix-matrix multiplication on an 8-XE block of Runnemede. We start with a sequential version of each program, and use the R-Stream compiler [27] to generate parallel versions for hardwarecoherent caches with different sets of cache locality optimizations. We also modify R-Stream to automatically compile applications to a scratchpad-based memory hierarchy, although, at the moment, the compiler can only target one level of scratchpads. Finally, we hand-code versions of the applications to take advantage of the two-level scratchpad hierarchy present in a Runnemede block. Our results show the active memory energy consumed by each benchmark, neglecting leakage energy because because our trace-driven simulator does not model time accurately. We model the scratchpad-based memory hierarchy of a single block as shown in Figure 1, with 64KB L1 scratchpads in each core and a 2MB L2 scratchpad (rounding to the nearest power-of-two bytes). We also simulate a memory hierarchy with hardware-coherent 64KB L1 and 2MB L2 caches. Both levels of cache use 64-byte lines and are 8-way set-associative. We model an “oracular” directorybased MESI coherence protocol in which each cache has complete knowledge about the contents of the other caches in the system. These results assume DDR DRAM off-chip memories, because more detailed information about their power consumption is available than for stacked DRAMs. DRAM Energy L2 Energy L1 Energy Network Energy Naïve Cache Parallelization Best Compiled Cache Compiled 1-Level Scratchpad (Copy Loops) Compiled 1-Level Scratchpad (Block Transfers) Hand-Coded 2-Level Scratchpad (Copy Loops) Hand-Coded 2-Level Scratchpad (Block Transfers) Hand-Coded 2-Level Scratchpad Alg. On Cache Intel MKL DGEMM On Cache 5.0E+12  2.5E+11  9.6E+11  0 2E+10 4E+10 6E+10 Active Memory Energy (FM64)  8E+10 Figure 9. Memory energy for matrix mult. 5.3.1. Matrix Multiplication Figure 9 shows the results of our matrix multiplication experiments. The Na¨ıve Cache Parallelization bar shows the energy consumed in the memory system when executing an 8-threaded matrix multiplication with no blocking for cache locality on our cache hierarchy. The Best Compiled Cache bar shows the energy consumed on the same hierarchy when all of R-Stream’s cache locality optimizations are applied and the arrays holding each matrix are padded to prevent set conﬂicts. Applying these optimizations reduces memory system energy by over two orders of magnitude, from 4.96x1012 FM64 to 4.47x1010 FM64. Next, we use R-Stream to compile versions of matrix multiplication for our scratchpad-based memory hierarchy, generating variants that use either copy loops or block transfer operations to move data. The Compiled 1-Level Scratchpad bars show the energy used by these versions of the computation. When block transfers are used to copy data, RStream is able to achieve the same energy consumption as a two-level hardware-coherent cache hierarchy, in spite of only being able to take advantage of the L1 scratchpads. In contrast, the copy loop version of the scratchpad-based algorithm consumes almost 6x more energy than the best cache-based algorithm, due to the 7x increase in energy per byte when performing single-word accesses to the DRAMs. The Hand-Coded 2-Level Scratchpad bars show the energy consumed by a hand-written matrix multiplication that takes advantage of both the L1 and L2 scratchpads in a block. When copy loops are used, the hand-coded computation approaches the energy of the best cache-based code, DRAM Energy L2 Energy L1 Energy Network Energy 5.7E+11  3.6E+10  Naïve Cache Parallelization Best Compiled Cache Compiled 1-Level Scratchpad (Copy Loops) Compiled 1-Level Scratchpad (Block Transfers) Hand-Coded 2-Level Scratchpad (Copy Loops) Hand-Coded 2-Level Scratchpad (Block Transfers) Hand-Coded 2-Level Scratchpad Alg. On Cache 0 5E+09 1E+10 1.5E+10 Active Memory Energy (FM64)  2E+10 Figure 10. Memory energy for Givens QR even with the energy/byte penalty of single-word DRAM accesses. When block transfers are used, energy consumption decreases to 49% of the best cache-based code. The last two bars of the graph are included as sanity checks on our results. The Hand-Coded 2-Level Scratchpad Alg. on Cache bar shows the energy consumed when the scratchpad-based code is run on a hardware-coherent cache hierarchy, to demonstrate that our energy efﬁciency gains are due to scratchpads, not to a more-efﬁcient underlying algorithm. The Intel MKL DGEMM on Cache bar shows the memory energy consumed by an eight-threaded 2,048x2048 matrix multiplication using the DGEMM routine from Intel’s Math Kernel Libraries [29]. Because the MKL DGEMM was optimized for a different memory hierarchy, it is not possible to directly compare its results to our other results, but the fact that the MKL DGEMM did not beat our compiled codes argues that R-Stream does a good job optimizing for cache locality. 5.3.2. Givens QR Figure 10 shows the results of our experiments with Givens QR decomposition. Applying locality optimizations to the cache-based code reduces memory energy by 40x over a na¨ıve parallelization. When block transfers are used for data movement, the R-Stream-compiled scratchpadbased code consumes 20% less energy than the best cachebased code, again using only the L1 scratchpads. The hand-coded version of Givens QR that takes advantage of both the L1 and the L2 scratchpads uses 30% less memory system energy than the best cache-based version, even when energy-inefﬁcient copy loops are used for data movement. When the block transfer operations are used, the scratchpad algorithm uses 76% less memory energy than the cache-based algorithm. Again, we also run the handcoded scratchpad code on a cache-based hierarchy to show that scratchpads are responsible for the improvement. These results suggest that, in some cases, giving programs direct control over on-chip memory can signiﬁcantly reduce memory system energy. This reduction in energy comes at a non-trivial cost in programmer effort, although our results and those of others [25][4][10] suggest that compilers may be able to automate scratchpad management, at least for regular codes. 6. Related Work Runnemede is one of four extreme-scale architecture research projects funded by the UHPC program. The NVIDIA-led Echelon team [21] developed a GPU-inspired architecture that integrates a large number of throughputoptimized cores and a smaller number of latency-optimized cores onto a chip. Sandia’s X-Caliber project combined latency-optimized cores with compute-near-memory units using 3-D stacking, and MIT’s Angstrom group [3] explored techniques for self-aware computing systems and factored operating systems. Our execution model [24] is based on the Codelet paradigm [14] [36], which is separately embodied in ETI’s SWARM runtime [11]. Our high-level compiler team is developing tools that use the Hierarchically-Tiled Arrays [13] [5] and Concurrent Collections [7] models to compile conventional programming languages into codelet-based programs. We have also been contributing to the development of the Open Community Runtime [31] and have been working with researchers from Rice University to apply concepts from their Habanero [9] model to our system. The power-gating, clock-gating, and NTV techniques used in Runnemede build on a large body of circuit research at Intel [20] [1] [16]. Recently, Intel Labs demonstrated an experimental NTV IA-32 processor, code-named Claremont [18], that achieves a 4.7x increase in energy efﬁciency by reducing its supply voltage from 1.2V to 0.45V. 7. Conclusions Runnemede is a “blank sheet of paper” research architecture designed to maximize energy efﬁciency without the constraints imposed by backward compatibility and the need to support conventional programming models. Runnemede’s focus on energy-efﬁciency begins at the circuit level, using NTV circuits and ﬁne-grained power and clock gating to minimize power dissipation. At the architectural level, we use many simple cores, organize them into hierarchical groups, and divide them into Execution Engines and Control Engines to allow separate optimization of hardware for OS code and application kernels. Our memory system emphasizes efﬁciency through a single address space, software-managed scratchpads, incoherent caches, and block operations. Our on-chip network is hierarchical and provides support for barriers and collectives. We continue to develop Runnemede through a co-design process that simultaneously explores hardware architectures, runtime/OS mechanisms, and applications. Our initial experience with Runnemede has shown that co-design can signiﬁcantly reduce application energy and has demonstrated the potential of application-managed memory hierarchies. However, many questions remain unanswered about programmability, execution engine architecture, and off-chip networking, to name a few. Our ongoing work is exploring these issues, and the Runnemede architecture will continue to evolve as this work proceeds. 8. Acknowledgments This research was, in part, funded by the U.S. Government. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the U.S. Government, the Intel Corporation, the University of Illinois at Urbana-Champaign, or Reservoir Labs, Inc. We would like to thank the anonymous reviewers, Doug Carmean, Jim Held, Justin Rattner, and Terry Smith for their feedback about early versions of this paper.  [1] A. Agarwal et al. A 320mV-to-1.2V on-die ﬁne-grained reconﬁgurable fabric for DSP/media accelerators in 32nm CMOS. In ISSCC, 2010. [2] R. Agarwal, P. Garg, and J. Torrellas. Rebound: Scalable checkpointing for coherent shared memory. In ISCA, 2011. [3] The MIT angstrom project. projects.csail.mit. edu/angstrom. [4] O. Avissar, R. Barua, and D. Stewart. An optimal memory allocation scheme for scratch-pad-based embedded systems. ACM Transactions on Embedded Computing Systems, 1(1):6–26, Nov. 2002. [5] G. Bikshandi et al. Programming for parallelism and locality with hierarchically tiled arrays. In PPOPP, 2006. [6] S. Borkar and A. A. Chien. The future of microprocessors. Communications of the ACM, 54(5):67–77, 2011. [7] Z. Budimlic et al. Concurrent collections. Scientiﬁc Programming, (18), 2010. [8] N. P. Carter, H. Naeimi, and D. S. Gardner. Design techniques for cross-layer resilience. In DATE, 2010. [9] V. Cave, J. Zhao, J. Shirako, and V. Sarkar. Habanero-java: the new adventures of old X10. In Principles and Practice of Programming in Java, 2011. [10] A. Dominguez, S. Udayakumaran, and R. Barua. Heap data allocation to scratch-pad memory in embedded systems. Journal of Embedded Comptuing, 1(4):521–540, 2005. [11] ET International. SWARM (swift adaptive runtime machine). Technical report, ET International, 2011. [12] M. Fillo et al. The M-Machine multicomputer. In MICRO, 1995. [13] B. B. Fraguela et al. The hierarchically tiled arrays programming approach. In LCR, Houston, Texas, 2004. [14] G. R. Gao, J. Suetterlein, and S. Zuckerman. Toward an execution model for extreme-scale systems–runnemede and beyond. Technical Report CAPSL TR 104, University of Delaware, April 2011. [15] R. Hameed et al. Understanding sources of inefﬁciency in general-purpose chips. In ISCA, 2010. [16] S. Hsu et al. A 280mV-to-1.1V 256b reconﬁgurable SIMD vector permutation engine with 2-dimensional shufﬂe in 22nm CMOS. In ISSCC, 2012. Intel R(cid:13) turbo boost [17] Intel Corporation. technology in Intel R(cid:13) CoreTM microarchitecture (Nehalem) based processors. White Paper, November 2008. [18] S. Jain et al. A 280mV-to-1.2V wide-operating-range IA-32 processor in 32nm CMOS. In ISSCC, pages 202–203, 2012. [19] U. R. Karpuzcu, K. B. Kolluru, N. S. Kim, and J. Torrellas. VARIUS-NTV: A microarchitectural model to capture the increased sensitivity of manycores to process variations at near-threshold voltages. In DSN, 2012. [20] H. Kaul et al. A 300mV 494GOPS/W reconﬁgurable dualsupply 4-way SIMD vector processing accelerator in 45nm CMOS. In ISSCC, 2009. [21] S. W. Keckler et al. GPUs and the future of parallel computing. IEEE Micro, 31(5):7–17, 2011. [22] J. Kim, W. Dally, S. Scott, and D. Abts. Technology-driven, highly-scalable dragonﬂy topology. In ISCA, 2008. [23] J. Kim, W. J. Dally, and D. Abts. Flattened butterﬂy: a costefﬁcient topology for high-radix networks. In ISCA, 2007. [24] R. Knauerhase et al. For extreme parallelism, your OS is Sooooo last-millennium. In HotPar, 2012. [25] T. Knight, J. Park, M. Ren, and M. Houston. Compilation for explicitly managed memory hierarchies. In PPOPP, 2007. [26] C.-K. Luk et al. Pin: Building customized program analysis tools with dynamic instrumentation. In PLDI, 2005. [27] B. Meister et al. R-stream compiler. In Encyclopedia of Parallel Computing. Springer "
2013,TS-Router - On maximizing the Quality-of-Allocation in the On-Chip Network.,"Switch allocation is a critical pipeline stage in the router of an Network-on-Chip (NoC), in which flits in the input ports of the router are assigned to the output ports for forwarding. This allocation is in essence a matching between the input requests and output port resources. Efficient router designs strive to maximize the matching. Previous research considers the allocation decision at each cycle either independently or depending on prior allocations. In this paper, we demonstrate that the matching decisions made in a router along time actually form a time series, and the Quality-of-Allocation (QoA) can be maximized if the matching decision is made across the time series, from past history to future requests. Based on this observation, a novel router design, TS-Router, is proposed. TS-Router predicts future requests to arrive at a router and tries to maximize the matching across cycles. It can be extended easily from most state-of-the-art routers in a lightweight fashion. Our evaluation of TS-Router uses synthetic traffic as well as real benchmark programs in full-system simulator. The results show that TS-Router can have higher number of matchings and lower latency. In addition, a prototype of TS-Router is implemented in Verilog, so that power consumption and area overhead are also evaluated.","TS-Router: On Maximizing the Quality-of-Allocation in the On-Chip Network Yuan-Ying Changy , Yoshi Shih-Chieh Huangy , Matthew Porembaz , Vijaykrishnan Narayananz , Yuan Xiez,x , and Chung-Ta Kingy yDepartment of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, felmo,yoshi,kingg@cs.nthu.edu.tw zDepartment of Computer Science and Engineering, Pennsylvania State University, University Park, USA, fmrp5060,vijay,yuanxieg@cse.psu.edu xAMD Research, yuan.xie@amd.com Abstract Switch allocation is a critical pipeline stage in the router of an Network-on-Chip (NoC), in which ﬂits in the input ports of the router are assigned to the output ports for forwarding. This allocation is in essence a matching between the input requests and output port resources. Efﬁcient router designs strive to maximize the matching. Previous research considers the allocation decision at each cycle either independently or depending on prior allocations. In this paper, we demonstrate that the matching decisions made in a router along time actually form a time series, and the Quality-of-Allocation (QoA) can be maximized if the matching decision is made across the time series, from past history to future requests. Based on this observation, a novel router design, TS-Router, is proposed. TS-Router predicts future requests to arrive at a router and tries to maximize the matching across cycles. It can be extended easily from most state-of-theart routers in a lightweight fashion. Our evaluation of TS-Router uses synthetic trafﬁc as well as real benchmark programs in full-system simulator. The results show that TS-Router can have higher number of matchings and lower latency. In addition, a prototype of TS-Router is implemented in Verilog, so that power consumption and area overhead are also evaluated. I. Introduction As the number of cores keeps increasing on chip multiprocessors, the Network-on-Chip (NoC) technology is becoming essential to interconnect these cores [16], [6]. Many prior efforts on the design of efﬁcient routers have been developed to design efﬁcient router for NoCs. One class of works focused on the resource allocation inside a router, including Virtual-channel Allocation (VA) and Switch Allocation (SA), because these operations are in the critical path of the router pipeline [24], [18]. A key issue in resource allocation is the Quality-of-Allocation (QoA), which is the subject of serveral recent works [2], [3], [21], [23]. QoA refers to the ability of a router to match input port requests with output port resources [3], which is often measured as the number of matches that can be made in a cycle. A higher QoA means that a router can move more packets across its internal switches in a cycle, resulting in higher throughput. Most existing works aim at maximizing the number of matches but only consider the allocation within a single cycle. In fact, the matching decisions made in a router along time actually form a time series1 . A greedy allocation algorithm which maximizes the matching in each cycle might not lead to best QoA across the whole time series. Recent works such as Packet Chaining [21] and PseudoCircuit [2] have discovered that allocation decisions made in the current cycle may be affected by those made in the previous cycles. Allocation strategies are thus proposed to improve the number of matches in a router by inheriting the results of previous cycles. Experiment results show this strategy can even outperform maximal matching via wavefront allocator [26] and maximum matching via augmenting path algorithm [9]. This implies that maximal/maximum matching within a single cycle is not good enough and back-to-back allocations should be considered. Although the idea of performing switch allocation based on past history works well, the requirement to achieve good performance is still quite rigid – the current allocation must have suitable similarity with the previous one. Unfortunately, this is only valid in some speciﬁc cases. If the similarity is too low, the performance of history-based strategies [21], [2] will degenerate to that of independent allocation [20]. On the other hand, if the similarity is too high, the performance will also be degraded because inheriting the previous allocation will starve the new requests. Therefore, a more general and less demanding strategy is needed. In this paper, we discuss Quality-of-Allocation of switch allocation from the perspective of time series of matching be1Generally speaking, a time series can be a sequence of data, results, or decisions, which have happen-before relationships and therefore can be represented along with time. izes the idea of time-series matching. This paper is organized as follows: In the next section, preliminaries of this paper are introduced, including a background of allocation and matching, followed by the representation of an allocation. A motivating example is given in Section III to show that maximal and maximum matching considered within a single cycle do not lead to a global optimal allocation. Instead, timeseries-based allocation outperforms these allocation algorithms. In Section IV, the design of TS-Router is introduced, including a discussion of the implementation overheads. For evaluating the performance of TS-Router, we present comprehensive experimental results in Section V to compare the TS-Router with the classic iSLIP allocator [20] and the most recent allocation algorithm [21]. Finally, related works are discussed in Section VI, and the conclusions and future work are given in Section VII. II. Preliminaries In this section, we give a brief introduction on the preliminaries on allocation in NoC architecture. A. Allocation and Matching An allocation or a matching in a router is to pair the input ports (in-ports) and output ports (out-ports). To avoid conﬂicts, one input port can only be paired with one output port, and vice versa. A maximal matching means that while there is an existing matching, it is impossible to add more pair(s) to the existing matching. On the contrary, a maximum matching means that it is the largest matching in terms of the number of paired input-to-output ports. Note that a maximum matching is certainly a maximal matching. However, a maximal matching is not necessarily a maximum matching. 2 B. Representation of Allocation We use a request matrix to show requesting-requested relationships between input ports and output ports in the stage of Switch Allocation (SA). Each row stands for an input port and the cells in a row are the requested output ports. Each column stands for an output port and the cells in a column are the input ports that are requesting for this output port. White circles stands for the requests. A V × U request matrix can be represented as a bipartite graph denoted as G = (V ; U ; E ), and vice versa. Once an allocation is done, the resultant matrix has the following properties: (1) Each row can only contains one white circle, that is, each input port can request at most one output port at a time. (2) Each column can contain only one white circle, that is, each output port can only be requested by at most an input port at a time. A request matrix can also be depicted as a bipartite graph G = (V ; U ; E ) but each element in V can only have one outgoing link to U , and vice versa. The corresponding bipartite graph of a request matrix forms a matching. 2 In the following paragraphs, if a matching is maximal and maximum simultaneously, we denote such matchings as max matchings for short. Fig. 1: An overview of different designs. (a) The initial works for maximizng the number of matchings. (b) History-based solution, which uses previous allocation(s) to improve current allocation. (c) TS-Router predicts the future requests to improve the current allocation. tween input port requests and output port resources. With such a perspective, the allocation decision made at each cycle should consider not only past decisions but also future requests. In the next section, we will show an example to illustrate that maximizing the number of allocations in a single cycle is not enough, since current allocation will affect future allocations. If the requests in the following cycles can be predicted, the allocation decision made in the current cycle can be adjusted accordingly, which results in more concurrent connections in consecutive allocations. Figure 1 illustrates an overview of previous works (maximizing matches in a single cycle and history-based) and our design. Based on this observation, a novel router design, TS-Router, is proposed. TS-Router predicts future requests to arrive at a router and tries to maximize the matching across cycles. In this paper, a forwarding design in the router architecture for maximizing the current and future allocations is proposed, in which possible conﬂicts in the future are predicted and resolved, resulting in more concurrent connections along time. In contrast to naive prediction, the forwarding datapath directly forwards the requests from virtual-channel allocation stage to switch allocation stage, which results in accurate prediction. To summarize, the main contributions of this paper are as follows: • To the best of our knowledge, this is the ﬁrst work to propose the concept of time-series matchings, which is in contrast to maximal and maximum matching. • A novel router design, TS-Router, is proposed, which realIII. Motivating Examples A. The General Router Architecture ′ In the following examples, we discuss the inﬂuences of max matching and show the cases that max matchings do not lead to the best results. We assume the pipelined router architecture as in [21]. For simplifying the discussion, we assume a 5-port router and the number of virtual channel is set to 2. In this example, we use request matrix to represent the relationship between input ports and output ports. We use white circles to state the requests in the stage of Switch Allocation (SA), and black dot is specially for representing the future requests of SA stage. The gray cells stands for the previous grants in last allocation, which is used by history-based strategy, such as in [21], [2]. Figure 2a shows two possible solutions denoted as i and i . Both of the two solutions achieve max matchings simultaneously (3 in this case). Then, Figure 2b shows the following allocation. ′ Previously unallocated requests are left in (i + 1) and (i + 1) , respectively. Moreover, the future request, depicted in Figure 2a with the black dot, becomes a current request in Figure 2b. Note ′ that both of the allocation (i + 1) and (i + 1) are max matchings ′ (2 and 1, respectively). However, allocation (i + 1) results in an input port conﬂict and therefore the latter max matching is worse than the former. This example shows that in two back-to-back allocations, the current allocation may affect the next allocation. If we only consider each single allocation, both of the two allocations achieve max matchings and the drawback cannot be found. Note that this example only shows the case of column conﬂicts. The same idea can be applied to row conﬂicts. Next, with the gray cells, we are able to observe the allocations made by history-based strategies [21], [2]. It shows that history-based strategies provide higher priorities to the previous grants. However, in this example, it would select the allocation ′ based on the previous grants, and it eliminates the possibility of concurrent transmission since it does not take the future requests into consideration. To conclude, history-based strategies are more conservative and tend to reserve the previous connections to lower the risks of ports being idle. In contrast, TS-Router takes the current allocation and incoming allocation into consideration and explores more possibilities of parallel connections. In other words, history-based strategies are lookbehind strategies which uses history of grants to improve the QoA. In contrast, TS-Router not only considers the history, but also contains a look-ahead strategy which uses the forwarding messages to predict the incoming matchings. (i + 1) IV. Design Concept of TS-Router In the following discussion, we ﬁrst give the router architecture to use. Second, we give the formulation of matching maximization for current-next allocations. Third, we give the detailed operations of an allocation with the future requests. An illustration is given for demonstrating the operations step-by-step. Finally, we discuss the modiﬁcations in the datapath to support the prediction of future requests. We use a general router architecture mentioned in [1], [24] as our baseline router architecture. The router has ﬁve pipeline stages: Routing Computation (RC), Virtual-channel Allocation (VA), Switch Allocation (SA), Switch Traversal (ST), and Link Traversal (LT). Each router has multiple virtual channels per input ports and VC ﬂow control is used [5]. Note that the reason that using a very general router architecture rather than optimized ones is that our idea of predicting the future requests can be realized by adding a few additional links and simple logics, which is orthogonal to the most modern designs. Therefore, using a general router architecture helps understand the spirit of our idea. However, the idea can be integrated into most modern router architectures as long as the router architecture involves VA and SA stages. B. Time-Series Switching To design a time-series-considered router, while doing a decision of allocation, the impact of the decision needs to be calculated by estimating the proﬁt brought by this allocation. In a single router, for the Current-Next allocations, the Matching Quality (MQ) is deﬁned as the resulting Numbers of Matchings (NoM) with a prospecting vision v > 0. The goal of MQ is as follows with a given v : ∑ max MQ(allocationi ) = max NoMi i<i+v A larger v can be set for more aggressive design. Theoretically v → ∞ gives the upper bound of MQ. However, setting v → ∞ is not practical in real cases since the incoming trafﬁc patterns are always unknown until the runtime. In TS-Router, v is set to 1, which means that when doing the i-th allocation, it also takes the next (i + 1)-th allocation into consideration. This prediction is performed inside the router by forwarding, so the accuracy is relatively high. However, it is still possible that when doing i-th allocation, more than one following allocations are considered, i.e., (i+v)-th allocations, where v > 1. To do the more aggressive prediction, the information required by the prediction may not only come from the local router, but also the other neighbor routers. Nevertheless, the accuracy will be relative low since the prediction is made across the routers. Therefore, we focus on conservative prediction in this paper rather than aggressively prediction across the routers. Figure 3 shows the datapath of TS-Router. A general separable arbitrators design can be applied. A forwarding link from VA to SA is for predicting the participants in the next allocation. C. Priority Propagation in Priority Matrix To represent the priority when conﬂict occurs in inport arbitration or outport arbitration, we use priority matrix which is similar to the request matrix mentioned in Section II-B. Differently, request matrix is a binary matrix, i.e., each entry in a cell is either 0 or 1, but an entry in a priority matrix is a value indicating the priority when conﬂict occurs. (a) The i-th request matrix (b) The (i + 1)-th request matrix Fig. 2: The white circles are the current requests and the black dot is the future request in the stage of Switch Allocation (SA). The gray cells stand for the prioritized cells, which is used by history-based strategy, such as in [21], [2]. In this case, the gray cells are ′ previously granted requests. Note that: (a) both allocations i and i achieve max matchings (3 in this case). (b) In the next allocation, ′ although allocation (i + 1) and (i + 1) also are both max matchings (2 and 1, respectively), the latter has input port conﬂicts, so the max matching is worse than the former. Fig. 3: Datapath of TS-Router. Dedicated links from the downstream routers provides the feedback information and stored in the VA/SA latch. One forwarding link is from the pending request in the VA stage for predicting the participants in the next allocation. An predicted request, represented by a black dot in the priority matrix, means that the occupied cell (standing for an inport and an outport) has a request in the next allocation, so it propagates the priorities to the requests in the same column and the same row except itself based on the following observations: 1) Clean the competitor(s): to avoid them occupying the inport or the outport in the next allocation. 2) Improve parallelism: Once the competitor(s) is cleared, the request(s) which is in the same row or the same column has higher probability to be transmitted with the predicted one in the next allocation. Speciﬁcally, assuming that the predicted request is located at (m; n), the cells in the m-th row and in the n-th column except (m; n) itself are prioritized. For simplifying the explanation, we use an illustration to help understand the process. As Figure 4a shows, an predicted request is located at (2; 2). According to our algorithm, the second row and the second column are prioritized except (2; 2) itself. Therefore, the inport conﬂict between (4; 1) and (4; 2) is resolved by letting (4; 2) win. Similarly, (2; 3) wins the outport arbitration when conﬂicting with (3; 3). Figure 4b shows the remaining requests, i.e., the new request (2,2) and the losers in the previous arbitration. Apparently, the remaining three requests do not have conﬂict because time-series switching has resolved the possible conﬂicts in the previous allocation, and therefore they can be transmitted in parallel in the next allocation. Note that the values in the priority matrix can be accumulated in one switch arbitration. Figure 5 shows an sample priority matrix which has two predicted requests, in which the overlapped prioritized cells have higher priorities and the values are accumulated to 2. Note also that the accumulations only occur in one switch allocation rather than accumulated along time. That is, the priority matrix is refreshed to 0s every switch arbitration. The request matrix can be implemented with several existing methods, such as Tree Arbiter, Matrix Arbiter, and so on [23], to reach the maximum frequency without violating timing constraint. Currently, the slowest path involves selecting the highest priority input port without causing conﬂicts on output port grants. This selection is an iterative process involving a sequential logic path. Each iteration checks for the i-th highest priority and grants the input if the output is not already granted. The list of output ports already granted is used when selecting the (i− 1)-th highest priority input port. To boost the speed, we can check grants of multiple input ports in parallel to speed up selection. To do this, each input port must also compare the priorities of other input ports, which adds additional comparators to the design. In Figure 6, the priority matrix is calculated and then the arbitration decision needs to be done based on the priority matrix. Take the arbitration of output port 4 as an example, input port 1, 2 and 3 are involved in arbitration. Note that the maximum priority is 6 in this example. First, they compare their priority values with 6,5,4,2 and 1 in parallel. Then, these compared results are sent to Conﬂict Solver, which can process conﬂicts in a round-robin manner when two or more request have the same priorities. After conﬂicts are solved, these results are treated as control signals to Selector, which chooses the request with highest priority. However, we only implement the selection process in a sequential way currently. Based on our synthesis results, our router can be run up to 333 MHz and the baseline up to 500 MHz. This results in a 33% slower router. However, this is the maximum slowdown since we can optimize the design to be more parallel at the cost of extra area. Alternatively, the iterative process can be split by dividing the switch allocation into two pipeline stages. With a completely iterative process, the design requires additional 867 gates per router than the baseline router with an absolute additional area of 61 × 61(cid:22)m2 . Besides, considering the impacts of power consumption due to NoC [28], [27], the power consumption is also compared using the design compiler tool. We re-synthesized the baseline router to operate at 333 MHz to compare the power consumption. The result shows that our design consumes additional 0.325 mW per router. V. Evaluation We use GEM5, a full-system simulator [4], with enabled Ruby and Garnet to model the system [17], [1], including detailed memory model and interconnection network. The evaluation is threefold. First, we focus on the network performance by switch GEM5 to network only mode, in which processing elements (PE) act as trafﬁc injectors, and we can synthesize several conventional trafﬁc patterns to examine the performance limitations of the network. Second, we focus on the effectiveness of timeseries consideration. We use moving average to observe the effectiveness by comparing TS-Router with Packet Chaining. Finally, we switch GEM5 to full-system mode for running the real benchmarks. The default settings are shown in Table I. If the settings are modiﬁed in a speciﬁc experiment, they will be mentioned in the paragraphs for avoiding confusion. Fig. 4: (a) Gray cells are prioritized by the predicted request (black dot). As a result, (2,3) and (4,2) have higher priorities when conﬂicting. (b) Remaining requests can be transmitted in parallel in the next allocation. Fig. 5: A sample of priority matrix with multiple predicted requests. [7]. In the following subsection, we implement the TS-Router with Verilog to investigate the hardware considerations. D. Router Implementation To investigate the overheads in terms of several considerations, we perform estimations on the power, area, and critical path latency of our router architecture using Synopsys Design Compiler. Behavioral RTL is synthesized using ST micron’s 65nm design libraries. We choose the nominal library with 1.20V core voltage. The design of our baseline is a simplistic design involving X-Y routing and only 4 stages: route computation, switch allocation, crossbar transversal, and link transversal. The stage of virtual-channel allocation is removed for simplifying the following analysis. Note that this simple baseline router may result in larger area overhead percentage when applying the logics for realizing time-series switching. However, it will be relatively small when applying the logics to other modern routers [18], [19]. a) Area overhead of priority matrix.: Before entering the RTL-based analysis, we calculate the area overhead of priority matrix to give a formal estimation. Assuming that the size of the crossbar is N × N , then each cell in the priority matrix has N − 1 neighbors in a row, and similarly N − 1 neighbors in ranges from [0; 2N − 2], and the required bits to store all the a column. Therefore, the accumulated priority value for a cell values are N 2 ⌈log2 (2N − 2)⌉ bits. Note that since the entries in priority matrix are cleared every switch arbitration, values in cells will not be accumulated along time, so the analysis above guarantees that values will not overﬂow. b) Critical path, area overhead, and power consumption.: Next, we estimate the critical path by synthesizing our designs Fig. 7: The latency comparison under tornado trafﬁc with different injection rates. Fig. 8: The latency comparison under bit-complement trafﬁc with different injection rates. section V-B1. As the injecting rate increases, the network latency increases exponentially due to the network saturation. Therefore, it is difﬁcult to observe that TS-Router outperform the other two allocators in low injection rates. We zoomed in Figure 9a before the injection rate achieves the saturation point (IR ≤ 0:33) under uniform trafﬁc pattern. As depicted in Figure 9b, it shows that TS-Router has lower latency when the network is not saturated. The same trends can be found in all the synthetic workloads when the injection rate is low. This property is important since most parallel programs run with the injection rate before saturation point according to [12], [25], and therefore achieving low latency before the saturation point is quite critical for most programs. B. Evaluation for Time-Series Switching 1) Number of Matchings: In the following two experiments, we investigate the effectiveness of time-series consideration. We use three different synthetic trafﬁc distribution with different injection rates to see the total resulted numbers of matchings by iSLIP, Packet Chaining, and TS-Router, respectively. As Figure 10 and Figure 11 show, the X-axis is the injection Fig. 6: Priority-based arbitration microarchitecture. TABLE I: Default simulation setup Simulator settings Processor family Frequency Number of cores Cache protocol NoC topology Average Packet size Number of VCs Input buffer size Routing algorithm ALPHA ISA 2 GHZ 64 MESI protocol 8-by-8 2D mesh network 6 ﬂits 4 5 ﬂits Dimension-order A. Evaluation for Network Performance In this subsection, we investigate the synthetic trafﬁc patterns with different injection rates to observe the performances of network latencies by iSLIP, Packet Chaining, and TS-Router, respectively. Note that in the following experiments we implement the second type of Packet Chaining (Same port, different VCs) since it has been shown that it strikes the best tradeoff between implementation overhead and performance gain [21]. c) Network latency of synthetic trafﬁc patterns.: Under tornado and bit-complement trafﬁc, TS-Router can have the better performance than the other two allocators. This is because that a high queueing latency may be incurred in Packet Chaining due to the reservation of the previous allocation. This phenomenon is due to starvation which is discussed in Section V-B3. In Figure 7 and Figure 8, TS-Router has a 76% lower average latencies compared to the other two allocators at the saturation point. Figure 9a shows the network latencies under uniform distribution trafﬁc. TS-Router outperforms the other two allocators except while the injection rate is between 0.5 and 0.7 due to too many future requests involved, which will be further discussed in the (a) Injection rates with saturation (b) Injection rates without saturation (Zoom-in) Fig. 9: The latency comparison under uniform trafﬁc with different injection rates. Fig. 10: The improved numbers of matchings by Packet Chaining and TS-Router under tornado distribution trafﬁc. rate (ﬂits/cycle), and the Y-axis is the difference of matching numbers, and the results show that TS-Router outperforms Packet Chaining. In addition, the total number of allocation of Packet Chaining is worse than the baseline (iSLIP-1). This is because the trafﬁc pattern of bit-complement is relatively more stable than the other two synthetic trafﬁc, and the existing allocation is always prioritized than the newcomers. The resulted network behaves as a pseudo circuit-switch network and starve the newcomers, as we observed in the network latency. Further discussion is in Section V-B3. However, Packet Chaining can take advantages of inheriting the pervious allocation while the injection is larger than 0.4 in uniform trafﬁc, as illustrated in Figure 12. We observe that the allocation of TS-Router may behave similarly as iSLIP while the network becomes congested in uniform pattern. This is because that the priorities of the requests are almost the same since too many future requests, which are generated in uniform pattern with high injection rate, participate in the priority propagation. Nevertheless, it cannot happen in tornado and bit-complement trafﬁc since the requests can be only generated in some inputs and outputs in these two trafﬁc patterns. 2) Analysis of time series: To further analyze the effectiveness of time-series consideration, we focus on the saturation point Fig. 11: The improved numbers of matchings by Packet Chaining and TS-Router under bit-complement distribution trafﬁc. (IR=0.4) and use moving average, which is a common technique in statistics for analyzing the trend in time series. We compared TS-Router with Packet Chaining since both of them are based on the observation that consecutive allocations affect each other. For the readability, we sampled 500 cycles of the router in the center of the network. However, the following observation can also be found when sampling other routers with longer period. In each cycle the number of matchings is recorded, denoted as mpc and mts for Packet Chaining and TS-Router, respectively. matching number, i.e., mts − mpc , and the X-axis is the cycles. As Figure 13 shows, the Y-axis is the difference between the The light gray lines shows the ﬂuctuation and it is hard to observe the trend. However, by applying moving average with period 10 (the black lines), the effects of time-series can be easily observed. As Figure 13 shows, the black lines are above the X-axis and thus positive at most time, which means considering the timeseries effect, TS-Router outperforms Packet Chaining. With larger period, the effects are more obvious. Similar trends are found under tornado and bit-complement trafﬁc distribution, as shown Fig. 12: The improved numbers of matchings by Packet Chaining and TS-Router under uniform distribution trafﬁc. Fig. 14: The moving average for the difference of matching numbers between Packet Chaining and TS-Router under tornado trafﬁc. Fig. 13: The moving average for the difference of matching numbers between Packet Chaining and TS-Router under uniform distribution. in Figure 14 and Figure 15. The above two experiments conclude that TS-Router’s lookahead strategy is more effective than Packet Chaining’s lookbehind strategy. This is because look-behind is empirical inferring from past allocation to current allocation. In contrast, TS-Router accurately predicts the requestors in the following allocations and uses the future requestors as the clues to do the priorities assignment in current allocation, which is intrinsically timeseries-considered rather than empirical inferring. 3) Starvation effects.: In this experiment, we compare the starvation effect when applying Packet Chaining and TS-Router. Since Packet Chaining is based on the assumption that the previous allocation is similar to the current allocation, the existing granted requests are prioritized to avoid joining the current allocation, which leads to the possibility of starving the new requests. For evaluating this affection, we compare Packet Chaining and TS-Router by increasing the packet length under the bitcomplement trafﬁc. As Figure 16 shows, when the packet length is between 1 and 3, Packet Chaining and TS-Router have the same performance in terms of the total number of matchings. When the packet length is between 4 and 5, TS-Router outperforms Packet Fig. 15: The moving average for the difference of matching numbers between Packet Chaining and TS-Router under bitcomplement trafﬁc. Chaining because the former leads to larger network capacity. Note that the network is getting saturated when the packet length is larger than 5. Unfortunately, when the packet length continues to increase, Packet Chaining inherits the previous allocation and the hit rate is very high due to the longer packet length. It makes the network behaves as a pseudo circuit-switch network, and the inherited allocations starve the new requests, which lead to exponential queuing delay in the network. Although the starvation effect can be avoided by setting a predeﬁned value to cut the chain, i.e., limit the maximum number of chained ﬂits, to avoid the starvation effect. However, it is related to the trafﬁc behavior and therefore ﬁnding a suitable value for various trafﬁc is not even possible. Nevertheless, TS-Router relies on the predicted allocation rather than the existing ones, and therefore it does not starve the new requests. The priority is assigned according to whether or not more parallel connections can be granted. To conclude, TS-Router is more general and independent of the packet length, which leads to a starvation-free network. C. Benchmark Evaluation In this experiment, we conﬁgure GEM5 as a ALPHA CMP with 64 CPUs which is connected by an 8 × 8 mesh network. advantages of the most recent allocation based on the observation that the last allocation tends to be similar to the current allocation [21], [2], so the last allocation is kept which results a circuit-switch-like behavior. The experiment results show that the performance can outperform Wavefront and Augmenting Path allocators due to the implicit time-series consideration. TS-Router shares similar concept, i.e., consecutive allocations, but with the explicit time-series consideration in the allocation algorithm. Instead of inheriting the past allocation which beneﬁts the existing connections, TS-Router predicts the following requests for exploring more parallel connections between current allocation and next allocation. Due to the native time-series consideration, TS-Router outperforms these history-based allocators. Other research works have explored the related problem with different points of views, such as running pipeline stage in parallel by speculation [24], [23], bypassing pipeline stages or having express channel for prioritized packets [15], [19], [18], and speed up the routing latency by looking-ahead design [10]. Most of these works are orthogonal to TS-Router. In other words, the concept of TS-Router can be implemented in these state-of-the-art routers and the existing advantages can be kept simultaneously. VII. Conclusion and Future Works In this paper, we ﬁrst summarize the state-of-the-art works and TS-Router with our time-series model, which includes the past allocation, current allocation, as well as next allocation. Next, we propose TS-Router, a time-series effects considered router, which leverages the forwarding information from the previous pipeline stage to make a foresighted arbitration. By the foresighted arbitrations, the numbers of parallel connections (pairs of inputs and outputs) of the routers can be increased and also the whole system performance. In the future work, we will apply TS-Router to many-core accelerator and hybrid design, such as the interconnection for GPU and Multi-Processor System-on-Chip (MPSoC). With more various processing elements interconnected by NoC, more factors are necessary to be explored, such as the impact of different trafﬁc patterns, the different topologies, and the performance of highradix TS-Router when being the communication fabric for the large-scale chip multiprocessors [8], [14], [22]. VIII. Acknowledgements This work was supported in part by NSF grants 1205618, 1213052, 1147388, 0916887, 0905365 and 0903432 as well as Industrial Technology Research Institute and National Science Council grant NSC 101-2220-E-007-025. "
2013,Application-to-core mapping policies to reduce memory system interference in multi-core systems.,"Future many-core processors are likely to concurrently execute a large number of diverse applications. How these applications are mapped to cores largely determines the interference between these applications in critical shared hardware resources. This paper proposes new application-to-core mapping policies to improve system performance by reducing inter-application interference in the on-chip network and memory controllers. The major new ideas of our policies are to: 1) map network-latency-sensitive applications to separate parts of the network from network-bandwidth-intensive applications such that the former can make fast progress without heavy interference from the latter, 2) map those applications that benefit more from being closer to the memory controllers close to these resources. Our evaluations show that, averaged over 128 multiprogrammed workloads of 35 different benchmarks running on a 64-core system, our final application-to-core mapping policy improves system throughput by 16.7% over a state-of-the-art baseline, while also reducing system unfairness by 22.4% and average interconnect power consumption by 52.3%.","Application-to-Core Mapping Policies to Reduce Memory System Interference in Multi-Core Systems Reetuparna Das∗ Rachata Ausavarungnirun† Onur Mutlu† Akhilesh Kumar‡ Mani Azimi‡ University of Michigan∗ Carnegie Mellon University† Intel Labs‡ Abstract Future many-core processors are likely to concurrently execute a large number of diverse applications. How these applications are mapped to cores largely determines the interference between these applications in critical shared hardware resources. This paper proposes new application-to-core mapping policies to improve system performance by reducing interapplication interference in the on-chip network and memory controllers. The major new ideas of our policies are to: 1) map network-latency-sensitive applications to separate parts of the network from network-bandwidth-intensive applications such that the former can make fast progress without heavy interference from the latter, 2) map those applications that beneﬁt more from being closer to the memory controllers close to these resources. Our evaluations show that, averaged over 128 multiprogrammed workloads of 35 different benchmarks running on a 64-core system, our ﬁnal application-to-core mapping policy improves system throughput by 16.7% over a state-of-the-art baseline, while also reducing system unfairness by 22.4% and average interconnect power consumption by 52.3%. 1. Introduction One important use of multi-core systems is to concurrently run many diverse applications. Managing critical shared resources, such as the on-chip network (NoC), among co-scheduled applications is a fundamental challenge. In a large many-core processor, which core is selected to execute an application could have a signiﬁcant impact on system performance because it affects contention and interference among applications. Performance of an application critically depends on how its network packets interfere with other applications’ packets in the interconnect and memory, and how far away it is executing from shared resources such as memory controllers. Which core an application is mapped to in a NoC-based multi-core system signiﬁcantly affects both the interference patterns between applications as well as applications’ distance from the memory controllers. Hence, the application-to-core mapping policy can have a signiﬁcant impact on both per-application performance and system performance, as we will empirically demonstrate in this paper. While prior research (e.g., [21, 29, 33]) tackled the problem of how to map tasks/threads within an application, the interference behavior between applications in the NoC is less well understood. Current operating systems are unaware of the onchip interconnect topology and application interference characteristics at any instant of time, and employ naive methods while mapping applications to cores. For instance, on a non-NUMA system that runs Linux 2.6.x [1], the system assigns a static numbering to cores and chooses the numerically-smallest core when allocating an idle core to an application.1 This leads to an application-to-core mapping that is oblivious to application characteristics and inter-application interference, causing two major problems that we aim to solve in this paper. First, overall performance degrades when applications that interfere signiﬁcantly with each other in the shared resources get mapped to closeby cores. Second, an application may beneﬁt signiﬁcantly from being mapped to a core that is close to a shared resource (e.g., a memory controller), yet it can be mapped far away from that resource (while another application that does not beneﬁt from being close to the resource is mapped closeby the resource), reducing system performance. To solve these two problems, in this work, we develop intelligent application-to-core mapping policies that are aware of application characteristics and on-chip interconnect topology. Our new application-to-core mapping policies are built upon two major observations. First, we observe some applications are more sensitive to interference than others: when interfered with, network-sensitive applications slow down more signiﬁcantly than others [12]. Thus, system performance can be improved by separating (i.e., mapping far away) networksensitive applications from aggressive applications that have high demand for network bandwidth. To allow this separation of applications, we partition the cores into clusters such that the cores in a cluster predominantly access the memory controller(s) in the same cluster, develop heuristics to estimate each application’s network sensitivity, and devise algorithms that use these estimates to distribute applications to clusters. While partitioning applications among clusters to reduce interference, our algorithms also try to balance the network load among clusters as much as possible. Second, we observe that an application that is both memoryintensive and network-sensitive gains more performance from being close to a memory controller than one that does not have either of the properties (as the former needs fast, highbandwidth memory access). Thus, system performance can be improved by mapping such applications to cores close to memory controllers. To this end, we develop heuristics to identify such applications dynamically and devise a new algorithm that maps applications to cores within each cluster based on each application’s performance sensitivity to distance from the memory controller. We make the following new contributions in this paper: • We develop a new core assignment algorithm for applications in a NoC-based many-core system, which aims to minimize destructive inter-application network interference and thereby maximize overall system performance. 1 The Linux kernels with NUMA extensions use a mapping similar to our CLUSTER+RND scheme that we discuss and compare to in Section 6.1. • We develop new insights on inter-application interference in NoC based systems, which form the foundation of our algorithm. In particular, we demonstrate that 1) mapping network-sensitive applications to parts of the network such that they do not receive signiﬁcant interference from network-intensive applications and 2) mapping memoryintensive and network-sensitive applications close to the memory controller can signiﬁcantly improve performance. We show that sometimes reducing network load balance to isolate network-sensitive applications can be beneﬁcial. • We demonstrate that intelligent application-to-core mappings can save network energy signiﬁcantly. By separating applications into clusters and placing memory-intensive applications closer to the memory controllers, our techniques reduce the communication distance and hence communication energy of applications that are responsible for the highest fraction of overall network load. • We extensively evaluate the proposed application-to-core mapping policies on a 60-core CMP with an 8x8 mesh NoC using a suite of 35 diverse applications. Averaged over 128 randomly generated multiprogrammed workloads, our ﬁnal proposed policy improves system throughput by 16.7% over a state-of-the-art baseline, while also reducing application-level unfairness by 22.4% and NoC power consumption by 52.3%. 2. Motivation and Overview A many-core processor with n cores can run n concurrent applications. Each of these applications can be mapped to any of n cores. Thus, there can be n! possible mappings. From the interconnect perspective, an application-to-core mapping can determine the degree of interference of an application with other applications in the NoC as well as how well the overall network load is balanced. The application-to-core mapping also determines how the application’s memory accesses are distributed among the memory controllers. These factors can lead to signiﬁcant variation in performance. For example, Figure 1 shows the system performance for 576 different application-to-core mappings for the same workload (detailed system conﬁguration is given in Section 5). The workload consists of 10 copies each of applications gcc, barnes, soplex, lbm, milc and leslie running together and an application’s memory accesses are mapped to the nearest memory controller. The Y-axis shows the system performance in terms of normalized weighted speedup of the different mappings (higher is better). Each data point represents a different mapping. It can be seen that the best possible mapping provides 24% higher system performance than the worst possible mapping. To understand the behavior of these mappings, we systematically analyze the characteristics of the best performing mappings across a wide range of workloads. Figure 2 illustrates the four general categories of mappings on our baseline tiled manycore architecture. Figure 2(a) shows the logical layout of a manycore processor with cores organized in an 8x8 mesh onchip network. The memory controllers (triangles) are located in corner tiles (we also evaluate other memory controller placeFigure 1: Performance of 576 different application to core mappings for one multiprogrammed workload (8x8 system) ments in Section 6.7). All other tiles consist of a core, L1 cache and L2 cache bank. Further, we divide the 8x8 mesh into four 4x4 clusters (marked by dotted lines). An application can be mapped to any of the 60 cores. We assume that integrated software and hardware techniques (discussed in Section 3.1) allow us to restrict most of an application’s on-chip network trafﬁc to the cluster where it is running. Figure 2(b), (c), (d), (e) show four possible application to core mappings. Each core tile in the ﬁgure is shaded according to the network intensity of the application running on it; a darker tile corresponds to an application with a higher L1 Misses per Thousand Instructions (MPKI), running on the core in the tile. Figure 2 (b) shows a possible random mapping of applications to clusters (called RND), which is the baseline mapping policy in existing general-purpose systems. Unfortunately, a random mapping does not take into account the balance of network and memory load across clusters, and as a result leads to high contention for available bandwidth and low system performance. Figure 2 (c) shows an example where applications are completely imbalanced between clusters (called IMBL). The upper left cluster is heavily loaded, or ﬁlled with application with high MPKI, while the lower right cluster has very low load. An imbalanced inter-cluster mapping can severely degrade system performance because of poor utilization of aggregate available bandwidth in the NoC and in the off-chip memory channels. Such an undesirable imbalanced mapping is possible in existing systems. Hence we need to develop policies to prevent such scenarios. Figure 2 (d) illustrates an example of a balanced mapping (called BL) which equally divides the load among different clusters. This mapping can achieve better performance due to effective utilization of network and memory channel bandwidth. However, we ﬁnd that a balanced mapping is not the best performing mapping. We observe that some applications are more sensitive to interference in the network compared to other applications. In other words, these applications experience greater slowdowns than other applications from the same amount of interference in the shared network. Thus, system performance can be improved by separating (i.e., mapping far away) network-sensitive applications from aggressive applications that have high demand for network bandwidth. Figure 2 (e) illustrates our balanced mapping with reduced interference (called BLRI). The BLRI mapping attempts to pro(a) (b) (c) (d) (e) (f) Figure 2: (a) A manycore processor organized in an 8x8 mesh network-on-chip divided into four clusters. Each cluster has a memory controller tile and core tiles. Memory controllers (triangle) are placed in corner tiles (alternative memory controller placement is explored in Section 6.7).The core tiles have a processor core, L1 cache and L2 cache. Different possible mappings: (b) Random (RND) mapping (c) Imbalanced (IMBL) (d) Balanced (BL) (e) Balanced with Reduce Interference (BLRI). Darker shades indicate applications that have higher network intensity (or injection rate). Three example mappings within a cluster are shown in (f): (1) Random (2) Radial (3) Inverse Radial. tect interference-sensitive applications from other applications by assigning them to their own cluster (the top left cluster in Figure 2(e)) while trying to preserve load balance in the rest of the network as much as possible. BLRI can improve performance signiﬁcantly by keeping the slowdowns of the most latency-sensitive applications under control. After mapping applications to different clusters, a question remains: which core within a cluster should an application be mapped to? Figure 2 (f) shows three possible intra-cluster mappings for a single cluster. Figure 2 (f) (1) depicts a random intra-cluster mapping; this is not the best intra-cluster mapping as it is agnostic to application characteristics. The applications placed closer to memory controller enjoy higher throughput and lower latency, as they have to travel shorter distances. Thus, it is beneﬁcial to place network-intensive applications (which beneﬁt more from higher throughput) and networksensitive application (which beneﬁt from lower latency) close to memory controller. Figure 2 (f) (2) shows an example mapping within a cluster where applications are placed radially in concentric circles around the memory controller. Darker (inner and closer) tiles represent network-intensive and interferencesensitive applications and lighter (outer and farther) tiles represent lower intensity applications with low sensitivity. Figure 2 (f) (3) shows an example of an opposite policy which we will evaluate, referred to as inverse-radial. 3. Mapping Policies In this section we develop policies for efﬁcient applicationto-core mappings. First, cores are clustered into subnetworks to reduce interference between applications mapped to different clusters (Section 3.1). Second, applications are distributed between clusters such that overall network load is balanced between clusters (Section 3.2.1), while separating the interference-sensitive applications from interferenceinsensitive ones (Section 3.2.2). Finally, after applications have been distributed between clusters, intra-cluster radial mapping (Figure 2 (f) (2)) is used to map applications to cores within a cluster such that applications that beneﬁt most from being close to the memory controller are placed closest to the memory controller (Section 3.3). 3.1. Cluster Formation What is a cluster? We deﬁne a cluster as a sub-network such that the majority of network trafﬁc originating in the subnetwork can be constrained within the sub-network. In our policies, cores are clustered into subnetworks to reduce interference between applications mapped to different clusters. Applications mapped to a cluster predominantly access the memory controller within that cluster (and share the L2 cache slices within that cluster). Clustering not only reduces interference between applications mapped to different clusters, but also 1) reduces overall congestion in the network, 2) reduces average distance packets traverse to get to the memory controllers or shared caches. How to enforce clustering for memory accesses? Clustering can be achieved by mapping physical pages requested by cores to memory controllers in an appropriate manner. Typically, physical pages (or even cache blocks) are interleaved among memory controllers such that adjacent pages (or cache blocks) are mapped to different memory controllers [43, 36, 26, 25, 34]. To enable clustering, page allocation and replacement policies should be modiﬁed such that data requested by a core is opportunistically mapped to the home MC of the core. To achieve this, we slightly modify the commonly-used CLOCK [22] page allocation and replacement algorithm to what we call the cluster-CLOCK algorithm. When a page fault occurs and free pages exist, the operating system gives preference to free pages belonging to the home MC of a requesting core when allocating the new page to the requesting core. If no free pages belonging to the home MC exist, a free page from another MC is allocated. When a page fault occurs and no free page exists, preference is given to a page belonging to the home MC, while ﬁnding the replacement page candidate. We look N pages beyond the default candidate found by CLOCK [22] algorithm to ﬁnd a page that belongs to home MC.2 If unsuccessful in ﬁnding a replacement candidate belonging to home MC when N pages beyond the default candidate, the algorithm simply selects the default candidate for replacement. The above modiﬁcations ensure that the new page replacement policy does not signiﬁcantly perturb the existing replacement order, and at the same time opportunistically achieves the effect of clustering. Note that these modiﬁcations to virtual memory management (for both page allocation and page replacement) do not enforce a static partitioning of DRAM memory capacity; they only bias the page replacement policy such that it likely allocates pages to a core from the core’s home memory controller. 2We use N = 512 in our experiments, a value empirically determined to maximize the possibility of ﬁnding a page in home MC while minimizing the overhead of searching for one. How to enforce clustering for cache accesses? Clustering can be enforced for cache accesses for shared cache architectures by slightly modifying state-of-the-art cooperative caching techniques [7, 38] or page coloring techniques [9] such that cache accesses of a core remain in the shared cache slices within the core’s cluster. Such caching techniques have been shown to improve system performance by trading-off cache capacity for communication locality. For most of our evaluations, we use private caches that automatically enforce clustering by restricting the cache accesses from an application to the private cache co-located in the same tile as the core running the application. For completeness, we evaluate our techniques on a shared cache architecture in Section 6.7, in which clustering is achieved by implementing Dynamic Spill Receive (DSR) caches [38]. 3.2. Mapping Policy between Clusters In this subsection, we devise clustering algorithms that decide to which cluster an application should be mapped to. The key idea is to map the network-sensitive applications to a separate cluster such that they suffer minimal interference. While doing so, we also try to ensure that overall network load is balanced between clusters as much as possible. 3.2.1. Balanced Mapping (BL) The goal of the Balanced (BL) policy (an example of which is shown in Figure 2 (d)) is to balance load between clusters, such that there is better overall utilization of network and memory bandwidth. For BL mapping, we start with the list of applications sorted with respect to their network intensity. Network intensity is measured in terms of injection rate into the network, which is quantiﬁed by private L2 cache MPKI, collected periodically at run-time or provided statically before the application starts. The BL mapping is achieved by mapping consecutively-ranked applications to different clusters in a round robin fashion. 3.2.2. Balanced with Reduced Interference Mapping (BLRI) The goal of the Balanced with Reduced Interference (BLRI) policy (an example of which is shown in Figure 2 (e)) is to protect interference-sensitive applications from other applications by assigning them their own cluster (the top left cluster in the ﬁgure) while trying to balance load in the rest of the network as much as possible. To achieve this, our mechanism: 1) identiﬁes sensitive applications 2) maps the sensitive applications to their own cluster only if there are enough such applications and the overall load in the network is high enough to cause congestion. How to Identify Sensitive Applications We characterize applications to investigate their relative sensitivity. Our studies show that interference-sensitive applications have two main characteristics. First, they have low memory level parallelism (MLP) [16, 35]: such applications are in general more sensitive to interference because any delay for the application’s network packet is likely cause extra stalls, as there is little or no overlap of packet latencies. Second, they inject enough load into the network for the network interference to make a difference in their execution time. In other words, applications with very low network intensity are not sensitive because their performance does not signiﬁcantly change due to extra delay in the network. We use two metrics to identify interference-sensitive applications. We ﬁnd that Stall Time per Miss (STPM) metric correlates with MLP. STPM is the average number of cycles for which a core is stalled because it is waiting for a cache miss packet to return from the network. Relative STPM is an application’s STPM value normalized to the minimum STPM among all applications to be mapped. Applications with high relative STPM are likely to have relatively low MLP. Such applications are classiﬁed as sensitive only if they inject enough load into network, i.e., if their L1 cache MPKI is greater than a threshold. How to Decide Whether or Not to Allocate a Cluster for Sensitive Applications After identifying sensitive applications, our technique tests if a separate cluster should be allocated for them. This cluster is called the RIcl ust er , which stands for Reduced-Interference Cluster. There are three conditions that need to be satisﬁed for this cluster to be formed: • First, there have to be enough sensitive applications to ﬁll majority of the cores in a cluster. This condition ensures that there are enough sensitive applications such that their separation from others actually reduces overall interference signiﬁcantly.3 • Second, the entire workload should exert a large amount of pressure on the network. We found that allocating interference-sensitive applications to their own cluster makes sense only for workloads that have a mixture of interference-sensitive applications and network-intensive (high-MPKI) applications that can cause severe interference by pushing the network towards saturation. As a result, our algorithm considers forming an RIcl ust er if the aggregate bandwidth demand of the entire workload is higher than 1500 MPKI (determined empirically). • Third, the aggregate MPKI of the RIcl ust er should be small enough so that interference-sensitive applications mapped to it do not signiﬁcantly slow down each other. If separating applications to an RIcl ust er ends up causing too much interference within the RIcl ust er , this would defeat the purpose of forming the RIcl ust er in the ﬁrst place. To avoid this problem, our algorithm does not form an RIcl ust er if the aggregate MPKI of RIcl ust er exceeds the bandwidth capacity of any NoC channel.4 If any of these three criteria are not satisﬁed, Balanced Load (BL) algorithm is used to perform mapping in all clusters, without forming a separate RIcl ust er . How to Map Sensitive Applications to Their Own Cluster The goal of the Reduced Interference (RI) algorithm is to ﬁll the RIcl ust er with as many sensitive applications as possible, as long as the aggregate MPKI of RIcl ust er does not exceed the capacity of any NoC channel. The problem of choosing p ( p = number of cores in a cluster) sensitive applications that have an aggregate MPKI less than a upper bound, while maximiz3We empirically found that at least 75% of the cores in the cluster should run sensitive applications. 4 For example. in our conﬁguration, each NoC channel has a capacity of 32 GB/s. Assuming 64 byte cache lines and throughput demand of one memory access per cycle at core frequency of 2 GHz, 32 GB/s translates to 250 MPKI. Thus the aggregate MPKI of RIcl ust er should be less than 250 MPKI (T hreshMPK IRI = 250 MPKI). ing aggregate sensitivity of RIcl ust er can be easily shown to be equivalent to the 0-1 knapsack problem [10]. We use a simple solution described in [10] to the knapsack problem to choose p sensitive applications. In case there are fewer sensitive applications than p, we pack the empty cores in the RIcl ust er with the insensitive applications that are the least network-intensive.5 Forming More than One RIcl ust er If the number of sensitive applications is more than 2 p, then our technique forms two clusters. Once the RIcl ust er has been formed, the rest of the applications are mapped to the remaining clusters using the BL algorithm. We call this ﬁnal inter-cluster mapping algorithm, which combines RI and BL algorithms, as the BLRI algorithm. 3.3. Mapping Policy within a Cluster Our intra-cluster algorithm decides which core within a cluster should an application be mapped to. We observed in Section 2 that memory-intensive and network-sensitive applications beneﬁt more from placement closer to memory controllers. Thus our proposed mapping maps these applications radially around the memory controllers. Figure 2 (f) (2) shows an example the mapping achieved by our proposal. Our intra-cluster algorithm differentiates applications based on both their 1) network/memory demand (i.e. rate of injection of packets) measured as MPKI, and 2) sensitivity to network latency measured as STPM at the core. It then computes a metric, stall time per thousand instructions, ST PK I = MPK I ∗ ST PM for each application, and sorts applications based on the value of this metric. Applications with higher ST PK I are assigned to cores closer to the memory controller. To achieve this, the algorithm maps applications radially in concentric circles around the memory controller in sorted order, starting from the application with the highest ST PK I . 3.4. Putting It All Together: Our Application-to-Core (A2C) Mapping Algorithm Our ﬁnal algorithm consists of three steps combining the above algorithms. First, cores are clustered into subnetworks using the cluster-CLOCK page mapping algorithm (Section 3.1). Second, the BLRI algorithm is invoked to map applications to clusters (Section 3.2.2). In other words, the algorithm attempts to allocate a separate cluster to interference-sensitive applications (Section 3.2.2), if possible, and distributes the applications to remaining clusters to balance load (Section 3.2.1). Third, after applications are assigned to clusters, they are mapped to cores within their clusters by invoking the intracluster radial mapping algorithm (Section 3.3). We call this ﬁnal algorithm, which combines clustering, BLRI and radial algorithms, as the A2C mapping algorithm. 4. Implementation 4.1. Proﬁling The proposed mapping policies assume knowledge of two metrics: a) network demand in terms of L2 MPKI and b) sensitivity in terms of STPM. These metrics can be either pre-computed for applications a priori or measured online during a proﬁling phase. We evaluate both scenarios, where metrics are known 5Note that this solution does not have high overhead as our algorithm is invoked at long time intervals at the granularity of OS time quantums. a priori (static A2C) and when metrics are measured online (dynamic A2C). For dynamic A2C, we proﬁle the workload for 10 million instructions (proﬁling phase) and then compute mappings that are enforced for 300 million instructions (enforcement phase). The proﬁling phase and enforcement phases are repeated periodically. The proﬁling to determine MPKI requires two hardware counters in the core: 1) instruction counter and 2) L2 miss counter. The proﬁling to determine STPM requires one additional hardware counter at the core which is incremented every cycle the oldest instruction cannot be retired because it is waiting for an L2 miss. Note that the A2C technique requires only a relative ordering among applications and hence we ﬁnd quantizing applications to classes based on the above metrics is sufﬁcient. 4.2. Operating System and Firmware Support Our proposal requires support from the operating system. First, the operating system page allocation and replacement routine is modiﬁed to enforce clustering, as described in Section 3.1. Second, the A2C algorithm can be integrated as part of the operating system task scheduler. If this is the case, the OS scheduler allocates cores to applications based on the optimized mapping computed by the A2C algorithm. The complexity of the algorithm is relatively modest and we found its time overhead is negligible since the algorithm needs to be invoked very infrequently (e.g., every OS time quantum). Alternatively, the A2C algorithm can be implemented as part of the ﬁrmware of a multi-core processor. The spatial decisions of where to schedule applications made by the A2C algorithm can be used to augment the OS timesharing scheduler’s temporal decisions of when to schedule an application. When a context switch is needed, the OS can invoke A2C to determine if there is a need for migration and can decide whether or not to perform migrations based on a costbeneﬁt analysis. 4.3. Adapting to Dynamic Runtime Environment The runtime environment of a manycore processor will be dynamic with continuous ﬂow of incoming programs (process creation), outgoing programs (process completion), and context switches. Thus, it is hard to predict a priori which set of applications will run simultaneously as a workload on the manycore processor. Our application-to-core mapping techniques have the capability to adapt to such dynamic scenarios via two key elements. First, application characteristics can be determined during runtime (Section 4.1). Second, since applicationto-core mapping of an application can change between different execution phases, we migrate applications between cores to enforce new mappings. We discuss the costs of application migration in the next subsection. 4.4. Migration Costs A new application-to-core mapping may require migration of an application from one core to another. We can split the cost associated with application migration into four parts: 1) A constant cost due to operating system bookkeeping to facilitate migration. This cost is negligible because all cores are managed by a single uniﬁed operating system. For example, the ﬁle handling, memory management, IO, network socket state, etc are Processor Pipeline Fetch/Exec/Commit width Memory Management L1 Caches L2 Caches Main Memory Network Router Network Interface Network Topology 2 GHz processor, 128-entry instruction window 2 instructions per cycle in each core; only 1 can be a memory operation 4KB physical and virtual pages, 512 entry TLBs, CLOCK page allocation and replacement 32KB per-core (private), 4-way set associative, 64B block size, 2-cycle latency, split I/D caches, 32 MSHRs 256KB per core (private), 16-way set associative, 64B block size, 6-cycle bank latency, 32 MSHRs, directories are co-located with the memory controller 16GB DDR3-DRAM, up to 16 outstanding requests per-core, 160 cycle access, 4 on-chip Memory Controllers. 2-stage wormhole switched, virtual channel ﬂow control, 4 VC’s per Port, 4 ﬂit buffer depth, 4 ﬂits per data packet, 1 ﬂit per address packet. 16 FIFO buffer queues with 4 ﬂit depth 8x8 mesh, 128 bit bi-directional links (32GB/s). Table 1: Baseline Processor, Cache, Memory, and Network Conﬁguration Functional Simulations for Page Fault Rates: We run 500 million instructions per core (totally 30 billion instructions across 60 cores) from the simulation fast forward point obtained from [37] to evaluate cluster-CLOCK and CLOCK page replacement algorithms. Performance Simulations for Static A2C mapping: To have tractable simulation time, we choose a smaller representative window of instructions, obtained by proﬁling each benchmark, from the representative execution phase obtained from [37]. All our experiments study multi-programmed workloads, where each core runs a separate benchmark. We simulate 10 million instructions per core, corresponding to at least 600 million instructions across 60 cores. Performance Simulations for Dynamic A2C mapping: To evaluate the dynamic application-to-core mapping faithfully, we need longer simulations that model at least one dynamic proﬁling phase and one enforcement phase (as explained in Section 4.1). We simulate an entire proﬁling+enforcement phase (300 million instructions) per benchmark per core, corresponding to at least 18.6 billion instructions across 60 cores. Table 2: Simulation Methodology shared between the cores due to the single operating system image and need not be saved or restored; 2) A constant cost (in terms of bytes) for transferring the application’s architectural context (including registers) to the new core; 3) A variable cache warmup cost due to cache misses incurred after transferring the application to a new core. We quantify this cost and show that averaged over the entire execution phase, this cost is negligibly small across all benchmarks (see Section 6.5); and 4) A variable cost due to potential reduction in clustering factor.6 This cost is incurred only when we migrate applications between clusters and, after migration, the application continues to access pages mapped to its old cluster. We quantify this cost in our performance evaluation for all our workloads (see Section 6.5). Our evaluations faithfully account for all of these four types of migration costs. 5. Methodology 5.1. Experimental Setup We evaluate our techniques using an instruction-trace-driven, cycle-level x86 CMP simulator. The functional frontend of the simulator is based on the Pin dynamic binary instrumentation tool [31], which is used to collect instruction traces from applications, which are then fed to the core models that model the execution and timing of each instruction. Table 1 provides the conﬁguration of our baseline, which consists of 60 cores and 4 memory controllers connected by a 2D, 8x8 mesh NoC. Each core is modeled as an out-of-order execution core with a limited instruction window and limited buffers. The memory hierarchy uses a two-level directorybased MESI cache coherence protocol. Each core has a private write-back L1 cache and private L2 cache. The memory controllers connect to off-chip DRAM via DDR channels and have sufﬁcient buffering to hold 16 outstanding requests per core. The bandwidth of DDR channels is accurately modelled. The network connects the core tiles and memory controller tiles. The system we model is self-throttling as real systems are: if the miss buffers are full the core cannot inject more packets into the network. Each router uses a state-of-the-art two-stage 6Clustering factor is deﬁned as the percentile of accesses that are constrained within the cluster. microarchitecture. We use the deterministic X-Y routing algorithm, ﬁnite input buffering, wormhole switching, and virtualchannel ﬂow control. We use the Orion power model to estimate the router power [42]. We also implemented a detailed functional model for virtual memory management to study page access and page fault behavior of our workloads. The baseline page allocation and replacement policy is CLOCK [22]. The modiﬁed page replacement and allocation policy, cluster-CLOCK, looks ahead 512 pages beyond the ﬁrst replacement candidate to potentially ﬁnd a replacement page belonging to home memory controller. The parameters used for our A2C algorithm are: T hreshMPILow = 5 MPKI, and T hreshMPIH igh = 25 MPKI, T hreshSensit ivit yRat io = 5 and T hreshMPK I−RI = 250 MPKI. These parameters are empirically determined but not tuned. The constant cost for OS book keeping while migrating applications is assumed to be 50K cycles. The migrating applications write and read 128 bytes to/from the memory to save and restore their register contexts. These are modelled as extra memory accesses in our simulations. 5.2. Evaluation Metrics Our evaluation uses several metrics. We measure system performance in terms of average weighted speedup [14], a commonly used multi-program performance metric, which is the average of the sum of speedups of each application compared to when it is run alone on the same system. We measure system fairness in terms of the maximum slowdown any application experiences in the system. We also report IPC throughput. IPCal one is the IPC of the application when run alone on our baseline system. (Average) Weight ed S peed u p = 1 N umT hread s N umT hread s × ∑ i=1 IPCshared i IPCal one i IPC T hrough put = N umT hread s ∑ IPCi U n f airnessI nd ex = max i=1 IPCal one i IPCshared i i (1) (2) (3) 5.3. Workloads and Simulation Methodology Table 2 describes the three different types of simulations we run to evaluate our mechanisms. Due to simulation time limitations, we evaluate the execution time effects of our static 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg o n r m . w h g e i t d e p u d e e p s BASE CLUSTER+RND A2C 0.0 0.5 1.0 1.5 2.0 2.5 3.0 MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg s y s t e m n u f a i r s s e n ( m x a . o s l w o d w n ) BASE CLUSTER+RND A2C 0.0 0.2 0.4 0.6 0.8 1.0 1.2 MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg N o r m a i l d e z N o C P o w e r BASE CLUSTER+RND A2C Figure 3: (a) System performance (b) system unfairness and (c) interconnect power of the A2C algorithm for 128 workloads 0% 5% 10% 15% 20% 25% 30% 0.0 0.2 0.4 0.6 0.8 1.0 1.2 weighted speedup of workload  with no clustering % s n a g i i n W p u d e e p S d e h g e i t 0 20 40 60 80 100 120 0 2 4 6 8 % s s e c c a w t i n h i s u c l t e r memory footprint of workload (GB) CLOCK cluster-CLOCK 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 2 4 6 8 o n r m a i l e g a p d e z f u a t l s memory footprint of workload (GB) 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 e g a p f u a t l r a t e CLOCK cluster-CLOCK (a) (b) (c) (d) Figure 4: Performance analysis of clustering across 128 workloads (a) Weighted speedup (b) Clustering Factor (c) Page Faults (d) A case study showing page faults per unique page accessed for different applications and dynamic mechanisms without taking into account the effect on page faults. We evaluate the effect of our proposal on page fault rate separately using functional simulation, showing empirically that our proposal actually reduces page fault rate (Section 6.2). We use a diverse set of multiprogrammed application workloads comprising scientiﬁc, commercial, and desktop applications. In total, we study 35 applications, including SPEC CPU2006 benchmarks, applications from SPLASH-2 and SpecOMP benchmark suites, and four commercial workloads traces (sap, tpcw,sjbb, sjas). We choose representative execution phases using PinPoints [37] for all our workloads except commercial traces, which were collected over Intel servers. Figure 6 (b) lists each application and includes results showing the MPKI of each application on our baseline system. Multiprogrammed Workloads and Categories: All our results are across 128 randomly generated workloads mixes. Each workload mix consists of 10 copies each of 6 applications randomly picked from our suite of 35 applications. The 128 workloads are divided into four subcategories of 32 workloads each: 1) MPKI500: relatively less network-intensive workloads with aggregate MPKI less than 500, 2) MPKI1000: aggregate MPKI is between 500-1000, 3) MPKI1500: aggregate MPKI is between 1000-1500, 4) MPKI2000: relatively more network-intensive workloads with aggregate MPKI between 1500-2000. 6. Performance Evaluation 6.1. Overall Results for the A2C Algorithm We ﬁrst show the overall results of our ﬁnal Application-toCore Mapping algorithm (A2C). We evaluate three systems: 1) the baseline system with random mapping of applications to cores (BASE), which is representative of existing systems, 2) our enhanced system which uses our cluster-CLOCK algorithm (described in Section 3.1) but still uses random mapping of applications to cores (CLUSTER+RND), 3) our ﬁnal system which uses our combined A2C algorithm (summarized in Section 3.4), which consists of clustering, inter-cluster BLRI mapping, and intra-cluster radial mapping algorithms (A2C). Figure 3 (a) and (b) respectively show system performance (higher is better) and system unfairness (lower is better) of the three systems. Solely using clustering (CLUSTER+RND) improves weighted speedup by 9.3% over the baseline (BASE). A2C improves weighted speedup by 16.7% over the baseline, while reducing unfairness by 22%.7 The improvement partly due to a 39% reduction in average network packet latency (not shown in graphs). Interconnect Power Figure 3 (c) shows the average normalized interconnect power consumption (lower is better). Clustering only reduces power consumption by 31.2%; A2C mapping reduces power consumption by 52.3% over baseline (BASE). 8 The clustering of applications to memory controllers reduces the average hop count signiﬁcantly, reducing the energy spent in moving data over the interconnect. Using inter-cluster and intra-cluster mapping further reduces hop count and power consumption by ensuring that network-intensive applications are mapped close to the memory controllers and network load is balanced across controllers after clustering. In the next three sections, we analyze the beneﬁts and tradeoffs of each component of A2C. 6.2. Analysis of Clustering and Cluster-CLOCK The goal of clustering is to reduce interference between applications mapped to different clusters. Averaged across 128 workloads, clustering improves system throughput by 9.3% in terms of weighted speedup and 8.0% in terms of IPC throughput. Figure 4 (a) plots the gains in weighted speedup due to clustering for each workload against the baseline weighted speedup of the workload. A lower baseline weighted speedup indicates that average slowdown of applications is higher and hence contention/interference is high between applications in the baseline. The ﬁgures show that performance gains due 7 System performance improvement ranges from 0.1% to 22% while the unfairness reduction ranges from 6% to 72%. We do not show graphs for instruction throughput due to space limitations. Clustering alone (CLUSTER+RND) improves IPC throughput by 7.0% over the baseline, while A2C improves IPC by 14.0%. 8 Power reduction ranges from 39% to 68%.                                       2.5 2.0 n w 1.5 o d w 1.0 o s l 0.5 0.0 h264ref barnes milc gromacs libquantum leslie p u d e e p s d e t h g e i w IMBL RND BL BLRI L B / I R L B r o f i n a g % 25% 20% 15% 10% 5% 0% -20% 1.2 1.0 0.8 0.6 0.4 0.2 0.0 IMBL RND BL MPKI500 MPKI1000 MPKI1500 MPKI2000 p u d e e p s d e t h g e i w RadialINV RND Radial 1.2 1.0 0.8 0.6 0.4 0.2 0.0 -10% 0% 10% 20% 30% % gain  for IMBL/BL MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg (a) (b) (c) (d) Figure 5: (a) Case study analyzing BL mapping (b) Average results for different inter-cluster mappings across 128 workloads (c) Average results for BLRI mapping for the MPKI2000 workload category (d) Average results for radial mapping across 128 workloads to clustering are signiﬁcantly higher for workloads with lower weighted speedup (i.e., higher slowdowns due to interference). This is intuitive because the main goal of clustering is to reduce interference between applications. Analysis of the cluster-CLOCK page replacement algorithm: To enforce clustering, we have developed the clusterCLOCK algorithm (Section 3.1) which modiﬁes the default page allocation and page replacement policies. The results in Figure 4 quantify the effect of cluster-CLOCK across 128 workloads. Figure 4 (b) plots the clustering factor with the baseline policy (CLOCK) and our new policy (clusterCLOCK). Recall that the clustering factor is the percentage of all accesses that are serviced by the home memory controller. On average, cluster-CLOCK improves the clustering factor from 26.0% to 97.4%, thereby reducing interference among applications. Figure 4 (c) shows the normalized page fault rate of clusterCLOCK for each workload (Y-axis) versus the memory footprint of the workload (X-axis). A lower relative page fault rate indicates that cluster-CLOCK reduces the page fault rate compared to the baseline. We observe that cluster-CLOCK 1) does not affect the page fault rate for workloads with small memory footprint, 2) in general reduces the page fault rate for workloads with large memory footprint. On average, clusterCLOCK reduces the page fault rate by 4.1% over 128 workloads. Cluster-CLOCK reduces page faults because it happens to make better page replacement decisions than CLOCK (i.e., replace pages that are less likely to be reused soon) by reducing the interference between applications in physical memory space: by biasing replacement decisions to be made within each memory controller as much as possible, applications mapped to different controllers interfere less with each other in the physical memory space. As a result, applications with lower page locality disturb applications with higher page locality less, improving page fault rate. Note that our execution time results do not include this effect of reduced page faults due to simulation speed limitations. To illustrate this behavior, we focus on one workload as a case study in Figure 4 (d), which depicts the page fault rate in terms of page faults incurred per unique page accessed by each application with CLOCK and cluster-CLOCK. Applications art and leslie have higher page fault rate but we found that they have good locality in page access. On the other hand, astar also has high page but low locality in page access. When these applications run together using the CLOCK algorithm, astar’s pages contend with art and leslie’s pages in the entire physical memory space, causing those pages to be evicted from physical memory. On the other hand, if clusterCLOCK is used, and astar is mapped to a different cluster from art and leslie, the likelihood that astar’s pages replace art and leslie’s pages reduces signiﬁcantly because cluster-CLOCK attempts to replace a page from the home memory controller astar is assigned to instead of any page in the physical memory space. We conclude that cluster-CLOCK can reduce page fault rates by localizing page replacement and thereby limiting page-level interference among applications to pages assigned to a single memory controller. 6.3. Analysis of Inter-Cluster Mapping We study the effect of inter-cluster load balancing algorithms described in Section 3.2. For these evaluations, all other parts of the A2C algorithm is kept the same: we employ clustering and radial intra-cluster mapping. We ﬁrst show the beneﬁts of balanced mapping (BL), then show when and why imbalanced mapping (IMBL) can be beneﬁcial, and later evaluate our BLRI algorithm, which aims to achieve the beneﬁts of both balance and imbalance. BL Mapping Figure 5 (a) shows application slowdowns in an example workload consisting of 10 copies each of h264ref, gromacs, barnes, libquantum, milc and leslie applications running together. The former three are very networkinsensitive and exert very low load. The latter three are both network-intensive and network-sensitive. An IMBL mapping (described in Section 3.2) severely slows down the latter three network-intensive and network-sensitive applications because they get mapped to the same clusters, causing signiﬁcant interference to each other, whereas the former three applications do not utilize the bandwidth available in their clusters. A random (RND) mapping (which is our baseline) still slows down the same applications, albeit less, by providing better balance in load across the network. Our balanced (BL) mapping algorithm, which distributes load in a balanced way among all clusters provides the best speedup (19.7% over IMBL and 5.9% over RND) by reducing the slowdown of all applications because it does not overload any single cluster. IMBL Mapping Figure 5 (b) shows average weighted speedups across 128 workloads, categorized by the overall intensity of the workloads. When the overall workload intensity is not too high (i.e., less than 1500 MPKI), BL provides significantly higher performance than IMBL and RND by reducing and balancing interference. However, when the workload intensity is very high (i.e., greater than 1500 MPKI), BL performs worse than IMBL mapping. The reason is that IMBL mapping isolates network-non-intensive (low MPKI) applications from network-intensive (high MPKI) ones by placing them into sep            0% 5% 10% 15% 20% 25% MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg % n a g i i n w h g e i t p u d e e p s d e Static Dynamic 0 10 20 30 40 50 60 70 80 90 0 2 4 6 8 10 12 14 16 18 v o p r y a t o n o t u c a c l i l x e p r l h c n e b a n m d a e d l c c g u p p a l w f r g n e s j a b r s e n b o g m k 4 6 2 h r e f g r o m s c a h m m e r c a c t s u s a t a r 2 p z b i n a e c o s a s j p a s t c p w b b s j x n h p s i a t r m l i c l s e i l e o m e n t n a a x l G e m s i l u n a u q b t m l b m x e p o s l s w i m m c f M K P I N o r m a i l d e z i c n r e s a e i n M K P I MPKI increase (10M-20M) MPKI-10M MPKI increase (10M-300M) MPKI-300M 0% 20% 40% 60% 80% 100% % o f e c c a s e s w i t n h i c l u t s e r Base Static Dynamic Figure 6: (a) Performance improvement of static and dynamic A2C for 8 workloads (b) Increase in MPKI due to migration for individual applications (c) Clustering factor for 128 workloads arate clusters. When the network load is very high, such isolation signiﬁcantly improves the performance of network-nonintensive applications without signiﬁcantly degrading the performance of intensive ones by reducing interference between the two types of applications. The main takeaway is that when network load is high, relatively less-intensive but network-sensitive applications’ progress gets slowed down too signiﬁcantly because other applications keep injecting interfering requests. As a result, if network load is very high, it is more beneﬁcial to separate the accesses of non-intensive applications from others by placing them into separate clusters, thereby allowing their fast progress. However, such separation is not beneﬁcial for non-intensive applications and harmful for performance if the network load is not high: it causes wasted bandwidth in some clusters and too much interference in others. This observation motivates our BLRI algorithm (which creates a separate cluster for nonintensive applications only when the network load is high), which we analyze next. BLRI Mapping Figure 5 (c) shows the speedup achieved by BLRI mapping over BL for all 32 workloads in MPKI2000 category (recall that BLRI is not invoked for workloads with aggregate MPKI of less than 1500) on the Y-axis against the speedup achieved for the same workload by IMBL over BL. We make two observations. First, for workloads where imbalanced mapping (IMBL) improves performance over balanced mapping (BL), shown in the right half of the plot, BLRI also signiﬁcantly improves performance over BL. Second, for workloads where imbalance reduces performance (left half of the plot), BLRI either improves performance or does not affect performance. We conclude that BLRI achieves the best of both worlds (load balance and imbalance) by isolating those applications that would most beneﬁt from imbalance (i.e. network sensitive applications) and performing load balancing for the remaining ones. 6.4. Effect of Intra-Cluster Mapping We analyze the effect of intra-cluster mapping policy, after applications are assigned to clusters using the BLRI intercluster policy. We examine three different intra-cluster mapping algorithms: 1) Radial: our proposed radial mapping described in Section 3.2, 2) RND: Cores in a cluster are assigned randomly to applications, 3) RadialINV: This is the inverse of our radial algorithm; those applications that would beneﬁt least from being close to the memory controller (i.e., those with low STPKI) are mapped closest to the memory controller. Figure 5 (d) shows the average weighted speedup of 128 workloads with BLRI inter-cluster mapping and different intra-cluster mappings. The radial intra-cluster mapping provides 0.4%, 3.0%, 6.8%, 7.3% for MPKI500, MPKI1000, MPKI1500, MPKI2000 category workloads over RND intracluster mapping. Radial mapping is the best mapping for all workloads; RadialINV is the worst. We conclude that our metric and algorithm for identifying and deciding which workloads to map close to the memory controller is effective. 6.5. Effect of Dynamic A2C Mapping Our evaluation assumed so far that a static mapping is formed with pre-runtime knowledge of application characteristics. We now evaluate the dynamic A2C scheme, described in Section 4.1. We use a proﬁling phase of 10 million instructions, after which the operating system forms a new application-tocore mapping and enforces it for the whole phase (300 million instructions). An application can migrate at the end of the dynamic proﬁling interval after each phase. Figure 6 (a) compares the performance improvement achieved by static and dynamic A2C schemes for workloads from each MPKI category, over a baseline that employs clustering but uses random application-to-core mapping. The performance of dynamic A2C is close to that of static A2C (within 1% on average) for these eight workloads. However, static A2C performs better for the two MPKI2000 workloads. We found this is because the BLRI scheme (which determines sensitive applications online and forms a separate cluster for them) requires re-mapping at more ﬁne-grained execution phases. Unfortunately, given the simulation time constraints we could not fully explore the best thresholds for the dynamic scheme. We conclude that the dynamic A2C scheme provides signiﬁcant performance improvements, even with untuned parameters. Migration Cost Analysis In Section 4, we qualitatively discussed the overheads of application migration. In this section, we ﬁrst quantify the increases in cache misses when an application migrates from one core to another. We then quantify the reduction in clustering factor due to migrations. Overall, these results provide quantitative insight into why the dynamic A2C algorithm works. Figure 6 (b) analyzes the MPKI of the 35 applications during the proﬁling phase (MPKI-10M) and the enforcement phase (MPKI-300M). It also analyzes the increase in the MPKI due to migration to another core during the 10M instruction interval right after the migration happens (MPKI increase (1020M)) and during the entire enforcement phase (MPKI increase (300M)). The left Y-axis is the normalized MPKI of the application when it is migrated to another core compared to the MPKI when it is running alone (a value of 1 means the MPKI of the application does not change after migration).                       Cache Size Performance Gain 256KB 16.7% (a) 512KB 13.6% 1MB 12.1% Number of MCs Performance Gain 4 MC 16.7% 8 MC 17.9% Placement of MCs in a Cluster Performance Gain Corner 16.7% Center 14.9% (b) (c) Table 3: Sensitivity to (a) Last level cache size (per-core), (b) Number of memory controllers and (c) Placement of memory controllers within a cluster Benchmarks on the X-axis are sorted from the lowest to highest baseline MPKI from left to right. We make several key observations: • The MPKI in the proﬁling phase (MPKI-10M) correlates well with the MPKI in the enforcement phase (MPKI-300M), indicating why dynamic proﬁling can be effective. • MPKI increase within 10M instructions after migration is negligible for high-intensity workloads, but signiﬁcant for lowintensity workloads. However, since these benchmarks have very low MPKI to begin with, their execution time is not signiﬁcantly affected. • MPKI increase during the entire phase after migration is negligible for almost all workloads. This increase is 3% on average and again observed mainly in applications with low intensity. These results show that cache migration cost of migrating an application to another core is minimal over the enforcement phase of the new mapping. The clustering factor (i.e., the ratio of memory accesses that are serviced by the home memory controller) is also affected by application migration. The clustering factor may potentially decrease, if, after migration, an application continues to access the pages mapped to its old cluster. Figure 6 (c) shows the clustering factor for our 128 workloads with 1) baseline RND mapping, 2) static A2C mapping, and 3) dynamic A2C mapping. The clustering factor reduces from 97.4% with static A2C to 89.0% with dynamic A2C due to inter-cluster application migrations. However, dynamic A2C mapping still provides a large improvement in clustering factor compared to the baseline mapping. We conclude that both dynamic and static versions of A2C are effective: static A2C is desirable when application information is proﬁled before runtime, but dynamic A2C does not cause signiﬁcant overhead and achieves similar performance. 6.6. A2C vs Application-Aware Prioritization We compare the beneﬁts of A2C mapping to application-aware prioritization in the network to show that our interferenceaware mapping mechanisms are orthogonal to interferenceaware packet scheduling in the NoC. Das et al. [12] proposed an application-aware arbitration policy (called STC) to accelerate network-sensitive applications. The key idea is to rank applications at regular intervals based on their network intensity (outermost private cache MPKI), and p u d e e p s d e t h g e i w . m r o n BASE BASE+STC A2C A2C+STC 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 prioritize packets of non-intensive applications over packets of intensive applications. We compare Application-to-Core mapping policies to application-aware prioritization policies because both techniques have similar goals: to effectively handle inter-application interference in NoC. However, both techniques take different approaches towards the same goal. STC tries to make the best decision when interference happens in the NoC through efﬁcient packet scheduling in routers while A2C tries to reduce interference by mapping applications to separate clusters and controllers. Figure 7 shows that STC9 is orthogonal to our proposed A2C technique and its beneﬁts are additive to A2C. STC prioritization improves performance by 5.5% over the baseline whereas A2C mapping improves performance by 16.7% over the baseline. When STC and A2C are combined together, overall performance improvement is 21.9% over the baseline, greater than the improvement provided by either alone. STC improves performance when used with A2C mapping because it prioritizes via packet scheduling non-intensive applications (shorter jobs) within a cluster in a coordinated manner, ensuring all the routers act in tandem.The packet scheduling effect of STC is orthogonal to the beneﬁts of A2C. Hence, we conclude that our mapping mechanisms interact synergistically with applicationaware packet scheduling. 6.7. Sensitivity Studies We evaluated the sensitivity of A2C to different system conﬁgurations. Table 3 shows A2C’s performance improvement over the baseline on 128 multiprogrammed workloads. As the last-level cache size increases, performance beneﬁt of A2C decreases because NoC trafﬁc and hence interference in the NoC reduces. However, A2C still provides 12.1% performance improvement even when total on-chip cache size is 60MB (i.e., 1MB/core). As we vary the number of memory controllers from 48, A2C’s performance beneﬁt stays similar because more controllers 1) on the one hand enable more ﬁne-grained clustering, improving communication locality and reducing interference, 2) on the other hand can reduce clustering factor when application behavior changes. We also study the placement of memory-controllers, and change their locations from corner tiles to tiles in the center of each cluster. While this placement can reduce the contention at memory controllers, it also increases the effectiveness of intra-cluster radial mapping algorithms. Overall, A2C still provides 14.9% performance improvement with central placement of memory-controllers. Finally, we study the effectiveness of our proposed techniques with a shared cache. An interleaved (striped) shared cache has poor locality because consecutive cache blocks are MPKI500 MPKI1000 MPKI1500 MPKI2000 Avg Figure 7: Performance comparison of A2C mapping and application-aware prioritization (STC) for 128 workloads. 9 The default parameters used for STC [12] are: 1) ranking levels R = 8, 2) batching levels B = 8, 3) ranking interval = 350,000 cycles, 4) batching interval = 16,000 cycles, 5) BCIP packet sent every U = 4000 cycles. Cache Conﬁguration Speedup over Shared+Striped Shared+DSR 42.6% Shared+DSR+A2C 59.5% Table 4: Performance beneﬁt over a shared cache statically mapped to consecutive banks. To improve locality as well as cache utilization, we implement a shared cache using the Dynamic Spill Receive (DSR) mechanism [38]. DSR effectively balances locality with cache capacity. The key idea is to spill (i.e., have the ability to store) cache blocks of a core that beneﬁts from more cache space than its local cache slice size to the closeby cache slices that are local to cores that do not beneﬁt from using their entire local cache slice. In our implementation of DSR, a core’s local cache slice can spill to cache slices within a 2-hop distance. Table 4 shows the normalized performance gain with DSR and DSR combined with A2C over a striped cache mapping. As shown previously in [38], DSR improves the system performance over striped cache mapping because it allows applications that beneﬁt more from shared cache to utilize the extra cache space effectively while also improving locality. Applying A2C on top of DSR also signiﬁcantly improves system performance due to three major reasons. First, A2C reduces contention at the memory controllers and in the interconnect, which are still signiﬁcant with a shared cache. Second, A2C reduces the distance that a network packet needs to travel, lowering network interference as well as packet latency. Third, A2C distributes applications with different network intensities across different clusters, thereby enabling applications that beneﬁt from low latency (57% lower network latency) to make progress with less interference from applications that are intensive. Thus, A2C’s beneﬁts are signiﬁcant in systems with shared caches. We conclude that our proposed techniques provide consistent performance improvements when we vary major relevant system parameters. 7. Related Work To our knowledge, this is the ﬁrst work that tackles the problem of how to map competing applications to cores in a network-onchip based memory system to minimize inter-application interference in multiprogrammed workloads. We brieﬂy describe the closely related works in this section. Thread-to-Core Mapping: Prior works have proposed threadto-core mapping to improve locality of communication [29, 33] and shared cache management [8] in parallel applications by placing frequently communicating threads/tasks closer to each other. Their goal is to reduce inter-thread or inter-task communication. Our techniques solve a completely different problem: inter-application interference. As such, our goals are different: 1) reduce interference in the network between different independent applications and 2) reduce contention at the memory controllers. Previous thread-to-core mappings can potentially be combined with our techniques within an application when multiple applications are run in parallel, and this could be a promising future direction. Reducing Interference in NoCs: Recent works propose quality-of-service [28, 18, 19], packet scheduling [12, 13] and routing techniques [32] to reduce interference in NoCs. These works are complementary to our mechanism.We already quantitatively showed that A2C is orthogonal to application-aware packet prioritization [12] (see Section 6.6). Memory Controllers and Page Allocation: Awasthi et al. [4] explored page allocation and page migration in the context of multiple memory controllers in a multi-core processor with the goal of balancing load between memory controllers and improving DRAM row buffer locality. The problem we solve is different and our techniques are complementary to theirs. First, our goal is to reduce interference between applications in the interconnect and the memory controllers. Second, our mechanism addresses the problem of mapping applications to the cores while [4] balances the load at the memory controllers for a given appplication-to-core mapping. We believe the page migration techniques proposed in [4] can be employed to reduce the costs of migration in our dynamic application-to-core mapping policies. Kim et al. [25] propose Thread Cluster Memory scheduling (TCM), an application-aware memory access scheduling algorithm implemented within a memory controller. TCM does not consider the effects of core selection on contention between applications at the memory controllers, nor does it address the problem of interference in the interconnect. However, TCM can still be used as a baseline memory scheduling algorithm with our proposal. Page allocation and migration have been explored extensively to improve locality of access within a single application executing on a NUMA multiprocessor (e.g., [6, 15]). These works do not aim to reduce interference between multiple competing applications sharing the memory system, and hence do not solve the problem we aim to solve. In addition, these works do not consider how to map applications to cores in a networkon-chip based system. Page coloring techniques have been employed in caches to reduce cache conﬂict misses [24, 5] and to improve cache locality in shared caches (e.g., [9]). These works solve an orthogonal problem and can be combined with our techniques. Abts et al. [2] explore placement of memory controllers in a multi-core processor to minimize channel load. Memory controller placement is also complementary to our mechanisms, and as we showed in Section 6.7, our mechanism provides performance improvements with different memory controller placement conﬁgurations. Thread Migration: Thread migration has been explored to manage power [39, 20], thermal hotspots [17, 11] or to exploit heterogeneity in multi-core processors [30, 23, 41, 3]. We use thread migration to enable dynamic application-to-core mappings to reduce interference in NoCs. OS Task Scheduling: OS task schedulers [27, 44] have been proposed to reduce the contention in shared memory resources (the last level cache and memory) by co-scheduling tasks such that interference between tasks in these resources is minimized. Voltage Smoothing [40] has been proposed to co-schedule phases of different applications to mitigate voltage error recovery overheads in future resilient processor designs. In contrast to A2C, these techniques consider neither interference in the NoC nor the interference-sensitivity of different applications in the NoC when making core allocation decisions. They also operate at a higher-level than A2C in that they select which applications concurrently share memory system resources. Our technique reduces interference between already co-scheduled applications, and hence 1) is complementary to these proposals, 2) can provide performance improvements even if a desirable co-schedule of tasks that reduces interference is not possible due to the mix of applications in the system. 8. Conclusion We presented application-to-core mapping policies that largely reduce inter-application interference in network-on-chip based multi-core systems. We have shown that by intelligently mapping applications to cores in a manner that is aware of applications’ characteristics, signiﬁcant increases in system performance, fairness, and energy-efﬁciency can be achieved at the same time. Our proposed algorithm, A2C, achieves this by using two key principles: 1) mapping network-latency-sensitive applications to separate node clusters in the network from network-bandwidth-intensive applications such that the former makes fast progress without heavy interference from the latter, 2) mapping those applications that beneﬁt more from being closer to the memory controllers close to these resources. We have extensively evaluated our mechanism and compared it both qualitatively and quantitatively to different application mapping policies. Our main results show that averaged over 128 multiprogrammed workload mixes on a 60-core 8x8mesh system, our proposed A2C improves system throughput by 16.7%, while also reducing system unfairness by 22.4% and interconnect power consumption by 52.3%. We conclude that the proposed approach can be an effective way of improving overall system throughput, fairness, and power-efﬁciency and therefore can be a promising way to exploit the non-uniform structure of network-on-chip-based multi-core systems. Acknowledgments We thank members of the SAFARI research group and the anonymous reviewers for their feedback. This research was partially supported by grants from NSF (CAREER Award CCF-0953246, CCF-1256203, CCF-1147397 and CCF1212962), GSRC, SRC, Intel ISTC-CC, and the Intel URO Memory Hierarchy Program. We thank the generous support of our industrial sponsors, including Intel and Oracle. Rachata Ausavarungnirun is supported by the Royal Thai Government Scholarship. "
2013,Breaking the on-chip latency barrier using SMART.,"As the number of on-chip cores increases, scalable on-chip topologies such as meshes inevitably add multiple hops in each network traversal. The best we can do right now is to design 1-cycle routers, such that the low-load network latency between a source and destination is equal to the number of routers + links (i.e. hops×2) between them. OS/compiler and cache coherence protocols designers often try to limit communication to within a few hops, since on-chip latency is critical for their scalability. In this work, we propose an on-chip network called SMART (Single-cycle Multi-hop Asynchronous Repeated Traversal) that aims to present a single-cycle data-path all the way from the source to the destination. We do not add any additional fast physical express links in the data-path; instead we drive the shared crossbars and links asynchronously up to multiple-hops within a single cycle. We design a router + link microarchitecture to achieve such a traversal, and a flow-control technique to arbitrate and setup multi-hop paths within a cycle. A place-and-routed design at 45nm achieves 11 hops within a 1GHz cycle for paths without turns (9 for paths with turns). We observe 5-8X reduction in low-load latencies across synthetic traffic patterns on an 8×8 CMP, compared to a baseline 1-cycle router. Full-system simulations with SPLASH-2 and PAR-SEC benchmarks demonstrate 27/52% and 20/59% reduction in runtime and EDP for Private/Shared L2 designs.","Breaking the On-Chip Latency Barrier Using SMART Tushar Krishna Chia-Hsin Owen Chen Woo Cheol Kwon Li-Shiuan Peh Computer Science and Artiﬁcial Intelligence Laboratory (CSAIL) Massachusetts Institute of Technology, Cambridge, MA 02139 {tushar, owenhsin, wckwon, peh}@csail.mit.edu∗ Abstract As the number of on-chip cores increases, scalable on-chip topologies such as meshes inevitably add multiple hops in each network traversal. The best we can do right now is to design 1-cycle routers, such that the low-load network latency between a source and destination is equal to the number of routers + links (i.e. hops×2) between them. OS/compiler and cache coherence protocols designers often try to limit communication to within a few hops, since on-chip latency is critical for their scalability. In this work, we propose an on-chip network called SMART (Single-cycle Multi-hop Asynchronous Repeated Traversal) that aims to present a single-cycle data-path all the way from the source to the destination. We do not add any additional fast physical express links in the data-path; instead we drive the shared crossbars and links asynchronously up to multiple-hops within a single cycle. We design a router + link microarchitecture to achieve such a traversal, and a ﬂow-control technique to arbitrate and setup multi-hop paths within a cycle. A place-and-routed design at 45nm achieves 11 hops within a 1GHz cycle for paths without turns (9 for paths with turns). We observe 5-8X reduction in low-load latencies across synthetic trafﬁc patterns on an 8×8 CMP, compared to a baseline 1-cycle router. Full-system simulations with SPLASH-2 and PARSEC benchmarks demonstrate 27/52% and 20/59% reduction in runtime and EDP for Private/Shared L2 designs. 1. Introduction Over the last decade, computer architects have been delivering higher FLOPS/cycle by increasing the number of on-chip cores, instead of increasing the clock frequency, because of the power wall. While increasing the number of cores is not very hard (due to Moore’s Law), connecting these cores is. The reason is that more cores translates to more hops1 to get from one core to another. Every hop adds ∗ The authers acknowledge the support of DARPA UHPC, SMART LEES, and MARCO C-FAR. A special thanks to Sunghyun Park from MIT and Michael Pellauer from Intel for highly useful discussions on the SMART interconnect and pipeline respectively. 1We deﬁne hop to be the physical distance between neighboring tiles, which is typically 1-2mm [14, 15]. In this paper, 1-hop = 1mm based on place-and-route of a Freescale PowerPC e200z7 core in 45nm. an additional on-chip router (required at each hop to enable multiplexing of multiple ﬂits over shared links) along the route, which increases the latency and energy overhead of every network traversal. The equation for network latency (T ) of a packet is [11]: T = H · tr + H · tw + Tc + L/b (1) H is the number of hops, tr is the router pipeline delay, tw is the wire (between two routers) delay, Tc is the contention delay at routers, and L/b is the serialization delay for the body and tail ﬂits, i.e. time for a packet of length L to cross a channel with bandwidth b. One proposed approach to reduce this latency is topology, by using high-radix routers [13, 19, 8, 22, 34]. The idea is to reduce H , by adding explicit links between physically distant routers, thus reducing the number of routers on the route. However, this is done at the cost of thinner channels (i.e. smaller b) which increases serialization delay. Moreover, higher number of input/output ports at routers leads to increased complexity of the routing, allocation and crossbar blocks, increasing router delay tr and router power. Instead, most commercial and research multicore prototypes [15, 16, 36, 1] have opted for simpler topologies like rings and meshes to ease design (layout and veriﬁcation) and reduce router delay and energy. Network latency in such systems has been mitigated by shrinking tr to 1, using router microarchitectural and ﬂow-control techniques [24, 27, 32, 23, 25, 31, 30]. As core count increases though, H inevitably increases. Average hop counts in a k × k mesh increase linearly with k. As we design 1024 core chips [2, 28, 12, 18] for the exascale era, high hop counts will lead to horrendous on-chip network traversal latency and energy creating a stumbling block to core count scaling. How critical is on-chip latency for overall system performance? We compare three networks: (1) 3-cycle router, i.e. tr = 3 (modeled similar to Intel’s recent 48-core SCC [16]), (2) 1-cycle router, i.e. tr = 1 (the state-of-the-art in academic literature to date, described in Section 2), and (3) an ideal 1-cycle network, i.e. T = 1 + L/b (every ﬂit is magically sent from the source NIC to its destination NIC after 1-cycle with zero contention, which essentially implies that every core is 1-hop away). tw (i.e. wire delay per hop) is assumed to be 1 PARSEC trafﬁc shows 27/52% reduction in average runtime for Private/Shared, compared to a 1-cycle router. The paper is organized as follows. Section 2 describes the baseline 1-cycle router. Section 3 introduces the SMART link, and how it is embedded into a router. Section 4 demonstrates the design for a k-ary 1-Mesh, and Section 5 extends it to a k-ary 2-Mesh. Section 6 describes implementation details, up to layout. Section 7 presents our evaluations. Section 8 contrasts against prior art and Section 9 concludes. 2. Background Networks-on-Chip (NoCs) consist of shared links, with routers at crosspoints. Routers perform multiplexing of ﬂits on the links, and buffer ﬂits in case of contention. Each hop consists of a router + link traversal. A router performs the following actions [11]: Buffer Write (BW): The incoming ﬂit is buffered. Route Compute (RC): The incoming head ﬂit chooses an output port to depart from. Switch Allocation (SA): Buffered ﬂits arbitrate among themselves for the crossbar switch. At the end of this stage, there is at most one winner for every input and output port of the crossbar. VC Selection (VS): Head ﬂits that win SA reserve a VC for the next router, from a pool of free VCs [26]. The winners of SA proceed to Switch Traversal (ST) and Link Traversal (LT) to the next router. Plethora of research in NoCs over the past decade coupled with technology scaling has allowed the actions within a router to move from serial execution to parallel execution, via lookahead routing [11], simpliﬁed VC selection [26], speculative switch arbitration [31, 30], non-speculative switch arbitration via lookaheads [24, 27, 32, 23, 25] to bypass buffering and so on. This has allowed the router delay tr (Equation 1) to drop from 3-5 cycles in industry prototypes [15, 16] to 1-cycle in academic NoC-only prototypes [25, 32]. We use this state-of-the-art 1-cycle router as our baseline. ST and LT can be done together within a cycle [16, 32] giving us tw = 1. Thus our baseline incurs 2-cycles-per-hop, and is shown in Figure 2. In case of contention, ﬂits have to get buffered and could wait multiple cycles before they win SA and VS, increasing Tc , as shown at Routern+i . 3. The SMART Interconnect Adding asynchronous repeaters3 is a standard way of reducing wire delay [20, 33]. We perform place-and-route for a 128-bit repeated wire in a commercial 45nm SOI technology using Cadence Encounter. We keep increasing the length of the wire, letting the tool size the repeaters appropriately, till it fails timing closure at 1ns (i.e. 1GHz). Figure 3 shows that a place-and-routed repeated wire in 45nm can Figure 1: Impact of on-chip latency on full-system runtime for (1) and (2). We perform full-system simulations on a 64core system2 laid out as a mesh, and look at runtime across a suite of SPLASH-2 [3] and PARSEC [9] benchmarks for both a Private and a (distributed) Shared L2 conﬁguration. Figure 1 shows 26% and 52% reduction in runtime on average for (2) and (3) compared to (1) for a Private L2 design, where only L2 misses traverse the network. For a Shared L2 design, both L1 and L2 misses traverse the network, making network latency more critical, which is reﬂected by a 63% and 85% runtime reduction for (2) and (3) compared to (1). A high on-chip latency not just delays requests and responses, but also slows down the injection of other requests and responses (due to dependencies), leading to poorer throughput and overall system slowdown. This is the reason why coherence protocol designers prefer Private L2 designs, while programmers and compiler/OS designers try to map data close to sharers to minimize the average network hops. However, there is only so much that the protocol or software can do since a core has limited 1-hop neighbors. In this work, we present a solution to approach the ideal 1cycle network for any source-destination pair in a mesh. Our proposed NoC is named SMART, for Single-cycle Multi-hop Asynchronous Repeated Traversal. As the name suggests, we embed asynchronous repeaters within each router’s crossbar, and size them to drive signals up to multiple hops within a single-cycle. We optimize network latency as follows: T = (H /HPC) · tr + (H /HPC) · tw + Tc + L/b (2) where HPC stands for number of Hops Per Cycle. We reduce the effective number of hops to (cid:100)(H /HPC)(cid:101), without adding any additional physical wires in the data-path or reducing b like the high-radix router solutions do. This paper makes the following contributions: • We advocate for a single-cycle traversal across multiple routers in a network. • A single-cycle multi-mm interconnect circuit is presented, integrated into a regular mesh topology. • A network ﬂow-control mechanism is presented that enables ﬂits to setup arbitrary multi-hop paths (with turns) within a cycle, and then traverse them within a cycle. On a 64-core mesh, synthetic trafﬁc shows 5-8X reduction in average network latency, while full-system SPLASH-2 and 2Refer to Section 7 for methodology and conﬁgurations. 3A pair of inverters. Figure 2: Baseline 1-cycle Router (2-cycles-per-hop) go up to 16mm in a ns4 . We deﬁne the maximum length in mm that can be traversed in a cycle as HPCmax . Figure 3 shows a similar trend for HPCmax at 32nm and 22nm, with energy going down by 19% and 42% respectively, using the timing-driven NoC power modeling tool DSENT [35]5 . Router logic delay limits the network frequency to 12GHz at 45nm [16, 32]. Link drivers are accordingly sized to drive only 1mm (1-hop) in 0.5-1ns, before the signal is latched at the next router. SMART removes this constraint of latching signals at every hop. We exploit the positive slack in the link traversal stage by replacing clocked link drivers by asynchronous repeaters at every hop, thus driving signals H PCmax -hops within a cycle. HPCmax is a design-time parameter, which can be inferred from Figure 3. If we choose a 2mm tile size, or a 2GHz frequency, HPCmax will go down by half. Asynchronous repeaters also consume 14.3% lower energy/bit/mm than conventional clocked drivers, as shown in Figure 3, giving us a win-win. SMART is a better solution for exploiting the slack than deeper pipelining of the router with a higher clock frequency (e.g. Intel’s 80-core 5GHz 5-stage router [15]) which, even if it were possible to do, does not reduce traversal latency (only improves throughput), and adds huge power overheads due to pipeline registers. Figure 4a shows a SMART router. For simplicity, we only show Corein (Cin )6 , Westin (Win ) and Eastout (Eout ) ports. All other input ports are identical to Win , and all other output ports are identical to Eout . Each repeater has to be sized to drive not just the link, but also the muxes (2:1 bypass and 4:1 Xbar) at the next router, before a new repeater is encountered. Using the same methodology with Cadence Encounter, this reduces HPCmax to 11 at 1GHz (Section 6). Figure 4a shows the three primary components of the design: (a) Buffer Write enable (BWena ) at the input ﬂip ﬂop which determines if the input signal is latched or not, (2) 4 The sharp rise in energy past 13mm can be attributed to a limitation of the place-and-route tool, which zig-zags wires to ﬁt to a ﬁxed global grid, that is unfortunately not a multiple of M6 width, adding unnecessary wire length. A custom design can potentially go farther and with ﬂatter energy proﬁle. 5DSENT’s HPCmax projections are slightly overestimated because it does not model inter-layer via parasitics (needed to access the repeater transistors on M1 from the link on M6), which become signiﬁcant when there are many repeaters. 6Cin does not have a bypass path like the other ports because all ﬂits from the NIC have to get buffered at the ﬁrst router, before they can create SMART paths, which will be explained later in Section 4. Figure 3: Achievable HPCmax for Repeated Links at 1GHz. Wire Width: DRCmin , Wire Spacing: 3 · DRCmin , Metal Layer: M6. Repeater Spacing: 1mm Bypass Mux select (BMsel ) at the input of the crossbar to choose between the local buffered ﬂit, and the bypassing ﬂit on the link, and (3) Crossbar select (XBsel ). Figure 4b shows an example of a multi-hop traversal: a ﬂit from Router R0 traverses 3-hops within a cycle, till it is latched at R3. The crossbars at R1 and R2 are preset to connect the Win to Eout , with their BMsel preset to choose bypass over local. A SMART path can thus be created by appropriately setting BWena , BMsel , and XBsel at intermediate routers. In the next section, we describe the ﬂow control to preset these signals. 4. SMART in a k-ary 1-Mesh Table 1 deﬁnes terminology that will be used throughout the paper. We start by demonstrating how SMART works in a k-ary 1-Mesh, shown in Figure 5. Each router has 3 ports: West, East and Core7 . As shown earlier in Figure 4a, Eout _xb can be connected either to Cin_xb or Win _xb. Win_xb can be driven either by by pass, l ocal or 0, depending on BMsel . The design is called SMART_1D (since routers can be bypassed only along one dimension). The design will be extended to a k-ary 2-Mesh to incorporate turns, in Section 5. For purposes of illustration, we will assume HPCmax to be 3. 4.1. SMART-hop Setup Request (SSR) The SMART router pipeline is shown in Figure 6. A SMART-hop starts from a start router, where ﬂits are buffered. Unlike the baseline router, Switch Allocation in SMART occurs over two stages: Switch Allocation Local (SA-L) and Switch Allocation Global (SA-G). SA-L is identical to the SA stage in the conventional pipeline (described earlier in Section 2): every start router chooses a winner for each output port from among its buffered (local) ﬂits. In the next cycle, instead of the winners directly traversing the crossbar (ST), they broadcast a SMART-hop setup request (SSR) via dedicated repeated wires (which are inherently multi-drop8 ) up to HPCmax . These dedicated SSR wires are shown in Figure 5. These are l og2 (1 + HPCmax )-bits wide, and are part of the control-path. The SSR carries the length (in hops) up to which the ﬂit winner wishes to go. For instance, SSR = 2 indicates a 2-hop path request. Each ﬂit tries 7 For illustration purposes, we only show Cin , Win and Eout in the ﬁgures. 8Wire cap is an order of magnitude higher than gate cap, adding no overhead if all nodes connected to the wire receive. (a) SMART Router Microarchitecture (b) Single-cycle Multi-hop Asynchronous Repeated Traversal Example Figure 4: SMART: Single-cycle Multi-hop Asynchronous Repeated Traversal Table 1: Terminology Meaning Term HPC HPCmax SMART-hop injection router ejection router start router inter router stop router turn router local ﬂits bypass ﬂits SMART-hop Setup Request (SSR) premature stop Prio=Local Prio=Bypass SMART_1D SMART_2D Hops Per Cycle. The number of hops traversed in a cycle by any ﬂit. Maximum number of hops that can be traversed in a cycle by a ﬂit. This is ﬁxed at design time. Multi-hop path traversed in a Single-cycle via a SMART link. It could be straight, or have turns. The length of a SMART-hop can vary anywhere from 1-hop to HPCmax . First router on the route. The source NIC injects a ﬂit into the Cin port of this router. Last router on the route. This router ejects a ﬂit out of the Cout port to the destination NIC. Router from which any SMART-hop starts. This could be the injection router, or any router along the route. Any intermediate router on a SMART-hop. Router at which any SMART-hop ends. This could be the ejection router or any router along the route. Router at a turn (Win /Ein to Nout /Sout , or Nin /Sin to Wout /Eout ) along the route. Flits buffered at any start router. Flits which are bypassing inter routers. Length (in hops) for a requested SMART-hop. For example, SSR=H indicates a request to stop H -hops away. Optimization: Additional ejection-bit if requested stop router is ejection router. A ﬂit is forced to stop before its requested SSR length. Local ﬂits have higher priority over bypass ﬂits, i.e. Priority α 1/(hops_from_start_router). Bypass ﬂits have higher priority over local ﬂits, i.e. Priority α (hops_from_start_router). Design where routers along the dimension (both X and Y) can be bypassed. Flits need to stop at the turn router. Design where routers along the dimension and one turn can be bypassed. Figure 5: k-ary 1-Mesh with dedicated SSR links. Figure 6: SMART Pipeline to go as close as possible to its ejection router, hence SSR = min(HPCmax , Hremaining ). During SA-G, all inter routers arbitrate among the SSRs they receive, to set the BWena , BMsel and XBsel signals. The arbiters guarantee that only one ﬂit will be allowed access to any particular input/output port of the crossbar. In the next cycle (ST+LT), SA-L winners that also won SA-G at their start routers traverse the crossbar and links upto multiple hops till they are stopped by BWena at some router. Thus ﬂits spend at least 2 cycles (SA-L and SA-G) at a start router before they can use the switch. Flits can end up getting prematurely stopped (i.e before their SSR length) depending on the SA-G results at different routers. SSR traversal and SA-G occur serially (see Section 6 for timing implications). We illustrate all these with examples. In Figure 7, Router R2 has F l itA and F l itB buffered at Cin , and F l itC and F l itD buffered at Win , all requesting Eout . Suppose F l itD wins SAL during Cycle-0. In Cycle-1, it sends out SSRD = 2 (i.e. request to stop at R4) out of Eout to Routers R3, R4 and R5. SA-G is performed at each router. At R2, which is 0-hops away (< SSRD ), BMsel = local, XBsel = Win _xb→Eout _xb. At R3, which is 1-hop away (< SSRD ), BMsel = bypass, XBsel = Win_xb→Eout _xb. At R4, which is 2-hops away (= SSRD ), BWena = high. At R5, which is 3-hops away (> SSRD ), SSRD is ignored. In Cycle-2, F l itD traverses the crossbars and links at R2 and R3, and is stopped and buffered at R4. What happens if there are competing SSRs? In the same example, suppose R0 also wants to send F l itE 3-hops away to R3, as shown in Figure 8. In Cycle-1, R2 sends out SSRD as before, and in addition R0 sends SSRE = 3 out of Eout to R1, R2 and R3. Now at R2 there is a conﬂict between SSRD and SSRE for the Win _xb and Eout _xb ports of the crossbar. SA-G priority decides which SSR wins the crossbar. More details about priority will be discussed later Figure 7: SMART Example: No SSR Conﬂict Figure 8: SMART Example: SSR Conﬂict with Prio=Local Figure 9: SMART Example: SSR Conﬂict with Prio=Bypass in Section 4.2. For now, let us assume Prio=Local (which is deﬁned in Table 1) so F l itE loses to F l itD . The values of BWena , BMsel and XBsel at each router for this priority are shown in Figure 8. In Cycle-2, F l itE traverses the crossbar and link at R0 and R1, but is stopped and buffered at R2. F l itD traverses the crossbars and links at R2 and R3 and is stopped and buffered at R4. F l itE now goes through BW and SA-L at R2 before it can send a new SSR and continue its network traversal. A free VC/buffer is guaranteed to exist whenever a ﬂit is made to stop (see Section 4.4). 4.2. Switch Allocation Global: Priority Figure 9 shows the previous example with Prio=Bypass instead of Prio=Local. This time, in Cycle-2, FlitE traverses all the way from R0 to R3, while FlitD is stalled. Do all routers need to enforce the same priority? Yes. This guarantees that all routers will arrive at the same consensus about which SSRs win and lose. This is required for correctness. In the example discussed earlier in Figures 8 and 9, BWena at R3 was low with Prio=Local, and high with Prio=Bypass. Suppose R2 performs Prio=Bypass, but R3 performs Prio=Local, FlitE will end up going from R0 to R4, instead of stopping at R3. This is not just a misrouting issue, but also a signal integrity issue because HPCmax is 3, but the ﬂit was forced to go up to 4 hops in a cycle, and will not be able to reach the clock edge in time. Note that enforcing the same priority is only necessary for SA-G, which corresponds to the global arbitration among SA-L winners at every router. During SA-L, however, different routers/ports can still choose to use different arbiters (round robin, queueing, priority) depending on the desired QoS/ordering mechanism. Can a ﬂit arrive at a router, even though the router is not expecting it (i.e. false positive9 )? No. All ﬂits that arrive at a router are expected, and will stop/bypass based on the success of their SSR in the previous cycle. This is guaranteed since all routers enforce the same SA-G priority. Can a ﬂit not arrive at a router, even though the router is expecting it (i.e. false negative)? Yes. It is possible for the router to be setup for stop/bypass for some ﬂit, but no ﬂit 9 The result of SA-G (BWena , BMsel and XBsel ) at a router is a prediction for the null hypothesis: a ﬂit will arrive the next cycle, and stop/bypass. arrives. This can happen if that ﬂit is forced to prematurely stop earlier due to some SSR interaction at prior inter routers that the current router is not aware of. For example, suppose a local ﬂit at Win at R1 wants to eject out of Cout . A ﬂit from R0 will prematurely stop at R1’s Win port if Prio=Local is implemented. However, R2 will still be expecting the ﬂit from R0 to arrive10 . Unlike false positives, this is not a correctness issue but just a performance (throughput) issue, since some links go idle which could have potentially been used by other ﬂits if more global information were available. 4.3. Ordering In SMART, any ﬂit can be prematurely stopped based on the interaction of SSRs that cycle. We need to ensure that this does not result in re-ordering between (a) ﬂits of the same packet, or (b) ﬂits from the same source (if Pt-to-Pt ordering is required in the coherence protocol). The ﬁrst constraint is in routing (relevant to 2D topologies). Multi-ﬂit packets, and Pt-to-Pt ordered virtual networks should only use deterministic routes, to ensure that prematurely buffered ﬂits do not end up choosing alternate routes, while bypassing ﬂits continue on the old route. The second constraint is in SA-G priority. Every input port has a bit to track if there is a prematurely stopped ﬂit among its buffered ﬂits. When a SSR is received at an input port, and there is either (a) a prematurely buffered Head/Body ﬂit, or (b) a prematurely buffered ﬂit within a Pt-to-Pt ordered virtual network, the incoming ﬂit is stopped. 4.4. Guaranteeing free VC/buffers at stop routers In a conventional network, a router’s output port tracks the IDs of all free VCs at the neighbor’s input port. A buffered Head ﬂit chooses a free VCid for its next router (neighbor), before it leaves the router. The neighbor signals back when that VCid becomes free. In a SMART network, the challenge is that the next router could be any router that can be reached within a cycle. A ﬂit at a start router choosing the VCid before it leaves will not work because (a) it is not guaranteed to reach its presumed next router, and (b) multiple ﬂits at different start routers might end up choosing the same VCid. Instead, we let the VC selection occur at the stop router. Every SMART router receives 1-bit from each neighbor to signal if at least one VC is free11 . During SA-G, if an SSR requests an output port where there is no free VC, BWena is made high and the corresponding ﬂit is buffered. This solution does not add any extra multihop wires for VC signaling. The signaling is still between neighbors. Moreover, it ensures that a Head ﬂit comes into a 10 The val id -bit from the ﬂit is thus used in addition to BWena when deciding whether to buffer. 11 If the router has multiple virtual networks (vnets) for the coherence protocol, we need a 1-bit free VC signal from the neighbors for each vnet. The SSR also needs to carry the vnet number, so that the inter routers can know which vnet’s free VC signal to look at. router’s input port only if that input port has free VCs, else the ﬂit is stopped at the previous router. However, this solution is conservative because a ﬂit will be stopped prematurely if the neighbor’s input port does not have free VCs, even if there was no competing SSR at the neighbor and the ﬂit would have bypassed it without having to stop. How do Body/Tail ﬂits identify which VC to go to at the stop router? Using their injection_router id. Every input port maintains a table to map a VCid to an injection router id12 . Whenever the Head ﬂit is allocated a VC, this table is updated. The injection router id entry is cleared when the Tail arrives. The VC is freed when the Tail leaves. We implement private buffers per VC, with depth equal to the maximum number of ﬂits in the packet (i.e. virtual cutthrough), to ensure that the Body/Tail will always have a free buffer in its VC13 . What if two Body/Tail ﬂits with same injection_router id arrive at a router? We guarantee that this will never occur by forcing all ﬂits of a packet to leave from an output port of a router, before ﬂits from another packet can leave from that output port (i.e virtual cut-through). This guarantees a unique mapping from injection router id to VCid in the table at every router’s input port. What if a Head bypasses, but Body/Tail is prematurely stopped? The Body/Tail still needs to identify a VCid to get buffered in. To ensure that it does have a VC, we make the Head ﬂit reserve a VC not just at its stop router, but also at all its inter routers, even though it does not stop there. This is done from the valid , type and injection_router ﬁelds of the bypassing ﬂit. The Tail ﬂit frees the VCs at all the inter routers. Thus, for multi-ﬂit packets, VCs are reserved at all routers, just like the baseline. But the advantage of SMART is that VCs are reserved and freed at multiple routers within the same cycle, thus reducing the buffer turnaround time. 4.5. Additional Optimizations We add additional optimizations to SMART to push it towards an ideal 1-cycle network described in Section 1. Bypassing the ejection router. So far we have assumed that a ﬂit starting at an injection router traverses one (or more) SMART-hops till the ejection router, where it gets buffered and requests for the Cout port. We add an extra e ject ion-bit in the SSR to indicate if the requested stop router corresponds to the ejection router for the packet, and not any intermediate router on the route. If a router receives a SSR from H -hops away with value H (i.e. request to stop there), H < HPCmax , and the ejection-bit is high, it arbitrates for Cout port during SA-G. If it loses, BWena is made high. 12 The table size equals the number of multi-ﬂit VCs at that input port. 13 Extending this design to fewer buffers than the number of ﬂits in a packet would involve more signaling, and is left for future work. (a) k-ary 2-Mesh with SSR wires (b) Conﬂict between two (c) Fixed Priority at Nout por t of inter router. (d) Fixed Priority at Sin por t of inter router. from shaded star t router. SSRs for Nout por t. Figure 10: SMART_2D: SSRs and their SA-G priorities. Bypassing SA-L at low load. We add no-load bypassing [11] to the SMART router. If a ﬂit comes into a router with an empty input port and no SA-L winner for its output port for that cycle, it sends SSRs directly, in parallel to getting buffered, without having to go through SA-L. This reduces tr at lightly-loaded start routers to 2, instead of 3, as shown in Figure 6 for Routern+i . Multi-hop traversals within a single-cycle meanwhile happen at all loads. 4.6. Summary In summary, a SMART NoC works as follows: • Buffered ﬂits at injection/start routers arbitrate locally to choose input/output port winners during SA-L. • SA-L winners broadcast SSRs along their chosen routes, and each router arbitrates among these SSRs during SA-G. • SA-G winners traverse multiple crossbars and links asynchronously within a cycle, till they are explicitly stopped and buffered at some router along their route. In a SMART_1D design with both ejection and no-load bypass enabled, if HPCmax is larger than the maximum hops in any route, a ﬂit will only spend 2 cycles in the entire network in the best case (1-cycle for SSR and 1-cycle for ST+LT all the way to the destination NIC). 5. SMART in a k-ary 2-Mesh We demonstrate how SMART works in a k-ary 2-Mesh. Each router has 5 ports: West, East, North, South and Core. 5.1. Bypassing routers along dimension We start with a design where we do not allow bypass at turns, i.e. all ﬂits have to stop at their turn routers. We re-use SMART_1D described for a k-ary 1-Mesh in a k-ary 2-Mesh. The extra router ports only increase the complexity of the SA-L stage, since there are multiple local contenders for each output port. Once each router chooses SA-L winners, SA-G remains identical to the description in Section 4.1. The Eout , Wout , Nout and Sout ports have dedicated SSR wires going out till H PCmax along that dimension. Each input port of the router can receive only one SSR from a router that is H -hops away. The SSR requests a stop, or a bypass along that dimension. Flits with turning routes perform their traversal one-dimension at a time, trying to bypass as many routers as possible, and stopping at the turn routers. 5.2. Bypassing routers at turns In a k-ary 2-Mesh topology, all routers within a H PCmax neighborhood can be reached within a cycle, as shown in Figure 10a by the shaded diamond. We now describe SMART_2D which allows ﬂits to bypass both the routers along a dimension and the turn router(s). We add dedicated SSR links for each possible XY/YX path from every router to its H PCmax neighbors. Figure 10a shows that the Eout port has 5 SSR links, in comparison to only one in the SMART_1D design. During the routing stage, the ﬂit chooses one of these possible paths. During the SA-G stage, the router broadcasts one SSR out of each output port, on one of these possible paths. We allow only one turn within each H PCmax quadrant to simplify the SSR signaling. SA-G Priority. In the SMART_2D design, there can be more than one SSR from H -hops away, as shown in the example in Figure 10b for router R j . R j needs a speciﬁc policy to choose between these requests, to avoid sending false positives on the way forward to Rk . Section 4.2 discussed that false positives can result in misrouted ﬂits or ﬂits trying to bypass beyond H PCmax , thus breaking the system. To arbitrate between SSRs from routers that are the same number of hops away, we choose Straight > Left Turn > Right Turn. For the inter router R j in Figure 10b, the SSR from Rm will have higher priority (1_0) over the one from Rn (1_1) for the Nout port, as it is going straight, based on Figure 10c. Similarly at Rk , the SSR from Rm will have higher priority (2_0) over the one from Rn (2_1) for the Sin port, based on Figure 10d. Thus both routers R j and Rk will unambiguously prioritize the ﬂit from Rm to use the links, while the ﬂit from Rn will stop at Router R j . Any priority scheme will work as long as every router enforces the same priority. 6. SMART Implementation In this section, we describe the implementation details of SMART_1D and SMART_2D designs, and discuss overheads. All numbers are for a k-ary 2-Mesh, i.e. the crossbar has 5-ports (with u-turns disallowed). (a) Implementation of SA-G at Win and Eout (Figure 4a) for SMART_1D (b) Energy/Access (i.e. Activity = 1) for each bit sent Figure 11: SMART Implementation The SMART data-path, shown earlier in Figure 4, is modeled as a series of 128-bit 2:1 mux (for bypass) followed by a 4:1 mux (crossbar), followed by a 128-bit 1mm link. The SMART control-path consists of H PCmax -hops repeated wire delay (SSR traversal), followed by logic gate delay (SA-G). In SMART_1D, each input port receives one SSR from every router up to H PCmax -hops away in that dimension. The logic for SA-G for Prio=Local in a SMART_1D design at the Win and Eout ports of the router is shown in Figure 11a14 . The input and output signals correspond to the ones shown in the router in Figure 4a15 . In SMART_2D, all routers that are H -hops away, H ∈[1, H PCmax ], together send a total of (2 × HPCmax − 1) SSRs to every input port. SA-GSSR_ priorit y_arbit er is similar to Figure 11a in this case and chooses a set of winners based on hops, while SA-Gout put _ port disambiguates between them based on turns, as discussed earlier in Section 5.2. We choose a clock frequency of 1GHz based on SA-L critical path in the baseline 1-cycle router at 45nm [32]. We design each of the SMART components in RTL, run it through synthesis and layout for increasing H PCmax values, till timing fails at 1GHz. This gives us energy and area numbers for every H PCmax design point. We incorporate these into energy and area numbers for the rest of the router components from DSENT [35]. Figure 11b plots the energy/bit/hop for accessing each component. For instance, if a ﬂit wins SA-L and SA-G and traverses a SMART-hop of length 4 in an H PCmax=8 design, the energy consumed will be ESA−L + 8 · ESSR + 4 · ESA−G + Ebuf _rd + 4 · EXbar + 4 · ELink + Ebu f _wr . The SMART data-path is able to achieve a H PCmax of 11. The extra energy consumed by the repeaters for driving the bypass and crossbar muxes is part of the Xbar component in Figure 11b, and increases comparatively insigniﬁcantly till about H PCmax=8, beyond which it shows a steep rise, consuming 3X of the baseline Xbar energy at H PCmax=11. The 14 The implementation of Prio=Bypass is not discussed but is similar. 15 To reduce the critical path, BWena is relaxed such that it is 0 only when there are bypassing ﬂits (since the ﬂit’s valid-bit is also used to decide when to buffer), and BMsel is relaxed to always pick local if there is no bypass. X Bsel is strict and does not connect an input to an output port unless there is a local or SSR request for it. total data-path (Xbar+Link) energy for H PCmax=11 goes up by 35fJ/bit/hop, compared to the baseline. However, compared to the buffer energy (110fJ/bit/hop) that will be saved with the additional bypassing brought about by longer H PCmax , and coupled with additional network latency savings along with further reduction of data-path energy per bit as technology scales, we believe it will be worthwhile to go with higher H PCmax as we scale to hundreds or a thousand cores. The repeaters do not add any area overhead since the crossbar area is wire dominated. SMART_1D’s control-path is able to achieve a H PCmax of 13 (890ps SSR, 90ps SA-G). But the overall H PCmax gets limited to 11 by the data-path. SA-G adds less than 1% of energy or area overhead. SMART_2D’s control-path is able to achieve a H PCmax of 9 (620ps SSR, 360ps SA-G), at which point the energy and area overheads go up to 8% and 5% respectively, due to the quadratic scaling of input SSRs with H PCmax . However, not all the input SSRs are likely to be active every cycle. The total number of SSR-bits entering an input port are of O(HPCmax · l og2 (HPCmax )) and O(HPCmax l og2 (HPCmax )) in SMART_1D and SMART_2D respectively. But these do not affect tile area. However, the SSRs add energy overheads due to HPCmax -mm signaling whenever a SSR is sent. Based on the energy results, we choose H PCmax=8 for both SMART_1D and SMART_2D for our evaluations. For SMART_1D, H PCmax=8 allows bypass of all routers along the dimension and the ejection, in our target 8-ary 2-Mesh. 2 · 7. Evaluation We use the GEMS [29] + Garnet [5] infrastructure for all our evaluations, which provides a cycle-accurate timing model. Full-system simulations use Wind River Simics [4]. Network energy is calculated using DSENT [35] and our place-and-route results from Section 6. Our target system is shown in Table 2. The baseline design in all our runs is a state-of-the-art 1-cycle router described earlier in Section 2. All SMART designs are named as SMART-H PCmax _1D/2D. Prio=Local is assumed unless explicitly mentioned. Table 2: Target System and Conﬁguration On-chip Network Process Technology Vd d Frequency Link Length Synthetic Trafﬁc Virtual Channels Full-system Trafﬁc 45nm 1.0 V 1.0 GHz 1mm Topology Router Ports Routing Flit Width 8-ary 2-Mesh 5 XY 128-bit 12 [1-ﬂit/VC] Processors 64 in-order SPARC L1 Caches Private 32kB I&D L2 Caches Private/Shared 1MB per core Cache Coherence MOESI distributed directory Virtual Networks 3 (req, fwd, resp) Virtual Channels 4 (req), 4 (fwd) [1-ﬂit/VC], 4 (resp) [5-ﬂit/VC] 7.1. Synthetic Trafﬁc 7.1.1. SMART across different trafﬁc patterns.We start by running SMART with synthetic trafﬁc patterns. In the interest of space, we only show three of these in Figure 12. We compare 3 SMART designs: SMART-8_1D and SMART8_2D (which are both achievable designs as discussed in Section 6), and SMART-15_2D which reﬂects the best that SMART can do in an 8×8 Mesh (with maximum possible hops = 15). We inject 1-ﬂit packets to ﬁrst understand the beneﬁts of SMART without secondary effects due to ﬂit serialization, and VC allocation across multiple routers etc. For the same reason, we also give enough VCs (12, derived empirically) to allow both the baseline and SMART to be limited by links, rather than VCs for throughput. The striking feature about SMART from Figure 12 is that it pushes low-load latency to 4 and 2 cycles, for SMART_1D and SMART_2D respectively, across all trafﬁc patterns, unlike the baseline where low-load latency is a function of the average hops, thus truly breaking the locality barrier. SMART-8_2D achieves most of the beneﬁt of SMART15_2D for all patterns, except Bit Complement, since average hop counts are ≤ 8 for an 8×8 Mesh. 7.1.2. SA-G priorities.We study the effects of priority in Figure 13a for the best possible 1D and 2D SMART designs. While both priority schemes perform identically at very low-loads, Prio=Bypass has a sudden throughput degradation at an injection rate of about 44-48% of network capacity. Intuitively, we would expect Prio=Bypass to be better than Prio=Local as it allows for longer bypass paths, and avoids unnecessary stopping and buffering of ﬂits already in ﬂight. Moreover, it is often the priority scheme used in non-speculative 1-cycle router designs [24, 27] when choosing between a lookahead and a local buffered ﬂit. However, for SMART, where multiple allocations are occurring in the same cycle, it suffers from a unique problem, highlighted in Figure 13b. In this example, Router’s R0, R1 and R3 send SSRs up to R2, R4 and R5 respectively, in Cycle-1. In a Prio=Local scheme, R0’s SSR would lose at R1, and R1’s SSR would lose at R3, leading to the traversals shown in Cycle-2. For Prio=Bypass, R0’s SSR will win at R1, and the corresponding ﬂit will be able to go all the way to its stop router R2. However, even though R1’s SSR lost SA-G at its start router, it wins over R3’s SSR at R3, preventing R3 from sending its own ﬂit. This cascading effect can continue, leading to poor link utilization, and heavy throughput loss. This effect is reﬂected in the percentage of false negatives (cases where a router was expecting a ﬂit but no ﬂit came) going up to 25-40% in Prio=Bypass, killing its throughput, as opposed to less than 10% in Prio=Local. On the plus side, Prio=Bypass always creates SMART-hops with high H PCs, since a ﬂit that starts only stops at its requested stop router, or at the turn router in this priority scheme. This can be seen in Figure 13c where SMART-8_1D_Prio=Bypass achieves an average H PC of 3, while SMART-15_2D_Prio=Bypass maintains an H PC of 4-5. Prio=Local, on the other hand, forces the achievable H PCs to drop to 1 at high loads. 7.1.3. Impact of H PCmax . Next we study the impact of H PCmax on performance. We plot the average ﬂit latency for BC trafﬁc (which has high across-chip communication) for H PCmax from 1 to 12, across 1D and 2D in Figure 14a. SMART-1_1D is identical to the baseline_1-cycle router (as it does not need SA-G). H PCmax of 2 itself gives a 1.8X lowload latency reduction, while 4 gives a 3X reduction. These numbers indicate that even with a faster clock, say 2.25GHz, which will drop H PCmax to 4, a SMART-like design is a better choice than a 1-cycle router. It should also be noted that as we scale to smaller feature sizes, cores shrink while die sizes remain unchanged, so the same SMART interconnect length will translate to larger H PCmax . Adding SMART_2D, and increasing H PCmax to 12 pushes low-load latency close to a 2-cycles: an 8.4X reduction over the baseline. This result highlights that a heavily-pipelined higher frequency baseline can only match SMART if it runs at 8.4GHz. 7.1.4. Impact of multi-ﬂit packets.SMART locks an input and output port till all ﬂits of a packet leave for virtual cut-through (Section 4.4). This leads to a poorer switch allocation compared to the baseline which implements ﬂitby-ﬂit wormhole switching with VCs. Figure 14b evaluates SMART with UR trafﬁc with all packets having 5-ﬂits (a worse case adversarial trafﬁc scenario). We see that SMART achieves its peak throughput with 4-6 VCs, but shows 11% lower throughput than the baseline, even with 12 VCs. 7.1.5. SMART on a 16×16 mesh.Figure 14c plots the performance of SMART on a 256-core mesh with UR trafﬁc. SMART scales well, with H PCmax=4 lowering network latency from 23 to 6-7 cycles at low loads. SMART-11_1D and SMART-9_2D lower it even further to 3-4 cycles. SMART also gives a 12% throughput improvement. (a) Uniform Random (UR) (Avg Hops = 5.33) (b) Bit Complement (BC) (Avg Hops = 8) (c) Transpose (TP) (Avg Hops = 6) Figure 12: SMART with synthetic trafﬁc (a) Average Network Latency (b) Throughput Loss in Prio=Bypass (c) Average achievable HPC Figure 13: Prio=Local vs Prio=Bypass for Uniform Random Trafﬁc. (a) Impact of H PCmax (Bit Complement) (b) Impact of 5-ﬂit packets (Uniform Random) (c) 256-core (Uniform Random) Figure 14: Features of SMART Figure 15: SMART vs Flattened Butterﬂy (Uniform Random) 7.1.6. Comparison with High-Radix Topology. We compare SMART with a Flattened Butterﬂy [22] topology. Each FBﬂy router has dedicated single-cycle links to every other node in that dimension (7 ports per direction + NIC port, i.e. radix-29). We assume that the router delay is 1-cycle. This is a very aggressive assumption, especially because the SA stage needs to perform 22:1 arbitrations. All high-radix routers assume > 4-cycle pipelines [22, 21, 34]. We use 8VCs per port with virtual cut-through in both SMART and FBﬂy (thus giving more buffer resources to FBﬂy). In Figure 15, we plot three conﬁgurations where the total number of wires, i.e. Bisection Bandwidth (BB), of the FBﬂy is 1x, 3.5x and 7x that of SMART (leading to 7-ﬂits, 2-ﬂits and 1-ﬂit per packet respectively for 128-bit packets). At BB=1x, FBﬂy loses both in latency and throughput due to heavy serialization delay. At BB=3.5x, FBﬂy can match SMART in throughput. Despite an aggressive 1-cycle router, at BB=7x the best case latency for FBﬂy is 6 cycles (2 at injection, 2 at turning, and 2 at ejection router) as compared to 4 and 2 for SMART-1D and SMART-2D respectively. The radix29 FBﬂy_BB=3.5x router, modeled in DSENT [35], incurs an area, dynamic power (at saturation) and leakage power overhead of 8.6x, 1.5x and 10x respectively over SMART. If we are willing to use N times more wires, a better solution would be to just have N meshes, each with SMART, so that fast latency is achieved in addition to scalable bandwidth. 7.2. Full-system Trafﬁc We evaluate the parallel sections of SPLASH-2 [3] and PARSEC [9] for both Private and Shared L2. Each run consists of 64 threads of the application running on our CMP. We run 5-10 times with different random seeds to capture variability in parallel workloads [6], and average the results. 7.2.1. Performance Impact.Figure 16 shows that SMART8_1D and SMART-8_2D lower application runtime by 26% and 27% respectively on average, for a Private L2, which is only 8% away from an ideal 1-cycle network. The runtime reduction goes up to 49% and 52% respectively with a Shared L2 design, which is 9% off from an ideal 1-cycle network. SMART-15_2D does not give any signiﬁcant runtime beneﬁt over SMART-8_2D. (a) Private L2 (b) Shared L2 Figure 16: Full-system application runtime with SMART because of a lower runtime. The EDP goes down by up to 59%. SMART with Prio=Local consumes 18-46% higher energy in the buffers than both SMART with Prio=Bypass and the baseline (which also prioritizes incoming ﬂits over already buffered local ﬂits in SA to reduce buffering), since Prio=Bypass, by deﬁnition, reduces the number of times ﬂits need to stop and get buffered. SA-G energy contributes less than 1% of network energy for SMART_1D, and goes up to about 10% for SMART_2D. All these ups and downs are however negligible when we also consider leakage. We observed leakage to contribute more than 90% of the total energy, since the network activity is very low in full-system scenarios16 . However, even with high leakage, the total network power was observed to be about 3W for both baseline and SMART, while chip power budgets are usually about 100W. Thus the energy overheads of SMART are negligible. 8. Related Work High-radix routers. High-radix router designs such as CMesh [8], Fat Tree [11], Flattened Butterﬂy [22], BlackWidow [34], MECS [13], Clos [19] are topology solutions to reduce average hop counts, and advocate adding physical express links between distant routers. These express Pt-to-Pt links can be further engineered for lower delay with advanced signaling techniques like equalization [20] and capacitively-driven links [24]. Each router now has > 5 ports, and channel bandwidth (b) is often reduced proportionally to have similar buffer and crossbar area/power as a mesh (radix-5) router. More resources however imply a complicated routing and Switch+VC allocation mechanism, with a hierarchical SA and crossbar [21], increasing router delay tr to 4-5 at the routers where ﬂits do need to stop. The pipeline optimizations described earlier are hard to implement here. These designs also complicate layout since multiple Pt-to-Pt global wires need to span across the chip. Moreover, a topology solution only works for certain trafﬁc, and incurs higher latencies for adversarial trafﬁc (such as near neighbor) because of higher serialization delay. 16 Leakage could be reduced by aggressive power gating solutions, which itself is a research challenge. Figure 17: Impact of H PCmax and Priority Figure 18: Total Network Dynamic Energy 7.2.2. Impact of H PCmax and priority. Figure 17 sweeps through H PCmax and SA-G priority, and plots the normalized runtime and achieved H PC, on average across all the benchmarks. Since these full-system trafﬁc fall in the lower end of the injection rates in the synthetic trafﬁc graphs, Prio=Bypass performs almost as well as Prio=Local, except at low H PCmax in a Shared L2. H PCmax of 4 sufﬁces to achieve most of the runtime savings. 7.2.3. Total Network Energy.Figure 18 explores the energy trade-off of SMART by plotting the total dynamic energy of the network consumed for running the benchmarks to completion, on average across all the benchmarks. For Private L2, the dynamic energy for SMART goes up by 1012% across designs primarily due to the data-path, though the overall EDP goes down by 20%. For Shared L2, the dynamic energy goes down by 6-21% across the designs, In contrast, SMART provides the illusion of dedicated physical express channels, embedded within a regular mesh network, without having to lower the channel bandwidth, or increase the number of router ports. Asynchronous NoCs. Asynchronous NoCs [10, 7] have been proposed for the SoC domain for deterministic trafﬁc. Such a network is programmed statically to preset contention-free routes for QoS, with messages then transmitted across a fully asynchronous NoC (routers and links). Instead, SMART couples clocked routers with asynchronous links, so the routers can perform fast cycle-by-cycle reconﬁguration of the links, and thus handle general-purpose CMPs with non-deterministic trafﬁc and variable contention scenarios. Asynchronous Bypass Channels [17] target chips with multiple clock domains across a die, where each hop can incur signiﬁcant synchronization delay. They aim to remove this synchronization delay. This leads them to propose sending a clock signal with the data so that the data can be latched correctly at the destination router. Besides this difference in link architecture, the different goals also lead to distinct NoC architectures. Due to the multiple clock domains, ABC needs to buffer/latch ﬂits at every hop speculatively, discarding them thereafter if ﬂits successfully bypassed. Also, the switching between bypass and buffer modes cannot be done cycle-by-cycle, which increases latency. In contrast, SMART targets a single clock domain across the entire die, so SSRs can be sent in advance to avoid latching ﬂits at all along a multi-hop path and allow routers to switch between bypass and buffer modes each cycle. 9. Conclusion Aggressive NoC pipeline optimizations have been able to lower router delays to just 1-cycle. However, this is not good enough for large networks with multi-hop paths. The solution of adding explicit fast physical channels to bypass routers comes with its own set of problems in terms of layout complexity, area and power. We present SMART, a solution to traverse multi-hop paths within a single-cycle, by virtually bypassing all routers along the route, without adding any physical channels on the data-path. This work opens up a plethora of research opportunities in circuits, NoC architectures and many-core architectures to optimize and leverage SMART NoCs. We see SMART paving the way for localityoblivious CMPs, easing the burden on coherence protocol and/or software from optimizing for locality. "
2014,QORE - A fault tolerant network-on-chip architecture with power-efficient quad-function channel (QFC) buffers.,"Network-on-Chips (NoCs) are quickly becoming the standard communication paradigm for the growing number of cores on the chip. While NoCs can deliver sufficient bandwidth and enhance scalability, NoCs suffer from high power consumption due to the router microarchitecture and communication channels that facilitate inter-core communication. As technology keeps scaling down in the nanometer regime, unpredictable device behavior due to aging, infant mortality, design defects, soft errors, aggressive design, and process-voltage-temperature variations, will increase and will result in a significant increase in faults (both permanent and transient) and hardware failures. In this paper, we propose QORE - a fault tolerant NoC architecture with Quad-Function Channel (QFC) buffers. The use of QFC buffers and their associated control (link and fault controllers) enhance fault-tolerance by allowing the NoC to dynamically adapt to faults at the link level and reverse propagation direction to avoid faulty links. Additionally, QFC buffers reduce router power and improve performance by eliminating in-router buffering. Our simulation results using real benchmarks and synthetic traffic mixes show that QORE improves speedup by 1.3× and throughput by 2.3× when compared to state-of-the art fault tolerant NoCs designs such as Ariadne and Vicis. Moreover, using Synopsys Design Compiler, we also show that network power in QORE is reduced by 21% with minimal control overhead.","QORE: A Fault Tolerant Network-on-Chip Architecture with Power-Efﬁcient Quad-Function Channel (QFC) Buffers Dominic DiTomaso†, Avinash Kodi†, and Ahmed Louri‡, †Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701 ‡Electrical and Computer Engineering, University of Arizona, Tucson, AZ 85721 dd292006@ohio.edu, kodi@ohio.edu, louri@email.arizona.edu Abstract—Network-on-Chips (NoCs) are quickly becoming the standard communication paradigm for the growing number of cores on the chip. While NoCs can deliver sufﬁcient bandwidth and enhance scalability, NoCs suffer from high power consumption due to the router microarchitecture and communication channels that facilitate inter-core communication. As technology keeps scaling down in the nanometer regime, unpredictable device behavior due to aging, infant mortality, design defects, soft errors, aggressive design, and process-voltage-temperature variations, will increase and will result in a signiﬁcant increase in faults (both permanent and transient) and hardware failures. In this paper, we propose QORE - a fault tolerant NoC architecture with Quad-Function Channel (QFC) buffers. The use of QFC buffers and their associated control (link and fault controllers) enhance fault-tolerance by allowing the NoC to dynamically adapt to faults at the link level and reverse propagation direction to avoid faulty links. Additionally, QFC buffers reduce router power and improve performance by eliminating in-router buffertrafﬁc mixes show that QORE improves speedup by 1.3× and ing. Our simulation results using real benchmarks and synthetic throughput by 2.3× when compared to state-of-the art fault tolerant NoCs designs such as Ariadne and Vicis. Moreover, using Synopsys Design Compiler, we also show that network power in QORE is reduced by 21% with minimal control overhead. I . IN TRODUC T ION The shrinking of transistor sizes has enabled the remarkable growth in the number of cores that can be integrated within a single chip (called Chip Multiprocessors (CMPs)). As the number of cores continue to scale and conventional on-chip bus-based communications approach their limits, architects were urged to consider other scalable communication strategies. Network-on-Chips (NoCs) have emerged as the de facto communication paradigm by offering scalability through a modular design [1], [2]. In NoCs, segments of links are connected via routers in order to overcome global wire delays and scalability requirements. However, the combination of links and routers incur a power and area expense which adversely affects NoC performance. Extensive power optimization techniques have been used to mitigate the NoC power consumption. The NoC of Intel’s 80-core TeraFlops chip [3] consumes 28% of the total tile power using simple cores, whereas the NoC in the more recent Intel 48-core SCC [4] consumes 10% of the tile power using regular cores. Power optimizations of the NoC fabric is a critical piece of the puzzle to sustain and continue the drastic growth in CMP performance. A typical NoC hardware consists of routers and communication channels connecting the routers. The router is comprised of input/output ports, buffers, routing logic, and a crossbar connecting input ports to output ports for packet routing. Research has shown that router buffers are responsible for 46% of router power [5] and 30% of router area [6]. This has motivated architects to implement buffer optimization techniques such as elastic buffering [7], [8], [9] and bufferless routing [10], [11]. By completely eliminating buffers and implementing bufferless routing, recent work has reduced the average network energy by 40% [10]. Buffers have been moved from the router to the channels by replacing repeaters on the channel with either ﬂip-ﬂops called elastic buffers [8] or tri-state repeaters called channel buffers [9]. These channel buffers can store packets when the register buffers are congested or propagate data forward when necessary, thereby mitigating power and area penalties associated with router buffers. Since the buffers have been moved to the channels, hard errors in the channel buffers can cause complete failure, impeding communication between routers. Researchers have been tackling channel failure in NoCs [12], [13], [14], [15], [16], [17], [18], [19]. Recently, the fault tolerant Ariadne network [12] overcame channel faults by reconﬁguring packet routing to move around failures. Using up*/down* routing and a series of ﬂag broadcasts, the two unidirectional links between routers were dynamically assigned as up or down to create a tree network that avoid faulty channels. Another fault tolerant NoC called Vicis [13] routed around faults and placed turn restrictions at routers to avoid deadlocks. The BulletProof architecture [20] concentrates on the router (not the channels) and provides efﬁcient fault tolerance schemes for routers to overcome transient and permanent faults. With the increasing number of cores, NoCs must manage the communication demands, especially when faults are present. Since NoCs are designed to handle peak trafﬁc loads, many communication channels can go under-utilized when network load is high or the workload is unbalanced. Recent research on NoC performance has tackled above mentioned problems using techniques such as reversibility or coding schemes [21], [22], [23], [24], [25], [26]. Hesse et al. propose a bandwidthadaptive router (BAR) that aims to take advantage of these under-utilized links with bidirectional, adaptive channels [22]. These bidirectional channels adapt channel bandwidth at a ﬁne-granularity according to network trafﬁc demands. Research has shown that channel reversibility can achieve higher throughput and lower average packet latency in NoCs. In this paper, we propose QORE - a fault tolerant network-on-chip architecture with power-efﬁcient QuadFunction Channel (QFC) buffers. We use forward and backward propagation in channel buffers to simultaneously target both power reduction as well as fault tolerance. Utilizing these QFC buffers, we differ from previous work in that we create reversible channel buffers with multiple functionalities: on-demand data storage, on-demand forward data propagation, and backward data propagation. On-demand data storage enables communication channels to act as buffers and store data when the network load is low and function as repeaters when the network load is high. QFC buffers simplify buffering because packets can be routed straight from the router input channel to the crossbar without being buffered in the router as conventionally done in baseline NoC routers. Often, the trafﬁc patterns of real applications leave some channels under-utilized which can waste bandwidth. We use QFCs to improve performance by adapting them to trafﬁc demands, thereby, efﬁciently utilizing network resources. In addition to improved performance, QORE uses these QFCs to also increase NoC fault tolerance. QORE avoids non-minimal paths around faults by reversing non-faulty channels to allow back and forth communication between routers. We design controllers to reverse links on a router by router basis to improve performance and propose a backup ring network to further enhance reliability in the presence of faults. While prior research has analyzed channel buffers for reducing power in routers with marginal performance penalty and channel reversibility primarily for improving performance, to be best of our knowledge no prior work has examined the use of channel buffers and channel reversibility to simultaneously tackle the challenging issues of power, performance and fault-tolerance. The proposed QORE architecture attempts to address three issues of power, performance and fault-tolerance in a cohesive manner. The major contributions of this work include: • Quad-Function Channel (QFC) Buffers: We design channel buffers that can dynamically function as (1) forward repeaters, (2) backward repeaters, (3) forward buffers, and (4) backward buffers. This enables signiﬁcant power reduction with no router buffers while enabling circuits to both improve performance and reliability. • Improved Performance: We use our QFC concept to enhance network performance by dynamically monitoring channel utilization and adapting to network trafﬁc. QORE is able to maintain similar throughput and latency to the high performance BAR network [22]. • Fault Tolerance: Using the proposed QFC buffers and a backup ring network, we design a fault tolerant network that can adapt to faults with minimum control overhead. Our QORE network is able to handle faults better than the Ariadne and Vicis networks while improving throughput by 2.3×, reducing network power by 21%, and improving speedup by approximately 1.3× on average when running real applications (PARSEC [27], SPEC CPU2006 [28], and SPLASH-2 [29] benchmarks). I I . MOT IVAT ION In this paper, we focus on two important concerns in NoCs while also maintaining network performance: high power dissipation and declining reliability. As previously mentioned, buffers consume a major portion of the router power. This has been the key concern that has motivated researchers to develop novel ideas such as bufferless networks [10], [11], dynamic VC allocation [30], and elastic or channel buffers [8], [9]. Buffers consume signiﬁcant dynamic power when trafﬁc load is high as well as static power due to leakage. Figure 1(a-b) shows the total power breakdown (in mW) for a 5x5 router from Synopsys Design Compiler using the TSMCLPBWP 40 nm technology library with a nominal supply voltage of 1.0 V and an operating frequency of 2 GH z . The dynamic power breakdown in Figure 1(a) shows that buffers consume 33% of router power (buffers+crossbar). With the same amount of buffer space, channel buffers can lower dynamic power by 90%. Figure 1(b) shows the leakage power breakdown of the router components in μW . As shown, the leakage power of the buffers consume 68% of the total router leakage power (buffers+crossbar). Channel buffers dissipate more leakage power than the register buffers; however, this increase is compensated by the very low dynamic power. Clearly, high buffer power is a problem in NoCs that needs to be addressed. The next major concern in NoCs is reliability. The extreme shrinking of transistor feature sizes has made NoCs vulnerable to failures and data corruption. To examine the number of link faults in an NoC, a fault model was used which was similar to the model used in [12] in which a router design consists of 20,413 gates. Faults were injected randomly and weighted by the size of the gates. Therefore, gates with a larger number of transistors have a higher probability of failing. Figure 1(c) shows the number of faulty links caused by gate failures for reversible channel buffers (explained in Section III), non-reversible channel buffers, and conventional links without channel buffers. Non-reversible channel buffers are less reliable than conventional links due to the extra two transistors added to each link. Reversible channel buffers are even less reliable because of the eight additional transistors as explained in Section IV-A. Therefore, robust fault tolerant techniques are even more critical when using channel buffers. In addition to power and fault tolerance, high network performance is another concern in NoCs. Looking at a NoC router, the amount of trafﬁc entering and leaving the router ) W m ( r e w o P 140 120 100 80 60 40 20 0 ) W µ ( r e w o P 20 15 10 5 0 Crossbar Buffer Channel Buffer (a) Dynamic Power  Crossbar Buffer Channel Buffer (b) Leakage Power  s t l u a F k n L i 50 40 30 20 10 0 ) % ( Rev. Channel Buf. Channel Buf. Conv. Links n o i t a z i l i t U l e n n a h 0 50 Gate Failures  C 100 (c) Link Failures  6 5 4 3 2 1 0 i t t n n n n u u u u o o o o t t i i i i t t n n n n u u u u o o o o t t i i i +x -x +y FMM  (d) Channel Utilization  -y +x -x +y -y blackscholes  Fig. 1: (a-b) Dynamic and Leakage power of buffers, (c) link failures with and without channel buffers, and (d) link utilization of a router. will be similar when averaged across the whole application. However, due to dynamic trafﬁc patterns in NoC applications, there will be period of time where the majority of trafﬁc will be either entering or leaving the router. This unbalanced trafﬁc can cause certain links to become under-utilized during certain epochs. Figure 1(d) shows the link utilization of a router in a 64 core network for two real applications. Each side of the router (+x, -x, +y, and -y) has a link going in and out. For the applications FMM and blackscholes [27], [29], many links are under-utilized, thereby, wasting bandwidth. For example, on the +x side of the router, the ”in” channel utilization is approximately double the ”out” channel utilization. On other links, the ”in” utilization is much lower than the ”out” utilization. Using reversibility, links can change direction providing bandwidth where needed. Channel buffers can reduce dynamic power while marginally increasing leakage power; and reversible channel buffers could maximize resource utilization and improve execution time, but would need fault tolerant techniques to overcome the higher fault rates observed in channel buffers. I I I . QUAD -FUNC T ION CHANN E L BU FF ER S In this section, we will explain the circuit and implementation details of our proposed QFC buffers. Channel buffers have been shown to eliminate router buffer power by moving storage to the channels with the side beneﬁt of reducing the area overhead with marginal performance penalty [9], [8]. In this work, we uniquely modify the previously proposed channel buffers to function as bidirectional channel buffers with similar advantages of reduced power while providing ondemand storage. Figure 2(a) shows two physical channels with four channel buffer stages per channel. The inset shows a conventional channel buffer which uses four transistors and a release (rel) control line to store or propagate packets in one direction. The working of channel buffers to either store or propagate packets based on router congestion and receive signals via a control block has been discussed previously [9]. The proposed reversible channel buffer circuit is shown in Figure 2(b). By adding eight transistors to act as four transmission gates, the channel buffers can propagate packets in both directions in addition to storage. The four transmission gates are controlled by the reverse signal (rev ) sent from the router. A table showing all possible functions of the reversible channel buffer based on the inputs rel and rev are also shown in Figure 2(b). Figure 2(c) shows various combinations of reversible channel buffer functionalities; either as on-demand storage or repeater, and with data propagating either in forward or backward directions. • Forward Buffer: When rel=0 and rev=0 data can be stored in the forward direction (left to right). The data is cut off from Vdd and GN D and the data is stored on the capacitance of the transistors. • Backward Buffer: When rel=0 and rev=1 data can be stored in the backward direction (right to left). Again, the data is cut off from Vdd and GN D and the data is stored on the capacitance of the transistors. • Forward Propagation: When rel=1 and rev=0 data can propagate forward. The transistors connected to Vdd and GN D are enabled to allow propagation and the forward propagation transmission gates are also enabled. • Backward Propagation: When rel=1 and rev=1 data can propagate backward. Again, the transistors connected to Vdd and GN D are enabled to allow propagation and now the backward propagation transmission gates are enabled. We show four functions of our QFC for high network loads (forward and backward buffers) and for low network loads (forward and backward propagation). When our QFCs act as buffers, the capacitance of the transistors must be large enough to store the data for many cycles. Figure 3 shows the discharge time of a channel buffer implemented with 130 nm transistors using the Virtuoso Analog Design Environment from the Cadence tools. As shown, the discharge time of the channel buffers is in the magnitude of milliseconds which corresponds to millions of clock cycles with a 1 GHz clock. IV. QORE ARCH I T EC TUR E In this section, we will describe the QORE architecture including reversibility, the design and operation of the fault tolerant network, the details of the fault and link controllers, the router microarchitecture, and proof of deadlock-freedom. A. QFC without Faults Conventional routers, that use virtual channels (VCs) and ﬁxed connections between routers, can become a bottleneck if there is high trafﬁc in any direction. To reduce the buffering bottleneck, QORE uses our reversible channel buffers to dynamically allocate buffers to adapt to trafﬁc patterns. Figure 4(b) shows the links between routers in QORE. In order to have the same amount of buffering as a conventional 4 VC/input router, we place a set of N =4 links between routers                   rev  rev’  rev’  rev  rel  rev’  rev  rev  rev’  rel rev Function  0  0  Store  0  1  Store  1  0  Forward  1  1 Backward  Reversible channel buffer  (b)  rel  Conventional  channel buffer  (a)  off  rev=0  off  rev’=1  rev’=1  rev=0  off  rel=0  rev’=1  rev=0  rev’=1  rev=0  off  off  rev’=0  rev=1  rev=1  off  rev’=0  off  rev=1  rev’=0  rev’=0  rev=1  off  rel=0  Forward Buffer (rel=0, rev=0)  Backward Buffer (rel=0, rev=1)  off  rev=0  rev’=1  rev’=1  rev=0  rev’=1  rev=0  rev’=1  rev=0  off  off  rev=1  rev’=0  rev’=0  rev=1  rev’=0  rev=1  rev=1  off  rev’=0  rel=1  Forward Propagation (rel=1, rev=0)  (c)  rel=1  Backward Propagation (rel=1, rev=1)  Fig. 2: (a) Conventional channel buffer, (b) our reversible channel buffer, and (c) storage and propagation for both forward and backward links. ) V ( e g a t l o V 1.4 1.2 1 0.8 0.6 0.4 0.2 0 ctrl in out 0.0 0.5 1.0 Time (ms)  1.5 2.0 Fig. 3: Discharge time of channel buffer. each consisting of two channel buffer lines. Each link consists of two channel buffer lines to alleviate HoL blocking [7]. Additionally, since QORE has more links between routers than the two links in conventional routers, we have reduced the bandwidth of our links for a fair comparison, as explained in the evaluation section. Therefore, the wire area overhead of QORE is equal to the conventional baseline networks. However, a designer can choose N to be a different number de2 Channel Buffer  Lines each with  4 stages  Low Congestion  More Buffers for East  a  b  c  d  East  Xbar  Router 1  N=4 Router  Links  Xbar  -x  West  Router 2  Fig. 4: QORE’s four reversible router links each consisting of two channel buffer lines. pending on system requirements. Each router link is reversible, allowing communication in both directions. However, the two channel buffer lines in each link will always be directed the same way. This will ensure that at any time, a packet will have at least two VCs to choose from, which in turn will alleviate HoL blocking. In QORE, when there is high trafﬁc in one direction, the links can change direction according to the trafﬁc load, thereby increasing buffer space. For example, in Figure 4(b), when there is high eastbound trafﬁc, three links (a-c) can be allocated to the east direction while one link (d) remains in the west direction. The three east links can, therefore, use the under-utilized westbound buffers and provide more buffering for eastbound trafﬁc. This additional buffering will relieve congestion at router 2 as well as router 1 and other upstream routers. Meanwhile, the one west link can still provide buffering for westbound trafﬁc. As a result, both eastbound and westbound trafﬁc can have ample buffering, thereby, decreasing packet latency. Therefore, reversing router links in QORE can reduce trafﬁc bottlenecks caused by underutilized links and buffers. Determining which direction to allocate links is critical in QORE. Network trafﬁc is measured using hardware counters to store the number of link traversals in each direction. A two-stage controller, which is detailed more in Section IV-C, is used to allocate links to the appropriate direction based on trafﬁc demands. The ﬁrst stage (link controller (LC)) of the controller uses the counters to determine which direction has the highest trafﬁc called as the ”majority”. The second stage (fault controller (FC)) will assign all but one link to the majority direction or allocate equal links to both directions if the link utilizations are similar. In the example in Figure 4(b), each time a ﬂit traverses links (a-d), both routers 1 and 2 will increment their counters. Since there is high eastbound trafﬁc in this example, the link controllers will determine that the majority of the trafﬁc is moving from west to east. At this point, the fault controllers in both router 1 and 2 will allocate the ﬁrst three links (a-c) to the east and allocate link (d) to the west. If there are packets currently stored in the channel buffers when the reversing occurs, then these packets will be ﬂushed out to escape VCs inside the downstream router.         LC  12  FC  LC  8  FC  LC  4  FC  LC  0  FC  a  b  c  d  LC  13  FC  LC  9  FC  LC  5  FC  LC  1  FC  LC  14  FC  LC  10  FC  LC  6  FC  LC  2  FC  LC  15  FC  LC  11  FC  LC  7  FC  LC  3  FC  Router  Reversible  Link   Backup  Ring  LC  FC  Link  Controller  Fault  Controller  n+x entries  -x  +y  -y  +x  n-y entries  -xLnk[0]  -xLnk[n-x]  -yLnk[0]  +yLnk[0]  Router  +yLnk[n+y]  +xLnk[0]  +xLnk[n+x]  -yLnk[n-y]  +x Link Status Table  Direction  Flit Count  Faulty  Link Address  log2(n+x) bits  1 bit  log2( Rw ) bits  1 bit  Total Good  Links  log2(n+x)  +xLnk[0]  +xLnk[1]  +xLnk[2]  +xLnk[n+x]  In/Out  In/Out  In/Out  In/Out  Count  Count  Count  Count  Yes/No  Yes/No  Yes/No  Yes/No  -y Lnk[0]  -y Lnk[1]  -y Lnk[2]  -y Lnk[n-y]  -y Link Status Table  In/Out  Count  Yes/No  In/Out  Count  Yes/No  In/Out  Count  Yes/No  In/Out  Count  Yes/No  0-n+x  -  -  -  0-n-y  -  -  -  Fig. 6: Link status tables. Fig. 5: Layout of QORE showing links conﬁgured to an arbitrary trafﬁc pattern. B. QFC with Faults QORE uses QFC buffers to overcome hard faults in the network. When a link in one direction is faulty, another link can reverse its direction to overcome this fault. Figure 5 shows the overall layout of the QORE network for 16 routers and can be easily scaled to large numbers. The routers are connected to each other in a grid-like fashion similar to a mesh network. However, instead of the two unidirectional links between routers as in a mesh, QORE has four, narrower reversible links between each router. Again, each reversible link consists of two channel buffer lines. Also, the links are narrower than the baseline links as explained in the Evaluation section so there is no area overhead. The additional links create redundant paths between routers to improve both performance and reliability while avoiding HoL blocking. The link setup shown in Figure 5 is arbitrary; each link can reverse in either direction depending on trafﬁc demands. QORE also has a backup ring network which is used when there are a large number of faults that potentially could isolate healthy routers. Each router has a link controller (LC) and a fault controller (FC) (Detailed in Section IV-C) that analyze link utilization and determine which links to reverse. Each set of four links can handle up to three faulty links before using the backup ring. If a fault is detected in any of the links of a set, then the remaining non-faulty links will point in the directions speciﬁed by the LC and FC. For example, suppose the four links on the +x side of router 0 are initially setup as shown in Figure 5 with two links facing east (E) and two links facing west (W). If faults are detected in both links 0 and 1, then links 2 and 3 can overcome these faults by changing their directions to E and W, respectively. This will maintain connectivity between routers 0 and 1 so that packets can still be transmitted to both sides. If three of the four links fail then the fourth link can be used to communicate both ways since it is reversible. However, if all four links between two routers fail, then the backup ring network must be used. The backup ring network consists of two unidirectional rings, so that packets can traverse the shortest path, either clockwise or counterclockwise, to their destination. For example, if all four +x links of router 0 fail and the destination is router 5 then the packet will be routed on the ring network from router 0 to router 1, and so on up to router 5. Once a packet is on the ring network, it must stay on the ring network until it reaches its destination in order to avoid livelocks and deadlocks. C. Link and Fault Controllers In order to keep track of the status of each link, Link Status Tables (LSTs) are implemented in hardware. There are four LSTs per router in QORE; one for each set of links. The naming convention is shown in the top portion of Figure 6. The set of links on the right-side of the router are labelled as the +x links, links on the left are labelled -x links, etc. Each set of links has a LST containing information about the links. Each table has as many entries as links in each direction. In this paper, there are always 4 links in each direction. Hence, n+x = n−x = n+y = n−y = 4 and each table has four entries. Each link in a speciﬁed direction has a unique identiﬁer stored in the Link Address ﬁeld. Whether the link is facing in towards the router or out away from the router is speciﬁed in the Direction ﬁeld. This ﬁeld will be read by the routing computation (RC) to determine valid routing paths and will be set up by the algorithm in the FC. The F lit C ount data ﬁeld stores the number of ﬂit traversals on the link within the reconﬁguration window, Rw . These counters are read by the LC to determine trafﬁc demands. Each counter is incremented every time its corresponding link receives a ﬂit and is decremented every time its corresponding link sends a ﬂit. The F aulty data ﬁeld stores whether or not the link is useable. This data ﬁeld is read by the FC and RC. The ﬁeld is set when its corresponding link detects a fault. Detection of faults can be done by implementing BIST (Built-In System Test) [14], [31]; however, fault detection is beyond the scope                                 +x  count of link 0  count of link 1  count of link 2  count of link 3  +x LC  -y  -y LC  +x Majority (E, W, or B)  # of good links  link 0 faulty?  link 1 faulty?  link 2 faulty?  link 3 faulty?  -y Majority (N, S, or B)  log2(n+x) Link  Address  Direction  +x FC  -y FC  log2(n-y) Link  Address  Direction  Enable (end of Rw)  (a)  No Faults  One Fault  Two Faults  Three Faults  Faulty Link  (b)  Good Link  Fig. 7: (a) Block diagrams of link controller (LC) and fault controller (FC) and (b) Example of fault adaptability. of this work. Finally, each table stores the total number of working links which is set each time a fault is detected. The block diagrams for the LC and FC are shown in Figure 7(a). The LC and FC are split into 4 independent blocks corresponding to each direction (+x, -x, etc.). The inputs of the LCs are the direction ﬁelds for each of the 4 router links. The output of the LCs indicates which direction (N=north, E=east, S=south, W=west, or B=both) the majority of the ﬂits were traveling during the last Rw cycles. If the trafﬁc was roughly equal (within Δ where Δ=5% of total ﬂit traversals in this paper) then a B is output and an equal number of links will face in each direction. The LC output gives a good measure on the trafﬁc demand so that link bandwidth can be properly allocated. The simple algorithm to determine the majority of the +x (px) links is shown in Algorithm 1. At the end of Rw , the LCs total up the counts from their corresponding LSTs. Since the counters are incremented when a ﬂit is received and decremented otherwise, a positive total would indicate the majority of the trafﬁc is moving ”West” for the set of +x links and a negative total would indicate more ”East” Trafﬁc. At the end, the counts in the LSTs are cleared for next Rw . The majority output is then fed to the FCs. The inputs for the FC, shown in Figure 7(a), are the majority signal, the total number of good links, and the fault status of each link. The FC determines the new directions for each link by outputting their link address and updating the direction ﬁeld in the LST. The algorithm to determine the directions for the +x set of links is shown in Algorithm 1. If the LC determines that the majority is W , then the FC will try to assign a majority of the links to the W direction as shown in Figure 7(b). The FC also tries to maintain connectivity by assigning at least one link to the opposite direction of the majority when possible. When there is only one non-faulty link, then the FC must break connectivity and assign the link to the majority direction. However, this will cause starvation as packets cannot be sent in one direction. We resolve this by allocating 60% of Rw to the majority direction and reserve 40% of Rw to the opposite direction. We chose 60% because Algorithm 1 Link Controller and Fault Controller Pseudocode for +x (px) Links if(Enable){ // Link Controller for(all links 0 to n+x − 1) total count = total count + pxLnk[i].count; if(total count (cid:2) 0) else if(total count (cid:3) 0) pxMajority = West; pxMajority = East; else pxMajority = Both; clear all counts(); } } if(Majority of trafﬁc is West){ // Fault Controller if(pxLnk.totalGood == 1) else{ assign one link(W est); assign one link(E ast); assign remaining links(W est); } } else if(Majority of trafﬁc is East){ // Same as above except interchange West and East } else if(Trafﬁc is similar in both directions){ assign half links(W est); assign half links(E ast); our simulation results showed that this value gave the best average performance over all the benchmarks. D. Router Architecture Figure 8 shows the router microarchitecture of QORE. The four links to the left of the router can act as outputs or inputs. When acting as an output, the data comes from the crossbar and is demultiplexed onto the four channel buffers. As an input, the data is multiplexed into the crossbar. After a signal is multiplexed, it is normally sent straight to the crossbar. However, it can be sent to an escape buffer. This escape buffer is used to move packets from the channel buffers when the links are reversed. They are also used to avoid deadlocking [32] as explained in Section IV-E. When the escape buffers are full the upstream router will receive a congestion signal and will not send packets to the channel buffers; therefore, guaranteeing that the escape buffers will have enough room to ﬂush out the channel buffers. Six buffers are used because at most six channel buffer lines will face in one direction. Therefore, the six escape buffers ensures that a packet will have a buffer to go to when the links reverse. Each time a ﬂit traverses the links, the counters in the LSTs are incremented or decremented based on the link direction. The inset in Figure 8 shows the counter for link 0. When a ﬂit traverses a link, it signals the counters and increments the f litcount in the LST if the direction is in or decrements the f litcount if the direction is out. The LC and FC blocks access information from the LSTs as described in the previous section. The route computation (RC) is modiﬁed to determine which link to send data on in addition to which direction to send the packet. The                       flit  signal  Link 0  dir  link 0  count  +  1  -  1  LC  LC Counter  Counter  updates  updates SC  SC FC  FC LSTs  LSTs RC  RC 6 Esc.  6 Esc.  Buffers Buffers  Xbar  Xbar Ring  Ring VCs  VCs From Cores  Router  To Cores Fig. 8: Router Microarchitecture showing inputs/outputs, LC, FC, and RC. link decision is based on which link has the lowest count in the LST. Therefore, the trafﬁc will be spread evenly among the links. The switching control (SC) sends the release (rel) and reverse (rev ) signals to the channel buffers. When there is contention at the crossbar or downstream router, the SC notiﬁes the channel buffers to store the data by setting the rel signal to 0 as explained in Section III. The SC also reads the LSTs to obtain the rev signal, notifying the channel buffers of the correct direction. E. Deadlock Avoidance and Reliability Concerns In conventional NoCs, XY routing algorithm is used to avoid deadlocks by avoiding turns (Y-to-X). However when links reverse, if not handled properly, there is a potential for deadlock as communication in one direction can be cut off leading to starvation. In QORE, we avoid deadlocks by a) maintaining connectivity, thereby, eliminating starvation, b) using escape VCs to ﬂush out channel buffers during reversing, and c) keeping packets on the backup ring network until their destination is reached. To prove that our network is deadlockfree we examine the three possible states of the N links between routers: Case I: Zero to N-2 links are faulty. In order to prevent deadlocks, connectivity must remain between routers. FC algorithm 1 ﬁrst assigns one link to the non-majority direction then assigns the remaining links to the majority direction. This ensures that there is always a connection in both directions. Conventional deadlock-free algorithms such as XY routing can, therefore, be applied and deadlocks are completely avoided. Case II: N-1 links are faulty. Again, in order to prevent deadlocks, connectivity must remain between routers. However, in this case only one link is available. The algorithm of the FC will assign this one link to the majority direction. Then at 60% of Rw , FC will change the Direction ﬁeld in the LST to the opposite direction. This will cause the link to ﬂush out the data from the channel buffers to the escape VCs located at the downstream router, thereby, allowing packets to be sent in the opposite direction. Therefore, 60% of Rw will be allocated to the majority direction and 40% of Rw will be allocated for the opposite direction, providing full connectivity. Case III: All N links are faulty. In this case, no channel buffers are available and protocol states that packets must use the backup ring network to proceed. To avoid deadlocks and livelocks, packets must remain on the backup ring network until their destination is reached. We ensure the packet stays on the ring by adding a one bit ring ﬁeld to the packet that indicates to the RC whether or not the ring network should be used. When the ring bit is ”1”, the RC will always send the packet on the ring even if router links are available. If the ring bit is ”0” then the router links must be used. To avoid circular dependencies once on the bidirectional ring, a separate set of VCs is allocated to each direction. Protocol deadlocks can be avoided since each link has two buffer lines. One buffer line can be assigned to requests while the other is used for response trafﬁc. Other than deadlocks and livelocks, another concern may be the issue of the fault tolerant components themselves failing such as the backup ring network or the fault controllers. The backup ring network adds redundancy to links between routers. Moreover, since this backup ring network does not use reversible channel buffers, it has 10 less transistors at every repeater creating a more robust connection between routers. For the LC, FC, and LSTs, since they have a very small overhead, as shown in Section V-A, these components would be ideal for dual modular redundancy (DMR) or triple modular redundancy (TMR). V. EVA LUAT ION In this section, we ﬁrst consider the overhead for our reconﬁguration controllers and reversible buffers. Next, we evaluate the fault tolerant performance of QORE compared to the Ariadne [12], Vicis [13] networks by evaluating throughput and power on synthetic trafﬁc as well as speedup on real benchmarks. Lastly, we consider the effect of our reversibility on the overall performance of QORE when no faults are present by comparing to BAR [22] which is not a fault tolerant network. For open-loop measurement, we varied the network load from 0.1-0.9 of the network capacity. The simulator was warmed up under load without taking measurements until steady state was reached. Then a sample of injected packets were labeled during a measurement interval. The simulation was allowed to run until all the labeled packets reached their destinations. All designs were tested with different synthetic trafﬁc traces such as Uniform Random (UN), non-uniform random (NUR), Bit-Reversal (BR), Butterﬂy (BFLY), Matrix Transpose (MT), Complement (COMP) and Perfect Shufﬂe (PS). For closed-loop measurement, the full execution-driven simulator SIMICS from Wind River [33] with the memory package GEMS [34] was used to extract trafﬁc traces from real applications. The Splash-2, PARSEC, and SPEC CPU200 TABLE I: Cache and core parameters used for Splash-2, PARSEC, and SPEC2006 application suite simulation. Parameter L1/L2 coherence L2 cache size/assoc L2 cache line size L2 access latency (cycles) L1 cache/assoc L1 cache line size L1 access latency (cycles) Core Frequency (GHz) Threads (core) Issue policy Memory Size (GB) Memory Controllers Memory Latency (cycle) Directory latency (cycle) Value MOESI 4MB/16-way 64 4 64KB/4-way 64 2 5 2 In-order 4 16 160 80 workloads were used to evaluate the performance of 64-core networks. Table I shows the parameters for the cache and core used for the Splash-2, PARSEC, and SPEC2006 benchmarks. We assume a 2 cycle delay to access the L1 cache, a 4 cycle delay for the L2 cache, and a 160 cycle delay to access main memory. The power and area results were estimated using the Synopsys Design Compiler with the 40 nm TSMC technology library. For fair comparison, every network had 4 VCs per input and each network was assumed to have a concentration of four cores to a single router as this has been shown to minimize energy and latency while allowing a larger number of cores on a chip [35]. Additionally, we maintained similar bi-sectional bandwidths for each network. The conventional router design (both Ariadne and Vicis) will have two links between each router (one for each direction) and QORE has at most six links between routers (four reversible links, two unidirectional links for the ring) for a ratio of 1:3. However, the bandwidth of each link in QORE is 32 bits/cycle so the total bandwidth between routers will be 192 bits/cycle. Therefore, each link in the conventional design will be 192/2=96 bits/cycle which is 3X the bandwidth of a QORE link. We have assumed that the backup ring network is fault-free and the packet size is four ﬂits each 128 bits. A. Power, Area, and Timing of Reversibility Overhead Table II shows the power overhead for the network components of one router estimated from the Synopsys Design Compiler with a nominal supply voltage of 1.0 V and an operating frequency of 2 GH z . A buffer for the baseline design is a a four ﬂit register buffer and a buffer for QORE is a four stage reversible channel buffer. Each router, in either design, contains 32 buffers (4 inputs × 8 buffers). The buffers in QORE consume 19.8 mW of power; approximately 82.3% less than the baseline register buffers. The amount of leakage power for the reversible channel buffer was found to be 2.44 nW. The overhead of the LC and FC is approximately 96 nW of power and a timing of 0.07 ns. The power is a minimal fraction of the total router and the timing is within or clock period. The link power for both baseline and QORE are equal TABLE II: Power overhead for the components of one router. Storage LC FC Link Crossbar Total Baseline 111.6 mW 0 0 (2×96 bits) 307.2 mW (8×8) 67.4 mW 486.2 mW QORE 19.8 mW 96.27 nW 96.64 nW (6×32 bits) 307.2 mW (9×9) 86.2 mW 413.2 mW Percent Diff. -82.3% 0% +27.9% -15.0% TABLE III: Area overhead for the components of one router. Baseline (μm2 ) 43,712 0 0 (2×96 bits) 23,629 (8×8) 580,007 647,348 QORE (μm2 ) 147,392 1.41 1.42 (6×32 bits) 23,629 (9×9) 622,418 793,442 Percent Diff. +237.2% 0% +7.3% +22.6% Storage LC FC Link Crossbar Total since the total link bandwidth is kept equal. An crossbar power overhead of 27.9% is due to the backup ring network in QORE leading to a slightly larger crossbar. Table III shows the area overhead of each router component. The buffers in QORE occupy 147,392 μm2 which is 3.4× more area than the baseline register buffers. However, unlike register buffers and conventional channel buffers, our channel buffers serve three functions: storage, reversibility, and a link repeater. The area overhead of the LC and FC components are approximately 1.4 μm2 which is minimal compared to the other router components. The timing for our reversible channel buffers was estimated to be 0.39 ns which is within our speciﬁed clock period of 0.50 ns. The critical path of the four stage reversible channel buffers was composed of eight pass gates (0.22 ns) and four non-reversible channel buffers (0.17 ns). The timing of the critical path as well as estimate of power and area accounted for all additional wiring required between routers. B. Speedup on Real Applications The speedup of BAR (B), QORE (Q), Ariadne (A) relative to Vicis (V) for different real applications is shown in Figure 9. The networks were simulated on all applications; however, we only have space to show four applications in the ﬁgure. QORE reconﬁgures its links every Rw = 50 cycles. Different values of Rw are evaluated in Section V-F. Before runtime, faults were randomly inserted into a percentage of links ranging from 0% to 50%. Since BAR is not a fault tolerant network, it is only shown for 0% faults. At 0% faults, the performance optimized BAR has the largest speedup for all applications as expected. At low to medium faults (0-30%), QORE has an average speedup of 1.68× across applications for all a worse speedup of 0.51× on average. However, this can be benchmarks. At a high number of faults (40-50%), QORE has misleading because the high number of faults causes Ariadne p u d e e p S 3.5 3 2.5 2 1.5 1 0.5 0 p u d e e p S 3.5 3 2.5 2 1.5 1 0.5 0 * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  *High number  of subnetworks  BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% FFM  FFT  (a)  bzip  freqmine  * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  * *  *High number  of subnetworks  BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% LU  (b)  Fig. 9: Speedup of relative to Vicis with varying number of faults where BAR (B), QORE (Q), Ariadne (A) relative to Vicis (V) for 64 cores with SPLASH-2, PARSEC, and SPEC CPU2006 benchmarks. swaptions  Ocean  streamcluster  b u S s k o o f r r w e b e t m N u N 14 12 10 8 6 4 2 0 Ariadne/Vicis QORE 10 15 20 25 30 35 Percent Faults (%)  40 45 50 Fig. 10: Average number of subnetworks of QORE compared to Ariadne and Vicis. and Vicis to be split into small subnetworks. Subnetworks are very undesirable because cores from one subnetwork will not be able to communicate with cores from another subnetwork. The average number of subnetworks for each network is shown in Figure 10. QORE always maintains connectivity through the backup ring network. The number of subnetworks in Ariadne and Vicis increase with the number of faults. Subnetworks partition the chip, blocking communication to many cores. The subnetworks, therefore, lead to a false increase in speedup as also observed in [12]. Whereas, the reversibility of links makes QORE more resilient to communication blocking. C. Network Throughput with Faults The saturation throughput of the networks for different synthetic trafﬁc mixes is shown in Figure 11. Four different types of trafﬁc mixes we examine are shown in Table IV using the abbreviations deﬁned previously in Section V; However, due to space constraints we only show two mixes. Each mix randomly cycles through each pattern every TP=250 cycles. QORE reconﬁgures its links every Rw = 50 cycles. In Figure 11, QORE consistently has similar throughput to BAR and a higher throughput than both Ariadne and Vicis. Averaged over each trafﬁc mix and fault percentage, QORE’s saturation throughput is 2.3× and 2.9× higher than Ariadne and Vicis, respectively. Similar to the speedup results, an increase in throughput can be seen in all mixes when the fault percentage changes from 20% to 30%. Again, this is due to link faults causing the network to be partitioned into smaller subnetworks. When the faults increase to a high percentage (40-50%), few ﬂits are sent on a network that has many cores, so the throughput (ﬂits/cycle/core) starts to decrease again. From 0-20% faults, QORE only sees a drop in performance of 3.5% averaged over all mixes compared to an approximately 70% drop for Ariadne and Vicis. QORE is able to sustain performance due to the adaptability of its links. When a wire between two routers is faulty in Ariadne or Vicis, then all communication between those two routers is blocked even if other wires are non-faulty. With many faults, this limits the number of paths in the network. Therefore, many packets are sharing the same paths which causes a drastic increase in contention for links. QORE, on the other hand, can overcome one or more faulty wires by reversing the available non-faulty links. Reversibility preserves paths between routers which relieves contention. Maintaining minimal contention for links is a main factor for maintaining high throughput.           TABLE IV: Trafﬁc Mixes Mix Mix 1 Mix 2 Mix 3 Mix 4 Patterns BR, BFLY, COMP NUR, BR, PS UN, BFLY, MT UN, BR, COMP, PS D. Packet Latency with Faults Figure 12 shows multiple plots for the packet latency at various fault percentages for trafﬁc mix 1. Latency plots at 10%, 20%, 40%, and 50% faults were not shown due to space constraints. At 0% faults, BAR saturates at the highest load due to its adaptability, and ﬁne-grained ﬂit transmission. The low load latency for both BAR at 0% faults and QORE for all faults is higher than both Ariadne and Vicis. This is due to the serialization delays combined with narrow links in BAR and QORE. However, QORE saturates at a higher load for most fault percentages. At 10%, 20%, and 30% faults, QORE saturates at least 77%, 160%, and 150% higher than Ariadne and Vicis. Faults in Ariadne and Vicis can easily shut down communication between routers. The fault tolerant schemes in these networks forces many packets to take additional hops to reach their destinations because they must move around routers. The increase in hop count greatly increases packet latency for the Ariande and Vicis networks. QORE is able to route more packets minimally to their destination to keep latency low. At 50% faults, Ariadne and Vicis saturate 87.5% higher than QORE. However, this is due to the many unreachable cores in Ariadne and Vicis which create very small subnetworks resulting in packets with little to no contention. E. Network Power The total network power for the networks is shown in Figure 13 for different numbers of link faults and two mixes, although we evaluated the network on all four mixes. Even though reversible channel buffers have a smaller power than register buffers, we have assumed that all the networks have the same buffer power of 618.5 nW and area of 4,606 μm2 as shown in Table I. This is done to ensure that the no network has an unfair advantage due to a different buffer technology which trades off t r / l ) r u p h e g u o o c h e T c y n c o s a / i t t i r l f ( t u a S 0.25 0.2 0.15 0.1 0.05 0 BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% Mix 1  Mix 2  Fig. 11: Saturation throughput for varying percentage of link failures for different trafﬁc mixes for BAR (B), QORE (Q), Ariadne (A) and Vicis (V). ) s e l c y c ( y c n e t a L 700 600 500 400 300 200 100 0 0.01 ) s e l c y c ( y c n e t a L 700 600 500 400 300 200 100 0 0.01 BAR QORE Ariadne Vicis 0.06 0.16 0.11 Offered Load  (a) 0%  0.21 QORE Ariadne Vicis 0.06 0.11 Offered Load  (b) 30%  0.16 Fig. 12: Latency plots for trafﬁc mix 1. ) W m ( t i l F r e p r e w o P 1400 1200 1000 800 600 400 200 0 BQAV QAV QAV QAV QAV QAV BQAV QAV QAV QAV QAV QAV 0% 10% 20% 30% 40% 50% 0% 10% 20% 30% 40% 50% Mix 1  Mix 2  Fig. 13: Network power for different trafﬁc mixes for BAR (B), QORE (Q), Ariadne (A) and Vicis (V). area for power. For mix 1, QORE saves at least 15% power over Ariadne and Vicis on average. Additionally, QORE saves 25%, 22%, and 23% on trafﬁc mixes 2, 3, and 4. The main contribution to the power savings is the link power. Ariadne and Vicis route around faulty links which many times leads to packets taking non-minimal paths to the destination. QORE can avoid this as long as there is one working link between routers. The only time QORE has the possibility to take a nonminimal path is when the backup ring is used which, in this simulation, only occurred when the fail percentage was 50%. As seen in Figure 13 for 50% faults, the power of QORE is higher than Ariadne because of the backup ring routes packets on non-minimal paths for this particular trafﬁc mix. BAR has a power 9.5% less than QORE due to the backup ring in QORE which increases the crossbar size by one. However, when the number of faults increases, QORE cannot be compared to BAR since BAR is not a fault tolerant network. Therefore, QORE can save approximately 21% power on average while providing better fault coverage with a speedup 1.3× higher and improved throughput by 2.3×.                       t ) u p h e g u o o c r r / l h e T c n y o c s a / i t t i r l f ( t u a S t ) u p h e g u o o c r r / l h e T c n y o c s a / i t t i r l f ( t u a S 0.25 0.2 0.15 0.1 0.05 0 0.25 0.2 0.15 0.1 0.05 0 BAR QORE Baseline mix 1 mix 2 mix 3 mix 4 (a) TP=250 Rw=50  BAR QORE Baseline mix 1 mix 2 mix 3 mix 4 (c) TP=500 Rw=50  t ) u p h e g u o o c r r / l h e T c n y o c s a / i t t i r l f ( t u a S t ) u p h e g u o o c r r / l h e T c n y o c s a / i t t i r l f ( t u a S 0.25 0.2 0.15 0.1 0.05 0 0.25 0.2 0.15 0.1 0.05 0 BAR QORE Baseline mix 1 mix 2 mix 3 mix 4 (b) TP=250 Rw=100  BAR QORE Baseline mix 1 mix 2 mix 3 mix 4 (d) TP=500 Rw=100  Fig. 14: Effect Rw on saturation throughput for varying trafﬁc mixes. F. Network Performance of Reversibility We have shown that QORE can handle errors very well using reversibility. In this section, we will show that QORE can overcome faults while maintaining performance by comparing QORE to the non-fault tolerant, high performance BAR. In BAR, links are reconﬁgured every cycle (Rw = 1 cycle). In this section, we evaluate the effect of a longer Rw on QORE as well as the difference between the two networks. Figure 14 shows the saturation throughput of QORE compared to BAR and a baseline network which is QORE without reversibility. In the ﬁrst two parts of Figure 14, TP=250 is the same and Rw varies from 50 cycles in Figure 14(a) to 100 cycles in Figure 14(b). When Rw = 50 the saturation throughput of QORE is 2.5% less than BAR. When Rw increases to 100 cycles, QORE has less opportunities to reconﬁgure and the performance drop increases from 2.5% to 4.6%. In Figure 14(c) and 14(d) TP increases to 500 cycles and Rw changes from 50 cycles to 100 cycles again. In this case, the performance drop changes from 1.7% when Rw = 50 to 7.3% when Rw = 100. The uniform nature of mixes 3 and 4 give BAR a slight advantage over QORE since BAR reconﬁgures every cycle and at a ﬁner granularity. Compared to the baseline, QORE can improve throughput by an average of 7.9% when Rw = 100 and the improvement can increase to 12.2% when Rw is changed to 50 cycles. Overall, when Rw = 50 cycles QORE performance is only 2.5-4.6% lower than BAR and 12.2% higher than the baseline, but has the additional beneﬁt of being able to handle faults. V I . R E LATED WORK With the increase of soft and hard errors in NoCs due to decreasing technology sizes, much research has gone into the detection and handling of errors. Built-In Self Tests (BISTs) are commonly used to detect errors in systems. Recently, NoCAlert [14] was proposed which detected faults in real-time with 0% false negatives. Low overhead checkers were used to detect faults without the need of periodic or triggered-based testing. As described in the introduction, the Ariadne [12] network uses up*/down* routing to move around faults. Each time a fault was detected, new routing paths were created by transmitting a series of ﬂag broadcasts to all routers. This created a deadlock-free tree network for the irregular topology. The Vicis [13] network also changes its routing algorithm to move around faults when detected. To avoid deadlocks, turn restrictions are placed at certain routers. The Immunet [15] design avoids faults by adaptively routing packets while using escape VCs to avoid deadlocks. Our design differs from these previous works in that we try to avoid additional hops when possible by using reversible links. The implementation of reversibility eliminates the need for routing tables and multiple ﬂag broadcasts to reconﬁgure the network as seen in Ariadne. Furthermore, in other networks, if one of the two unidirectional links fails then neither link can be used because this would create a one way path to a router. We mitigate this problem by using reversible links as opposed to unidirectional links. Therefore, as long is there is one good link to a router, communication will not be halted. The work in [36] uses bi-directionality in the two channels between routers to provide fault-tolerance to the links. QORE differs in that we move the buffers to the links and create reversible channel buffers to lower power and provide fault coverage to buffers. Additionally, we can provide higher fault coverage by implementing more than two links between each router. In [22], a bandwidth-adaptive router (BAR) was created to increase channel utilization without affecting network latency. BAR increased channel utilization by using narrower channels while also improving performance through adaptive bidirectional channels. Our work also differs from BAR in that we reverse links as well as buffering by using reversible channel buffers. The reversing of buffers as well as links allows the downstream routers to store the increasing number of packets. Additionally, we reverse links/buffers at a coarser granularity to reduce serializer/deserializer overhead. Another reconﬁgurable design was proposed in BiNoC [21]. BiNoC dynamically reconﬁgured bidirectional channels to improve performance. We differ from BiNoC in that we reverse buffering as well as links to provide fault tolerance. V I I . CONC LU S ION S With the decreasing technology sizes and increasing number of cores number integrated on a single chip, the design of fault tolerant NoCs that do not degrade performance is critical. In this paper, we propose QORE - a fault tolerant NoC architecture using reversible channel buffers. We use QORE’s reversibility for increased performance and to overcome faulty links. Our results on real benchmarks (SPEC CPU2006, PARSEC, and SPLASH-2) show an increase in speedup of 1.3× and improved throughput by 2.3× on synthetic trafﬁc compared to related work. Using the Synopsys design compiler, we show that QORE reduces network power by 21% while requiring minimal control overhead.                         ACKNOW L EDGM EN T We would like to thank the anonymous reviewers as well as Prof. Savas Kaya for his feedback. This research was supported by the Stocker Research Assistantship and NSF awards ECCS-0725765, CCF-1054339 (CAREER), ECCS1129010, ECCS-1342657, ECCS-1342702, CNS-1342984, CCF-0915537, and CNS-1318997. "
2014,MP3 - Minimizing performance penalty for power-gating of Clos network-on-chip.,"Power-gating is a promising technique to mitigate the increasing static power of on-chip routers. Clos networks are potentially good targets for power-gating because of their path diversity and decoupling between processing elements and most of the routers. While power-gated Clos networks can perform better than power-gated direct networks such as meshes, a significant performance penalty exists when conventional power-gating techniques are used. In this paper, we propose an effective power-gating scheme, called MP3 (Minimal Performance Penalty Power-gating), which is able to achieve minimal (i.e., near-zero) performance penalty and save more static energy than conventional power-gating applied to Clos networks. MP3 is able to completely remove the wakeup latency from the critical path, reduce long-term and transient contention, and actively steer network traffic to create increased power-gating opportunities. Full system evaluation using PARSEC benchmarks shows that the proposed approach can significantly reduce the performance penalty to less than 1% (as opposed to 38% with conventional power-gating) while saving more than 47% of router static energy, with only 2.5% additional area overhead.","MP3: Minimizing Performance Penalty for Power-gating of Clos Network-on-Chip  Lizhong Chen1, Lihang Zhao2, Ruisheng Wang1, and Timothy M. Pinkston1  1Ming Hsieh Department of Electrical Engineering, 2Information Sciences Institute,  University of Southern California, Los Angeles, USA  {lizhongc, lihangzh, ruishenw, tpink}@usc.edu  Abstract  Power-gating is a promising technique to mitigate the  increasing static power of on-chip routers. Clos networks  are potentially good targets for power-gating because of  their path diversity and decoupling between processing elements and most of the routers. While power-gated Clos  networks can perform better than power-gated direct networks such as meshes, a significant performance penalty  exists when conventional power-gating techniques are used.  In this paper, we propose an effective power-gating scheme,  called MP3 (Minimal Performance Penalty Power-gating),  which is able to achieve minimal (i.e., near-zero) performance penalty and save more static energy than conventional power-gating applied to Clos networks. MP3 is able  to completely remove the wakeup latency from the critical  path, reduce long-term and transient contention, and actively steer network traffic to create increased power-gating  opportunities. Full system evaluation using PARSEC  benchmarks shows that the proposed approach can significantly reduce the performance penalty to less than 1% (as  opposed to 38% with conventional power-gating) while saving more than 47% of router static energy, with only 2.5%  additional area overhead.  1. Introduction  With tightening power constraints and growing demand  for high performance, current and future chip multiprocessors (CMPs) need to be designed to optimize both power  and performance. As a key component in CMPs for connecting various on-chip resources, the network-on-chip  (NoC) can draw a substantial percentage of chip power [1, 9,  10, 26]. In particular, the static power consumption of routers accounts for an increasing percentage of the total NoC  power, exceeding 43% for 45nm and beyond. As more cores  are integrated on a CMP, the need to reduce on-chip latency  will become even more pronounced so as not to degrade  system performance. It is thus imperative to devise effective  techniques that can dramatically reduce NoC static power  without sacrificing performance.  Power-gating is a very useful circuit-level technique to  enable trade-offs between static power and performance,  especially for circuit blocks that exhibit enough idleness  [11]. On-chip routers are potentially good targets for powergating because of their relatively low average utilization, but  recent research shows that it is difficult to power-gate mesh  networks effectively [7, 23]. Due to the processor noderouter dependence (i.e., sending/receiving packets from/to  the local processor node depends on the powered-on status  of the connected router), the idle periods of routers in mesh  networks are often fragmented and not long enough to compensate for power-gating energy overhead. Moreover, due to  limited path diversity (particularly with dimension-order  routing), packets are likely to encounter gated-off routers on  path(s) to the destination, in which case packets suffer additional latency to wait for routers to wake up, resulting in  serious performance degradation. These fundamental but  inherent limitations of meshes greatly reduce the usefulness  of applying power-gating to network routers.  In this paper, we investigate the largely unexplored  power-gating opportunities of Clos networks. The Clos topology has been used in off-chip networks for supercomputers and data-centers [30], and recent studies show promise  for adopting Clos as on-chip networks [14, 15, 16, 34]. As  Clos belong to the general class of indirect networks, the  majority of the routers are not coupled to PEs. Also, Clos  have excellent path diversity that can increase the chances  of packets avoiding wakeup latency. While power-gating  Clos can mitigate the energy and performance overhead  compared with meshes, there can still be a significant performance penalty if power-gating is conventionally applied  to Clos even with state-of-the-art optimizations (e.g., 38%  increase in average packet latency and 15% increase in execution time, as shown in Section 6).  To fully exploit the potential of power-gating when applied to Clos networks, we propose an effective powergating scheme called MP3 (Minimal Performance Penalty  Power-gating) for Clos NoCs which can achieve minimal  (i.e., near-zero) performance penalty and, at the same time,  save more static energy than conventional power-gating.  MP3 consists of three techniques which, collectively, are  able to completely remove the wakeup latency from the  critical path of packet transport and reduce long-term as  well as transient contention that may occur during changes  in traffic load, thereby addressing all the major sources of  performance degradation associated with power-gating. Furthermore, MP3 can steer network traffic based on load conditions and actively create additional power-gating opportunities in a coordinated fashion, thus improving overall energy savings. Full system simulation shows that, compared to  an optimized conventional power-gating technique applied  to Clos, MP3 achieves a reduction of 36.8% in network performance penalty while saving 9.8% more router static energy. When compared with not using power-gating, MP3  reduces router static energy by 47.7% while incurring only  0.65% increase in execution time.  This research increases understanding of the key factors  affecting the effectiveness of power-gating on-chip network  routers. The proposed scheme and simulation results provide valuable insights on how to address critical performance and energy issues. While both mesh and Clos networks are evaluated, the main objective is not to establish  that one topology is better than the other but, rather, to in            % r e w o p c i t a t s C o N 100% 80% 60% 40% 20% 0% e m i t n u r d e z i l a m r o N 1.4 1.2 1 0.8 0.6 0.4 0.2 0 45nm 32nm 22nm 32cycle 44cycle 56cycle   (a) NoC static power % vs.     (b) Normalized runtime vs.          process technology              average packet latency  Figure 1: Need for reducing NoC static power without  increasing packet latency.  vestigate effective architectural solutions for Clos networks  from the perspective of power-gating. To our knowledge,  this is the first study to explore power-gating trade-offs for  indirect networks such as Clos and demonstrate the viability  of realizing minimal performance penalty when applying  power-gating to NoCs.  The rest of the paper is organized as follows. Section 2  provides more background on power-gating and identifies  fundamental limitations in power-gating mesh networks.  Section 3 analyzes the opportunities and challenges of power-gating Clos networks. Section 4 provides details of the  proposed MP3 design for optimizing power-gating. Section  5 discusses evaluation methodology, and Section 6 presents  simulation results. Finally, related work is summarized in  Section 7, and Section 8 concludes the paper.  2. Background and Motivation  2.1 Need for Performance-aware Static Power Reduction  While on-chip networks provide a more scalable interconnection solution for many-core CMPs compared with  traditional buses and point-to-point interconnects, the added  complexities of buffers, crossbars and control logic in the  NoC greatly increase power demand. Industrial and research  chips have shown that on-chip networks can draw a substantial percentage of chip power [1, 9, 10, 26]. In particular, a  large percentage of the NoC power consumption comes  from static power which is trending upward as technology  scales. To illustrate, Figure 1(a) plots the percentage of static power of a 64-node NoC at 2GHz for different process  generations. Results are obtained from the latest DSENT [32]  NoC power simulator fed with statistics from full system  simulation (detailed simulation infrastructure is described in  Section 5). As shown in the figure, the percentage of static  power consumption increases continuously as the transistor  feature size shrinks, from 43% at 45nm, to 54% at 32nm, to  over 65% at 22nm under representative workloads. This  trend only gets worse as technology scales beyond 22nm,  indicating a pressing need to reduce NoC static power.  The design of the on-chip network is key to supporting  fast communication among various on-chip resources. Care  should be taken when trading off NoC performance for  power-savings as any non-local data access, coherence messaging and handshaking signaling relies on the on-chip network which is critical to maintaining system performance.  Figure 1(b) from our simulations show that, on average, the    (a) Power-gating concept     (b) Energy savings and costs Figure 2: Power-gating technique and breakeven time.  runtime of PARSEC benchmarks on a 64-node CMP is increased by 15% and 36% when the average on-chip packet  latency increases from 32 cycles to 44 cycles and 56 cycles,  respectively. With more cores integrated on a chip in the  near future, the on-chip network will have an even larger  impact on system performance. Given the worsening problem of static power consumption and the growing importance of low packet latency, it is imperative to design  effective techniques that can dramatically reduce NoC static  power without sacrificing performance.  2.2 Power-gating and Associated Trade-offs  Power-gating is a promising technique for enabling  tradeoffs between static energy savings and performance.  As depicted in Figure 2(a), it is implemented by inserting  appropriately sized header (or footer) transistor(s) – a nonleaky “sleep switch” with high threshold voltage – between  Vdd and the block (or the block and GND). By asserting the  sleep signal when the power-gated block is idle, the supply  voltage to the block can be turned off, thus avoiding static  power consumption by removing the leakage currents in  both subthreshold conduction and reverse-biased diodes.   The effectiveness of power-gating is determined by two  aspects: net energy savings and performance penalty.  Net energy savings: Static energy can be saved during  the power-gated period. However, there are energy overheads that come from distributing the sleep signal at the  beginning of each power-gating operation (from t0 to t1 in  Figure 2(b)) and from waking up the gated-off block at the  end (from t2 to t3 in Figure 2(b)). Consequently, powergating is useful only when the cumulative static energy savings exceed the energy overhead. This condition of positive  net energy savings is reflected in the concept of “breakeven  time” (BET) defined to be the minimum number of consecutive cycles that a gated block needs to remain in idle state  before being awoken to offset power-gating energy overhead [11, 21, 22]. For on-chip routers, the BET value is  around 10 cycles as estimated in prior research using analytical modeling and simulation [5, 11, 23].   Performance penalty: Despite the net energy savings  that can come from power-gating, a potential drawback is  the detrimental impact on system performance it may have.  Whenever a gated-off block needs to be used again, it first  has to be awoken by restoring virtual Vdd. Under typical  technology parameters, the wakeup latency for on-chip  routers is usually a few nanoseconds (or around 5-15 cycles  depending on the frequency) according to previous studies                  [7, 23, 25]. Since a powered-off block cannot perform the  assumed operations until it becomes fully functional,  stalling in the system may occur that can result in serious  performance penalty if power-gating is performed frequently and the gated-off periods are short.   Therefore, to utilize power-gating effectively, we need  to maximize net energy savings by increasing the idleness of  unneeded functional blocks and, at the same time, minimize  performance penalty by partially or even completely reducing/hiding the wakeup latency.  2.3 Limitations in Power-gating Mesh Networks  Power-gating has been applied successfully in cores and  execution units [11, 21, 22] for some time and shown to  enable viable trade-offs between performance and energy.  Only recently has research efforts started to consider the  application of power-gating in on-chip network routers [5, 7,  23, 24, 25, 29], all of which assume mesh-based topologies.  Owing to its planar topology, the mesh is a popular network  used in chip multiprocessors. However, there are several  fundamental limitations in applying power-gating usefully  to meshes and other direct networks. As shown in Figure  3(a), in direct networks such as the mesh, every router (denoted by the labeled square) is connected to a processing  element (PE, denoted by the circle); whereas in indirect  networks such as the Clos of Figure 3(b), only the input and  output routers at the edge of the network are associated with  PEs, so that packets sent from PEs are forwarded indirectly  through the middle-stage routers. Compared with Clos,  there are two distinctive properties of mesh networks that  greatly limit the effectiveness of applying power-gating: 1)  dependence between each PE-router pair and 2) less path  diversity.   From the energy perspective, due to the PE-router dependence, a mesh router must be awoken whenever the connected PE needs to send a packet to the network or receive a  packet from the network, thus breaking the potentially long  idle period of the router into fragmented intervals that may  fall below the required BET. Moreover, the BET limitation  is further intensified in meshes due to the fewer alternative  paths as more non-local packets have to be forwarded  through the local router, making the idle intervals even  shorter. For example, any packet sent from router 0-5 in  Figure 3(a) needs to be forwarded through router 6 to get to  router 7 assuming a minimal routing algorithm. Our full  system evaluation on PARSEC benchmarks shows that, for  an 8x8 mesh, the number of idle periods having a length less  than the BET constitutes more than 67.2% of the total number of idle periods, which severely limits the potential to  achieve large net energy savings.  In addition, from the performance perspective, powergating of mesh routers can have a considerable negative  impact on NoC performance. When a PE needs to  send/receive a packet, due to the PE-router dependence, a  wakeup is inevitable if the associated router is in the powered-off state, and the wakeup latency is exposed directly to  the critical path of the packet’s transport to the next hop.  Furthermore, a packet routed over multiple hops can experience wakeup latency multiple times as routers at many hops  along the path could be gated-off. This cumulative wakeup  latency problem is severe in meshes as there are few alternative node-disjoint paths from which to choose at any particular hop.  To improve the effectiveness of power-gating mesh networks, several optimization techniques can be used. However, they all have limited capability in mitigating the above  energy and performance issues. For example, early-wakeup  signal generation [23] can only hide up to 3 cycles of the  entire wakeup latency, assuming a canonical 3-stage router  with look-ahead routing. The Idle-detect [11] technique can  usually only filter out idle intervals that are shorter than  around 4 cycles [7] without substantially losing static power  saving opportunities. It is also possible to implement powergating for smaller circuit blocks within each router, such as  per input port or per virtual channel [24, 25]. However, individual components have only slightly longer idle periods,  and this method requires prohibitive implementation overhead (e.g., 35 power domains are needed in a single router  [25] to implement this method in addition to the complex  coordination among different components). These techniques have only limited effectiveness as they can neither  remove the inherent dependence between the PE and router  in a mesh nor increase path diversity. We address these issues by exploring power-gating on another class of topology  that expands the possibility for power-performance tradeoffs.  3. Analysis of Power-gating Clos NoC  3.1 Clos Networks  Whereas most of the NoC power-gating work to-date  tries to combat critical problems in applying power-gating  to mesh networks, very little research has explored the opportunities of power-gating Clos NoCs belonging to the  large class of indirect networks. The Clos topology has long  been studied since first being proposed in 1953 [6]. Early  applications of Clos were for circuit switching in telephone  exchange systems due to the topology’s superior capability  for establishing many concurrent connections. More recently, packet-switched Clos and its variants have been proposed for off-chip networks in supercomputers as well as  on-chip networks for chip multiprocessors [16, 30, 34].  A packet-switched Clos network consists of three types  of routers: input routers (IRs) that receive input packets  from PEs through the injection channels, output routers  (ORs) that output packets to PEs through ejection channels,  and middle-stage routers (MRs) that do not connect to any  PE and only perform forwarding functions. In general, a  Clos network can be made of any odd number of stages.  Figure 3(b) shows an example of a 5-stage Clos composed  of 4x4 routers to connect 64 PEs using unidirectional links.  The respective top and bottom PEs are the same repeated for  simplicity of representation, per usual convention.  In the past, the main concern for adopting Clos NoCs  was long wires. With specialized floor-planning optimizations to reduce total wire length of the Clos [16, 33] and  with routing-over-logic techniques to largely remove the  area overhead of long wires [28, 34], the hardware complexity of Clos NoCs can be greatly mitigated. Moreover, Clos  also has the flexibility to be implemented with lower radix  routers (e.g., 2x2 router) to increase clock frequency or with  higher radix routers (e.g., 8x8 routers) to reduce hop count,  making Clos very competitive to other traditional and ad          0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 Inject  Input  routers  Middle-  stage  routers  Output  routers  Eject  (a) Mesh (direct network, 64 5x5 routers)                         (b) 5-stage Clos (indirect network, 80 4x4 routers)  Figure 3: Direct network (mesh) vs. indirect network (Clos) for connecting 64 PEs; all links are unidirectional.   PE-router dependence only at input and output routers in indirect networks.  hoc topologies [16] (Section 6.8 provides more discussion).  These recent optimizations and flexibility on implementation make it interesting to explore Clos NoCs and their  power-savings capabilities.   3.2 Opportunities  As an indirect network, Clos has at least three major advantages for applying power-gating. First, except for the  input and output routers, all the middle-stage routers are not  coupled to PEs. Therefore, sending and receiving packets in  PEs do not necessarily trigger the wakeup of most routers.  This not only reduces the number of router wakeups but also  mitigates the energy overhead and delay associated with the  wakeup. It also increases the chances of routers being idle  longer than the required BET.   Second, for a given network size, the number of stages  in a reasonably designed Clos is usually smaller than the  average hop count in a mesh. As routers at each hop could  be gated-off, the Clos topology can essentially alleviate the  aforementioned cumulative wakeup latency problem by  reducing the total number of encountered routers that are in  gated-off state. The result is accelerated packet forwarding.  Third, the Clos provides path diversity, so that in-transit  packets have multiple routing options and can avoid waiting  for router wakeup as long as one of the downstream routers  allowed by the routing algorithm is not in the gated-off state.  It is possible, in theory, for packets to avoid all wakeups  along a packet’s entire path from source to destination,  thereby eliminating the wakeup delay and minimizing the  overall performance penalty of applying power-gating.   3.3 Challenges  Although the above opportunities suggest that indirect  Clos networks are promising candidates for power-gating,  applying the circuit-level power-gating technique conventionally (or conventional power-gating for short) to Clos can  have limited effectiveness, especially in terms of reducing  performance penalty. Our simulations show that conventional power-gating of the Clos, even with early-wakeup and  idle-detect optimizations mentioned in Section 2.3, can still  incur 38% increase in average packet latency and 15% increase in execution time. This significant performance penalty is caused by a number of reasons, as explained below.  First, as can be observed, besides the middle-stage routers, there are still a sizable number of input or output routers  (e.g., 32 out of 80 routers in the 5-stage Clos). Similar to the  mesh, these routers connect to PEs directly, thus suffering  from the same problem: either the router idleness is upperbounded by the local PE’s traffic, or packets from/to PEs  have to experience the wakeup latency of the directly associated router. Therefore, a way to allow packets to be forwarded through the input and output routers with low overhead is needed while allowing part or all of the static energy  of these routers to be saved.  Second, even though the Clos has better path diversity  and smaller average hop count, the wakeup latency is still  on the critical path of packet transport. Moreover, the cumulative wakeup latency remains as packets at some particular  routers may be left with one unique path to the destination.  For instance, there are 16 different paths from R1 to R64  overall (we use Ri to denote the labeled router in Figure 3).  However, if a packet is currently in R32 and destined to PEs  connected to R64, then the only reachable path is R32 =>  R48 => R64. If both R48 and R64 are powered-off, the  packet will experience wakeup latency twice with no alternative paths. To make things worse, power-gating saves  more static energy when network load is low, in which case  routers are also likely to be powered-off, making packets  more likely to encounter multiple wakeups. One effective  approach to solve this problem is to completely remove the  wakeup latency from the critical path by always providing a  minimal set of carefully selected powered-on paths between  any PE pair, as proposed in the next section.  Third and most importantly, conventional power-gating  of the Clos is uncoordinated in the sense that every router  makes routing decisions unaware of the global network status, thus switching between powered-on and off states independently based only on local traffic information. This  wastes energy-saving opportunities and incurs unnecessary  performance penalties in various ways. For example, even  when the overall network load is low, packets in the upstream router can still be routed to multiple downstream  routers, requiring more powered-on routers that could otherwise be gated-off. Also, due to the unhidden portion of the  wakeup latency, if a gated-off router starts to wake up only                                    after it receives a wakeup signal, the router will not be ready  by the time the packets actually arrive, unless some hints  about the traffic between the up and downstream routers can  be exchanged in advance. In addition, as some routers in the  network may be in sleep state, a sudden increase in the  amount of injecting packets are forwarded temporarily only  through the remaining powered-on routers, which may  cause transient congestion and performance degradation  until more routers are gradually awoken. To address these  issues, we need a more coordinated way to efficiently power-gate all the routers in the network.  In summary, while Clos networks have great potential to  reap energy benefits without incurring excessive performance overhead, this is hard to achieve through conventional power-gating approaches but, instead, requires considerable support at the architecture level as proposed in this work.  4. Minimal Performance Penalty Power-gating  In this section, we propose an effective power-gating  scheme called MP3 (Minimal Performance Penalty Powergating) for Clos NoCs which is able to achieve minimal (i.e.,  near-zero) performance penalty and, at the same time, save  more static energy than conventional power-gating. The  basic idea is to first guarantee network connectivity by constructing a minimum resource set that is always powered-on  so that regardless of the on/off status of other resources,  packets always have the last resort of using this resource set  for transporting packets without suffering any wakeup latency. Then, dynamic traffic diversion actively steers traffic  between the minimum and maximum available resources of  the network in a coordinated fashion based on load conditions. In this way, contention at any particular load level is  kept low while more resources can be powered-off through  increased power-gating opportunities. Finally, rapid wakeup  further reduces any transient contention that may occur during sudden load increases by powering on a selective and  necessary set of downstream routers in advance. This enables those routers to be ready when packets arrive. The following subsections describe these techniques in detail.  4.1 Guaranteed Connectivity  To minimize the performance penalty of power-gating,  the foremost task is to remove wakeup latency from the critical path of packet transport. We achieve this by providing  guaranteed connectivity in the Clos. The basic idea is to turn  ON a minimal set of resources, S, to ensure that at least one  powered-on path always exists between any source and destination PE pair. The set S can be composed of routers or  components within routers. As this set of resources is always ON regardless of the network load, the key is to minimize S to maximize energy savings, with low implementation overhead. We use the example in Figure 4 to explain  our method of constructing S. The procedure is generally  applicable to other Clos instances.   There are two main steps. The first step is to reduce the  number of powered-on routers in the NoC to a minimum,  and the second step is to reduce the amount of ON components within that minimum set of routers. Specially, as can  be seen immediately from the figure, no PE is disconnected  even if all the black routers are gated-off. Hence, the resources associated with the 39 black routers are not needed  in S. However, this is not sufficient as every input or output  router is still needed. To reduce S further, notice that when  all the black routers are turned off, each input router only  needs to forward packets from four input ports to one output  port (e.g., R0 only forwards packets to R16). Based on this  observation, we split the resources of input routers into two  power domains.   As depicted in Figure 5, the striped components are in  one power domain and are needed in S, whereas the rest of  the components form the other power domain. Essentially,  to maintain the connectivity from four input ports to one  output port, only one of the four 4-to-1 multiplexers in the  crossbar is needed in S. Also, only one virtual channel (VC)  for each message class is needed in an input port to correctly  buffer packets without message-dependent deadlock. In  general, assuming the original router has m dependent message classes, p input ports, and v VCs per class per port, the  minimal number of VCs needed in S is m×p – one VC for  each message class per port. Hence, the amount of VC resources in S is 1/v of the total VC resources. The higher the  value of v, the more static energy that can be saved. In most  wormhole routers, the value of v is typically two or more in  order to mitigate head-of-line blocking effectively (e.g.,  Intel’s 48-core SCC chip has 8 VCs for two message classes  [10]). In addition to the four input ports and one output port,  we also conservatively put all the router arbitrator components into S given that arbitrators usually consume a very  small portion (less than 5%) of the total router energy. A  two-domain separation for router arbitrators can also be  used if some customized router designs employ very large  arbitrators. The two-domain split approach incurs much  lower hardware overhead than implementing power-gating  at per port or per VC level, and allows the majority of router  components to be powered off without losing the required  forwarding functionality.  Likewise, R16-R19 perform the same 4-to-1 minimal  forwarding and can follow the same two-domain design.  Similarly, all the output routers and R48-R51 only need to  forward packets from one input port to four output ports, so  the minimal resources in S for these routers include one input port with m (out of m×v) VCs, one-fourth of the crossbar,  four output ports with m (out of m×v) latches per port, and  control logic. All the remaining resources are put in the other power-domain.  Overall, the above approach based on identifying a minimal resource set enables a wide range of power-gating configurations. At one end of the spectrum, all 80 routers can be  turned on to support high network load during dataintensive phases of an application’s execution. At the other  end, when the load intensity allows it, only 1 router (white)  needs to be fully powered-on while 40 routers (gray) can be  partially powered-off and 39 routers (black) can be fully  powered-off, allowing maximum static energy savings.  More importantly, network connectivity is guaranteed at all  times, so that any packet can always use the resource set S  as the last resort for transporting packets regardless of the  on/off status of other resources, thus eliminating wakeup  latency from the critical path of packet forwarding.          0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 IR  16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 UR  32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 CR  … … 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 LR  64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 OR  Figure 4: Illustration of MP3 for guaranteeing connectivity, dynamically  steering traffic, and reducing transient contention by rapid wakeup.   Figure 5: Partially-on design for  saving energy while maintaining  minimal forwarding functions. 4.2 Dynamic Traffic Diversion  While the guaranteed connectivity approach lays the  foundation for effective power-gating of Clos NoCs, it accomplishes only part of our objective as packets are not automatically concentrated to only those resources needed for  a specific load. In order to perform power-gating in a more  coordinated fashion, we propose dynamic traffic diversion  which systematically steers traffic to certain resources based  on prevailing load conditions to 1) allow non-essential resources to be powered off via concentration and 2) gradually  power on more resources as load increases to reduce contention and balance performance via distribution. To achieve  these objectives, an appropriate metric is first selected for  monitoring traffic intensity and then, based on the load status, the routing algorithm is augmented to enforce a network-wide consistent order of concentrating and distributing  traffic to resources. Finally, a handshaking mechanism is  carefully designed to power on/off resources correctly. The  details are explained below.  Traffic Intensity Metric: An appropriate metric is needed  as an indicator of traffic intensity. R. Das, et al. found that  several intuitive metrics are actually ineffective in assessing  load status [7]. For example, the metric of average buffer  occupancy per router does not perform well as some input  buffers along the congested paths may be heavily occupied  while the average occupancy is still low. Injection rate also  is not satisfactory as there is no universal threshold that  works well for all traffic patterns (e.g., uniform random and  transpose saturate at different injection rates, making it difficult to choose a predetermined threshold). In addition, the  average blocking delay per flit is theoretically accurate but  prohibitively expensive to implement in practice. Therefore,  the use of the maximum buffer occupancy as an appropriate  metric is suggested [7], where the occupancy of each input  port is counted, and then the maximum value among all the  input ports is computed and compared with predetermined  thresholds. We use a similar metric with a slight difference  in that the thresholds are adjusted based on the number of  powered-on VCs to make the metric suitable for both partially-on and fully-on routers. This metric allows the threshold to be determined empirically and performs well for different traffic patterns and benchmarks.  Routing: After an appropriate metric is selected, the next  augmentation is to allow the routing algorithm to become  aware of the load status reflected in the metric and to steer  traffic accordingly. For Clos networks, packets in earlier  stages have more routing freedom than packets in later stages. For example, in Figure 4, packets in input routers (IRs)  or upper routers (URs) have up to four output port choices,  but packets in center routers (CRs) or lower routers (LRs)  can choose only one output port to reach the destination.  Therefore, steering traffic is achieved during earlier stages  of packet forwarding.   In the case of the example depicted in Figure 4, we assume the metric threshold is divided into four ranges to create a 4-level configuration that corresponds to increasing  load conditions. The threshold increases one level when the  load condition makes the packet latency exceed 15% of the  zero-load latency under the router on/off configurations for  the previous level. For each router belonging to IR or UR,  its four downstream routers are numbered 1 to 4 from left to  right. When the load condition of a router reaches level k (k  = 1, 2, 3, 4), the router is allowed to forward packets to its  downstream routers numbered from 1 to k, but not above k  (i.e., the leftmost k downstream routers). Adaptive routing  among the k options is used based on the number of available credits (or any other commonly used criteria). In this  way, 1) no downstream routers are used beyond the minimally needed k routers corresponding to current load conditions and 2) among the downstream routers, utilization is  maximized through load-balancing adaptive routing. At the  highest load, all four downstream routers can be used in this  method, which is the same as the no-power-gating case with  no sacrifice in throughput. It is worth noting that, by enforcing the left-to-right order at every router, the entire network  agrees on a consistent order of which resource set to concentrate or expand (e.g., when load is on level-1, R0, R4, R8  and R12 will consistently all forward packets to R16), thus  avoiding the inefficiencies of uncoordinated power-gating.  Also, if some routers at a particular stage, e.g., CRs, are  accidently turned off, the upstream stage routers, URs, will  experience higher maximum buffer occupancy and consequently wake up more downstream routers, which are the  exact same stage CRs. This will restore the balance between  load intensity and powered-on routers.                          Handshaking: Finally, we discuss the details of the required conditions and handshaking mechanism for routers to  correctly transition between power states. No extra signal is  needed between up and downstream routers besides what is  already provided in conventional power-gating. Based on  the types of routers, there are four cases.  Case 1 – white routers: Since white routers are always  powered-on, no transition is need.  Case 2 – black routers: Black routers transition from on  to off if 1) the datapath of the router is empty and 2) all of  the wakeup signals from its upstream routers are de-asserted.  The router transitions from off to on if any of its upstream  routers asserts the wakeup signal. Here, an upstream router  asserts the wakeup signal to a downstream router if a packet  needs to be forwarded to that router. An optimization for  wakeup signal generation will be presented in the next subsection, but the conditions for state transitions are the same.  Case 3 – gray routers in IRs and URs: A gray router in  this category transitions from fully-on to partially-on if 1)  the metric indicates load is on level-1 and 2) the datapath to  the three rightmost downstream routers are empty (any new  incoming packets will be forwarded only to the leftmost  downstream router after detecting the low load). The router  transitions from partially-on to fully-on if the load is on level-2 or above. Note that a fully-on router does not necessarily need to forward packets to all its downstream routers.   Case 4 – gray routers in LRs and ORs: A gray router in  this category transitions from fully-on to partially-on if 1)  the datapath of the resources that are not in S is empty and 2)  all of the wakeup signals from its three rightmost upstream  routers are de-asserted. The router transitions from partiallyon to fully-on if any of its three rightmost upstream routers  asserts the wakeup signal.  4.3 Rapid Wakeup  One effect of dynamic traffic diversion is to reduce performance degradation caused by power-gating, as more resources will eventually be turned on to accommodate the  increase of traffic in the long run. However, transient contention may still be possible during the time that the new  resource is being awoken. For example, suppose the network load suddenly jumps from level-1 to level-2 at R0, so  that R0 tries to wake up R20 to distribute the traffic. Yet,  R20 will not be ready until it is fully awake after the unhidden portion of wakeup latency, during which the packets  still have to be forwarded to R16. Then, after R20 is powered on, packets routed through R20 will find that R36 is  asleep. So again, packets need to wait for R36 to wake up,  and so on. In such cases, while R20, R36, R52 are sequentially waking up, incoming packets are queued in the input  buffers along this path. When the backpressure propagates  back to R0, most of the new packets of level-2 load are still  forwarded through R16. This leads to transient contention  since the resources from R16 and beyond are supposed to  handle only level-1 load without contention.  To avoid this type of pathological performance degradation, we propose rapid wakeup, which relays the wakeup  signal from upstream routers to downstream routers in a  chained fashion to wake up needed downstream routers in  advance so that those routers will be ready when packets  arrive. In order to realize rapid wakeup effectively, the key  is to minimize the needed router set, which is achieved by  limiting the breadth and depth of the signal relay tree from  the upstream router. First, to limit the breadth of the relay  tree, the wakeup signal is relayed to only one downstream  router if multiple options are available. For instance, R20  relays the wakeup signal only to R36 as R37-R39 are not  additionally needed for packets to reach any destination  provided that R36 is powered on. In contrast, R36 relays the  wakeup signal to R52-R55 as they are indispensable for  packets to reach any destination. This is because the destination of a particular packet is unknown beforehand and, more  importantly, most of the destinations will in fact be visited  since a batch of packets likely will arrive due to the load  increase.   Second, notice that packets themselves take a few cycles  to traverse each router, so downstream routers that are several hops away do not need to be awoken too early. In general, an Nhop-away downstream router can wake up in time if  Nhop_min = (cid:1727)Tunhidden_wakeup /Trouter(cid:1728)  Nhop × Tlink + Tunhidden_wakeup  ≤  Nhop × (Trouter + Tlink)  This reduces to  which is about 2-3 hops depending on actual parameter values. This means that the relay depth only needs to be 2-3  hops. After limiting the breadth and depth, the remaining  relay tree from a particular router is minimal in the sense  that all the remaining relays are necessary and any delay in  waking up these downstream routers will cause some performance penalty. Note that, although the gated-off time of  these routers may be slightly reduced, the reduced amount is  only a few cycles upper-bounded by Tunhidden_wakeup assuming  the above formula to limit the depth while still being able to  wake up in time. The majority of routers are not affected.  Hence, rapid wakeup can largely remove the transient contention penalty while retaining most of the power-gating  opportunities. Moreover, since the wakeup signal is required  for power-gating anyway, no additional signaling network is  needed.  4.4 Impact of MP3 on Performance and Energy  Putting the three techniques together, the proposed MP3  scheme fully exploits the power-gating potential offered by  indirect Clos networks while effectively addressing its performance and energy challenges.  From the performance perspective, the guaranteed connectivity technique is first used to remove the wakeup latency from the critical path of packet forwarding. Then, dynamic traffic diversion is used to guard against contention in  the long-run and rapid wakeup is used to reduce transient  contention. Therefore, MP3 removes all the major sources  of possible performance degradation, thereby minimizing  performance penalty of power-gating the Clos.  From the energy perspective, guaranteed connectivity  enables a wide spectrum of energy-performance tuning opportunities by constructing a minimally needed resource set.  Dynamic traffic diversion then utilizes these opportunities to  coordinate router power-gating by steering traffic and turning on/off resources dynamically. This not only extends the  idle periods of the majority of routers, but also reduces the  number of wakeups and the associated energy overhead that  causes BET limitation in the first place. As a result, MP3 is  able to save more static energy with less energy overhead,  thus effectively increasing the net energy savings.           Mesh‐No‐PG Mesh‐ConvOpt‐PG Clos‐No‐PG Clos‐ConvOpt‐PG Clos‐MP3 Mesh‐No‐PG Mesh‐ConvOpt‐PG Clos‐No‐PG Clos‐ConvOpt‐PG Clos‐MP3 60 50 40 30 20 10 0 ) G P ‐ o N ‐ s l o C o t . m r o n ( e m i t n o i t u c e x E 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 ) s e l c y c ( y c n e t a l t e k c a p e g a r e v A                         Figure 6: Average packet latency.                                                   Figure 7: Execution time.  5. Evaluation Methodology  The proposed MP3 scheme is evaluated quantitatively  under full system simulation with the combined use of multiple architecture-level and circuit-level simulators. Cycleaccurate SIMICS and GEMS are used for processor functional and memory timing simulation. GARNET [2] is used  for detailed NoC performance evaluation, from which the  network activity statistics are collected and fed into DSENT  [32] for network power estimation. We modify the simulators to model all the key additional hardware in MP3, such  as handshaking  logic, buffer occupancy comparators,  wakeup signal relay, and so on. Each PE in the network  contains an UltraSPARC III+ core running at 2GHz, a  32KB I/D private L1 cache and a 256KB shared L2 cache  slice. Coherence is managed by the MOESI protocol. Four  memory controllers are provided. The cache and memory  controller on a PE share the injection channel in the network  interface. All topologies under comparison have the same  bisection bandwidth of 1TB/s. To accurately reflect the link  delays of the Clos, we follow the floorplan optimization in  [16] to estimate every link length. GARNET is configured  to have the delay of each link to be proportional to its length.  We model a canonical 3-stage router with look-ahead routing [18]. Two virtual channels per message class are provided, though MP3 can achieve more energy savings with more  VCs, as mentioned in Section 4.1. Also, as the Clos has  more bisection links than the mesh, for comparison purposes, both the Clos and mesh networks are configured with the  same total bisection bandwidth and the same total buffer  sizes.    Given that the metric of maximum buffer occupancy is  insensitive to traffic patterns (one of the main benefits),  thresholds for congestion levels are determined empirically.  However, router wakeup latency has a large impact on system performance. To estimate wakeup latency accurately,  we generate the physical layout of a router at 45nm technology with 1.0V voltage using a standard VLSI design flow.  Synopsys Design Compiler is used for logic synthesis, and  Cadence Encounter is used to process the gate-level netlist  to generate the power grid, floorplan, clock trees and routes.  Parasitic extraction is performed on a 451um-by-451um  layout to obtain the parasitic resistance and capacitance as  well as the cell load on the Vdd wiring. Finally, the extracted data is fed into a SPICE RC model, providing a wakeup  latency of 8 cycles. Because of the criticality of wakeup  latency, additional sensitivity studies are also conducted to  shed more light on the applicability of different schemes.   The following schemes are compared on a 64-core system: (1) Mesh-No-PG: mesh network with no power-gating;  (2) Mesh-ConvOpt-PG: conventional power-gating of mesh  optimized with early-wakeup and idle-detect – these optimizations not only improve performance by hiding 3 cycles of  wakeup latency, but also reduce energy overhead by avoiding powering-off all idle periods that are shorter than 4 cycles; (3) Clos-No-PG: Clos network with no power-gating;  (4) Clos-ConvOpt-PG: conventional power-gating of Clos  with early-wakeup and idle-detect optimizations; (5) ClosMP3: Clos with the proposed power-gating scheme. All the  five schemes allow adaptive routing for fair comparison.  While the mesh is included in the evaluation as a point of  reference, the main objective is to evaluate the power-gating  opportunities of Clos and how its power-gating potential can  be exploited by our proposed scheme.  6. Results and Analysis  6.1 Impact on Performance  As one of the primary targets, we first examine the performance impact of different schemes by running multithreaded PARSEC benchmarks [4]. Figure 6 compares the  average packet latency, and Figure 7 shows the execution  time of the five schemes normalized to Clos-No-PG. Results  are consistent across the range of benchmarks. As Mesh-NoPG and Clos-No-PG do not use power-gating, they provide  a lower bound of performance for the mesh and Clos, respectively. As can be seen from Figure 6, even with earlywakeup and idle-detect optimizations, the conventional  power-gating scheme for the mesh, Mesh-ConvOpt-PG, still  significantly increases the average packet latency by 64.5%  on average compared with Mesh-No-PG; whereas ClosConvOpt-PG causes 38.6% increase in the average packet  latency compared with Clos-No-PG. This indicates that the  indirect network nature of Clos indeed helps to reduce performance degradation as compared to the mesh, but it still  cannot entirely mitigate the negative effects of wakeup latency. In contrast, Clos-MP3 completely removes the  wakeup latency from the critical path and reduces both longterm and transient contention. Consequently, Clos-MP3  achieves a remarkable reduction of average packet latency,  having only 1.8% increase on average. This is equivalent to                        a 36.8% improvement compared with Clos-ConvOpt-PG.  Similar trends are also reflected in execution time. As  shown in Figure 7, Mesh-ConvOpt-PG and Clos-ConvOptPG increase execution time by 36.3% and 14.9% on average,  respectively. In comparison, the proposed Clos-MP3 incurs  a minimal increase of only 0.65% in execution time, effectively realizing near-zero performance penalty.   6.2 Impact on Router Static Energy  The performance advantage of Clos-MP3 does not sacrifice its energy savings at all. Figure 8 presents the results of  router static energy of different designs normalized to ClosNo-PG. As can be seen, Mesh-ConvOpt-PG reduces the  router static energy by 38.2% relative to Mesh-No-PG. In  comparison, Clos-ConvOpt-PG is able to reduce the static  energy by 41.1% relative to Clos-No-PG, which is slightly  better than Mesh-ConvOpt-PG due to the inherent suitability of Clos for power-gating. The lowest router static energy  is achieved in the proposed Clos-MP3, with an average reduction of 47.7%. This improvement mainly is attributed to  the ability of Clos-MP3 to dynamically concentrate traffic  and actively create power-gating opportunities. When compared relatively, the proposed Clos-MP3 saves 9.8% more  router static energy than Clos-ConvOpt-PG. This highlights  the effectiveness of Clos-MP3 in both providing higher performance and lower energy simultaneously.  6.3 Comparison of Power-gating Overheads  To gain more insight on the feature of Clos-MP3 to reduce unnecessary wakeups by steering traffic, Figure 9  shows the energy overhead (left vertical axis) caused by  router wakeup for the conventional power-gating schemes  and the Clos-MP3 scheme, normalized to Mesh-ConvOptPG. As can be observed, the power-gating overhead in ClosMP3 is substantially lower than the other two schemes. Figure 9 further compares the reduction in the total number of  wakeups in the different schemes (right vertical axis).  Whereas Clos-ConvOpt-PG decreases  the number of  wakeups by 60.3% compared to Mesh-ConvOpt-PG, ClosMP3 is able to reduce wakeups by 87.6%, on average, owing to its coordinated power-gating among all routers in the  network. This explains the large reduction in energy overhead and demonstrates the usefulness of Clos-MP3.  6.4 Impact on NoC Energy  Figure 10 plots the breakdown of NoC energy across the  benchmarks normalized to Mesh-No-PG, showing the relative impact of each energy component. Several observations  can be drawn from the figure. First, although Clos may consume more link energy than mesh for the no power-gating  cases, the total NoC energy of the Clos is still lower than  that of the mesh, indicating that Clos is a competitive NoC  topology. Second, the large power-gating overhead in MeshConvOpt-PG makes it very ineffective, leading to a less  than 6.3% reduction in overall NoC energy; whereas Clos  with conventional power-gating saves 19.4% of overall NoC  energy. Third, the proposed Clos-MP3, while significantly  reducing the performance penalty, can also save 22.5% of  NoC energy. This means that, compared with the state-ofthe-art power-gating scheme for Clos (i.e., Clos-ConvOpt                           Figure 8: Router static energy.                               Figure 9: Comparison of power-gating overhead.  Figure 10: Breakdown of NoC energy.  0 0.2 0.4 0.6 0.8 1 1.2 1.4 u o R t e r t s a t i c e n e r y g ( o n r m . t o C o l s ‐ N o ‐ P G ) Mesh‐No‐PG Mesh‐ConvOpt‐PG Clos‐No‐PG Clos‐ConvOpt‐PG Clos‐MP3 Overhead‐Clos‐MP3 Wakeups‐Clos‐MP3 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% Overhead‐Mesh‐ConvOpt‐PG Overhead‐Clos‐ConvOpt‐PG Wakeups‐Mesh‐ConvOpt‐PG Wakeups‐Clos‐ConvOpt‐PG 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% N o r m a i l z u n d e m e b r o f w p u e k a s e o n o P m w . ‐ r o a g t i e P n e p d a v e n h o C e v h o s e g n r r y g ( r t M ‐ O ‐ t G ) 0% 20% 40% 60% 80% 100% M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M M o h s v v n o n C s e o C o C C ‐ N N O O o o p p ‐ ‐ ‐ ‐ t t ‐ 3 P P P P P G G G G M e s h ‐ l ‐ o C l s ‐ l s M blackscholes bodytrack canneal dedup ferret fluidanimate raytrace swaptions x264 AVG B r ( o d k a o e n w m e o n o f N e P n e o C o h ) r y g r . t M s ‐ N ‐ G Link Router_dynamic Router_static Power‐gating overhead                                           PG), MP3 is better in terms of both performance and energy.  6.5 Comparison across Full Network Load Range  In order to understand the behavior of different schemes  more fully, we leverage synthetic traffic and vary the network load across the full range: from zero load to saturation  load. Figure 11 presents the performance and power results  for uniform random, transpose and bit-complement traffic  patterns. On the performance side, while typical behavior is  observed for Clos-No-PG, interesting results are found for  Clos-ConvOpt-PG. It can be seen that, at low load, the average packet latency of Clos-ConvOpt-PG is actually very  high. This is because many routers are gated-off at this load,  so packets are likely to experience wakeup latency multiple  times (i.e., cumulative wakeup latency), which increases the  packet latency considerably. When load increases, the average packet latency first decreases as more routers are awoken, and then starts to rise again as load approaches saturation. In contrast, the average packet latency of Clos-MP3  follows Clos-No-PG closely across the entire load range,  showing that it only incurs minimal performance penalty. It  is important to note that Clos-MP3 can indeed reach the  maximum throughput of the no-power-gating case. This  means that all routers can be correctly woken up in ClosMP3 if needed, which is important and necessary for supporting high network load phases of application execution.  When comparing static power, the proposed Clos-MP3  clearly has a significant advantage for various traffic patterns. As shown in the figure, the static power savings of  Clos-ConvOpt-PG are less than 10% when the load rate  only reaches 50% of saturation. In comparison, Clos-MP3  saves more than 10% of the static power even when load  passes 75% of saturation. These results suggest that ClosMP3 is much more energy-proportional than conventional  power-gating.  6.6 Effect of Rapid Wakeup  We also perform simulations to demonstrate the ability  of rapid wakeup to reduce transient contention. To assess  this effect quantitatively, the injection rate is quickly increased from 5% to 25% when the simulation time passes  the 10k-cycle mark (sufficient for reaching steady state in  synthetic uniform random traffic). Figure 12 plots the  changes of average packet latency as more resources are  waking up to accommodate the new load. Compared with  the normal wakeup, rapid wakeup mitigates transient contention in two ways: 1) rapid wakeup stabilizes the packet  latency within 75 cycles, which is 42% shorter than the  normal wakeup and 2) rapid wakeup also reduces the peak  increase of average packet latency during the transition by  34%. Due to these features, rapid wakeup is very helpful in  minimizing performance penalty of Clos-MP3.  6.7 Wakeup Latency Tolerance  As mentioned previously, cumulative wakeup latency is  a big obstacle for reducing the performance overhead of  conventional power-gating, particularly in multi-hop networks. The evaluation so far has shown that Clos-MP3 incurs minimal performance penalty when the wakeup latency  is 8 cycles (which is obtained from our detailed circuit-level  simulation). To illustrate that Clos-MP3 can effectively address the challenge of wakeup latency, Figure 13 compares  the average packet latency of Clos-No-PG, Clos-ConvOptPG and Clos-MP3 with varying values of wakeup latency.  The load rate is set to the average load rate of PARSEC  benchmarks. As can be seen, the average packet latency of  Clos-ConvOpt-PG increases by 56% when the wakeup latency increases from 5 to 14 cycles; whereas the latency of  Clos-MP3 remains very similar (less than 3.5% increase) for  different wakeup latencies. This demonstrates the ability of  Clos-MP3 to hide wakeup latency and its wide applicability  to various designs (e.g., under different frequencies).   6.8 Discussion  Hardware overhead: When configured with the same total buffer size and total bisection bandwidth, the hardware  cost of the 5-stage Clos (80 4x4 routers plus links) is 17%  lower than that of mesh (64 5x5 routers plus links), so the          (a) Uniform random                       (b) Transpose                     (c) Bit-complement  Figure 11: Behavior across full range of network loads.  Figure 12: Rapid wakeup.  Figure 13: Wakeup latency tolerance. 0 20 40 60 5 8 11 Wakeup latency (cycles) 14 A e v r e k c a p e g a t l a t n e y c ( e c y c l s ) Clos‐No‐PG Clos‐ConvOpt‐PG Clos‐MP3 0 15 30 45 60 75 90 Clos‐No‐PG 105 0 0.05 0.1 0.15 0.2 0.25 0.3 Injection rate (flits/node/cycle) 0.35 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) s Clos‐ConvOpt‐PG Clos‐MP3 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 0.05 0.1 0.15 0.2 0.25 0.3 Injection rate (flits/node/cycle) 0.35 o T t a l r u o t e r t s a t i c o p w e r ( W ) Clos‐No‐PG Clos‐ConvOpt‐PG Clos‐MP3 0 15 30 45 60 75 90 Clos‐No‐PG 105 0 0.05 0.1 0.15 0.2 0.25 0.3 Injection rate (flits/node/cycle) 0.35 A e v r e k c a p e g a t l a t n e y c ( e c y c l ) s Clos‐ConvOpt‐PG Clos‐MP3 0.0 0.5 1.0 1.5 2.0 2.5 Clos‐No‐PG 3.0 0 0.05 0.1 0.15 0.2 0.25 0.3 Injection rate (flits/node/cycle) 0.35 o T t a l r u o t e r t s a t i c o p w e r ( W ) Clos‐ConvOpt‐PG Clos‐MP3 0 15 30 45 60 75 90 Clos‐No‐PG 105 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Injection rate (flits/node/cycle) A e v r e k c a p e g a t l a t n e y c ( e c y c l ) s Clos‐ConvOpt‐PG Clos‐MP3 0.0 0.5 1.0 1.5 2.0 2.5 Clos‐No‐PG 3.0 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Injection rate (flits/node/cycle) o T t a l r u o t e r t s a i t c o p w e r ( W ) Clos‐ConvOpt‐PG Clos‐MP3 20 9900 9950 10000 10050 10100 10150 10200 10250 Simulation time for assessing transient contention (cycles) 25 30 35 40 45 50 55 60 A e v r e k c a p e g a t l a t n e y c ( ) s e c y c l Normal Wakeup Rapid Wakeup                                                               Clos is a viable option for on-chip networks in terms of implementation cost. Second, for any power-gating technique,  there is hardware overhead for sleep switch and state retention, which is typically within 4-10% depending on circuitlevel optimizations [11, 13]. More of a concern is the additional hardware induced by Clos-MP3. Our simulation results show that the added components in Clos-MP3, including the modified routing logic, handshaking control, wakeup  signal relay and so on,  have a hardware overhead of less  than 2.5% compared with conventional power-gating.   Scalability: The proposed Clos-MP3 does not have any  particular element that limits its scalability (e.g., no central  controller, no global signaling, etc.) and can be used for any  size of Clos NoCs. Thus, the scalability of Clos-MP3 is only  bounded by the Clos topology itself which has been shown  to have similar scalability as mesh NoCs [16].   Other topologies: Thus far, a 5-stage Clos is used as a  case-in-point to illustrate the proposed MP3 scheme. This  Clos example is compared to a traditional mesh network that  is also composed of low radix routers. When higher radix  routers are allowed under design constraints (e.g., to meet  certain clock frequency criteria), several other topologies  are available to increase network performance. For example,  with an 8x8 router radix, mesh can use a concentration degree of 4 to reduce the network diameter to 6 for a 64-node  system [3]. Flattened butterfly [19] can further reduce this  diameter to 2 by directly connecting the nodes in a dimension, with a router radix of 10x10. In addition, folded Clos  (fat-tree) also has a network diameter of 2 with 8x8 router  radix. While these topologies are able to reduce packet latency considerably, Clos remains competitive given that a 3stage Clos can also achieve a network diameter of 2 with  8x8 router radix, resulting in a similar reduction of packet  latency.  Prior research has shown that high-radix Clos has comparable hardware complexity but higher power efficiency  (assuming no power-gating) than several other mainstream  topologies [16]. However, in terms of static power savings  potential, the aforementioned topologies (i.e., concentrated  mesh, flattened butterfly and fat-tree) are much more limited  by the router-PE coupling than Clos. This is because, with  high-radix routers, a packet to any of the many input ports  needs to wake up the router, which reduces the router idleness and causes wakeup delay. In contrast, this effect is  greatly mitigated in Clos with our proposed MP3 technique.  For example, all the 8 input routers and 8 output routers in a  3-stage Clos can benefit from the two-domain partial powergating (which is even better than the 5-stage Clos as now  roughly only 1/8th of the router needs to be turned on minimally). Dynamic traffic diversion also works better due to  increased adaptivity (8 outputs to choose from at each router). Rapid wakeup may have reduced benefits but can still  hide the majority of wakeup latency. Hence, high-radix Clos  can have similar packet latency advantages as other highradix topologies while being a better target for power gating.   These findings together with the 5-stage Clos example presented in previous sections lead to the conclusion that Clos  is a competitive topology for both low-radix and high-radix  networks.  Applicability: The proposed MP3 scheme is an important extension to enhance the power-gating capabilities  of a variety of interconnection networks. First, MP3 is applicable to both on-chip and off-chip Clos networks with  different radices and network sizes. Second, MP3 can also  be applied to other topologies that have multiple nodedisjoint paths (excluding edge routers), which are often provided in indirect networks such as Benes, Omega, and nonflattened Butterfly with extra stages. A similar methodology  of dividing the edge routers into two power domains and  dynamically turning on and off edge/middle routers with  wakeup signal relay can be applied. The proposed MP3  scheme, however, has limited applicability to direct networks (even with multiple paths) due to direct coupling between router and PE that may trigger the power-state to  transition very frequently and because the two power domains may not be sufficient to maintain network connectivity (e.g., concentrated mesh requires all inputs and outputs to  be on).  7. Related Work  A couple of related works have already been mentioned  in previous sections. In addition, a multiple network-on-chip  power-gating design is proposed in [7] and a power-gating  bypass design is proposed in [5]. Both designs mainly target  power-gating mesh networks, and the increase in average  packet latency is considerable. A router parking scheme is  introduced in [29] to power-gate routers in meshes when the  core is idle, but it needs to flush private caches before turning off routers, which may cause serious performance issue.  Some research is also conducted to power-gate individual  components within a router [24, 25], but this approach is  very costly (16% hardware overhead) with still limited energy-savings and non-negligible performance degradation  even if the wakeup latency is only 4 cycles. Our work differs from these works in that we explore power-gating opportunities for Clos networks, and the proposed scheme is  able to entirely remove wakeup latency from the critical  path, thus achieving near-zero performance penalty.  Some work has gone into improving Clos for off-chip  interconnects [30], and recent research has shown that it is  also very promising to adopt Clos for on-chip networks [14,  15, 16, 34] as new floorplan and layout techniques emerge.  However, none of the off-chip or on-chip works looked into  the power-gating of Clos. Our work provides insight on the  power-gating characteristics of Clos networks and helps to  facilitate more efficient use of Clos NoC.  Much research has been conducted to reduce buffer requirements and improve buffer utilization, which directly or  indirectly saves buffer static power [12, 27]. Aggressive  bufferless routers can even eliminate buffers and their associated power consumption at the complexity of potential  livelock, misrouting and packet reassembly [8]. However,  besides buffers, there are other components in a router that  also consume a substantial percentage of the total static  power (42% as observed in our simulation), which are not  addressed by the bufferless approach but can be avoided  using our approach.  Prior research has also proposed various techniques to  save dynamic and static power of links [17, 31]. DVFS [20]  is also extensively studied to reduce power consumption.  These works and other dynamic power-saving techniques  (such as clock-gating) are largely orthogonal and comple          mentary to this work, and can be used together with MP3 to  provide more efficient Clos on-chip networks.   8. Conclusion  Current and future many-core systems require on-chip  networks to be designed with both power and performance  awareness. While mesh networks present several fundamental limitations for effective power-gating, this paper investigates the power-gating opportunities and challenges of Clos  networks. To combat the various limitations and inefficiencies in conventional power-gating of Clos, a minimal performance penalty power-gating scheme (MP3) is proposed  in this work. MP3 not only removes the wakeup latency  from the critical path and reduces long-term and transient  contention, but also actively steers network traffic to create  increased power-gating opportunities coordinated globally  across the network. Simulation results show significant reduction in the performance penalty while saving more router  static energy than conventional power-gating. These results  demonstrate the viability of using power-gating in NoCs  with only minimal performance overhead.  "
2014,Transportation-network-inspired network-on-chip.,"A cost-efficient network-on-chip is needed in a scalable many-core systems. Recent multicore processors have leveraged a ring topology and hierarchical ring can increase scalability but presents different challenges, including higher hop count and global ring bottleneck. In this work, we describe a hierarchical ring topology that we refer to as a transportation-network-inspired network-on-chip (tNoC) that leverages principles from transportation network systems. In particular, we propose a novel hybrid flow control for hierarchical ring topology to scale the topology efficiently. The flow control is hybrid in that the channels are allocated on flit granularity while the buffers are allocated on packet granularity. The hybrid flow control enables a simplified router microarchitecture (to minimize per-hop latency) as router input buffers are minimized and buffers are pushed to the edges, either at the output ports or at the hub routers that interconnect the local rings to the global ring - while still supporting virtual channels to avoid protocol deadlock. We also describe a packet-quota-system (PQS) and a separate credit network that provide congestion management, support prioritized arbitration in the network, and provide support for multiflit packets. A detailed evaluation of a 64-core CMP shows that the tNoC improves performance by up to 21% compared with a baseline, buffered hierarchical ring topology while reducing NoC energy by 51%.","Transportation-Network-Inspired Network-on-Chip Hanjoon Kim, Gwangsun Kim, Seungryoul Maeng, Hwasoo Yeo† , John Kim Dept. of Computer Science KAIST {hanj, gskim, jjk12, maeng}@kaist.ac.kr †Dept. of Civil & Environmental Engineering KAIST hwasoo@kaist.ac.kr Abstract A cost-efﬁcient network-on-chip is needed in a scalable many-core systems. Recent multicore processors have leveraged a ring topology and hierarchical ring can increase scalability but presents different challenges, including higher hop count and global ring bottleneck. In this work, we describe a hierarchical ring topology that we refer to as a transportationnetwork-inspired network-on-chip (tNoC) that leverages principles from transportation network systems. In particular, we propose a novel hybrid ﬂow control for hierarchical ring topology to scale the topology efﬁciently. The ﬂow control is hybrid in that the channels are allocated on ﬂit granularity while the buffers are allocated on packet granularity. The hybrid ﬂow control enables a simpliﬁed router microarchitecture (to minimize per-hop latency) as router input buffers are minimized and buffers are pushed to the edges, either at the output ports or at the hub routers that interconnect the local rings to the global ring – while still supporting virtual channels to avoid protocol deadlock. We also describe a packet-quota-system (PQS) and a separate credit network that provide congestion management, support prioritized arbitration in the network, and provide support for multiﬂit packets. A detailed evaluation of a 64-core CMP shows that the tNoC improves performance by up to 21% compared with a baseline, buffered hierarchical ring topology while reducing NoC energy by 51%. 1. Introduction The network-on-chip (NoC) is necessary to enable scalable manycore processors. The NoC is critical in determining overall system performance since it impacts latency and bandwidth as well as overall cost, including area and energy [34]. Some recent multicore processors, such as Intel Nehalem [36] and Larrabee [42], have adopted a ring topology for NoCs because of its simplicity. To scale the ring networks, hierarchical ring topology can be used but it is not clear how to properly scale the hierarchical ring topology efﬁciently. The hierarchical ring topology increases the hop count and the global ring that interconnects the local rings can become the bottleneck. In this work, we propose a novel hybrid packet-ﬂit ﬂow control for hierarchical ring topology that enables simpliﬁed router microarchitecture while providing high performance through the packet quota system (PQS) that we propose. We leverage principles from the transportation network domain and propose a transportation-network-inspired network-on-chip (tNoC). Recent urban transportation design avoids grid-like road structure, such as stop lights and trafﬁc lights, as much as possible since they are inefﬁcient in handling large amounts of trafﬁc [15]. Instead, roundabouts or ring roads are more efﬁcient in handling trafﬁc, and these roads can be scaled using a hierarchical structure, by introducing hubs. A hierarchical structure minimizes the complexity at the “endpoints” but it introduces some complexity in the hubs as they allow trafﬁc to be transferred from one region to another region. Properly managing trafﬁc or cars is also crucial as trafﬁc congestion results in delays and other social costs, including pollution and lost man-hours. As a result, a vehicle quota system has been proposed in Singapore to limit the number of vehicles on the roads and to manage congestion [38]. Based on these principles from urban transportation system design, we describe a hierarchical ring topology that we refer to as the transportation-network-inspired network-onchip (tNoC). We propose a novel hybrid packet-ﬂit ﬂow control where channels are allocated on ﬂit granularity while the buffers are allocated on packet granularity. This ﬂow control simpliﬁes the router microarchitecture such that the router input buffers are minimized and buffers only exist at the ejection ports and at the intermediate hub routers that interconnect the local rings to the global ring. The hybrid ﬂow control leverages our proposed packet-quota-system (PQS) such that the router microarchitecture can be simpliﬁed with prioritized arbitration while supporting conventional virtual channels [5] . The topology is similar to other hierarchical ring topologies that have been previously proposed [39] consisting of local rings and a global ring but the tNoC differs in the ﬂow control and implementation of the router. We introduce two types of routers – a terminal router that has minimal complexity and is connected to the endpoint terminals in the local ring and a hub router that interconnects the local rings to the global ring. In particular, the novel contributions of this work include the following: • We propose a novel hybrid packet-ﬂit ﬂow control for the hierarchical ring topology, in which channels are allocated on ﬂit granularity while the buffers are allocated on packet granularity. • Based on this hybrid ﬂow control, we propose packet quota system (PQS), which enables a lightweight router microarchitecture with support for virtual channels while simplifying the switch arbitration. (a) k = 8, n = 2 (b) k = 4, n = 3 Figure 1: Different hierarchical ring organizations for 64-node network. • We propose a credit network in parallel with the data network to support hybrid ﬂow control with PQS. • Our detailed comparison shows the tNoC improves performance by up to 21% compared with a buffered hierarchical ring while reducing NoC energy by 51%, and compared with the ﬂattened butterﬂy, improves performance (energy) by up to 7% (20%). 2. Background/Motivation 2.1. Hierarchical Ring Networks The ring networks have been used in multicore CPUs (including Intel Nehalem [36], Larrabee [42], and IBM Cell [18]) because of their simplicity. The detailed implementation of the ring architecture is not publicly available and some aspect of our work is similar to these ring architectures (e.g., prioritized arbitration [48]); however, our main contribution of this work is in how to create a scalable hierarchical ring network with the proposed novel hybrid ﬂow control. The ring network can be greatly simpliﬁed if we assume a single-ﬂit packet – i.e., assume the channels are wide enough to transmit an entire cache line in a single cycle. This approach might be feasible in a small-scale network but is very inefﬁcient since a signiﬁcant number of packets tend to be short packets [30]. Thus, the wide datapath would be not utilized for most packets. In addition, scaling such wide datapath to larger network size also becomes very inefﬁcient. A hierarchical ring [40] consists of local rings and a global ring that interconnects the local rings together. Using a similar notation as k-ary n-cube [4], a hierarchical ring can be described with k and n, where k is the number of terminal nodes in the local ring and n is the number of levels. If n = 1, the topology is a k-node ring. For n > 1, the hierarchical ring topology consists of n − 1 global rings. In this work, we assume a bidirectional ring architecture with minimal routing. Thus, for each ring (both local and global), it consists of two rings – a clockwise (CW) ring and a counter-clockwise (CCW) ring. We also focus on a 64-node network where we choose k = N using n = 2, where N is the number of nodes but the topology can be scaled by increasing either k or n (Scalability is discussed in Section 4.4). A 64-node hierarchical ring example with n = 2 and n = 3 is shown in Figure 1. Scaling a ring topology with a hierarchical ring topology presents two challenges: (1) high hop count and (2) global ring performance bottleneck. In this work, we try to address the √ (a) (b) Figure 2: (a) Average hop count and (b) average network latency comparison for alternative topologies. challenges of (1) by simplifying the router microarchitecture to reduce zero-load latency while for (2), we propose a hybrid ﬂow control to minimize blocking and improve the throughput of the hierarchical ring. 2.1.1. Network Latency The zero-load latency (To ) [6] of a packet can be summarized as follows. To = Th + Ts = H tr + L/b where Th is the header latency, Ts is the serialization latency, H is the hop count, tr is the per-hop router latency, L is the packet size, and b is the channel bandwidth. Alternative topologies [21, 2, 11] have been proposed to reduce network latency, by reducing the network diameter (H ). However, increase in the router radix can increase router complexity (which increases tr ) and can also increase Ts since the network channels are narrower, assuming constant bisection bandwidth across the different topologies. Low-dimensional topologies, such as a ring, often provide higher channel bandwidth (lower Ts ) at the cost of higher network diameter (H ). In Figure 2(a), the average hop count (H ) for the different topologies are shown for uniform random trafﬁc with minimal routing. The hierarchical ring (HRING) reduces H compared with the RING topology but it is still higher than other alternative topologies. For example, for N = 64, HRING reduces the hop count by 52.5% compared with the RING but it is still 43% higher than that of the 2D mesh topology and 5.1× higher than 2D ﬂattened butterﬂy (FBFLY). Instead of trying to reduce H , we explore reducing tr through a simpliﬁed router microarchitecture. The zero-load latency for a 64-node network is shown in Figure 2(a), based on tr from Table 6. Comparing FBFLY and HRING, the zero-load latency for long packets is relatively similar but the latency of short packets is signiﬁcantly higher with HRING because of higher H . However, with the same H as HRING, tNoC is able to reduce latency for both short and long packets by reducing tr . 2.1.2. Hybrid Flow control Conventional, buffered ﬂow control with virtual channels (VCs) [5] complicates router microarchitecture and makes it hard to reduce tr due to complex VC allocation stage. Recently proposed bufferless ﬂow control [8] can simplify the router microarchitecture but the high deﬂection routing across the global ring can reduce performance while increasing the complexity of the router. In addition, (a) (b) Figure 3: Block diagram of (a) grid roadway and (b) hierarchical, ring roadway. supporting VCs to avoid protocol deadlock in bufferless ﬂow control becomes very difﬁcult. In this work, we propose a hybrid ﬂow control where buffers are allocated on packet granularity while channels are allocated on ﬂit granularity. The hybrid ﬂow control enables simpliﬁed router microarchitecture with prioritized arbitration for packets in-ﬂight while providing support for multiﬂit packets and fairness. The proposed architecture is also able to provide support for virtual channels – e.g., prior work on reducing router complexity [20] simpliﬁed the router microarchitecture but did not support VCs. One key observation that we make is that VCs are not necessarily needed at all buffers if the packet does not hold on to the buffers indeﬁnitely. With prioritized arbitration and the hybrid ﬂow control (Section 3), packets in-ﬂight are guaranteed to make progress towards their destinations; thus, avoid adding VCs at all buffers in the network but introduce them only at the ejection ports and hub routers. 2.2. Transportation Network There are many similarities between roadway trafﬁc and network trafﬁc. In roadway trafﬁc, controlling of high demand trafﬁc can be achieved using trafﬁc signal that allows a group of vehicles to pass for a direction without conﬂict with trafﬁc for other directions. However, this type of control necessarily stops movement of vehicles and causes delay (Figure 3(a)). Therefore, highways including ring roads without trafﬁc signal can process higher volume of trafﬁc than arterials [15]. Highway is used for high volume trafﬁc and arterials with intersections are used for local accesses. Connecting arterials to main highway, controlling trafﬁc from arterial to highway is important to keep the highway from congestion. In this approach, highway has higher priority than arterial trafﬁc, because when the highway is congested, it will also block arterials soon. In grid-type arterial network with a number of intersections, the main purpose of control is to prevent ”spill over” of queue to the adjacent intersection. If a queue length increases and blocks the upstream intersection, we can sometimes meet a severe congestion state called ”gridlock” in which no vehicle can move to any direction. To prevent this disastrous situation, we can increase the number of lanes (increase bandwidth), give special lane for turning vehicles, and control the number of incoming vehicles from upstream. Ramp metering (injection control) is widely used to adjust the entering ﬂow rate to highway using trafﬁc sigFigure 4: Roadway example of new city that is built hierarchically [43]. nal. For the similar purpose, vehicle quota system has been proposed in Singapore to limit the amount of vehicles on the roads and manage congestion [38, 25]. Pursuing efﬁcient urban transportation system design, which minimizes total delay, strategies to mix of highway systems and arterials can be set with hierarchical network design. In hierarchical trafﬁc network design, transfer of people or goods follows three steps: (1) aggregating trafﬁc demand from local areas, (2) transferring the aggregated trafﬁc using wide roads or high-speed mass transit, and (3) distribution of the aggregated trafﬁc to destinations using local roads. Hub-and-spoke network is one example widely used for air transportation and freight transportation [10]. The trafﬁc is signiﬁcantly inﬂuenced by overall patterns of urban form. Although there is no clear metropolitan organization that is most efﬁcient, an urban form based on “decentralized concentration” attempted in Denmark and Sweden is one promising approach [41, 33]. This approach groups infrastructures and housings to form different “concentration” of urban location and they are interconnected with other concentration. Similarly Tokyo has also announced “Circular (Ring) Megalopolis Structure” to create a polycentric cities connected with ring roads in 2025 [50, 45]. The Sejong city [43] in Korea, which is a new settlement of Korea government complex that is under development, is another example of the hierarchical design of transportation network (Figure 4). An inner ring road is placed to connect major destination centers, functionally grouped, and local transportation using arterials delivers people to the nearby station on the ring road. Outer ring highway is used for trafﬁc from outbound to other cities and inbound to the city. By shaping trafﬁc pattern and volume, the Sejong city is expected to achieve high efﬁciency with minimum delay for travel. The trafﬁc found in transportation networks is not necessarily similar to on-chip network trafﬁc, but our goal is to leverage insights from transportation network in the design of a scalable, efﬁcient on-chip network. Thus, we leverage various principles of the transportation network (as summarized in Table 1) and propose transportation-network-inspired networkon-chip (tNoC) in this work – a hierarchical ring topology that leverage the novel hybrid ﬂow control. Table 1: Comparison of the transportation network and the proposed tNoC. Transportation network hierarchical road structure [33, 50] hub-and-spoke network [10] priority of highway trafﬁc over arterial trafﬁc [15] vehicle quota system [38] Proposed tNoC hierarchical ring topology terminal and hub router with network buffering only at the hub router prioritized arbitration for in-ﬂight packets packet quota system Topology Router microarchitecture Flow control Congestion management (a) (b) Figure 6: Microarchitecture of a (a) terminal router (Rt ) and a (b) hub router (Rh ). Table 2: Flow Control Categorization. Channel allocation unit Packet t Packet-based ﬂow control (store&forward, virtual cut-through) e k c a P Flit hybrid ﬂow control Flit-based ﬂow control (wormhole, virtual channel) t i r n u e n o u a c o f f i t B l l a t i l F N/A 1 Figure 5: Floorplan diagram of 64 cores connected with hierarchical ring topology 3. Hybrid Flow Control In this section, we ﬁrst provide an overview of our proposed tNoC architecture. We then describe the hybrid ﬂow control that enables simpliﬁed router microarchitecture with prioritized arbitration for a hierarchical ring topology. Leveraging the concept of vehicle quota system from transportation network, we describe the packet-quota system (PQS) for the hybrid ﬂow control and describe how fairness can be provided. 3.1. Transportation-Network-Inspired Network-on-Chip (tNoC) Architecture The topology of the tNoC is based on a hierarchical ring topology and consists of two types of routers – a terminal router (Rt ) for the local ring and a hub router (Rh ) that is used within the global ring. A high-level ﬂoorplan diagram of a 64-node network is shown in Figure 5. The ports in a router can be classiﬁed as either terminal ports, which are connected to terminal or endpoint nodes, or network ports, which are connected to other routers. A high-level block diagram of the router microarchitecture of the terminal and the hub router are shown in Figure 6. The router data path is simpliﬁed to include only a 2-to-1 mux and a pipeline register [20]. In addition, the router microarchitecture supports prioritized arbitration – i.e., packets in-ﬂight have priority over packets that are being injected into the network. However, we introduce intermediate buffers at the hub routers for packets that need to be routed from one local ring to another local ring through the global ring. The intermediate buffers are organized per network (one set of buffers for the CW ring and another set of buffers for the CCW ring). As necessary, multiple virtual channels are provided to avoid protocol deadlock. In addition, we leverage the ejection buffers that exist at the output of the ejection ports in the network. As we discuss in the following sections, the ejection buffer needs to be deeper than that of other topologies to support the tNoC and its ﬂow control. However, since there is only two ejection buffers for each router, the total number of buffers required is smaller. The number of buffers is proportional to O(N 1/k + N ) in the tNoC and not O(N p) as in conventional, input-buffered router networks, where p is the number of router ports, and N is the number of nodes in the network. The ejection buffers are included in the cost evaluation presented in Section 4. 3.2. Flow Control Description Flow control determines how the network resources (i.e., buffers and channels) are allocated [6]. Packets are often partitioned into one or more ﬂits or ﬂow control units. Most ﬂow controls in interconnection networks are either packetbased ﬂow control or ﬂit-based ﬂow control (Table 2). In a packet-based ﬂow control, which is commonly used in offchip networks, both the channels and the buffers are allocated in units of packets. In comparison, ﬂit-based ﬂow control allocates both resources in units of ﬂits. Examples of the ﬂit-based ﬂow control include wormhole or virtual channel ﬂow controls. On-chip networks have often utilized the ﬂit-based ﬂow control. In this work, we propose a hybrid ﬂow control where the buffers are allocated in units of packets while the channels are allocated on ﬂit granularity. An example illustrating the different ﬂow controls is shown in Figure 7(a) with a time diagram of a channel that is used by two packets. With the ﬂit-based ﬂow control, the ﬂits from different packets can be interleaved on the channel while with packet-based ﬂow control, packets are not interleaved. With the hybrid ﬂow control, interleaving can still occur, similar to the ﬂit-based ﬂow control. However, the main difference is in how the buffer resource is allocated. 1 Packet-channel and ﬂit-buffer ﬂow control is not possible since if a channel is allocated on packet granularity but there is not sufﬁcient buffers, the ﬂow control would not operate properly. (a) (b) Figure 7: Different ﬂow controls comparisons with (a) time diagram and (b) destination buffer occupancy. Figure 8: Block diagram of packet 2 interrupting multi-ﬂit packet 1 with prioritized arbitration. In our tNoC architecture with prioritized arbitration, packetbased ﬂow control cannot be supported since channels cannot be allocated on packet granularity. An example of this problem is shown in Figure 8 – R0 is injecting packet1 into the network and if packet2 arrives, packet2 has priority. Thus, packet1 will be interrupted and the channel cannot be allocated on packet granularity. In comparison, ﬂit-based ﬂow control can be used but it signiﬁcantly complicates the destination buffer since ﬂits from different packets can be interleaved (Figure 7(b)) and a complex, re-order buffer is needed. To overcome this, the hybrid ﬂow control allows ﬂit-granularity allocation of the channels while the buffers are allocated on packet granularity. Before any packet is injected into the network, the buffer for the entire packet needs to be allocated; thus, for a packet of L ﬂits, the router needs to obtain L credits before the packet can be sent. Once the buffer for the entire packet has been allocated, the channel resource can be allocated on ﬂit granularity. Because of prioritized arbitration, the injection of a multi-ﬂit packet can be interrupted; thus, the packet will not necessarily be sent continuously. However, when the head ﬂit arrives at the destination, it reserves the next L − 1 slots in the buffer (Figure 7(b)) such that L ﬂits can be continuously written in case the packet transmission is interrupted. There are several characteristics of the proposed hybrid ﬂow control in the tNoC that simplify the design of re-order buffers; 1) ﬂits from a single packet will still arrive in-order but not necessarily in consecutive cycles, and 2) there will not be multiple partial packets within a re-order buffer that are sent from the same source to the same destination. Thus, to support re-ordering, each ﬂit needs to carry the source and destination information within the local ring or the global ring (e.g., 2l og(k/2) bits), and in the ejection buffer, multiple write pointers are needed for each source that is sending a packet to this particular destination. Similar to a FIFO, only a single read pointer is needed since the read is done in order. 3.3. Packet Quota System (PQS) The purpose of ﬂow control in the tNoC is for packets injected into the ring to proceed to the destination buffer without conFigure 9: Block diagram of the proposed credit network. tention – where destination buffer is either the intermediate buffers at the hub router or the ejection buffers at the packet destination. This can be achieved by the following policies: (1) prioritizing in-ﬂight packets, (2) removing contention at the ejection port, and (3) guaranteeing a buffer at the destination network interface. Although there is no contention in the network, there can be contention for the ejection ports in a bidirectional ring since packet can arrive from both the CW and CCW ring. This problem can be avoided with per-network buffering at the ejection port, as shown earlier in Figure 6(a). However, even with such buffering, backpressure can build (i.e., the terminal node can be stalled and not be able to eject a packet) – which is problematic without any buffers in the router. 3.3.1. Credit Network In this work, we propose packet quota system (PQS) to provide support for hybrid ﬂow control, while minimizing network congestion. The PQS ensures that when packet arrives at the destination (or the intermediate buffer at the hub router), the packet can be buffered. The PQS is based on conventional the credit-based ﬂow control where a credit signals the availability of a buffer entry. We propose adding a narrow credit network in parallel to the data network. The credit network is responsible for circulating the credits within each level of the hierarchical ring. The credit network that we propose is different from prior token ring [46] approaches used in LAN networks and other ring networks as tokens represented the ability to access the channel (or the medium). In comparison, the credits that we circulate in the ring represent buffer availability at the destination (either the output ejection port buffer or the intermediate buffer at the hub routers) and are decoupled from the channel usage. As long as there is buffer space available, the destination or the “home” node injects a credit into the credit network. The source is only allowed to inject a packet after it obtains a credit for its destination (or the intermediate buffer). For a multi-ﬂit packet, multiple credits are needed. Once a packet arrives at the destination, the particular credit can be re-injected into the credit network. By ﬁrst grabbing credits, it ensures that, when a packet arrives at the destination, it will be ejected (or removed from the network) since there is buffer space available and not cause backpressure. Thus, packets that are injected into the network, in combination with priority arbitration, are guaranteed to make progress towards the destination or the hub router and avoid the need for network input buffers. A block diagram of a credit network is shown in Figure 9, which illustrates credits ﬂowing from a single “home” router (a) (b) Figure 10: (a) Baseline credit network and (b) folded credit network. to the other “leaf ” routers in the ring. The home router is the router that injects the credits into the credit network while the leaf routers either consume the credit or pass the credit to the neighboring router. For a k-node ring, this simple network needs to be duplicated k times since each terminal (endpoint) or the hub router needs to be a home router in the credit network to distribute its credits. Initially, each home router initializes each credit counter (Ccr ) to the size of the buffer. If (Ccr > 0), the home node router injects a credit into the credit network. Unused credits will return back to the home router. 3.3.2. Fairness Fairness is an issue in the credit network in a ring topology since upstream nodes can grab more credits than downstream nodes. An example of the problem is shown in Figure 10(a) where credits are injected from R0. The credit network consists of bidirectional rings as credits are injected in both directions. Without any support for fairness, the nodes closest to R0 will grab all of the credits. To illustrate such situation, a hotspot trafﬁc where all nodes send trafﬁc to R0 is simulated and the results are shown in Figure 11(a). The results show a bimodal distribution of throughput across all of the nodes. The two nodes closest to the hotspot nodes (R1 and R8) have the highest throughput, while the nodes farthest away (R4 and R5) are unable to inject any packet into the network and are starved. To prevent this unfairness, we impose a quota on the number of credits that each node can grab. Although the credits are injected into the credit network in both directions, with minimal routing, only half of the nodes use each type of credit. That is, nodes R1, R2, R3 and R4 would only use the CCW data network to send trafﬁc to R0 and only use the CW credit network. Thus, we restrict each router to grab only 1/d of the credits where d is the number of downstream routers, including itself, that can use the credits. For example, R1 is restricted to only grab 1/4 of the credits that pass R1, and R2 is restricted to grab 1/3 of the credits, while R3 is allowed to grab 1/2 of the credits that pass by. The unused credits ﬂow across the ring and return to the source (e.g., R0). However, since the other downstream nodes (R5, R5, R7, R8) cannot use these credits, we fold the credit network and allow the credits to return in the opposite direction, as shown in Figure 10(b). This allows the unused credits to be grabbed by the nodes (e.g., R1, R2, R3) without any restriction and enables higher utilization of the credits. The PQS algorithm implementation is described in Algorithm 1. The PQS and the quota at each router node is implemented using a quota counter (Cq ) that describes the amount of quota that the router has, another counter (Cgrab ) that describes Algorithm 1 PQS Implementation init: Cq = 0 At each router cycle: if current ﬂit is a head ﬂit then % L : packet size cost = k/2 − D + 1 % D : d ist ance t o t he home rout er Cgrab = L end if if Cq > cost and Cgrab > 0 and credit is available then grab a credit Cq = Cq − cost Cgrab = Cgrab − 1 end if if credit is available then Cq = Cq + 1 end if (a) (b) Figure 11: (a) Packet accept rate for node0 hotspot trafﬁc and (b) latency-throughput curve of tNoC with folded credit network in UR trafﬁc. how many credits are currently needed, and cost parameter to provide fairness. The cost parameter is equal to d or the number of downstream routers and is essentially the cost that a router needs to pay to grab a token to ensure fairness. Routers closer to the home node have a higher cost compared with downstream routers. Cgrab is initialized to the packet size, in terms of the number of ﬂits, and represents the number of credits that needs to be grabbed. For each credit that passes the router node, Cq is incremented by 1, and when quota is available (Cq > cost ), the credit is consumed. Using this folded credit network with a quota, the fairness across all of the nodes improves signiﬁcantly as shown in Figure 11(a). In addition, the impact of the folded credit network and quota is shown using a latency-throughput curve for uniform random trafﬁc in Figure 11(b) 2 . Folding the credit network enables similar throughput with a smaller number of ejection buffers. In the rest of this work, we will assume the tNoC with the PQS, and the folded-credit network in our evaluation. 3.4. Deadlock A deadlock can occur in tNoC if all nodes have some credits while waiting for more credits to transmit a multi-ﬂit packet. To guarantee deadlock freedom, the destination buffers need to be deep enough to ensure such deadlock does not occur. The worst-case scenario is if all nodes are sending a 2 The evaluation setup is described in Section 4.1. The evaluation in this section focuses on a single ring while the evaluation in Section 4 presents the hierarchical ring. Table 3: Simulation Parameters Processor L1 Caches L2 Caches Cache coherence Memory controllers Buffered ﬂow control (BUFF) tNoC Common parameter 64 in-order/out-of-order cores @ 2GHz Split I&D, 32 KB 4-way set associative, 2-cycle access time, 64B line Shared L2, 256KB per tile (total 16MB) with various sharing degree [16], 64B line Directory-based MOESI with sharing degree of 8 and 64 8 NoC parameters for different ﬂow controls speculative 2-stage virtual channel router 3 message class, 4 VCs/class, 8 ﬂits/VC dateline routing for routing deadlock avoidance 3 message class, 13 ﬂits/class for interm. and ejection buffer 1 cycle router delay for terminal router 3 cycles for passing through intermediate buffer Table 4: Workload description SpecCPU2006 benchmarks perlbench bzip2 gcc bwaves gromacs cactusADM leslie3D namd gobmk dealII soplex povray calculix hmmer sjeng GemsFDTD libquantum h264 tonto lbm perlbench gamess gromacs gobmk mcf soplex libquantum GemsFDTD perlbench povray calculix tonto mcf lbm milc libquantum leslie3d soplex sphinx3 GemsFDTD barnes, cholesky, fft, fmm, radiosity, radix, raytrace, volrend MIX 1 MIX 2 MIX 3 MIX 4 MIX 5 MIX 6 MIX 7 MIX 8 MIX 9 MIX 10 SPLASH2 packet with a size of max_ packet _size ﬂits and each node has max_ packet _size − 1 credits. Thus, the amount of buffers needed is k/2 ∗ (max_ packet _size − 1) + 1, in units of ﬂits, to avoid such deadlock. With the wide channels in the hierarchical ring, the maximum packet size, in terms of ﬂits, is relatively small (4 ﬂits in our tNoC evaluation); thus, the number of ejection buffers needed is 13 entries to avoid deadlock. To reduce the cost of ejection buffer, we leverage the network interface buffer that often exists and is often big enough to hold the maximum size packet for depacketization. Note that only two ejection buffers are need per router in tNoC, compared with 5 set of input buffers for a conventional 2D mesh router. 4. Evaluation 4.1. Methodology In this work, we modiﬁed Booksim [17], a cycle accurate network simulator, to model our proposed tNoC network, including the hierarchical topology, both the data and the credit network, the router microarchitecture, and the ﬂow control. Booksim was used for synthetic workload evaluation and integrated into McSimA+ [1] to evaluate a 64-node CMP architecture for multiprogrammed/multithreaded workloads. GEMS [31] was used to model the cache hierarchy. The parameters used in our simulation are summarized in Table 3. In our workload evaluation, we evaluate cache locality with hierarchical cache using the concept of the sharing degree (SD) [16]. Figure 12 illustrates the hierarchical cache with cache sharing degree of 8 and fully shared cache in a 64-node CMP. We used SpecCPU 2006 [14] benchmark suite for multiprogrammed workloads and SPLASH2 [53] benchmarks for multi-threaded workloads. For multithreaded workload results, (a)sharing degree = 8 (SD8) (b)sharing degree = 64 (SD64) Figure 12: Various sharing degree of shared L2 cache Table 5: Comparison of alternative ﬂow controls in the hierarchical ring topology. Contention Deadlock Buffers Router delay (cycles) Fairness Additional overhead N/A virtual channels per-hop buffering Buffered (BUFF) buffer backpressure Bufferless (HiRD [9]) Hybrid (tNoC) deﬂection or minimized by drop/retransmit PQS injection guarantee by virtual channels global coordinator, at intermediate swap at global router buffer re-order buffer, intermediate and intermediate buffers ejection buffers 2 (global router) 2 (global to local 3 ) 2 1 (local router) 1 local (round-robin) N/A PQS global coordinator, complex coherence protocol support credit network performance results represent the execution time of each workload. The multiprogrammed workload results presented in this paper are weighted speedup results from 10 mixes shown in Table 4. We choose four benchmarks from SpecCPU 2006 benchmarks and a copy of each benchmark is simulated on 16 cores. Verilog RTL was implemented for the alternative router microarchitecture and synthesized for critical path analysis as well as area/power comparison. All of the overhead, including the credit network and the hybrid ﬂow control, are faithfully modeled and their overheads are included in the cost evaluation. McPAT [27] was used for chip-level power and area estimation. 4.2. Comparison to Alternative Flow Controls We ﬁrst compare the proposed hybrid ﬂow control of the tNoC with baseline, buffered ﬂow control (BUFF) on the hierarchical ring topology. Recently, bufferless ﬂow control (with intermediate buffers) [9] 4 has been proposed and we provide a qualitative comparison in Table 5. Both the tNoC and the HiRD have intermediate buffering between the local and the global ring but main difference is that the HiRD uses deﬂection routing when congestion occurs while we leverage the hybrid ﬂow control and PQS and avoid the cost of deﬂection. In addition, the HiRD requires a global coordinator to prevent deadlock and provide fairness in the network and conventional virtual channel semantics are not supported in the bufferless HiRD architecture. In comparison, no global structure is necessary in the tNoC and the main overhead is the additional 3Additional latency of one cycle is only added when a packet switches from a local ring to a global ring, or vice versa. 4 This work was done concurrently with [9] but all of the details of the HiRD architecture were not clear from the technical report. As a result, providing an accurate quantitative comparison with HiRD was difﬁcult to achieve. (a) UR (b) LOC Figure 13: Latency-throughput curve comparison of alternative ﬂow control for (a) UR, and (b) LOC=0.9 trafﬁc. (a) in-order (b) out-of-order Figure 15: Normalized latency breakdown in SD64. (a) in-order (b) out-of-order Figure 14: Normalized performance results credit network. 4.2.1. Synthetic Workload For the synthetic workloads, we evaluate two types of trafﬁc: uniform random (UR) and trafﬁc with locality (LOC). UR trafﬁc represents a fully shared cache organization, while LOC represents trafﬁc with various degrees of locality in the cache organization. For the LOC synthetic trafﬁc pattern, we use SD8 organization and vary the locality, where a locality of 0.9 means 90% of the trafﬁc is sent to the shared local caches, while the remaining 10% of the trafﬁc is sent to remote caches. Figure 13 shows latency-throughput comparison for the different trafﬁc patterns. Since we are comparing alternative ﬂow controls on the same hierarchical topology, we assume single-ﬂit packets in our initial comparison. Other results with real workloads include both short packets and long (multi-ﬂit) packets. For the BUFF, we evaluate with 2, 4, and 8 virtual channel conﬁgurations. The performance of the BUFF increases as VCs are increased but results in higher zero-load latency (because of the higher per-hop latency) and reduced throughput because of the blocking in the network. For UR trafﬁc, the zero-load latency for the tNoC is 32% lower than that of the BUFF. The latency for the tNoC includes the latency to grab the credits but it has minimal impact on overall performance. Since there are sufﬁcient credits at zero-load and no additional latency is required to grab the credits. At high load, near saturation, the network data channel becomes the bottleneck and thus, the additional waiting time for credits has minimal impact on overall performance. In addition, the tNoC improves throughput by 33% and 25% on UR and LOC respectively, compared with the BUFF. Simulations show that increasing the number of VCs increased throughput but at additional cost. With the BUFF, the in-ﬂight packets can be blocked by packet injection by terminal node or packets (a) energy (b) router area Figure 16: Energy breakdown results and router area normalized to the BUFF. Rt : terminal router, Rh : hub router. blocked at hub router and it results in reduction in throughput. By minimizing the network contention, the tNoC is able to provide higher throughput compared with the BUFF. 4.2.2. Real Workload Figure 14(a) shows the performance results of SPLASH2 and SpecCPU normalized to the BUFF. In SD8, the tNoC improves performance by up to 9.6%, compared with the BUFF because of lower per-hop latency. The tNoC also shows better performance in SD64, especially for memory intensive workloads such as fft, raytrace, and radix – improving performance by up to 11% (8% on average) compared to the BUFF. With out-of-order (OoO) cores, the additional network trafﬁc generated from the OoO cores results in high contention for the BUFF (Figure 14(b)). Thus, there is an increase in the performance gap as the tNoC exceeds the performance of the BUFF by up to 21%. 5 Figure 15 shows the latency breakdown of the different ﬂow controls. For in-order core, both contention delay in the BUFF and latency to grab credits in the tNoC are low. The tNoC reduces the average packet latency by 22% by reducing head latency with lower per-hop latency. For OoO core, the latency to grab credits in the tNoC is lower than contention delay in the BUFF. While for the high trafﬁc load, the BUFF blocks the packets in the router input buffers which can result in blocking of other packets that can proceed, the tNoC only blocks the packet at injection or intermediate router. As a result, the tNoC reduces the packet latency by 23% for OoO cores. 4.2.3. Overhead The costs of the BUFF and the tNoC are compared in Figure 16 in terms of network energy and area. This includes all of the overhead for the tNoC, including the ejection/intermediate buffer the credit network, and the termi5Because of page limitation, only OoO core results for the alternative ﬂow control comparison is shown. The rest of the results are shown only for in-order cores. Table 6: Alternative NoC Comparison parameters. Ports Message class VCs / class Buffers depth Critical path(ns) Router delay Channel delay MESH CMESH FBFLY HNET HRING tNoC 5 8 10 12 3 3 3 3 3 3 3 3 4 4 4 4 4 1 8 8 12 8 8 13 0.99 1.10 1.16 1.22 0.86 0.42 2 2 2 2 2 1 for Rt 1 1 0.5/tile 1 1 2 for Rh nal and the hub routers. The tNoC reduces the buffer leakage energy by reducing the buffer size and also reduces the buffer read/write energy and pipeline register access energy by removing input port buffers and reducing pipeline stages. Although the tNoC introduces additional energy consumption at credit network and additional storage, the tNoC reduces NoC energy by 52% and 50% compared with the BUFF, in SD8 and SD64 respectively. The tNoC also reduces router area with the reduction in the amount of input buffers. 4.3. Comparison to Other Topologies We compare the tNoC to alternative topologies, including MESH, MESH with 4-way concentration (CMESH) [2], ﬂattened butterﬂy with 4-way concentration (FBFLY) [21], and a hierarchical network (HNET) with an 8-way external concentration [24] as an implementation of a hybrid network of bus and mesh [7]. 6 The router designs of the different topologies were based on the Verilog model [47] which implements a two-cycle speculative virtual channel routers. The proposed tNoC was also implemented in Verilog and all overhead was modeled, including both the data network router and the credit network router, as well as the buffers for both terminal routers and hub routers. Synopsys Design Compiler with topographical mode was used to provide a better estimate of the wires, and we used TSMC 45nm technology. 4.3.1. Timing Analysis The synthesis results of the virtual channel routers in the various topologies and the tNoC router are shown in Table 6. Except for the tNoC, all the routers are synthesized as two-stage routers. The tNoC terminal routers are single cycle per hop, while the hub routers are two cycles. The critical paths for the other routers are the virtual channel/switch allocation, and the critical paths increase with higher port count. However, the removal of the arbitration from the critical path in the tNoC signiﬁcantly reduces the critical path for both terminal and hub routers, by up to 57% compared with the ﬂattened butterﬂy topology router. In the tNoC router, the control (credit network) and the data network are decoupled, and this helps reduce the critical path. Synthesis results show that the tNoC critical path in the credit network is 0.42ns while the datapath of the tNoC is only 0.33ns. Even though critical path reduction enables higher network frequency for the tNoC, we conservatively assume that the tNoC runs at the same network frequency for the initial comparisons. We also presents results when applying the dif6We do not model X-share [7] in our HNET implementation. X-share will likely improve performance at additional cost, but can also be applied to other topologies. (a) UR (b) LOC Figure 17: Latency-throughput comparison of different topologies with (a)UR and (b)LOC=0.9 trafﬁc patterns. (a) SPLASH2 (b) SpecCPU Figure 18: Total performance vs energy scatter plot for SD8. ferent network frequency for each topologies, which enables the tNoC to have more network bandwidth with higher router frequency. 4.3.2. Synthetic Workload We evaluate various topology under constant bisection bandwidth. With constant bisection bandwidth, packets have different numbers of ﬂits – 4 ﬂits for MESH, 2 ﬂits for CMESH, 8 ﬂits for FBFLY, and 1 ﬂits for HNET, HRING, and tNoC. Figure 17 shows latency throughput curves under UR and LOC trafﬁc. For UR trafﬁc, despite the large hop count, the tNoC provides a comparable latency because of lower router delay with simpliﬁed router pipeline stages. FBFLY shows higher zero-load latency because of high serialization latency due to its narrow channel. HNET has the lowest zero-load latency since it has the lowest average hop count (with 8-way concentration). However, throughput of HNET is very limited because of the 8-way external concentration [24]. For LOC trafﬁc, the tNoC outperforms other topologies in both latency and throughput. The tNoC and HNET show a lower zero-load latency because both topology have wider channels that minimize serialization latency while header latency is relatively small because of the low average hop count from the LOC trafﬁc. Similarly, the higher per channel bandwidth improves the throughput of the tNoC compared with the alternative topologies. 4.3.3. Real Workload Figure 18 shows the performance and energy results for alternative topologies and different channel widths. The x-axis is the overall performance, while the y-axis is the inverse of the total chip energy. Each curve represents the same cost per performance metric; different points on the same line represent a trade-off between performance and energy. An ideal network approaches the upper right corner of the plot, Figure 19: Packet latency breakdown in SD8. which indicates the network with the lowest overall system cost and highest system performance. In Figure 18(a), we plot the performance of each network as we vary the network channel width from 8B to 72B, as the short packets are 8B and the long packets are 72B to better understand the topology trade-off. Our results show that the optimal network designs for various topologies are different. Thus, for the rest of the comparison, instead of assuming constant bisection bandwidth, we choose the most energy-efﬁcient channel width for each topology, as highlighted as an example with circles in the Figure 18(a). The various NoCs are compared in Figure 18. The tNoC results in 21% performance improvement over MESH, while reducing energy by 20% for SpecCPU and resulting in 16% (24%) improvement in performance (energy) for SPLASH2. Compared to CMESH, the tNoC achieves 5% (13%) improvement for SpecCPU workloads while the tNoC achieves 7% (20%) improvement over FBFLY. Latency breakdown of the different topologies is shown in Figure 19, where latency is divided into Th , Ts , as well as contention latency Tc (or queuing latency in the network) components. For the tNoC, additional component is the latency to acquire a credit as well as queuing latency in the intermediate buffer of the hub router. The tNoC shows lower header latency (Th ) by reducing hop delay and lower serialization latency with wider channels. As described earlier, the channel width of other topologies can also be increased but the design points used in the comparisons are based on the most efﬁcient design for each topology. On average, the tNoC reduces latency by 53% compared with the mesh and by 24% compared with the FBFLY. Figure 20 is the same comparison as Figure 18 but different NoC frequencies are used based on synthesis results for each topology. By leveraging the higher frequency of the tNoC, the efﬁciency of the tNoC is further improved – energy efﬁciency and performance by 38% (34%) and 20% (16%), respectively, for multiprogrammed (multithreaded) workloads, compared with the most efﬁcient, alternative topology. 4.4. Scalability We evaluate the scalability of the tNoC by comparing the performance in a 256-node network with SD8 and SD256 organization (Figure 21). For the tNoC, we still maintain a two-level hierarchical ring (k = 16, n = 2) in our comparison. With SD8, the tNoC is able to achieve both improvement in performance and reduction in energy, compared with the most efﬁcient alternative topology (CMESH). For a fully shared or(a) SPLASH2 (b) SpecCPU Figure 20: Energy vs. performance results for SD8 with different NoC frequencies based on synthesis timing results. (a) SD8 (b) SD256 Figure 21: tNoC Comparison for N=256 system. ganization (SD256), the most efﬁcient topology is the FBFLY and there is a performance-cost trade-off – the tNoC results in 9% reduction in performance but 17% reduction in energy. Even though the per-hop delay is still lower with tNoC, the much higher hop count from global trafﬁc in N = 256 network results in performance degradation. However, complexity of the routers, especially the high-radix routers in FBFLY increases the energy consumed in the routers and thus, the tNoC results in improvement in energy consumption. Thus, the tNoC and the hybrid ﬂow control proposed in this work are scalable, compared with alternatives. In addition, to further scale the network, concentration can also be leveraged as well – i.e., instead of having a single core connected to a terminal router, multiple cores can share a single terminal router. 4.5. Worst-case Trafﬁc Pattern Analysis The hierarchical nature of the proposed tNoC can result in an adversarial trafﬁc pattern when all trafﬁc is sent from the local ring through the global ring to another local ring. An adversarial trafﬁc pattern in a ring topology is the tornado trafﬁc [6] and the worst-case trafﬁc pattern 7 for a hierarchical ring is where all trafﬁc pass through the global ring (i.e., all trafﬁc generated from one local ring is send to a different local ring) and the global trafﬁc pattern results in tornado trafﬁc. As shown in Figure 22, the throughput of the tNoC suffers compared with other topologies, resulting in a reduction of throughput by 44% compared with CMESH. However, it is very unlikely that such an adversarial trafﬁc pattern will occur 7Note that the worst-case trafﬁc pattern is not necessarily the worst-case for other topologies. minimizing the number of buffers while introducing intermediate buffers. However, prior work had no support for VCs and credit-based ﬂow control impacted the router cycle time. In this work, the proposed router microarchitecture provides support for VCs while removing credit-based ﬂow control. Bufferless NoC [8] has been proposed which removes router input buffers. By properly accounting for the cost of buffers and channels, [32] showed minimal beneﬁts of a bufferless NoC compared with a buffered NoC. Another problem with a bufferless NoC is avoiding protocol deadlock since virtual channels are not supported. NoC-out [28] also proposed a simpliﬁed router microarchitecture by exploiting the communication characteristics of the scale-out workloads. However, the communication pattern for the workloads that we evaluate is different from the scale-out workloads. Flow Control: The token-ring has been widely used in local area networks. Recently, the token-ring has been proposed for channel arbitration in the on-chip nanophotonics [51, 35] but tokens are primarily used for channel arbitration to guarantee a slot on the data channel. The folded-credit network proposed in this work is similar to the two-pass token network proposed for on-chip nanophotonics [35]. However, the key difference is that in the ﬁrst pass in [35], each token was dedicated for a particular node which can increase the amount of time it takes to grab a token whereas in our approach, such dedication is not required but is determined by the predetermined quota. In addition, prior works on nanophotonic assume single-ﬂit packets which simplify the arbitration. Token ﬂow control (TFC) [23] is similar to our proposed credit-token network as the guaranteed token represents buffer space availability, similar to our credit token. However, the main difference is that the tokens are used to generate lookahead signals that enable bypassing of intermediate routers, while our proposed token is leveraged to minimize arbitration in the network. In addition, TFC is implemented on top of a baseline buffered ﬂow control and requires large buffers and complex management of the different tokens while our mechanism is implemented on top of simpliﬁed router microarchitecture. Flit-reservation ﬂow control [37] shares similarity as channel and buffer resources are reserved by a control ﬂit. However, we decouple the allocation of channel and buffer and avoid the complexity of a reservation table. Quota-based schemes have been previously proposed to provide qualityof-service (QoS) [26, 12]. However, they rely on complex mechanisms such as global barrier network and large source buffers or managing per-ﬂow states by each router to provide QoS. The purpose of our PQS is not necessarily to support QoS but provide support for the proposed hybrid ﬂow control. It remains to be seen if PQS can be extended to provide QoS support. Source throttling [52, 19] has been proposed to prevent buffer congestion but these have been proposed on top of conventional, buffered ﬂow control or bufferless networks. Grot et al. [13] also proposed a hybrid ﬂow control in their kilo-NoC but their hybrid ﬂow control combined Figure 22: Worst-case trafﬁc pattern for the tNoC comparison. Figure 23: Comparison with atomic VC allocation. in real workloads. In addition, a recent study on scale-out processors [29] showed that hierarchical and modular memory hierarchy makes optimal use of die area. Recent processors including SPARC T4 [44] and AMD Bulldozer [3] also have hierarchical memory hierarchy. However, if the system needs to support such adversarial trafﬁc pattern as described above, the tNoC can be enhanced to provide more bandwidth by either increasing the channel bandwidth or duplicating the global network. 4.6. Atomic VC Allocation Comparison Atomic virtual channel allocation (AVC) [6] has some similarity with the proposed hybrid ﬂow control, since with the AVC the head ﬂit of a packet is not buffered behind the tail ﬂit of another packet in the same virtual channel buffer. In effect, buffers are implicitly allocated on packet granularity, even if ﬂit-based ﬂow control is used. However, one challenge with the AVC is the amount of buffers required to provide a high throughput since the buffer depth needs to be equal to the size of long packets and the buffer utilization will be low for short (single-ﬂit) packets. As a result, much more buffering is necessary with the AVC. Figure 23 compares the AVC with the tNoC as the amount of endpoint buffers (T ) is varied with single-ﬂit packets to understand the limitation of the AVC. Since we assume large packets are 4 ﬂits, the AVC needs to dedicate 4 entries to each “VC” and thus, for AV C(T = 4), there is signiﬁcant loss in throughput. To achieve similar throughput as t N oC(T = 4), the AVC needs approximately 3× to 4× the amount of buffers to achieve similar throughput as the tNoC. 5. Related Work Hierarchical Topologies: The hierarchical ring has been proposed in the multi-processor domain to extend ring topology to scale to a large number of nodes [40]. Our hierarchical ring topology is similar to prior topologies, but it differs in how the routers and the ﬂow control are implemented. Udipi et al. [49] proposed a segmented bus-based hierarchical bus with a bloom ﬁlter to effectively track data presence and restrict bus broadcasts to a subset of segments. However, a hierarchical bus differs as the arbitration is done by a centralized arbiter. Router Microarchitecture: The proposed router microarchitecture is similar to previously proposed router microarchitecture [20, 22] – simplifying the router microarchitectures and elastic-buffered ﬂow control with minimal VC ﬂow control. 6. Conclusion In this work, we proposed a hybrid ﬂow control for a hierarchical ring topology where the channels are allocated on ﬂit granularity while buffers are allocated on packet granularity. The hybrid approach enables the terminal routers to be simpliﬁed with minimal buffers while supporting priority-based arbitration that simpliﬁes the router allocation in a hierarchical ring topology. Borrowing the idea of the vehicle quota system from transportation networks, we propose packet quota system (PQS) that minimizes network congestion. Although we minimize buffers, we provide support for conventional virtual channels – which simpliﬁes the network support for protocol deadlock. Detailed evaluations with various workloads show that the tNoC is able to improve performance by up to 21% compared with the baseline hierarchical ring topology while reducing NoC energy by 51%. We also compare with the alternative topologies, and our results show the tNoC is able to improve performance by up to 7% compared with the ﬂattened butterﬂy topology while reducing chip energy by up to 20%. Acknowledgements We would like to the thank the anonymous reviewers for their comments. This work was supported in part by the IT R&D program of MSIP/KEIT (10041313, UX-oriented Mobile SW Platform) and in part by Basic Science Research Program through the NRF of Korea funded by the MSIP (NRF-20110015039). "
2015,Power punch - Towards non-blocking power-gating of NoC routers.,"As chip designs penetrate further into the dark silicon era, innovative techniques are much needed to power off idle or under-utilized system components while having minimal impact on performance. On-chip network routers are potentially good targets for power-gating, but packets in the network can be significantly delayed as their paths may be blocked by powered-off routers. In this paper, we propose Power Punch, a novel performance-aware, power reduction scheme that aims to achieve non-blocking power-gating of on-chip network routers. Two mechanisms are proposed that not only allow power control signals to utilize existing slack at source nodes to wake up powered-off routers along the first few hops before packets are injected, but also allow these signals to utilize hop count slack by staying ahead of packets to 'punch through ' any blocked routers along the imminent path of packets, preventing packets from having to suffer router wakeup latency or packet detour latency. Full system evaluation on PARSEC benchmarks shows Power Punch saves more than 83% of router static energy while having an execution time penalty of less than 0.4%, effectively achieving near non-blocking power-gating of on-chip network routers.","Power Punch: Towards Non-blocking Power-gating of NoC Routers  Lizhong Chen1, Di Zhu2, Massoud Pedram2, and Timothy M. Pinkston2  1School of Electrical Engineering and Computer Science, Oregon State University, USA  2Department of Electrical Engineering, University of Southern California, USA  chenliz@eecs.oregonstate.edu, {dizhu, pedram, tpink}@usc.edu  Abstract  As chip designs penetrate further into the dark silicon  era, innovative techniques are much needed to power off  idle or under-utilized system components while having minimal impact on performance. On-chip network routers are  potentially good targets for power-gating, but packets in the  network can be significantly delayed as their paths may be  blocked by powered-off routers. In this paper, we propose  Power Punch, a novel performance-aware, power reduction  scheme that aims to achieve non-blocking power-gating of  on-chip network routers. Two mechanisms are proposed  that not only allow power control signals to utilize existing  slack at source nodes to wake up powered-off routers along  the first few hops before packets are injected, but also allow  these signals to utilize hop count slack by staying ahead of  packets to “punch through” any blocked routers along the  imminent path of packets, preventing packets from having to  suffer router wakeup latency or packet detour latency. Full  system evaluation on PARSEC benchmarks shows Power  Punch saves more than 83% of router static energy while  having an execution time penalty of less than 0.4%, effectively achieving near non-blocking power-gating of on-chip  network routers.  1. Introduction  A significant challenge for the design of current and future many-core chips is how to provide fast and efficient onchip communication. While network-on-chip (NoC) offers a  potentially scalable solution, current designs consume substantially more power than may be needed (e.g., up to 28%  in Intel Teraflop [16] and 19% in Scorpio [10]), with a large  percentage of static power (over 60% even for simpler router designs) due to relatively low average traffic load of real  applications. Static power consumption is exacerbated as  transistor feature sizes continue to shrink. Meanwhile, with  more cores being integrated on chips, there is an increasing  demand for low latency NoCs due to the longer core-to-core  hop distance. Thus, it is of paramount importance to devise  effective NoC static power saving techniques that do not  compromise NoC performance.   Power-gating is a very promising technique that can reduce the static power component dramatically. When applied to on-chip network routers, however, power-gating is  prone to incur a significant increase in packet latency due to  blocking. When a router is powered off, it essentially blocks  all paths that intersect with the router. Packets thus have to  wait for the router to wake up before proceeding and experience wakeup latency multiple times before delivery in cases  where many routers along the path are powered off. This  leads to large cumulative delay that is pronounced even  when common optimizations are applied which are designed  to hide the wakeup latency, at least partially. Another approach to mitigate this blocking problem in power gating is  to deflect packets via routing and topology reconfiguration.  Nevertheless, existing reconfiguration methods either are  too slow (~10K cycles) to capture the short but exploitable  router idle periods (~10-100 cycles) or can cause a large  number of packet detours due to simplified reconfiguration  algorithms. Moreover, dynamic reconfiguration unnecessarily complicates the already complex designs of on-chip  networks.   In addressing this blocking problem comprehensively, in  this work we propose Power Punch, a novel performanceaware power-saving scheme that aims to achieve nonblocking power-gating of on-chip network routers. The  basic idea of Power Punch is to always send power control  signals ahead of packets to “punch through” any blocked  routers in power-gated mode along the imminent path of  packets so that packets can be transported without having to  suffer any router wakeup latency or packet detour latency.  Power Punch consists of two mechanisms that solve major challenges in sending punch-through power control signals. The first challenge concerns the tension between the  amount of power control information needed to be propagated across multiple hops ahead of packets and the tight  constraints in allocating control wires for this purpose. Using dedicated wires for each wakeup control signal incurs  prohibitive hardware cost whereas sharing wakeup signals  introduces serious contention delay that could defeat the  purpose of sending wakeup signals in advance. The first  proposed mechanism utilizes the properties and constraints  of the network to minimize the needed control information  and is able to merge all the signals arriving at a router in the  same cycle, thereby propagating punch-through power control signals in a low-cost and contention-free fashion. In  addition, the multi-hop power control signals forewarn routers to know precisely whether there will be packets arriving  in the next few cycles. This helps to filter out short counterproductive idle periods (i.e., less than the break-even time),  and ensure the resulting router wakeups are accurate and  necessary. The second challenge concerns how to punch  through powered off routers that are close in vicinity to the  injection node and, therefore, do not have enough remaining  hops (i.e., hop count slack) for sending wakeup control signals to cover the wakeup latency. Our second proposed  mechanism holistically exploits existing slack in the network interface at injection nodes. In essence, the mechanism  allows wakeup control signals to be sent before packets are  978-1-4799-8930-0/15/$31.00 ©2015 IEEE 1  378     Vdd Sleep Signal PG Controller VC allocator & Switch allocator Input port … Output port … Input port Switch Output port GND Router C WU PG Router B WU PG Router A WU PG Router D   (a) Router with look-ahead routing WU PG Router E Figure 1: Power-gating technique.  Figure 2: Power-gating handshaking.  (b) Router with look-ahead routing  and speculative SA  Figure 3: Router pipeline designs.  generated, thus compensating for the otherwise insufficient  slack in hop count.   Full-system simulations show that, compared to an optimized conventional power-gating technique applied to onchip network routers, Power Punch achieves a reduction of  61.2% in network performance penalty while also saving  3.8% more router energy. When compared with not using  power-gating, Power Punch reduces router static energy by  83.7% while incurring only 0.4% increase in execution time,  essentially achieving near non-blocking power-gating which  is the goal.  This research increases our understanding of how to  maintain performance in the presence of power-gating of  on-chip network routers and provides key insights on the  viability of achieving non-blocking power-gating. The rest  of the paper is organized as follows. Section 2 provides  more background on power-gating and its blocking issue.  Section 3 describes the rationale of Power Punch and discusses challenges in achieving it. Section 4 provides details  of the proposed Power Punch scheme. Section 5 presents  our evaluation methodology, and Section 6 provides simulation results. Finally, related work is summarized in Section  7, and Section 8 concludes the paper.  2. Background and Motivation  2.1 High Static Power in On-chip Network Routers  On-chip networks provide a scalable approach for supporting parallel communication in many-core CMPs. They  should be designed so as not to incur considerable overhead  in chip area and power. While the area overhead becomes  less of a concern as more and more transistors are being  integrated on a chip, the NoC power problem has been escalating across each technology node due to ever tighter power  constraints. Consequently, despite numerous previously  proposed novel topologies and routing algorithms, most  taped-out commercial and research many-core chips adopt  planar mesh-based topologies with dimension-order routing  in practice to reduce NoC overhead (e.g., Intel SCC [17],  TRIPS [13], Scorpio [10], Adapteva Epiphany family [1],  Tilera TILE-Pro and TILE-Gx families [30]). Even with  these simpler implementations, the NoC still consumes substantially more power than necessary, with a large amount  consumed by its static power component.   To illustrate, we conduct full-system simulations using  gem5 [4] and multi-threaded PARSEC benchmarks [3] on  an 8x8 mesh network with XY routing and wormhole  switching. The number of virtual networks is configured to  be three, the minimum number needed for correctly running  the MESI coherence protocol without deadlocks. Each virtual network also has a relatively small buffer configuration,  with two 3-flit sized data virtual channels (VCs) and one 1flit sized control VC. Simulation results from the DSENT  [29] NoC power simulator integrated in gem5 show that,  under this simple NoC design and minimal resource configuration, router static power still accounts for nearly 64% of  the total router power consumption assuming 45nm technology. This is because router components other than buffers also consume noticeable power [6] and because the average network utilization in real benchmarks is relatively  low. As chip designs penetrate further into the dark silicon  era, the static power component of NoCs will only get worse  as more processing cores can be powered off or are operated  at lower frequencies, thereby generating less network traffic  and leading to higher static power percentage.  2.2 Applying Power-gating to On-chip Network Routers  One way that can dramatically reduce static power is to  apply power-gating techniques to each NoC router. This can  be very effective as it exploits the idleness exhibited in each  router while reducing the static power of all components in  a router. As depicted in Figure 1, power-gating is implemented by inserting an appropriately sized header transistor(s), usually a high threshold and non-leaky “sleep switch”,  between Vdd and the router. When the sleep signal is asserted by the power-gating controller (which is a small hardware component that is always powered on), the supply  voltage to the router is cut off, thus eliminating the leakage  currents in both the subthreshold conduction and reversebiased diodes.   Different from other system components, when applying  power-gating to on-chip network routers, extra handshaking  signaling is needed between neighboring routers to ensure  the correct delivery of packets. As shown in Figure 2, besides generating the sleep signal, the power-gating control2  379   ler also monitors the emptiness of the router datapath and  the wakeup signals from neighbors. When the datapath of a  router, for example, router A is empty (i.e., input buffers,  output registers and crossbar are empty) and no wakeup  signals (WU) come from neighbors, the controller in router  A asserts the sleep signal after a timeout period1 to put router A into gated-off state and notifies its neighbors by asserting the PG signal. Upon detecting the asserted PG signal,  neighboring routers mark the corresponding output ports as  unavailable in their switch allocator. Later, if a packet in  router B or in other neighbors of router A needs to be forwarded to router A, a WU signal will be asserted which triggers the controller in router A to de-assert its sleep signal.  The packet is then stalled in router B until router A is fully  awoken and the PG signal is cleared. Hence, the wakeup  latency of router A, including the blocking latency due to  being powered off, is directly part of the overall latency of  the packet forwarding process.  2.3 Blocking Problem in Conventional Power-gating  As can be seen, the primary concern with the above conventional way of applying power-gating to routers is the  negative performance impact caused by wakeup latency.  Essentially, when a router is powered off, it blocks all the  paths of a packet that overlap with the router (i.e., forwarding path from any of the router’s input ports to its output  ports). In what follows, we use the term blocking powergating to refer to this phenomenon.   The blocking problem in conventional power-gating can  be prohibitive. Prior works [6, 7, 9, 24, 28] as well as results  given in this work show that the wakeup latency of on-chip  network routers is around 6-12 cycles depending on implementation, which is quite sizable. More importantly, a packet may experience wakeup latency multiple times in the  network as more than one router along the packet’s path  could be powered off. Thus, the cumulative delay caused by  blocking power-gating is comparable to the zero-load packet  latency which also is on the order of tens of cycles. As onchip network latency is very critical to the overall system  performance, the blocking problem must be addressed adequately before power-gating can be applied most effectively.  Several approaches have been proposed so far to combat  blocking power-gating, but they have various degrees of  effectiveness. One approach is to deflect packets when their  current paths are blocked by powered-off routers. This  needs to be achieved through intricate reconfiguration of  routing, topology, or both. For example, a fast reconfiguration method [6] that uses pre-determined paths to bypass  gated-off routers reconfigures quickly and can capture both  long (i.e., >100 cycles) and short idle intervals (i.e., 10-100  cycles). However, it introduces considerable packet detours  and degrades system performance. More extensive but complex reconfiguration algorithms [27, 28] use dynamic information to minimize detours. However, they are slow by  comparison and, as a result, reconfigure only on a per-epoch  basis (~10K cycles for an epoch) to capture idleness on a  very coarse granularity. Also, their uses are limited to scenarios in which a couple of cores and the co-located caches  are completely idle with no communication with other cores  and caches, which might be impractical for typical shared  cache architectures. Additionally, the routing and topology  reconfiguration due to power-gating unnecessarily complicates the simple design of deterministic routing in mesh  networks.  Another approach to deal with blocking power-gating is  to control powered-off routers more effectively using techniques directly aimed at reducing waiting time. A common  technique is to send the wakeup signal early to the next  router as soon as the output direction is computed [24]. This  hides a few cycles, equivalent to the number of router pipeline stages but typically is not sufficient to cover the entire  wakeup latency. Another technique is to apply a timeout  mechanism after a router is detected as being idle. This  technique intends to filter out short idle periods that are less  than the break-even time 2 to reduce the possibility of encountering a powered-off router. However, the timeout value cannot be too long (around 4 cycles [7, 9]) as false filtering essentially wastes the remaining idle cycles that can be  exploited by power-gating. Finally, several techniques have  been proposed to power-gate individual components within  a router [24, 25, 26]. This approach reduces the chance of  encountering powered-off router components at the cost of  substantially higher implementation complexity. Yet, it still  cannot entirely remove the blocking when a powered-off  component is needed for packet forwarding, and it does not  mitigate the cumulative wakeup latency problem either.   Therefore, given the severe performance penalty that can  be caused by wakeup latency and the many limitations in  existing approaches, it is imperative for a novel scheme to  be devised that can solve the blocking problem comprehensively, ideally achieving non-blocking power-gating.  3. Challenges in Achieving Non-blocking Power-gating  Non-blocking power-gating of on-chip network routers  can be achieved by pre-powering up routers along network  paths taken by packets in advance of packet arrival. To accomplish this, fundamental challenges must be adequately  addressed. One major challenge concerns how to completely  hide router wakeup latency across multiple hops with minimal overhead; another concerns how to wake up routers  located at or neighboring nodes that inject packets into the  NoC at the source. Below, these challenges are discussed in  sent ahead of a packet is determined by (cid:1727)Twakeup/Trouter(cid:1728), asdetail.  In order to hide wakeup latency of Twakeup cycles completely, the number of hops that a wakeup signal should be  suming the packet takes (Trouter+Tlink) cycles per hop, whereas the wakeup signal takes Tlink cycles per hop. For example,  1 A minimum of two-cycle timeout is needed to allow packets that already  left upstream routers to be received correctly. Additional timeout cycles  can be used to filter short idle periods. More details are in next subsection.  2 Break-even time (BET) is the minimum number of consecutive cycles  that a gated circuit block needs to remain idle before waking up to offset  the energy overhead of one power-gating process (e.g., charge capacitance,  distribute sleep signal). BET is around 10 cycles for on-chip routers [7].  3  380                                                                                                                           Figure 3 shows two common router pipeline designs [20].  The router design in Figure 3(a) uses look-ahead routing,  resulting in a 4-stage pipeline of buffer writing (BW), VC  allocation (VA), switch allocation (SA) and switch traversal  (ST). The router design in Figure 3(b) further optimizes  with speculative switch allocation, which reduces another  pipeline stage if the speculation is successful. With a Twakeup  of 8 cycles, wakeup signals need to be sent at least 2 hops in  advance for the case in Figure 3(a) and 3 hops in advance  for the case in Figure 3(b) in order to hide the entire Twakeup.   However, for a given router, the total number of routers  that needs to be monitored may quickly become very large  even within a short hop distance. Figure 4 shows an example of an 8x8 mesh network. There are 24 routers within 3  hops of router 27 (denoted as R27 hereafter), which accounts for nearly 38% of all routers on the chip. This means  that, assuming the 3-stage speculative router pipeline design  shown in Figure 3(b), R27 needs to monitor the wakeup  signals from all the 24 routers that are either sent to wake up  R27 (e.g., from R3 to R27) or sent to wake up other routers  but need to be relayed at R27 (e.g., from R26 to R29). This  makes it very challenging to monitor and propagate effectively all the needed wakeup signals in the network.  A straightforward way to achieve this is to allocate dedicated wire channels for every wakeup control signal between a router and its monitored routers (e.g., 24 separate  incoming wire channels are needed for R27). Note that it is  not enough to have only 1-bit wire channels for the wakeup  control signal. For instance, if the wire channel from R26 to  R27 is only 1 bit, R27 will have no idea about if and where  this signal should be forwarded after it arrives as the router  cannot distinguish whether this 1-bit wakeup signal is intended to wake up R29, or R43, or any of the 9 routers that  are within 3 hops of R26 with the first hop being R27.  Hence, to allow correct relay of wakeup signals, minimally  4 bits are needed to distinguish the 9 different cases, totaling  96 bits of wire channels in this example which is prohibitively high, e.g., by comparison, packet payload channel  widths typically are 128 bits or 256 bits.  An alternative approach is to share wire channels for the  wakeup control signals. This solution may be more viable  from the perspective of hardware cost, but it immediately  brings to fore the critical issue of possible contention among  wakeup signals. In the case of Figure 4, a single wire channel such as the one from R27 to R28 may be shared by up to  9 wakeup signals. If only one of them can be transmitted in  a given cycle, other wakeup signals arriving at the same  cycle will be unavoidably delayed. As any delayed cycle is  translated directly into delayed wakeup of the needed router,  this can seriously degrade the effectiveness of sending  wakeup signals in advance unintentionally, causing blocking  to persist. To avoid such contention issues, multiple wakeup  control signals must be allowed to be transmitted simultaneously. Nevertheless, merging wakeup signals is very difficult due to the large number of distinct cases. We illustrate  by continuing to use as an example the wire channel from  R27 to R28. Nine routers (i.e., R11, R18, R19, R25, R26,  R27, R34, R35, and R43) may send wakeup signals that use  this wire channel, and different signals may be intended to  Y+ X+ 2 10 18 26 34 42 50 58 1 9 17 25 33 41 49 57 0 8 16 24 32 40 48 56 3 11 19 27 35 43 51 59 4 12 20 28 36 44 52 60 5 13 21 29 37 45 53 61 6 14 22 30 38 46 54 62 7 15 23 31 39 47 55 63 Figure 4: Power Punch challenges and solutions.   wake up different routers leading to a prohibitively large  number of combinations. For example, in one cycle, R27  may need to merge the wakeup signal from R26 to R36 and  the wakeup signal from R27 to R21. In another cycle, R27  may need to merge wakeup signals from R26 to R20 and  from R27 to R37. The merged results are different in these  two cases and, therefore, need to be distinguishable in the  wire channels. As a result, wire channels need to be wide  enough to have different values that can differentiate between all the various combinations of needed wakeup signals. This could lead to very wide wire channels comparable  in size to the aforementioned dedicated wire approach. In  the next section, we show how this challenge can be addressed by presenting an innovative mechanism that can  collectively propagate wakeup signals in a contention-free  manner while requiring narrow wire channels.  The other major fundamental challenge arises from the  situation of there not being enough routing hop slack to send  wakeup signals in advance to fully cover the wakeup latency.  This problem is most severe for routers located at or neighboring injection nodes. For instance, if R24 in Figure 4 is  powered off, the associated local node will experience the  entire Twakeup latency before being able to inject packets. Our  evaluations using PARSEC benchmarks show that, on average, more than 13% of packets received by routers come  from local nodes, causing the above performance penalty to  occur when those routers are powered off. To address this  challenge, other time slack opportunities that holistically  include exploiting behavior at injection nodes need to be  explored to increase the effectiveness of hiding router  wakeup latency, as is achieved with our proposed Power  Punch.  4. Power Punch  In this section, we present a detailed description of the  proposed Power Punch, a novel scheme that incorporates  innovative mechanisms for addressing aforementioned chal4  381   lenges to achieving near non-blocking power-gating. The  key rationale for Power Punch is the following: if wakeup  information can be cleverly generated and transmitted sufficiently early, power control signals can be sent to “punch  through” the network ahead of packets to power up needed  routers along the path of packet destinations. From the perspective of packets, transport in the network can be accomplished without having to suffer any router wakeup latency  or packet detour latency, as if all NoC routers were virtually  always powered on.  In merging wakeup signals, the main obstacle is the tension between the amount of power control information  needed to be propagated and the limited power control  bandwidth available. The basic idea behind the mechanism  for addressing this concern is to utilize the routing and topological properties of the network to minimize the needed  information and reduce the width of the merged signals via  clever encoding. The proposed mechanism allows all of the  wakeup signals arriving at a router in the same cycle to be  efficiently merged and relayed, thereby eliminating contention delay. In addressing the challenge of not having enough  slack in hop count at or near injection nodes, the basic idea  behind the mechanism that addresses this concern is to exploit existing slack at the network interface (NI) from when  information for generating wakeup control signals is available and when a packet is generated and ready for injection.  This allows wakeup signals to be sent ahead to the source  and neighboring routers well enough in advance of packet  injection, thus compensating for the otherwise insufficient  hop count slack.  Collectively, these mechanisms work in tandem to enable power control signals to “punch though” blocked routers  along the entire path of packet destinations, thereby allowing packets to be transported in the network in a near nonblocking fashion. The following subsections describe these  mechanisms in further detail.  4.1 Low-cost and Contention-free Multi-hop Punch  To merge and relay wakeup signals across multiple hops,  a five-step encoding process can be used to minimize hardware implementation. In this subsection, we explain these  steps using the example of sending a wakeup signal 3 hops  in advance (i.e., for the speculative router pipeline shown in  Figure 3(b)). If needed, a simplified 2-hop and an extended  4-hop design can be derived using similar procedures. It is  important to note that wakeup signals should not be sent too  early, as this would wake up routers before they are actually  needed and, thus, squander powered-off cycles. In practice,  3 hops of slack typically is able to cover router wakeup latency (e.g., hide Twakeup up to 9 cycles for 3-stage routers and  up to 12 cycles for 4-stage routers). To facilitate discussion,  the term targeted router is used to refer to the router that is  3 hops away from the current router (e.g., in Figure 4, if a  packet has source R0, destination R7 and is currently in R3,  then R6 is the targeted router for the wakeup signal from  R3). The term punch signal is used to refer to the final  merged wakeup signals encoded using the proposed Power  Punch scheme.   Without loss of generality, we implement Power Punch  assuming a 2D mesh network with XY routing. The rationale is the following. First, most commercial and research  chips use dimension-order routing to keep NoC overhead  low as mentioned in Section 2.1, therefore the proposed  scheme can be readily adopted and have high practical impact. Second, the advantage of adaptive routing over deterministic routing becomes prominent only when traffic load  approaches saturation. Power-gating is best applied when  traffic load is low to medium, a region where there is little  distinguishable performance difference between the two  routing methods in the absence of power-gating. Third, if  certain many-core applications require very high throughput  that is not sustainable by deterministic routing, previous  works have proposed theory and methodology (e.g., [11, 22])  that allow the routing algorithm to switch from deterministic  to adaptive, and vice versa, according to prevailing network  loads. This enables the use of Power Punch to save static  power under low load conditions while not compromising  throughput under high load conditions. Below are the steps.  (1) Determine targeted router based on network topology  and routing algorithm  To merge multiple wakeup signals into one punch signal,  the targeted router for each wakeup signal must first be determined. We assume a mesh NoC with deterministic routing. The main advantage of mesh XY routing is that, at any  given router, the targeted router of the wakeup signal can be  easily determined using destination information stored in  the packet header. For instance, in Figure 4, a packet currently at R26 with destination R31 knows precisely that the  targeted router is R29, and a wakeup signal will be sent to  notify that router (the actual sent wakeup signal will be  merged with other wakeup signals into one encoding, as  described shortly).  (2) Reduce information for waking up intermediate routers  With an identified targeted router and mesh XY routing,  no additional information is needed in the wakeup signal for  any intermediate routers along the path to the targeted router  that need to be notified (e.g., R27 and R28 are along the path  from R26 to R29; thus they are implicitly notified if the targeted router is R29). This helps to reduce the information  needed in wakeup signals and the number of bits of encoding for the final merged punch signals.  (3) Reduce the number of wakeup signals  After reducing the information in each wakeup signal,  the next step is to reduce the number of wakeup signals that  need to be merged at a given router. Take R27 as an example. In its X+ direction (i.e., from R27 to R28), wakeup signals from up to 9 routers (i.e., R11, R18, R19, R25, R26,  R27, R34, R35, and R43) may need to be relayed from R27  to R28 in general. However, due to restrictions in XY routing for avoiding deadlock, only three out of these 9 routers  are possible, namely R25, R26 and R27. Packets from the  remaining six routers do not use the link from R27 to R28,  hence do not send wakeup signals along that link (e.g., path  R19→R27→R28 is not valid as Y+ to X+ turns are illegal).  Similarly, in the X- direction of R27, only three routers can  5  382   Table 1: All possible sets of targeted routers in the X+  direction of R27 (“||” means or; “&” means and).  #  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  Represented  Wakeup Signals  27→28 || 26→28  27→12 ||  27→12 & 26→20 ||  27→12 & 26→28  27→21 ||  27→21 & 26→29 ||  27→21 & 26→28  27→30 ||  27→30 & 26→29 ||  27→30 & 26→28  27→37 ||  27→37 & 26→29 ||  27→37 & 26→28  27→44 ||  27→44 & 26→36 ||  27→44 & 26→28  27→20 || 26→20 ||  27→20 & 26→20 ||  27→20 & 26→28  27→29 || 26→29 ||  27→29 & 26→29 ||  27→29 & 26→28  27→36 || 26→36 ||  27→36 & 26→36 ||  27→36 & 26→28  27→12 & 26→29  27→12 & 26→36  27→21 & 26→20  27→21 & 26→36  27→30 & 26→20  27→30 & 26→36  27→37 & 26→20  27→37 & 26→36  27→44 & 26→20  27→44 & 26→29  27→20 & 26→29 ||  27→29 & 26→20  27→20 & 26→36 ||  27→36 & 26→20  27→29 & 26→36 ||  27→36 & 26→29  Set of  Targeted Routers  { 28 }  { 12 }  { 21 }  { 30 }  { 37 }  { 44 }  { 20 }  { 29 }  { 36 }  { 12, 29 }  { 12, 36 }  { 21, 20 }  { 21, 36 }  { 30, 20 }  { 30, 36 }  { 37, 20 }  { 37, 36 }  { 44, 20 }  { 44, 29 }  { 20, 29 }  { 20, 36 }  { 29, 36 }  Punch  Signal 00000 00001  00010  00011  00100  00101  00110  00111  01000  01001 01010 01011 01100 01101 01110 01111 10000 10001 10010 10011  10100  10101  be the source for wakeup signals as well. This greatly reduces the number of combinations for targeted routers. As  for the Y+ and Y- directions of R27, while all nine routers  can still be the source of wakeup signals, their targeted routers are limited, so it is straightforward to optimize as explained in the next step.   (4) Reduce combinations of targeted routers  Next, the minimal number of bits needed for encoding  the punch signals in order to merge all possible wakeup signals arriving at a router in the same cycle should be determined. As mentioned in Section 3, different sets of targeted  routers require distinct punch signals. For instance, a punch  signal from R27 to R28 with an encoding of “01100” can be  used to represent the merged result of wakeup signals from  R26 to R36 and from R27 to R21 (i.e., the set of targeted  routers is {36, 21}; see Table 1, entry 13). A different encoding for the punch signal, such as “01111”, is needed to  merge a different combination of wakeup signals from R26  to R20 and from R27 to R37 (i.e., the set of targeted routers  is {20, 37}), even though the source routers are the same in  both cases. However, if the targeted router of one wakeup  signal is along the path of the targeted router of another  wakeup signal (e.g., R26 to R29 is along the path from R27  to R21), the same punch signal encoding of “00010” can be  used as in the case where there is only one wakeup signal  from R27 to R21. In other words, the width of the punch  signal can be optimized to be just wide enough to distinguish all distinctive sets of targeted routers, and no wider.  Based on the results from step (3), up to three routers  can be the source of wakeup signals in the X+ direction.  R27 has 9 possible targeted routers (i.e., R12, R20, R21, R28,  R29, R36, R37, and R44); R26 has 4 (i.e., R20, R28, R29,  and R36) and R25 has 1 (i.e., R28). Table 1 lists all the distinctive sets of targeted routers for the X+ direction of R27  (the third column) and the corresponding wakeup signals  represented (the second column). The wakeup signal from  R25 is not listed for clarity, as the targeted router for this  wakeup signal is always R28, which is along the path of a  targeted router in any table entry. In total, due to the reduction through previous steps and the use of targeted routers to  remove the cases where other targeted routers are implicitly  contained, there are only 22 different sets. Therefore, only 5  bits are needed in the punch signal of the X+ direction to  distinguish between these sets. Similarly, the width of the  punch signal for the X- direction is also 5 bits. Note that we  do not use the targeted router numbers in punch signals directly which would otherwise cost 8 bits of encoding each.   For the Y+ and Y- directions, although there are up to 9  routers for sending wakeup signals, the targeted routers have  only 3 possibilities in each direction due to the illegal turns  from Y to X dimensions (e.g., only R35, R43 and R51 for  Y+). The combination of the three targeted routers result in  three distinctive sets in each direction, e.g., {R35}, {R43},  {R51} in Y+ (note that if both R35 and R51 are the targeted  routers, the resulting set is {R51} as R35 is implicitly contained). Therefore, only 2 bits of encoding are needed for  the punch signals in each of the Y directions.  (5) Punch signals from/to neighbors  Figure 5 depicts the resulting punch signals and their  widths between neighboring routers. Each cycle, up to four  sets of punch signals can arrive from neighbors in the two  dimensions with values reflecting the targeted router(s) that  need to be controlled. Along with additional targeted routers  generated from the local router, the power-gating controller  sends newly generated punch signals to at most the four  neighbors. As multiple targeted routers can be merged and  communicated through a punch signal in one cycle, no contention delay is incurred, and targeted routers can always  receive the notification as scheduled and wake up in time. In  addition, this mechanism requires only 5 bits of encoding  for X directions and 2 bits for Y directions in the case of 3hop wakeup signal slack. Therefore, the hardware cost of  punch signals and the power-gating control logic is very low,  particularly compared to the main datapath and control path  in routers that operate at the size of flits with 128 bits or 256  bits. It can be shown that, for the case of 4-hop wakeup sig6  383   Router 5 5 Router 2 2 Router 2 2 Router 5 5 Router Figure 5: Power Punch signals.  Figure 6: Exploiting slack at injection nodes.  nal slack, the width of punch signals is 8-bit for the X directions and 2-bit for the Y directions, which is still relatively  small. Sending wakeup signals with 5 hops or more would  be counter-productive as the wakeup latency is not that high  (~20 cycles) and more powered-off cycles would be wasted.  4.2 Injection Node Punch  Power Punch has an additional mechanism to address  the blocking issue at routers due to there not being enough  hop count slack at the injection node to fully cover the  wakeup latency of the local router. Our proposed mechanism holistically exploits two potential sources of existing  slack at injection nodes to send punch signals before packets  are generated.  Figure 6 shows the timeline for generating and sending a  packet at an injection node. Normally, after accessing local  resources (e.g., cache, directory) and generating a message,  several operations are performed in the network interface.  This includes encapsulating the message into packets and  flits, arbitrating among multiple ready VCs (as only one VC  from all virtual networks can send a flit through the physical  link in each cycle), checking the availability of the connected input port of the local router, and passing the packet to  the VC buffer of the input port. If the local router is found to  be powered off when checking the availability, a wakeup  signal is sent to the power-gating control of the router. Also,  additional wakeup signals (in the form of punch signals as  described before) are sent to other non-local routers one or  more hops away based on packet destinations.   As can be seen, packets at the NI need to wait for the entire wakeup latency before being injected into the router  input port. However, there is slack between the time that the  destination is known (as part of the message passed to the  7  384 Table 2: Key parameters for simulation.  Network topology  Input buffer depth  Link bandwidth  Router  Private I/D L1$  Shared L2 per bank  Cache block size  Virtual channel  Coherence protocol  Memory controllers  Memory latency  4x4, 8x8, 16x16 mesh  3-flit for data VC, 1-flit for control VC  128 bits/cycle  3-stage and 4-stage  32KB, 2-way, LRU, 1-cycle latency  256KB, 16-way, LRU, 6-cycle latency  64Bytes  2 VCs/VN, 3 VNs  Two-level MESI  4, located one at each corner  128 cycles  NI) and the time that the availability of the router is checked  that can be exploited. Therefore, both types of wakeup signals can be sent at the beginning of NI delay instead of at  the end of NI delay, as shown in Figure 6 “slack 1”. With  this slack, several cycles equivalent to the NI latency (usually 3 or 4 cycles) can be hidden from Twakeup.   The above “slack 1” extends only up to the beginning of  the NI as the destinations may not be known earlier. For  example, sharers of a cache line for an invalidation coherence message cannot be known without accessing the cache  directory on the home node. Therefore, wakeup signals to  non-local routers, which rely on destination information to  determine the targeted routers, cannot be generated earlier.  In contrast, for the local router, as long as there is a packet  that needs to be sent from this node, the local router will  always be used, even though the destination is not known.  This is represented as “slack 2” in Figure 6. Consequently, it  is possible to send the wakeup signal for the local router at  the beginning of accessing L2 cache or directory when it is  known for sure that a packet will be generated and the local  router will be used. This hides several additional cycles  from Twakeup (e.g., hiding ~6 cycles if accessing L2). One  limitation, however, is that it might not always be possible  to send the wakeup signal before accessing L1 as not all  accesses result in non-local packets. A straightforward solution to this simply is not to exploit “slack 2” for L1 cache  accesses. A valid bit is added for each type of resource to  signify whether that resource type uses this slack (i.e., “1”  for L2 cache and directory, and “0” for L1 cache). Note that  “slack 2” is relatively long, so it is very effective in hiding  wakeup latency when this slack is used.  4.3 Putting It All Together: Power Punch Impact  To summarize, Power Punch enables punch signals to  utilize existing slack at source nodes to wake up poweredoff local and neighboring routers along the first few hops  even before packets are injected. It also enables punch signals to utilize hop count slack by propagating ahead of  packets to “punch through” any blocked routers along the  imminent path of packets, waking them up along the way.  The contention-free signal propagation mechanism guarantees on-time delivery of wakeup signals and the on-time  wakeup of routers, essentially hiding the wakeup latency  from packets and not requiring packet detours. In addition,  with multi-hop wakeup signals, a router knows exactly    No‐PG ConvOpt‐PG PowerPunch‐Signal PowerPunch‐PG No‐PG ConvOpt‐PG PowerPunch‐Signal PowerPunch‐PG 80 70 60 50 40 30 20 10 0 1.2 1 0.8 0.6 0.4 0.2 0 ) G P ‐ o N o t d e z i l a m r o n ( e m i t n o i t u c e x E ) s e l c y c ( y c n e t a l t e k c a p e g a r e v A                         Figure 7: Average packet latency.                                                   Figure 8: Execution time.  whether there is any incoming packet in next few cycles,  thus avoiding power-gating short idle periods. This is superior to timeout techniques as no false filtering can happen  and the filtering length is considerably longer. These features allow Power Punch to have significant static energy  savings with minimal performance penalty, as shown in the  following evaluation.  5. Evaluation Methodology  Power Punch is evaluated under full system simulation  with the combined use of architecture-level and circuit-level  simulators. The cycle-accurate gem5 [4] simulator enhanced  with GARNET [2] is used for detailed timing simulation of  the processor, memory and on-chip network. We also integrate the latest DSENT [29] NoC power tool with gem5 and  GARNET to obtain runtime network activity statistics and  estimate router power consumption more accurately. Significant effort has been made to implement various powergating functionalities in the previous simulation settings.  Besides regular power-gating components, we also modify  the simulators to model all the key additional hardware in  Power Punch, such as punch signals in all directions, extra  logic in the power-gating controller for punch signal relay  and handshaking, wakeup signals in network interface and  cache controllers for holistically exploiting slack, and so on.  The cache architecture assumes a 32KB I/D private L1  cache and 16MB shared L2 cache. The coherence protocol  uses two-level MESI implemented with 3 logically separated virtual networks to avoid message-dependent deadlock.  An 8x8 mesh network is used for most of the simulations  while both 4x4 and 16x16 meshes are used for scalability  analysis. All the NI operations are packed compactly in  three cycles, although other loosely packed NI designs with  longer latency would give Power Punch larger advantage  due to increased slack. Table 2 lists other key configuration  parameters.  Router wakeup latency is estimated using a standard  VLSI design flow with 45nm technology. Parasitic extraction is performed on a 451um by 451um layout, and the  extracted data is fed into a SPICE RC model. The wakeup  latency is estimated to be 8 cycles. Additional sensitivity  studies on various wakeup latencies are also conducted to  demonstrate the applicability of Power Punch over a practical range of alternatives. The break-even time is 10 cycles  and the timeout is 4 cycles, consistent with prior works [6, 7,  9].  We compare the following four schemes: (1) No-PG:  baseline design with no power-gating; (2) ConvOpt-PG:  conventional power-gating optimized with timeout and  sending of wakeup signals early (these techniques partially  hide the wakeup latency and avoid powering off short idle  periods); (3) PowerPunch-Signal: proposed scheme with  multi-hop punch signal only (no use of NI slack); (4) PowerPunch-PG: comprehensive scheme with multi-hop and NI  punch signals.  6. Results and Analysis  6.1 Effect on Performance  We first evaluate one of the primary objectives of Power  Punch for reducing the performance penalty of power-gating.  Figure 7 compares the average packet latency, and Figure 8  shows the execution time of the four schemes for the multithreaded PARSEC benchmarks [3]. Results are normalized  to the No-PG scheme which generally provides a lower  bound for average packet latency and execution time. As  can be seen from Figure 7, even with the timeout and earlywakeup optimizations, ConvOpt-PG still increases the average packet latency substantially, by 69.1% on average compared with No-PG. This large penalty mainly comes from  the fact that powered-off routers in conventional powergating essentially block the path of packets and traditional  optimization techniques are far from being able to cover the  wakeup latency. In contrast, PowerPunch-Signal uses multihop punch signals to wake up the needed routers in advance,  completely hiding the wakeup latency when there are  enough hops. Consequently, PowerPunch-Signal reduces  average packet latency significantly, with only 12.6% increase on average. By exploiting slack to compensate for the  cases where there are not enough hops, the PowerPunch-PG  achieves an additional 4.7% reduction, resulting in an average of only 7.9% increase in packet latency compared to  No-PG. This amounts to a remarkable 61.2% improvement  compared to ConvOpt-PG.   Similar trends are also observed in execution time. As  shown in Figure 8, while the degree of reduction in execution time may vary among benchmarks due to their different  sensitivities to network latency, Power Punch always has  8  385                 9  the lowest performance penalty (for ferret, it is not clear  why PowerPunch-PG actually has a slight decrease in execution time compared to No-PG, but likely causes are  changes in thread criticality and synchronization traffic from  altered packet timing). On average, PowerPunch-Signal and  PowerPunch-PG have only 2.3% and 0.4% increase in execution time, respectively, essentially achieving non-blocking  power-gating.  6.2 Effect on Reducing Blocking  To gain more insight on the effectiveness of Power  Punch in mitigating blocking due to power-gating of routers,  Figure 9 compares the average number of powered-off routers (i.e., blocked routers) that a packet encounters when  transported from source to destination. As can be seen, the  average number of blocked routers is dramatically reduced  from 4.21 in ConvOpt-PG to 1.09 in PowerPunch-Signal;  PowerPunch-PG further reduces that number to 0.96 due to  the use of slack at the injection node (11.8% improvement  over PowerPunch-Signal). However, this metric cannot fully  reveal the advantage of exploiting NI slack since a blocked  router is always counted as one even if the majority of its  wakeup latency is hidden by the slack. To reflect this difference, Figure 10 plots the actual number of cycles that packets spend in waiting for routers to become fully awoken.  While both PowerPunch-Signal and PowerPunch-PG significantly decrease the number of waiting cycles compared  with ConvOpt-PG, the improvement of PowerPunch-PG  over PowerPunch-Signal is actually 36.2%, revealing the  true advantage of exploiting NI slack. Figures 9 and 10 illustrate the substantial reduction in performance penalty  evidenced in the previous subsection and clearly demonstrate the impact of Power Punch.  6.3 Effect on Router Energy  The performance advantage of Power Punch does not  come at a sacrifice in energy savings. Figure 11 shows the  breakdown of router energy across the benchmarks, normalized to No-PG. The router energy is decomposed into dynamic energy, static energy and power-gating energy overhead. The power-gating overhead includes all the energy  wasted owing to power-gating and its control, such as the  energy consumed in powering on/off routers, in distributing  sleep signals, and in generating and propagating punch signals.  We first compare router static energy. For fair comparison with No-PG, the power-gating overhead is added to the  router static energy as the total router static energy for the  three power-gating schemes (i.e., the total height of the bottom two bars) to reflect the net static energy savings. On  average, the three power-gating schemes have similar static  Figure 11: Breakdown of router energy.  5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 0 o # f n e c n u o t e r o p d e w e r d e ‐ o f f r u o t e s r ConvOpt‐PG PowerPunch‐Signal PowerPunch‐PG 0 5 10 15 20 25 o # f c y c l e / s e k c a p t w a i t i g n f o r w p u e k a ConvOpt‐PG PowerPunch‐Signal PowerPunch‐PG      Figure 9: Number of encountered powered-off routers.      Figure 10: Number of cycles waiting for router wakeup.  0% 20% 40% 60% 80% 100% N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G N o ‐ P G v n o C O p ‐ t P G o P w e r n u P c h ‐ S i a n g l o P w e r n u P c h ‐ P G blackscholes bodytrack canneal dedup ferret fluidanimate swaptions x264 AVG B r o d k a e w o n f r u o t e r e n e r y g ( o n r m a i l z d e t o N o ‐ P G ) Router_dynamic Router_static Power‐gating overhead 386                                 10  energy savings, with an improvement of around 83% (relative to the static energy in No-PG).  As for the total router energy, compared with No-PG,  ConvOpt-PG saves 50.3%, with PowerPunch-Signal saving  52.9% and PowerPunch-PG saving 54.1%. Hence, Power  Punch has slightly more energy savings. This is mainly due  to two reasons: 1) Power Punch provides better filtering for  short idle periods, and 2) Power Punch has a shorter execution time, which leads to less total router energy. Considering Figure 7 to Figure 10 together, it can be seen that, compared with the optimized conventional power-gating (ConvOpt-PG), Power Punch is better in both performance and  energy.   6.4 Comparison across Full Network Load Range  To understand the behavior of different schemes more  fully, we conduct simulations with synthetic traffic and vary  the network load from zero to saturation. Figure 12 presents  the performance and power results for three common traffic  patterns: uniform random, transpose and bit-complement.  Statistics are collected after sufficiently long NoC warm up.   Regarding performance, the typical “power-gating curve”  is observed for ConvOpt-PG. That is, for low loads, the average packet latency is very high as many routers are powered off and packets are likely to be blocked multiple times.  As load increases, the packet latency starts to decrease as  more routers are powered on, and then rises again as load  approaches saturation. This creates a massive performance  gap compared with No-PG. In contrast, the average packet  latency of PowerPunch-PG is almost identical to that of NoPG across the entire load range, essentially achieving nonblock power-gating. Note that at high load for the transpose  traffic pattern, due to the uneven load distribution among  routers, it is possible that some routers are still powered off  while other routers are congested. ConvOpt-PG performs  poorly in this case, whereas PowerPunch-PG is able to reach  the same maximum throughput as the no-power-gating case.  This highlights the advantage of Power Punch in supporting  high traffic load phases of application execution.  With regard to static power savings, both ConvOpt-PG  and PowerPunch-PG save considerable static power as expected. ConvOpt-PG is slightly better for some medium  loads but is achieved at the cost of significant performance  penalty.  6.5 Sensitivity on Wakeup Latency and Router Pipeline  To illustrate that Power Punch is applicable to a variety  of designs, Figure 13 compares the average packet latency  of No-PG, ConvOpt-PG and PowerPunch-PG with varying  values of wakeup latency and router pipeline stages. In this  sensitivity study, uniform random traffic is used with load  rate set to the average load rate of PARSEC benchmarks. A  3-hop punch signal is used in Power Punch. As can be seen,  compared with No-PG, ConvOpt-PG has a large penalty of  average packet latency in all cases, varying from 1.5X to  more than 2X. In contrast, PowerPunch-PG has only 2.4%  to 9.2% increase in the average packet latency. The highest  increase of 9.2% occurs in the case of Twakeup = 10 and Trouter  = 3, where the 3-hop punch signal does not cover the entire  wakeup latency. We intentionally include this case to show  that most of the penalty reduction of Power Punch indeed  comes from hiding wakeup latency; otherwise performance  penalty may occur. For this case, the performance penalty of                        (a) Uniform random                                  (b) Bit-complement                                        (c) Transpose  Figure 12: Packet latency and router static power across full range of network loads.  0 20 40 60 80 100 No‐PG 120 0 0.05 0.1 0.15 0.2 Injection rate (flits/node/cycle) 0.25 A e v r e n e g a t w o r k l a t n e y c ( c y c l e s ) ConvOpt‐PG PowerPunch‐PG 0 0.4 0.8 1.2 1.6 2 0 0.05 0.1 0.15 0.2 Injection rate (flits/node/cycle) 0.25 u o R t e r t s a t i c o p w e r ( W ) No‐PG ConvOpt‐PG PowerPunch‐PG 0 20 40 60 80 100 No‐PG 120 0 0.05 0.1 Injection rate (flits/node/cycle) 0.15 A e v r e n e g a t w o r k l a t n e y c ( c y c l e s ) ConvOpt‐PG PowerPunch‐PG 0 0.4 0.8 1.2 1.6 2 0 0.05 0.1 Injection rate (flits/node/cycle) 0.15 u o R t e r t s a t i c o p w e r ( W ) No‐PG ConvOpt‐PG PowerPunch‐PG 0 20 40 60 80 100 120 0 0.05 0.1 Injection rate (flits/node/cycle) 0.15 A a e v r e n e g t w o r k l a t n e y c ( c y c l e s ) No‐PG ConvOpt‐PG PowerPunch‐PG 0 0.4 0.8 1.2 1.6 2 0 0.05 0.1 Injection rate (flits/node/cycle) 0.15 u o R t e r t s a t i c o p w e r ( W ) No‐PG ConvOpt‐PG PowerPunch‐PG 387                                       No‐PG ConvOpt‐PG PowerPunch‐PG 80 60 40 20 0 ) s e l c y c ( y c n e t a l t e k c a p e g a r e v A 6 8 10 8 10 12 3‐stage router                                  4‐stage router Wakeup latency (cycles) Figure 13: Wakeup latency sensitivity.  Power Punch becomes negligible when a 4-hop punch signal is used.   6.6 Discussion  (1) Hardware implementation and cost  While the prior description in Section 4 on how and why  Power Punch works is detailed, the final implementation is  actually very simple, without the need of any table or complex hardware. In the example of Figure 5, each bit in the 5bit punch signal in the right X+ direction is a direct combinational logic function of the 5-bit punch signal from the  left (no need to monitor Y directions due to routing restrictions). Similarly, each bit in the 2-bit Y+ direction  punch signal is also a direct and simple logic function of the  punch signals in the X and Y- directions. The overall area  overhead of these logic gates, plus the 5-bit or 2-bit signal  lines and other minor control logic, consumes only 2.4% of  additional NoC area as compared to conventional powergating.  (2) Scalability  Power Punch provides very good scalability and is suitable for larger network sizes. Power Punch does not have  any particular central element that limits its scalability. The  key parameter – the width of the punch signals – depends on  the number of targeted router hops, not network size. This is  unlike reconfiguration approaches that have complexity  which depends greatly on the size of the network. In addition, conventional power-gating suffers from cumulative  wakeup latency, which increases linearly with network size.  In contrast, Power Punch does not have any of these issues,  thus achieves relatively higher improvement for larger networks. For example, at an  injection  rate of 0.01  flits/node/cycle, compared with ConvOpt-PG, PowerPunchPG reduces the average packet latency by 43.4%, 54.9% and  69.1% for 4x4, 8x8, and 16x16 networks, respectively.   (3) Comparison to other recent power-gating schemes  Previous work has proposed to send wakeup signals early using information already provided in look-ahead routing  [24]. However, wakeup signals in this method can be sent at  most 2 hops ahead, and sending beyond 2 hops requires  monitoring a large number of dedicated signals. This paper  follows a similar intuition, but proposes schemes that solve  the critical problems of minimizing and merging wakeup  signals with minimal hardware cost and no contention. This  enables wakeup signals to be sent multiple hops ahead to  hide router wakeup latency completely. In [5], NR-Mesh is  proposed to improve NoC topology for power-gating by  connecting a core to multiple routers. This scheme does not  hide any wakeup latency at the injection if all the connected  routers are powered off, and latency penalty due to detours  is significant. It also requires high-radix routers. NoRD [6]  is a recently proposed scheme that uses bypass paths to circumvent powered-off routers. It falls under the category of  fast reconfiguration-based schemes as mentioned in Section  2.3. As NoRD relies on packet detours, its performance  overhead is about 5 times that of Power Punch (9.3 cycles of  packet latency penalty in NoRD versus 1.8 cycles in Power  Punch for the 64-node system). NoRD also requires extra  VCs for deadlock avoidance whereas Power Punch works  with any number of VCs. Router Parking [28] and Panthre  [27] are two other reconfiguration-based NoC power saving  schemes with reasonably efficient algorithms. However,  these schemes similarly suffer from issues associated with  reconfiguration such as detour, long epoch and limitations  on core-to-core communication. Compared with these two  schemes, Power Punch has more router static energy savings  capability and less packet latency penalty. MP3 [7] is a recently proposed NoC power-gating scheme that is very effective at achieving near-zero performance penalty. However, it is applicable to Clos and other indirect networks, not  meshes (a direct network) targeted in this work. Another  recent power-gating scheme, Catnap [9], uses multiple narrow networks to increase the efficiency of power-gating, but  it is proposed mainly for CMPs with high-bandwidth.   7. Related Work  A number of closely related works on power-gating of  NoC routers have already been discussed in detail [6, 7, 9,  24, 25, 26, 27, 28]. In addition, topology-aware powergating has also been proposed recently [31] that specifically  targets Flattened bufferfly [19] and MECS [14] networks.  Besides on-chip network routers, power-gating techniques  have been successfully applied to cores and execution units  in CMPs and GPGPUs [18, 21, 23]. These applications of  power-gating highlight the potential of this approach to save  static power. The notion of NoC slack has been used before  [8] in the context of the criticality of packet delivery to an  application’s execution, which is quite different from the  slack concept exploited in this paper. Another related approach is bufferless routing [12, 15] which saves router  power by eliminating buffers, but it may introduce potential  livelock, misrouting and packet reassembly issues that must  be handled appropriately. Moreover, besides buffers, there  are other key components in NoC routers that also consume  a considerable amount of static power which is reduced with  Power Punch.  8. Conclusion  Current and future many-core chips require on-chip networks to be designed with both low power and high perfor11  388         mance. While conventional power-gating of on-chip routers  incurs significant performance penalty due to the blocking  problems, this paper investigates the challenges and viability of achieving non-blocking power-gating. Power Punch, a  novel and effective power-gating scheme, is proposed in this  work. Power Punch exploits the slack in hop count as well  the slack at source nodes to send power control signals  ahead of packets to “punch through” any blocked routers  along the imminent path of packets, turning them on. With  Power Punch, packets do not suffer router wakeup latency  or detour latency. Simulation results verify that significant  router static energy savings with little performance penalty  can be had, demonstrating the viability of achieving near  non-blocking power-gating.  Acknowledgement  We sincerely thank the anonymous reviewers for their  helpful comments and suggestions. This research was supported, in part, by the National Science Foundation (NSF),  grant CCF-1321131 and the Software and Hardware Foundations program of the NSF.   "
2016,The runahead network-on-chip.,"With increasing core counts and higher memory demands from applications, it is imperative that networks-on-chip (NoCs) provide low-latency, power-efficient communication. Conventional NoCs tend to be over-provisioned for worst-case bandwidth demands leading to ineffective use of network resources and significant power inefficiency; average channel utilization is typically less than 5% in real-world applications. In terms of performance, low-latency techniques often introduce power and area overheads and incur significant complexity in the router microarchitecture. We find that both low latency and power efficiency are possible by relaxing the constraint of lossless communication. This is inspired from internetworking where best effort delivery is commonplace. We propose the Runahead NoC, a lightweight, lossy network that provides single-cycle hops. Allowing for lossy delivery enables an extremely simple bufferless router microarchitecture that performs routing and arbitration within the same cycle as link traversal. The Runahead NoC operates either as a power-saver that is integrated into an existing conventional NoC to improve power efficiency, or as an accelerator that is added on top to provide ultra-low latency communication for select packets. On a range of PAR-SEC and SPLASH-2 workloads, we find that the Runahead NoC reduces power consumption by 1.81 as a power-saver and improves runtime and packet latency by 1.08× and 1.66× as an accelerator.","The Runahead Network-On-Chip Zimo Li University of Toronto Joshua San Miguel University of Toronto zimo.li@mail.utoronto.ca joshua.sanmiguel@mail.utoronto.ca Natalie Enright Jerger University of Toronto enright@ece.utoronto.ca ABSTRACT With increasing core counts and higher memory demands from applications, it is imperative that networkson-chip (NoCs) provide low-latency, power-eﬃcient communication. Conventional NoCs tend to be overprovisioned for worst-case bandwidth demands leading to ineﬀective use of network resources and signiﬁcant power ineﬃciency; average channel utilization is typically less than 5% in real-world applications. In terms of performance, low-latency techniques often introduce power and area overheads and incur signiﬁcant complexity in the router microarchitecture. We ﬁnd that both low latency and power eﬃciency are possible by relaxing the constraint of lossless communication. This is inspired from internetworking where best eﬀort delivery is commonplace. We propose the Runahead NoC, a lightweight, lossy network that provides single-cycle hops. Allowing for lossy delivery enables an extremely simple buﬀerless router microarchitecture that performs routing and arbitration within the same cycle as link traversal. The Runahead NoC operates either as a power-saver that is integrated into an existing conventional NoC to improve power eﬃciency, or as an accelerator that is added on top to provide ultra-low latency communication for select packets. On a range of PARSEC and SPLASH-2 workloads, we ﬁnd that the Runahead NoC reduces power consumption by 1.81× as a power-saver and improves runtime and packet latency by 1.08× and 1.66× as an accelerator. 1. INTRODUCTION With increasing on-chip core counts, networks-onchip (NoCs) are an eﬀective way of communicating between these many components. However, NoCs consume a signiﬁcant amount of power in modern chip multiprocessors (CMPs) [26, 42], and energy eﬃciency has been a primary concern for researchers and designers [10, 11]. Reducing the power of the NoC while increasing performance is essential for scaling up to larger systems for future CMP designs. Minimizing power consumption requires more eﬃcient use of network resources. Though buﬀers consume a signiﬁcant portion of network power and area [26], traditional NoC designs tend to provision large amounts of buﬀers to meet worst-case throughput requirements. Yet large buﬀers are often unnecessary as single-ﬂit 978-1-4673-9211-2/16/$31.00 c(cid:2)2016 IEEE packets represent a high fraction of the total network traﬃc in real applications [33]. Several buﬀerless NoC designs have been proposed in the past [18, 23, 35, 37]. These designs achieve signiﬁcant power savings at a cost of lower saturation throughput compared to conventional buﬀered routers. NoC channel utilization of single-threaded and multi-threaded CMP workloads tends to be low, with average injection rates of only 5% [5, 22, 25]. Low resource utilization translates into ineﬃcient use of network resources. To address this, several multi-NoC systems have been proposed in the past [1, 16, 17, 19, 39, 40, 44]. Multi-NoCs use total bandwidth more eﬃciently since they can be designed with heterogeneous physical subnetworks; messages can be categorized and injected into diﬀerent networks depending on packet type. For example, latency sensitive messages are injected into a low-latency, high-power network, while non-critical messages are injected into a low-power network [1, 40]. Minimizing NoC latency is essential to meet the higher communication demands of future CMPs. Techniques include reducing the number of router pipeline stages through lookahead routing [20] and bypassing via express virtual channels [31]. Non-speculative singlecycle routers allocate router switches in advance of packet arrival [30]. Route predictions can also reduce NoC latency [24,34]. Though these designs improve performance, they come at a cost of increased complexity, power and area. We propose the Runahead NoC 1 which serves as 1) a power-saver that exploits heterogenous traﬃc for more eﬃcient use of network resources, or 2) an accelerator that provides lightweight, low-latency communication on top of a conventional NoC. The Runahead NoC is designed for simplicity; it is buﬀerless with a lightweight router architecture which consumes very little area and power. To accomplish this simplicity, the Runahead NoC is lossy, allowing packets to be dropped in the presence of contention. It is inspired by the “best ef1 The proposed network allows select packets to take a head start compared to the rest of the network traﬃc, hence the name Runahead. The name for our network is also inspired by runahead execution [38] which allows the processor to speculatively prefetch loads from the instruction window to tolerate long latency operations. The Runahed NoC is an orthogonal design that could be easily combined with processor optimizations such as runahead execution. 333 fort” concept in internetworking, meaning that there is no guarantee a packet will arrive at its destination.2 Our design is not meant to be a stand-alone network; it is meant as a plug-and-play network that either replaces resources in an existing NoC to save power or is added on top as an accelerator. Contributions. We make the following contributions: • Propose the Runahead NoC, which through its simplicity, provides single-cycle hops and “best ef• Evaluate the Runahead NoC as a power-saver and fort” delivery for latency-sensitive packets; show that it achieves 1.81× and 1.73× savings in 1.33× lower latency (1.05× application speedup); power and active silicon area while still providing • Evaluate the Runahead NoC as an accelerator and show that it improves packet latency by 1.66× on average (1.08× application speedup), with only 10% and 16% overheads in power and active area. 2. THE RUNAHEAD NETWORK In this section, we present our Runahead NoC architecture which is lightweight, lossy, and achieves a singlecycle per-hop latency. It must be paired with a regular lossless NoC to provide guaranteed delivery of all packets. The Runahead NoC can be 1) added to a regular NoC as an accelerator, providing low-latency transmission of latency-sensitive packets, or 2) integrated into a regular NoC as a power-saver, providing power and area savings without harming performance. As an Accelerator: When used as an accelerator, the conﬁguration of the existing regular NoC is left unchanged and its operation is undisturbed. All packets are injected into the regular NoC, while only latencysensitive single-ﬂit packets are injected into the Runahead NoC. Multi-ﬂit packets are excluded to minimize the complexity when one or more ﬂits of a packet are dropped. The Runahead network carries all coherence control packets, which are typically single ﬂit. It also carries single-ﬂit data response packets. These packets are sent in response to a cache miss and only contain the critical word (i.e., the initially requested word) of the data block. This is described in Section 2.3. Since the regular NoC is lossless, any packets dropped by the Runahead NoC will still arrive at their destination. The goal of the accelerator is to provide an opportunity for ultra-fast delivery of latency-sensitive packets while incurring low power and area overheads. As a Power-Saver: When used as a power-saver, the existing regular NoC is under-provisioned to allow for the integration of the Runahead NoC. As in the accelerator case, the Runahead NoC only carries latencysensitive single-ﬂit packets. The regular NoC still carries all packets to guarantee delivery of any packets that may be dropped. In our experiments, we assume a regular multi-NoC system and replace one of the subnetworks with our Runahead NoC. The Runahead network 2 In contrast, ”best eﬀort” in NoC literature usually means that there is no guarantee on bandwidth and latency. Best eﬀort NoCs have been explored in the context of quality of service [2, 32, 43]. consumes very little power and area. This is because it is buﬀerless and consists of a simpliﬁed router architecture with no routing tables nor complex arbiters. Despite the increased congestion in the smaller regular NoC, overall application performance is unharmed since latency-sensitive packets are transfered quickly. The goal of the power-saver is to minimize area and power consumption while maintaining comparable or better performance. Overview. In the following sections, we ﬁrst give a high-level overview of our Runahead router architecture (Section 2.1). We then describe how routing computation and port arbitration are performed (Section 2.2), enabling the single-cycle per-hop latency. Finally, we discuss critical word forwarding for data response packets (Section 2.3) and how to integrate our Runahead NoC with the regular NoC (Section 2.4). 2.1 The Runahead Routers To achieve single-cycle hops and ensure low area and power consumption, the routers in the Runahead network need to be simple. In this work, we target a 2D mesh, which is commonplace in modern systems (e.g., Tilera [45] and Intel Terascale [26]). Figure 1 illustrates the design of the Runahead router. It consists of ﬁve multiplexers: one for each of the output ports of the four cardinal directions and one for the ejection port. The Runahead routers share the same injection port as the routers in the regular network. Runahead routers are buﬀerless. Only four latches are needed to store up to four single-ﬂit packets that may come in from the input ports of the 4 cardinal directions at any cycle. Injected packets are stored in the input buﬀer in the regular router. Header information is extracted from incoming packets at the input ports and directed to the routing computation and port arbitration unit. For clarity, data connections are not shown in the ﬁgure. Lossy Delivery. Our Runahead routers use XY dimension-order routing (DOR), which greatly simpliﬁes routing and arbitration. Port arbitration directs packets from input ports to their corresponding output ports and determines which packets to drop in the event of a conﬂict. The Runahead router does not collect dropped packets, and the Runahead NoC does not try to forward them again. This is diﬀerent from prior work, such as SCARAB [23] and BPS [21], where a NACK is sent to the source for packet re-transmission. In the Runahead NoC, the dropped packet will always be delivered by the lossless regular NoC. The Runahead NoC is inherently deadlock-free since packets are dropped instead of blocked upon conﬂicts. This eliminates the need for complex deadlock prevention mechanisms. Section 2.2 describes the routing computation and port arbitration. Single-Cycle Hops. Unlike conventional virtualchannel routers with 3 to 5 pipeline stages, the Runahead router delivers packets in a single cycle per hop. Route computation, port arbitration and link traversal are all combined into a single step. The Runahead routers are hardcoded for XY DOR; no routing tables 334 Legend:  (cid:4)(cid:6)(cid:6)(cid:1) (cid:5)(cid:18)(cid:16)(cid:15)(cid:25)(cid:21)(cid:20)(cid:1) (cid:8)(cid:1) (cid:4)(cid:13)(cid:24)(cid:13)(cid:1) (cid:6)(cid:23)(cid:21)(cid:19)(cid:1) (cid:8)(cid:1) (cid:4)(cid:13)(cid:24)(cid:13)(cid:1) (cid:6)(cid:23)(cid:21)(cid:19)(cid:1) (cid:5)(cid:1) (cid:12)(cid:1) (cid:10)(cid:21)(cid:27)(cid:24)(cid:16)(cid:1)(cid:3)(cid:21)(cid:19)(cid:22)(cid:27)(cid:24)(cid:13)(cid:25)(cid:21)(cid:20)(cid:1)(cid:29)(cid:1)(cid:9)(cid:21)(cid:23)(cid:24)(cid:1) (cid:2)(cid:23)(cid:14)(cid:17)(cid:24)(cid:23)(cid:13)(cid:25)(cid:21)(cid:20)(cid:1) (cid:5)(cid:1) (cid:4)(cid:13)(cid:24)(cid:13)(cid:1) (cid:6)(cid:23)(cid:21)(cid:19)(cid:1) (cid:12)(cid:1) (cid:4)(cid:13)(cid:24)(cid:13)(cid:1) (cid:6)(cid:23)(cid:21)(cid:19)(cid:1) (cid:11)(cid:1) (cid:11)(cid:1) (cid:7)(cid:20)(cid:18)(cid:16)(cid:15)(cid:25)(cid:21)(cid:20)(cid:1)(cid:9)(cid:21)(cid:23)(cid:24)(cid:1) Figure 1: Runahead router design are necessary. The output port of an incoming packet is quickly determined from the destination information in the header. By allowing for dropped packets, the Runahead router greatly simpliﬁes port arbitration. Our design uses a ﬁxed arbitration scheme where the priority of incoming packets is static for each output port. Speciﬁcally, our port arbitration scheme always prioritizes packets going straight over packets turning. Thus the logic that controls the output port multiplexers is very simple, allowing the entire process to ﬁt within one cycle. Since all hops are single cycle, the latency of a packet is fully deterministic from source to destination, assuming it is not dropped along the way. The latency in cycles is equal to the number of hops travelled. Because of this, it is impossible for a packet to arrive on the regular NoC earlier than on the Runahead network. Single-Flit Packets. Since there are no buﬀers, the Runahead router does not have virtual channels, just a single physical channel per output port. Handling multi-ﬂit packets introduces too much complexity in the Runahead NoC since at any moment, any ﬂit can be dropped. As a result, a multi-ﬂit packet could arrive at its destination with some missing ﬂits. We would need additional complexity at the cache controllers and memory controllers to support incomplete data packets. Thus, to minimize overheads and complexity, our design only supports single-ﬂit packets. 2.2 Route Computation and Port Arbitration Route computation and port arbitration are performed together to allow packets to traverse each hop in a single cycle. The destination is encoded in the header as signed X and Y values that indicate the relative number of hops remaining until the destination. The sign indicates the direction of travel. Route computation is a simple matter of determining the correct output port based on these values. We employ a ﬁxed arbitration scheme for each output port. A packet that is going straight has higher priority than a packet that is turning. If packets from two diﬀerent input ports are turning towards the same output port, one of the input ports is hardcoded to always take precedence. For example, if both the east and west input ports are contending for the north port, the arbitration always selects the west port. Similarly, for the ejection port, arbitration is hardcoded such that speciﬁc input ports always win. This minimizes complexity and allows us to combine the route computation and port arbitration steps into a single cycle. With XY DOR routing, our ﬁxed arbitration scheme yields only three places where a packet can be dropped: 1) At injection, 2) When the packet is making a turn from the X to Y direction, or 3) At ejection when a routing conﬂict occurs at the ejection port. This applies to all packets no matter how far they are traveling. Thus the number of places where a packet can be dropped is constant and does not scale with network size. The route computation and port arbitration unit is shown in Figure 2. The inputs are obtained from the header information of the incoming packets at each input port. The required signals from each input packet are denoted by Xdirection , Ydirection , Vdirection , which correspond to the destination X and Y values and a valid bit; the valid bit indicates that there exists a valid packet waiting at the input port. The ﬁgure shows the logic for diﬀerent output ports. In parallel with route computation and port arbitration, the X or Y value in the packet header is updated for the next hop. East and West Output: The advantage of using XY DOR is that it simpliﬁes east and west routing and arbitration, as shown in Figure 2a. The east and west direction output ports only need to consider the latches of their opposing input ports, as well as the injection port. Anytime a packet arrives at either the east or west input port with a non-zero X value, it is guaranteed to be forwarded straight since it has the highest priority in our ﬁxed arbitration scheme. It is impossible for a packet to turn on to either the E or W directions. It is also impossible for a packet to be forwarded back to the direction from which it arrived. North and South Output: Routing and arbitration are more complicated for the north and south output ports since they need to consider packets that are turning. Figure 2b shows that arbitration is hardcoded such that the outermost multiplexer always chooses the opposing input port if there is a valid incoming packet; this enforces our straight-ﬁrst arbitration. The logic for this only needs to look at the header’s Y value to check that the packet has not reached its destination. If there is no valid packet at the opposing input port, the ﬁxed arbitration scheme ﬁrst checks to see if the west input port is turning, followed by the east input port, and ﬁnally the injection port. In our implementation, a packet at the west input port always takes precedence over a packet at the east port when both of them are trying to turn to the same output port. A packet at the east or west input port is determined to be turning if it contains a zero X value with a non-zero Y value. Note 335 East Input  VE  XE  YE  Packet  West Input  Vw  XW  YW  Packet  North Input  VN  XN  YN  Packet  South Input  VS  XS  YS  Packet  Injection  VI  XI  XI  Packet  VW && XW != 0  1 1 1  0 0 0  East Output  VE && XE != 0  VE && 1  1 0  0 W West Output  (a) Structure for E and W output East Input  VE  XE  YE  Packet  West Input  Vw  XW  YW  Packet  North Input  VN  XN  YN  Packet  South Input  VS  XS  YS  Packet  Injection  VI  XI  XI  Packet  South Output  S North Output  N 1  1 0  0 VN && YN != 0  VN && Y 1  1 1  0  0 0 0  VW && XW == 0 && YW > 0  VW &&  XW == 0 && YW > X W 1  1  1 0  0 0 0 0 VE && XE == 0 && YE > 0  VE && XE == 0 && YE > 0 = 0 1  1  1 0  0 VS && YS != 0  VS && Y 1  1  1 0  0  0 VW && XW == 0 && YW < 0  1  1 1 0  000 VE && XE == 0 && YE < 0  VE && X (b) Structure for N and S output East Input  VE  XE  YE  Packet  West Input  Vw  XW  YW  Packet  North Input  VN  XN  YN  Packet  South Input  VS  XS  YS  Packet  Injection  VI  XI  XI  Packet  VN && XN == 0 && YN ==0  1 1  0 0  Ejection  E 1  1  1 0  0  0 VS && XS == 0 && YS == 0  VS && X 1  1  1 0  0 0 VW && XW == 0 && YW == 0  VW && XW == 0 && YW (c) Structure for Ejection Figure 2: Route computation and port arbitration. that a packet traverses at most three 2-to-1 multiplexers from its input port to its output port, keeping the critical path delay low. Ejection Output: The ejection port, shown in Figure 2c, is similar to that of the north and south output ports. Incoming packets are ranked based on the following order of input ports: N, S, W, E. As with the north and south output ports, an ejecting packet traverses at most three multiplexers. To determine if a packet is destined for the ejection port, both the X and Y values need to be zero. A packet can never be injected with the same source and destination nodes, thus eliminating the need to connect the ejection port to the injection port. Starvation and Fairness. Hardcoding arbitration leads to some potential unfairness or starvation. However, since all data injected into the Runahead NoC is also injected into the regular NoC, forward progress for the application is guaranteed. Our port arbitration scheme is stateless. There is no mechanism to detect and prevent starvation. For example, when arbitrating for the north output port, packets at the south input port will always take precedence over those at the east and west input ports even if it means the packets at these two ports are always dropped. This keeps complexity at a minimum, allowing for low overhead and ensuring that the design ﬁts within a single clock period. The goal of the Runahead network is not to provide fair communication because the regular NoC would already provide such a platform. In Section 4, we evaluate unfairness; in practice, arrival rates are relatively uniform across all source nodes. Due to low contention in these networks, packets are often delivered successfully, mitigating any concerns about fairness. Without mechanisms to prevent starvation and ensure fairness, the Runahead NoC has less overhead once it is combined with a regular NoC. 2.3 Critical Word Forwarding The Runahead network is meant to carry only latency-sensitive packets. In a cache-coherent CMP, all control packets (i.e., requests, invalidations, acknowledgements) and data response packets are latencysensitive. However, the Runahead network is designed for single-ﬂit packets to avoid the complexity of dropped ﬂits in multi-ﬂit packets. As a result, data response packets cannot be carried on the Runahead network. Fortunately, 67% of the critical words are the ﬁrst word in a cache block in real applications [12]. Also, many modern CMPs can support critical word forwarding. When composing the data response packet, the initially requested (critical) word is sent in the ﬁrst ﬂit. This way, when the ﬁrst ﬂit arrives at the L1 cache, the critical word is forwarded directly to the processor so that it can continue executing before the rest of the packet has arrived. Naturally, the critical word is the most latency-sensitive word in the data block. Thus in our implementation, we assume critical word forwarding and inject the ﬁrst ﬂit of all data response packets (bound for the L1 cache) into the Runahead network. 2.4 Integration into the Regular Network The Runahead network can be easily integrated with an existing network. Its routers’ injection and ejection ports are simply connected to the injection and ejection queues of the regular network. The injection port of each Runahead router connects directly to the head of 336 the regular input buﬀer, so that single-ﬂit packets waiting to be injected into the regular network are also injected into the Runahead network. In our experiments, we ﬁnd that a large portion of packets are dropped at injection, accounting for up to 50% of all dropped packets in the Runahead network. This is because in port arbitration, packets from the injection ports have lowest priority, as explained in Section 2.2. To improve this, we design the Runahead router to try to inject a packet multiple times for as long as it is at the head of the injection queue. If the packet at the head of the queue does not succeed in arbitrating for its output port, we try again in the next cycle if the packet is still at the head (i.e., if the packet has not yet been injected into the regular network either). If the packet is injected into the Runahead network successfully, a ﬂag is set at the input port so that we do not try to inject it again in subsequent cycles. The ejection ports connect to the regular output buﬀers. When a packet is ejected from the Runahead network, it is immediately forwarded to the corresponding cache controller or memory controller. It is then stored in a small buﬀer until the same packet is ejected from the regular network. This ensures that packets that are successfully delivered via the Runahead network are not sent to the controllers twice. Note that a packet will never be ejected from the regular network before the Runahead network, as discussed in Section 2.1. For the applications we have studied, the maximum number of entries that a buﬀer needs to hold is 15. Conservatively assuming packet IDs of 8 bytes, this buﬀer would be less than 128 bytes which is small compared to the size of buﬀers in regular NoC routers. In the unlikely event that the buﬀer is full, the network interface will discard packets that arrive on the Runahead network. This is safe since any packet that arrives in the Runahead network will also arrive on the regular network. 2.5 Discussion This section discusses and reiterates some key points in our design. The Runahead network does not compromise correctness in the communication fabric despite being a lossy design, since it serves as a companion to a lossless network. The use of such specialized networks with general-purpose networks is timely in the dark silicon era, providing eﬃciency gains analogous to accelerators for general-purpose cores. As discussed previously, to ensure correctness, the Runahead network requires buﬀers at ejection ports for packets that are still inﬂight in the lossless network. However, this does not introduce much overhead; the size of these buﬀers can be ﬁxed and does not need to scale with network size nor network usage. In the rare event that the buﬀers are full, packets can simply be dropped upon ejection without compromising correctness. The drop rate also does not scale with network size due to the fact that for any given packet, there will always be 3 (and only 3) places where the packet can be dropped: 1) at injection, 2) when turning, and 3) at ejection. Furthermore, the drop rate does not necessarily scale with network usage; Topology Channel width Virtual channels Router pipeline stages Routing algorithm Flit size Packet size 4×4 mesh (8×8 for SynFull) 8 byte 6 per port (4 ﬂit each) 3 X-Y dimension-order 8-byte 1 (Control) / 9 (Data) Flits Table 1: Baseline network simulation parameters Topology Channel width 4×4 mesh (8×8 for SynFull) 10 byte (8B for ﬂit, 2B for other metadata) Virtual channels None Routing algorithm X-Y dimension-order Flit size 8-byte Table 2: The Runahead network simulation parameters # of Cores/Threads Private L1 Cache Shared L2 Cache # of Directories Cache Coherence 16/16, 1GHz 16KB 4-way LRU 64Byte blocks fully distributed, 8-way LRU, 4 MB total 4 directories located at each corner of the topology MOESI distributed directory Table 3: Full-system simulation system parameters the Runahead network can handle high network load by being selective. Our design thrives on the common case of low network load. However, at high load, the Runahead network can simply be more selective when injecting packets, selecting only those deemed to be most latency critical. 3. METHODOLOGY We evaluate the eﬀectiveness of the Runahead network in conjunction with a baseline lossless network. The conﬁguration parameters for our baseline network are listed in Table 1. The Runahead network simulation parameters are listed in Table 2. NoC Conﬁgurations. We compare our proposed Runahead network against conventional lossless NoCs, some of which are multi-NoC designs. We also compare the Runahead network against two existing designs: Aergia [15], a prioritization scheme, and DejaVu switching [1], a multi-plane NoC design. The conﬁgurations are listed below: • Baseline64(cid:2) : This conﬁguration has a single loss• Baseline128 Random(cid:2) : less NoC with 64-bit channels, as in Table 1. In this conﬁguration, the NoC is composed of two independent lossless networks, each conﬁgured as in Baseline64. Total channel width is 16 bytes (two 64-bit networks). The workload is shared evenly between the two NoCs (i.e., 50% of traﬃc is randomly injected into • Baseline128 Select each network). : In this conﬁguration, the † (cid:2) † Used in both full-system and SynFull evaluations. Used only in SynFull evaluations. 337 ‡ ‡ NoC is conﬁgured identically to that of Baseline128 Random. However, instead of sharing the traﬃc evenly, Network 1 is responsible for latencysensitive traﬃc (i.e., packets that we would inject into the Runahead network). This includes singleﬂit packets and critical words. Network 2 handles all other traﬃc. Since delivery is guaranteed, single-ﬂit packets are only injected into Network 1 • Aergia instead of both networks. is a prioritization scheme that uses the notion of slack to determine the priorities of packets [15]. Aergia calculates packet priority based on local slack which is deﬁned to be the number of cycles a packet can be delayed without delaying any subsequent instructions. Aergia uses L2 hit/miss status, number of L2 miss predecessors and number of hops to estimate the packet slack. In our full-system simulations, we use the same setup as the Baseline64 network. We modify Aergia’s prioritization scheme for allocations and arbitrations; we conservatively assume a perfect L2 miss predic• DejaVu Switching tor for accurate slack calculations. is a multi-plane NoC design where single-ﬂit control packets and multi-ﬂit data packets are separated in to diﬀerent planes [1]. Reservation packets are sent out on the control plane to reserve network resources on the data plane routers. The reservations enable the data packets to be forwarded without suﬀering delays from making routing decisions at every router. In our full-system simulations, the control plane uses the same parameters as the baseline network listed in Table 1. The reservation packets are sent 3 cycles ahead of data packets and they travel in the control plane. We assume the reservation queues in the data plane have inﬁnite size. To model the simpliﬁed router design in the data plane of DejaVu switching, we use routers with single cycle router delay and one VC. Also, we forward the critical words to the processor as soon as the head of the data packets arrive. Though DejaVu switching can use a slower data plane for energy savings, we opt not to for a conservative performance com• Runahead(cid:2) : In this conﬁguration, we have a sinparison against Runahead. gle Baseline64 network, which carries 100% of the injected packets, along with the proposed Runahead network that carries latency-sensitive packets (i.e., single-ﬂit packets and critical words). As delivery is not guaranteed in the Runahead network, duplicate injection of latency-sensitive packets into both networks is required. The total channel width in this case is 18 bytes (8 bytes for the Regular network and 10 bytes for the Runahead network). Note that we allocate two extra bytes to the Runahead network channel width to conservatively to account for any additional metadata for supporting critical word forwarding. This does not give our Runahead network a performance advantage ‡ Used only in full-system evaluations. since all packets are single-ﬂit; in fact, it incurs a power and area disadvantage. As an accelerator, Runahead is evaluated relative to Baseline64 and compared against Aergia. As a powersaver, Runahead is evaluated relative to the Baseline128 conﬁgurations and compared against DejaVu Switching. Synthetic Traﬃc. We evaluate latency and throughput of the Runahead network under several synthetic traﬃc patterns covering a wide range of network utilization scenarios. We use a modiﬁed version of Booksim, a cycle-level network simulator [28]. All conﬁgurations use an 8×8 2D mesh network, and we assume all packets are single-ﬂit. Full-System Simulation. To evaluate the real system performance of our Runahead network, we use Booksim and Gem5 [8]. The system parameters are listed in Table 3. All network conﬁgurations use a 4×4 mesh topology with parameters listed in Tables 1 and 2. The full-system simulation workloads consist multithreaded workloads from SPLASH-2 [46] and PARSEC [7]. For each multi-threaded workload, we run 16 threads with the simmedium input set until completion. We measure the execution time in the application’s region of interest. For full-system simulations, the memory controllers are located at the corners of the mesh network. We keep the cache sizes small to provide greater stress on the network. This does not give the Runahead network an advantage because it drops more packets when there is more network contention. SynFull Workloads. To further evaluate our Runahead network design in a larger network, we use multiprogrammed SynFull traﬃc workloads [3] with Booksim. SynFull workloads are designed to reproduce the cache coherent behavior of multi-threaded applications from SPLASH-2 [46] and PARSEC [7]. These workloads packets. All conﬁgurations use an 8×8 2D mesh netconsist of single-ﬂit control packets and multi-ﬂit data work. Other network conﬁguration parameters are the same as previous experiments. For each 64-core workthreaded application. Each instance is assigned a 4×4 load, we run 4 identical instances of a 16-way multilocated at the left and right edge nodes of the 8×8 mesh. quadrant of cores. For SynFull, memory controllers are All four instances send memory traﬃc throughout the chip. To keep measurements consistent across all conﬁgurations, we only measure the latency of unique packets that are seen by the application. This means that if a packet is injected into both the Runahead and regular lossless networks, we only measure the latency of the packet that arrives ﬁrst; subsequent arrival of the same packet is discarded by the NoC. For data packets, latency is taken for the entire packet to arrive, not just the critical word. To measure the potential beneﬁt of accelerating critical words, we report the diﬀerence in arrival times between the critical word and the rest of the data block. We did not compare with Aergia and DejaVu switching as Aergia relies on information from real system conditions and DejaVu generates extra reservation packets; neither of these can be easily modeled with SynFull. Power and Area. We model power and area us338 ing DSENT [41] and RTL. DSENT results are collected using a 22nm bulk/SOI, low-V process node. Dynamic power is obtained by modelling the system using the average injection rates collected from the SynFull workloads. To ensure the feasibility of the Runahead router, we use an open source RTL router design [6] as a conventional router. The Runahead router is constructed on top of the existing RTL design. We use Synopsys design compiler with TSMC 65nm technology to evaluate the power and area for a single Runahead router. 4. EVALUATION This section provides performance, power and area evaluations of our proposed Runahead network. We ﬁrst evaluate latency and throughput under synthetic traﬃc. We then evaluate the performance improvements in the Runahead network in full-system simulation, followed by a performance evaluation of the Runahead network using real application models from SynFull. We then measure area and power consumption of the Runahead network. 4.1 Latency and Throughput Figure 3 shows the average packet latency for the Baseline64 and Runahead conﬁgurations on diﬀerent synthetic traﬃc patterns. All simulations are done using single-ﬂit packets, and all packets are injected into the Runahead network. The Runahead NoC shows a signiﬁcant decrease in average packet latency compared to Baseline64. Note that the packet latency increases faster prior to saturation in the Runahead NoC for several traﬃc patterns. This is because as injection rate increases, the arrival rate decreases, leading to lower effectiveness of the Runahead network; more packets rely on the regular lossless network for delivery. Both NoC setups saturate around the same injection rate for most traﬃc patterns. This is because our Runahead network’s injection ports are connected to the same injection queues of the routers in the regular lossless network. If congestion occurs at injection in the lossless network, the Runahead network does not provide any beneﬁt. However, in other traﬃc patterns such as Bit Reverse where congestion occurs within the network rather than at the injection ports, the Runahead NoC saturates at a higher injection rate. 4.2 Full-System Simulation In this section, we evaluate the performance of Runahead as an accelerator and as a power-saver, compared against various NoC conﬁgurations (Section 3). We simulate benchmarks from PARSEC and SPLASH-2 using the Gem5 simulator. As an Accelerator. We compare speedup between Baseline64, Aergia and Runahead. The speedups are shown in Figure 4. These results are normalized to Baseline64, since the Runahead conﬁguration is essentially our proposed design added on top of the baseline 64-bit lossless network. We ﬁrst notice that in our experiments, Aergia has little impact on system performance. The reasons are twofold. First, most PARSEC (cid:40)(cid:36)(cid:1) (cid:39)(cid:41)(cid:1) (cid:39)(cid:36)(cid:1) (cid:38)(cid:41)(cid:1) (cid:38)(cid:36)(cid:1) (cid:37)(cid:41)(cid:1) (cid:37)(cid:36)(cid:1) (cid:41)(cid:1) (cid:36)(cid:1) (cid:1) (cid:32) (cid:11) (cid:22) (cid:13) (cid:27) (cid:10) (cid:5) (cid:1) (cid:27) (cid:13) (cid:19) (cid:19) (cid:11) (cid:1) (cid:10) (cid:6) (cid:13) (cid:15) (cid:10) (cid:25) (cid:13) (cid:31) (cid:2) (cid:37)(cid:45)(cid:1) (cid:37)(cid:37)(cid:45)(cid:1) (cid:38)(cid:37)(cid:45)(cid:1) (cid:39)(cid:37)(cid:45)(cid:1) (cid:4)(cid:22)(cid:18)(cid:13)(cid:11)(cid:28)(cid:23)(cid:22)(cid:1)(cid:7)(cid:10)(cid:27)(cid:13)(cid:1) (cid:9)(cid:22)(cid:17)(cid:14)(cid:23)(cid:25)(cid:21)(cid:34) (cid:7)(cid:30)(cid:22)(cid:10)(cid:16)(cid:13)(cid:10)(cid:12)(cid:1) (cid:9)(cid:22)(cid:17)(cid:14)(cid:23)(cid:25)(cid:21)(cid:34) (cid:3)(cid:10)(cid:26)(cid:13)(cid:20)(cid:17)(cid:22)(cid:13)(cid:42)(cid:40)(cid:1) (cid:8)(cid:25)(cid:10)(cid:22)(cid:26)(cid:24)(cid:23)(cid:26)(cid:13)(cid:34) (cid:7)(cid:30)(cid:22)(cid:10)(cid:16)(cid:13)(cid:10)(cid:12)(cid:1) (cid:8)(cid:25)(cid:10)(cid:22)(cid:26)(cid:24)(cid:23)(cid:26)(cid:13)(cid:34) (cid:3)(cid:10)(cid:26)(cid:13)(cid:20)(cid:17)(cid:22)(cid:13)(cid:42)(cid:40)(cid:1) (cid:2)(cid:26)(cid:32)(cid:21)(cid:21)(cid:13)(cid:27)(cid:25)(cid:17)(cid:11)(cid:34) (cid:7)(cid:30)(cid:22)(cid:10)(cid:16)(cid:13)(cid:10)(cid:12)(cid:1) (cid:2)(cid:26)(cid:32)(cid:21)(cid:21)(cid:13)(cid:27)(cid:25)(cid:17)(cid:11)(cid:34) (cid:3)(cid:10)(cid:26)(cid:13)(cid:20)(cid:17)(cid:22)(cid:13)(cid:42)(cid:40)(cid:1) (cid:40)(cid:37)(cid:45)(cid:1) (cid:1) (cid:29) (cid:9) (cid:19) (cid:11) (cid:24) (cid:8) (cid:5) (cid:1) (cid:24) (cid:11) (cid:16) (cid:9) (cid:8) (cid:6) (cid:11) (cid:12) (cid:8) (cid:1) (cid:22) (cid:11) (cid:28) (cid:2) (a) Uniform Random, Transpose and Asymmetric (cid:37)(cid:33)(cid:1) (cid:36)(cid:38)(cid:1) (cid:36)(cid:33)(cid:1) (cid:35)(cid:38)(cid:1) (cid:35)(cid:33)(cid:1) (cid:34)(cid:38)(cid:1) (cid:34)(cid:33)(cid:1) (cid:38)(cid:1) (cid:33)(cid:1) (cid:34)(cid:44)(cid:1) (cid:40)(cid:44)(cid:1) (cid:34)(cid:36)(cid:44)(cid:1) (cid:34)(cid:41)(cid:44)(cid:1) (cid:4)(cid:19)(cid:15)(cid:11)(cid:9)(cid:25)(cid:20)(cid:19)(cid:1)(cid:7)(cid:8)(cid:24)(cid:11)(cid:1) (b) BitRev and BitComp (cid:3)(cid:14)(cid:24)(cid:22)(cid:11)(cid:28)(cid:31) (cid:7)(cid:27)(cid:19)(cid:8)(cid:13)(cid:11)(cid:8)(cid:10)(cid:1) (cid:3)(cid:14)(cid:24)(cid:22)(cid:11)(cid:28)(cid:31) (cid:3)(cid:8)(cid:23)(cid:11)(cid:17)(cid:14)(cid:19)(cid:11)(cid:39)(cid:37)(cid:1) (cid:3)(cid:14)(cid:24)(cid:9)(cid:20)(cid:18)(cid:21)(cid:31) (cid:7)(cid:27)(cid:19)(cid:8)(cid:13)(cid:11)(cid:8)(cid:10)(cid:1) (cid:3)(cid:14)(cid:24)(cid:9)(cid:20)(cid:18)(cid:21)(cid:31) (cid:3)(cid:8)(cid:23)(cid:11)(cid:17)(cid:14)(cid:19)(cid:11)(cid:39)(cid:37)(cid:1) Figure 3: Load-latency curves under synthetic traﬃc (cid:3)(cid:8)(cid:24)(cid:12)(cid:18)(cid:16)(cid:19)(cid:12)(cid:41)(cid:39)(cid:1) (cid:2)(cid:12)(cid:23)(cid:14)(cid:16)(cid:8)(cid:1) (cid:6)(cid:28)(cid:19)(cid:8)(cid:15)(cid:12)(cid:8)(cid:11)(cid:1) (cid:1) (cid:21) (cid:28) (cid:11) (cid:12) (cid:12) (cid:21) (cid:7) (cid:37)(cid:33)(cid:38)(cid:36)(cid:30)(cid:1) (cid:37)(cid:33)(cid:37)(cid:40)(cid:30)(cid:1) (cid:37)(cid:33)(cid:37)(cid:36)(cid:30)(cid:1) (cid:37)(cid:33)(cid:36)(cid:40)(cid:30)(cid:1) (cid:37)(cid:33)(cid:36)(cid:36)(cid:30)(cid:1) (cid:36)(cid:33)(cid:42)(cid:40)(cid:30)(cid:1) (cid:36)(cid:33)(cid:42)(cid:36)(cid:30)(cid:1) (cid:9)(cid:8)(cid:23)(cid:19)(cid:12)(cid:24)(cid:1) (cid:9)(cid:18)(cid:8)(cid:10)(cid:17)(cid:24)(cid:10)(cid:15)(cid:20)(cid:18)(cid:12)(cid:24)(cid:1) (cid:9)(cid:20)(cid:11)(cid:31)(cid:25)(cid:23)(cid:8)(cid:10)(cid:17)(cid:1) (cid:13)(cid:1) (cid:18)(cid:28)(cid:34)(cid:19)(cid:10)(cid:9)(cid:1) (cid:24)(cid:29)(cid:8)(cid:21)(cid:26)(cid:20)(cid:19)(cid:1) (cid:29)(cid:8)(cid:25)(cid:12)(cid:23)(cid:34)(cid:19)(cid:24)(cid:22)(cid:28)(cid:8)(cid:23)(cid:12)(cid:11)(cid:1) (cid:30)(cid:38)(cid:41)(cid:39)(cid:1) (cid:4)(cid:12)(cid:20) (cid:5) (cid:12)(cid:8)(cid:19)(cid:1) Figure 4: Runahead speedup as an accelerator and SPLASH benchmarks do not have high L2 miss rates, which is one of the key factors in computing priority in Aergia.6 Second, the benchmarks have very little congestion. One of our key motivations is that average channel utilization is low in real-world applications. Because of this, prioritization schemes are unlikely to ﬁnd opportunities to accelerate packets in the absence 1.08× speedup compared to the baseline (Baseline64). of congestion. On the other hand, Runahead achieves As a Power-Saver. When using the Runahead network as a power-saver, speedup is shown in Figure 5. Unlike in the previous section, these results are normalized to Baseline128 Random, since the Runahead conﬁguration eﬀectively under-provisions the baseline 128bit lossless NoC to make space for our proposed design. DejaVu has a speedup of 1.035× compared to Baseline128 Random. Runahead delivers a greater speedup of 1.045×, due to its lower per-hop latency for control packets and critical data words. Note that DejaVu has an advantage in situations where applications tend to access other words in the cache line (aside from the crit6 Aergia was originally proposed and evaluated using multiprogrammed SPEC workloads. 339 (cid:1) (cid:23) (cid:30) (cid:12) (cid:13) (cid:13) (cid:23) (cid:7) (cid:41)(cid:35)(cid:41)(cid:44)(cid:32)(cid:1) (cid:41)(cid:35)(cid:41)(cid:40)(cid:32)(cid:1) (cid:41)(cid:35)(cid:40)(cid:44)(cid:32)(cid:1) (cid:41)(cid:35)(cid:40)(cid:40)(cid:32)(cid:1) (cid:40)(cid:35)(cid:47)(cid:44)(cid:32)(cid:1) (cid:40)(cid:35)(cid:47)(cid:40)(cid:32)(cid:1) (cid:2)(cid:9)(cid:26)(cid:13)(cid:19)(cid:16)(cid:21)(cid:13)(cid:41)(cid:42)(cid:46)(cid:36)(cid:6)(cid:9)(cid:21)(cid:12)(cid:22)(cid:20)(cid:1) (cid:3)(cid:13)(cid:17)(cid:9)(cid:8)(cid:30)(cid:1) (cid:6)(cid:30)(cid:21)(cid:9)(cid:15)(cid:13)(cid:9)(cid:12)(cid:1) (cid:1) (cid:35) (cid:10) (cid:22) (cid:12) (cid:28) (cid:8) (cid:5) (cid:1) (cid:28) (cid:12) (cid:19) (cid:10) (cid:1) (cid:8) (cid:6) (cid:12) (cid:16) (cid:8) (cid:26) (cid:10)(cid:9)(cid:25)(cid:21)(cid:13)(cid:26)(cid:1) (cid:10)(cid:19)(cid:9)(cid:11)(cid:18)(cid:26)(cid:11)(cid:15)(cid:22)(cid:19)(cid:13)(cid:26)(cid:1) (cid:10)(cid:22)(cid:12)(cid:33)(cid:27)(cid:25)(cid:9)(cid:11)(cid:18)(cid:1) (cid:14)(cid:1) (cid:19)(cid:30)(cid:37)(cid:21)(cid:11)(cid:10)(cid:1) (cid:26)(cid:31)(cid:9)(cid:23)(cid:28)(cid:22)(cid:21)(cid:1) (cid:31)(cid:9)(cid:27)(cid:13)(cid:25)(cid:37)(cid:21)(cid:26)(cid:24)(cid:30)(cid:9)(cid:25)(cid:13)(cid:12)(cid:1) (cid:32)(cid:42)(cid:45)(cid:43)(cid:1) (cid:4)(cid:13)(cid:22) (cid:5) (cid:13)(cid:9)(cid:21)(cid:1) (cid:12) (cid:32) (cid:2) (cid:7)(cid:31)(cid:22)(cid:8)(cid:17)(cid:12)(cid:8)(cid:11)(cid:1) (cid:3)(cid:8)(cid:27)(cid:12)(cid:20)(cid:18)(cid:22)(cid:12)(cid:45)(cid:43)(cid:1) (cid:42)(cid:44)(cid:1) (cid:42)(cid:39)(cid:1) (cid:41)(cid:44)(cid:1) (cid:41)(cid:39)(cid:1) (cid:40)(cid:44)(cid:1) (cid:40)(cid:39)(cid:1) (cid:44)(cid:1) (cid:39)(cid:1) (cid:9)(cid:8)(cid:26)(cid:22)(cid:12)(cid:27)(cid:1) (cid:9)(cid:20)(cid:8)(cid:10)(cid:19)(cid:27)(cid:10)(cid:17)(cid:23)(cid:20)(cid:12)(cid:27)(cid:1) (cid:9)(cid:23)(cid:11)(cid:35)(cid:28)(cid:26)(cid:8)(cid:10)(cid:19)(cid:1) (cid:10)(cid:17)(cid:23)(cid:20)(cid:12)(cid:27)(cid:19)(cid:35)(cid:1) (cid:13)(cid:8)(cid:10)(cid:12)(cid:27)(cid:18)(cid:21)(cid:1) (cid:15)(cid:1) (cid:14)(cid:31)(cid:18)(cid:11)(cid:8)(cid:22)(cid:18)(cid:21)(cid:8)(cid:28)(cid:12)(cid:1) (cid:20)(cid:31)(cid:37)(cid:10)(cid:9)(cid:1) (cid:20)(cid:31)(cid:37)(cid:22)(cid:10)(cid:9)(cid:1) (cid:26)(cid:8)(cid:11)(cid:18)(cid:23)(cid:27)(cid:18)(cid:28)(cid:35)(cid:1) (cid:26)(cid:8)(cid:11)(cid:18)(cid:34)(cid:1) (cid:26)(cid:8)(cid:35)(cid:28)(cid:26)(cid:8)(cid:10)(cid:12)(cid:1) (cid:27)(cid:33)(cid:8)(cid:24)(cid:29)(cid:23)(cid:22)(cid:27)(cid:1) (cid:33)(cid:8)(cid:28)(cid:12)(cid:26)(cid:37)(cid:22)(cid:27)(cid:25)(cid:31)(cid:8)(cid:26)(cid:12)(cid:11)(cid:1) (cid:4)(cid:12)(cid:23) (cid:21)(cid:12)(cid:8)(cid:22)(cid:1) Figure 5: Runahead speedup as a power-saver ical word) much earlier. This is because data packets, as a whole, travel faster in DejaVu due to advanced reservations at data plane routers. The separation of data and control packets in DejaVu causes both networks to be less congested, unlike in Runahead where the smaller lossless network carries all types of packets. Although we forward the critical word in both cases, DejaVu populates lines in the cache sooner than Runahead after unblocking the stalled processor with the critical word. This explains the higher speedups for f f t, lu ncb and x264 compared to the Runahead network. Despite this, the Runahead network achieves 1.045× speedup while using fewer network resources compared to both Baseline128 Random and DejaVu switching. We obtain Runahead network activity by collecting results without the critical word forwarding optimization. Critical word packets are single-ﬂit duplicated packets that travel in the Runahead network; to keep the percentage of single-ﬂit packets accurate, we do not include these packets when collecting the results. Considering that data packets are 9 ﬂits in size, on average, only 23% of all ﬂits that travel through the network are injected into the Runahead network. We observe that applications with a higher percentage of single-ﬂit packets see more performance beneﬁt from the Runahead NoC. Fortunately, since over 72% of packets in the network are single-ﬂit, the Runahead network is still capable of speeding up the ma jority of network messages despite the fact that it only carries 23% of the ﬂit trafﬁc. The Runahead NoC is most eﬀective at improving performance if a large fraction of packets are successfully delivered; all applications studied have over 95% arrival rate. 4.3 SynFull In this section, we evaluate Runahead as an accelerator and as a power-saver given a larger network topology, compared against varying NoC conﬁgurations (Section 3). We simulate 14 diﬀerent SynFull applications. As an Accelerator. Figure 6 compares the average packet latency between Runahead and Baseline64. Recall that in the Runahead conﬁguration, the NoC is conﬁgured with a regular lossless network (identical to Baseline64) augmented with our proposed Runahead design, which serves as an accelerator. Given that the our design oﬀers very low per-hop latency, Runahead achieves 1.66× faster packet delivery on average. Note Figure 6: Average packet latency as an accelerator that the performance increase is signiﬁcant since arrival rates are very high, even with a larger network topology, as we show later in this section. As a Power-Saver. Figure 7 compares the average packet latency of Runahead against the two 128-bit baselines: Baseline128 Random and Baseline128 Select. The packet latency is normalized to Baseline128 Random. Recall that both of these baselines are conﬁgured with two 64-bit lossless subnetworks each; the only diﬀerence is that Baseline128 Select selectively injects packets based on latency-sensitivity. Baseline128 Random generally performs better than Baseline128 Select due to better load balancing between subnetworks. Given that both subnetworks are identical, selective injection oﬀers minimal latency improvement. However, the Runahead NoC performs the best ers packets 1.33× and 1.49× faster compared to Baseacross all benchmarks. On average, Runahead delivline128 Random and Baseline128 Select. To further investigate the beneﬁts of the Runahead network, we look at the cycle count between the time a critical word arrives and the time when the rest of the data block arrives in Baseline128 Select and the Runahead NoC in Figure 8. The Runahead network on average delivers critical words almost 23 cycles faster than the rest of the cache block. The arrival rate of packets in the Runahead network for each application is listed in Table 4. The average arrival rate across all applications is over 97% and the average hop count is 3.7. This means that the Runahead network delivers almost all of the single-ﬂit packets that travel through it, with an average latency of 3.7 cycles. In comparison, the lowest latency that can be achieved in the lossless regular network, which has a (3.7×(3+1)). As the ma jority of packets in the network 3-stage pipeline router plus link traversal, is 14.8 cycles are single ﬂit, the use of the Runahead network enables average packet latency to drop signiﬁcantly. To investigate the fairness of our port arbitration scheme, we show the arrival rate per source node for blackscholes (Figure 9a) and ﬀt (Figure 9b). Blackscholes exhibits the lowest arrival rate while ﬀt exhibits the highest hop count, as shown in Table 4. Lower average arrival rate may be an indication that some nodes experience starvation especially when applications have light traﬃc. On the other hand, as the distances trav340 (cid:5)(cid:1) (cid:5)(cid:2)(cid:7)(cid:1) (cid:5)(cid:2)(cid:8)(cid:1) (cid:5)(cid:2)(cid:9)(cid:1) (cid:5)(cid:2)(cid:10)(cid:1) (cid:6)(cid:1) (cid:6)(cid:2)(cid:7)(cid:1) (cid:6)(cid:2)(cid:8)(cid:1) (cid:10)(cid:9)(cid:26)(cid:22)(cid:13)(cid:27)(cid:1) (cid:10)(cid:20)(cid:9)(cid:11)(cid:19)(cid:27)(cid:11)(cid:17)(cid:23)(cid:20)(cid:13)(cid:27)(cid:1) (cid:10)(cid:23)(cid:12)(cid:34)(cid:28)(cid:26)(cid:9)(cid:11)(cid:19)(cid:1) (cid:11)(cid:17)(cid:23)(cid:20)(cid:13)(cid:27)(cid:19)(cid:34)(cid:1) (cid:14)(cid:9)(cid:11)(cid:13)(cid:27)(cid:18)(cid:21)(cid:1) (cid:16)(cid:1) (cid:15)(cid:31)(cid:18)(cid:12)(cid:9)(cid:22)(cid:18)(cid:21)(cid:9)(cid:28)(cid:13)(cid:1) (cid:20)(cid:31)(cid:37)(cid:11)(cid:10)(cid:1) (cid:20)(cid:31)(cid:37)(cid:22)(cid:11)(cid:10)(cid:1) (cid:26)(cid:9)(cid:12)(cid:18)(cid:23)(cid:27)(cid:18)(cid:28)(cid:34)(cid:1) (cid:26)(cid:9)(cid:12)(cid:18)(cid:33)(cid:1) (cid:26)(cid:9)(cid:34)(cid:28)(cid:26)(cid:9)(cid:11)(cid:13)(cid:1) (cid:27)(cid:32)(cid:9)(cid:24)(cid:29)(cid:23)(cid:22)(cid:27)(cid:1) (cid:32)(cid:9)(cid:28)(cid:13)(cid:26)(cid:37)(cid:22)(cid:27)(cid:25)(cid:31)(cid:9)(cid:26)(cid:13)(cid:12)(cid:1) (cid:3)(cid:13)(cid:23) (cid:21)(cid:13)(cid:9)(cid:22)(cid:1) (cid:5) (cid:23) (cid:26) (cid:21) (cid:9) (cid:18) (cid:20) (cid:35) (cid:9) (cid:6) (cid:12) (cid:13) (cid:1) (cid:13) (cid:19) (cid:11) (cid:28) (cid:1) (cid:9) (cid:4) (cid:28) (cid:22) (cid:13) (cid:34) (cid:11) (cid:1) (cid:2)(cid:9)(cid:27)(cid:13)(cid:20)(cid:18)(cid:22)(cid:13)(cid:39)(cid:40)(cid:41)(cid:37)(cid:7)(cid:9)(cid:22)(cid:12)(cid:23)(cid:21)(cid:1) (cid:2)(cid:9)(cid:27)(cid:13)(cid:20)(cid:18)(cid:22)(cid:13)(cid:39)(cid:40)(cid:41)(cid:37)(cid:8)(cid:13)(cid:20)(cid:13)(cid:11)(cid:28)(cid:1) (cid:7)(cid:31)(cid:22)(cid:9)(cid:17)(cid:13)(cid:9)(cid:12)(cid:1) Figure 7: Normalized packet latency as a power-saver (cid:39)(cid:1) (cid:43)(cid:1) (cid:40)(cid:39)(cid:1) (cid:40)(cid:43)(cid:1) (cid:41)(cid:39)(cid:1) (cid:41)(cid:43)(cid:1) (cid:42)(cid:39)(cid:1) (cid:10)(cid:9)(cid:27)(cid:23)(cid:13)(cid:28)(cid:1) (cid:10)(cid:21)(cid:9)(cid:11)(cid:20)(cid:28)(cid:11)(cid:18)(cid:24)(cid:21)(cid:13)(cid:28)(cid:1) (cid:10)(cid:24)(cid:12)(cid:35)(cid:29)(cid:27)(cid:9)(cid:11)(cid:20)(cid:1) (cid:11)(cid:18)(cid:24)(cid:21)(cid:13)(cid:28)(cid:20)(cid:35)(cid:1) (cid:14)(cid:9)(cid:11)(cid:13)(cid:28)(cid:19)(cid:22)(cid:1) (cid:17)(cid:1) (cid:16)(cid:32)(cid:19)(cid:12)(cid:9)(cid:23)(cid:19)(cid:22)(cid:9)(cid:29)(cid:13)(cid:1) (cid:21)(cid:32)(cid:37)(cid:11)(cid:10)(cid:1) (cid:21)(cid:32)(cid:37)(cid:23)(cid:11)(cid:10)(cid:1) (cid:27)(cid:9)(cid:12)(cid:19)(cid:24)(cid:28)(cid:19)(cid:29)(cid:35)(cid:1) (cid:27)(cid:9)(cid:12)(cid:19)(cid:34)(cid:1) (cid:27)(cid:9)(cid:35)(cid:29)(cid:27)(cid:9)(cid:11)(cid:13)(cid:1) (cid:28)(cid:33)(cid:9)(cid:25)(cid:30)(cid:24)(cid:23)(cid:28)(cid:1) (cid:33)(cid:9)(cid:29)(cid:13)(cid:27)(cid:37)(cid:23)(cid:28)(cid:26)(cid:32)(cid:9)(cid:27)(cid:13)(cid:12)(cid:1) (cid:5)(cid:13)(cid:24) (cid:22)(cid:13)(cid:9)(cid:23)(cid:1) (cid:3) (cid:27) (cid:19) (cid:30) (cid:9) (cid:11) (cid:1) (cid:21) (cid:8) (cid:24) (cid:27) (cid:11) (cid:35) (cid:3) (cid:12) (cid:1) (cid:21) (cid:13) (cid:1) (cid:15) (cid:4) (cid:19) (cid:13) (cid:27) (cid:23) (cid:13) (cid:11) (cid:13) (cid:1) (cid:1) (cid:6)(cid:32)(cid:23)(cid:9)(cid:18)(cid:13)(cid:9)(cid:12)(cid:1) (cid:2)(cid:9)(cid:28)(cid:13)(cid:21)(cid:19)(cid:23)(cid:13)(cid:40)(cid:41)(cid:44)(cid:37)(cid:7)(cid:13)(cid:21)(cid:13)(cid:11)(cid:29)(cid:1) Figure 8: Average time diﬀerence between arrival of critical word and corresponding cache block eled by packets increase, packets stay in the network longer and have a higher chance of causing contention in the Runahead network. For ﬀt, the source node arrival rate is very even across all nodes; we do not see any particular node suﬀering from low arrival rate. On the other hand, blackscholes exhibits low arrival rate for some nodes due to small variations in packet destination. Packets are destined to only a few nodes, creating congestion around these nodes, which causes the Runahead network to drop more packets. However, the difference in arrival rates is modest, leading us to believe that the unfairness of our arbitration scheme does not have a negative impact on application performance. 4.4 Power and Area In this section, we evaluate power and area consumption of the Runahead network. We ﬁrst evaluate the 8×8 network power and area usage using DSENT. Next, we evaluate the power and area of a single router using RTL modeling. Critical Path: DSENT reports a minimum clock period for the Runahead network of 481.073 ps when optimized for a frequency of 2GHz. As a result, we are conﬁdent that a Runahead network router can be traversed in a single cycle in a 2GHz system. The critical path reported by Synopsys in 65nm for a single regular router is 1.5 ns. However, the RTL results show that the critical path of the Runahead router does not increase compared to the regular lossless router. This ensures that the combined design can still operate in the origApplication barnes blackscholes bodytrack cholesky facesim ﬀt ﬂuidanimate lu cb lu ncb radiosity radix raytrace swaptions water nsquared Average Packet Arrival Rate Hop Count 96.29% 3.44905 95.63% 3.61983 95.78% 3.41479 98.43% 3.77890 97.66% 4.10944 97.12% 4.43595 96.94% 3.57521 97.93% 3.98956 98.17% 3.56591 97.54% 3.52332 97.33% 4.41145 96.51% 3.48362 98.69% 3.51006 97.18% 3.45227 97.23% 3.7371 Table 4: Packet arrival rate and hop count (cid:5)(cid:1) (cid:7)(cid:1) (cid:9)(cid:1) (cid:11)(cid:1) (cid:5)(cid:1) (cid:5)(cid:2)(cid:7)(cid:1) (cid:5)(cid:2)(cid:9)(cid:1) (cid:5)(cid:2)(cid:11)(cid:1) (cid:5)(cid:2)(cid:13)(cid:1) (cid:6)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1) (cid:7)(cid:1) (cid:8)(cid:1) (cid:9)(cid:1) (cid:4)(cid:1) (cid:10)(cid:1) (cid:11)(cid:1) (cid:12)(cid:1) (cid:5)(cid:1) (cid:2) (cid:10) (cid:10) (cid:8) (cid:6) (cid:12) (cid:1) (cid:9) (cid:6) (cid:3) (cid:11) (cid:7) (cid:1) (a) blackscholes (cid:5)(cid:1) (cid:7)(cid:1) (cid:9)(cid:1) (cid:11)(cid:1) (cid:5)(cid:2)(cid:12)(cid:1) (cid:5)(cid:2)(cid:11)(cid:1) (cid:5)(cid:2)(cid:10)(cid:1) (cid:5)(cid:2)(cid:13)(cid:1) (cid:5)(cid:2)(cid:14)(cid:1) (cid:6)(cid:1) (cid:6)(cid:1) (cid:7)(cid:1) (cid:8)(cid:1) (cid:9)(cid:1) (cid:10)(cid:1) (cid:4)(cid:1) (cid:11)(cid:1) (cid:12)(cid:1) (cid:13)(cid:1) (cid:5)(cid:1) (cid:2) (cid:10) (cid:10) (cid:8) (cid:6) (cid:12) (cid:1) (cid:9) (cid:6) (cid:3) (cid:11) (cid:7) (cid:1) (b) ﬀt Figure 9: Source node arrival rate inal clock speed. To ensure that the Runahead router can forward packets in one cycle, we measure the critical path of the Runahead router alone. The Synopsys design compiler reports a critical path of 800 ps for the Runahead network logic alone, after timing optimizations. The Runahead network can operate almost twice as fast as the regular router. As a result, we are conﬁdent that the Runahead network can function correctly in a single router cycle. Area: The Runahead network incurs 16% active silicon area overhead compared to the 64-bit baseline achieves 1.73× area savings compared to the 128-bit NoC (i.e., the Runahead network as an accelerator) and baseline NoC (i.e., the Runahead network as a powersaver), as shown in Figure 10a. However, the Runahead NoC has the highest usage of global wire area. This is due to the wider channels used to carry additional metadata, as discussed in Section 3. Table 6 shows area consumption of the Runahead 341 (cid:1) (cid:28) (cid:34) (cid:29) (cid:16) (cid:16) (cid:27) (cid:1) (cid:8) (cid:12) (cid:19) (cid:2) (cid:33)(cid:35)(cid:26)(cid:32)(cid:32)(cid:1) (cid:33)(cid:34)(cid:26)(cid:32)(cid:32)(cid:1) (cid:33)(cid:32)(cid:26)(cid:32)(cid:32)(cid:1) (cid:37)(cid:26)(cid:32)(cid:32)(cid:1) (cid:36)(cid:26)(cid:32)(cid:32)(cid:1) (cid:35)(cid:26)(cid:32)(cid:32)(cid:1) (cid:34)(cid:26)(cid:32)(cid:32)(cid:1) (cid:32)(cid:26)(cid:32)(cid:32)(cid:1) (cid:1) (cid:19) (cid:10) (cid:21) (cid:17) (cid:5) (cid:9) (cid:10) (cid:1) (cid:23) (cid:12) (cid:14) (cid:7) (cid:15) (cid:19) (cid:17) (cid:4) (cid:27)(cid:24)(cid:28)(cid:25)(cid:1) (cid:27)(cid:24)(cid:25)(cid:25)(cid:1) (cid:26)(cid:24)(cid:28)(cid:25)(cid:1) (cid:26)(cid:24)(cid:25)(cid:25)(cid:1) (cid:25)(cid:24)(cid:28)(cid:25)(cid:1) (cid:25)(cid:24)(cid:25)(cid:25)(cid:1) (cid:4)(cid:15)(cid:18)(cid:9)(cid:8)(cid:15)(cid:1)(cid:7)(cid:14)(cid:19)(cid:12)(cid:1)(cid:2)(cid:19)(cid:12)(cid:8)(cid:1) (cid:2)(cid:10)(cid:21)(cid:24)(cid:12)(cid:1)(cid:6)(cid:14)(cid:15)(cid:14)(cid:10)(cid:18)(cid:17)(cid:1)(cid:2)(cid:19)(cid:12)(cid:8)(cid:1) (a) Area comparison (cid:2)(cid:5)(cid:15)(cid:8)(cid:11)(cid:10)(cid:13)(cid:8)(cid:23)(cid:24)(cid:28)(cid:19)(cid:3)(cid:5)(cid:13)(cid:7)(cid:14)(cid:12)(cid:1) (cid:2)(cid:5)(cid:15)(cid:8)(cid:11)(cid:10)(cid:13)(cid:8)(cid:23)(cid:24)(cid:28)(cid:19)(cid:4)(cid:8)(cid:11)(cid:8)(cid:6)(cid:16)(cid:1) (cid:3)(cid:17)(cid:13)(cid:5)(cid:9)(cid:8)(cid:5)(cid:7)(cid:1) (cid:2)(cid:5)(cid:15)(cid:8)(cid:11)(cid:10)(cid:13)(cid:8)(cid:27)(cid:25)(cid:1) (cid:24)(cid:18)(cid:22)(cid:22)(cid:1) (cid:24)(cid:18)(cid:22)(cid:22)(cid:1) (cid:23)(cid:18)(cid:29)(cid:28)(cid:1) (cid:23)(cid:18)(cid:29)(cid:28)(cid:1) (cid:23)(cid:18)(cid:22)(cid:26)(cid:1) (cid:23)(cid:18)(cid:22)(cid:28)(cid:1) (cid:23)(cid:18)(cid:23)(cid:23)(cid:1) (cid:23)(cid:1) (cid:23)(cid:18)(cid:23)(cid:22)(cid:1) (cid:23)(cid:1) (cid:23)(cid:18)(cid:23)(cid:22)(cid:1) (cid:23)(cid:1) (cid:2)(cid:22)(cid:16)(cid:7)(cid:15)(cid:12)(cid:8)(cid:1)(cid:18)(cid:17)(cid:21)(cid:10)(cid:19)(cid:1) (cid:3)(cid:10)(cid:7)(cid:13)(cid:7)(cid:11)(cid:10)(cid:1)(cid:18)(cid:17)(cid:21)(cid:10)(cid:19)(cid:1) (cid:6)(cid:17)(cid:20)(cid:7)(cid:14)(cid:1)(cid:18)(cid:17)(cid:21)(cid:10)(cid:19)(cid:1) (b) Power comparison Figure 10: Power and area comparison Conﬁguration Area(um2) Diﬀerence in Area Baseline64 218777.398 1 Baseline128 437554.796 2 Runahead 225427.679 1.030 Table 5: RTL router area comparison network alone as well as total area consumption when added on top of the regular lossless 64-bit baseline. The Runahead subnetwork only accounts for 13.67% of the active silicon area of the total NoC. The Runahead network uses more than half of the global wire area because it has wider physical channels as discussed previously. The details of the RTL area comparison for a single router can be found in Table 5, for the Baseline64, Baseline128 and Runahead conﬁgurations (Section 3). Links are not included in this evaluation. As an accelerator, there is only a 3.2% increase in area usage. This slight increase consists of the additional multiplexers and registers in the Runahead router. As a power-saver, Runahead decreases area usage by almost 1.94×. This is due to the fact that Runahead eﬀectively replaces network resources in the 128-bit baseline NoC with our lightweight design. Power: Figure 10b shows dynamic and leakage power normalized to Baseline64, obtained using DSENT. As shown, Runahead signiﬁcantly reduces leakage power compared to the baseline 128-bit NoCs (i.e., the Runahead network as a power-saver) and incurs only a 10% overhead in leakage power compared to the baseline 64-bit NoC (i.e., the Runahead network as an accelerator). When measuring dynamic power, we use the average injection rate obtained in our Syn(cid:3)(cid:8)(cid:20)(cid:12)(cid:15)(cid:14)(cid:17)(cid:12)(cid:36)(cid:35)(cid:1) (cid:5)(cid:23)(cid:17)(cid:8)(cid:13)(cid:12)(cid:8)(cid:11)(cid:1) (cid:3)(cid:8)(cid:20)(cid:12)(cid:15)(cid:15)(cid:14)(cid:17)(cid:12)(cid:33)(cid:34)(cid:37)(cid:1) Conﬁguration Baseline64 Baseline128 Runahead Power(mW) Diﬀerence in Power 50.5851 1 101.1702 2 52.3573 1.035 Table 7: RTL router power comparison Full simulations across all benchmarks. In general, the Runahead NoC consumes more dynamic power than the other network setups due to packet duplication in the Runahead network. Unlike in a multi-NoC design (such as the Baseline128 conﬁgurations) where the injection rates of each subnetwork is lower, Runahead has an injection rate that is no less than that of Baseline64, incurring additional switching in the Runahead network to carry duplicate latency-sensitive packets. However, in our measurements, dynamic power accounts for only a small portion of total power, ranging from 1.1% for the 128-bit baselines to 2.09% for Baseline64 and Runahead. Leakage power tends to dominate total NoC duces total power usage by 1.81× when used as a powerpower consumption. Overall, the Runahead NoC resaver and incurs only a 10% total power overhead when used as an accelerator. As shown in Table 6, the Runahead subnetwork only accounts for 9.13% of total power usage in the combined network (i.e., Runahead subnetwork on top of the 64-bit baseline). As discussed, much of this is attributed to leakage power due to the wider channels and larger multiplexers to accommodate additional metadata in the Runahead network. Fortunately, the Runahead network generally accelerates application performance, which naturally leads to additional overall energy savings due to shorter runtimes. RTL results for the individual routers in TSMC 65nm are listed in Table 7. Compared to the baseline 64-bit lossless router, the Runahead router only uses 3.5% additional power as an accelerator. However, as a powersaver, power consumption is decreased by 1.93× compared to the routers of the 128-bit baseline networks. This is expected as the Runahead network eﬀectively replaces the routers in the multi-NoC baselines with our proposed lightweight router microarchitecture. 5. RELATED WORK In this section, we explore related work in multiNoCs, buﬀerless NoCs, low-latency designs and critical word optimizations. Multi-NoC Designs. Employing multiple NoCs can improve performance while simultaneously improving area and power eﬃciency [4]. Flit-reservation ﬂow control [39] uses a separate network for reservation messages; these messages reserve buﬀers and channels for the exact time a data packet will use them. Doing so speeds up message handling, improving performance. Deja Vu switching [1] proposes a two-network design: one network for control and coherence packets and one for data packets. Both of their NoC planes use conventional NoC routers with VCs. They achieve power savings by slowing down the data plane. Flores et al. [19] 342 Dynamic Power Leakage Power Total Power Global Wire area Active Silicon Area Total Usage Runahead Usage 0.0237 W 0.00398 W 1.11 W 0.0995 W 1.13 W 0.103 W 11.8 mm2 6.55 mm2 1.34 mm2 0.183 mm2 Percentage 16.81% 8.97% 9.13% 55.56% 13.67% Table 6: Power composition of Runahead on top of 64-bit baseline propose two networks for critical and non-critical trafﬁc. They use heterogeneous networks composed of lowlatency wires for critical messages and low-energy wires for non-critical ones. Mishra et al. [36] propose a heterogeneous multi-NoC system where one network has low latency routers, and the other has high bandwidth channels. Multiple networks also provide opportunities for traﬃc partitioning [44] and load balancing [4]. Catnap [16] is an energy-proportional multi-NoC design; rather than separating the types of traﬃc sent to each NoC, networks are turned on and oﬀ to respond to changes in network load. Enright Jerger et al. [17] propose a hybrid NoC design where a separate NoC exists on a silicon interposer. In their design, the interposer NoC carries memory traﬃc while the NoC on the chip carries the rest of the traﬃc. Prioritization Schemes in NoCs. Traditional NoCs employ simple arbitration strategies like roundrobin or age-based arbitration for packets. Bolotin et al. [9] propose prioritizing control packets over data packets in the NoC. They see substantial performance improvement when small control packets are prioritized over data packets. Globally Synchronized Frames (GSF) [32] is proposed as a local arbitration, QoSoriented prioritization scheme. GSF provides prioritization mechanisms within the network to ensure each application receives equal amount of network resources. Application-Aware Prioritization Mechanism (STC) [14] is proposed as a prioritization scheme to accelerate network-sensitive applications. STC ranks applications at regular intervals based on their network intensity. Aergia [15] uses the notion of slack to prioritize packets. Aergia may increase network throughput if network is congested. However, from our evaluations, we see little performance impact because of the absence of contention in our simulations. Prioritization schemes can best show their full potential when the network carries heavy traﬃc. On the other hand, the Runahead network performs well with a lack of contention. Buﬀerless NoC Designs. Buﬀerless networks have received signiﬁcant research attention [18, 23, 35, 37]. In BLESS [37] and CHIPPER [18], packets are deﬂected until they reach their destination. In SCARAB [23] and Millberg et al. [35], packets are dropped upon contention and a retransmission message is issued to the source. Our Runahead network does not react to dropped packets and does not deﬂect packets in the face of contention. This keeps the design of the Runahead network routers simple. Low-Latency NoC Designs. The goal of our network design is to accelerate packet transmission. Similarly, there has been signiﬁcant research on low-latency NoCs to improve performance. Express virtual channels [31] reduce latency by allowing packets to bypass intermediate routers. A non-speculative single-cycle router pipeline improves performance by allocating the switch in advance of the message arrival [30]. A lowcost router design [29] reduces latency using a simple ring-stop inspired router architecture for fast traversal of packets traveling in one direction; packets changing direction pay additional latency when they are buﬀered. Lookahead routing [20] is another common technique to reduce the number of pipeline stages in the router. Route predictions can also speed up the network [24,34]. Often these low-latency designs increase complexity, energy and area in order to achieve better performance. SMART [13] is proposed to reduce overall communication latency by allowing packets to travel multiple hops in a single cycle. They observed that the wire delay is much shorter then a typical router cycle. The links in SMART require specialized repeaters to enable multi-hop traversal. Our simple Runahead network achieves performance improvements with minimal power and area overhead. Critical Word Optimizations. Delivering the critical word as soon as possible can improve application performance. Separating critical data in either main memory [12] or the caches [27] can eﬃciently deliver critical data faster. NoCNoC [40] proposes a two network design that separates critical words from noncritical ones in a cache line. It saves power by DVFS for the non-critical network. 6. CONCLUSION In this paper, we propose the Runahead NoC, which can serve as either a power-saver for more eﬃcient use of network resources, or as an accelerator that provides lightweight, low latency communication on top of a conventional NoC. The Runahead NoC is designed to provide single-cycle hops across the network. To accomplish this, the network is lossy in nature, dropping packets when contention occurs. We present the design of the Runahead NoC router architecture that combines route computation and port arbitration with link traversal. From experiments with SynFull workloads, we ﬁnd that the Runahead network can maintain over 97% packet arrival rate on average. As an acceland packet latency by 1.08× and 1.66× with only 10% erator, the Runahead network reduces average runtime overhead. As a power-saver, Runahead achieves 1.73× and 1.81× savings in active area and power respectively. 343 Acknowledgements The authors thank the anonymous reviewers for their insightful feedback. This work is supported by a Queen Elizabeth II Scholarship in Science and Technology, the Natural Sciences and Engineering Research Council of Canada, the Canadian Foundation for Innovation, the Ministry of Research and Innovation Early Researcher Award and the University of Toronto. 7. "
2016,DVFS for NoCs in CMPs - A thread voting approach.,"As the core count grows rapidly, dynamic voltage/frequency scaling (DVFS) in networks-on-chip (NoCs) becomes critical in optimizing energy efficacy in chip multiprocessors (CMPs). Previously proposed techniques often exploit inherent network-level metrics to do so. However, such network metrics may contradictorily reflect application's performance need, leading to power over/under provisioning. We propose a novel on-chip DVFS technique for NoCs that is able to adjust per-region V/F level according to voted V/F levels of communicating threads. Each region is composed of a few adjacent routers sharing the same V/F level. With a voting-based approach, threads seek to influence the DVFS decisions independently by voting for a preferred V/F level that best suits their own performance interest according to their runtime profiled message generation rate and data sharing characteristics. The vote expressed in a few bits is then carried in the packet header and spread to the routers on the packet route. The final DVFS decision is made democratically by a region DVFS controller based on the majority election result of collected votes from all active threads. To achieve scalable V/F adjustment, each region works independently, and the voting-based V/F tuning forms a distributed decision making process. We evaluate our technique with detailed simulations of a 64-core CMP running a variety of multi-threaded PARSEC benchmarks. Compared with a network without DVFS and a network metric (router buffer occupancy) based approach, experimental results show that our voting based DVFS mechanism improves the network energy efficacy measured in MPPJ (million packets per joule) by about 17.9% and 9.7% on average, respectively, and the system energy efficacy measured in MIPJ (million instructions per joule) by about 26.3% and 17.1% on average, respectively.","DVFS for NoCs in CMPs: A Thread Voting Approach Yuan Yao and Zhonghai Lu KTH Royal Institute of Technology, Stockholm, Sweden {yuanyao, zhonghai}@kth.se ABSTRACT As the core count grows rapidly, dynamic voltage/frequency scaling (DVFS) in networks-on-chip (NoCs) becomes critical in optimizing energy efﬁcacy in chip multiprocessors (CMPs). Previously proposed techniques often exploit inherent network-level metrics to do so. However, such network metrics may contradictorily reﬂect application’s performance need, leading to power over/under provisioning. We propose a novel on-chip DVFS technique for NoCs that is able to adjust per-region V/F level according to voted V/F levels of communicating threads. Each region is composed of a few adjacent routers sharing the same V/F level. With a voting-based approach, threads seek to inﬂuence the DVFS decisions independently by voting for a preferred V/F level that best suits their own performance interest according to their runtime proﬁled message generation rate and data sharing characteristics. The vote expressed in a few bits is then carried in the packet header and spread to the routers on the packet route. The ﬁnal DVFS decision is made democratically by a region DVFS controller based on the majority election result of collected votes from all active threads. To achieve scalable V/F adjustment, each region works independently, and the voting-based V/F tuning forms a distributed decision making process. We evaluate our technique with detailed simulations of a 64-core CMP running a variety of multi-threaded PARSEC benchmarks. Compared with a network without DVFS and a network metric (router buffer occupancy) based approach, experimental results show that our voting based DVFS mechanism improves the network energy efﬁcacy measured in MPPJ (million packets per joule) by about 17.9% and 9.7% on average, respectively, and the system energy efﬁcacy measured in MIPJ (million instructions per joule) by about 26.3% and 17.1% on average, respectively. 1. INTRODUCTION Network-on-Chip (NoC) is becoming the de-facto mainstream interconnect scheme for today’s and future high performance CMPs and MPSoCs. As multi-core computers enjoy consistently performance enhancement with a growing number of cores, energy/power consumption has emerged as the main constraint for current chip design. According to the studies in [7, 11, 17], on-chip networks can consume a substantial fraction (which may potentially amount to 30-40%) of the entire chip power. An important direction towards increasing the energy efﬁcacy of NoC in multi- or many-core systems has focused on Dynamic Voltage/Frequency Scaling (DVFS). A DVFS mechanism is desired to co-optimize system-level performance and power, i.e., to obtain as much as performance by spending as less power as possible. Conventionally, DVFS mechanisms in NoC have been designed [9, 23, 3, 20] to utilize network-level performance metrics, such as network latency, router buffer load or network throughput. These metrics capture inherent performance characteristics of the network itself, but are often insufﬁcient to reﬂect applicationlevel or system-level performance. Speciﬁcally, network delay may not be indicative of network-related stall-time at the processing core [10]. This is because much of the packets’ latency can be hidden by various latency-hiding techniques such as read/write buffers. The router buffer load might not accurately reﬂect the data criticality to system performance because it has no knowledge of data dependency among different threads. Given that the buffer-load is low, it may not always be proper to lower the on-chip voltage/frequency (V/F) level since the completion time of one request may affect the progress of other data-sharer cores. Relying on network throughput might not be effective in V/F tuning since CMPs are self-throttling: a core cannot inject new requests into the network once it ﬁlls up all of its miss request buffers. A low throughput can either indicate an under-saturated network or an over-saturated network when the request buffers of cores are fully ﬁlled. Because of the above reasons, NoC DVFS mechanisms relying merely on network-level metrics may improperly adjust on-chip V/F levels, leading to overand under-provisioning of power and thus inferior energy efﬁcacy. In the paper, we propose a novel on-chip DVFS strategy that is able to adjust network V/F levels according to a thread’s direct performance indicatives in terms of message generation rate and data sharing metrics. Like the conventional network metric based approaches, the injection rate allows to capture a thread’s communication need. The data sharing characteristics in a cache-coherent multicore enable to take data criticality and dependency into consideration. In this way, our strategy can effectively overcome the power overand under-provisioning problems whenever present. In order to achieve a light-weight solution, our technique allows a thread to vote a preferred V/F level according to its own performance indicatives, and the V/F votes (a few bits) are carried by packets and spread into the routers. A region978-1-4673-9211-2/16/$31.00 c(cid:2)2016 IEEE 1 1 309 based V/F controller collects all votes of passing packets and makes a democratic decision on the ﬁnal V/F level for its region composed of a few routers. As such, we call our technique a thread voting approach, which means a thread’s active participation in inﬂuencing the V/F decisions, in sharp contrast to existing thread data-sharing oblivious V/F adjustment techniques. Under a multi-threaded programming environment, we evaluate our DVFS mechanism with cycle-accurate full-system simulations in Gem5 [5] using PARSEC benchmarks [4]. We further exploit McPAT [18] for reporting power related results. In the benchmark experiments, we expose the power over- and under-provisioning problem of a purely networkmetric based DVFS mechanism and show the problem’s sensitivity to a program’s network utilization and data-sharing characteristics. Furthermore, we observe our approach’s great potential in leveraging network energy efﬁcacy (up to 21%) and system energy efﬁcacy (up to 35%) in the experiments. The rest of the paper is organized as follows. Section 2 illustrates power over/under provisioning in a network-metric based DVFS approach. Section 3 presents the overview and principles of our thread voting based DVFS technique. In Section 4 we detail the thread communication characterization with vote generation and propagation, as well as the vote counter organization and region DVFS controller for region V/F adjustment. In Section 5 we report experiments and results, and ﬁnally we conclude in Section 7. 2. MOTIVATION We illustrate the power over/under provisioning problems observable in network-metric based DVFS mechanisms. To be speciﬁc, we exemplify the phenomenons using a router buffer load based DVFS technique. In this section, we also motivate the selection of region-based DVFS. 2.1 Target CMP architecture R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI A core+L1 L2 Mem. Ctrl. 8 Columns 8 R o w s 8 7 2 1 (a) Hotspot trafﬁc R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI R NI 8 Columns 8 R o w s 8 7 2 1 H G B A (b) Dispersed trafﬁc Figure 1: The target CMP architecture (NI: Network Interface and R: Router) We ﬁrst introduce the target CMP architecture we exploit through all experiments in the paper. Figure 1 shows a typical architecture for a 64-core CMP, where the NoC interconnects processor nodes (a CPU core with its private L1 cache), secondary on-chip shared L2 cache banks, and onchip memory controllers together. Routers are organized in a popular mesh topology on which the XY dimensional routing algorithm is implemented to achieve simplicity and deadlock-free resource allocation. There are 8 memory controllers, which are connected to the central 4 nodes of the ﬁrst and last row for architectural symmetry. In the ﬁgure we only illustrate 1 memory controller in the ﬁrst and last row due to space limitation. For maintaining data consistency, a directory based MOESI cache coherence protocol is implemented. On the target CMP, once a core issues a memory request, ﬁrst the local L1/L2 is checked for the presence of the requested data. If this fails, the request is forwarded to the corresponding remote L2/DRAM via the NoC. 2.2 Power over/under provisioning scenarios Power over provisioning. We deﬁne power over provisioning as the circumstance in which the on-chip V/F level is highly increased but the application-level performance improvement is marginal. As illustrated in Figure 1a, assume that core 1, 2,..., 7, 8 periodically issues write requests to different data blocks within the same L2 cache bank A (such gather trafﬁc pattern frequently occurs in multiplication of different partitions of a matrix). Assume further that the write requests are all exclusive and have no data sharer threads. In this case, the issuing cores can hide the write miss delay with write-buffers, and the return time of the request can be relaxed. Under this circumstance, although the trafﬁc pattern forms a hot-spot in the highlighted router column, it may not always be appropriate to increase the routers’ V/F level once their buffer load goes high, since the core can tolerate part of the network delay without loss of performance. However, in buffer load based DVFS, the frequency of routers in the highlighted column will be highly increased due to the hotspot trafﬁc but the performance improvement can be very limited. Power under provisioning. We deﬁne power under provisioning as the circumstance in which the on-chip V/F is slightly decreased but the application-level performance degradation is very notable. As illustrated in Figure 1b, assume that core 1, 2,..., 7, 8 periodically issues write requests to L2 cache banks A, B,..., G, H, respectively. Assume further that each of the issuing thread has multiple data-sharer threads (such trafﬁc frequently occurs in for example multiple threads competing to lock on shared variables). In order to maintain memory consistency, successive requests issued by the data-sharer threads to the same data address are prohibited from returning the newly written value until all duplicated cache copies have acknowledged the receipt of the invalidation or update messages. Under this circumstance, even though the trafﬁc pattern is dispersed and each passing router may have light buffer load, it may not be proper to lower the router’s V/F level, since the completion time of the ongoing write request affects both the issuing and datasharer threads’ progress. In router buffer load based DVFS, the frequency of the passing routers in Figure 1b will be improperly decreased due to low buffer load, which may sacriﬁce considerable application performance for limited power saving gains. 2.3 DVFS tuning granularity The NoC DVFS control can be implemented in differ2 2 310 ent granularity, such as in the entire NoC chip [9, 23], in coarse-grained V/F island [3, 13], or in ﬁne-grained perrouter micro-architecture [20]. Although the chip-wide approach offers small inter-domain interfacing overhead and is often cheaper to implement, the recent trend towards CMPs executing multi-threaded workloads motivates the need for ﬁner-grained DVFS mechanism [12, 20]. However, having each router in a separate voltage domain requires bridging of such domains on every single link, which will introduce 2-3 cycles of additional delay for each link [12]. Facing this dilemma, this paper tends to seek a hybrid solution to balance hardware complexity and DVFS tuning effectiveness: region based DVFS tuning, where several adjacent routers on the chip forms a DVFS region and share the same V/F level. Being different from the V/F island approach where V/F islands are formed dynamically over time, our approach adopts predeﬁned static V/F regions for lower hardware complexity. 3. THREAD VOTING BASED DVFS MECHANISM 3.1 Design overview As the motivational examples show, adjusting DVFS merely according to network metrics may lead to power over/under provisioning. What is equally important is to take datasharing situations into consideration. With this in mind, we develop a DVFS mechanism exploiting both network utilization and data-sharing characteristics according to a program’s runtime behavior. On the way to seek for a scalable solution, we develop a thread voting approach which, in contrast to thread-passive V/F turning, allows a thread to actively vote a V/F level to suit its own performance interest, and then a region-based DVFS controller makes V/F decision by the collected votes of involved threads.   Steps: Communication Characterization V/F Vote Generation & Propagation  Regional DVFS Decision Figure 2: The three-step DVFS tuning mechanism Technically, our voting based on-chip DVFS approach requires solving three essential problems: 1) How to capture a thread’s communication characteristics reﬂecting both network utilization and data sharing? 2) How to generate and propagate the V/F vote into the NoC efﬁciently? and 3) How to make an effective DVFS decision according to the votes received? Figure 2 shows our technique which comprises three steps, with each step addressing one of the problems. • Step 1 Communication characterization conducts a step window based thread communication demand characterization. Since per-thread communication characteristics are reﬂected by a thread’s own message generation rate as well as its data sharing relations with other threads, we suggest a (γ , s) model where γ represents message generation rate and s level of data sharing. • Step 2 V/F vote generation and propagation performs vote generating according to the proﬁled (γ , s) values. We treat votes from different threads with the same weight. In order to achieve a fair voting scheme, the total amount of votes a thread generates should be proportional to its network utilization level. Under this consideration, we use network packets to carry and propagate the votes into the network routers. The more packets a thread injects, the more votes the thread spreads and thus the more inﬂuence the thread has on the V/F election results. • Step 3 Region DVFS decision making: each V/F region makes its own DVFS decision in favor of the majority based on the collection of received votes. 3.2 Per-thread communication need characterization using the (γ , s) model The (γ , s) model. In the target CMP described in Figure 1, one key challenge lies in that threads running on core processors do not explicitly specify their on-chip communication demand. To adequately capture per-thread communication demand on the NoC, we deﬁne a (γ , s) model which can indicate not only a thread’s own network utilization but also its data sharing relations with other threads. Speciﬁcally, γ is the message generation rate capturing the average number of outstanding L1/L2 misses in miss status holding registers (MSHRs) per epoch, and s is the average number of data sharers per data request per epoch as a thread’s communication characteristic parameters. Because a core’s L1/L2 misses must be encapsulated into packets and forwarded to the corresponding L2 or DRAM via the network, γ is indicative of the average network access intensity of the thread. Additionally, s reﬂects the criticality of a thread’s data request with respect to other data-sharer threads. The more data sharers a thread has, the more critical the thread’s data requests likely become, since the requests’ ﬁnish time may inﬂuence the progress of the data-sharing threads. (γ , s) values for each core at runtime. To obtain the γ value, we develop a communication characterizer within network interface (NI) to monitor the network requests from a thread, as shown in Figure 3a. Note that, since γ is deﬁned to proﬁle a thread’s utilization of the NoC, only those cache behaviors that incur network messages are counted. Second, we use s to reﬂect the degree of a thread’s data sharing status through characterizing the average number of data copies per epoch each thread has. Given a directory based cache coherence protocol, characterizing on s is done by checking the number of data sharers in the directory on outgoing network requests. Note that in order to allow continuously characterizing (γ , s) with a proper time granularity, we adopt a step-window based data sampling approach, where the (γ , s) values are computed on a per time interval basis. To ensure characterization continuity, adjacent windows may be overlapped. Communication characterization. We proﬁle per-thread 3.3 Vote generation and delivery As shown in Figure 3a, in Step 2 each thread votes for a preferred V/F level according to its proﬁled communication needs and then delivers its votes into the network for region V/F adjustment. 3 3 311 node 1 T1  R1 , ch ar. node 2 T2  R2 ch ar. node 3 T3 R3 ch ar.  R4 c h a r. node 4 T4 packet route vote delivery V/F setting Addr Vote Payload packet header ﬂit Region DVFS Controller V/F Set Dir. 1 (a) Step 1 and 2 Region Region DVFS DVFS Controller Controller Controller   V/F Set V/F Set Router 2 Router 2 Router 4 Router 4 Router 3 Router 3 Router 1 Router 1 Region Region DVFS DVFS Controller Controller Controller   V/F Set V/F Set Router 2 Router 2 Router 4 Router 4 Router 3 Router 3 Router 1 Router 1 A s n y c h r u o n o s i n r e t f e c a A s n y c h r u o n o s i n r e t f e c a A s n y c h r u o n o s i n r e t f e c a Asynchronous interface Asynchronous interface Asynchronous interface Asynchronous interface (b) Step 3 Figure 3: Overview of the thread voting based DVFS mechanism. (a) Steps 1 and 2 conduct per-thread communication demand characterization and vote generation/delivery. (b) Step 3 sketches the region based DVFS decision making. Table 1: Mapping table from (γ , s) to a V/F vote γ high mid low read Conf.A Conf.A Conf.A Conf.B Conf.B Conf.A Conf.B Conf.B Conf.C Conf.B Conf.B Conf.C write s high mid low Vote generation. After characterizing the (γ , s) values, a thread votes for a preferred V/F level by mapping the obtained (γ , s) values to a proper DVFS conﬁguration vote. For generality, we adopt a look-up table based approach for the mapping function, which is shown in Table 1. We divide the whole γ and s value span into, e.g., three intervals: high, middle, and low. We also pre-deﬁne, e.g. three V/F levels per region, which are denoted as Configuration A, B, and C, where Conf.A operates routers in a V/F region at maximum V/F, Conf.B 87.5% of maximum V/F, and Conf.C 75% of maximum V/F. As shown in the table, the combination of γ and s uniquely indexes a V/F conﬁguration and thus a vote. For a fair voting scheme, the total amount of votes a thread generates should be proportional to its network utilization level. In general, one V/F vote can be generated on every nth read/write message from a thread. Note that, as shown in the table, since read accesses to shared data blocks do not affect data-sharing threads’ progress, only γ is considered during the vote mapping on a read request. The intuition of the mapping rules in Table 1 is directed towards avoiding both power over and under provisioning. A proper V/F conﬁguration is decided considering not only trafﬁc metric γ but also data-sharing status s. Light-weight vote delivery. The per-thread generated V/F votes have to be spread into the network in order to guide the region V/F adjustment. Although a centralized vote delivery approach is conceptually simple to design, it has limited scalability, i.e., the design overhead cannot be paid off when the system size enlarges to a certain degree. To amend this problem, we propose a distributed light-weight vote-delivery approach as follows. We treat the vote as payload, part of the head ﬂit of a network packet. As shown in Figure 3a, speciﬁc bits in the header-ﬂit are added for the vote. The number of the vote-bits depends on the number of supported V/F conﬁgurations. More added bits means ﬁner V/F adjustment granularity, but also implies higher design complexity. With three candidate V/F conﬁguration levels, we use 2 bits in the paper. We further illustrate the vote delivery progress in Figure 3a, where thread T 1 spreads its votes to router R1, R2, R4 along the packet route. 3.4 Region DVFS decision making In our mechanism, each region realizes the DVFS adjustment process in two phases: a vote receiving phase and a DVFS decision making phase. During the vote receiving phase, as packets are continuously routed through routers in a V/F region, the passing routers record the carried votes upon receiving the packet headers, and re-direct the votes to the Region DVFS Controller (RDC) for vote counting. When the vote receiving phase ends, the RDC begins the DVFS decision making phase, where the ﬁnal DVFS adjustment decision is made in favor of the majority based on the number of votes received. The ﬁnal DVFS decision is then input to the V/F setting logic for V/F level tuning. In Step 3, the RDC consolidates the ﬁnal DVFS decision from the collective statistics of the received votes. We maintain three counters, one for each V/F level, within each RDC to count the number of votes each candidate V/F level receives. In the DVFS decision making phase, the RDC compares the vote counters and sets the region V/F to the level denoted by the maximum counter, which represents the most elected V/F level by passing packets. Figure 3b shows Step 3, where two adjacent regions (each contains a 2×2 router mesh) are conﬁgured to two different V/F levels. Communication through different V/F regions are bridged by an asynchronous communication interface (ACI), which brings 2 cycles overhead. In an 8 × 8 mesh NoC with X-Y routing, the worst-case delay caused by the ACI is limited to at most 12 cycles since the longest path (network diameter) of the NoC consists of 7 different V/F regions. In Section 4.1 and 4.2, we will introduce the design details of the thread communication demand characterization and the region-based DVFS adjustment, respectively. 4. DESIGN DETAILS 4.1 Characterization and Vote Generation Our proposed DVFS mechanism relies on thread network access demand characterization for V/F vote generation and delivery. Figure 4 shows the micro-architecture of a modi4 4 312 c1 or c2 increases by 1, respectively. At the end of each sampling window, γL1 is computed for L1 cache miss rate by c1 /Ts , γL2 for L2 cache miss rate by c2 /Ts , and γ for total cache miss rate by γ = γL1 + γL2 . To continuously characterize γ , the sampling window slides forward with a step length Tp after each characterization. Then the above procedure repeats through the system execution. Characterization of s. The s proﬁling involves ﬁnding a thread’s data sharing density, i.e., the average number of data sharers. This is done by performing a data-sharer number check on critical message mc during a sampling window Ts , where the issuing time t (mc ) of mc satisﬁes t (mc ) ∈ [0, Ts ]. A message is considered critical if it is a message accessing to a shared data block and its delay may affect the progress of other data-sharing threads. Such message can either be a cache miss or hit message, since in the write hit case the issuing thread must inform other data-sharing threads about the modiﬁcation of a shared data block, and those data-sharing threads must wait for the completion of the write operation for memory consistency maintenance. To proﬁle s, we use a counter, namely cs , to incrementally count the number of data sharers from each critical message. At the start of a sampling window, cs is reset to 0. As time advances, the critical message check is performed at each message. If a critical message is detected, cs is increased by s(cid:5) , which is the number of data-sharers associated with the critical message. Obtaining s(cid:5) can be achieved by checking the coherence directory in a directory based cache coherence protocol, or by setting the s(cid:5) value to the number of all destinations in a broadcast/multi-cast based protocol. In directorybased cache coherence protocol, the original request issuer is responsible for obtaining the number of sharers for critical messages. This requires that the request issuer fetches the number of data sharers from the directory controller that may exist in another node of the NoC. At the end of the sampling window, s is calculated by s = cs , which denotes the average number of data sharers a thread has during the past sampling window. tion 3.3, after proﬁling the (γ , s) values, a thread generates votes for a preferred DVFS conﬁguration by looking up the Vote Generation Table (VGT, as shown in Table 1 and Figure 4). New V/F votes are generated every nth critical cache read/write or coherence request transactions. For reply transactions, the reply packet follows the same vote of the corresponding request packet. This ensures that both the request and reply packets have consistent impact on the per-region DVFS decision making progress. To control the V/F adjustment sensitivity to (γ , s) values, we divide the γ and s value span into several levels, with each level denoting one candidate V/F conﬁguration. Votes of a different V/F conﬁguration start to generate only when γ or s shifts into a different level. The generated votes then get encapsulated into the packet header by the packet sender and spread into the network. V/F vote generation and delivery. As introduced in SecTs 4.2 Region-based V/F Adjustment We now introduce the design details of the region-based V/F adjustment in two parts. We ﬁrst discuss vote counter organization and then present the architecture of the Region DVFS Controller (RDC). e r o C L1 L1 MSHR mux L2 L2 MSHR demux Core N I R1 R2 Region DVFS Logic I N (γ , s) Message inject Message receive R3 R4 Char. VGT Sender Receiver vote C o N Addr Vote Payload header R Step-window based characterization. To allow continuFigure 4: Modiﬁcations of the network interface, which conduct (1) communication demand characterization and (2) DVFS vote generation. ﬁed network interface where a characterizer, a vote generation table (VGT) and a message sender work cooperatively for this purpose. The characterizer monitors and proﬁles the network access demand of a thread periodically and proﬁles the (γ , s) values. The vote generation table generates V/F votes based on the proﬁled (γ , s) values and the message sender encapsulates the votes into outgoing packets. ously characterize the (γ , s) parameters with a proper time granularity, we adopt a step-window approach for the characterization. We design two windows: 1) the sampling window with length Ts , in which the parameters are sampled and characterized, and 2) the step window with length Tp , which low sampling continuity, Tp ≤ Ts . The step window enables controls the advancing step of the sampling window. To ala ﬂexible characterization scheme because one can adjust Ts and Tp to achieve quality-complexity trade-off. Speciﬁcally, let the ratio λ = Ts /Tp deﬁnes the degree of overlapping among consecutive sampling windows. Indeed, a larger λ brings higher continuity of the consecutively characterized results but with higher computational complexity. To reduce computation complexity, we also set Ts = 2n , where n is a natural. Characterization of γ . To characterize the γ value, we have to examine how data messages are generated from a thread over time. In cache-based CMPs, network data message producing is always indicated by cache read/write misses. In state-of-the-art non-blocking/lock-up free caches, when an L1/L2 cache miss occurs, an access request to the L2/DRAM is created by ﬁrstly allocating an MSHR entry. The higher the number of outstanding misses in MSHRs for a thread, the more frequently the thread’s data exchange occurs and the higher its data communication demand on NoC. Thus, the number of MSHR entries allocated by a thread directly reﬂects the trafﬁc intensity of a thread from the NoC perspective. For implementation efﬁciency, we keep the calculation of γ simple. At each sampling window Ts , two counters, namely c1 and c2 , which are reset to 0 at the beginning of each sampling window, are maintained to record the total number of the MSHR entries for L1/L2 misses, respectively. When a new MSHR entry is allocated due to L1 or L2 miss, 5 5 313 Router 1 C n t 1 vote... Router 3 C n t 3 vote... Router 2 C n t 2 vote... Router 4 C n t 4 vote... RDC Region DVFS Setting V/F V/F V/F V/F (a) Distributed counter organization RDC Cnt. Router 1 Router 3 vote... vote... Router 2 Router 4 vote... vote... Region DVFS Setting V/F V/F V/F V/F (b) Centralized counter organization Figure 5: Distributed vs. centralized counter organization, where RDC refers to region DVFS controller. R1 Micro-architecture Input Unit 1 VC Selection V C0 V Cm M U X D E M U X I u p n t P o t r 1 ... Input Unit n VC Selection V C0 V Cm M U X D E M U X I u p n t P o t r n Route Computation VC Allocation Region DVFS Controller (RDC) Data Path Control Path V/F Setting Region DVFS Setting Logic V/F conﬁg to R1 V / F g ﬁ n o c t o R 2 V/F conﬁg to R3 V / F c o n ﬁ g t o R 4 P O 1 L C V vote vote ... L C V vote vote ... L C V vote vote ... L C V vote vote ... Core R3 R1 R4 R2 Region DVFS Logic N I Vote Counters DVFS Decision Logic F r o R m 2 F r o R m 3 F r o R m 4 C A I P O n Figure 6: Router and region DVFS controller architectures Vote counter organization and conﬁguration. The vote counters within a V/F region record the number of votes each candidate V/F level receives. Figure 5 shows two vote counter organizations: the distributed organization and the centralized organization. In the distributed organization shown in Figure 5a, each router maintains its own vote counters. As packets are routed through the region, each router counts the received votes. At the beginning of the DVFS decision making phase, a 2-stage comparison is performed to ﬁnd the maximum counter value. The ﬁrst-stage comparison is performed locally in each router, then the local maximum values are output to the RDC for the second-stage comparison, based on which the ﬁnal DVFS decision is made. Finally, the region DVFS setting logic adjusts the region V/F level based on the ﬁnal DVFS decision. Although this approach keeps the RDC simple when the V/F region scales up, maintaining vote counters within each router adds however considerable hardware overhead. Figure 5b shows the region-centralized counter organization, where centralized vote counters in the RDC count all the votes received by routers in the region. In the DVFS decision making phase, only a one-stage comparison is needed to ﬁnd the regional maximum counter value. In our current implementation (each V/F region contains a 2 × 2 router mesh), we choose the centralized counter organization as it brings about most hardware implementation efﬁciency with fast counter comparison speed. Region DVFS Controller (RDC) architecture. Figure + + + FF Centralized Counters Counter.H ... + + + FF Counter.M ... + + + FF Counter.L ... port 0 port 1 port 2 port 3 port 4 V o s e t r f o m R u o r e t 3 V D F S D c e i s i n o L g o i c T o V D F S S t t e i g n L g o i c VCL for Router 3 vote receiving disable signal Figure 7: Schematic of the Region DVFS Controller (RDC) 6 sketches the RDC architecture and its connection to one of the routers (Router 3) in region. As the ﬁgure shows, one RDC consists of four vote counting logic (VCL, one for each router), a set of centralized vote counters (one for each candidate V/F level), and a DVFS decision making logic, which work cooperatively to achieve the region based V/F adjustment. The router is a 2-stage speculative router [21] in which the ﬁrst stage performs Route Computation (RC), Virtual Channel (VC) Allocation (VA), and Switch Allocation (SA) in parallel and the second stage is Switch Traversal (ST). In order to operate at different V/F levels, different V/F regions should be able to communicate asynchronously with each other. This is enabled by the asynchronous communication interface (ACI) on the boundary of the V/F region. We utilize the dual clock I/O buffer from [20] for the ACI implementation, which maintains independent read and write clock signals. Figure 7 draws the schematic of the RDC. For space restriction, we only show the VCL for Router 3 and its connection to the three centralized counters with counter.H counting high V/F level votes, counter.M middle V/F level votes, and counter.L low V/F level votes. The DVFS tuning procedure can be divided into two consecutive phases: a vote receiving phase and a DVFS decision making phase. Each router in the V/F region connects to the RDC through a VCL, which acts as the vote receiving interface. During the vote receiving phase, a router passes the received votes for a V/F level through VCL to the counter that counts votes for that V/F candidate. At the DVFS decision making phase, the DVFS decision making logic compares the vote counters and ﬁnds the maximum one. Then the region DVFS setting logic updates the region V/F to the level denoted by the maximum vote counter. Counter A (high V/F) a Counter B (mid V/F)  (ineligible) b Counter C (low V/F) c θ2 θ1 Figure 8: Comparison of vote counters. θ1 denotes absolute threshold, and θ2 denotes relative threshold. To increase the comparison efﬁciency and reduce delay in the DVFS decision phase, we design two thresholds: an 6 6 314 Algorithm 1 V/F decision making 5.1 Methodology Experimental setup. Our DVFS mechanism is evaluated with well accepted simulators and benchmarks. We implement and integrate our design with the cycle-accurate fullsystem simulator GEM5 [5], in which the embedded network GARNET [1] is enhanced with our thread voting based DVFS mechanism. Further, we exploit McPAT [18] as the power measuring tool, in which the default system architecture is tailored to the conﬁguration in GEM5, and the default technology is set to 45 nm. The details of the simulation platform setup are shown in Table 2. In the experiments, we consisting of 4 adjacent routers organized into a 2 × 2 mesh. divide the 64 routers into 16 DVFS regions, with each region To count the additional power/area cost brought by the thread characterizer and the RDC, we synthesis our design with Synopsis Design Compiler (using 45 nm technology) under the three V/F levels in Table 2. The synthesis report shows that 64 characterizers and 16 RDCs consume additional power, which is 13.1 mW in 2.0 GHz, 12.5 mW in 1.75 GHz and 10.8 mW in 1.5 GHz, with 3% (11162.7 standard NAND gates) additional network area. DVFS tuning window conﬁguration. The length of the DVFS tuning window determines for how long a DVFS conﬁguration is valid. In general, the shorter the tuning window is, the faster the DVFS adjustment becomes. However, the total system-level power saving may reduce due to frequent charge and discharge of voltage regulators. In this work, we use a pre-determined static tuning window, which is the same length as the thread behavior characterization window (as shown in Table 2) for lower complexity. PARSEC conﬁguration. To scale the PARSEC benchmarks well to 64 cores, we choose large input sets (simlarge) for all programs. Table 3 summarizes the problem size of the benchmarks. For data validity, we only report results obtained from the parallel execution phase (called as regionof-interest, ROI) in the experiments. Table 3 also shows the characteristics of PARSEC in terms of network utilization and data sharing degree among threads based on the data in [4]. As can be observed, four programs (blackscholes, canneal, ferret and x264) show consistent characteristics (both low or both high) in network utilization and data sharing, while the eight other programs exhibit inconsistent (one low one high or one high one low) characteristics in network utilization and data sharing. Comparison study. For comparison purpose, we implement two baseline cases. The ﬁrst case, which we denote AHV (Always High V/F), does not consider a DVFS mechanism and always operates the NoC at highest V/F level. The second case implements a router Buffer-Load based DVFS mechanism, denoted BLD, in which the NoC allows perregion V/F adaption by monitoring the averaged router buffer load in a region. Just like our Thread Voting based DVFS scheme (denoted TVD), BLD uses the same three (high, middle, low) V/F levels. When a gather trafﬁc pattern (such as the case illustrated in Figure 1a) creates a hop spot in the network, the routers in the corresponding V/F region can adaptively switch to high V/F mode, increasing frequency to accelerate transmitting packets for performance assurance. When a disperse trafﬁc pattern exists and the routers enjoy temporarily low buffer loads, the V/F region adaptively de5: 6: 7: 8: 1: Input: centralized counters array cnt _array 2: Initialization: max_cnt ← 0, max_cnt _record ed ← Fal se 3: Output: Updated region V/F level 4: if at vote receiving phase then if any cnt in cnt _array satisﬁes cnt ≥ (cid:8) N×P×W +θ2 max_cnt ← cnt max_cnt _record ed ← T rue Exit vote receiving progress 9: if at DVFS decision making phase then if max_cnt _record ed (cid:10)= T rue then for each cnt in cnt _array do if cnt > θ1 and max_cnt < cnt − θ2 then max_cnt ← cnt 14: win_count er ← the corresponding counter with value max_cnt 15: Update region V/F to the V/F level represented by win_count er 10: 11: 12: 13: (cid:9) then 2 absolute threshold θ1 and a relative threshold θ2 , as illustrated in Figure 8. The absolute threshold θ1 determines the eligibility of a counter. The intuition is straightforward: if a V/F level is seldom voted, it can be ignored in the ﬁnal DVFS decision making progress. As shown in the ﬁgure, since very few packets vote the V/F level represented by counter B, we denote it as ineligible and only consider counters A and C during the DVFS decision making phase. The relative threshold θ2 ensures that two counters are different only if their difference exceeds θ2 , which avoids comparing two close values and also enables ﬂexible design trade-offs. For example, as shown in Figure 8, counter A (representing high V/F level) and C (representing low V/F level) satisfy |a − c| < θ2 . The ﬁnal DVFS policy can then be either biased towards high V/F level for better performance or towards low V/F level for better power saving, depending on the system running mode. The pseudo code for the DVFS decision making logic is shown in Algorithm 1. Given that the DVFS tuning frame ceive during a time frame can then be determined by N × is W cycles, the maximum number of votes a region can reP ×W , where N is the number of routers in the region and P is the number of input ports per router. During the vote receiving phase, if a counter excesses (cid:8) N×P×W +θ2 (cid:9), then no other V/F levels can receive more votes during the same DVFS tuning frame. The DVFS decision making logic records the V/F level as the majority selected one and disables the vote receiving signal (as shown in Figure 5b) to inform each router in the region to stop the vote receiving process. Otherwise, if no counter excesses (cid:8) N×P×W +θ2 (cid:9), the DVFS decision logic searches for the maximum counter value before making the DVFS decision, during which two threshold θ1 (absolute threshold) and θ2 (relative threshold) guides the searching progress as described in Section 3. As shown in Figure 7, both the VCL and the centralized counters can be implemented with limited hardware resources. After synthesizing with Synopsis Design Compiler using 45 nm technology, one RDC consumes 697.7 standard NAND gates, which is about 3% area overhead averaged to each router. The power comsumption of RDC under 1.7 V/2.0 GHz condition is about 642.1 μW, with 634.2 μW dynamic power and 7.9 μW leakage power. 2 2 5. EXPERIMENTS AND RESULTS 7 7 315 Table 2: Simulation platform conﬁguration Item Processor L1-Cache L2-Cache Memory Amount 64 cores 64 banks 64 banks 8 ranks NoC 64 nodes Thread char. 1/NI DVFS ctrl. 1/region Description Alpha based 2.0 GHz out-of-order processors. 32-entry instruction queue, 64-entry load queue, 64-entry store queue, 128-entry reorder buffer (ROB). Private, 32 KB per-core, 4-way set associative, 128 B block size, 2-cycles latency, split I/D caches, 32 MSHRs. Chip-wide shared, 1 MB per-bank, 16-way set associative, 128 B block size, 6-cycles latency, 32 MSHRs. 4 GB DRAM, 512 MB per-rank, up to 16 outstanding requests for each processor, 8 memory controllers. 8×8 mesh network. Each node consists of 1 router, 1 network interface (NI), 1 core, 1 private L1 cache, and 1 shared L2 cache. 4 routers form a DVFS region, in which a region DVFS controller implements the proposed DVFS policy and tunes the V/F level on a per time frame basis. X-Y dimensional routing. Router is 2-stage pipelined, 6 VCs per port, 4 ﬂits per VC. 128-bit datapath. Directory based MOESI cache coherence protocol. One cache block consists of 1 packet, which consists of 8 ﬂits. One coherence control message consists of 1 single-ﬂit packet. 65,536 (216 ) cycles of sampling window, 16,384 (214 ) cycles of step window. Overlapping ratio λ = 4. 16,384 (214 ) cycles of tuning window (the same size of a thread characterization step window). 3 supported V/F levels: 1.5V/1.5GHz, 1.6V/1.75GHz, 1.7V/2.0GHz. utilization level is low but the data sharing degree is high. The same goes for two other programs dedup and freqmine. In the following, we closely look into swaptions for power over-provisioning and bodytrack for power under-provisioning. Power over-provisioning. Figure 9 demonstrates the occurrence of power over provisioning in swaptions, where the results are averaged over four consecutive V/F tuning windows (the same length as a sampling step window for thread characterization) in order to show stabilized V/F tuning effects. We number different V/F regions according to router coordinates and annotate the V/F regions where power over provisioning happens with red borders. As the ﬁgure shows, in the inspected DVFS windows, power over provisioning has occurred in 12 out of the 16 V/F regions. Figure 9a shows the effects of BLD, where the passing routers in the annotated V/F regions all boost to nearly maximum V/F level to accelerate packet transmission. Figure 9b shows the effects of our TVD, where routers in the annotated regions operate averagely at 85% of maximum frequency. Figure 9c illustratively reveals 1) average router buffer load and the ratio between packets passing through routers in Region 1 (4 routers with coordinates (1, 1), (1, 2), (2, 1) and (2, 2)) and Region 2 (4 routers with coordinates (3, 1), (3, 2), (4, 1) and (4, 2)) to access shared data blocks and nonshared data blocks, 2) relative system-level ROI ﬁnish time in both BLD and T V D mechanisms. As the ﬁgure shows, all routers in region 1 and 2 suffer temporary high buffer load. However, most of the packets are accessing to non-shared data blocks. Thus they are non-critical and their delay can be tolerated by the issuing cores with several memory latency tolerance techniques such as read/write buffers [15, 14]. Figure 9c also shows that in terms of system-level performance, although the V/F levels in the 12 annotated regions are decreased by 15%, our T V D mechanism still achieves good system performance (only 4% slowdown in ROI ﬁnish time than BLD). Power under-provisioning. Figure 10 shows the occurrence of power under provisioning in benchmark bodytrack, where the results are also obtained over four consecutive DVFS tuning windows. As the ﬁgure shows, power under provisioning has happened in 14 out of the 16 V/F regions in the inspected DVFS windows. Similar to Figure 9, we annotate the V/F regions where power under-provisioning occurs with blue borders. Figure 10a shows the effects of BLD, where the routers in the annotated regions averagely reduce to 85% of maximum frequency for power saving. Figure 10b Table 3: PARSEC benchmarks and characteristics [4] Application Problem size (all simlarge) blackscholes bodytrack canneal dedup facesim ferret ﬂuidanimate freqmine streamcluster swaptions vips x264 65,536 options 4 frames, 4,000 particles 400,000 netlist elements 184MB data 80,598 particles, 372,126 tetrahedra 256 queries, 34,973 images 5 frames, 300,000 particles 990,000 click streams 16,384 points, 128 point dimension 64 swaps, 20,000 simulations 2,662×5,500 pixels 128 frames, 640×360 pixels Characteristics Network Data Util. Sharing low low high low high high high low high high high high low high high high low high low high low low low high creases the V/F level of the routers to low V/F mode for power saving. Refer to the conﬁguration in Table 2, in our experiments the maximum buffer capacity of a virtual channel is 16 ﬂits/cycle. If the region-wide average buffer load exceeds 12 ﬂits/cycle (75% of maximum buffer load), BLD decides that the local congestion is high, and operates the routers in the region with high V/F level. If the regionwide average buffer load is below 4 ﬂits/cycle (25% of maximum buffer load), BLD decides that the local routers are less loaded, and operates the routers in low V/F level. Otherwise, (with region-wide average buffer load goes between 4 to 12 ﬂits/cycle), routers in the region operate in middle V/F level. 5.2 Experimental results We report experimental results, ﬁrst with detailed illustration of power over/under provisioning cases in PARSEC benchmarks and then the network and system energy efﬁcacy improvements brought by our technique. 5.2.1 Power over/under provisioning in PARSEC According to Table 3, we analyze possible power over/under provisioning to the benchmark programs by BLD. For swaptions, the BLD mechanism tends to create power over provisioning problem because the benchmark’s network utilization level is high but the data sharing degree among different threads is low. The same goes for four other programs facesim, ﬂuidanimate, streamcluster, and vips. For bodytrack, the BLD mechanism tends to create power under provisioning problem because the benchmark’s network 8 8 316 (a) Router buffer load based DVFS (BLD) (b) Thread voting based DVFS (TVD) (c) Avg. router buffer load and packets access ratio Figure 9: Power over provisioning example in swaptions. (a) Network-metric based DVFS tuning results. (b) The proposed DVFS tuning results. (c) Average router buffer load and ratio between packets accessing to shared and non-shared data block. (a) Router buffer load based DVFS (BLD) (b) Thread voting based DVFS (TVD) (c) Avg. router buffer load and packets access ratio Figure 10: Power under provisioning example in bodytrack. (a) Network-metric based DVFS tuning results. (b) The proposed DVFS tuning results. (c) Average router buffer load and ratio between packets accessing to shared and non-shared data block. shows the effects of our T V D, where routers in the annotated regions averagely boost to 95% of maximum frequency to accelerate packet transmission. Figure 10c illustratively reveals the average router buffer load and ratio between packets accessing to shared data blocks and non-shared data blocks through V/F Regions 11 and 15. As the ﬁgure shows, all routers in Regions 11 and 15 enjoy temporary lower buffer load. However, most of the packets are accessing to shared data blocks, and their delay can further affect the progress of other data-sharer threads. Figure 10c also shows that in terms of system-level performance, although the V/F levels in the 14 annotated regions are increased by 10%, our T V D improves the system-level ROI ﬁnish time by 27% than the BLD approach. 5.2.2 Network power and performance Before presenting the energy efﬁcacy results, we compare the three schemes, i.e. AHV , BLD and T V D, in terms of network power consumption and network access rate. Network power consumption. Figure 11a shows the network power consumption for the three comparison cases. In each benchmark, the results are normalized with respect to AHV , which consumes the most power due to constant high router V/F. In terms of network power consumption, BLD and T V D spend less power than AHV across all the PARSEC benchmarks, showing the power saving gains brought by DVFS mechanisms. However, the power saving of T V D against BLD depends on the network utilization and datasharing characteristics of the benchmarks. • In blackscholes, canneal, ferret and x264, both the network utilization and data sharing degree have the same characteristics (both are high or low). In this case, power over/under provisioning seldom occur, and BLD and T V D achieve similar power saving results. • In facesim, ﬂuidanimate, streamcluster, swaptions and vips, each benchmark’s network utilization level is high but the data sharing degree among different threads is low. In this case, BLD tends to create power over provisioning problem since it increases on-chip V/F on high buffer load even if the on-going packets are accessing to non-shared data blocks. However, T V D alleviates this problem by providing lower V/F level and thus consumes averagely 20.0% less power. • In bodytrack, dedup, and freqmine, each benchmark’s network utilization level is low but the data sharing degree among different threads is high. In this case, BLD tends to create power under provisioning problem since it lowers on-chip V/F on low buffer load even if the on-going packets are accessing to shared data blocks. As the ﬁgure shows, as an expense of alleviating this problem, TVD consumes averagely 10.9% more network power than BLD. 9 9 317 (a) Relative network power consumption results (b) Relative network packet injection rate results Figure 11: Network-level power consumption and packet injection rate results. (a) Relative MIPJ results (b) MIPJ comparison in bodytrack (c) MIPJ comparison in swaptions Figure 13: System energy efﬁcacy results in terms of million instructions per Joule (MIPJ) Network Energy Efﬁcacy (NEE). Figure 12 depicts the Figure 12: Network energy efﬁcacy for all programs Network packet injection rate. Figure 11b shows the network access rate results in the benchmarks. Since the NoC is self-throttling (a core cannot inject new requests into the network once it ﬁlls up all of its miss request buffers), the network packet injection rate directly correlates to the service speed of a core’s miss requests and thus a core’s performance. In each benchmark, the results are normalized with respect to AHV , which achieves the fastest network access rate across all benchmarks due to constant high V/F level. The ﬁgure also illustrates that in benchmarks where power over/under provisioning seldom occurs (blackscholes, canneal, ferret and x264), BLD and T V D achieve similar network access rate. In benchmarks where power over provisioning frequently occurs (facesim, ﬂuidanimate, streamcluster, swaptions and vips), BLD achieves averagely 5.5% higher packet injection rate than T V D. Comparing with the additional (20.0% more) power BLD consumes, the improvement in packet injection rate is limited. In benchmarks where power under provisioning frequently occurs (bodytrack, dedup, and freqmine), T V D gains averagely 21.5% higher packet injection rate than BLD with 10.9% extra power. 5.2.3 Network and system energy efﬁcacy 10 10 318 relative network energy efﬁcacy results measured in MPPJ (Million Packets Per Joule), which reﬂects the network’s energy efﬁcacy on serving packets from cores. As the ﬁgure shows, in blackscholes, canneal, ferret and x264, T V D behaves similarly to BLD since the power over/under provisioning problem is insigniﬁcant. However, T V D consistently achieves better NEE results than AHV and BLD in the remaining 8 out of 12 benchmarks, where the average improvement is 19.6% than AHV and 14.6% than BLD. System Energy Efﬁcacy (SEE). Figure 13a presents the relative system energy efﬁcacy results in MIPJ (Million Instructions Per Joule), which measures the amount of energy expended by the CMP for executed instructions [2]. As can be seen, since DVFS increases the NoC power efﬁciency, both BLD and T V D increase the SEE against AHV across all benchmarks, where the average improvement in BLD against AHV is 8.4%, in T V D against AHV is 26.3%. Moreover, T V D avoids power over/under provisioning and constantly gives better SEE than BLD in all benchmarks. This reveals that both AHV and BLD consume more power than T V D for the same improvements in system-level performance. Figure 13b and Figure 13c further illustrate SEE results in two benchmarks bodytrack and swaptions over different V/F tuning windows. As the ﬁgures show, the SEE curve of T V D lies above AHV and BLD. Since BLD tends to cause power under provisioning in bodytrack and over provisioning in swaptions, the SEE curve of BLD swings between that of BLD and T V D across different V/F tuning windows. This is not the case in both AHV and T V D. In AHV , the router V/F level remains constant. In T V D, power over/under provisioning are both effectively alleviated. DVFS, in which the PI controller bootstraps the ANN during the ANN’s initial learning process. To capture the impact of the uncore upon overall system performance, [8] proposes an uncore DVFS approach using average network delay and memory access time as the V/F adjustment hints. Work in [22] partitions a multi-core chip into multiple V/F islands. The authors then propose a rule-based DVFS control for each island according to both link utilization and router queue occupancy. Bogdan et al. [6] illustrates that computational workloads in multicores are highly complex and exhibit fractal characteristics. A characterization approach based on the dynamism of queue utilization and fractional differential equations has been proposed to guide on-chip DVFS in different V/F islands. In [20], Mishra et al. propose a ﬁne-grained DVFS approach for NoC routers. They monitor router input queue occupancy, based on which the upstream router changes its V/F level. When a gather trafﬁc pattern (like all-to-one trafﬁc to a shared memory) creates a hot spot region in the network, the routers in the region can adaptively switch to a turbo mode, increasing frequency to accelerate transmitting packets for performance assurance. To guide per-core DVFS, memory request criticalty is considered in [19] for predicting DVFS’s impact on system performance by identifying critical memory requests (e.g., leading load requests) in a realistic DRAM model and predicting the network delay and the memory request ﬁnish time. Recently in [16], the authors propose to utilize highly predictable properties of cache-coherence communication to derive more speciﬁc and reliable NoC trafﬁc predictions, based on which an improved V/F island based DVFS mechanism can be achieved. When designing DVFS for NoCs, we consider applicationlevel characteristics in message generation and data sharing which are directly linked to system performance. Furthermore, we leverage the role of cores or threads in actively inﬂuencing the DVFS decision through sending preferred V/F levels as ""votes"" to express their wishes to the region DVFS controllers, which make ""democratic"" decisions based on the majority vote for their network regions. 7. CONCLUSION We have introduced a novel on-chip DVFS strategy that improves energy efﬁcacy in CMP systems. To address the problems of power over/under provisioning in network metric based approaches, our DVFS technique adjusts NoC V/F levels according to per thread’s direct performance indicatives in terms of message generation rate and data sharing metrics. The proposed DVFS adopts a light-weight voting based approach in which each thread votes a preferred V/F level according to its own performance indicatives. The V/F votes are then encapsulated into packet headers and propagated into the network. With a few routers composing a V/F sharing region, a region-based V/F controller collects all votes of passing packets and makes the ﬁnal V/F decision by the majority vote in a ""democratic"" way. By realizing and evaluating our technique with a 64-core CMP architecture simulated in GEM5 running all PARSEC benchmarks, we ﬁnd that the workload-sensitive improvements in energy efﬁcacy correlate consistently with the network utilization and data-sharing characteristics of the benchTable 4: Summary of energy-efﬁcacy results Application Characteristics Network Data Util. Sharing Leading probl. OP∗ UP∗ Improvement(%) MPPJ MIPJ (non-opportun.) (non-opportun.) (non-opportun.) (non-opportun.) blackscholes low 0% 2% canneal high 1% 0% ferret high 0% 2% x264 high 0% 1% facesim high 17% 28% ﬂuidanimate high 21% 33% streamcluster high 19% 30% swaptions high 14% 35% vips high 18% 30% bodytrack low 9% 14% dedup low 8% 19% freqmine low 10% 10% Avg. of all benchmarks 9.7% 17.1% Avg. of opportunistic benchmarks 14.6% 25.0% *OP denotes power over provisioning and UP power under provisioning low high high high low low low low low high high high         5.2.4 Summary of energy-efﬁcacy results Table 4 illustrates the likelihood of power over/under provisioning in all benchmarks according to their network utilization and data-sharing characteristics. In 4 out of the 12 benchmarks where network utilization and data sharing degree among different threads have the same characteristics (both are low or high), there is little opportunity for TVD to outperform BLD because neither over- nor under- provisioning of power often occurs. Thus, we call these benchmarks as non-opportunistic benchmarks. However, the remaining 8 benchmarks are opportunistic where network utilization and data sharing degree have different characteristics (one low the other high or one high the other low). For these programs, TVD constantly delivers better energy efﬁcacy in terms of MPPJ and MIPJ. As summarized in the table, the average improvements of TVD over BLD across all benchmarks is 9.7% in terms of MPPJ and 17.1% in terms of MIPJ. As expected, for those opportunistic benchmarks where power over or under provisioning problem frequently occurs, the average improvements of TVD against BLD increase to 14.6% in terms of MPPJ and 25.0% in terms of MIPJ, and the maximum improvement of TVD against BLD is 21% in terms of MPPJ and 35% in terms of MIPJ. 6. RELATED WORK As shown in [12], on-chip DVFS can be implemented efﬁciently through on-chip voltage regulators in which the scaling delay can be reduced to tens of processor cycles, or on the order of the memory access latency. Such results justify DVFS for on-chip implementations. There are rich works in the literature which propose various DVFS techniques to increase energy efﬁcacy in CMP systems. Often, these schemes rely on network metrics which indirectly reﬂect application/thread communication characteristics to guide the V/F adjustment in different granularity. In [9], the entire NoC is considered as a single V/F domain to offer coarse-grain DVFS management. A centralized PI (proportional and integral) controller is used to dynamically adjust the V/F scale according to network throughput and average packet delay so as to minimize energy consumption under application performance constrains. Further in [23], treating the entire shared last level caches (LLC) and NoC as the CMP uncore, the authors combine PI controller with Artiﬁcial Neural Network (ANN) to achieve proactive 11 11 319 Scaling With Optimal Core Allocation and Thread Hopping for the 80-Core TeraFLOPS Processor,” Journal of Solid-State Circuits, vol. 46, no. 1, pp. 184–193, 2011. [12] S. Eyerman and L. Eeckhout, “Fine-grained DVFS Using On-chip Regulators,” ACM Transation on Architecture and Code Optimization (TACO), 2011. [13] S. Garg, D. Marculescu, R. Marculescu, and U. Ogras, “Technology-driven Limits on DVFS Controllability of Multiple Voltage-frequency Island Designs: A System-level Perspective,” in Design Automation Conference (DAC), 2009. [14] J. Handy, The Cache Memory Book. Academic Press Professional, 1993. [15] J. L. Hennessy and D. A. Patterson, Computer Architecture: A Quantitative Approach, Fifth Edition. Morgan Kaufmann Publishers Inc., 2011. [16] R. Hesse and N. E. Jerger, “Improving DVFS in NoCs with Coherence Prediction,” in International Symposium on Networks on Chip (NoCS), 2015. [17] J. Howard, S. Dighe, Y. Hoskote, S. Vangal, D. Finan, G. Ruhl, D. Jenkins, H. Wilson, N. Borkar, G. Schrom, F. Pailet, S. Jain, T. Jacob, S. Yada, S. Marella, P. Salihundam, V. Erraguntla, M. Konow, M. Riepen, G. Droege, J. Lindemann, M. Gries, T. Apel, K. Henriss, T. Lund-Larsen, S. Steibl, S. Borkar, V. De, and V. D. Wijngaart, “A 48-Core IA-32 Message-passing Processor with DVFS in 45nm CMOS,” Journal of Solid-State Circuits, vol. 46, no. 1, pp. 173–183, 2011. [18] S. Li, J. H. Ahn, R. Strong, J. Brockman, D. Tullsen, and N. Jouppi, “McPAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures,” in International Symposium on Microarchitecture (MICRO), 2009. [19] R. Miftakhutdinov, E. Ebrahimi, and Y. N. Patt, “Predicting Performance Impact of DVFS for Realistic Memory Systems,” in International Symposium on Microarchitecture (MICRO), 2012. [20] A. K. Mishra, R. Das, S. Eachempati, R. Iyer, N. Vijaykrishnan, and C. R. Das, “A Case for Dynamic Frequency Tuning in On-chip Networks,” in International Symposium on Microarchitecture (MICRO), Jul. 2009. [21] L.-S. Peh and W. J. Dally, “A Delay Model and Speculative Architecture for Pipelined Routers,” in International Symposium on High-Performance Computer Architecture (HPCA), 2001. [22] A. Rahimi, M. E. Salehi, S. Mohammadi, and S. M. Fakhraie, “Low-energy GALS NoC with FIFO-Monitoring Dynamic Voltage Scaling,” Microelectronics Journal, 2011. [23] J.-Y. Won, X. Chen, P. Gratz, J. Hu, and V. Soteriou, “Up By Their Bootstraps: Online Learning in Artiﬁcial Neural Networks for CMP Uncore Power Management,” in International Symposium on High-Performance Computer Architecture (HPCA), 2014. marks. There are no or marginal improvements in four programs (blackscholes, canneal, ferret and x264), because these programs have the same tendency in network utilization and data-sharing, being either both low or both high, and thus providing little opportunity to improve network-metric based DVFS mechanism. However for the other eight programs, our technique gives signiﬁcant boost in energy efﬁcacy, because these programs have inconsistent tendency in network utilization and data-sharing, being one low and the other high or vice versa, and thus offer salient opportunities for our technique to win over pure network-metric based DVFS approach. Indeed, compared to a router buffer occupancy based DVFS mechanism, our technique enhances the network energy efﬁcacy in MPPJ by 9.7% on average and 21% in maximum as well as the system energy efﬁcacy in MIPJ by 17.1% on average and 35% in maximum. We therefore believe that our technique gives a new insight (necessity of jointly considering message generation rate and data sharing property) and opens a viable approach (light-weight threading V/F voting with region-based DVFS) to designing highly efﬁcient DVFS schemes for many-core networks. 8. "
2016,"Efficient synthetic traffic models for large, complex SoCs.","The interconnect or network on chip (NoC) is an increasingly important component in processors. As systems scale up in size and functionality, the ability to efficiently model larger and more complex NoCs becomes increasingly important to the design and evaluation of such systems. Recent work proposed the ""SynFull"" methodology that performs statistical analysis of a workload's NoC traffic to create compact traffic generators based on Markov models. While the models generate synthetic traffic, the traffic is statistically similar to the original trace and can be used for fast NoC simulation. However, the original SynFull work only evaluated multi-core CPU scenarios with a very simple cache coherence protocol (MESI). We find the original SynFull methodology to be insufficient when modeling the NoC of a more complex system on a chip (SoC). We identify and analyze the shortcomings of SynFull in the context of a SoC consisting of a heterogeneous architecture (CPU and GPU), a more complex cache hierarchy including support for full coherence between CPU, GPU, and shared caches, and heterogeneous workloads. We introduce new techniques to address these shortcomings. Furthermore, the original SynFull methodology can only model a NoC with N nodes when the original application analysis is performed on an identically-sized N-node system, but one typically wants to model larger future systems. Therefore, we introduce new techniques to enable SynFull-like analysis to be extrapolated to model such larger systems. Finally, we present a novel synthetic memory reference model to replace SynFull's fixed latency model; this allows more realistic evaluation of the memory subsystem and its interaction with the NoC. The result is a robust NoC simulation methodology that works for large, heterogeneous SoC architectures.","Efﬁcient Synthetic Trafﬁc Models for Large, Complex SoCs Jieming Yin* jieming.yin@amd.com Onur Kayiran* onur.kayiran@amd.com Matthew Poremba* matthew.poremba@amd.com Natalie Enright Jerger* † enright@ece.utoronto.ca *Advanced Micro Devices, Inc. Gabriel H. Loh* gabriel.loh@amd.com †University of Toronto ABSTRACT The interconnect or network on chip (NoC) is an increasingly important component in processors. As systems scale up in size and functionality, the ability to efﬁciently model larger and more complex NoCs becomes increasingly important to the design and evaluation of such systems. Recent work proposed the “SynFull” methodology that performs statistical analysis of a workload’s NoC trafﬁc to create compact trafﬁc generators based on Markov models. While the models generate synthetic trafﬁc, the trafﬁc is statistically similar to the original trace and can be used for fast NoC simulation. However, the original SynFull work only evaluated multi-core CPU scenarios with a very simple cache coherence protocol (MESI). We ﬁnd the original SynFull methodology to be insufﬁcient when modeling the NoC of a more complex system on a chip (SoC). We identify and analyze the shortcomings of SynFull in the context of a SoC consisting of a heterogeneous architecture (CPU and GPU), a more complex cache hierarchy including support for full coherence between CPU, GPU, and shared caches, and heterogeneous workloads. We introduce new techniques to address these shortcomings. Furthermore, the original SynFull methodology can only model a NoC with N nodes when the original application analysis is performed on an identicallysized N-node system, but one typically wants to model larger future systems. Therefore, we introduce new techniques to enable SynFull-like analysis to be extrapolated to model such larger systems. Finally, we present a novel synthetic memory reference model to replace SynFull’s ﬁxed latency model; this allows more realistic evaluation of the memory subsystem and its interaction with the NoC. The result is a robust NoC simulation methodology that works for large, heterogeneous SoC architectures. 1. INTRODUCTION Modern computer systems have evolved from relatively simple microprocessors to complex systems-on-chips (SoCs). Current systems already integrate CPUs, GPUs, networks on chips (NoCs), memory controllers [1, 2], and more. Looking forward, system sizes in terms of CPU cores and/or GPU compute units are likely to continue to scale to support increasingly complex processing for immersive virtual reality applications [3], “Big Data” and “Big Compute” [4], and more. In particular, recent industry papers point toward future exascale high-performance computing systems making use of heterogeneous compute nodes with extensive GPU capabilities [5, 6]. As future SoCs scale in both functional diversity (inclusion of GPUs or other accelerators) and size (number of CPU/GPU compute resources), the SoC’s NoC and memory systems become increasingly 978-1-4673-9211-2/16/$31.00 c(cid:3)2016 IEEE critical components in determining the overall performance of the SoC. To design effective NoC and memory systems for large heterogeneous systems, computer architects need tools to model the behavior and predict the performance of different candidate designs. However, conventional simulation tools driven by system emulation are too slow, and simulations driven by simple synthetic trafﬁc patterns (e.g., uniform random injections), while fast, may not capture important application-dependent behaviors. One recently proposed methodology “SynFull” takes a ﬁrst step toward providing the “best of both” [7]. SynFull takes trafﬁc traces from detailed cycle-level simulation of application executions, and then analyzes these to create stochastic (i.e., synthetic) Markov model-based trafﬁc generators that are statistically similar to the original applications in terms of representation of different program phases, distributions of message sources and destinations, per-node injection rates, etc. As a result, SynFull enables the efﬁcient NoC simulation using fast synthetic models while still capturing critical application-dependent and time-varying behaviors absent from conventional simplistic synthetic trafﬁc patterns. For large heterogeneous SoCs, however, the current SynFull methodology is lacking in several dimensions. SynFull only considers multi-core CPU systems. However, with the proliferation accelerated processing units (APUs) with both CPUs and integrated GPUs, along with the growth in general-purpose GPU (GPGPU) computing, evaluations of future NoCs need to account for the differences that APUs and GPGPU applications introduce. Whereas SynFull considered a multi-core CPU with a simple MESI cache coherence protocol, modern APUs that support shared virtual memory between CPU and GPU components [8] use significantly more complex coherence protocols with many more states and message types. GPGPU applications, especially due to the presence of very distinct CPU and GPU phases, can cause the SynFull approach to generate NoC trafﬁc with signiﬁcant application-scale deviations. We propose a variety of extensions to SynFull to address these and other issues to provide a robust methodology capable of modeling NoC behaviors for GPGPU applications on APUs. Especially in the context of large future systems, a key limitation of SynFull is that the trace collection and analysis performed on an N-node system can only create synthetic trafﬁc generators for other N-node systems. If one needs to evaluate a system with M(cid:2)N nodes, then one must recollect and reanalyze traces from an M-node system. However, for large M, running a full simulation may take an intractably long time (or impractically large memory capacity); having enough applications that can meaningfully scale up to 297 an M-node system poses another challenge. We introduce a methodology by which we can generate an “extrapolated trace” that is similar to a trace collected from an M-node system, which in turn can be fed to our version of SynFull to generate synthetic trafﬁc models for the larger system of interest under both strong and weak scaling scenarios. Finally, the original SynFull methodology focused only on the NoC and employed a simple constant-latency memory model. However, memory performance is dependent on a wide variety of factors related to the arrival time of requests, the distribution of requests among channels and banks, and myriad DRAM timing parameters. In the spirit of SynFull’s synthetic modeling approach for the NoC, we introduce application-dependent synthetic memory models that capture critical memory-related behaviors such as bank conﬂicts/row-buffer locality and non-uniform distributions of requests among channels and banks. Through all of these extensions and enhancements to the original SynFull, we arrive at an overall NoC and memory modeling methodology that can handle the full combination of heterogeneous systems, large scalable systems, and realistic memory systems. While we present results for a speciﬁc implementation based on the gem5 simulator [9], the methodology is general and the computer architecture research community can use this for NoC and memory studies of large APUs on other simulator platforms. 2. THE SYNFULL METHODOLOGY SynFull [7] generates synthetic trafﬁc that resembles the trafﬁc of a homogeneous multi-core CPU with coherent caches. The ﬁrst step is to ﬁnd only a few phases of the execution that can accurately represent the network trafﬁc of the whole execution. This is accomplished by using hierarchical clustering (at the macro and micro level) on an application trace that records all network injections. The trace is divided into macrophases each with a ﬁxed duration (macrophase length). Using a clustering algorithm, these macrophases are grouped into clusters. Different features of the trafﬁc can be used for clustering, such as node injection rates or source-destination ﬂows. The median macrophase for each cluster is selected as the representative macrophase. Similarly within each macrophase, clustering is performed at the micro-level to bin each microphase (each phase is microphase length long) into a cluster. Microphases are designed to better capture the variations in trafﬁc behaviors within individual macrophases. SynFull generates a synthetic trafﬁc model for each cluster, and executes them at the granularity of both micro and macrophases. Transition probabilities calculated by analyzing the application trace govern the transitions between phases. SynFull groups network trafﬁc into initiating messages (e.g., a read or a write request) and reactive messages that are triggered by initiating messages. Reactive messages consist of forward requests, invalidate requests, and responses. The synthetic trafﬁc model deﬁnes the behavior of initiating messages, forward requests, and invalidations for each cluster. For each initiating message type, three probability distributions govern its behavior. The ﬁrst distribution generates a source node for a packet. Similarly, the second distribution generates a destination node. The third distribution generates the number of packets that will be injected during a CPU/L1 CPU LLC CPU Cluster GPU Cluster Core L1D L1I CPU LLC  Complex Compute  Units L1D L1I GPU L2 APU Directories/Memory Controllers Memory CU/L1D APU Dir CU/L1D APU Dir CU/L1D APU Dir CU/L1D APU Dir CU/L1D CU/L1D CU/L1D GPU L2 GPU L2 GPU L1I CU/L1D CU/L1D CU/L1D GPU L2 GPU L2 GPU L1I CU/L1D GPU L1I CU/L1D GPU L1I CU/L1D CU/L1D GPU L2 GPU L2 CU/L1D CU/L1D GPU L2 GPU L2 CU/L1D CU/L1D CU/L1D CU/L1D CU/L1D CU/L1D GPU L2 GPU L2 GPU L1I GPU L1I GPU L2 GPU L2 CU/L1D CU/L1D CU/L1D GPU L2 GPU L2 GPU L1I CU/L1D GPU L1I CU/L1D CU/L1D GPU L2 GPU L2 CU/L1D APU Dir CU/L1D APU Dir CU/L1D APU Dir CU/L1D APU Dir (a) Baseline system. (b) Baseline topology. Figure 1: Baseline APU system. micro phase. Forward messages are deﬁned by a probability that a given message type will be forwarded, and a distribution of its possible destinations. Invalidations are similar to forward messages, but they are sent to multiple destinations that are generated by a different distribution. Responses are deterministic as deﬁned by the cache coherence protocol, and therefore Synfull does not need to provide a synthetic model for them (although the underlying NoC simulator still generates these responses). In the next sections, we keep much of the original SynFull terminology and share its hierarchical approach of macro and micro phases and initiating and reactive messages. However, we make drastic changes to many of the underlying details to yield a new approach that is more accurate for large-scale heterogeneous SoCs and GPGPU workloads. Throughout the rest of the paper we refer to the original SynFull methodology simply as SynFull and our new methodology as APU-SynFull. 3. CHALLENGES OF A SYNFULL APU In this section, we ﬁrst present our baseline accelerated processing unit (APU) architecture, and then we enumerate the challenges associated with applying the SynFull methodology to APUs. 3.1 Baseline System Fig. 1a shows an overview of our baseline APU system. The system contains both a CPU and a GPU cluster. The CPU cluster consists of CPU cores, private L1 caches, and a last-level-cache (LLC) complex. CPU caches are write-back and are kept coherent through a read-for-ownership MOESI directory protocol; the LLC keeps track of all the cache blocks in the CPU cluster with shadow tags. The GPU cluster consists of compute units (CUs), private L1 data caches, L1 instruction caches shared by every 4 CUs, and a banked uniﬁed L2 cache. GPU caches are write-through and writeno-allocate. L1 caches in the GPU are kept coherent by writing through dirty data and invalidating the caches at kernel launch. The CPU and GPU have a uniﬁed memory address space. The APU directories are responsible for keeping the CPU LLC and the GPU L2 cache coherent. All memory requests generated by the CUs access the APU directories to stay coherent with the CPU LLC. APU directories are connected to the memory controllers for off-chip memory accesses. The CPU and GPU clusters each have their own coherence protocols, and a system-level protocol (SLP) enables coherent communication between these clusters. The topology of our baseline system is shown in Fig. 1b. The GPU cluster has 32 CUs that are connected by a 4×8 mesh. APU directories are placed along the left and right edges, and 8 GPU L1 instruction caches are located in the 298 center of the mesh. The GPU cluster contains 16 addressinterleaved L2 cache banks. The mesh is augmented with two additional nodes for the CPU cluster. One node is connected to the CPU L1 cache, and the other is connected to the CPU LLC. The topology shown in Fig. 1b is used as a working example, but the methodologies proposed in this paper are not tied to this speciﬁc layout. 3.2 The Need for an Enhanced SynFull In this section, we describe the limitations of the original SynFull methodology when applied to complex SoCs consisting of coherent CPU and GPUs. 3.2.1 Cache Coherence Protocol(s): One of the challenges in simulating an APU using SynFull is the incorporation of a complex cache coherence protocol. While the SynFull methodology uses a MESI-like protocol, our baseline APU has CPU and GPU coherence protocols plus the global SLP. Compared to MESI’s four stable coherence states, the combination of CPU, GPU, and SLP results in 18 stable states. SynFull uses four initiating message types while the initiating messages in our APU cannot easily be grouped into less than ten categories. For example, write requests from CPU and GPU are classiﬁed in different categories because CPU caches are write-back while GPU caches are write-through. In total, SynFull uses ten message types, whereas there are more than 80 message types for our APU. Although not all of the message types are modeled since we group them into fewer types where possible, the grouping must be able to abstract away most of the complexity inside the protocol while still being able to represent critical communication behaviors. Another important challenge is to model the depth of a complex protocol. Protocol depth is deﬁned as the longest chain of dependent messages generated by a single initiating request. It is dependent on the complexity of the protocol and the total number of levels in the memory hierarchy. In MESI, an initiating request usually results in two or three additional network messages before the coherence transaction is completed. For our APU, an initiating request might result in more than ten network messages during the entire transaction. Additionally, SynFull assumes a simple one-toone mapping between requests and responses, which does not hold in our APU. 3.2.2 APU Workload Phase Behavior: APU workloads have signiﬁcantly different behaviors than CPU workloads. Current APU workloads typically consist of multiple phases, where in each phase either the CPU or the GPU cores are active.1 The network trafﬁc during a GPU phase is much higher than during a CPU phase. Fig. 2a shows the number of initiating messages generated by the BFS application over time. The big spike corresponds to the GPU execution where the network trafﬁc is much higher compared to all the other phases where the GPU is idle. If the APU application ofﬂoads only a single kernel for GPU computation, as in this simple example, then there is likely to be a single spike in the network trafﬁc similar to Fig. 2a. 1 Future task-based APU applications may exhibit more concurrent execution of tasks across both CPU and GPU resources that would change the observation that phases predominantly exercise only the CPU or only the GPU. s e g a s s l i s e m 0 0 0 0 e 1 g c n y c t a K 0 0 n 1 0 0 5 e o p r f t i i i r e b m u N 0 0            100M        200M        300M Cycles 0.9983 0.0017 0.6667 CPU phase GPU  phase (a) The number of initiating messages over time. Figure 2: Phase behavior of BFS. (b) Macro-level transition probabilities. 0.3333 e s a h p U P G e s a h p U P C s e g a s s e s a h p U P G e s a h p U P C Time Time (a) 300M cycles. (b) 3B cycles. Figure 3: Challenges of probability-based transitions 0 0 0 5 1 GPU_READ GPU_WRITE l i s e m 0 0 0 e 0 g c 1 n y t c a K 0 0 0 n 1 0 5 e o p r f t i i i r e b m u N 0 s n o i t c e n j I t e k c a P d e z i l a m 0          2M      4M      6M      8M Cycles r o N 1.2 1 0.8 0.6 250K 625K Macro-phase length 750K (a) The number of initiating messages in Hotspot over time. (b) Normalized packet injection rate per message type w.r.t. to the baseline, across different macrophase lengths. Figure 4: Sensitivity to macrophase length. Such phase behavior might lead to problems in SynFull. SynFull uses a probability-based transition model between macro phases. In cases where we observe a single spike, a small error in the ratio between the number of executed CPU and GPU phases might result in signiﬁcant errors in the generated network trafﬁc. This is exempliﬁed by BFS in Fig. 2b which shows the transition probabilities between CPU and GPU phases. A methodology that uses this probability-based transition model is also heavily dependent on the simulation length. If we simulate 300M cycles, as shown in Fig. 3a, the steadystate condition is reached after the ﬁrst executed GPU phase causing the simulation to ﬁnish. In this scenario, the ratio between the executed CPU and GPU phases is very similar to that of the real execution, but the GPU spike comes at the end, although this is not how the real application behaves. This also requires a priori knowledge of the number of cycles to be simulated. If we run the simulation for 3B cycles (Fig. 3b), the spikes come at random times, and sometimes back to back; back-to-back trafﬁc spikes may overwhelm the NoC, causing high latencies due to congestion that is not present in the real application. If the simulation is capped around 2B cycles, the ratio of GPU phases to CPU phases would be much higher than in real execution. Finally, if the simulation is limited to 150M cycles, the GPU phase is never observed. These results demonstrate that the probability-based transition model does not always emulate real application behavior, or could otherwise require very long simulation times to achieve Markov steady state. 299                                                                                                                         3.2.3 Sensitivity to Macrophase Length: While the original SynFull did not report a strong sensitivity of the methodology to the macrophase length for multicore CPU applications, we found that this does not appear to hold for APU workloads. To demonstrate this, we execute the Hotspot benchmark using different macrophase lengths (microphase length is kept constant), as shown in Fig. 4a. Unlike BFS (Fig. 2a), Hotspot has a periodic injection pattern of approximately 1M cycles. Fig. 4b shows that choosing a macro-resolution of 250K cycles, which is very close to a quarter of the period between peaks, leads to a close match of the number of initiating messages per unit time compared to the original cycle-level simulation. However in such applications, choosing a poor macrophase length can result in the representative cluster misestimating the number of injected messages. For example, a macrophase length of 625K cycles results in most of the macrophases being misaligned with respect to the underlying trafﬁc periodicity, resulting in approximately 8% fewer message injections. It is interesting to note that the macrophase length need not be perfectly chosen; Fig. 4b also shows an example where the macrophase length is 750K cycles. In this case, the macrophases line up with the underlying trafﬁc patterns once every four macrophases; while the error is slightly greater than the 250K-cycle case, it is still relatively low. These results demonstrate when applications exhibit regular periodic behaviors, simulations will be more accurate if the period is carefully determined. 3.2.4 Capturing Bursts: Section 3.2.3 showed that choosing a good macroresolution is necessary to provide an accurate representation of the underlying trafﬁc patterns; however, this alone is insufﬁcient to achieve accurate latency results because it does not adequately capture the ﬁne-grained bursty behavior of GPU applications. Temporal bursts: A SIMD compute unit executing a single vector-load instruction can generate up to 64 memory requests in a very short amount of time.2 Fig. 5a shows the inter-arrival time of GPU read requests in the Hotspot benchmark. Most messages are injected very close in time, which results in a very bursty injection pattern. However, SynFull injects messages into the network at uniform intervals within a microphase. At typical microphase lengths of a few hundred cycles, this causes the bursts to be spread out too much and no longer captures the effects of executing the GPU’s vector memory operations. Spatial bursts: The trafﬁc generated by the GPU in an interval of few hundred cycles (microphase granularity) is rarely spread out across all source and destination nodes. While a few source-destination pairs experience high trafﬁc, others do not inject/receive any messages in the same interval (but do so in another interval). Fig. 5b and Fig. 5c show the spatial burstiness of GPU read requests within 500Kcycle and 250-cycle sampling windows, respectively. For the large sampling window of 500K cycles, GPU L1 cache nodes have similar probabilities for injecting requests, resulting in an evenly distributed network trafﬁc pattern. How2 64 is a typical wavefront length. The actual number of requests per burst is typically less than the maximum possible due to request coalescing, branch divergence, etc. y c n e u q e r F 6000 4000 2000 0 08 6 4 2 0 8 6 4 2 0 8 6 4 2 0 8 6 4 1 2 3 4 4 5 6 7 8 8 9 0 1 2 2 3 4 1 1 1 1 1 1 (a) Frequency of different inter-arrival times of GPU read requests in Hotspot during its GPU phase. Inter-arrival Time y . t i 0 l i 3 0 0 b a b o r 5 1 0 0 P n o . i t 0 0 0 0 c e n . j I 0 2 4 6 y t i l i b a b o 2 1 0 . r 6 0 0 P n o . 8 10 12 14 16 18 20 22 24 26 28 30 GPU L1Cache nodes i t 0 0 0 . j c e n I 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 GPU L1Cache nodes (b) Node injection probability with (c) Node injection probability with 500K-cycle sampling window. 250-cycle sampling window. Figure 5: Temporal and spatial burstiness of Hotspot. l ) s e c y c ( y c n e t a l . g v A 50 40 30 20 10 0 n o i t u b i r t s i D e g a s s e M 100% 80% 60% 40% 20% 0% CPU_READ GPU_READ GPU_WRITE Baseline 500K_500 500K_1K Configurations 500K_2K Baseline 500K_500 500K_1K Configurations 500K_2K (a) Average latency for the base(b) Distribution of initiating mesline and different microphase sages for the baseline and differlengths. ent microphase lengths. Figure 6: Effect of the default SynFull injection model on simulation accuracy in Hotspot. ever, during an interval of 250 cycles, less than half of the L1 cache nodes inject trafﬁc; and we further observe that this trafﬁc actually ﬂows to less than half of the L2 cache nodes. In contrast to the case with larger sampling granularity where trafﬁc is evenly distributed, parts of the interconnection network exhibit transient congestion caused by trafﬁc bursts. Because SynFull considers the injection probability and trafﬁc ﬂow at a very large macrophase granularity, it is likely to generate uniform network trafﬁc in a microphase if the source nodes have similar injection probabilities. Fig. 6 demonstrates how the inability to capture bursts affects the estimated latency in Hotspot. We set the macrophase length to 500K cycles, and compare the NoC behavior with microphase lengths of 500, 1000, and 2000 cycles against the baseline of full cycle-level simulation. Fig. 6a shows that for all chosen microphase lengths, the average NoC message latency is about 21-22 cycles while full cycle-level simulation exhibits much higher average latency. This happens despite the fact that for this benchmark SynFull actually does a good job of capturing the distribution of initiating message injections, as shown in Fig. 6b. These results show that the default SynFull injection model is not able to capture the bursty (both temporal and spatial) behavior of GPU workloads. 3.2.5 Scaling: Computer architecture researchers are focused on developing and evaluating innovations for future systems. Given the scaling trends of CPU multi-/many-cores and aggressive GPU/APU architectures, future systems are likely to continue to increase in core counts. Evaluating these new designs requires tools that scale with the proposed systems. SynFull requires a model ﬁle that is generated from a trace ﬁle collected during a full cycle-level simulation. For Syn300           r o r r E y c n e t a L e g a r e v A 80% 60% 40% 20% 0 2500 5000 7500 10000 Memory Phase Figure 7: Average latency error of ﬁxed-latency memory in CoMD compared to gem5 built-in model. Each phase on the x-axis represents a ﬁxed number of memory requests. Full to simulate larger systems, these trace ﬁles must ﬁrst be generated from cycle-level simulations of correspondingly larger system. This is problematic for two key reasons. First, cycle-level simulation of the large system may not be practical due to excessively long simulation times, or if the system is large enough the memory requirements of the simulation may well exceed the host computer’s resources. Second, because such large systems do not yet exist, many CPU and GPU applications have not yet been written/re-optimized in such a way so that they can scale to make use of the additional compute resources. So even if the cycle-level simulator could handle a much larger system, there may not be enough interesting workloads to run. 3.2.6 Memory: Memory latency is a key contributor to overall performance especially during periods of high-burst trafﬁc and directory misses. SynFull uses a ﬁxed-latency memory model, with the latency chosen to generally represent the typical/expected latency of a single memory request (e.g., read). The memory latency is simply added to the latency of a directory response. This type of model does not capture the dynamic behavior of memory performance that results from DRAM timing parameters, bank conﬂicts, row buffer locality, etc. These factors lead to individual memory request latencies deviating from the mean. We compared the average memory latency between a ﬁxed-latency memory controller and a cycle-accurate model across several applications using average percentage error in memory latency over a memory phase. One memory phase represents a ﬁxed number of memory requests rather than time, which allows for better visualization of GPU kernel execution. Fig. 7 shows an example of the error in memory latency over several memory phases in CoMD, which executes multiple GPU kernels. The average error is 55.4% with no error less than 22.0% and as high as 77.5%. We show the average, minimum, and maximum error over all memory phases for nine benchmarks in Table 1. We see that the ﬁxed-latency memory model performs poorly across a range of APU workloads, with an average error of 54.2% across all nine benchmarks. SynFull abstracts away memory addresses to keep the model simple to focus on cache coherence protocol behavior. To model memory latency more accurately, we must have a model for generating the address of each memory request. Knowing the address enables the modeling of important DRAM behaviors such as bank conﬂicts that affect the latency of individual requests. 4. OUR NEW METHODOLOGY In this section, we describe our new methodology to address the challenges identiﬁed in Section 3.2. Average Minimum Maximum bitonic 50.4% 30.6% 68.6% dct 49.2% 18.9% 68.6% histogram 57.0% 26.6% 68.6% matrixmul 65.1% 35.7% 69.0% spmv 63.8% 48.6% 68.6% comd 55.4% 22.0% 77.5% bfs 32.8% 19.2% 68.5% hotspot 59.1% 29.0% 86.4% nw 55.4% 8.61% 87.1% Table 1: Summary of average, minimum, and maximum percentage error when using a ﬁxed latency memory controller. 4.1 Modeling Complex Coherence Protocols The ﬁrst challenge of evaluating an APU using SynFull is to model the more complex coherence protocols in APUs. We address this by classifying different types of initiating messages that trigger similar reactive ﬂows into the same category. For example, a cache generates different types of coherence messages based on a cache line’s state, the ones that are bound to the same destination(s) with the same message size can be collapsed into a single message type to reduce the complexity of the coherence model. In total, we model 53 message types after grouping the 86 message types, which is still substantially more than in the original SynFull. 8 write 1 write 2 inv 3 inv 4 inv GPU L1 GPU L2 10 7 DIR. 9 CPU LLC CPU 6 5 Figure 8: Protocol depth for a GPU write request. The second challenge is tracking packet dependencies and modeling the protocol depth. SynFull assumes a simple oneto-one mapping between requests and responses, where a read request generates a forward request if data is cached on chip, and a write request generates invalidations if data is shared by multiple readers. In an APU with a combination of coherence protocols, a single initiating request can potentially result in multiple forward/invalidation requests. Furthermore, the generation of forward and invalidation packets in SynFull is based on forward and invalidation probabilities, while in a multi-level memory hierarchy, the probability at each level is independent of the others. Therefore, to model the transaction chain correctly, we introduce new forward/invalidation request types for each level of the memory hierarchy, considering their potentially distinctive behaviors. Consider the following example in which a write request initiates from a GPU core, while the CPU L1 cache has a copy of the same cache line. As shown in Fig. 8, the dashed arrow 1 is the initiating GPU write request. Because GPU caches are write-through, two requests are generated: an invalidation request 2 , and later on a write request with data 8 . The invalidation request is required because the GPU L2 and CPU caches are kept coherent. After receiving the invalidation, the APU directory notices that the CPU has a copy of the data. Therefore, the directory forwards this invalidation to the CPU LLC 3 , and the CPU LLC forwards the invalidation to CPU L1 cache 4 . Notice that the coherence transaction 3 is directly caused by 2 (but not caused 301     by 1 ), while transaction 4 is directly caused by 3 (but not caused by 2 ). Therefore a forwarding probability for each individual step is required; and messages in 2 and 3 are classiﬁed into different message types. Once the acknowledgments, denoted by 5 , 6 , and 7 , are received by the corresponding controllers, the GPU L2 is allowed to update the memory with data 8 . It is worth pointing out that the generation of write packet 8 is dependent on both 1 and 7 , which breaks the one-to-one request-response mapping assumption in SynFull. Such dependencies must be tracked properly so as to correctly model a complex APU protocol. Finally, the memory update acknowledgements are sent back from directory to GPU L2 9 , and from GPU L2 to GPU L1 10 . In summary, the differences between our model and the SynFull approach lie in the following aspects. First, we model a chain of transactions by introducing new packet classiﬁcations. Second, we introduce the necessary dependency tracking logic to handle the one-to-many and manyto-one mappings between requests and responses. 4.2 Deterministic Macrophase Replay The probability-based macro-level transition model can lead to inaccuracies as described in Section 3.2.2. To ensure that the high-level behavior of our simulation is consistent with the workload behavior, we introduce a deterministic macrophase replay methodology. This approach records the sequence of cluster IDs of each macrophase. Note that this is a very lightweight approach that stores a single number for every n cycles (n is the length of a macrophase), whereas a true trace-based approach would collect information on every network injection. We still use probability-based transitions between microphases. The only potential downside of this approach is the increase in simulation time in situations where the probability-based approach quickly converges on its Markov steady state behavior. For this reason, we reduce the simulation time by reducing the number of executed microphases. To achieve this, for each macrophase, the minimum number of microphases that needs to be executed to reach steady state is calculated within an error margin (we use 2%). The ﬁnalized number of microphases per macrophase is determined by selecting the largest numbers calculated from the previous step. For example, in a scenario where macrophase and microphase resolutions are 5M and 250 (the ratio is 20000), if our scripts determine that macrophases 1 and 2 should execute at least 2000 and obtain a simulation speedup close to 5× by executing only 4000 microphases to reach steady state, respectively, we can 4000 microphases per macrophase, instead of 20000. 4.3 Spectral Analysis for Interval Selection To automate the process of ﬁnding a good macrophase length, we make use of Fourier transform-based frequencydomain analysis. Our approach consists of ﬁve main steps. First, the application network trace is processed to generate the injection rate of initiating messages over time, as shown in Fig. 9a. Second, we convert this time-domain information to the frequency domain via a Fast Fourier Transform (FFT). Third, we remove the offset component, and then trim the second half of the FFT result (because the FFT always produces a symmetric/mirrored result). Fourth, as the generated FFT series consist of complex numbers, we calculate the absolute values of these numbers. Fig. 9b shows the absolute value of the ﬁrst 2500 frequency components for CoMD. The ﬁnal step is to calculate the period of injection rate using this information. In Fig. 9b, the component with the highest absolute value is the fourth component. This component represents a period of 450M cycles; this is in line with the period that can be observed in Fig. 9a. If this period is too large to be efﬁcient for hierarchical clustering, it can be divided into smaller periods that still align with the periodicity of the overall trace. As shown in Section 3.2.3, Hotspot is another application that shows a periodic injection rate behavior. Our FFT approach is able to automatically determine the period to be 1120K cycles. 0 0 0 0 0 5 2 0 0 0 0 0 5 1 0 0 0 0 0 5 0 e w r o P 0                  500             1000              1500 Cycles (Millions) 0 500 1000 1500 Frequency component 2000 2500 (a) The number of initiating mes(b) FFT coefﬁcients vs. sages in CoMD over time. quency components. Figure 9: Spectral analysis of CoMD. fre4.4 Bursty Injection Model Because SynFull has limitations in capturing bursty network behavior during GPU execution, we propose a new injection model that generates bursts similar to the real workload. Capturing temporal bursts: SynFull uniformly injects messages into the network during each micro cluster. The amount of trafﬁc is calculated based on a distribution of node injection rate, deﬁned as the number of messages injected in a microphase. In addition to this distribution, we introduce two more distributions. The ﬁrst one is a distribution of inter-arrival times between two consecutive bursts of initiating message injections. The second distribution collects the number of initiating messages generated in the same cycle. Note that the probability distribution functions of the interarrivals and bursts directly observed from the trace (i.e., not a mathematical function like an exponential). However, generating trafﬁc based on these two independent distributions might result in unrealistic trafﬁc patterns. Therefore, we collect the joint distribution of inter-arrival times and bursts. We generate a random inter-arrival time and burst pair from this joint distribution. First, based on the generated interarrival time, our injection model determines when a burst occurs. Then, based on the generated burst, the model determines how many initiating messages are injected at once. We continue generating initiating messages until the injection amount is reached, which is calculated by SynFull based on the injection rate distribution. This approach allows the number of generated messages per microphase to be similar to the real workload behavior, while capturing the bursty behaviors in the temporal dimension. Capturing spatial bursts: To limit the number of injecting nodes and the source-destination pairs, for each micro 302 cluster and initiating message type, we collect the distribution of the number of unique nodes that inject trafﬁc into the network in a microphase, and the number of unique sourcedestination pairs that observe trafﬁc. Based on these distributions, the trafﬁc generator generates a limit on the number of source nodes as well as the source-destination pairs for each microphase. Using these limits and the distributions that SynFull uses to generate source and destination nodes, APU-SynFull generates a set of source nodes and a set of source-destination pairs that can generate and receive trafﬁc in that microphase. This mechanism allows the generated messages per microphase to have a spatially bursty behavior, similar to what is observed in real executions. 4.5 Trace Extrapolation A key challenge to simulating large, heterogeneous SoCs is to take statistical trafﬁc data from a smaller system to accurately evaluate a larger system. We ﬁrst focus on strong scaling scenarios where an application’s problem size is ﬁxed and a larger system is used to ﬁnd a solution faster. We will later revisit weak scaling where the problem size increases with the compute resources. To model larger systems, we introduce a novel trace extrapolation methodology. Trace extrapolation is the process of taking traces collected on smaller systems and projecting the traces to a parametrically different system. The goal is to ensure that the extrapolated trace is representative of the NoC communication behavior of the target system. Trace extrapolation faces the following challenges. First, parallel programs exhibit nondeterminism. A parallel program in general has a large number of possible execution paths, resulting in different communication patterns. Even if the execution path is deterministic when the same input set is given, depending on how computation is allocated and scheduled, on-chip trafﬁc patterns vary for different systems. Second, runtime information is not available during trace extrapolation. For example, a cache miss generates a request or a directory miss causes a broadcast. However, neither cache miss nor directory miss information is available without a real simulation. Therefore, it is challenging to predict the timing and ﬂow of onchip communication in trace extrapolation. In this subsection, we present a methodology to generate synthetic traces for a larger system by extrapolating communication behavior from application traces collected on a series of smaller systems. The proposed mechanism considers three behavior characterizations: 1) injection ﬂow, which indicates where a message originates from and is destined to; 2) execution time, which indicates how long a kernel executes until completion; and 3) injection rate, which refers to the number of messages generated in an interval. Injection ﬂow extrapolation: Extrapolation of injection ﬂow involves projecting the source and destination nodes for each injection individually. To achieve this goal, we must know the injection distribution of all source nodes, as well as the receiving distribution of all destination nodes. Such distributions inform the probability of which individual nodes inject/receive messages. Assuming injection and receiving processes are independent of each other, a series of injection ﬂows can be constructed based on the probability. Both injection and receiving distributions can be obtained in the three steps described below. Step 1: Group the network nodes into a source cluster and a destination cluster. For example, consider GPU L1 cache miss events: all GPU L1 caches are grouped into one source cluster, while all GPU L2 caches fall into a destination cluster. Other nodes such as CPU caches and directory nodes, are not considered for this particular event. Similarly, for GPU L2 cache miss events, only GPU L2 cache nodes and directory nodes are considered for grouping. In general, communication taking place between different levels of the memory hierarchy can be extrapolated separately, which simpliﬁes the projection process. Step 2: Calculate the distribution across all nodes within each cluster. Speciﬁcally, the injection probability of each node is calculated in the source cluster, and the receiving probability is calculated for nodes in the destination cluster. Fig 10a shows the injection distribution of GPU L1 caches in a 32-CU system (sorted by injection rate), with the x-axis being a collection of GPU L1 cache nodes and the y-axis being the injection probability. A linear regression is applied to the distribution curve (regression trend line shown). The statistics collected from this step are the slope and intercept of the ﬁtted line. Step 3: Project distributions for the target system. Given a series of slope and intercept values for smaller systems, we apply another curve ﬁtting to project the slope and intercept for the target system. Then a distribution can be constructed accordingly. Fig 10b and Fig 10c demonstrate the curve ﬁtting process for slope and intercept. The projected distribution result is shown in Fig 10d. Four data points collected from 8-CU, 16-CU, 32-CU, and 64-CU systems are used to project the distribution of the target 128-CU system. To form an injection ﬂow, we probabilistically select a node from the source cluster and a node from the destination cluster following the projected distribution. Execution time extrapolation: Given the amount of trafﬁc injection, execution time impacts the overall network utilization, which in turn affects the network performance. The execution time of an application may or may not scale as the system size grows. The scaling factor is dependent on the parallelism of the application, as well as the hardware architecture. When extrapolating execution time, we assume all hardware resources scale proportionally (i.e., the compute unit to storage bandwidth ratio remains the same as the system scales). Our methodology is applicable to nonproportional scaling cases, but we have not yet evaluated these scenarios. Similar to the approach described in Step 3 of the injection ﬂow extrapolation, we project execution time for the target system by curve ﬁtting. A single GPGPU application exhibits multiple phases; some of the phases correspond to CPU execution while some of them correspond to different kernels. We differentiate these phases and extrapolate the execution time for each phase individually. The purpose is to separate scalable phases from non-scalable ones. During extrapolation, different scaling factors are applied to different phases, resulting in reasonable accuracy for overall execution time extrapolation. Injection rate extrapolation: The spatial distribution of on-chip communication is determined by injection ﬂow extrapolation, while the injection rate extrapolation is respon303 0.04 0.035 0.03 0.025 0.02 y t i l i b a b o r P n o i t c e n j I 0 4 8 12 16 20 24 GPU L1Cache nodes (a) Injection distribution. 0.0004 0.0003 0.0002 0.0001 e p o S l 28 32 0 0 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 t p e c r e t n I 80 0 20 40 60 Number of GPU L1Caches 20 40 60 Number of GPU L1Caches (b) Slope curve ﬁtting. (c) Intercept curve ﬁtting. Figure 10: Injection ﬂow extrapolation process. y t i l i I b a b o r P n o i t c e n j 0.01 0.009 0.008 0.007 0.006 0.005 Simulated Projected 80 0 50 100 Gpu L1Cache nodes  150 (d) Projection result. sible for deciding the temporal distribution of the trafﬁc injection. To extrapolate the injection rate, we use a probabilistic approach in our model. Each CPU/GPU phase is further divided into 1000-cycle epochs. Injection rate for the entire phase is assumed to follow a Weibull distribution. However, within each epoch, trafﬁc has a constant injection rate λ . Given the probabilistic (Weibull) distribution and the amount of packets injected for each phase, the trafﬁc injection amount for each epoch is calculated individually. Here we apply the same curve ﬁtting approach described above to extrapolate the parameters for the Weibull distribution. Compared to a deterministic approach where the injection rate is constant throughout the entire phase, this probabilistic approach is capable of capturing trafﬁc burstiness. Weak scaling: The strong scaling scenarios are somewhat more difﬁcult because the amount of work per compute resource tends to decrease, which affects cache, network, and memory behaviors. With weak scaling, the work-percompute resource remains constant, and so individual compute units in a larger system tend to continue to behave like the compute units from smaller systems. When extrapolating the injection rate in weak scaling, we make use of the same injection ﬂow methodology described above to generate the proper distribution of sources and destinations for the larger system. However, for injection rates, we simply select a smaller-sized system as a baseline and preserve the same injection rate (per source) in the extrapolated system. 4.6 Memory Using synthetic addresses, we want to capture memory characteristics during a SynFull phase without the need for a full address trace. Our address generation approach decomposes address traces into memory phases with a ﬁxed number of requests and analyzes those phases to produce synthetic addresses with similar memory request behavior. We observe that utilizing a bank access distribution (i.e., number of accesses to each unique bank), request ordering, and rowbuffer hit rate information on a per-phase basis can provide accurate modeling of synthetic access counts, row-buffer hit rates, and memory latencies. Exactly generating the same absolute addresses is not necessary so long as the result is that the ﬁnal DRAM bank access distributions, row-buffer hit rates, and overall memory latencies are similar to what would otherwise be observed with a full cycle-level simulation. The general process of creating this synthetic model is shown in Fig. 11. An input trace is ﬁrst binned into bank and channel pairs based on, for example, a typical memory address decoder or bank/channel interleaving function. The number of accesses to each destination are counted to generate a bank access distribution. We also consider the ordering of addresses by generating a stochastic matrix representing the probability t u p n I Distribution FFT Transition Matrix Hit Rate Statistics g n i r e t s l u C c i l t e e d h o n t M y S Figure 11: Overview of generation of synthetic model from address trace. of transitioning from one bank to another. This aims to capture bank conﬂicts more accurately by capturing bursts of requests to the same bank or lack thereof. Using this transition probability, we generate “resequenced” bank accesses containing a number of requests to each bank corresponding to the bank access distribution. At this point we can generate an intermediate trace by combining the new sequence and hit rates together. The previous row address is used when a row-buffer hit should occur while a random row address is selected otherwise. Addresses are encoded by reversing the binning function to create a synthetic address from the row addresses and bank/channel pairs. In order to reduce the phase count and provide the ability to extrapolate traces, clustering is used similar to original SynFull. To cluster phases together, we compare the similarity of bank access distributions, transition probabilities, and row-buffer hit rates after completing the steps above. For this, we return to a spectral analysis approach and use a clustering algorithm to group similar phases. Applying an FFT to the bank access distribution allows us to decompose aggregated memory requests into more simplistic periodic functions. From this point, we can choose a ﬁxed number of frequencies for the periodic functions that reduce the error on the bank distribution to represent the phase. This allows for signiﬁcant reduction in the number of dimensions when clustering phases and greatly simpliﬁes the process. 5. EVALUATION 5.1 Simulation Methodology We use an APU simulation platform consisting of gem5 [9] and a modiﬁed version of the GPU model [10] to collect traces for APU-SynFull model generation and to compare the accuracy of our models against the baseline full-cycle simulation. We use Garnet [11] to simulate the network. We use 2-stage routers; and each router input port has 4 virtual channels, with 8-ﬂit deep buffers. Our baseline system is shown in Fig. 1. We replace the CUs and the CPU core with SynFull models to evaluate APU-SynFull and SynFull. Our memory model utilizes the built-in gem5 model [12] modiﬁed with HBM timings [13]. There are 8 memory channels with 8 banks in each channel. We ignore the impact of refresh to focus on reducing modeling error considering all other timing parameters. We execute the ap304       Application bitonic [14] dct [14] histogram [14] matrixmul [14] spmv [15] Input size Application 262144 comd [18] 2048 backprop [16, 17] 1024 bfs [16, 17] 512 hotspot [16, 17] 256 nw [16, 17] Table 2: List of workloads. Input size 16 131072 65536 1024 2048 2048 10 plications listed in Table 2 from the AMD SDK [14], Open Dwarfs [15], Rodinia [16, 17] and Proxyapps [18] suites. We generate the trafﬁc models using a modiﬁed version of SynFull scripts. We use a microphase length of 250 for all applications. For non-periodic applications, we use a macrophase length of 5M in order to keep the ratio between these resolutions high. A higher ratio increases accuracy, and also provides more scope for increasing simulation speed (see Section 4.2). However, it also causes clustering to take very long, and becomes impractical. CoMD and Hotspot use the automated macrophase length generation methodology (Section 4.3) and use macrophase lengths of 7.03M and 560K, respectively. We report three metrics to evaluate accuracy. We use average network latency for an overview of the trafﬁc behavior. We also compare the latency and initiating message type distributions that APU-SynFull yields with those of the baseline. Comparing the latency distributions is useful to determine if congestion is accurately modelled by APU-SynFull. We use the Hellinger Distance deﬁned in Equation 1 to calculate the similarity between two distributions, where P and Q are two discrete distributions (in our case, packet latency distributions or the initiating message type distributions), and pi and qi are the it h element of P and Q, respectively. H (P, Q) = 1√ 2 (cid:2)(cid:3)(cid:3)(cid:4) k∑ i=1 ( √ pi − √ qi )2 (1) 5.2 APU-SynFull Results In this section, we evaluate our APU-SynFull methodology. Fig. 12a shows the ratio between the number of injected messages for four different initiating message types for matrixmul, with APU-SynFull and the baseline. Fig. 12b shows the same for hotspot. APU-SynFull is successful in generating the initiating messages with a distribution close to that of the baseline. The overall accuracy of initiating message distributions for all applications are given in Fig. 14b. 0% 20% 40% 60% 80% 100% APU-SynFull Baseline D i r t s i u b i t n s e g o a s n s o e f i i t i a i t g n m CPU_READ GPU_WRITE GPU_READ GPU_L2_EVICT (a) The ratio of different initiating (b) The ratio of different initiating messages in matrixmul. messages in hotspot. Figure 12: Accuracy of generated initiating messages. 0% 20% 40% 60% 80% 100% APU-SynFull Baseline D i r t s i u b i t n s e g o a s n s o e f i i t i a i t g n m CPU_READ GPU_READ GPU_WRITE Next, we describe the simulation accuracy of each benchmark in detail. Fig. 13 shows the error in average network latency with SynFull and APU-SynFull. Fig. 14a shows the Hellinger distance for SynFull and APU-SynFull latency distributions compared to the baseline. Fig. 14b shows the 60% 50% 40% 30% 20% 10% 0% y e e c e n n g e e a g a n a e r c e r v e a P t r r t o o r r i n k w l t SynFull APU-SynFull Figure 13: Percentage error in average network latency with respect to the baseline. 0 0.1 0.2 0.3 0.4 0.5 0.6 H e i l l d d e g n i r t i s r i t s n u a b o n e o i c t f s l a t n e y c SynFull APU-SynFull (a) Latency distributions. 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 H e i l l n a d t e s g d a r s e s g e n i u o b i e r c t s f i i t i t a s n o n i t i g n m i SynFull APU-SynFull (b) Initiating message distributions. Figure 14: Hellinger distance for SynFull and APU-SynFull compared to the baseline. Hellinger distance for SynFull and APU-SynFull initiating message distributions compared to the baseline. The lower the distance, the more similar the distributions are. • bitonic: bitonic is a very bursty application (it is the application with the highest network latency in our suite), and SynFull fails to generate trafﬁc with high enough latency, mainly due to its inability to capture the spatial bursts. The average latency with APU-SynFull is very close to the baseline. Moreover, the latency and the initiating message distributions are close to the baseline as well. • dct: Although the Hellinger distance of initiating message distribution is low, the total number of generated GPU read and write requests is lower compared to the baseline. This is mainly due to clustering, and thus results in low latency distribution accuracy for both SynFull and APU-SynFull. However, APU-SynFull performs slightly better than SynFull due to its better injection model. • histogram: This is a short running application with a very short GPU phase. Due to this, clustering is not very effective in representing all phases of the application, leading to inaccurate initiating message distribution. This causes both SynFull and APU-SynFull to have inaccurate latency distributions. SynFull also yields lower latency than the baseline, mainly due to the inaccurate number of simulated CPU and GPU phases. • matrixmul: SynFull yields low average latency due to its uniform injection model. APU-SynFull captures the bursty behavior for the GPU requests, both temporally and spatially. However, in this application, many cache lines are requested by multiple GPU cores simultaneously. Once such a requested cache line reaches the GPU L2 cache, it is replicated and sent to its requesters, causing a burst in the GPU reply network. APU-SynFull is currently unable to emulate this behavior, resulting in slightly lower latency for the GPU reply network than that of the baseline. Latency and message distributions are reasonably accurate. • spmv: This application, similar to histogram, is short running with a very short GPU phase, causing clustering to be 305                                       ineffective in representing the whole execution. The initiating message and latency distributions are not accurate. The signiﬁcant improvement in average latency with APUSynFull over SynFull is attributed to its injection model. • CoMD: This application uses our automated macrophase generation methodology. SynFull suffers from very inaccurate initiating message distribution because of its probability-based transition model. Using the macrophase replay method provides much better message distribution and better latency, and APU-SynFull provides signiﬁcantly more accurate latency results. • backprop: This is a bursty application, and SynFull fails to generate representative network trafﬁc. APU-SynFull provides very accurate latency distribution and average latency. Its initiating message distribution is less accurate due to CPU requests. Because CPU requests have much lower latency impact compared to GPU requests, this inaccuracy in initiating message distribution does not impact overall la• BFS: APU-SynFull is accurate in average network latency, tency signiﬁcantly. latency distribution, and the initiating message distribution, and is more accurate than SynFull due to both its ability to • hotspot: This application uses our automated macrophase capture temporal bursts and the macrophase replay model. generation methodology, and provides good initiating message distribution. It demonstrates high spatial bursts, and SynFull is unable to capture it, causing it to be very inaccurate. APU-SynFull, due to its improved injection model, provides better latency. • nw: Due to the problem described in Section 3.2.2, the probability-based transition model in SynFull fails to generate a GPU phase, and thus generates very low network trafﬁc. Although this is a low-trafﬁc application, SynFull latency is still far from that of the baseline. Overall, APUSynFull provides reasonable accuracy in terms of initiating message and latency distributions. These results demonstrate that the trafﬁc generated by APU-SynFull resembles the trafﬁc of real APU workloads. Moreover, average network latency is within 11% of the system emulation, outperforming SynFull in terms of accuracy. Fig. 15a shows the simulation speedup obtained by APUSynFull. Applications such as bitonic, matrixmul, CoMD and hotspot obtain signiﬁcant speedups. Short running applications such as histogram, spmv, and backprop do not beneﬁt signiﬁcantly in terms of simulation speedup. In dct, we determine that the ratio of macrophase length to microphase length should be high. This does not allow discarding the execution of some microphases, limiting simulation of ten applications by 4.5×, on average (geometric mean). speedup. Overall, APU-SynFull reduces the simulation time Fig. 15b shows the sensitivity of APU-SynFull to the choice of macrophase and microphase lengths, using four representative applications. We change the macrophase and microphase lengths, while keeping the ratio between them constant. We do so to ensure that all applications execute enough microphases in a macrophase to reach the steadystate condition. The legend shows the choice of macrophase and microphase lengths, respectively. Although using the joint distribution of inter-arrival times and injection bursts reduces the sensitivity to phase lengths, augmenting the in50 40 30 20 10 0 S i m a u l i t p u d e e p S n o (a) Simulation speedup with APU-SynFull. 0.6 0.8 1.0 1.2 1.4 bitonic bfs nw comd n e S s i l t i y g n v e t i t t e s a h p o h s 2M_100 5M_250 10M_500 15M_750 (b) Sensitivity of average network latency to macrophase and microphase lengths. Figure 15: Simulation speedup and phase length sensitivity. jection model with the ability to capture spatial bursts introduces a slight dependency to the choice of phase lengths. The maximum observed discrepancy is 20%. 5.3 Scaling Results In this section, we validate our scaling methodology with a 128-core system in the context of strong scaling. We also present a use case of NoC design space exploration with weak scaling. Three representative applications are considered. Histogram is a short running application with a lightweight kernel, where CPU and GPU trafﬁc injection volumes are comparable. Matrixmul, a highly parallel application, whose performance scales well as core count grows. BFS which is an application with bursty trafﬁc injection during kernel execution. 100% 80% 60% 40% 20% 0% d e a c S l s a B e i l e n d e a c S l s a B e i l e n d e a c S l s a B e i l e n histogram matrixmul bfs D i r t s i u b t i n e g o a n s s o e f i i t i a t i g n m s CPU_READ GPU_WRITE CPU_WRITE GPU_L2_EVICT GPU_READ (a) Initiating message distribution. 0 10 20 30 40 50 60 d e a c S l s a B e i l e n d e a c S l s a B e i l e n d e a c S l s a B e i l e n histogram matrixmul bfs A e v r e n e g a t w o r k l a t n e y c (b) Average network latency. Figure 16: Accuracy of strong scaling in 128-CU system. For strong scaling validation, an extrapolated trace for a 128-CU system is generated based on our trace extrapolation methodology. Then we generate a model ﬁle and use it for APU-SynFull simulation (referred to as Scaled in this evaluation). In comparison, we launch a full-cycle simulation with the same conﬁguration (referred to as Baseline). Fig. 16a shows the initiating message distribution comparison. In histogram, the scaled system generates more CPU trafﬁc than GPU trafﬁc, which is also observed in the baseline. In strong scaling, the amount of on-chip storage (i.e., L1 caches and L2 caches) increases as system size grows. Therefore, a lower cache miss rate is expected in a larger system for matrixmul. However, our trace extrapolation methodology currently does not account for runtime information such as cache miss rates, thus it ends up generating more requests than necessary. The cause of inaccuracy in ratio is multifaceted: the algorithm, cache thrashing, address mapping, and use of local memory can all simultaneously impact the generation of read/write requests. BFS has an injection spike after kernel launch, but the scaling factor of initiating messages is relatively constant compared to the other two applications. The proposed scaling methodology is able to project the trafﬁc injections accurately. Average network latency is compared in Fig. 16b. Packet latency is 306                   k r o w t y e c n n e e g a a t l r e v A 16B_2VC 16B_4VC 32B_4VC 60 40 20 0 Fixed Latency APU-SynFull 80% 70% 60% 50% 40% 30% 20% 10% 0% Figure 17: NoC exploration with weak scaling in 128-CU system. histogram matrixmul bfs related to network congestion. While network congestion varies during runtime depending on both application phases and on-chip hardware resources, our methodology is able to reasonably mimic the congestion behavior. We believe better accuracy can be achieved if detailed runtime information is provided for trace extrapolation. It is hard to validate weak scaling accuracy because ﬁnding an application whose computation and storage complexity scales by the same factor is difﬁcult. In addition, current applications may not scale well to large system sizes. Our weak scaling approach provides a synthetic evaluation environment that allows one to project how applications might behave on future systems in the absence of applications that are well-tuned for such large SoCs. Instead, we present how weak scaling can be used for NoC design space exploration in Fig. 17. In this experiment, we vary the channel width (16-Byte/32-Byte) as well as the number of virtual channels (2-VC/4-VC) using model ﬁles generated from extrapolated weak scaling traces. Virtual channels impact the network congestion while channel width affects the number of ﬂits in a data packet. From the results, we can tell that both matrixmul and BFS are sensitive to NoC bandwidth variation. By further sweeping through the network parameters, we will be able to ﬁnd out a combination of parameters that satisﬁes the design requirement. In summary, weak scaling enables NoC design exploration for future applications. 5.4 Memory Results Our memory results focus on reduction of average read, write, and hit rate error over a ﬁxed latency model. The baseline for our results is the memory latency and row-buffer hit rate from gem5’s built-in memory model when running in cycle-level system emulation mode. Latency error and standard deviation across all applications is shown in Fig. 18. Overall, the error in memory latency is reduced by about 37% compared to a ﬁxed-latency model, which provides a substantial improvement, although we readily admit that there is more that can be done. The observed row-buffer hit rates of our synthetic address models were also within 12% of the cycle-level model. Sources of remaining error include read/write interleaving in each phase, FFT component selection, and transition matrix clustering. Interleaving of writes with reads typically resulted in the largest latency error. Write times and addresses are highly dependent on higher-level memory replacement policy, special requests such as GPU read-modify-writes, and memory controller optimizations such as write buffering. For example, our memory system models separate read/write queues, queue is drained. In BFS this resulted in up to 10× increased the rate at which the write queue ﬁlls will impact when the latency error during heavy write phases. By modeling a simFigure 18: Memory latency error and deviation comparing ﬁxedlatency SynFull and APU-SynFull model to the baseline. ple, single queue controller, latency error in BFS is reduce to at most 50%. In address generation, using a subset of FFT components described in Section 4.6 is inherently lossy and results in differences between input bank distribution and synthetic model bank distribution. Clustering phases together based on similar FFT components produces additional errors, as the cluster center may be averaged to a different subset of frequency components. However, across the entire application run, we observed negligible difference in latency error using all 64 FFT components compared to as little as 5-10 components when selected by largest magnitude. Similar results were observed for the transition matrix, which was introduced to model bank conﬂicts by ordering synthetic addresses more similar to the input ordering compared to random sampling. A small amount of information can be used to replay addresses without much variation in accuracy. For example, average transition probability can be used to determine if a distribution of addresses are sequential or random. We found in most cases the transition matrix does not provide enough beneﬁt to justify increasing the complexity of clustering. The introduction of address generation provides the ability to utilize existing memory models within the APUSynFull framework. These results show that synthetic approaches are promising and ﬁxed-latency memory models should not be used. 6. RELATED WORK Modeling and simulation. As discussed in Section 2, the most closely related work to ours is SynFull [7]. gem5gpu [19] combines gem5 [9] and GPGPU-Sim [20] to model a ﬂexible and cache-coherent [21] APU-like system. Macsim [22] simulates the network trafﬁc of multiprogrammed CPU and GPU workloads, but does not model a uniﬁed memory address space and cache coherence. Multi2Sim [23] models both CPU and GPU cores but with distinct memory hierarchies. Several network-on-chip simulators [24, 25, 26] provide timing and power analysis of NoCs. Recent works that use DSLs provide both hardware and software model generator tools for NoCs [27], and CMPs [28]. Our work is the only work that provides a fast simulation methodology for cache-coherent APUs, and enables scalability studies for larger future systems. A broad range of memory models have developed in the past for use in simulation. Most work focuses on detailed cycle-level models rather than ﬁxed-latency of queuing model studies. DRAMSim2 [29] and USIMM [30] are two popular simulators. DrSim [31] extends DRAMSim2 to provide ﬂexibility. Later simulators such as gem5’s model [12], NVMain [32], 307     and Ramulator [33] aim to provide ﬂexibility and extendibility for newer memories such as NVM and die-stacked memory, and standards such as HBM [13] and HMC. Synthetic and statistical models. Prior works [34, 35] investigate generating synthetic workloads that represent application behavior. Wunderlich et al. [36] propose methodologies that use statistical sampling to increase simulation speed. Several works [37, 38] use synthetic trafﬁc models in the context of NoC simulation methodologies. Eeckhout et al. [39] use statistical methods to generate statistically correct synthetic benchmark traces. Bighouse simulator [40] uses stochastic modeling to simulate power, performance and reliability of data centers. To our knowledge, our work is the only statistical method that generates synthetic application trafﬁc for APUs. Workload cloning allows synthetic workloads to be generated and released from proprietary ones [41, 42, 43]. Speciﬁcally, work focusing on replicating cache behavior in workload clones has been explored [42]; they analyze the cache statistics needed to accurately capture memory access behavior. In our work, we take a similar approach in identifying memory access characteristics that are needed to synthetically generate a memory address stream. Spatio-temporal memory cloning (STM) [43] is a methodology to accurately generate clones of memory access patterns. STM focuses on caches and TLB behavior using a variety of stride patterns while we capture memory behavior such as bank conﬂicts and row buffer hit rates. 7. CONCLUSIONS In this work, we propose novel extensions to the SynFull methodology to tackle challenges of large scale heterogeneous computing systems. With this new methodology, computer architecture researchers can now explore NoC and memory designs at scale without being bogged down by otherwise slow or unscalable simulations. Overall, APUSynFull is a robust evaluation methodology targeting NoCs and memory systems for future large scale heterogeneous SoCs that will be of great value to the architecture community looking forward. Acknowledgment AMD, the AMD Arrow logo, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identiﬁcation purposes only and may be trademarks of their respective companies. 8. "
2017,Design and Evaluation of AWGR-Based Photonic NoC Architectures for 2.5D Integrated High Performance Computing Systems.,"In future performance improvement of the basic building block of supercomputers has to come through increased integration enabled by 3D (vertical) and 2.5D (horizontal) die-stacking. But to take advantage of this integration we need an interconnection network between the memory and compute die that not only can provide an order of magnitude higher bandwidth but also consume an order of magnitude less power than today's state of the art electronic interconnects. We show how Arrayed Waveguide Grating Router-based photonic interconnects implemented on the silicon interposer can be used to realize a 16 × 16 photonic Network-on-Chip (NoC) with a bisection bandwidth of 16 Tb/s. We propose a baseline network, which consumes 2.57 pJ/bit assuming 100% utilization. We show that the power is dominated by the electro-optical interface of the transmitter, which can be reduced by a more aggressive design that improves the energy per bit to 0.454 pJ/bit at 100% utilization. Compared to recently proposed interposer-based electrical NoC's we show an average performance improvement of 25% on the PARSEC benchmark suite on a 64-core system using the Gem5 simulation framework.","2017 IEEE International Symposium on High Performance Computer Architecture Design and Evaluation of AWGR-based Photonic NoC Architectures for 2.5D Integrated High Performance Computing Systems Paolo Grani, Roberto Proietti, Venkatesh Akella, S. J. Ben Yoo Department of Electrical and Computer Engineering, University of California, Davis, CA, USA Email: pgrani,rproietti,akella,sbyoo@ucdavis.edu Abstract—In future performance improvement of the basic building block of supercomputers has to come through increased integration enabled by 3D (vertical) and 2.5D (horizontal) die-stacking. But to take advantage of this integration we need an interconnection network between the memory and compute die that not only can provide an order of magnitude higher bandwidth but also consume an order of magnitude less power than today’s state of the art electronic interconnects. We show how Arrayed Waveguide Grating Router-based photonic interconnects implemented on the silicon interposer can be used to realize a 16 × 16 photonic Network-on-Chip (NoC) with a bisection bandwidth of 16 Tb/s. We propose a baseline network, which consumes 2.57 pJ/bit assuming 100% utilization. We show that the power is dominated by the electro-optical interface of the transmitter, which can be reduced by a more aggressive design that improves the energy per bit to 0.454 pJ/bit at 100% utilization. Compared to recently proposed interposer-based electrical NoC’s we show an average performance improvement of 25% on the PARSEC benchmark suite on a 64-core system using the Gem5 simulation framework. I . IN TRODUC T ION It is becoming increasingly clear that performance improvement through technology scaling (the so-called Moore’s law) is slowing down if not coming to an end. What this means is that performance has to improve by increasing the level of integration. Much like we moved from discrete transistors to Large Scale Integration (LSI) and later on Very Large Scale Integration (VLSI) chips in the late 1970s, we are seeing the emergence of a new type of chip that consists of a set of vertically stacked die (3D integration) connected to each other using a silicon interposer (2.5D integration). 3D/2.5D integrated systems have many crucial advantages such as the ability to provide signiﬁcantly higher memory bandwidth and memory capacity without going off-chip, the ability to integrate heterogeneous accelerators/co-processors and non-volatile memory, and the ability to realize a chipscale multiprocessor with many smaller die in different technologies to improve yield and lower cost [1]. However, there are several challenges with 3D/2.5D integration that need to be addressed. Loh et al. [2] describe the requirements of an interposer-based integrated system that consists of four stacks of High Bandwidth Memory (HBM), [3]) connected to a 64-core processor organized either as a single die or defragmented into four smaller dies housing 16 cores each. Each HBM stack is a 4-layer vertically integrated 2 Gb DRAM die with a 1024 bit interface operating at 1 Gb/s for a total I/O bandwidth of 128 GB/s (in second generation HBM systems, such as from Hynix, the bandwidth can be as high as 256 GB/s [4]). Since the stacked DRAM capacity is limited (nowadays just 1 GB stack), Loh et al. [2] assume the chip has four additional channels of conventional DDR4 operating at 2666 MHz for a total bandwidth of 83 GB/s. The key requirement of the connection between the processor and memory is, in this case, a bisection bandwidth of roughly 298 GB/s [2] for adversarial trafﬁc pattern and about 148 GB/s for uniform trafﬁc pattern. A more futuristic compute node suitable for Exascale computing is described by AMD researchers in [5], where each node is a 2.5D/3D integrated system with 32 processor cores, a large GPU, connected to about eight stacks of 8-layer DRAM, with each stack providing about 16 GB capacity and 1 Tb/s HBM interface as before. Each node is expected to provide a computational capacity of 10 TFlops. This is along the lines of the target set by DoE except that the DoE requirement 1 calls for 1 TB of memory and a memory-access bandwidth of about 32 Tb/s. The ﬁrst challenge is how to develop the interconnection network for 2.5D integrated systems that can provide a bandwidth that is more than an order of magnitude higher than what is possible today. The related challenge is how to provide such a high bandwidth at a power consumption that is practical. This is important because not only are the 3D die-stacks more susceptible to thermal issues, but also, power budget of the chip has to be shared by the processors, memory, accelerators, and the inter-die interconnection network. If more power is spent in the interconnection between the die, then there is less power available to do the actual computation. Indeed, the overall power budget for the chip is still set by the Thermal Design Power (TDP) and is unlikely to be much different from today. So, the energy per bit of the interconnection network has to improve signiﬁcantly (by an order of magnitude or more) for 2.5D integration to actually deliver the improvement in performance. Even with 8 GB or 16 GB per memory stack in the future, scaling a system to 1 TB or more to meet the Exascale computing requirement, 1 FastForward 2 R&D Draft Statement of Work, LLNL-PROP-652542, Lawrence Livermore Nat’l Lab., https://asc.llnl.gov/fastforward/rfp/04 DraftSOW 04-03-2014.pdf report 2014; 2378-203X/17 $31.00 © 2017 IEEE DOI 10.1109/HPCA.2017.17 289 means that the interconnection network has to span multiple chips, so we need an interconnection network that can scale with the same energy efﬁciency to tens of chips. We argue that photonics is a compelling solution to address the challenges outlined above for the following reasons. In conventional electrical signaling, data is transported by charging and discharging capacitance of an electrical wire, which is not only extremely wasteful, but also scales poorly with distance. Optical communication completely avoid this by using the photo-electric effect (to transport data), which allows the generation of a large voltage in a detector with very little energy (the so-called quantum impedance conversion [6]). This results in exceptional energy efﬁciency, especially when the distance is long as is the case with 2.5D integrated systems. With an optical link it is possible to modulate the signals at very high rates, and transmit data in parallel without interference using different wavelengths (Wavelength Division Multiplexing, WDM), which can be harnessed to create interconnection networks with low latency and high bandwidth density (bandwidth per unit area) to meet the challenges listed above. Moreover, over the past decade, there has been signiﬁcant progress in silicon photonics with the development and experimental demonstration of efﬁcient modulators, switches, receivers, waveguides, and lasers (see [7], [8], [9], [10]), which has resulted in better understanding of the design space tradeoffs of photonic networks (see [11], [12], [13]) which can help us develop cost-effective photonics NoCs to meet the requirements of interposer-based systems. There has been a signiﬁcant amount of prior work in intradie photonic interconnection networks (see [11], [13], [14]) but the design tradeoffs for intra-die networks is different from an interposer-based inter-die one. First, the interposer is a large separate die almost 900 mm2 in area that can be exclusively used for the interconnection. Therefore, fully connected topologies and switch fabrics that use a large number of waveguides can be used, which is not the case with an intra-die network where the optical interconnects share the die area with the rest of the processor electronics. Second, as opposed to an ad-hoc network that is closely coupled to the processors, for interposer-based systems we need a ﬂexible, free-standing, general purpose switch fabric that can be used to interface CPUs, GPUs, HBM memory stacks, and other on-chip accelerators (heterogeneous architectures). Third, the network should be cost-effective and practical in the near term (as the 7 nm technology node is just a couple of generations away and there are no easy solutions to scale CMOS beyond that) which means it should be easy to fabricate and not rely on technologies that are speculative, like architectures exploiting WDM values higher than 64, and with hundreds of thousands of microrings like some of the intra-die networks proposed in literature(see [9], [15]). Keeping these issues in mind we propose to use an Arrayed Waveguide Grating Router (AWGR) as the optical switch fabric to construct a photonic NoC that is suitable for interposer-based implementation. The AWGR (see [16], [17]) is a passive optical crossconnect that is realized using two star couples connected by waveguides of unequal length. The functionality of a diffraction grating is obtained by letting the length of the waveguides increase linearly. In this router, when every input port carries the same set of optical wavelengths, each output port will receive the set of wavelengths with each wavelength coming from a different input. This provides, in the strict sense, a nonblocking cross connect. Though an AWGR is a mature technology with widespread use in telecommunication applications, it was not considered for intra-die photonic interconnects because its area was still quite large and it was hard to scale it to a very large number of ports. However, recent developments in fabrication (described in Section II) have made it possible to implement extremely compact AWGRs (which takes a few mm2 ); moreover, for interposer-based NoCs, we do not need AWGRs with a very large number of ports, because the number of die on an interposer is not going to be arbitrarily large (in the order of tens). Therefore, it is time to evaluate the potential Speciﬁcally, we design and evaluate a 16 × 16 photonic of AWGR-based photonic NoCs in computing applications. NoC that can connect eight compute dies and eight vertically stacked DRAM dies, based on the High Bandwidth Memory (HBM) standard. Though AWGR is capable of implementing all-to-all, fully connected network, we deploy it as a crossbar with an all-optical control plane for arbitration. The network provides a total bisection bandwidth of 16 Tb/s. The main contributions of the paper as follows: (1) We show how Free Spectral Range (FSR) and parallelism can exploited to realize a multi-bit AWGR-based interposer photonic network that can be used to interconnect 16 dies on a chip with a 16 Tb/s crossbar topology. (2) We identify the electrical/optical interface (SERializer-DESerializer, SERDES) as the main contributor to the latency/power consumption, and provide an optimized a network that reduces the SERDES requirements by a factor of 6 in terms of energy consumption and by a factor of 3 in terms of latency. (3) We evaluate the performance of the proposed networks on PARSEC benchmarks by modeling it in the Gem5 simuimprovement of ∼25% compared to the state-of-the-art lation environment and demonstrate an average performance electrical interposer-based NoCs proposed in recent research literature [1]. (4) We show that the high bandwidth and energy efﬁciency of photonic NoCs could be exploited to reduce the cache sizes of the processors which not only frees up die area to add more cores or accelerators, but also, helps in reducing the overall power consumption/energy efﬁciency of 2.5D/3D integrated compute nodes. The rest of the paper is organized as follows. We start with the detailed discussion of the enabling technology, the AWGR and the design space of multi-bit AWGRs, 290 (a) shows the Wavelength Routing Property in a 5 × 5 AWGR - input 1 uses λ2 , input 2 uses λ1 , input 3 uses λ5 , input 4 uses λ4 , and Figure 1. input 5 uses λ3 to go to output a as described in the wavelength assignment table (b). m, m+1 and m-1 denote the current, the next, and the previous Free Spectral Range (FSR). (c) shows the realized layout of an 8 × 8 AWGR. including the implementation issues in Section II. Then we will show how to use the AWGR to construct the photonic NoC architecture in Section III. This is followed by the performance analysis and power analysis in Section IV and the discussion of the results in Section V. We conclude the paper with an overview of related work and future work. I I . ENAB L ING T ECHNO LOGY In this section, we will describe the key enabling technology underlying the proposed photonic interconnect architecture. Given that the number of dies on a chip in a 2.5D/3D is going to be relatively small (in the tens, not hundreds), we believe that a topology that connects the computational units and memory directly without intermediate routers, is the key. We propose to use Arrayed Waveguide Grating Router (AWGR) to implement a crossbar topology. AWGR is a mature technology that has been used in the telecom industry for many years [16] and it is possible to make compact chip-scale AWGR with as many as 512 ports, with an 8 × 8 AWGR consuming only about 1 mm2 , as experimentally demonstrated in our laboratories [18], [19]. To the best of our knowledge, this is the ﬁrst time a chip-scale AWGR is being proposed for an intra-die interconnect. A. AWGR Principle of Operation Figure 1 illustrates that the routing property of an AWGR allows any input port to communicate with any output port simultaneously using different wavelengths. As the wavelength assignment table in Figure 1(b) shows, input 1 uses wavelength λ2 to communicate with output a, wavelength λ1 to communicate with output b, wavelength λ5 to communicate with output c, wavelength λ4 to communicate with e. Thus, a passive N × N AWGR intrinsically provides output d, and wavelength λ3 to communicate with output simultaneous, all-to-all communication capability (a fully connected topology) as long as each input is equipped with N transmitters and N receivers, which is prohibitively expensive and unnecessary most of the time. Hence, we restrict the topology to a crossbar by allowing the connection between one input-output at any given time. However, since multiple inputs might seek to connect to the same output, some mechanisms for contention resolution are needed at the receiver side, as in any crossbar topology. One interesting feature of AWGR operation is that the wavelength routing is cyclic with the period, being called the Free Spectral Range (FSR), that we will be denoted in the following by the symbol Δ. This means that a given output j can be reached from input i using wavelength λij±kΔ , where k is an integer. B. Multibit AWGR AWGR, as described so far, is a single bit device, which means that at a given time a single bit can be transmitted from an input port i to an output port j by modulating λij using a modulation scheme like simple ON/OFF keying. In computing application, we need to be able to transmit multiple bits in parallel which introduces additional complexity and design tradeoffs especially in terms of power consumption and area. There are several possibilities. The simplest option is to use Time-Division Multiplexing (TDM) by using m times higher data rate to send m bits at a time. However, this requires the electronic interface including the SERDES to operate at a much higher rate, which increases the power consumption. The next option is to take advantage of the FSR to send k bits simultaneous to an output. In the example above, if k=4, input 1 could use wavelengths λ2 , λ2+Δ , λ2+2Δ , λ2+3Δ , to send four bits to output a. However, this means that the tunable range of the laser is k times larger, or we need k times more lasers, which could be a challenge. Another option is to use more advanced modulation schemes such as Quadrature Amplitude Modulation (QAM) where multiple bits could be encoded by the quadrature and the phase of the optical signals using multiple levels. However, this requires very complex receivers and transmitters that are not appropriate for on-chip photonic networks. The ﬁnal option is to use Space Division Multiplexing (SDM) or multiple AWGRs in parallel with each AWGR transmitting 291 Technology Silica PLC SiNx /SiON or SiNx /SiO2 PICs SiP Size Large (e.g., 40 mm × 40 mm for 8 × 8 100 GHz spacing) Medium (e.g., 4 mm × 4 mm for 8 × 8 100 GHz spacing) Small (e.g., 1 mm × 1 mm for 8 × 8 100 GHz spacing) Loss Crosstalk Low (e.g., 4 dB for 8 × 8) Low (e.g., -27 dB for 8 × 8) Medium (e.g., 5 dB for 8 × 8) Medium (e.g., -20 dB for 8 × 8) High (e.g., 7 dB for 8 × 8) High (e.g., -12 dB for 8 × 8) AWGR IM P L EM EN TAT ION O P T ION S AND TRAD EO FF S . Table I one or more bits using the ﬁrst three approaches. This is the bit-slice approach, which has been used since the early days of hardware. Each AWGR is responsible for transmitting a particular subset of contiguous bits of the ﬂit or memory word. However, wiring the multiple AWGRs together, especially when the number of ports of the AWGR (N ) is large, is challenging because of the extremely large number of waveguide crossing that increases the optical loss and crosstalk. Furthermore, if there are p AWGRs in parallel the number of lasers per node increases by a factor of p. We propose to use only a single set of lasers for all the p parallel AWGRs with splitters, if necessary. We investigate using fewer AWGR slices in parallel but each operating at a much higher data rate (32 Gb/s or so). As far as dealing with laying out multiple AWGR slices (a 2D planar layout on the interposer), given the relative large area of the interposer, we think that it is possible to layout the AWGRs on the same plane. Indeed, the AWGR chips are very small (e.g., in the order of 1 mm2 , as shown in Table I). Using a planar layout, the 16 chiplets must be integrated in the same plane. From [1] a 16-core CPU requires ∼75 mm2 . From [3] a HBM stack module occupies ∼35 mm2 . Therefore the total footprint required to place the 8 CPU and 8 HBM modules, place and route, is ∼1000 mm2 . This according to [1] is a the required AWGR chips plus space for the waveguide well established value for silicon interposer size. According to TSMC 2 projects, even bigger silicon interposer with an area of 1200 mm2 can be realized. C. Implementation Issues Tunable lasers and AWGRs are practical technologies that can be implemented today in many commercial foundries such as NeoPhotonics, Enablence, NEL, LioNIX, AIMPhotonics, and IME. Table I summarizes the different implementation options for an AWGR and the design tradeoffs. Silicon Photonics (SiP) offers the smallest footprint but optical losses and crosstalk are relatively high but it has the additional beneﬁt that low-power modulators and detectors can be integrated with the AWGR. The design and fabrication of 512 × 512, 25-GHz AWGR with a channel spacing of 25 GHz and a footprint of 11 mm × 16 mm is demonstrated in [18] and a 16 port AWGR with 50 GHz channel spacing on a Silicon Nitride (SiN) platform exhibiting signiﬁcantly 2 http://www.eetimes.com/document.asp?doc id=1329217 &print=yes lower loss was demonstrated in [20]. The 16 port AWGR is quite compact (just 3.7 mm × 0.7 mm) and is closer to what we need in terms of a building block for the proposed interconnection network in this paper. I I I . IN T ER PO S ER -BA SED PHOTON IC ARCH I T EC TUR E In this section we describe a 16 × 16 network that uses a crossbar topology realized using a multi-bit AWGR described before. The main advantage of using an AWGRbased interconnection is that to have the possibility to ﬂat the topology, achieving a lower number of hops between the dies. The network has four key design parameters. N is the number of nodes (that correspond to the number of dies on the chip where a die could be a vertically integrated die stack corresponding to a processor, GPU, hardware accelerator or HBM memory), k is the number of FSRs used, p is the number of AWGR slices used to realize a multi-bit datapath, and L is the line rate of each port. A. Implementation Figure 2(a) shows the high-level implementation of the optical NoC that we call Baseline. Here we assume N=16, k=4, p=8, and L=2 Tb/s corresponding to HBM2 memory standard speciﬁcation [4]. The crossbar network has a total bandwidth of 16 Tb/s as every node can be communicating with another (disjoint) node concurrently. Figure 2(b) shows the details of the electro/optical interface for each port. Note that there are eight transmitters (dark circles) since p=8, one for each slide of the multi-bit AWGR and similarly eight receivers (gray circles). We assume that processing nodes such as GPUs, CPUs, etc, have 256 I/Os operating at 4 GHz to realize the 1 Tb/s bandwidth in each direction, for a total bidirectional bandwidth of 2 Tb/s (denoted as the parameter L). We have four tunable lasers per transceiver, one for each FSR (note that k=4) and four, 8:1 serializers for time domain multiplexing, eight bits per time slot. Therefore, the resultant optical data rate is 32 Gb/s to achieve 4 × 8 × 32 = 1 Tb/s link bandwidth in each direction. The HBM I/O interface is slightly different. It has 1024 I/Os operating at 1 Gb/s (in each direction, see [4]), and, therefore, we use a 32:1 serializer for the HBM modules. A receiver as a similar architecture, as shown in Figure 2(b), bottom side. As we will show in Section IV, the transmitter side of the electro-optical interface dominates the energy per bit of the optical Baseline. We propose to increase the parallelism, i.e., 292 Figure 2. (a) High level view of the interposer-based photonic NoC called Baseline connecting 16 dielets. (b) Details of I/O interface of the NoC. The notation *, **, *** is to denotes the four wavelengths corresponding with different FSR, since k=4. In general, when realizing the inter-chip network, some of the ports of the NoC will be connected to in other chips. to increase p, which means to use more parallel AWGRs, to reduce the clock frequency of the electro-optical interface, maintaining the same overall bandwidth. Indeed, one of the key insight/ideas in the paper, is how to ﬁnd the right balance between parallelism, SERDES date rate (which affects power), number of FSRs, to achieve the target bandwidth and reduce energy per bit signiﬁcantly. This optical network is called Optimized, and it has the following design parameters. N is still 16, k=12, p=17, and the data rate on the optical link is about 5 Gb/s instead of 32 Gb/s of the Baseline network. The product of k and p gives us the total number of wavelengths which happens to be 204. When multiplied by the frequency of the optical link (5 Gb/s), gives us 1 Tb/s link bandwidth per direction. We would like to note that these parameters were chosen so that the optical link rate and the I/O clock rate of the compute nodes are the same (5 Gb/s). In this case, we can eliminate the SERDES in the transmitter and receiver sides for the compute nodes which can save signiﬁcant power. This design is aggressive since k=12 and p=17 are challenging, but we believe they are feasible in the future with the 3D writing technology. B. Optical Contention Resolution The gain saturation effect in a Reﬂective Semiconductor Optical Ampliﬁer (R-SOA) [21] can be used to realize optical contention resolution. N different nodes can make requests R1 , R2 , ..., Rn to the R-SOA associated with a given AWGR output port using different wavelengths λ1 , λ2 , . . . , λn (see Figure 3). The ﬁrst request, say Ri that arrives at the R-SOA saturates it, which results in some fraction of the power (Ptot ) power reﬂected back to the sender node i. The R-SOA stays saturated as long as the request on λi is held. A detector that is set to trigger at Ptot produces the grant signal. If another request Rj (on λj ) from node j arrives while Ri is still active, the power reﬂected at λj will be ∼Ptot /2 (because of the saturation effect in the R-SOA), which is not enough to set the trigger condition; hence, the second request will be excluded [see Figure 3(c)]. If two requests arrive at approximately the same time at the RSOA (with a time interval comparable or lower than the R-SOA gain dynamics, i.e., few hundreds of picoseconds), both the requestors receive approximately Ptot /2 reﬂected power and hence the detectors at neither node triggers, which corresponds to a situation that neither requestor has been granted. Note that this is different from a classic electronic mutex element where eventually one of the requestor gets a grant. In the R-SOA-based mutex element, it is possible for none of the requestors to get a grant, that is okay because the requirement of mutual exclusion element is that at most one of the requestors be granted the resource, zero grants is okay 293 Parameter Cores L1 cache L2 cache Directory E-Interposer O-Interposer Memory Description 64, 64 bits, out-of-order, 2 GHz 32/16 kB(I)+32/16 kB(D), 2-way, 1 cycle hit 512/256 kB banks, 8-way, 3/12 cycles tag/tag+data MOESI, 64 slices, 3 cycles Concentrated Mesh (8:1), 2 GHz, 64 bits, 1 cycle/hop All-to-All (AWGR), 32/5 GHz, 8/17 bits, 1 cycle/hop 64 GB, 16/8 channels, 128 bits, 1.6 GHz, ∼200 cycles Table II PARAM E T ER S O F TH E S IMU LATED ARCH I T EC TUR E . Figure 3. (a) N:1 Mutual Exclusion scheme with R-SOA. (b) Typical R-SOA Pout /Pin characteristic and minimum input power to operate the R-SOA in strong saturation regime. (c) Three different possible cases in the R-SOA-based mutual exclusion scheme. Figure from [22]. from the correctness of the protocol perspective. R-SOAbased mutual exclusion has an interesting property. Suppose, a request Rj arrives at the R-SOA after request Ri has been granted, i.e., while Ri is still asserted, then the reﬂected power to node i drops from Ptot to Ptot/2 , which could serve as a signal to node i that some other node has made a request to the same resource. This information is used to ensure fairness while removing the overhead for arbitration. This all-optical contention resolution scheme, inspired by Carrier Sense Multiple Access with Collision Detection technique, performs well for short distance communications. More details can be found in [22], [23]. IV. R E SU LT S In this section we will develop analytical models for latency and power consumption of the proposed photonic NoC and evaluate its performance on benchmarks. We will also compare our performance results with state-of-the-art interposer-based electrical NoCs from recent literature [1]. A. Modeling the Latency of the Photonic NoC In the proposed networks, the main contributors to the latency are: the SERDES, the waveguides in AWGR-based switch fabric, the control plane for arbitration resolution, and the wavelength tuning time for the tunable lasers. For the control plane we assume the all-optical contention resolution mechanism proposed in [22] where the control plane latency for a 16-node network is shown to be around 10 ns. We use the equation from [24] (8.3 × 10−3× L + 70.9 ps, where L is the length of the waveguide) to estimate the propagation delay through the waveguides and the optical interface circuitry (modulators, detectors, and drivers). In our proposed architecture the length of the waveguides is of the order of 10 mm so the propagation latency is around 80 ps. The SERDES is the main contributor to the latency. For the Baseline network, we assume four, 8:1 and 1:8 serializers/deserializers for the processing dies operating at 4 Gb/s and for the memory dies we need 32, 32:1 and 1:32 serializers/deserializers operating at 1 Gb/s to achieve the 1 Tb/s bandwidth. Using data from [25] and [26], we estimate that the total SERDES latency to be around 36 ns. Finally, according to [27], the wavelength tuning time is about 8 ns. Therefore the total end-to-end latency in the worst-case Baseline network is about 54 ns. As pointed out in Section III-A in the Optimized case, we completely avoid SERDES in the processor dies by reducing the data rate from 32 Gb/s to 5 Gb/s. Therefore, the estimated overall latency for the Optimized network is around 18 ns. B. Experimental Setup Our goal is to model the system shown in Figure 2 which consists of 16 dies, eight of which are compute nodes and the remaining eight are memory nodes. Each compute node is a 8-core processor for a total of 64 cores. We assume the intradie network is a 2D mesh-based electrical network and the inter-die network is the proposed photonic NoC. We use the Gem5 simulator [28] in Full-System (FS) mode to evaluate the performance of the proposed architecture. The simulator booted a complete Linux 2.6.27 Operating System (OS) for multi-threaded application scheduling and support. We modeled Chip Multi Processors (CMP) architectures with 64 cores, based on the Alpha Instruction Set Architecture (ISA) in the simulator. Each core has private L1 caches for instruction and data and a slice of a shared distributed L2 cache (Last-Level Cache, LLC). The directory information is distributed, and a directory-based, coherence protocol (i.e., MOESI) manages the caches. Table II summarizes the main architectural features of the overall architecture. We use the PARSEC benchmarks suite [29], a collection of heterogeneous parallel applications spanning different application domains (e.g., media processing, search and ﬁltering, 3D, and physics simulations) and representative of a diverse workload. Benchmarks were modiﬁed to enforce that each spawned thread is pinned to a ﬁxed core of the processor (i.e., core afﬁnity). This approach prevents some non-determinism in the parallel benchmark execution. We compared the performance results obtained with our proposed architectures against a recent electronic interposerbased architecture discussed in [1], which we will call 294 (a) Full-Cache (b) Half-Cache Figure 4. Execution time results of the proposed solutions in the (a) Full-cache case, and (b) Half-cache one, normalized to the E-Interposer setup. The results are presented for both the O-Interposer-Baseline and the O-Interposer-Optimized cases. E-Interposer in the rest of this paper. We think that the adopted methodology is a rigorous method to evaluate the potential beneﬁts of the proposed architectures. Table II shows the parameters of the different conﬁguration simulated. We modeled the DRAM-stacks as described in [1] using the same amount of DRAM in both the electrical and photonic conﬁgurations except that it is distributed in four stacks in the E-interposer case, and in eight stacks for our proposed optical architectures. O-Interposer-Baseline and O-Interposer-Optimized are the conﬁguration with the baseline photonic NoC and the optimized NoC as described in Section III. In the E-Interposer case, the memory stacks are deployed on the electronic interposer substrate so they can be accessed in the same way that one core can reach another core, it is just another hop to the memory. We are assuming that Gem5 models the hop delay and the latency of the electrical network including the arbitration/control overhead for the given topology appropriately. In the proposed design, memory is deployed on the optical interposer. We modeled the latency of having the undergo electric to optical conversion, SERDES, overhead of the control plane, and AWGR-based switch fabric, and the receiver, and the ﬁnal optical to electrical conversion as described in Section IV-A. C. Performance Analysis We experimented two conﬁgurations to estimate the beneﬁts of a high bandwidth inter-die interconnection network. • Full-Cache case: 32 kB(I)+32 kB(D), 2-way, L1 cache, and 512 kB per core, 8-way, L2 cache • Half-Cache case: 16 kB(I)+16 kB(D), 2-way, L1 cache, and 256 kB per core, 8-way, L2 cache Figure 4a shows the performance of the Full-Cache case. Note that the photonic network outperforms the E-Interposer on all the benchmarks. On average, the O-Interposer-based setups outperform the E-Interposer setup (blue columns in the ﬁgure) of ∼25%. Especially canneal benchmark does particularly well due to its more random memory trafﬁc pattern which requires all-to-all communication as described in [29]. This ﬁts well with the photonic NoC topology described in this paper and is especially encouraging because emerging applications such as large scale data analytics and large scale graph mining are expected to be more irregular and with large working sets and little data reuse. Another interesting thing to note is that there is not much difference in terms of performance between the O-InterposerBaseline and the O-Interposer-Optimized conﬁgurations. As we will show in the Section IV-D, the main beneﬁt of the optimized network is that improves the power consumption signiﬁcantly by reducing the SERDES power. Figure 4b shows the performance of Half-cache conﬁguration. Clearly, reducing the cache has a more signiﬁcant impact on the EInterposer as opposed to the O-Interposer. In fact, the OInterposer-based system is almost 2× better in terms of the performance. This is because the E-interposer-based system has a NoC based on multi-hop mesh topology and has lower bisection bandwidth than the photonic NoCs. Figure 5 is a better demonstration of the impact of the reduction in the cache size. Note that the simulated systems only have a relatively small L1 caches (32 kB), so halving the L1 cache size (16 kB) and the L2 cache size (256 kB per core), has a profound effect on the overall miss rate. Given that the DRAM access latency even with HBMs is still quite high, the overall performance suffers. However, it is still encouraging to note that even with a 50% reduction in the cache sizes, the performance of our proposed systems with photonic NoC is only about 30% lower than a system with full cache sizes and an electrical interposer-based NoC. This might work better with systems with a large L3 as LastLevel Cache, where reducing the size of the L3 will have less impact on performance because the L2 and the savings in area and power consumption will also be more substantial. 295 Parameter Name Baseline Optimized Frequency [GHz] Laser Efﬁciency Laser-Output-Power [mW/bit] Laser Tuning [mW] Photodetector Sensitivity [dBm] Transmitter (Driver+Mod+PLL) [fJ/bit] Receiver (Diode+TIA+Clock) [fJ/bit] Serializer xPU [fJ/bit] Serializer HBM [fJ/bit] Deserializer xPU [fJ/bit] Deserializer HBM [fJ/bit] Reﬂective SOA [mW] Total TX+RX per xPU [W] Total TX+RX per HBM [W] Total-Network+I/O [W] 32 4.5% 0.5 8.76 -10 1520.8 708.3 39 156.25 39 156.25 100 2.64 2.88 ∼45 Table III POW ER / EN ERGY PARAM E T ER S . 5 4.5% 2.5 28 -15 16.6 135.7 41.6 67 100 0.3 0.3 ∼7 NoCs considered in this paper. According to the HP paper (Table 4 in [8]) the power consumption of a 144 port implementation) is ∼154 Watts and a microring resonatorelectronic switch (operating at 320 Gb/s per port in a 22 nm based optical switch is about ∼76 Watts. Though these switches might have other advantages such as the ability to allocate bandwidth more ﬂexibly and resilience, we believe that the high power consumption makes them unsuitable for interposer-based applications where there is a stringent chiplevel power budget which has to be shared by the compute units, memory, and the interconnection network. V. D I SCU S S ION In this section we will interpret the results presented in the last section and revisit the underlying assumption and opportunities for extending this work. First, note that the related work in interposer-based electrical NoCs [1], does not present absolute power numbers in terms of energy per bit for their networks and the utilization (they only present relative power numbers comparing different topologies of their own). Therefore, we cannot exactly compare our results Figure 6. Energy breakdown for the O-Interposer-Baseline (a) and OInterposer-Optimized (b) NoCs, HBM case. The total energy consumption for case (a) is 2.57 pJ/bit, while for case (b) is 0.454 pJ/bit, ∼6× reduction. 296 Figure 5. Execution time comparison between the Full-cache and Halfcache scenarios, normalized to the E-Interposer, Full-Cache case (blue solid columns). The yellow solid columns represent the E-Interposer, Half-Cache case. The dashed columns represent the O-Interposer-Baseline and the OInterposer-Optimized setups for Full-cache (red-dashed and green-dashed columns) and the O-Interposer-Baseline and the O-Interposer-Optimized setups Half-cache (lightblue-dashed and purple-dashed columns). D. Power Analysis We used the parameters listed in Table III to construct a power model for the proposed photonic NoCs. The tunable laser parameters for the O-Interposer-Baseline network were obtained from [30], [31]. The transmitter parameters were obtained from [32] by scaling the frequency from 48 GHz to 32 GHz and the receiver values were obtained from [33]. The SERDES values were obtained from [32]. Similarly, for the O-Interposer-Optimized network, the parameter values were obtained from [34] and scaled to meet the design parameters of our network. The values for the control plane were obtained from [23]. The data for insertion and waveguide losses were obtained from [35], [36], [37], [38], [7], [39] and are summarized in Table IV. Based on these parameters, we estimate the total power consumption for the O-InterposerBaseline network to support the bisection bandwidth of 16 Tb/s is ∼45 Watts assuming 100% link utilization, and the relative energy consumption is ∼2.57 pJ/bit. For the OInterposer-Optimized case, we estimate the power is ∼7 Watts or 0.454 pJ/bit to support the 16 Tb/s bandwidth assuming 100% utilization. Figure 6(a) and Figure 6(b) show the breakdown of the power in the two networks. Note how, by taking advantage of higher parallelism (both in terms of additional FSRs and number of parallel AWGRs, and scaling down the data rates appropriately, the transmitter energy was reduced from reduction of a ∼6 factor. How does this compare with high 1.52 pJ/bit to only 16 fJ/bit for an overall total energy bandwidth state-of-the-art electronic and optical switches from recent research literature such as the work from HP reported in [8]? The paper from HP is based on switches operating at 320 Gb/s (32 bits @ 10 Gb/s) per input port. Hence, a very high radix switch (we estimate 144 port switch) is required to provide the same bandwidth as the Parameter Name Baseline Optimized Fiber [dB/cm] Grating Coupler [dB] Waveguide [dB/cm] Coupler (x2) [dB] AWGR [dB] Splitters [dB] Photodetector Mux/Demux Total-Power-Penalty -0.0001 -1 -0.1 -1 -3 -9 -0.1 -1 ∼-15 Table IV -0.0001 -1 -0.1 -1 -3 -12 -0.1 -1 ∼-19 IN S ERT ION LO S S PARAM E T ER S . with theirs. Second, one could argue that the electrical interposer-based NoC has lower bisection bandwidth than the proposed network so it might be unfair to compare them directly. However, it should be noted that the overall memory bandwidth for the PARSEC benchmarks [29] is well below the bandwidth of both the electrical and photonic networks compared here. The beneﬁt of the photonic network is due to lower latency of the crossbar topology of the photonic NoC, as opposed to a 8 × 8 mesh in the case of the electrical interposer-based NoC and the fact that we have more parallelism (eight DRAM stacks versus four). A. Architectural Implications Interposer-based photonic NoCs with extremely high energy-efﬁcient bandwidth might require new thinking in terms of the architecture of 2.5D integrated systems. For example, recent work in graph exploration [40] and in the evaluation of the beneﬁts of stacked high bandwidth DRAM [41], has shown that memory-level parallelism is quite low, which means that it is questionable whether a very high bandwidth network to memory can be fully utilized. In Section IV we proposed the idea of using smaller LLC which not only has the beneﬁt of reducing the die size but also reducing the power consumption of the compute die (especially leakage) since a signiﬁcant fraction of the die area is occupied by the cache itself. Memory bandwidth and power consumption due to off-chip communications, constrained the design space of computer architecture for decades but with very high bandwidth memory interconnect and with very low energy consumption per bit, the 2.5D systems could employ different architectures. These solutions can trade area/power and simpler programming models at the expense of more communications such as simper cache coherence protocols, off-die LLCs. Another possibility is to implement a mechanism so that the bandwidth of the network can be scaled dynamically. This is somewhat challenging in a photonic network because the external laser is always ON. Therefore, power is always consumed even if the network is not fully utilized. In the proposed NoC we address this issue in two ways. First, we use a tunable laser instead of an Optical Frequency Comb (OFC) source to generate the different wavelengths at each port, to reach the different destinations. We assume that it is very likely that 297 a compute node is communicating with some node at any given time. Therefore, the laser can be constantly used or, in other words, using a tunable laser improves the utilization of the light source itself. Second, we designed the NoCs to support k bandwidth states [analogous to power management states in Dynamic Voltage and Frequency Scaling (DVFS) solutions by taking advantage of the FSR-based parallelism in the AWGR. Indeed, in many computing applications, the trafﬁc is bursty [42] hence we may need a very high bandwidth for a short period of time and a much lower bandwidth during other periods. For example, as shown in Figure 2(b), by turning OFF one transmitter section (say SER-3) we can provide 3/4 of the link bandwidth; by turning OFF two sections (say SER-3 and SER-2) we can provide 1/2 the peak link bandwidth, and by turning OFF three sections we can provide 1/4 the peak bandwidth. Though turning the sections back ON (including recalibration of the lasers) could take several microseconds, we think it is still advantageous to use bandwidth scaling. It is possible for the communication scheduler in the memory/network controller to initiate the turning ON/OFF basing on the appropriate sections of the transmitter in advance. B. Scaling to Larger Systems We expect future high performance compute nodes that are geared towards meeting the requirements of Exascale computing to have a 3-level interconnect strategy - (a) intradie network that is most likely going to be based on the electrically signaling in the foreseeable future, (b) inter-die network that is based on the interposer, which was the main focus of this paper, and (c) inter-chip network that connects multiple chips together on a board or multiple boards. In today’s technology the interposer is around ∼900 mm2 [1], and assuming 8 GB HBM memory stacks (that are possible in the near future), we can support 64 GB of DRAM on a chip. Therefore, to build a compute node with 1 TB of memory, to meet the DoE requirements outlined in the introduction of this paper, we need 16 chips. One way to interconnect these 16 chips is to use a second level network implemented on a separate chip that is identical to the one proposed here as ﬁrst level network. The ability to scale to larger system with modest degradation in latency and power is the advantage of photonic interconnects and the proposed network is designed to be modular so that it can be integrated with an identical second level network on a board. V I . R E LAT ED WORK There has been signiﬁcant interest in 3D/2.5D integration in recent years. In [2] researchers describes silicon interposer technology for on-chip architectures, and tradeoffs about memory integration, thermal managements and cost analysis. The authors in [43] envision the next generation 3D-stacked chip integrating different and novel technologies like nonvolatile memories, efﬁcient heat removal, and in-memory computation model. As noted before, [1] is perhaps the ﬁrst work in research literature to thoroughly evaluate the design space of interposer-based electrical NoCs and propose two enhacements to existing network topologies called folded torus and enhanced butterﬂy. We use this as a baseline to compare our performance results. However, this work does not mention absolute power numbers, so we are not sure exactly what their energy per bit numbers are. But, given the distances involved in interposer-based implementation, they are likely to scale poorly compared with the photonic NoCs described here in terms of bandwidth and energy efﬁciency. The beneﬁts of photonics, design tradeoffs of photonic and electronic links, and the design of key building blocks such as modulators, waveguides, and lasers are described in [6], [14], [7]. Researchers at MIT/Berkeley [10] demonstrated a chip with dual-core RISC-V processors with 1 MB of SRAM on a commercial 45 nm CMOS SOI process without any changes to the foundry processes. This zero change proves that photonic circuits can be fabricated using standard design ﬂows and tool. The related work in the area of photonic interconnects in computing applications can be classiﬁed into four categories - intra-die networks such as HP’s Corona [9] and its derivatives [44], [45], CPU-DRAM interface [40], [46], [10], inter-chip networks such as MIT’s work [47], Galaxy [39], and high-radix switches [8], [48]. A majority of the existing work uses microring-based resonators and often a bus-based crossbar (though work such as [49] uses a CLOS topology as the switching fabric). The core of any interconnection network is the switch that implements the cross-connect. Instead of an AWGR, it is possible to realize an optical switch using MRRs-based. In this case, a large number of microrings and waveguides are needed to realize an all-toall network topology similar to what is possible with an AWGR. Exploiting AWGR th switch fabric is microringsfree and we need rings just in the transmitter and receiver sides. Therefore, a much lower number of microrings (and a lower energy consumption due to thermal tuning) is achievable with an AWGR-based interconnection. Also, as noted in Section I, the design space tradeoffs of intradie photonic networks are different from interposer-based implementation. For example, the intra-die networks assume very Dense WDM (DWDM, with 64 wavelenghts) so they do not have to worry about SERDES. The novelty of our approach is the use of Arrayed Waveguide Grating Router (AWGR) as the passive optical switch fabric. To the best of our knowledge this has not be considered for computing applications because, as described in Section II, in the past, the area requirements of AWGR was quite large, and it was not scalable to a very large number of ports (a key requirement in many intra-die networks) due to crosstalk between the waveguides. Furthermore, AWGR has always been used as a serial device (one bit at a time). Only very recently, with the advances in fabrication technology, low loss millimeter scale AWGRs have become practical. These advances made AWGR-based architectures attractive to implement new interposer photonic networks. Furthermore, given that there is a physical overall area constraint (the size of the interposer cannot be too large), these interposer-based architectures do not require AWGRs with a very large number of ports. In this case, losses due to crosstalk and propagation can be quite low. In this paper we showed how to realize multibit AWGRs by taking advantage of multiple FSRs and transmission parallelism in computing applications. Another key difference with related work is that, in this work, we use tunable lasers as opposed to Optical Frequency Combs to realize WDM-based communication. As already explained in Section V-A, this is more cost-effective (in terms of the number of components required), given that we do not need 64 wavelengths, and the fact that the lasers will be implemented on the interposer itself. V I I . CONC LU S ION AND FU TUR E WORK In this paper, we argued that 3D/2.5D die-stacked architectures are inevitable considering the fact that we are approaching the end of the era of performance improvement through cost-effective CMOS scaling. However, connecting the different die-stacks with a high-bandwidth interconnect, especially one that can scale to Terabytes of memory and tens of Terabits per second bisection bandwidth, is a huge challenge. We proposed an interposer-based photonic NoC that can provide a bandwidth of 16 Tb/s at an extremely high energy efﬁciency of 454 fJ/bit. The novelty of our solution is to use Arrayed Waveguide Grating Router (AWGR) as the optical switching fabric to support computing applications. With photonic networks we can scale the interconnect to multiple chips to provide tens of Tﬂops of computing and tens of Terabytes of memory. This is really exciting because from the dawn of computing, architects have always been shackled by the so-called memory wall - both memory capacity and bandwidth (given that it was off-chip) have always been a bottleneck. But with the advent of 3D diestacked memories that can provide very high capacity and high bandwidth interface, and of extremely energy efﬁcient, scalable inter-die photonic networks, it is perhaps time to think again about the role and size of caches and cache coherence protocols. Instead of plunking existing compute die on an interposer, perhaps the architecture of 2.5D computing systems has to be rethought without being clouded by the memory wall but, more generally, from how to maximize the performance per Watt, keeping in mind that memory latency is still a huge issue as none of these technologies directly mitigate it. This will form our future work. ACKNOW L EDGM EN T This work was supported in part under DoD Agreement Number: W911NF-13-1-0090. 298 "
2017,Near-Ideal Networks-on-Chip for Servers.,"Server workloads benefit from execution on manycore processors due to their massive request-level parallelism. A key characteristic of server workloads is the large instruction footprints. While a shared last-level cache (LLC) captures the footprints, it necessitates a low-latency network-on-chip (NOC) to minimize the core stall time on accesses serviced by the LLC. As strict quality-of-service requirements preclude the use of lean cores in server processors, we observe that even state-of-the-art single-cycle multi-hop NOCs are far from ideal because they impose significant NOC-induced delays on the LLC access latency, and diminish performance. Most of the NOC delay is due to per-hop resource allocation. In this paper, we take advantage of proactive resource allocation (PRA) to eliminate per-hop resource allocation time in single-cycle multi-hop networks to reach a near-ideal network for servers. PRA is undertaken during (1) the time interval in which it is known that LLC has the requested data, but the data is not yet ready, and (2) the time interval in which a packet is stalled in a router because the required resources are dedicated to another packet. Through detailed evaluation targeting a 64-core processor and a set of server workloads, we show that our proposal improves system performance by 12% over the state-of-the-art single-cycle multi-hop mesh NOC.","2017 IEEE International Symposium on High Performance Computer Architecture Near-Ideal Networks-on-Chip for Servers Pejman Lotﬁ-Kamran§ , Mehdi Modarressi† § , and Hamid Sarbazi-Azad‡ § § School of Computer Science, Institute for Research in Fundamental Sciences (IPM) † School of Electrical and Computer Engineering, College of Engineering, University of Tehran ‡Department of Computer Engineering, Sharif University of Technology Abstract—Server workloads beneﬁt from execution on manycore processors due to their massive request-level parallelism. A key characteristic of server workloads is the large instruction footprints. While a shared last-level cache (LLC) captures the footprints, it necessitates a low-latency network-on-chip (NOC) to minimize the core stall time on accesses serviced by the LLC. As strict quality-of-service requirements preclude the use of lean cores in server processors, we observe that even state-ofthe-art single-cycle multi-hop NOCs are far from ideal because they impose signiﬁcant NOC-induced delays on the LLC access latency, and diminish performance. Most of the NOC delay is due to per-hop resource allocation. In this paper, we take advantage of proactive resource allocation (PRA) to eliminate per-hop resource allocation time in single-cycle multi-hop networks to reach a near-ideal network for servers. PRA is undertaken during (1) the time interval in which it is known that LLC has the requested data, but the data is not yet ready, and (2) the time interval in which a packet is stalled in a router because the required resources are dedicated to another packet. Through detailed evaluation targeting a 64-core processor and a set of server workloads, we show that our proposal improves system performance by 12% over the state-of-the-art single-cycle multi-hop mesh NOC. Keywords-Latency; network-on-chip; resource allocation; server I . IN TRODUC T ION Server workloads are sensitive to last-level cache (LLC) access latency because of their large instruction footprint [1], [2]. Prior research shows that server workloads lose as much as half of the potential performance due to long latency LLC hits [3]. A noticeable fraction of LLC access latency is due to on-chip communications [1] — a request for a piece of data or an instruction should be sent to a destined LLC slice and the response should be sent back to the requesting core. A common network-on-chip (NOC) in today’s many-core processors is a two-dimensional mesh. It has been shown that a mesh-based fabric leads to poor performance on server workloads [1], [4]. The performance in mesh-based designs suffers as a result of a large average hop count, each hop involving a router traversal. To reduce NOC latency, researchers have proposed singlecycle multi-hop networks [5], [6]. Such networks beneﬁt from the fact that wires are relatively fast, and as such, in a single clock cycle, a packet can pass over more than one hop. Reducing the number of hops, a single-cycle multi-hop network improves performance over a mesh-based design by accelerating accesses to the LLC. However, server workloads have strict quality-of-service requirements, so they require relatively fat cores with high clock frequency [7]. Large cores increase the link length between two adjacent hops, and high clock frequency reduces the time budget for link traversal. Together, these two factors limit the effectiveness of single-cycle multi-hop networks in reducing the number of hops for server processors. Consequently, even single-cycle multi-hop networks impose signiﬁcant router delay on the LLC access latency. Most of the per-hop delay is due to resource allocation. On arriving at a router, a packet’s ﬂit ﬁrst needs to allocate the required resources and then use the allocated resources to go to the next hop. To reduce per-hop delay, prior work proposed allocating the resources to a ﬂit a few cycles before its arrival at a router [8]. Using this concept, we propose proactive resource allocation (PRA) to eliminate the resource allocation delay of a single-cycle multi-hop network. In contrast to prior work [8] that pre-allocates resources on a per-ﬂit basis, PRA pre-allocates resources for the whole packet to avoid ﬂit reordering in a single-cycle multi-hop network [5]. With PRA, packets may pass up to a few hops (e.g., two) in a single cycle. PRA is effective because it takes advantage of two opportunities to allocate resources to packets ahead of time on the way downstream to the destination: (1) the period between the end of tag and data lookup in the LLC [9], and (2) the in-network blocking period in which requested resources are not free. When an LLC slice receives a request for a piece of data or an instruction, if the request turns into a hit in the LLC, there will be a response to the requesting core. Last-level caches of most processors beneﬁt from a serial tag and data lookup to reduce energy usage [10], [11]. For such LLCs, the whole data lookup time is available for PRA. In cases when LLC uses a parallel tag and data lookup, data lookup takes longer than the tag lookup, as the data array is much larger, and the time between the end of the tag and data lookup is available for proactive resource allocation. Moreover, if a packet (either a request or a response) is waiting in a router because the output port is busy forwarding a multi-ﬂit packet, PRA beneﬁts from the waiting time by proactively allocating the required resources for the waiting packet on the way downstream to the destination. 2378-203X/17 $31.00 © 2017 IEEE DOI 10.1109/HPCA.2017.16 277 We make the observation that if the downstream router has enough buffers to store the in-transfer packet that holds the requested resources, we can determine exactly when the transmission of the in-transfer packet will end and consequently when the waiting packet can be transmitted. In this paper, we make the following contributions: • To the best of our knowledge, this is the ﬁrst work that shows that single-cycle multi-hop networks are far from ideal for server processors. • We show that pre-allocating resources to packets (and not individual ﬂits) in a single-cycle multi-hop fashion within the two suggested time intervals results in a nearideal network for servers, which is within 4% of the performance of an ideal network. • To the best of our knowledge, this is the ﬁrst time that packet stall time in a router is used for pre-allocating resources in the network. • We use a full-system simulation infrastructure to evaluate PRA in the context of a 64-core server processor on a set of server workloads. Our results show that PRA offers 12% higher performance as compared to a single-cycle multi-hop network. I I . BACKGROUND In this section, we examine features of server workloads, and then describe trends in many-core processors. Last, we survey on-die interconnect schemes and describe their implications for performance in the context of many-core server processors. A. Server Workloads Server workloads have several features that are common across a wide range of applications, such as web search and media streaming [2], [3], [7]. Three of the common features are (1) request independence, (2) quality-of-service (QoS) requirements, and (3) sensitivity to LLC access latency. We examine these three common features in the following sections. Request Independence: Users’ requests that are processed by server workloads are mostly mutually independent. The independence of requests makes server workloads a good candidate for execution on many-core processors. Quality-of-Service (QoS) Requirements: Many server workloads (e.g., web search and media streaming) have latency requirements as part of their service-level agreement. Moreover, server workloads increasingly invoke computationally intensive and performance-critical kernels [2], [7]. As lean cores (tiny cores with low frequency, e.g., [12]) may jeopardize application quality-of-service and latency constraints, they are not commonly used in server processors. Sensitivity to LLC Access Latency: Server workloads have large instruction footprints beyond what can be captured in L1-I caches [1], [2]. Consequently, last-level caches Figure 1. Elements of tiled server processors. hold the instruction footprints. As a result, server workloads are sensitive to LLC access latency. B. Server Processors The observations captured in the previous section are reﬂected in several contemporary server processors. One such design is the Intel Xeon E5 series processors. Depending on the model, the E5 series features up to 12 cores, a banked LLC with 6-30 MB of storage capacity, and a ring interconnect for connecting cores and cache banks. While appropriate for a modest number of cores, the ring interconnect stands as a major obstacle for scaling up the core count, as its delay has linear dependence on the number of interconnected components. To overcome the scalability limitations of ring-based designs, emerging many-core processors, such as Intel Knights Landing [13], use a tiled organization. Figure 1 shows an overview of a generic tiled processor. Each tile consists of a core, one bank of the distributed last-level cache, directory slice, and a router. The tiles are linked via a routed, packetbased, multi-hop interconnect in a mesh topology. C. NOC Architecture Even mesh-based designs expose the core-LLC communications to signiﬁcant network delays, and diminish performance. Each hop in a mesh network involves a router traversal, which adds delay due to the need to access the packet buffers, arbitrate for resources, and navigate the switch. These delays diminish the performance of a meshbased tiled processor on server workloads [4]. To overcome the performance drawbacks of mesh-based interconnects, researchers developed a single-cycle multihop network named SMART [5] for on-die communications. SMART takes advantage of a dedicated multi-drop network to set up multi-hop paths. When a header ﬂit wins the arbitration, instead of sending the ﬂit to the link in the next cycle, SMART attempts to establish a multi-hop path using the multi-drop network, which takes one cycle. In the next cycle, the header ﬂit goes to the link, potentially passing over multiple hops (e.g., eight hops) before getting latched in the input buffer of a router. SMART reduces the contributions of routers to the end-to-end delay at the expense of an additional clock cycle delay to set up a multi-hop path. 278                                       -(0 -(. -(, ,(3 ,(2 ,(0 ,(. ,(,  	 $"" ""  Figure 2. Performance of SMART and ideal NOCs, normalized to mesh. Unfortunately, SMART does not offer a signiﬁcant performance boost in the context of server processors. As cores in server processors are relatively fat with high clock frequency, a single-cycle multi-hop NOC can send a packet over just a few hops in a single cycle (e.g., two hops). Given the extra cycle needed to set up a multi-hop path and the probability that not all links are idle at the same time, the net effect of SMART in server processors is negligible. Figure 2 compares the performance of the SMART network to that of an ideal network with zero router latency (link delay and contention are accounted for) for two representative server workloads. Both performance numbers are normalized to that of a mesh network. Packets may pass over two hops in a single cycle in both SMART and ideal networks. The details of the methodology can be found in Section IV. The results show that the performance of SMART is almost the same as that of the mesh interconnect. Moreover, a hypothetical network that does not impose router delay on the end-to-end packet latency results in an average 28% performance improvement for Media Streaming and Web Search workloads as compared to the mesh. In summary, to get good performance from server workloads, we need to minimize the contributions of routers to the on-die communication delay in a single-cycle multi-hop network. I I I . OUR PRO PO SA L This work aims to eliminate resource allocation time from the end-to-end packet transmission latency. For this purpose, on the way downstream to the destination, we proactively allocate resources to packets before they demand the resources. To proactively allocate resources, we need to know what resources are needed for transmission of a packet and in which timeslots. The former can be determined by the destination of a packet, as we know the whole path to the destination. The latter can be calculated if one knows when the packet starts passing through the network. Knowing the starting time, it is easy to calculate when the packet enters and exits each hop, given the assumption that proactive resource allocation is successful in prior hops. We need to know the required information (i.e., starting time and destination) a few cycles before a packet starts 279 traveling in the network. We observe that under two frequent events, the required information is known a few cycles before the actual packet transmission: (1) upon an LLC hit, and (2) when a packet is stalled because the requested output port is busy sending a multi-ﬂit packet. Depending on the available time, the distance between the node that initiates PRA and the destination, and the status of the required resources, PRA allocates part or even all of the required resources to the destination. For part of the path to the destination where resources are proactively allocated, the packet just uses the resources, which greatly speeds up the transfer, and for other parts, the packet ﬁrst allocates the resources, as in a standard network, before using them. Proactive resource allocation is a general idea and can be implemented on any NOC. As a case study, we provide details for the implementation of PRA on a standard mesh network. A. PRA on a Mesh Network In server processors, networks need to have three message classes—request, response, and coherence—to avoid protocol deadlock [14]. L1 caches are effective at ﬁltering access to the network in server processors, so the trafﬁc in the network is not heavy [4], [15].1 Consequently, each message class usually consists of a single virtual channel (VC). Moreover, as the coherence trafﬁc is negligible [4], [16], [17], the request and response trafﬁc determines the performance. The mesh network, with slight modiﬁcations to support PRA, is used for packet transmission. We refer to the mesh network as the data network. We augment the data network with a narrow bufferless control network to proactively allocate resources in the data network. In the following sections, we ﬁrst explain the modiﬁcations needed in the data network for PRA and then discuss the control network in detail. B. Data Network Without PRA, the data network (i.e., the standard mesh network) does not support single-cycle multi-hop traversal. A header ﬂit ﬁrst goes to the VC and crossbar allocation, and if the required resources are allocated to it, goes to the crossbar and link in the following cycle. The rest of the ﬂits follow the head ﬂit in subsequent cycles. With PRA, however, resources are pre-allocated to packets so that packets can beneﬁt from single-cycle multi-hop traversal. We assign resources to packets on a cycle-by-cycle basis. When a cycle in a router is assigned to a packet, all the necessary resources in the router are assigned to that packet. Moreover, for multi-ﬂit packets, PRA either allocates the necessary resources for the transmission of all ﬂits or fails. 1While the trafﬁc is moderate in server workloads, most of the NOC trafﬁc is due to instruction misses, and so the latency of the network signiﬁcantly affects performance [2]. 	       Figure 3. A 2-cycle proactively allocated path from R1 to R4.  Figure 3 shows a proactively allocated path between R1 and R4. Assuming two hops can be passed in a single cycle, the 3-hop path consists of two single-cycle traversals of length 2 and 1. The second part of the path is shorter because either R4 is the ﬁnal packet destination or the required resources are not idle at R4. In router R1 and at time t, a packet is read from VC1 ’s buffer, and is passed through the crossbar and R1–R2 link. In router R2 and at the same cycle, the mux and demux are set so that whatever comes out of the R1–R2 link goes to the crossbar and R2–R3 link. So the packet bypasses R2 and goes directly toward R3, where the demux is set so that the packet goes to the latch. The latches are used for temporary storage of ﬂits after passing over n hops, where n is the number of hops that a ﬂit can pass over in a single cycle (while PRA only supports twohops-per-cycle traversal, the data network is general). In the following cycle (i.e., t+1), the packet similarly goes to R4 and is buffered in VC1 . As resource allocation is performed proactively, the whole transfer time from R1 to R4 is two cycles. Figure 4 shows components of a router in the data network. Components that are unique to or modiﬁed by PRA are shaded in gray. One of the modiﬁed components is the input unit. In addition to standard VCs, we need to add two extra VCs to the input units for operation of PRA. One VC is a bypass link that lets packets bypass the VC buffers and go directly to the crossbar. The other added VC is a latch that is used as a temporary 1-cycle storage within a proactively allocated path. Moreover, for each output port there is a set of bit vectors that store the allocation status of the output port for several timeslots starting from the next cycle. For each cycle, the bit vectors indicate whether resource allocation is done for that cycle (Valid vector in Figure 4). If the resource allocation is done, the bit vectors also indicate the input port that the packet comes from (Input Select), the exact VC within the port (Local VC Select), and the exact VC of the downstream router to which the packet should go (Downstream VC Select). The content of the bit vectors is shifted to the left each cycle and one free timeslot becomes available in the last cycle. The control network sets the bit vectors, as explained in Section III-C. The arbiter is one of the components that are slightly modiﬁed to support proactive resource allocation. The arbiter consists of a local arbiter, as in a standard mesh network, and a PRA arbiter. If the bit vectors indicate that no proactive Figure 4. Mesh+PRA router in the data network. resource allocation is recorded for a given timeslot, the local arbiter will be in charge; otherwise, the PRA arbiter decides what will happen in the cycle. Figure 4 shows how local and PRA arbiters are connected together. The PRA arbiter uses the bit vectors to decide what to do in each cycle. If there is no recorded resource allocation for the next cycle, it does nothing. Otherwise, based on the bit vectors, it sets the select of the mux to connect the right VC to the crossbar, sets the control signal of the crossbar to send the packet to the output port, and sets the select of the demux of the downstream router to guide the packet to the right VC. Finally, the data network includes a Long Stall Detection (LSD) unit that checks for a stalled packet waiting for the end of transmission of a multi-ﬂit packet. If there are enough buffers in the downstream router for the multi-ﬂit packet, this unit injects a control packet into the control network to proactively allocate resources for the stalled packet. C. Control Network The control network is a narrow, bufferless NOC that is used for proactive resource allocation in the data network. A control packet pre-allocates resources in the data network to accelerate packet transmission. If a control packet cannot pre-allocate resources in a router, it simply gets dropped. A control network consists of a mesh of single-cycle multidrop segments, as shown in Figure 5. The ﬁgure highlights the internal structure and connections of a control network’s router in the X dimension. The router has the same connections in the Y dimension. Each router is connected to the next two routers in each direction using a multi-drop segment. Turns are not allowed in multi-drop segments as a way of minimizing the overhead. With multi-drop segments, when a router sends a control packet, two subsequent routers receive the packet. We use 2-hop multi-drop segments to enable control packets to pass over two hops in two cycles: one cycle for packet processing and one for packet transmission. A router has two multi-drop inputs per direction. For each direction, there are three latches: two corresponding to the two multi-drop inputs and one associated to the LSD, as we will discuss shortly. The three latches are statically prioritized such that the closest multi-drop segment has the 280 1RGH[ 1RGH[ D 1RGH[ 0XOWLGURS ; $&. $&. 0XOWLGURS ; 1RGH[ 1RGH[ /6'35$UHTXHVW 0XOWLGURS 0XOWLGURS $&. /DWFK /DWFK /DWFK 5HVRXUFH $OORF8QLW &RQWURO 3DFNHW 5 W 5 W W $&. W W E 5 W 5 W 5 W W W 5 W 5 W W W W 7RFRQWUR O QHWZRUN FURVVEDU $&. 7R)URP35$ 6WDWXV9HFWRUV Figure 5. Control network: (a) links and signals and (b) signaling for sending a control packet from R1 to R7. highest and the LSD has the lowest priority. PRA prioritizes the closest multi-drop segment over the farthest one to enable the second node of a 2-hop multi-drop to locally determine whether the ﬁrst node can proactively allocate the required resources or not, as we explain in this section. If a router receives more than one control packet in a direction in a single cycle, the lower priority packets are dropped. Control packets, which are one ﬂit long, consist of the destination address, the lag between the control and the data packet (number of cycles), the size of the data packet (long or short), the message class (VC number), and look-ahead routing information. The destination is either the source ﬁeld of the request packet or the destination ﬁeld of the stalled packet, and the lag is the number of cycles between the control and its corresponding data packet. On arriving at the LLC, a request packet is queued within the LLC and waits for its lookup time. If the tag lookup indicates a hit, the LLC controller will notify the network interface (NI). The NI creates a control packet and places it in the local latch of the control network if the latch is empty. Otherwise, the control packet will be dropped. Moreover, the long stall detection (LSD) unit checks to see if a packet is waiting for a multi-ﬂit packet, and if there are enough buffers for transmission of the multi-ﬂit packet. If the conditions hold, the LSD unit generates a control packet and injects it into the control network to proactively allocate resources for the waiting packet. On receiving a control packet in a given direction, the packet is passed through the route computation unit and resource allocation unit in parallel. If there is more than one control packet in a direction, one is statically chosen and the rest are dropped. The resource allocation unit determines whether the requested timeslots on the requested output port and downstream VC buffers can be granted. Note that the control network always allocates buffers for a full packet. If the packet needs to pass the bypass VC or the latch of Figure 4, the decision is updated later, as we discuss. If the request cannot be granted, the control packet will be dropped. Otherwise, the granted timeslot(s) will be recorded in the bit vectors and the required buffer space in the downstream router is allocated properly for the full packet. If the two routers in a multi-drop can allocate the required resources, the second router forwards the control packet to the subsequent multi-drop segment, provided that the control packet’s lag is greater than zero. As it takes two cycles for the control packet to pass over a multi-drop segment while it takes only one cycle for the corresponding data packet to pass over the pre-allocated multi-hop path, routers decrement the lag to account for this difference and drop control packets when the lag becomes zero. With the lag being zero, the data packet has reached the control packet and no further pre-allocation is possible. As a control router does not know if proactive resource allocation will succeed in the downstream router, it should reserve a downstream buffer for the packet in the data network. Each control router that successfully pre-allocates the required resources passes an ACK signal back to the upstream router. The ACK signal notiﬁes the upstream router that it is not the last node of a pre-allocated path. Upon receiving the ACK signal, the control router frees up the allocated buffer space and changes the Down Stream VC Select (see Figure 4) for the data packet to pass through the latch or bypass VC that are included in the input unit of the data network. If the router is the second router in the multi-drop segment, the latch VC will be selected (Router 3 in Figure 5(b)); otherwise the bypass link (Router 2 in Figure 5(b)) is selected. While only the second node in a multi-drop segment is responsible for transmitting the control packet to the next multi-drop segment, the transmission should happen only if the two nodes in the multi-drop segment can pre-allocate the required resources. Fortunately, the second node knows whether the ﬁrst node can allocate the required resources or not, because the second node knows the status of its input port and buffer, which are the output port and downstream buffer of the ﬁrst node, and PRA gives higher priority to the closest multi-drop segment, as mentioned earlier. Finally, PRA needs to avoid the possibility of two multiﬂit packets, one with normal and one with proactive resource allocation, getting interleaved in buffers of a standard VC. When a router allocates timeslots to a multi-ﬂit packet on an output port, it sets a special ﬂag corresponding to the message class. While this ﬂag is set, no multi-ﬂit packet can use the message class, but single-ﬂit packets can still use the message class. The ﬂag is cleared when either the multi-ﬂit packet passes over the output port or the downstream router, through the ACK signal, informs the router that the multi-ﬂit packet will not be using the VC buffer. IV. M E THODO LOGY Table I summarizes the key elements of our methodology, and the following sections detail the evaluated designs, technology parameters, workloads, and simulator. 281 Parameter Value Table I EVA LUAT ION PARAM E T ER S . Technology Processor features Core Cache NOC Organizations: Mesh SMART Mesh+PRA Ideal 32 nm, 0.9 V, 2 GHz 64 cores, 8 MB NUCA LLC, Four DDR3-1600 memory channels ARM Cortex-A15-like: 3-way out-of-order, 64-entry ROB, 16-entry LSQ, 2.9 mm2 , 1.05 W per MB: 3.2 mm2 , 500 mW Router: 5 ports, 3 VCs/port, 5 ﬂits/VC, 1-stage (speculative) pipeline. Link: 1 cycle Router: 5 ports, 3 VCs/port, 5 ﬂits/VC, 2-stage pipeline. Link: up to 2 tiles per cycle Data network: 5 ports/router, 3 VCs/port Packets with PRA support: Bypassing pipeline stages, Link: 2 tiles per cycle Others: 1-stage speculative pipeline. Link: 1 tile per cycle Control network: 4-output and 13-input ports/router, 1-stage bufferless pipeline. Link: 2 tiles per cycle Router: 5 ports, 3 VCs/port, 5 ﬂits/VC, Bypassing pipeline stages. Link: 2 tiles per cycle A. Processor Parameters Our target is a 64-core processor based on the ScaleOut Processor design methodology [18], [19], which seeks to maximize throughput per die area. The chip features a modestly sized last-level cache to capture the instruction footprint and shared OS data, and dedicates the rest of the die area to the cores to maximize throughput. The architectural features are listed in Table I. We consider four system organizations, as follows: Mesh: Our baseline is a mesh-based tiled processor, as shown in Figure 1. The 64 tiles are organized as an 8-by-8 grid, with each tile containing a core, a slice of the LLC, and a directory node. A mesh hop consists of a single-cycle crossbar and link traversal followed by a one-stage router pipeline for a total of two cycles per hop at zero load. The router performs routing, VC allocation, and speculative crossbar (XB) allocation in the ﬁrst cycle, followed by XB and link traversal in the next cycle. Each router port has three VCs to guarantee deadlock freedom across three message classes: request, coherence, and response. Each VC is ﬁve ﬂits deep, which is the minimum needed to cover the roundtrip credit time. SMART: The SMART-based processor has the same tiled organization as the mesh baseline, but enjoys singlecycle multi-hop traversal. A SMART hop consists of a twostage router pipeline followed by a single-cycle (potentially) multi-tile link traversal for a total of three cycles per hop at zero load. The router performs routing, VC allocation, and speculative crossbar (XB) allocation in the ﬁrst cycle, a multi-tile link allocation in the second cycle, and ﬁnally XB and link traversal. Each router port has three VCs to guarantee deadlock freedom across three message classes. Each VC is ﬁve ﬂits deep. Mesh+PRA: The proposed proactive resource allocation (PRA) is implemented on top of the baseline mesh. LLC waiting time and in-network blocking time are used to proactively allocate resources using a dedicated bufferless control network with 15-bit-wide links. At the controlnetwork, a hop consists of a single-stage router pipeline followed by a single-cycle two-tile multi-drop link traversal. At the data network, a Mesh+PRA hop consists of a singlestage router pipeline followed by a single-cycle single-tile traversal for a total of two cycles per hop at zero load without PRA (i.e., baseline mesh). However, when proactive resource allocation takes place, a packet passes over up to two tiles (crossbars and links) in a single cycle. Each router port has three VCs to guarantee deadlock freedom across three message classes. Each VC is ﬁve ﬂits deep. Ideal: The ideal interconnection network is a hypothetical network-on-chip with router delay of zero cycles. For the ideal network-on-chip, only wire delays are considered. A header ﬂit can pass over up to two hops in a single cycle if the required crossbars and links are free. Body ﬂits follow the header ﬂit in subsequent cycles. While router delay is zero, packets may get blocked in a router due to contention (e.g., two packets competing for the same output port). Each router port has three VCs for request, coherence, and response packets. Each VC is ﬁve ﬂits deep. B. Technology Parameters We use publicly available tools and data to estimate the area and energy of the various network organizations. Our study targets a 32 nm technology node with an on-die voltage of 0.9 V and a 2 GHz operating frequency. We use custom wire models, derived from a combination of sources [20], [21], to model links and router switch fabrics. For links, we model semi-global wires with a pitch of 200 nm and power-delay-optimized repeaters. For SMART and Mesh+PRA, we choose the number of repeaters to get a link latency of 85 ps/mm. Given the delay of wires and the aspect ratio of the tiles, two tiles can be traversed in a single clock cycle. On random data, links dissipate 50 fJ/bit/mm, with repeaters responsible for 19% of link energy. For area estimates, we assume that link wires are routed over logic or SRAM and do not contribute to network area; however, repeater area is accounted for in the evaluation. 282                 #% #$ #"" ""' ""& ""% ""$ """"   	 (	    	          Figure 6. System performance, normalized to a mesh-based design. Our buffer models are taken from DSENT [22]. We model ﬂip-ﬂop based buffers as all NOCs have relatively few buffers. Cache area, energy, and delay parameters are derived via CACTI 6.5 [23]. A 1 MB slice of the LLC has an area of 3.2 mm2 and dissipates 500 mW of power (mostly leakage). The tag and data lookups take 1 and 4 cycles, respectively. Finally, parameters for the ARM Cortex-A15 core are borrowed from Microprocessor Report [24] and scaled down from the 40 nm technology node to the 32 nm target. Core area, including L1 caches, is estimated at 2.9 mm2 . Core power is 1.05 W at 2 GHz. Core features include 3-way decode/issue/commit, 64-entry ROB, and 16-entry LSQ. C. Workloads We use server workloads from CloudSuite [25]. The workloads include Data Serving, MapReduce, Media Streaming, SAT Solver, Web Frontend, and Web Search. Two of the workloads—SAT Solver and MapReduce—are batch, while the rest are latency-sensitive and tuned to meet the response time objectives. Prior work [2] has shown that these workloads have characteristics representative of the broad class of server workloads. D. Simulation Infrastructure We estimate the performance of various processor designs using the Flexus full-system simulation [26]. Flexus extends the Virtutech Simics functional simulator with timing models of cores, caches, on-chip protocol controllers, and interconnect. Flexus models the SPARC v9 ISA and is able to run unmodiﬁed operating systems and applications. Flexus uses the BookSim 2.0 network simulator [27] for modeling the on-chip network. We use the SimFlex multiprocessor sampling methodology [28]. Our samples are drawn over an interval of 10 seconds (except Media Streaming samples, which are drawn over 30 seconds) of simulated time. For each measurement, we launch simulations from checkpoints with warmed caches and branch predictors, and run 100 K cycles to achieve a steady state of detailed cycle-accurate simulation before collecting measurements for the subsequent 50 K cycles. We use the ratio of the number of application instructions to the total number of cycles (including the cycles spent executing operating system code) to measure performance; this metric has been shown to accurately reﬂect overall system throughput of multiprocessors [28]. Performance measurements are computed with 95% conﬁdence and an error of less than 4%. V. EVA LUAT ION We ﬁrst examine system performance and area efﬁciency of the Mesh, SMART, and Mesh+PRA designs, given a 128-bit link bandwidth. We then present an area-normalized performance comparison, followed by a discussion of power trends. A. System Performance Figure 6 shows full system performance, normalized to the mesh, for various NOC organizations. Mesh and SMART offer almost the same performance. SMART enables packets to go over two hops in a single cycle but, unlike Mesh, requires an extra cycle to set up the multi-hop path. As Figure 6 shows, the net effect is negligible. The proposed Mesh+PRA offers the highest performance when compared to realistic networks. Compared to Mesh, Mesh+PRA improves performance by 7–29%, with a geomean of 14%. On average, the proposed Mesh+PRA improves performance over SMART by 12%. The highest performance gain is registered on the Media Streaming workload, which is characterized by very low instructionlevel parallelism (ILP) and memory-level parallelism (MLP), making it particularly sensitive to the LLC access latency. As a point of reference, we also include the performance of an ideal network with zero router latency. Across all benchmarks, the proposed Mesh+PRA closely follows the performance of the ideal network. On average, Mesh+PRA is only 4% behind the performance of the ideal network with zero router latency. B. Why is PRA Effective? To demonstrate why PRA is capable of reducing the onchip network’s delay, and consequently, improving system 283     	    !  ( % ( $ ( # ( "" (  (  	   	 	 	   	   ! ""                       	   Figure 8. NOC area breakdown. Figure 7. Distribution of control packets’ lags when they are dropped. The maximum lag in our setup is four. performance, it is essential to investigate how effectively control packets proactively allocate resources for the data packets traveling in the network. Figure 7 shows the distribution of control packets’ lags when they are dropped. Because a control packet’s lag is decremented in each multi-drop, the lower the lag becomes, the more resource allocation is done for the corresponding data packet (ideally the lag becomes zero before the control packet is dropped). Figure 7 shows that across all workloads, 53–67% of control packets have a lag of zero (ideal case) when they are dropped (the average across all workloads is 61%). Moreover, 15–20% of the control packets have a lag of one, and 17–27% have a lag of two. More than 98% of the control packets have a lag of 0–2 across all workloads (less than 2% of control packets have a lag of greater than two: the maximum lag in our setup is four). The results clearly show the effectiveness of control packets in pre-allocating resources to the data packets in the on-chip network. Moreover, we measure the number of control packets that are injected into the control network. On average, we have 1.60 (SAT Solver) to 1.89 (Data Serving) control packets for a single data packet (either a request or a response). As there is more than one control packet per data packet and control packets are effective at proactive resource allocation (see Figure 7), a considerable number of a data packet’s required resources is proactively allocated on the way downstream to the destination. Finally, when resources are proactively allocated on an output port to be used later by a packet, the output port becomes unusable by multi-ﬂit packets (short packets, i.e., requests, can still use the output port) until either the allocated resources are released or an ACK signal is received from the downstream router. This resource underutilization may have a negative impact on the effectiveness of the proposed resource allocation scheme. We measure the number of cycles that an output port cannot be used by a packet because the port is proactively allocated to another packet, and normalize it to the time the packet travels in the network. Across all workloads, a packet only spends 0.01% of the end-to-end latency waiting in the network because the resources are proactively allocated to other packets. The large number of control packets per data packet, the effectiveness of control packets in allocating resources, and the negligible impact of resource underutilization for server workloads explain the performance improvements observed in Figure 6. C. NOC Area Figure 8 breaks down the NOC area of the three organizations by links, buffers, and crossbars. Only repeaters are accounted for in link area, as wires are routed over tiles. For SMART and Mesh+PRA, the area of the interconnect is 4.5 mm2 and 4.9 mm2 , respectively. Compared to the Mesh, SMART and Mesh+PRA require 31% and 40% more area, respectively. Because interconnect has a small footprint (i.e., Mesh’s footprint is 3.5 mm2 ), the 1.0 mm2 and 1.4 mm2 area overheads of SMART and Mesh+PRA seem considerable, but as compared to the area of the whole chip (i.e., over 200 mm2 ), they are relatively small. D. Performance-Density Comparison The area analysis in the previous section indicates different NOC area costs (and hence chip area) for the examined networks. To better understand how well the various designs use the chip silicon area, we assess the performance density (i.e., performance per square millimeter) of various processors. We only consider the area of cores, caches, and interconnect, disregarding the area of memory channels and IO devices. Figure 9 summarizes the results of the study, with performance density of the four organizations normalized to that of the mesh (we idealistically assume the area of a mesh for the area of the ideal network). On realistic designs, Mesh+PRA offers the highest performance density, followed by SMART. The lowest performance density is registered for Mesh. While Mesh+PRA has the highest network area, due to its effectiveness in boosting performance and relatively low area overhead at the chip level, it is the most area-efﬁcient organization. Mesh+PRA boosts performance density by 14% over Mesh, and 12% over SMART. Mesh+PRA is only 5% behind the performance density of the ideal network. 284 	                    #% #$ #"" ""' ""& ""% ""$ """"   	 (	    	          Figure 9. System performance per square millimeter (i.e., performance density), normalized to a mesh-based design. E. Power Analysis Our analysis shows that the NOC is not a signiﬁcant consumer of power at the chip level (corroborating prior work [4], [29]). For all organizations, NOC power is below 2 W. In contrast, cores alone consume in excess of 60 W. Low ILP and MLP of server workloads [2] is the main reason for the low power consumption at the NOC level. V I . R E LAT ED WORK Various proposals have pointed out the need for lowlatency on-chip communication mechanisms [30], [31]. Existing low-latency NOC designs often target reducing (1) hop counts, (2) blocking latency, or (3) per-hop latency. Hop-count reduction. Packet hop-count reduction has long been a major target in many low-latency NOC designs. Prior work has focused on low-diameter topologies, including high-radix networks [32]–[34], reconﬁgurable networks [35], and mesh-based topologies equipped with extra irregular links that are inserted either randomly [36] or based on applications’ trafﬁc patterns [37]. Core-to-network mapping and customized topology generation [38] are also effective application-speciﬁc methods that reduce average hop count for a target application, when the application and its trafﬁc pattern can be pre-characterized at design time. Blocking-latency reduction. Adaptive routing is a technique to reduce blocking latency by directing packets to less congested paths. Among adaptive routing schemes, those methods that leverage both local and global congestion metrics [39]–[43] or are aware of the running applications’ trafﬁc behavior [44] often make more appropriate routing decisions. Moreover, arbitration plays an important role in managing the inevitable blocking latency in favor of total application performance and quality of service requirements. Slack-based arbitration [45] and prioritization schemes such as QoS-aware prioritization [46], application-aware prioritization [47], and message class-based prioritization [48] are effective in increasing applications’ performance. Prior work also showed the effectiveness of predictive switch allocation [49], packet-chained allocation [50], run-time adaptive buffer sizing [51], smart VC allocation [52], packet 285 compression [53], and heterogeneous router design [54], [55] in NOC latency reduction. Per-hop latency reduction. To decrease router latency, efforts seek to cut down or bypass the pipeline stages of routers. A single-stage router [56] utilizes extensive precomputation techniques to forward packets in a single cycle under low trafﬁc. Router bypassing, which is implemented in prior work (such as Express Virtual Channels [57], Token Flow Control [58], and Pseudo Circuits [59]), enables ﬂits to travel one hop per cycle on pre-established paths. In ﬂit-reservation ﬂow control [8], a control packet traverses the network ahead of data ﬂits to reserve buffers and channel bandwidth in advance. Each control packet leads one or multiple ﬂits of a packet. Unlike PRA, this method does not support single-cycle multi-hop traversal, and reserves resources for individual ﬂits, which makes it difﬁcult to support single-cycle multi-hop traversal (e.g., ﬂits may be reordered [5]). Bufferless NOCs cut down router pipeline stages by always forwarding received packets to an output port in a single cycle [60], [61]. Most bufferless methods enable single-cycle packet forwarding, but at the price of deﬂecting or dropping the packet when the preferred output port is busy, hence increasing network latency under moderate trafﬁc loads. The design of efﬁcient circuit-switched NOCs has been the focus of many proposals [62], [63]. Traversing dedicated paths, circuit-switched data need not go through buffering, routing, arbitration, and ﬂow control once circuits are set up. However, this switching method often suffers from performance degradation due to long circuit setup delay and poor bandwidth utilization. The time-division multiplexing (TDM) scheme mitigates the low bandwidth utilization of circuit switching [63], but its complexity introduces difﬁculties using the circuits. We use the concept of the timeslot in allocating link bandwidth to packets, but (1) avoid the long setup time of circuit switching by overlapping resource allocation and packet waiting time, and (2) relax the complex timeslot allocation and alignment of TDM by storing packets locally in case of unsuccessful allocation. Proactive circuit-switching [62] is a recent effort to hide long     	  	 circuit setup time by having request packets reserve circuits for their anticipated response packets as they go toward the destination. Although pre-allocation reduces the latency for those packets that travel on circuits, it requires multiple NOC planes. In addition, early reservation of circuits results in underutilization of network bandwidth. PRA pre-allocates resources for the exact packet transmission time; hence bandwidth loss is minimized. Likewise, a circuit-switched memory access NOC [9], called CIMA, pre-establishes circuits for long response packets. Routers attached to caches set up circuits a few cycles before actual response data transmission. Unlike PRA, CIMA establishes circuits only for response packets, does not use in-network packet stall time to set up circuits, and does not beneﬁt from multihop forwarding. NOC-Out beneﬁts from high-radix ﬂattened butterﬂy topology to reduce hop count and low latency simple routers to reduce per-hop latency [4]. V I I . CONC LU S ION Server processors require a fast fabric for core-LLC communications in order to maximize performance. Due to strict quality-of-service requirements, lean cores are not usually used for execution of server applications. Consequently, even state-of-the-art single-cycle multi-hop on-chip networks impose signiﬁcant delays on the core-LLC communications. This work identiﬁes resource allocation as the major obstacle to a fast on-chip network for server processors that use single-cycle multi-hop networks. To eliminate this obstacle, this work takes advantage of (1) LLC data lookup time, and (2) packet blocking time to proactively allocate resources to packets. Experimental evaluation indicates that our proposal improves system performance over the stateof-the-art single-cycle multi-hop network by 12%. ACKNOW L EDGM EN T The authors would like to thank Mohammad Sadrosadati for his help on the wire-delay analysis, Abbas Mazloumi for his help implementing NOCs in Booksim, and anonymous reviewers for their valuable comments and suggestions. "
2017,Static Bubble - A Framework for Deadlock-Free Irregular On-chip Topologies.,"Future SoCs are expected to have irregular onchip topologies, either at design time due to heterogeneity in the size of core/accelerator tiles, or at runtime due to link/node failures or power-gating of network elements such as routers/router datapaths. A key challenge with irregular topologies is that of routing deadlocks (cyclic dependence between buffers), since conventional XY or turn-model based approaches are no longer applicable. Most prior works in heterogeneous SoC design, resiliency, and power-gating, have addressed the deadlock problem by constructing spanning trees over the physical topology; messages are routed via the root removing cyclic dependencies. However, this comes at a cost of tree construction at runtime, and increased latency and energy for certain flows as they are forced to use non-minimal routes. In this work, we sweep the design space of possible topologies as the number of disconnected components (links/routers) increase, and demonstrate that while most of the resulting topologies are deadlock prone (i.e., have cycles), the injection rates at which they deadlock are often much higher than the injection rates of real applications, making the current solutions highly conservative. We propose a novel framework for deadlock-freedom called Static Bubble, that can be applied at design time to the underlying mesh topology, and guarantees deadlock-freedom for any runtime topology derived from this mesh due to powergating or failure of router/link. We present an algorithm to augment a subset of routers in any n×m mesh (21 routers in a 64-core mesh) with an additional buffer called static bubble, such that any dependence chain has at least one static bubble. We also present the microarchitecture of a low-cost (less than 1% overhead) FSM at every router to activate one static bubble for deadlock recovery. Static Bubble enhances existing solutions for NoC resiliency and power-gating by providing up to 30% less network latency, 4x more throughput and 50% less EDP.","2017 IEEE International Symposium on High Performance Computer Architecture Static Bubble: A Framework for Deadlock-free Irregular On-chip Topologies Aniruddh Ramrakhyani School of ECE Georgia Institute of Technology aniruddh@gatech.edu Tushar Krishna School of ECE Georgia Institute of Technology tushar@ece.gatech.edu Abstract—Future SoCs are expected to have irregular onchip topologies, either at design time due to heterogeneity in the size of core/accelerator tiles, or at runtime due to link/node failures or power-gating of network elements such as routers/router datapaths. A key challenge with irregular topologies is that of routing deadlocks (cyclic dependence between buffers), since conventional XY or turn-model based approaches are no longer applicable. Most prior works in heterogeneous SoC design, resiliency, and power-gating, have addressed the deadlock problem by constructing spanning trees over the physical topology; messages are routed via the root removing cyclic dependencies. However, this comes at a cost of tree construction at runtime, and increased latency and energy for certain ﬂows as they are forced to use non-minimal routes. In this work, we sweep the design space of possible topologies as the number of disconnected components (links/routers) increase, and demonstrate that while most of the resulting topologies are deadlock prone (i.e., have cycles), the injection rates at which they deadlock are often much higher than the injection rates of real applications, making the current solutions highly conservative. We propose a novel framework for deadlock-freedom called Static Bubble, that can be applied at design time to the underlying mesh topology, and guarantees deadlock-freedom for any runtime topology derived from this mesh due to powergating or failure of router/link. We present an algorithm to augment a subset of routers in any n×m mesh (21 routers in a 64-core mesh) with an additional buffer called static bubble, such that any dependence chain has at least one static bubble. We also present the microarchitecture of a low-cost (less than 1% overhead) FSM at every router to activate one static bubble for deadlock recovery. Static Bubble enhances existing solutions for NoC resiliency and power-gating by providing up to 30% less network latency, 4x more throughput and 50% less EDP. Keywords-Networks on chip; Deadlocks; NoC Power Gating; NoC Fault tolerance; Irregular topologies; I . IN TRODUC T ION With increasing core count in chips [1], [2], Networkon-chip (NoC) has today emerged as the de-facto on-chip communication fabric because of its proven scalability compared to the bus based interconnect [3]. Regular topologies such as meshes are preferred on-chip due to their simplicity and ease of layout. However, in future, we should expect to see increasing instances of the on-chip topology becoming irregular. As Fig. 1 shows, this could occur at design time - due to heterogeneous sized big cores, little cores, GPUs, and other accelerators interconnected together-, or at runtime during the lifetime of a chip - due to link/node failures [4], [5], [6], [7], [8], or even due to power-gating of network elements such as routers/datapaths/link-drivers [9], [10], ∗We thank Chia-Hsin Chen and Suvinay Subramanian from MIT for feedback on the motivation for this work, and Swati Gupta from MIT for helping us create a closed form for the static bubble placement algorithm. 2378-203X/17 $31.00 © 2017 IEEE DOI 10.1109/HPCA.2017.44 253 	  	      	  	   Figure 1: Irregular On-Chip Topologies due to (a) Heterogeneous SoCs (b) Router or Link failures/gating. ""!!                    )! '! %! #! ! ""' "" "" ' ""   "" # ' # "" $ ' $ "" % ' % "" &    	 ' & "" ' ' ' "" ( ' ( "" ) ' ) "" * ' * Figure 2: Percentage of deadlock-prone irregular topologies for a given number of faulty/absent/off routers and links in a 8×8 Mesh. (See Section V-A for simulation methodology). [11], [12]. A key problem in irregular network topologies is that of deadlocks. A deadlock occurs when there is a cyclic buffer dependency chain in the network such that no forward progress can be made (Fig. 1(b)). The problem of deadlocks becomes more severe in irregular topologies as these topologies offer much less pathdiversity compared to a regular topology like a Mesh and thus are more prone to deadlocks. In Fig. 2 we sweep the design space of all resulting topologies as the number of links and routers in an underlying 8×8 mesh substrate are removed, and count the percentage of topologies which are deadlock-prone1 , i.e., have cycles in their topology graph. Even with very few random faults, we see that all the probable topologies are deadlock-prone. This evinces the need for providing a solution to this problem for functional correctness of the chip. Beyond 65 link and 30 router faults, the resulting topologies are heavily partitioned and no longer have any cycles. However at this point, the chip itself may be unusable if certain key components such as the memory controller become unreachable. Deadlocks in irregular topologies is one of the key themes 1We obtain this plot by injecting a ﬂit every cycle from every node for a random destination in every topology for a million cycles, and observing if the network deadlocks. Each ﬂit randomly chooses from one of its possible minimal routes without any routing restrictions. A network with zerofaults is also deadlock-prone by deﬁnition, unless a deadlock-free routing algorithm like XY is chosen.  	        topologies only start to deadlock at injection rates around 0.1-0.3 ﬂits/node/cycle, which are fairly high since most real workloads on multicores have an order of magnitude lower network injection rates due to high L1 hit rates, as we observed via full-system simulations on a 64-core system with PARSEC 2.0 [16] and Rodinia [17] benchmarks. Based on this insight, we make a case for deadlockrecovery, rather than avoidance, for heterogeneous/resilient NoCs going forward. The only known techniques for deadlock recovery rely on escape VCs providing a deadlockfree route to drain deadlocked ﬂows [18], [19], [20]. While this can address the ﬁrst challenge of non-minimal routing prior to deadlocks, it does not address the second challenge of constructing a deadlock-free route over the irregular topology for the escape VCs. Moreover, the “deadlock-free route” may disallow certain links and make parts of the original NoC inaccessible as soon as a deadlock occurs, making this approach infeasible. In this work, we address both the listed challenges. We present a novel plug-and-play framework for deadlock recovery that can be applied to any mesh topology at design time and guarantees deadlock-freedom across any topology (regular/irregular) derived from this mesh, either statically (to build a heterogeneous SoC) or at runtime (due to faults/gated components). There are no escape channels with routing restrictions. All ﬂows can use minimal routes all the time, removing the spanning-tree construction challenge completely and associated performance/energy penalties. Our framework consists of two components: (1) A novel algorithm to augment a subset of routers in a mesh (21 in 64 core, 89 in 256 core) with an additional buffer called Static Bubble at design time, such that any dependency chain passes through at least one node with a static bubble. (2) A low-cost FSM microarchitecture embedded in every router that intelligently activates (and deactivates) static bubbles upon detection of a deadlock and performs recovery. We demonstrate that Static Bubble is a signiﬁcantly more performance, energy and area efﬁcient solution than conservative spanning-tree or escape-channel based approaches. Across the irregular topology design space sweeping both 4× throughput, and 53% network EDP improvement with router and link faults, we demonstrate a 20% latency, up to synthetic and real (PARSEC and Rodinia) apps. Since our framework is general-purpose and plug-and-play, it is easy to design and verify, and can augment MPSoC topology generators [13] and current state-of-the-art NoC resiliency and NoC power-gating solutions. The rest of the paper is organized as follows. Section II discusses background and related work. Section III introduces our algorithm for static bubble placement and Section IV presents the microarchitecture and implementation of our deadlock recovery scheme. Section V presents evaluation results and Section VI concludes. Figure 3: Heat-map of the cumulative frequency distribution of irregular topologies that deadlock at a particular injection rate for a given number of faulty links with uniform random trafﬁc. (See Section V-A for simulation methodology). in works across 3 domains: heterogeneous SoC synthesis [13], NoC resiliency [4], [7], [5], [6], [14], [8], and NoC power-gating [9], [10], [12]. It gets exacerbated in the resiliency and power-gating domains as the irregular topology changes dynamically. The most common solution to address this, is to construct spanning trees over the irregular topology, and route packets in all Virtual Channels (VCs) (or within an escape VC) via the root to avoid deadlocks. This approach has two challenges: (1) Routing via the root makes certain routes non-minimal (for instance A’s packet being routed via the root to B (10 hops) instead of minimal (2-hops) in Fig. 1, to avoid a cyclic dependency) and reduces path diversity. This adds latency, throughput, and energy penalties at the networklevel, which in turn affect full-system runtime, as we show in our evaluations. There is also a huge variance in the potential performance impact, depending on the topology instance and spanning tree. (2) Constructing an optimized spanning tree across all possible root nodes, while maintaining a high-connectivity, reducing average hops, and providing sufﬁcient bandwidth over the irregular topology is an exponential state-space search, and often requires optimization solvers [15] running in software and 1000s of cycles for reconﬁguration [5]. A signiﬁcant body of work in the NoC resiliency domain is solely focused on coming up with better heuristics for hardware/software co-solutions for this unavoidable problem [4], [5], [8]. Unfortunately, this state space exploration is required every time a new link/router turns off/on or fails, since the optimal/heuristic solution may be very different, which adds to design and veriﬁcation complexity. And the resulting performance can still be up to 2-4× worse than that with all minimal routes (for Rodinia workloads (Section V)). In this work, we make the following key observation despite most irregular topologies being deadlock-prone as Fig. 2 showed, the chances of deadlocks actually occurring at runtime are fairly low. Fig. 3 plots a heat map of the percentage of topologies that deadlock at increasing injection rates with uniform trafﬁc, as a function of increasing number of disconnected links in a 8×8 mesh. Most 254 I I . BACKGROUND AND R E LAT ED WORK We present mechanisms for NoC deadlock-freedom employed by recent solutions across resiliency and power gating domains, providing adequate background as necessary. A. Deadlock Avoidance Turn Models. Traditionally, regular topologies like Mesh have relied on using deadlock avoidance schemes like dimension-ordered XY routing [21] to achieve deadlock freedom. These schemes, based on the turn model given by Glass and Ni [22], place turn restrictions to avoid cyclic dependencies. For instance, in XY routing, ﬂits are restricted from making a Y (North/South) to X (West/East) turn. However, turn model based schemes rely on having at least two paths, and do not work in a scenario where the topology is irregular and changes dynamically. This is because the turn restrictions may lead to certain core/set of cores not being able to communicate with others leading to a deadlock or a disconnected topology even though healthy and fully functional links are present in the otherwise irregular topology that connect them [23]. Spanning Trees. To overcome this limitation of traditional deadlock avoidance schemes, state-of-the-art NoC designs in the resiliency and power-gating domains construct a spanning tree over the surviving nodes and links and use it to route packets in the irregular topology and avoid deadlocks [4], [5], [8], [6], [12], [10]. Ariadne [4] adapts the topology-agnostic off-chip up-down [24] routing algorithm to ﬁnd deadlock-free paths in the irregular topology. Updown routing enforces strict ordering of nodes in the network by marking the links towards the root node as up, those away as down, and arbitrarily tagging the equidistant ones [8]. All cyclic dependencies in the network are broken by disallowing turns from a down-link to an up-link. uDIREC [5] extends this work to cover unidirectional link failures by modifying the methodology of spanning tree construction. Panthre [12], a recent work in the NoC power-gating domain, also leverages up-down routing. Spanning tree based routing, however, makes certain paths non-minimal. We model this as our ﬁrst baseline in the evaluations. Alternate approaches in Resilient NoCs. Vicis [7] uses a heuristic to determine routing turn restrictions for deadlock avoidance. This heuristic however fails to guarantee deadlock freedom as prior works point out [4]. Immunet [6] uses local Bubble Flow Control (BFC) [25] in a ring constructed using the spanning tree of remaining nodes in the network. This work however uses three routing tables and offers poor performance compared to our ﬁrst baseline [4]. BLINC [8] and Balboni et al. [26] use segment routing [23], where the network is divided into segments, each with a different turn restriction. They, however, place a restriction on the number and/or the location of faults and thus cannot handle arbitrary irregular topologies. Wachter et al. [27] partition the VCs into 2 classes where each class uses a different deadlockfree turn-restriction based routing (for example west-ﬁrst for Class I and east-ﬁrst for Class II). A packet requesting an illegal turn in Class I is put into Class II. However a packet in Class II cannot go back to Class I. This again has the limitation of not being able to guarantee connectivity in any arbitrary irregular topology. Fattah et al. [14] use deﬂection routing [28] on encountering faulty links, but this adds high complexity for achieving deadlock and livelock freedom. Alternate approaches in Power-gated NoCs. Two recent techniques, Power Punch [11] and CatNap [29], maintain network regularity for routing purposes by switching-on routers that fall in the path of the ﬂits which are routed using deadlock-free XY routing. This is orthogonal to our work as we target irregular topologies (both static and dynamic). B. Deadlock Detection and Recovery Traditionally, deadlock detection and recovery has not been a very popular approach for achieving deadlock freedom in regular topologies like a Mesh because deadlock avoidance schemes like XY are easier to implement, while providing adequate path diversity to trafﬁc despite turn restrictions. In irregular topologies, however, as discussed earlier, traditional deadlock avoidance schemes do not work, while spanning-tree based schemes provide non-minimal paths to trafﬁc. Based on our analysis in Fig. 3 that shows deadlocks to be rare in irregular topologies at low loads, we look at deadlock recovery schemes as a possible solution to provide minimal paths to the network trafﬁc and guarantee deadlock freedom at the same time. DISHA [18] was an early work in the deadlock recovery domain that detected deadlocks using counters present in each buffer queue in the network. Upon the detection of a deadlock, a router would wait to capture a token that circulated through the network all the time using dedicated links. After capturing the token, the packet would be put in a reserved network of buffers (one buffer per router) and routed minimally. After the packet reached its destination, the token would be released, breaking the dependency chain. DISHA and its recent variants [19], [20] will not work in a dynamically changing irregular topology as they need a path connecting all nodes to circulate the token. Computing this path in a dynamically changing irregular topology is a non-trivial task. In addition, the scheme uses XY routing for its reserved network which as pointed out earlier, cannot guarantee packet delivery between any source and destination pair in an arbitrary irregular topology. If DISHA were to use spanning-tree based routing for the reserved network it would still be inefﬁcient due to the latency and energy overhead of the circulating token. Ping and Bubble [30] was a subsequent idea to DISHA for off-chip networks that sends a ping from an output port upon detection of a possible deadlock. The ping traverses a control network to trace the deadlocked dependency chain and reserve the output ports at all routers along the route. If 255 the ping returns, it is a deadlock and an extra deadlock buffer is turned on to drain the deadlock (and turned off when the bubble returns). False positives may occur, i.e., paths may be reserved even if there was no real deadlock, but the design does not handle them. This scheme was proposed for offchip networks where area and energy are not as precious as in on-chip router implementations. We employ a similar ping for the dependence chain detection phase. Escape VCs. An alternate approach to spanning trees for achieving deadlock freedom in an irregular topology is to use an escape-VC [31]. In this approach, all VCs use deadlock-prone minimal paths to route trafﬁc except one (the escape-VC) which uses a deadlock-free routing path. Deadlocked packets drain out using the escape-VC. The concept of escape VCs can be used as an avoidance scheme, if packets can actively go into it, or as a recovery scheme if they are enabled upon detection of a deadlock. Escape VCs require an additional VC (buffer) per message class per input port at every router in the network. A separate routing table is also required for identifying the deadlock-free escape path in the irregular topology. In addition to the energy and area overhead of the extra buffers and routing tables, prior works [4] have shown that escape VCs cause throughput loss since one VC per message class per input port always needs to be kept reserved. NoRD [9], recently proposed for NoC power-gating, uses a high-latency deadlock-free ring snaking around the network as the escape VC path. Packets are made to enter the escape-VC after their misrouted hop count increases by a certain threshold. Router Parking [10] replaces the high-latency ring of NoRD with a spanning tree constructed using up-down routing. Deadlocks are detected using a timer and packets in a deadlock get routed using the escape path. Since escape paths based on spanning trees offer better performance (in terms of lower hop count in the escape path) compared to the ring connecting all routers, we model this as our second baseline in the evaluations. C. Bubble Flow Control in Rings Bubble Flow Control [25] is a popular ﬂow control technique for ring topologies (or each dimension in a Torus) that avoids deadlocks by ensuring that there is at least one bubble (one empty buffer) in the ring all the time via intelligent injection. In this work we leverage the underlying theory behind this technique: as long as there is one bubble within a dependence chain, there will be no deadlock and forward progress can be made by ﬂits. The Static Bubble scheme places a buffer (called static bubble) in a subset of routers in a mesh via a novel placement algorithm at design time that guarantees the presence of at least one static bubble in any dependency chain in the mesh network. Upon detection of a deadlock, a static bubble is introduced in the deadlocked ring at runtime and a novel ﬂow-control strategy is run to recover from the deadlock and break the dependence chain. Since Static Bubble can 256 (4k+2,4l-1) y (4k+1, 4l) (4k+3, 4l) (4k+2, 4l) (4k+2, 4l+1)  k=1 l= 1  x Figure 4: Placement of static bubbles on a 8x8 mesh at designtime to guarantee a bubble in any possible cycle. handle any possible dependence chain in the mesh network, any irregular topology based on the mesh topology can be made deadlock-free. D. Routing over Irregular Topologies. Prior works across resiliency and power-gating use a mix of hardware [4], [5] and software [8], [10], [12] techniques to identify connectivity among the currently active routers and links upon detection of a fault or upon turning on/off certain nodes. Disconnected components are discarded, and routing tables are populated at the source NI or at every router. We leverage this rich body of work, and add a routing table at every source NI that populates every packet with a route to its destination. In our spanning tree baseline, this route is spanning-tree based and may be non-minimal, while for escape VC and Static Bubble, this route is minimal (but deadlock-prone). For the escape VC baseline, a spanning tree routing table is used within the escape VCs. I I I . S TAT IC BUBB L E P LAC EM EN T We present an algorithm for the placement of static bubbles in an arbitrary n×m mesh topology that guarantees that there will be at least one static bubble in every possible cycle within every possible irregular topology on the underlying mesh, without having to add a bubble to every router. The algorithm describes a systematic way to decide the placement, but alternate hand-optimized placements, some with fewer static bubbles, are also possible. For node (x, y ) in any n×m mesh, we add a static bubble if x > 0 and y > 0 (i.e., no bubbles on the ﬁrst row and column), and any one of the following conditions hold: (1) x mod 4 ≡ y mod 4 (2) x mod 4 ≡ 1 and y mod 4 ≡ 3 (3) x mod 4 ≡ 3 and y mod 4 ≡ 1 Fig. 4(a) shows the placement of 21 static bubbles in a 8×8 Mesh. Visually, the nodes on the solid diagonals satisfy condition (1), while the ones on the dotted diagonals satisfy conditions (2) or (3). Lemma: There is at least one static bubble in every possible cycle within the mesh. Proof: Starting at node (x,y) in a mesh, any cyclic buffer dependency chain needs to return to the same node (x, y). Case I. Node (x, y) itself contains a static bubble. The proof is trivial in this case since any cycle going through it will have at least one static bubble. Case II. Node (x, y) does not contain a static bubble. The coordinates of any such node (except on the ﬁrst row and column) will be of one of the following 5 forms: (4k+2, 4l), (4k+1, 4l), (4k+3, 4l), (4k+2, 4l−1), (4k+2, 4l+1), or the mirror images of this (swap k and l). Fig. 4(b) demonstrates this. All ﬁve nodes are bounded by static bubbles. Every hop or turn is an increment or decrement of k or l. It is not possible to make 4 turns (the requirement to get a cycle2 ), without encountering a node that satisﬁes one of the 3 conditions of the placement algorithm3 . As a corollary, any irregular topology derived from such an underlying mesh will also have at least one static bubble in any dependence cycle. Even if the nodes with static bubbles are themselves faulty/turned-off, the dependence chain gets broken and the network will still be deadlock free. The same static bubble node could be part of multiple dependency chains and resolve deadlocks in all of them (Section IV-B). The number of static bubbles in a n×m mesh used by our algorithm is: 4 −1](cid:2) [ m (min(m − 4k , n) − 1) + 2 ](cid:2) [ m k=0 4 −1](cid:2) [ n (min(m, n − 4p) − 1) + l=1,lodd 2 ](cid:2) [ n + (min(m − 2l, n) 2 (min(m, n − 2r) 2 p=1 r=1,rodd (1) where [] represents the Greatest Integer Function (GIF). The bubble count scales linearly with the min. of (m,n) which keeps the complexity of the scheme low. IV. D EAD LOCK R ECOV ERY W I TH S TAT IC BUBB L E S We deﬁne Static Bubble (SB) routers as the nodes that the algorithm in Section III picks. In each SB router, we assign one extra packet-sized buffer4 called static bubble and one special counter with a ﬁnite-state machine at design time. When the system starts, all the static bubbles are off; they are turned on by the counter FSM upon detection of a deadlock. The FSM has 6 states, as shown in Figure 5, and manages deadlock detection and recovery. The counter can count5 from 0 up to two possible thresholds (depending on the FSM state): tDD (DD = Deadlock Detection), which is a conﬁgurable parameter and tDR (DR = Deadlock Recovery), which is set dynamically based on the length of the deadlocked cycle. There are four special messages that aid in deadlock detection and recovery: probe, disable, check probe and enable. 2We assume packets cannot take 180 degree, i.e., u-turns in our design. 3 The ﬁrst row and column do not have static bubbles since turns in all directions are not possible and thus fewer bubbles are required. 4 For simplicity, we size the static bubble to be as deep as data packets (5-ﬂits); though sometimes it may be occupied by 1-ﬂit control packets. 5 The counters can be off if the entire mesh is ON with no faults/gated components and using say XY routing. 257   """" &$ $"" $ #  %$!%$ ##  	    ""&   $& ""#$ 	 #$ ! %$"" '$""#  $&#  $& #$ ! %$""   ""& #$& ""$ %$""! $"" ""#$ 	 ""# $&#&#$& ""$ %$""! $""""#  	    ""&#$ ""!$ ""##   %$""##    	   	""#$""$ %$""  $$	%  $$   %$""# #   # %$    	   	""#'$ 	 ""# #	 	 ""&#'$ 	 #$ ! %$""  %$""#  #   %$""# # 	       ""& #$ #   !%$ !"" ""$(%"" #'$ 	 #$ ! %$"" 	 	  Figure 5: Finite State Machine of Counter A. Walk-through Example The FSM starts in the SOF F state. When a new ﬂit arrives at the router (at any port other than the local), the state is changed to SDD and points to the VC it occupies with the threshold set to tDD . This deadlock detection threshold is a conﬁgurable parameter in every static bubble router. If the ﬂit leaves within the threshold time, the FSM points to the next non-empty VC (VC in active state) in the router in a round-robin manner and the counter is reset and restarted. If all VCs at the router are idle, the FSM goes back to SOF F . We explain the operation of the FSM and all its states using the walk-through example in Fig. 6. Each buffer dependence in the ﬁgure is marked with the packet(s) that want to use it to go to the next hop. As can be seen, there exists a deadlock due to the following cyclic buffer dependency chain: (A,B)→(C)→(E,F)→(G,H)→(I,J)→(K)→(A,B) Each VC can hold one packet. We assume Virtual CutThrough (i.e., packet-sized VCs) and describe all dependencies at the packet level for simplicity, though a ﬂit-level design would also work. 1) Probe Traversal (Fig. 6(a)): The counter at node 5 reaches its threshold in SDD state (Step 1) as packet I does not leave within the threshold time. Node 5 sends out a probe message (Step 2) from the North output port (output port for packet I) to detect if there is actually a deadlock, and not a false positive due to congestion. The counter is reset and restarts counting with the same threshold tDD . At each router, if all VCs at the input port of the probe are active, the probe is forked out of all the output ports that any of the VCs at that input port are waiting on (except ejection). Otherwise it is dropped. The forking operation creates identical copies of the probe message, so all the information already present in the probe is retained. The router appends the input to output turn (Left Turn (L) or Right Turn (R) or Straight (S)) in each output probe. For instance, packet K wants to go West while packet Z wants to go East, hence the probe is forked out of these output ports (Step 3). Node 2 appends a L to the probe that goes West, while it appends a R to the probe that went East.                                              	 #!  ##'  $#! & ! ) !# * ! ! + !!   , !!%!"" , !% ## .  ""&&""! 	 &&	'  $""& (%  ! 	'$#!!   	        ""#& (a) Probe Traversal.                          $""      	 	 / $$)  #       %$"" $$* ""# #$$   !$,$ -0 2 #$ 3 # ""&""# # &    	  	 / 	 	               +!""  "" $(%"" #$ 4 	   4  $  $ '#$! ""$ ""  ""$ #$ !! 4  %""+ #$ ""     $  $ '#$! ""$ "" #$#$ !! 4 4  +!""  "" $( %"" #$ #'$     1   	 4   (b) Disable Traversal.                          ""   """"%   #"" """"  &  "" $!&*(!  (,+$ +*%#$+ +,*' $!&*(! +$ ,""""!*!)!' !'! ' 	 	          '      	 	      !$""   	         (   	 &     (c) Check Probe Traversal.             ##'    	 	 	 	 	 	 	 	 	 	 	 	  #! 	   $#! ##  # !%!"" %   ) 	 	        )      	 	 $!( ! 	    # !""#! #!%  ( ! ! #& $! !  # !""#! # !%  # !""#! #!%  ( ! ! #& $! !            $!$!  !    	  (d) Enable Traversal. Figure 6: Walk-through Example At node 3, the probe is dropped (Step 4a) as packets M within this time, unless it is dropped, as will be explained and N are waiting to get ejected, and thus are not part of later in Section IV-C. The same value of tDR is also used any deadlock dependence chain. by the check probe and enable messages, as shown later. At nodes 1, 4, 6, and 7, the probe is forwarded out of the A disable message is sent (Step 8), embedded with the south, south, east, and north output ports (Step 4b), and the path of the probe and the node-id of the sender (node 5). turns L, S, L, and L are appended respectively. Upon receiving the disable, each router disables injection When node 5 receives the probe back (Step 5), the of trafﬁc from any other port into the turn speciﬁed by the dependence chain is conﬁrmed, and the path acquired by disable. For instance, node 2 (Step 9) extracts the ﬁrst turn the probe (L, L, S, L, L) is latched in a special buffer called ﬁeld, Left (L) from the disable entering at the south input Turn Buffer (Step 6). port, and identiﬁes that this corresponds to a south to west turn. It stores this in a IO priority buffer and the node-id of the sender in a source-id buffer. It also sets an is deadlock bit to 1. The is deadlock bit, if set, instructs the switch allocator at node 2 to disable injection into the West output port from every input port except the South input port. In other words, no other ﬂit is allowed to enter the detected dependence chain. Node 2 then removes the ﬁrst turn from the disable message and sends it out of the West port. All nodes along the dependence chain (1, 4, 6 and 7) do the same thing (Step 10). At each node the ﬁrst turn is stripped away from the disable message and it is forwarded out. This ensures that the turn corresponding to the node is always the ﬁrst turn in the disable message when it is received, speeding up the forwarding circuitry. What if the counter expires before the probe returns, or all copies of the probe got dropped? The counter restarts and resets, and the FSM sends out a new probe. This however cannot continue inﬁnitely. If there is deadlock, the probe would return. Else things may be moving slow due to congestion. Eventually, the congestion will clear-up and the ﬂit would leave. What if the ﬂit leaves by the time the probe returns? This is just a false positive and does not affect correctness. The next set of actions still occur. 2) Disable Traversal (Fig. 6(b)): Node 5 changes the FSM state to SDisable and the counter threshold to tDR (Step 7). tDR is set to 2 times the length of the path brought back by the probe, as the disable message is guaranteed to return 258     	     	     	            	 	        	 Once the disable is received back at node 5 (Step 11), it begins deadlock recovery (Step 12) by setting its is deadlock bit and the ports for its IO priority buffer to South and North respectively. The static bubble is now switched ON (Step 13), and the FSM moves to SSB Active . In this state, there is no threshold, and the counter does not increment its count. We have now introduced a bubble into the deadlocked ring, and disabled any other packets from entering it except the ones already there. This will allow packets in the deadlocked ring to move forward one step. This can be seen by looking at the buffer occupancy change between Figure 6(b) and (c). Once the SB is switched on, this is conveyed to node 7 via standard credit ﬂow control messages (not shown). Packet G from node 7 comes and occupies it. This allows packets E, C, A and K to each move by 1 hop. Packet G sitting in the static bubble moves to VC1 at node 2 and the static bubble becomes empty again6 . Why is the disable signal necessary? If we turn on the static bubble without the disable, a new packet may come and occupy it. In this case, we will be back to a deadlocked ring without any recovery mechanism. 3) Check Probe Traversal (Fig. 6(c)): Once the deadlocked ring moves forward one step, the static bubble is re-claimed and switched off (Step 14). At this point, the FSM moves to SC heck P robe (Step 15), tDR remains the same as before, and a check probe message is now sent out along the same path as the disable (Step 16). Unlike a regular probe, the check probe is not forked, but simply forwarded along the same dependency cycle as long as at least one VC is still a part of that dependence chain (indicated by the IO priority buffer). If the check probe returns, the static bubble is again switched on and the dependence ring moves forward one more step7 . In the example, the check probe is dropped at node 4 (Step 17) as both the packets (A,D) in north input port VCs do not want to use the south output port. If the check probe does not come back, it indicates that the deadlock due to the previously detected dependency chain has been resolved. 4) Enable Traversal (Fig. 6(d)): If the counter reaches its threshold, and the check probe has not returned, the FSM moves to SEnable , tDR is again retained to be 2 times the length of the path in the Turn Buffer and the counter is restarted (Step 18). An enable message is now sent out along the same path (Step 19), embedded with the turns and the node-id, just 6 If packet G does not want to use the north output port of the router after moving to node 5, and is stuck waiting for some other output port, we still have packet I that wants to go north. Packet I would then move north vacating VC1 at node 5. Packet G would move from the static bubble to VC1 and thus the SB would be freed and re-claimed/switched off. 7 This is an optimization to speed up deadlock recovery. Even if the check probe did not exist, eventually the counter will again expire, and send out a regular probe, repeating the same process. 259 like the disable. Each router along the path checks if the node-id ﬁeld in the enable matches with its source-id buffer, and if it does, clears the is deadlock bit and IO-priority buffers (Step 20 and 21). This resumes normal trafﬁc ﬂow across all ports since the deadlock has been resolved. Once the enable returns to the originating node (Step 22), it resets the is deadlock bit and clears the Turn Buffer and IO-priority buffer (Step 23). The deadlock has been resolved. The FSM points to another non-empty VC (in a round-robin manner), its state is updated to (SDD ), and the counter starts counting up to tDD . If all input ports, other than the local injection port, are empty, the FSM moves to SOF F . Why do routers need to check if the node-id ﬁeld in the enable matches its source-id buffer? It is possible for a router to receive an enable from a different router than the one that sent it the disable, as discussed in Section IV-B. Next, we discuss how the design guarantees deadlock recovery in the midst of multiple special messages from multiple static bubbles. B. The Devil is in the Details A strict priority order (Sec. IV-C) in processing of messages at each node prevents races and ensures all routers in a deadlocked chain maintain a consistent micro-architectural state. At the static bubble node, in addition to the priority order, the FSM provides additional control in processing of messages which ensures that the FSM state cannot be changed by other nodes once it has started the recovery operation. Together these guarantee functional correctness. Here we discuss some of the interesting corner cases. What happens if there are two or more static bubble nodes in a deadlocked cycle and both send out probes? The static bubble node with the higher id is responsible for resolving the deadlock. If a static bubble node receives a probe from another static bubble node with a lower id, it drops that probe, ensuring that only its own probe, sent earlier or later, will complete the full loop and later send out the disable/enable messages. What if there are deadlocks in two cycles that are both sharing only one static bubble? The static bubble will successfully resolve the deadlocks one after the other, depending on which direction it sent the probe out ﬁrst. What happens if a static bubble node sends a probe, followed by a disable, and then receives a copy of its probe back? This means that there are two dependence cycles that this static bubble is part of, in the same direction. Since the disable for the ﬁrst one has been sent, the second probe will be dropped. Once the ﬁrst deadlock resolves, the timeout counter will send out a new probe and resolve the second deadlock if it still exists. Multiple deadlocks can be resolved in parallel by multiple static bubbles, but if the same static bubble is part of multiple deadlocked rings, it resolves them serially. Why do we need to fork the probe? Can we not drop the probe if all VCs at the input port do not want to use the same output port? There may be buffer dependency scenarios where one buffer dependency cycle may depend on another. In the walk-through example, if the probe message was dropped at node 4 and there was such a dependency cycle, the deadlock would never get resolved. Can a probe loop around inﬁnitely due to buffer dependency? No. Each turn takes 2-bits to encode. Since special messages like probes, disables, etc. are all one ﬂit messages, in a 64 core mesh assuming 128-bit wide links, the probe can only carry a maximum of 59 turns (3-bits for message type + 6 bits for sender node-id). After the turn capacity of the probe is exhausted, it is dropped. Can false positives (i.e., no real deadlock) lead to enabling of the static bubble? If there is no dependence cycle, the probe will get dropped without returning. There may however be dependence cycles due to congestion which make the probe return. In this scenario, the disable is sent out. If any of the intermediate nodes, including the sender, no longer have the same buffer dependence as earlier, the disable is dropped. In some cases congestion may lead to the probe and disable successfully returning, and the static bubble turning ON. It will let the dependence chain move forward by one and turn OFF again, and so there is no correctness problem. What if a node receives two probes or disables or enables in the same cycle? Send the one with the higher node-id and drop the other. The FSM at the sender of the dropped probe/disable/enable will handle retransmissions. Can a non static bubble node receive more than one disable, one after the other? A node will not receive more than one disable signal from the same sender. But it can be part of two dependence cycles and receive disables from static bubble nodes in each cycle. If the is deadlock bit is already set, the second disable would be dropped. What happens if a disable gets dropped midway and does not return to the sender node? The static bubble router FSM in SDisable state will timeout and send an enable (Figure 5) since the nodes that the disable went to before getting dropped would have processed it and placed injection restrictions, which now need to be removed. Can a static bubble node receive a disable/enable when it has itself sent a disable/enable? The same static bubble may be a part of two dependence cycles. In the ﬁrst cycle it may be the highest id and send a probe and then a disable/enable. In the second cycle it may have the lower id and let the probe from the higher id pass through earlier. Now it receives an enable/disable from that cycle. Since it is in SDR state, it will drop that disable/enable. Effectively the ﬁrst chain clears its deadlock followed by the second one. 	   VC out-port info   Msg_Type       x u m e D               Mux             !    Mux VC out-port info                E W N S E W N S         ! Disable/Enable_Sel 	 	  	    !   Disable/Enable x u m e Mux D        	""   Stat ic  Bubble Turn  Buffer Counter:         	   Figure 7: Router Microarchitecture: Probes/Disable/Enable Circuitry (in gray). Additional Which state the does the FSM of a static bubble node go to, if it receives a disable from a higher-id static bubble node? The counter would go to the SOf f state. When the enable messages arrives, the counter pointer will be incremented to point to a non-empty VC and its state changed to SDD . What if a node receives an enable from a node that is different from the node that sent it the disable? This can happen since different VCs at the various ports of a node can be part of multiple dependence chains. If the node-id carried by the enable does not match the source-id stored at the node, the enable message is not processed and is simply sent out of the port calculated from the turn, not dropped. C. Router Microarchitecture We use the following features to make our scheme lowcost and plug-and-play: • All the special messages: probe, enable, disable and check probe are not buffered anywhere in the network. When these messages arrive at a node, they are either sent out of their intended output port or dropped at the node. Thus, the transmission of these special messages is completely bufferless which saves area and energy. • All the special messages use the same links as the regular ﬂits and get higher priority8 . Thus there are no • The processing of the special messages is carried out in parallel units off the critical path of the router pipeline for ﬂits. The only component we add is a mux at the output port that selects between different messages (including ﬂit) during the link traversal stage. Implementation of the design in 32nm DSENT [32] shows that this does not increase our critical path, which is dominated by the switch allocator, and we can still use a state-ofthe-art single cycle router [33]. Fixed Delay of Messages. A highly unique and useful feature of our design is that once a probe returns with additional wires. 8During a deadlock the output links are idle since the ﬂits are stuck, and hence leveraging these for the special messages does not have any major performance implications. 260       the deadlocked path, the delay of disables, check probes, enable is ﬁxed: 2×path length9 . This is due to the traversals being bufferless, and having higher priority over ﬂits. If the message does not return in this time, it means it was dropped. The FSM (Fig. 5) transitions to an appropriate state and transmits an enable in all these cases without having to worry about race conditions. If there is only one static bubble in a dependence cycle, things are simple: a router will receive one of the special messages, as the walk-through example showed. However, if there are multiple static bubbles within a dependency cycle, or a router is a part of more than one dependency chain, multiple static bubbles can start sending probes and other signals which arrive at a router in the same cycle. In any cycle, a router can receive up to 4 special messages (probe/disable/enable/check probe) from its 4 input ports leading to 43 different combinations. Since there are no buffers, the router will forward upto one message from the output port(s) and drop the rest/all. A strict priority order is enforced to guarantee correct deadlock recovery. Figure 7 shows the microarchitecture of each router. The router contains standard units like VCs at each input port, Switch Allocator, Virtual Channel Allocator and the Crossbar Switch. The units colored grey show the components that we have added in a standard NoC router. Each incoming special message is demuxed into an appropriate unit. • Probe Fork Unit (per input port): This unit sends one (or none) probes to every output port. It checks the output ports that the VCs at the input port want to use and creates a copy of the probe for each. If multiple probes want to use the same output, Probe Sel selects the one from the highest node-id and drops the rest. • Enable/Disable Processing Unit (centralized): This unit is responsible for sending one (or none) disable or enable to an output port. It sets (clears) the is deadlock bit, the IO priority buffer and source-id buffer from the disable (enable). If both an enable and disable are received for the same output port, then if the is deadlock bit is set, the enable is sent and the disable dropped, else the opposite happens. • Buffer Dependency Check: (centralized): This unit checks if there exists at least one VC at the input port that wants to use the output port stored in the IO Priority Buffer. If yes, the check probe is forwarded out of that output port, else it is dropped. After going through these 3 units, at the output mux the following priority is enforced by the Msg Sel signal: check probe > disable or enable > probe > ﬂit10 . Static Bubble Routers. At a static bubble router, in 9 It takes one-cycle to process/forward each message in the router, and one-cycle to traverse the link. 10 The switch allocator disables the arbitration for this output port if any special message is received for this output port. 261 Table I: Static Bubble vs. Escape VC Operating Mode Pre-Deadlock Post-Deadlock Control Additonal Buffers in n×m Mesh Area Overhead FSM (Sec IV-C) Equation 1 21 in 64 core 89 in 256 core ∼0% Static Bubble Deadlock Recovery Minimal Minimal Escape VC Deadlock Avoidance or Recovery Minimal Non-Minimal Spanning Tree/Ring Routing Table n×m×5 320 in 64 core 1280 in 256 core 18% addition to the above units we also add the FSM, counter, a static bubble and a Turn Buffer, as shown in Fig. 7. A static bubble is like any other VC. Static Bubble vs. Escape VC. Escape VCs are a powerful framework for deadlock-freedom, and Table I compares Static Bubble against them qualitatively and quantitatively in terms of cost. Performance comparisons are done later in Section V. The deadlock resolution time for escape VCs depends on the misrouting penalty through the tree root; for SB it depends on the length of the deadlocked path, as the disable and enable need to traverse it to resolve the deadlock. Implementation. We implemented the SB microarchitecture in DSENT [32] at 32nm and observed less than 0.5% area overhead compared to a conventional 1-cycle mesh router (where the buffers and crossbar dominate area), and 18% lower area than escape VCs. Moreover, since SB does not require a deadlock-free spanning tree for its operation unlike the deadlock avoidance schemes or escape VC (which needs it for its escape path), we can reduce reconﬁguration cost signiﬁcantly compared to prior works [4], [8]. V. EVA LUAT ION S State-space Exploration with Fault Model. For all A. Simulation Methodology our simulations, we assume an underlying 8×8 mesh. We develop two models, where we randomly inject faults in the network and map them to link failures in one, and router failures in the other, and remove these components from the topology graph. Our fault model is in line with previous works in the resiliency domain [14], [5], [7]. For simplicity, we call these faults throughout this section, though they can can also be viewed as power-gated link-drivers or routers. For each fault number, since the state-space of possible topologies is exponential, a full exploration is infeasible. Instead, we increase the number of topologies till the average value of the trend we wish to study (rate at which they deadlock, average network latency, throughput, application runtime, etc) stabilizes11 . A key observation that Fig. 2 and Fig. 3 show is that at high number of faults/power-gated links or routers, the 11Because of the high symmetry of the mesh topology, many of the generated irregular topologies gave similar results and the trend stabilized within 100 topologies in most cases. Table II: System Conﬁguration Network Conﬁguration Topology Routing Num VCs Latency Flit Size SB tDD 8x8 Mesh Source Routing (Sec II-D) 3 Vnets, 4VCs per VNet per port 1-cycle router + 1-cycle link 128b 34 Random [14], [5], [7] (links & routers) Fault Model Trafﬁc (using gem5 [34] + Garnet [35]) Synthetic Multi-threaded Heterogeneous Uniform Random and Bit-Complement with mix of 1-ﬂit and 5-ﬂit packets PARSEC [16] running on HyperTransport [36] protocol Rodinia [17] traces topologies become highly fragmented lacking cycles and thus do not deadlock. At low number of faults/powergating, though, which is expected to be the common case, a signiﬁcant number of topologies can deadlock. Routing Algorithms. As described in Section II-D, we model a routing table at each NIC that populates the route in each packet, across our baselines and Static Bubble, leveraging prior work on routing over irregular faulty NoCs [4], [8], [5]. With uniform random trafﬁc, if the destination is not reachable (due to disconnected topologies), the packet is simply dropped. With real application trafﬁc (PARSEC2.0 and Rodinia), the application is mapped on cores that are part of a connected sub-network, and only those topologies that do not disconnect the Memory Controllers are considered. B. Conﬁguration and Baselines We use the gem5 [34] full-system simulator with the Garnet [35] network model for our cycle-accurate simulation studies. Network energy and area is estimated using DSENT [32]. We model 32nm and 2GHz. Table II lists the system conﬁguration. We use the following two baselines across all our simulations that reﬂect the state-of-the-art. Deadlock Avoidance with Spanning Tree. We model a deadlock avoidance scheme using an up-down routing similar to state-of-the-art works in NoC resiliency [4], [5], [8], [6] and power-gating [12]. We assume zero cycles to reconﬁgure for spanning tree construction, though this cost is in 1000s of cycles [4], [8]. All packets come embedded with a deadlock-free route (Section II-D). Deadlock Recovery with escape VCs. We model a deadlock recovery scheme, where upon detection of a deadlock, packets move to an escape VC and use a deadlock-free route within that [10], [9]. Packets inside regular VCs use minimal routes set by the source (Section II-D) with 1cycle routers, while escape VCs use a per-router routing table conﬁgured with a spanning tree. Again we assume zero cycles to reconﬁgure for spanning tree reconﬁguration. C. Network Performance and Energy Sweep We start with a performance sweep of the entire designspace of irregular topologies with synthetic trafﬁc. Low-load latency. Fig. 8 plots the average latency beneﬁt that Static Bubble (SB) provides over the baselines at lowloads for (a) uniform-random and (b) bit-complement trafﬁc patterns. Since deadlocks do not occur at this point, both escape VC and SB show the same performance, providing around 22% latency savings with uniform random and 15% with bit-complement trafﬁc on average across all the topologies, with low number of link and router faults. This reiterates our motivation that restricting path diversity in an already irregular topology that the baselines do is not very robust. Beyond 53 link faults, the topologies become highly disconnected with no cycles and very little path diversity in the topology itself, so minimal routes show similar performance to the spanning tree. Throughput. Fig. 9 plots the average network saturation throughput as a function of link and router faults. SB provides up to 3.5 to 4X throughput beneﬁt over a spanning tree. This is due to higher path diversity that the tree limits. We also observe a 1.2 - 1.3X higher throughput than escape VCs. This is due to escape VCs having to reserve one VC solely for deadlock recovery at all nodes, limiting throughput at high loads, while SB reserves an extra VC only at a few nodes. At around 21 router faults, the performance of all 3 designs is very similar. This is because there are very few deadlock-prone topologies at this fault number. Beyond this, the network becomes fragmented, and there are cycles within each fragment which leads to performance improvements. Energy. Fig. 10 plots the network energy using DSENT [32] as routers are turned off. Across the design space, we observe about 10% energy reduction compared to spanning tree, and 20% compared to escape VC which are modeling state-of-the-art power-gating NoC designs [12], [10], [9]. In addition, we see 22% leakage energy improvement at low router faults. SB can thus be used to increase the static energy savings of existing works in NoC power-gating domain. At high router faults, leakage becomes a larger part of the overall energy as the topology becomes fragmented and so the average hop count reduces, leading to a dip in dynamic energy, but SB still provides 20% savings. D. Deadlock Detection Threshold Sweep Next, we study the impact of the Deadlock Detection threshold (tDD ), the only conﬁgurable parameter in our design. Intuitively, a very low value of tDD will result in a lot of probes being sent out while a very high value may delay deadlock resolution. In practice, however, we observed that at low (0.01 ﬂits/node/cycle) and medium (0.1 ﬂits/node/cycle) loads, even with a tDD of just 5-cycles, no probes were sent out since a ﬂit would leave within this time. Fig. 11 sweeps tDD at high loads, when the network is deadlock-prone (Fig. 3), for 10K cycles. The NoC has 20 router faults and the average values across all topologies are plotted. A very low value of tDD results in over 4000 (0.4 per cycle) probes across the NoC. As tDD increases, there 262     	 !           	     	     	     	     	                    	           	                                   	    	 !   	  Figure 8: Avg and Max network latency improvements demonstrated by Static Bubble, normalized to Spanning Tree, across the irregular topology space with uniform random trafﬁc at low-loads. 	  	  	  	                                     	          	 	                                       	   	  	  	  	  	  	                                                             	  	  	  	                               	       		     		 Figure 9: Average network throughput, normalized to Spanning Tree, of all designs across the irregular topology space with uniform random trafﬁc.       !  ""     	              	   	   	                           	    	   	   	   Figure 10: Average Network Energy. Figure 11: Deadlock Detection Threshold Sweep.                                  	                                    		 	   	   	  	   	   	   Figure 12: Scatter Plot of Application Throughput with escape VC and SB, normalized to Spanning Tree, for Rodinia workloads, with increasing link and router faults. is an exponential decline in the number of probes being sent out and saturates to about 200 (0.02 per cycle). While more probes steal bandwidth from ﬂits, it turns out that probes are only being sent when the network is truly deadlocked in which case the links are idle. At this point the network is saturated which is reﬂected by the extremely high packet latency. Recall that the number of probes does not affect the     		   	                   	             Figure 13: (a) Application Runtime and (b) Network EDP for PARSEC with 4 faults. functional correctness of the design; it just affects the time to detect deadlocks and link energy consumption. While we did not see any noticeable difference in the average latency of a ﬂit as the threshold is swept since link traversal by probes is orders of magnitude lower than by ﬂits, a slight improvement in packet latency is seen at low tDD as the deadlocks get detected faster. Fig. 11 also shows the link utilization of the message classes as tDD is varied. Link utilization by probes falls from 5% when tDD is low to 2% at medium tDD to 1.5% at high tDD . The other special messages have constant link utilization (enable(0.45%), disable(0.45%), check probe(0.6%)) at all values of tDD thus showing that these messages use the links only in case of a deadlock and thus do not steal bandwidth from ﬂits which remain the dominant users of the NoC (>93% utilization across all values of tDD ). E. Real Applications. Fig. 12 plots the application throughput of Rodinia [17] benchmarks as a function of link and router faults across the topology space, normalized to the Spanning Tree. At low faults, the system with SB is consistently higher performing than both escapeVC, and Spanning Tree, by up to 2-4X. The only exception is Hadoop which shows similar performance with all systems due to high collective trafﬁc which saturates all the NoCs very early. At high link fault rates, BPlus and SRAD show throughput improvements with SB, while the performance of others drops. At 20 router faults, all designs perform almost identically as very few connected topologies to run Rodinia exist at this fault rate, and there is hardly any path diversity that minimal routes can exploit. We observed deadlock occurrence (and resolution) in Hadoop, BFS, and SRAD for some instances of the topologies at low faults. 263    	    	   	  	 	      	         	                  	 	                            	   	  	   	  	 	      	         	       	     	   	          	           	      	    	    	 Fig. 13 runs a full-system 64-core simulation of PARSEC with 4 link faults. Both escape VC and SB provide ∼15% reduction in application runtime, on average, validating our case for deadlock recovery solutions over Spanning Tree solutions. Results with 32 router faults were very similar. PARSEC workloads have very low network injection rates, and hence no deadlocks were observed which is why SB and escape VC have identical performance. The beneﬁt of SB over escape VC is visible in Fig. 13(b) where the network EDP is plotted. SB has a 53% lower EDP than Spanning Tree, and a 17% lower EDP than escape VC. V I . CONC LU S ION Current solutions for deadlock freedom for irregular topologies (that may occur due to NoC faults or power gating) require expensive spanning tree constructions and non-minimal routing over them. The alternative of using escape VCs still requires such a tree for providing a deadlockfree escape path. We perform a state space exploration and conclude that while most irregular topologies are deadlockprone, the actual occurrence of deadlocks at runtime is rare. We present a plug-and-play solution for deadlock recovery, known as Static Bubble, that augments a set of routers in a mesh with an extra buffer via a novel algorithm that guarantees the existence of at least one static bubble in any dependency cycle. We provide a low-cost mechanism for deadlock recovery and demonstrate performance gains and energy reduction over state-of-the-art solutions. Static Bubble does not require any tree construction and can augment current solutions in the space of heterogeneous design, NoC resiliency and power-gating. "
2017,"Designing Low-Power, Low-Latency Networks-on-Chip by Optimally Combining Electrical and Optical Links.","Optical on-chip communication is considered a promising candidate to overcome latency and energy bottlenecks of electrical interconnects. Although recently proposed hybrid Networks-on-chip (NoCs), which implement both electrical and optical links, improve power efficiency, they often fail to combine these two interconnect technologies efficiently and suffer from considerable laser power overheads caused by high-bandwidth optical links. We argue that these overheads can be avoided by inserting a higher quantity of low-bandwidth optical links in a topology, as this yields lower optical loss and in turn laser power. Moreover, when optimally combined with electrical links for short distances, this can be done without trading off latency. We present the effectiveness of this concept with Lego, our hybrid, mesh-based NoC that provides high power efficiency by utilizing electrical links for local traffic, and low-bandwidth optical links for long distances. Electrical links are placed systematically to outweigh the serialization delay introduced by the optical links, simplify router microarchitecture, and allow to save optical resources. Our routing algorithm always chooses the link that offers the lowest latency and energy. Compared to state-of-the-art proposals, Lego increases throughput-per-watt by at least 40%, and lowers latency by 35% on average for synthetic traffic. On SPLASH-2/PARSEC workloads, Lego improves power efficiency by at least 37% (up to 3.5×).","2017 IEEE International Symposium on High Performance Computer Architecture Designing Low-power, Low-latency Networks-on-chip by Optimally Combining Electrical and Optical Links Sebastian Werner, Javier Navaridas and Mikel Luján The University of Manchester, Manchester, M13 9PL, UK {sebastian.werner, javier.navaridas, mikel.lujan}@manchester.ac.uk ABSTRACT Optical on-chip communication is considered a promising candidate to overcome latency and energy bottlenecks of electrical interconnects. Although recently proposed hybrid Networks-on-chip (NoCs), which implement both electrical and optical links, improve power eﬃciency, they often fail to combine these two interconnect technologies eﬃciently and suﬀer from considerable laser power overheads caused by high-bandwidth optical links. We argue that these overheads can be avoided by inserting a higher quantity of low-bandwidth optical links in a topology, as this yields lower optical loss and in turn laser power. Moreover, when optimally combined with electrical links for short distances, this can be done without trading oﬀ latency. We present the eﬀectiveness of this concept with Lego, our hybrid, mesh-based NoC that provides high power eﬃciency by utilizing electrical links for local traﬃc, and low-bandwidth optical links for long distances. Electrical links are placed systematically to outweigh the serialization delay introduced by the optical links, simplify router microarchitecture, and allow to save optical resources. Our routing algorithm always chooses the link that oﬀers the lowest latency and energy. Compared to state-of-the-art proposals, Lego increases throughputper-watt by at least 40%, and lowers latency by 35% on average for synthetic traﬃc. On SPLASH-2/PARSEC workloads, Lego improves power eﬃciency by at least 37% (up to 3.5×). 1. INTRODUCTION Many high-performance computing (HPC) systems are already equipped with chip multiprocessors that exhibit up to 100 cores [1, 2, 3] – a number expected to further increase. This shift rendered the on-chip network to be the limiting factor in terms of performance and power. Meanwhile, technology scaling has been creating energy and delay bottlenecks in electrical interconnect technologies [4]. It is commonly expected that electrical interconnects will not be able to satisfy the demands of future HPC applications [5]. Enabled by silicon photonics, optical on-chip communication has become a viable candidate to supplement or even replace electrical interconnects. Immense bandwidth scalability through Dense Wavelength Division Multiplexing (DWDM), signal propagation of light, and almost distance-independent energy consumption are compelling properties to assume that optical interconnects might be a key disruptive technology for future many-core chips. Optical Networks-on-chip (ONoCs) could either implement optical links only (all-optical) or combine them with electrical links (hybrid). In either case, their design is a challenging task which requires detailed knowledge of the properties of silicon photonic devices, power and throughput requirements of many-core chips, and a careful analysis of the relationship between optical bandwidth and power consumption. Nanophotonic devices and materials, as well as NoC architectures, are therefore hot research areas and essential to reach the full potential of ONoCs. In this paper, we provide a detailed study of the tradeoﬀs regarding latency and power consumption of optical and electrical links for current technologies, and identify the cases in which one interconnect technology should be preferred to another. Moreover, we conduct a systematic exploration of optical bandwidth vs. power consumption in DWDM links. Based on our study, we propose Lego, a low-power hybrid ONoC design based on a design-friendly, meshbased layout. We utilize electrical links for local communication with direct neighbors, where they provide low latency at low energy costs. Optical links are used in the mesh rows and columns for destinations residing at larger distances where they are superior to electrical links regarding energy and latency. We implement optical links with lower bandwidth than recent proposals and supplement them with electrical links to outbalance latency drawbacks introduced by serialization delays. Our routing algorithm minimizes the number of hops (at most two between any source-destination pair), and always chooses the path that provides the lowest energy and latency by performing distance-based routing to either route on electrical links, optical links, or a combination thereof. Minimizing link and router traversals, and always choosing the lowest energy and latency link, makes Lego highly eﬃcient in terms of latency and dynamic power. 2378-203X/17 $31.00 © 2017 IEEE DOI 10.1109/HPCA.2017.23 265 or even 5μW/ring for some technologies [7] [8]. Athermal microring resonators are also a hot research topic which would reduce temperature dependency to a tolerable level that would cancel the need for ring heating of microrings [9] [10]. Ring heating power is therefore likely to be manageable in the near future. Laser power depends on losses of silicon photonic devices, which require the laser source to provide more output power to drive all receivers at satisfactory biterror rates. The optical path that introduces the highest insertion loss (I Lmax ) determines the output power per wavelength. Current laser eﬃciencies and device losses require a very careful design of optical links to avoid excessive laser power [11]. Although devices are evolving, there is no clear roadmap for nanophotonics. The NoC layout should therefore aim to minimize I Lmax . Besides I Lmax , laser power depends on the number of wavelengths provided by the laser source and the number of readers/detectors it has to drive, depicted in Fig. 2 and Fig. 3, respectively. We modeled laser power with DSENT [12] using IL parameters listed in Tab. 2. Fig. 2 demonstrates the relation between laser power and number of wavelengths in an 10mm Single-WriterSingle-Reader (SWSR) bus. Laser power grows exponentially with increasing number of wavelengths since more wavelengths do not only have a direct eﬀect on the laser source itself, but also increase I Lmax as they lead to more microrings that are passed by wavelengths (’ring-through’ loss), as well as crosstalk noise which signiﬁcantly increases with the number of wavelengths [11]. From a power perspective, it is therefore more eﬃcient, for instance, to implement two SWSR buses with 16λ each rather than one 32λ bus. A larger number of low-bandwidth links in a topology could thus be more power-eﬃcient than a small number of high-bandwidth links shared by a number of nodes. These eﬀects are further aggravated when there is more than one reader attached to an optical bus, as shown in Fig. 3, as even more microrings lead to even higher I Lmax . Increasing numbers of readers lead to higher laser power since the laser source has to drive more photodetectors, and is increasingly critical with higher bandwidth. It is therefore desirable to keep the number of readers and wavelengths of an optical bus as low as possible; however, as these two aspects also determine bandwidth/throughput, it is crucial consider these aspects carefully for low-power, low-latency designs. 2.1.2 Electrical Links Global electrical wires have become increasingly energyhungry in many-core architectures [4], as they require repeaters, regenerators or buﬀers to provide satisfactory signal integrity and latency, with increasing energy consumption for longer link lengths. Fig. 4 shows the diﬀerence in energy per 64-bit ﬂit over an electrical and optical link with increasing link length, modeled in DSENT with a 22nm technology. For short distances, the electrical link is more energy-eﬃcient as it does not require E/O and O/E conversion circuitry. However, for link lengths > 1mm, the almost distance-independent Figure 1: Single-Writer-Multiple-Reader Optical Bus We make the following novel contributions: (i) We propose a novel hybrid NoC design Lego that saves power by combining electrical and optical links in an optimal, distance-based fashion without trading-oﬀ latency or throughput. (ii) Optical links are more abundant in Lego than in recent proposals but have lower bandwidth. This saves laser power without latency drawbacks when combined with electrical links for local traﬃc. (iii) Electrical links for neighbor traﬃc allow 1) energyeﬃcient short-distance communication, 2) a much simpliﬁed router microarchitecture and 3) savings of optical resources. (iv) Lego improves throughput-per-watt by up to 4x (40% on average) and packet latency by 35% on average (for synthetic traﬃc), and power eﬃciency by at least 37% (SPLASH-2/PARSEC). 2. OPTICAL VS. ELECTRICAL LINKS Deciding on when to utilize optical and electrical links depends on a number of design aspects aﬀecting latency and power consumption. In this section we discuss these implications for electrical and optical links, and identify their beneﬁts and drawbacks. Fig. 1 depicts a basic Single-Writer-Multiple-Reader optical bus (SWMR) in which Tile 0 sends and x number of tiles receive. A laser source provides wavelengths (λ1 ..λn ) which are coupled into a waveguide. Tile 0 sends data by modulating on the n wavelengths, thus requiring a modulator bank with n modulators. To receive data, tiles implement ﬁlter banks – one ring ﬁlter for each λ. Optical data transmission includes optical data generation and serialization in the electricalto-optical (E/O) backend circuitry, wavelength modulation, waveguide traversal, wavelength ﬁltering, and optical-to-electrical data conversion (O/E) through detection and deserialization. 2.1 Power Consumption 2.1.1 Optical Links Static power is the main contributor to the total power consumption in optical interconnects and consists of laser and ring heater power. The latter is required to mitigate temperature variations and post-manufacturing geometric mismatches of microring resonators. Ring heater power used to be prohibitively high in NoCs on a larger scale (> 10000 rings) [6]; however, signiﬁcant research eﬀorts in ring tuning techniques in recent years lead to a decrease in ring heating down to 20μW/ring 266 ) W m ( r e w o P r e s a L 50 40 30 20 10 0 Laser Power 0 16 32 48 64 80 96 112 128 Number of Wavelengths 100 80 60 40 20 ) W m ( r e w o P r e s a L 0 1 8 Wavelengths 16 Wavelengths 32 Wavelengths 64 Wavelengths 2 3 4 5 6 7 8 Number of Readers ) J p ( t i ﬂ r e p y g r e n E 8 6 4 2 0 Electrical Link Optical Link 0 1 2 3 4 5 Link Length (mm) Figure 2: Laser Power vs. Num. of Wavelengths in a SWSR bus Figure 3: Laser Power vs. Num. of Receivers in a SWMR bus Figure 4: Energy for transmitting a 64-bit ﬂit Table 1: Latency of a 64-bit ﬂit on an optical link. For simplicity: tprop = 1 Number of Wavelengths Serialization Degree Delay (E/O + tprop + O/E) 32λ 16λ 8λ 4λ 1:1 2:1 4:1 8:1 3 (1 + 1 + 1) 4 (2 + 1 + 1) 6 (4 + 1 + 1) 10 (8 + 1 + 1) energy consumption of optical data transmission outperforms electrical links. From an energy perspective, it is therefore only beneﬁcial to utilize electrical links for destinations < ∼1mm. For instance, in a 64-core chip, tile widths/lengths are often between 1-2mm for common die sizes of 225mm2 . This would mean that only communication to direct neighbors should be electrical. Besides, router traversal of a 64-bit ﬂit in 22nm at 5Ghz requires 2pJ, which is similar to the energy needed to traverse a link of 1.3mm - further emphasizing the impact electrical links have on total energy consumption. 2.2 Latency Electrical signal propagation takes 131ps/mm in an optimally repeated wire at 22nm [13]. At 5Ghz, one hop over an electrical link in a NoC is therefore commonly accepted to take one clock cycle (we note that this is also sub ject to clock frequency, layout, ﬁnal link lengths, etc.). Optical links, on the other hand, require E/O and O/E conversions and on-the-ﬂy propagation delay (tprop ), which take at least one clock cycle each (3 cycles in total). However, optical links leverage the signal propagation of light (11.4ps/mm for current technologies [14]), which is particularly beneﬁcial for long-distance communication, especially because optical links, as opposed to electrical, do not require pipelining and do not introduce further distance-related latencies. For instance, with 11.4ps/mm propagation delay, data can be sent within one clock cycle to any core located at distances < 17.5mm assuming a core clock frequency of 5Ghz (200ps clock cycle). Although all optical components add to the optical delay (modulator (3.1ps), detector(0.22ps), E/O(9.5ps) and O/E(4.0ps) [15]) the ma jor contributor is data modulation, i.e. the time it takes to serialize a packet based on the available bandwidth. This is outlined in Tab. 1, which lists the impact on the delay of a 64-bit ﬂit with diﬀerent number of wavelengths, assuming link propagation delay of 1 cycle for simplicity, 10Gb/s modulators and 5Ghz clock frequency. These values are an important guideline to trade-oﬀ power and latency. For instance, increasing link bandwidth from 16λ to 32λ decreases latency only by one clock cycle, but more than doubles laser power (see Fig. 2). Bandwidths lower than 8λ introduce too much latency for too little power beneﬁts. To result in minimum packet latencies, these delays must be compared to electrical delay. Although electrical links do not need E/O and O/E conversions, the only energy-eﬃcient way of reaching distant cores is through several hops in a topology, which includes router delay. Router traversal delay depends on the clock frequency, where high clock frequencies of 5Ghz may need up to 5 pipeline stages, as in Intel’s TeraFLOPS design [2]. If we assume aggressively pipelined routers that can be traversed in two clock cycles (assuming enough link bandwidth), one hop would take 3 cycles. While this delay adds up for each additional hop to reach a destination, hardly any delay is added on optical links when the distance increases (assuming direct connections). From this perspective, optical links are superior when destinations are at 2-hop distances or further away in a topology when optical bandwidth is at least 8λ. 3. NETWORK ARCHITECTURE This section introduces Lego, in which we apply the ﬁndings of the previous section to minimize laser power, dynamic power, and packet latency, by • Minimizing the number of wavelengths available for optical data transmission while still staying superior to electrical links in terms of latency. • Minimizing the number of readers in optical links. • Minimizing energy consumption by 1) using electrical links only for 1-hop distances, 2) using optical links otherwise, and 3) keeping the total number of link traversals low by allowing paths of at most of 2 hops for any source-destination pair. 3.1 Topology Lego arranges nodes in a 2D mesh as this allows an eﬃcient layout of the optical links, as we will discuss in Section 3.1.2. Each node is connected to its direct neighbors via electrical links, illustrated in green in Fig. 5/6. Moreover, they are connected to every 267           Figure 5: Lego8: 8 router groups (four row/column) connect 16 nodes each Figure 6: Lego16: 16 router groups (eight row/column) connect 8 nodes each node in their optical router group with an optical link. We study two diﬀerent variants of Lego, Lego8 and Lego16, which both utilize the same routing algorithm and router architecture, but provide a diﬀerent number and arrangement of optical links. This allows us to study the trade-oﬀ of varying levels of bisection bandwidth and power consumption. Lego8 has four optical router groups in its rows and columns (8 in total), with 16 nodes belonging to one router group. In Lego16, each row/column is an optical router group (16 in total), with 8 nodes belonging to one router group. The optical layout is similar to LumiNoC [16], however, LumiNoC deploys optical links only, resulting in a diﬀerent routing algorithm and router microarchitecture. 3.1.1 Optical Router Group All nodes belonging to the same optical router group are connected to each other in a crossbar fashion, with a distinct SWMR bus for each node. Fig. 7 gives a closeup to an optical router group with 16 nodes, as implemented in Lego8. For simplicity, the ﬁgure only shows 268 Figure 7: Optical router group layout (here for Lego8). Control network has the same layout but is omitted for simplicity. Nodes modulate data on the tx path (red) and receive on the rx path (green). the SWMR buses of a few nodes. Every other node owns an equal, separate bus for sending. We chose a U-shaped layout (like in [16]) of the optical links which allows nodes to reach every other node by modulating data on the transmit side of the link (red). All receiving nodes attached to the bus ﬁlter out the optical data on the receive side (green). Optical router groups with 8 nodes like in Lego16 have the same layout like in Fig. 7, just without the lower row of nodes (08 -15). In our topology, nodes do not need modulators, ring ﬁlters and detectors for communication with its direct neighbors because they are connected to them electrically. This reduces the total number of microrings, and thus area and ring heater power. For instance, in Fig. 7, node 01 and 08 have no ring ﬁlters on the SWMR bus of node 00, because our routing algorithm will choose electrical links for neighbor traﬃc. The same applies to every other node in our topology and its neighbors. The diﬀerence between Lego8 and Lego16 topology-wise is that the former provides fewer optical link groups, but in each group twice as many nodes are connected. This allows to reach more nodes in one-hop distance, decreasing zero-load latency. However, this provides less bandwidth as each node has to use the same optical link to reach a larger number of nodes. Lego16 has twice as many optical link groups, but connects half the number of nodes, which decreases the number of nodes in one-hop distance, and thus zero-load latency, but also increases bandwidth which might be beneﬁcial for certain workloads and injection rates. More optical links, however, also lead to higher laser and ring heater power. In Section 4, we study these diﬀerent topological considerations and their eﬀect on latency and power. Control Network. Based on our study in Section 2, we aim to keep the number of receivers on the SWMR buses as low as possible to result in low laser power. So far, for N number of nodes attached to each router group and n number of the sender’s direct neighbors, the number of readers the coupled laser source of each SWMR bus has to drive is N − n − 1 (−1 for the sender itself, N being either 8 or 16). This would result in high laser power overheads as Fig. 3 illustrates. We therefore implement a parallel, low-overhead control network for each SWMR bus, similar to the ’reservation-assisted’ SWMR bus in [17]. The purpose of this control network is to control the receiver’s ring ﬁlters so that, at any time, only one node on the receive side of the bus ﬁlters out modulated data. This signiﬁcantly reduces laser power as it sets the number of readers the laser source has to drive to one. This is enabled by ring heaters capable of tuning microrings by shifting their resonance wavelength(s) so that they respond to particular wavelengths, or detune them to let wavelengths pass without ﬁltering them - allowing to switch on and oﬀ microrings as desired. Tuning speeds have been sub ject to extensive research [18, 19, 20]. Recent studies found microrings to optically stabilize in less than 100ps [18], and total tuning times of at most 500ps [19] - 2-3 clock cycles at 5Ghz clock frequency. As E/O conversion and optical signal propagation takes at least one cycle, we pipeline data transmission by letting the sender start sending one cycle before ring tuning is ﬁnished. This leads to one clock cycle of ring tuning/detuning delay, with a reasonable assumption of 400ps ring tuning latency. Each node in the router group owns one SWMR control bus for its SWMR data bus to realize this functionality. Both buses have the same layout, merely the number of wavelengths, and thus modulators and ring ﬁlters, differ. Transmitting data over optical links thus obeys the following process: 1. Initially all nodes are detuned and do not ﬁlter the wavelengths. 2. When a node wants to send data, it ﬁrst sends out a control packet containing the destination node and packet size on the control network to all receivers. 3. The destination node tunes in and all other nodes keep their ﬁlters on the data bus detuned. The packet size indicates the duration of which ring ﬁlters have to stay tuned/detuned. 4. The sender starts transmitting its data. Control packets are very small and therefore only require low bandwidth. We support two packet sizes, 64 bit and 576 bit, as they are present in common CPU architectures for miss request/coherence traﬃc and cache line transfers, respectively. Depending on the router group size, either 3 or 4 bits are needed to encode the destination ID (< 8 (Lego16) or < 16 (Lego8), sender and direct neighbors are not part of the possible destinations). Adding one bit to encode the packet size, this makes 4 bits / 5 bits, which can be modulated in one clock cycle with 2/3 wavelengths, respectively. Assuming one clock cycle for packet processing and ring tuning each, this would result in a latency overhead of 5 clock cycles (on-the-ﬂy delay of optical signals in Lego is always 1 clock cycle as the distance between two nodes in a router group is always < 17.5mm). As we will show later, the latency and power overhead introduced by the control network is negligible compared to the laser power savings it provides. We note, however, that this latency overhead slightly distorts our latency analysis in Section 2, and would make electrical links the faster medium for 2-hop distances, too, and not just for direct neighbor traﬃc. We still stick to our restriction to use electrical links only for one hop traﬃc as our analysis in DSENT showed that 2-hops in the electrical domain require 3.5x the energy of 1-hop in the optical domain. We leave the study of whether this large overhead is worth the latency beneﬁts in this case to future work. 3.1.2 Layout Implementing ONoCs with silicon photonics requires a careful consideration of the implications of layout and device technologies. We target low laser power by providing a larger number of low-bandwidth links rather than few high-bandwidth links. It is important to note that this approach decreases laser power only if the higher number of waveguides does not lead to a higher number of waveguide crossings, which could increase I Lmax and possibly diminish some of the power savings. In our layout, waveguide crossings occur when optical links located in the columns cross the ones in the rows. For that reason, we assume 3D-integration with the optical circuitry of row and column router groups placed separate photonic layers to eliminate in-plane waveguide crossings [21]. Depending on the topology, an optical router group has either 8 (Lego16) or 16 (Lego8) nodes attached to it, which requires 16 or 32 waveguides for the data and control network. Although current technologies allow waveguide dimensions of 520nm width [11], suﬃcient clearance is needed to avoid optical signal interference. As microring resonators have a diameter of 5μm [22], we assume a waveguide pitch of 15μm, leaving 5μm clearance [16]. For 16 and 32 waveguides per router group, this layout requires < 0.25mm and < 0.5mm, respectively, for the optical links in the rows and column. With a die size of 225mm2 , this would allow the conventional tile sizes of 1mm2 while providing suﬃcient area for interfacing and placement of the photonic devices in the topology’s rows and columns. Our mesh-based layout is not only benign to VLSI ﬂoorplanning, but also omits the need for a large number of laser sources and allows to place laser sources on the edges of the chip. This is important as chip packaging is one of the ma jor cost factors of silicon photonic chips due to expensive coupling of the oﬀ-chip laser source. Therefore, designs are likely to have to oblige to tight packaging constraints which have to be taken into account by designers. 3.2 Routing Algorithm Our routing algorithm aims to minimize link traversals and always chooses the link that oﬀers the lowest energy and latency to keep dynamic power and latency 269 as low as possible. Therefore, based on the relative position of the sender and destination, the former either sends on an optical link, electrical link, or a combination thereof. We classify routing into four cases, demonstrated in Fig. 8 - 11 in a 6x6 Lego8 design for simplicity (green links indicate hops over electrical links and blue links over optical): 1. Case 1 (Fig. 8): Source and destination node are direct neighbors in the 2D mesh. In this case, the source sends its packet directly to the destination using electrical mesh links. 2. Case 2 (Fig. 9): Source and destination node are in the same column or row group, but not direct neighbors. In this case, the source node uses its column/row optical link to send data directly to the destination node (e.g. node 00 to 07, and node 14 to 23). 3. Case 3: Source and destination node are neither in the same row nor column group. In this case, the source will ﬁrst use its optical row link to send to the node that resides in the same column group as the destination node. Once the packet is received by the intermediate node, two possibilities exist: (a) Case 3.1 (Fig. 10): The destination node is a direct neighbor. The intermediate node proceeds by sending data to the destination via the electrical mesh link (e.g. node 00 to 26, and node 14 to 29). (b) Case 3.2 (Fig. 11): The destination node is not a direct neighbor. The intermediate node then proceeds by sending data to the destination via its optical column link (e.g. node 00 to 35). Each node merely needs to compute is its own and its destination’s position, and possibly one intermediate node in case the destination is not directly reachable. To provide our architectural simpliﬁcations to the electrical network, these are the only existing routing scenarios (e.g. it is not possible to use ﬁrst an electrical and then an optical link). In the worst case (Case 3.2), optical data transmission is performed twice, and router traversal three times. This leads to high eﬃciency both in terms of packet latency and dynamic power consumption. Moreover, the electrical mesh network is simpliﬁed substantially because no actual routing needs to be performed on the electrical links: A node uses a mesh link only if the packet’s destination is on the other side of this link, making routing computation dispensable since each incoming packet on this link is forwarded to the local tile. 3.3 Router Microarchitecture In Lego, each router has to handle data communication through both its electrical and optical ports. We illustrate our router microarchitecture proposal in Fig. 12. Depending on the relative position of the router in the mesh (either at the border or center), the number of output ports for the electrical mesh links varies. In both Lego8 and Lego16, each node has two output ports for the optical links in its row and column group. Optical input and output ports implement the E/O and O/E signal conversion circuitry prior to the input buﬀers. Based on our routing algorithm, incoming data on the mesh links is always intended for the local core. Therefore, no routing computation has to be performed. It suﬃces to store incoming ﬂits in buﬀers and multiplex them to the output port leading to the local core. This requires a small crossbar for arbitration between packets incoming from the mesh links, and those that were received on the other ports. However, this greatly simpliﬁes the router’s crossbar from a 7x7 to 3x7 crossbar, leading to a much lower power and area footprint than conventional mesh routers. For every other input port, routing computation and switch allocation is executed in the conventional way. 4. EVALUATION 4.1 Experimental Set-up We compare Lego to a wide range of the most recent NoC proposals. Several ONoCs utilizing optical links for long-distance and electrical links for short-distance communication have been proposed in recent literature. We chose to compare Lego to the most competitive designs that come closest to our goal of combining electrical and optical links in the most eﬃcient manner, i.e. Atac [23], Fireﬂy [17], and Meteor [15]. In addition, we compare Lego to the state-of-the-art, low-power alloptical NoC QuT [7], and LumiNoC [16]. To outline the beneﬁts of implementing optical links, we also add an electrical baseline 2D Mesh. We use DSENT [12] for area and power estimations, and Sniper [24] for performance and energy modeling of SPLASH-2 [25] and PARSEC [26] applications with the sim-large input set. Results are measured during the parallel phase of the applications after caches have been warmed up. Sniper is conﬁgured according to Xeon X550 Gainestown chip multiprocessor [27], and uses private L1I and L1D caches (32kB) and a shared LLC (16MB), with memory controllers placed on the top and bottom rows. We use a 22nm technology, 5Ghz clock, 10Gb/s modulators/detectors, and a die size of 225mm2 with square tiles. For synthetic traﬃc, we use the cycle-accurate simulator HNOCS [28], and assume a data packet size of 256 bits and ﬂit size of 64 bit. We study our NoC topologies for 64 nodes and assume an 8x8 layout. Atac [23] consists of a 2D electrical mesh that is overlayed by an optical network (ONET). In the 64-node version, each node is connected to the ONET, which is a bundle of 32 SWMR links that carry 64 wavelengths each. Packets for destinations less than four hops away are sent on the electrical mesh, and on the ONET otherwise. Both electrical and optical links are 64-bit wide. Atac+ [10] improves Atac by adding a more performance-eﬃcient star network and an adap270 Figure 8: Routing Case 1 Figure 9: Routing Case 2 Figure 10: Routing Case 3.1 Figure 11: Routing Case 3.2 destination using the mesh. The ONET connects the PRI hubs using four Multiple-Writer-Multiple-Reader (MWMR) buses, on which each hub can send on a 128bit wide link. We assume 64-bit wide electrical links. QuT [7] is a low-power, all-optical NoC that uses passive microring resonators to route optical signals according to their wavelength. Senders modulate data on the wavelength set that is assigned to the destination they want to address. For N nodes, QuT uses N/4 wavelength sets for addressing to reduce the number of microrings and laser power. As every destination has one ejection channel, a separate control network is required to resolve contention at the destinations, implemented by exchanging control messages prior to data transmission. The control network is implemented with MWSR buses. Control packets are modulated on one wavelength, and data packets on 8 wavelengths [7]. LumiNoC [16] has the same topology as Lego16, but does not implement electrical links and uses optical data transmission only. If the destination is in the same row or column, it can be reached in one hop. Otherwise, XY optical routing is performed like in routing case 3.2 (Fig. 11). LumiNoC implements a MWMR bus combined with an arbitration mechanism to share optical bandwidth. In this paper, we are interested in the power and performance beneﬁts of topologies that optimally combine electrical and optical links. Therefore, we implement the router groups in LumiNoC like in Lego, with 8λ on each link, without the arbitration functionality. This allows us to study how much beneﬁts we can get by combining electrical and optical links, rather than using optical links only. We note that this arbitration mechanism can be adopted to Lego as well. EMesh is a conventional 2D electrical mesh network with 64-bit wide links and 5Ghz clock. Routers implement XY-routing and use wormhole switching. We assume an optimistic design with aggressively pipelined routers and three cycles per hop: two within each router and one for traversing a link. We chose the electrical mesh, as it is the de facto standard in industry [29] and constitutes a baseline electrical NoC. We study Lego8 and Lego16 at two diﬀerent bandwidth levels, i.e. optical links carrying 8λ (Lego8 8λ, Lego16 8λ), and 16λ (Lego8 16λ, Lego16 16λ). Increasing the bandwidth to 16λ halves serialization delay without excessive power overheads (see Tab. 1 and Fig. 2). Control links carry 2λ (Lego16) and 3λ (Lego8). We assume 64-bit wide electrical links. Figure 12: Router Architecture tive Ge laser that allows to adjust the output power to the traﬃc demands. As the latter is a technological advancement, and we are focusing on architectural improvements in this paper, we only adapt the star network to the traditional Atac design. Fireﬂy [17] divides the 64 nodes into four similar sized clusters of 16 nodes each. Within each cluster, four hub routers form an electrical 2D mesh, with four nodes concentrated at each hub. Each of these four hub routers has a dual in every other cluster, with which they are connected optically. Hubs use the electrical mesh to send to destinations residing in the same cluster, and optical links to their duals for inter-cluster communication. Optical links are implemented as reservationassisted SWMR buses (R-SWMR), as introduced earlier. Therefore, prior to optical data transmission, control packets are exchanged and rings tuned/detuned. We assume enough optical link bandwidth so that control packets (4 bits) and data packets are modulated in one clock cycle, i.e. 2λ on the control channels, and 32λ on the data channels. Electrical links are 64-bit wide. Meteor [15], similar to Atac, implements a 2D electrical mesh and overlays it with an optical network. However, in Meteor, there are only four optical hubs through which the ONET can be accessed. Photonic Regions of Inﬂuence (PRI) determine the grouping of nodes to the hubs. With an 8x8 layout, their study shows that grouping 16 nodes to each PRI is the most eﬃcient design variant. We divide the 8x8 layout into four square 4x4 submeshes and place the hub router in the middle of each submesh for the highest eﬃciency. If the destination node is closer than the node’s PRI hub, it will send the data packet over the electrical mesh. Otherwise, it will route the packet to the hub, which will then send the packet optically to the PRI region of the destination node, which will then forward the packet to the 271 4.2 Packet Latency 4.2.1 Synthetic Workloads Fig. 13 illustrates the average packet latency for some of the traditional synthetic traﬃc patterns. We report latency in processor cycles. In Hotspot traﬃc, 80% of the nodes send all of their traﬃc randomly to 20% of the nodes, while the rest distributes their traﬃc uniformly. In Neighbor traﬃc, each node sends packets randomly to one of its neighbors. Compared to Atac, Fireﬂy, and Meteor, which utilize electrical links for local traﬃc and optical links for distant traﬃc, both Lego topologies tremendously decrease packet latency across all traﬃc patterns. Only in neighbor traﬃc, Meteor shows similar latencies. At a fairly low injection rate of 1Tbps, both Lego topologies manage to decrease packet latency by 50% on most patterns. At the same time, our topologies at least double throughput on all patterns. Compared to LumiNoC, the electrical links inserted in our topology for local traﬃc, along with our novel routing algorithm, show to be a considerable improvement. Packet latency is decreased by at least 30% on all patterns, and substantial throughput gains can be observed. Even Lego8, which provides fewer optical links than LumiNoC and Lego16, demonstrates these improvements for most patterns (apart from Bit-Reversed and BitComplement). QuT shows very constant latency and throughput across all patterns as the distance to the destination does not have a large impact on packet latency for optical signal propagation, as opposed to congestion and contention resolution, which are the main contributor to latency in QuT. Therefore, QuT performs particularly poorly in Hotspot traﬃc. On all traﬃc patterns, both Lego topologies decrease latency (up to 50%, 20% on average at 1Tbps). Moreover, they provide fair throughput gains (apart from the Bit-Permutation patterns, where QuT provides slightly more throughput). For most traﬃc patterns, Lego8 and Lego16 show similar throughput and latency values, with Lego16 having slightly lower latencies for Uniform Random, BitReserved, and Bit-Complement. Only for Bit-Reversed and Bit-Complement, Lego16 oﬀers twice the throughput of Lego8, while having similar throughput levels for every other traﬃc pattern. It is interesting to observe that there is no workload on which Lego seems to perform particularly poorly compared to the alternative NoCs. Moreover, both Lego designs are very eﬃcient for Neighbor traﬃc, which is a desired property for NoCs as it supports the current shift to data centric computing in large-scale many-core systems, where software tends to exploit spatial locality through near-data processing [30]. Increasing the bandwidth from 8λ to 16λ provides only slight throughput and latency gains for both Lego8 and Lego16. Section 4.3 evaluates whether these gains justify the entailed power overheads. 4.2.2 PARSEC/SPLASH-2 Workloads Fig. 14 shows the latency results of a range of SPLASH2 and PARSEC applications normalized to Lego8 8λ. Apart from Atac, every alternative NoC has a higher packet latency than both Lego implementations. Atac has optical links between all source-destination pairs which provides high bandwidth, but also high power consumption as we will see in the following section. Compared to Lego8 8λ, Fireﬂy and Meteor exhibit a slight latency overhead of 14% and 6%, respectively. The same applies to QuT and EMesh, which have a latency overhead of 56% and 30%, respectively. Lego saves up to 40% by inserting electrical links compared to LumiNoC. In addition, Lego8 8λ has the highest latency of all Lego networks. Similar to synthetic traﬃc, Lego16 8λ lowers packet latency compared to Lego8 8λ. On average, 13% fewer cycles are required. Increasing the optical link bandwidth in both Lego topologies leads to larger savings in packet latency than for synthetic trafﬁc, with 20% lower packet latency for both Lego8 16λ and Lego16 16λ. 4.3 Power Consumption Fig. 16 depicts the power breakdown normalized to Lego8 8λ. The power values are the average power consumption across all synthetic traﬃc patterns before network saturation at 1Tbps. We report ring heater power for thermally-tunable microring resonators that require 20μW/ring for a typical on-chip temperature range of 20K [22]. Encasing the photonic die in a thermal insulator can further decrease this value to ∼5μW [8]. In this work, we use 20μW/ring to have a more pessimistic assessment of the silicon photonic technology. Compared to the other locally-electrical, globally-optical NoCs Atac, Fireﬂy, and Meteor, both Lego implementations decrease power consumption signiﬁcantly. Even when increasing the bandwidth to 16λ, Lego exhibits lower power consumption than the ma jority of alternative NoCs. Atac provides optical links for each node, which is very ineﬃcient in terms of laser and ring heater power (4x the power consumption of Lego8 8λ). Meteor only has very few optical links in its topology and thus low laser and ring heater power. However, it heavily relies on the underlying electrical mesh for most sourcedestination pairs, resulting in higher dynamic power. Our results show a 49% and 46% power overhead compared to Lego8 8λ and Lego16 8λ, respectively. Fireﬂy implements fewer optical links than Lego, but with larger optical bandwidth, leading to higher static optical power. Its topology also leads to more electrical link and router traversals than Lego, which is reﬂected in higher electrical dynamic power. In total, Fireﬂy increases power consumption by 62% and 59% compared to Lego8 8λ and Lego16 8λ , respectively. Being all-optical, QuT exhibits according static optical power overheads. As only optical communication is performed, dynamic power is very low due to the highly energy-eﬃcient optical communication. Nevertheless, with power consumption overheads of 76% and 73% compared to Lego8 8λ and Lego16 8λ, its power requirements are high. 272 Uniform Random P e k c a t a L t y c n e ( C s e c y l ) 150 135 120 105 90 75 60 45 30 15 0 Offered Load (Tbps) 0 1 2 3 4 5 6  0 7 0 Bit-Reversed Offered Load (Tbps) 1 2 3 4 5 6 7  0 8 0 Lego8_8λ Fireﬂy Lego8_16λ Meteor Lego16_8λ LumiNoC Lego16_16λ QuT Atac EMesh Bit-Complement Offered Load (Tbps) 1 2 3 4 5 6 7  0 8 0 Bit-Rotation Offered Load (Tbps) 1 2 3 4 Shufﬂe P e k c a t a L t y c n e ( C s e c y l ) 150 135 120 105 90 75 60 45 30 15 0 Offered Load (Tbps) 0 1 2 3  0 4 0 Tornado Offered Load (Tbps) 1 2 3 4 5 6  0 7 0 Neighbor Offered Load (Tbps) 1 2 3 4 5 6  0 7 0 Hotspot Offered Load (Tbps) 1 2 3 4 Figure 13: Average packet latency on synthetic workloads N o r m a i l d e z P e k c a t a L t y c n e 0 0.5 1 1.5 2 2.5 3 3.5 b l a c k s c h o l e s ﬂ u i d a n i m a t e vi p s b o d y tr a c k c a n n e a l f e rr e t fr e q m i n e b a r n e s r a y tr a c e w a t e r _ s p a t w a t e r _ n s q u fft c h o l e s k y l u _ c o n t l u _ n o n c o n t g e o m e a n Lego8_8λ Lego8_16λ Lego16_8λ Lego16_16λ Atac Fireﬂy Meteor LumiNoC QuT EMesh Figure 14: Average packet latency on SPLASH-2/PARSEC benchmarks normalized to Lego8 8λ Compared to LumiNoC, electrical links in Lego increase energy eﬃciency as they allow considering locality and provide fewer total link traversals. Given that we do not have to perform routing computation when a packet is received over an electrical link, Lego saves dynamic router power. Moreover, Lego has fewer optical links and microrings because they are not required for communicating with direct neighbors. These architectural aspects lead to a 22% and 19% power overhead of LumiNoC compared to Lego8 8λ and Lego16 8λ , respectively. Compared to the electrical mesh, these savings are 74% and 71%. power consumption by ∼50% for both Lego topologies, Increasing the optical bandwidth to 16λ increases the which shows the susceptibility of static optical power to optical bandwidth. However, slight power savings can still be observed. 4.3.1 Throughput-per-Watt We show the throughput-per-watt (TPW) of all NoCs for all synthetic traﬃc patterns in Fig. 15. Both Lego designs dramatically increase TPW compared to each of the alternative NoCs for each traﬃc pattern. LumiNoC is the closest competitor, and only provides 71% of the TPW of Lego8 8λ. For every other NoC, Lego8 8λ at least doubles the TPW, proving its high energy efTable 2: Insertion Loss Parameters Parameter Value Laser eﬃciency 0.25 [31] Coupler 1 dB [31] Ring: Through 0.01 dB [12] Ring: Drop 1 dB [12] Waveguide Bending 0.005 dB [7] Waveguide propagation 0.1 dB/mm [32] Waveguide crossing 0.05 dB [22] Splitter 0.1dB [7] Photodetector loss 1 dB [12] ﬁciency. Lego16 8λ shows the highest TPW and increases TPW compared to Lego8 8λ by ∼8%. Increasing the optical bandwidth of Lego from 8λ to 16λ still provides good TPW improvements, but does not seem to oﬀer suﬃcient throughput to justify its power overheads. In both Lego topologies, doubling the link bandwidth leads to ∼25% lower TPW. 4.3.2 Power-Delay-Product We calculate the power-delay-product (PDP) for the SPLASH-2/PARSEC applications by multiplying the average packet latency with the consumed power. We present the results in Fig. 17. Our Lego topologies 273                             P T W o n r m a i l d e z t λ 8 _ 8 o g e L o 2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 U n i f o r m R a n d o m B i t - R e v e r s e d B i t - C o m p l e m e n t B i t - R o t a t i o n S h u f ﬂ e T o r n a d o N e i g h b o r H o t s p o t G e o m e a n Lego8_8λ Lego8_16λ Lego16_8λ Lego16_16λ Atac Fireﬂy Meteor LumiNoC QuT EMesh Figure 15: Normalized Throughput-per-Watt P o w e r o n r m a i l d e z t λ 8 _ 8 o g e L o 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 L e g o 8 _ 8 λ L e g o 8 _ 1 6 λ L e g o 1 6 _ 8 λ L e g o 1 6 _ 1 6 λ A t a c M e t e o r F ir e ﬂ y Q u T L u m i N o C  E M e s h Optical Links Electrical Links Router Leakage  Ring Heating Laser  Figure 16: Power breakdown show signiﬁcant improvements in PDP compared to the alternative NoCs. The large power overhead of Atac makes it the least energy-eﬃcient of all designs. QuT is less energy-eﬃcient than the remaining NoCs as it has large static, data-independent optical power consumption and many applications of the SPLASH-2 and PARSEC benchmark suite have fairly low injection rates over the whole duration of executing time [33]. Meteor is the closest competitor and has a 46% higher PDP than Lego8 8λ. The diﬀerence in PDP of Lego8 8λ and Lego16 8λ are marginal. Increasing the bandwidth to 16λ raises PDP by 25% in both cases, but they are both still considerably more eﬃcient than the other NoCs. 4.4 Area The area breakdowns are shown in Fig. 18. Lego basically trades oﬀ latency and power for area compared to most NoCs. Given their large abundance of optical components, Atac and QuT require more area than Lego8 8λ and Lego16 8λ. Meteor, LumiNoC, Fireﬂy, and EMesh save 22%, 28%, 38%, and 30% in area compared to Lego8 8λ, respectively, which is not negligible; however, both Lego8 8λ and Lego16 8λ outperform these alternative NoCs in most of the other metrics. It is commonly assumed that area constraints are going to be less signiﬁcant in future on-chip network designs than power, especially with shrinking transistors sizes and 3D-Integration enabling to integrate silicon photonic devices on a separate layer. Nevertheless, the area overheads of Lego are not of prohibitive extent and we believe are more than justiﬁed considering the latency, throughput, and power gains provided by Lego. 5. RELATED WORK As one of the most promising emerging technologies to overcome the energy and performance limitations of metal wires, optical NoCs have gained large interest in the research community, ranging from improving photonic device technologies [8, 34, 35], thermal management [36, 37], and adaptive laser sources [38, 39] to novel NoC architectures that makes use of this nascent technology in the most eﬃcient manner. The paper at hand contributes to this ﬁeld by proposing a novel, lowpower ONoC architecture that studies the trade-oﬀs of electrical and optical links more carefully and utilizes them more eﬃciently. A number of NoC designs have been proposed which we classify into 1) all-optical NoCs that use optical data transmission only, and 2) hybrid NoCs that combine electrical and optical links. Passive microring resonator based ONoCs are referred to as Wavelength-Routed ONoCs (WRONoCs). Nonblocking WRONoC topologies [40, 41, 42, 43] provide each receiver; however, this requires (N − 1) ﬁlters at simultaneous switching capability from each sender to each node, leading to very limited scalability in terms of power as the number of rings is proportional to ring heater power. To tackle this, several WRONoCs [14, 7, 44] have been proposed that require only one ﬁlterdetector pair at each node and resolve contention by using a control network like QuT. This decreases power as fewer microrings are used, but fairly large static optical power is still present in these NoCs, especially for higher core counts. Hybrid NoCs are more likely to be adopted in near future than all-optical NoCs because of the currently large static power overheads of optical interconnects. A large number of interesting hybrid optical NoCs has been proposed in recent literature [45, 46, 47, 17, 22, 48, 49, 50, 51, 52, 31, 10, 16, 15]. Phastlane [47] combines a packetswitched mesh network with an optical, contention-free crossbar to transmit cache lines over several hops in one cycle. The silicon-photonic Clos network (PClos) [22] uses point-to-point optical channels for low-energy, long distance data transmission. Only the router in the intermediate clos stages and the links from/to the cores to the output/input routers are electrical, optical links are used in all other stages. It could thus also be considered an all-optical NoC. BLOCON [51] is a buﬀerless implementation of PClos that features a scheduling algorithm and path allocation scheme for managing routing in the Clos. It provides low latency and high throughput, but also has higher ring heater and laser power compared to PClos. FlexiShare [48] deploys a channel sharing architecture to reduce channel over-provisioning and thus laser power at the cost of less throughput and additional arbitration channels. PROPEL [49] combines an optical crossbar with an electrical mesh. The number of wavelengths required in the NoC equals the number of nodes in the topology. R-3PO [31] is a 3D NoC utilizing an optical crossbar with a token-based control network to handle accesses. Just like WRONoCs, optical crossbars have limited scalability as the number of microrings, and thus ring heater power, increases rapidly for larger network sizes. Also, high-bandwidth optical links are shared by a number of nodes, which leads to higher IL and laser power. Lego improves these designs by providing smaller crossbars with lower throughput channels, 274               N o r m a i l d e z P D P 0.5 0 1 1.5 2.5 2 3 4 3.5 4.5 b l a c k s c h o l e s ﬂ u i d a n i m a t e vi p s b o d y tr a c k c a n n e a l f e rr e t fr e q m i n e b a r n e s r a y tr a c e w a t e r _ s p a t w a t e r _ n s q u fft c h o l e s k y l u _ c o n t l u _ n o n c o n t g e o m e a n Lego8_8λ Lego8_16λ Lego16_8λ Lego16_16λ Atac Fireﬂy Meteor LumiNoC QuT EMesh Figure 17: Power-Delay-Product normalized to Lego8 8λ A r o n a e r m a i l d e z t λ 8 _ 8 o g e L o 0 0.5 1 1.5 2 2.5 L e g o 8 _ 8 λ L e g o 8 _ 1 6 λ L e g o 1 6 _ 8 L λ e g o 1 6 _ 1 6 λ A t a c M e t e o r F i r e ﬂ y L u m i N o C Q u T E M e s h Optical Active Area Waveguides Global Wire  Figure 18: Normalized Area Breakdown and supplements them more eﬃciently with electrical links to outbalance throughput drawbacks. Utilizing 3D stacking technology for the sake of mitigating losses caused by waveguide crossings has also been successfully studied [50, 53], which is why we adapted this approach in Lego, too. Iris [52] combines optical links, a dielectric antenna-array-based broadcast network, and a circuit-switched electrical mesh network. Lego does not impose any limitations on implementing emerging technologies, such as adaptive laser sources. Channel allocation schemes can also be implemented in Lego, which would further lower laser power [16]. 6. CONCLUSION We present Lego, a hybrid ONoC topology that decreases power consumption without latency and throughput drawbacks by eﬃciently combining electrical and optical links. Our novel routing algorithm utilizes electrical and optical links based on the distance to the destination, which always picks the technology that provides the lowest energy and latency to transmit a packet. Low-bandwidth optical links oﬀer satisfactory throughput when deployed for suﬃciently large distances. Our evaluation results prove the eﬀectiveness of Lego by exhibiting large savings in throughput-per-watt and powerdelay-product compared to various alternative NoC proposals. We intend to extend our study in the future to larger network sizes to study scalability. Moreover, the eﬀect of applying channel allocation schemes and adaptive laser sources in Lego to further decrease laser power are also interesting aspects to investigate. 7. ACKNOWLEDGEMENTS This research was conducted with support from the UK Engineering and Physical Sciences Research Council (EPSRC) PAMELA EP/K008730/1 and the European Union’s Horizon 2020 research and innovation programme under grant agreement No 671553 (ExaNeSt). Dr. Luj´an is supported by a Royal Society University Research Fellowship. 8. "
2017,Camouflage - Memory Traffic Shaping to Mitigate Timing Attacks.,"Information leaks based on timing side channels in computing devices have serious consequences for user security and privacy. In particular, malicious applications in multi-user systems such as data centers and cloud-computing environments can exploit memory timing as a side channel to infer a victim's program access patterns/phases. Memory timing channels can also be exploited for covert communications by an adversary. We propose Camouflage, a hardware solution to mitigate timing channel attacks not only in the memory system, but also along the path to and from the memory system (e.g. NoC, memory scheduler queues). Camouflage introduces the novel idea of shaping memory requests' and responses' interarrival time into a pre-determined distribution for security purposes, even creating additional fake traffic if needed. This limits untrusted parties (either cloud providers or coscheduled clients) from inferring information from another security domain by probing the bus to and from memory, or analyzing memory response rate. We design three different memory traffic shaping mechanisms for different security scenarios by having Camouflage work on requests, responses, and bi-directional (both) traffic. Camouflage is complementary to ORAMs and can be optionally used in conjunction with ORAMs to protect information leaks via both memory access timing and memory access patterns. Camouflage offers a tunable trade-off between system security and system performance. We evaluate Camouflage's security and performance both theoretically and via simulations, and find that Camouflage outperforms state-of-the-art solutions in performance by up to 50%.","2017 IEEE International Symposium on High Performance Computer Architecture Camouﬂage: Memory Trafﬁc Shaping to Mitigate Timing Attacks Yanqi Zhou, Sameer Wagh, Prateek Mittal, and David Wentzlaff Electrical Engineering Department Princeton University Princeton, NJ, United States yanqiz@princeton.edu, swagh@princeton.edu, pmittal@princeton.edu, and wentzlaf@princeton.edu Abstract—Information leaks based on timing side channels in computing devices have serious consequences for user security and privacy. In particular, malicious applications in multi-user systems such as data centers and cloud-computing environments can exploit memory timing as a side channel to infer a victim’s program access patterns/phases. Memory timing channels can also be exploited for covert communications by an adversary. We propose Camouﬂage, a hardware solution to mitigate timing channel attacks not only in the memory system, but also along the path to and from the memory system (e.g. NoC, memory scheduler queues). Camouﬂage introduces the novel idea of shaping memory requests’ and responses’ interarrival time into a pre-determined distribution for security purposes, even creating additional fake trafﬁc if needed. This limits untrusted parties (either cloud providers or coscheduled clients) from inferring information from another security domain by probing the bus to and from memory, or analyzing memory response rate. We design three different memory trafﬁc shaping mechanisms for different security scenarios by having Camouﬂage work on requests, responses, and bi-directional (both) trafﬁc. Camouﬂage is complementary to ORAMs and can be optionally used in conjunction with ORAMs to protect information leaks via both memory access timing and memory access patterns. Camouﬂage offers a tunable trade-off between system security and system performance. We evaluate Camouﬂage’s security and performance both theoretically and via simulations, and ﬁnd that Camouﬂage outperforms state-of-the-art solutions in performance by up to 50%. Keywords-hardware; security; memory system; I . IN TRODUC T ION Running VMs on the same physical machine has become prevalent in Clouds and data centers. Workload consolidation and novel architectures improve server utilization though they can compromise the security of VMs due to leaked information caused by resource sharing [1], [2], [3], [4]. While Secure software [5] and hardware have been proposed, including an authentication circuit [6], Intel’s TXT [7], eXecute Only Memory (XOM) [8], Aegis [9], and Ascend [10], they do not prevent information leakage through side channels caused by resource sharing. Applications from different security domains can share hardware resources (caches, on-chip networks, memory channels, etc.). Contention or interference in shared resources results in information leakage across security domains. For instance, memory queuing delay and scheduling delay highly depend on co-running workloads. Interference  	  	   Δt2!=Δt1 Figure 1. An Example of Timing Leakage. Attacker measures its own response latency to estimate a co-scheduled VM’s memory trafﬁc. between different security domains can create performance interference on some applications, posing a security threat by creating an opportunity for timing channels [11], [12]. For example, when two VMs share a memory system, their memory requests will be co-scheduled by a centralized memory controller. The memory request service time of one application can be greatly impacted by the other application. Figure 1 shows a timing channel attack, where the malicious VM can infer information from the co-running VM by observing its own memory service time (response time). In our work, we focus on timing channel attacks in a shared memory system and in the shared channel (NoC, etc.) connecting the processor cores and memory controllers, where the attacker can measure the timing of memory requests and responses, and statistically infer information of victim VMs. We make no assumption on who the malicious entity is. It can be a malicious server, who controls the processor and wants to learn more about the user’s data by monitoring the processor’s I/O pins or memory buses. Alternatively, it can be a malicious client, who tricks the server into being scheduled on the same physical machine as the victim and infers the victim’s memory timing information by measuring its own memory access latency. Side-channel attacks and their countermeasures have been widely studied in the context of shared caches, on-chip networks, and shared memory channels. Most of them rely on static spatial/temporal partitioning of resources and reducing interference between security domains, thus incurring signiﬁcant performance degradation. Higher security can be achieved at the cost of performance or higher hardware overhead. An efﬁcient way to remove side-channels is to use a static scheduling or partitioning algorithm. For instance, 2378-203X/17 $31.00 © 2017 IEEE DOI 10.1109/HPCA.2017.36 337 rather than using a First Ready-First Come First Serve (FR-FCFS) memory scheduling algorithm to improve row buffer hit rate, a leakage-aware scheduler can allocate a static scheduling window for every process/thread that shares the memory system. This potentially impairs throughput and utilization because of its ﬁxed scheduling windows. Alternatively, memory banks/ranks can be partitioned so that each thread accesses a different bank/rank. However, this reduces the effective memory capacity for each thread and could potentially lead to imbalanced memory accesses. We propose a novel memory trafﬁc shaping and trafﬁc generation mechanism, Camouﬂage, that is able to camouﬂage the timing information of memory requests and responses. In order to create a memory trafﬁc distribution exactly matching a pre-determined one, Camouﬂage limits trafﬁc rate as well as generates fake trafﬁc. Different from ORAM, we do not solve the memory address/data encryption/obfuscation problem, but only focus on timing channel attacks. However, Camouﬂage is complementary to address/data encryption/obfuscation or techniques like ORAM. Camouﬂage can mitigate multiple threat models when used together with ORAM. We don’t time partition the memory scheduling window or spatially partition memory capacity, but rather only camouﬂage memory requests and responses to make it difﬁcult or impossible for a VM to infer any useful information. Speciﬁcally, Camouﬂage uses three different strategies to address different attack scenarios. Request Camouﬂage (ReqC) shapes only memory request interarrival distribution at the processor core side. This limits obtaining timing information from probing or monitoring I/O pins or the path from the core to and from memory. Response Camouﬂage (RespC) shapes only memory response interarrival distribution at the egress of the memory controller. This prevents an untrusted VM from inferring trafﬁc patterns of other VMs by observing its own memory response latencies. Bi-directional Camouﬂage (BDC) shapes both requests and responses, securing both request and response timing information. The hardware mechanism takes inspiration from the MITTS hardware memory trafﬁc shaper [13], but applies memory trafﬁc shaping for security and not for increasing performance. Camouﬂage provides a richer security/performance tradeoff space compared with a constant rate shaper (CS) [14]. Compared with Temporal Partitioning (TP) [15], Camouﬂage mitigates memory timing leakage while not compromising performance and hardware efﬁciency. Different from Fixed Service (FS) [16], which relies on constant rate shaping and spatial (bank/rank) partitioning, Camouﬂage does not rely on spatial partitioning for higher performance and is scalable to larger number of threads (more than the total number of banks/ranks). Figure 2 shows the tradeoff space provided by Camouﬂage compared with CS, TP, and FS. Camouﬂage can be conﬁgured to be a constant rate shaper by using only one bin, but also can be used 338 $ ! &  #&"" $""   ""  "" &    	   '              "" ""    Figure 2. Camouﬂage Security and Performance Trade-off Space. This plot is based on analysis of the mutual information vs. performance. to explore alternative security/performance trade-off points. In simulation, Camouﬂage on average improves program throughput by 1.12x, 1.5x, and 1.32x compared with CS, TP, and FS respectively. We also conduct a real covert channel attack and show that Camouﬂage is able to mitigate practical covert channel attacks. Overall, our work demonstrates the feasibility of new design points for mitigating timing channel leaks that provide a combination of strong security and performance, and can serve as a key enabler for the practical deployment of hardware-based timing defenses. Our main contributions include: 1) We design three hardware trafﬁc shaping and generation mechanisms that shape memory requests, responses, and both in terms of inter-arrival times, and generate fake trafﬁc if needed. This enables securitysensitive VMs to camouﬂage their memory request and response timing. 2) Camouﬂage protects timing information not only in the memory system, but also the shared channel between processor cores and memory controllers. 3) We leverage information theory and the idea of mutual information to analyze the amount of information that Camouﬂage leaks. We show that it is less than 0.1% of the transmitted information. 4) We design Camouﬂage to provide a larger security and performance trade-off space. Camouﬂage can be conﬁgured to leak zero (constant rate shaping), or can be conﬁgured to leak slightly more (optimized for performance). I I . MOT IVAT ION A. Threat Model This paper focuses on mitigating or preventing two types of threats: memory-based side-channel and covert-channel attacks, and monitoring of I/O pins or memory buses. Memory Side-Channel and Covert-Channel: The adversary measures its own overall execution time or memory response latencies in order to estimate its co-scheduled application VM’s memory intensity over time. Applications create interference in the memory system due to contention        in hardware resources such as NoCs, queues, row buffers, and the memory scheduler. Increasing memory intensity of one application is very likely to slow down other applications’ service rate. Therefore, by monitoring the memory response rate change or the overall execution time change, the adversary can infer the memory access pattern of another application. Pin/Bus Monitoring: The adversary (eg. a data center administrator) has physical access to the processor’s I/O pins, system buses, and peripherals. Such information includes the program’s start and termination time, the addresses and data sent to and read from the main memory, and when each memory access occurs. In this threat scenario, we focus on the timing aspect of memory access and assume the address and data are protected by ORAM [17] or are encrypted. For example, the adversary can observe communications over the bus between the processors and the memory, in terms of access number and frequency. We assume only off-chip components are insecure while on-chip components are secure. Moreover, we assume the adversary can conduct ﬁne-grain timing measurements, at a per-memory-request level. These ﬁne-grain measurements can lead to direct information leakage of the victim’s program characteristics. B. Timing Protection Overheads Temporal Partitioning (TP) [15] divides time into ﬁxedlength turns during which only requests from a particular security domain can be issued. It provides security against timing based side-channels. Static temporal partitioning reduces the amount of ﬂexibility in a scheduler, impairing throughput and utilization. For example, application memory trafﬁc is unlikely to be a constant. When there are not sufﬁcient requests from a speciﬁc process in its own time division, it is desirable to schedule requests from other processes. Temporal Partitioning applications based on several security domains is feasible, however, it is not scalable if hundreds of applications don’t trust each other. For example, if one hundred processes are running on a manycore processor and each ask for separate security domains, TP will have trouble providing high bandwidth, as each of them only receives 1 of the memory bandwidth. A secure processor, Ascend [18] prevents leakage over the ORAM timing channel by forcing ORAM to be accessed at a single, strictly periodic rate. An enhanced version [14] splits a program into coarse-grain time epochs and chooses a new ORAM rate out of a set of rates at the end of each epoch. This technique bounds the leakage to E × log R, where E is total number of epochs and R is total number of rates. Fixed Service (FS) [16] forces every thread to have a constant memory injection rate and a uniform memory access pattern. Similar to CS, it provides little tradeoff opportunity in selecting between security and performance. Combined with memory bank/rank partitioning, it improves performance compared with TP. Spatial partitioning memory 100     	 		  !     	 	  ! 		 		 		 Figure 3. Conceptual Difference between Camouﬂage and Two Prior Work (CS [14] and TP [15]). CS has only requests/responses in one bin. TP has more requests/responses in high latency bins due to time multiplexed resource. Malicious Program: for (i = 0; i < |D|; i++)  if (D[i]) wait else Mem[4*i]++ DRAM Rate 1 0 1 1 0 Intrinsic  Distribution Frequency Time Camouflage Inter-arrival Times DRAM Rate Frequency 1 0 1 0 1 Camouflaged Keys Time Inter-arrival Times Figure 4. Camouﬂage a Vector of Keys. Camouﬂage slightly changes the request inter-arrival time distribution to distort the inferred keys. reduces the effective memory capacity and bandwidth for each thread and could potentially create imbalanced accesses to a few banks/ranks. Spatial partitioning does not work well when there are massive number of threads that is greater than the total number of ranks or banks. These three techniques negatively impact program performance and provide little tradeoff opportunity for choosing between security and performance. Having more than three points in the security and performance tradeoff space is highly desirable. C. Memory Trafﬁc Distribution and Timing Channel Protection Ideally, Camouﬂage only tunes the trafﬁc pattern slightly so that a malicious user cannot infer the desired information from the camouﬂaged trafﬁc pattern without signiﬁcantly changing the intrinsic trafﬁc distribution. There are two different aspects of memory access patterns, bulk bandwidth (total number of memory requests within a timing window) and burstiness. Executing a certain branch of code results in changes in memory bulk bandwidth. A program that deliberately conveys a bit array of sensitive key information can encode that information in memory burstiness. Camouﬂage uses a memory inter-arrival time distribution to encapsulate both aspects of memory accesses. A distribution describes how an application’s memory requests/responses occur at different intervals, and what percentage of requests/responses fall into a speciﬁc inter-arrival time. In our proposed memory distribution, the horizontal axis 339 Techniques ReqC RespC BDC TP [15] CS [14] FS [16] Capable of Preventing Pin/Bus Memory Side-Channel Monitoring Covert-Channel Yes No No Yes Yes Yes No Yes Yes No No Yes Performance 	 High High High Impacted by the number of security domains Low for workloads with non-constant memory request rates Requires spatial partitioning for better performance 	        	      D I FF ER EN T M EMORY T IM ING PROT EC T ION T ECHN IQU E S . F IR S T THR E E AR E CAMOU FLAGE . Table I represents the time difference between two subsequent memory requests, while the vertical axis determines how frequent a request falls into a certain inter-arrival category. The interarrival time along with the frequency at which memory requests occur with that inter-arrival time determines the bandwidth consumed. Camouﬂage shapes memory request/response inter-arrival times into pre-determined statistical distributions. This predetermined distribution is independent of the intrinsic distribution, thus reducing leaked timing information. Different from conventional static partitioning or static rate limiting, Camouﬂage enables the choice of ﬂexible distributions of request/response inter-arrival times, which improves performance and memory channel utilization while still hindering timing attacks. As shown in Figure 3, Camouﬂage does not necessarily shape the intrinsic trafﬁc into a constant rate as a constant rate shaper, or delay a signiﬁcant number of requests due to time partitioning of the scheduling window (which results in a large number of entries in the high latency bar for Temporal Partitioning). However, Camouﬂage can be conﬁgured as a constant rate shaper if necessary. Intuitively, Camouﬂage is able to hide the frequency domain information in a more generalized and efﬁcient way. Figure 4 shows an example where a malicious program leaks a vector of secret keys. With Camouﬂage, we slightly change the request inter-arrival time distribution so that the inferred keys are distorted. Table I summarizes the differences between different techniques. I I I . ARCH I T EC TUR E A. Hardware Design Camouﬂage can shape either memory requests, memory responses, or even a combination of both. As shown in Figure 5, a memory request shaper is placed locally after a processor core’s LLC to limit memory request rate for a particular core or thread. The request shaper (ReqC) can transform a process’s intrinsic trafﬁc into a ﬁxed pre-determined inter-arrival time distribution. This prevents timing leakage in multiple shared channels, such as NoC (SC1), the memory controller (SC2, SC4), and DRAM (SC3). The request shaper needs to be able to throttle down the request rate when the desired request rate is lower than the intrinsic request rate, as well as generate fake requests if the desired request rate is higher than the intrinsic request rate. The post-shaper memory trafﬁc distribution does not vary with different program phases or branches. Request   	    	 Figure 5. Request, Response, and Bi-directional Camouﬂage Camouﬂage can effectively mitigate I/O pin or memory bus timing channel attack, when combined with address/data encryption/obfuscation techniques. Similarly, a memory response shaper (RespC) is placed at the egress of the memory controller, before entering a particular processor’s LLC. The response shaper eliminates timing leakage generated by the memory system (SC3), providing timing security for shared channels (SC) 4 and 5. For instance, issuing requests from different threads to the same memory rank or bank creates contention on memory bus and row buffers. This can lead to memory side-channel or covertchannel attacks. The RespC throttles down response rate by buffering the response and accelerates response rate by signaling the memory scheduler to give higher priority to the requesting application or generating fake memory responses. This technique reduces the correlation between the victim’s request rate and the attacker’s response rate and effectively camouﬂages the interaction occurred in the memory system. Bi-directional Camouﬂage (BDC) shapes both memory requests and responses by combining ReqC and RespC. This technique is desirable when we require a shaping mechanism for both memory requests and responses and do not want to change memory controller scheduling policies. Camouﬂage is able to shape memory requests and responses into arbitrary statistical distributions, many more options than a ﬁxed rate (Ascend). Requests/responses occur at the attack point at different rates, while the overall interarrival distribution within a time window is ﬁxed. This obfusticates the timing information between the shaped distribution compared with the intrinsic one, thereby significantly reducing mutual information of the intrinsic trafﬁc timing and the shaped trafﬁc timing. 1) Bin-based Trafﬁc Shaper: Camouﬂage uses a binbased request/response shaper to shape inter-arrival times. In order to control the Camouﬂage hardware, the hypervisor writes special purpose control registers to conﬁgure the shape of the request/response distributions. Each individual core has its own request/response shapers. The use of distributed memory bandwidth trafﬁc shaping can scale up with multicore and manycore systems, and it doesn’t necessarily require changes to the centralized memory controller. The trafﬁc shaper tracks cache miss information and measures 340 request/response inter-arrival times. It generates a stall signal to the processor core when the request/response rate exceeds the pre-determined value. The trafﬁc shaper also generates fake trafﬁc if the memory request rate is lower than the predetermined value. In order to track ﬁne-grained inter-arrival times, we have multiple bins that contain available credits for memory requests. Each bin contains credits that represent one memory transaction at a certain request interval determined by the bin. The scheduling of a memory transaction consumes a single credit. If the memory is shaped into a constant request/response rate, there will be only one of the hardware bins that contains credits. Bin conﬁguration can be arbitrary. Instead of being forced to choose a constant rate, Camouﬂage enables better performance optimization and leverages applications’ constructive trafﬁc. The maximum number of credits in a bin is bounded by the total memory bandwidth that a memory controller can serve. The request/response shaper enforces that a core’s memory request/response distribution does not exceed the prescribed/pre-determined distribution by delaying (stalling) a memory transaction if there are no credits available in a bin that represent lower or equal to the memory transaction’s inter-arrival time. The memory transaction will be delayed enough until its inter-arrival time matches a corresponding bin that has credits, or until credits have been replenished. Each bin is a container holding credits for memory requests with a certain request inter-arrival time. The total number of bins N can be determined by how ﬁne-grain the quantization of inter-arrival interval is desired. We choose ten bins in our design in order to enable the trafﬁc shaper to choose from enough distinct inter-arrival times. 2) Bin Credits Replenishment and Fake Request Generation: Credits are replenished with a ﬁxed period. During credit replenishment, if the hardware detects any unused credits, it saves the credits in another array of unused credits. Whenever there are credits in the unused credit registers, the processor core generates non-cached fake memory requests to random memory addresses and enqueues these fake requests to the miss handling registers. However, the fake trafﬁc generated always has lower priority than the intrinsic requests, and will only be generated when there isn’t a real memory request at the same cycle. 3) Hardware Overhead: Camouﬂage’s implementation is very similar to MITTS (less than 0.1% in area compared to a two-way OoO processor) with the addition of registers and logic for fake trafﬁc generation [19]. Each Camouﬂage hardware module (of which in each design there may be multiple) contains a register per bin to track current credits, a register per bin to hold the number of credits to replenish, and a register per bin to track unused credits at each replenishment interval. We assume ten bins where each bin is 10-bits. This added hardware overhead is minimal when compared to the size of most security mechanisms such as hardware implementations of ORAM. 	         	                !             	     	  	  		   	      	  	    Figure 6. Response Queue. Responses are queued after credits are depleted. Credits are given to queued responses ﬁrst. 1. There are available credits. 2. There are available credits but no pending responses. 3. There are available unused credits but no pending responses or responses directly from MC. B. Prevention Mechanisms 1) Memory Side-Channel and Covert-Channel: To combat side-channel and covert-channel attacks caused by response inspection, we propose “Response Camouﬂage” (RespC). RespC puts shaping hardware at the egress of memory on a per-core or a per-application basis. The hardware accelerates as well as throttles memory responses. For throttling, a response queue buffers responses when not enough credits are available. After bin replenishment, the response queue will be checked to dequeue any response that has been buffered, as shown in Figure 6. However, accelerating responses is challenging if the intrinsic memory intensity is lower than the desired one such that there might not be enough memory responses available. Not enough responses can be caused by the memory being hogged by co-running applications, which slows down the affected application’s memory responses. In this case, Camouﬂage accelerates responses by giving high priority to that particular application in the memory scheduler. The RespC hardware monitors the response inter-arrival time distribution and compares the distribution with a target one. If the response rate is lower than the required one, the RespC sends a warning to the memory scheduler, asking for higher priority for the affected application. At each replenishment, the response shaper sums up unused credits in the hardware bins, and sends the total number of credits along with the warning signal to the memory controller. The memory scheduler will give more priority to the affected application in proportion to the number of unused credits. However, this alone cannot handle changes to a VM’s request distribution. In order to maintain a ﬁxed response distribution when a VM drops its request rate, Camouﬂage generates fake memory responses. Similar to fake request generation, a fake response generator creates fake responses when there are not any pending responses or new responses from the memory controller and there are unused credits accumulated, as shown in Figure 6. 2) Pin/Bus Monitoring: For the I/O pin or memory bus inspection attack, we propose “Request Camouﬂage” 341 		 		  	 	         	            		              		 Figure 7. Request Shaping and Fake Requests Generation. Use another set of bins to store unused credits. (ReqC). ReqC puts the request shaping hardware at the processor core side or after the LLC, so that any distribution of intrinsic memory trafﬁc can be camouﬂaged into a totally different distribution. We are not addressing address/data obfuscation or encryption problem, but rather only focus on memory timing channel. In order to guarantee the generated memory trafﬁc distribution matches the predetermined distribution, the hardware needs to be able to both throttle and accelerate memory request rate. If the desired request rate is higher than the actual request rate, the hardware generates fake memory requests so that the memory trafﬁc adds up to the desired distribution. In order to generate fake trafﬁc, we add a register per bin to store unused credits for each replenishment period. Using these credits, Camouﬂage generates fake requests in the next period to random addresses with the needed distribution. Figure 7 gives an example of generating fake requests. At the end of one replenishment period, Camouﬂage detects unused credits in Bin1 and Bin2. It immediately saves the unused credits to the unused credit bins. The next replenishment cycle, if the application’s intrinsic trafﬁc remains the same as the previous one, the extra fake trafﬁc and the intrinsic trafﬁc will add up to the desired distribution. Even if the distributions of adjacent replenishment periods are different, the added fake trafﬁc compensates for requests missing from the previous replenishment period. The overall trafﬁc distribution will match the desired one. 3) Bi-directional Prevention: Bi-directional technique is desirable when both memory requests and responses are required to be shaped or memory scheduling policies cannot be changed. In order to conﬁgure the hardware bins, a software runtime is co-designed to achieve higher performance. BDC can effectively camouﬂage any suspicious VM’s requests and responses when the request distribution changes. With Bi-directional Camouﬂage, a malicious VM is unlikely to detect timing channel leakage on the memory bus, as memory trafﬁc from the VMs under protection is ﬁxed. A malicious VM can hardly infer memory trafﬁc of the co-running applications, as its own memory response distribution is ﬁxed. In order to utilize Bi-directional CamouCore Number of Cores L1 Caches L2 Caches Memory controller Memory 2.4GHz, 4-wide issue, 128-entry instruction window 4 32 KB per-core, 4-way set associative, 64B block size, 8 MSHRs 64B cache-line, 8-way associative, Single-program: 128KB, multi-program: 128KB private 32-entry transaction queue depth Timing: DDR3, 1333 MHz Organization: 1 channel, 1 rank-per-channel, 8 banks-per-rank, 8 KB row-buffer Table II BA S E S IMU LAT ION CON FIGURAT ION ﬂage, we use a genetic algorithm described in Section IV-C to optimize request and response distributions for each application. Camouﬂage can be conﬁgured to constant shape the requests and responses by giving each core the same number of credits in the same bin. By provising more credits than can be used over the replenishment period, Camouﬂage degenerates into a constant rate shaper that turns into C. Fletcher et al’s work [14], which showed no information leakage for unchanging rates. Different from Fletcher et al’s work, the return trafﬁc can be shaped by Camouﬂage as well. Constant response shaping eliminates leakage generated by interference in the DRAM. IV. EVALUAT ION A. Simulation and Workloads The CPU core and memory system are modeled with a cycle-accurate simulator called SDSim which is adapted from the cycle-accurate core simulator SSim [20], and the DRAM simulator DRAMSim2 [21]. SSim’s frontend is integrated with DRAMSim2. This enables us to model outof-order cores with out-of-order memory systems. SSim is driven by the GEM5 Alpha ISA full system simulator [22]. We model a shared LLC and memory system for the multi-program workloads. Table II shows the details of the simulated system. We evaluate the SPECInt 2006 benchmark suite together with the Apache web server benchmark. In the evaluation of Response Camouﬂage and Bi-directional Camouﬂage, we run two workloads (ADVERSARY, astar, astar, astar) and (ADVERSARY, mcf, mcf, mcf) each time. We name them w(ADVERSARY, astar) and w(ADVERSARY, mcf) for short. ADVERSARY is the untrusted application that inspects the memory bus or its own memory responses to infer use information from the co-scheduled VMs. astar and mcf have wildly different memory intensities and access patterns and are used to simulate the change in memory access. We run 11 workloads (SPECInt 2006 and Apache web server) as the ADVERSARY and compares the performance impacts of Camouﬂage on both the ADVERSARY and victims. B. Security Analysis In order to analyze the effectiveness of different schemes, we leverage mutual information (MI) which comes from classical information theory [23], [24], [25], [26], [27], [28]. 342 Mutual information (MI) is a rigorous metric to characterize statistical privacy in the security/privacy community. In fact, recent work demonstrate strong connections between mutual information and even differential privacy [29]. 1) Metric: Mutual Information: The mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. Speciﬁcally, it quantiﬁes the amount of information obtained about one random variable, through the other one. Equation 1 shows the mathematical formulation of MI of variables X and Y. I (X, Y ) = (cid:2) (cid:2) y∈Y x∈X p(x, y)log( p(x, y) p(x)p(y) ) (1) Camouﬂage is considered to be secure if the MI between the intrinsic memory inter-arrival distribution and shaped inter-arrival distribution is zero. In this experiment, we compare the long term MI between memory request interarrival time distribution before and after Camouﬂage. More speciﬁcally, X and Y contain timing intervals ti . p(x = ti ) and p(y = ti ) are probability densities for the intrinsic memory request/response and shaped memory request/response at inter-arrival time ti . p(x = ti , y = ti ) is the joint probability density at inter-arrival time ti . In our implementation, we have ten different intervals. 2) Mutual Information Measurement: In order to prove that the MI between trafﬁc distributions before and after Camouﬂage is minimal, we measure memory request interarrival time distributions before and after ReqC and compute the MI between the intrinsic request distribution and the shaped distribution. Without any trafﬁc shaping, the mutual information will be I (X, X ) = H (X ), which is just its self information. After a trafﬁc shaper, the MI becomes I (X, Y ), and ideally should be zero. In our experiment, we evaluate MI for a non-shaping case, a constant rate shaper, and ReqC for w(ADVERSARY, bzip). While the non-shaping case has a MI of 4.4, a constant rate shaper (without fake trafﬁc) reduces the MI to 0.002, ReqC (without fake trafﬁc) reduces the MI to 0.006. With fake trafﬁc, a constant shaper further reduces the MI to 0, and ReqC reduces the MI to 0.002. Other benchmarks generate similar results. This result implies that Camouﬂage at most leaks 0.1% of the information compared to non-shaping case, and leaks slightly more than the constant rate shaper. In other words, ReqC leaks only 0.1 byte if a non-shaping scheme leaks 100 bytes. 3) Mutual Information with BDC: In this section, we analyze the MI of BDC. We show below that it is never worse than ReqC or RespC. Assume there are two VMs sharing the memory channel. Request inter-arrival time of Alice is converted to Ai by ReqC. Assuming the best case scenario for an adversary Bob, the memory response Br contains all possible information about the response to Alice (which is Ar ) and thus the request of Alice (which is Ai ). Now the RespC modiﬁes this response (Br ) to give Bob a further shaped response B. Schematically, A → Ai B ← Br = Ar The arrows represent ReqC and RespC, which make the inputs more unlikely to leak information. We will complete this argument using a data processing inequality for the best case scenario for an adversary Bob (Ar = Br ). According to data processing inequality, the inequality loosely states that processing data does not increase MI1 . This implies: I (A; B ) ≤ I (A; Ai ) and using it the other way I (A; B ) ≤ I (B ; Ai ) So the overall system will be at following: least as good as the I (A; B ) ≤ min(I (A; Ai ), I (B , Ai )) which in other words BDC will be at least as good as the best of ReqC and RespC. Since ReqC and RespC use the same throttling and fake trafﬁc generation mechanism, BDC will be at least as good as ReqC, as described in section IV-B2. 4) Leakage Within A Replenishment Window: Camouﬂage focuses on preventing leakage of long term timing information (analyzed above), on the order of thousands of cycles (longer than the replenishment period of Camouﬂage), because long term timing information is the easiest to exploit with practical attacks. Nevertheless, we analyze Camouﬂage in the extreme case where very ﬁne grain timing information can be used within a replenishment period to gain information in a side-channel attack. As a worst case attack, we assume the adversary can receive a bit of information for every request of its own. If its request is delayed, it knows the victim had a request at the same time, else the victim did not have a request. We also make a conservative assumption that the adversary knows its shaped distribution, the distribution of the victim, and it can speciﬁcally control the timing of its requests. Given these conservative assumptions, leaked information is bounded by the number of credits that the adversary has. In practice, this leakage is mitigated by the lack of the timing accuracy that the adversary can use to time memory requests. This severely degrades the number of (fractional) bits that can be leaked per memory conﬂict. Also, any time that the victim is actively shaped or fake trafﬁc is created by Camouﬂage, the intrinsic timing information is obfuscated. If leakage within replenishment window is considered to be a signiﬁcant threat, the timing of memory requests can be randomized within the interval (that a credit represents) to increase the timing uncertainty and probability of memory conﬂict in a 1More formally, if X → Y → Z is a Markov Chain i.e., Z conditioned on Y is independent of X, then I (X ; Y ) (cid:2) I (X ; Z ) 343 0   1       & 0 & 1 & 2 & 3 &   	5/	,4 ,, . 	5/	  ,, . 0 0   1 1 2 2 3 3  	 ,5	/ ,, ,  5""#!!#) 5!$                                    Figure 8. Online Genetic Algorithm. Same GA as in MITTS [13] Figure 9. Memory Request Return Time Difference of Two Techniques randomized manner. Also, short term information leakage can be mitigated by reducing the size of the replenishment window. C. Optimizing Bin Conﬁguration Camouﬂage uses a genetic algorithm (GA) to optimize the hardware bin conﬁgurations initially, and guarantees the request/response inter-arrival time distributions do not change due to application interference. Genetic algorithms work well for non-convex search spaces like what we have. For the BDC, we need an algorithm to optimize performance while camouﬂaging the memory trafﬁc. With a 10bin Camouﬂage that shapes both requests and responses, the search space could be (M AX CREDI T S 20 ), where M AX CREDI T S is the total number of credits allowed in a bin. For all of the results presented for the BDC, we use an online genetic algorithm to optimize bin conﬁgurations. The online genetic algorithm trades off program performance for timing information leakage by reconﬁguring the hardware bins. When a constant amount of information leakage is allowed, the online genetic algorithm reconﬁgures the request/response hardware bins after a ﬁxed amount of time or after a program phase change. As a result, distributions are ﬁxed within a reconﬁguration window, but are different across conﬁguration windows. To avoid leakage due to reconﬁguration, the online genetic algorithm can be used at the beginning of the program, the proposed conﬁguration will be used for the rest of the program after the conﬁguration phase. Genetic Algorithms are ﬂexible enough to optimize for any objective functions, such as program performance, multi-program system throughput, fairness, program security, or a combination of all. In our example, we are interested in preventing timing information leakage within a multi-program system without compromising system throughput. Speciﬁcally, the Genetic Algorithm optimizes for multi-program average slowdown, by minimizing n i=1 . The online genetic algorithm in Figure 8 conﬁgures Camouﬂage at the the beginning of a program phase (Conﬁg Phase), and uses the optimal conﬁguration for the rest n(cid:2) slowdowni ), of program phase (Run Phase). The GA needs to run several generations (typically 20 or 30) with around 2030 conﬁgurations tested in each generation. We name each conﬁguration a child within a generation. The genetic algorithm is able to search for optimal bin conﬁguration for either a single-program workload or a multi-program workload. For a multi-program workload, the GA optimizes all bins from all programs simultaneously. We use MISE’s [30] online proﬁling design to measure application slowdown (slowdown of an App = (1 − α)(α Request S ervice Rate with H ighest P riority Shared Request S ervice Rate α = C ycles spent stalling on memory requests T otal number of cycles ). At the beginning of each generation, the GA runs each workload with highest priority in the memory controller in order to measure workload performance without interference in the memory system. This information is combined with performance evaluated in each child conﬁguration to measure workload slowdown when a workload is mixed with other applications. After each run, the runtime saves the measured objective function in memory. After all candidates in one generation are evaluated, the software GA selects the few best conﬁgurations and uses them to create the genomes for the subsequent generation (children). We have evaluated different conﬁguration sizes (child size), and have decided to use 20000 cycles for an individual conﬁguration measurement. We run 20 generations in total for each reconﬁguration, with about 5000 cycles runtime overhead each generation. D. Memory Side-Channel and Covert-Channel In this section, we measure the effectiveness of RespC in preventing malicious VMs from inferring timing information by measuring their own memory response times. We use multi-program workloads made out of SPEC2006 benchmarks and the Apache web server to verify interference can be largely removed. 1) Leakage Evaluation: We empirically evaluate the security features of Camouﬂage. We measure the accumulated response time difference of w(ADVERSARY, astar) and w(ADVERSARY, mcf) observed by the ADVERSARY application, where ADVERSARY is the adversary application that is stealing timing information from the co-running applications. In order to prevent the ADVERSARY from 344   	  	     (a) astar (b) mcf Figure 10. Performance and Throughput Comparison Between Response Camouﬂage and No Shaping. inferring memory trafﬁc patterns from the co-running applications, Camouﬂage guarantees that the ADVERSARY’s memory response time for each request won’t noticeably change when the co-running application changes its request distribution. As shown in Figure 9, Camouﬂage maintains a ﬂat line of accumulated response time difference between the above workloads, indicating minimal timing channel leakage via response time inspection. First Ready-First Come First Serve (FR-FCFS) memory scheduling on the contrary, has an increased accumulated response time difference. We ran this experiment with all benchmarks we used in Section IV-E, and see similar proﬁles for all the benchmarks. 2) Response Camouﬂage Performance: We evaluate Camouﬂage on two different cases when an application becomes more memory intensive or less intensive. We use workload w(ADVERSARY, astar) and w(ADVERSARY, mcf) to measure memory request return time for the adversary benchmark. We run each application in our 11 workloads as the ADVERSARY. In the IaaS Cloud, the adversary can be any workload in a lower security domain. As we protect astar and mcf from the adversary application, we name astar and mcf as applications under protection. As mcf is more memory intensive compared with astar, the adversary will notice signiﬁcant response time increase when co-running with mcf. Ideally, a secure memory scheduler will not change the response time of a benchmark no matter how the co-running benchmarks change. In this case, we put a response shaper at the egress of the memory controller for the adversary application. The bin conﬁguration is set the same as the response distribution as workload w(ADVERSARY, astar). Because the actual response rate is slower than the desired response rate, Camouﬂage will 345 Figure 11. Camouﬂage Shapes Different Request Inter-arrival Time Distribution into a Desired One send a signal to the memory controller asking for higher priority for the adversary application. Similarly, if we need to maintain the response distribution of the adversary application for workload w(ADVERSARY, mcf), we set the response shaper bin conﬁguration of w(ADVERSARY, astar) the same as w(ADVERSARY, mcf). In this case, response rate is throttled as the adversary experiences higher response rate compared with workload w(ADVERSARY, mcf). We measure the ADVERSARY application performance slowdown and overall throughput slowdown due to response Camouﬂage. Figure 10(a) shows the results of shaping w(ADVERSARY, astar) to the same response inter-arrival distribution as w(ADVERSARY, mcf). And Figure 10(b) shows the result of shaping w(ADVERSARY, mcf) to the same response inter-arrival distribution as w(ADVERSARY, astar). As mcf is more memory intensive than astar, shaping the ADVERSARY’s response of w(ADVERSARY, astar) slows down the ADVERSARY, resulting in an illusion that it is still running with mcf. In the other case, shaping mcf requires requesting higher memory request priority in the memory controller to match the behavior of running with astar. Therefore, the ADVERSARY’s performance improves because of higher scheduling priority. The throughput, however, is degraded as mcf has lower priority compared with no shaping case. This shows the tradeoff between security and performance and Camouﬂage guarantees security at a minor cost of performance. E. Pin/Bus Monitoring 1) Security Evaluation: Distribution Accuracy We evaluate Camouﬂage’s effectiveness in shaping any request distribution into another desired one. In this section, we measure 11 application’s intrinsic memory request distribution, and set the desired request distribution to a ﬁxed DESIRED distribution. The DESIRED distribution has decreasing size of bins, as shown in the bottom right of Figure 11. As shown in Figure 11, different applications have totally different request distributions. We use another hardware bin to measure the post-Camouﬂage memory request distribution, and ﬁnd all the applications have the same distribution as the DESIRED one. This result shows that Camouﬂage can Figure 12. Performance Gain of Request Camouﬂage Compared with Static Rate Limiter 10: while E lapsedT ime < P U LSE do B igBuf f er [N extC acheLine] ← 1. (cid:2) Generate cache ← N extC acheLine + miss for duration of time N extC acheLine C acheLineS ize. end while goto loop. close; else Algorithm 1 Covert Channel Attack 1: procedure G EN ERATE COVERT CHANN E L Keylen ← length of Key top: if i > Keylen then return 0 end if loop: if Key(i) = 1 then while E lapsedT ime < P U LSE do DoN othing end while end if 19: end procedure 2: 3: 4: 5: 6: 7: 8: 9: 11: 12: 13: 14: 15: 16: 17: 18: (a) astar (b) mcf Figure 13. Workload Average Slowdown Compared with TP and FS (with bank partitioning only). Workloads under protection are astar and mcf respectively. camouﬂage any request distribution into a totally different one, mitigating memory timing channel attack at I/O pins and memory buses. 2) Performance Evaluation: We evaluate Camouﬂage’s speedup compared with a static memory request shaper. The static shaper limits a program’s memory requests into a constant rate but it cannot take into account inter-arrival times. By comparing program speedups, we show Camouﬂage always outperforms the static rate limiter with the same average bandwidth. In this experiment, we choose 1GB/s bandwidth for each application. The constant rate limiter only allows REQU EST S IZE requests/second of request rate, which Camouﬂage is able to shape the requests into a distribution which adds up to 1GB/s. Figure 12 shows the performance gain that Camouﬂage achieves. Compared to constant rate shaper, Camouﬂage has 1.12x better geometric mean. This shows Camouﬂage does not compromise performance while trafﬁc shaping. 1GB F. Bi-Directional Camouﬂage Performance In this section, we evaluate BDC by having request shapers for applications under protection and have a response shaper for the adversary application. We ﬁrst run 346 the workload w(ADVERSARY, astar) with an online genetic algorithm to conﬁgure the hardware bin conﬁgurations of the shapers. The genetic algorithm optimizes overall throughput of the workload. In order to evaluate Camouﬂage’s effectiveness in request/response shaping when workload trafﬁc pattern changes, we apply the same conﬁguration to workload w(ADVERSARY, mcf), and evaluate the throughput under trafﬁc shaping. This mimics a workload trafﬁc pattern change from astar to mcf. Camouﬂage keeps ﬁxed request/response inter-arrival distributions. Then we ﬁrst run workload w(ADVERSARY, mcf) with the online genetic algorithm and apply the optimal conﬁguration for workload w(ADVERSARY, astar). As we shape both requests and responses, the response distribution is guaranteed to be the same for these two workloads. We run the experiments, and ﬁnd the response distributions match in two workloads. We compare Camouﬂage with TP [15] where each application is allocated a ﬁxed timing channel in the memory scheduler and FS [16] with bank partitioning. As we use only one rank for all the evaluation section, we did not evaluate FS with rank partitioning. As shown in Figure 13(a) and Figure 13(b), Camouﬂage has minimal impact on workload throughput compared with TP and FS. Camouﬂage provides better performance than FS because FS still requires a constant memory request rate while Camouﬂage does not. G. Covert Channel Prevention In order to empirically evaluate Camouﬂage, we implement an algorithm to conduct a covert channel attack. The program will generate memory requests for a ﬁxed amount of time (PULSE) by writing data to different cache lines if the indexed bit in the key is one, otherwise, it does nothing until the same ﬁxed amount of time has passed. Algorithm 1 shows the pseudocode for this algorithm. We use Camouﬂage request shaping as a demonstration              Figure 14. Memory Trafﬁc Before Key:32hx2AAAAAAA. and After Camouﬂage.               	 Figure 15. Memory Trafﬁc Before Key:32hx01010101. and After Camouﬂage. of covert channel protection. From Figure 14 and Figure 15, we can see Camouﬂage hides the intrinsic trafﬁc effectively by shaping the memory distribution into another distribution. During the idle period, Camouﬂage detects unused credits and generates fake memory trafﬁc as a result. V. R E LATED WORK Covert Channels and Side Channels: Processor architecture features such as simultaneous multithreading, branch prediction, and shared caches inadvertently introduce covert channels and side channels [31], [32]. Camouﬂage complements existing covert channel and side channel protection techniques, preventing memory timing attacks without compromising performance. Memory Attacks: Data encryption and oblivious RAM [17] have been used to prevent leaking sensitive information. While data encryption does not prevent information leakage through statistical inference, ORAM conceals memory access pattern by continuously shufﬂing and reencrypting data as they are accessed. Camouﬂage focuses on memory timing channel protection, and can be combined with data encryption and ORAM for other security aspects. Constant Rate Shaping: A secure processor, Ascend [18] prevents leakage over the ORAM timing channel by forcing ORAM to be accessed at a single, strictly periodic rate. A later enhanced version [14] splits programs into coarse-grain time epochs and chooses a new ORAM rate out of a set of allowed rates at the end of each epoch. These two design points are subsets of Camouﬂage. Camouﬂage provides a larger security and performance tradeoff space as it is more ﬂexible in determining memory trafﬁc inter-arrival times. Interference Reduction: There has been a growing interest in the study of timing channel attacks and mitigation through micro-architectural states, such as cache interference [33], [34], branch predictors [35], and on-chip networks. Non-interference in various hardware resources, such as memory controllers and NoCs, have been studied to prevent timing channels. Ethearal proposed a timedivision multiplexed virtual circuit switching network [36] to provide guaranteed services for applications with real-time deadlines. Temporal Partitioning (TP) divides the memory scheduling window into multiple security domains, and only allows applications in the same security domain to be scheduled in the same time window. Camouﬂage provides a scalable solution compared with TP, as application performance is not inﬂuenced by the number of security domains. A bandwidth reservation technique is proposed [37] to avoid information leakage. Camouﬂage leverages statistical memory inter-arrival time distributions, rather than relying on a ﬁxed scheduling policy. Fixed Service: FS [16] is a memory controller design that guarantees memory requests to be issued at a constant rate. Combined with spatial partitioning (bank/rank partitioning), it improves performance compared with TP. It requires modiﬁcations to the memory controller which Camouﬂage does not necessarily require. It does not handle timing leakage in shared channels such as NoCs and the path to and from memory nor does it work well with thread counts greater than the number of available memory partitions. Camouﬂage can furthermore guarantee ﬁxed response distribution in face of request distribution change. MITTS: MITTS [13] is a distribution-based memory bandwidth shaper that is designed for manycore memory system fairness/throughput and ﬁne-grain pricing in IaaS systems. Camouﬂage is an extension of MITTS that addresses the memory system’s timing-channel leakage problem. Unlike MITTS, Camouﬂage places trafﬁc shapers at different locations (both request and response channels). Unlike MITTS, Camouﬂage generates fake trafﬁc for security purposes which generally hurts performance and is antithetical to MITTS’ purpose. Finally, Camouﬂage communicates with the memory controller in certain circumstances to rate limit responses and prevent overﬂow on the return channels. V I . CONC LU S ION Camouﬂage is a hardware mechanism designed to mitigate memory channel timing attacks. Camouﬂage shapes memory requests/responses inter-arrival time into a predetermined distribution, even creating additional trafﬁc if needed. This prevents malicious parties from inferring information from another security domain by probing the memory bus, or analyzing memory response rate. We compare Camouﬂage with Temporal Partitioning, constant rate limiting (Ascend) and Fixed Service (with bank partitioning). We ﬁnd Camouﬂage on average improves program performance by 1.5x, 1.12x, and 1.32x respectively. Camouﬂage provides 347 a larger performance/security tradeoff space than CS, TP, and FS. We analyze the mutual information which can be communicated with Camouﬂage in place. We also show Camouﬂage defends against a real covert channel attack. ACKNOW L EDG EM EN T This work was partially supported by the NSF under Grants No. CCF-1217553, CCF-1453112, CCF-1438980, and CNS-1409415, AFOSR under Grant No. FA9550-141-0148, and DARPA under Grant No. N66001-14-1-4040. "
2018,Routerless Network-on-Chip.,"Traditional bus-based interconnects are simple and easy to implement, but the scalability is greatly limited. While router-based networks-on-chip (NoCs) offer superior scalability, they also incur significant power and area overhead due to complex router structures. In this paper, we explore a new class of on-chip networks, referred to as Routerless NoCs, where routers are completely eliminated. We propose a novel design that utilizes on-chip wiring resources smartly to achieve comparable hop count and scalability as router-based NoCs. Several effective techniques are also proposed that significantly reduce the resource requirement to avoid new network abnormalities in routerless NoC designs. Evaluation results show that, compared with a conventional mesh, the proposed routerless NoC achieves 9.5X reduction in power, 7.2X reduction in area, 2.5X reduction in zero-load packet latency, and 1.7X increase in throughput. Compared with a state-of-the-art low-cost NoC design, the proposed approach achieves 7.7X reduction in power, 3.3X reduction in area, 1.3X reduction in zero-load packet latency, and 1.6X increase in throughput.","2018 IEEE International Symposium on High Performance Computer Architecture Routerless Networks-on-Chip Fawaz Alazemi, Arash Azizimazreah, Bella Bose, Lizhong Chen Oregon State University, USA {alazemif, azizimaa, bose, chenliz}@oregonstate.edu ABSTRACT Traditional bus-based interconnects are simple and easy to implement, but the scalability is greatly limited. While router-based networks-on-chip (NoCs) offer superior scalability, they also incur signiﬁcant power and area overhead due to complex router structures. In this paper, we explore a new class of on-chip networks, referred to as Routerless NoCs, where routers are completely eliminated. We propose a novel design that utilizes on-chip wiring resources smartly to achieve comparable hop count and scalability as router-based NoCs. Several effective techniques are also proposed that signiﬁcantly reduce the resource requirement to avoid new network abnormalities in routerless NoC designs. Evaluation results show that, compared with a conventional mesh, the proposed routerless NoC achieves 9.5X reduction in power, 7.2X reduction in area, 2.5X reduction in zero-load packet latency, and 1.7X increase in throughput. Compared with a state-of-the-art low-cost NoC design, the proposed approach achieves 7.7X reduction in power, 3.3X reduction in area, 1.3X reduction in zero-load packet latency, and 1.6X increase in throughput. 1. INTRODUCTION As technologies continue to advance, tens of processing cores on a single chip-multiprocessor (CMP) has already been commercially offered. Intel Xeon Phi Knight Landing [12] is an example of a single CMP that has 72 cores. With hundreds of cores in a CMP around the corner, there is a pressing need to provide efﬁcient networks-on-chip (NoCs) to connect the cores. In particular, recent chips have exhibited the trend to use many but simple cores (especially for special-purpose many-core accelerators), as opposed to a few but large cores, for better power efﬁciency. Thus, it is imperative to design highly scalable and ultra-low cost NoCs that can match with many simple cores. Prior to NoCs, buses have been used to provide on-chip interconnects for multi-core chips [7, 8, 14, 17, 37, 38]. While many techniques have been proposed to improve traditional buses, it is hard for their scalability to keep up with modern many-core processors. In contrast, NoCs offer a decentralized solution by the use of routers and links. Thanks to the switching capability of routers to provide multiple paths and parallel communications, the throughput of NoCs is signiﬁcantly higher than that of buses. Unfortunately, routers have been notorious for consuming a substantial percentage of chip’s power and area [20, 21]. Moreover, the cost of routers increases rapidly as link width increases. Thus, except for a few ad hoc designs, most on-chip networks do not employ link width higher than 256-bit or 512-bit, even though additional wiring resources may be available. In fact, our study shows that, a 6x6 256-bit Mesh only uses 3% of the total available wiring resources (more details in Section 3). The high overhead of routers motivates researchers to develop routerless NoCs that eliminate the costly routers but use wires more efﬁciently to achieve scalable performance. While the notion of routerless NoC has not been formally mentioned before, prior research has tried to remove routers with sophisticated use of buses and switches, although with varying success. The goal of routerless NoCs is to select a set of smartly placed loops (composed of wires) to connect cores such that the average hop count is comparable to that of conventional router-based NoCs. However, the main roadblocks are the enormous design space of loop selection and the difﬁculty in avoiding deadlock with little or no use of buffer resources (otherwise, large buffers would defeat the purpose of having routerless NoCs). In this paper, we explore efﬁcient design and implementation to materialize the promising beneﬁts of routerless NoCs. Speciﬁcally, we propose a layered progressive method that is able to ﬁnd a set of loops that meet the requirement of connectivity and the limitation of wiring resources. The method progressively constructs the design of a large routerless network from good designs of smaller networks, and is applicable to any n × m many-core chips with superior scalability. Moreover, we propose several novel techniques to address the challenges in designing routerless interface to avoid network abnormalities such as deadlock, livelock and starvation. These techniques result in markedly reduced buffer requirement and injection/ejection hardware overhead. Compared with a conventional router-based Mesh, the proposed routerless design achieves 9.48X reduction in power, 7.2X reduction in area, 2.5X reduction in zero-load packet latency, and 1.73X increase in throughput. Compared with the current state-of-the-art scheme that tries to replace routers with less costly structures (IMR [28]), the proposed scheme achieves 7.75X reduction in power, 3.32X reduction in area,1.26X reduction in zero-load packet latency, and 1.6X increase in throughput. 2. BACKGROUND AND MOTIVATION 2.1 Related Work Prior work on on-chip interconnects can be classiﬁed into bus-based and network-based. The latter can be further categorized as router-based NoCs and routerless NoCs. The main difference between bus-based interconnects and routerless NoCs is that bus-based interconnects use buses in a direct, simple and primitive way, whereas routerless NoCs use a network of buses in a sophisticated way and typically need some sort of switching that earlier bus systems do not need. Each of the three categories is discussed in more detail below. Bus-based Interconnects are centralized communication systems that are straightforward and cheap to implement. While buses work very well for a few cores, the overall performance degrades signiﬁcantly as more cores are connected to the bus [17, 37]. The two main reasons for such 2378-203X/18/$31.00 ©2018 IEEE DOI 10.1109/HPCA.2018.00049 492 degradation are the length of the bus and its capacitive load. Rings [7,8,14] can also be considered as variants of bus-based systems where all the cores are attached to a single bus/ring. IBM Cell processor [38] is an improved bus-based system which incorporates a number of bus optimization techniques in a single chip. Despite having a better performance over conventional bus/ring implementations, IBM Cell process still suffers from serious scalability issues [4]. Router-based NoCs are decentralized communication systems. A great deal of research has gone into this (e.g., [10, 13, 18, 23, 25, 26, 31, 33], too many to cite all here). The switching capability of routers provides multiple paths and parallel communications to improve throughput, but the overhead of routers is also quite substantial. Bufferless NoC (e.g., [15]) is a recent interesting line of work. In this approach, buffer resources in a router are reduced to the minimal possible size (i.e. one ﬂit buffer per input port). Although bufferless NoC is a clever approach to reduce area and power overhead, the router still has other expensive components that are eliminated in the routerless approach (Section 7.5 compares the hardware cost). Routerless NoCs aim to eliminate the costly routers while having scalable performance. While the notion of routerless NoC has not been formally mentioned before, there are several works that try to remove routers with sophisticated use of buses and switches. However, as discussed below, the hardware overhead in these works is quite high, some requiring comparable buffer resources as conventional routers, thus not truly materializing the beneﬁts of routerless NoCs. One approach is presented in [34], where the NoC is divided into segments. Each segment is a bus, and all the segments are connected by a central bus. Segments and central bus are linked by a switching element. In large NoCs, either the segments or the central bus may suffer from scalability issues due to their bus-based nature. A potential solution is to increase the number of wires in the central bus and the number of cores in a segment. However, for NoCs larger than 8 × 8, it would be challenging to ﬁnd the best size for the segments and central bus without affecting scalability. Hierarchical rings (HR) [16] has a similar design approach to [34]. The NoC is divided into disjoint sets of cores, and each set is connected by a ring. Such rings are called local rings. Additionally, a set of global rings bring together the local rings. Packets switch between local and global rings through a low-cost switching element. Although the design has many nice features, the number of switching element is still not small. For example, for an 8 × 8 NoC, there are 40 the 8 × 8 network. Recently, a multi-ring-based NoC called switching element, which is close to the number of routers in isolated multiple rings (IMR) is proposed in [28] and has been shown to be superior than the above Hierarchical rings. To our knowledge, this is the latest and best scheme so far along the line of work on removing routers. While the proposed concept is promising, the speciﬁc IMR design has several major issues and the results are far from optimal, as discussed in the next subsection. 2.2 Need for New Routerless NoC Designs 2.2.1 Principles and Challenges We use Figure 1 to explain the basic principles of routerless NoCs. This ﬁgure depicts an example of a 16-core chip. The 4 × 4 layout speciﬁes only the positions of the cores, not (a) (b) Figure 1: An example of loops in a 4 × 4 grid. Table 1: Number of unidirectional loops in n × n grid [2]. n # of loops n # of loops 1 0 2 2 3 26 4 426 5 18,698 6 2,444,726 7 974,300,742 8 1,207,683,297,862 any topology. A straightforward but naive way to achieve routerless NoC is to use a long loop (e.g., a Hamiltonian cycle) that connects every node on the chip as shown in Figure 1(a). Each node injects packets to the loop and receives packets from the loop through a simple interface (referred to as RL interface hereafter). Apparently, even if a ﬂit on the loop can be forwarded to the next node at the speed of one hop per cycle, this design would still be very slow because of the average O(n2 ) hop count, assuming an n × n many-core chip. Scalability is poor in this case, as conventional topology as such Mesh has an average hop count of O(n). To reduce the hop count, we need to select a better set of loops to connect the nodes, while guaranteeing that every pair of nodes is connected by at least one loop (so that a node can reach another node directly in one loop). Figure 1(b) shows an example with the use of three loops, which satisﬁes the connectivity requirement and reduces the all-pair average hop count by 46% compared with (a). Note that, when injecting a packet, a source node chooses a loop that connects to the destination node. Once the packet is injected into a loop, it stays on this loop and travels at the speed of one hop per cycle all the way to the destination node. No changing loops is needed at RL interfaces, thus avoiding the complex switching hardware and per-hop contention that may occur in conventional router-based on-chip networks. Several key questions can be asked immediately. Is the design in Figure 1(b) optimal? Is it possible to select loops that achieve comparable hop count as conventional NoCs such as Mesh? Is there a generalized method that we can use to ﬁnd the loops for any n × n network? How can this be done without exceeding the available on-chip wiring resources? Unfortunately, answering these questions is extremely challenging due to the enormous design space. We calculated the number of possible loops for n × n chips based on the method used in [2], where a loop can be any unidirectional circular path with the length between 4 and n. Table 1 lists the results up to n = 8. As can be seen, the number of possible loops grows extremely rapidly. To make things more challenging, because the task is to ﬁnd a set of loops, the design space that the routerless NoC approach is looking at is not the number of loops, but the combination of these loops! A large portion of the combinations would be invalid, as not all combinations can provide the connectivity where there is at least one loop between any source and destination pair. Meanwhile, any selected ﬁnal set of loops needs to comfortably ﬁt in the available wiring resources on the chip. Specifically, when loops are superpositioned, the number of over493 Table 2: Wiring resources in a many-core processor chip. Many Core Processor Number of Cores NoC Size Die Area Technology Interconnect Inter-core Metal Layers Xeon Phi, Knights Landing 72 6×6 (31.9mm x 21.4mm) 683 mm2 [3] FinFET 14nm 13 Metal Layers Metal Pitch [22] [30] Layer M4 80nm M5 104nm Figure 2: A long wire in NoCs with repeaters. lapped loops between any neighboring node pairs should not exceed a limit. In what follows, we use overlapping to refer to the number of overlapped loops between two neighboring nodes (e.g., in Figure 1(b) some neighboring nodes have two loops passing through them while others have only one loop passing), and use overlapping cap to refer to the limit of the overlapping. Note that the cap should be much lower than the theoretical wiring resources on chip due to various practical considerations (analyzed in Section 3). As an example, if the overlapping cap is 1, then Figure 1(a) has to be the ﬁnal set. If the overlapping cap increases to 2, it provides more opportunity for improvement, e.g., the better solution in Figure 1(b). The overlapping cap is a hard limit and should not be violated. However, as long as this cap is met, it is actually beneﬁcial to approach this cap for as many neighboring node pairs as possible. Doing this indicates more wires are being utilized to connect nodes and reduce hop count. 2.2.2 Major Issues in Current State-of-the-Art There are several major issues that must be addressed in order to achieve effective routerless NoCs. We use IMR [28] as an example to highlight these issues. IMR is a state-of-theart design that follows the above principle to deploy a set of rings such that each ring joins a subset of cores. While IMR has been shown to outperform other schemes with or without the use of routers, the fundamental issues in IMR prevent it from realizing the true potential of routerless NoCs. This calls for substantial research on this topic to develop more efﬁcient routerless designs and implementations. (1) Large overlapping. For example, IMR uses a large number of superpositioned rings (equivalent to the above-deﬁned overlapping cap of 16) without analyzing the actual availability of wiring resources on-chip. (2) Extremely slow search. A genetic algorithm is used in IMR to search the design space. This general-purpose search sults for 16 × 16, and is not able to produce good results in a algorithm is very slow (taking several hours to generate rereasonable time for larger networks). Moreover, the design generated by the algorithm is far from optimal with high hop counts, as evaluated in Section 6. Thus, efforts are much needed to utilize clever heuristics to speed up the process. (3) High buffer requirement. Currently, the network interface of IMR needs one packet-sized buffer per ring to avoid deadlock. Given that up to 16 rings can pass through an IMR interface, the total number of buffers at each interface is very close to a conventional router. The above issues are addressed in the next three sections. Section 3 analyzes the main contributing factors that determine the wiring availability in practice, and estimates reasonable overlapping caps using a contemporary many-core processor. Section 4 proposes a layered progressive approach to select a set of loops, which is able to generate highly 128 × 128). Section 5 presents our implementation of routerscalable routerless NoC designs in less than a second (up to less interface. This includes a technique that requires only one ﬂit-sized buffer per loop (as opposed to one packet-sized buffer per loop). This technique alone can save buffer area by multiple times. 3. ANALYSIS ON WIRING RESOURCES 3.1 Metal Layers As technology scales to smaller dimensions, it provides a higher level of integration. With this trend, each technology comes with an increasing number of routing metal layers to meet the growing demand for higher integration. For example, Intel Xeon Phi (Knights Landing) [1] and KiloCore [9] are fabricated in the process technology with 11 and 13 metal layers, respectively. Each metal layer has a pitch size which deﬁnes the minimum wire width and the space between two adjacent wires. The physical difference between metal layers results in various electrical characteristics. This allows designers to meet their design constraints such as delay on the critical nets by switching between different layers. Typically, lower metal layers have narrower width and are used for local interconnects (e.g., within a circuit block); higher metal layers have wider width and are used for global interconnects (e.g., power supply, clock); middle metal layers are used for semi-global interconnects (e.g., connecting neighboring cores). Table 2 lists several key physical parameters of Xeon Phi including the middle layers that can be used for on-chip networks. 3.2 Wiring in NoC To estimate the actual wiring resources that can used for 494 routing, several important issues should be considered when placing wires on the metal layers. Routing strategy: In general, two approaches can be considered for routing interconnects over cores in NoCs. In the ﬁrst approach, dedicated routing channels are used to route wires in NoCs. This method of routing was widely used in earlier technology nodes where only three metal layers were typically provided [36], and it has around 20% area overhead. In the second approach, wires are routed over the cores at different metal layers [32]. In the modern technology nodes with six to thirteen metal layers, this approach of routing over logic becomes more common for higher integration. This can be done in two ways: 1) several metal layers are dedicated for routing wires, and 2) a fraction of each metal layer is used to route the wires. The ﬁrst way is preferable given that many metal layers are available in advanced technology nodes [32, 36]. Repeater: Wires have parasitic resistance and capacitance which increase with the length of wires. To meet a speciﬁc target frequency, a long wire needs to be split into several segments, and repeaters (inverters) are inserted between the segments, as shown in Figure 2. The size of repeaters should be considered in estimating the available wiring resources. For a long wire in the NoC, the size of each repeater (h times of an inverter with minimum size) is usually not small, but the number of repeaters (k) needed is small [27]. In fact, it has been shown that increasing K has negligible improvement in reducing the delay [27]. For a 2GHz operating frequency, using only one repeater with the size of 40 times W/L of the minimum sized inverter can support a wire length of 2mm [32], which is longer than the typical distance between two cores in a many-core processor [35]. Coping with cross-talk: Cross-talk noises can occur either between the wires on the same metal layer or between the wires on different metal layers, both of which may affect the number of wires that can be placed. The impact of cross-talk noises on voltage can be calculated by Equation (1) as the voltage changes on a ﬂoated victim wire [19]. ΔVvict im = Cad j Cvict im + Cad j × ΔVaggressor (1) where ΔVvict im is the voltage variation on the victim wire, ΔVaggressor is the voltage variation on the aggressor, Cvict im is the total capacitance (including load capacitance) of the victim wire, and Cad j is the coupling capacitance between the aggressor and the victim. It can be observed from Equation (1) that the impact of cross-talk on the victim wire depends on the ratio of Cad j to Cvict im . Hence, the cross-talk on the same layer has much larger impact on the power, performance, and functionality of the NoC since the adjacent wires which run in parallel on the same metal layer has larger coupling capacitance (Cad j ) [19]. There are two major techniques to mitigate cross-talk noises, shielding and spacing. In the shielding approach, crosstalk noises are largely avoided between two adjacent wires by inserting another wire (which is usually connected to the ground or supply voltage) between them. In the spacing approach, adjacent wires are separated by a certain distance that would keep the coupling noise below a level tolerable by the target process and application. Compared with spacing, shielding is much more effective as it can almost remove crosstalk noises [5]. However, shielding also incurs more area overhead as the distance used in the spacing approach is usually smaller than that of inserting a wire. Layer 1 Layer 2 Layer 3 Layer 4 Figure 3: Layers of an 8 × 8 grid. 3.3 Usable Wires for NoCs To gain more insight on how many wiring resources are usable for on-chip networks under current manufacturing technologies, we estimated the number of usable wires by taking into account the above factors. The estimation is based on using two metal layers to route wires over the cores. The area overhead of the repeater insertion including the via contacts and the area occupation of the repeaters are considered based on the layout design rules of each metal layer. We used the conservative way of shielding to reduce crosstalk noises (and the inserted wires are not counted towards usable wires), although spacing may likely offer more usable wires. In addition, in practice, 20% to 30% of each dedicated metal layer for routing wires over the cores is used for I/O signals, power, and ground connections [32]. This overhead is also accounted for. The maximum values of h and K are used for worst-case estimation. As such, the above method gives a very conservative estimation of the usable wires. Assuming that there is a chip with similar physical conﬁguration as Table 2, the two metal layers M4 and M5 under 14nm technology can provide 101,520 wires in the cross-section. This translates into 793 unidirectional links of 128-bit, or 396 unidirectional links of 256-bit, or 198 unidirectional links of 512-bit in the cross-section. In contrast, a 6 × 6 mesh only uses 12 unidirectional 256-bit links in the bisection, which is about 3% of the usable wires. It is important to note that the conventional router-based NoCs do not use very wide links for good reasons. For instance, router complexity (e.g., the number of crosspoints in switches, the size of buffers) increases rapidly as the link width increases. Also, although wider links provide higher throughput, it is difﬁcult to capitalize on wider links for lower latency. The reduction in serialization latency by using wider links quickly becomes insigniﬁcant as link width approaches the packet size. This motivates the need for designing routerless NoCs where wiring resources can be used more efﬁciently. The above estimation of the number of usable wires helps to decide the overlapping cap mentioned previously. To avoid taxing too much on the usable wiring resources and to have a scalable design, we propose to use an overlapping cap of n for n × n chips. In the above 6 × 6 case, this translates into 4.5% of the usable wires for 128-bit loop width, or 9.1% for 256-bit loop width. This parameterized overlapping cap helps to provide the number of loops that is proportional to chip size, so the quality of the routerless designs can be consistent for larger chips. 4. DESIGNING ROUTERLESS NOCS 4.1 Basic Idea 495 Layer 1 = + Figure 4: Loops in L1 , and M2 = L1 . Our proposed routerless NoC design is based on what we call layered progressive approach. The basic idea is to select the loop set in a progressive way where the design of a large routerless network is built on top of the design of smaller networks. Each time the network size increments, the newly selected loops are conceptually bundled as a layer that is reused in the next network size. k × k grid (2 ≤ k ≤ n) that meets the connectivity, overlapping Speciﬁcally, let Mk be the ﬁnal set of selected loops for and low hop count requirements. We construct Mk+2 by combining Mk with a new set (i.e., layer) of smartly placed loops. The new layer utilizes new wiring resources that are available when expanding from k × k to (k + 2) × (k + 2). The resulting Mk+2 can also meet all the requirements and deliver superior performance. For example, as shown in Figure 3, the grid is logically split into multiple layers with increasing sizes. Let Lk be the set of loops selected for Layer k. Firstly, suppose that we already ﬁnd a good set of loops for 2 × 2 grid that connects all the nodes with a low hop count and does not exceed an overlapping of 2 between any neighboring nodes. That set of loops is M2 , which is also L1 as this is the base case. Then we ﬁnd another set of loops L2 , together with M2 , can form a good set of loops for 4 × 4 grid (i.e., M4 = L2 ∪ M2 ). The resulting M4 can connect all the nodes with a low hop count and do not exceed an overlapping of 4 between any neighboring nodes. And so on so forth, until reaching the targeted n × n grid. In general, we have Mn = L(cid:4)n/2(cid:5) ∪ Mn−2 = L(cid:4)n/2(cid:5) ∪ L(cid:4)n/2(cid:5)−2 ∪ Mn−4 = . . . = L(cid:4)n/2(cid:5) ∪ L(cid:4)n/2(cid:5)−2 ∪ L(cid:4)n/2(cid:5)−4 ∪ . . . ∪ L1 . Apparently, the key step in the above progressive process is how to select the set of loops in Layer k, which enables the progression to the next sized grid with low hop count and overlapping. In the next subsections, we walk through several examples to illustrate how it is done to progress from 2 × 2 grid to 8 × 8 grid. 4.2 Examples 4.2.1 2 × 2 Grid possible loops, one in each direction, in a 2 × 2 grid. Both This is the base case with one layer. There are exactly two of them are included in M2 = L1 , as shown in Figure 4. The resulting M2 satisﬁes the requirement that every source and destination pair is connected by at least one loop. The maximum number of loops overlapping between any neighboring nodes is 2, which meets the overlapping cap. This set of loops achieves a very low all-pair average hop count of 1.333, which is as good as the Mesh. 4.2.2 4 × 4 Grid M4 consists of loops from two layers. Based on our layered progressive approach, L1 is from M2 . We select 8 loops to form L2 , as illustrated in Figure 5. The 8 loops fall into four groups (from this network size and forward, each new layer is constructed using four groups with the similar heuristics as discuss below). The ﬁrst group, A4 (the subscript indicates the size of the grid), has only one anti-clockwise loop. It provides connectivity among the 12 new nodes when expanding from Layer 1 to Layer 2. The loops in the second group, B4 , have the ﬁrst column as the common edge of the loops, but the opposite edge of the loops moves gradually towards the right (this is more evident in group B6 in Figure 6). Similarly, the third group, C4 , uses the last column as the common edge of the loops and gradually moves the opposite edge towards the left. It can be veriﬁed that groups B4 and C4 provide connectivity between the 12 new nodes in Layer 2 and the 4 nodes in Layer 1. Since the connectivity among the 4 requirement of 4 × 4 grid is met by having L1 , A4 , B4 and inner nodes has already been provided by L1 , the connectivity C4 . The fourth group, D4 , offers additional “shortcuts” in the horizontal dimension. A very nice feature of the selected M4 is that the wiring resources are efﬁciently utilized, as the overlapping between many neighboring node pairs is close to the overlapping cap of 4. For example, for the ﬁrst (or the last) column, each group of loops has exactly one loops passing through that column, totaling an overlapping of 4, which is the same as the cap. Thus, no overlapping “ration” is under-utilized. For the second column (or the third) column, groups A4 and D4 have no loop passing through, and groups B4 and C4 have two loops passing through in total. However, note that the ﬁnal M4 also includes L1 which has two loops passing through the second (or the third) column. Hence, the total overlapping of the middle columns is also 4, exactly the same as the cap. Simple counting can show that the overlapping on the horizontal dimension is also 4 for each row. Owing to this efﬁcient use of wiring resource “ration”, the all-pair average hop count is 3.93 for the selected set of loops in M4 . The ﬁnal set is M4 = L2 ∪ M2 = L2 ∪ L1 . 4.2.3 6 × 6 Grid M4 , and L3 is formed in a similar fashion as 4 × 4 grid from M6 consists of loops from three layers. L1 and L2 are from four groups, as illustrated in Figure 6. Again, connectivity is provided by M4 and groups A to C. Together with group D, the number of overlapping on each column and row is 6, thus fully utilizing the allocated wiring resources. Additionally, for the purpose of reducing hop count and balancing horizontal and vertical wiring utilization, when we combine M4 and L3 to form M6 , every loop in M4 is reversed and then rotated for 90◦ clockwise1 . If this slightly changed M4 is denoted as M (cid:7) 4 , the ﬁnal set can be expressed as M6 = L3 ∪ M (cid:7) = L3 ∪ (L2 ∪ L1 )(cid:7) , with an all-pair average hop count of 6.07. 4 4.2.4 8 × 8 Grid in Figure 7. The ﬁnal set M8 is M8 = L4 ∪ M (cid:7) Similar to earlier examples, L4 consists of loops shown = L4 ∪ (cid:2) L3 ∪ (L2 ∪ L1 )(cid:7) (cid:3)(cid:7) with an all-pair average hop count of 8.32. 6 4.3 Formal Procedure For an n × n grid, the loops for a routerless NoC design can be recursively found by the procedure shown in Algorithm 1. The procedure is recursive and denoted as RLrec. The procedure begins by generating loops for the outer layer, say layer i, and then it recursively generates loops for layer i − 1 1 In 4 × 4 grid, reversal and rotation of M2 is not necessary because M2 and M (cid:7) 2 have the same effect on L1 . 496 Layer 2 = +   +   + + + + +   Figure 5: Loops in L2 . M4 = L2 ∪ L1 .   A 6 B 6 A 8 B 8 C 6 D6 Figure 6: Loops in L3 . M6 = L3 ∪ L2 ∪ L1 . : NL , NH ; the low and high numbers Algorithm 1: RLrec 2 3 Input 1 begin if NL = NH then return {} if NH − NL = 1 then 4 5 6 7 8 9 10 11 12 13 14 15 16 17 return M Let M = {} M = M ∪ G(NL , NH , NL , NH , clockwise) M = M ∪ G(NL , NH , NL , NH , anticlockwise) M = M ∪ G(NL , NH , NL , NH , anticlockwise) for i = NL + 1 → NH − 1 do M = M ∪ G(NL , NH , NL , i, clockwise) M = M ∪ G(NL , NH , i, NH , clockwise) M = M ∪ G(i, i + 1, NL , NH , clockwise) M (cid:7) = RLrec(NL +1, NH -1) Reverse and rotate for 90◦ every loop in M (cid:7) for i = L → H − 1 do return M ∪ M (cid:7) // Group A // Group B // Group C // Group D and so on until the base case is reached or the layer has a single node or empty. Procedure G(r1 , r2 , c1 , c2 , d ) is a simple function that generates a rectangular shape loop with corners (r1 , c1 ), (r1 , c2 ), (r2 , c1 ) and (r2 , c2 ) and direction d . When processing each layer in this algorithm, procedure G is called repeatedly to generate four groups of loops. Additionally, the generated loops rotate 90 degrees and reverse directions after processing each layer to balance wiring utilization and reduce hop count, respectively. The ﬁnal loops generated by the RLrec algorithm have an overlapping of at most n. While it would be ideal if an analytical expression can be derived to calculate the average hop count for this heuristic 497 C 8 Figure 7: Loops in L4 . M8 = L4 ∪ L3 ∪ L2 ∪ L1 . D8 Ejection  Links Link Selector    & arbitrator Injection  Link Routing   Table Output 1 Output m Loop 1 Loop m Single flit  buffer Single flit  buffer s B X E f o l o o p A EXB  EXB k Figure 8: Routerless interface components. approach, this seems to be very challenging at the moment. However, it is possible to calculate the average hop count numerically. This result is presented in the evaluation, which shows that our proposed design is highly scalable. 5. IMPLEMENTATION DETAILS After addressing the key issue of ﬁnding a good set of loops, the next important task is to efﬁciently implement the routerless NoC design in hardware. Because of the routerless nature, no complex switching or virtual channel (VC) structure is needed at each hop, so the hardware between nodes and loops has a small area footprint in general. However, due to various potential network abnormalities such as deadlock, livelock, and starvation, a certain number of resources are required to guarantee correctness. If not addressed appropriately, this may cause substantial overhead that is comparable to router-based NoCs. In this section, we propose a few       Clock cycle i Clock cycle i+1 Clock cycle i+2 Clock cycle i+3  X Injection Q Y     Z  X Injection Q  Y    Z    X Injection Q Y   Z     X Injection Q Y  Z Figure 9: Injecting a long packet requires a packet-sized buffer per loop at each hop in prior implementation (X, Y and Z are interfaces). effective techniques to minimize those overhead. In a routerless NoC, each node uses an interface (RL interface) to interact with one or multiple loops that pass through this node. Figure 8 shows the main components of a RL interface. While details are explained in the following subsections, the essential function of the interface includes injecting packets into a matching loop based on connectivity and availability, forwarding packets to the next hop on the same loop, and ejecting packets at the destination node. Notice that packets cannot switch loops once injected. All the loops have the same width (e.g., 128-bit wires). 5.1 Injection Process 5.1.1 Extension Buffer Technique A loop is basically a bundle of wires connected with ﬂipﬂops at each hop (Figure 8). At clock cycle i, a ﬂit arriving at the ﬂip-ﬂop of loop l must be consumed immediately by either being ejected at this node or forwarded to the next hop on loop l through output l. If no ﬂit arrives at loop l (thus not using output l), the RL interface can inject a new ﬂit on loop l through output l. However, it is possible that an injecting packet consists of multiple ﬂits and requires several cycles to ﬁnish the injection, during which other ﬂits on loop l may arrive at this RL interface. Therefore, addition buffer resources are needed to hold the incoming ﬂits temporarily. If routerless NoC uses the scheme proposed in prior ringbased work (e.g., IMR [28]), a full packet-sized buffer per loop at each hop would be needed to ensure correctness, which is very inefﬁcient. As illustrated in Figure 9, a long packet B with multiple ﬂits is waiting for injection (there is no issue if it is a short single-ﬂit packet). At clock cycle i, the injection is allowed because packet B sees that no other ﬂit in Interface Y is competing with B for the output to Interface Z . From cycle i + 1 to i + 3, the ﬂits of B are injected sequentially. However, while packet B is being injected during these cycles, another long packet A may arrive at Interface Y . Because RL interfaces do not employ ﬂow control to stop the upstream node, Interface Y needs to provide a packet-sized buffer to temporarily store the entire packet A. A serious inefﬁciency lies in the fact that, if there are m loops passing through a RL interface, the interface needs to have m packet-sized buffers, one for each loop. To address this inefﬁciency, we notice that an interface injects packets one at a time, so not all the loops are affected simultaneously. Based on this observation, we propose the extension buffer technique to share the packet-sized buffer among loops. As shown in Figure 8, each loop has only a ﬂit-sized buffer, but the interface has a pool of extension buffers (EXBs). The size of each EXB is the size of a long packet, so when a loop is “extended” with an EXB, it would be large enough to store a long packet. Minimally, only one EXB is needed in the pool, but having multiple EXBs may have slight performance improvement. This is because another injection might occur while the previous EXB is not entirely released (drained) due to a previous injection (e.g., clock cycle i + 3 in Figure 9). However, as shown later in the evaluation, the performance difference is negligible. As a result, our proposed technique of using one shared EXB can essentially achieve the same objective of ensuring correctness as IMR but reduces the buffer requirement by m times. This is equivalent to an 8X saving in buffer resources in 8 × 8 networks and 16X saving in 16 × 16 networks. 5.1.2 Injection Process The injection process with the use of EXBs is straightforward. To inject a packet p of n f ﬂits, the ﬁrst step is to look up a small routing table to see which loop can reach p’s destination. The routing table is pre-computed since all the loops are pre-determined. The packet p then waits for the loops to become available (i.e., having sufﬁcient buffer space). Assume l is a loop that has the shortest distance to the destination among all the available loops. When the injection starts, the interface holds the output port of l for n f cycles to inject p, and assigns a free E X B to l if n f > 1 and l is not already connected to another E X B. During those n f cycles, any incoming ﬂit through the input port of l is enqueued in the extension buffer. The EXB is released later when its buffer slots are drained. 5.2 Ejection Process The ejection process starts as soon as the head ﬂit of a packet p reaches the RL interface of its destination node. The interface ejects p, one ﬂit per cycle. Once p is ejected, the interface will wait for another packet to eject. There is, however, a potential issue with the ejection process. While unlikely, a RL interface with m loops may receive up to m head ﬂits simultaneously in a given cycle that are all destined to this node. Because any incoming packets need to be consumed immediately and the packets are already at the destination, the interface needs to have m ejection links in order to eject all the packets in that cycle. As each eject link has the same width as the loop (i.e., 128-bit), this incurs substantial hardware overhead. probability of having k packets (1 < k ≤ m) arriving at the To reduce this overhead, we utilize the fact that the actual same destination in the same cycle is low, and this probability decreases drastically as k increases. Based on this observation, we propose to optimize for the common case where only e ejection links are provided (e (cid:9) m). If more than e packets arrive at the same cycle, (k − e) packets are forwarded to the next hop. Those deﬂected packets will continue on their respective loops and will circle back to the destination later. As shown in the evaluation, having two ejection links can reduce the percentage of circling packets to be below 1% on average (1.6% max) across the benchmarks. This demonstrates that this is a viable and effective technique to reduce overhead. 5.3 Avoiding Network Abnormalities As network abnormalities are theoretically possible but 498 practically unlikely scenarios, our design philosophy is to place very relaxed conditions to trigger the handling procedures, so as to minimize performance impact while guaranteeing correctness. 5.3.1 Livelock Avoidance A livelock may occur if a packet circles indeﬁnitely and never gets a chance to eject. We address this issue by having a byte-long circling counter at each head ﬂit with an initial value of zero. Every time a packet reaches its destination interface and is forced to be deﬂected, the counter is incremented by 1. If the circling counter of a packet p reaches 254 but none of the ejection link is available, the interface marks one of its ejection links as reserved and then deﬂects p for the last time. The marked ejection link will not eject any more packets after ﬁnishing the current one, until p circles back to the ejection link (by then the marked ejection link will be available; otherwise there is a possible protocol-level deadlock, discussed shortly). Once p is ejected, the interface will unmark the ejection link for it to function normally. Due to the extremely low circling percentage (maximum 3 times of circling for any packet in our simulations), this livelock avoidance scheme has minimal performance impact. 5.3.2 Deadlock Avoidance With no protocol-level dependence at injection/ejection endpoints, routing-induced deadlock is not possible in routerless NoCs as packets arriving at each hop are either ejected or forwarded immediately. Hence, a packet can always reach its destination interface without being blocked by other packets. The above livelock avoidance ensures that the packet can be ejected within a limited number of circlings. With more than one dependent packet types, the marked ejection link in the above livelock avoidance scheme may not able to eject the current packet (say a request packet) in the ejection queue, because the associated cache controller cannot accept new packets from the ejection queue (i.e., input of the controller). This may happen when the controller itself is waiting for packets (say a reply packet) in the injection queue (i.e., output of the controller) to be injected into the network. A potential protocol-level deadlock may occur if that reply packet cannot be injected, such as the loop is full of request packets that are waiting to be ejected. To avoid such protocol-level deadlock, the conventional approach is to have a separate physical or virtual network for each dependent packet type. While similar approach can be used for routerless NoCs, here we propose a less resource demanding solution. which is made possible by the circling property of loops. This solution only needs an extra reserved EXB, as well as a separate injection and ejection queue for each dependent packet type. The separate injection/ejection queues can come from duplicating original queues or from splitting the original queues to multiple queues. In either case, the loops and wiring resources are not duplicated, which is important to keep the cost low. Following the above livelock avoidance scheme, when a packet p on loop l completes the ﬁnal circling (counter value of 255) and ﬁnds that the marked ejection link is still not available, p is temporarily buffered in the reserved EXB instead of forwarding to output l. Meanwhile, we allow the head packet q in the injection queue of the terminating packet type (e.g., a reply packet in the request-reply example) to inject into loop l through output l. Once q is injected, the cache controller is able to put another reply packet in its output (i.e., the injection queue) which, in turn, allows the controller to accept a new request from its input (i.e., the ejection queue). This creates space in the ejection queue to accept packet p that is previously stored in the reserved EXB. Once p moves to the ejection queue, the EXB is freed. Essentially, the reserved EXB acts as a temporary exchanging space while the separate injection/ejection queues avoid blocking of different packet types at the endpoints. 5.3.3 Starvation Avoidance The last corner case we address is starvation. With the previous livelock and deadlock handling, if a packet is consumed at its destination RL interface, the interface can use the free output to inject a new packet. However, it is possible that a particular interface X is not the destination of any packets and there is always a ﬂit passing through X every single cycle. This never occurred in any of our experiments as it is practically impossible that a cache bank is not accessed by any other cores. However, it is theoretically possible and, when occurred, prevents X from injecting new packets. We propose the following technique to avoid starvation for the completeness of the routerless NoC design. If X cannot inject a packet after a certain number of clock cycles (a very long period, e.g., hundreds of thousand cycles or long enough to have negligible impact on performance), X piggybacks the next passing head ﬂit f with the ID of X. When f is ejected at its destination interface Y, instead of injecting a new packet, Y injects a single-ﬂit no-payload dummy packet that is destined to X. When the dummy packet arrives at X, X can now inject a new packet by using the free slot created by the dummy packet. This breaks the starvation conﬁguration. 5.4 Interface Hardware Implementation Figure 8 depicts the main components of a RL interface. We have explained the extension buffers (EXBs), single-ﬂit buffers, routing table, and multiple ejection links in the previous subsections. The arbitrator receives ﬂits from input buffers and selects up to e input loops for ejection based on the oldest ﬁrst policy. The arbitrator contains a small register that holds the arbitration results. The link status selector is a simple state machine associated with the loops. It monitors the input loops and arbitration results, and changes the state of the loops (e.g., ejection, stall in extension buffers, etc.) in the state machine. There are several other minor logic blocks that are not shown in Figure 8 for better clarity. Note that the RL interface does not use the information of neighboring nodes, which differs from most conventional router-based NoCs that need credits or on/off signals for handshaking. To ensure the correctness of the proposed interface hardware, we implement the design in RTL Verilog that includes all the detailed components. The Verilog implementation is veriﬁed in Modelsim, synthesized in Synopsys Design Compiler, and placed and routed using Cadence Encounter tool. We use the latest 15nm process NanoGate FreePDK 15 Cell Library [29] for more accurate evaluation. As a key result, the RL interface is able to operate at up to 4.3GHz frequency while keeping the packet forwarding process in one clock cycle. This is fast enough to match up with most commercial many-core processors. Injecting packets may take an additional cycle for table look-up. In the main evaluation below, both the interfaces and cores are operating at 2GHz. 499                           	     	                                                           Figure 10: Throughput of routerless NoC under different number of ejection links and extension buffers (EXBs). 6. EVALUATION METHODOLOGY We evaluate the proposed routerless NoC (RL) extensively against Mesh, EVC, and IMR in Booksim [24]. For synthetic trafﬁc workloads, we use uniform, transpose, bit reverse, and hotspot (with 8 hotspots nodes). BookSim is warmed up for 10, 000 clock cycles and then collects performance statistics for another 100, 000 cycles at various injection rates. The injection rate starts at 0.005 ﬂit/node/cycle and is incremented by 0.005 ﬂit/node/cycle until the throughput is reached. Moreover, we integrate Booksim with Synfull [6] for performance study of PARSEC [11] and SPLASH-2 [39] benchmarks. Power and area studies are based on Verilog post-synthesis simulations, as described in Section 5.4. In the synthetic study, each router in Mesh is conﬁgured with relatively small buffer resources, having 2 VCs per link and 3 ﬂits per VC. The link width is set to 256-bit. Also, the router is optimized with lookahead routing and speculative switch allocation to reduce pipeline stages to 2 cycles per router and 1 cycle per link. EVC has the same conﬁguration as Mesh except for one extra VC that is required to enable express channels. For IMR, the ring set is generated by the evolutionary approach described in [28]. To allow a fair comparison with RL, the maximum number of overlapping cap, for both RL and IMR, is set to n for n × n NoC. We also follow the original paper to faithfully implement IMR’s network interface. Each input link in an IMR’s interface is attached with a buffer of 5 ﬂits and the link width is set to 128-bit (the same as the original paper). In RL, loops are generated by RLrec algorithm and accordingly the routing table for each node is calculated. Each interface is conﬁgured with two ejection links and each input link has a ﬂit-size buffer. Also, an EXB of 5 ﬂits is implemented in each interface. The link width is 128-bit (the same as IMR). In all the designs, packets are categorized into data and control packets where each control packet has 8 bytes and each data packet has 72 bytes. Accordingly, data packets in Mesh, EVC, IMR, and RL are of 3, 3, 5 and 5 ﬂits, respectively, and the control packets are of a single ﬂit. For benchmark performance study, we also add 2D Mesh with various conﬁgurations as well as a 3D Cube design into the comparison. RL has the same conﬁguration as the synthetic study. For 2D Mesh, we use 9 conﬁgurations, each having the conﬁguration M(x, y) where x ∈ {1, 2, 3} is the router delay and y ∈ {1, 2, 3} is the buffer size, i.e., routers with 1-cycle, 2-cycle and 3-cycle delay, and with 1-ﬂit, 2-ﬂit and 3-ﬂit buffer size. 3D Cube is conﬁgured with 2 VCs per link, 3 ﬂits per VC, and 2-cycle per hop latency. 7. RESULTS AND ANALYSIS 7.1 Ejection Links and Extension Buffers 500 The proposed RL scheme is ﬂexible to use any number of ejection links and EXBs. On the ejection side, the advantages of having more ejection links are higher chance for packet ejection and lower chance for packet circling in a loop. However, adding more ejection links complicates the design of the interface and leads to additional power and area overhead in the interface and the receiving node. On the injection side, EXBs have a direct effect on the injection latency of long packets. Recall that, a loop must be already attached with an EXB or a free EXB is available to be able to inject a long packet. Similar to ejection links, having more EXBs can lower injection latency but incur larger area and power overhead. We studied the throughput of RL with different conﬁgurations of ejection links and EXBs on various synthetic trafﬁc patterns. The NoC size for this study is 8 × 8. The results are shown in Figure 10. In the ﬁgure, each conﬁguration is denoted by (x, y) where x is the number of ejection links and y is the number of EXBs. The basic and best in terms of area and power overhead is (1, 1) conﬁguration but it has the worst performance. By adding up to three EXBs with a single ejection link, the throughput is only slightly changed (less than 5%). This indicates that the number of EXBs is not very critical to performance, and it is possible to use only one EXB for injecting long packets while saving buffer space. For (2, 1) conﬁguration, it doubles the chance for packet ejection when compared to (1, x) conﬁgurations. The throughput is notably improved by an average of 38% for all the patterns when compared to (1, 1) conﬁgurations. For instance, hotspot trafﬁc pattern has 0.125 throughput in (2, 1) conﬁguration but only 0.065 in (1, 1), a 92.5% improvement). However, on top of (2, 1) conﬁguration, adding up-to three EXBs (i.e., (2, 3)) improves throughput only by 5% on average. Given all the results, we choose the (2, 1) conﬁguration as the best trade-off point, and use it for the remainder of this section. We also plot the (16, 16) conﬁguration which is the ideal case (no blocking in injection or ejection may happen). As can be seen, (2, 1) is very close to the ideal case. Section 7.3 provides a detailed study for the number of times packet circling in loops for the (2, 1) conﬁguration. 7.2 Synthetic Workloads trafﬁc patterns for an 8 × 8 NoC. RL has the lowest zero-load Figure 11 plots the performance results of four synthetic packet latency in all four trafﬁc patterns. For example, in uniform random, the zero-load packet latency is 21.2, 14.9, 10.5, and 8.3 cycles for Mesh, EVC, IMR, and RL, respectively. When averaged over the four patterns, RL has an improvement of 1.59x, 1.43x, and 1.25x over Mesh, EVC, and IMR, respectively. RL achieves this due to low per hop latency (one cycle) and low hop count. In terms of throughput, the proposed RL also has advantage over other schemes. For example, the throughput for hotspot is 0.08, 0.05, 0.06, and 0.125 (per ﬂit/node/cycle) for Mesh, EVC, IMR, and RL, respectively. In fact, RL has the highest throughput for all the trafﬁc patterns. When averaged over the four patterns, RL improves throughput by 1.73x, 2.70x, and 1.61x over Mesh, EVC, and IMR, respectively. This is mainly owing to the better utilization of wiring resources in RL. Note that, EVC has a lower throughput than Mesh as EVC is essentially a scheme that trades off throughput for lower latency at low trafﬁc load. 7.3 PARSEC and SPLASH-2 Workloads      	      Mesh EVC IMR RL 5 15 25 35 45 0.005 0.06 0.115 A e v r e g a  l y c n e a t ( e c y c l ) Injection rate (flits/node/cycle) Hotspot 5 0.005 15 25 35 45 0.06 0.115 0.17 0.225 A e v r e g a  l y c n e a t ( e c y c l ) Injection rate (flits/node/cycle) Bit reverse 5 0.005 15 25 35 45 0.06 0.115 0.17 0.225 A e v r e g a  l y c n e a t ( e c y c l ) Injection rate (flits/node/cycle) Transpose 5 0.005 15 25 35 45 0.06 0.115 0.17 0.225 A e v r e g a  l y c n e a t ( e c y c l ) Injection rate (flits/node/cycle) Uniform Figure 11: Performance comparison for synthetic trafﬁc patterns.                      	         	  	          Figure 12: PARSEC and SPLASH-2 benchmark performance results (y-axis represents average pack latency in cycles.) RL is compared with different Mesh conﬁgurations, EVC, and IMR in (a), (b) and (c). In (d), RL is also compared with a 3D Cube.                                                                                                                ""&""&    $ $      &  & $ # $ "" !        ""         ""  '        %        (  	  Figure 13: Breakdown of power consumption for different PARSEC and SPLASH-2 workloads (normalized to Mesh). We utilize Synfull and Booksim to study the performance of RL, 2D Mesh with different conﬁgurations, EVC, IMR, and a 3D Cube under 16 PARSEC and SPLASH-2 benchmarks. The NoC sizes under evaluation are 4 × 4, 8 × 8 and 16 × 16 for RL, 2D Mesh, EVC and IMR, and 4 × 4 × 4 for 3D cube. Figure 12 shows the results. In Figure 12(a)-(c), RL is compared against 2D Mesh, EVC and IMR. From the ﬁgures, the best conﬁguration for Mesh is M(1,5) (i.e. per hop latency of 1 and buffer size of 5) and the worst is M(3,1). Lowering per hop latency in Mesh helps to improve overall latency, and reducing buffer sizes may cause packets to wait longer for credits and available buffers. The average packet latency of RL in 4 × 4, 8 × 8, and 16 × 16 are 4.3, 8.9 and 20.1 cycles, respectively. This translates into an average latency reduction of RL over M(1,5) by 57.8%, 38.4% and 22.2% in 4 × 4, 8 × 8 and 16 × 16, respectively. The IMR rings in 16×16 are very long and seriously affects its latency. RL reduces the average latency by 23.3% over EVC and 41.2% over IMR. In Figure 12(d), the performance of 3D cube is clearly better than all the Mesh conﬁgurations in (b) mainly due to lower hop count and larger bisection bandwidth. Despite this, RL still offers better performance than 3D cube. The average latency of RL is 8.9 cycles, which is 41% lower than the 15.2 cycles of 3D cube. 7.4 Power Figure 13 compares the power consumption of Mesh (i.e. M(2,3)), EVC, IMR and RL for different benchmarks, normalized to the Mesh. All the power consumption shown in this Figure are reported after P&R in NanGate FreePDK 15 Cell Library [29] by Cadence Encounter. The activity factors for the power measurement are obtained from Booksim, and the power consumption includes that of all the wires. The average dynamic power consumption for RL is only 0.26mW, and for Mesh, EVC and IMR the average is 2.88mW, 4.27mW and 2.91mW, respectively. Because RL has no crossbar, it requires only 9%, 6.1% and 8.9% of the dynamic power consumed by Mesh, EVC and IMR, respectively. Meanwhile, 501         	 	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	   	          	  45281 µm2 IMR 6286 µm2 RL  20930 µm2 28516 µm2 	 Figure 14: Area comparison under 15nm technology. static power is mostly consumed by buffers. Unlike Mesh, EVC and IMR, RL has a much lower buffer requirement. As a result, RL consumes very low static power of 0.18mW on average, while Mesh, EVC and IMR consume 1.39mW, 1.64mW and 0.58mW, respectively. Adding dynamic and static power together, on average, RL reduces the total NoC power consumption by 9.48X, 13.1X and 7.75X over Mesh, EVC and IMR, respectively. 7.5 Area Figure 14 compares the router or interface area of the different schemes we are studying. The results are obtained from Cadence Encounter after P&R2 . We also add a bufferless design to the comparison. The largest area is 60731μ m2 for EVC (not shown in the ﬁgure) followed by 45281μ m2 , 28516μ m2 , 20930μ m2 and 6286μ m2 for Mesh, Bufferless, IMR and RL, respectively. The EXB and ejection link sharing techniques as well as the simplicity of the RL interface are the main contributors for the signiﬁcant reduction of area overhead. Overall, RL has an area saving of 89.6%, 86.1%, 77.9% and 69.9% compared with EVC, Mesh, Bufferless3 and IMR, respectively. The wiring area is not included as wires are spread throughout the metal layers and cannot be compared directly. We do acknowledge that IMR and RL use more wiring resources than other designs. RL uses a small percentage of middle metal layers for wires and, as a result, more repeaters are needed. The total area for all the link repeaters is 0.127mm2 which is 4.3% of the mesh router area. However, as middle layers are above the logic area, RL is unlikely to increase the chip size. 0 10 20 30 40 4x4 8x8 16x16 A e v r e g a H n u o c p o t Bit reverse 0 10 20 30 40 4x4 8x8 16x16 A e v r e g a H n u o c p o t Transpose 0 10 20 30 40 4x4 8x8 16x16 A e v r e g a H n u o c p o t Uni form  0 50 100 150 200 4x4 8x8 16x16 A e v r e g a H n u o c p o t Hotspot                     Figure 15: Average hop count for synthetic workloads. 8. DISCUSSION 8.1 Scalability and Regularity 2Our CAD tools limit P&R for processing cores. 3 In addition to area reduction, RL also has 2.8X higher throughput (under UR) and 64.3% lower latency than bufferless NoC. Table 3: Average overlapping and loops/rings in RL/IMR Network Overlap  cap Avg overlap(%)  of links Max loops/ rings in node Avg loops/ rings(%) in node Longest  loop/ring RL 4x4 4 3.33 (83.3%) 6 5 (62%) 12 IMR 4x4 4 2.33 (58.3%) 4 3.5 (43%) 14 RL 8x8 8 6 (75%) 14 10.5 (65%) 28 IMR 8x8 8 4.71 (58.9%) 10 8.2 (54%) 48 RL 16x16 16 11.33 (70.8%) 30 21.2 (66%) 60 IMR 16x16 16 8.13 (50.8%) 18 15.2 (47%) 240 Figures 11 and 12 already showed the advantage of RL in terms of latency and throughput for large networks. Figure 15 further compares the average hop count (zero-load hop count) of RL, IMR, and optimal Mesh. As can be seen, IMR has very high average hop count because of its lengthy rings. In contrast, the average hop count of RL is only slightly higher than optimal Mesh. Note that RL achieves this low hop count without having the switch capability of conventional routers. Routerless NoC is not as irregular as it appears in the ﬁgures. In our actual design and evaluation, all the RL interfaces use the same design (some ports are left unused if no loops are connected), so the main irregularity is the way that links form loops. One way to quantify the degree of link irregularn − 1 for n × n NoC. This degree is similar to that of Flattened ity is how many different possible lengths of links, which is Butterﬂy [25] and MECS [18]. 8.2 Average Overlapping We discussed before that as long as the overlapping cap is met, it is beneﬁcial to approach this cap for as many neighboring node pairs as possible to increase resource utilization and improve performance. Table 3 presents this statistics for RL and IMR. It can be seen that the average overlapping between adjacent nodes in RL is at least 20% more than that of IMR. Also, the longest loop in RL is always shorter than the longest ring in IMR, and the difference increases as the NoC gets bigger. Shorter loops reduce average hop count and offer a lower latency. For example, in 16 × 16 the longest loop in RL is of 60 nodes while in IMR it is of 240 nodes. 8.3 Impact on Latency distribution                                                 Figure 16: Latency distribution of benchmarks for RL 8 × 8 NoC.       The extension buffer technique and the reduced ejection link technique save buffer resources at the risk of increasing packet latency. Figure 16 shows distribution of average packet latency, averaged over different benchmarks. The RL interface is conﬁgured the same as previous sections with one EXB and two ejectors. The take away message from the ﬁgure is that the two techniques has minimal impact on latency under tight resource allocation. For example, the average packet latency is only 8.3 cycles for RL, and only 0.71% of the packets having latency larger than 20 cycles, with the 502                   	 	   	        	 largest being 39 cycles. The tail in the latency distribution is thin and short. 8.4 RL for n x m Chip The RL design can be easily extended to any n × m network sizes. The RL interface design and functionalities remain unchanged. The RLrec algorithm needs to be modiﬁed slightly. With rectangular shapes instead of squares, NL and NH are not sufﬁcient to denote the four corners of a layer. Two more variables are needed to specify the corners of a layer correctly. For Instance, NL r and NH r for low and high rows, and NL c and NH c for low and high columns. Once a layer is correctly speciﬁed, the four groups of loops can be generated in similar fashion. The rotation step is skipped as this is not possible for rectangular networks, but the reversing direction step remains. The overlapping calculation needs to reﬂect the orientation of the rectangular loops as well. 9. CONCLUSION Current and future many-core processors demand highly efﬁcient on-chip networks to connect hundreds or even thousands of processing cores. In this paper, we analyze on-chip wiring resources in detail, and propose a novel routerless NoC design to remove the costly routers in conventional NoCs while still achieving scalable performance. We also propose an efﬁcient interface hardware implementation, and evaluate the proposed scheme extensively. Simulation results show that the proposed routerless NoC design offers signiﬁcant advantage in latency, throughput, power and area, compared with other designs. These results demonstrate the viability and potential beneﬁts of the routerless approach, and also call for future works that continue to improve various aspects of routerless NoCs such as performance, reliability, and power efﬁciency. Acknowledgments We sincerely thank the anonymous reviewers for their helpful comments and suggestions. We appreciate the authors of IMR [28] for sharing the source code of generating IMR. We also thank Timothy M. Pinkston for providing valuable feedback to the work. This research was supported, in part, by the National Science Foundation (NSF) grants #1619456, #1566637, #1423656, #1619472 and #1321131. 10. "
2018,Power and Energy Characterization of an Open Source 25-Core Manycore Processor.,"The end of Dennard's scaling and the looming power wall have made power and energy primary design goals for modern processors. Further, new applications such as cloud computing and Internet of Things (IoT) continue to necessitate increased performance and energy efficiency. Manycore processors show potential in addressing some of these issues. However, there is little detailed power and energy data on manycore processors. In this work, we carefully study detailed power and energy characteristics of Piton, a 25-core modern open source academic processor, including voltage versus frequency scaling, energy per instruction (EPI), memory system energy, network-on-chip (NoC) energy, thermal characteristics, and application performance and power consumption. This is the first detailed power and energy characterization of an open source manycore design implemented in silicon. The open source nature of the processor provides increased value, enabling detailed characterization verified against simulation and the ability to correlate results with the design and register transfer level (RTL) model. Additionally, this enables other researchers to utilize this work to build new power models, devise new research directions, and perform accurate power and energy research using the open source processor. The characterization data reveals a number of interesting insights, including that operand values have a large impact on EPI, recomputing data can be more energy efficient than loading it from memory, on-chip data transmission (NoC) energy is low, and insights on energy efficient multithreaded core design. All data collected and the hardware infrastructure used is open source and available for download at http://www.openpiton.org.","2018 IEEE International Symposium on High Performance Computer Architecture Power and Energy Characterization of an Open Source 25-core Manycore Processor Michael McKeown, Alexey Lavrov, Mohammad Shahrad, Paul J. Jackson, Yaosheng Fu∗ , Jonathan Balkind, Tri M. Nguyen, Katie Lim, Yanqi Zhou† , David Wentzlaff Princeton University {mmckeown,alavrov,mshahrad,pjj,yfu,jbalkind,trin,kml4,yanqiz,wentzlaf}@princeton.edu ∗ Now at NVIDIA † Now at Baidu Abstract—The end of Dennard’s scaling and the looming power wall have made power and energy primary design goals for modern processors. Further, new applications such as cloud computing and Internet of Things (IoT) continue to necessitate increased performance and energy efﬁciency. Manycore processors show potential in addressing some of these issues. However, there is little detailed power and energy data on manycore processors. In this work, we carefully study detailed power and energy characteristics of Piton, a 25-core modern open source academic processor, including voltage versus frequency scaling, energy per instruction (EPI), memory system energy, network-on-chip (NoC) energy, thermal characteristics, and application performance and power consumption. This is the ﬁrst detailed power and energy characterization of an open source manycore design implemented in silicon. The open source nature of the processor provides increased value, enabling detailed characterization veriﬁed against simulation and the ability to correlate results with the design and register transfer level (RTL) model. Additionally, this enables other researchers to utilize this work to build new power models, devise new research directions, and perform accurate power and energy research using the open source processor. The characterization data reveals a number of interesting insights, including that operand values have a large impact on EPI, recomputing data can be more energy efﬁcient than loading it from memory, on-chip data transmission (NoC) energy is low, and insights on energy efﬁcient multithreaded core design. All data collected and the hardware infrastructure used is open source and available for download at http://www.openpiton.org. Keywords-processor; manycore; power; energy; thermal; characterization; I . IN TRODUC T ION Power and energy have become increasingly important metrics in designing modern processors. The power savings resulting from newer process technologies have diminished due to increased leakage and the end of Dennard’s scaling [1]. Transistor power dissipation no longer scales with channel length, leading to higher energy densities in modern chips. Thus, economically cooling processors has become a challenge. This has led researchers to a number of possible solutions, including Dark Silicon [2]–[4]. Due to increased power density, design decisions have become increasingly motivated by power as opposed to performance. Energy efﬁciency has also become a major research focus [5]–[11]. Moreover, emerging applications continue to demand more energy efﬁcient compute. Cloud computing and data centers, where power is a ﬁrst class citizen with direct impact on total cost of ownership (TCO) [14], [15], are growing 2378-203X/18/$31.00 ©2018 IEEE DOI 10.1109/HPCA.2018.00070 762 CB Chip Bridge (CB) PLL Tile 0 Tile 1 Tile 2 Tile 3 Tile 4 Tile 5 Tile 6 Tile 7 Tile 8 Tile 9 Tile 10 Tile 11 Tile 12 Tile 13 Tile 14 Tile 15 Tile 16 Tile 17 Tile 18 Tile 19 Tile 20 Tile 21 Tile 22 Tile 23 Tile 24 (a) (b) Figure 1. Piton die, wirebonds, and package without epoxy encapsulation (a) and annotated CAD tool layout screenshot (b). Figure credit: [12], [13]. more pervasive. On the other end of the spectrum, mobile and Internet of Things (IoT) devices demand higher performance in a relatively constant power and energy budget. Manycore processors have shown promise in curbing energy efﬁciency issues. Single-threaded performance scaling has come to a halt with the end of clock frequency scaling largely due to the power wall. Manycore processors provide an alternative, enabling more efﬁcient computation for parallelizeable applications through the use of many simple cores. Intel’s Xeon Phi processor [16], Cavium’s ThunderX processor [17], Phytium Technology’s Mars processor [18], Qualcomm’s Centriq 2400 processor [19], and the SW26010 processor in the Sunway TaihuLight supercomputer [20] are examples of manycore processors that are in use today and are beginning to attract attention for these reasons. Unfortunately, detailed power characterization data on manycore processors is scarce. Industry provides high-level power data, but not enough detail to drive research directions or power models. Several academics have characterized industry chips [21]–[26], however the lack of access to proprietary design details and register transfer level (RTL) models makes detailed characterization difﬁcult. Additionally, academics are unable to correlate results with RTL models to ensure that designs behave as expected during measurement. Thus, veriﬁcation of the results is impossible. The test setup used in this work was designed speciﬁcally for power characterization, which has limited other studies [24], [26]. Detailed silicon power data is necessary to build intuition, drive innovation, and build accurate power models. Chip Bridge Tile0 Tile1 Tile2 Tile3 Tile4 Tile5 Tile6 Tile7 Tile8 Tile9 Tile10 Tile11 Tile12 Tile13 Tile14 Tile15 Tile16 Tile17 Tile18 Tile19 Tile20 Tile21 Tile22 Tile23 Tile24 L2 Cache Slice + Directory Cache NoC Routers (3) Modified  OpenSPARC T1  Core MITTS (Traffic Shaper) L1.5 Cache CCX Arbiter FPU (a) (b) Figure 2. Piton chip-level architecture diagram (a) and tile-level diagram (b). Figure credit: [12], [13]. In this paper, we perform detailed power characterization of Piton [12], [13]. Piton is a 25-core manycore research processor, shown in Figure 1. It utilizes a tile-based design with a 5x5 2D mesh topology interconnected with three 64bit networks-on-chip (NoCs). Cache coherence is maintained at the shared, distributed L2 cache and the NoCs along with the coherence protocol extend off-chip to support inter-chip shared memory in multi-socket systems. Piton leverages the multithreaded OpenSPARC T1 core [27]. Piton was tapedout on IBM’s 32nm silicon-on-insulator (SOI) process, and ﬁts in a 36mm2 die with over 460 million transistors. The characterization reveals a number of insights. We show energy per instruction (EPI) is highly dependent on operand value and that recomputing data can be more energy efﬁcient than loading it from memory. Our NoC energy results contradict a popular belief that NoCs are a dominant fraction of a manycore’s power [28]–[32]. This matches results from other real system characterizations [33]–[35] and motivates a reassessment of existing NoC power models [29], [36]. Additionally, our study of the energy efﬁciency of multithreading versus multicore provides some design guidelines for multithreaded core design. Piton was open sourced as OpenPiton [37], including the RTL, simulation infrastructure, validation suite, FPGA synthesis scripts, and ASIC synthesis and back-end scripts. Thus, all design details and the RTL model are publicly available, enabling meaningful, detailed power characterization and correlation with the design. Most projects taped-out in silicon do not publicly release the source [38], [39], and those that do [27], [40], [41] have not published detailed silicon data for the architecture. The power characterization of an open source manycore processor provides several beneﬁts, including insights for future research directions in improving energy efﬁciency and power density in manycore processors. It also enables researchers to build detailed and accurate power models for an openly accessible design. This is particularly useful when researchers use the open design in their research, as the results directly apply. All data collected and all hardware infrastructure is open source for use by the research community and is available at http://www.openpiton.org. Table I. P I TON PARAM E T ER SUMMARY Process Die Size Transistor Count Package Nominal Core Volt. (VDD) Nominal SRAM Volt. (VCS) Nominal I/O Volt. (VIO) Off-chip Interface Width Tile Count NoC Count NoC Width Cores per Tile Threads per Core Total Thread Count Core ISA Core Pipeline Depth IBM 32nm SOI 36mm2 (6mm x 6mm) > 460 mllion 208-pin QFP (Kyocera CERQUAD R(cid:3)) 1.0V 1.05V 1.8V 32-bit (each direction) 25 (5x5) 3 64-bit (each direction) 1 2 50 SPARC V9 6 stages L1 Instruction Cache Size L1 Instruction Cache Assoc. L1 Instruction Cache Line Size L1 Data Cache Size L1 Data Cache Associativity L1 Data Cache Line Size L1.5 Data Cache Size L1.5 Data Cache Associativity L1.5 Data Cache Line Size L2 Cache Slice Size L2 Cache Associativity L2 Cache Line Size L2 Cache Size per Chip Coherence Protocol Coherence Point 16KB 4-way 32B 8KB 4-way 16B 8KB 4-way 16B 64KB 4-way 64B 1.6MB Directory-based MESI L2 Cache The contributions of this work are as follows: • The ﬁrst detailed power characterization of an open source manycore processor taped-out in silicon, including characterization of voltage versus frequency scaling, EPI, memory system energy, NoC energy, thermal properties, and application performance and power. • To the best of our knowledge, the most detailed area breakdown of an open source manycore. • A number of insights derived from the characterization, such as the impact of operand values on EPI. • An open source printed circuit board (PCB) for manycore processors with power characterization features. • Open source power characterization data enabling researchers to build intuition, formulate research directions, and derive power models. I I . P I TON ARCH I T EC TUR E BACKGROUND Piton [12], [13] is a 25-core manycore academic-built research processor. It was taped-out on IBM’s 32nm SOI process on a 36mm2 , 6mm x 6mm, die with over 460 million transistors. Piton has three supply voltages: core (VDD), SRAM arrays (VCS), and I/O (VIO). The nominal supply voltages are 1.0V, 1.05V, and 1.8V, respectively. The die has 331 pads positioned around the periphery and is packaged in a wire-bonded 208-pin ceramic quad ﬂat package (QFP) with an epoxy encapsulation. The Piton die, wirebonds, and package, without the encapsulation, are shown in Figure 1a. The high-level architecture of Piton and the implemented layout are shown in Figure 2a and Figure 1b, respectively. Piton contains 25 tiles arranged in a 5x5 2D mesh topology, interconnected by three physical 64-bit (in each direction) NoCs. The tiles maintain cache coherence, utilizing a directory-based MESI protocol implemented over the three NoCs, at the distributed, shared L2 cache. The NoCs and coherence protocol extend off-chip, enabling multi-socket Piton systems with support for inter-chip shared memory. The chip bridge connects the tiles (through tile0) to offchip chipset logic implemented in FPGA and other Piton chips in multi-socket systems. When going off-chip, it multiplexes the three physical NoCs using logical channels for transmission over the pin-limited (32bit each way) channel. Each tile contains a modiﬁed OpenSPARC T1 [27] core, a 64KB slice of the shared, distributed L2 cache, three NoC routers, a memory trafﬁc shaper, an L1.5 cache, a ﬂoatingpoint unit (FPU), and a CPU-cache crossbar (CCX) arbiter as 763 Piton Test Board Piton Configuration Bits and JTAG  Interface I2C Voltage/Current  Monitors s e i l p p u S r e w o P h c n e B s r o t i c a p a C s s a p y B d n a e r o C M A R S O / I Piton Socket Gateway FPGA Chipset FPGA  Board FMC Interface FMC Interface DRAM DRAM Ctl Chip Bridge Demux Serial Port SD Card I/O Ctls North Bridge South Bridge C h p i s e t F P G A (a) (b) Figure 3. Piton experimental system block diagram (a) and photo (b). Figure (a) credit: [12], [13]. 12V VIO Gateway FPGA + VIO Chip BF Rsense 2.5V Figure 4. Thermal image of custom Piton test board, chipset FPGA board, and cooling solution running an application. shown in Figure 2b. The Piton core is a single-issue, 6-stage, in-order SPARC V9 core. It contains two-way ﬁne-grained multithreading and implements Execution Drafting [5] for energy efﬁciency when executing similar code on the two threads. The use of a standard instruction set architecture (ISA) enables the leveraging of existing software (compilers, operating systems, etc.). The L2 cache slice contains an integrated directory cache for the MESI cache coherence protocol and implements Coherence Domain Restriction (CDR) [42], enabling shared memory among arbitrary cores in large core-count multi-socket systems. The L2 cache in aggregate provides 1.6MB of cache per chip. The three NoC routers implement dimension-ordered, wormhole routing for the three physical networks with a one-cycle-per-hop latency and an additional cycle for turns. The L1.5 cache is an 8KB write-back private data cache that encapsulates the core’s write-through L1 data cache to reduce the bandwidth requirement to the distributed L2 cache. It also transduces between the core interface (CCX) and the Piton NoC protocol. Last, a memory trafﬁc shaper, known as the Memory Inter-arrival Time Trafﬁc Shaper (MITTS) [43], ﬁts memory trafﬁc from the core into a particular inter-arrival time distribution to facilitate memory bandwidth sharing in multi-tenant systems. The architectural parameters for Piton are summarized in Table I. I I I . EX P ER IM EN TA L S E TU P Figure 3 shows a block diagram and photo of the experimental setup used in this work. The setup consists of three major components: a custom Piton test printed circuit 7 r e y a L 8 r e y a L 3 1 r e yy y a L 5V 3.3V VDD BF Rsense VCS AF Rsense 2.5V 2.7V VIO Config 1.2V BF Rsense VIO Chip AF Rsense 1.2V AF Rsense 12V 3.5V Figure 5. Split power plane layers of the Piton test board. Planes delivering power to Piton are labeled in orange. VCS BF Rsense VDD AF Rsense board (PCB), a chipset FPGA board, and a cooling solution. Figure 4 shows a thermal image of all three components. A. Piton Test PCB A custom PCB was designed to test Piton. The board design is derived from the BaseJump Double Trouble board [44]. The Piton test board contains a 208-pin QFP socket to hold the Piton chip. The Piton chip bridge interface signals connect to a Xilinx Spartan-6 FPGA (XC6SLX1503FGG676C), the gateway FPGA. The gateway FPGA acts as a simple passthrough for the chip bridge interface, converting single-ended signals to and from Piton into differential signals for transmission over a FPGA Mezzanine Connector (FMC) to the chipset FPGA board. The gateway FPGA drives the Piton PLL reference clock, I/O clock, and resets. The PCB was carefully designed with power characterization in mind. The three Piton supply voltages, VDD, VCS, and VIO, can be driven either by on-board power supplies or by bench power supplies. While the on-board power supply output voltages are variable, bench power supplies provide more ﬁne-grained control over voltages and a wider voltage range. Each bench power supply is equipped with remote voltage sense, compensating for voltage drop across the cables and board. Only the VDD power supply integrated 764                                               Thermal Paste Chip Socket PCB Figure 6. Piton heat sink cross-section. Heat Sink Aluminum  Spacers Chip Fasteners (a) (b) Figure 7. Piton heat sink photo (a) and thermal image (b). on the Piton board has remote voltage sense (when powered from bench power supplies, remote sense is used). For these reasons, the bench power supplies were used for all studies. Three layers of the 14-layer PCB are dedicated to split power planes, shown in Figure 5. The planes delivering power to Piton are labeled in orange. Sense resistors bridge split planes delivering current to the three chip power rails. Care was taken to ensure that only the current delivered to Piton is measured by the sense resistors. Voltage monitors controlled via I2C track the voltage at the chip socket pins and on either side of sense resistors. All reported measurements are taken from the on-board voltage monitors. The monitors are polled at approximately 17Hz, a limitation of the monitor devices and the processing speed of the host device. Unless otherwise speciﬁed, all experiments in this work record 128 voltage and current samples (about a 7.5 second time window) after the system reaches a steady state. We report the average power calculated from the 128 samples. Unless otherwise speciﬁed, error is reported as the standard deviation of the samples from the average. Note, the recorded voltages are measured at the socket pins and do not account for current induced voltage drop (IR drop) across the socket, wirebonds, or die. The board design is open source for other researchers to leverage when building systems. It can serve as a good reference or starting point. The board ﬁles are available at http://www.openpiton.org. B. Chipset FPGA Board The chipset FPGA board, a Digilent Genesys2 board with a Xilinx Kintex-7 FPGA, connects to the Piton test board via a FMC connector. The FPGA logic implements a chip bridge demultiplexer which converts the 32-bit logical channel interface back into three 64-bit physical networks, a north bridge, and south bridge. The north bridge connects 765 Table II. EX P ER IM EN TA L SY S T EM FR EQU ENC I E S Gateway FPGA ↔ Piton 180 MHz Gateway FPGA ↔ FMC ↔ Chipset FPGA 180 MHz Chipset FPGA Logic 280 MHz DRAM DDR3 PHY 800 MHz (1600 MT/s) DDR3 DRAM Controller 200 MHz SD Card SPI 20 MHz UART Serial Port 115,200 bps Table III. D E FAU LT P I TON M EA SUR EM EN T PARAM E T ER S Core Voltage (VDD) 1.00V SRAM Voltage (VCS) 1.05V I/O Voltage (VIO) 1.80V Core Clock Frequency 500.05MHz to a DDR3 DRAM controller implemented in the FPGA, while the south bridge connects to I/O controllers for various devices, including an SD card, network card, and a serial port. The chipset FPGA board also includes 1GB of DDR3 DRAM with a 32-bit data interface and the I/O devices. C. Cooling Solution The Piton test system uses a cooling solution consisting of a heat sink and fan. Figure 6 shows a cross-sectional diagram of the heat sink setup and Figure 7 shows a photo and thermal image. A stock heat sink is used with aluminum spacers to ﬁll the gap between the top of the chip and the top of the socket walls. Thermal paste is used at each thermal interface between the top of the chip and the heat sink. A PC case fan with airﬂow volume of 44cfm is used to circulate hot air out of the system and introduce cool air. The fan direction is parallel to the heat sink ﬁns (out of the plane in Figure 6). Note, the thermal capacity of this cooling solution was purposely over-engineered and is likely excessive. D. System Summary This system allows us to boot full-stack Debian Linux from an image on the SD card (also hosts the ﬁle system) or load assembly tests over the serial port into DRAM. The system is used as described for all studies in this work, unless otherwise speciﬁed. The frequencies at which interfaces operate are listed in Table II. IV. R E SU LT S This section presents the results from careful characterization of Piton. Unless otherwise stated, all results are taken at the Piton supply voltages and core clock frequency listed in Table III at room temperature. All error bounds are reported as the standard deviation of samples from the average, unless otherwise speciﬁed. Note, all data presented in this section is open source and is available at http://www.openpiton.org. A. Chip Testing Out of the 118 Piton die received from a multi-project wafer manufacturing run of two wafers, 45 have been packaged. A random selection of 32 chips have been tested so far and Table IV shows the testing statistics. 59.4% of the chips are stable and fully functional. 25% of the chips are functional but encounter failures during testing. These chips likely have SRAM errors. Piton has the ability to remap rows and columns in SRAMs to repair such errors, but a Table IV. P I TON T E S T ING S TAT I S T IC S Status Symptom Good Stable operation Unstable* Consistently fails deterministically Bad High VCS current draw Bad High VDD current draw Unstable* Consistenly fails nondeterministically * Possibly ﬁxable with SRAM repair Possible Cause N/A Bad SRAM cells Short Short Unstable SRAM cells Chip Count 19 7 4 1 1 Chip Percentage 59.4 21.9 12.5 3.1 3.1 Chip Area: 35.97552 mm2 Tile Area: 1.17459 mm2 Core Area: .55205 mm2 Clock Circuitry 0.26% Chip Bridge 0.12% ORAM 2.73% Timing Opt  Buffers I/O Cells 3.75% 0.07% Unutilized 2.12% NoC3 Router 0.95% NoC2 Router 0.95% NoC1 Router 0.98% L1.5 Cache 7.62% FPU 2.64% MITTS 0.17% FP Front-End 1.85% JTAG Multiply 0.10% 1.53% Config Regs Trap Logic 0.05% 6.42% Config Regs CCX Buffers 0.11% Clock Tree 0.06% 0.13% Timing Opt  Buffers 3.83% Filler  9.32% L2 Cache 22.16% Tile0 3.27% Core 47.00% Tile 1-24 78.37% Integer RF 16.81% Filler 26.13% Execute 2.38% Load/Store 22.33% Filler 16.32% Unutilized 0.73% Clock Tree 0.01% Timing Opt Buffers 0.34% Fetch 17.52% Unutilized 0.90% Figure 8. Detailed area breakdown of Piton at chip, tile, and core levels. repair ﬂow is still in development. 15.6% of the chips have abnormally high current draw on VCS or VDD, indicating a possible short circuit. These results provide understanding on what yield looks like in an academic setting, but the number of wafers and tested die are small thus it is difﬁcult to draw strong conclusions. During our characterization, only fully-working, stable chips are used. B. Area Breakdown Figure 8 shows a detailed area breakdown of Piton at the chip, tile, and core levels. These results were calculated directly from the place and route tool. The area of standard cells and SRAM macros corresponding to major blocks in the design were summed together to determine the area. Filler cells, clock tree buffers, and timing optimization buffers are categorized separately, as it is difﬁcult to correlate exactly which design block they belong to. The total area of the core, tile, and chip are calculated based on the ﬂoorplanned dimensions and unutilized area is reported as the difference of the ﬂoorplanned area and the sum of the area of cells within them. The area data is provided to give context to the power and energy characterization to follow. The characterization is of course predicated on the design and thus does not represent all designs. Thus, it is useful to note the relative size of blocks in order to provide context within the greater chip design space. For example, stating the NoC energy is small is only useful given the relative size of the NoC routers and the tile (indicative of how far NoCs need to route). Designs with larger NoC routers or larger tiles (longer routing distance) may not have identical characteristics. This is discussed more in Section IV-K. C. Voltage Versus Frequency Figure 9 shows the maximum frequency that Debian Linux successfully boots on Piton at different VDD voltages for three different Piton chips. For all values of VDD, 766 Figure 9. Maximum frequency at which Linux successfully boots at different voltages for three different Piton chips. V C S = V DD + 0.05V for all VDD values. Error bars indicate noise due to quantization. V C S = V DD + 0.05V . Since the PLL reference clock frequency that the gateway FPGA drives into the chip is discretized, the resulting core clock frequency has quantization error. This is represented by the error bars in the ﬁgure indicating the next discrete frequency step that the chip was tested at and failed. However, the chip may be functional at frequencies between the plotted value and the next step. The maximum frequency at the high-end of the voltage spectrum is thermally limited. This is evident from the decrease in maximum frequency at higher voltages for Chip #1. Chip #1 consumes more power than other chips and therefore runs hotter. At lower voltages it actually has the highest maximum frequency of the three chips as the heat generated is easily dissipated by the cooling solution. However, after 1.0V it starts to drop below the other chips until 1.2V where the maximum frequency severely drops. This is because the chip approaches the maximum amount of heat the cooling solution can dissipate, thus requiring a decrease in frequency to reduce the power and temperature. We believe the thermal limitation is largely due to packaging restrictions, including the die being packaged cavity up, the insulating properties of the epoxy encapsulation, and the chip being housed in a socket. This results in a higher die temperature than expected, reducing the speed at which the chip can operate at greater voltages and frequencies. This is not an issue of the cooling solution thermal capacity, but an issue of the rate of thermal transfer between the die and the heat sink. Results for the default measurement parameters in Table III are measured at non-thermally limited points. Another limitation of Piton’s current packaging solution is the wire bonding. As previously stated, the voltages presented in this paper are measured at the socket pins. However, IR drop across the socket, pins, wirebonds, and die results in a decreased voltage at the pads and across the die. This reduces the speed at which transistors can switch. A ﬂip-chip packaging solution would have allowed us to supply power to the center of the die, reducing IR drop issues, and for the die to be positioned cavity down, enabling more ideal heat sinking from the back of the die. However, ﬂip-chip packaging is expensive and increases design risk. Figure 10. Static and idle power averaged across three different Piton chips for different voltages and frequency pairs. Separates contributions due to VDD and VCS supplies. Table V. D E FAU LT POW ER PARAM E T ER S (CH I P #2 ) Static Power @ Room Temperature Idle Power @ 500.05MHz 389.3±1.5 mW 2015.3±1.5 mW D. Static and Idle Power Figure 10 shows static and idle power at different voltages averaged across three Piton chips. The frequency was chosen as the minimum of the maximum frequencies for the three chips at the given voltage. The contribution from VDD and VCS supplies is indicated in the ﬁgure. Again, for all values of VDD, V C S = V DD + 0.05V . The static power was measured with all inputs, including clocks, grounded. The idle power was measured with all inputs grounded, but driving clocks and releasing resets. Thus, the idle power represents mostly clock tree power, with the exception of a small number of state machines and counters that run even when the chip is idle. The power follows an exponential relationship with voltage and frequency. Chip #2 as labeled in Figure 9 will be used throughout the remainder of the results, unless otherwise stated. The static and idle power for Chip #2 at the default measurement parameters in Table III are listed in Table V. E. Energy Per Instruction (EPI) In this section, we measure the energy per instruction (EPI) of different classes of instructions. This was accomplished by creating an assembly test for each instruction with the target instruction in an inﬁnite loop unrolled by a factor of 20. We veriﬁed through simulation that the assembly test ﬁts in the L1 caches of each core and no extraneous activity occurred, such as off-chip memory requests. The assembly test was run on all 25 cores until a steady state average power was achieved, at which point it was recorded as Pinst , summing the contributions from VDD and VCS. We measure the power while executing the test on 25 cores in order to average out inter-tile power variation. The latency in clock cycles, L, of the instruction was measured through simulation, ensuring pipeline stalls and instruction scheduling was as expected. In order to calculate EPI, we subtract the idle power presented in Table V, Pidle , Figure 11. EPI for different classes of instructions with minimum, random, and maximum operand values for instructions with input operands. Table VI. IN S TRUC T ION LAT ENC I E S U S ED IN EP I CA LCU LAT ION S nop and add mulx sdivx Instruction Latency (cycles) Integer (64-bit) 1 1 1 11 72 FP Double Precision 22 25 79 faddd fmuld fdivd Instruction Latency (cycles) FP Single Precision fadds fmuls fdivs 22 25 50 Memory (64-bit) L1/L1.5 Hit 3 10 10 ldx stx stb full stx stb space Control beq taken bne nottaken 3 3 from Pinst to get the power contributed by the instruction execution. Dividing this by the clock frequency results in the average energy per clock cycle for that instruction. The average energy per cycle is multiplied by L to get the total energy to execute the instruction on 25 cores. Dividing this value by 25 gives us the EPI. The full EPI equation is: 1 25 × Pinst − Pidle F requency EP I = × L This EPI calculation is similar to that used in [24]. The store extended (64-bit) instruction, stx, requires special attention. Store instructions write into the eight entry store buffer in the core, allowing other instructions to bypass them while the updated value is written back to the cache. The latency of stores is 10 cycles. Thus, an unrolled inﬁnite loop of stx instructions quickly ﬁlls the store buffer. The core speculatively issues stores assuming that the store buffer is not full. Since the core is multithreaded, it rolls-back and re-executes the store instruction and any subsequent instructions from that thread when it ﬁnds the store buffer is full. This roll-back mechanism consumes extra energy and pollutes our measurement result. To accurately measure the stx EPI, we inserted nine nop instructions following each store in the unrolled loop. The nop instructions cover enough latency that the store buffer always has space. We subtract the energy of nine nop instructions from the calculated energy for the stx assembly test, resulting in the EPI for a single stx instruction. We present results using this method of ensuring the store buffer is not full (stx (NF)) and for the case the store buffer is full and a roll-back is incurred (stx (F)). This special case 767 Table VII. M EMORY SY S T EM EN ERGY FOR D I FF ER EN T CACH E H I T /M I S S SC ENAR IO S Latency (cycles) Mean LDX Energy (nJ) 0.28646±0.00089 3 1.54±0.25 34 1.87±0.32 42 1.97±0.39 52 308.7±3.3 424 Cache Hit/Miss Scenario L1 Hit L1 Miss, Local L2 Hit L1 Miss, Remote L2 Hit (4 hops) L1 Miss, Remote L2 Hit (8 hops) L1 Miss, Local L2 Miss highlights the importance of having access to the detailed microarchitecture and RTL when doing characterization. Figure 11 shows the EPI results and Table VI lists latencies for each instruction characterized. We use 64-bit integer and memory instructions. In this study, ldx instructions all hit in the L1 cache and stx instructions all hit in the L1.5 cache (the L1 cache is write-through). Each of the 25 cores store to different L2 cache lines in the stx assembly test to avoid invoking cache coherence. The longest latency instructions consume the most energy. The instruction source operand values affect the energy consumption. Thus, we plot data for minimum, maximum, and random operand values. This shows that the input operand values have a signiﬁcant effect on the EPI. This data can be useful in improving the accuracy of power models. Another useful insight for low power compiler developers and memoization researchers is the relationship between computation and memory accesses. For example, three add instructions can be executed with the same amount of energy and latency as a ldx that hits in the L1 cache. Thus, it can be more efﬁcient to recompute a result than load it from the cache if it can be done using less than three add instructions. F. Memory System Energy Table VII shows the EPI for ldx instructions that access different levels of the cache hierarchy. The assembly tests used in this study are similar to those in Section IV-E. They consist of an unrolled inﬁnite loop (unroll factor of 20) of ldx instructions, however consecutive loads access different addresses that alias to the same cache set in the L1 or L2 caches, depending on the hit/miss scenario. We control which L2 is accessed (local versus remote) by carefully selecting data addresses and modifying the line to L2 slice mapping, which is conﬁgurable to the low, middle, or high order address bits through software. Latencies are veriﬁed through simulation for L1 and L2 hit scenarios. The L2 miss latency was proﬁled using performance counters since real memory access times are not reﬂected in simulation. We use an average latency for L2 miss, since memory access latency varies. Similar to the caveats for the stx instruction discussed in Section IV-E, the core thread scheduler speculates that ldx instructions hit in the L1 cache. In the case the load misses, the core will roll-back subsequent instructions and stall the thread until the load returns from the memory system. This roll-back mechanism does not pollute the energy measurements for ldx instructions, as a roll-back will always occur for a load that misses in the L1 cache. The energy to access a local L2 is noticeably larger than an L1 hit. This mostly comes from L2 access energy, however the request will ﬁrst go to the L1.5 cache. The L1.5 cache is basically a replica of the L1 cache, but is write-back. Thus, a miss in the L1 will also miss in the L1.5. However, it is important to note that the energy to access the L1.5 is included in all results in Table VII except for L1 hit. The difference between accessing a local L2 and remote L2 is relatively small, highlighting the minimal impact NoCs have on power consumption. We study this in more detail in Section IV-G. The energy for an L2 cache miss is dramatically larger than an L2 hit. This is a result of the additional latency to access memory causing the chip to stall and consume energy until the memory request returns. Note that this result does not include DRAM memory energy. Comparing the results in Table VII to the energy required for instructions that perform computation in Figure 11, many computation instructions can be executed in the same energy budget required to load data from the L2 cache or off-chip memory. Similar to loading data from the L1 cache, it may be worthwhile to recompute data rather than load it from the L2 cache or main memory. G. NoC Energy NoC energy is measured by modifying the chipset logic to continually send dummy packets into Piton destined for different cores depending on the hop count. We use an invalidation packet type that is received by the L1.5 cache. The dummy packet consists of a routing header ﬂit followed by 6 payload ﬂits. The payload ﬂits reﬂect different bit switching patterns to study how the NoC activity factor affects energy. The center-to-center distance between tiles is 1.14452 mm in the X direction and 1.053 mm in the Y direction, indicative of the routing distance. For a baseline, we measure the steady state average power when sending to tile0, Pbase . We then measure the steady state average power when sending to different tiles based on the desired hop count, Phop . For example, sending to tile1 represents one hop, tile2 represents two hops, and tile9 represents ﬁve hops. The measurement is taken for one to eight hops, the maximum hop count for a 5x5 mesh. Due to the bandwidth mismatch between the chip bridge and NoCs, there are idle cycles between valid ﬂits. However, the NoC trafﬁc exhibits a repeating pattern which allows us to calculate the energy per ﬂit (EPF). Through simulation, we veriﬁed that for every 47 cycles there are seven valid NoC ﬂits. In order to calculate the EPF, we ﬁrst calculate the average energy per cycle over the zero hop case by subtracting Pbase from Phop and dividing by the clock frequency. To account for idle cycles we multiply the average energy per cycle by 47 cycles to achieve the energy consumed for one repeating trafﬁc pattern. Dividing by 7 valid ﬂits results 768 in Figure 12 to the EPI data in Figure 11, sending a ﬂit across the entire chip (8 hops) consumes a relatively small amount of energy, around the same as an add instruction. Other instructions consume substantially more energy. This shows that computation dominates the chip’s power consumption, not on-chip data movement. We can also calculate the NoC energy from the memory system energy results in Table VII by taking the difference of a local L2 cache hit and a remote L2 cache hit. A remote L2 cache hit results in a three ﬂit request sent from the L1.5 cache to L2 cache and a three ﬂit response. The memory system energy results indicate this consumes 330 pJ for four hops and 430 pJ for eight hops. This result is consistent with the EPF data for HSW (the memory system energy results are based on random data), 268 pJ for four hops and 536 pJ for eight hops. The difference can be explained by different NoC activity factors and measurement error. H. Microbenchmark Studies We developed a few microbenchmarks in order to study power scaling with core count and multithreading versus multicore power and energy efﬁciency. These microbenchmarks include Integer (Int), High Power (HP), and Histogram (Hist). Int consists of a tight loop of integer instructions which maximize switching activity. HP contains two distinct sets of threads. One set of threads performs integer computation in a tight loop, while the other set executes a loop consisting of a mix of loads, stores, and integer instructions with a ratio of 5:1 computation to memory. This application consumes about 3.5W when run on all 50 threads with each core executing one of each of the two different types of threads. Note, HP exhibits the highest power we have observed on Piton. Hist is a parallel shared memory histogram computation implementation. Each thread computes a histogram over part of a shared array. Threads contend for a lock before updating the shared histogram buckets. We wrap the application in an inﬁnite loop so it runs long enough to achieve a steady state power for measurement. Thus, each thread continually recomputes the same portion of the histogram. Note, Hist differs from Int and HP in the way it scales with thread count. While HP and Int maintain work per thread, increasing the total amount of work as thread count scales, Hist keeps the total work constant (input array size) and decreases the work per thread (per thread portion of array). Chip #3 as labeled in Figure 9 is used for all microbenchmark studies. Static power for this chip at room temperature is 364.8±1.9 mW and idle power is 1906.2±2.0 mW for the default measurement parameters listed in Table III. 1) Core Count Power Scaling: In this section we study how power scales with core count. Each of the three microbenchmarks are executed on one to 25 cores with both one thread per core (1 T/C) and two threads per core (2 T/C) conﬁgurations. HP has two distinct types of threads, Figure 12. NoC energy per ﬂit for different hop counts. Flit size is 64-bits. Results for different ﬂit bit switching patterns are presented: no switching (NSW), half switching (HSW), and full switching (FSW and FSWA). in the average EPF. The ﬁnal EPF equation is: 47 7 × Phop − Pbase EP F = F requency The EPF results for hop counts of zero to eight are shown in Figure 12 for different consecutive ﬂit switching patterns. No switching (NSW) refers to all payload ﬂit bits set to zero. Half switching (HSW) ﬂips half of the bits between consecutive payload ﬂits, i.e. consecutive ﬂits alternate between 0x3333...3333 and all zeros. Full switching (FSW) ﬂips all bits on consecutive ﬂits, i.e. consecutive ﬂits alternate between all ones and all zeros. Full switching alternate (FSWA) refers to alternating ﬂits of 0xAAAA...AAAA and 0x5555...5555, which represents the effect of coupling and aggressor bits over FSW. The standard deviation of measurement samples from the average for this experiment is relatively large, but there is a general trend from which we can draw conclusions. The energy to send a single ﬂit scales linearly with the number of hops, with approximately 11.16 pJ/hop for the HSW case, as each additional hop is additive and all links are identical. Note that these results are for a single ﬂit sent over one physical network in one direction. It is interesting to note the effect of different NoC switching patterns. The NoC routers consume a relatively small amount of energy (NSW case) in comparison to charging and discharging the NoC data lines. Comparing FSW and HSW, the energy scales roughly linearly with the NoC activity factor. The FSWA case consumes slightly more energy, but is within the measurement error. Thus, it is difﬁcult to draw any conclusions on the affect of coupling and aggressor bits. While data transmission consumes more energy than the NoC router computation, our data contradicts a popular belief that on-chip data transmission is becoming a dominant portion of a manycore’s power consumption [28]–[32]. Note that our results match those from other real system characterizations [33]–[35] and motivate a reassessment of current NoC power models [29], [36]. Comparing the NoC EPF data 769 Figure 13. Power scaling with core count for the three microbenchmarks with 1 T/C and 2 T/C conﬁgurations. so thread mapping must be taken into account. For 1 T/C, the two types of threads are executed on alternating cores. For 2 T/C, each core executes one thread from each of the two different types of threads. Figure 13 shows how the full chip power consumption scales from one to 25 cores for each application and T/C conﬁguration. Power scales linearly with core count. Additionally, 2 T/C scales faster than 1 T/C since each core consumes more power. Comparing across applications, Hist consumes the lowest power for both T/C conﬁgurations. This is because each thread performs both compute, memory, and control ﬂow, causing the core to stall more frequently, and because of contention for the shared lock. This is in contrast to Int, where each thread always has work to do, and HP, where at least one of the threads always has work. HP (High Power) consumes the most power for both T/C conﬁgurations since it exercises both the memory system and core logic due to the two distinct types of threads. This is especially true in the 2 T/C conﬁguration since the core will always have work to do from the integer loop thread, even when the mixed thread (executes integer and memory instructions) is waiting on memory. Hist has a unique trend where power begins to drop with increasing core counts beyond 17 cores for the 2 T/C conﬁguration. This is a result of how Hist scales with thread counts, decreasing the work per thread. With large thread counts, the work per thread becomes small and the ratio of thread overhead to useful computation becomes larger. Additionally, threads spend less time computing their portion of the histogram and more time contending for locks to update buckets, which consumes less power due to spin waiting. This is exacerbated by increased thread counts since there is more contention for locks. 2) Multithreading Versus Multicore: To compare the power and energy of multithreading and multicore, we compare running the microbenchmarks with equal thread counts for both 1 T/C and 2 T/C conﬁgurations. For example, for a thread count of 16, each microbenchmark is run on 16 cores with 1 T/C versus 8 cores with 2 T/C. 2 T/C represents Figure 14. Power and energy of multithreading versus multicore. Each microbenchmark is run with equal thread counts for both 1 T/C (multicore) and 2 T/C (multithreading) conﬁgurations. multithreading and 1 T/C represents multicore. The same thread mappings are used for HP as in Section IV-H1. Energy is derived from the power and execution time. The microbenchmarks are modiﬁed for a ﬁxed number of iterations instead of inﬁnite, as was the case for power measurement, to measure execution time. The power and energy results for thread counts of two to 24 threads are shown in Figure 14. Note, we break power and energy down into active and idle portions. Idle power does not represent the full chip idle power, but the idle power for the number of active cores. This is calculated by dividing the full chip idle power by the total number of cores and multiplying by the number of active cores. Effectively, multicore is charged double the idle power of multithreading. Interestingly, for Int and HP multithreading consumes more energy and less power than multicore. For Int, each core, independent of the T/C conﬁguration, executes an integer computation instruction on each cycle. However, there are half the number of active cores for multithreading. Comparing active power, multithreading consumes similar power to multicore, indicating the hardware thread switching overheads are comparable to the active power of an extra core. This indicates, that a two-way ﬁne-grained multithreaded core may not be the optimal conﬁguration from an energy efﬁciency perspective. Increasing the number of threads per core amortizes the thread switching overheads over more threads and the active power will likely be less than that for the corresponding extra cores for multicore. While switching overheads will increase with increased threads per core, we think the per thread switching overhead may decrease beyond two threads. However, multithreading is charged idle power for half the cores compared to multicore. Thus, the total power for multicore is much higher than multithreading, but not double since the active power is similar. Translating this into energy, since the multithreading/multicore execution time ratio for Int is two, as no instruction overlapping occurs for multithreading, the total energy is higher for 770 Table VIII. SUN F IR E T2000 AND P I TON SY S T EM S P EC I FICAT ION S System Parameter Operating System Kernel Version Memory Device Type Rated Memory Clock Frequency Actual Memory Clock Frequency Rated Memory Timings (cycles) Rated Memory Timings (ns) Actual Memory Timings (cycles) Actual Memory Timings (ns) Memory Data Width Memory Size Memory Access Latency (Average) Persistent Storage Type Processor Processor Frequency Processor Cores Processor Thread Per Core Processor L2 Cache Size Processor L2 Cache Access Latency Sun Fire T2000 Debian Sid Linux 4.8 DDR2-533 266.67MHz (533MT/s) 266.67MHz (533MT/s) 4-4-4 15-15-15 4-4-4 15-15-15 64bits + 8bits ECC 16GB 108ns HDD UltraSPARC T1 1Ghz 8 4 3MB 20-24ns Piton System Debian Sid Linux 4.9 DDR3-1866 933MHz (1866MT/s) 800MHz (1600MT/s) 13-13-13 13.91-13.91-13.91 12-12-12 15-15-15 32bits 1GB 848ns SD Card Piton 500.05MHz 25 2 1.6MB aggregate 68-108ns Table IX. SPEC IN T 2006 P ER FORMANC E , POW ER , AND EN ERGY Benchmark/Input bzip2-chicken bzip2-source gcc-166 gcc-200 gobmk-13x13 h264ref-foreman-baseline hmmer-nph3 libquantum omnetpp perlbench-checkspam perlbench-diffmail sjeng xalancbmk Execution Time (mins) UltraSPARC T1 Piton 11.74 57.36 23.62 129.02 5.72 38.28 9.21 70.67 16.67 77.51 22.76 71.08 48.38 164.94 201.61 1175.70 72.94 727.04 11.57 92.56 23.13 184.37 122.07 569.22 102.99 730.03 Piton Slowdown 4.89 5.46 6.70 7.67 4.65 3.12 3.41 5.83 9.97 8.00 7.97 4.66 7.09 Piton Avg. Power (W) 2.199 2.119 2.094 2.156 2.127 2.149 2.400 2.287 2.096 2.137 2.141 2.080 2.148 Piton Energy (kJ) 7.566 16.404 4.809 9.139 9.889 9.162 23.750 161.363 91.431 11.863 22.320 71.043 94.077 multithreading. Note, we charge multicore for the idle power of a multithreaded core. While this is not quite accurate, the idle power for a single-threaded core will be lower, only making multicore power and energy even lower. The results for HP have similar characteristics to Int. HP exercises the memory system in addition to performing integer computation, thus the overall power is higher. Since the mixed thread (executes integer and memory instructions) presents opportunities for overlapping memory and compute for multithreading, the multithreading/multicore execution time ratio is less than two. However, the percentage of cycles that present instruction overlapping opportunities is low because memory instructions hit in the L1 cache, which has a 3 cycle latency. Thus, the multithreading/multicore execution time ratio is close to two. Consequently, the trends for Int and HP are similar. In contrast, multithreading is more energy efﬁcient than multicore for Hist. Hist presents a large number of opportunities for overlapping memory and compute, causing multithreaded cores to stall less frequently while accessing the memory system, increasing power. Multithreading also has thread switching overheads. Multicore includes the active power for twice the number of cores, each of which stall more frequently while accessing the memory system. In total, active power for both conﬁgurations is nearly identical. The performance of multicore and multithreading is also similar due to the large number of overlapping opportunities. This translates to similar active energy and double the idle energy for multicore, as it is charged double the idle power. This makes multithreading more energy efﬁcient overall. The increased overlapping opportunities presented by Hist makes multithreading more energy efﬁcient than multicore. As a result, from an energy efﬁciency perspective, multiPiton Tile Array 28 cycles Chip  Bridge 5 cycles Gateway  FPGA 39 cycles Chipset FPGA Chip Bridge Demux 11 cycles North  Bridge 8 cycles DRAM Ctl 16 cycles DRAM ~70 cycles AFIFO + Buf FFs +  Req Send 2x Mem Ctl +  DRAM Access L1 Miss + L2 Miss AFIFO + Mux Buf FFs + AFIFO Buf FFs + AFIFO Buf FFs + Route C M F 17 cycles 63 cycles 39 cycles 12 cycles 6 cycles 11 cycles L2 Fill + L1 Fill Buf FFs + AFIFO Buf FFs + AFIFO Buf FFs + Mux Buf FFs + Mux Resp Process + AFIFO R D D ~395 Total Round Trip Cycles = ~790ns Figure 15. Piton system memory latency breakdown for a ldx instruction from tile0. All cycle counts are normalized to the Piton core clock frequency, 500.05MHz Figure 16. Time series of power broken down into each Piton supply over entire execution of gcc-166 threading favors applications with a mix of long and short latency instructions. Multicore performs better for integer compute heavy applications. Last, Hist exhibits a different energy scaling trend than Int and HP. Hist energy remains relatively constant as thread count increases, while Int and HP energy scales with thread count. This results from how the work scales with thread count. As previously stated, Hist maintains the same amount of total work and decreases work per thread as the thread count increases. Thus, the total energy should be the same for performing the same total amount of work. Int and HP increase the total work and maintain the work per thread with increasing thread counts, thus total energy increases. Hist energy, however, is not perfectly constant as thread count scales for 1 T/C. The data shows that 8 threads is the optimal conﬁguration for 1 T/C from an energy efﬁciency perspective. This is because 8 threads is the point at which the thread working set size ﬁts in the L1 caches. Beyond that, energy increases since useful work per thread decreases and lock contention increases, resulting in threads spending more energy contending for locks. These results only apply to ﬁne-grained multithreading. Results for simultaneous multithreading may differ. I. Benchmark Study In order to evaluate performance, power, and energy of real applications we use ten benchmarks from the SPECint 2006 benchmark suite [45]. We ran the benchmarks on both the Piton system and a Sun Fire T2000 system [46] with an UltraSPARC T1 processor [27]. A comparison of the system speciﬁcations is listed in Table VIII. The UltraSPARC T1 processor has the same core and L1 caches as Piton, except 771 actual memory timings than the memory devices are capable of supporting. Additionally, the Piton system DRAM has a 32bit interface, while the SunFire T2000 DRAM has a 64bit interface. This requires the Piton system to make two DRAM accesses for each memory request. Another reason for the discrepancy is the latency to get to memory and back in the Piton system. Figure 15 shows a breakdown of where cycles are spent in the journey of a ldx instruction from tile0 to memory and back. All cycle counts are normalized to the Piton core clock frequency, 500.05MHz. Almost 80 cycles are spent in the gateway FPGA and a number of cycles are wasted in offchip buffering and multiplexing. We believe with additional optimization, some of this overhead can be reduced. Further, if the Piton PCB were to be redesigned, it could include DRAM and I/O directly on the board and eliminate the need for the gateway FPGA. For optimal performance, the DRAM controller should be integrated on chip, as in the UltraSPARC T1. The average power for SPECint benchmarks is marginally larger than idle power, as only one core is active, and is similar across benchmarks. hmmer and libquantum are exceptions, largely due to high I/O activity (veriﬁed by analyzing VIO power logs). This is likely due to high ratios of memory instructions and high sensitivity to data cache sizes among the SPECint applications [47]. Energy results correlate closely with execution times, as the average power is similar across applications. Figure 16 shows a power breakdown time series for gcc-166. More breakdowns can be found at http://www.openpiton.org. J. Thermal Analysis Researchers have studied thermal aspects of multicore and manycore architectures to deliver lower power consumption, better power capping, and improved reliability [48], [49]. In this section, we characterize some thermal aspects of Piton to quantify the relationship between temperature and power and how application scheduling affects power and temperature. We conducted our experiment without the heat sink to have access to the surface of the package. To ensure that we don’t harm the chip, core frequency, VDD, and VCS were decreased to 100.01MHz, 0.9V, and 0.95V, respectively. Additionally, we use a different chip for this experiment which has not been presented in this paper thus far. We used the FLIR ONE [50] thermal camera to measure surface temperature. We ﬁxed the fan position with respect to the chip to guarantee similar airﬂow for all tests. The room temperature was 20.0 ± 0.2◦C and the chip temperature under reset was around 60◦C . Figure 17 shows the total power consumption as a function of the package hotspot temperature for different numbers of active threads running the High Power (HP) application. The temperature is actively adjusted by changing the fan’s angle. The exponential relationship between Figure 17. Chip power as a function of package temperature for different number of active threads. Cooling is varied to sweep temperature. Figure 18. Power variations and power/temperature dependence for synchronized (blue) and interleaved (green) scheduling of the two-phase test application. with four threads per core instead of two. However, the Piton uncore is completely different. Table IX shows the execution time for the UltraSPARC T1 and Piton and the average power and energy for Piton. Power measurements are taken throughout the application runtime, not just for 128 samples as in previous studies. There are a number of reasons for the difference in performance. The UltraSPARC T1 processor runs at two times the clock frequency of Piton. The Sun Fire T2000 has much more main memory and the UltraSPARC T1 has almost two times the amount of L2 cache as Piton. Additionally, Piton’s L2 access latency is larger (but the cache is more scalable) and there is an 8x discrepancy in memory latency. There are a couple reasons for the large discrepancy in memory latency. One is the latency of the memory devices. While the Piton system uses DDR3 DRAM and the SunFire T2000 uses DDR2 DRAM, the latency is the same, as indicated by the memory parameters in Table VIII. The DDR3 DRAM in the Piton system is rated for a 933MHz clock frequency, but due to a limitation in the Xilinx memory controller IP, we are only able to clock it at 800MHz. Since the memory controller must honor the rated memory timings in nanoseconds, the actual memory timing in cycles differs from what the devices are rated for. Due to the quantization of memory timings by clock cycles, this results in longer 772 power and temperature has been shown to be caused by leakage power in CMOS transistors [51] and is part of a larger trend which can be explored under a wider range of room temperatures and cooling solutions. Many scholars have studied manycore scheduling strategies and power budgeting with respect to thermal design power (TDP), power capping, and lifetime concerns [52], [53]. In order to analyze such impacts, we developed a test application with two distinct phases: a compute heavy phase (arithmetic loop) and an idle phase (nop loop). We ran this application on all ﬁfty threads with two different scheduling strategies, synchronized and interleaved. In synchronized scheduling all threads alternate between phases simultaneously, executing the compute and idle phases together. Interleaved schedules 26 threads in one phase and 24 in the opposite phase, thus half the threads execute the compute phase while the other half execute the idle phase. Figure 18 shows the total power consumption with respect to time and package temperature for synchronized (blue) and interleaved (green) scheduling. The changes in power consumption resulting from application phase changes causes corresponding changes in temperature. Changes in temperature feedback and cause corresponding changes in power. The hysteresis caused by this feedback loop can be observed in the power/temperature plot for both scheduling strategies. However, synchronized exhibits a much larger hysteresis, indicated by blue arrows in the ﬁgure. The average temperature for interleaved scheduling is 0.22◦C lower than synchronized, highlighting the impact different scheduling strategies for the same application can have. This shows how a balanced schedule can not only limit the peak power but also decrease the average CPU temperature. K. Applicability of Results It is important to note in which cases the results and insights in this work are applicable, as a single characterization cannot make generalizations about all chips. Ideally, researchers who would like to make use of the data should use OpenPiton in their research, as the characterization data would directly apply. However, this is not the case for designs dissimilar to Piton. We believe ISA has less of an impact on characterization results, so our results are likely applicable to somewhat similar ISAs. Microarchitecture will likely impact results to a greater extent. For example, our results for a ﬁne-grained multithreaded core will not apply to a simultaneous multithreaded core. Thus, it is difﬁcult to apply the results to very different microarchitectures, and this should not be expected. However, researchers studying similar microarchitectures or microarchitectural mechanisms can make use of the results. V. R E LATED WORK Manycore chips have been built in both industry and academia to explore power-efﬁcient, high-performance comTable X. COM PAR I SON O F INDU S TRY AND ACAD EM IC PROC E S SOR S Published Detailed Power/ Energy Characterization  [23], [24]  [26]  [26]  [26] Processor Open Source Academic/ Manycore/ Industry Multicore Intel Xeon Phi Knights Corner [59] Industry Manycore Intel Xeon Phi Knights Landing [16] Industry Manycore Intel Xeon E5-2670 [60] Industry Multicore Marvell MV78460 [61] (ARM Cortex-A9) Industry Multicore TI 66AK2E05 [62] (ARM Cortex-A15) Industry Multicore Cavium ThunderX [17] Industry Manycore Phytium Technology Mars [18] Industry Manycore Qualcomm Centriq 2400 Processor [19] Industry Manycore Tilera Tile-64 [57] Industry Manycore Tilera TILE-Gx100 [58] Industry Manycore Sun UltraSPARC T1/T2 [27], [63] Industry Multicore IBM POWER7 [64] Industry Multicore MIT Raw [38] Academic Manycore UT Austin TRIPS [39] Academic Multicore UC Berkeley 45nm RISC-V [41] Academic Unicore UC Berkeley 28nm RISC-V [40] Academic Multicore MIT SCORPIO [32], [55] Academic Manycore U. Michigan Centip3De [54] Academic Manycore NCSU AnyCore [66] Academic Unicore NCSU H3 [67] Academic Multicore Celerity [56] Academic Manycore Princeton Piton [12], [13] Academic Manycore 1 Minor power numbers provided, no detailed characterization 2 Performed power/energy characterization of on-chip DC-DC converters, not processor architecture                                    [65]  [33]  [41]1  [40]2  [54]  [66]1 puting. Many of these chips, such as Cavium’s ThunderX processor [17], Phytium Technology’s Mars processor [18], Qualcomm’s Centriq 2400 Processor [19], University of Michigan’s Centip3De processor [54], and the SW26010 processor in the Sunway TaihuLight supercomputer [20] are designed to be used in supercomputers or high-performance servers for cloud computing and other applications. Like Piton, manycore chips including MIT’s Raw processor [38], Intel’s Knights Landing Xeon Phi processor [16], MIT’s SCORPIO processor [32], [55], the Celerity processor [56], and Tilera’s Tile-64 [57] and TILE-Gx100 [58] processors utilize a tile-based architecture with NoCs. Unfortunately, little detailed power and energy data has been publicly released for these chips, evident from Table X which compares academic and industry chips taped-out in silicon. Academics have characterized some manycore chips [23], [24], [29], [33], however the source for these chips was never released publicly. Thus, researchers are unable to correlate the results to design details and RTL models. Further, academic characterizations of proprietary designs are not able to verify, through simulation, that the design behaves as expected during measurement. Piton has been open sourced and this work presents detailed power and energy characterization which veriﬁes expected behavior through simulation. Moreover, the test setup was speciﬁcally designed with power characterization in mind, which can be a limiting factor for some of the mentioned characterizations. This work represents the ﬁrst detailed power and energy characterization of an open source manycore processor, enabling correlation of the data to the RTL and design. Of course, other work has characterized and/or modeled power, energy, and/or thermal aspects of processors [21], [22], [25], [26], [65], [68]–[71]. This paper performs characterization in the context of a tiled manycore architecture. V I . CONC LU S ION In this work, we present the ﬁrst detailed power and energy characterization of an open source manycore research processor taped-out in silicon. Speciﬁcally, we studied volt773 age versus frequency scaling, energy per instruction (EPI), memory system energy, NoC energy, and application power and energy. The characterization revealed a number of insights, including the impact of operand values on EPI, that recomputing data can be more energy efﬁcient than loading it from memory, on-chip data transmission energy is low, and energy efﬁcient multithreaded design insights. All hardware infrastructure along with all data collected has been open sourced and is available at http://www.openpiton.org. We hope that this characterization, including the data and the insights derived, enables researchers to develop future research directions, build accurate power models for manycore processors, and make better use of OpenPiton for accurate power and energy research. ACKNOW L EDGM EN T We thank Samuel Payne for his work on Piton’s off-chip interface and the chipset memory controller, Xiaohua Liang for his work on chipset routing and transducing, and Ang Li and Matthew Matl for their work on the chipset SD controller. This work was partially supported by the NSF under Grants No. CCF-1217553, CCF-1453112, and CCF1438980, AFOSR under Grant No. FA9550-14-1-0148, and DARPA under Grant No. N66001-14-1-4040, HR0011-13-20005, and HR0011-12-2-0019. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of our sponsors. "
2018,iNPG - Accelerating Critical Section Access with In-network Packet Generation for NoC Based Many-Cores.,"As recently studied, serialized competition overhead for entering critical section is more dominant than critical section execution itself in limiting performance of multi-threaded shared variable applications on NoC-based many-cores. We illustrate that the invalidation-acknowledgement delay for cache coherency between the home node storing the critical section lock and the cores running competing threads is the leading factor to high competition overhead in lock spinning, which is realized in various spin-lock primitives (such as the ticket lock, ABQL, MCS lock, etc.) and the spinning phase of queue spin-lock (QSL) in advanced operating systems. To reduce such high lock coherence overhead, we propose in-network packet generation (iNPG) to turn passive ""normal"" NoC routers which only transmit packets into active ""big"" ones that can generate packets. Instead of performing all coherence maintenance at the home node, big routers which are deployed nearer to competing threads can generate packets to perform early invalidation-acknowledgement for failing threads before their requests reach the home node, shortening the protocol round-trip delay and thus significantly reducing competition overhead in various locking primitives. We evaluate iNPG in Gem5 using PARSEC and SPEC OMP2012 programs with five different locking primitives. Compared to a state-of-the-art technique accelerating critical section access, experimental results show that iNPG can effectively reduce lock coherence overhead, expediting critical section access by 1.35x on average and 2.03x at maximum and consequently improving the program Region-of-Interest (ROI) runtime by 7.8% on average and 14.7% at maximum.","2018 IEEE International Symposium on High Performance Computer Architecture iNPG: Accelerating Critical Section Access with In-Network Packet Generation for NoC Based Many-Cores Yuan Yao KTH Royal Institute of Technology Stockholm, Sweden yuanyao@kth.se Zhonghai Lu∗ KTH Royal Institute of Technology Stockholm, Sweden zhonghai@kth.se ABSTRACT As recently studied, serialized competition overhead for entering critical section is more dominant than critical section execution itself in limiting performance of multi-threaded shared variable applications on NoC-based many-cores. We illustrate that the invalidation-acknowledgement delay for cache coherency between the home node storing the critical section lock and the cores running competing threads is the leading factor to high competition overhead in lock spinning, which is realized in various spin-lock primitives (such as the ticket lock, ABQL, MCS lock, etc.) and the spinning phase of queue spin-lock (QSL) in advanced operating systems. To reduce such high lock coherence overhead, we propose in-network packet generation (iNPG) to turn passive “normal” NoC routers which only transmit packets into active “big” ones that can generate packets. Instead of performing all coherence maintenance at the home node, big routers which are deployed nearer to competing threads can generate packets to perform early invalidation-acknowledgement for failing threads before their requests reach the home node, shortening the protocol round-trip delay and thus signiﬁcantly reducing competition overhead in various locking primitives. We evaluate iNPG in Gem5 using PARSEC and SPEC OMP2012 programs with ﬁve different locking primitives. Compared to a state-of-the-art technique accelerating critical section access, experimental results show that iNPG can effectively reduce lock coherence overhead, expediting critical section access by 1.35× on average and 2.03× at maximum and consequently improving the program Region-of-Interest (ROI) runtime by 7.8% on average and 14.7% at maximum. 1. INTRODUCTION As the core count grows quickly, there is a greater potential to speed up the execution of parallel and concurrent programs. While this trend helps to linearly expedite the concurrent execution part, the ultimate application speedup is however limited by the sequential execution part of programs, as reﬂected in the well-known Amdahl’s law [20]. Figure 1 shows the typical execution of a multi-threaded program. After an initialization phase, N threads start parallel executions and encounter a synchronization point where each thread competes to access and execute a critical section (CS). In particular, serialized CS access incurs competition ∗Zhonghai Lu is the corresponding author. Par alle l phas e Se ria lized pha se In it ializ a tion  phas e Thread ɽ1 CS1 COH 1 Thread ɽ2 Thread ɽ3 . . . Thread ɽN CS2 COH 2 CS3 COH 3 ... COHN-1 CSN Figure 1: Parallel program execution. overhead (COH) due to locking retries in spin-lock primitives [31, 2, 16, 14] and context switching & sleep overhead in queue spin-lock primitive (See details in Section 2). To expedite parallel application execution, most previous works emphasize on accelerating the CS parts shown in Figure 1 by exploiting various mechanisms such as running the CS code on fat cores in asymmetric chip multi-processors (CMPs) [36, 21, 27, 38, 37], or predicting and prioritizing threads that are executing critical sections [9, 39, 24]. Recent research in [40] demonstrates that COH may indeed exceed the time for CS execution in network-on-chip (NoC) based many-cores, and become the dominating factor limiting the performance of parallel applications. By opportunistically avoiding the OS-level context switching & sleep overhead, the proposed OCOR (Opportunistic Competition Overhead Reduction) technique in [40] shows great COH reduction (over 30% on average for PARSEC [3] and SPEC OMP2012 [28] benchmarks) and application acceleration up to 24.5%. However, this work uses only queue spin-lock for experiments and does not investigate the effect of cache coherence in inﬂuencing COH in cache-coherent many-cores. In the paper, we show that the lock coherence overhead (LCO) in lock spinning for both spin-lock primitives and the spinning phase of queue spin-lock primitive is a major source of COH. This is because, in lock spinning, each competing thread will generate an exclusive access request to the home node where the lock variable is stored. However, only one of these exclusive accesses will succeed, and the home node will then invalidate the CS lock copies in all the other threads’ L1 caches. Only when the winning thread, which is the new owner of the lock variable, receives all acknowledgements from the losing threads, it can proceed to execute the following critical section. To reduce such high overhead coherence round-trip latency, we propose an in-network packet generation (iNPG) technique that can perform early cache invalidation to locking fail2378-203X/18/$31.00 ©2018 IEEE DOI 10.1109/HPCA.2018.00012 15 ure threads, thereby reducing LCO and accelerating execution of multi-threaded applications. iNPG is enabled by active “big” routers, which can not only transmit packets like normal routers but also generate packets in the network as early invalidations to L1 caches of failing threads. The big routers then wait for the acknowledgements from the losing threads, and then forward the acknowledgements to the winning thread to send a valid lock copy to each of the losing thread. In this way, the home node is largely freed from the lock coherence maintenance burden, and the winning thread can move forward to the CS more quickly. Focusing on lock spinning, iNPG is an orthogonal scheme to OCOR suggested for queue spin-lock in [40]. As such, it can be combined seamlessly with OCOR to achieve the highest COH reduction with various spin-lock primitives and the queue spinlock primitive, as shown in experimental results. The rest of the paper is organized as follows. Section 2 gives background and motivation. Section 3 illustrates lock spinning in cache coherent architecture and describes our design principle. Section 4 gives design and synthesis details of big router. In Section 5 we report experiments and results. After discussing the related work in Section 6, we ﬁnally conclude in Section 7. 2. BACKGROUND AND MOTIVATION 2.1 Lock spinning schemes In lock spinning, when a thread fails to lock a variable protecting a CS, it will keep re-trying (after a short spin interval) until succeed [33]. During the OS runtime quantum (usually in the order of hundreds of milliseconds) of the spinning thread, no other threads are allowed to use the core, leading to a waste of core processing time and energy. However, by persistently polling on a lock variable, lock spinning can be quickly successful once the lock is released. Modern OSes provide different lock spinning mechanisms, including spinlocks and queue spin-lock, to support critical section synchronization for concurrent programs. 1) Test-and-set lock (TAS). The test-and-set lock is a spinlock that employs a polling loop to access a global boolean ﬂag that indicates whether the critical section is held. Each core repeatedly executes an atomic test_and_set instruction in an attempt to change the ﬂag from false to true, thereby acquiring the lock. The shortcoming of the test-and-set lock is the contention for the lock, as each waiting core continually spins the single shared lock until success. 2) The ticket lock (TTL). As proposed in [31], the ticket lock is a spin-lock that consists of two counters, one request counter containing the number of cores to acquire the lock, and one release counter recording the number of cores that have released the lock. A core competes for the lock by fetching a ticket from the request counter, and waiting until its ticket is equal to the release counter. A core releases the lock by incrementing the release counter. 3) Array-based queuing lock (ABQL). By having all competing cores spinning on different lock variables, array-based queuing lock is a spin-lock ensuring that on a lock release only one core attempts to acquire the lock. In this way, ABQL signiﬁcantly decreases the number of lock contentions when a lock is released. ABQL has two versions, which are corespondingly proposed by Anderson [2] and Graunke and Thakkar [16], in which the Anderson’s version further protects the ABQL with an atomic test_and_set. 4) Mellor-Crummey and Scott (MCS) lock. As inspired by [14], Mellor-Crummey and Scott [26] propose a per-core structured spin-lock, which is able to eliminate much of the cache-line bouncing experienced by spin-locks. In MCS lock, when a core attempts to secure a global lock variable, it will create an mcs_spin_lock structure of its own. Using an atomic exchange operation (compare_and_swap), it stores the address of its own mcs_spin_lock structure into the global lock’s “next pointer” ﬁeld. The atomic exchange operation will then return the previous value of the next pointer. If that pointer was null, the acquiring core is successful in acquiring the lock. Otherwise, the core is put into a queue waiting for its predecessor to release the lock. 5) Queue spin-lock (QSL). QSL is a variant of spin-lock which is adopted in most modern OSes such as Linux 4.21 and Unix BSD 4.42 . It starts with a spin-lock phase to secure a CS, and if not successful after a certain times of retry (by default 128 times in Linux 4.2), it yields to enter a sleep phase after context switching (thus releasing the core for processing other tasks or in power-saving mode) and the thread’s locking request is placed in a request queue. The thread continues sleeping until being woken up to secure the lock when the CS is unlocked by the holding thread. The spin-lock phase of a queue spin-lock can be one of the four spin-lock alternatives (TAS, TTL, ABQL, or MCS) discussed above. Because of its advantages in reducing synchronization trafﬁc and OS overhead, we select QSL in the paper with the spin-lock implemented as MCS lock. 2.2 Motivation To show the criticality of lock coherence overhead (LCO) in different locking primitives, we experimented on a 64core architecture in Gem5 (See experimental setup in Section 5), with each program running alone in 64 concurrent threads. Figures 2 reports the percentage of LCO in application running time under TAS, TTL, ABQL, MCS and QSL in three applications: kdtree from SPEC OMP2012 and facesim, ﬂiudanimate from PARSEC. O C L f o e g a t n e c r e P 100% 80% 60% 40% 20% 0% S A T L T T L Q B A S C M L S Q S A T L T T L Q B A S C M L S Q S A T L T T L Q B A S C M L S Q kdtree fluidanimate facesim Figure 2: Percentage of LCO in application running time. From Figure 2 we can observe that 1) across different benchmarks, the percentage of LCO in application running time differs. 2) TAS lock has the most percentage of LCO, 1 https://kernelnewbies.org/Linux_4.2 2 http://www.unixdownload.net/ 16     followed by TTL and ABQL locks; LCO percentage in MCS and QSL locks are relatively smaller among all locking primitives. As shown in the ﬁgure, in benchmark ketree, TAS spends nearly 50% of application running time synchronizing critical section lock among competing threads. With TTL and ABQL, the percentage of LCO is reduced to 31% and 27%, correspondingly. In MCS and QSL, because locking contention among threads is further minimized, the percentage of LCO reaches 14% and 17% application running time, correspondingly. The same phenomenon has been consistently observed in ﬂuidanimate and facesim, where LCO occupies 65% and 90% application running time under TAS, 47% and 57% under TTL, 50% and 56% under ABQL, 20% and 30% under MCS, and 25% and 32% under QSL, respectively. From the experiment we can conclude that LCO indeed lies straightly on application execution critical path, causing heavy overhead for lock spinning. 3. IN-NETWORK PACKET GENERATION 3.1 Target many-core architecture R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C Core C Pr ivate L1 Memory Con troller Shared L2 and  Network interface Normal rou ter Big rou ter R R Figure 3: A 64-core 8×8 many-core architecture. Figure 3 shows a typical architecture for a 64-core 8×8 many-core, where the NoC (routers and network interfaces) communicates messages among processor nodes (a core with its private L1 cache), secondary on-chip shared L2 cache banks, and on-chip memory controllers. Routers are organized in a popular mesh topology on which the XY dimensional routing algorithm is implemented to achieve simplicity and deadlock-free. Eight memory controllers are symmetrically connected to the middle nodes on the top and bottom rows. Due to its layout and packaging conveniences, this pattern has been used in academic and industrial manycore designs such as MIT SCORPIO 36-core processor [7] and latest Intel Knights Landing 36-tile processor [34]. To support cache coherency, a directory based MOESI cache coherence protocol is implemented. On the target manycore, once a core issues a memory request, the private L1 is ﬁrst checked for the presence of the requested data. If this fails, the request is then forwarded to the local shared L2 or via the NoC to remote shared L2/DRAM. Figure 3 also depicts a big router deployment with 32 big routers interleaving with 32 normal routers. 3.2 Lock spinning under cache coherency We look into how lock spinning works and how its associated LCO is caused on NoC based many-cores with cache coherence protocol. Algorithm 1 shows an example assembly code [19] for a simple test-and-set spin-lock implementation on many-cores. In the code, value 0 denotes a lock “available” and value 1 “occupied”. In Line 1, a thread ﬁrst spins to check on a local copy of the lock variable (whose memory address is stored in local register R1 and the lock itself is stored remotely at a home node) until it sees that the lock is available. Once a thread ﬁnds that the lock is freed (by loading 0 into register R2), it then passes Line 2. In Line 3, a thread ﬁrst modiﬁes the local lock copy from 0 to 1 to assert its ownership for the lock (but the lock variable at the home node still remains as 0). Then, in Line 4, threads compete with each other to secure the lock by sending an atomic swap request (SWAP) to the home node, which writes the locally modiﬁed lock (with value 1) to the one in the home node (with value 0). The winning SWAP request will then set the lock variable at the home node to 1 to assert its exclusive access right to the lock. Consequently, all loser SWAP requests will only see 1 in the lock variable which was set by the winner request. After the swap operation, the winning thread passes Line 5 to execute the critical section, with all loser threads spinning for the lock again from Line 1. θ1 , θ2 and θ3 on an example 3×3 CMP based on the MOESI Figure 4 shows how Algorithm 1 is executed in three threads coherence protocol introduced in [35]. The lock variable in the home node is initialized to the “available” state (value 0). Step 1: When θ1 , θ2 and θ3 execute Line 1 of Algorithm 1, because none of them has a copy of the lock in their local cache, they all encounter a cold cache read miss. Each thread then sends a data read request to the home node, which inturn sends a valid copy of the lock (value 0) back to the corresponding L1 cache of each thread. At the same time, the home node marks θ1 , θ2 and θ3 as data sharers of the lock variable in its coherence directory. Step 2: When the competing threads load 0 into their locally copied CS locks, they pass Line 2. In Line 3, each thread locally modiﬁes the lock to the occupied state by changing its value to 1. Then, the three threads compete with each other by executing the swap operation (SWAP in Line 4). At the hardware level, this is done by each thread sending an atomic “read-for-modiﬁcation” operation (GetX request in Gem5), which tries to get the exclusive access right to the lock in the home node. The winning thread will then execute the swap operation and write 1 into the lock variable. Step 3: In Figure 4, assuming that GetX from θ1 succeeds, and GetX requests from θ2 and θ3 fail. In this case, θ1 wins the exclusive access right to the lock. The directory controller at the home node will perform three tasks. First, it notiﬁes θ1 that it now has the exclusive access right of the lock. Second, it searches its coherence directory and sends invalidations to the lock copies in θ2 ’s and θ3 ’s caches. θ2 and θ3 , upon receiving the Invalidation, will then each send an Invalidation-Acknowledgement (InvAck in Gem5) to θ1 , which is now the owner of the lock variable. Third, it forwards the GetX requests from θ2 and θ3 to θ1 . Moreover, with the forwarded GetX requests, the directory controller at 17 ɽ1 ɽ2 ɽ3  (1) Lock: LD R2, 0(R1)  (2)           BENZ R2, Lock  (1) Lock: LD R2, 0(R1)  (2)           BENZ R2, Lock  (1) Lock: LD R2, 0(R1)  (2)           BENZ R2, Lock (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ɽ2 0 (3,1) (3,2) H 0 (3,3) ɽ3 0 ɽ1 0 V a l i d c o p y V a l i d c o p y V a l i d c o p y (a) Step 1. (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ɽ2 1 (3,1) (3,2) H 0 (3,3) ɽ3 1 ɽ1 1 ɽ1 ɽ2 ɽ3  (3) ADD R2, R0, #1  (4) SWA P R2, 0(R1)  (3) ADD R2, R0, #1  (4) SWA P R2, 0(R1)  (3) ADD R2, R0, #1  (4) SWA P R2, 0(R1) Win ning  GetX Failed  GetX Failed  GetX (b) Step 2. (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ɽ2 I (3,1) (3,2) H 0ї1 (3,3) ɽ3 I ɽ1 1 ɽ1 ɽ2 ɽ3  (5) BENZ R2 successe s.        Ente r crit ical sec t ion.  (5) BENZ R2 fai ls.        CS locking fai ls.  (5) BENZ R2 fai ls.        CS locking fai ls. o F r ɽ d a G n e d X ɽ w t f 2 3 AckCo unt I n v Inv (c) Step 3. ɽ1 ɽ1 ɽ2 ɽ2 ɽ3 ɽ3 &U LWLFDOVHFWLRQUHOHDVH :U LWHWRKRPHQRGH  (QFRXQ WHUVUHDGPLVV 1HZYDOXHORDGHG (QFRXQ WHUVUHDGPLVV 1HZYDOXHORDGHG (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ɽ2 1 (3,1) (3,2) H 1 (3,3) 0 ɽ3 1 ɽ1 1  (5) BENZ R2 successe s.        Ente r crit ical sect ion.  (5) BENZ R2 fai ls.        CS locking fai ls.  (5) BENZ R2 fai ls.        CS locking fai ls. InvAck Data D a t a I n v A c k (d) Step 4. Figure 4: Spin-lock execution on an example 3×3 CMP under MOESI. In Step 1, the home node responds to read misses from θ1 , θ2 and θ3 and sends a valid lock copy to the local cache of each thread. In Step 2, each thread individually modiﬁes the lock to the “occupied” status (value 1) and competes with each other to get the exclusive access right of the lock. In Step 3, the winning thread gets exclusive access to the lock, with all losing threads’ lock copies invalidated. GetX requests from θ2 and θ3 are further forwarded by the directory controller of the home node to θ1 . In Step 4, θ1 sends valid lock copies to θ2 and θ3 and then enters critical section after collecting all acknowledgements from θ2 and θ3 . Algorithm 1 Assembly code for lock spinning As shown in the motivational Figure 2, this lock coherence 1: Lock: LD R2, 0(R1) ;Load of lock to R2 overhead (LCO) is a signiﬁcant part of application running BENZ R2, Lock ;Lock unavailable, spin time because the home node is solely responsible for sending invalidations to all failing threads and θ1 has to collect ADD R2, R0, #1 ;Change lock value to 1 all the invalidation acknowledgements to move forward. SWAP R2, 0(R1) ;Swap BENZ R2, Lock ;Loop to line 1 if seeing lock 1 We aim to shorten the lock coherence latency, i.e., LCO, so as to reduce COH [40] in lock spinning. To this end, the home node also forwards the total number of data sharers we propose in-network packet generation (iNPG) enabled by (AckCount in Gem5) of the lock variable to θ1 , so that θ1 “big” router (denoted BR in Figure 5b), a normal router encan monitor and count the receiving of InvAck messages. hanced with active packet generation functionality for mainStep 4: After receiving the forwarded GetX requests and taining cache coherency. This is possible because intermediall of the InvAck messages from θ2 and θ3 , θ1 sends a valid ate routers can be used to provide a temporary “barrier” for a copy of the lock to the L1 caches of θ2 and θ3 and then lock variable. Once an intermediate router transfers a GetX knows that there are no longer any invalid copy of the lock request for lock variable (cid:2) (denoted GetX[(cid:2)]), a temporary variable. Thus θ1 can write to the lock without violating “barrier” is set up for lock (cid:2), which will stop delivering failcoherence. As a result, θ1 will successfully execute the SWAP ing GetX[(cid:2)] requests that lose arbitration to the delivered operation and executes the following critical section, while GetX[(cid:2)], or subsequent GetX[(cid:2)] requests that arrive later at θ2 and θ3 will again loop back to Line 1. After θ1 ﬁnishes the big router. Since the “barriers” are set up distributively at its critical section, it releases the lock by changing the lock intermediate routers instead of aggregated at the home node, value from occupied (value 1) to available (value 0) in the multiple early invalidations can be generated and sent by home node. intermediate big routers to shorten LCO. More details are presented in Section 4. The principle of iNPG can be illustrated in Figure 5b. When the losing or later arriving GetX operations reach a big router, their further transmissions are stopped. Instead, cache invalidations are immediately generated and sent by the big router to their issuing threads (θ2 and θ3 in Figure 5). The big router then forwards the GetX requests and the acknowledgements (InvAcks) from the failing threads to the home node, which are in turn forwarded by the home node to the winning thread (θ1 in Figure 5). In this way, the invalidation and acknowledgement of the lock copy are done on the way to the home node instead of being done after the GetX requests of the losing threads reach the home node. Note that if the winning GetX request is determined at the home node (e.g., two GetX requests arrive at the home node at the same time but from different inports), the home node is then responsible for sending invalidation to and waiting 2: 3: 4: 5: 3.3 Reduce lock coherence overhead with iNPG Figure 5a depicts the transactional procedure for Step 2, 3 and 4 in Figure 4. When the three competing threads θ1 , θ2 and θ3 execute spin-lock, three atomic swaps (corresponding to three GetX requests in Gem5) are issued, which are routed through NoC routers (denoted R in Figure 5a) to the home node. The winning GetX will exclusively secure the lock. Directory controller in the home node then invalidates the lock copies in θ2 ’s and θ3 ’s L1 caches, and forwards the GetX requests from θ2 and θ3 to θ1 , which is now the owner of the lock. When θ1 receives the forwarded GetX requests and all InvAcks from θ2 and θ3 , it then sends valid lock copies to θ2 ’s and θ3 ’s L1 caches and moves forward to execute codes in the critical section. From ﬁgure 5a, we can observe that the coherence trafﬁc between the home node and θ1 , θ2 , θ3 lies on the critical path of program execution time. 18             ɽ1  Ge tX AckC ount and Fw dGe tX for ɽ2 and ɽ3  ɽ2  Ge tX R H Inv Ge tX from ɽ1, ɽ2, ɽ3 Inv Ge tX ɽ3  InvAck ɽ2  InvAck ɽ3  Fw dGe tX , AckC ount and  InvAck for ɽ2 and ɽ3  ɽ1  CS Ge tX ɽ1  Ge tX f rom ɽ1 and Fw dGe tX for ɽ2 and ɽ3  Valid Lock ɽ2  Valid Lock ɽ3  Spin a t  local  cache Spin a t  local  cache ɽ2  Ge tX BR Inv . ɽ2  InvAck for  ɽ2 and ɽ3  InvAck BR ɽ3  Ge tX Inv . InvAck ɽ3  H ɽ1  Lock cohe rence  ove rhead CS Valid Lock ɽ2  Valid Lock ɽ3  Spin a t  local  cache Spin a t  local  cache (a) Conventional spin-lock transactional procedure. (b) Big router enabled spin-lock transactional procedure. Figure 5: Big router enabled iNPG aiming to shorten lock coherence overhead (LCO) in lock spinning. for acknowledgement from the thread that loses the local arbitration at the home node, just as in the original operation. 4. BIG ROUTER DESIGN 4.1 Micro-architecture The idea of iNPG for early cache coherence completion is realized by “big” router, which enhances a normal NoC router with packet-generation functionality. Figure 6 shows the micro-architecture of a big router, in which the packet generator is added to maintain a locking barrier table and a packet generation logic to generate cache coherence packets in the network. Our baseline router is a 2-stage pipelined speculative router [29] in which the ﬁrst stage performs Route Computation (RC), Virtual Channel (VC) Allocation (VA), and Switch Allocation (SA) in parallel and the second stage is Switch Traversal (ST). In implementation, packet generation is realized in the same pipeline stage as ST. In a big router, the VC allocator interacts with the packet generator as follows. (1) On detecting the ﬁrst GetX lock request transferred at a big router, the VC allocator informs the packet generator to create a temporary “lock barrier” in the locking barrier table. (2) Once a lock barrier is created, subsequent or arbitration-failed GetX requests for the same lock will be stopped. As shown in Figure 6, for every stopped GetX request, an early invalidation (EI) entry is created under the corresponding lock barrier to track the status of executing early invalidation. When a GetX is stopped, an Inv packet will be generated, stored to a separate VC, and sent to the corresponding thread. Meanwhile, the VC allocator changes the failed GetX request to a FwdGetX request, which is forwarded to the home node. (3) Once receiving the InvAck packet for an early Inv packet that was sent out by the big router, the big router forwards the received InvAck to the home node, which further sends it to the lock owner. Figure 6 sketches the locking barrier table. As shown in the ﬁgure, each lock barrier entry contains the memory address of the lock variable and its time-to-live (TTL) duration, which is by default set to 128 cycles. The purpose of TTL is to delete its corresponding lock barrier when it counts down to zero. The TTL counts down only if there is no EI entry and resets to the default value whenever an EI entry is created. Each EI entry contains the issuing core’s ID of a stopped GetX request and consists of 4 phases: Inv generated (denoted Inv), GetX forwarded (denoted GetXFwd), InvAck for an early Inv received (denoted InvAck), and InvAck forwarded (denoted AckFwd). An EI entry is freed 19 Input Channe l 1 Input Channe l 2 Input Channe l 3 Input Channe l 4 Input Channe l 5 . . . . . . . . . . . . . . . Packet generator Lock bar r ier Lock ing b ar rier  table Packet  generation log ic . . . VC/Switch  Allocator . . . MemA ddr TTL 0x783AD236 128  Co re ID Phase E I entry 7 InvAck 42 AckFwd ... 0x3B04F784 36 GetX Fwd InvAck 54 Inv Addr Tra ns Inv AckFwd Inv AckFwd Msg. Type Des. MemAddr Packet header Payload Packet generator Output Channe l 1 . . . Output Channe l 5 5×5 crossbar Figure 6: Big router architecture. when all the four phases are ﬁnished. Further, when all EI entries of a lock barrier are released, its TTL starts countdown from the default value. Finally, a lock barrier entry is deleted after its TTL reaches 0. Figure 6 also shows the format of generated packets. We omit the information ﬁelds that are used for ﬂow control in all network packets (such as ﬂit type, VC identiﬁer, etc.), and focus on the additional information ﬁelds that are generated. As shown in the ﬁgure, the packet generator generates packet in the Inv phase (generates invalidation packet) and the AckFwd phase (changes the destination of the received InvAck message to the ID of the home node). When the locking barrier table is full, following GetX requests will pass through as in a normal router. 4.2 Synthesis and chip ﬂoorplan & layout Methodology. We have made RTL implementations of the normal and big routers. We perform RTL synthesis in Synopsys Design Compiler and physical ﬂoorplanning & layout in Cadence SoC Encounter using TSMC’s 40 nm low power library (typical case). During synthesis, the core/uncore voltage is set to 1.1V and frequency to 2.0GHz. During ﬂoorplanning, we use 1.7V as the chip input voltage, which drives all I/O pins of the chip and is scaled down to 1.1V for core/un-core voltage. We select OpenRISC 12003 (OR1200) open source CPU as the core model, whose conﬁgurations are adjusted according to our architectural setup in Table 1. Integrating 32 normal and 32 big routers to 64 OpenRISC cores, we build a 64-core chip following Figure 3. 3 https://openrisc.io/implementations     Technology Total layers Metal layers Metal stack  strategy Power mesh  strategy Gate count SC count Net count Total SC area Cell density Total wire length Chip area Core Big router TSMC 40 nm  Router Low power, Typical case (lpbwptc)  28 10 8 normal thick metal layers, 2 ultra thick metal layers Top 2 metal layers (M10 and M9)  dedicated for power mesh 152.5 K 22.4 K 19.9 K 23.2 K 4.0 K 3.6 K 60.9 K 11.1 K  10.0 K 0.97 mm2 0.14 mm2 0.13 mm2 48.26% 66.67% 61.90% 8.81 m 1.42 m 1.28 m 2.03 mm2 0.21 mm2 11395 um 460 um Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R 1 1 3 9 5 u m 460 um Input  buffer 1 4 6 0 u m Input  buffer 3 Input  buffer 2 Input  buffer 4 iNPG 4 6 0 u m Input  buffer 5 VA, SA and  Switch (a) Module synthesis and layout (b) Chip ﬂoorplan with clock tree (c) Big/normal router dimension (d) Big router components Figure 7: Synthesis and physical ﬂoorplan & layout results for one big/normal router and the whole 64-core chip. Table 1: Simulation platform conﬁgurations. Item Core L1-Cache L2-Cache Memory NoC Amount 64 cores 64 banks 64 banks 8 ranks 64 nodes OCOR [40] iNPG Description Alpha 2.0 GHz out-of-order cores. 32-entry instruction queue, 64-entry load queue, 64-entry store queue, 128-entry reorder buffer. Private, 32 KB per-core, 4-way set associative, 128 B block size, 2-cycle latency, split I/D caches, 32 MSHRs. Chip-wide shared, 1 MB per-bank, 16-way set associative, 128 B block size, 6-cycle latency, 32 MSHRs. 4 GB DRAM, 512 MB per-rank, up to 16 outstanding requests for each processor, 8 memory controllers. 8×8 mesh network. Each node consists of 1 router, 1 network interface (NI), 1 core, 1 private L1 cache, and 1 shared L2 cache. X-Y dimensional order routing. Router is 2-stage pipelined, 6 VCs per port, 4 ﬂits per VC, 4 virtual networks. 128-bit data path. Directory based MOESI. One cache block consists of 1 8-ﬂit packet. One coherence control message consists of 1 single-ﬂit packet. 128 retry times in the spinning phase. 9 priority levels, 8 higher levels for requests in the spinning phase; each priority level is mapped to 16 retry times; 1 lowest priority level for wakeup requests. Unless otherwise speciﬁed, 32 normal routers and 32 big routers, where one big router is deployed between every two normal routers. 16-entry locking barrier table in a big router. Synthesis results. As shown in Figure 7a, a normal router consumes 19.9K (equivalent NAND) gates while a big router consumes 22.4K gates. A packet generator consumes 2.5K gates with the majority coming from the locking barrier table which by default contains 16 lock barriers and 16 EI entries. The added packet generator does not signiﬁcantly increase the critical path inside a big router. Under 2.0GHz timing constraint, we observed 2.9% of all end-to-end paths violating the endpoint slack, which are all eliminated via placement optimization during ﬂoorplanning. A packet generator consumes 8.4 mW dynamic power, adding 9.9% overhead to a normal router. On the target architecture, one big tile (1 core plus 1 big router) consumes 716.1 mW dynamic power, with the core consuming 623.5 mW and the big router 92.6 mW. One normal tile (1 core plus 1 normal router) consumes 707.7 mW dynamic power, with the core power the same as in a big tile but the router power decreasing to 84.2 mW. Floorplan & layout results. As summarized in Figure 7a, the number of ﬂoorplan layers is 28, which includes 10 metal routing layers, 11 via layers, 5 implant layers, 1 master-slice layer and 1 AP layer. In the 10 metal layers, the top 2 layers (layers M10 and M9) are dedicated for power mesh, which are featured with ultra thick wires. As shown in Figure 7b, on the target architecture, each tile (both big tile and normal tile) consumes equal chip area, with the distance between neighbour tiles being 1.8 um to accommodate the 256-bit bidirectional wires between routers (128-bit per direction with 1-bit wire width 0.007 um). Further, to implement both big and normal routers with practical physical layout, we manipulate the standard cell (SC) density to accommodate both big and normal router within the same dimension, which is 460 um by 460 um (0.21 mm2 ) as shown in Figure 7c. The cell density (before ﬁller insertion) is 66.67% in a big router while 61.90% in normal router, as reported in Figure 7a. 5. EXPERIMENTS 5.1 Methodology We evaluate iNPG with timing-detailed full-system simulations using both PARSEC (10 programs 4 5 with all large input sets) [3] and SPEC OMP2012 (all 14 programs) [28] benchmarks. For data validity, results are obtained from the multi-threaded execution phase called Region-of-Interest (ROI). We implement and integrate our design in Gem5 [4], in which the embedded network GARNET [1] is enhanced according to our technique. The key conﬁgurations of the simulation platform are listed in Table 1. Each application runs on 64 cores with one core running one thread. The operating system is Linux 4.2. Unless otherwise speciﬁed, the synchronization primitive is the default queue spin-lock in Linux 4.2 with the spin-lock phase implemented as MCS lock. We set up four comparative cases as follows. Case 1 is Original, the baseline architecture that Gem5 simulates according to the conﬁgurations in Table 1. Case 2 is OCOR (Opportunistic Competition Overhead Reduction), the technique proposed for queue spin-lock in [40], which maximizes the chance that a thread gets CS in low-overhead lock-spinning phase and minimizes the chance that a thread gets CS during the high-overhead sleep phase. OCOR is realized by software and hardware co-design. In software, OCOR monitors the remaining times of retry (RTR) in a thread’s spinning phase, which reﬂects in how long the thread must enter the high-overhead sleep phase. 4We excluded blackscholes and swaptions, as the former contains merely barrier synchronization and the latter only one CS. 5For legibility, we use short names to denote applications with long names, in which body is short for bodytrack, can for canneal, face for facesim, ﬂuid for ﬂuidanimate, freq for freqmine, and stream for streamcluster. 20       0 50 100 150 200 12 10 8 6 4 2 0 y d o b n a c p u d e d f e c a f e r r e t l f d u i r f q e r t s a e m p v i s 4 6 2 x m d b w e v a s b a n b t 1 3 3 o b a s t o b s s t l i d b c f m d 3 a s m w i i m g a m g r i d u p p a l s m i t h w a d k r t e e PARSEC OMP2012 P C U c y c l e s C r i t i u e o h a c l s t s c i d n n o a t ) i m e s t ( Total CS times Average CS cycles (a) Total CS times and the average CPU cycles of each CS. 1.4 1.2 1 0.8 0.6 0.4 0.2 0 s w i m p v i s d k r t e e r f q e m d i m g a 4 6 2 x o b s t a b w e v a s y d o b f m d 3 a l f d u i r t s a e m u p p a l f e r r e t o b s s t m g r i d n a c s m i t h w a l i d b c f e c a b t 1 3 3 p u d e d b a n P C U c y c l e s ( m i l l i n o ) COH CSE Group 3 Group 1 Group 2 (b) Total CS CPU cycles breakdown. Figure 8: Total CS times, average CPU cycle of each CS, and total CS execution time breakdown. In hardware, OCOR integrates the RTR information into the packets of SWAP requests, and let the NoC routers prioritize SWAP request packets according to the RTR information. The principle is that the smaller RTR a SWAP request packet carries, the higher priority it gets and thus quicker delivery. However, when a thread is already in the sleep mode, it should access CS at a later time, since waking the thread up will introduce considerable overhead. It is thus preferable to allow threads in the low-overhead spinning phase to win. Besides, program progress information is embedded in request packets to avoid starvation for low-priority requests. Following [40], OCOR conﬁguration is shown in Table 1. Case 3 is our iNPG. In iNPG, we follow the NoC architecture introduced in Figure 3, in which a big router interleaves with a normal router. Case 4 is iNPG+OCOR, which combines iNPG with OCOR. In this case, all routers support OCOR. 5.2 Experimental results 5.2.1 Benchmark CS characteristics Figure 8 reports the CS characteristics of the 10 PARSEC and the 14 OMP2012 programs on the target 64-core manycore. Figure 8a gives the total CS access times and average CPU cycles per CS for each program. Since tasks in each program vary, both metrics differ from program to program. For example, in application ﬂuid of PARSEC, critical sections (secured by pthread_mutex_lock) are used to synchronize indexes of liquid particles. Although each CS takes limited CPU cycles to ﬁnish (average 81.47 CPU cycles), the total number of critical sections is however high (10,240 times in total). In contrast, in application imag of OMP2012, critical sections (secured by #pragma omp critical) are used to atomically modify an image. Although the total number of critical sections is smaller (4,000 times in total), each CS performs relatively heavy tasks and thus takes more CPU cycles (average 179 CPU cycles) to ﬁnish. Figure 8b breaks the total CS execution time (CS times × average cycles per CS) of each application into competition overhead (COH) and the CS execution time itself (CSE). It is evident that compared to CSE, COH contributes more signiﬁcantly to application runtime. In the ﬁgure, we sort applications according to the total CS execution time (COH+CSE) in the ascending order and divide them into three groups, with Group 1 (6 programs) featuring lower, Group 2 (12 programs) medium, and Group 3 (6 programs) higher total CS execution time. 5.2.2 Application execution timing proﬁle To look into the details and impact of CS entry and access, we proﬁle the program execution timing in Figure 9 with program freqmine for the four comparative cases. For clarity, we show the execution results of 30,000 CPU cycles of the ﬁrst 8 threads in freqmine, where we divide the program execution timing diagram into three phases. (1) Parallel phase, where threads perform concurrent computation tasks; (2) COH phase, where threads compete with each other to enter the next critical section; (3) CSE phase, where threads execute code in critical sections. The Original application execution timing proﬁle is shown in Figure 9a. As illustrated, 62.1% of the CPU cycles are spent in the parallel phase, with 28.3% in COH and 9.6% in CSE. Moreover, 78 critical sections are completed during the reported 30,000 CPU cycles. However, with OCOR, COH across different threads is signiﬁcantly reduced. As shown in Figure 9b, 69.8% of the CPU cycles are spent in the parallel phase, with 19.8% in COH and 10.4% in CSE. Moreover, 92 critical sections are completed during the reported 30,000 CPU cycles, achieving 17.9% application progress improvement than Original. This is because more threads secure the critical section in the low-overhead spinning phase instead of the high overhead sleep phase. Further, with iNPG, COH is also remarkably reduced. As shown in Figure 9c, 73.0% of the CPU cycles are spent in the parallel phase, with 17.0% in COH and 10.0% in CSE. Moreover, 96 critical sections are completed during the reported 30,000 CPU cycles, achieving 23.1% application progress improvement than Original. This is because invalidations to failing or later arriving SWAP requests are sent from big routers instead of from the home node. In this way, the losing threads get early invalidations so that the application avoids heavy coherence trafﬁc latency. Finally, with iNPG+OCOR in which both CS grant order and the coherence trafﬁc latency are optimized, COH is further reduced than with iNPG or OCOR alone. As drawn in Figure 9d, 80.1% of the CPU cycles are spent in the parallel phase, with 9.0% in COH and 10.9% in CSE. Moreover, 104 critical sections are completed during the reported 30,000 CPU cycles, achieving 33.3% progress improvement than Original. 5.2.3 Analysis on LCO reduction To reveal the effects of iNPG on reducing lock coherence overhead (LCO), Figure 10 compares the average and histogram of coherence packet round-trip delay for Original and 21             8 7 6 5 4 3 2 1 8 7 6 5 4 3 2 1 0 0 8 7 6 5 4 3 2 1 8 7 6 5 4 3 2 1 0 0 Parallel phase COH (a) Original. CSE 30000 Parallel phase COH (c) iNPG. CSE 30000 Parallel phase COH (b) OCOR. CSE 30000 Parallel phase COH (d) iNPG+OCOR. CSE 30000 Figure 9: Comparison of the execution timing proﬁles of program freqmine with the four comparison mechanisms. Each thread execution is divided into three phases: parallel phase, COH phase, and CSE phase. (a) Avg. coh. pkt. r-trip delay (b) Coh. pkt. r-trip delay hist. (c) Avg. coh. pkt. r-trip delay (d) Coh. pkt. r-trip delay hist. Figure 10: Average coherence packet round-trip delay and coherence packet round-trip delay histogram comparisons for Original (Figure (a), (b)) and iNPG (Figure (c), (d)) with benchmark freqmine. on the distance between the home node and the competing iNPG in benchmark freqmine. The results are obtained for a scenario where all 64 threads compete for the lock variable threads. This is because in iNPG, invalidation and the corthat is hosted at the shared L2 cache of core (5,6). The measresponding acknowledgement can be done with distributed urement starts at the time when all threads begin to compete big routers instead of aggregately at the home node. In this for the critical section and ends at the time when the last way, when a GetX request loses arbitration, the L1 cache in its issuing thread will be invalidated by the nearest big thread gets its critical section. router instead of the home node. Figure 10d further shows Figure 10a shows the average coherence Invalidation-Acknowledgement (Inv-Ack) round-trip delay between the home the corresponding coherence Inv-Ack round-trip delay hisnode and all competing threads in Original. Without iNPG, togram. Compared to Figure 10b, the maximum coherence the home node holding the critical section lock is solely repacket round-trip delay is reduced to 15 CPU cycles, where sponsible for all the coherence invalidations, with the winthe “long tail” delay is eliminated. The average coherence packet round-trip delay is decreased from 39.2 to 9.5 cycles. ning thread responsible for collecting the acknowledgements from failing threads. Thus, depending on the distance between a core and the home node, cores near the home node enjoy low coherence overhead (since it can be invalidated at an early time), while cores that are farther away from the home node suffers from higher coherence overhead. Figure 10b further shows the corresponding coherence Inv-Ack round-trip delay histogram. The maximum coherence packet round-trip delay reaches 97 CPU cycles, exhibiting a “long tail” on the delay histogram, dominating the coherence completion time among the home node and competing threads. Figure 10c shows the effects of iNPG in reducing the average coherence Inv-Ack round-trip delay. Applying iNPG, the coherence Inv-Ack round-trip delay has less dependence Figure 11 shows the overall critical section (including both COH and CSE) expedition results of the four mechanisms. We normalize the results obtained from Original to 1 in each benchmark. We can observe that the critical section expedition results are proportional to the total CPU cycles that an application spends in critical sections as illustrated in Figure 8b. This is because the more CPU cycles that a program spends in critical sections, the more opportunity is opened for iNPG and OCOR to achieve higher competition overhead reduction. As shown in Figure 11, across Group 1 applications, critical sections are averagely expedited by 1.2× in 5.2.4 CS and application ﬁnish time reduction 22 t n e m e v o r p m i S C e v i t l a e R Group 1 Original OCOR iNPG Group 2 iNPG+OCOR Group 3 6x 5x 4x 3x 2x 1x 0x Figure 11: Critical section expedition results achieved by the four comparison mechanisms. Parallel phase Original OCOR iNPG iNPG+OCOR Group 1 Group 2 Group 3 120% 100% 80% 60% 40% 20% 0% e g a t n e c r e P Figure 12: Application ROI ﬁnish time achieved by the four comparison mechanisms. OCOR, 1.4× in iNPG, and 1.8× in iNPG+OCOR. Across the average ROI ﬁnish time to 75.3% by 24.7%, exhibiting the highest ROI ﬁnish time reduction. Compare iNPG over Group 2 applications, average critical section expedition is further increased to 1.5× in OCOR, 1.9× in iNPG, and 2.5× OCOR, iNPG improves the ROI ﬁnish time by 7.8% on average and 14.7% at maximum (with program bt331). in iNPG+OCOR. Across Group 3 applications, critical sections are further expedited by 1.6× in OCOR, 2.7× in iNPG, We can observe from Figure 11 and Figure 12 that the and 4.0× in iNPG+OCOR, respectively. Across all 24 prooverall beneﬁt of iNPG+OCOR is not an accumulation of grams, OCOR expedites critical sections averagely by 1.45× beneﬁts from stand-alone iNPG and OCOR. This is because and maximumly by 1.90× (with program dedup). Further, in iNPG, LCO in the spinning phase is signiﬁcantly reduced, iNPG averagely expedites critical section by 1.98× and maxthus more threads can gain CS access without entering into imumly by 3.48× (with program nab). In iNPG+OCOR, the the sleep phase. Since OCOR also aims to have more threads enter CS in their spinning phase, when combined with iNPG, average and maximum critical section completion is expedited to 2.71× and 5.45× (with program nab), respectively. the opportunity for iNPG+OCOR is reduced. Compare iNPG over OCOR, iNPG effectively expedites critical section access by 1.35× on average and 2.03× at maximum (with program nab). Figure 12 shows the relative application ROI ﬁnish time of the four comparison mechanisms, in which we normalize the results obtained from Original to 100%. We can observe that all the four techniques have negligible effects on application parallel execution phase, where each thread executes parallel computation tasks and encounters no critical section. The ROI ﬁnish time reduction achieved by OCOR, iNPG and iNPG+OCOR correlates to the CS characteristics of each application. In Group 1 applications, compared with Original, the average ROI ﬁnish time is reduced by 6.5% with OCOR, 10.6% with iNPG, and 16.3% with iNPG+OCOR. Across Group 2 applications, the average ROI ﬁnish time is reduced by 12.6% with OCOR, 19.5% with iNPG, and 25.2% with iNPG+OCOR. In Group 3, the average ROI ﬁnish time is reduced by 17.3% in OCOR, 27.1% in iNPG, and 32.4% in iNPG+OCOR. Across all 24 programs, OCOR reduces the average ROI ﬁnish time to 87.7% by 12.3% compared to Original. iNPG decreases the average ROI ﬁnish time to 80.1% by 19.9%. Finally, iNPG+OCOR reduces We now explain and show the effectiveness of iNPG in reducing application ROI ﬁnish time with other locking primitives, including test-and-set (TAS) lock, the ticket lock (TTL) [31], array-based queuing lock (ABQL) [2, 16] and the MCS lock [14]. In TAS, each competing thread keeps spinning on the local CS lock variable until it successfully enters the critical section. Thus, TAS generates extensive read-modify-write operations because every time the lock is freed, each thread generates an exclusive access request among which only one will succeed, with all failed ones invalidated by the coherence protocol. The number of read-modify-write operation is signiﬁcantly reduced in TTL and ABQL. In both primitives, competing threads are served in the FIFO order. When a critical section is released, only one thread can issue a readmodify-write operation to enter the next critical section. The thread then takes the CS lock, and invalidates all CS locks in other threads’ caches. In the MCS lock, the lock contention trafﬁc is further reduced. Instead of polling on one unique iNPG’s effectiveness with other locking primitives 5.2.5 23     TAS TTL ABQL QSL MCS Figure 13: Application ROI ﬁnish time improvements with different locking primitives. CSE COH expedition with 4 BR COH expedition with 32 BR (This paper's choice) COH expedition with 0 BR (Original) COH expedition with 16 BR COH expedition with 64 BR 100% 80% 60% 40% 20% 0% 4x 3x 2x 1x 0x e g a t n e c r e P t n e m e v o r p m i S C e v i t l a e R c r e P t n e e g a 70% 60% 50% 40% 30% 20% 10% 0% 4 lock barriers and EI entries 16 lock barriers and EI entries 64 lock barriers and EI entries Figure 14: Critical section expedition results with different big router deployments. lock variable, in the MCS lock, different threads poll on its previous thread to access the next critical section. That is, each thread (except for the ﬁrst one) does not poll on a CS lock, but checks to see if its previous thread has ﬁnished its critical section execution. When a thread ﬁnishes the critical section, it directly notiﬁes its successor to enter the next critical section. Our experimental results are in accordance with the above analysis. Figure 13 compares the ROI ﬁnish time reduction achieved by iNPG with TAS, TTL, ABQL, QSL and the MCS lock. From the ﬁgure we can observe that 1) with different locking primitives, iNPG consistently reduces application ROI ﬁnish time across all benchmark programs. 2) The beneﬁts of iNPG differ with different locking primitives. In average, after applying iNPG, ROI ﬁnish time is reduced by 52.8% in TAS, by 33.4% in TTL, by 32.6% in ABQL, by 19.9% in QSL and by 16.5% in the MCS lock. This shows that with TAS, TTL and ABQL, iNPG achieves more effective results than with QSL and MCS, which impose less lock competition trafﬁc in the NoC. 5.2.7 4×4 2×2 8×8 10×10 16×16 Figure 15: iNPG’s effectiveness with different NoC dimensions and locking barrier table sizes. a “sweet spot” between big router number and COH expedition, and are thus used as the default big router deployment for experiments in the paper. Sensitivity to NoC dimension and number of entries in locking barrier table Figure 15 shows iNPG’s effectiveness on reducing avergrams with different NoC dimensions ranging from 2×2, age application ROI ﬁnish time across all benchmark pro4×4, to 8×8 (default setup in the paper), 16×16 cores. From the ﬁgure we can observe that with the NoC dimension increasing, iNPG brings more effectiveness on reducing application ROI ﬁnish time. This is because as the number of cores scales, more threads are involved in competing for the same critical section, and thus more critical LCO becomes. For example, in the 2×2 NoC, iNPG averagely reduces application ROI ﬁnish time by 4.7%, which is increased to 19.9% in the 8×8 NoC and 57.5% in the 16×16 NoC. Figure 15 also shows iNPG’s effectiveness with 4, 16 (default number in the paper) and 64 lock barriers and EI entries in the locking barrier table within a big router. From the ﬁgure we can observe that in 2×2 and 4×4 NoC, different size of locking barrier table brings marginal performance difference. However, in the 8×8, 10×10, and 16×16 NoC, 4 lock barriers and EI entries in the locking barrier table severely 5.2.6 Sensitivity to big router deployment Since iNPG relies on big routers to function, we investigate its sensitivity to big router deployment by varying the number of big routers from 0 to 4, 16, 32 and 64. We distribute all big routers evenly on the chip, where 0 big router is the Original setup and 32 big routers represent the case illustrated in Figure 3. Figure 14 reports CS expedition results (including both CSE and COH) normalized to Original. We can make the following two observations. First, as expected, across different benchmarks, iNPG signiﬁcantly expedites COH, with the CSE results remaining the same as in Original. Second, as the number of big routers increases, COH expedition is increased accordingly. However, the further COH expedition gains from 32 to 64 big routers are marginal. Therefore for the 64-core CMP, 32 big routers achieve 24     restricts iNPG’s performance. This is because as the NoC dimension scales up, reducing the locking barrier table size directly limits the capability of big routers in sending early invalidations, thus reduces the beneﬁt of iNPG. However, increasing the number of lock barriers and EI entries more than 16 does not proportionally increase iNPG’s performance. This is because each router in the NoC can only transfer a limited number of lock competing requests. Based on these observations, we choose 16 as the default number of lock barriers and EI entries in a locking barrier table within a big router. 6. RELATED WORK Analysis and identiﬁcation of critical sections and threads. Eyerman and Eeckhout analyzed Amdahl’s law with the notion of critical section in [12]. They developed an analytical model to show that mutual excluded critical section execution imposes a fundamental limit on parallel programs’ performance. In [5], Chen and Stenström proposed a critical lock analysis method for diagnosing critical section bottlenecks in multi-threaded applications. This method can reliably identify the critical sections that are critical for performance, and quantify the impact of such critical sections on the overall performance. In [11], based on the analysis of why multi-threaded workloads typically show sublinear speedup on multi-core chips, the concept of speedup stack was develop to quantify the impact of various scaling delimiters such as LLC and memory subsystem interference, lock spinning & yielding, workload imbalance, cache coherency, etc. Due to synchronization behavior, threads can be critical for program performance. In [8], the concept of thread criticality was developed and the thread criticality metric was deﬁned to take into account both a thread’s execution time and the number of concurrent waiting threads. Further, criticality stacks were designed to visually break down the total execution time into criticality-related components, facilitating detailed analysis of parallel imbalance. Combining synchronization networks. Combining synchronization networks can be catogorized into 1) networks that can combine concurrent accesses to the same memory location [15, 13, 32, 30] and 2) networks in which atomic fetch_and_add operations from different cores are combined [14, 25]. In [10], Eisley et al. propose to implement cache coherence protocols within the network to allow in-transit optimizations of read and write operations. To reduce hot-spot contention for synchronization variables, Yew et al. [41] have proposed a data structure known as a software combining tree. Like hardware combining in a multi-stage interconnection network [15], a software combining tree combines multiple accesses to the same shared variable into a single access. Recently, Hendler et al. [17] present ﬂat-combining, in which a combiner thread holding a global lock combines requests of all other competing threads. In [18], they apply the ﬂat-combining mechanism to develop a synchronous queue algorithm. The algorithm ﬁrst uses a single “combiner” thread to acquire a global lock and then serves other threads’ CS requests. To accelerate multi-threaded applications, both iNPG and combining synchronization networks exploit the opportunity that synchronization packets often pass through the same router. However, iNPG is fundamentally different from the concept of combining network such as [15, 13, 32, 30, 6]. In a combining network, the combining switches are able to recognize that two messages are directed to the same memory location and in such cases they can combine the two messages into a single one. However, iNPG seeks to invalidate shared cache copies of lock variables in an early stage of cache coherence protocol by generating new packets rather than combining the underlying synchronization operations. More importantly, combining network is invoked when two or more competing requests collide in-ﬂight in the router: they both try to arbitrate in the same cycle with one win and the others fail. As investigated in [13], even in a large scale system such as a 512-node NYU’s ultra-computer system, the occurrence of such scenario is still very low. However, in our iNPG, we do not require that two locking requests arbitrate at the same cycle at the same router. Instead, when the ﬁrst locking request travels through a big router, a temporary “barrier” is then set up to stop subsequent locking requests. In-network techniques for collective communication. In the context of providing efﬁcient 1-to-M and M-to-1 communications in NoCs, in-network techniques for packet forking and aggregating were developed. In [23], Krishna et al. proposed Flow Across Network Over Uncongested Trees (FANOUT) and Flow AggregatioN In-Network (FANIN) to realize efﬁcient 1-to-M forking and M-to-1 aggregation, respectively. On-chip routers were customized to support FANOUT and FANIN. At most routers along ﬂow path, packets incur only single-cycle delays, thus approaching an ideal network with only wire delay/energy. By using clockless repetitive wires on the datapath, FANOUT and FANIN were leveraged to SMART-FANOUT and SMART-FANIN [22] to enable forking and reduction, respectively, across multiple routers in a network dimension in a single cycle, thus providing a scalable collective communication scheme for NoCs. The above in-network techniques aim to optimize collective communication via optimized routers, which perform efﬁcient packet forking (like copy & paste) and merging but never create new packets. In contrast, our big routers generate new packets in the network, aiming to reduce COH. 7. CONCLUSION We have presented an iNPG technique to reduce lock coherence overhead (LCO) in serialized critical section access so as to expedite multi-threaded applications running on NoCbased cache-coherent many-cores. Based on the observation that LCO lies straightly on the critical path dominating the overhead for lock spinning, iNPG intends to shorten LCO by turning long-range centralized coherence trafﬁc into short-range distributed coherence trafﬁc. iNPG is enabled by “big” router which is a conventional router enhanced with active cache-coherence packet generation capability. Extensive full-system simulation results in Gem5 running PARSEC and SPEC OMP2012 with ﬁve different locking primitives (test-and-set lock TAS, the ticket lock TTL, array-based queuing lock ABQL, Mellor-Crummey and Scott MCS lock, and the default queue spin-lock QSL in Linux 4.2) show that iNPG achieves signiﬁcant improvements in COH and program runtime reduction over the state-of-the-art COH reduc25 tion technique OCOR [40]. Due to the nature of orthogonality, iNPG can be combined with OCOR to achieve the best improvements across the ﬁve locking primitives. 8. "
2020,A Deep Reinforcement Learning Framework for Architectural Exploration - A Routerless NoC Case Study.,,
2020,EquiNox - Equivalent NoC Injection Routers for Silicon Interposer-Based Throughput Processors.,"Throughput-oriented many-core processors demand highly efficient network-on-chip (NoC) architecture for data transferring. Recent advent of silicon interposer, stacked memory and 2.5D integration have further increased data transfer rate. This greatly intensifies traffic bottleneck in the NoC but, at the same time, also brings a significant new opportunity in utilizing wiring resources in the interposer. In this paper, we propose a novel concept called Equivalent Injection Routers (EIRs) which, together with interposer links, transform the few-to-many traffic pattern to many-to-many pattern, thus fundamentally solving the bottleneck problem. We have developed EquiNox as a design example. We utilize N-Queen and Monte Carlo Tree Search (MCTS) methods to help select EIRs by considering comprehensively from topological, architectural and physical aspects. Evaluation results show that, compared with prior work, the proposed EquiNox is able to reduce execution time by 23.5%, energy consumption by 18.9%, and EDP by 32.8%, under similar hardware cost.",
2020,Experiences with ML-Driven Design - A NoC Case Study.,"There has been a lot of recent interest in applying machine learning (ML) to the design of systems, which purports to aid human experts in extracting new insights leading to better systems. In this work, we share our experiences with applying ML to improve one aspect of networks-on-chips (NoC) to uncover new ideas and approaches, which eventually led us to a new arbitration scheme that is effective for NoCs under heavy contention. However, a significant amount of human effort and creativity was still needed to optimize just one aspect (arbitration) of what is only one component (the NoC) of the overall processor. This leads us to conclude that much work (and opportunity!) remains to be done in the area of ML-driven architecture design.",
