title,abstract,full_text
Binary de Bruijn on-chip network for a flexible multiprocessor LDPC decoder.,"This paper proposes a novel on-chip interconnection network adapted to a flexible multiprocessor LDPC decoder based on the de Bruijn network. The main characteristics of this network - including its logarithmic diameter, scalable aggregate bandwidth, and optimized routing technique- allow it to efficiently support the communication intensive nature of the application. We present a detailed hardware implementation of the routers and the network interfaces as well as the packet format and the routing algorithm. The latter is a parallelized version of the shortest path with deflection routing algorithm. In order to evaluate the performance of the proposed network, a generic RTL VHDL description has been developed and synthesized with CMOS STMicroelectronics 0.18 mum technology. The flexibility and the scalability of this on-chip communication network enable it to be used for any kind of LDPC code.","25.1
Binary de Bruijn on-Chip Network for a Flexible 
Multiprocessor LDPC Decoder 
Hazem Moussa  
Amer Baghdadi  
Michel Jézéquel 
Institut TELECOM; TELECOM Bretagne; CNRS Lab-STICC FRE 3167 
Technopôle Brest Iroise, 29238 Brest, FRANCE 
{firstname.lastname@telecom-bretagne.eu} 
ABSTRACT 
This paper proposes a novel on-chip interconnection network 
adapted to a flexible multiprocessor LDPC decoder based on the 
de Bruijn network. The main characteristics of this network –
including its logarithmic diameter, scalable aggregate bandwidth, 
and optimized routing technique– allow it to efficiently support the 
communication intensive nature of the application. We present a 
detailed hardware implementation of the routers and the network 
interfaces as well as the packet format and the routing algorithm. 
The latter is a parallelized version of the shortest path with 
deflection routing algorithm. In order to evaluate the performance 
of the proposed network, a generic RTL VHDL description has 
been developed and synthesized with CMOS STMicroelectronics 
0.18 µm technology. The flexibility and the scalability of this onchip communication network enable it to be used for any kind of 
LDPC code. 
Categories and Subject Descriptors 
C.1.2. [Processor Architectures]: Multiple Data Stream Architectures 
(Multiprocessors) – Interconnection architectures. C.2.1. [ComputerCommunication Networks] : Network Architecture and Design – Network 
communications, Network 
topology, Packet-switching networks. E.4. 
[Coding and Information Theory] : Error control codes. 
General Terms 
Algorithms, Design, Performance. 
Keywords 
Flexible LDPC Decoder, De Bruijn graph, Multiprocessor, NoC. 
1. INTRODUCTION 
Low-Density Parity-Check (LDPC) codes [1] are a powerful class 
of error correcting codes able to reach performance very close to 
the Shannon limit. Providing very high decoding throughput and 
communications performance, they are more and more proposed in 
vast number of upcoming standards like DVB-S2 [2], WiFi (IEEE 
802.11n) [3], and WiMax (IEEE 802.16e) [4]. 
LDPC codes are linear block codes whose parity check matrix H is 
sparse and where all the codewords satisfy the relation HcT = 0. 
The code can also be described by a bipartite graph, called Tanner 
graph [5], made up of two types of nodes: variable nodes (VNs) 
corresponding to the bits of a codeword and check nodes (CNs) 
corresponding to the parity constraints (Figure 1). An edge in the 
graph represents a one in the parity check matrix. 
check
check
nodes
nodes
var iable
var iable
nodes
nodes
Figure 1. Tanner graph of an LDPC code 
LDPC codes are divided into two classes: regular codes and 
irregular codes. The former are characterized by the fact that the 
number of ""1s"" in each line (respectively each column) of the 
parity check matrix is constant, whereas it is not the case for 
irregular codes. Thus, the number of edges connected to a node of 
the graph being defined as the degree of that node, the node degree 
will be identical for all the nodes in the case of a regular code, and 
different for an irregular one. 
The decoding process is done by an iterative message-passing 
algorithm called ""Belief Propagation Algorithm"". Each iteration is 
usually carried out in two steps: first, each VN sends a softinformation message on its probable value to all the adjacent CNs, 
then each CN returns a soft-information to qualify the likelihood 
of the considered variable according to the associated parity-check 
equation. Furthermore, 
to avoid probability multiplications, 
decoding is generally performed in the logarithmic domain with a 
Log-Likelihood Ratio (LLR) leading to the ""Sum-Product"" 
algorithm. Nevertheless, the decoder complexity can be reduced 
through sub-optimal decoding algorithms, where the number of 
messages is minimized. 
Finally, the iterations stop if the maximum number of iterations is 
reached or if a codeword is decoded. However, the computations 
being relatively simple, implementation issues mainly come from 
the communication structure between the VNs and CNs. Indeed, 
the communications rapidly become intensive because they depend 
on the number of nodes, the node degrees and the number of 
iterations. 
429
 
 
 
 
 
 
M Variable Node Processors
M Variable Node Processors
V1
V1
V 2
V 2
VM-1
VM-1
VM
VM
V1
V1
VNP
VNP
V2
V2
VNP
VNP
VM-1
VM-1
VNP
VNP
VM
VM
VNP
VNP
TANNER GRAPH EDGES
TANNER GRAPH EDGES
COMMUNICATION STRUCTURE
COMMUNICATION STRUCTURE
VNP
VNP
P 1
P 1
P2
P2
P N-1
P N-1
PN
PN
CNP
CNP
CNP
P1
P1
P1
CNP
CNP
P 2
P 2
CNP
CNP
PN -1
PN -1
CNP
CNP
P N
P N
N Check Node Processors
N Check Node Processors
VN_2_CN
VN_2_CN
MEMORY
MEMORY
CN_2_VN
CN_2_VN
MEMORY
MEMORY
M
M
O
O
R
R
CNP
CNP
(a) 
(b) 
Figure 2. (a) Generalized architecture of an LDPC decoder, (b) Serial LDPC decoder architecture 
For example, in the DVB-S2 standard, the codeword length can be 
up to 64800 bits and the decoder has to iterate 30 times with up to 
300000 data to exchange at each iteration. This huge quantity of 
data requires an efficient 
interconnection structure, which 
represents a challenge for the decoder implementation. 
Moreover, 
the codes defined 
for digital communication 
applications like DVB-S2 represent only a subset of the currently 
existing codes. Consequently, to be able to decode any type of 
present and future codes, a flexible decoder must be designed in 
order to support regular and irregular codes taking into account the 
variation of the parity check matrices, the codeword lengths and 
the code rates. 
In this paper, we propose an on-chip interconnection network 
supporting the intensive communications generated by a flexible 
LDPC decoder. The network is based on the de Bruijn graph [6] 
topology. We also describe a parallelized version of the shortest 
path with deflection routing algorithm guaranteeing that packets 
will cross routers in only one clock cycle. 
The remainder of the paper is organized as follows. Section 2 
presents a state of the art in LDPC decoder design and focuses 
particularly on partially-parallel implementations. Then section 3 
details the architecture of the proposed network, including 
topology, routing algorithm, packet format, routers and network 
interfaces. Section 4 presents the synthesis results and provides 
comparison analyses. Finally, section 5 concludes the paper. 
2. RELATED WORK ON LDPC DECODER 
DESIGN 
As the LDPC decoding algorithm is based on the Tanner graph, 
two types of processors are necessary: Variable Node Processors 
(VNP) and Check Node Processors (CNP). Thus, we can define 
the architecture of a decoder as being a system made up of VNPs 
and CNPs as well as an interconnection structure allowing the 
exchange of messages between processors. 
Mapping the Tanner graph on this architecture consists in: (1) 
associating one or more nodes (VN or CN) to one processor (VNP 
or CNP) and (2) implementing Tanner graph edges through a 
corresponding message addressing in the interconnection structure 
(Figure 2-a). 
Thus, three types of decoder hardware implementation exist: no 
parallelism or serial architectures where there is only one 
processor to treat the VNs and one processor for the CNs (Figure 
2-b), full parallelism where the number of processors is equal to 
the number of nodes (variable and check) (Figure 2-a), and partial 
parallelism where the number of processors for each of the two 
types of nodes is lower than the total number of nodes (Figure 2a). 
These three types of decoder hardware implementation can be 
compared in terms of area, throughput, and flexibility. Indeed, 
serial architectures have a minimum area cost with a complete 
flexibility but the throughput is very low. At the opposite, full 
parallel architectures have no flexibility, a maximum throughput, 
and the area cost is prohibitive. Finally, the partially-parallel 
architectures are in between because the area cost is reduced, and 
they offer high flexibility with very high throughputs. 
Consequently, the solution offering the best compromise in terms 
of area, throughput, and flexibility is the partially-parallel 
architecture. However, it introduces a problem of conflicts for 
access to the memories containing the exchanged messages (softinformation). 
This is why several solutions have been proposed in order to avoid 
or to alleviate the effects of conflicts. We can divide them into two 
approaches [7]: the first one is a joint design of LDPC codes and 
decoders to simplify the decoding, whereas the second approach 
does not impose any constraints in the construction of the code. 
2.1 Joint Code-Decoder Design 
Here, the parity check matrix of the codes is structured in order to 
simplify the interconnection between the VNs and CNs leading to 
a memory organization which avoids conflicts. 
For example, in [8], the authors build the parity check matrix 
starting from a macro-matrix made up of macro-rows and macrocolumns. Each sub-matrix is obtained on the basis of the identity 
matrix via row permutations under the constraint of not having 
more than one ""1"" in each column. The selected structure 
simplifies the decoder since the exchanged messages can be 
retrieved from memories with a simple addressing scheme (i.e. 
shift registers). Moreover, each macro-row can be processed 
independently and thus more macro-rows can be evaluated at the 
same time, which leads to a higher decoding throughput. 
In [9], a decoder based on the same approach as [8] is proposed, 
but it differs in the implementation of the decoder. The VNP and 
CNP exchange their messages by means of memories and via 
""router"" and ""inverse-router"" networks which are simplified by the 
code construction. Several other papers address the construction of 
codes. In particular, these codes have been selected, for example, 
for the DVB-S2 standard [10] since both encoder and decoder 
architectures are greatly simplified by their structure. 
2.2 Code Independent Decoder Design 
The decoders resulting from the joint code-decoder design 
approach are designed to support one class of LDPC codes. In 
430
 
 
 
 
 
 
order to handle the diverse existing and future LDPC codes, 
flexible architectures are mandatory. 
However, in this case, the memory access conflicts become more 
complex to avoid. Indeed, in the previous approach, avoiding these 
conflicts was done by the code construction. In the codeindependent approach, in order to alleviate the conflicts, a flexible 
interconnection network integrating an adequate routing algorithm 
is required. 
A solution based on a 2D-mesh network can be found in [11]. 
Regular and irregular codes with different block sizes and code 
rates are supported by using configuration memories. 
We can also find in [12] a solution based on an heterogeneous 
network (MDN) whose topology is a randomly generated graph 
with non-regular node degrees. However, this solution implies 
reductions in the achievable throughput, increasing the decoding 
latency. The implementation of the MDN network requires many 
resources, which must be taken into account in the decoder design 
because it leads to an increase in area and power consumption. 
Finally, in [13], the authors propose to apply the permutation 
network scheme of Tarable with a modified version of the belief 
propagation algorithm. The Tarable scheme ensures interleaving 
without conflicts in a flexible decoder thanks to a mapping 
function. Moreover, a Beneš network replaces the two original 
crossbars used for the permutation network because, with the new 
algorithm, only one scrambling is necessary and the data are read 
in the natural order. 
However, these network architectures remain limited in terms of 
scalability, area or latency. That is why the need for a scalable 
network, with an optimal diameter to reduce latency, that is not 
area costly and that can easily route the packets, led us to propose 
a de Bruijn network as the communication structure. 
3. BINARY DE BRUIJN NETWORK 
The de Bruijn network topology is based on the directed de Bruijn 
graph [6]. A de Bruijn digraph dB(k, m) consists of N=km nodes, 
where each node is represented as a vector of size m in the k-ary 
number system. For our network, we chose the binary system, thus 
the nodes are interconnected by unidirectional links according to 
the following rule: the node vmvm-1….v1 has its two output edges 
connected with nodes vm-1vm-2…v10 and vm-1vm-2…v11. Figure 3 
illustrates the case of a de Bruijn graph with 16 nodes. 
0
0
0
0
8
8
8
8
1
1
1
1
4
4
4
4
2
2
2
2
9
9
9
9
12
12
10
10
5
5
3
3
6
6
6
6
13
13
13
13
11
11
11
11
14
14
14
14
7
7
7
7
15
15
15
15
Figure 3. Binary de Bruijn graph with 16 nodes 
The choice of the de Bruijn network was, above all, based on its 
appropriateness to support the communication patterns generated 
by a flexible multiprocessor LDPC decoder. Flexibility implies 
supporting various LDPC codes and parameters, and thus various 
parity-check matrices and induced communication profiles. In fact, 
the structure of the de Bruijn network allows any N-to-N 
communication profile to be routed efficiently thanks to a reduced 
latency 
(maximum 
latency of 
log2N without deflection). 
Conventional topologies with an enhanced locality characteristic, 
suitable for typical embedded applications, are not adequate for the 
considered communication profiles. 
Moreover, 
thanks 
to de Bruijn 
topology path diversity, 
communication conflicts are alleviated because the conflicting 
packets can be deflected appropriately until they attain the targeted 
processor rather than being blocked or buffered. Binary de Bruijn 
topology with its two output edges nodes allows reducing the 
complexity of the obtained on-chip network while preserving the 
minimum required path diversity 
In addition, the binary de Bruijn network has a logarithmic optimal 
diameter equal to log2(N), with N the number of routers. Then, it is 
a direct network reducing the number of routers. Moreover, its 
recursive structure makes it highly scalable. Indeed, a graph of 
order m can be obtained by replacing every edge of the graph of 
order m-1 with a node, and then inserting a directed edge between 
pairs of nodes that corresponds to consecutively directed edges in 
the de Bruijn graph of order m-1. 
Finally, the widely-studied de Bruijn graph has been proposed as a 
communication network suitable for VLSI implementation [14]. 
NASA developed an 8192-processor de Bruijn network used by a 
Viterbi decoder for the Galileo space mission to Jupiter [15]. A 
generalized version of the de Bruijn graph has been recently 
proposed as a reliable network topology for Network on Chip 
design [16]. 
3.1 Network Architecture and Processor 
Mapping 
Figure 4 presents the schematic view of a complete de Bruijn 
network for N=8 processors. (m=3). Each processor is connected 
to a network interface and each network interface connected to a 
router. In this example, the node 4 or 100 has an edge to the node 
000 (or 0) and to the node 001 (or 1). 
RT
RT
NI
NI
P3
P3
RT
RT
NI
NI
P1
P1
P7
P7P7
NI
NI
RT
RT
RT
RT
NI
NI
P5
P5
RT
RT
NI
NI
P2
P2
RT
RT
NI
NI
P0
P0
P6
P6
NI
NI
RT
RT
P4
P4
NI
NI
RT
RT
Figure 4. Schematic view of an 8-processor network with 
processors, routers, and network interfaces 
The network topology construction is described in the above 
section. The architecture of the routers and the network interfaces 
are detailed in the following sections. The implemented routing 
algorithm is also specified as well as the strategy to manage the 
conflicting packets and the packet format. 
Concerning the mapping of LDPC nodes on the processors 
connected by the de Bruijn network, as the decoder architects are 
semi-parallel, the VNs and CNs are allocated to each processor 
according to the size of the soft-information memories, the code 
rate, and the nodes degree. Indeed, the code rate gives the 
information of the number of VNs and CNs. Moreover, because 
the sum of all the VNs degrees is equal to the sum of all the CNs 
degrees, so half of the processors can be allocated to process VNs 
and half to process CNs. However, each processor having a soft431
 
 
information memory which contains the messages of the nodes 
associated with the processor, so according to the considered 
memory size and the degree of each node, we can estimate the 
number of nodes which can be associated with each processor. 
Indeed, the sum of the node degrees to be allocated to one 
processor must be equal to (or lower than) the memory depth. 
Furthermore, the edges of the Tanner graph are mapped through 
the packet addressing. Indeed, the targeted processor identifier 
with the destination memory write address represent the Tanner 
edge between a VN (resp. CN) and a CN (resp. VN). 
3.2 Routing Algorithm and Packet Arbitration 
The shortest path routing algorithm [17] was chosen because of its 
ability to reduce latencies. However, the algorithm calculating the 
shortest path output being naturally sequential, the packet 
transmissions from the inputs to the outputs of the router can take 
between 1 and log2(N) clock cycles, N being the number of 
routers. 
Therefore, to bring the number of cycles back to one, we modified 
the algorithm of [17] by unrolling the “for” loop, and proposed 
another method to compute the shortest path output. Indeed, with 
the aim to compute the shortest path router output, we compare bit 
sub-sets of the current and destination router identifiers in order to 
choose one bit of the destination router identifier which will be 
used as the router output port number as shown in Table 1. With 
this parallelized version, all the comparisons can be done 
concurrently and thus, we can obtain the shortest path router 
output in only one clock cycle. 
Table 1. Parallelized version of the shortest path router output 
calculation 
computeShor testOutput (currRt, destRt) {
computeShor testOutput (currRt, destRt) {
cmp_1 = compare_eq_1(currRt[0],destRt[log2(N)-1]);
cmp_1 = compare_eq_1(currRt[0],destRt[log2(N)-1]);
cmp_2 = compare_eq_2(currRt[1:0],destRt[log2(N)-1:log2(N)-2]);
cmp_2 = compare_eq_2(currRt[1:0],destRt[log2(N)-1:log2(N)-2]);
. . .
. . .
cmp_log2N_1 = compare_eq_log2N_1(currRt[log2(N)-2:0],destRt[log2(N)-1:1]) ;
cmp_log2N_1 = compare_eq_log2N_1(currRt[log2(N)-2:0],destRt[log2(N)-1:1]) ;
if(cmp_log2N_1) {
if(cmp_log2N_1) {
return destRt[0];
return destRt[0];
}
}
. . .
. . .
} else if(cmp_2) {
} else if(cmp_2) {
return destRt[log2(N)-3];
return destRt[log2(N)-3];
} else if(cmp_1) {
} else if(cmp_1) {
return destRt[log2(N)-2];
return destRt[log2(N)-2];
} else {
} else {
return destRt[log2(N)-1];
return destRt[log2(N)-1];
}}
}}
a
a
a
a
p
p
a
a
p
p
o
o
c
c
r
r
l
l
l
l
o
o
e
e
a
a
l
l
t
t
m
m
r
r
r
r
s
s
o
o
c
c
m
m
a
a
e
e
o
o
d
d
o
o
a
a
c
c
n
n
s
s
b
b
n
n
a
a
i
i
t
t
r
r
r
r
i
i
l
l
r
r
t
t
In order to reduce the size of routers by avoiding packet buffering 
when two packets want to borrow the same output, the routing 
algorithm was supplemented by a mechanism deflecting one of the 
packets [17]. The most common way to choose the packet to be 
deflected is to use round-robin arbitration. We can improve this 
arbitration by deflecting the “youngest” packet. This requires 
storing “age” information (Time Stamp TS) within the packet 
updated at each hop. 
Moreover, in [18], the authors propose to improve the packet 
arbitration inside a parallel turbo decoder thanks to a defined 
priority criterion. This criterion represents the contribution of the 
packets to the convergence of the iterative process. Consequently, 
we also propose to apply this criterion for iterative LDPC 
decoding with the effect of adding a priority field in the packet 
header. The arbitration is then modified with, first, a priority 
comparison, followed, if necessary, by a TS comparison and then a 
switch to the round-robin scheme in the case of equality. 
3.3 Packet Format 
A packet is divided into two parts: the header and the payload. The 
header is composed of several fields: the identifier of the 
destination node on k bits, 2k being the number of routers in the 
network, a value indicating the number of cycles run since the 
entrance of the packet into network (TS) coded on 6 bits, and a 
priority field on 4 bits used with the preceding field in the 
arbitration of the conflicting packets inside a router. 
Concerning the payload, it is made up of the write address in the 
destination 
memory 
with 
a 
width 
of 
 + 1 bits and of the soft
)))
nbnode
(
ree
)(
p
×
(deg
max
(
log 2
p
processors
information (LLR) on 8 bits. 
3.4 Router 
Each of the input/output ports of a de Bruijn router has an 
associated control signal used to validate the data passing through 
the port. Moreover, there is an output signal corresponding to the 
read request of the emission FIFO contained in the network 
interface. Indeed, the packets from the processor are memorized in 
a FIFO until they can enter the network. 
The architecture of a router (Figure 5) is divided into two parts: the 
operative unit and the control unit. 
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
CONTROL UNIT
CONTROL UNIT
valid_from_rt_0
valid_from_rt_0
valid_from_rt_1
valid_from_rt_1
datain_from_rt_0
datain_from_rt_0
datain_from_rt_1
datain_from_rt_1
valid_to_rt_0
valid_to_rt_0
valid_to_rt_1
valid_to_rt_1
dataout_to_rt_0
dataout_to_rt_0
dataout_to_rt_1
dataout_to_rt_1
a
a
v
v
i
i
l
l
_
_
d
d
t
t
p
p
_
_
o
o
r
r
0
0
_
_
c
c
o
o
a
a
v
v
i
i
l
l
_
_
d
d
t
t
p
p
_
_
o
o
r
r
1
1
_
_
c
c
o
o
a
a
d
d
t
t
_
_
u
u
o
o
a
a
t
t
t
t
p
p
_
_
o
o
r
r
0
0
_
_
c
c
o
o
a
a
d
d
t
t
u
u
o
o
a
a
t
t
p
p
_
_
o
o
_
_
t
t
r
r
1
1
_
_
c
c
o
o
a
a
d
d
t
t
_
_
n
n
a
a
i
i
r
r
f
f
o
o
m
m
p
p
_
_
r
r
c
c
o
o
a
a
v
v
i
i
l
l
_
_
d
d
r
r
f
f
o
o
m
m
p
p
_
_
r
r
c
c
o
o
r
r
_
_
d
d
a
a
e
e
r
r
s
s
e
e
u
u
q
q
e
e
t
t
s
s
w
w
t
t
i
i
n
n
o
o
c
c
_
_
h
h
c
c
i
i
f
f
g
g
i
i
u
u
p
p
n
n
t
t
e
e
s
s
_
_
0
0
l
l
i
i
u
u
p
p
n
n
t
t
e
e
s
s
_
_
1
1
l
l
Figure 5. Router architecture 
The first one is composed of flip-flops and registers to synchronize 
the input/output ports. There are also multiplexers to implement 
the packet switching and the selection between the packets coming 
from upstream routers and those coming from the processor. 
Indeed, packets coming from upstream routers have highest 
priority. This arbitration technique, besides the Time Stamp 
information and the deflecting routing, enable to avoid deadlocks. 
The control unit generates the multiplexer selection signals as well 
as the validity signals according to the routing algorithm and 
conflicting packet arbitration. The latter is done by using a 
comparator for the priority field and a comparator for the TS field. 
Two multiplexers are thus necessary to choose the arbitration 
methods and another multiplexer to select the address field of the 
highest priority packet. 
432
 
 
 
 
 
 
 
According to shortest outputs calculation function, (log2(N) - 1) 
comparators with size 1 bit to (log2(N) - 1) bits between the 
current router identifier and that of the destination router of the 
highest priority packet are used in addition to a transcoder. 
3.5 Network Interface 
The network interface (Figure 6) manages the packet flow in both 
directions, from the processor to the network and vice versa.  
The interface contains a table for the construction of the packet 
header according to the parity check matrix of the code. Moreover, 
the packets are stored at the output of the interface in a FIFO, 
whereas at the input, two FIFOs memorize the packets coming 
from the network. However, as only one packet can leave the 
interface at a time, a multiplexer is required to choose between one 
of the two buffers. 
Furthermore, a basic flow control is implemented between a 
processor and its network interface by the means of the emission 
FIFO’s ""full"" signal. Indeed, the processor must wait until the 
FIFO is not full before again being able to emit the packets. 
C
C
n
n
o
o
r
r
t
t
o
o
l
l
U
U
n
n
t
t
i
i
S
S
F
F
.
.
.
.
M
M
.
.
S
S
F
F
.
.
.
.
M
M
.
.
S
S
F
F
.
.
.
.
M
M
.
.
F
F
F
F
I
I
O
O
F
F
F
F
I
I
O
O
F
F
F
F
I
I
O
O
Header Table
Header Table
f
f
u
u
l
l
l
l
a
a
d
d
t
t
_
_
a
a
r
r
f
f
o
o
m
m
p
p
_
_
r
r
c
c
o
o
a
a
v
v
i
i
l
l
_
_
d
d
r
r
f
f
o
o
m
m
p
p
_
_
r
r
c
c
o
o
a
a
v
v
i
i
l
l
_
_
d
d
t
t
e
e
n
n
_
_
o
o
t
t
w
w
a
a
d
d
t
t
_
_
a
a
t
t
e
e
n
n
_
_
o
o
t
t
w
w
r
r
_
_
d
d
r
r
s
s
q
q
t
t
a
a
d
d
t
t
_
_
a
a
r
r
f
f
o
o
m
m
e
e
n
n
_
_
t
t
w
w
0
0
_
_
a
a
d
d
t
t
_
_
a
a
r
r
f
f
o
o
m
m
e
e
n
n
_
_
t
t
w
w
1
1
_
_
w
w
n
n
e
e
a
a
d
d
t
t
_
_
a
a
t
t
p
p
_
_
o
o
r
r
c
c
o
o
w
w
r
r
d
d
d
d
a
a
_
_
r
r
s
s
s
s
e
e
a
a
v
v
i
i
l
l
_
_
d
d
r
r
f
f
o
o
m
m
e
e
n
n
_
_
t
t
w
w
0
0
_
_
a
a
v
v
i
i
l
l
_
_
d
d
r
r
f
f
o
o
m
m
e
e
n
n
_
_
t
t
w
w
1
1
_
_
Figure 6. Network interface 
4. SYNTHESIS AND RESULT ANALYSIS 
In order to evaluate the performance of our architecture, we 
developed a program in C language automatically generating a 
description in generic RTL VHDL of the complete system (routers 
and network interfaces). Then, we used ST 0.18 µm technology to 
synthesize the system with a priority field of 4 bits, a payload of 8 
bits and a TS of 6 bits. 
We compared the results obtained with other previously proposed 
LDPC Decoder architectures for a given codeword size around 
1024 bits and based on an interconnection network whose different 
topologies are: 2D-mesh [11], MDN [12] and Beneš [19]. They 
also differ at the level of design parameters: the degree of 
parallelism which represents the total number of node processors, 
the message quantization and the targeted technology. This is why 
we indicate in Table 2 for each network: its topology, the targeted 
technology, the area in mm², the system clock frequency in MHz, 
the estimated aggregate bandwidth in Gbps, the message size in 
bits, the degree of parallelism and the codeword size. The points of 
comparison are area, frequency, aggregate bandwidth, scalability, 
and flexibility. 
Table 2. Comparison ASIC synthesis results in terms of area, 
frequency and aggregate bandwidth 
[11] 
[12] 
[19] 
Proposed 
2D-mesh MDN Beneš de Bruijn 
0.16 
0.18 
0.13 
0.18 
25 
7.92 
0.98 
3.56 
500 
200 
245 
266 
400 
19.2 
62.72 
85.12 
32 
4 
8 
8 
25 
12 
32 
16 
1024,  
1024, 
1024, 
1024     
3/4 
1/2 
1/2 
1/4-7/8 
Ref 
Topology 
Technology (µm) 
Network Area (mm²) 
Decoder Freq. (MHz) 
Aggregate Bw (Gbps) 
Message size (bit) 
Degree of Parallelism 
Code 
Area, frequency and aggregate bandwidth are directly related to 
the targeted technology, the decoder's degree of parallelism, the 
message quantization and the complexity of the networking 
elements. Consequently, 
trying 
to 
compare 
the values 
quantitatively is difficult. Nevertheless, taking into consideration 
the results that we obtained in terms of area, frequency and 
aggregate bandwidth, we can conclude that the use of a de Bruijn 
network as an 
interconnection 
structure 
for a 
flexible 
multiprocessor LDPC decoder offers a very good trade-off 
between flexibility and performance (scalability, frequency, area). 
Indeed, we can use a comparison metric considering the ratio 
between aggregate bandwidth and occupied area. This combined 
metric measures the architecture efficiency which is almost the 
best for the de Bruijn network. 
Regarding scalability, the recursive structure of the Beneš, the 2Dmesh, and the de Bruijn networks makes them highly scalable. The 
MDN network scalability is poor because the network structure is 
not recursive and it is randomly built by a complex algorithm 
based on a constructive heuristic. 
Although flexible, the Beneš network [19] requires a precalculation related to the code to configure the switches. The MDN 
[12] uses a general approach to network design with random 
graphs as basic topologies. This network lacks flexibility because 
all permutation patterns must be simulated prior to their use in 
order to determine buffer size at design-time. Finally, the 2D-mesh 
network [11] is designed for a flexible decoder. However, the 
described network can only manage codes with a maximum 
codeword size of 1024 bits. Consequently, the de Bruijn network, 
allowing to support any LDPC code, constitutes the most flexible 
solution. 
Besides the above analyzed on-chip networks, other solutions exist 
using a barrel-shifter communication structure [10]. Although this 
structure is generally area efficient, the drawback with the barrel 
shifter concerns its flexibility where the decoder can process only 
a sub-class of LDPC codes based on specific permutation matrices. 
Moreover, the LDPC codes must be adapted in order to be 
decoded by barrel-shifter based architectures. 
To finish, Table 3 summarizes the complexity of the de Bruijn 
network for an unspecified N number of routers in terms of area, 
aggregate bandwidth and latency. 
The evolution of average latency taking into account deflections 
and according to the number of processors is illustrated in Figure 
11 (for a random traffic pattern and a network load of 10%). It 
clearly shows the logarithmic increase of latency with the number 
of processors. We thus pass from 4 cycles of average latency for 
an 8-processor network to 5 cycles (respectively 7, 10, 14, 19 and 
433
 
 
 
Draft. 
[5] Tanner, R. M., 1981. A recursive approach to low complexity 
codes. IEEE Trans. Inform. Theory, 533-547. 
[6] De Bruijn, N.G., 1946. A Combinatorial Problem. 
Koninklijke Nederlandse Akademie v. Wetenschappen 49, 
758–764. 
[7] Quaglio, F., 2006. Implementation of Architectures for High 
Performance 
and Flexible Decoders. Ph.D. Thesis, 
Politecnico di Torino, Italy. 
[8] Mansour, M. M., and Shanbhag, N. R., 2003. High 
Throughput LDPC Decoders. IEEE Trans. VLSI Syst., 976996. 
[9] Chen, Y., and Hocevar, D., 2003. A FPGA and ASIC 
implementation of rate 1/2, 8088-b irregular low density 
parity check decoder. 
in Global Telecommunications 
Conference, 113-117. 
[10] Brack, T., Kienle, F., and Wehn, N., 2006. Disclosing the 
LDPC Code Decoder Design Space. In Design, Automation 
and Test in Europe. 
[11] Theocharides, T., Link, G., Vijaykrishnan, N., and Irwin, M. 
J., 2005. Implementing LDPC Decoding on a Network-onChip. in Proc. of the International Conference on VLSI 
Design, 134-137. 
[12] Kienle, F., Thul, M. J., and Wehn, N., 2003. Implementation 
Issues of Scalable LDPC-Decoders. in Proc. 3rd International 
Symposium on Turbo Codes and Related Topics, Brest, 
France, 291–294. 
[13] Quaglio, F., Vacca, F., Castellano, C., Tarable, A., and 
Masera, G., 2006. Interconnection Framework for HighThroughput, Flexible LDPC Decoders. in Proc. Design, 
Automation and Test in Europe Conference and Exhibition. 
[14] Samatham, M. R., and Pradhan, D. K., 1989. The de Bruijn 
multiprocessor network: A versatile parallel processing and 
sorting network for VLSI. IEEE Transactions on Computers, 
567-581. 
[15] Rowley, R. A., Bose, B., 1993. Fault-tolerant ring embedding 
in de Bruijn networks. IEEE Transactions on Computers, 
1480-1486. 
[16] Hosseinabady, M., Kakaoee, M. R., Mathew, J., and Pradhan 
D. K., 2007. Reliable Network-on-Chip Based on Generalized 
de Bruijn Graph. in Proc. High Level Design Validation and 
Test Workshop, 3-10. 
[17] Francalanci, C., and Giacomazzi, P., 2006. High-Performance 
Self-Routing Algorithm for Multiprocessor Systems with 
Shuffle Interconnections. IEEE Transactions on Parallel and 
Distributed Systems, 38-50. 
[18] Moussa, H., Muller, O., Baghdadi, A., Jezequel, M., 2007. 
Butterfly 
and Benes-Based on-Chip Communication 
Networks for Multiprocessor Turbo Decoding. in Proc. of the 
conference on Design, Automation and Test in Europe. 
[19] Masera, G., Quaglio, F., and Vacca, F., 2007. Implementation 
of a Flexible LDPC Decoder. IEEE Transactions on Circuits 
and Systems, 542-546. 
22 cycles) for a network of 16 processors (respectively 32, 64, 
128, 256 and 512 processors). 
Table 3. Complexity estimation of the de Bruijn network 
in terms of area, aggregate bandwidth and latency 
De Bruijn 
266 MHz 
O(N) 
O(log2
2N) 
O(N x log2
2N) 
×
(deg
ree
)(
p
nbnode
(
p
9) x Freq x N 
Frequency 
# of RTs 
Area of one RT 
Total Area 
Aggregate 
Bandwidth (Gbps) 
max
log 2
(
(


processors
)))
+


Latency Min / Max 
w/o deflection (cyc.) 
Average Latency 
with deflection 
1 
log2N 
Figure 7 
Av e r a g e la te n cy v s . Netw o r k s iz e
25
20
15
10
5
0
)
.
c
y
c
(
y
c
n
e
t
a
l
e
g
a
r
e
v
A
0
100
200
300
400
500
N etw o rk s iz e (# proce ssors)
Figure 7. Variation of average latency according to the 
number of processors 
5. CONCLUSION 
In this paper, we have presented the architecture of a new on-chip 
interconnecti"
An area-efficient high-throughput hybrid interconnection network for single-chip parallel processing.,"Single-chip parallel processing requires high bandwidth between processors and on-chip memory modules. A recently proposed Mesh-of-Trees (MoT) network provides high through put and low latency at relatively high area cost.In this paper, we introduce a hybrid MoT-BF network that combines MoT network with the area efficient butterfly network. We prove that the hybrid network reduces MoT network's area cost. Cycle-accurate simulation and post-layout results all show that significant area reduction can be achieved with negligible performance degradation, when operating at same clock rate.","25.2
An Area-Efﬁcient High-Throughput Hybrid Interconnection
Network for Single-Chip Parallel Processing
Aydin O. Balkan
Gang Qu
Uzi Vishkin
University of Maryland, Depar tment of Electrical and Computer Engineering
{balkanay, gangqu, vishkin}@umd.edu
ABSTRACT
Single-chip parallel processing requires high bandwidth between processors and on-chip memory modules. A recently
proposed Mesh-of-Trees (MoT) network provides high throughput and low latency at relatively high area cost.In this paper,
we introduce a hybrid MoT-BF network that combines MoT
network with the area eﬃcient butterﬂy network. We prove
that the hybrid network reduces MoT network’s area cost.
Cycle-accurate simulation and post-layout results all show
that signiﬁcant area reduction can be achieved with negligible performance degradation, when operating at same clock
rate.
Categories and Subject Descriptors
B.4.3 [Interconnections (Subsystems)]: Topology; C.2.1
[Network Architecture and Design]: Packet-switching
Networks
General Terms
Design, Performance
Keywords
On-chip networks, Mesh-of-Trees, Hybrid networks
1.
INTRODUCTION
Some easy-to-program parallel processing approaches require high memory bandwidth to satisfy concurrent memory
requests from multiple processors on the same chip. Usually,
the global memory space is partitioned into multiple smaller
modules to allow concurrent accesses. An on-chip network
is required to interconnect parallel processors and multiple
memory modules, and convey memory requests and data between them. Studies have shown that traditional bus-based
networks will not be able to provide suﬃcient bandwidth [2],
and Network-on-Chip (NoC) architectures will soon replace
them as the number of on-chip processors increases rapidly.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2008, June 8–13, 2008, Anaheim, California, USA.
Copyright 2008 ACM ACM 978-1-60558-115-6/08/0006 ...$5.00.
Earlier studies [5] proposed a speciﬁc Mesh-of-Trees (MoT)
on-chip network that provides high performance (high throughput and low latency) for large amounts of parallelism with
high traﬃc rates. On average, MoT provides a throughput
up to 0.98 ﬂits per cycle on a 64-terminal network (2T bps
cumulative throughput with a 1GH z hypothetical clock and
32-bit ﬂits), much higher than butterﬂy and hypercube. However, the register area of MoT grows quadratically with number of network terminals, making it impractical for large
systems with many terminals.
In this study, we propose hybrid MoT networks called
MoT-BF, where we replace part of MoT network by butterﬂy (BF) networks of small scale. A BF network is area
eﬃcient, but it performs poorly under heavy traﬃc in terms
of throughput and latency, particularly when the number of
network terminals is large.
In a hybrid network, traﬃc is
diluted through MoT network; hence each mini-BF is subject to low traﬃc, mitigating the high traﬃc performance
loss of pure BF network. We conduct a comprehensive evaluation of the proposed hybrid MoT-BF network in terms of
area, latency and throughput. Mathematical analysis, cycleaccurate simulation and post-layout results all show that the
proposed hybrid MoT-BF network can signiﬁcantly reduce
the area cost of MoT network with negligible performance
degradation.
2. BACKGROUND AND RELATED WORK
We ﬁrst brieﬂy describe the underlying memory model.
Then, we summarize key features of the MoT network and
two butterﬂy networks that we will use for comparison.
2.1 A Memory Architecture for Single-Chip
Parallelism
Using multiple memory modules (or banks) has been a
common approach to increase memory bandwidth. In general, the global memory space is partitioned over the modules, and accesses to diﬀerent modules are handled concurrently. A universal hashing type approach can be used to
avoid pathological access patterns [1, 3, 10]. Figure 1 depicts
a Uniform Memory Access (UMA) type memory structure
used in a recent single-chip parallel architecture, which is
designed to optimize single-task completion time [14]. The
globally shared memory space is partitioned into multiple
memory modules, the same number as the number of processing clusters (PCs) Pi s on chip. Each memory module
consists of on-chip cache and oﬀ-chip memory portions. This
structure completely avoids cache coherence issues because
the processors do not have their private caches [14]. How435
P0
P1
Pn
Interconnection Network
L1
L2
L1
L2
L1
L2
Off−Chip
Mem.
Off−Chip
Mem.
Off−Chip
Mem.
Figure 1: Global memory is partitioned into modules (separated by dashed lines). Each module has
its own possibly multi-level on-chip caches (within
dotted lines) [5].
ever, this structure requires the connection between each PC
and each memory module. It signiﬁcantly increases performance demands of the interconnection network, particularly
when today’s single-chip multi-processor is pushing dozens
and even hundreds of processing clusters.
2.2 Mesh of Trees Network
The MoT network is designed to provide the needed bandwidth for UMA-type memory architectures such as the one
described in Section 2.1. NoC architectures that are built
with 2D-Mesh topology have O(√N ) bisection bandwidth
[7], where N is the number of PCs. Therefore, they cannot
eﬃciently support the expected traﬃc of this memory architecture. Other networks with O(N ) bisection bandwidth
such as butterﬂy, hypercube and fat trees will run into switch
complexity and packet contention issues, and will yield low
performances [5].
Figure 2 shows the MoT topology of [5] with four PCs and
four memory modules (MMs). Unlike earlier MoT topologies
of [8, 12, 13], PCs and MMs are placed at the roots of the
trees instead of the leaves.
s
r
e
t
s
u
l
C
g
n
i
s
s
e
c
o
r
P
0
1
2
3
0
1
2
3
0
1
2
3
s
e
l
u
d
o
M
y
r
o
m
e
M
(a) Processing Clusters
and Memory Modules
0
1
2
3
1
2
3
4
0
1
2
3
(b) Fan−out Trees
0
1
2
3
0
1
2
3
(c) Fan−in Trees
(d) Communication Paths for three
memory requests (0,2), (2,1), and (3,2)
Figure 2: Mesh of Trees with 4 Processing Clusters
and 4 Memory Modules [5].
The MoT network consists of two main structures: a set of
fan-out trees and a set of fan-in trees Figure 2(b) shows the
binary fan-out trees, where each PC is a root and connects
to two children, and each child has two children of their own.
The 16 leaf nodes also represent the leaf nodes in the binary
fan-in trees that have MMs as their roots (Figure 2(c)).
A MoT network that connects N PCs and N MMs has
logN levels of fanout and logN levels of fanin trees. A memory request packet travels from source root to one of the
leaves of the corresponding fanout tree. It passes to the leaf
of a corresponding fanin tree, and travels to the root of that
fanin tree to reach its destination (Figure 2(d)).
In the MoT network packets can compete with other packets from the same source while passing through fan-out trees;
and with other packets to the same destination while passing
through fan-in trees. It is guaranteed that, unless the memory access traﬃc is extremely unbalanced, packets between
diﬀerent sources and destinations will not interfere. Therefore, MoT network provides high average throughput that is
very close to its peak throughput. Furthermore, MoT consists of less complex switches compared to other networks,
and packets are routed without global scheduling. This allows higher operating frequencies and higher peak throughput. However, both fan-in and fan-out trees ave O(N 2 ) complexity and will take large area.
2.3 High-Performance Butterﬂy Networks
A binary butterﬂy network connects N = 2n terminals
as shown in Figure 3(a). 16 sources and destinations are
connected to each other through intermediate switch nodes.
Butterﬂy networks have been studied previously for singlechip parallel processing [20]. In general, butterﬂy networks
can have switches with k input and output ports (k = 2 in
Figure 3(a)). However, switch delay increases with increasing k [15].
Registers called virtual channels (VCs) can be used to
improve butterﬂy performance by increasing the amount of
hardware. VCs act as buﬀers for incoming data packets
that are stalled due to contention in later stages. A packet
is stored in a VC in the switch until an output port and
physical channel toward its destination becomes available
[6, 7].
There are several variants of butterﬂy networks. One
group of networks extend the regular butterﬂy vertically,
by adding parallel resources. Extra hardware provides additional bandwidth, reduces congestion and improves throughput. Examples of this approach include multi-butterﬂy [18],
dilated butterﬂy [11, 17], and replicated butterﬂy [9, 11].
Another group of networks extend the regular butterﬂy
horizontally, by adding extra stages. This approach adds alternative paths between sources and destinations, improves
traﬃc distribution in the network, and reduces congestion.
However, without additional bandwidth throughput improvement is limited.
Advantage of replicated butterﬂy (RBF) over virtual-channel
butterﬂy (VCBF) is that the logic delay of RBF switches
does not increase with increased hardware. In VCBF networks, additional VCs increase logic delay [15], and may
reduce system-wide clock frequency.
3. HYBRID MOT NETWORK
Earlier studies considered hybrid networks to optimize
network cost and performance. A notable example is the
Cube Connected Cycles (CCC) network [16], proposed to
optimize high switch degree of hypercube networks. CCC
network is built by replacing corners of a 3-dimensional
cube with a group of terminals that are interconnected by a
smaller ring network. This reduces the degree of each switch
436
 
 
0
1
2
3
4
5
6
7
BF
BF
BF
BF
BF
BF
BF
BF
BF
BF
BF
BF
(a)
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
From 0
From 1
To 0
To 1
To other fan−out nodes
From other fan−in nodes
Detail of MoT
BF
From 0
From 1
To 0
To 1
To other mini−BF
From other mini−BF
Detail of MoT−1−BF
(b)
(c)
(d)
Figure 3: (a) 8-terminal butterﬂy network with 2x2 BF switch primitives, connecting 8 sources (numbered
squares) and 8 destinations (numbered circles). (b) 8-Terminal MoT network. Small empty circles represent
fan-out tree nodes, small empty squares represent fan-in tree nodes. Paths from sources {0,1} to destinations
{0,1} are marked. (c) 8-Terminal MoT-1-BF network. 3 Innermost columns of MoT network are replaced
by mini-BF networks (black squares). (d) Details of the marked paths, showing MoT (top) to MoT-1-BF
(bottom) transition. BF box in (a) and (d) represents the butterﬂy primitive in Figure 4(d).
node of a CCC network from O(log N ) to O(1), where N is
the number of terminals.
We propose a hybrid MoT-BF network, where inner levels
of trees are replaced by mini-butterﬂy networks. We chose
BF network due to its proven area eﬃciency [7].
3.1 Network Architecture
In a regular MoT network with N PCs and N MMs, we
enumerate the levels of fan-out and fan-in trees by {0, 1, ..., log N −
1}, where the root node is at level 0, its children are at level
1, and so on. In a hybrid MoT-h-BF network, we replace
the h inner levels (levels numbered {log N − h, log N − h +
1, ... log N − 1}) of both fan-out and ﬁn-in trees by BF networks. We refer the number h ∈ {0, 1, 2, ... log N − 1, log N }
to as the hybridization level. The remaining fan-out and fanin trees both have log N − h levels. They are connected by
(N/2h )2 mini-BF networks with h stages or 2h terminals.
(see Figures 3(b) and 3(c) for an MoT-1-BF network with
N=8 terminals). Also note that pure MoT and pure BF networks can be represented as MoT-0-BF and MoT-log N -BF,
respectively.
The main drawback of BF network is its poor performance
(low throughput and high latency) at high traﬃc rate [5].
The proposed hybrid MoT-h-BF network reduces the trafﬁc through mini-BF networks by the fan-out trees. Each
root of the fan-out tree in the MoT-h-BF network will have
2log N −h = N/2h leaves. If λ is the amount of uniform traﬃc
in terms of ﬂits per cycle that enters the root of fan-out tree,
each input to the mini-BF networks, which is the leaf of the
fan-out tree, will have a reduced traﬃc rate of 2h λ/N in average, which is 0.25λ for the MoT-1-BF network with N=8
in Figure 3(c). This will signiﬁcantly reduce the congestion
and performance loss in BF networks at high traﬃc rates.
3.2 Switch Primitives
The original MoT tree [5] is built with three primitives
(Figure 4(a, b, c)). The fan-out tree primitive performs a
routing operation by directing an incoming ﬂit to one of
the two outputs. A fan-in tree primitive performs an arbitration between two incoming ﬂits and sends the winner to
the next stage. Finally, a pipeline primitive is used to cut
long wires in shorter segments if necessary. An additional
butterﬂy primitive (Figure 4(d)) is used for building the hybrid MoT-BF network. Each primitive consists of 2 clocked
b-bit1 registers per input channel, several mux and demux
and control logic that handles routing and arbitration. In
an empty network, a packet spends one clock cycle in each
primitive. In case of contention and stalls, proper backward
signaling and using the second b−bit buﬀer prevents overwriting stalled data.
4. EVALUATION
We evaluate the proposed hybrid MoT-BF network in ﬁve
categories, register count, minimum latency, throughputarea trade-oﬀ, network latency at diﬀerent traﬃc rates, and
post-layout throughput. We compare the proposed network
to MoT, replicated BF, and virtual-channel BF networks.
4.1 Register Count
Modern VLSI processes can provide almost up to 10 metal
layers, and this number increases every few generations.
Earlier layout-accurate studies [4] suggest that, consequently,
wire complexity becomes a secondary concern at least for
reasonably small scale networks, such as 64 terminals. The
network area is dominated by the data registers. Therefore,
we measure register count of networks, which is directly related to the area cost.
In typical virtual-channel routing switches [7], there are
v virtual channels per input and output port to improve
performance. Each virtual channel uses at least one b-bit
register for one data packet. In MoT, RBF and MoT-h-BF
networks, each switch primitive has either one or two input
and output ports and no virtual channels. In all types of
switches, the control circuit consumes negligible area compared to data registers.
Mesh of Trees A MoT network consists of N fan-out
and N fan-in trees, each with (N − 1) nodes. The leaves
do not contain switching circuits, since they are only wire
connections. Using the primitive circuits of [5], the total
number of b-bit registers is R = 6N (N − 1) = O(N 2 ).
Virtual-Channel Butterﬂy Switches of butterﬂy network have a total of 2 · N log N input and output ports with
1 b is the number of bits in the data path, typically 32.
437
in
B0
B1
read
write
destination
select
req
ks
Control
out0
ks0
ks1
out1
B00
B0
B00
in0
ks0
write0
B01
read0
ks1
write1
B10
read1
req0
Control
req1
in1
select
in
req
ks_out
Control
out
ks
out
in0
req0
out0
ks0
write0
B01
read0
select0
ks_in0
ks1
write1
B10
read1
select1
Control
req1
ks_in
in1
ks_in1
out1
write
B1
read
B11
B11
(a) Fan-out tree primitive
(b) Arbitration primitive
(c) Pipeline primitive
(d) Butterﬂy primitive
Figure 4: Switch primitives for MoT and MoT-BF networks. Data paths are marked with thick lines. Control
paths are simpliﬁed. (a) Fan-Out tree primitive: One input channel, two output channels; (b) Fan-In tree
(arbitration) primitive: Two input channels, one output channel; (c) Pipeline primitive: One input channel,
one output channel. (d) Butterﬂy primitive: Two input channels, two output channels; Signals: req : Request;
ks : Kill-and-Switch; write /read : Write and Read pointers; B : Storage Buﬀer; select :Result of Arbitration;
destination : Destination address
v virtual channels each. Then, the number of registers becomes R = 2 · v · N log N = O(vN log N ).
Replicated Butterﬂy Replicated butterﬂy switches have
two registers per input, and no virtual channels. The network consists of r copies of a regular butterﬂy, and binary
trees between the network and source/destination modules
In total, they have R = 6 · N (r − 1) + 2 · r · N log N =
O(rN log N ) registers.
Hybrid MoT-BF A MoT-h-BF network with N terminals has N fan-out and fan-in trees, with log N − h levels. Additionally, there are (N/2h )2 mini-BF networks with
h stages. BF primitives have two registers per input, and
no virtual channels. As a result, a MoT-h-BF network has
R = 6N (N/2h − 1)+ (N/2h )2 · 2h · 2h = O(hN 2 /2h ) registers.
Table 1 compares register counts of MoT-BF and pure
MoT networks for small number of terminals, up to N = 64.
A 64-terminal MoT-1-BF network has approximately 34%
less registers compared to pure MoT.
It is also important to observe the asymptotical behavior
of register count. Since h varies between 0 and log N , the
number of registers for MoT-h-BF network is asymptotically
upper bounded by pure MoT network; and asymptotically
lower bounded by pure BF network. For example, if h =
log N/2, then R = O(N √N log N ).
N
8
16
MoT
336
1440
MoT-1-BF 0.62
0.64
MoT-2-BF 0.33
0.38
MoT-3-BF 0.14
0.20
64
24192
0.66
0.41
0.24
32
5952
0.66
0.40
0.23
Table 1: Register count of some hybrid MoT-BF
networks normalized to MoT with same number of
terminals.
4.2 Minimum Latency
Minimum latency is the time in clock cycles, for a packet
to travel from source to destination through an empty network. Usually it is averaged over all source-destination pairs,
however it does not vary between such pairs in any of the
considered networks.
Mesh of Trees A packet travels log N stages in the fanout tree, and log N stages in the fan-in tree. Each stage
takes one clock cycle [5]. The overall latency is L = 2 log N .
Virtual-Channel Butterﬂy The butterﬂy network has
log N stages of switch nodes. A packet takes three cycles to
pass through a regular virtual-channel switch, or at least two
cycles to pass through a speculative virtual-channel switch
[15]. Assuming regular switches, the minimum latency of a
438
virtual-channel butterﬂy is L = 3 log N .
Replicated Butterﬂy In a replicated butterﬂy network
with r copies, the packets travel through a log r stage trees
before and after the butterﬂy. Assuming single-cycle switches,
the minimum latency is L = 2 log r + log N .
Hybrid MoT-BF In a MoT-h-BF network, packets pass
through log N − h levels of fan-out and fan-in trees before
and after h level butterﬂy. With single-cycle switches, the
minimum latency is L = 2 log N − h.
As h is limited between 0 and log N , the minimum latency
of MoT-h-BF network varies between log N and 2 log N for
diﬀerent hybridization levels.
4.3 Throughput-Area Trade-off
We evaluated maximum throughput of each network by
cycle-accurate simulations. For virtual-channel butterﬂy network, we used the simulator of [7]. For other networks, we
use the simulator of [5].
In order to evaluate the maximum throughput provided
by each network model, we assume the maximum packet
generation rate of one ﬂit per cycle at each input port of the
network2 . At this generation rate, the network will saturate
with packets, and the injection and delivery rates will come
to balance at the maximum throughput. We assume uniform
traﬃc pattern, which is expected for the memory architecture described in Section 2.1. Uniform traﬃc pattern is a
reasonable assumption due to the use of hashing mechanism,
which has an eﬀect similar to randomization that distributes
the memory accesses evenly among modules [1, 3, 10].
We simulated networks for N = 8, 16, 32 and 64 (see Figure 5 for N = 8 and N = 64). For each network size, we
tuned the throughput by modifying the amount of registers,
which are directly related to area cost. Speciﬁcally, we modiﬁed number of virtual channels v in virtual-channel butterﬂy, and number of copies r in replicated butterﬂy networks.
As expected, we see that the maximum throughput increases
for each network as the number of registers increases, Hybrid
MoT-BF network outperforms both BF networks.
4.4 Latency and Throughput vs. Trafﬁc
As network traﬃc increases, packets will experience longer
latencies, and network throughput will saturate. We use
a Bernoulli model to generate packets [7], with generation
rates varying from 0.1 to 1.0 ﬂits per cycle per port. The net2Note that in several other studies of interconnection networks, long data packets may be divided into shorter units,
called ﬂits. In this network, each packet consists of a single
ﬂit.
0
200
400
600
800
Total number of registers
1000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
m
x
a
i
m
u
m
e
p
r
−
e
c
y
c
l
t
h
r
u
p
h
g
u
o
t
t
i
l
f
(
s
e
p
r
e
p
e
c
y
c
l
r
o
p
)
t
r
8 Terminal Networks
Hybrid MoT−BF
Butterfly with VCs
Replicated Butterfly
MoT
0
2
4
6
Total number of registers
8
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
m
x
a
i
m
u
m
e
p
r
−
e
c
y
c
l
t
h
r
u
p
h
g
u
o
t
t
i
l
f
(
s
e
p
r
e
p
e
c
y
c
l
r
o
p
)
t
r
64 Terminal Networks
Hybrid MoT−BF
Butterfly with VCs
Replicated Butterfly
MoT
v=2
v=4
v=8
r=1
r=8
h=3
h=0
Figure 5: Cost-performance comparison of diﬀerent network conﬁgurations. In each plot, upper left region
represents high performance and low area. On the curves, number of virtual channels for VCBF doubles
from left (v = 2) to right (v = N ). For RBF, the number of copies doubles from left (r = 1) to right (r = N ).
For MoT-h-BF, the hybridization level decreases from left (h = log N ) to right (h = 0).
work is warmed up until throughput saturates, then marked
packets are injected for latency measurement. We are particularly interested in the case when traﬃc rate, or the on-chip
parallelism is high.
Results are shown in Figure 6. At lower traﬃc rates networks with high hybridization levels have lower latency, because they have fewer stages. At higher traﬃc rates, packets start to interfere with each other. Networks with lower
hybridization levels perform better, and their throughput
saturates at higher traﬃc rates.
0
0.6
0.65
0.7
0.75
0.8
0.85
Input Traffic (flits per cycle per port)
0.9
0.95
1
20
40
60
80
100
a
L
t
y
c
n
e
(
s
e
c
y
c
l
)
MoT
MoT−1−BF
MoT−3−BF
MoT−5−BF
0.6
0.6
0.65
0.7
0.75
0.8
0.85
Input Traffic (flits per cycle per port)
0.9
0.95
1
0.7
0.8
0.9
1
h
T
r
u
p
h
g
u
o
t
t
i
l
f
(
s
e
p
r
e
c
y
c
l
e
p
r
o
p
)
t
r
MoT
MoT−1−BF
MoT−3−BF
MoT−5−BF
Figure 6: Latency and throughput of 64-terminal
hybrid networks as input traﬃc changes. There is
no notable diﬀerence among networks when traﬃc
rate is lower than 0.6 ﬂits per cycle. Pure MoT
results are also plotted for comparison.
4.5 Post-Layout Throughput
In general, throughput is measured in terms of Gigabits
per second (Gbps). This value is determined by number of
0
100
200
300
400
500
Total number of registers
600
700
800
0
20
40
60
80
100
120
140
160
180
200
220
A
e
v
r
e
g
a
u
c
m
a
u
l
i
t
e
v
t
h
r
u
p
h
g
u
o
t
(
G
s
p
b
)
8 Terminal Networks
Replicated Butterfly
MoT
MoT−h−BF
r=1
r=2
r=3
r=4
h=3
h=2
h=1
h=0
Figure 7: Post-layout throughput of MoT, replicated BF and MoT-h-BF networks. Each ﬂit is assumed to be 32-bits wide.
bits in a ﬂit, number of ﬂits delivered per cycle (per-cycle
throughput ), and clock rate. The number of bits in a ﬂit
usually depends on the width of the data path, and it depends on the environment, i.e. the parts of the system that
remain outside the network. We assume that this parameter
will remain constant with diﬀerent networks and conﬁgurations. Per-cycle throughput depends on network type and
architecture. It is usually measured through network simulations (Sections 4.3 and 4.4). The clock rate depends on
technology-speciﬁc parameters, network and router architecture, and physical layout of the network. Many earlier
studies of interconnection networks omit this component, or
make safe assumptions because VLSI issues such as wire delays could be neglected at older technologies. On the other
hand, some recent studies recognize the importance of the
issues and discuss clock rate as well [2].
439
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We create layouts for 8-terminal MoT, RBF and MoTh-BF networks in order to compute their highest clock frequency and layout-accurate throughput in terms of Gbps.
We use ARM regular-Vt standard cell library and IBM 90nm
(9SF) CMOS technology. Results are shown in Figure 7.
Clock rate of a pure MoT network is the highest among
all measured networks, because MoT primitives have shortest delays. Therefore, it has highest throughput in Figure 7.
BF primitives perform more complex operation, and this
increases their delay. Hybrid networks contain both faster
MoT primitives and slower BF primitives. Place and route
tools are capable of balancing the wire delays among these
primitives to optimize clock rate. As a result, the operation frequency of hybrid MoT-BF networks lies between
pure MoT and pure BF networks.
5. CONCLUSION
A hybrid network architecture incorporating mesh-of-trees
(MoT) and butterﬂy (BF) networks is presented. MoT provides superior performance with O(N 2 ) area cost, where
N is the number of network terminals. BF network provides poor performance with O(N log N ) area cost. We replaced inner levels of MoT network with mini-BF networks
to build the MoT-BF hybrid network. Based on our analysis, area cost of MoT-BF network lies between pure MoT
and BF networks. Under uniform traﬃc assumption, traﬃc
through mini-BF networks is diluted by preceding fan-out
trees. This reduces congestion and related performance loss
in mini-BF networks. Simulation results show that MoT-BF
performance is between MoT and BF networks up to 64 network terminals. Note that according to [19], each network
terminal can support up to 16 light-weight processor.
Operating at same clock rate, a 64-terminal MoT-1-BF
network gains 34% area by sacriﬁcing only 0.5% throughput
with respect to pure MoT network.
It also has approximately 2.5% higher throughput with respect to a RBF network with similar area. Earlier layout-accurate studies observed that the power consumption of MoT network grows
at the same rate as the number of cells in the layout [4].
A reduction in register area by hybridization is expected
to reduce power consumption accordingly. Hence, MoT-BF
hybrid networks support similar UMA-type memory architectures more cost-eﬀectively than pure MoT networks.
Combining simple MoT primitives with more complex BF
primitives allows place-and-route tools to balance wire delays accordingly. Resulting layouts on hybrid networks have
maximum clock frequencies between pure MoT and pure
BF networks. Post-layout throughput of 8-terminal MoT1-BF is 22% higher than a RBF network with comparable
area cost. Pure MoT network has much higher throughput,
mostly because of its higher clock rate.
We plan to study MoT-Ring hybrid networks by replacing
inner levels of MoT networks with mini-ring networks. Ring
network is less expensive compared to BF, but with weaker
performance. With proper tuning, ring traﬃc will be suﬃciently diluted, so that congestion and related performance
loss in mini-ring networks are reduced. A challenge in this
direction is preventing deadlocks. We are studying alternative deadlock-free structures instead of simple rings.
6. "
A reconfigurable routing algorithm for a fault-tolerant 2D-Mesh Network-on-Chip.,"In this paper we present a reconfigurable routing algorithm for a 2D-mesh network-on-chip (NoC) dedicated to fault- tolerant, massively parallel multi-processors systems on chip (MP2-SoC). The routing algorithm can be dynamically reconfigured, to adapt to the modification of the micro-network topology caused by a faulty router. This algorithm has been implemented in a reconfigurable version of the DSPIN micro-network, and evaluated from the point of view of performance (penalty on the network saturation threshold), and cost (extra silicon area occupied by the reconfigurable version of the router).","25.3
A Reconﬁgurable Routing Algorithm for a Fault-Tolerant
2D-Mesh Network-on-Chip
Zhen Zhang
zhen.zhang@lip6.fr
Alain Greiner
alain.greiner@lip6.fr
Sami Taktak
sami.taktak@lip6.fr
Univ Pierre et Marie Curie & LIP6-SOC
4, Place Jussieu, 75252 Paris, France
ABSTRACT
In this paper we present a reconﬁgurable routing algorithm
for a 2D-Mesh Network-on-Chip (NoC) dedicated to faulttolerant, Massively Parallel Multi-Processors Systems on Chip
(MP2-SoC). The routing algorithm can be dynamically reconﬁgured, to adapt to the modiﬁcation of the micro-network
topology caused by a faulty router. This algorithm has been implemented in a reconﬁgurable version of the DSPIN
micro-network, and evaluated from the point of view of performance (penalty on the network saturation threshold), and
cost (extra silicon area occupied by the reconﬁgurable version of the router).
Categories and Subject Descriptors
B.8.1 [PERFORMANCE AND RELIABILITY]: Reliability, Testing, and Fault-Tolerance; C.1.2 [PROCESSOR
ARCHITECTURES]: Multiprocessors—Interconnection architectures
General Terms
Design, Algorithms, Reliability
Keywords
2D-Mesh NoC, fault-tolerant, routing algorithm, MP2-SoC,
reconﬁguration, DSPIN
1.
INTRODUCTION
The Network-on-Chip (NoC) has been recognized to solve the bandwidth bottleneck, when interconnecting a huge
number of IP-cores in Massively Parallel Multi-Processors
Systems on Chip (MP2-SoC). According to the prediction
from an INTEL commentator in [10]: “within a decade we
wil l see 100 bil lion transistor chips”, namely, the NoC-based
MP2-SoC will integrate thousands of IP-cores. However, (cid:16)
bil lion of those transistors wil l fail in manufacture and a further 10 bil lion wil l fail in the ﬁrst year of operation”. Thus,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2008, June 8–13, 2008, Anaheim, California, USA.
Copyright 2008 ACM 978-1-60558-115-6/08/0006 ...$5.00.
a 20%-30% device failure rate means that the fault-tolerant
approach must be considered in MP2-SoC design.
As any MP2-SoC architecture will contain a large number
of replicated components, a simple fault-tolerant approach
is to deactivate the defective components (such as a processor core or an embedded RAM), once it has been detected
as a failure, and to map the software application on the remaining hardware components. Unfortunately, this simple
deactivation approach can’t deal with a faulty router in the
NoC itself. In order to save silicon area, and to minimize
the network latency, most NoCs use dedicated routing algorithms, taking advantage on the regular micro-network
topology. Once a router has failed, the deactivation of the
faulty router is not enough, as the micro-network topology
is modiﬁed and irregular. If the routing algorithm continues
to route the packets toward the faulty router, the micronetwork will block. Therefore, the routing algorithm must
be modiﬁed (reconﬁgured) to adapt to the modiﬁcation of
the micro-network topology. To realize such reconﬁguration,
it is necessary to solve 3 problems:
A The faulty router must be detected by an appropriate
built-in self-test mechanism.
B A fault-tolerant, distributed, reconﬁgurable routing algorithm, must be deﬁned, and implemented in the routers.
C A robust conﬁguration bus must be implemented in
the hardware to distribute the conﬁguration information to the routers.
In this paper, we address problem B, and we present a
fault-tolerant, distributed, reconﬁgurable routing algorithm
that can be used in any 2D-Mesh NoC. This algorithm has
been implemented in a reconﬁgurable version of the DSPIN
[7, 15] micro-network. Problems A and C are not addressed
in this paper.
Related results: During the last two decades, a lot of approaches have been published about fault-tolerant wormhole
routing algorithms (a review of wormhole routing algorithms
can be found in [14]) for 2D-Mesh network. These approaches could be split in two classes: the virtual channel (VC)
model and the turn model.
The VC-based fault-tolerant routing algorithms in [2–4,
13, 16] allow a single physical channel to be shared by multiple transactions, using some form of time multiplexing [5].
According to Duato’s results in [8, 9], a VC-based faulttolerant routing algorithm can route the packets around a
faulty router or a faulty region (including multiple faulty
routers) using diﬀerent VCs to avoid deadlocks. In a NoC,
the hardware cost of the router must be kept very low. We
441
believe that the high complexity and large area cost associated to the VC-based approach are not compatible with this
low-cost constraint.
The turn model is originated from Glass and Ni’s studies
in [11]. They published three adaptive routing algorithms:
West-First (WF), North-Last (NL) and Negative-First (NF).
These routing algorithms eliminate deadlocks without adding VC, by prohibiting some global turns as shown in Figure 1 (<A>, <B>, <C>). However, they can’t deal with some
one-faulty-router topologies as shown in Figure 1 (<D>).
<A>
<B>
<C>
D
S
S
S
<D>
Figure 1: Six global turns allowed (solid lines) and
two global turns prohibited (dotted line) by WF
(<A>), by NL (<B>) and by NF (<C>). The prohibited turn in these three routing algorithms prohibites also the packet from S to D (X is a faulty
router) as shown in <D>.
Glass and Ni proposed a turn-based fault-tolerant routing algorithm in [12] that is based on modiﬁcation of the
Negative-First routing algorithm. It can deal with any onefaulty-router topology. But each routing function depends
on the coordinates (Y,X) of the router, the packet destination, the input channels, and the size of the N × M mesh.
This last dependency is the weakest point, as the hardware complexity associated to this modiﬁed routing algorithm
depends on the mesh size. Moreover, this routing algorithm
copes with one faulty router, but cannot handle one faulty
region containing several faulty routers.
Finally, a fault-tolerant wormhole routing algorithm for a
2D-Mesh NoC must respect the following constraints:
Low cost
The hardware cost resulting from the reconﬁgurability must be a small percentage
of the total router silicon area.
The
reconﬁgurable
routing algorithm
must handle any one-faulty-router (or onefaulty-region) topology.
The reconﬁguration hardware must be independent on the 2D-Mesh size.
Dead lock free Any reconﬁgured routing algorithm must
be deadlock free.
Deterministic The resulting routing algorithm must guaranty the In-order delivery property
Generic
Scalable
Organization of the paper: The section 2 presents a
typical 2D-Mesh NoC: DSPIN. The section 3 explains the
principles of our reconﬁgurable routing algorithm. The section 4 presents a reconﬁgurable version of the DSPIN router,
and the section 5 contains experimental results related to the
performance (penalty on the network saturation threshold),
and to the hardware cost (extra silicon area occupied by the
reconﬁgurable version of the router).
by ST Microelectronics. It is a typical 2D-Mesh NoC, supporting MP2-SoCs architectures, and the GALS (Globally
Asynchronous Locally Synchronous) approach. Each subsystem is a synchronous domain. In the following, we call
“cluster” such a subsystem, containing one or several processor cores, a local interconnect, a network interface controller (NIC), and two routers. In order to avoid deadlocks in
commands/responses traﬃc, each cluster contains two independent routers (as shown in Figure 2 (<A>)) implementing
two separated subset-networks for commands and responses.
Each subset-network has a 2D-Mesh topology as shown in
Figure 2 (<B>).
Router
CMD
Router
RSP
NIC
Local Interconnect
......
PROC
0
PROC
N
Local
MEMs
<A>
Commands
<B>
Responses
Figure 2: <A> shows a generic DSPIN-based MP2SoC. In such a DSPIN-based MP2-SoC, a dotted
rectangle means a generic DSPIN-based MP2-SoC’s
cluster as shown in <B>.
Routing algorithm: DSPIN is a packet-switching network: a packet is broken into smallest ﬂow control unit called
ﬂits. The ﬁrst ﬂit is the head ﬂit and the last is the tail ﬂit.
The head ﬂit includes the destination “cluster address” deﬁned
in absolute coordinates Y and X. Once the head ﬂit of a
packet is received by a router, the destination ﬁeld is analyzed and the ﬂit is routed to the corresponding output port.
The rest of the packet is also routed to the same port until
the tail ﬂit.
DSPIN uses a determinist and deadlock free routing algorithm: X-First [6]. With this routing algorithm, the packets
are ﬁrstly routed on the X direction and then on the Y direction. The X-First routing function depends on the coordinates of the router (Y Local, X Local) and the coordinates
of the destination (Y Destination, X Destination) as presented in Listing 1.
Listing 1: The X-First routing function source code
in SystemC
i f ( X _ D e s t i n a t i o n > X _ L o c a l )
O u t = E A S T ;
e l s e i f ( X _ D e s t i n a t i o n < X _ L o c a l )
O u t = W E S T ;
e l s e i f ( Y _ D e s t i n a t i o n > Y _ L o c a l )
O u t = N O R T H ;
e l s e i f ( Y _ D e s t i n a t i o n < Y _ L o c a l )
O u t = S O U T H ;
e l s e
O u t = L O C A L ;
1
2
3
4
5
6
7
8
9
10
2. A TYPICAL 2D-MESH NOC ⇒ DSPIN
DSPIN & MP2-SoC: The DSPIN micro-network (Distributed Scalable Predictable Interconnect Network) was designed by the LIP6 laboratory and physically implemented
One important feature of the X-First routing algorithm
is the following: The packet path from a node (y , x) to the
node (y ′ , x′ ) is a Unique Path :
L = {R0 (y , x), ..., R|x′−x| (y , x′ ), ..., R|y′−y|+|x′−x| (y ′ , x′ )}
442
The router’s architecture: As shown in Figure 3, the
DSPIN router is composed of 5 modules (North, East, South,
West & Local), and the DSPIN router is not a full crossbar:
some interconnections between modules have been removed, according to the X-First routing algorithm (NORTH →
WEST, NORTH → EAST, SOUTH → WEST and SOUTH
→ EAST), reducing the complexity of the multiplexers in
the EAST and WEST modules.
NORTH
E
A
S
T
NORTH
(Y−1,X+1)
SOUTH
W
T
S
E
LOCAL
(Y−1,X)
(Y,X)
W
T
S
E
(Y,X+1)
Figure 3: A generic DSPIN router’s architecture.
The faulty router’s model: We make the assumption
that a faulty router can be detected by a dedicated build-in
self-test mechanism, which won’t be described in this paper.
Even if the DSPIN router is architecturally composed of 5
modules, when any module or interconnection of a router
is detected as faulty, the entire router will be considered as
faulty. Moreover, if one of the two routers in a cluster is detected as faulty, it breaks the commands/responses protocol.
So the other router of this cluster must be also considered
as faulty, the corresponding cluster must be considered as
a “hole” in the mesh. All components in the hole must be
deactivated. As the Unique Path has been broken by a hole, the dynamic reconﬁguration mechanism must restore the
broken Unique Path.
3. THE RECONFIGURABLE ROUTING ALGORITHM
The main idea of the deterministic, fault-tolerant,
distributed, reconﬁgurable routing algorithm is to route the packets through a cycle free contour surrounding a faulty router, so as to restore al l broken Unique
Paths.
Deﬁnition 1: Neighbors.
In a 2D-Mesh, a node (Y,X) has 4 direct neighboring nodes
(N,S,W,E) and 4 indirect neighboring nodes (NE,NW,SE,SW).
We call those 8 nodes the neighbors, as shown in Figure 4.
Deﬁnition 2: Natural contour.
The neighbors of a hole (Y,X) deﬁne a natural contour as
shown in Figure 4. It separates the network into two parts:
normal part A and defective part B.
Deﬁnition 3: Cycle free contour.
The location of a single hole has N × M possibilities in
a N × M 2D-Mesh. Thus, a natural contour has 9 possible
shapes corresponding to 9 types of locations: at each corner,
at each side and at other positions, as shown in Figure 5.
B
A
N
NW
NE
E
W
SW
S
SE
Figure 4: A generic hole’s neighbors and the natural
contour.
C2
C3
C1
C4
C5
C6
C7
C8
C9
S
SE
E
W
SW
S
SE
E
W
SW
S
N
NE
E
S
SE
NW
N
NE
W
SW
S
SE
E
NW
N
W
SW
S
N
NE
E
NW
N
NE
W
NW
N
W
E
Figure 5: 9 natural contours.
Following Dally’s condition in [6], the channel dependency
graphs (CDG) can be used to prove the 8 natural contours
(C1,...,C4,C6,..,C9) to be deadlock free. In a CDG, the nodes
are the communication channels (not the routers). There is
a directed edge from node (i) to node (j) when (i) is an input
channel for router R, (j) is an output channel for router R,
and the routing function associated to R deﬁnes a possible
path from (i) to (j). The natural contour C5 is NOT deadlock
free, as there is 2 cycles in C5’s CDG, as shown in Figure 6.
C9
C7
C8
C5
C4
C6
C1
C2
C3
E
SE
S
SW
W
S
SE
E
W
SW
S
S
E
N
NE
SE
N
NE
E
W
N
E
S
NE
NW
SW
SE
W
N
NE
E
NW
N
W
S
SW
NW
W
N
NW
Figure 6: The CDGs of 9 natural contours. 2 cycles
are found in the C5’s CDG, so C5 can introduce
deadlock.
We adopt a turn-based fault-tolerant approach to break
the 2 cycles in C5’s CDG, by prohibiting the two NE turns,
as shown in Figure 7. As a result, we deﬁned 9 cycle free con443
tours, corresponding to the 9 possible locations for a faulty
router.
As shown in Figure 9, a similar approach can be deﬁned
for the 8 others cycle free contours.
NW
N
NE
W
E
C5
SW
S
SE
Figure 7: The two turns prohibited (dotted line) in
C5’s NE can break the 2 cycles.
Deﬁnition 4: Modiﬁed routing function.
We deﬁne a modiﬁed routing algorithm for all routers that
are part of a cycle free contour around a faulty router, in order to restore all broken Unique Paths. In case of a faulty
router, we make the assumption that all components in the
cluster have been deactivated, and the faulty cluster is neither the source nor the destination of any packet. Therefore,
the 8 paths Li (corresponding to the X-First routing function) broken by this hole can be explicitly listed in Table 1.
As described in Figure 8, for each broken path Li, the modiﬁed routing function deﬁnes a new path N ewLi also listed
in Table 1.
Table 1: the 8 Li are restored by the 8 N ewLi
L1
N ewL1
L2
N ewL2
L3
N ewL3
L4
N ewL4
L5
N ewL5
L6
N ewL6
L7
N ewL7
L8
N ewL8
{RW , Rx , RN }
{RW , RNW , RN }
{RE , Rx , RN }
{RE , RSE , RS , RSW , RW , RNW , RN }
{RW , Rx , RS }
{RW , RSW , RS }
{RE , Rx , RS }
{RE , RSE , RS }
{RW , Rx , RE }
{RW , RSW , RS , RSE , RE }
{RE , Rx , RW }
{RE , RSE , RS , RSW , RW }
{RN , Rx , RS }
{RN , RNW , RW , RSW , RS }
{RS , Rx , RN }
{RS , RSW , RW , RNW , RN }
C5
NE
NL7
NL2
NL1
L2
N
L7
NW
L1
NL5
W
L5
NL6
L6
E
L3
L4
L8
S
SW
SE
NL3 NL4
NL8
E
W
E
W
S
SE
SW
S
SE
SW
S
C1
C3
C2
N
NE
NW
N
E
C4
C6
W
S
SE
SW
S
C7
C9
N
NE
NW
N
NE
NW
N
C8
E
W
E
W
Figure 9: The broken Li and N ewLi in other cycle
free contours.
4. HARDWARE IMPLEMENTATION
In a 2D-Mesh, a given router R can be in 9 diﬀerent situations: If none of the 8 neighboring routers is faulty, R
is conﬁgured as NORMAL, implementing the classical XFirst routing function. If one of the neighbors is faulty, R
is part of a cycle free contour, and must be conﬁgured accordingly (N OF x, S OF x, E OF x, W OF x, NE OF x,
NW OF x, SE OF x, SW OF x), implementing a modiﬁed
routing function. To implement the reconﬁgurable routing
algorithm, two main modiﬁcations have been introduced in
the DSPIN router micro-architecture:
• The interconnections NORTH → WEST, NORTH →
EAST, SOUTH → WEST and SOUTH → EAST must
be restored, and the multiplexers in the EAST and
WEST modules must have 4 inputs, as described in
Figure 10.
NORTH
(Y,X)
(Y,X+1)
T
S
E
W
E
A
S
T
T
S
E
W
SOUTH
LOCAL
NORTH
(Y−1,X)
(Y−1,X+1)
Figure 8: the 8 Li (dotted line) broken by a hole are
restored by the 8 N ewLi (solid lines).
Figure 10: A generic architecture of reconﬁgurable
DSPIN’s router.
444
• As there is 9 possible conﬁgurations for a given router,
the conﬁguration information must be stored in a 4
bits register, and the X-First routing function must be
modiﬁed to introduce a dependency on the value stored
in the conﬁguration register, as shown in Listing 2.
Listing 2: The X-First routing function modiﬁed by
the conﬁguration register (SystemC code)
1
i f ( X _ D e s t i n a t i o n > X _ L o c a l ) {
i f ( R E G I S T E R = = N E _ O F _ x | |
R E G I S T E R = = E _ O F _ x
| |
R E G I S T E R = = S E _ O F _ x | |
R E G I S T E R = = S _ O F _ x
| |
R E G I S T E R = = N O R M A L )
O U T = E A S T ;
e l s e i f ( R E G I S T E R = = N _ O F _ x ) {
i f ( Y _ L o c a l = = 1
| |
X _ L o c a l = = 0
| |
Y _ D e s t i n a t i o n >= Y _ L o c a l
| |
X _ D e s t i n a t i o n > X _ L o c a l + 1 )
O U T = E A S T ;
2
3
4
5
6
7
8
9
10
11
12
13
14
e l s e
15
O U T = W E S T ;
} e l s e i f ( R E G I S T E R = = N W _ O F _ x ) {
i f ( Y _ L o c a l = = 1
| |
Y _ D e s t i n a t i o n >= Y _ L o c a l
| |
X _ D e s t i n a t i o n > X _ L o c a l + 2 )
O U T = E A S T ;
16
17
18
19
20
21
e l s e
22
O U T = S O U T H ;
} e l s e i f ( R E G I S T E R = = W _ O F _ x ) {
i f ( Y _ L o c a l = = 0
Y _ D e s t i n a t i o n > Y _ L o c a l )
O U T = N O R T H ;
23
24
| |
25
26
27
e l s e
28
O U T = S O U T H ;
29
} e l s e {
i f ( Y _ D e s t i n a t i o n <= Y _ L o c a l
| |
X _ D e s t i n a t i o n > X _ L o c a l + 1 )
O U T = E A S T ;
30
31
32
33
e l s e
34
O U T = N O R T H ;
35
}
} e l s e i f ( X _ D e s t i n a t i o n < X _ L o c a l ) {
i f ( R E G I S T E R = = N _ O F _ x
| |
R E G I S T E R = = N W _ O F _ x | |
R E G I S T E R = = W _ O F _ x
| |
R E G I S T E R = = S W _ O F _ x | |
R E G I S T E R = = S _ O F _ x
| |
R E G I S T E R = = N O R M A L )
O U T = W E S T ;
e l s e i f ( R E G I S T E R = = N E _ O F _ x ) {
i f ( X _ D e s t i n a t i o n < X _ L o c a l - 1 | |
Y _ D e s t i n a t i o n >= Y _ L o c a l )
O U T = W E S T ;
36
37
38
39
40
41
42
43
44
45
46
47
48
e l s e
49
O U T = S O U T H ;
} e l s e i f ( R E G I S T E R = = S E _ O F _ x ) {
i f ( X _ L o c a l = = 1
& &
Y _ D e s t i n a t i o n > Y _ L o c a l + 1 )
O U T = N O R T H ;
50
51
52
53
54
e l s e
55
O U T = W E S T ;
56
} e l s e {
i f ( Y _ L o c a l = = 0
| |
( X _ L o c a l = = 1
& &
Y _ D e s t i n a t i o n > Y _ L o c a l ) )
O U T = N O R T H ;
57
58
59
60
61
e l s e
62
O U T = S O U T H ;
63
}
} e l s e i f ( Y _ D e s t i n a t i o n > Y _ L o c a l ) {
i f ( R E G I S T E R ! = S _ O F _ x )
O U T = N O R T H ;
e l s e i f ( X _ L o c a l ! = 0 )
O U T = W E S T ;
64
65
66
67
68
69
e l s e
70
O U T = E A S T ;
} e l s e i f ( Y _ D e s t i n a t i o n < Y _ L o c a l ) {
71
72
i f ( R E G I S T E R ! = N _ O F _ x )
O U T = S O U T H ;
e l s e i f ( X _ L o c a l ! = 0 )
O U T = W E S T ;
73
74
75
76
e l s e
77
O U T = E A S T ;
78
} e l s e
O U T = L O C A L ;
79
This routing function has been analyzed from the point
of view of deadlocks: For the defective part B, all contours
are deadlock free. For the normal part A, the reference XFirst routing algorithm is also deadlock free. In order to
prove that this reconﬁgurable routing algorithm is deadlock
free, we used the formal proof tool ODI [17], developed at
LIP6. This tool is dedicated to deadlock analysis in packet
switching networks. It is based on the analysis of “Strongly Connected Components” (SCC) of the Extended Dependency Graph deﬁned by the micro-network topology on one
hand, and by the routing algorithm on the other hand. Each
router can have a diﬀerent routing function, and the routing
function depends on the destination deﬁned in the packet
header. This tool tries to build a suﬃcient condition proving
the routing algorithm to be deadlock free. We have proved
the proposed routing algorithm to be deadlock free in any
one-faulty-router topology, for a 10 × 10 2D-Mesh.
5. EXPERIMENTAL RESULTS
Performance (penalty on the network saturation
threshold): The cycle-accurate, bit-accurate SystemC simulation model of the DSPIN router has been modiﬁed to
implement the reconﬁgurable routing algorithm described in
section IV. We simulated a 2D-Mesh containing 5 × 5 clusters. Each cluster contains one traﬃc generator and one target. For each initiator, the oﬀered load (deﬁned as the ratio
between the number of injected ﬂits and the total number of
cycles) can be precisely adjusted. The traﬃc has a uniform
random distribution (each initiator sends packets to all targets). The packet length is 8. The average network latency
is measured as the average number of cycles for a round trip
from an initiator to a target, and back to the same initiator. If we plot the average latency versus the oﬀered load,
the saturation threshold is the maximal accepted load where
the latency increases to inﬁnity. We simulated all one-faultyrouter topologies, and the Figure 11 presents the results for
5 cases: no hole, hole in (0,0), hole in (0,2), hole in (1,1),
hole in (2,2).
Normal
hole:[0][0]
hole:[0][2]
hole:[1][1]
hole:[2][2]
 30
 40
 50
 60
 70
 80
 90
 100
 110
 120
 0
 5
 10
 15
 20
 25
 30
 35
 40
L
a
t
y
c
n
e
(
c
y
c
l
)
s
e
Offered load (%)
Figure 11: Some saturation thresholds in 5 × 5 2DMesh.
445
 
When the load is not too high, the impact of the modiﬁed
routing algorithm on the average latency is negligible, but
the saturation threshold can be strongly modiﬁed, when the
hole is located at the center of the mesh.
Cost (extra silicon area): The synthesizable VHDL
model of the DSPIN router has been modiﬁed to introduce the reconﬁgurable routing algorithm described in section
IV. We used the SXLIB standard cell library [1] for a 90nm
CMOS technology, and the Synopsys synthesis environment
to evaluate the cost of the reconﬁgurability from the point
of view of the silicon area.
DSPIN
Router
Total
Without FIFOs With FIFOs
λ
mm2
λ
mm2
Original
Reconﬁgurable
Increase (area)
Increase (%)
1836600
0.015
2459250
0.020
622650
0.005
33.90%
7831600
0.063
8454250
0.068
622650
0.005
7.95%
The router footprint is increased by only 8%. This is a very
low cost, as the routers represent about 3% of the silicon area
for a typical cluster. This numbers do not take into account
the BIST logic for network testability.
6. CONCLUSION
We propose an ultra-low-cost reconﬁgurable routing algorithm supporting any one-faulty-router topology. It requires
only a 4bits conﬁguration register per router. It has been
physically implemented in the DSPIN micro-network. The
silicon area penalty is only 8% of the router footprint, and
about 0.2% of the total chip area. The impact on the latency and saturation threshold has been evaluated. The reconﬁgurable routing algorithm is fully scalable. It has been
demonstrated in the DSPIN micro-network, but can be used
in any 2D-Mesh Network-on-Chip.
Moreover, this algorithm can be extended to one-faultyregion topology. The faulty region is a rectangle covering all
faulty routers as shown in Figure 12. All internal clusters in
this faulty region must be considered as faulty and deactivated.
NW
N
N
NE
W
W
E
E
SW
S
S
SE
Figure 12: In a faulty region, a rectangular contour
is built around this region.
In massively parallel multi-processors architecture, this reconﬁguration capability can become mandatory to improve
the yield issues.
446
7. "
A practical approach of memory access parallelization to exploit multiple off-chip DDR memories.,"3D stacked memory enables more off-chip DDR memories. Redesigning existing IPs to exploit the increased memory parallelism will be prohibitively costly. In our work, we propose a practical approach to exploit the increased bandwidth and reduced latency of multiple off-chip DDR memories while reusing existing IPs without modification. The proposed approach is based on two new concepts: transaction id renaming and distributed soft arbitration. We present two on-chip network components, request parallelizer and read data serializer, to realize the concepts. Experiments with synthetic test cases and an industrial strength DTV SoC design show that the proposed approach gives significant improvements in total execution cycle (21.6%) and average memory access latency (31.6%) in the DTV case with a small area overhead (30.1% in the on-chip network, and less than 1.4% in the entire chip).","25.4
A Practical Approach of Memory Access Parallelization to 
Exploit Multiple Off-chip DDR Memories 
Woo-Cheol Kwon, Sungjoo Yoo, Sung-Min Hong, Byeong Min, Kyu-Myung Choi, Soo-Kwan Eo 
CAE Team, System LSI Division, Semiconductor Business, Samsung Electronics 
ABSTRACT 
3D stacked memory enables more off-chip DDR memories. 
Redesigning existing IPs 
to exploit 
the 
increased memory 
parallelism will be prohibitively costly. In our work, we propose a 
practical approach to exploit the increased bandwidth and reduced 
latency of multiple off-chip DDR memories while reusing existing 
IPs without modification. The proposed approach is based on two 
new concepts: transaction id renaming and distributed soft 
arbitration. We present two on-chip network components, request 
parallelizer and read data serializer, to realize the concepts. 
Experiments with synthetic test cases and an industrial strength 
DTV SoC design show that the proposed approach gives significant 
improvements in total execution cycle (21.6%) and average memory 
access latency (31.6%) in the DTV case with a small area overhead 
(30.1% in the on-chip network, and less than 1.4% in the entire 
chip). 
Categories and Subject Descriptors 
B.7 [Hardware]: Integrated circuits. 
General Terms Algorithms, Design. 
Keywords Memory, parallelization, and arbitration. 
1. 
Introduction 
DDR memory is widely used for heavy (high bandwidth and 
large volume) data flows. With 3D stacked memory which has 
multiple DDR memory ports 1 , master IPs (in short, masters) can 
exploit the increased bandwidth and reduced latency by accessing 
multiple memory ports (in short, multiple memories) simultaneously. 
The problem of simultaneously accessing multiple slaves (e.g., 
small SRAMs or multiple peripheral devices) may not be new in onchip communication. In reality, it was not a performance critical 
issue. However, the performance to access multiple off-chip DDR 
memories is an important performance problem since it will 
determine the entire system performance.  
A naïve parallelization of memory accesses of existing masters 
(e.g. via an address remapping to be explained in Section 3) may 
not be able to exploit the parallelism of multiple DDR memories 
due to a limitation of state-of-the-art on-chip networks (OCNs). The 
1 Tens of 64b ports per memory die can be achieved via vertical die 
to die interconnect, e.g. face-to-face [10] or through silicon via 
(TSV) [3]. 
limitation results from a deadlock problem that occurs when inorder accesses are bound for multiple destinations (e.g., memories) 
and dynamic reordering is applied at the destinations (e.g., memory 
controllers). In order to resolve the deadlock problem, existing 
OCNs limit multiple outstanding transactions, which prevents from 
exploiting the increased bandwidth of multiple memories (we will 
explain the details of this problem and the limitation of state-of-theart solutions in Section 4).  
Masters, especially, data intensive masters may need to be redesigned in order to overcome the limitation of the existing OCNs 
(by performing parallel memory accesses with multiple transactions 
and bus ports). However, such redesigns may need a significant 
restructuring of internal architectures of those IPs, e.g., loop and 
memory access parallelization 2 [4], etc. Thus, the redesigns may 
require prohibitively high design efforts due to the complexity and 
number of those IPs (e.g., multi-channel HD codecs, +10M triangle 
3D graphics, sophisticated pixel processing IPs, etc.). A practical 
solution is required to reuse existing IPs while exploiting the full 
potential of multiple memories. 
In this paper, we present a practical approach to exploit multiple 
off-chip DDR memories. The key ideas are two-fold. We present an 
idea called transaction id renaming to make the original in-order 
memory accesses independent from each other on the OCN. Then, 
their concurrent accesses to multiple memories and dynamic 
reordering can be tried to increase memory bandwidth. 
Dynamic reordering of original in-order memory read accesses 
requires the reorder buffer, which has a limitation in terms of read 
latency due to out-of-order data arrivals. We present the other idea 
called distributed soft arbitration to resolve the limitation. It 
serializes out-of-order read data before they enter the OCN from the 
memory controllers. Thus, the read data reach the reorder buffer, in 
most cases, in the original order requested by their master thereby 
reducing the latency. It also improves the utilization of reorder 
buffer thereby further increasing the parallelism supported by the 
reorder buffer. To realize these ideas, we present two new OCN 
components, request parallelizer and read data serializer.  
This paper is organized as follows. Section 2 reviews related 
work. Section 3 gives our assumptions. Section 4 explains the 
deadlock problem and the limitation of existing solution. Section 5 
introduces the basic idea of our approach. Section 6 presents the 
OCN architecture with request parallelizers and read data serializers. 
Section 7 gives experimental results. Section 8 concludes the paper. 
2 We define memory access parallelization to be an ensemble of 
spatial parallelization (for memory port parallelism), e.g., 
address remapping for multiple memories to be explained in 
Section 3 and temporal parallelization, i.e., dynamic reordering 
of memory accesses enabled by transaction id renaming in this 
work.  
447
 
 
                                                                 
 
                                                                 
2. 
Related Work 
Recently, OCN for 3D stacked memory is becoming one of hot 
research topics [1][2][3]. An NoC router is presented for vertical 
interconnect (e.g., via Through Silicon Via) as well as conventional 
horizontal interconnect [1]. In order to fully utilize both the 
increased bandwidth and reduced latency of 3D stacked memory, 
on-chip cache architectures are also explored over the cache design 
space (e.g., shared or private L2, inclusion or not, etc.) [2].  
Memory access parallelization has been studied actively for 
FPGA-based systems where multiple DDR memories can be easily 
interconnected with FPGAs. A method of loop and memory access 
parallelization is presented in [4]. A memory controller which 
exploits multiple memory banks in multiple memories is presented 
in [5]. Address remapping of vector access enables to exploit the 
memory bandwidth from multiple banks in this work. In [6], several 
methods of parallelizing 2D memory accesses, e.g., 8x8 block read, 
are presented. Existing work in [4][6] will be useful in redesigning 
data intensive IPs to exploit multiple memories. 
The work in [7] presents representative methods of memory 
access parallelization in the context of single memory: out-of-order 
and speculative memory accesses, dynamically unresolved loads 
and stores, memory renaming, etc. Our work adopts some of them 
(out-of-order memory accesses) while presenting new ideas of 
transaction id renaming and distributed soft arbitration in the 
context of multiple memories. 
Compared with the existing work mentioned above, our work 
differs in that it enables the parallelization of in-order memory 
accesses from existing masters without requiring their redesigns. 
3. 
Assumptions 
In this section, we present our assumptions in terms of memory 
access ordering imposed by masters, memory address remapping, 
and memory consistency model adopted in this paper. 
Memory accesses from masters are ordered on a transaction basis. 
A transaction is identified with ‘transaction id’. 3 The order of 
memory accesses issued by the master with the same transaction id 
needs to be maintained at the I/O port of the master while memory 
accesses with different transaction id’s can be processed out of 
order.  
There have been presented several methods of address remapping 
to enable more chance of exploiting bank parallelism [8][9] or 
memory parallelism [4][6]. In the case that bank interleaving is used 
as in [8][9], the original memory address, represented with <bank, 
row, column>, is remapped to the one suitable for bank interleaving, 
represented with <row, bank, column>. By doing that, neighboring 
rows in the same bank of the original address map are mapped to 
different memory banks. The remapping can be applied at the 
memory controller or at the master.   
In our work, we insert memory bits to the remapped address such 
as <row, memory, bank, column>. Thus, field ‘memory’ has log2M 
bits, where M is the number of DDR memories. Address remapping 
by memory field is to distribute memory locations from one 
memory (port) to multiple ones. For instance, assume that the 
original memory accesses are (read, 0x1000), (read, 0x2000), (write, 
0x1000), and (write, 0x3000)4, and they are bound for a single DDR 
memory in the original address map. If the address remapping, 
based on the new address, <13 bits row, 2 bits memory, 2 bits bank, 
10 bits column> 5 , is applied at the master’s I/O port, the four 
memory accesses now have three different memories as their 
destinations: (read, 0x1000) and (write, 0x1000) to memory #1, 
(read, 0x2000) to memory #2, and (write, 0x3000) to memory #3. 
The address-remapped accesses can be issued to multiple 
destinations simultaneously. For instance, in the above case of 
address remapping, the second and fourth accesses, (read, 0x2000) 
and (write, 0x3000) to memories #2 and #3 can be reordered to be 
executed on the OCN before the first or the third one without 
violating dependencies. 
Note that the methods of address remapping (whether it is either 
memory bit-based one or others in [4][6]) are orthogonal to the two 
new concepts of transaction id renaming and distributed soft 
arbitration presented in this paper. Thus, our concepts can be 
applied together with the advanced memory remapping techniques. 
Reordering of memory accesses of multiple masters needs to be 
applied based on a well-defined memory consistency model [11]. In 
this paper, we assume weak consistency model that most of existing 
OCNs (PL301, SMX, etc.) adopt. It allows out-of-order memory 
accesses on the OCN (while meeting the in-order requirement of 
each master) and requires synchronizations (memory barriers) for 
inter-master communication.  
4. 
Deadlock Problem and Existing Solution 
The in-order requirement at the I/O port of master, described in 
Section 3, may cause a deadlock problem in the case of multiple 
memory accesses if no proper measure is applied. Figure 1 (a) 
shows an example that two masters access two memories.  
OCN
RP1
RDS1
RP2
RDS2
Mas ter
1
Mas ter
2
Memory
Controlle r
1
Memory
Controlle r
2
D
Memo ry
1
A
C
Memo ry
2
B
(a) Mul tiple memory accesses: an example
Memory 1
Memory 2
Mas ter 1
C D
Mas ter 2
A
B
D is blocked at master 1
D
B
A
C
B is b locked at mas ter 2
Time
(b) Deadlock problem
Figure 1 Multiple memory accesses and deadlock problem 
First, assume that each master uses its own single transaction id. 
That is, the order of memory accesses needs to be maintained at the 
I/O port of each of the two masters. We assume that, as Figure 1 (b) 
illustrates, master 2 sends request A (bold box in the figure) to 
3 AXI protocol has ‘transaction id’ and OCP has ‘tag id’ to 
represent to which transaction the request/data/response belongs. 
4 The first access, (read, 0x1000), is the oldest. 
5 The address of 1Gb DDR memory can have 2 bits for banks, 13 
bits for rows, and 10 bits for columns. 
448
                                                                 
 
                                                                 
memory 1, master 1 sends requests C and D to memories 2 and 1, 
respectively, and finally, master 2 sends request B to memory 2. All 
the four requests are assumed to be read requests to different 
memories obtained after applying the address remapping to the 
original accesses as explained in Section 3. For simplicity, we 
assume that there is zero cycle delay in the OCN and memory 
controllers. We also assume that, as Figure 1 (b) shows, memory 
controller 1 (2) reorders requests A and D (requests B and C) to 
improve memory utilization and that request B’s (D’s) data become 
ready earlier than request A’s (C’s). 
Note that requests B and C are independent. Thus, for instance, if 
request B accesses the currently open row in the DDR memory, then 
request B can be served earlier than request C, to improve memory 
utilization, though request B arrives later than request C. 
When request B’s data (blank rectangle) are ready and try to 
reach master 2, the data cannot enter master 2 due to the in-order 
requirement of master 2 that request A’s data need to be taken in 
first. Thus, request B’s data remain, in fact, are blocked at the 
memory controller. Request D’s data have the same situation since 
request C’s data need to arrive at master 1 earlier than request D’s. 
Since both request B’s and D’s data are blocked and occupy the 
ports of memory controllers, when request A’s and C’s data are 
ready in the memory controllers, they cannot be delivered to their 
destinations until the blocked data (of request D’s and B’s) become 
flushed out of the memory controllers. However, due to the 
blocking of request A’s and C’s data, request B’s and D’s data 
remain blocked forever. Thus, a deadlock situation occurs. 
Figure 2 illustrates an existing solution [9][12][15] to tackle the 
deadlock problem. The solution allows multiple outstanding 
requests with the same transaction id bound only for a single slave 
at one time [15]. It does not allow multiple outstanding requests 
with the same transaction id bound for different slaves at one time. 
Only after finishing requests for one slave, the master can send 
requests to another slave. 
Mas ter 2
Memory 1
Memory 2
Mas ter 1
C
D
A
A
A
B
B
C
C
D
1
2
3
4
5
6
7
8
9
10 11
12 13
14
15
D
B
16
17 
Figure 2 Execution scenario in the existing solution 
In Figure 2, masters 2 and 1 send requests A and C, respectively, 
at time 1 and 2. Due to the above limitation, master 1 (master 2) 
cannot make further memory access requests until request C 
(request A) is served. At time 9, master 2 sends request B after the 
read data of request A arrive completely at master 1. Master 1 sends 
request D at time 11 only after the read data of request C are 
completely received by master 1.   
As shown in Figure 2, the existing solution resolves the deadlock 
problem at a cost of long latency since it limits the number of 
outstanding requests. Most of existing masters may use a limited 
number of transaction id’s (1 or 2 at most), which may be smaller 
than the number (easily up to 16) of possible off-chip DDR 
memories. Thus, without a proper measure, such a scheme may 
prevent masters from exploiting increased memory bandwidth by 
limiting the number of outstanding memory requests. 
5. 
Basic Idea of the Proposed Approach 
The idea of transaction id renaming is that the original 
transaction id of memory access (called master side transaction id, 
in short, master id) is renamed to a new transaction id used only in 
the OCN (called OCN id). For the renaming, we insert a new bus 
component called request parallelizer (RP) between master and 
OCN as the ovals (annotated with RP1 and RP2) illustrate in Figure 
1 (a). Note that the RP has a reorder buffer to handle out-of-order 
arrivals of read data. 
Figure 3 illustrates the idea of transaction id renaming. At time 1, 
master 2 sends request A to request parallelizer 2 (RP2). RP2 
assigns a new OCN id to request A and sends it to the OCN. For the 
other three requests B, C, and D, corresponding RPs assign 
different transaction id’s as OCN id’s (the annotated numbers show 
different OCN id’s). The assigned OCN id’s enable the four 
requests to be issued out of order in the OCN. As far as the RPs 
have reorder buffers enough to serve the read data of all the 
outstanding requests, there is no deadlock problem. Note that the 
RP satisfies the original in-order requirement at the I/O port of 
master.  
Mas ter 2
C D
Memory 1
Memory 2
A1
D3
Mas ter 1
A
B
RP2 (rece iving)
D3
D
A
C
1
2
3
4
5
6
7
8
9
10 11
12 13
14
15
To ta l execut ion 
cyc le reduct ion
RP1 (rece iving)
C2
A1
C2
B4
B4
B
16
17 
Figure 3 Memory access parallelization w ith RPs 
The idea of transaction id renaming allows for multiple 
outstanding requests, which might not be allowed when just reusing 
master IPs with the existing OCNs. The outstanding requests hide 
memory access latency and reduce the total execution cycle of 
memory accesses. As shown in Figure 3, the transaction id 
renaming reduces the total execution cycle by 1 time unit. The 
reduction comes mainly from the fact that, in Figure 3, requests B 
and D are issued to the OCN earlier than in the case of Figure 2. 
Such early request issues are enabled by the transaction id renaming. 
The idea of serializing read data can give further reduction in 
total execution cycle. The possibility of benefit from 
the 
serialization can be found in the case of Figure 3. At time 9, the 
read data of request A are ready at memory controller 1. However, it 
cannot be delivered to RP2 since RP2 is busy in receiving the data 
of request B. If there is a way to preempt the data transfer (of 
request B) to RP2, we may allow the data of request A to be 
transferred to RP2 and, finally, to master 2 by one time unit earlier.  
Master 2
C D
Memory 1
Memory 2
Master 1
A
B
RP2 (receiv ing)
RP1 (receiv ing)
B4
A
1
2
3
4
5
6
7
8
9
10 11
12 13 14
15
B4
Total execution
cyc le reduction
A1
A1
D3
RDS1 (receiv ing)
RDS2 (receiv ing)
B4
A1
D3
C2
B4
B
D3
C2
C
D
16 17
C2
Figure 4 Memory access parallelization with RPs and RDSs 
The preemption of data transfer required in the above example 
can be implemented by serializing read data transfers in the original 
449
 
order of memory accesses issued at the I/O ports of masters. In the 
case of time 9, the data of request A are required before those of 
request B since request A was issued (at time 1) before request B (at 
time 4) in the original order at the master side. Thus, the master side 
order needs to be used to serialize the transfers of read data. 
The key point here is to exploit the original master side order 
when arbitrating multiple read data bound for the same RP. The 
serialization can be applied whenever there are multiple data 
transfers contending for the same RP regardless of whether the 
preemption is applied or not.  
For this purpose of serialization, we insert new bus components 
called read data serializers (RDSs) between the OCN and memory 
controllers as the ovals (annotated with RDS1 and RDS2) in Figure 
1 (a) illustrates. Figure 4 shows the case of applying both RPs and 
RDSs to the case of Figure 1 (a). At time 9 when the data of request 
A are ready, RDS2 comes to know (from the information of the 
other RDSs) that there is a ready data transfer with an earlier OCN 
id (which serves as a dynamic sequence number), and stops sending 
its data of request B. At the same time, RDS1 starts sending those of 
request A to RP2. As a result, as Figure 4 shows, we can have a 
reduction in total execution cycle by one additional time unit. In 
terms of latency in memory accesses, the reduced total execution 
cycle corresponds to the reduced (average) latency in memory 
accesses.  
Read data serialization also gives a chance of optimizing the 
utilization of reorder buffers. Since there is less chance of keeping 
data in the RPs, there can be more free slots in the reorder buffers. 
Thus, the RPs can accommodate more outstanding requests thereby 
further improving the performance. 
6. 
 Proposed OCN Architecture 
Figure 5 illustrates the proposed OCN architecture. The master 
can be a sub-system having an internal OCN or a single master IP. 
Contrary to the RP, the RDS is inserted only in the read data 
network. For instance, in the case of AXI-based OCN, the read data 
network corresponds to the network connecting only the read data 
channels among the five AXI channels (read/write address, 
read/write data, and response channels) [9]. 
Memory
Contro ller 1
Memory
Contro ller 2
Memory
Controller 3
Memory
Controller 4
OL
OL
OL
OL
RDS
On-chip Ne twork
GI
Network
Ctrl
RN
C trl
RN
Ctrl
RN
C trl
RN
RP
Mas ter 1
Maste r 2
Master 3
Master 4
Figure 5 Proposed OCN architecture  
RDSs communicate with each other via a global information (GI) 
network. The GI network broadcasts the ordering information, i.e. 
OCN id (which serves as a dynamic sequence number), to help 
RDSs to serialize read data transfers. For this purpose, each RDS 
provides the GI network with the information of its currently ready 
data transfer. Each RDS also receives via the GI network the 
information of currently ready data transfers of the other RDSs.  
6.1 Request Parallelizer  
Figure 6 shows the internal structure of RP. It consists of  
transaction lists, reorder buffer, and logic for address remapping 
(AR), transaction id renaming (RN) and ordering.  
Transaction id renaming 
Transaction id consists of two fields: source/destination and 
ordering fields as in [9]. Transaction id renaming changes only the 
ordering field. The renamed OCN id (to be exact, the ordering field) 
is incremented each time a memory access request is issued by the 
master. It wraps around when the ordering field becomes NR where 
NR is the maximum number of outstanding renamed memory 
requests that the RP can handle. NR is determined by the maximum 
of sizes of reorder buffer and transaction lists (to be explained later). 
The size of ordering field in the OCN id is log2(2NR+1) to enable 
their comparison by RDSs.
In order to manage the ordering information of outstanding 
requests, the transaction lists are managed as exemplified in Figure 
6. A linked list is managed for a master id. A memory access 
request corresponds to an entry in the linked list. An entry has five 
fields <master id, OCN id, r/w bit, address, burst length>. The 
figure shows that the RP has received five requests from the master. 
Transaction id renaming is applied to three of them. Thus, we have 
three entries which have their own OCN id’s.  
To control network
From  data network
Transaction lists
Reorder buf fer
Ctrl
RN
AR
2
2
1
1
1
1
2
2
<0, 0, r, 0x1000, 4>
<0,1, r, 0x2000, 4>
<0, N/A, w, 0x1000, 4>
<0, 2, r, 0x3000, 4>
<1, N/A, w, 0x4000, 8>
From master
To mas ter
Figure 6 Internal structure of request parallelizer 
Dependency 
In the dynamic reordering of memory accesses, it is necessary to 
reorder memory accesses without violating dependencies (RAW, 
WAW, and WAR). When a memory access request is issued by the 
master, the transaction list corresponding to the master id is 
searched to see if there is a conflicting access with the same master 
id to the same address. The memory access request is allowed to 
proceed (i.e. renamed and sent to the OCN) if there is no conflict. If 
the address matches a previous one with the same master id, the 
request is not issued to the OCN. It is served when the preceding 
access to that address completes. For instance, in Figure 6, the 
request <0, N/A, w, 0x1000, 4> is registered in the transaction id 
list. However, due to the WAR dependency with the previous 
request <0, 0, r, 0x1000, 4>, it is not issued to the OCN. Thus, no 
OCN id is yet assigned to it.  
Management of transaction id list and reorder buffer 
When a new request is issued by the master, if there is enough 
space in both the reorder buffer and the corresponding transaction 
list, the request is processed. In the case that the limit of reorder 
buffer is reached and the transaction list has still an available slot, 
the RP does not block all the incoming requests. Instead, it checks 
to see if the transaction id of new request matches any master id in 
the transaction list. If there is no match, it registers it in a new 
450
 
 
 
 
transaction list, bypasses the transaction id renaming and sends it to 
the OCN following the conventional ordering scheme (in Section 4). 
The entry <1, N/A, w, 0x4000, 8> in Figure 6 is the case. If both the 
limits of reorder buffer and transaction lists are reached, the RP 
blocks the request at the I/O port of master. 
Reorder buffer 
The reorder buffer accepts incoming data from the OCN and 
manages their order. Since read data serialization can preempt data 
transfers, each of the incoming data transfer (e.g., burst 4 read) is 
handled with a linked list as the arrows in the reorder buffer show in 
Figure 6. For instance, the read data of request with OCN id, 2 are 
broken into two parts since those of request with OCN id, 1 preempt 
them. 
The memory access request retires from the transaction list in the 
original access order when all of its read data in the reorder buffer 
(for read request) or its write response (for write request) are 
transferred to the master.  
Write data handling  
As shown in Figure 6, write requests are handled (transaction id 
renaming is applied to them) by the RP. However, the RP does not 
buffer write data, but bypasses them. 
6.2 Read Data Serializer 
The RDS consists of a shared data buffer and an ordering logic 
(OL) for read data serialization as exemplified in Figure 5. The 
RDS receives data from the memory controller while sending data 
to the OCN in a pipelined manner. The RDS provides the GI 
network with its information of ready data transfers, i.e. OCN id 
and the status of data transfer. For instance, in the case of AXIbased OCN, the status of data transfer corresponds to signal rvalid 
in the read data channel of AXI interface (if rvalid is high, data are 
ready to be transferred). Each RDS has two ports for the GI network. 
The output port has BOCN+1 signals, where BOCN is the bit width of 
OCN id. The input port has (NRDS-1)x(BOCN+1) signals where NRDS 
is the number of total RDSs connected to the OCN.  
When serializing read data from multiple RDSs to a single RP, 
the OCN id works as a sequence number. The serialization is an 
arbitration among multiple RDSs that have ready data whose 
destinations are the same RP. A centralized arbitration on an SoC is 
prohibitive due to the long wire delay. Thus, we take a distributed 
version. The basic assumption of distributed soft arbitration is that 
the GI network has multi-cycle delay. Thus, each RDS sees cycles 
old information and makes a local decision of whether or not to 
send its ready data based on the cycles old information.  
1  Check the GI network to f ind the earliest OCN id per RP
2  If there is any data whose OCN id is earlier than the found at line 1,
3
If there are multiple eligible data, select one via round robin.
4
Transfer the selected data to the destination, and goto line 1.
5 Else, wait for one clock cycle and goto line 1.
Figure 7 Distributed soft arbitration algorithm 
Figure 7 gives the algorithm of distributed soft arbitration that is 
implemented in the ordering logic in each RDS. It checks the GI 
network to find the earliest OCN id per RP (line 1). Then, it checks 
to see if there is any of its ready data whose OCN id is earlier than 
the earliest one per RP found in the GI network (line 2). If there are 
multiple data (bound for different RPs) each having the earlier OCN 
id per RP, then select one based on a round robin scheme (line 3). 
Then, the data is transferred to the destination RP (line 4). If there is 
no data with an earlier OCN id, then the RDS waits one clock cycle 
and resumes the arbitration (line 5). 
Since the GI network can have multi-cycle delay, the arbitration 
is based on an old information. Thus, there is a possibility that more 
than one RDSs send data to the same RP or that there is no data 
transfer even though there is some RDS(s) with ready data. In order 
to reduce such a probability, a careful topology and physical design 
of GI network is required, which is our future work. In our 
experiments, we evaluate the effect of GI network delay.  
Note also that RDSs can be used without RPs (i.e. without 
transaction id renaming and reorder buffer). Thus, the arbitration 
(not the serialization) among RDSs, based on only the destination 
information in the transaction id, can also serve to reduce network 
congestion at the read data ports of masters. This is beyond the 
scope of this paper. More details can be found in [13]. 
7. 
Experimental Result 
In our experiments, we use two sets of test cases: synthetic ones and 
an industrial strength DTV SoC case. 
7.1 Synthetic Test Cases 
The architecture of synthetic test cases is similar to the one in 
Figure 5. It consists of M 32b AXI masters and 8 DDR memories 
(tCL-tRP-tRCD=3-3-3, 4 banks/memory). We change the number 
of masters, M (2 ~ 8). We use commercial AXI crossbar (PL301) as 
the OCN and memory controller (PL340) [9]. We assume that the 
GI network has a point-to-point topology where each link has a 
latency of L cycles (L is varied). We set the sizes of reorder buffer 
and transaction list of RP to 32 words (accommodating 4 requests 
with burst size 8) and 15 entries, respectively. We set also that of 
data buffer of RDS to 16 words. 
In terms of memory access pattern, each master accesses all the 
memories simultaneously by continuously generating memory read 
requests (with random memory addresses and burst size 8). We set 
the bit width of ordering field of master id (OCN id) to 0 and 1 (5). 
Thus, there are two cases in terms of the number of master id’s per 
master: 1 or 2 master id’s. We run RTL simulation. In the 
simulation, all the masters read the same total amount of data from 
multiple memories.  
Latency, 2x8
1 master id
2 master ids
Latency, 8x8
100000
90000
80000
70000
60000
50000
40000
30000
20000
10000
0
40000
35000
30000
25000
20000
15000
10000
5000
0
Execution cycle, 2x8
Execution cycle, 8x8
350
300
250
200
150
100
50
0
400
350
300
250
200
150
100
50
0
Figure 8 Performance comparison of synthetic test cases 
Figure 8 shows the performance comparison (total execution 
cycle to complete the entire simulation and average memory access 
latency seen by the masters) among six cases: conventional design 
case (base) without RPs and RDSs, one only with RPs (RP), and 
451
 
 
four with both RPs and RDSs (RP+RDS). We also change the 
latency cycle of GI network from 1 to 4 cycles when RDSs are 
applied. The figure shows results for the two configurations of 
masters and memories (e.g., 2x8 represents the case of 2 masters 
and 8 memories). 
As shown in Figure 8, compared with the baseline cases, the 
proposed approach gives significant improvements in both total 
execution cycle (i.e., memory utilization) and average memory 
access latency by up to 44.5% and 63.2%, respectively. We obtain 
the best case in RP+RDS1 when both RP and RDS are applied and 
that each link in the GI network has a unit cycle delay. As the GI 
network latency increases up to four clock cycles, the total 
execution cycle and average memory access latency increase slowly. 
The results show that the proposed method is still effective when 
the GI network has a relatively long latency cycle. However, in the 
case that the latency becomes more severe, an application-specific 
optimization (topology and floorplan) of GI network design will be 
required, which is our future work. 
As the number of master id’s per master increases from one to 
two, the gain becomes smaller, but still significant. Considering that 
most of existing IPs have one or two (high bandwidth) master id’s 
at most, these results show that the proposed method is effective in 
parallelizing the memory accesses of existing IPs. 
7.2 Digital TV SoC Case 
The industrial strength digital TV SoC consists of +30M gates 
and more than 50 masters/slaves for QFHD size (3840x2160) video 
processing [14]. Many of masters/slaves are connected to low 
bandwidth local AXI/AHB/APB buses. We apply the proposed 
approach to the backbone OCN. The OCN architecture is similar to 
the one in Figure 5 except that there are 8 memories and 8 masters 
(four video codec IPs and four different pixel processing IPs, e.g., 
noise reduction, mixer, 3D graphics, etc.). Each IP has two master 
id’s. We use the same configurations of RP, RDS and memory 
controller as in the synthetic test cases. In terms of memory access 
pattern of masters, each master accesses all the eight memories 
while their communication periods do not always overlap with each 
other. 
Execut ion cycle (normalized)
1.2
1
0.8
0.6
0.4
0.2
0
Latency
250
200
150
100
50
0
Figure 9 Runtime and latency comparison in DTV SoC case 
Table 1 "
Variation-adaptive feedback control for networks-on-chip with multiple clock domains.,"This paper discusses the use of networks-on-chip (NoCs) consisting of multiple voltage-frequency islands to cope with power consumption, clock distribution and parameter variation problems in future multiprocessor systems-on-chip (MPSoCs). In this architecture, communication within each island is synchronous, while communication across different islands is achieved via mixed-clock mixed-voltage queues. In order to dynamically control the speed of each domain in the presence of parameter and workload variations, we propose a robust feedback control methodology. Towards this end, we first develop a state-space model based on the utilization of the inter-domain queues. Then, we identify the theoretical conditions under which the network is controllable. Finally, we synthesize state feedback controllers to cope with workload variations and minimize power consumption. Experimental results demonstrate robustness to parameter variations and more than 40% energy savings by exploiting workload variations through dynamic voltage-frequency scaling (DVFS) for a hardware MPEG-2 encoder design.","35.1
Variation-Adaptive Feedback Control for Networks-on-Chip with 
Multiple Clock Domains
Umit Y. Ogras, Radu Marculescu, Diana Marculescu
Department of Electrical and Computer Engineering
Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: {uogras,radum,dianam}@ece.cmu.edu
ABSTRACT
This paper discusses the use of networks-on-chip (NoCs) consisting of multiple voltage-frequency islands to cope with power consumption, clock distribution and parameter variation problems in
future multiprocessor systems-on-chip (MPSoCs). In this architecture, communication within each island is synchronous, while communication across different islands is achieved via mixed-clock,
mixed-voltage queues. In order to dynamically control the speed of
each domain in the presence of parameter and workload variations,
we propose a robust feedback control methodology. Towards this
end, we first develop a state-space model based on the utilization of
the inter-domain queues. Then, we identify the theoretical conditions under which the network is controllable. Finally, we synthesize state feedback controllers to cope with workload variations and
minimize power consumption. Experimental results demonstrate
robustness to parameter variations and more than 40% energy savings by exploiting workload variations through dynamic voltagefrequency scaling (DVFS) for a hardware MPEG-2 encoder design.
Categories and Subject Descriptors
B.7 [Hardware]: Integrated circuits.
General Terms
Algorithms, Design.
Keywords
Voltage-frequency island, dynamic voltage-frequency scaling,
parameter variation, MPSoC, networks-on-chip, feedback control.
1. INTRODUCTION
 Due to continuous scaling of CMOS technology, systems containing a large number of processors and on-chip memories have
become a reality [14]. Consequently, novel architectures that allow
various cores communicate to each other via the Network-on-Chip
(NoC) paradigm have emerged as a promising alternative to traditional bus-based approaches [3]. By eliminating global wires, the
NoC approach provides the needed scalability and predictability,
while facilitating design reuse.
In addition to increasing complexity, systems designed in nanoscale technologies suffer from systematic and random variations in
process, voltage, and temperature (PVT). While the threshold voltage variations result in chips with widely varying leakage and frePermission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. To copy otherwise, or republish, to post on
servers or to redistribute to lists, requires prior specific permission and/or a fee.
DAC’08, June 8-13, 2008, Anaheim, California, USA
Copyright 2008 ACM 978-1-60558-115-6/08/0006 ...$5.00.
614
Different voltage-frequency islands
Different voltage-frequency islands
Different voltage-frequency islands
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
Mixed clock-mixed voltage 
Mixed clock-mixed voltage 
Mixed clock-mixed voltage 
interface queues
interface queues
interface queues
Temperature
Temperature
Temperature
Target and actual 
Target and actual 
Target and actual 
queue utilizations
queue utilizations
queue utilizations
Delay
Delay
Delay
Controller 
Controller 
Controller 
Frequency
Frequency
Frequency
Voltage
Voltage
Voltage
Workload
Workload
Workload
System under control 
System under control 
System under control 
Dynamic Power
Dynamic Power
Dynamic Power
Static Power
Static Power
Static Power
(a)
(b)
Figure 1 (a) Loop showing how the variations in temperature
and voltage affect the system power consumption which eventually feeds back to temperature. (b) An NoC architecture
with multiple voltage-clock domains are shown.
quency values, the dynamic variations in the workload and
temperature cause uneven voltage distribution and hence, performance mismatches in the system [1]. This in turn results in variations in power consumption and affects the chip temperature
generating a feedback loop as shown in Figure 1(a). Adaptive body
biasing and frequency binning are used to cope with leakage and
frequency variations across different chips [7]. However, within die
variations play an increasingly important role in system power consumption and performance as the technology scales down [1].
Since designers cannot rely on the accuracy of the nominal parameter values, there is a tremendous need for on-line techniques that
can cope with such dynamic variations. More precisely, there is a
need for efficient algorithms and built-in circuitry able to adapt the
system behavior to workload variations (see Figure 1(a)) and, at the
same time, cope with the parameter variations which cannot be predicted or accurately modeled at design time.
Towards this end, we explore distributed, variation-adaptive control techniques that can improve the performance, power consumption, and reliability of future NoC architectures in a synergistic
manner. We consider NoC architectures consisting of multiple voltage-frequency islands (VFIs), or voltage-clock domains, as shown
in Figure 1(b). Each island is a synchronous region, i.e., the cores
within the same island share a single clock and supply voltage that
can be controlled independently of other islands. Communication
across different clock domains is asynchronous and it can be
achieved via mixed-clock mixed-voltage FIFOs [2]. We use the utilization of these queues as a performance metric to adapt the voltage and frequency values of various islands.
The contribution of our work is twofold: First, we develop a
state-space model for multiple voltage-clock domain NoC designs,
and identify the theoretical conditions under which such a system
is controllable and stable. Then, we propose a novel methodology
to dynamically control the speed of each island, as shown in
Figure 1(a). The controller not only considers the dynamic workload variations, but also ensures that the operating frequency does
not exceed the maximum allowed value for a given temperature.
Within this framework, we explore two feedback control techniques, namely regulation and tracking, as follows: 
1) On-line regulation: For this scenario, we assume that the nominal values of the operating voltage and frequency are determined by
an algorithm running at a high level of abstraction (e.g., [10]). The
actual operating conditions such as voltage, frequency and temperature are usually different than the values assumed at design time
due to PVT and dynamic workload variations [1]. Hence, optimizations carried out for nominal values are likely to result in poor performance figures. Our goal is to regulate the voltage and clock
frequency around the nominal values and counteract the effects of
parameter variation and uncertainties that are too expensive or too
difficult to be characterized and dealt with at design time. Regulating the voltage and clock frequency around nominal values makes
sense also from a technology perspective, since the window of
change for the supply voltage is continuously shrinking [14].
Finally, this approach enables the re-use of existing off-line
Dynamic Voltage and Frequency Scaling (DVFS) algorithms that
will otherwise fail due to the PVT variations.
2) On-line tracking: This scenario is more general, in the sense that
it can scale the voltage and clock frequency within a wider range of
values in order to minimize the power consumption, as well as provide robustness in operation. In this case, we assume there exists a
lower bound on the operating frequency (hence voltage) for a subset of voltage-frequency islands that satisfies certain deadline or
rate constraints. The proposed controller sets the speed of each
island such that the constraints on the minimum operating values
are satisfied. For instance, an audio/video decoder has to maintain a
minimum level of decoding rate such that the output can be displayed without distortion. In this case, the proposed methodology
takes this minimum output rate as input. The voltage-frequency
island that generates the output stream works at the minimum permissible speed, while the speeds of all other islands are adjusted
dynamically to match the desired output rate. As detailed in
Section 4, the proposed controller throttles the clock frequency to
exploit the dynamic variations in the workload, thereby saving
about 46% of power without any loss in performance.
The remainder of this paper is organized as follows. In Section 2,
we review the related work and highlight our contributions.
Section 3 overviews the proposed framework and provides a
detailed analysis. Experimental results are included in Section 4.
Finally, Section 5 concludes the paper.
2. RELATED WORK
Generally speaking, feedback control reduces the systems sensitivity to parameter variations. So, design of nanoscale MPSoCs is
likely to benefit from the systematic approaches from modern control theory [5,11]. As such, in [9], the authors propose using adaptive body biasing and dynamic voltage scaling simultaneously to
reduce power in processors with a single clock domain. Partitioning NoCs into multiple voltage-clock islands to minimize the
energy consumption is considered in [10]. Power management for
NoCs have been recently considered by several authors. The work
in [13] presents a stochastic power management technique for
NoCs. In [12], the authors present a run-time technique to satisfy
Table 1: Notation used in the paper
Symbol
qi(t)
Q(t)
λi(k) 
(µi(k))
fj(k) (Vj(k))
λi, (µi)
K, K0, K1
R(k)
D(k)
Explanation
Occupancy of queue i at time t
State vector containing the occupancy of all queues.
Average arrival (departure) rate to (from) queue i in the kth 
control interval
Frequency (voltage) of domain j in the kth control interval
Average write (read) operations per cycle for queue i, e.g. 
λi(k) = λifj(k), where the input of queue i is in island j.
The gain matrices for the feedback controller
queue occupancy
Independent frequency input to the network
peak power consumption constraints in interconnection networks
by controlling the local power consumption of each router.
An on-line DVFS algorithm based on proportional-integralderivative (PID) controller for multiple clock domain processors is
presented in [16]. PID control requires manual tuning of the control
gain which may become prohibitive as the number of voltage-clock
domains increases. Moreover, PID control is useful for singleinput-single output systems, so a coordination mechanism is
needed in order to use it for controlling multiple queues across a
network [5]. 
To address these shortcomings, we develop a state-space model
for multiple clock domains and take the advantage of tools from
modern control theory. More precisely, we propose: 
• A state-space model for multiple voltage-frequency island NoC
design, and analysis of the system controllability and stability
• Feedback control algorithms that control the speed of the voltagefrequency islands to cope with the PVT and workload variations.
3. NoC CONTROL METHODOLOGY
3.1.  State-space Clock Domain Model and Notation
The utilization of any queue i at time t is denoted by qi(t), as
shown in Table 1. Since the voltage and frequency values cannot be
changed instantaneously, we define a control interval T during
which the operating values are fixed, e.g., the kth control interval is
given as [kT, (k+1)T]. 
We denote the utilization of the queue at the beginning of the kth
control interval by qi(k). The average arrival and service rates for
the kth interval associated with queue i are denoted by λi(k) and
µi(k), respectively. Therefore, the dynamics of a single queue can
be written as [16]:
q i k( )
=
q i k 1–(
) T λ i k
+
(
1–(
) µ i k 1–(
–
)
)
We note that the occupancy of finite queues is bounded from below
by zero, and from above by the queue capacity. So, Equation 1 represents the linearized dynamics around an operation point specified
by the reference inputs.
We show next that the closed loop system can be stabilized
around this operating value. We assume that the average arrival and
service rates are proportional to the clock frequency1, so we have:
λ i k( )
=
λ i
×
f1 k( )
,
µ i k( )
=
µ i
×
f 2 k( )
where f1(k) and f2(k) are the clock frequencies, while λi and µi are
the average write and read operations per cycle at the input and out(1)
1. If this relation does not hold, linearizing feedback as in [16] can be used.
615
put of the interface queue, respectively. Therefore, we can rewrite
Equation 1 as follows:
(2)
(3)
]N 1×
]N 1×
µ– i
q i k( )
=
) T λ i
+
q i k 1–(
f 1 k 1–(
)
f 2 k 1–(
)
Equation 2 describes the dynamics of a single queue that connects two different VFIs, as in Figure 2(a). Since, in general, in a
system there exist multiple islands, more than one queue needs to
be controlled. Hence, we define the state of the network with N
interface queues, at time k, as: 
Q k( )
[
q 1 k( ) q 2 k( ) … q, N k( )
,
,
]T
=
For a system with M VFIs and N queues, the state-space of the collective queue dynamics is given by:
[
Q k( )
=
[
Q k 1–(
)
+
T B[
[
F k 1–(
)
In this equation, the M×1 vector F(k) is the control input where the
ith entry gives the frequency of the clock domain i. The N×M B
matrix, on the other hand, describes the average read and write
operations to the queue based on the system topology. 
Illustration of the modeling approach: Matrix B in Equation 3
has one row for each state, and one column for each control input.
The (i,j)th entry of B is the rate of write (read) operations at the
input (output) of the ith queue due to the activity in the jth voltagefrequency island. So, each row has at most two nonzero entries. For
example, in Figure 2(a), B is a 1×2 matrix: B(1,1) gives the rate at
which island 1 writes to the queue, while B(1,2) is the rate at which
island 2 reads from the queue. For this system, B=[λi – µi], as
illustrated in Figure 2(a). A larger example with two VFIs and two
queues is illustrated in Figure 2(b). 
]N M×
]M 1×
B = [λ1 –µ1], (The minus sign signifies the read operation)
(a)
(b)
λ1
λ1
µ2
f1
f1
µ1
µ1
λ2
 f2
 f2
F k( )
=
f 1 k( )
f 2 k( )
⇒ BF(k)=λ1f1(k)–µ1f2(k)
as in Equation 2
=
B
λ1
µ– 1
µ– 2 λ2
{For the first queue
{For the second queue
Governed by the second domain
Governed by the first domain
Figure 2 Illustration of the modeling framework with two
VFIs when there are (a) one and (b) two interface queues.
3.2.  Controllability of Queue Dynamics
We first analyze the state-space model given in Equation 3 and
derive the conditions under which the system is controllable. Referring to Figure 1(b), these conditions determine which queues can
be controlled for a given VFI configuration. As explained in
Section 3.1, the number of interface queues (N) determines the
dimensions of the state vector Q, while the number of VFIs (M)
gives the number of control inputs, assuming that the operating
voltage and frequency of each island can be controlled independently. We note that in general, there may exist more interface
queues than islands (N ≥ M), as shown in Figure 1(b). Our first
original result determines the maximum number of queues that can
be controlled in a network with M islands: 
Theorem: In the multiple voltage-frequency island system with M
islands (i.e., M independently controllable clocks) described by
Equation 3, utilization of at most M queues at the interface of VFIs
can be controlled and the system is controllable iff rank(B) = N.
Proof: The proof follows from the analysis of the model in
Equation 3; namely, the controllability matrix U for this system is:
B IB I2B … IN 1– B
U
=
[
]
where I is the N×N identity matrix. The rank of U is obviously equal
to the rank of B which is of size N×M:
rank(U) = rank(B) ≤ min(M,N)
We know that the system is controllable iff rank(U) =N [11]. So, the
system can be state controllable only if N ≤ M, i.e., the number of
queues under control is less than or equal to the number of VFIs.
Furthermore, since rank(U) = rank(B), the system is state controllable iff rank(B) = N,                                                                    
As a result, we can only control M out of N interface queues. In
fact, the reduced order system with M queues represents a controllable subspace. Since M, i.e., the number of VFIs is expected to be
small [10,14], this also implies a low controller complexity and
implementation overhead, as discussed in Section 4.3. In the
remainder of this paper, we focus on this controllable subspace.
Whenever there exists more than one interface queue between two
VFIs, we choose to control the one with heavier traffic, since this has
the highest impact on power consumption and performance. However, we note that the proposed controllers are independent of this
choice provided that B is of full rank; so the technique is general.
3.3.  The Regulator Synthesis Problem
In this section, we consider the regulation problem where the
nominal operating voltage and frequency for each island, (Vi , fi) for
i=1… M, are already determined (off-line or on-line) by another
algorithm such as [10]. For instance, this could be done during partitioning of the network into distinct VFIs to optimize the energy
consumption while balancing the workload. Since this optimization
depends on nominal parameter values known at design time, this
may, in fact, result in a performance mismatch between different
islands; e.g., one of the VFIs may end up working unnecessarily
fast. Our goal here is to design a regulation mechanism such that
the voltage and frequency values are dynamically-controlled
around these nominal values as follows: 
V i k( )
=
V i ∆V i k( )
+
,
f i k( )
+=
f i ∆ f i k( )
The block diagram of the control system used to achieve this goal is
depicted in Figure 3(a). In this diagram, R(k) is an N×1 vector which
denotes the reference utilization of the controlled queues. Our goal
is to find the M×N gain (K0) and state feedback (K) matrices that
will place the eigenvalues of the closed loop system inside the unit
circle and hence stabilize the system around the reference points. 
3.3.1.  Controller Design
We can rewrite Equation 3 by setting 
F k( ) K 0R k( ) KQ k( )
=
–
Q k( )
=
–(
I T BK
)Q k 1–(
) TBK 0R k( )
+
(4)
The system described by Equation 4 is asymptotically stable around
the set point if and only if, the eigenvalues of (I – TBK) are within
the unit circle [11]. Moreover, a steady-state analysis reveals that
K0 should be set equal to K so that the steady-state queue utilizations are equal to the set point given by R(k).
1:
1. With some manipulations, average utilization can be also used as feedback.
616
 
Gain matrix
Gain matrix
R(k)
R(k)
K0 
K0 
queue occupancies
queue occupancies
F(k)
F(k)
+
+
+
∑
∑
∑
B 
B 
+
+
+
+
∑
∑
z-1IN×N
z-1IN×N
Q(k)
Q(k)
R(k)
R(k)
+
+
+
∑
∑
∑
+
+
+
+
+ ∑
+
∑
∑
IN×N
IN×N
Queues under control (Equation 3)
Queues under control (Equation 3)
z-1IN×N
z-1IN×N
Integrator  stage
Integrator  stage
(a)
K 
K K 
State feedback matrix
State feedback matrix
(b)
D(k)
D(k)
+
+
+
∑
∑
∑
F(k)
F(k)
B 
B 
C 
C 
+
+
+
+
K1
K1
Integral
Integral
gain
gain
Independent 
Independent 
frequency input
frequency input
∑
∑
z-1IN×N
z-1IN×N
Q(k)
Q(k)
IN×N
IN×N
Queues under control (Equation 3)
Queues under control (Equation 3)
K State feedback matrix
K K State feedback matrix
Figure 3 Block diagrams for the control systems for (a) regulation and (b) tracking problems are shown. 
The state formalism brings the opportunity to use a rich set of
describe how the F(k) and D(k) affect the queue dynamics, as in the
control techniques to determine the state feedback matrix K. For
previous case. We also note that, for the simple system described by
instance, we can set K to place the eigenvalues of (I – TBK) to the
Equation 6, B = –µ1 and C = λ1.
desired locations inside the unit circle. We can also choose K using
We first show why the controller developed in the previous secthe so called linear quadratic regulator (LQR) to minimize a pertion cannot be directly used to regulate the queue utilizations in the
formance index (J):
presence of such external frequencies. For this purpose, we take the
z-transform of Equation 7 to compute the transfer function from
D(k) to Q(k) assuming that F(k) = –KQ(k)
Q z( )
(
z I
I–
T BK
) 1– TCD z( )
=
+
When the input frequencies are described by a step input, D(z) = z/
(z-1)×IM2×1, where IM2×1 =[1,1,...,1]T is a M2×1 column vector. By
the final value theorem [11]:
lim
Q k( )
=
lim
1 z 1––(
where G and H are positive definite matrices that weight the relative importance of the state and control variables, respectively. For
example, the H weights can be selected to prohibit large overshoots
in frequency, hence voltage, and to minimize power consumption.
3.3.2.  Further considerations
QT k( )GQ k( ) FT k( )HF k( )
+
∑=
)Q z( )
BK(
k
0=
(5)
(8)
∞
[
(9)
=
) 1– C IM 2
1×
J
]
The techniques developed herein target on-chip implementations
and so assume that the controller parameters are computed off-line
to minimize controller overhead. Further robustness to parameter
variations can be achieved by considering on-line gain scheduling
techniques or off-line computation of feedback matrices for various
operating conditions.
3.4.  The Tracking Problem
The system studied in Section 3.3 is homogeneous, in the sense
that all local frequencies are controlled by the proposed controller.
In general, the frequency of one or more VFIs can be fixed to satisfy
some given constraints. For example, input to an encoder system
(or output of a decoder system) is set to satisfy an encoding (decoding) rate. Our goal for this scenario is to control the voltage and
clock frequency of the remaining VFIs, so as to minimize the power
consumption, while maintaining the reference queue utilizations.
The diagram of the control system is depicted in Figure 3(b). In
contrast to the regulation case discussed above, in this case there
exists an external input (D(k)) which captures the frequency values
that are set independently. Again, to provide some intuition, we
start with a two-clock domain network with a single interface
queue. The state-space model can be written as:
q k( )
=
q k 1–(
) Tµ1 f2 T+ λ
–
Assume that the frequency of the first VFI (i.e., f1) is fixed. We
need to control f2 and V2 to minimize the power consumption, while
maintaining the reference queue utilization. Intuitively, f2 should
follow f1 such that the queue utilization is stabilized. In general,
when there are M1 VFIs under control and M2 independent VFIs,
the state-space model becomes:
Q k( )
Q k 1–(
) TBF k 1–(
) TCD k 1–(
)
=
+
+
where [F(k)]M1×1 and [D(k)]M2×1 represent the controlled and
external frequencies, respectively. Likewise, the matrices B and C
1 f1
(6)
(7)
∞→
k
z
1→
Thus, the external inputs do affect the steady-state queue utilizations. To make the queue utilization independent of the external frequency, we add an integrator stage in the front of the state feedback
controller, as highlighted in Figure 3(b). Intuitively, the integrator
adds a (z-1) term to Q(z), so the right hand side of Equation 9 vanishes as z→1. The addition of the integrator doubles the number of
states, since the error term for each state is integrated. Consequently, the original system needs to be augmented with new states.
We note that the controllability of the original system is a sufficient
condition for the controllability of the augmented system [11].
Then, the state feedback matrix K and gain matrix K1 can be computed using the techniques discussed in Section 3.3 to track the
changes workload variations and input frequency, thereby achieving significant power savings.
3.4.1.  Robustness to workload variations
To illustrate the impact of workload variation on the controller
design, we consider a simple network with two VFIs. Suppose the
first VFI operates at a fixed frequency, f1, and determines the workload to the second VFI. We consider the following simple closed
loop system:
(10)
q k( )
=
–(
1 TµK
) q k 1–(
) T λ f1
+
–<–
1 TµK 1<
The stability conditions is given as 
1
. However, if
the nominal service rate µ can vary by ∆µ, we should write:
–<–
1
1 T µ ∆ µ+(
) K 1
<
0
<
K
⁄<
2 T µ ∆µ+(
(
)
)
Consequently, selecting K based on the highest possible service rate
guarantees stability in the presence of variations of the nominal values. We note that the PID based controllers [11] do not provide a
systematic approach to deal with such variations and they would
need manual tuning even for this simple system. 
(11)
617
 
It is also possible to answer the converse question, i.e., for a
given K, how much variation can the system tolerate before losing
stability? To answer this question for this example, we reorganize
Equation 11 as follows:
-µ < ∆µ < 2/(ΤΚ) − µ
This equation implies that a negative variation whose amplitude is
less than µ, and a positive variation with amplitude less than (2/TKµ), are acceptable. In general, for systems of larger dimensions, the
feedback matrix K should be selected such that the stability condition is satisfied for a range of parameter variations.
(12)
4. EXPERIMENTAL RESULTS 
In this section, we demonstrate the effectiveness of the proposed
control techniques on several hardware designs. The interface
queues we control are block RAM-based mixed-clock FIFOs from
the Xilinx library [18]. Voltage conversion is also performed at the
VFI interfaces; we assume 0.9 efficiency and 10µF load capacitance for voltage conversion to compute the switching overhead as
in [16]. Whenever slowing down a clock domain, first the frequency is set to the desired value and then the voltage is scaled
down. Similarly, the voltage is ramped up first when speeding a
domain up. Since, the voltage transitions may take more than 10
µsec [4,17], we conservatively set the control interval, T ≈ 100
µsec; for implementation purposes, the exact value is selected such
that T is a power of two times the slowest clock in the system.
4.1.  Robustness to Parameter Variations
In the first set of experiments, we evaluate the performance of the
proposed controllers in the presence of parameter variations and
bursty write and read operations. When there is no closed loop controller, even a small deviation from the nominal frequency values
results in fully utilized or empty interface queues, i.e., a performance mismatch. On the other hand, the proposed regulation
scheme successfully stabilizes the queue utilization under different
application scenarios, as described below. 
Comparison with PID controller: We first design a state-space
based controller for the system in Section 3.4.1 (described by
Equation 10). We verified both theoretically (using Equation 12)
and experimentally that the closed loop system is stable in presence
of 80% variation in the nominal value of the service rate µ. On the
other hand, a manually tuned PID controller for the same system
becomes unstable for as low as 10% variation in service rate.
V1, f1
V1, f1
V1, f1
V1, f1
V2, f2
V2, f2
V1, f1
V1, f1
V2, f2
V2, f2
q2
q2
q1
q1
V2, f2
V2, f2
V1, f1
V1, f1
V2, f2
V2, f2
q1
q1
q2
q2
V3, f3
V3, f3
q3
q3
q2
q2
q1
q1
q3
q3
(a)
q4
q4
V4, f4
V4, f4
V3, f3
V3, f3
V3, f3
V3, f3
Motion
Motion
Estimation
Estimation
Motion
Motion
Compensation
Compensation
Input
Input
Buffer
Buffer
R
R
R
R
Variable 
Variable 
Length 
Length 
Encoder
Encoder
(b)
Inv. Quantization
Inv. Quantization
Inv. DCT
Inv. DCT
DCT &
DCT &
Quantization
Quantization
Frame
Frame
Buffer
Buffer
Figure 4 NoCs studied in (a) Section 4.1 and (b) Section 4.2.
618
y
c
n
a
e
u
p
e
u
u
c
c
Q
O
i
t
y
n
c
o
n
e
a
u
q
a
e
i
r
r
V
F
%
n
i
10
5
0
0
4 
0 
- 4
- 8
0
y
c
n
a
p
u
c
c
O
e
u
e
u
Q
10
5
0
0
Queue 1
Queue 2
10
20
30
Con tr o l I n te r va ls
40
50
10
20
30
Con tr o l I n te r va ls
f 1
40
f 2
50
Queue 1
Queue 2
Queue 3
Bu r s ty wr i te
Bu r s ty r ead
(a)
(b)
10
20
30
Con tr o l I n te r va ls
40
50
Figure 5 (a) Queue occupancy and variation in the input frequency for the NoCs with (a) two and (b) three VFIs.
Regulator with a larger number of islands: We also consider networks with two, three, and four islands shown in Figure 4 and
design regulators as explained in Section 3.3. We design the regulator system for the two-clock domain network to obtain a fast
response with less than 10% overshoot around the nominal frequency. For the resulting regulator system, we identify the maximum permissible variation in the arrival and service rates for the
closed loop system as being about 20% of the nominal values.
Then, the simulation is repeated by varying the arrival and service
rates of both domains by 20% around the nominal value. As shown
in Figure 5(a), we observe that the controller indeed stabilizes the
network successfully within six control intervals.
Bursty read/write operations: Similarly, simulations are performed for the three-clock domain network shown in the center of
Figure 4(a). In this case, we also inject (eject) bursty data to (from)
queue q1 in the 20th (40th) control interval, as shown in Figure 5(b).
We observe that this sudden change in the utilization of q1 is rapidly
pushed back to the reference point. It is also interesting to note that
in order to reduce the utilization of q1, the second VFI in Figure 4(a)
has to operate faster. This results in a smaller spike in the utilization
of q2, and eventually q3, during the 22nd control interval. Finally,
we obtain similar results for the 4-domain network in Figure 4(a);
they are not shown here due to space limitations.
4.2.  Experiments with MPEG-2 Encoder
The second set of experiments demonstrate the application of the
tracking controller to an NoC-based hardware implementation of
an MPEG-2 encoder presented in [6]. We divided this design into
three domains (Figure 4(b)). The first domain contains the input
buffer and related control logic which prepares the macroblocks to
be read by the discrete-cosine transform (DCT) and motion estimation (ME) modules. The third domain contains the run-length
encoder, Huffmann encoder and zig-zag modules, while the second
domain contains the remaining cores. We compare the power consumption of this design against the power consumption of a baseline design which has only a single clock domain.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The encoder system needs to meet a target frame rate of 50
Frames/sec for 352×288 CIF frames. The baseline design has to
operate at 100MHz to achieve this frame rate. Direct measurements
on the hardware reveal that this system has an average power consumption of 1.90W. The minimum frequency for the single clock
domain system is dictated by the ME and DCT modules. In order to
achieve the target throughput with the design with three VFIs, we
set the frequency of the first island (f1) such that it will accept the
raw frames and prepare data for the ME, MC and DCT modules at
the target rate of 50 Frames/sec; that is, in terms of the block diagram shown in Figure 3(b), the independent frequency is f1, i.e.,
D(k) =f1 in Equation 7. Once f1 and the reference utilizations for
the queues are set, the frequency of the second domain (f2) is set
automatically by the controller to match to the speed of the first
domain. Likewise, the controller adjusts the frequency of the third
domain (f3) accordingly. We note that when the domains operate at
their nominal workloads, f2 is indeed set to 100MHz.
Obviously, we can achieve about 10% power savings by operating the first and third domains at lower frequencies without affecting the overall performance. However, the real savings are obtained
due to the variation in the workload. It has been observed that there
is significant variation in the contents (hence size and processing
time) of different macroblocks [15]. So the workload of the second
domain varies significantly for each macroblock. As a result, the
controller can dynamically lower the operating frequency whenever needed. Indeed, the frequencies of the second and third VFIs
adjust to the workload changes, as we show in Figure 6. As a result
of adjusting the frequency, about 20% power savings can be
achieved. When we also scale voltage accordingly, power savings
jump to about 46% after accounting for the DVS overhead. 
(a)
y
c
n
e
u
q
z
e
)
H
G
r
F
(
k
c
o
l
C
(b)
H
G
r
F
y
c
n
e
u
q
z
e
)
(
k
c
o
l
C
0 . 2
0 . 1
0
0
0 . 04
0 . 02
0
- 0 . 02
0
Nom ina l c loc k f r equenc y ( f 2 )
Ac t ua l c lo c k f r equenc y
20
40
60
Con tr o l I n ter va l
80
100
Nomi na l c loc k f r equenc y ( f 3 )
20
40
60
Con tr o l I n ter va l
Ac tua l c loc k f r equenc y
80
100
Figure 6 The frequency adopted by the second (a) and third
(b) clock domain in response to workload variations, thereby"
Application mapping for chip multiprocessors.,"The problem attacked in this paper is one of automatically mapping an application onto a network-on-chip (NoC) based chip multiprocessor (CMP) architecture in a locality-aware fashion. The proposed compiler approach has four major steps: task scheduling, processor mapping, data mapping, and packet routing. In the first step, the application code is parallelized and the resulting parallel threads are assigned to virtual processors. The second step implements a virtual processor-to-physical processor mapping. The goal of this mapping is to ensure that the threads that are expected to communicate frequently with each other are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to memories attached to CMP nodes. The main objective of this mapping is to place a given data item into a node which is close to the nodes that access it. The last step of our approach determines the paths (between memories and processors) for data to travel in an energy efficient manner. In this paper, we describe the compiler algorithms we implemented in detail and present an experimental evaluation of the framework. In our evaluation, we test our entire framework as well as the impact of omitting some of its steps. This experimental analysis clearly shows that the proposed framework reduces energy consumption of our applications significantly (27.41% on average over a pure performance oriented application mapping strategy) as a result of improved locality of data accesses.","Application Mapping for Chip Multiprocessors∗
35.2
Guangyu Chen
Microsoft
guchen@microsoft.com
Feihui Li
NVIDIA
ﬂi@nvidia.com
S. W. Son, M. Kandemir
Penn State University
{sson,kandemir}@cse.psu.edu
ABSTRACT
The problem attacked in this paper is one of automatically mapping an application onto a Network-on-Chip (NoC) based chip multiprocessor (CMP)
architecture in a locality-aware fashion. The proposed compiler approach
has four major steps: task scheduling, processor mapping, data mapping,
and packet routing.
In the ﬁrst step, the application code is parallelized
and the resulting parallel threads are assigned to virtual processors. The
second step implements a virtual processor-to-physical processor mapping.
The goal of this mapping is to ensure that the threads that are expected to
communicate frequently with each other are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to
memories attached to CMP nodes. The main objective of this mapping is to
place a given data item into a node which is close to the nodes that access it.
The last step of our approach determines the paths (between memories and
processors) for data to travel in an energy efﬁcient manner. In this paper,
we describe the compiler algorithms we implemented in detail and present
an experimental evaluation of the framework. In our evaluation, we test our
entire framework as well as the impact of omitting some of its steps. This
experimental analysis clearly shows that the proposed framework reduces
energy consumption of our applications signiﬁcantly (27.41% on average
over a pure performance oriented application mapping strategy) as a result
of improved locality of data accesses.
Categories and Subject Descriptors
D.3.m [Software]: Programming Languages – Miscellaneous
General Terms
Languages, Experimentation
Keywords
Compilers, NoC (Network on Chip), Power Optimization, Application Mapping, Chip Multiprocessing.
1.
INTRODUCTION
While there already exist efforts on building robust Network-on-Chip
(NoC) based computation platforms, programming these platforms is an
entirely different matter. In particular, current optimizing and parallelizing
compilers do not give much support for mapping applications onto NoC
∗ This work is supported by NSF grants CNS #0720645, CCF #0702519, a
grant from Microsoft Corporation, and a support from the Gigascale Systems Research Focus Center, one of the ﬁve research centers funded under
SRC’s Focus Center Research Program.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2008, June 8–13, 2008, Anaheim, California, USA.
Copyright 2008 ACM ACM 978-1-60558-115-6/08/0006 ...$5.00.
based parallel execution platforms (e.g., an NoC based chip multiprocessor,
CMP) in an energy-aware fashion. While one may try to employ known
code parallelization techniques from the high-performance computing domain [2, 3, 10, 24], such techniques are not very suitable for NoC-based
CMPs mainly because (1) these techniques are mostly performance oriented
and do not consider other important metrics such as power and (2) they do
not take the network structure explicitly into account, and thus, the only
distinction (as far as the locations of data elements are concerned) these
prior techniques consider is local versus nonlocal data. In an NoC based
execution environment, however, the exact location of non-local data matters a lot, in particular from the power consumption angle. This is because,
fewer the number of links used to access data, the less energy consumption
is incurred. Therefore, customized compiler support for NoC based chip
multiprocessors is critical in our opinion.
The main contribution of this paper is a compiler framework that takes
the source code of an application and maps it to a chip multiprocessor system, the processors of which are connected to each other using a mesh
based NoC. The proposed application mapping approach tries to optimize
the locality of data accesses, and can be beneﬁcial from both the power and
performance perspectives. It has four major steps: task scheduling, processor mapping, data mapping, and packet routing. In the ﬁrst step, the application code is parallelized and the resulting parallel threads are assigned to
virtual processors. The second step implements a virtual-to-physical processor mapping. The goal of this mapping is to ensure that the threads that
are expected to communicate frequently are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to
the memories attached to the CMP nodes. The main objective of this mapping is to place a given data item into a node which is close to the nodes
that access it. The last step of our approach determines the paths (between
memories and processors) for data to travel in an energy efﬁcient manner.
In this paper, we describe the compiler algorithms we implemented using the SUIF framework [1] and present an experimental evaluation of the
framework. In our evaluation, we test our entire framework as well as the
impact of omitting some of its steps. This experimental analysis clearly
shows that the proposed framework reduces energy consumption of our applications signiﬁcantly (27.41% on average over a pure performance oriented application mapping scheme).
Section 2 introduces the architectural model we consider in this paper
and Section 3 explains the application execution model. The details of our
four-step approach are presented in Section 4. Section 5 gives an experimental evaluation of the proposed approach. Section 6 concludes the paper
by summarizing our major contributions.
2. ARCHITECTURAL MODEL
Our architectural model is an NoC based CMP with two dimensional
mesh topology. Each node in the mesh consists of a router, a processor,
and a memory component. The memory space in a node is managed by the
compiler. We use ηi to denote the ith node of the mesh, and each pair of
adjacent nodes, ηi and ηj , are assumed to be connected by a pair of directed
channels, one from ηi to ηj and the other from ηj to ηi . A processor
accesses a nonlocal data element by sending and receiving packets over the
network. Let us assume that the processor in node ηi needs to read a data
element that is stored in the memory of node ηj . The processor in node ηi
ﬁrst sends a data packet containing the address of the data element to node
ηj ; node ηj responds by sending back a packet containing the value of the
data element. On the other hand, if the processor in ηi needs to update a data
element that is stored in the memory of node ηj , it sends a packet containing
both the address and the new value of the data element to be updated to
node ηj ; upon receiving this packet, the latter updates the value of the data
element accordingly. It is important to mention that this NoC based CMP
architecture is very generic and represents several possible instantiations.
It is also worth mentioning that, while we discuss our application mapping
scheme in the context of this two-dimensional mesh NoC, our approach is
applicable to other types of network topologies as well, provided that the
targeted topology is exposed to the compiler. Going to a different topology
can change the concept of locality we have (as there will be a change in
the number of neighbors for a given node), but our algorithms can be easily
adapted to work with this change.
620
A[0..99]
A[100..199]
A[200..299]
A[300..399]
B[0..99]
B[100..199]
C[0..99]
C[100..199]
D[0..99]
D[100..199]
t1
t2
t3
t4
t5
t6
t7
t8
t9 t10
0
0
0
0
50
r
0
0
0
0
50
0
50
0
0
w 100
0
0
0
0
0
0
0
0
r
0
0
0
0
50
0
50
0
0
w
0 100
0
0
0
0
0
0
0
r
0
0
0
0
0
50
0
0
0
w
0
0 100
0
0
0
0
0
0
r
0
0
0
0
0
50
0
50
0
0
w
0
0
0 100
0
0
0
0
0
0
r
0
0
0
0
0
0
0
0 100
0
w
0
0
0
0 100
0
0
0
0
0
r
0
0
0
0
0
0
0
0
0 100
w
0
0
0
0
0 100
0
0
0
0
r
0
0
0
0
0
0
0
0 100
0
w
0
0
0
0
0
0 100
0
0
0
r
0
0
0
0
0
0
0
0
0 100
w
0
0
0
0
0
0
0 100
0
0
r
0
0
0
0
0
0
0
0
0
0
w
0
0
0
0
0
0
0
0 100
0
r
0
0
0
0
0
0
0
0
0
0
w
0
0
0
0
0
0
0
0
0 100
Table 1: The values of r(t, d) and w(t, d) for the tasks shown in
Figure 1(b).
assume that r(t, d) and w(t, d) for each task-data pair (t, d) are stored in
a two-dimensional data access table. Note that the values of r(t, d) and
w(t, d) can be determined either by static analysis of the code or through
proﬁling (our implementation can use both).
As an example, let us assume a code fragment, which contains four loops,
as shown in Figure 1(a). Assume further that, using a parallelizing compiler,
we extract tasks t1 , t2 , t3 , and t4 from loop L1; tasks t5 and t6 from loop
L2; t7 and t8 from loop L3; and tasks t9 and t10 from loop L4. Figure 1(b)
shows the corresponding task graph. For this code fragment, we have:
T = {t1 , t2 , t3 , t4 , t5 , t6 , t7 , t8 , t9 , t10 }
D = {A[0..99], A[100..199], A[200..299], A[300..399],
B [0..99], B [100..199], C [0..99], C [100..199],
D [0..99], D [100..199]}
The values of r(t, d) and w(t, d) for these tasks are listed in Table 1, assuming that each statement in the code has the same unit cost.
4. OUR APPROACH
Our goal is to schedule the tasks on the available processors and map
the data blocks into the memories of the CMP nodes such that the overall execution time of the application and the energy it spends during its
execution are reduced. The proposed approach achieves this goal by improving the locality of data accesses, that is, by reducing the number of
communication links that are visited in accessing data. Our scheme takes
a task graph (such as the one shown in Figure 1(b)) and a data access table
(such as the one shown in Table 1) as input. It works in four steps as depicted in Figure 2: task scheduling, processor mapping, data mapping, and
packet routing. The goal of the task scheduling step is to cluster the tasks
that share a large amount of data blocks among them into the same processor. In order to isolate task scheduling from processor mapping, we ﬁrst
schedule the tasks on a set of virtual processors, which will be explained
shortly. After that, in the processor mapping step, we map the virtual processors onto physical processors to improve inter-processor data locality.
That is, two virtual processors sharing a large amount of data are mapped
to a pair of neighboring physical processors. In the next step, we map data
blocks into the memories of the NoC nodes such that the overall memory
access cost is minimized. Finally, we determine the routings for the data
packets, i.e., the set of links that are exercised during data accesses. Note
that the cost for accessing a data block is determined by the distance between the processor that issues the access and the node that contains the
data block. Speciﬁcally, a longer distance to data (which is a measure of
locality) means that the access request and the data block need to be transferred over a larger number of network links1 , and thus, incurs more energy
consumption and longer delay (depending on the network switching mechanism employed). Our compiler-based approach improves performance and
reduces energy consumption by mapping the tasks and the data blocks into
the CMP nodes such that we can reduce the distances between the processors and the data blocks that are frequently accessed by these processors.
In our experimental evaluation, we also compare this locality-oriented approach to pure performance-oriented and pure-energy oriented schemes.
1We use the terms “link”, “channel”, and “hop” interchangeably.
(a) Example code fragment
(b) A possible task graph for the code fragment in (a).
Figure 1: A code fragment and its task graph.
The path consisting of channels used to transfer a message from the
source node to the destination is determined by the routing scheme employed by the underlying implementation of the mesh. Routing schemes
[18] can be classiﬁed into two categories: dynamic and static. Dynamic
routing determines the communication channels used to transfer each message dynamically based on the network trafﬁc state during the message
transfer time. Static routing, on the other hand, determines the channels
to be used to transfer each message statically based on the source and destination nodes of the message, irrespective of the dynamic network trafﬁc
state at runtime. The steps of the compiler-based approach proposed in this
paper, except for the step described in Section 4.4, can work under both dynamic and static routing based NoCs. When there is no confusion, we use
the terms “node” and “processor” interchangeably. Also, we use [[i, j ]] to
denote the logical connection between nodes ηi and ηj .
3. APPLICATION EXECUTION MODEL
As noted by prior work [9, 11, 7, 8, 13, 21, 19], application mapping
is critical for NoC based platforms. To our knowledge, this is the ﬁrst
compiler-directed fully automated application mapping scheme for mesh
based CMPs. In this paper, we focus on array-based, loop-intensive applications. Such an application typically consists of a set of loop nests that
access a set of arrays. Array-based codes frequently appear in the embedded image/video processing domain [6], an important application segment
for the NoC based systems. We further assume that a parallelizing compiler
transforms each loop nest of the application into a set of parallel tasks such
that each task contains a subset of the iterations of a given loop nest. The
question of how these tasks are generated is orthogonal to the problem addressed in this paper. These tasks, also called threads in this paper, are then
scheduled, by our approach, to be executed in parallel in our NoC based
CMP system. Note that, in this paper, a task is the smallest unit for scheduling. The order in which the tasks of a given application can be executed is
determined by data dependencies, which are captured in our approach by
a directed graph called the task graph. Speciﬁcally, a task graph can be
represented as G = (T, E), where T and E are the set of vertices and the
set of edges, respectively. Each vertex in T corresponds to a task. An edge
(t1 , t2 ) ∈ E ⊆ T × T indicates that task t1 must be completed before the
execution of task t2 due to data/control dependences or other constraints.
For a given task t, we use ρ(t) to denote the length of its execution time
(in terms of cycles), under the assumption that all the data processed by t
can be accessed without any network delay, i.e., all the data accessed by
task t are stored in the memory that is in the same node as the CPU which
executes task t.
We further assume that the data operated by the application code can
be divided into a set of blocks. This is a reasonable assumption since, as
mentioned earlier, our framework targets at applications that process mostly
array data, which can be (logically) divided into blocks. We use D to denote
the set of data blocks operated by the application. The location of a data
block in the mesh based NoC is identiﬁed using a tuple (i, o), where i is
the id of the node that contains the block and o is the address of the block
within the memory component of this node. For a given task t and a data
block d, we deﬁne functions r(t, d) and w(t, d) as the number of times that
task t reads and writes, respectively, data block d during its execution. We
621
Figure 2: High level view of our approach.
4.1 Task Scheduling
Our task scheduling algorithm takes the number of virtual processors P
and the task graph of the application as input. It determines to which virtual
processor each task should be assigned. Our goal in this step is to exploit
the parallelism in the application code as much as possible, and maximize
data reuse within each processor by scheduling the tasks that share a large
number of data blocks on the same virtual processor.
Note that, instead of directly mapping tasks to physical processors in
our NoC based CMP, we ﬁrst schedule them on virtual processors. The
advantage is that, using this concept of virtual processors, the scheduling
and mapping steps of our approach can be optimized independently. This
also reduces the complexity of retargeting the same source code to different CMP architectures with different number of physical processors. The
number of virtual processors can be equal to or greater than the number of
physical processors. In our current implementation however, the number of
the virtual processors is the same as that of the physical processors.
Figure 3 gives our task scheduling algorithm. The complexity of this
algorithm is O(|T|2 ), where T is the set of tasks to be scheduled. In this
algorithm, for each virtual processor v, we maintain two properties: Q[v]
and D [v]. Speciﬁcally, Q[v] gives the time when virtual processor v becomes available, and D [v] tracks the set of data blocks that are accessed
by virtual processor v. At each iteration of the while-loop in our algorithm
shown in Figure 3, we schedule a task on virtual processor v at the earliest available time captured by Q[v]. Our algorithm terminates when all the
tasks in the task graph have been scheduled. After we schedule a task t on
virtual processor v, we update the values of Q[v] and D [v] as follows:
Q[v] = Q[v] + ρ(t);
D [v] = D [v] ∪ B [t],
where B [t] is the set of data blocks used by task t. When selecting a task t to
be scheduled on virtual processor v, we try to maximize the value of |D [v]
Input:
P – the number of virtual processors.
χ[1..P, 1..P ] – χ[i, j ] is value of inter-processor sharing function
for virtual processors vi and vj .
Output:
X [1..P ], Y [1..P ] – (X [i], Y [i]) is the coordinate of the
physical processor in the mesh to which virtual processor vi is mapped.
k = 1;
for i = 1 to N
for j = 1 to N {
X [k] = i; Y [k] = j ; k = k + 1;
}
done = false; g = G();
while(not done) {
done = true;
for i = 1 to N
for j = 1 to N {
swap(X [i], X [j ]); swap(Y [i], Y [j ]);
g ′ = G();
if(g′ < g) {
g = g′ ; done = false;
} else {
swap(X [i], X [j ]); swap(Y [i], Y [j ]);
}
}
}
function int G() {
g = 0;
for i = 1 to N
for j = 1 to N
g = g + (|X [i] − X [j ]| + |Y [i] − Y [j ]|)χ[i, j ];
return g ;
}
Figure 5: Processor mapping algorithm.
depicted in Figure 6(c). We can calculate the distances among these processors, in terms of the network hops, as follows:
|p1 − p2 | = 2;
|p1 − p3 | = 1;
and
|p2 − p3 | = 1.
We now need to map virtual processors v1 , v2 , and v3 to these physical
processors. Our algorithm starts with π1 , which maps virtual processors v1 ,
v2 , and v3 to physical processors, p1 , p2 , and p3 , respectively, as shown in
Figure 6(c). The cost function for this mapping, G(π1 ), can be calculated
as 900. By switching the target physical processors of a pair of virtual processors, we can obtain three different mappings, π2 , π3 , and π4 from mapping π1 . Since G(π4 ) < G(π1 ), we continue our search from π4 . From
π4 , we obtain π5 , π6 , and π7 by switching the target physical processors
of a pair of virtual processors. Since none of these mappings yields a lower
data access cost (i.e., the G() function) than the current one, our algorithm
terminates giving π4 as the ﬁnal result, i.e., we map virtual processors v1 ,
v2 , and v3 to physical processors, p1 , p3 , and p2 , respectively. Figure 6(d)
gives the solution space explored by our algorithm for this example.
Note that, in general, for a large application, the solution space that has
to be explored by this mapping scheme can be large. Therefore, in our
baseline implementation, we limited the number of swaps performed by the
algorithm to 50 (i.e., at most 50 swaps are performed), a number which,
we found, performs well in practice. However, we also made experiments
with other (limit) values and with an implementation that does not adopt
any limit.
4.3 Data Mapping
The data mapping step of our approach is applied after the processor
mapping step. At this step, we distribute data blocks into the memories
attached to the NoC nodes such that the overall memory access cost is minimized. A data mapping can be expressed using a data mapping function φ
such that φ(d) gives the NoC node to which data block d is mapped. For
a given processor mapping π and a data mapping ψ , the overall memory
access cost can be computed as:
H (π , ψ) = Xd∈D Xv∈V
where function F is as deﬁned in Expression (1), and |π(v) − ψ(d)| is
the distance (in terms of network hops) between the physical processor of
virtual processor v and the node to which data block d is mapped. Note
that, in this equation,
|π(v) − ψ(d)|F (v, d),
|π(v) − ψ(d)|F (v, d)
Xv∈V
gives the overall cost due to the accesses to data block d.
Figure 7 shows our data mapping algorithm. The complexity of this
algorithm is O(|D|N 2 ), where N is the number of physical processors, and
623
v2
v3
v1
F (v, d)
A[0..99]
200
100
0
A[100..199]
100
200
0
A[200..299]
0
100
200
A[300..399]
0
100
200
B[0..99]
300
0
0
B[100..199]
0
300
0
C[0..99]
200
100
0
C[100..199]
0
200
100
D[0..99]
100
0
0
D[100..199]
0
100
0
(a) Access cost (i.e., F (v, d)) for each data
block from each virtual processor.
(c) Physical processors in a mesh.
v3
v1
v2
χ(vi , vj )
v1
0
300
0
v2
300
0
300
v3
0
300
0
(b)
Inter-processor data sharing values
(i.e., χ(vi , vj )) for the virtual processors
shown in Figure 4.
(d) Searching the solution space.
Figure 6: An example application of processor mapping.
D is the set of data blocks. For each data block d, this algorithm searches
every node in the NoC with sufﬁcient free memory space to hold data block
d. If there are more than one node with sufﬁcient free memory space to
hold data block d, we select the one with the minimum overall cost due to
the accesses to data block d.
4.4 Packet Routing
As discussed earlier in Section 2, in our NoC-based chip multiprocessor
architecture, CPUs access data by sending and receiving data packets over
communication links. We say that an application uses logical connection
[[i, j ]] if one of the following two conditions is satisﬁed:
• A task t reads or writes data block d, t is mapped to the physical
processor in node ηi , and d is mapped to the memory in node ηj . In this
case, task t in ηi sends the access request to node ηj through the logical
connection [[i, j ]].
• A task t reads data block d, t is mapped to the physical processor
in node ηj , and d is mapped to the memory in node ηi . In this case, the
memory in ηi sends the data required by task t in node ηj through the
logical connection [[i, j ]].
The goal of packet routing is to map each logical connection to a path
of communication channels that transfers the data packets from the source
node to the destination node. Two logical connections are said to interfere with each other if they might be exercised by the application simultaneously. If two logical connections interfere with each other, the routing
algorithm should map them to two paths that do not share any channels
between them. Otherwise, the packets transferred by these two logical connections might compete for the shared channels, which can in turn increase
the communication delays. On the other hand, if two logical connections
do not interfere with each other, i.e., the application never uses them simultaneously, we want to maximize the number of communication channels
that are shared by these logical connections, so that we can minimize the
number of communication channels exercised by the application, and thus,
more communication channels can be kept in the low power mode to conserve energy.
Input:
X [1..P ], Y [1..P ] – (X [v], Y [v]) is the coordinates of the physical
processor to which virtual processor v is mapped.
F [1..P, 1..P ] – F (v, d) gives the number of times that virtual
processor v accesses data block d.
Output:
DX [1..D], DY [1..D] – (DX [d], DY [d]) is the coordinate of the mesh
node to which data block d is mapped.
for each dk ∈ D {
c = ∞
for i = 1 to N
for j = 1 to N
if(s[i, j ] < S ) {
c′ = cost(dk , i, j );
if(c′ < c) {
DX [k] = i; DY [k] = j ; c = c′ ;
}
}
s[DX [k], DY [k]] = s[DX [k], DY [k]]+ sizeof(dk );
}
function int cost(d, i, j ) {
c = 0;
for v = 1 to P
c = c + (|X [v] − i| + |Y [v] − j |)F [v, d];
return c;
}
Figure 7: Data mapping algorithm.
Parameter
NoC Topology
NoC Dimensions
Processor IPC
Local Memory Capacity
Local Memory Access Latency
Off-Chip Memory Access Latency
Link Power
Switching Technique
Flit Size
Packet Header Size
Packet Size
Value
2D Mesh
6 × 6
2
64 KB/processor
2 cycles
160 cycles
0.145 W
Wormhole
39 bits
3 ﬂits
18 ﬂits
Table 2: Default system conﬁguration parameters.
For routing packets across the mesh in an energy-aware fashion, we employ a technique inspired by the algorithm proposed in [7]. This algorithm
reuses communication channels as much as possible to reduce energy consumption without signiﬁcantly increasing communication delay. Our implementation takes a communication interference graph as input and determines the routing for each data packet set. A communication interference
graph captures the interference between the pairs of logical connections.
Speciﬁcally, each vertex in this graph corresponds to a logical connection
used by the application, and each edge between a pair of vertices indicates
that the corresponding logical connections interfere with each other. In our
implementation, we built the communication interference graph from our
task graph.
5. EXPERIMENTAL EVALUATION
5.1 Setup
The compiler algorithms presented in Section 4 are implemented within
the SUIF compiler infrastructure from Stanford University [1]. The increase
in compilation time due to our approach (its four steps) over the case when
the codes in our experimental suite are compiled (parallelized) without our
passes was about 57%, when averaged over all twelve benchmarks we have.
The largest compilation time we observed with our approach was slightly
over 2.5 minutes. Also, the code size increase due to our approach was
less than 5% for all the applications tested. In order to quantify the beneﬁts
of our approach, we performed experiments using a simulation infrastructure, which has two components: Simics [25] and Orion [26]. Speciﬁcally,
an enhanced version of Simics (with cycle accurate memory access models) is used for simulating parallel execution of processors that share the
on-chip memory space, whereas Orion is used for modeling NoC based
communication and calculating communication energy. This simulation infrastructure is fully-coupled, i.e., the network delays are accounted for calculating CPU execution timings.
In addition, this simulation platform is
tuned (scaled) to simulate large number of CPUs and made cycle accurate.
This infrastructure is executed on a Solaris 9 machine. Our simulation environment takes, as input, an application executable and a network topology
description and generates, as output, several statistics, including the average
number of links traversed by each packet, the overall execution cycles, and
the energy consumed by the application (including network, memory and
W/out Pow Sav Tech
W/ Pow Sav Tech
Benchmark Number of Execution
Energy
Execution
Energy
Name
Links
Cycles Consumption Cycles Consumption
wupwise
swim
mgrid
applu
mesa
galgel
art
equake
ammp
lucas
fma3d
apsi
3.41
1.96
2.27
1.86
1.88
1.63
2.64
3.24
2.52
1.74
1.80
2.05
1083.5
1339.2
1690.9
1527.2
1298.8
11083.9
1791.7
1473.8
1440.1
814.6
1323.7
1264.9
3.87
4.96
5.12
5.07
4.74
14.51
4.94
4.71
4.57
2.36
4.82
3.95
1127.4
1408.1
1722.3
1599.3
1306.7
11507.6
1905.4
1488.2
1511.7
903.3
1352.0
1286.6
3.30
4.13
4.49
4.35
4.07
12.6
4.81
4.29
3.48
2.17
4.16
3.05
Table 3: Important statistics on the applications used in our
experiments.
computation energies). The energy numbers presented below include both
dynamic energy (i.e., the energy consumed due to switching activities) and
leakage energy (i.e., the energy consumed as long as the circuit is powered
on) components. As mentioned above, in computing the network energy,
we used an enhanced version of Orion. On the other hand, for computing
the CPU execution and memory access energies, we enhanced Simics with
energy models similar to those used by Wattch [5]. Table 2 presents the
default values of the simulation parameters used in the experiments.
The important statistics on the benchmark codes used in this study are
given in Table 3. We included all the SpecFP2000 benchmarks except facerec and sixtrack, which could not be executed through our simulation platform. The second, third, and fourth columns show, respectively, the average
number of links traversed by a packet, the total execution cycles, and energy consumption when the computation and data are mapped using the
approach described in Anderson’s thesis [2]. Anderson formulates the data
and computation mapping problem using a linear algebraic framework and
solves it using a heuristic.
In her formulation, each locality requirement
is expressed as a constraint. The energy consumption results presented in
Table 3 include both computation and communication energies. It is important to mention at this point that the approach in [2] is a pure performanceoriented one and does not take into account the network structure (though
it does an excellent job, in our opinion, in reducing the number of accesses
to remote data). In other words, this performance-oriented approach minimizes the number of nonlocal accesses but it does not care where the nonlocal data resides or how it is accessed (i.e., through which links). The last
two columns of Table 3 give the execution cycles and energy consumption
results when the approach in [2] is followed by an energy-saving scheme,
which turns off the unused links and scales down the voltage/frequency on
others whenever there is an opportunity to do so. The speciﬁc link shutdown and voltage scaling algorithms used are adapted from [16] and [8],
respectively. The link shut-down scheme [16] identiﬁes the communication links that will not be exercised by the current computation and turns
them off to save energy. The voltage scaling scheme [8], on the other hand,
identiﬁes the best voltage/frequency level for each communication link such
that energy consumption is reduced without affecting performance. In our
implementation of these schemes, we ﬁrst applied the link shut-down algorithm, and then, for the communication links that could not be shut down
completely, we applied the voltage scaling algorithm to reduce their power
consumptions. The important point to note from Table 3 is that applying
energy saving techniques reduces energy consumption by 13.71% on average and increases execution cycles by 3.77%. In our setting, the processors
are turned off when they are waiting in synchronization points; this prevents
them from consuming extra energy on synchronization points. Recall from
Section 3 that our approach can work with any code parallelization (task
generation) scheme. The speciﬁc one used in this work parallelizes the outermost loop from each dependence-free nest across available processors.
The sequential nests on the other hand are executed by a single processor.
5.2 Results
We now quantify the impact of our compiler-based application mapping
approach on the number of communication links traversed per packet, energy consumption and execution cycles. Our ﬁrst set of results are presented
in Figure 8 and show the average number of links traversed per packet when
our approach is employed. For ease of comparison, we also reproduce the
results from the second column of Table 3. We see from these results that
our approach cuts the number of links traversed signiﬁcantly. Speciﬁcally,
the average number of links traversed are 1.49 and 2.3 for our approach and
the approach in [2], respectively. This result underlines the importance of
taking the network topology into account in mapping tasks and data to the
NoC nodes, and also demonstrates that our approach improves locality of
data accesses on the mesh signiﬁcantly.
Before continuing with the rest of our experimental evaluation, we want
to point out that the average number of traversed links per message is not
very large (it is 2.30), even with the scheme in [2]. The reason for this is
624
0
w u p wis e
0.5
1
1.5
2
2.5
3
3.5
s wi m
m grid
a p plu
m e s a
g alg el
art
e q u a k e
a m
m p
lu c a s
f m a 3 d
a p si
A v era g e
A
e
v
r
e
g
a
N
u
m
e
b
r
o
f
s
k
n
L
i
e
P
r
e
k
c
a
P
t
Performance Oriented
Ours
Figure 8: Average number of
links exercised per packet under our approach and Anderson’"
Concurrent topology and routing optimization in automotive network integration.,"In this paper, a novel automatic approach for the concurrent topology and routing optimization that achieves a high quality network layout is proposed. This optimization is based on a specialized binary Integer Linear Program (ILP) in combination with a Multi-Objective Evolutionary Algorithm (MOEA). The ILP is formulated such that each solution represents a topology and routing that fulfills all requirements and demands of the network. Thus, in an iterative process, this ILP is solved to obtain feasible networks whereas the MOEA is used for the optimization of multiple even non-linear objectives and ensures a fast convergence towards the optimal solutions. Additionally, a domain specific preprocessing algorithm for the ILP is presented that decreases the problem complexity and, thus, allows to optimize large and complex networks efficiently. The experimental results validate the performance of this methodology on two state-of-the-art prototype automotive networks.","35.3
Concurrent Topology and Routing Optimization in
Automotive Network Integration
Mar tin Lukasiewycz† , Michael Glaß† , Christian Haubelt† , Jürgen Teich† ,
Richard Regler‡ , and Bardo Lang‡
†Depar tment of Computer Science 12
University of Erlangen-Nuremberg, Germany
{mar tin.lukasiewycz,glass,haubelt,teich}@cs.fau.de
‡EE-81
Audi AG, Ingolstadt, Germany
{richard.regler,bardo.lang}@audi.de
ABSTRACT
In this paper, a novel automatic approach for the concurrent topology and routing optimization that achieves a high
quality network layout is proposed. This optimization is
based on a specialized binary Integer Linear Program (ILP)
in combination with a Multi-Ob jective Evolutionary Algorithm (MOEA). The ILP is formulated such that each solution represents a topology and routing that fulﬁlls all requirements and demands of the network. Thus, in an iterative process, this ILP is solved to obtain feasible networks
whereas the MOEA is used for the optimization of multiple
even non-linear ob jectives and ensures a fast convergence towards the optimal solutions. Additionally, a domain speciﬁc
preprocessing algorithm for the ILP is presented that decreases the problem complexity and, thus, allows to optimize
large and complex networks eﬃciently. The experimental results validate the performance of this methodology on two
state-of-the-art prototype automotive networks.
Categories and Subject Descriptors
C.0 [General]: system architectures
General Terms
Algorithms, Design
Keywords
Automotive, Network Integration, Optimization
1.
INTRODUCTION AND PRIOR WORK
Communication networks are found amongst applications
of industrial controls or in the avionics and automotive area.
Due to the increasing complexity of the applications in these
areas, the size and communication demand of these networks
are steadily growing. In particular, state-of-the-art automotive networks consist of up to one hundred Electronic Control Units (ECUs). Distributed functions or applications,
respectively, are implemented on these ECUs whereas the
communication is performed via buses and gateways using
periodic messages.
Since automotive networks are operating in a safety-critical
area, the network requires a ﬁxed topology and a static routing. The event-triggered Control ler Area Network (CAN)
[2] is the dominating bus system in the automotive area.
In fact, it is still a common method that stringent busload
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2008, June 8–13, 2008, Anaheim, California, USA.
Copyright 2008 ACM ACM 978-1-60558-115-6/08/0006 ...$5.00.
626
constraints are applied to validate a feasible communication
and low latencies. This busload constraint is manufacturer
dependent and usually deﬁned by a maximal utilization of
the buses ranging from 40% to 60% [14].
In the integration phase, one ma jor task of the designer
is to determine the topology and routing for a given set
of communicating ECUs. The topology of the network is
determined by interconnecting these ECUs via buses and
gateways.
In a consecutive step, the routing of each single message has to be deﬁned on this topology such that
all communication demands and busload constraints are fulﬁlled. Nowadays, this integration is performed mainly manually by a designer in an incremental process. Therefore, the
complexity arising from the stringent requirements prohibits
an optimal layout of the network since many solutions are
neglected.
Common automatization approaches for the integration
phase in the automotive area are restricted to the optimization of parameters, like message priorities [16] or period determination [4]. To the best of our knowledge, an automatic
integration of automotive networks, i.e, a concurrent optimization of the topology and routing, is still unexplored.
Most common strategies for the optimization of network
layouts are based on Integer Linear Programs (ILPs) or Evolutionary Algorithms (EAs). Known ILP approaches [5, 13]
as well as the ma jority of EA approaches [6, 8] are restricted
to predeﬁned topologies like, e.g., spanning tree networks or
ring layouts and are not applicable on heterogeneous automotive networks. Additionally, these ILPs are mainly limited to relatively small problems and restricted to the optimization of a single linear function.
Recently, multi-stage EA approaches that also cover general topology layouts have been researched [1, 7, 15]. Though
being a fast heuristic that can handle several also non-linear
ob jectives, EAs tend to fail on constrained discrete optimization problems with only a few feasible solutions.
In fact,
many real-world network layout problems have just a few
feasible solutions due to the stringent busload constraints.
To overcome these drawbacks, we formalize the network
problem as an ILP and incorporate a PB solver that performs
the search for feasible solutions. The actual optimization is
realized by a combined PB and EA optimization approach
known as SAT decoding [9]. This methodology combines
the beneﬁts of both, EAs and ILPs, and allows an eﬃcient
optimization of the network layout of real-world automotive
systems.
The remainder of this paper is outlined as follows: Section 2 introduces the problem deﬁnition. The automatic
network design and optimization approach is proposed in
Section 3. Section 4 presents experimental results before the
paper is concluded in Section 5.
2. PROBLEM DEFINITION
One ma jor task in the integration phase of automotive
network design is to ﬁnd a feasible network for a given spec(cid:88)
m∈M ∧v∈Vm
busload constraints are of a special interest. Since the busload can be additively calculated, the constraint for a bus
v ∈ Va is
(Lm + O) ≤ l · Lv .
(1)
Where the capacity Lv ∈ N of the bus and the communication overhead of one message O ∈ N is given in kilobytes
per seconds, and the maximal utilization l is usually a value
between 40% and 60%.
With the deﬁnition of a feasible network, the task of network layout optimization can be formulated as the following
multi–ob jective optimization problem:
Definition 1
(Network Layout Optimization).
Figure 1: Graph representation of the integration task. The vertices EC U1 , ..., EC U4 are ECUs,
CC1 , ..., CC6 are communication controllers, and
BU S1 , BU S2 are buses.
iﬁcation, cf. Figure 1.
In the integration phase, all functions are already implemented on the ECUs and the designer
has to determine a network that fulﬁlls all system requirements. The speciﬁcation includes the layout varieties, i.e.,
the architecture as well as communication demands in the
form of messages. The architecture deﬁnes a set of all available resources that can be used to model a feasible network.
These resources are interconnected ECUs, buses, gateways,
and communication controllers. The messages are given by
a sender ECU and multiple receiver ECUs to allow multicast
communications. The network consists of the topology and
the routing of the messages. The topology is directly deduced from the architecture. By integrating a subset of the
resources from the architecture, the actual network topology is deﬁned. A network is feasible if each message from
the speciﬁcation is routed correctly on this topology and, at
the same time, all constraints are fulﬁlled.
More formally, the network integration can be deﬁned as a
graph problem. The speciﬁcation is given by the architecture
graph Ga and a set of messages M :
• The architecture is given by a directed graph Ga (Va , Ea ).
The vertices Va represent resources such as ECUs, buses,
gateways, and communication controllers. The edges
Ea indicate available communication connections between two resources. ECUs are always connected to
buses via communication controllers.
• The messages are given as the set M . Each message
m ∈ M consists of a sender and a set of receivers to
allow multicast communications. The sender is deﬁned
as sm ∈ Va and the receivers are deﬁned as elements
in the set Dm ⊂ Va . All messages have a constant
size and are sent periodically or in a minimal interval,
respectively, allowing the determination of a load Lm ∈
N in kilobytes per second.
A feasible network consists of a topology graph Gt and a
routing graph Gm for each message m ∈ M .
• The topology is given as a directed graph Gt (Vt , Et )
that is a subgraph of the architecture graph Ga . The
topology contains all resources that are actually integrated into the network.
• For each message m ∈ M the routing is deﬁned as tree
graph Gm (Vm , Em ) that is a subgraph of the topology
Gt . The root of the tree equals the sender sm and the
leafs are the receivers from the set Dm .
Moreover, a feasible implementation requires that all constraints of the network are fulﬁlled. In the automotive area
optimize f (x)
subject to: x is a feasible network
In real-world networks, the ob jective function f consists of
multiple functions including also non-linear calculations. In
single–ob jective optimization, the feasible set of networks
is totally ordered, whereas in multi–ob jective optimization
problems, the feasible set is only partially ordered and, thus,
there is generally not only one global optimum, but a set
of Pareto solutions. A Pareto-optimal solution is better in
at least one ob jective when compared to any other feasible
solution.
3. OPTIMIZATION
The proposed network layout optimization approach is
based on the combination of a Pseudo-Boolean (PB) solver
[3] and a modern Multi-Objective Evolutionary Algorithm
(MOEA) [17]. A PB solver is based on a backtracking strategy and eﬃciently solves Integer Linear Programs (ILPs)
task of a PB solver is to ﬁnd an x ∈ {0, 1}n that satisﬁes
with an empty ob jective function and binary variables. The
a set of linear constraints with binary variables. A single
linear constraint is formulated as
aT x ◦ b
(2)
with a ∈ Zn , b ∈ Z and ◦ ∈ {<, ≤, =, ≥, >}. Commonly, the
backtracking strategy in PB solvers is guided by two vectors
ρ ∈ Rn and σ ∈ {0, 1}n deﬁning the priority and desired
phase of a binary variable.
MOEAs are a population -based optimization approach taking advantage of the principles of biological evolution. The
optimization is done iteratively in two alternating steps, the
variation and selection. In the variation, new solutions, i.e.,
a new generation, is created from a set of existing solutions
in the population. This is done by crossover and mutation
operators. Correspondingly, the selection sorts out the worst
solutions to ensure a convergence to the optimal solutions.
Combining the PB solver with an MOEA allows the optimization of the network layout considering multiple, also
conﬂicting and non-linear ob jectives. The main optimization
ﬂow is illustrated in Figure 2. This novel approach known
as SAT decoding has been presented in [9]. It is known to
be superior to common methods that are based on ILPs or
MOEAs only [10].
To utilize this optimization approach, it is necessary to
encode the network integration problem into a binary ILP
by deﬁning a set of linear constraints. Additionally, an efﬁcient preprocessing algorithm is introduced that eﬀectively
reduces the size of this ILP and, thus, allows to solve the
ILPs even for large and complex networks.
3.1 Problem Encoding
Referring to Deﬁnition 1, the feasibility of a network can
be encoded into a set of linear constraints with binary variinto a binary vector x ∈ {0, 1}n . Secondly, linear constraints
ables. This task is twofold: First, a network x is encoded
are deﬁned such that these are satisﬁed if and only if the network x is feasible.
627
Figure 2: SAT decoding: A combined MOEA and
PB solver optimization ﬂow.
m˜v,i ≥ 0
(3a)
(3b)
(3c)
−mv,i+1 + (cid:80)
˜v∈Va ∧e=(˜v,v)∈Ea
lowing: One variable v is introduced for each resource v ∈ Va
The binary representation of a network is deﬁned as folindicating whether this resource is part of the topology (v
is 1) or not (v is 0). For each message m ∈ M and each
resource v ∈ Va the variable mv indicates whether the message m is routed on the resource v (mv is 1) or not (mv is
0). Additionally, the variables mv,i are used to determine
on which communication step i the message m is routed on
the resource v . Starting from the sender, a message is propagated through the network in communication steps. In a
single communication step, a message can be passed from
one resource to a second adjacent resource. In general, the
maximal communication steps n can be deduced from the
number of nodes on the longest expected or allowed path
from the sender to one receiver of one message. With the
deﬁned binary representation of a network the constraints
have to be formulated as following:
∀m ∈ M , v ∈ Va :
v − mv ≥ 0
mv,1 + mv,2 + ... + mv,n − mv ≥ 0
mv,1 + mv,2 + ... + mv,n ≤ 1
∀m ∈ M , v ∈ Va , i = {1, .., n − 1} :
−mv,i + (cid:80)
∀m ∈ M , v ∈ Va \Dm , i = {1, .., n − 1} :
∀m ∈ M , s = sm :
ms,1 = 1
∀m ∈ M , v ∈ Va \sm : mv,1 = 0
∀m ∈ M , d ∈ Dm :
∀v ∈ Va is a bus(cid:80)
md = 1
m∈M (Lm + O) · mv ≤ (cid:98)l · Lv (cid:99)
Solving this search problem results in a binary vector x
representing a network x that is feasible with respect to the
deﬁnition given in Section 2:
Equation (3a) ensures that a message can only be routed
on resources of the current topology. Equation (3b) states
that a message has to be existent in one communication step
on a resource in order to be correctly routed on this resource.
A message can pass a resource at most once (3c). In (3d) it is
stated that a message can only be passed between adjacent
resources and this message has to be existent on the predecessor resource exactly one communication step ago. On
the other hand, (3e) and (3f) says that a leaf of the routing
graph can only be a receiving node. Equation (3g) and (3h)
deﬁne the sender by stating that each message is only existent at the sender at communication step 1. Accordingly,
the receivers are deﬁned in (3i). Additionally, the busload
constraints from Equation (1) are expressed in (3j).
m˜v,i+1 ≥ 0
˜v∈Va ∧e=(v,˜v)∈Ea
mv,n = 0
(3d)
(3e)
(3f )
(3g)
(3h)
(3i)
(3j)
3.2 Problem Encoding Preprocessing
The prior introduced problem encoding can grow large in
the number of variables and constraints. In particular, the
overall number of the mv,i variables is |M | · |Va | · n. On the
other hand, due to the network characteristics many of these
variables can never become 1 and, therefore, can be statically
set to 0 or removed from the constraints to decrease the size
of the search space, respectively. A suitable preprocessing
algorithm is applied for each message separately to identify
these redundant variables. This preprocessing is stated in
Algorithm 1.
Algorithm 1 Preprocessing algorithm to reduce the number
of variables
1: Input C // All constraints for one m ∈ M
2: Input V // All variables from C
3: while ﬁnd x that satisﬁes C do
C = C ∧ ((cid:80)
X = { xi | xi = 1 }
4:
V = V \X
5:
6:
v∈V v > 0)
7: end while
8: Output V
The algorithm starts with the set of all constraints C that
are generated by (3a) to (3i) for one message m ∈ M (line 1).
The candidate set V contains all variables from the search
problem deﬁned by the constraints C (line 2).
The preprocessing algorithm is repeatedly started until
there exists no solution x that fulﬁlls the constraints C (line
3). The search for a satisfying x is done by a PB solver.
If an x that satisﬁes C is found, all variables that are 1
are removed from the candidate set V (line 4, 5). An additional constraint is determined by the remaining candidate
variables such that in the next iteration at least one of the
candidate variables has to become 1.
In the case that C is unsatisﬁable, the algorithm terminates. The remaining variables in V can be statically set to
0 in the original problem encoding since there exists no feasible solution where any of these variables is 1. That also indicates, these variables can be completely removed from the
linear constraints and the search vector x. This eﬀectively
reduces the search space and complexity of the problem with
a reasonable eﬀort. This will be shown in Section 4.
4. EXPERIMENTAL RESULTS
The proposed methodology is validated on two state-ofthe-art prototype automotive network testcases TC1 and
TC2. TC1 is a midsize speciﬁcation with 39 ECUs, 5 CAN
buses, one gateway, and 75 communication controllers with
140 messages. The second speciﬁcation TC2 contains 71
ECUs, 7 CAN buses, one gateway, 170 communication controllers, and 202 messages. This speciﬁcation represents a
current high-end automotive network integration problem.
For all testcases, the maximal utilization of the buses is set to
40%. All experiments were carried out on an Intel Pentium 4
3.2 GHz machine with 1 GB RAM. The used optimization
framework is Opt4J [11].
4.1 Preprocessing
First, the beneﬁt of the proposed preprocessing algorithm
from Section 3.2 is studied. For each testcase, the number
of decodings performed by the PB solver is 100 such that
a meaningful average is calculated. The results are given
in Table 1.
In both cases, the preprocessing that has to
be made singularly in the optimization process, needs only
a few seconds, i.e., 2.7s for TC1 and 3.9s for TC2. The
preprocessing eﬀectively reduces the number of variables by
a factor 3 to 4. The decoding time is reduced by two orders
of magnitude to 0.33 seconds in average for TC1 and 0.50
for TC2. Since the proposed iterative heuristic is based on
628
TC1
preprocessing oﬀ
preprocessing on (2.7 s)
TC2
preprocessing oﬀ
preprocessing on (3.9 s)
decoding [s]
30.7 (208)
0.33 (0.39)
decoding [s]
37.3 (51.5)
0.50 (0.48)
variables
27711
7731
variables
42853
11248
Table 1: Comparison of the eﬀect of the preprocessing for the testcases TC1 and TC2. Given is the
preprocessing time, the time for the decoding of one
network (with the deviation inside the brackets), and
the number of variables of the problem.
Figure 3: Optimization results for TC1.
consistently decoding feasible networks, this preprocessing
has a huge impact on the overall runtime of the optimization.
Moreover, the PB solver is capable of learning from conﬂicts,
such that the decoding time further decreases with a growing
number of iterations.
4.2 Optimization
n
The optimization integrates the SPEA2 MOEA algorithm
[17] with the following parameters: The population size is
100 with 25 parents and 25 oﬀspring for each generation.
The number of generations is 200, leading to overall 5075
evaluations and decodings. The mutation rate is set to p = 1
with n being the number of variables of the ILP.
The optimization methodology is compared to existing reference network integrations. One reference network exists
for each testcase representing a network that was created
manually by a designer. The two optimization ob jectives
are the monetary costs and the average worst-case response
times of all messages having a huge impact on the robustness
of the network. Both ob jectives have to be minimized. The
costs are an abstract value approximated by a linear function. The average worst-case response times are calculated
by the non-linear functions presented in [12].
In TC1, the optimization approach has an overall runtime
of 825 seconds. Figure 3 shows the results of the optimization compared to the reference network. Costs and response
times are considerably optimized. The response times are
optimized by 11% at the same costs level and the costs are
optimized by 12% at the same response times.
In TC2, the optimization approach has a runtime of 1360
seconds showing a good scaling compared to the runtime of
TC1. All optimized networks are better in both ob jectives
compared to the reference network, cf. Figure 4. The costs
are optimized up to 13% and the response times up to 14%.
All improvements are reached by an determination of the
topology and routing only. Known methodologies for the
integration phase, like message priority [16] or period optimization [4], can still be applied on the found networks.
Thus, a further optimization of the response times as well as
ECU and bus utilization is reachable.
5. CONCLUSION
In this paper, we presented a novel optimization method629
Figure 4: Optimization results for TC2.
ology for the determination of a topology and routing of automotive networks in the integration phase. This approach
is based on SAT decoding that combines a PB solver and
MOEA. The presented graph-based problem speciﬁcation is
converted into a set of linear constraints and a preprocessing
algorithm is used to reduce the search space eﬀectively.
The experimental results validate the methodology on two
state-of-the-art automotive examples. Hereby, the preprocessing algorithms accelerates the PB solver by two orders
of magnitude for a marginal cost of a few seconds. The optimization process is able to considerably improve both networks in an acceptable time. Both ob jectives are separately
optimized above 10%.
6. "
A dynamically-allocated virtual channel architecture with congestion awareness for on-chip routers.,"In this paper, the dynamically-allocated virtual channels (VCs) architecture with congestion awareness is introduced. All the buffers are shared among VCs whose structure varies with traffic condition. In low rate, this structure extends VC depth for continual transfers to reduce packet latencies. In high rate, it dispenses many VCs and avoids congestion situations to improve the throughput. We modify the VC controller and VC allocation modules, while designing simple congestion avoidance logic. The experiment shows that the proposed routers outperform conventional ones under different traffic patterns. They provide 8.3% throughput increase and 19.6% latency decrease while saving 27.4% of area and 28.6% of power.","A Dynamically-Allocated Virtual Channel Architecture 
with Congestion Awareness for On-Chip Routers 
35.4
Mingche Lai, Zhiying Wang, Lei Gao, Hongyi Lu, Kui Dai 
School of Computer, National Univ. of Defense Tech. Changsha, China. 
mingchelai@nudt.edu.cn 
ABSTRACT. 
In this paper, the dynamically-allocated virtual channels (VCs) 
architecture with congestion awareness is introduced. All the 
buffers are shared among VCs whose structure varies with traf- 
fic condition. In low rate, this structure extends VC depth for 
continual transfers to reduce packet latencies. In high rate, it 
dispenses many VCs and avoids congestion situations to im- 
prove the throughput. We modify the VC controller and VC 
allocation modules, while designing simple congestion avoid- 
ance logic. The experiment shows that the proposed routers 
outperform conventional ones under different traffic patterns. 
They provide 8.3% throughput increase and 19.6% latency de- 
crease while saving 27.4% of area and 28.6% of power. 
Categories and Subject Descriptors: B.4.3 [Hardware]: 
Input/Output and Data Communication - Interconnections. 
General Terms: Design, Performance. 
Keywords: Network-on-Chip, Virtual Channel, Congestion. 
1 
Introduction 
With the shift advancement of semiconductor technology, the 
network-on-chip (NoC) which enjoys the fault tolerance and 
scales better than traditional interconnects has been adopted to 
facilitate a high-bandwidth communication. However, the head- 
of-line (HoL) blockings and low link-utilizations are the critical 
issues, especially for communication-intensive applications. The 
VC flow control scheme providing an alternative to reduce the 
HoL blockings may improve the link utilization. But it brings 
lots of area and power burdens. For instance, the VC buffers 
will take up to nearly 50% of area and 64% of leakage power at 
router implemented under 70nm CMOS technology [1]. 
There have been significant works on the VC organizations 
to save buffers and exploit performance. Huang [2] customized 
the VCs and achieved 40% buffer savings without any perfor- 
mance degradation. However, it was a static approach which 
was based on a detailed analysis of application-specific traffic 
patterns. Y. Tamir [3] proposed a dynamically allocated multi- 
queue structure for communication on multiprocessor. This 
architecture did not adapt to NoC due to the complex controller, 
the limited channel number and three-cycle delay for each flit 
arrival/departure. In [4], C.A.Nicopoulos introduced a dynamic 
VC regulator which allocated a large number of VCs in high 
traffic conditions. It couldn’t scale well to apply to different 
traffic conditions, and dispensed many VCs with much comple- 
xity, whose overhead especially for VC control table would in- 
crease non-linearly. For instance, in the case of 16 buffers per 
port and 9-flits packet size, the table size supporting 10 VCs 
will reach 220B, nearly 21.5% of the total buffers. 
The key inhibitor to performance in conventional VCs is the 
fixed structure, resulting in a large number of blockings or pac- 
ket cross-transfers. In this paper, a dynamic VC architecture 
with congestion awareness is proposed. It makes full use of 
buffers by linked lists and improves performance under different 
conditions. In low rate, it tends to extend the channel depth. The 
deep VC makes packets distribute over few routers and leads to 
low latencies. In high rate, it dispenses many VCs to increase 
the channel multiplexing without HoL blocking, and grants the 
priority to packets towards low traffic regions beyond neighbors 
to avoid congestion. Our results indicate that for different traffic 
rates or patterns, it can achieve better performance. 
2 Dynamic VCs with Congestion Awareness 
The conventional virtual channel router was firstly presented by 
Li-Shiuan Peh [5]. It was an input-queued router with four dir- 
ection ports and a local injection port. The key components in- 
cluded the buffer storages, VC controller, routing module, VC 
allocation module, switch allocation module and switch compo- 
nent. The routers used VCs to avoid deadlock and improve the 
throughput. Ideally, allocating more VCs implies better utiliza- 
tion on a given link, but it decreases the VC depth due to fixed 
buffer size. Rezazad [6] ever revealed that the optimal number 
of VCs depended on traffic patterns when the buffer size was 
fixed. In low rate, increasing VC depths resulted in better per- 
formance. In high rate, the optimal structure depended on the 
distributing patterns. It was advisable to increase VCs under 
uniform pattern but extend VC depth under matrix transpose or 
hotspot patterns. The statically-allocated VCs structure lacks of 
flexibility on various traffic conditions and corresponds to low 
buffer utilization. Supposing routers are configured with few 
deep VCs, many blockings due to HoL or lack of VCs in high 
rate will lead to low throughput. Inversely, if many shallow VCs 
are arranged, the packets are distributed over a large number of 
routers. In low rate, the continual packet transfers will be inter- 
rupted by many contentions and then increase the latency. 
Based on the analysis above, we introduce a dynamic VC sch- 
eme to overcome the limitations of static VCs. Figure 1 shows 
the dynamic VC structure in detail. It includes v linked lists and 
a buffer track unit. All the buffers are shared by the linked lists, 
and each VC corresponds to a list for the single packet without 
any buffer reservation. When the flit is incoming, the right list is 
selected according to VC identifier and the arrival flit is stored 
into tail buffer which is dynamically allocated by the track unit. 
For the departure flit, the track unit is responsible for releasing 
the head buffer of winner VC, which is arbitrated by the switch 
allocation unit. In our structure, the VCs may be dynamically re- 
gulated according to the traffic conditions. In low rate, only few 
channels are used. Each input port has enough buffers to be allo- 
cated for arrival flits. This structure ensures the continual trans- 
fers of packets without flow control, and is helpful to form few 
deep channels. The packets distributing among few routers will 
reduce the average latency. In high rate, lots of VCs are allocate- 
ed for packets propagation. The increasing number of packets 
will result in many shallow channels, which improve throughput 
on two aspects. Firstly, the scheme that each packet uses sepa- 
rate list avoids the HoL blockings which are frequent in static 
VCs. Dispensing more VCs for the local packets will increase 
630
 
the channel multiplexing of physical links. Secondly, more VCs 
granted to upstream packets will decrease the blockings due to 
lack of VCs. In conventional router, many VCs may be occupied 
by the packets whose tail flits are blocked at upstream routers. 
At this moment, lots of buffers may be unused but the packets at 
neighbor routers are blocked due to lack of VCs. More dispens- 
ed VCs here will accommodate many packets from the neigh- 
bors, improving throughput with a better buffer utilization. 
3.1 VC Control Module 
The conventional routers are deployed with several fixed VCs to 
share physical links. Each flit arrival/departure selects its indivi- 
dual FIFO controller unit according to the VC identifier. Instead, 
each input port in our modified routers adopts the uniform VC 
control logic which generates the buffer indexes by VC identi- 
fiers in one cycle delay. The port includes v linked lists, and the 
relative information about each VC involves the valid tag (valid), 
the link head pointer (HP), the tail pointer (TP), the requested 
channel identifier (ID), the transfer directions at local hop (DIR) 
as well as at the next hop (NDIR). The main information about 
each list item includes the buffer storage and next flit pointer 
(NP) to the following flit. 
Figure 1: Dynamic VC structure Figure 2: Congestion avoid scheme 
In addition, we introduce a congestion awareness scheme to 
further improve the performance in high traffic rate. From the 
point of link utilization, three key factors restraining throughput 
in conventional routers involve the blockings without VC, the 
blockings due to flow control, and the crossbar contentions, 
where two individual packets from different ports compete for 
the same physical link will make some other link idle. Dispens- 
ing lots of VCs may reduce the first type of blockings, and is 
helpful for reducing the crossbar contentions by increasing the 
channel multiplexing. However, allocating more VCs has none 
contribution to performance when congestion situation happens, 
where all buffers at the port are exhausted and the flow control 
is generated to prevent the upstream packets. Especially under 
hotspot pattern, a great number of packets transferring towards 
routers with few buffers will result in frequent congestions. 
Once the packets are blocked at local routers, the congestion 
will degrade the link throughputs seriously. Figure 2 (a) shows 
the effect of congestion situation. Packets 1 and packet 2 which 
transfers towards router C are blocked at router B due to flow 
control or contentions. A number of flits swarming into router B 
will lead to the congestion at its west port. In the following, the 
upstream packets 3 and 4 will be blocked sequentially due to 
flow control, incurring the idle of links A->B, B->E and B->D. 
If more buffers and VCs are allocated to packet 3 or 4 at router 
B, the link utilization may be increased linearly. But the buffers 
at west port of router B have already been exhausted by packet 1 
and 2 at this moment. Therefore, this scheme predicts the con- 
gestion of neighbors and inspects the traffic conditions around 
them. Once the local router senses a possible congestion in 
advance, we allocate buffers and grant transmission priority to 
the packets which transfer towards low traffic regions beyond 
neighbor. Then, these packets won’t be blocked by flow control 
at neighbor router, and are good at avoiding the congestions by 
increasing the channel multiplexing. As depicted in Figure 2(b), 
supposing the west port of router B won’t receive any flit in the 
next k cycles, the number of available buffers is aggregated 
k-cycles ahead of time by predicting the minimum flits from the 
west port. If the aggregation value is no more than k, the 
continual transfer towards the west port of node B may cause 
possible congestion. In Figure 2 (b), the transfer request of pac- 
ket 2 is cancelled and the transmission priority is granted to 
packet 3 or 4 which transfers towards the low traffic routers 
beyond neighbor. In the end, the congestion at west port of node 
B is avoided and the throughputs of links A->B, B->E and B->D 
are improved. 
3 Proposed Micro-architecture 
Based on conventional routers [5], we modify VC controller and 
VC allocation modules, while designing the simple congestion 
avoidance logic. 
631
Figure 3: VC control logic for flit arrival/departure 
Figure 3 shows the VC control logic for flit arrival/departure 
in detail. The key elements include the head pointer register file, 
the tail pointer register file, the next slot pointer register file and 
a free buffer track unit. For each arrival flit, the write index is 
generated when completing three operations as shown in equa- 
tions (1)(2)(3). At the beginning of each cycle, the track unit 
returns an available buffer index to accommodate the arrival flit. 
Also, using the VC identifier, the TP information of right link 
and the NP information of right tail are updated by available 
buffer index at the next clock rising edge. 
data[track()] <= flit_arrival 
(1)
list[i] .TP <= track() 
(2)
NP[list[i].TP] <= track() 
(3)
flit_departure <= data[list[i] .HP] 
(4)
list[i] .HP <= NP[read_index] 
(5)
track[list[i].HP] <= 1 
(6)
On the other hand, each departure flit corresponds to other 
three operations as shown in equations (4)(5)(6). As depicted in 
Figure 3, the compare unit first judges the continual transfer of a 
certain packet by comparing VC identifiers of successive requ- 
estors. If the latter flit follows the former one within the same 
channel, the compare unit generates the true result and the NP 
content selected by the last read index will be the current read 
index. Otherwise, the departure VC identifier is used to select 
the right HP content to be the read index as shown in equation 
(4). Each departure flit also needs to update the head pointer 
register file. As shown in equation (5), if the input port wins the 
2nd switch allocation phase, the right head pointer will be updat- 
ed by NP information of the last departure flit. Finally, for each 
departure flit, the track unit clears the corresponding bit to re- 
lease the occupied buffer as shown in equation (6). In this str- 
ucture, all the operations work in parallel with switch allocation 
process within one cycle delay, and there is negligible impact on 
the critical path. The extra hardware cost is also limited due to 
few register files. Supposing the flit width bits, VC number, 
buffer number and max VC depth are 128, 10, 16, 9 respectively, 
the storage overhead is nearly 7.0%. 
3.2 Metrics Aggregation and Congestion Avoidance 
As discussed in section 2, we predict the congestion situation at 
immediate neighbors. Figure 4 shows the congestion metrics 
 
 
aggregation module. Supposing the current time stamp is t, 
cd(t,k) (d= E,…,N) predicts how many flits will leave from the 
immediate neighbor in d direction over the next k cycles. bd(t,k) 
predicts the availability of its input buffers in a k-cycle ahead of 
time. The right box of Figure 4 shows input port of the neighbor. 
Its key components include the 1st switch allocation unit and a 
prediction unit. The switch allocation unit arbitrates among all 
the requests from the same port and grants a transfer priority to 
the winner. The prediction unit calculates the number of depar- 
ture flits from its input port over the next k cycles. The neighbor 
has already aggregated the availability information bq(t,k) in the 
winner’s output direction. In wormhole switching scheme, the 
winner packets are served continuously until the tail flit, flow 
control or empty channel [7]. When any condition happens, the 
1st allocation phase grants the transfer priority in a round robin 
order to other packets. Thus, the prediction unit of neighbor cal- 
culates the number of departure flits as shown in equation (7), 
where k denotes the stride and lw denotes the list length of the 
winner. The left box of Figure 4 represents local router. The 
canonical router records the available buffers of its immediate 
neighbors for flow control, and we name them as xd(t). The mo- 
dified module aggregates the available buffers at its neighbor 
input port k-cycles ahead of time by the sum of xd(t) and cd(t,k), 
as depicted in Figure 4. If the prediction value is no more than k, 
the congestion metric congd will be true. It means that the packet 
transfers to d direction may cause the possible congestion. 
cd(t+1,k)=min{bq(t,k), k, lw}  (d,q {E,S,W,N})
   (7) 
∈
Figure 4: Contention Metrics Aggregation Module 
Then, the transmission priority is granted to packets which 
transfer towards low traffic regions beyond neighbor routers to 
avoid congestion. As shown in Figure 4, each router collects the 
available buffers at neighbor inputs, and aggregates the traffic 
metrics, where, σE, σs, σW, σN denote metrics at different direc- 
tions. We performed a detailed empirical evaluation to deter- 
mine the threshold for metric σ, and found that the value which 
was a little more than stride k was reasonable for packet bypass. 
Since the routers can sense the possible congestion situations at 
neighbors, they may use traffic metrics to advance the packets 
which transfer towards low traffic regions beyond neighbors. As 
shown in Figure 5, the modified flow control module is per- 
formed in two parts. Firstly, similar with conventional router, 
each channel compares its remote VC length with FIFO depth to 
decide the flow control. Our modified module adopts the FIFO 
depth which varies with the number of VCs. In high rate, the VC 
depth at neighbor is reduced reasonably for more packet propa- 
gations due to many VCs. Secondly, the transfer requests tow- 
ards high traffic regions beyond neighbors are masked when 
predicting congestion. Each input port is deployed with several 
avoidance units which correspond to other output directions res- 
pectively. Utilizing DIR information, each channel exports its 
NDIR information to the right avoidance unit by DEMUX unit. 
The avoidance unit collects all NDIR information within the 
same port and specifies the mask bit of each VC. It uses multi- 
plexers to select the traffic metric of next output direction. The 
relative mask bit will set to be true when the packet transfers 
towards high traffic region beyond neighbor. However, if all the 
packets are transferring towards high traffic regions beyond 
same neighbor, the avoidance unit won’t mask any request and 
set all bits to be false. In the modified module, each list uses its 
632
DIR information to select congestion metric and mask bit. When 
both conditions are true, the transfer valid signal will be cancel- 
ed. Using this approach, the packets towards low traffic regions 
beyond neighbors are transferred in advance when predicting 
congestions. Note that, for the packets from different input ports, 
the transfer priority is decided by the 2nd switch allocation phase 
according to traffic metrics. We assume prediction metric cd(t,k) 
is summarized into three bits, plus three bits for traffic metrics, 
this scheme requires extra six bits per link and wire overhead is 
just 4.1%. This scheme has no influence to critical path, and the 
area overhead from prediction and avoidance units is confirmed 
to be low in the following synthesis. 
Figure 5: Flow Control Module 
3.3 VC Allocation Module 
The VC allocation in conventional router was performed into 
two stages, which arranged v:1 and 5v:1 arbiters respectively. 
With the increasing number of VCs, the VC allocation module 
which relies on the critical path will influence the performance. 
The modified VC allocation module adopts another two-phase 
arbitration. The 1st phase arranges five v:1 arbiters at each input 
port and every arbiter gains a winner request at corresponding 
direction. The 2nd phase logic distributes at each output port, 
which arranges a 5:1 arbiter to generate the final winner to occ- 
upy dispensed VCs. Although the proposed module allocates the 
single VC at each direction, it has negligible impact on the per- 
formance. Even if more VCs are dispensed, the flits still transfer 
towards the output port one by one. The main advantage of mo- 
dified structure is that the area overhead and critical path length 
are reduced due to few 5:1 arbiters instead of many 5v:1 ones. 
In addition, each output port arranges a VC manage unit for VC 
dispensation. It dispenses lots of VCs for request packets until 
VCs are exhausted, and releases VCs according to the credits 
from neighbors. For allocation scheme, the 2nd phase arbitrates 
for the winners according to the traffic metrics from neighbors, 
granting the VC allocation priority to the packets which transfer 
towards the low traffic regions beyond neighbors. 
4 Performance Evaluation 
We use a cycle-accurate simulator to evaluate the performance. 
The conventional router employs simple dimension order rout- 
ing algorithm and adopts look-ahead scheme which corresponds 
to the pipeline stage of three. In the experiment, each input of 
routers is deployed with 32 buffers. There are two kinds of con- 
ventional routers denoted as T-2 and T-4 respectively, according 
to the VC number, in contrast with 10 VCs at most in our mo- 
dified routers. We arrange different 8-by-8 mesh networks using 
three routers respectively and simulate under uniform and hots- 
pot patterns with the packet size of 9-flits (1flit/head+8flits/body, 
16B/flit). Results are obtained by simulating 1×106cycles for 
different rates or patterns after a ware-up phase of 2×105cycles. 
In the first phase, the effect of prediction stride k on average 
latency is investigated. Figure 6(a) graphs the latency as a func- 
tion of stride k in different traffic rates. When the k increases 
from 0 to 8, the latencies keep steady in low rate but decrease 
rapidly in high rate. The stride value of 0 means that congestion 
awareness scheme is not adopted. In this case, many allocated 
VCs may reduce the blockings caused by lack of VCs, but are 
helpless for the flow control, which will let many links idle. 
 
 
Figure 6: Simulation and Synthesis Results 
for improving area/power by reducing the queue sizes. Secondly, 
Figure 6(f)(g) compare the performance under different patterns. 
In random pattern, there are enough VCs to be allocated to pac- 
kets in both routers, but our DVC router predicts congestion and 
reduces the number of blockings due to flow control. Figure 6(f) 
shows that the saturated throughput of DVC router outperforms 
ViChar one by nearly 8.6% in random pattern. In hotspot pattern, 
there are many packets towards low traffic destinations, and the 
DVC routers at high traffic nodes propagate them in advance to 
reduce latencies. As shown in Figure 6(g), although the satu- 
rated throughput is constrained by the accepted rate at hotspot 
node, our DVC router still provides 11.2% reduction in latency 
compared with ViChar one. 
Finally, we complete the RTL level description of DVC router 
using 16 buffers and 10 VCs. The VLSI design results under 
90nm process show that the router can operate at 600MHz. The 
area information of each module is illustrated in Figure 6(h). 
The main area overhead comes from VC control module due to 
accessing index generation and congestion avoidance logic. But 
half buffer savings and simplified VC allocation module result 
in more area reduction, and the total area saving achieves to 
nearly 27.4%. Using Prime Power tools, we also estimate the 
power of routers which excludes the wire power. The power of 
DVC router decreases by 28.6% compared with T-4 one. 
With the increasing of k, the congestion situations will be gra- 
dually reduced to improve link utilization. However, when stride 
k increases from 8 to 16, the average latency begins to ascend. 
Using greater k, it may always predict congestion situations 
which will interrupt the continual packet transfers. Thus, we set 
k to be 8 with buffer size of 32 to gain the better performance. 
Following, other networks by conventional routers are evalu- 
ated. Figure 6(b) illustrates the results under random pattern. In 
low rate, T-2 and dynamic VC (DVC) routers may provide deep 
channels which correspond to low latency. But with the in- 
creasing of rate, the performance using T-2 router tends to be 
saturated rapidly. The T-4 router uses more VCs to improve the 
throughput, but its performance is still limited by lots of HoL 
blockings or blockings caused by lack of VCs. Here we observe 
the advantages of DVC router in detail. It avoids HoL blockings 
and dispenses many VCs in high rate. Also, it avoids the conges- 
tions to improve throughput. As a result, the saturated through- 
put of DVC router outperforms T-2 and T-4 routers by nearly 
33.9% and 22.1% respectively. Figure 6(c) illustrates the results 
under hotspot pattern. In low rate, DVC router also has low 
latency due to deep channels. With the increasing of rate, few 
routers become the high traffic nodes and others are still in low 
traffic condition. DVC routers allocate VCs according to local 
conditions. At low traffic nodes, they extend VC depth to de- 
crease average latency. Around the hotspot, they dispense many 
VCs to increase channel multiplexing. Especially, it grants 
transfer priority to packets towards low traffic region, and then 
many packets towards low traffic destinations won’t be blocked 
around the hotspot node. In Figure 6(c), we select typical inject 
rates from 0.10flit/cycle to saturated points, and the average 
latency drops by nearly 30.2% and 25.2% respectively. 
In conventional routers, the buffer utilization is low due to the 
limited VCs. We also evaluate the performance of DVC router 
when reducing buffer size to be 16. Using the same quantified 
method, the stride k is set to be 4 for better performance in all 
cases. Figure 6(d)(e) show the average latency as a function of 
the inject rate. Under random pattern, the saturated rates of three 
routers are about 0.38, 0.34, 0.31flit/cycle respectively. Com- 
pared with T-2 router, the average latency of DVC router is 
decreased by 16.5% when selecting 0.1, 0.2, 0.275, 0.3 flit/cycle 
for typical rates. With the rates of 0.1, 0.2, 0.3 and 0.325flit/ 
cycle, the latency of DVC router outperforms T-4 router by 
about 21.6%. Then, the saturated rates are measured to be 0.20, 
0.21 and 0.22flit/cycle under hotspot pattern and DVC router 
achieves 20.3% and 17.6% reductions in latency towards other 
two routers respectively. Averagely, compared with T-4 router, 
which is superior to T-2 one, the DVC router provides 8.3% 
throughput increase and 19.6% latency decrease averagely. 
Next, ViChar router is integrated in the simulator. Both DVC 
and ViChar routers are deployed with 16 buffers, supporting 10 
VCs at the most. Firstly, there is a comparison with ViChar in 
terms of the extra storages. The VC control table size of Vichar 
with VC depth of 9 has already achieved to about 220B, in 
contrast with 72B of DVC router. Obviously, Vichar is less fit 
5 Conclusion  
This paper introduces a dynamically-allocated VC architecture 
with congestion scheme. In low rate, it extends deep VCs to 
reduce packet latency, while it increases VC number and avoids 
congestion situations to improve throughput in high rate. The 
implementation of the modified router is completed under 90nm 
CMOS process. The experiment results show that DVC router 
which adapts to different injection rates or traffic patterns may 
provide 8.3% throughput increase and 19.6% latency decrease 
averagely, with the savings of 27.4% router area and 28.6% 
power when compared to conventional routers. 
Acknowledgment 
This work is supported by National Basic Research Program (2007CB31 
0901), Natural Science Foundation (60773024), and National 863 Pro- 
gram (2007AA01Z101). 
"
PARM - power supply noise aware resource management for NoC based multicore systems in the dark silicon era.,"Reliability is a major concern in chip multi-processors (CMPs) due to shrinking technology and low operating voltages. Today's processors designed at sub-10nm technology nodes have high device densities and fast switching frequencies that cause fluctuations in supply voltage (Vdd) and ground networks, which can adversely affect the execution of applications running on them. In this paper, we propose a novel runtime framework to reduce the power supply noise (PSN) in cores and routers at runtime. Experimental results for 7nm FinFET process nodes show that our framework not only achieves up to 4.5× reduction in PSN, and up to 34.3% improvement in application performance, but also manages to map up to 38% more applications when the CMP is oversubscribed, compared to the state-of-the-art.","PARM: Power Supply Noise Aware Resource Management for 
NoC based Multicore Systems in the Dark Silicon Era 
ABSTRACT 
Venkata Yaswanth Raparti, Sudeep Pasricha 
Department of Electrical and Computer Engineering 
yaswanth@rams.colostate.edu, sudeep@colostate.edu   
Colorado State University, Fort Collins, CO, U.S.A. 
with technology scaling by going beyond the permissible noise margin, 
making the PSN threat a serious concern for chip designers.  
Simultaneously, applications are becoming increasingly parallel to 
utilize the abundance of compute resources on a chip. This has led to 
increasing communication between cores on a chip. Modern CMPs are 
embracing network-on-chips (NoCs) as their de-facto communication 
fabric to cope with increasing on-chip traffic. But NoCs in today’s 
chips can consume a significant amount of chip power (~ 30% in the 
80-core TeraFLOPS chip [6]). The varying workloads and NoC traffic 
profiles which are typical of most parallel applications also contribute 
heavily to the supply noise. Thus, it is important to address voltage 
variations in both processor cores and the NoC, to comprehensively 
mitigate the negative effects of PSN in CMPs. 
Traditional approaches have addressed the PSN issue at the circuit 
and micro-architectural levels. Although PSN is most readily observed 
at the circuit level, the compute intensity and distribution of the 
workload on the cores decides the magnitude of PSN observed at each 
cycle. Hence it is important to address the issue of PSN at a higher level 
of abstraction than the circuit level.  
In this paper, for the first time, we propose a novel PSN-aware runtime 
resource management framework (PARM) that employs dynamic 
voltage scaling (DVS), adaptable application degrees of parallelism 
(DoP), and an intelligent mapping scheme for a NoC-based CMP that 
operates at near threshold voltages within dark silicon constraints. 
PARM selects the mapping region, Vdd, and DoP for every application 
that arrives at runtime in such a way that the peak PSN in the mapping 
region and its vicinity is kept below a threshold, minimizing the number 
of voltage emergencies. Our key contributions in this paper are: 
• We study the correlation of PSN with application activity, proximity 
of concurrently executing threads, and core supply voltages;  
• We utilize DVS and adaptable application DoP, to reduce the peak 
PSN and optimally utilize the available dark silicon power slack 
while maximizing the number of applications serviced at runtime; 
• We propose a novel PSN aware application mapping heuristic for 
emerging sub-10nm fabricated CMPs, to reduce the PSN in a region 
and minimize communication latency between tasks; 
• We devise a novel PSN-aware routing scheme that balances router 
activity near highly switching cores, and reduces the impact of the 
NoC traffic on PSN without incurring any additional latency. 
Reliability is a major concern in chip multi-processors (CMPs) due to 
shrinking technology and low operating voltages. Today’s processors 
designed at sub-10nm technology nodes have high device densities and 
fast switching frequencies that cause fluctuations in supply voltage 
(Vdd) and ground networks, which can adversely affect the execution 
of applications running on them. In this paper, we propose a novel 
runtime framework to reduce the power supply noise (PSN) in cores 
and routers at runtime. Experimental results for 7nm FinFET process 
nodes show that our framework not only achieves up to 4.5× reduction 
in PSN, and up to 34.3% improvement in application performance, but 
also manages to map up to 38% more applications when the CMP is 
oversubscribed, compared to the state-of-the-art. 
Categories and Subject Descriptors: [EDA] System level design 
methodology; [Reliability] Reliability for Power Supply Noise; 
[Resource Management] Task mapping to meet given constraints  
General Terms – Reliability, Performance, Application mapping 
Keywords – Power Supply Noise, Dark Silicon 
1.  INTRODUCTION 
The advent of sub-10nm technology has led to chip multi-processors 
(CMPs) that are densely packed with transistors. In these CMPs, 
multiple parallel applications frequently execute at the same time on 
multiple cores, creating variations in workload activity at runtime. In 
cases where several cores switch at the same time, noise is introduced 
in the power supply due to the resistive drop of chip parasitics or the 
inductive droop between circuits [1]. Studies have shown that power 
supply noise (PSN) increases critical path latency and degrades chip 
performance. Moreover, PSN above a certain threshold can also lead 
to timing errors, called voltage emergencies (VE), which if left 
uncorrected can result in incorrect outputs from application execution.  
Recent studies have shown that at any given time, large sections of 
a chip remain inactive, to not exceed thermal design budgets [2]. This 
phenomenon is called dark silicon. Studies have shown that the percent 
of the chip that remains inactive (dark) is increasing with technology 
scaling [3]. Researchers have proposed operating cores at near 
threshold voltages, dubbed as near threshold computing (NTC), to 
reduce the chip power footprint and minimize the amount of dark 
silicon [5]. But this results in a shrinking headroom between supply 
voltage Vdd, and threshold voltage Vth, with a low margin of error for 
noise in Vdd. Fig. 1 shows peak PSN in the power delivery wires on the 
chip due to inter-core interference [4], which is increasing alarmingly 
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from Permissions@acm.org. 
DAC '18, June 24–29, 2018, San Francisco, CA, USA  
© 2018 Association for Computing Machinery. 
ACM ISBN 978-1-4503-5700-5/18/06 $15.00  
https://doi.org/10.1145/3195970.3196090 
Figure 1: Peak supply noise percentage, relative to the nominal near 
threshold supply voltage, across fabrication process technology nodes. 
2. RELATED WORK 
Several techniques have been proposed to cope with PSN at the 
circuit-level. In [7], conservative noise margins are used to ensure safe 
operation even when worst-case PSN is observed. In [8], decoupling 
 
 
 
 
 
 
 
 
capacitors are used to reduce core-to-core voltage interference. In [17][18] mechanisms were presented to predict PSN and the occurrence of 
voltage emergencies. Other efforts have proposed micro-architectural 
solutions, to reduce inter-core interference, e.g., pipeline throttling, 
instruction rescheduling, relaxed entry/exit at synchronization barriers, 
etc. [9]-[15]. But these solutions are primarily designed for single-core 
or few-core systems. The use of on-die digital sensors was proposed in 
[16] for the runtime measurement of PSN and to take reactive 
(corrective) measures post-detection. However, these solutions are not 
very beneficial in preventing PSN-induced voltage emergencies. Also, 
the penalty for error correction is expensive when the system is oversubscribed. In [19], Hu et al. proposed a thread mapping and migration 
scheme for Single Program Multiple Data (SPMD) applications to 
minimize large voltage fluctuations in CMPs. In [33]-[34], reliability 
aware task mapping and NoC routing schemes have been proposed to 
minimize the effects of aging and soft errors. In [35]-[36], Power 
Delivery Network aware resource management frameworks for 
voltage islands based 2D and 3D CMPs have been proposed. In [20][21], PSN-aware workload assignment schemes are proposed for 2D 
and 3D CMPs. The schemes map highly active threads at longer 
Manhattan distances from each other to minimize PSN in a region. 
However, these schemes do not consider the impact of activity in NoC 
routers on PSN. In [22], PSN-aware routing and flow control schemes 
are proposed, to reduce NoC router activity. However, application 
mapping plays a crucial role in overall NoC activity as tasks separated 
by longer distances cause more NoC routers to switch. Also, none of 
these works consider the low voltage margins imposed by NTC to meet 
dark silicon constraints.  
To the best of our knowledge, this is the first work that addresses 
PSN due to both core and NoC switching activity in the presence of 
dark silicon power constraints for CMPs executing multi-application 
workloads and designed at sub-10nm technology.  
3. BACKGROUND: MODELS AND ASSUMPTIONS 
3.1   Processor Model 
We assume a CMP with N tiles Ʈ = {Ʈ 1, Ʈ 2…Ʈ N}. Each tile Ʈ i , has 
a processing core, a NoC router, and L1 instruction and data caches as 
shown in Fig. 2. The tiles also have a shared global L2 cache, 
organized in banks. The tiles are connected by a NoC fabric. The CMP 
is constrained by a dark silicon power budget (DsPB) which is the 
thermally safe power limit that the cooling system of the chip can 
operate effectively within. The chip supports dynamic voltage scaling 
and can operate at different supply voltages V = {V1, V2... Vs}.    
 3.2   Application Model 
We assume the applications A = {A1, A2, ... AM} that execute on the 
CMP to be multithreaded, with each application Aj able to spawn up to 
K threads {T1, T2 … TK}. Each thread executes on a dedicated core Ci 
ϵ Ʈi. An application Aj can execute with different thread counts hence 
allowing for variable degree of parallelism (DoP). Each application 
has a performance deadline constraint.  
Application communication requirements are represented by an 
application graph APG = G(V, E) which is a directed acyclic graph, 
where each vertex vi ϵ V represents a thread and each edge ei,j ϵ E 
represents communication volume between thread i and thread j. All 
cores that execute the threads of an application are supplied with the 
same Vdd. The applications are stored in a service queue upon arrival 
at runtime and are considered for mapping to the CMP on a first-comefirst-serve (FCFS) basis. Applications are mapped on non-overlapping 
regions on the CMP for inter-application isolation. In the rest of the 
paper, the terms thread and task are used interchangeably. 
3.3   Power Delivery Network (PDN) in CMPs 
We assume a baseline PDN with multiple independent domains as 
shown in Fig 2. A domain is a group of four tiles that has its own 
voltage regulator module (VRM). These domains are physically 
separated so that there is no interference between tiles from different 
domains. Each domain is powered by an independent source, which 
enables efficient monitoring of the power consumption on the chip as 
the core count scales up. All of the tiles in a domain are supplied with 
the same Vdd, although the actual voltage received at the tile varies due 
to PSN-induced variation. We assume the presence of digital sensors 
[16] to monitor the runtime PSN levels at cores and NoC routers. We 
also assume that tasks of different applications are not mapped into a 
single domain, which is ensured by limiting the DoP values of each 
application to be multiples of 4.  
Figure 2: Baseline CMP with power supply domain of four tiles; each tile 
is powered by a voltage regulator (VRM) connected to a power source.  
(ii) inductive droop due to wire inductance (L.Δi/Δt). While resistive 
3.4   Power Supply Noise (PSN) Modeling and Estimation 
PSN is caused by (i) resistive drop of power delivery wires (IR), and 
drop is proportional to current flowing in the wires, inductive droop is 
proportional to the switching activity of the wires carrying current. We 
model the PDN as in previous works as shown in Fig 2, where Lb and 
Rb are inductance and resistance at a bump, Rc is resistance of the PDN 
wires, and Cdecap is the decapacitance between cores. The workload on 
a tile is modeled as a current source, similar to [19]-[21], based on 
power consumption of the core and NoC router in a tile. The dynamic 
values of PSN observed at each tile is given by: 
 																																						∆(cid:1848) (cid:3404) (cid:1848)(cid:3029)(cid:3048)(cid:3040)(cid:3043) (cid:3398) (cid:1848)(cid:3021)(cid:3036)                                   (1) 
where Vbump is the voltage supplied by the source and VTi is the voltage 
observed at tile i after on-chip parasitic drop. As in [12], we consider 
a PSN of 5% as a margin for a VE in near threshold voltages that leads 
to faulty outcomes for a thread executing on the PSN-affected tile. 
                              (a)                                                         (b) 
Figure 3: (a) Peak PSN (as % of supply voltage) observed in a domain for 
communication- and compute-intensive workloads; (b) Normalized PSN 
due to interference between pairs of tasks of different switching activity 
(High or Low) and separated by Manhattan distances of 1 and 2 hops.  
3.5  Impact of Mapping Decisions and DVS on PSN 
PSN is significantly impacted by variations in switching activity of 
transistors that leads to interference between the current flows in wires. 
Moreover, as shown in Fig. 3(a), the peak PSN observed in a domain 
is also directly proportional to its operating voltage (Vdd), which 
decides the maximum operating frequency (Fmax) of cores and routers 
in that domain. The trend exists for both communication-intensive and 
computation-intensive applications. To reduce PSN, one solution is to 
reduce Vdd. However, this also reduces Fmax, which diminishes 
application performance. Dynamic adaptation of application DoP is 
one way to improve performance while running at a low Vdd [25].  
The switching characteristics of tasks executing in close proximity 
to each other on a chip also have a considerable impact on PSN. Fig. 
 
 
 
 
 
 
 
 
 
3(b) shows interference effects of different combinations of switching 
activities for two tasks executing on adjacent cores. We categorize 
application tasks into two bins, “High” and “Low” active, based on 
extensive analysis of their switching activity. The PSN observed due 
to interference between tasks with High-Low switching activity is up 
to 35% higher than tasks with High-High and Low-Low switching 
activity. Cores running low switching activity tasks get affected by the 
resistive and inductive interference from the power drawn by high 
switching tasks running on the neighboring cores in the same domain. 
This behavior is also observed in [24]. Interestingly, Fig. 3(b) indicates 
that highly interfering tasks mapped at a distance of 2-hops away 
interfere up to 10% less than tasks mapped at a distance of 1-hop away. 
None of the prior works have exploited this observation to reduce the 
negative impacts of PSN.  
Given application performance deadlines and the dark silicon power 
budget (DsPB) for a CMP, our objective is to use the above 
observations to opportunistically select a combination of Vdd, DoP, and 
mapping region for each application that arrives for execution at 
runtime, to minimize the PSN observed in power supply domains. The 
next section discusses our proposed framework to meet this objective. 
Figure 4: Overview of the proposed PARM framework 
4. PSN AWARE RESOURCE MANAGEMENT (PARM) 
Fig. 4 gives an overview of our proposed PARM framework. Offline 
profiling information about applications, and online voltage noise 
feedback from on-chip voltage noise sensors are inputs to the 
framework. The application profiling collects statistics on switching 
activity, power consumption, and NoC communication characteristics 
for all of the tasks of an application at different Vdd’s and DoPs. The 
output of the framework is a task to core mapping, Vdd assignment, and 
DoP selection for each application that arrives for execution on the 
NoC-based CMP with independent power domains. Our PARM 
framework first selects an appropriate Vdd and DoP for an application 
to be mapped, based on the application performance and chip-level 
DsPB constraints. After Vdd and DoP selection, PARM uses a PSN 
aware mapping heuristic, to find a mapping in such a way that the total 
PSN in PDN domains is minimized; and communication distance 
between tasks is minimized (to improve performance). The following 
subsections discuss the components of the PARM framework. 
4.1 Vdd and DoP Selection  
Algorithm 1 presents our Vdd and DoP selection method. The inputs 
are a set of permissible voltages sorted in increasing order V = {V1, 
V2, ... VS}, and the set of applications waiting in the service queue A = 
{A1, A2, … AM}. To consume low power (and generate low peak PSN) 
while ensuring that application deadlines are met, the algorithm starts 
with the lowest Vdd and the highest DoP combination. This is because 
peak PSN is always low at lower Vdd values (Fig. 3(a)). Also, to meet 
application deadlines, it is intuitive to utilize the available tiles to 
spawn more number of threads (i.e., use a higher application DoP) 
without violating the DsPB constraint. To ensure this, the permitted 
DoP values of an application are sorted and considered in a decreasing 
order (line 1). The minimum DoP considered in our work is 4. 
Our selection algorithm then iteratively searches for a suitable (Vdd, 
DoP) combination. First, the Worst Case Execution Time (WCET) of 
an application for a selected (Vdd, DoP) combination is estimated using 
the offline application profile data (line 5) and checked to see if the 
application deadline constraint will be met (line 6). If the deadline 
constraint is met, this (Vdd, DoP) combination is sent as an input to the 
PSN-aware mapping heuristic (line 7; this heuristic is discussed in the 
next subsection). If the mapping is successful, the next application in 
the service queue is processed (line 8). If not, the algorithm waits till 
the CMP completes a currently executing application (which would 
free up tiles for mapping), and tries again to find a mapping region 
(lines 9-11). If the algorithm fails to find a mapping region, it selects 
the next DoP (lower value) from the list D, and performs the same 
operations as above (line 12). This can be useful to map an application 
when there are a lack of sufficient number of tiles, or a limited power 
budget. Selecting a lower DoP would resolve both of these concerns. 
However, if the estimated WCET (from line 5) does not meet the 
application deadline constraint, the algorithm skips iterating through 
DoPs, as the lower values of DoP cannot satisfy the deadline 
constraint, and continues searching for a new (Vdd, DoP) combination 
with the next Vdd from the list V, that is higher than the current Vdd value 
(line 13). If the deadline constraint is not met or if the mapping region 
is not found on the CMP after exploring all Vdd and DoP combinations, 
the current application is dropped and the next waiting application is 
processed, to avoid stagnation in the service queue due to the stalled 
application. 
Algorithm 1: Vdd, DoP selection 
Inputs: Vdd values sorted in increasing order V ={V1, ...VS}, applications A 
= {A1, … AM} 
 1:  for all Aj in A do  
2:       D ← Sort(Ai.{ D1, … DT }) //sorted in descending order 
3:       for all Vi in V do   
4:            for all Dk in D do    
5:                  WCET ← EstimateExecutionTime(Vi, Dk, Aj) 
6:                  if WCET < Deadline (Aj) then    
7:                        if PSNAwareMapping (Vi, Dk, Aj) is successful then  
8:                              goto line 1 (continue to next app in A) 
9:                        else stall till an app exit event on CMP   
10:                              if PSNAwareMapping (Vi, Dk, Aj) is successful then 
11:                                  goto line 1 (map the next app in A)  
12:                            else goto line 3 (Try with lower Dk,) 
13:                else goto line 2 (Try with next Vi) 
 Outputs: Vdd , DoP, and a valid region to map the application 
4.2 PSN Aware Mapping Heuristic 
Given a Vdd and DoP that satisfy the deadline constraint for the 
application to be mapped, we next attempt to find a mapping that fulfils 
the CMP dark silicon (DsPB) constraint and minimizes PSN in CMP 
power domains, as well as minimizing the total Manhattan distance of 
communication between tasks. This can be formulated as multiobjective optimization problem which has been shown to be NP-Hard. 
Traditional multi-objective optimization methods (e.g., 
integer 
programming, genetic algorithms) to solve the problem are too slow 
for decision making at runtime. Hence, we propose a fast runtime 
heuristic to select a suitable mapping region and meet all constraints. 
Algorithm 2: PSN Aware Mapping 
Inputs: Vdd, DoP, application A, Sorted APG edges A(E) = {e12, …, enn-1} 
1:  if  EstimatedPowerConsumption(A) > DsPB then 
2:      return False  //unable to find viable mapping  
6:        if Tj. ∉ H or Tj. ∉ L then  
3:  H ← {Ø} L ← {Ø}  // Set of clusters 
4:   for all ei in A(E) do 
5:        for each tasks Tj, connected to ei do 
7:           if Tj .High then push_back(Tj,H) 
8:           else push_back(Tj,L) 
9:    num_cluster ← create_clusters(H, L)       
10:    if num_available_domains < num_cluster then    
11:        return False  //unable to find viable mapping 
12:  else  
13:      task-cluster-to-domain-mapping() 
14:      return True 
Output: Successful mapping or indication of failed mapping  
 
 
 
 
 
  
 
 
 
Algorithm 2 shows our PSN-aware mapping approach. Given Vdd 
and DoP for the application A as inputs, the mapping heuristic aims to 
map all of the tasks of application A on to the CMP without violating 
the DsPB constraint, while minimizing the observed PSN. The 
algorithm first checks if the power consumption estimated from offline 
profiling is more than the available DsPB and returns false if the 
condition is not met without going further (lines 1-2). The tasks are 
labeled as high switching active or low switching active based on 
offline profile data. The heuristic utilizes the application graph APG of 
the application to be mapped to extract a sorted list of edges in the 
decreasing order of edge weights (communication volumes). To 
reduce PSN due to inter-task interference, the heuristic maps as many 
tasks with similar switching activity into the same power supply 
domain as possible. To reduce the NoC traffic, the heuristic also tries 
to map tasks with the highest communication volumes in the same 
domain. To achieve this, the heuristic iterates through the sorted edge 
list and creates clusters of 4 tasks, corresponding to the power supply 
domains of 4 cores. As the tasks are categorized into two types (high 
and low switching), we create two lists corresponding to the two task 
types (line 3). The algorithm checks if tasks connected to the edge 
being evaluated are already assigned to a list (line 4); if not, they are 
pushed to one of the two lists (lines 5-8).  
The two lists end up with tasks arranged in the decreasing order of 
communication volumes, as the edges have been evaluated in the 
decreasing order of their weights. Each list is then divided into clusters 
of four tasks in the order in which they are stored in the list (line 9). 
Any remaining un-clustered tasks from each list (< four; if the list size 
is not a multiple of four) are grouped into a single cluster. Clustering 
is done to ensure that (1) all but one of the created clusters will have 
tasks with similar switching activity, to be mapped in the same domain, 
(2) tasks with high communication volume between them are not 
mapped far from each other. If the available domains are less than the 
number of clusters (line 10) the algorithm returns false (i.e., no 
mapping found). If there are sufficient number of domains, each task 
cluster is mapped on to domains (line 13), in a manner that minimizes 
the hop distance between inter-domain mapped tasks. Further details 
of this step are omitted due to lack of space.   
Fig. 5 presents an example of the mapping heuristic for an APG and 
a sorted edge list. When mapping a task cluster on to a domain, if a 
cluster has two tasks of each switching activity level, tasks of the same 
level are mapped adjacent to each other, as shown in Fig. 5, to reduce 
PSN due to inter-task interference (Section 3.5). On successful 
mapping, the heuristic returns true (line 11), indicating a successful 
mapping). After mapping, the tasks of the mapped application are 
scheduled using the fast and efficient earliest deadline first (EDF) 
scheduling scheme. For EDF, each task is assigned a deadline 
(priority) based on the deadline of the entire application, using a 
technique proposed in our prior work on task-graph scheduling [23]. 
4.3 Time Complexity Analysis of PARM  
The Vdd and DoP selection step runs in linear time complexity with 
respect to the permissible Vdd and DoP levels (|V | = 5, |D| = 4 in this 
work). In the mapping step, task clustering runs in linear complexity 
with respect to the number of edges in the APG. The total number of 
possible edges in an APG is T×(T+1)/2, where T is the total number of 
tasks of an application. So, the clustering step has O(T2) complexity. 
Task-cluster-to-domain-mapping() has linear complexity with respect 
to number of tiles, hence the mapping step takes O(Ʈ), where Ʈ is the 
total number of tiles in the CMP. The runtime complexity of EDF 
scheduling scheme, given by O(T×logT), where T is the total number 
of tasks of an application, is masked by running in parallel with the 
mapping scheme that takes longer. So, PARM runs with a complexity 
of O(V×D×{max(Ʈ, T2)}) which depends on the number of CMP tiles, 
or number of tasks of an application, while V and D are small integers.   
Figure 5: Overview of PSN aware mapping heuristic 
4.4 PSN and Congestion Aware NoC Routing (PANR) 
To complement our PSN-aware mapping framework (PARM), we 
propose a PSN- and congestion-aware NoC routing scheme (PANR). 
PANR builds on and enhances a deadlock-free turn model based 
routing scheme called west-first routing [32]. Algorithm 3 shows the 
decisions made at each hop in a route. For each header flit in the input 
channel buffers of a NoC router, the routing scheme first computes the 
permitted destination hop directions (lines 1-3). It then selects a hop 
direction, from a set of permitted directions, by considering the voltage 
noise sensor data and incoming data rate (flits/cycle) from the routers 
in the tiles that are adjacent to the current tile. If the buffer occupancy 
of the input channel is beyond a threshold B, the output direction with 
the least incoming data rate is chosen to minimize the congestion (line 
5). In the case of lower buffer occupancy than B, the output direction 
with the least observed PSN is chosen (line 6), to reduce the activity in 
the router in that direction, which in turn minimizes the overall PSN 
observed in the domain. Once a direction is decided for a header flit, 
the remaining flits in a packet follow the header flit. 
Algorithm 3: PSN and Congestion Aware NoC Routing 
Inputs: destination tile coordinates, PSN activity of adjacent tiles, traffic 
load in adjacent NoC routers   
1:   for each channel in Input_channels do  
2:      flit ← channel.packet.header_flit 
3:     {permissible directions} ← WestFirstRouting (flit.src, flit.dest) 
4:     if channel.buffer_occupancy > B then 
5          hop ← min_data_rate{permissible directions}  
6:     else  hop ←  min_PSN{permissible directions} 
7:   return hop 
Output: Next hop direction for packets in input channels 
Overhead computation: The overhead of our routing scheme involves 
registers to store the values of noise and router traffic levels of the 
adjacent tiles, and additional wires to transmit those values between 
tiles. In addition, two 64-bit comparators are used per router to find the 
minimum values of PSN and incoming data rates of adjacent routers. 
The additional circuitry at each router consumes ~1 mW (3%) power 
and ~115 μm2 (0.5%) area overhead over the baseline NoC router, at 
the 7nm node. Hop selection takes 1 cycle in a router, at 1 GHz. This 
latency is masked by executing the selection step in parallel with the 
route computation step. The overhead of the network of digital PSN 
sensors used to sense the voltage noise [16], is around 413μm2 which 
is negligible compared to the core area which is ~4 mm2, and the router 
area of ~71300 μm2, at a 7nm FinFET node. 
4.5 Fault Detection and Correction  
Our proposed PSN aware mapping and routing minimizes voltage 
fluctuations due to PSN. However, there may still be some cases when 
inter-core interferences lead to PSN above a certain threshold which 
leads to voltage emergencies (VE). VEs have the potential to cause 
errors in the functionality of logic devices and faulty application 
execution. To prevent such scenarios, applications are checkpointed at 
periodic intervals [26]. When a VE is detected using on-chip sensors 
in a tile that runs an application, it is rolled back to its last saved 
checkpoint and begins execution from there  
 
 
 
 
 
 
 
 
5. EXPERIMENTS 
5.1 Simulation Setup 
We conducted experiments on 13 different parallel applications 
from the SPLASH-2 [28] and PARSEC [29] benchmark suites. We 
used the GEM5 [30] multicore simulator to generate the offline profile 
data for applications. We modeled the PDN as discussed in section 3.3 
using the SPICE simulator. We estimated the power consumption of 
different applications at various Vdd, clock frequency values, and appDoP, at a 7nm FinFET technology node, using data from McPAT [31] 
and ITRS [4]. The DoP values used range from 4 to 32 (in multiples of 
4, beyond which most of the applications were observed to have lower 
performance due to communication (synchronization) overheads. The 
switching activity of the core and NoC router in a tile was observed to 
be proportional to its power consumption. We also sampled PSN 
values of the tiles at periodic intervals, and when a new application 
begins or a current one ends execution on the CMP, using the PDN 
SPICE model. The buffer occupancy threshold B for hop selection in 
PANR is set to 50% after analyzing the effects of different occupancy 
levels on router throughput, with a cycle-accurate NoC simulator.  
We categorized 13 benchmarks into two groups: (i) communicationintensive benchmarks: {cholesky, fft, radix, raytrace, dedup, canneal, 
vips}; and (ii) compute-intensive benchmarks: {swaptions, fluidanimate, streamcluster, blackscholes, radix, bodytrack, radiosity}. As 
radix has properties of both, we use it in both groups. We employed 
three sequences of application with up to 20 applications picked 
randomly from each of the groups mentioned above. The sequences 
are categorized as compute-intensive, communication-intensive, and 
mixed, according to the type of applications used in each sequence.  
We also experimented with three different inter-application arrival 
rates of 0.2s, 0.1s, and 0.05s to test the efficiency of our framework for 
different CMP utilization (workload subscription) scenarios.  
We consider a 60 core 2D NoC-based CMP designed at 7nm 
FinFET technology node for our studies. The tiles are arranged in a 10 
×6 mesh layout. Each tile has an ARM Cortex A-73 low power mobile 
core, a NoC router, and a private L1 cache. We assume that PARM is 
part of the OS or middleware that assigns Vdd, DoP, and task-to-core 
mapping regions to each application according to the availability of 
DsPB and idle cores at runtime. The Vdd values supported by each tile 
(core + router) are between 0.4V (NTC) to 0.8V in steps of 0.1V. We 
assume that all of the cores on which an application is mapped to run 
at the same Vdd. We assume a dark silicon power budget (DsPB) of 
65W. We treat PSN above 5% as a voltage emergency, similar to prior 
work [12]. We assume an overhead of ~256 cycles for periodic 
checkpointing with a 1ms checkpoint period, and ~10000 cycles of 
overhead for rolling back (restart) to an earlier state, after an error. 
5.2 Simulation Results   
We compare our PARM framework against a prior work [21] that 
tries to minimize PSN using a harmonic mapping scheme, where tasks 
with high activity are mapped far away from each other. We refer to 
the scheme as HM. To show the impact of NoC routing on PSN, we 
compare our proposed PANR routing scheme with an XY routing 
sch"
Coding approach for low-power 3D interconnects.,"Through-silicon vias (TSVs) in 3D ICs show a significant power consumption, which can be reduced using coding techniques. This work presents an approach which reduces the TSV power consumption by a signal-aware bit assignment which includes inversions to exploit the MOS effect. The approach causes no overhead and results in a guaranteed reduction of the overall power consumption. An analysis of our technique shows a reduction in the TSV power consumption by up to 48 % for real correlated data streams (e.g., image sensor), and 11 % for low-power encoded random data streams.","Coding Approach for Low-Power 3D Interconnects
Lennart Bamberg
ITEM.ids, University of Bremen
bamberg@item.uni- bremen.de
Robert Schmidt
ITEM.ids, University of Bremen
rschmidt@item.uni- bremen.de
Alberto Garcia-Ortiz
ITEM.ids, University of Bremen
agarcia@item.uni- bremen.de
Abstract
Through-silicon vias (TSVs) in 3D ICs show a significant power
consumption, which can be reduced using coding techniques. This
work presents an approach which reduces the TSV power consumption by a signal-aware bit assignment which includes inversions to
exploit the MOS effect. The approach causes no overhead and results
in a guaranteed reduction of the overall power consumption. An
analysis of our technique shows a reduction in the TSV power consumption by up to 48 % for real correlated data streams (e.g. image
sensor), and 11 % for low-power encoded random data streams.
1 Introduction
3D integration is a promising solution to overcome the challenges
that arise with the limit of Moore’s law. To connect the dies of
a 3D system on chip (3D SoC), through-silicon via (TSV) arrays
are typically used as they yield to a short delay and a high reliability [1]. Previous work shows that shifting from 2D to 3D integration,
employing TSVs, allows for a significant reduction in the circuit
footprint and delay, but often increases the power consumption [2].
The system power consumption is significantly affected by TSVs
as they suffer from capacitive coupling which additionally impairs
the signal integrity [3]. In TSV arrays, the coupling capacitances are
large due to the relatively large TSV dimensions and the conductive
substrate [4]. Additionally, the high number of aggressors in 3D
further increases the coupling. Thus, coupling is a critical design
concern for 3D integrated circuits (3D ICs) and consequently caught
the attention of academia and industry (e.g. [4–15]).
Most previous works deal with coupling modeling [4–12] and
coupling suppression using manufacturing techniques [9–12]. However, these techniques significantly increase the production cost and
further impair the already critical TSV yield [1]. Additionally, most
manufacturing techniques aim for signal integrity optimization,
while leaving the overall power consumption unaffected [9–11].
Since coupling is a pattern dependent phenomena [16], data encoding approaches have recently been proposed which reduce the
coupling peaks, without affecting the manufacturing [13–15]. These
techniques again improve the signal integrity but also increase the
TSV count, leading to an even increased overall TSV power consumption [3]. Thus, despite its importance, low-power techniques
for TSVs have not yet been properly researched.
Low-power coding is very efficient for planar metal-wires [3].
Metal-wires only show a significant coupling with their two adjacent neighbors and the coupling capacitance between each adjacent
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and /or a
fee. Request permissions from permissions@acm.org.
DAC ’18, June 24–29, 2018, San Francisco, CA, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5700-5/18/06. . . $15.00
https://doi.org/10.1145/3195970.3196010
metal-wire pair has the same size. In contrast, TSVs have a maximum of eight adjacent neighbors and due to the different distances
between direct and diagonal adjacent neighbors, combined with the
E-field sharing effect [10], several capacitance values exist in a TSV
array [5]. Hence, traditional low-power coding techniques are not
directly applicable for TSV arrays. Thus, there is a need for new
efficient low-power strategies for the TSVs in 3D integration.
In this work we present the first 3D low-power coding approach.
Typical 3D SoCs take advantage of heterogeneous integration [17]:
sensor, processor and memory elements are fabricated in individual dies, using the most efficient technology for each die. Afterwards the dies are stacked and connected by TSVs. In such systems, the patterns traversing the TSVs are often temporally correlated and/or normally distributed, resulting in bits with different
switching properties [18]. We show that these properties can be
exploited to effectively reduce the power consumption by an intelligent bit-to-TSV assignment, since the capacitances of TSV arrays
are heterogeneous [5]. An additional fixed inversion of some bits
before the transmission, realized by using inverting TSV drivers,
can further decrease the power consumption, mainly due to the
MOS effect. Our approach only affects the local bit-to-TSV assignments within the individual TSV arrangements, while the global
net-to-TSV-bundle assignment remains routing optimal. Thus, the
overhead costs are negligible.
A key contribution of this work is a formal method to find the optimal bit-to-TSV assignment (including inversions), that minimizes
the TSV power consumption for any given data stream and TSV
arrangement. To overcome the need for the exact data properties,
systematic bit-to-TSV assignments, generally applicable for DSP
signals, are contributed as well. A wide set of analyses, for real and
synthetic data, shows that our approach can reduce the power consumption of modern TSVs by over 40 %, despite its negligible costs.
The remainder of this work is structured as follows: after some
preliminaries, the method to determine the optimal assignment
is derived in Sec. 3. Systematic assignments for DSP signals are
presented in Sec. 4, which are compared to an optimal assignment for
real data streams in Sec. 5. In Sec. 6, the combination of our approach
with traditional low-power codes is briefly discussed. Experimental
results are presented in Sec. 7. Finally, a conclusion is drawn.
2 Preliminaries: TSV model
The power consumption of TSVs can be precisely estimated by
the power consumption related with its capacitances [6]. Thus, to
calculate the TSV power consumption, the capacitance matrices
for modern TSV arrays are required. In this work, the capacitance
matrices are extracted by means of electromagnetic field simulations
for 3D structures of TSV arrays, using Ansys Q3D extractor.
In the analyzed structures, the TSVs are regularly placed in a
M ×N array, where M and N are arbitrarily defined. The distance
between the centers of two direct neighbored vias is constant and
denoted by d . The cylindrical TSVs of length l and radius r are
made up of copper. The TSVs traverse through the p-doped silicon
substrate, which has a conductivity σ of 10 S/m. For DC insulation,
each TSV is surrounded by a SiO2 dielectric of thickness r/5. In the
model, the geometry parameters d and r are varied, in order to
analyze different global TSV dimensions predicted by the ITRS for
the year 2018. The length of the TSVs is defined by the substrate
thickness equal to 50 µm. A TSV, its dielectric and the substrate
form a metal oxide semiconductor (MOS) junction. Thus, a TSV is
surrounded by a depletion region, which is modeled in the Q3D
extractor as a depleted substrate area (σ = 0) [19]. The width of
a depletion region surrounding TSV i is calculated by solving the
exact Poisson’s equation for an average TSV voltage of pr i ·Vd d ,
where pr i is the probability of a logical 1 on TSV i . Vd d = 1 V is
the power supply voltage.
For the final validation of our work, circuit simulations are used.
Therefore, full 3π -RLC circuits of the TSV arrays are also extracted.
3 Power-optimal bit-to-TSV assignment
In this section we derive our approach to reduce the TSV power
consumption by choosing the optimal (fixed) bit-to-TSV assignment.
The approach does not induce a circuit nor a TSV overhead. The
only overhead cost is a slight increase in the local metal-wire lengths
as the prior global net-to-array assignment remains routing optimal.
To quantify precisely the costs of our approach, we analyze a
3×3 TSV array, including the local routing, for a commercial 40 nm
technology and TSVs with a radius of 2 µm and a minimum pitch
of 8 µm. In detail, we analyze all possible bit-to-TSV assignments,
while considering the array symmetry. The worst-case routing only
increases the path parasitics by a maximum of 0.4 %, versus a routing
which aims for a local wire length minimization. The overall mean
parasitic increase for all assignments is below 0.2 % with a standard
deviation below 0.1 %. Thus, the effect of the local routing is marginal
as TSV parasitics are dominant. Additionally, due to Keep-out-Zone
restrictions, no active components are located nearby TSV arrays.
Thus, we do not face a metal-layer-utilization problem. Summarized,
the overhead costs for our approach are in fact negligible.
In the following, we derive a formula to calculate the poweroptimal bit-to-TSV assignment. Thereby, we do not only consider
the possibility of a mere reordering, but also the transmission of
negated bits over some interconnects.
First of all, we review the model for the power consumption of
(capacitive) interconnect structures, stated in Ref. [6]. Due to the
thick oxides, leakage currents can be neglected for interconnects.
The mean dynamic power consumption of an N bit interconnect for
an initial assignment, mapping bit i (bi ) to interconnect i , is equal to:
E{∆b 2
i }Ci ,i +
E{∆b 2
i − ∆bi ∆b j }Ci , j
(1)
(cid:42)(cid:46)(cid:44) N(cid:88)
i
P =
2 f
Vd d
2
N(cid:88)
i , j
(cid:43)(cid:47)(cid:45) .
Here, the first term Vd d
2 f /2 depends on the power supply voltage
Vd d and the clock frequency f , which are not affected by a coding
approach. Thus, in the following we use the mean power consumption normalized by this factor: Pn = 2P/Vd d
2 f . In Eq. 1, Ci ,i is the
ground capacitance of interconnect i , and Ci , j is the coupling capacitance between the two interconnects i and j . Furthermore, E{ } is the
expectation operator. ∆bi represents the switching of bit i , which
is either 1 (0 to 1 transition), 0 (no transition), or −1 (1 to 0 transition). Thus, E{∆b 2
i } is the self switching probability of interconnect i .
While the power consumption due to the ground capacitance of
an interconnect i is determined only by its self switching, ∆bi , the
power consumption associated with a coupling capacitance Ci , j , is
additionally affected by a switching on interconnect j , ∆b j . Compared to the scenario where only interconnect i toggles (∆b j = 0):
the contribution of Ci , j to the power consumption is doubled when
interconnect j toggles in the opposite direction (∆bi ∆b j = −1) and
vanishes if it toggles in the same direction (∆bi ∆b j = 1).
The normalized power consumption Pn , can also be expressed
using Frobenius inner product (⟨⟩) of two matrices T and C:
(2)
Here, C is the capacitance matrix, with capacitance Ci , j on entry i ,j .
T presents the switching probabilities of the bits:
Pn = ⟨T, C⟩.
T = Ts 1N ×N − Tc ,
i }
(3)
where Ts is a matrix with the self switching probabilities E{∆b 2
on the diagonal entries, and zeros on the remaining entries. Tc
represents the coupling probabilities with zeros on the diagonal
entries and E{∆bi ∆b j } on entry i , j . 1N×N is a matrix of ones.
Since the capacitances of the C matrix are heterogeneous [5, 10],
the assignment of the logical bits to the TSVs affects the power
consumption. Moreover, a fixed inversion of some of the logical
bits before the transmission may potentially decrease the T entries. If some bit pairs of the data stream are negatively correlated
(E{∆bi ∆b j } < 0), initially in between them the likelihood of transitions in the opposite direction (∆bi ∆b j = −1, causing the highest
power consumption) is higher than the likelihood of aligned transitions (∆bi ∆b j = 1, causing the lowest power consumption). In this
scenario, the transmission of one of the two bits i or j negated over
an interconnect (e.g. bi → interconnect x ) results in a positive spatial
correlation, since E{∆bi ∆b j } = −E{∆bi ∆b j } > 0, and consequently
to a reduced power consumption. Additionally, the bit assignment,
including inversions, can also affect the TSV capacitances. Due to
the MOS effect, an increased 1-bit probability on a TSV enlarges
the width of its depletion region, resulting in up to 40 % lower capacitance values [6]. Therefore, for TSVs, transmitting data streams
where the bit probabilities are not equally balanced (E{b} (cid:44) E{b}),
the capacitance values depend on the assignment, including possible
inversions. In the following we model these aspects.
First, we consider the switching matrix T. The effect of a reassignment, including inversions, on T is mathematically expressed as:
π .
(4)
Here, Aπ is a permutation matrix [20], which also performs inversions. A valid Aπ has one 1 or one −1 in each column/row while all
other matrix entries are 0. To assign the i t h bit of the data stream to
line j , Aπ j ,i is set to 1. To assign the negated bit to the line, Aπ j ,i
is set to −1. Thus, for an exemplary 3 bit interconnect structure, to
assign bit 3 negated to line 1, bit 1 to line 2 and bit 2 to line 3:
0 −1
1
0
0
0
1
0
Second, we derive a mathematical method to estimate the TSV
capacitance matrix C depending on the bit-to-TSV assignment. The
exact bit probability — capacitance relation is very complex, and
consequently not suitable to determine the optimal assignment at
high levels of abstraction. However, a linear regression to estimate
the capacitance values as a function of the bit probabilities has a normalized root mean square error below 2 % [6]. Thus, the following
equation can be used to estimate the size of a coupling capacitance
for an assignment of the bits i and j to the TSVs i and j :
T ′ = T ′
s 1N ×N − T ′
π 1N ×N − Aπ TcAT
c = Aπ TsAT
 .
0
Ci , j = C0,i , j + ∆Ci , j (E{bi } + E{b j } ) ,
(6)
where C0,i , j is the capacitance value for all 1-bit probabilities equal
to zero and ∆Ci , j is the derivation of the capacitance value with
increasing 1-bit probability E{bi } or E{b j } . Since our requirement is
Aπ =
(5)
a formula where an inversion of the bits leads to one simple negation
in the formula, we use a shifted form of Eq. 6:
LSB
MSB
T ) ,
ϵi = E{bi } − 1
Ci , j = CR,i , j + ∆Ci , j (ϵi + ϵ j ) .
C ′ = CR + ∆C ◦ (Aπ ϵ 11×N + 1N ×1ϵT Aπ
(7)
Here CR,i , j is the capacitance value for all bit probabilities equal to
1/2 (CR,i , j = C0,i , j + ∆Ci , j ). ϵi is mathematically expressed as:
2 .
(8)
Since E{b i } = 1 − E{bi } , an inversion of bi , negates the ϵi value.
Thus, the capacitance matrix as a function of Aπ is expressed as:
(9)
where CR and ∆C are matrices containing the CR,i , j and ∆Ci , j
values. ϵ is the vector of ϵi values. ◦ is the Hadamard operator.
Finally, we can determine the power-optimal bit assignment ˆAπ :
ˆAπ = arg min
,
(10)
where Eq. 4 and Eq. 9 are substituted for T ′ and C ′ . S N is the set of
valid permutation matrices including all possible inversions.
In practice ˆAπ is determined with any of the several optimization
tools available to reduce the computational complexity. Although
overall up to several hundreds of TSVs exist in modern 3D ICs, the
runtime of an optimization is negligibly low for our problem as it is
executed for each TSV bundle individually whose size is relatively
small. In this work, we exemplary use simulated annealing [21] to
determine the optimal mapping.
, C ′⟩(cid:17)
(cid:16)⟨T ′
Aπ ∈S N
4 Systematic TSV assignments for DSP signals
In some scenarios a sample data stream, required to obtain T, may
not be known at design time. In this case, the basic characteristics of
the data can be used to obtain systematic assignments. As an example, in this section we focus on systematic assignments, applicable
for DSP signals as they build an important data type in SoCs.
The bit-level characteristic of DSP signals are well understood [18],
and only briefly summarized in the following. In many DSP signals,
due to a zero mean normal distribution of the patterns, MSB pairs
are strongly correlated (E{∆bi ∆b j } ≫ 0). Additionally, a temporal
pattern correlation affects the self switching (E{∆b 2
i }) of the MSBs.
The self switching probability is 1/2 for no pattern correlation and decreases with an increasing pattern correlation. Generally, the LSBs
tend to be uncorrelated (E{∆bi ∆b j } = 0; E{∆b 2
i } = 1/2). Furthermore,
all bit probabilities are equal to 1/2. Consequently, the capacitance
matrix is assignment independent, resulting in:
n = ⟨T ′
s 1N ×N − T ′
c , CR ⟩.
(11)
Because of the positive bit correlations, we present systematic
assignments without bit inversions. More precisely, we present two
systematic approaches: one exploiting a temporal pattern correlation
and one exploiting a mean-free normal distribution of the patterns.
First, we analyze temporally correlated, equally distributed patterns. An equal distribution causes no spatial bit correlation, which
implies: E{∆bi ∆b j } = 0 for all i (cid:44) j . Thus, for equally distributed
signals, all elements of T ′
c are zero. Therefore, Eq. 11 simplifies as:
(12)
s 1N ×N , CR ⟩ =
n = ⟨T ′
s i ,i CT ,i ,
T
(cid:88)
P
P
′
′
′
i
where CT ,i is the sum of all capacitances connected to interconnect
i . T ′
s i ,i is the i t h diagonal entry of T ′
s , which is equal to the self
switching probability of the bit transmitted over interconnect i .
Therefore, to minimize P ′ , bits with the highest self switching
probability E{∆b 2
i } have to be transmitted over TSVs with the lowest
MSB
LSB
a) Spiral
b) Sawtooth
Figure 1. Systematic bit-to-TSV assignments: Spiral for correlated
signals and Sawtooth (ST) for normally distributed signals.
20
10
]
%
[
d
e
r
P
Opt. – 4×4; r=2 µ m
Spiral – 4×4; r=2 µ m
Opt. – 5×5; r=1 µ m
Spiral – 5×5; r=1 µ m
0
0 .2
0 .4
0 .6
0 .8
1
Branch Probability
Figure 2. Decrease in power consumption (Pr e d ) due to the optimal
and the Spiral bit-to-TSV assignment for sequential data streams.
overall capacitance CT ,i and vice versa. In TSV arrays, corner TSVs
have the lowest overall capacitance, and edge TSVs have a lower
overall capacitance than TSVs in the middle of an array [5]. Thus,
the optimal assignment maps the bits with the highest self switching
to the array corners. The bits with the next highest self switching
are mapped to the array edges. The remaining bits are mapped to the
array middle. Since the MSBs of correlated patterns show the lowest
self switching, our systematic assignment for correlated patterns
forms a spiral, as illustrated in Fig. 1.a.
We validate our Spiral mapping for synthetic sequential data
streams with varying branch probability, as their patterns are equally
distributed and temporally correlated. With the branch probability,
the temporal pattern correlation varies. The simulated power consumption reductions, compared to a worst-case random assignment,
for two TSV arrays: a 4×4 array with r = 2 µm; d = 8 µm, and a
due to the Spiral and the optimal assignment, are shown in Fig. 2
5×5 array with r = 1 µm; d = 4.5 µm. Fig. 2 reveals that the power
consumptions for both assignments, optimal and Spiral, are almost
equal. This proves the optimality of the systematic approach.
As a second scenario, we investigate a systematic assignment
for mean-free normally distributed but temporally uncorrelated
patterns. This implies that the self switching probability of each bit
is 1/2. Thus, all diagonal elements of T ′
s are 1/2 independent of the
assignment, which results in a normalized power consumption of:
(13)
2 · 1N ×N − T ′
c , CR ⟩ = 1
c i , j Ci , j ,
T
n = ⟨ 1
(cid:88)
CT ,i − (cid:88)
i , j
where T ′
c i , j is the correlation between the two bits transmitted over
the interconnects i and j . Therefore, in order to reduce the power
consumption, highly correlated bit pairs (large E{∆bi ∆b j }) have to
be assigned to TSV pairs connected by a large coupling capacitance.
In TSV arrays, the biggest coupling capacitances are located between
corner TSVs and their two direct adjacent edge TSVs, due to the
reduced E-field sharing effect [5]. MSBs of normally distributed signals have the highest cross-correlation. Thus, our second systematic
assignment has to map the MSB onto a corner and the next lower
P
2
′
′
i
d)
e)
b)
c)
a)
4
6
8
10
12
14
0
10
20
30
log2 (σ )
P
e
r
d
[
%
]
ρ = 0
Opt.
Sawtooth (ST)
Spiral
4
6
8
10
12
14
40
30
20
10
0
log2 (σ )
P
e
r
d
[
%
]
ρ = −0 .9
4
6
8
10
12
14
40
30
20
10
0
log2 (σ )
P
e
r
d
[
%
]
ρ = −0 .5
4
6
8
10
12
14
0
10
20
log2 (σ )
P
e
r
d
[
%
]
ρ = 0 .5
4
6
8
10
12
14
20
15
10
5
0
log2 (σ )
P
e
r
d
[
%
]
ρ = 0 .9
Figure 3. Power consumption reduction (Pr e d ) due to our mapping
approaches for uncorrelated (3.a) and correlated (3.b-3.e; ρ (cid:44) 0),
Gaussian distributed data streams with standard deviation σ .
significant bit onto one of its direct adjacent edge TSVs. The following bits, recursively, have to be mapped by finding the TSV in the
array that has the biggest accumulated coupling capacitance with all
previously assigned TSVs. Finally, our systematic assignment results
in the MSB to LSB mapping illustrated in Fig. 1.b. Over the first two
rows the bits, from the MSB downwards, are mapped in a sawtooth
manner. From the third row on, a simple row-by-row mapping is
used. Fig. 3.a shows the reduction in the power consumption due to
the optimal and the Sawtooth (ST) assignment for the transmission
of Gaussian distributed 16 b pattern sets, over a 4×4 TSV array (r =
2 µm; d = 8 µm). The results are plotted over the standard deviation
of the patterns, to analyze different normal distributions. The figure underlines the optimal nature of the Sawtooth assignment for
normally distributed, temporally uncorrelated patterns.
However, in some real applications, temporally correlated and
normally distributed signals occur. For these data streams, the optimal TSV assignment is not trivial and dependent on the correlation
quantities. As shown in Fig. 3.b-3.e, for negatively correlated, Gaussian distributed patterns the Sawtooth mapping leads to the lowest
power consumption (reduction up to 40 %), while for a positive
temporal correlation neither Sawtooth nor Spiral mapping lead to
the optimal power consumption. However, compared to a random
assignment both approaches still lead to a significant improvement.
Summarized, if it is not possible to determine the optimal assignment by means of Eq. 10, which guarantees the lowest possible
power consumption, the proposed Sawtooth mapping should be
applied for normally distributed signals and the Spiral mapping for
primarily temporally correlated signals.
5 Comparison of systematic and optimal
mapping for real DSP signals
Until this point, we investigated the efficiency of our proposed
technique for synthetic DSP signals. In the following, we analyze
and compare the efficiencies of the systematic and the optimal
bit-to-TSV assignments for real DSP signals. Thereby, we focus
on an important class of systems: heterogeneous 3D SoCs. Two
R G B
R G B
+
4
S
R G B M u
x .
+
1
S
G r
a
y
+
1
S
5
10
15
P
e
r
d
[
%
]
Opt. – r =1 µ m
Spiral – r =1 µ m
Opt. – r =2 µ m
Spiral – r =2 µ m
Figure 4. Power consumption reduction (Pr e d ) for an optimal/Spiral
assignment and image sensor patterns. “+x S” indicates x stable lines.
commercially relevant examples are 3D VSoCs [17], including dies
for image sensing and dies for digital image processing, and SoCs
with an dedicated MEMS sensor die, bonded to a digital die [22].
5.1 Vision system on chip
In contrast to pure CMOS image sensors, VSoCs are used to capture
and process the images in a single chip. This overcomes the limitations of traditional systems due to expensive image transmissions
between sensor and processor, especially for high frame rates [17].
In a 3D VSoC some dies are dedicated for image sensing and
digitalization and some for image processing. In this subsection, we
investigate our approach for the transmission of digitalized image
pixels from a sensing layer to a processing layer.
The first three analyses are performed for data stemming from
a 0–255 RGB image sensor using a Bayer pattern filter [23]. First,
we analyze the parallel transmission of all four RGB colors (1 red, 2
green, 1 blue) of each Bayer pattern pixel over one 32 b (4×8) TSV
array. For the second analysis, we assume four additional TSVs in the
array (resulting in a 6×6 array): one TSV carrying an enable signal,
one redundant TSV for yield enhancement and two power/ground
TSVs to supply the sensor. In the third analysis the four colors of
each pixel are transmitted one after another (RGB Mux.) over a 3×3
array including an additional enable signal. The fourth analysis, is
performed for a data stream stemming from a 0–255 grayscale image
sensor. Here, the transmission of one pixel per clock cycle over a 3×3
array including an enable signal is investigated. All analyzed data
streams are composed of pictures of cars, people and landscapes.
For all analyses, the reduction in the power consumption, against
random assignments, is investigated for the optimal and the Spiral
assignment, since the strong correlation of adjacent pixels generally
results in a temporal pattern correlation. Redundant, enable and
power/ground signals are considered as (almost) stable. Redundant
and enable signals are assumed as set to logical 0 when unused,
which may be exploited by inversions. Vd d and GND lines are always
on logical 1 and logical 0, respectively, but an inversion for power
lines is not possible and consequently forbidden for the assignment.
For the simultaneous transmission of a complete RGB pixel, the bits
of the four color components are interleaved one-by-one for the
Spiral mapping. Since stable lines are perfectly correlated, they are
added as MSBs for the Spiral mapping.
For the global TSV dimensions, we choose the minimum ones
predicted for the year 2018 (r = 1 µm; d = 4 µm). To show the effect
of varying TSV geometries, the power consumption for the 3×3 and
the 6×6 array is also investigated for r = 2 µm and d = 8 µm.
The simulated power reductions due to the various assignments
are reported in Fig. 4. The results show that the Spiral mapping
is almost optimal for the transmission of image sensor patterns
without stable lines and always leads to a power reduction of 11–
13 %, except for the multiplexed colors, where the reduction is only
]
%
[
d
e
r
P
20
15
10
5
0
g
M a
Opt.
ST
Spiral
R M S
n .
o .
r
G y
R M S
c .
A c
R M S
M a
n .
g
Z
Y
X
o .
r
G y
Z
Y
X
Z
Y
X
c .
A c
A l l
Figure 5. Decrease in the power consumption (Pr e d ) for our optimal/systematic approach and MEMS sensor signals.
5 %. Here, due to the multiplexing, the pixel correlation is lost why
only the reassignment of the stable line results in a power reduction.
With stable lines in the TSV array, the power reduction due to an
optimal assignment is up to 2.5 percentage point higher, as it considers inversions and the coupling properties of stable lines. Therefore,
with stable lines, the potential to reduce the power consumption,
using our approach, is also higher.
Summarized, the simulations show that both, optimal and systematic, bit-to-TSV assignments can effectively reduce the TSV power
consumption in 3D VSoCs. However, in the presence of additional
stable lines, the optimal approach has a noticeably higher gain.
5.2 MEMS sensors in a 3D system on chip
In this subsection we analyze the efficiency of our technique for
MEMS sensor data, transmitted from a sensing to a processing layer.
Therefore, sensor signals from a modern smartphone in various
daily use scenarios are used. Analyzed is a magnetometer, an accelerometer and a gyroscope, all sensing on three axes (x, y and z)
with a resolution of 16 b. We assume a transmission of one sample
per time step over a 4×4 array with r = 2 µm and d = 8 µm. We analyze the transmission of the single data streams for two scenarios.
In the first one, only the root mean square (RMS) values resulting
from the three axis values are transmitted. In the second one, the
x-, y- and z-axis values are regularly interleaved/multiplexed (XYZ).
For completeness, we also analyze the transmission of all three data
streams over one TSV array. Thereby, a regular pattern-by-pattern
multiplexing of the three XYZ-interleaved data streams is assumed.
Here, we investigate both systematic bit-to-TSV assignments since
normally distributed and temporally correlated data streams occur.
The simulated mean power consumption reductions against random assignments are shown in Fig. 5. The figure reveals that, for
the interleaved data streams, the proposed Sawtooth mapping is
only slightly worse than the proposed optimal assignment which
reduces the power consumption by up to 21.1 %. Generally, the single axis values are normally distributed and temporally correlated.
However, for interleaved data streams temporal correlation is lost.
Thus, these scenarios build examples for temporally uncorrelated,
normally distributed signals, since an interleaving does not affect
the pattern distribution. The small gain for the optimal bit-to-TSV
assignment over the Spiral mapping is caused by the fact that not
all sensor signals are perfectly mean-free.
In contrast, for the RMS data streams, the Spiral mapping significantly outperforms the Sawtooth mapping because here the patterns
are unsigned (no zero mean) and spatially correlated. However, for
the RMS data streams, the maximum possible power reduction due
to a reassignment is 13.3 %, which is significantly lower than the
maximum power reduction for the interleaved data streams.
In conclusion, for real data streams, the exploitation of a meanfree normal distribution is more efficient than the exploitation of a
temporal pattern correlation. Furthermore, due to non idealities in
real signals, the optimal approach has a slightly higher gain than
a systematic one. But generally, both assignments, systematic and
optimal, lead to a significantly reduced TSV power consumption.
6 Combination with data encoding
Modifying the data properties using encoding techniques is a well
established low-power approach [3]. Our proposed technique enables the use of existing low-power coding techniques, designed to
reduce the power consumption of metal-wires and/or gates, for 3D
integration in the most efficient way by finding the optimal bit-toTSV assignment. Thus, if an encoding is already applied for other
components, no additional overhead is required for the TSV coding.
Unencoded, most data streams generally have a balanced number
of 0- and 1-bits. However, data encoding techniques often lead to
a large fraction of 0-bits [3], which affects the power consumption
in 3D negatively. Here, our optimal mapping further boosts the
efficiency of the coding approach by transmitting inverted bits. Generally, an inversion is realized by using inverting buffers instead of
non-inverting ones (or vice versa) on both sides of the TSV. However,
inversions can be also hidden in the coder/decoder.
For example, Gray coding is a popular approach to reduce the
power consumption of gates and metal-wires. The nt h binary-toGray encoders output is equal to the nt h input XORed with the
n+1t h input (Y [n] = X [n] ⊕ X [n + 1]). Consequently, due to the
spatial MSB correlation in normally distributed signals, Gray coding
results in bits nearly stable on logical 0 for this kind of data. This
reduces the switching activities but also decreases the 1-bit probabilities. Here the required inversions for the optimal bit-to-TSV
assignment can be realized inside the Gray encoder and decoder:
XOR operations are swapped with XNOR operations to obtain the
negated code words which increases, instead of decreases, the 1-bit
probabilities, while leaving the switching activities unaffected. Since
XOR and XNOR operations have the same costs, this optimization
of the data encoding technique is overhead free.
7 Experimental results
In this section, our approach is investigated for real signals and traditional coding approaches by means of Spectre circuit simulations
in combination with the results from the Q3D extractor. Here, TSV
arrays with r = 1 µm and d = 4 µm, including the connection to
the metalization are analyzed. For the circuit simulations, 22 nm
Predictive Technology Model drivers of strength six are employed.
The clock frequency is set to 3 GHz. Considered is the overall power
consumption, including leakage and the drivers. To report values
independent of the TSV "
SOTERIA - exploiting process variations to enhance hardware security with photonic NoC architectures.,Photonic networks-on-chip (PNoCs) enable high bandwidth on-chip data transfers by using photonic waveguides capable of dense-wave-length-division-multiplexing (DWDM) for signal traversal and microring resonators (MRs) for signal modulation. A Hardware Trojan in a PNoC can manipulate the electrical driving circuit of its MRs to cause the MRs to snoop data from the neighboring wavelength channels in a shared photonic waveguide. This introduces a serious security threat. This paper presents a novel framework called SOTERIA† that utilizes process variation based authentication signatures along with architecture-level enhancements to protect data in PNoC architectures from snooping attacks. Evaluation results indicate that our approach can significantly enhance the hardware security in DWDM-based PNoCs with minimal overheads of up to 10.6% in average latency and of up to 13.3% in energy-delay-product (EDP).,"SOTERIA: Exploiting Process Variations to Enhance  
Hardware Security with Photonic NoC Architectures 
Sai Vineel Reddy Chittamuru, Ishan G Thakkar, Varun Bhat, Sudeep Pasricha 
Department of Electrical and Computer Engineering 
Colorado State University, Fort Collins, CO, U.S.A. 
{sai.chittamuru, ishan.thakkar, varun.kilenje_nataraj, sudeep}@colostate.edu 
ABSTRACT 
Photonic networks-on-chip (PNoCs) enable high bandwidth on-chip 
data transfers by using photonic waveguides capable of dense-wavelength-division-multiplexing (DWDM) for signal traversal and microring resonators (MRs) for signal modulation. A Hardware Trojan 
in a PNoC can manipulate the electrical driving circuit of its MRs to 
cause the MRs to snoop data from the neighboring wavelength channels in a shared photonic waveguide. This introduces a serious security threat. This paper presents a novel framework called SOTERIAϯ 
that utilizes process variation based authentication signatures along 
with architecture-level enhancements to protect data in PNoC architectures from snooping attacks. Evaluation results indicate that our 
approach can significantly enhance the hardware security in DWDMbased PNoCs with minimal overheads of up to 10.6% in average latency and of up to 13.3% in energy-delay-product (EDP). 
Categories and Subject Descriptors: [Networks] Network on chip; 
[Security and privacy] Security in hardware; [Hardware] Emerging 
optical and photonic technologies 
General Terms – Security, Performance, Experimentation 
Keywords – Process Variations, Hardware Security, Photonic NoCs 
1. INTRODUCTION 
To cope with the growing performance demands of modern Big 
Data and cloud computing applications, the complexity of hardware 
in modern chip-multiprocessors (CMPs) has increased. To reduce the 
hardware design time of these complex CMPs, third-party hardware 
IPs are frequently used. But these third party IPs can introduce security risks [1]-[2]. For instance, the presence of Hardware Trojans 
(HTs) in the third-party IPs can lead to leakage of critical and sensitive information from modern CMPs [3]. Thus, security researchers 
that have traditionally focused on software-level security are now increasingly interested in overcoming hardware-level security risks. 
Many CMPs today use electrical networks-on-chip (ENoCs) for inter-core communication. ENoCs use packet-switched network fabrics 
and routers to transfer data between on-chip components [4]. Recent 
developments in silicon photonics have enabled the integration of 
photonic components and interconnects with CMOS circuits on a 
chip. Photonic NoCs (PNoCs) provide several prolific advantages 
over their metallic counterparts (i.e., ENoCs), including the ability to 
communicate at near light speed, larger bandwidth density, and lower 
dynamic power dissipation [5]. These advantages motivate the use of 
PNoCs for inter-core communication in modern CMPs [6].  
Several PNoC architectures have been proposed to date (e.g., [7]Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org. 
DAC '18, June 24–29, 2018, San Francisco, CA, USA 
© 2018 Association for Computing Machinery. 
ACM ISBN 978-1-4503-5700-5/18/06…$15.00 
https://doi.org/10.1145/3195970.3196118 
ϯSoteria is the Greek goddess of safety and deliverance from harm 
[9]). These architectures employ on-chip photonic links, each of 
which connects two or more gateway interfaces. A gateway interface 
(GI) connects the PNoC to a cluster of processing cores. Each photonic link comprises one or more photonic waveguides and each 
waveguide can support a large number of dense-wavelength-divisionmultiplexed (DWDM) wavelengths. A wavelength serves as a data 
signal carrier. Typically, multiple data signals are generated at a 
source GI in the electrical domain (as sequences of logical 1 and 0 
voltage levels) which are modulated onto the multiple DWDM carrier 
wavelengths simultaneously, using a bank of modulator MRs at the 
source GI [10]. The data-modulated carrier wavelengths traverse a 
link to a destination GI, where an array of detector MRs filter them 
and drop them on photodetectors to regenerate electrical data signals.  
In general, each GI in a PNoC is able to send and receive data in 
the optical domain on all of the utilized carrier wavelengths. Therefore, each GI has a bank of modulator MRs (i.e., modulator bank) and 
a bank of detector MRs (i.e., detector bank). Each MR in a bank resonates with and operates on a specific carrier wavelength. Thus, the 
excellent wavelength selectivity of MRs and DWDM capability of 
waveguides enable high bandwidth parallel data transfers in PNoCs.  
Similar to CMPs with ENoCs, the CMPs with PNoCs are expected 
to use several third party IPs/, and therefore, are vulnerable to security 
risks [11]. For instance, if the entire PNoC used within a CMP is a 
third-party IP, then this PNoC with HTs within the control units of its 
GIs can snoop on packets in the network. These packets can be transferred to a malicious core (a core running a malicious program) in the 
CMP to determine sensitive information. 
Unfortunately, MRs are especially susceptible to security threatening manipulations from HTs. In particular, the MR tuning circuits that 
are essential for supporting data broadcasts and to counteract MR 
resonance shifts due to process variations (PV) make it easy for HTs 
to retune MRs and initiate snooping attacks. To enable data broadcast 
in PNoCs, the tuning circuits of detector MRs partially detune them 
from their resonance wavelengths [8], [12]-[13], such that a significant portion of the photonic signal energy in the data-carrying wavelengths continues to propagate in the waveguide to be absorbed in the 
subsequent detector MRs. On the other hand, process variations (PV) 
cause resonance wavelength shifts in MRs [14]. Techniques to counteract PV-induced resonance shifts in MRs involve retuning the resonance wavelengths by using carrier injection/depletion or thermal 
tuning [6], implemented through MR tuning circuits. An HT in the GI 
can manipulate these tuning circuits of detector MRs to partially tune 
the detector MR to a passing wavelength in the waveguide, which 
enables snooping of the data that is modulated on the passing wavelength. Such covert data snooping is a serious security risk in PNoCs. 
In this work, we present a framework that protects data from 
snooping attacks and improves hardware security in PNoCs. Our 
framework has low overhead and is easily implementable in any existing DWDM-based PNoC without major changes to the architecture. To the best of our knowledge, this is the first work that attempts 
to improve hardware security for PNoCs. Our novel contributions are:   
 • We analyze security risks in photonic devices and extend this analysis to link-level, to determine the impact of these risks on PNoCs; 
• We propose a circuit-level PV-based security enhancement scheme 
 
 
 
 
that uses PV-based authentication signatures to protect data from 
snooping attacks in photonic waveguides;  
• We propose an architecture-level reservation-assisted security enhancement scheme to improve security in DWDM-based PNoCs; 
• We combine the circuit- and architecture-level schemes into a holistic framework called SOTERIA; and analyze it on the Firefly [8] 
and Flexishare [9] crossbar-based PNoC architectures. 
2. RELATED WORK 
Several prior works [11], [16], [17] discuss the presence of security 
threats in ENoCs and have proposed solutions to mitigate them. In 
[11], a three-layer security system approach was presented by using 
data scrambling, packet certification, and node obfuscation to enable 
protection against data snooping attacks. A symmetric-key based 
cryptography design was presented in [16] for securing the NoC. In 
[17], a framework was presented to use permanent keys and temporary session keys for NoC transfers between secure and non-secure 
cores. However, no prior work has analyzed security risks in photonic 
devices and links; or considered the impact of these risks on PNoCs. 
Fabrication-induced PV impact the cross-section, i.e., width and 
height, of photonic devices, such as MRs and waveguides. In MRs, 
PV causes resonance wavelength drifts, which can be counteracted 
by using device-level techniques such as thermal tuning or localized 
trimming [6]. Trimming can induce blue shifts in the resonance wavelengths of MRs using carrier injection into MRs, whereas thermal 
tuning can induce red shifts in MR resonances through heating of 
MRs using integrated heaters. To remedy PV, the use of device-level 
trimming/tuning techniques is inevitable; but their use also enables 
partial detuning of MRs that can be used to snoop data from a shared 
photonic waveguide. In addition, prior works [18]-[19] discuss the 
impact of PV-remedial techniques on crosstalk noise and proposed 
techniques to mitigate it. None of the prior works analyze the impact 
of PV-remedial techniques on hardware security in PNoCs.     
Our proposed framework in this paper is novel as it enables security against snooping attacks in PNoCs for the first time. Our framework is network agnostic, mitigates PV, and has minimal overhead, 
while improving security for any DWDM-based PNoC architecture. 
Fig. 1(a) shows the malicious operation of a modulator MR. A malicious modulator MR is partially tuned to a data-carrying wavelength 
(shown in purple) that is passing by in the waveguide. The malicious 
modulator MR draws some power from the data-carrying wavelength, which can ultimately lead to data corruption as optical ‘1’s in 
the data can lose significant power to be altered into ‘0’s. Alternatively, a malicious detector (Fig. 1(b)) can be partially tuned to a 
passing data-carrying wavelength, to filter only a small amount of its 
power and drop it on a photodetector for data duplication. This small 
amount of filtered power does not alter the data in the waveguide so 
that it continues to travel to its target detector for legitimate communication [12]. Thus, malicious detector MRs can snoop data from the 
waveguide without altering it, which is a major security threat in photonic links. Note that malicious modulator MRs only corrupt data 
(which can be detected) and do not covertly duplicate it, and are thus 
not a major security risk.  
3.2 Link-Level Security Concerns  
Typically, a photonic link is comprised of one or more DWDMbased photonic waveguides. A DWDM-based photonic waveguide 
uses a modulator bank (a series of modulator MRs) at the source GI 
and a detector bank (a series of detector MRs) at the destination GI. 
DWDM-based waveguides can be broadly classified into four types: 
single-writer-single-reader (SWSR), single-writer-multiple-reader 
(SWMR), multiple-writer-single-reader (MWSR), and multiplewriter-multiple-reader (MWMR). As SWSR, SWMR, and MWSR 
waveguides are subsets of an MWMR waveguide, and due to limited 
space, we restrict our link-level analysis to MWMR waveguides only. 
(a) 
                              (a)                                                   (b) 
 Fig. 1: Impact of (a) malicious modulator MR, (b) malicious detector MR 
on data in DWDM-based photonic waveguides. 
3. HARDWARE SECURITY CONCERNS IN PNOCS 
3.1 Device-Level Security Concerns 
Process variation (PV) induced undesirable changes in MR widths 
and heights cause “shifts” in MR resonance wavelengths, which can 
be remedied using localized trimming and thermal tuning methods. 
The localized trimming method injects (or depletes) free carriers into 
(or from) the Si core of an MR using an electrical tuning circuit, 
which reduces (or increases) the MR’s refractive index owing to the 
electro-optic effect, thereby remedying the PV-induced red (or blue) 
shift in the MR’s resonance wavelength. In contrast, thermal tuning 
employs an integrated micro-heater to adjust the temperature and refractive index of an MR (owing to the thermo-optic effect) for PV 
remedy. Typically, the modulator MRs and detectors use the same 
electro-optic effect (i.e., carrier injection/depletion) implemented 
through the same electrical tuning circuit as used for localized trimming, to move in and out of resonance (i.e., switch ON/OFF) with a 
wavelength [19].  A HT can manipulate this electrical tuning circuit, 
which may lead to malicious operation of modulator and detector 
MRs, as discussed next. 
(b) 
Fig. 2: Impact of (a) malicious modulator (source) bank, (b) malicious detector bank on data in DWDM-based photonic waveguides. 
An MWMR waveguide typically passes through multiple GIs, connecting the modulator banks of some GIs to the detector banks of the 
remaining GIs. Thus, in an MWMR waveguide, multiple GIs (referred to as source GIs) can send data using their modulator banks 
and multiple GIs (referred to as destination GIs) can receive (read) 
data using their detector banks. Fig. 2 presents an example MWMR 
waveguide with two source GIs and two destination GIs. Fig. 2(a) and 
2(b), respectively, present the impact of malicious source and destination GIs on this MWMR waveguide. In Fig. 2(a), the modulator 
bank of source GI S1 is sending data to the detector bank of destination GI D2. When source GI S2, which is in the communication path, 
becomes malicious with an HT in its control logic, it can manipulate 
its modular bank to modify the existing ‘1’s in the data to ‘0’s. This 
ultimately leads to data corruption. For example, in Fig. 2(a), S1 is 
supposed to send ‘0110’ to D2, but because of data corruption by malicious GI S2, ‘0010’ is received by D2. Nevertheless, this type of data 
corruption can be detected or even corrected using parity or error correction code (ECC) bits in the data. Thus, malicious source GIs do 
not cause major security risks in DWDM-based MWMR waveguides. 
Let us consider another scenario for the same data communication 
path (i.e., from S1 to D2). When destination GI D1, which is in the 
communication path, becomes malicious with an HT in its control 
 
 
 
 
 
 
 
 
 
logic, the detector bank of D1 can be partially tuned to the utilized 
wavelength channels to snoop data. In the example shown in Fig. 
2(b), D1 snoops ‘0110’ from the wavelength channels that are destined to D2. The snooped data from D1 can be transferred to a malicious core within the CMP to determine sensitive information. This 
type of snooping attack from malicious destination GIs is hard to detect, as it does not disrupt the intended communication among CMP 
cores. Therefore, there is a pressing need to address the security risks 
imposed by snooping GIs in DWDM-based PNoC architectures. To 
address this need, we propose a novel framework SOTERIA that improves hardware security in DWDM-based PNoC architectures. 
4. SOTERIA FRAMEWORK: OVERVIEW 
Our proposed multi-layer SOTERIA framework enables secure 
communication in DWDM-based PNoC architectures by integrating 
circuit-level and architecture-level enhancements. Fig. 3 gives a highlevel overview of this framework. The PV-based security enhancement (PVSC) scheme uses the PV profile of the destination GIs’ detector MRs to encrypt data before it is transmitted via the photonic 
waveguide. This scheme is sufficient to protect data from snooping 
GIs, if they do not know about the target destination GI. With target 
destination GI information, however, a snooping GI can decipher the 
encrypted data. Many PNoC architectures (e.g., [11], [27]) use the 
same waveguide to transmit both the destination GI information and 
actual data, making them vulnerable to data snooping attacks despite 
using PVSC. To further enhance security for these PNoCs, we devise 
an architecture-level reservation-assisted security enhancement 
(RVSC) scheme that uses a secure reservation waveguide to avoid the 
stealing of destination GI information by snooping GIs. The next two 
sections present details of our PVSC and RVSC schemes. 
Fig. 3: Overview of proposed SOTERIA framework that integrates a circuit-level PV-based security enhancement (PVSC) scheme and an architecture-level reservation-assisted security enhancement (RVSC) scheme. 
5. PV-BASED SECURITY ENHANCEMENT 
As discussed earlier (Section 3.2), malicious destination GIs can 
snoop data from a shared waveguide. One way of addressing this security concern is to use data encryption so that the malicious destination GIs cannot decipher the snooped data. For the encrypted data to 
be truly undecipherable, the encryption key used for data encryption 
should be kept secret from the snooping GIs, which can be challenging as the identity of the snooping GIs in a PNoC is not known. Therefore, it becomes very difficult to decide whether or not to share the 
encryption key with a destination GI (that can be malicious) for data 
decryption. This conundrum can be resolved using a different key for 
every destination GI so that a key that is specific to a secure destination GI does not need to be shared with a malicious destination GI for 
decryption purpose. Moreover, to keep these destination specific keys 
secure, the malicious GIs in a PNoC must not be able to clone the 
algorithm (or method) used to generate these keys.  
To generate unclonable encryption keys, our PV-based security 
(PVSC) scheme uses the PV profiles of the destination GIs’ detector 
MRs. As discussed in [14], PV induces random shifts in the resonance 
wavelengths of the MRs used in a PNoC. These resonance shifts can 
be in the range from -3nm to 3nm [14]. The MRs that belong to different GIs in a PNoC have different PV profiles. In fact, the MRs that 
belong to different MR banks of the same GI also have different PV 
profiles. Due to their random nature, these MR PV profiles cannot be 
cloned by the malicious GIs, which makes the encryption keys generated using these PV profiles truly unclonable. Using the PV profiles 
of detector MRs, PVSC can generate a unique encryption key for each 
detector bank of every MWMR waveguide in a PNoC. 
Our PVSC scheme generates encryption keys during the testing 
phase of the CMP chip, by using a dithering signal based in-situ 
method [15] to generate an anti-symmetric analog error signal for 
each detector MR of every detector bank that is proportional to the 
PV-induced resonance shift in the detector MR. Then, it converts the 
analog error signal into a 64-bit digital signal. Thus, a 64-bit digital 
error signal is generated for every detector MR of each detector bank. 
We consider 64 DWDM wavelengths per waveguide, and hence, we 
have 64 detector MRs in every detector bank and 64 modulator MRs 
in every modulator bank. For each detector bank, our PVSC scheme 
XORs the 64 digital error signals (of 64 bits each) from each of the 
64 detector MRs to create a unique 64-bit encryption key. Note that 
our PVSC scheme also uses the same anti-symmetric error signals to 
control the carrier injection and heating of the MRs to remedy the PVinduced shifts in their resonances. 
To understand how the 64-bit encryption key is utilized to encrypt 
data in photonic links, consider Fig. 4 which depicts an example photonic link that has one MWMR waveguide and connects the modulator banks of two source GIs (S1 and S2) with the detector banks of two 
destination GIs (D1 and D2). As there are two destination GIs on this 
link, PVSC creates two 64-bit encryption keys corresponding to them, 
and stores them at the source GIs. When data is to be transmitted by 
a source GI, the key for the appropriate destination is used to encrypt 
data at the flit-level granularity, by performing an XOR between the 
key and the data flit. This requires that the size of an encryption key 
match the data flit size. We consider the size of data flits to be 512 
bits. Therefore, the 64-bit encryption key is appended eight times to 
generate a 512-bit encryption key. In Fig. 4, every source GI stores 
two 512-bit encryption keys (for destination GIs D1 and D2) in its 
local ROM, whereas every destination GI stores only its corresponding 512-bit key in its ROM. Note that we store the 512-bit keys instead of the 64-bit keys as this eliminates the latency overhead of affixing 64-bit keys to generate 512-bit keys, at the cost of a reasonable 
area/energy overhead in the ROM. As an example, if S1 wants to send 
a data flit to D2, then S1 first accesses the 512-bit encryption key corresponding to D2 from its local ROM and XORs the data flit with this 
key in one cycle, and then transmits the encrypted data flit over the 
link. As the link employs only one waveguide with 64 DWDM wavelengths, therefore, the encrypted 512-bit data flit is transferred on the 
link to D2 in eight cycles. At D2, the data flit is decrypted by XORing 
it with the 512-bit key corresponding to D2 from the local ROM. In 
this scheme, even if D1 snoops the data intended for D2, it cannot decipher the data as it does not have access to the correct key (corresponding to D2) for decryption. Thus, our PVSC encryption scheme 
protects data against snooping attacks in DWDM-based PNoCs. 
Fig. 4: Overview of proposed PV-based security enhancement scheme 
Limitations of PVSC: The PVSC scheme can protect data from 
being deciphered by a snooping GI, if the following two conditions 
about the underlying PNoC architecture hold true: (i) the snooping GI 
does not know the target destination GI for the snooped data, (ii) the 
snooping GI cannot access the encryption key corresponding to the 
target destination GI. As discussed earlier, an encryption key is stored 
only at all source GIs and at the corresponding destination GI, which 
makes it physically inaccessible to a snooping destination GI. 
 
 
 
 
 
 
However, if more than one GIs in a PNoC are compromised due to 
HTs in their control units and if these HTs launch a coordinated 
snooping attack, then it may be possible for the snooping GI to access 
the encryption key corresponding to the target destination GI.  
For instance, consider the photonic link in Fig. 4. If both S1 and D1 
are compromised, then the HT in S1’s control unit can access the 
encryption keys corresponding to both D1 and D2 from its ROM and 
transfer them to a malicious core (a core running a malicious 
program). Moreover, the HT in D1’s control unit can snoop the data 
intended for D2 and transfer it to the malicious core. Thus, the 
malicious core may have access to the snooped data as well as the 
encryption keys stored at the source GIs. Nevertheless, accessing the 
encryption keys stored at the source GIs is not sufficient for the 
malicious GI (or core) to decipher the snooped data. This is because 
the compromised ROM typically has multiple encryption keys 
corresponding to multiple destination GIs, and choosing a correct key 
that can decipher data requires the knowledge of the target destination 
GI. Thus, our PVSC encryption scheme can secure data 
communication in PNoCs as long as the malicious GIs (or cores) do 
not know the target destinations of the snooped data.  
Unfortunately, many PNoC architectures, e.g., [11], [27], that 
employ photonic links with multiple destination GIs utilize the same 
waveguide to transmit both the target destination information and 
actual data. In these PNoCs, if a malicious GI manages to tap the 
target destination information from the shared waveguide, then it can 
access the correct encryption key from the compromised ROM to 
decipher the snooped data. Thus, there is a need to conceal the target 
destination information from malicious GIs (cores). This motivates 
us to propose an architecture-level solution, as discussed next. 
(a) 
(b) 
Fig. 5: Reservation-assisted data transmission in DWDM-based photonic 
waveguides (a) without RVSC, (b) with RVSC. 
6. RESERVATION-ASSISTED SECURITY ENHANCEMENT 
In PNoCs that use photonic links with multiple destination GIs, 
data is typically transferred in two time-division-multiplexed (TDM) 
slots called reservation slot and data slot [11], [27]. To minimize pho5(a), (cid:2755)1 and (cid:2755)2 are the reservation selection wavelengths correspondtonic hardware, PNoCs use the same waveguide to transfer both slots, 
as shown in Fig. 5(a). To enable reservation of the waveguide, each 
destination is assigned a reservation selection wavelength. In Fig. 
ing to destination GIs D1 and D2, respectively. Ideally, when a destination GI detects its reservation selection wavelength in the reservation slot, it switches ON its detector bank to receive data in the next 
data slot. But in the presence of an HT, a malicious GI can snoop 
using one of its detectors to snoop (cid:2755)2 from the reservation slot. By 
signals from the reservation slot using the same detector bank that is 
used for data reception. For example, in Fig. 5(a), malicious GI D1 is 
snooping λ2, D1 can identify that the data it will snoop in the subsequent data slot will be intended for destination D2. Thus, D1 can now 
choose the correct encryption key from the compromised ROM to 
decipher its snooped data. 
To address this security risk, we propose an architecture-level reservation-assisted security enhancement (RVSC) scheme. In RVSC, we 
add a reservation waveguide, whose main function is to carry reservation slots, whereas the data waveguide carries data slots. We use 
double MRs to switch the signals of reservation slots from the data 
waveguide to the reservation waveguide, as shown in Fig. 5(b). Double MRs are used instead of single MRs for switching to ensure that 
the switched signals do not reverse their propagation direction after 
switching [29]. Compared to single MRs, double MRs also have 
lower signal loss due to steeper roll-off of their filter responses [29].  
The double MRs are switched ON only when the photonic link is 
in a reservation slot, otherwise they are switched OFF to let the signals of the data slot pass by in the data waveguide. Furthermore, in 
ing to their reservation selection wavelengths (cid:2755)1 and (cid:2755)2, respectively, 
RVSC, each destination GI has only one detector on the reservation 
waveguide, which corresponds to its receiver selection wavelength. 
GI D1 to snoop (cid:2755)2 from the reservation slot as shown in Fig. 5(b), as 
D1 does not have a detector corresponding to (cid:2755)2 on the reservation 
For example, in Fig. 5(b), D1 and D2 will have detectors correspondon the reservation waveguide. This makes it difficult for the malicious 
waveguide. However, the HT in D1’s control unit may still attempt to 
snoop other reservation wavelengths (e.g., λ2) in the reservation slot 
by retuning D1’s λ1 detector. But succeeding in these attempts would 
require the HT to perfect the timing and target wavelength of its 
snooping attack, which is very difficult due to the large number of 
utilized reservation wavelengths. Thus, D1 cannot identify the correct 
encryption key to decipher the snooped data. In summary, RVSC enhances security in PNoCs by protecting data from snooping attacks, 
even if the encryption keys used to secure data are compromised.  
7. IMPLEMENTING SOTERIA FRAMEWORK ON PNOCS 
We characterize the impact of SOTERIA on two popular PNoC architectures: Firefly [8] and Flexishare [9], both of which use DWDMbased photonic waveguides for data communication. We consider 
Firefly PNoC with 8×8 SWMR crossbar [8] and a Flexishare PNoC 
with 32×32 MWMR crossbar [9] with 2-pass token stream arbitration. We adapt the analytical equations from [29] to model the signal 
power loss and required laser power in the SOTERIA-enhanced Firefly and Flexishare PNoCs. At each source and destination GI of the 
SOTERIA-enhanced Firefly and Flexishare PNoCs, XOR gates are 
required to enable parallel encryption and decryption of 512-bit data 
flits. We consider a 1 cycle delay overhead for encryption and decryption of every data flit. The overall laser power and delay overheads for both PNoCs are quantified in the results section. 
Firefly PNoC: Firefly PNoC [8], for a 256-core system, has 8 clusters 
(C1-C8) with 32 cores in each cluster. Firefly uses reservation-assisted SWMR data channels in its 8x8 crossbar for inter-cluster communication. Each data channel consists of 8 SWMR waveguides, with 
64 DWDM wavelengths in each waveguide. To integrate SOTERIA 
with Firefly PNoC, we added a reservation waveguide to every 
SWMR channel. This reservation waveguide has 7 detector MRs to 
detect reservation selection wavelengths corresponding to 7 destination GIs. Furthermore, 64 double MRs (corresponding to 64 DWDM 
wavelengths) are used at each reservation waveguide to implement 
RVSC. To enable PVSC, each source GI has a ROM with seven entries of 512 bits each to store seven 512-bit encryption keys corresponding to seven destination GIs. In addition, each destination GI 
requires a 512-bit ROM to store its own encryption key.     
Flexishare PNoC: We also integrate SOTERIA with the Flexishare 
PNoC architecture [9] with 256 cores. We considered a 64-radix 64cluster Flexishare PNoC with four cores in each cluster and 32 data 
channels for inter-cluster communication. Each data channel has four 
 
 
 
 
 
 
 
MWMR waveguides with each having 64 DWDM wavelengths. In 
SOTERIA-enhanced Flexishare, we added a reservation waveguide to 
each MWMR channel. Each reservation waveguide has 16 detector 
MRs to detect reservation selection wavelengths corresponding to 16 
destination GIs. To enable PVSC, each source GI requires a ROM 
with 16 entries of 512 bits each to store the encryption keys, whereas 
each destination GI requires a 512-bit ROM.  
8. EVALUATIONS 
8.1. Evaluation Setup 
To evaluate our proposed SOTERIA (PVSC+RVSC) security enhancement framework for DWDM-based PNoCs, we integrate it with 
the Firefly [8] and Flexishare [9] PNoCs, as explained in Section 7. 
We modeled and performed simulation based analysis of the SOTERIA-enhanced Firefly and Flexishare PNoCs using a cycle-accurate SystemC based NoC simulator, for a 256-core single-chip architecture at 22nm. We validated the simulator in terms of power dissipation and energy consumption based on results obtained from the 
DSENT tool [22]. We used real-world traffic from the PARSEC 
benchmark suite [23]. GEM5 full-system simulation [24] of parallelized PARSEC applications was used to generate traces that were fed 
into our NoC simulator. We set a “warmup” period of 100 million 
instructions and then captured traces for the subsequent 1 billion instructions. These traces are extracted from parallel regions of execution of PARSEC applications. We performed geometric calculations 
for a 20mm×20mm chip size, to determine lengths of SWMR and 
MWMR waveguides in Firefly and Flexishare. Based on this analysis, we estimated the time needed for light to travel from the first to 
the last node as 8 cycles at 5 GHz clock frequency [13]. We use a 
512-bit packet size, as advocated in the Firefly and Flexishare PNoCs. 
Similar to [29], we adapt the VARIUS tool [20] to model random and 
systematic die-to-die (D2D) as well as within-die (WID) process variations in MRs for the Firefly and Flexishare PNoCs. 
The static and dynamic energy consumption values for electrical 
routers and concentrators in Firefly and Flexishare PNoCs are based 
on results from DSENT [22]. We model and consider the area, power, 
and performance overheads for our framework implemented with the 
Firefly and Flexishare PNoCs as follows. SOTERIA with Firefly and 
Flexishare PNoCs has an electrical area overhead of 12.7mm2 and 
3.4mm2, respectively, and power overhead of 0.44W and 0.36W, respectively, using gate-level analysis and CACTI 6.5 [25] tool for 
memory and buffers. The photonic area of Firefly and Flexishare 
PNoCs is 19.83mm2 and 5.2mm2, respectively, based on the physical 
dimensions [21] of their waveguides, MRs, and splitters. For energy 
consumption of photonic devices, we adapt model parameters from 
recent work [26], [28] with 0.42pJ/bit for every modulation and detection event and 0.18pJ/bit for the tuning circuits of modulators and 
photodetectors. The MR trimming power is 130μW/nm [30] for current injection and tuning power is 240μW/nm [30] for heating. 
8.2. Overhead Analysis of SOTERIA on PNoCs 
Our first set of experiment"
LEAD - learning-enabled energy-aware dynamic voltage/frequency scaling in NoCs.,"Network on Chips (NoCs) are the interconnect fabric of choice for multicore processors due to their superiority over traditional buses and crossbars in terms of scalability. While NoC's offer several advantages, they still suffer from high static and dynamic power consumption. Dynamic Voltage and Frequency Scaling (DVFS) is a popular technique that allows dynamic energy to be saved, but it can potentially lead to loss in throughput. In this paper, we propose LEAD - Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for NoC architectures wherein we use machine learning techniques to enable energy-performance trade-offs at reduced overhead cost. LEAD enables a proactive energy management strategy that relies on an offline trained regression model and provides a wide variety of voltage/frequency pairs (modes). LEAD groups each router and the router's outgoing links locally into the same V/F domain, allowing energy management at a finer granularity without additional timing complications and overhead. Our simulation results using PARSEC and Splash-2 benchmarks on a 4 × 4 concentrated mesh architecture show an average dynamic energy savings of 17% with a minimal loss of 4% in throughput and no latency increase.","LEAD: Learning-enabled Energy-Aware Dynamic
Voltage/frequency scaling in NoCs
Mark Clark
Ohio University
Athens, Ohio
mc591611@ohio.edu
Razvan Bunescu
Ohio University
Athens, Ohio
bunescu@ohio.edu
Avinash Kodi
Ohio University
Athens, Ohio
kodi@ohio.edu
Ahmed Louri
George Washington University
Washington, D.C.
louri@email.gwu.edu
ABSTRACT
Network on Chips (NoCs) are the interconnect fabric of choice for
multicore processors due to their superiority over traditional buses
and crossbars in terms of scalability. While NoC’s offer several
advantages, they still suffer from high static and dynamic power
consumption. Dynamic Voltage and Frequency Scaling (DVFS) is a
popular technique that allows dynamic energy to be saved, but
it can potentially lead to loss in throughput. In this paper, we
propose LEAD - Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for NoC architectures wherein we use machine learning techniques to enable energy-performance trade-offs
at reduced overhead cost. LEAD enables a proactive energy management strategy that relies on an offline trained regression model
and provides a wide variety of voltage/frequency pairs (modes).
LEAD groups each router and the router’s outgoing links locally
into the same V/F domain, allowing energy management at a finer
granularity without additional timing complications and overhead.
Our simulation results using PARSEC and Splash-2 benchmarks on
a 4 × 4 concentrated mesh architecture show an average dynamic
energy savings of 17% with a minimal loss of 4% in throughput and
no latency increase.
ACM Format:
Mark Clark, Avinash Kodi, Razvan Bunescu, and Ahmed Louri. 2018. LEAD:
Learning-enabled Energy-Aware Dynamic Voltage/frequency scaling in
NoCs. In DAC ’18: DAC ’18: The 55th Annual Design Automation Conference
2018, June 24–29, 2018, San Francisco, CA, USA. ACM, New York, NY, USA,
6 pages. https://doi.org/10.1145/3195970.3196068
1 INTRODUCTION
As technology scales further into the sub-nanometer region, an
increasing number of transistors are packed onto chips. Single-chip
processors already contain more than 7.2 billion transistors (Intel
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and /or a
fee. Request permissions from permissions@acm.org.
DAC ’18, June 24–29, 2018, San Francisco, CA, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5700-5/18/06. . . $15.00
https://doi.org/10.1145/3195970.3196068
Broadwell-EP Xeon), and with this astronomical number of transistors comes several unique power challenges. Prior research has
focused on reducing the excessive dynamic energy consumption
resulting from storing and switching data within routers and links
using Dynamic Voltage and Frequency Scaling (DVFS). As technology continues to scale down in size, static power consumption
continues to grow, already accounting for a significant portion of
the total power consumption of the chip. Power-gating [1] is a
useful technique that seeks to save static power, however wakeup
delay and break even time remain critical challenges.
The main goal when applying any DVFS strategy is the reduction
of dynamic energy consumption at runtime [2], [3], [4], [5]. Because
static power is not related to clock frequency, it is rarely considered,
even though multi supply voltage designs assume that any increase
or decrease in clock frequency is caused by a subsequent increase
or decrease in supply voltage. This is because the supply voltage
should be increased at times of high network traffic in order to
increase throughput, and the supply voltage should be decreased at
times of low network traffic in order to save dynamic energy. Various metrics have been used to measure network traffic such as the
round-trip time (RTT) [5], Voltage Frequency Island (VFI) utilization [6], network slack [7], and buffer utilization [3]. The nominal
supply voltage required for operation is hardware dependent, but
the supply voltage and frequency always increase/decrease proportionally.
When dealing with any DVFS scheme, it is often the case that
the logic behind when to increase or decrease the supply voltage
becomes the most crucial part of the design (mode selection model).
Recently some designs have begun to apply machine learning (ML)
to their DVFS scheme in order to control the mode selection logic,
thus determining when to switch modes proactively rather than
older data-dependent reactive schemes [8], [9], [10], [11], [12], [13].
These approaches often apply online learning because it allows
the algorithm to learn as data becomes available instead of learning from a static data set. However, online learning is expensive
in terms of overhead cost as well as being inaccurate until after
several iterations of learning.
In this paper we propose Learning-enabled Energy-Aware Dynamic
voltage/frequency scaling (LEAD), a collection of linear regression
based DVFS techniques that are all trained offline. All LEAD models
are proactive. They use only local router information when calculating the label and are trained offline in order to maximize the
e n e r дy
reduction in overhead associated with traditional machine learning
approaches. LEAD also scales the router and the router’s outgoing
links simultaneously in order to avoid inefficient use of network
bandwidth or excess energy consumption. In order to achieve optimal energy-performance trade-offs, LEAD predicts different network metrics specific to the model. LEAD-τ predicts future buffer
utilization, LEAD-∇ predicts the change in buffer utilization between the current and future epoch, and LEAD-G predicts change
in
t h r ou дhpu t 2 between the current and future epoch. Based on
these predicted values, our proactive ML techniques are used to select the appropriate mode on a per router basis without the need for
global coordination, thereby reducing the overhead and complexity.
Machine learning uses linear regression algorithms that minimize
the error of a model and make the most accurate prediction possible [14], [15], [16]. For a 4 × 4 concentrated mesh architecture,
our simulation results show that LEAD-τ achieves an average of
17% savings in total dynamic energy with minimal loss of 2-4% in
throughput for real applications.
2 RELATED WORK
There has been a significant amount of work in applying DVFS
schemes to various on-chip components including the processor,
caches, memory as well as the NoC. DVFS has also been applied
at different levels of granularity ranging from very fine grained to
very coarse grained. The trade-off between operating at a coarsegrain or a fine-grain comes in terms of system complexity and
maximum amount of energy savings [7]. If the links operate at a
much lower frequency than the router, packets can queue up and
the network can saturate quickly. If the links operate at a much
higher frequency than the router, then unnecessary dynamic energy will be consumed when little work is being done. Prior works
have used various parameters to measure network traffic such as
round-trip time (RTT) [5], VFI utilization [6], network slack [7],
buffer utilization [3], or cache-coherence properties [17]. There has
also been research focused on using different reactive DVFS mode
selection models such as a threshold-based model, a proportionalintegral (PI) based model, and a greedy model [6].
Recently, DVFS and machine learning have been combined such
that the resulting regression based learning algorithm can automatically learn a DVFS scheme that maximizes dynamic energy
savings within an allowable amount of performance degradation.
While one approach has applied regression based learning to a
heterogeneous embedded system [8], another approach has applied
online reinforcement learning techniques [9]. One recent work has
applied online learning to multi-tasking systems using only three
entities: a controller, an expert, and the CPU [10]. It has also been
shown that low-overhead reinforcement learning can be applied to
multi-processor systems to achieve a required balance in temperature, performance, and power [13].
While prior work has applied reinforcement learning as well as
regression models to the processor or shared computing resources,
we specifically apply regression to NoCs to optimize dynamic energy and performance. Combined with offline learning, our proposed LEAD models greatly reduce overhead and implementation
cost. Applying DVFS to the router and the router’s outgoing links
locally guarantees that our design has the necessary link bandwidth
Figure 1: Topology: We apply LEAD to a concentrated mesh
with 16 routers and 64 cores. We use on chip voltage regulators that can adjust the supply voltage between 0.8V and
1.2V, allowing us to apply DVFS to individual routers and
their corresponding links.
at times of high network traffic, while still saving dynamic energy
when lower link bandwidth is sufficient.
3 LEAD TOPOLOGY
In this section, we will discuss the proposed LEAD topology, NoC
microarchitecture, machine learning techniques and DVFS implementation.
LEAD Layout: LEAD is built on a concentrated mesh topology
using on-chip voltage regulators that allow the selection of multiple
voltage modes as shown in Figure 1. Our network consists of 16
routers, 64 cores, and 48 unidirectional links. We propose per router
DVFS such that the router as well as the outgoing links are scaled
simultaneously to operate at the same mode of operation. Each
router consists of 8 input ports, 8 output ports, and 4 virtual channels per port. Each processor has an individual L1 cache and each
router has an L2 cache shared among the four cores connected to
each router. When a packet is first generated, the packet is stored in
the input buffer. The output port is computed using XY dimensionorder routing (DOR) in the route computation (RC) stage of the
router pipeline. After a virtual channel is allocated, the head packet
competes for the output channel in the switch allocation (SA) stage.
After successfully competing and being awarded the channel, the
packet is sent across the crossbar to the destination port in the
switch traversal (ST) stage. The proposed router microarchitecture
is shown in Figure 2(a).
Operating V/F Modes: Our models use five modes of operation
with voltage and frequency levels similar to those proposed in
previous work [17]. The supply voltage changes in 100 mV steps
with proportional changes in clock frequency. The V/F pairs our
models use include {0.8 V/1 GHz, 0.9 V/1.5 GHz, 1.0 V/1.8 GHz, 1.1
V/2 GHz and 1.2 V/2.25 GHz} which correspond to modes 1-5 as
shown in Figure 2(b). We carefully chose five modes because using too many modes leads to increased voltage regulator overhead,
whereas too few modes will not allow for dynamic energy reduction.
e n e r дy
into adjacent modes. This model emphasizes the importance of being able to select the optimal mode at any given epoch versus other
designs which are constrained to only being allowed to transition
into adjacent modes.
LEAD-∇: LEAD-∇ starts each router in the highest mode. Every
epoch routers transition one mode up/down based on the predicted
change in input buffer utilization between the previous and current
epochs. Mode transitions only occur if this predicted change in
buffer utilization falls within certain criteria. It must be carefully
chosen so that small variations in network traffic do not govern
mode selection, but also such that the router can adequately adapt to
changes in network traffic patterns. The buffers must be predicted
to increase by at least 6-10% of their maximum utilization in order
to warrant a mode transition into a higher mode, and they must be
predicted to decrease by at least 3-5% of their maximum utilization
in order to warrant a mode transition into a lower mode. We ensure
dynamic energy savings by requiring the predicted change in buffer
utilization required to move down a voltage level be less than the
change required to move up. This model is used to compare and
contrast the trade-offs associated with being able to transition only
into adjacent modes at every epoch, but we still assume that each
transition takes one cycle. This model is more suited to gradual
traffic changes where adjacent mode transitions are optimal.
LEAD-G: LEAD-G [6], [18] explores to find the mode that minimizes a predicted future
t h r ou дhpu t 2 . This model adds explorative
logic and introduces both dynamic energy and throughput into the
model in the hopes of better balancing the trade-off between the
two. LEAD-G starts each router in the highest mode and in a downwards explorative direction. If the predicted change in
between the current and future epoch is less than or equal to 0,
then the router will move one mode further in the current exploration direction (downward/upward). If the predicted change in
t h r ou дhpu t 2 between the current and future epoch is greater than
0, then the router is put into a hold phase. The hold phase lasts 2
epochs, and during the holdphase the router can not change modes.
After the hold phase expires, the exploration direction is flipped
and the model begins to explore in the opposite direction until the
predicted
t h r ou дhpu t 2 is greater than 0 again. This model seeks
to minimize the predicted
t h r ou дhpu t 2 and assumes that routers
may only transition into adjacent modes. The logic behind all three
LEAD models is further explained in Figure 3.
DVFS Implementation: As shown in Figure 2(a), LEAD uses four
components per router in order to perform reactive (non-ML) or
proactive (ML) model selection. The first component is called Feature Extract. It gathers the router/link features and supplies it to the
Label unit for proactive models. The next component is the NonML Model, which takes the label supplied from Feature Extract
and selects the appropriate mode for the router and the router’s
outgoing links (Greedy model). For proactive model selection, we
require the addition of two new units. The first is called the Label.
This component takes the features supplied by Feature Extract and
applies Ridge Regression in order to generate a corresponding label.
This label is then supplied to the ML Model unit in order to select
the appropriate mode for the router and the outgoing links.
e n e r дy
t h r ou дhpu t 2
e n e r дy
e n e r дy
e n e r дy
Figure 2: (a) Router Microarchitecture: The architecture as
well as additional units required for reactive or proactive
mode selection. Reactive model selection requires two units,
Feature Extract and Non-ML Model. Proactive model selection requires three units, Feature Extract, Label, and ML
Model. (b) VR Scheme: The on-chip voltage regulator setup
that allows the selection of voltage levels in the range of 0.8V
to 1.2V for every router and its’ associated outgoing links.
Since power-gating has several challenges (deadlocks, breakeven
time, loss in throughput), we have not considered implementing a
power-gated version of LEAD, leaving this for future work.
3.1 DVFS Models
In this work we focus on measuring the impact of different mode
selection models on dynamic energy savings and performance. We
propose three machine learning based models; LEAD-τ , LEAD-∇,
and LEAD-G. LEAD-G is based on an already proposed reactive
model called Greedy. This Greedy model is presented in [6] as an
adaptation of a Greedy search method presented in earlier work
[18]. Greedy and LEAD-G are used strictly for comparative purposes.
Baseline: The baseline model always operates all routers in mode 5
(highest V/F) and does not apply DVFS. This model has the highest
throughput and lowest latency, but has no dynamic energy savings.
LEAD-τ : LEAD-τ starts each router in the lowest mode. It chooses
the routers’ operation mode for the next epoch based on the predicted input buffer utilization of the router. If the router’s buffers
are predicted to be less than 5% full, then the lowest mode is chosen.
If the buffers are predicted to be between 5% and 10% full, then
the router will operate in mode 2. If the buffers are predicted to
be between 10% and 20% full, then the third mode is chosen. If
the buffers are predicted to be between 20% and 25% full, then the
router will operate in the fourth mode. Finally, if the buffers are
predicted to be greater than 25% full, then the router will operate in
the highest mode. For larger epoch sizes the thresholds are reduced
for more aggressive scaling. For example, at epoch size of 500, the
thresholds are reduced to 1%, 2%, 4%, and 5% respectively because
of how the maximum utilization is calculated as a worst case time
variant sum. For simplicity sake we will show the thresholds as if
they are all for epoch size of 100 cycles. This mode selection model
assumes a voltage regulator scheme that allows the transition from
any mode to any mode in one cycle without the need to transition
Figure 3: Mode Selection Models: LEAD-τ uses a predicted input buffer utilization to select the optimal mode per epoch. LEAD∇ uses a predicted change in input buffer utilization to move in the direction of the optimal mode per epoch. LEAD-G uses
a predicted change in
t h r ou дhpu t 2 to move up/down adjacent modes based on exploration direction such that
minimized.
t h r ou дhpu t 2 is
e n e r дy
e n e r дy
3.2 Machine Learning Algorithm
N
M
We use machine learning to train different Ridge Regression algorithms corresponding to LEAD-τ , LEAD-∇, and LEAD-G. There
are two arrays of values needed when using regression, the first
being the feature set and the second being the weight vector. Each
feature has a corresponding weight, a scalar factor representing
that particular features impact on predicting the output. The Ridge
Regression equation is shown below:
Ridge Regression:
E (w ) = 1
2
n=1 {y (xn , w ) − tn }2 + λ
2
j =1 w 2
j
2
j =1 w 2
M
In the Ridge Regression equation listed above, we minimize the
sum of squared errors between our predicted label y (xn , w ) and the
actual label tn . The feature results xn as well as the label tn are used
to train the system offline such that a corresponding weight vector
w is created. The weight vector is normalized during training by
adding L2 regularization. During tuning, different values of λ are
tried for the equation λ
j until the best fitting solution for
the training data is found. We used a total of 14 different trace files;
6 for training, 3 for validation, and 5 for testing.
Feature Set: The feature set is directly related to prediction accuracy and overhead cost. The feature set must be kept as small as
possible because every new feature leads to an increase in arithmetic
overhead. Our feature set is composed of 39 network parameters
(buffer utilization, incoming/outgoing link utilization per direction,
request/response packets, etc) as well as a label local to each of the
16 routers.
Label: A reactive version of each LEAD model is ran for each of
the 6 training trace files. This is done so that corresponding feature
results and labels can be extracted and supplied for training each
LEAD model. While the same features are used to train all LEAD
models, the label supplied for training is unique for each LEAD
model. The label for LEAD-τ is the future input buffer utilization of
the router for the next epoch. The label for LEAD-∇ is the difference
between the routers’ current and future buffer utilization. The
label for LEAD-G is the difference between the routers’ current
and future
t h r ou дhpu t 2 . These labels are supplied along with the
e n e r дy
corresponding feature results in order to train the ML algorithm
offline.
ML Overhead: A trained linear regression algorithm uses a series
of additions and multiplies to calculate a label, thus the overhead
cost can be simplified to the timing, power, and area cost required
to execute a set number of additions and multiplies. The energy
cost of a single 16 bit floating point add is estimated to be 0.4 pJ
and the area cost is 1360 um2 [19]. The energy cost of a multiply is
estimated to be 1.1 pJ and the area cost is 1640 um2 [19]. The total
energy overhead cost is 58.1 pJ (considering two-stage multiplies
followed by an addition), the total area overhead cost is 0.12 mm2 ,
and the total timing cost is 3-4 cycles. We use epoch sizes of 500
cycles and 1000 cycles, reducing overhead cost.
4 PERFORMANCE EVALUATION
Simulation Setup: In order to train our Ridge Regression model,
we first begin by gathering features and labels from a cycle accurate
simulator running a logically similar model with real traffic patterns.
The first step in achieving this is to gather real network traffic trace
files using Multi2sim. Multi2sim [20] is a full system simulator that
uses benchmarks from PARSEC 2.1 [21] and SPLASH2 [22] in order
to generate cycle accurate traces. These traces are used as input
for our in-house network simulator such that feature results and
labels can be extracted. We use the output of six trace files which
include feature values and target labels in order to train our Ridge
Regression algorithm in Matlab. The algorithm is then validated
on three different trace files. This process is repeated for all three
different LEAD models such that each has a uniquely trained Ridge
Regression algorithm. It is important that we tune the lambda hyperparameter as it controls the amount of L2 regularization used to
combat over-fitting, resulting in reduced model complexity. After
the models are trained and tuned, they are exported for testing back
in our network simulator where they predict the various target
labels. We use five untouched trace files as input during testing to
measure the performance of our trained Ridge Regression models.
These five traces are not used during testing or validation so that
Table 1: LEAD-τ mode selection accuracy.
Models
LEAD-τ 500
LEAD-τ 1000
Lu
Ls
Radix
Fluid
Canneal
88.3% 82.3% 62.7% 68%
95.9%
83.2% 73.2% 56.6% 53.2% 95.9%
but overall it did perform better than LEAD-∇ and was able to
achieve an even trade-off between dynamic energy savings and
loss in throughput. In a network with high amounts of contention,
LEAD-τ would be the optimal mode selection model. However, if
the network rarely became congested, then LEAD-G would be the
best option. Figure 6 shows the amount of time spent in different
modes across various applications. LEAD-τ shows that the ability
to switch from any-to-any mode allows it to avoid mode 4 altogether. This ensures that LEAD-τ minimizes loss in throughput
while maximizing energy savings. Both LEAD-∇ and LEAD-G show
significant amounts of time spent in the lower modes 2 and 1 respectively. Therefore, both LEAD-∇ and LEAD-G tend to minimize
energy at the cost of throughput. Table 1 shows the achieved mode
selection accuracy for various applications for LEAD-τ model. Here,
we measure the buffer utilization at the end of window and determine what mode should have been chosen. We compare this to
the actual mode to evaluate if the prediction was accurate for that
reconfiguration window. We achieve an average of 86% accuracy
across all benchmarks with the canneal application showing almost
95% accuracy.
5 CONCLUSIONS
We have shown how smart proactive mode selection techniques
such as LEAD-τ can lead to substantial dynamic energy savings, at
times with minimal loss in performance. LEAD-∇ highlights how
we can have an unequal trade-off between dynamic energy savings
and performance, showcasing the opposite side of the spectrum;
how a poorly tuned or otherwise underwhelming mode selection
model can lead to sub-optimal dynamic energy savings/performance
trade-off. LEAD-G shows a mode selection model that can get very
good energy savings for an even throughput trade-off in saturated
networks.
6 ACKNOWLEDGEMENT
This research was partially supported by NSF grants CCF-1054339
(CAREER), CCF-1420718, CCF-1318981, CCF-1513606, CCF-1703013,
CCF-1547034, CCF-1547035, CCF-1540736, and CCF-1702980. We
thank the anonymous reviewers for their excellent feedback.
"
PlanarONoC - concurrent placement and routing considering crossing minimization for optical networks-on-chip.,"Optical networks-on-chips (ONoCs) have become a promising solution for the on-chip communication of multi-and many-core systems to provide superior communication bandwidths, efficiency in power consumption, and latency performance compared to electronic NoCs. Serving as the critical part of ONoCs, an optical router composed of waveguides and photonic switching elements (PSEs) routes signals between two hubs or between a hub and a memory controller. Many studies focus on developing efficient architectures of optical routers, while their physical implementation that can seriously deteriorate the quality of the architectures is rarely addressed. The existing automatic place-and-route tools suffer from considerable insertion loss due to many waveguide crossings outside of PSEs, which leads to huge power consumption of laser sources. By observing that the logic schemes of most optical routers are actually planar, we develop a concurrent PSE placement and waveguide routing flow, called PlanarONoC, that guarantees optimal solutions in terms of crossings for planar logic schemes. Experimental results show that the proposed flow reduces the maximum insertion loss by 37% on average, guarantees no waveguide crossing outside of PSEs, and performs much more efficient compared to the state-of-the-art work.","PlanarONoC: Concurrent Placement and Routing Considering Crossing
Minimization for Optical Networks-on-Chip∗
Yu-Kai Chuang1 , Kuan-Jung Chen1 , Kun-Lin Lin1 , Shao-Yun Fang1 , Bing Li2 , and Ulf Schlichtmann2
1Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei 106, Taiwan
2Chair of Electronic Design Automation, Technical University of Munich, Germany
{m10507410, m10407412, m10307402, syfang}@mail.ntust.edu.tw; {b.li, ulf.schlichtmann}@tum.de
ABSTRACT
Optical networks-on-chips (ONoCs) have become a promising solution
for the on-chip communication of multi-and many-core systems to provide superior communication bandwidths, efficiency in power consumption, and latency performance compared to electronic NoCs. Serving as
the critical part of ONoCs, an optical router composed of waveguides
and photonic switching elements (PSEs) routes signals between two
hubs or between a hub and a memory controller. Many studies focus on
developing efficient architectures of optical routers, while their physical
implementation that can seriously deteriorate the quality of the architectures is rarely addressed. The existing automatic place-and-route tools
suffer from considerable insertion loss due to many waveguide crossings
outside of PSEs, which leads to huge power consumption of laser sources.
By observing that the logic schemes of most optical routers are actually
planar, we develop a concurrent PSE placement and waveguide routing
flow, called PlanarONoC, that guarantees optimal solutions in terms of
crossings for planar logic schemes. Experimental results show that the
proposed flow reduces the maximum insertion loss by 37% on average,
guarantees no waveguide crossing outside of PSEs, and performs much
more efficient compared to the state-of-the-art work.
CCS CONCEPTS
• Hardware → Physical design (EDA); Emerging optical and photonic technologies; Placement ; Wire routing ;
KEYWORDS
Optical networks-on-chip, optical router, PSE placement and routing
1 INTRODUCTION
With the advance of technology, the increasing transistor integration
capacity facilitates the development of multi-and many-core systems,
which require a subsystem known as networks-on-chip (NoC) for underlying on-chip communication among cores and memory components.
Optical NoCs (ONoCs) have been proposed and shown their potential
in providing superior communication bandwidths, efficiency in power
consumption, and latency performance compared to electronic NoCs.
Serving as the critical part of ONoCs, an optical router composed of
waveguides and switches routes signals between two hubs (which are
the interfaces between the electrical and optical domains) or between a
hub and a memory controller. The physical implementation of optical
routers is rarely addressed in literature. Fig. 1(a) shows the logic scheme
of the 4×4 λ-router [4], where each hub or memory controller contains
an initiator and a target serving as the source and the destination of
∗ This work was partially supported by Synopsys and MOST of Taiwan under Grant No’s
MOST 104-2628-E-011-009-MY3, MOST 106-2221-E-011-143-MY3, and MOST 107-2636-E011-002.
Permission to make digital or hard copies of all or part of this work for personal or classroom
use is granted without fee provided that copies are not made or distributed for profit or
commercial advantage and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or
to redistribute to lists, requires prior specific permission and /or a fee. Request permissions
from permissions@acm.org.
DAC ’18, June 24–29, 2018, San Francisco, CA, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5700-5/18/06. . . $15.00
https://doi.org/10.1145/3195970.3196093
Memory/hub
Waveguide
PSE
F1
F2
F3
F4
I1
I2
I3
I4
(a)
T1
T2
T3
T4
F2
F4
F1
F3
(b)
Figure 1: The gap between the logic scheme and a real physical implementation of a 4×4 λ -router. (a) The logic scheme. (b) A physical implementation of the router with fixed hubs and memory controllers.
signal paths. Each signal path passes through several photonic switching
elements (PSEs) that act as optical filters and determine the transmission
direction of input light. The ideality of the logic scheme of an optical
router can be seriously deteriorated because of the actual and fixed
positions of hubs and memory controllers determined by the target
architecture. As shown in Fig. 1(b), the initiator and target of a hub
or memory controller are actually on the same immovable component,
inducing additional waveguide crossings, wirelength overhead, and thus
much more power consumption.
There have been two studies on placement and routing for optical
routers. PROTON proposes the first automatic place-and-route tool,
where PSE placement is first done with nonlinear optimization, and then
waveguides are routed with a modified Lee’s algorithm [3]. To improve
the scalability of PROTON, the authors then developed PLATON, which
presents a force-directed placement algorithm and outperforms PROTON for larger routers [2]. However, there still exist some insufficiencies
in existing work. First, the orientations of PSEs are not considered, which
may result in unnecessary crossings. Since the locations of the four input
and output ports of each PSE are fixed, as shown in Fig. 2(a), placing PSEs
with uniform orientation like PROTON and PLATON do could result in
many crossings, as illustrated in Fig. 2(b). By simply rotating or flipping
PSEs, these crossings can be easily resolved, as demonstrated by the
example shown in Fig. 2(c). In addition, because two-stage approaches of
PSE placement followed by waveguide routing are adopted in PROTON
and PLATON, the number of waveguide crossings can only be estimated
instead of being directly optimized during placement. This opens up
significant opportunities for reduction of power dissipation, since the
large number of crossings after physical implementation in existing
approaches accounts for the majority of the insertion loss. Furthermore,
both the nonlinear and force-directed placement approaches require iterative optimization processes, suffering from inevitable trade-off between
solution quality and runtime efficiency.
By regarding each component (which can be either a hub, a memory
controller, or a PSE) as a vertex, and regarding each waveguide between
a pair of components as an edge connecting the two corresponding
vertices, the logic scheme of an optical router can be transformed into a
connection graph. The transformation makes the problem of crossing
minimization in physical implementation highly similar to the problem
of maintaining the planarity of the connection graph as much as possible.
Fig. 3 shows the symbolic connection graph transformed from the logic
scheme of the 4×4 λ-router. It can be observed that even if the initiator
(cid:1)(cid:3)
(cid:1)(cid:4)
(cid:2)(cid:4)
(a)
(cid:2)(cid:3)
(cid:1)(cid:3)
(cid:1)(cid:4) (cid:2)(cid:3)(cid:2)(cid:4)
(b)
(cid:2)(cid:3) (cid:2)(cid:4)
(cid:1)(cid:4)
(cid:1)(cid:3)
(c)
(cid:1)(cid:6)
(cid:1)(cid:6)
(cid:1)(cid:7)
(cid:1)(cid:6) (cid:8) (cid:1)(cid:7)
(cid:1)(cid:6)
(cid:1)(cid:7)
(cid:1)(cid:6) (cid:9) (cid:1)(cid:7)
(cid:1)(cid:6)
(a)
(cid:1)(cid:4)
(cid:1)(cid:4)
(cid:1)(cid:2)
(cid:1)(cid:2)
(cid:1)(cid:3)
(b)
(cid:1)(cid:5)
T1
T2
T3
T4
I1
I2
I3
I4
Figure 2: The orientation of PSEs. (a) The fixed order of four PSE ports.
(b) PSEs with uniform orientation could result in crossings. (c) By simply
flipping the middle PSE, the two crossings in (b) can be eliminated.
Figure 4: PSE functions and insertion loss. (a) Depending on the wavelengths of input signal and the MRRs, a PSE operates in drop or cross
function. (b) The highlighted signal path has one drop loss and two cross
losses inside PSEs.
F4
F2
F3
F1
Figure 3: The logic scheme of the 4×4 λ -router is still planar with
merged initiators and targets.
and the target of each hub or memory are regarded as a single vertex,
the connection graph is actually planar. In fact, the logic schemes of
most state-of-the-art optical routers are originally planar before physical
implementation. This means that a planar embedding exists without any
waveguide crossing outside of PSEs if the fixed position constraints of
hubs and memories are not considered. Our goal is to determine whether
the planarity can be maintained while keeping waveguide length short,
even if hubs and memory controllers are fixed. This ambition inspires our
work to develop a novel graph-based algorithm flow, called PlanarONoC,
to place and route state-of-the-art optical routers. The major features
and contributions of the proposed flow are listed as follows:
• The proposed flow performs concurrent placement and routing
based on planar embedding and guarantees optimal solutions
in terms of number of crossings if the logic scheme of a target
router is originally planar. The flow is also flexible and applicable
if non-planar routers will be introduced in the future.
• A connection graph considering the orientation of PSEs is proposed, and the flow automatically determines the PSE orientations
to avoid unnecessary and unexpected waveguide crossings.
• The approach is deterministic and does not require an iterative
optimization process. Thus it has extraordinary scalability.
• Experimental results show that the proposed flow can reduce the
maximum insertion loss by 37% on average, guarantee no waveguide crossing outside of PSEs, and perform much more efficiently
on the same benchmarks used by PROTON and PLATON.
The rest of this paper is organized as follows: Section 2 explains some
preliminaries and important literature inspiring our work. In Section 3,
the proposed concurrent placement and routing algorithm flow is presented, and major steps are presented in detail. Finally, experimental
results are presented in Section 4 and a brief conclusion of this work is
given in Section 5.
2 PRELIMINARIES
This section provides important preliminaries of this work. Section 2.1
gives the major sources of power consumption in optical routers. Section 2.2 briefly introduces an existing method on planar embedding at
fixed vertex locations which has inspired our approach.
2.1 Insertion Loss in Wavelength-Routed Optical
Router
Wavelength-routed optical routers are realized based on the wavelengthselective property of PSEs. Each PSE in the network may consist of
two micro-ring resonators (MRRs), two input ports, and two output
ports (called a 2×2 PSE), or one MRR, one input port, and two output
ports (called a 1×2 PSE). Fig. 4(a) illustrates a 2×2 PSE and its routing
functions. If the wavelength λi of an input signal equals the resonance
wavelength λr of an MRR, the signal is coupled and deflected by 90
degrees to the output port perpendicular to the input port, which is
referred to as the drop function. In contrast, if λi is different from λr , the
input signal directly crosses the PSE to the opposite output port, which
is corresponding to the cross function.
The power consumption of an optical router is dominated by a critical
signal path with the maximum insertion loss, if all laser sources have
equal laser power [2]. Insertion loss is mainly determined by (1) propagation loss pl of waveguides, (2) crossing loss cl due to two crossing
waveguides and thus two crossing signals inside or outside of a PSE, (3)
drop loss d l when a signal is dropped within an MRR, and (4) bending
loss bl at waveguide bends. The four loss amounts of a signal path p can
be computed as follows [2]:
· Lp ,
d B
pl (p ) = 1.5
cm
cl (p ) = 0.15d B · Cp ,
d l (p ) = 0.5d B · Dp ,
bl (p ) = 0.005d B · Bp ,
(1)
(2)
(3)
(4)
where Lp , Cp , Dp , and Bp are separately the waveguide length, the
number of crossings, the number of drops, and the number of bends in
p . Let il (p ) be the insertion loss of the path p , which is the summation
of the four loss amounts. Then the insertion loss of the critical path (the
path consuming the maximum insertion loss) is determined as follows:
ilma x = max
il (p ),
p ∈P
(5)
where P is the set of all signal paths. Fig. 4(b) shows a signal path in the
logic scheme of the 4×4 λ-router, where one drop loss and two crossing
losses are expected inside the three PSEs. The propagation loss, bending
loss, and crossing loss outside of PSEs are determined according to the
physical implementation of the router.
2.2 Planar Embedding at Fixed Vertex Locations
As mentioned in Section 1, state-of-the-art optical routers usually have
good planarity in the connection graphs of their logic schemes. However, observing from the experimental results of PROTON and PLATON,
insertion loss after physical implementation usually results mostly from
waveguide crossings outside of PSEs. These results motivate us to investigate the methodologies that can maintain the planarity of logic schemes
while simultaneously satisfying the fixed position constraints of hubs
and memory controllers, which requires a planar embedding algorithm
that is able to embed some vertices at pre-defined fixed positions.
(cid:3)(cid:5)
(cid:3)(cid:4)
(a)
(cid:3)(cid:6)
(b)
(cid:3)(cid:7)
(cid:3)(cid:8)
Hamiltonian edge
Inner edge
Outer edge
Inner region
(cid:1)
(cid:2)(cid:5)
(cid:2)(cid:4)
(cid:2)(cid:6)
(cid:2)(cid:7)
(cid:2)(cid:8)
(c)
Figure 5: The existing method to embed vertices at fixed locations for
planar graphs. (a) A planar graph. (b) The three types of edges differentiated by a Hamiltonian cycle. (c) The planar embedding with fixed vertex
locations.
Graph edge in the H-cycle
Dummy edge in the H-cycle
Dummy vertex
Input scheme
Orientation-aware Planar Embedding
Connection graph construction
Is the graph planar?
yes
Straight line embedding
no
Maximal planar graph 
extraction
Order-constrained Hamiltonian Cycle Finding
Cycle initialization for fixed vertices
PSE chaining and insertion
Orientation-aware Planar Embedding
Inner/outer edge identification
PSE placement & routing
Any removed edges?
no
Output layout
yes
Route removed edges 
with minimum 
#crossings
(a)
(b)
(c)
(d)
Figure 7: The overall algorithm flow of the proposed concurrent placement and routing engine.
Figure 6: The existing method [9] to find a Hamiltonian cycle in linear
time. (a) A given planar embedding. (b) A spanning tree of the planar
graph. (c) Traverse the tree clockwise. (d) Derive the Hamiltonian cycle
with dummy vertices and edges.
There exist some research studies on planar embedding at fixed vertex
locations [5, 9]. Given a set of n points, {p1 , p2 , . . ., pn } , the aim is to
find a planar embedding of a planar graph of n vertices, {v1 , v2 , . . ., vn } ,
where vi is fixed at the point pi . The main idea of their algorithms is
demonstrated in Fig. 5. Fig. 5(a) shows a planar graph of five vertices,
on which a Hamiltonian cycle is first found as indicated by the red
cycle in Fig. 5(b). This Hamiltonian cycle C divides the planar graph
into two outerplanar graphs that only have the edges of C in common,
and the edges belonging to C are defined as Hamiltonian edges. The
cycle C also partitions the other edges into two edge sets inside and
outside C relative to some (and arbitrary) planar embedding, which are
respectively defined as inner edges and outer edges. By renumbering the
vertices according to their order in C , the algorithms [5, 9] first embed
the Hamiltonian edges by using some “weaving technique” and form
an embedded contour Γ , as illustrated in Fig. 5(c). We define the region
surrounded by Γ as the inner region. Finally, the inner edges and the
outer edges are respectively drawn inside and outside the inner region
without destroying the planarity. Although finding a Hamiltonian cycle
is generally an NP-complete problem, [9] proposes a O (n)-time approach
by modifying a given graph, which is briefly introduced in Fig. 6. Fig. 6(a)
shows a graph that obviously has no Hamiltonian cycle, and a spanning
tree shown in Fig. 6(b) is found. Starting at any vertex, the approach
traverses clockwise around the spanning tree and uses curves to avoid
walking on repeated edges, as shown in Fig. 6(c). By adding a dummy
vertex wherever a curve intersects an edge, introducing dummy edges
for curves, and merging multiple edges, a Hamiltonian cycle can be
found, as illustrated in Fig. 6(d).
This sophisticated algorithm can well tackle the fixed position constraints of hubs and memories without causing any waveguide crossings
outside of PSEs. However, it is still not suitable for physical implementation of optical routers, because an arbitrary order of vertices in the
Hamiltonian cycle C may easily result in a grotesque region bounded by
Γ , as the inner region in Fig. 5(c), which considerably increases routing
complexity and waveguide lengths. Consequently, the advantage of diminishing crossing loss by maintaining the planarity of logic schemes
could be eliminated by the resulting huge propagation loss. In addition,
the existing algorithms are designed to embed all vertices at their fixed
points. Yet only few vertices representing hubs and memories in optical routers have fixed positions, and other PSE vertices are free to be
moved inside the chip. The freedom of PSE placement should be properly
employed to optimize waveguide lengths and thus propagation loss.
3 CONCURRENT PLACEMENT AND ROUTING
FOR CROSSING MINIMIZATION
This section presents the proposed PlanarONoC, the concurrent placement and routing approach for crossing minimization. The overall algorithm flow is described in Section 3.1, and the major steps in the flow
are detailed separately in the following subsections.
3.1 Algorithm Flow
Fig. 7 presents the overall optimization flow. The input is the logic scheme
of an optical router. To minimize waveguide crossings, our algorithm is
based on a planar embedding considering PSE orientations. A connection
graph is first constructed for the given logic scheme, and its straight-line
embedding is found. The planarity of the constructed connection graph
needs to be checked before planar embedding, but most of the logic
schemes of the state-of-the-art optical routers are originally planar. If
non-planar routers are introduced in the future, many heuristics can
be adopted to find a maximal connection sub-graph by removing a few
edges before planar embedding.
To embed the vertices of memories and hubs at fixed positions while
maintaining the low complexity of the inner routing region, a sophisticated Hamiltonian cycle finding approach is developed where the order
of the fixed vertices in the cycle can be pre-determined. By marking the
derived Hamiltonian path on the planar embedding found in the first
stage, the waveguides routed inside, outside, and on the periphery of the
inner routing region can be differentiated. Finally, PSE placement and
waveguide routing can be performed without causing any waveguide
crossing. If the initial connection graph is non-planar and thus some
edges are removed in the first stage, those edges can finally be routed
by using the routing algorithm proposed in [3] to minimize the number
of crossings.
In the following, we assume the connection graph of the input logic
scheme is planar and focus on detailing the three major stages in our flow:
(cid:2)(cid:3)(cid:1)(cid:5)
(cid:2)(cid:3)(cid:1)(cid:7)
(cid:2)(cid:3)(cid:1)(cid:6)
(a)
F1
F2
F3
F4
(cid:4)(cid:5)
(cid:4)(cid:9)
(cid:2)(cid:3)(cid:1)(cid:7)
(cid:2)(cid:3)(cid:1)(cid:8)
(cid:2)(cid:3)(cid:1)(cid:9)
(cid:2)(cid:3)(cid:1)(cid:5)
(cid:2)(cid:3)(cid:1)(cid:6)
(d)
(cid:4)(cid:8)(cid:4)(cid:6)
(cid:2)(cid:3)(cid:1)(cid:10)
(cid:4)(cid:7)
(cid:2)(cid:3)(cid:1)(cid:8)
(cid:2)(cid:3)(cid:1)(cid:9)
(b)
(cid:2)(cid:3)(cid:1)(cid:5)
(cid:2)(cid:3)(cid:1)(cid:7)
(cid:2)(cid:3)(cid:1)(cid:9)
(cid:2)(cid:3)(cid:1)(cid:8)
(cid:2)(cid:3)(cid:1)(cid:6)
(c)
Memory/hub vertex
PSE vertex
PSE edge
Connection edge
Figure 8: Connection graph construction. (a) Five connection PSEs. (b)
A planar embedding of an intuitive connection graph. (c) The resulting
placement and routing result of (b) with some unexpected crossings. (d)
The proposed connection graph of Fig. 1(a).
orientation-aware planar embedding, order-constrained Hamiltonian
cycle finding, and planarity-preserving PSE placement and routing.
3.2 Orientation-aware Planar Embedding
To take the advantage of the planarity of logic schemes and the existing
planar embedding algorithms for fixed hubs and memories, the first
step is to transform an input logic scheme into a connection graph. As
mentioned in Section 1, an intuitive way is to use a vertex to represent a
PSE and an edge between two vertices to indicate the connection of the
two corresponding PSEs. However, constructing the graph in such a way
may lead to unexpected crossings after planar embedding. Figs. 8(a)–(c)
illustrates an example. For the five connected PSEs in Fig. 8(a), Fig. 8(b)
shows one possible planar embedding of the intuitive connection graph,
where the i -th PSE is represented by the vertex vi .Since the intuitive
connection graph cannot model the fixed port ordering of PSEs by using
a single vertex, the embedded positions of v2 and v5 could be accidentally
switched compared to the logic scheme. The placement result of the five
PSEs according to this planar embedding is shown in Fig. 8(c), where the
three connections among P S E2 , P S E3 , and P S E5 are very likely to cause
crossings with each other or with other waveguides (the dashed lines in
Fig. 8(c)). To model the port ordering and to automatically determine
the orientations of PSEs during planar embedding, we use a 4-cycle to
represent each PSE in which each vertex specifically denotes one of the
four input or output ports. The proposed connection graph of the logic
scheme in Fig. 1(a) is given in Fig. 8(d).
Given a planar connection graph or a maximal planar subgraph derived from a non-planar router, a planar embedding is required to facilitate the following steps. Every embedding algorithm is applicable,
and we adopt straight-line embedding in our flow due to its efficiency
and simplicity [6]. Fig. 9(a) shows the straight-line embedding of the
connection graph in Fig. 8(d). The four vertices in the 4-cycle of each
PSE are then contracted on the plane to simplify the connection graph
and thus the problem size, as illustrated in Fig. 9(b). Note that this contraction operation will not destroy the planarity and would not cause
unexpected crossings during routing because the four incident edges on
a contracted vertex still follow the port ordering of the corresponding
4-cycle, which can be verified by comparing Figs. 9(a) and (b).
3.3 Order-constrained Hamiltonian Cycle Finding
As observed in Section 2.2, a Hamiltonian cycle with an arbitrary vertex
order could result in extremely complex inner and outer regions that
Memory/hub vertex
PSE vertex
PSE edge
Connection edge
(cid:1)(cid:6)
F4
F3
F4
F3
F1
F2
(a)
(cid:1)(cid:5)
(cid:1)(cid:7)
F1
F2
(b)
(cid:1)(cid:3)
(cid:1)(cid:2) (cid:1)(cid:4)
Figure 9: Planar embedding of connection graphs. (a) Straight-line embedding of the connection graph in Fig. 8(d). (b) The resulting embedding
by contracting 4-cycles to single vertices.
Memory/hub vertex
PSE vertex
Dummy vertex
Connection edge
Dummy edge
M1
(cid:1)
M0
(cid:1)
Inner region
(cid:1)
H1
(cid:1)
(a)
H0
H1
M1
M0
H0
(cid:1)(cid:4)
(b)
(cid:1)(cid:5)
(cid:1)(cid:6) (cid:1)(cid:7)
(cid:1)(cid:2)
(cid:1)(cid:3)
Figure 10: Hamiltonian cycle initialization for fixed vertices. (a) The
placement and routing planning of the 4 × 4 λ -router (b) The initiated
Hamiltonian cycle.
not only complicate detailed routing but also result in significant wirelength overhead. This subsection presents a novel approach to find a
Hamiltonian cycle on the connection graph to overcome this deficiency.
3.3.1 Cycle Initialization for Fixed Vertices. To keep the completeness of routing regions, a pre-defined order of fixed vertices can first
be determined by the user (as the number of fixed hubs and memories is small in state-of-the-art routers) to minimize routing region
complexity. For example, suppose the fixed positions of the two hubs
(H0 and H1 ) and two memory controllers (M0 and M1 ) of the 4 × 4
λ-router are shown in Fig. 10(a). If the four fixed vertices are in the
clockwise order < H 0, H 1, M 1, M 0 > or in the counterclockwise order
< H 0, M 0, M 1, H 1 > in the Hamiltonian cycle, the inner region can be
arranged in a rectangle shape by distributing PSEs between successive
fixed vertex pairs. We assume the counterclockwise order of fixed vertices for demonstration in the following. Finding such a Hamiltonian
cycle with pre-determined partial vertex order is obviously NP-complete
since finding a Hamiltonian cycle for an undirected planar graph of
maximum degree three is well-known to be NP-complete [7].
We propose a sophisticated and efficient algorithm to find a Hamiltonian cycle with a pre-determined partial vertex order for state-of-the-art
optical routers. The proposed algorithm first initiates a Hamiltonian
cycle for fixed vertices. We use the contracted connection graph of Fig. 3
for better demonstration, where the fixed vertices can be arranged on
a line. According to the counterclockwise order of fixed vertices, the
fixed hubs and memories H 0, M 0, M 1, and H 1 are assigned to F 1, F 2,
F 3, and F 4 in the logic scheme correspondingly, as shown in Fig. 10(b).
Due to the adjacency of the fixed vertices, the initial Hamiltonian cycle
can be constructed by simply connecting successive vertex pairs, such
as (H 0, M 0), (M 0, M 1), and (M 1, H 1) with dummy edges. To connect
the first vertex and the last vertex such as H 0 and H 1 in Fig. 10(b), two
dummy vertices are added at the intersection points of the dummy edge
Connection edge in the H-cycle 
H1
Dummy edge in the H-cycle
M1
M0
H0
(cid:1)(cid:4)
(a)
(cid:1)(cid:5)
(cid:1)(cid:6) (cid:1)(cid:7)
(cid:1)(cid:2)
(cid:1)(cid:3)
(cid:1)(cid:2)
(b)
(cid:1)(cid:4)
(cid:1)(cid:8)(cid:1)(cid:10)
(d)
(cid:1)(cid:9)
(cid:1)(cid:5)
(cid:1)(cid:2)
(cid:1)(cid:9)
(cid:1)(cid:11)
(cid:1)(cid:5)
(cid:1)(cid:11)
(c)
(cid:1)(cid:4)
(cid:1)(cid:8)(cid:1)(cid:10)
(e)
Figure 11: PSE chaining and insertion. (a) PSE chaining for the 4 × 4 λ router with shortest path finding. (b)(c) Include the left v4 into the Hamiltonian cycle with face propagation. (d)(e) Insert the remaining vd into
the Hamiltonian cycle by using a dummy edge.
and two connection edges, and the dummy edge is divided into three
segments to maintain the graph planarity.
3.3.2 PSE Chaining and Insertion. To loop PSEs into the initiated
Hamiltonian cycle, we re-examine each successive fixed vertex pair
in the initiated Hamiltonian cycle. If a path composed of connection
edges and PSE vertices exists between two successive fixed vertices
Fi and F j , the dummy edge connecting Fi and F j can be removed and
replaced with the path, which is equivalent to chaining some PSEs into
the Hamiltonian path between Fi and F j . We adopt breadth-first search
(BFS) to find shortest paths, and each path is found such that it does not
overlap with other existing paths. In addition, a dummy edge with some
inserted dummy vertices has higher priority to be replaced with such
a path, because the elimination of dummy vertices can lead to shorter
waveguide lengths in the PSE placement and routing stage. Fig. 11(a)
shows the PSE chaining result of the 4 × 4 λ-router, where the two
dummy vertices have been removed since the dummy edge between H 1
and H 0 is replaced with a path passing through PSEs v2 , v3 , and v1 .
The shortest path finding process may include some PSEs into the
Hamiltonian cycle, while some of them may be left out. One possible
solution is to replace the above step with longest path finding to include
as many PSEs as possible; however, finding a longest path on undirected
graphs is a well-known NP-complete problem, and it is not guaranteed
that no PSE will be left by adopting a longest path algorithm. Instead, we
propose an efficient strategy to insert remaining PSEs into the Hamiltonian cycle based on face propagation. In the resulting planar embedding
of the connection graph, if there exists a face where only a single edge
is in the Hamiltonian cycle, we can simply remove the edge and include
the other edges in the face into the Hamiltonian cycle. Figs. 11(a) and (b)
show that the PSE vertex v4 is left after shortest path finding. Since the
face composed of {v1 , v3 , v4 } has only one edge (v1 , v3 ) contained in the
Hamiltonian cycle, the edge is removed and the other two edges (v1 , v4 )
and (v3 , v4 ) (and thus the vertex v4 ) are included. This face propagation technique can be iteratively performed such that the Hamiltonian
path can cover most of PSEs. For any vertex remaining isolated after
face propagation, such as vd in Fig. 11(d), we insert the vertex into the
Hamiltonian cycle by using dummy edges without destroying the graph
planarity, as illustrated in Fig. 11(e). The resulting Hamiltonian cycle of
the 4 × 4 λ-router is < H 0, M 0, M 1, v6 , v5 , H 1, v2 , v3 , v4 , v1 >.
3.4 Planarity-preserving PSE Placement and
Routing
Fig. 12(a) illustrates the Hamiltonian cycle found in the previous step
on the planar embedding, which is composed of all fixed vertices, all
PSE vertices, some connection edges, and required dummy vertices
and edges. This Hamiltonian cycle in combination with a planar embedding facilitates the identification of inner and outer edges. As illustrated in Fig. 12(b), the green/blue edges inside/outside the Hamiltonian
(cid:1)(cid:6)(cid:1)(cid:7)
(cid:1)(cid:3)
(cid:1)(cid:2) (cid:1)(cid:4)
H0
M0
(cid:1)(cid:5)
(a)
H1
M1
Hamiltonian edge
Inner edge
Outer edge
H1
M1
(cid:1)(cid:6)(cid:1)(cid:7)
(cid:1)(cid:3)
(cid:1)(cid:2) (cid:1)(cid:4)
(cid:1)(cid:5)
H0
M0
(b)
Figure 12: Different types of edges identified on the planar embedding.
(a) The derived Hamiltonian cycle introduces the Hamiltonian edges and
some dummy edges. (b) Inner and outer edges are identified according to
the inner region surrounded by the Hamiltonian cycle.
(cid:1)(cid:7)
(cid:1)(cid:6)
(3)
M1
(4)
(5)
(6)
H1
(3)
M1
(4)
(5)
(6)
H1
(cid:1)(cid:3)
(7)
(cid:1)(cid:4)
(8)
(2)
M0
(cid:1)(cid:5)
(cid:1)(cid:2)
H0
(1)
(9)
(10)
(a)
(2)
M0
(7)
(8)
(9)
(10)
H0
(1)
(b)
Figure 13: Placement and routing of the 4 × 4 λ -router. (a) The placement
of PSEs between successive fixed vertices. (b) The routing result of the
three types of connection edges.
cycle are inner/outer edges. Identifying the Hamiltonian edges, inner
edges, and outer edges, the PSEs and waveguides are ready to be placed
and routed. According to the constructed Hamiltonian cycle, PSEs are
first placed between successive fixed nodes. Fig. 13(a) shows the PSE
placement result of the 4 × 4 λ-router according to the derived cycle,
< H 0, M 0, M 1, v6 , v5 , H 1, v2 , v3 , v4 , v1 > . The PSEs are uniformly placed
between successive fixed vertex pairs to maintain the simple rectangle
shape of the inner routing region and to evenly distribute routing resources.
Net ordering is critical to guarantee the planarity of waveguides. For
different types of edges, inner edges are routed prior to Hamiltonian
edges, and outer edges have the lowest priority. To consider the routing
order of edges of the same type, the vertices are re-labelled along the
Hamiltonian cycle with indices, as illustrated in Figs. 13(a) and (b), and
the edges are denoted with the indices. For inner edges, an edge is
indexed such that the first index is always smaller than the second index.
For example, the inner edge connecting M 0 and v6 is denoted as (2, 4).
The routing order of inner edges is determined with the following rules:
1. An edge with smaller second index has higher routing priority.
2. If two edges have the same second index, the edge with larger
first index has higher priority.
According to the rules, the five inner edges in Fig. 13(a) are routed with
the order (2, 4), (5, 7), (5, 8), (4, 9) and (1, 9), and each edge is routed by
minimizing wirelength and preserving enough routing resource for the
rest of unrouted inner edges. After that, the Hamiltonian edges can be
routed with any order and wirelength minimization. Finally, each outer
edge routed from vertex i to vertex j can be routed either clockwise or
counterclockwise. The routing direction minimizing waveguide length
Table 1: Results of PROTON, PLATON, and PlanarONoC. i lm a x : the insertion loss of the critical path. C : the number of crossings on the critical path.
L : the waveguide length of the critical path. C P U : runtime in seconds.
Topology
8x8 λ
8x8 GWOR
8x8 Crossbar
16x16 λ
Comp.
ilma x
8."
DyAD - smart routing for networks-on-chip.,"In this paper, we present and evaluate a novel routing scheme called DyAD which combines the advantages of both deterministic and adaptive routing schemes. More precisely, we envision a new routing technique which judiciously switches between deterministic and adaptive routing based on the network's congestion conditions. The simulation results show the effectiveness of DyAD by comparing it with purely deterministic and adaptive routing schemes under different traffic patterns. Moreover, a prototype router based on the DyAD idea has been designed and evaluated. Compared to purely adaptive routers, the overhead of implementing DyAD is negligible (less than 7%), while the performance is consistently better.","DyAD – Smart Routing for Networks-on-Chip ∗
Jingcao Hu
Carnegie Mellon University
Pittsburgh, PA 15213-3890, USA
jingcao@ece.cmu.edu
Radu Marculescu
Carnegie Mellon University
Pittsburgh, PA 15213-3890, USA
radum@ece.cmu.edu
ABSTRACT
In this paper, we present and evaluate a novel routing scheme called
DyAD which combines the advantages of both deterministic and
adaptive routing schemes. More precisely, we envision a new routing technique which judiciously switches between deterministic
and adaptive routing based on the network’s congestion conditions.
The simulation results show the effectiveness of DyAD by comparing it with purely deterministic and adaptive routing schemes under
different trafﬁc patterns. Moreover, a prototype router based on the
DyAD idea has been designed and evaluated. Compared to purely
adaptive routers, the overhead of implementing DyAD is negligible
(less than 7%), while the performance is consistently better.
Categories and Subject Descriptors
B.4 [Hardware]: Input/output and data communications
General Terms
Algorithms, Performance, Design
Keywords
Networks-on-Chip, Systems-on-Chip, router design
1.
INTRODUCTION
The regular tile-based NoC architecture was recently proposed
as a solution to the complex on-chip communication problems [3].
Such a chip consists of a grid of regular tiles where each tile can be
a general-purpose processor, a DSP, a memory subsystem, etc. A
router is embedded within each tile with the objective of connecting
it to its neighboring tiles. Thus, instead of routing design-speci ﬁc
global on-chip wires, the inter-tile communication can be achieved
by routing packets. The performance and the efﬁciency of the NoC
depends on the underlying communication infrastructure; this, in
turn, depends on the performance (latency and throughput) of the
on-chip routers. Thus, the design of efﬁcient, high performance
routers represents a critical issue for the success of the NoC approach.
Routers can be generally classiﬁed into deterministic and adaptive [8].
In deterministic routing (also called oblivious routing),
the path is completely determined by the source and the destination
address. On the other hand, a routing technique is called adaptive if, given a source and a destination address, the path taken by
∗
Research supported by NSF CCR-00-93104 and Marco Gigascale
Research Center (GSRC)
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2004, June 7–11, 2004, San Diego, California, USA.
Copyright 2004 ACM 1-58113-828-8/04/0006 ...$5.00.
a particular packet depends on dynamic network conditions (e.g.
congested links due to trafﬁc variability).
The main advantage of using deterministic routing is its simplicity of the routers design. Because of the simpliﬁed logic, the
deterministic routing provides low latency when the network is not
congested. However, as the packet injection rate increases, deterministic routers are likely to suffer from throughput degradation as
they can not dynamically respond to network congestion. In contrast, adaptive routers avoid congested links by using alternative
routing paths; this leads to higher throughput. However, due to
the extra logic needed to decide on a good routing path, adaptive
routing has a higher latency at low levels of network congestion.
In this paper, we present a novel routing scheme which combines
the advantages of both deterministic and adaptive routing schemes.
The proposed routing scheme, dubbed DyAD from Dynamic Adaptive Deterministic switching, is based on the current network congestion. More precisely, with DyAD routing each router in the network continuously monitors its local network load and makes decisions based on this information. When the network is not congested, a DyAD router works in a deterministic mode, thus enjoying the low routing latency enabled by deterministic routing.
On the contrary, when the network becomes congested, the DyAD
router switches back to the adaptive routing mode and thus avoids
the congested links by exploiting other routing paths; this leads to
higher network throughput which is highly desirable for applications implemented using the NoC approach. To propose a valid
approach, we also show how the freedom from deadlock and livelock [8] can be guaranteed when mixing deterministic and adaptive
routing modes into the same NoC.
Experimental results show that, compared to both deterministic
and adaptive routing, signiﬁcant performance improvements can be
achieved by using the DyAD approach. At the same time, by prototyping a DyAD router based on a popular adaptive routing strategy, we show that the chip area overhead is marginal (typically
less than 7%), while its performance consistently outperforms that
of a purely adaptive router. We believe that the proposed scheme
based on combining the deterministic and adaptive routing modes
has great potential for future NoC implementations.
The paper is organized as follows. In Section. 2 we review the
related work. In Section 3, we present the DyAD router architecture
and a practical implementation. Experimental results in Section 4
validate the performance improvements and the prototype design
(Section 5) shows that the implementation overhead compared to a
traditional adaptive router is indeed negligible.
2. RELATED WORK
There has been signiﬁcant work on efﬁcient routing schemes in
parallel and distributed computing areas [4][1]. Because of limited
space, the reader is referred to [8] for a survey on routing techniques developed for direct networks.
In [11], Shin et al. propose a hybrid switching scheme that
dynamically combines both virtual cut-through [6] and wormhole
switching [2] to provide higher achievable throughput values compared to wormhole switching alone, while reducing the buffer space
required at the intermediate nodes when compared to virtual cutthrough. In this paper, we are looking at the router design from
another perspective; that is, we try to combine the advantages provided by deterministic and adaptive routing instead of relying on
different switching schemes. Thus, the work presented here is orthogonal to that in [11]. Interestingly enough, they can be eventually combined, if needed.
From another perspective, several on-chip routers have been proposed for NoC (e.g. [10][7][9]). However, none of this previous
work has addressed the issue of combining deterministic and adaptive routing into a new routing scheme. As we will see later in this
paper, by doing so one can achieve signiﬁcant better performance
compared to purely adaptive routers only with negligible implementation overhead.
3. THE DyAD ROUTER ARCHITECTURE
Rather than a detailed implementation, DyAD is about a new
paradigm for NoC router design which exploits the advantages of
deterministic and adaptive routing. Indeed, based on this idea, any
suitable deterministic and adaptive routing scheme can be combined to form a DyAD router (although care must be taken to issues
such as deadlock freedom, as it will be discussed in Section 3.3).
Similar ideas can be extended to other routers (e.g.
routers for
multi-computer networks) as well.
3.1 Platform description
Due to its popularity, the platform under consideration is composed of a n × n array of tiles which are inter-connected by a 2D
mesh network (see [7][9]). In such a regular architecture, each tile
is composed of a processing element (PE) and a router. The router
embedded onto each tile is connected to the four neighboring tiles
and its local PE via channels. Each channel consists of two directional point-to-point links between two routers or a router and a
local PE [5].
Because of the limited silicon resources and the low-latency requirements for typical NoC applications, wormhole switching is
used as the switching scheme for the on-chip routers. Under this
scheme, a packet is split into so-called ﬂits
( ﬂow control digits),
which are then routed in a pipelined fashion. To minimize the implementation costs, the on-chip network should be implemented
with very little silicon area overhead. Thus, instead of having huge
memories (e.g. SRAM or DRAM) as buffering space, it ’s more
reasonable to use registers. A 5 × 5 crossbar switch is used as the
switching fabric because of its nice cost/performance trade-offs for
switches with small number of ports.
In our experiments, the XY routing is used as a representative deterministic routing scheme because of its simplicity and wide popularity. Obviously, XY routing is a minimal path routing algorithm
and is free of deadlock and livelock [8]. Unlike the deterministic routing where the routing path is ﬁxed once the source and the
destination addresses are given, the adaptive routing offers packets
more ﬂexibility in choosing their routing paths, if multiple routing
paths exist. However, when using adaptive routing, caution must be
taken in order to solve the deadlock problem, which may be caused
by packets waiting for each other in a cycle.
Starting from these observations, we use routing algorithms that
require no virtual channels for NoC. To be deadlock free, the routing algorithm needs to prohibit at least one turn in each of the possible routing cycles.
In addition, in order to preserve the adaptiveness, it should not prohibit more turns than necessary. Chiu in
[1] proposed the odd-even turn model which restricts the locations
where some types of turns can take place such that the algorithm
remains deadlock-free. More precisely, the odd-even routing prohibits the east→north and east→south turns at any tiles located in
an even column. It also prohibits the north→west and south→west
turns at any tiles located in an odd column. Compared to other
adaptive routing algorithms without virtual channel support (e.g.
[4]), the degree of the adaptiveness provided by the odd-even routing is distributed more evenly across the network. Thus, in this
paper, we choose the minimal1 odd-even routing as the adaptive
routing scheme for on-chip routers. The use of minimal routing
helps not only in reducing the energy consumption of communication, but also to keeping the network free from the livelock.
3.2 Motivation for the DyAD approach
As a motivational example, Fig. 1 shows how the performance
of the deterministic and adaptive routing changes with respect to
the network load for a 6 × 6 mesh under transpose1 trafﬁc pattern
(please refer to Section 4 for detailed description of the simulation
setup and the trafﬁc pattern’s characteristics).
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
600
500
400
300
200
100
0
0.005
XY
Odd −Even
0.01
0.015
0.02
0.025
0.03
0.035
Packet injection rate (packets/cycle)
80
70
60
50
40
30
20
10
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
0.04
0
0.005
XY
Odd− Even
0.01
0.015
Packet injection rate (packets/cycle)
0.02
0.025
0.03
0.035
0.04
(a)
(b)
Figure 1: Performance comparison between XY and odd-even
In Fig. 1(a), we report the measured average communication latency of the packets and the average sustainable network throughput, in terms of packet injection rates at each node. Fig. 1(b) shows
the magniﬁed view of the average packet latency at low injection
rates. These ﬁgures clearly show the trade off between deterministic (XY ) and adaptive (odd-even) routing. More precisely, the oddeven is able to achieve much higher saturation throughput compared to XY routing (more than 60% in this experiment). However,
at low network workloads (below 0.023 packets/cycle in this case),
XY beats the odd-even routing in terms of average packet latency2 .
This is exactly the trade off that motivated us to develop the DyAD
routing, which tries to combine the advantages of both deterministic and adaptive routing by judiciously choosing the appropriate
routing mode under different trafﬁc conditions.
3.3 DyAD-OE: A DyAD implementation of
adaptive odd-even routing
In this subsection, we present the actual router design, DyADOE, which implements the concept of DyAD for odd-even routing.
Combining odd-even and XY to form a DyAD router may lead to
deadlock. Thus, we develop a new routing scheme, called oe-ﬁxed ,
as the deterministic routing mode in DyAD-OE. Oe-ﬁxed is indeed
a deterministic version of odd-even based on removing the oddeven’s adaptiveness. For instance, in odd-even mode, if a packet
with a given source and destination can be routed to both output p1
and p2 , it will always be routed to p1 in oe-ﬁxed . Fig. 2 illustrates
the architecture of the DyAD-OE implementation.
1A minimal adaptive routing algorithm routes all packets through
the shortest paths to the destination.
2To fairly compare different
routing schemes, more trafﬁc
load/patterns and network conﬁgurations need to be tested, as
shown in Section 4.
 
 
 
 
 
 
Congestion Flag to North Neighbor
Addr Decoder
Port Controller
North Input FIFO
Congestion Flag to East Neighbor
Addr Decoder
Port Controller
East Input FIFO
Congestion Flag to South Neighbor
Addr Decoder
Port Controller
South Input FIFO
Congestion Flag to West Neighbor
Addr Decoder
Port Controller
West Input FIFO
Addr Decoder
Port Controller
Local Input FIFO
Congestion Flag of East Neighbor
Congestion Flag of West Neighbor
Congestion Flag of South Neighbor
Congestion Flag of West Neighbor
Crossbar Arbiter
Crossbar
Swtich
Mode Controller
North Out Port
East Out Port
South Out Port
West Out Port
Local Out Port
Figure 2: The DyAD-OE router architecture
Each input controller in Fig. 2 has a separate FIFO (typically
several ﬂits implemented by registers for performance and power
efﬁciency) which buffers the input packets before delivering them
to the output ports. When a new header ﬂit is received, the address
decoder processes that ﬂit and sends the destination address to the
port controller; this determines which output port the packet should
be delivered to. In the odd-even mode, there can be more than one
output direction to route packets. In this case, the port controller
will choose the direction in which the corresponding downstream
router has more empty slots in its input FIFO. Once the router has
made its decision on which direction to route, the port controller
sends the connection request to the crossbar arbiter in order to set
up a path to the corresponding output port.
Except for the local input controller, each input port controller
also monitors its FIFO occupation ratio. If the ratio reaches the
preset congestion threshold, a value 1 will be asserted (indicating
to the upstream router that the downstream router is congested) on
the corresponding congestion ﬂag wire. Otherwise, a value of 0
will be asserted, indicating to the upstream router that congestion
is not an issue.
The Crossbar Arbiter maintains the status of the current crossbar
connection and determines whether to grant connection permission
to the port controller. When there are multiple input port controllers
requests for the same available output port, the Crossbar Arbiter
uses the ﬁrst-come-ﬁrst-served policy to decide which input port to
grant the access, such that the starvation at a particular port can be
avoided.
The Mode Controller continuously monitors its neighboring congestion to determine if the deterministic or the adaptive routing
mode need to be used. Although more advanced techniques can be
used to determine the optimal routing mode, we use the following
simple policy: if any congestion ﬂag from its neighboring routers
are asserted, then the Mode Controller commands all the input port
controllers to work in the adaptive (odd-even) mode; otherwise, it
switches the port controllers to the deterministic (oe-ﬁxed ) mode.
4. EXPERIMENTAL RESULTS
To evaluate the performance gains that can be achieved with
DyAD, we simulate four types of mesh networks, which use XY,
odd-even, oe-ﬁxed and DyAD-OE, respectively. The efﬁciency of
each type of routing is evaluated through latency-throughput curves.
Similar to previous work, we assume that the packet latency spans
the instant from when the ﬁrst ﬂit of the packet is created, to the
time when the last ﬂit is ejected to the destination node, including the queuing time at the source. We assume that the packets
are consumed immediately once they reach their destination nodes.
Each simulation is run for a warm-up period of 2000 cycles. Thereafter, performance data is collected after 20,000 packets are sent. A
cycle-accurate interconnection network simulator (worm sim) was
implemented in C++. Worm sim supports 2D mesh networks with
wormhole switching; it has been designed to be easy customized
to simulate different designs under different trafﬁc patterns. Since
many factors (e.g. routing path selection delay, crossbar arbitration delay, etc.) have a signiﬁcant impact on the NoC performance,
worm sim models them accurately with their actual values taken
from the prototype router designs.
4.1 Evaluation under random trafﬁc
In this set of experiments, we use random trafﬁc to simulate the
performance of the network under different routing strategies. The
processing elements (PEs) generate 5-ﬂit packets at time intervals
chosen with exponential distributions. The network size during
simulation is ﬁxed to be 6 × 6 tiles. The input ports have a FIFO
size of 5 ﬂits, with the congestion threshold set at 60% of the total
FIFO capacity.
Fig. 3(a-b) show the latency/throughput ﬁgures under uniform
and transpose1 trafﬁc patterns, respectively. Under the uniform
trafﬁc pattern, a PE sends a message to any other node with equal
probability. Let E to be the edge size of the square mesh under simulation. Under the pattern transpose1, a PE at (i, j) (i, j ∈ [0, E ))
only sends messages to node (E − 1 − i, E − 1 − j).
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
120
100
80
60
40
20
0
0.005
XY
OE −
fixed
Odd − Even
DyAD − OE
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
XY
OE −
fixed
Odd − Even
DyAD − OE
300
250
200
150
100
50
0.01
0.015
0.02
0.025
0.03
0.035
Packet injection rate (packets/cycle)
0.04
0
0.005
0.01
0.015
0.02
Packet injection rate (packets/cycle)
0.025
0.03
(a) Uniform Trafﬁc
(b) Transpose1 Trafﬁc
Figure 3: Performance evaluation under random trafﬁc
As shown in Fig. 3(a), XY routing performs better than both oddeven and DyAD-OE routing under uniform trafﬁc load. This result is consistent with other results reported in the literature (e.g.
[1][4]). The reason why XY performs best under uniform trafﬁc is
because it embodies global, long-term information about this trafﬁc pattern. On the other hand, the adaptive algorithms select the
routing paths based on local, short-term information. This type of
decision beneﬁts only the packets in the immediate future, which
tend to interfere with other packets. Thus, the evenness of uniform
trafﬁc is not necessarily maintained in the long run [4].
However, for most of the applications in real world, each node
will communicate with some nodes more frequently compared to
others. XY routing has serious problems in dealing with such nonuniform trafﬁc patterns because of its determinism. More precisely,
XY routing blindly maintains the unevenness of the nonuniform
trafﬁc, just as it maintains the evenness for the uniform trafﬁc.
In this spirit, Fig. 3(b) shows that XY routing is clearly outperformed by odd-even and DyAD-OE under transpose1 trafﬁc. In this
case, the network using XY saturates at an injection rate of 0.0167
packets/cycle, while odd-even and DyAD-OE are able to achieve
a throughput of 0.0256 packets/cycle and 0.027 packets/cycle, respectively. This gives a 53.3% and 61.7% improvement in terms
 
 
 
 
 
 
of sustainable throughput. In fact, for the same trafﬁc pattern and
injection rate, DyAD-OE achieves shorter average packet latency
compared to odd-even throughout the experiments.
Another interesting fact is that DyAD-OE does keep the advantage of deterministic routing when the network is not congested.
As shown in Fig. 3, DyAD-OE has the same average packet latency
when network is not congested. On the other hand, the average latency a packet experiences in odd-even is 14% higher compared to
that in DyAD-OE, when the network is lightly loaded.
Other non-uniform trafﬁc patterns (like transpose2 and hot spot)
have been simulated as well and the results were similar to that under transpose1 trafﬁc pattern. We also simulated different network
sizes (ranging from 4 × 4 to 8 × 8 tiles) and different FIFO sizes
(ranging from 3 to 8 ﬂits). All the results reﬂect the same characteristic as in Fig. 3. For complete results, please refer to [5].
4.2 Evaluation under multimedia trafﬁc
Real world trafﬁc (both in macro and on-chip networks) frequently exhibits patterns with self-similarity and long-range dependencies [12][13]. This can be quite a different scenario compared
to the trafﬁc patterns used in subsection 4.1. In what follows, we
present some experimental results when the network is simulated
under realistic traces which exhibit self-similarity.
We ﬁrst proﬁled an H263 video decoder application using different video clips to retrieve 9 real traces and recorded the arrival
data for the motion compensation module. A 4 × 4 network is then
constructed in which nine PEs are randomly picked to generate the
packets according to the corresponding input trace ﬁles. The remaining PEs in the network generate uniform trafﬁc. We incrementally increase the packet injection rates to mimic the case when the
system increases its speed of playing the video clips. The results
are shown in Fig. 4.
XY
OE −
fixed
Odd − Even
DyAD − OE
1000
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
800
600
400
200
XY
OE−
fixed
Odd− Even
DyAD− OE
200
150
100
)
l
s
e
c
y
c
(
l
y
a
e
d
t
e
k
c
a
p
e
g
a
r
e
v
A
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
Normalized video playing speed
0.04
50
0
0.005
0.01
Normalized video playing speed
0.015
0.02
0.025
(a)
(b)
Figure 4: Performance evaluation under multimedia trafﬁc
Plotted in Fig. 4(a) is the measured average communication latency of the packets and video playing speed. As we can see, the
results show the same trend as those derived under the non-uniform
trafﬁc patterns in subsection 4.1. DyAD-OE always performs the
best under all playing rates, while odd-even performs second best.
Fig. 4(b) shows a magni ﬁed view of Fig. 4(a) for the low speed
region (that is, the region corresponding to 0.01 to 0.025 playing
speed). It is interesting to note that unlike the simulation results using random trafﬁc patterns where DyAD-OE has the same latency
as XY and oe-ﬁxed , DyAD-OE enjoys now a shorter latency compared to that of XY and oe-ﬁxed , even at very low playing speeds.
This behavior is due to the bursty nature of the multimedia traces.
5. PROTOTYPE ROUTER DESIGNS
To evaluate the overhead of DyAD-OE, we have implemented
several designs and checked the actual performance/area trade offs.
More precisely, we want to see whether or not implementing a
DyAD-OE router requires an almost negligible additional cost compared to an odd-even router. We implemented all four versions
of routers (XY, oe-ﬁxed , odd-even and DyAD-OE) using a 0.16µm
technology, with a clock rate of 333MHz. In our designs, the FIFOs are implemented using registers in order to achieve better performance/power efﬁciency. Each input port has a ﬁxed link width
of 32 bits. The ﬂit size is set to be 32 bits as well. For each version
of routers, several design instances were synthesized with different
input FIFO capacities, starting from 2 ﬂits per input FIFO to 8 ﬂits
per input FIFO.
As shown by our design results, the overhead of implementing
the extra logic for DyAD-OE is indeed negligible compared with
odd-even implementation. For instance, for odd column routers
with FIFO size of 8 ﬂits, DyAD-OE requires 25,971 gates, while
odd-even router requires 25,891 gates (less than 1% overhead). In
fact, for all those designs (starting from FIFO size of 2 ﬂits to FIFO
size of 8 ﬂits), the overhead compared to odd-even is below 7%,
with an average overhead as small as 0.54%. (Please refer to [5]
for detailed comparison and layout plots.)
6. CONCLUSION AND FUTURE WORK
We presented a novel NoC routing idea (called DyAD) which
combines the low latency of the deterministic routing (at low network load) and the high throughput of the adaptive routing. Based
on this new concept, an instance of the DyAD-OE was designed
based on minimum odd-even routing. The simulation results show
that DyAD-OE consistently outperforms odd-even under different
trafﬁc loads/patterns and different network conﬁgurations. At the
same time, DyAD-OE enjoys the same low packet latency as deterministic routing when the network is not heavily loaded.
As explained in the paper, DyAD routing is a new concept rather
than a particular design or implementation choice. To achieve the
best performance, the conﬁguration of DyAD (e.g. which adaptive/deterministic routing should be used, the mode switching policy, etc.) should be customized to match the given application trafﬁc characteristics. This remains to be done as future work.
"
Operating-system controlled network on chip.,,"Operating-System Controlled Network on Chip∗
Vincent Nollet
nollet@imec.be
Th ´eodore Marescaux
marescau@imec.be
IMEC vzw.
Kapeldreef 75
3001 Leuven, Belgium
†
Diederik Verkest
verkest@imec.be
ABSTRACT
Managing a Network-on-Chip (NoC) in an eﬃcient way is
a challenging task. To succeed, the operating system (OS)
needs to be tuned to the capabilities and the needs of the
NoC. Only by creating a tight interaction can we combine
the necessary ﬂexibility with the required eﬃciency. This
paper illustrates such an interaction by detailing the management of communication resources in a system containing
a packet-switched NoC and a closely integrated OS. Our
NoC system is emulated by linking an FPGA to a PDA. We
show that, with the right NoC support, the OS is able to
optimize communication resource usage. Additionally, the
OS is able to diminish or remove the interference between
independent applications sharing a common NoC communication resource.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Packet switching networks; C.2.3 [Network Operations]: Network management, network monitoring
General Terms
Management, Measurement, Performance
Keywords
Network on Chip, Operating System, MP-SoC
1.
INTRODUCTION
In order to meet the ever-increasing design complexity,
future sub-100nm platforms [1] will consist of a mixture of
∗
Part of this research is funded by the European Commission (IST-AMDREL pro ject IST-2001-34379) by the Flemish Government (GBOU-RESUME pro ject IWT-020174RESUME) and by Xilinx Labs, Xilinx Inc. R&D group.
Also professor at the VUB and at the KU-Leuven.
†
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
DAC 2004, June 7–11,
 2004, San Diego, California, USA.
Copyright 2004 ACM 1-58113-828-8/04/0006 ...$5.00.
heterogeneous processing elements (PEs), also denoted as
tiles. These tiles will be interconnected by a Network-onChip (NoC) [1, 2, 5].
The general problem of mapping a set of communicating
tasks onto the heterogeneous resources of such a platform
while dynamically managing the communication between
the tiles is an extremely challenging task. In a NoC environment, the OS is able to control the inter-processor communication. This ability should ensure that granted compute
power matches the communication needs, in order to provide
the required quality of service.
The rest of the paper is organized as follows. Section 2
provides a description of the emulated target platform and
the employed NoC. Section 3 describes the diﬀerent communication management tools used by the OS. Section 4
details a case study, showing the communication interaction
between two distinct applications executing on the target
platform. It also illustrates how undesired behavior is managed by the OS. Finally Section 5 concludes. Related work
[1, 5, 6] is discussed throughout the paper.
2. SYSTEM DESCRIPTION
This section provides an overview of the system and of
the components involved in communication management.
2.1 Multiprocessor NoC Emulation
Our multiprocessor system is emulated by linking a StrongARM processor, present inside a Compaq iPAQ PDA, to an
FPGA by means of the iPAQ expansion port. The FPGA
contains the slave processors, the NoC and the master ISP
interface component (Figure 2).
Our packet-switched NoC, called data NoC, is implemented
as a 3x3 bidirectional mesh and is responsible for delivering
data packets for tasks executing on the PEs. A second NoC,
the control NoC, is used for OS-control messages [3] (Figure 1a). Separation of both NoCs ensures that application
data circulating on the data NoC does not interfere with OS
control messages. Both NoC’s are clocked at 30 MHz, while
the StrongARM processor, present in the PDA, is clocked
at 206 MHz.
2.2 NoC Network Layer
The presented data NoC is a packet-switched network.
The routers in the network use virtual cut-through switching
and output buﬀering. The deterministic routing algorithm is
based on a OS-reconﬁgurable lookup table. It guarantees inorder packed delivery, hence avoiding the packet reordering
overhead required by the proposal of Guerrier et al. [6].
2.3 NoC Transport Layer
2.3.1 Data Network Interface Component
The PEs of our SoC are interfaced to the packet-switched
data NoC by means of a data Network Interface Component
(dNIC). From the PE viewpoint the main role of the dNIC is
to buﬀer input and output messages and to provide a highlevel interface to the data router (Figure 1a). The dNIC
is also responsible for collecting communication statistics.
This involves keeping track of the number of messages sent,
received and blocked. The blocked message count denotes
the number of received messages, that were blocked in the
data router buﬀer while waiting for the PE input buﬀer to
be released. Moreover, the dNIC implements an injection
rate control mechanism, allowing control of the amount of
messages the attached PE injects in the data NoC per unit
of time [3].
Core
OS
Data NoC
Control NoC
Control
Router
Control
NIC
Data
Router
Data
NIC
Processing
Element (PE)
Core OS
Local OS
(1)
(8)
(5)
(4)
cNIC i
stub
cNIC i
stub
(2)
(7)
(6)
(3)
Control
NoC
Master ISP
Control
NoC
Slave i
a.
b.
Figure 1: (a) dNIC and cNIC connect the PE to
data and control NoCs respectively. (b) Remote execution of an OS function call on a slave node.
2.3.2 Control Network Interface Component
Each node in our system is also connected to a control
Network Interface Component (cNIC). A main role of the
cNIC is to provide the OS with a uniﬁed view of the communication resources. For instance, the message statistics
collected in the dNIC are processed and communicated to
the core OS by the cNIC (Figure 1a). The cNIC also allows
the core OS to dynamically set the routing table in the data
router or to manage the injection rate control mechanism of
the dNIC. Another role of the cNIC is to provide the core
OS with an abstract view of the distributed PEs. Hence, it
is considered as a distributed part of the OS. This role of
the cNIC is discussed in-depth in [3].
2.4 Operating System
One of the PEs of our multi-core SoC is denoted as master
because it executes the core of the OS. Besides monitoring
the behavior of the global system, this mainly involves assigning tasks to the other PEs in the system (the slaves ).
Additionally, every slave node contains limited local operating system functionality (Figure 2a).
The close interaction between the core of the OS and the
slave OS functionality, resembles classic remote procedure
calling (RPC). The operating system maintains for each
cNIC a structure that describes its functionality and that
allows the OS to remotely execute a function on a slave
node. So the cNIC structure in the OS can be seen as the
RPC stub. Figure 1b illustrates how the slave OS functionS
S
S
S
S
S
S
S
S
S
S
S
Master
ISP
S
S
S
a.
S
S
S
S
S
S
S
S
SS
Operating System
Base
RTOS
S
Master ISP
S
S
S
S
S
S
S
Computing resource
OS functionality
b.
Figure 2: The core OS solely executes on top of the
master ISP. The slaves (S) have their own local OS
functionality.
ality is used. First of all, the OS makes a function call to the
respective cNIC stub (1) . This stub translates the call into
a control message containing the desired function number
and its required parameters. Consequently, this message is
sent to the slave node (2) . Once the message is received on
the slave node (3) , its function number and parameters are
unwrapped and the respective local OS function executes
at the slave node (4) . The return value (5) is packed into
a message (6) , sent over the control network to the cNIC
stub, where it is unpacked (7) . Finally, the original core OS
function call returns with the respective return value (8) .
Certain network events (e.g. synchronization) require action from the core OS. The slave node initiates then a function call toward the core OS using the same mechanism.
3. NOC CONTROL TOOLS
This section illustrates the tools used by a distributed,
NoC-aware OS to manage SoC communication.
3.1 Dynamic Statistics Collection
The OS monitors NoC communication by polling the cNICs
through a remote function call to obtain the traﬃc statistics. (cf. Section 2.3). Statistics such as the blocked message count are important: these messages potentially disturb other data traﬃc sharing the same channel. Blocked
messages occur when the receiving PE is unable to process
its input fast enough.
The OS is able to solve this blocking issue by forcing the
source of the messages to send fewer messages per time unit
or by avoiding the congested link. The NoC tools that enable
these solutions are presented in the following sections.
3.2 Dynamic Injection Rate Control
By providing a message injection rate control function,
the dNIC allows the OS to limit the time wherein a certain
processor is allowed to send messages onto the network. This
time is called the send window of the processor. A similar
traﬃc rate control mechanism was used by Kumar et al. [1].
By setting the low and high value, the OS is able to describe a single send window within the whole send spectrum
(Figure 3a). However, by also using a modulo value, this single send window can be spread over the whole send spectrum
(Figure 3b and Figure 3c). Setting a window is deterministic and fast: it takes on average 57 µs to change the window
values of a certain PE.
a.
b.
c.
0
0
0
L
L
H
L
H
M
H
M
Send window, sending messages allowed
6), that consumes messages slower than they are produced.
The chosen production/consumption message ratio guarantees that the NoC router of the message sink reaches a saturation level and hence severely impacts other communication
ﬂows sharing the same channel (Figure 4).
T
T
T
Figure 3: The OS can specify the size, the location
and the amount of spreading of the ’send window’
by adjusting the low (L), high (H) and modulo (M)
value.
3.3 OS-controlled Adaptive Routing
The OS can also manage communication by changing the
routing table of a router in order to divert a message stream
from one channel to another. Since it is a complex operation,
the OS requires three steps to change a routing table. The
ﬁrst step performs a synchronization of every ﬂow passing
through router R on output O in order to clear these channels and thus assure in order delivery of messages. This
is achieved by sending a synchronization request to every
source task and waiting on a synchronization acknowledge
from the receiving side, indicating that the channel is empty.
The second step updates the routing tables.
In the third
step, the operating system notiﬁes all previously synchronized tasks to resume sending messages.
A synchronization/release remote function call does not
require any parameters and takes on average 53 µs. However, the actual time needed to perform a complete synchronization also depends on other parameters (e.g. channel
congestion). The remote function to change a routing table
takes as many parameters as there are entries in the routing
table. For our 3x3 network (9 entries), changing a routing
table requires on average 61 µs.
Note that changing a routing table aﬀects al l streams
passing through the router on the respective output. This
means, for example, that satisfying the quality-of-service request for a single application, will potentially have a (minor)
interference with another application.
4. NOC MANAGEMENT CASE STUDY
This section describes and characterizes two applications
that concurrently share communication resources of the NoC.
Additionally, this section illustrates how the OS can manage
communication interference between the applications.
4.1 Application Description
The main application in this case study is a Motion-JPEG
video decoder. It is composed of four tasks running concurrently on the PEs of the platform (Figure 4). Two of these
tasks, the sender and the receiver, run on the StrongARM
processor (tile 3). The two other tasks, are hardware blocks:
a task that performs the huﬀman decoding and the dequantisation, further denoted as Huﬀman block (tile 1), and a task
that performs a 2D-IDCT and a YUV to RGB conversion,
further denoted IDCT block (tile 8).
To evaluate the inﬂuence of communication interference
between applications, we also designed a synthetic application, composed of a message generator (tile 7), that produces traﬃc at a constant rate, and a message sink (tile
0
Huff
0
3
6
6
msg
sink
1
4
7
7
msg
gen.
2
5
8
8
IDCT
Video decoder communication
Synthetic appl. communication
Figure 4: Mapping of Motion-JPEG application and
synthetic traﬃc generating application on the platform. Communication channel 7 → 6 is shared.
The communication of the video decoder (without interference), has been characterized by means of the message
statistics (Figure 5). The core OS samples the relevant
cNICs once every 20 ms.
x 104
Stack Plot of IDCT
4
3.5
3
2.5
2
1.5
1
0.5
s
e
g
a
s
s
e
m
f
o
r
e
b
m
u
N
relative window size
blocked
received
sent
(Peak 1)
(Peak 2)
3.2
3.25
3.3
3.35
3.4
3.45
3.5
Time in OS Ticks (0.271 µ s per tick) 
3.55
3.6
x 109
Figure 5: Communication characterization of IDCT
block in MJPEG video decoder.
The same video sequence1 has been played twice with different windowing techniques. Peak(1) in Figure 5 has been
obtained by applying a window spreading technique (Figure
3(b,c)) whereas the second peak was obtained by allocating
continuous blocks of bandwidth (Figure 3a). In both cases
the window size gradually decreases from 100% (98.85 MB/s
when clocked at 50 MHz [4]) down to 0.0244% (approximately 25 KB/s). The window spreading technique clearly
performs better: the throughput of the video decoder application only starts to decrease when the OS diminishes its
eﬀective window to less than 2% of the total bandwidth and
reaches half of the throughput for a total allocated window
of less than 1.5% (about 1.5 MB/s). In the case of the nonspreading technique, half-throughput is reached as soon as
the allocated bandwidth is less than 75%. Adequate OS control of the communication can improve NoC performance by
a factor of 50!
The communication characteristics of the synthetic application, when using window spreading, are shown in Figure
1For Peak(2) we show statistics on a truncated videosequence because the quickly decreasing bandwidth considerably reduces the frame-rate and thus increases the time to
completion.
 
 
7b. As expected, the message sink blocks as many messages as it receives. Only when the allocated bandwidth is
decreased below 0.05% that the blocking behavior stops.
4.2 OS Communication Management
After placing the video application tasks, we have mapped
the message generator and message sink from the perturbing application on tiles 7 and 6 respectively (Figure 4). This
cation channel (7 → 6) it shares with the video decoding
way, the perturbing application will congest the communiapplication. Measurements have been performed for both
bandwidth allocation techniques: window-spreading (Figure
7) and block-allocation windows (Figure 6).
The eﬀect of diminishing window size is clear on the message sink task in the case of the continuous-window allocation: the amount of messages sent is directly proportional
to the injection rate window set (Figure 6b). Optimal Video
Decoder performance is obtained when less than 1% of the
total bandwidth is allocated to the message generator (Figure 6a, time interval [3.91e9;3.95e9]). The OS can trade-oﬀ
performance between both applications by changing their respective injection rates. When using the window-spreading
0
3.6
3.65
3.7
3.75
3.8
3.85
3.9
3.95
x 109
1
2
3
4
x 104
u
N
m
e
b
r
o
f
m
s
e
g
a
s
s
e
(a) Block Window Allocation: Stack Plot of IDCT
blocked
received
sent
0
3.6
3.65
3.7
3.75
3.8
3.85
3.9
3.95
x 109
500
1000
1500
2000
u
N
m
e
b
r
o
f
m
s
e
g
a
s
s
e
(b) Block Window Allocation: Stack Plot of Message Sink
IDCTwin
Genwin
blocked
received
Figure 6: BW allocated in blocks on channel 7 → 6.
technique, the eﬀect of diminishing the total window size
is not directly proportional to the bandwidth allocated and
the trade-oﬀs obtained in the previous case are not possible
(Figure 7b). However, using window-spreading has other advantages: jitter is greatly reduced because communications
are evenly spread over time. Moreover, a proper window setting can hide the latency of the receiver side and completely
suppress blocking on the network. In Figure 7b at the OS
time-stamp 241e7, the message sink task no longer causes
message blocking in the NoC. This happens when the window of the message generator is less than 0.02% of the total
bandwidth. Note that the message sink, is not disturbed
by this window reduction: it still consumes 40000 messages
per second. The OS has simply matched the window size to
the optimal sending rate in the perturbing application. As
a consequence, thanks to the bandwidth saved by the OS,
the video decoder reaches its optimal frame-rate.
Besides the injection rate control mechanism, the OS can
also solve interference issues between applications in other
ways. First of all, as Section 3.3 explains, it is possible to
avoid the congested link by rerouting the video application
stream. Additionally, the OS can decide to dynamically
migrate the message generator task to another node in the
NoC.
2.28
2.3
2.32
2.34
2.36
2.38
2.4
2.42
2.44
x 109
0.5
1
1.5
2
2.5
3
3.5
x 104
u
N
m
e
b
r
o
f
m
s
e
g
a
s
s
e
(a) Spreading Window Allocation: Stack Plot of IDCT
1−gen win
blocked
received
sent
0
2.28
2.3
2.32
2.34
2.36
2.38
2.4
2.42
2.44
x 109
500
1000
1500
2000
u
N
m
e
b
r
o
f
m
s
e
g
a
s
s
e
(b) Spreading Window Allocation: Stack Plot of Message Sink
genwin
blocked
received
Figure 7: Window spreading on channel 7 → 6.
5. CONCLUSION
This paper features an OS that can manage communication on a packet-switched Network-on-Chip (NoC). Thanks
to its tight coupling with OS-support functionality distributed
in the NoC, the OS can optimize communication resource allocation and thus minimize interaction between concurrent
applications. The three main OS tools provided by the NoC
are: the ability to collect data traﬃc statistics, the ability to
limit the time interval in which a processing element (PE)
is allowed to send (called injection rate control) and, ﬁnally,
the ability to dynamically adapt the routing in the NoC. A
case study of a video decoding application and a synthetic
traﬃc generator, running on our multi-processor emulation
platform, illustrates how the OS can optimize channel usage
(by a factor of 50) and manage communication interference
using the injection rate control tool.
6. ADDITIONAL AUTHORS
Additional authors: Jean-Yves Mignolet and Serge Vernalde, IMEC v.z.w., email: (mignolet,vernalde)@imec.be.
7. "
An efficient scalable and flexible data transfer architecture for multiprocessor SoC with massive distributed memory.,,"An Efficient Scalable and Flexible Data Transfer 
Architecture for Multiprocessor SoC with Massive 
Distributed Memory 
Sang-Il Han*,**, Amer Baghdadi***, Marius Bonaciu**, Soo-Ik Chae*, Ahmed. A. Jerraya**
*Department of Electrical Engineering, 
 Seoul National Univ., Seoul, Korea 
**SLS Group, TIMA Laboratory, 
Grenoble, France 
{sihan,chae}@sdgroup.snu.ac.kr 
{marius.bonaciu,ahmed.jerraya}@imag.fr 
***Electronics Department,  
ENST Bretagne, Brest, France 
amer.baghdadi@enst-bretagne.fr
ABSTRACT 
Massive data transfer encountered in emerging multimedia 
embedded applications requires architecture allowing both highly 
distributed memory structure and multiprocessor computation to be 
handled. The key issue that needs to be solved is then how to 
manage data transfers between large numbers of distributed 
memories. To overcome this issue, our paper proposes a scalable 
Distributed Memory Server (DMS) for multiprocessor SoC 
(MPSoC). The proposed DMS 
is composed of: (1) highperformance and flexible memory service access points (MSAPs), 
which execute data transfers without intervention of the processing 
elements, (2) data network, and (3) control network. It can handle 
direct massive data transfer between the distributed memories of 
an MPSoC. The scalability and flexibility of the proposed DMS 
are illustrated through the implementation of an MPEG4 video 
encoder for QCIF and CIF formats. The experiments show clearly 
how DMS can be adapted to accommodate different SoC 
configurations 
requiring various data 
transfer bandwidths. 
Synthesis results show that bandwidth can scale up to 28.8 GB/sec. 
Categories and Subject Descriptors: B.4.3 
[Input/Output and Data Communications]: Interconnections 
(Subsystems); B.8 [Hardware]: Performance and Reliability 
General Terms: Design, Performance, Experimentation. 
Keywords: Multiprocessor SoC, Message passing, Data 
transfer architecture, Memory Server, Network on chip, Network 
Interface. 
1. Introduction 
Current multimedia and telecommunication applications require 
complex high-performance multiprocessor SoC (MPSoC). Raising 
communication abstraction level and decoupling communication 
from computation have been proposed as the solution to master the 
design complexity of MPSoC [1]. To that end, high level 
programming models, such as message passing [17][18], are quite 
appropriate. However, to achieve the required high-performance, 
more and more efficient architectures able to handle message 
passing programming models are still required. The main 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC 2004, June 7–11, 2004, San Diego, California, USA 
Copyright 2004 ACM 1-58113-828-8/04/0006…$5.00. 
constraint that such architectures should deal with is to handle 
high data transfer rates encountered in emerging complex 
applications.  
The data transfer architecture should be scalable to handle 
applications of varying complexity requiring a different number of 
processors, different bandwidth and/or different latency. The busbased MPSoC fails to scale well because it employs global order of 
data transfer defined by central arbitration and the limited number 
of data transfer. In order to overcome this issue, we adopt two 
assumptions: 1) the read and write operations of a data transfer 
should be decoupled, and 2) the data transfer engines should be 
distributed to allow massive data transfers concurrently. These 
two assumptions have already been employed 
in classical 
massively parallel processors architectures [2] and the work 
presented here adapts this concept to multiprocessor SoC with 
massively distributed memory. The key contribution is to allow 
customization of the communication infrastructure. 
The data transfer architecture should also be flexible to enable 
designs of different configurations in order to (1) fit the precise 
application needs for low-cost implementation and (2) adapt 
different kinds of computation and memory subsystems. This 
flexible data transfer architecture can be built using a componentbased design methodology [3][4]. 
This paper proposes an efficient scalable and flexible data 
transfer architecture for MPSoC with massive distributed memory. 
The architecture is organized as Distributed Memory Server 
(DMS) that consists of high-performance and flexible memory 
service access points (MSAPs), data network, and control 
network. The MSAP acts as Network Interface and Data Transfer 
Engine. Synthesis results show that a typical MSAP provides 
efficient results in terms of size and throughput. The scalability of 
the proposed DMS will be illustrated through two implementations 
of an MPEG4 video encoder. The experiments show clearly how 
the DMS can be adapted to accommodate different multiprocessor 
SoC configurations requiring different data transfer bandwidths. 
The rest of the paper is organized as follows. Section 2 presents 
state of the art on data transfer architectures. Section 3 presents the 
scalable data transfer architecture (i.e. DMS) model. Section 4 
details the high-performance and flexible memory service access 
point (MSAP). Section 5 presents several experiments to analyze 
the efficiency of the proposed DMS. Finally, section 6 gives our 
conclusions on the work presented in this paper. 
2. Related work on data transfer architecture 
Massive data transfer architectures are well handled in the 
classical massively parallel processors systems. Machines [2] like 
nCUBE, CM-5, 
Intel Paragon, and Meiko CS-2 
include 
sophisticated mechanism 
for direct data 
transfer between 
distributed memories. These architectures are generally scalable 
 
and support performance-efficient data transfers between large 
numbers of memories. However, all of them lack flexibility and 
have 
little concern about 
resource constraints. So 
these 
architectures are not suitable for data transfer in SoC that should 
satisfy severe performance, area, power, and cost constraints. 
These constraints can be achieved only when the communication 
structure can be customized to each specific application. 
Conventional DMA engines [5] support efficient data transfers, 
but they can execute only sequential data transfer. Multiple DMA 
engines used in classical multiprocessor system enable the 
execution of multiple concurrent data transfers, however such a 
solution will not be cost-effective because the architecture is not 
flexible. This will be shown in section 5. The DMA engines in 
[6][7] support performance-efficient and concurrent data transfers. 
But the scalability of data transfers is limited by the use of a 
unique global arbiter.  
Several on-chip network [10][11] and network interfaces [8][9] 
handle data transfers between distributed memories on a chip. 
These 
interfaces support scalable data transfers. The main 
restriction of existing NoC solutions is the restrictions of the 
number of channels used to link computation or memory nodes to 
the network. This generally introduces congestion in the nodes 
using the network. A typical case is a slow computation node 
inducing saturation of the shortage capacity of the network and 
then blocking all data transfers in the network. In order to avoid 
that, highly flexible access to the network is required. 
Additionally, several academic and industrial research projects 
proposed high performance architectures for massive data transfer 
applications. Among those we can cite Philips Nexperia™ DVP 
[12], and TI OMAP™ [13]. Even if their architectures are highly 
programmable, they lack scalability. 
The key contribution of this paper is to provide both scalable 
and flexible architecture allowing the specific needs of each 
application to be accommodated. It combines the scalability of 
NoC solutions with flexibility of component-based design to 
customize the data transfer architecture. 
3. The Distributed Memory Server 
Distributed memory server (DMS) acts as a server that services 
subsystems by 
executing data 
transfers between 
their 
corresponding memories. 
Figure 1 shows a global view of an MPSoC architecture using 
DMS. The system is composed of computation subsystems, 
memory subsystems and a DMS. A computation subsystem is 
composed of one processing element (PE), local memory and local 
bus. A memory subsystem consists of one or more memories. All 
subsystems are connected to the DMS through Memory Service 
Access Points (MSAP).  
Subsystem 0
Subsystem 0
Local Bus 0
Local Bus 0
Subsystem k
Subsystem k
Local Bus k
Local Bus k
Subsystem n
Subsystem n
m P
m P
MSAP
MSAP
…
…
…
…
m IP
m IP
MSAP
MSAP
M…
M…
…
…
MSAP
MSAP
…
…
…
…
Control Network
Control Network
Data Network
Data Network
DMS
DMS
m
m
P
P
M
M
IP
IP
Local memory
Local memory
Processor
Processor
Global memory
Global memory
IP
IP
MSAP
MSAP
Memory Service 
Memory Service 
Access Point
Access Point
Data
Data
Control
Control
Figure 1. MPSoC with the DMS 
The DMS is composed of MSAPs, control network and data 
network. Each subsystem is connected to a specific MSAP that 
allows data transfer from its local memories to other subsystems 
and from other subsystems to its local memories. Data transfers are 
requested by subsystems using the control network. MSAP 
delivers transaction requests ordered by its subsystem and 
synchronization information via control network. It also executes 
data transfer between its local memory and other subsystem’s 
memory via data network.  
The proposed DMS allows: 
1) Decoupling of read and write operations: MSAP 
receives a data transfer request from the attached PE and 
splits it into local request (read or write) and remote request 
(write or read). Then, it sends the remote requests to the target 
MSAPs via the control network.  
2) Distributing execution requests: the distributed MSAPs 
execute the memory-to-memory data transfers ordered by a 
distributed scheduler (each MSAP includes an independent 
local scheduler).  
The proposed DMS decouples computation from communication 
through the MSAP and allows parallel execution of computation 
and communication through the processing power of MSAP. 
In this paper, we use an AMBA bus for the control network and a 
partial point to point interconnect for data network. The proposed 
DMS has no assumption about control network and data network. 
MSAP can accommodate any type and any combination of control 
and data networks, e.g. bus, full point-to-point link, packet switch, 
and circuit switch. The structure of the networks themselves is out 
of the scope of this paper. We will discuss only the connection 
between the networks and the MSAPs.  
4. Memory Server Access Point 
A Memory Server Access Point (MSAP) acts as a Network 
Interface that provides the link between NoC and subsystems 
(Figure 1). It also acts as a data transfer engine that transfers data 
between the attached memory and other memories of other 
MSAPs.  
Figure 2 shows global structure of an MSAP. It is composed of 5 
basic functions: 
MSAP
MSAP
Data
Data
Network
Network
Control
Control
Network
Network
r
r
e
e
p
p
p
p
a
a
r
r
W
W
r
r
r
e
e
e
p
p
p
p
p
p
a
a
a
r
r
r
W
W
W
Memory
Memory
Activator
Activator
Remote
Remote
Request
Request
Acceptor
Acceptor
Memory
Memory
Scheduler
Scheduler
Local
Local
Request
Request
Acceptor
Acceptor
r
r
e
e
p
p
p
p
a
a
r
r
W
W
r
r
e
e
p
p
p
p
a
a
r
r
W
W
mem
mem
PE
PE
Subsystem
Subsystem
Figure 2. Basic functions of the MSAP 
1) Local Request Acceptor (LRA): It accepts data transfer 
requests from the attached PE and splits them into local 
requests and remote requests. It sends the remote requests to 
the target MSAPs via control network. (A data transfer is 
composed of local read (write) operations and remote write 
(read) operations.) 
2) Remote Request Acceptor (RRA): It receives the remote 
requests from other MSAPs.  
3) Memory Scheduler (MS): It schedules the received requests 
from LRA or RRA.  
4) Memory Activator (MA): It executes read (write) request 
selected by its memory scheduler. 
 
 
5) Wrappers: they adopt the MSAP to PE, memory, data 
network and control network. 
The different parts can be customized and assembled using a 
flexible scheme to 
- Accommodate different networks 
subsystems thanks to wrappers. 
- Accommodate different communication bandwidths and 
latency thanks to modular decomposition in separate basic 
functions. 
The flexibility and configuration of MSAP will be explained in 
the next section. Not all of the basic functions are required for all 
possible configurations. 
and heterogeneous 
4.1 MSAP architecture 
Figure 3 details the general architecture of the MSAP. Each 
component will be explained in the rest of this section. 
The MSAP is connected to the data network through data 
input/output ports. A data channel is established by linking a data 
input port and a data output port. The MSAP can transfer data to 
another MSAP through a data channel. For the control ports, 
control channels and the control network, we have exactly the 
same behavior. 
Each data port has a FIFO buffer that connects the Memory 
Activator (MA) to the data network. Each control port has two 
FIFO buffers: one connects Local Request Acceptor (LRA) to the 
control network and the other connects Remote Request Acceptor 
(RRA) to the control network. Note that RRA and LRA are 
separated in order to avoid unnecessary instantiation (flexibility). 
Data output 
Data output 
ports
ports
Data input 
Data input 
ports
ports
Data Network
Data Network
Ctrl input 
Ctrl input 
port
port
Ctrl Output
Ctrl Output
port
port
Control Network
Control Network
DNW
DNW
…
…
DNW
DNW
DNW
DNW
…
…
DNW
DNW
CNW
CNW
CNW
CNW
CONN, 
CONN, 
COMM
COMM
NETID,
NETID,
DONE
DONE
NETID,
NETID,
DONE
DONE
CONN, 
CONN, 
COMM
COMM
…
…
…
…
…
…
…
…
MA
MA
MW
MW
Local
Local
memory
memory
READY
READY
EXIT
EXIT
DONE
DONE
MS
MS
DONE
DONE
Local 
Local 
bus
bus
CHAIN
CHAIN
TD
TD
TD
TD
RRA
RRA
LRA
LRA
PW
PW
Processor
Processor
MSAP
MSAP
Figure 3. General detailed architecture of the MSAP 
The buffers can be operated by two clocks: 1) data or control 
network clock and 2) subsystem clock. The dotted lines on the 
buffers in Figure 3 shows three clock domains, i.e. data network 
clock domain, control network clock domain and subsystem clock 
domain. The separation of network clock domain from subsystem 
clock domain allows integrating subsystems with different clock 
frequency with DMS. 
4.1.1 Local Request Acceptor 
LRA is connected to the PE of a subsystem through PE Wrapper 
(PW) in order to receive and handle the requests of the local 
subsystem.  
LRA 1) receives connection setup/release requests and data 
transfer requests from the attached PE, 2) reports the status of 
requests to the attached PE, 3) sends remote requests to the target 
RRAs via the control network, and 4) sends a Transfer Descriptor 
(TD), to the Memory Scheduler (MS). A TD contains the 
information for the data transfer, i.e. memory address, transfer size, 
transfer mode, port number, and block transfer mode. 
MSAP does not need an LRA if the attached subsystem is a 
memory subsystem or slave computation subsystem.  
Possible configurations of the LRA include interrupt generation, 
scatter/gathering (i.e. automatic link of data transfers), and block 
transfer (e.g. macro-blocks in video encoder). 
4.1.2 Remote Request Acceptor 
The RRA is connected to the control network through a CNW in 
order to receive and handle the requests from LRAs.  
The RRA 1) receives connection setup/release and data transfer 
requests from other LRAs (including LRA of the same MSAP), 2) 
sends a TD in data transfer request to Memory Scheduler, and 3) 
makes acknowledgement of the received requests. 
The MSAP doesn’t need an RRA if the memory of the attached 
subsystem is not accessed by other LRAs.  
4.1.3 Data Queue 
A Data queue between the Memory Activator and a Data 
Network Wrapper hides delay jitter for avoiding the unnecessary 
activations of the data network and memory.  
A data queue provides warning signal. The warning signal is set 
or cleared according to its five latency levels: not empty, 1/4 filled, 
1/2 filled, 3/4 filled and not full. The Memory Scheduler uses the 
warning signal for avoiding unnecessary context switch. 
Possible configurations of a Data Queue include its size and 
latency level. 
In addition, the chain queue (CHAIN) shown in Figure 3 
contains the next transfer descriptor for scatter/gathering feature 
mentioned in section 4.1.1. 
4.1.4 Control Queue 
A control queue between the LRA (or RRA) and the control 
network hides the latency due to the contention of the control 
network. The possible configuration of a Control Queue concerns 
its size. 
In addition, the completion queue (DONE) shown in Figure 3 
contains the descriptors of the completed data transfer.  
4.1.5 Memory scheduler 
The Memory Scheduler contains a set of registers (called port 
context) describing the status of each data port. A port context 
consists of 6 registers as shown in Figure 4.  
Content
Content
Local memory address (laddr)
Local memory address (laddr)
Local memory address (laddr)
Remote memory address (raddr)
Remote memory address (raddr)
Remote memory address (raddr)
Chain address (caddr)
Chain address (caddr)
Chain address (caddr)
Transfer configuration (ch_cfg)
Transfer configuration (ch_cfg)
Transfer configuration (ch_cfg)
Remote control network ID (rcnetid)
Remote control network ID (rcnetid)
Remote control network ID (rcnetid)
Remote data network ID (rdnetid)
Remote data network ID (rdnetid)
Remote data network ID (rdnetid)
Offset
Offset
0x0
0x0
0x4
0x4
0x8
0x8
0x10
0x10
0x14
0x14
0x18
0x18
Figure 4. Context registers of a data port 
Memory Scheduler 1) selects one of the port contexts according 
to its scheduling policy, 2) sends the selected context to the local 
Memory Activator, and 3) updates the port contexts according to 
the received TDs.  
Possible configurations of the MS include scheduling policy, 
priority, the size of ready queue and the number of MA. 
4.1.6 Memory activator 
Memory Activator (MA) receives an active context from 
Memory Scheduler and executes it. 
 
 
 
 
MA 1) executes the data transfer by generating memory address, 
memory control signals and queue control signals according to the 
received context, 2) switches context when the data transfer is 
completed, a preemption condition is occurred, or the data queue is 
empty (read operation) or full (write operation).  
Figure 5 shows the detailed connections between the MA and 
the data input ports. 
Data 
Data 
Network
Network
Data input 
Data input 
ports
ports
Network clock
Network clock
Local clock
Local clock
nEMPTY
nEMPTY
WEN
WEN
nEMPTY
nEMPTY
REN
REN
nEMPTY
nEMPTY
DNW
DNW
…
…
DNW
DNW
nFULL
nFULL
REN
REN
…
…
WEN
WEN
nEMPTY
nEMPTY
REN
REN
nFULL
nFULL
REN
REN
MA
MA
MWEN
MWEN
MWDATA
MWDATA
MREADY
MREADY
Memory
Memory
or 
or 
Local bus
Local bus
nEMPTY
nEMPTY
RDATA
RDATA
REN
REN
MUX
MUX
Figure 5. Data flow between a subsystem and the data network 
The MSAP can have several MAs for concurrent data transfers 
if the subsystem has several memories, e.g. scratch-pad memories 
[15][16]. So the possible configuration of the MA concerns the 
number of ports. 
4.1.7 Wrappers 
An MSAP can be connected to four kinds of components: PE, 
memories, data network and control network. For flexible 
instantiation of MSAP, four kinds of wrapper are required, i.e. PE 
Wrapper (PW), Memory Wrapper (MW), Data Network Wrapper 
(DNW) and Control Network Wrapper respectively. 
A PW converts the protocol of LRA to the memory access 
protocol of PE. In the current version, the protocol of the LRA is 
the AMBA protocol.  
An MW provides the logical interface of memory to an MA. If 
the attached memory is a local memory and it is connected to a 
local bus, the memory wrapper converts the local bus protocol to 
the queue protocol or vice versa. If the memory is global, e.g. an 
embedded SRAM, an embedded flash memory, or an embedded 
DRAM, the MW is a physical memory controller [7]. 
DNW and CNW convert the queue protocol to the network 
protocol or vice versa. For example, if data network is packetswitch, DNW is packetsizer or de-packetizer. 
4.2 Data transfer Models 
Three procedures are required to transfer data between two 
MSAPs. 
1) Connection setup: the two MSAPs open a channel by 
reserving two ports and exchange the data network IDs of 
these ports. 
2) Data transfer: the two MSAPs transfer the data via the 
channel.  
3) Connection release: the two MSAPs release the two ports of 
the channel. 
We assume that the control network ID of the target port is 
known before the connection setup. 
Figure 6 shows an example of a data transfer from a global 
memory to a local memory. The procedure of the data transfer is as 
follows. 
(1) The processor writes the data transfer information into its 
control registers. (see Figure 4) 
(2),(3),(4),(5) The LRA sends a command (COMM) request to 
the RRA of the global memory via the control network. It also 
sends a TD to the local MS. 
(6) The RRA parses the COMM request and sends the 
corresponding TD to its MS. 
(7) The MS schedules the received TD. 
(8),(9) The MA of the global memory transfers data from the 
global memory to the data queue according to the received TD.  
(10) The data (of the data queue of the global memory) is 
transferred to the data queue of the local memory via the data 
network. 
(11),(12) The MA of the local memory transfers the data from 
the data queue to the local memory according to the received TD. 
If the TD is about write operation, the memory scheduler sends 
DONE signal to the LRA and the RRA for synchronization after 
transfer completion.  
MSAP2
MSAP2
MSAP2
MSAP2
MSAP2
MSAP2
P,m
P,m
P,m
M
M
M
(5)
(5)
(5)
(6)
(6)
(6)
(7)
(7)
(7)
(8)
(8)
(8)
(9)
(9)
(9)
(4)
(4)
(4)
DNW
DNW
DNW
DNW
DNW
DNW
(7)
(7)
(7)
(7)
(7)
(7)
M
M
M
M
M
M
(8)
(8)
(8)
(8)
(8)
(8)
(4)
(4)
(4)
(4)
(4)
(4)
(10)
(10)
(10)
(10)
(10)
(10)
(9)
(9)
(9)
(9)
(9)
(9)
(11)
(11)
(11)
(11)
(11)
(11)
(7)
(7)
(7)
(7)
(7)
(7)
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
(3)
(3)
(3)
(3)
(3)
(3)
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
DNW
DNW
DNW
DNW
DNW
DNW
(5)
(5)
(5)
(5)
(5)
(5)
(6)
(6)
(6)
(6)
(6)
(6)
RRA
RRA
RRA
RRA
RRA
RRA
MS
MS
MS
MS
MS
MS
(1)
(1)
(1)
(2)
(2)
(2)
(3)
(3)
(3)
DNW CNW
DNW CNW
DNW CNW
RRA MS
RRA MS
RRA MS
MA
MA
MA
MA MS LRA
MA MS LRA
MA MS LRA
CNW DNW
CNW DNW
CNW DNW
MSAP1
MSAP1
MSAP1
MSAP1
MSAP1
MSAP1
(12)
(12)
(12)
(12)
(12)
(12)




















































































Figure 6. A data transfer example between two MSAPs 
In distributed memory architectures, data consistency is an 
important issue. DMS assumes that this issue is considered by a 
higher-level protocol. That means, if the DMS is used in a 
message-passing architecture, the message-passing protocol will 
solve this issue [19]. 
COMM
COMM
COMM
COMM
COMM
COMM
Network
Network
Network
Network
Network
Network
(11)
(11)
(11)
(12)
(12)
(12)
MS
MS
MS
MS
MS
MS
LRA
LRA
LRA
LRA
LRA
LRA
(2)
(2)
(2)
(2)
(2)
(2)
(a)
(a)
(a)
MSAP1
MSAP1
MSAP1
Network
Network
Network
m
m
m
m
m
m
P
P
P
P
P
P
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
CNW
(10)
(10)
(10)
(b)
(b)
(b)
MSAP2
MSAP2
MSAP2
(1)
(1)
(1)
(1)
(1)
(1)
4.3 Flexible configuration of the MSAP 
The flexible architecture of 
the MSAP enables various 
configurations that allow to 1) integrate different networks and 
heterogeneous 
subsystems, 
2) 
accommodate 
different 
communication bandwidths and latency, and 3) implement DMS at 
low-cost. 
MSAP is able to adapt various types of PEs, memories, control 
networks, and data networks 
to 
the DMS 
through 
the 
corresponding wrapper, 
i.e. PW, MW, CNW and DNW. 
Furthermore, it is able to integrate various clock domains of 
subsystems and networks through queues that are operated by two 
clock frequencies. 
To accommodate such various data transfers, MSAP allows the 
configuration of the number of MAs, the number of ports of each 
MA, the queue size of each port, the priority of each port, the 
latency level of each port, and the scheduling policy.  
Besides that, to customize the MSAP according to a specific 
functionality, the MSAP can be configured to include or not the 
following components: RRA, LRA (Interrupt generation, Block 
Transfer, and scatter/gathering). 
Finally, the automatic configuration and generation of the DMS 
is important to reduce the design time. A component-based design 
methodology could be a proper approach. We believe that the 
proposed architecture model for the DMS is already suitable for a 
systematic design. Automation is out of the scope of the work 
presented in this paper. Yet, it concerns our future work. 
 
 
 
 
5. Experiments 
Several experiments were conducted in order to verify the 
scalability and flexibility of DMS under the parameters and 
environment shown in Table 1.  
Language 
Technology 
Compiler 
Control Network 
Data network 
Clock freq. 
SystemC RTL description 
TSMC (slow) 0.18µm 
Synopsis Design Compiler 
AMBA bus 
Point-to-point 
200 MHz 
Table 1. Parameters and environment of the experiments 
First, to evaluate the flexibility of MSAP, we have synthesized 
several MSAPs by using various configurations. Table 2 shows the 
area for each configuration. The basic configuration  (BASIC) of 
an MSAP is the one with only MA, MS, CNW, DNW, Data queue 
and Control queue. The INTR, BLOCK, CHAIN, and TWO_PRIO 
represent a configuration with interrupt feature, block transfer 
feature, linked-list feature, and two priority feature of MSAP 
respectively. 
# Configuration MSAP w/o 
data queue 
1 BASIC+RRA 
0.129 
2 BASIC+LRA 
0.200 
3 +RRA 
0.235 
4 +INTR 
0.240 
5 +BLOCK 
0.245 
6 +CHAIN 
0.321 
7 +TWO_PRIO 
0.348 
Data queue 
0.103 
0.103 
0.103 
0.103 
0.103 
0.121 
0.121 
Total area 
0.233 
0.303 
0.338 
0.344 
0.349 
0.442 
0.470 
Table 2. Areas (mm2) of an MSAP for various configurations 
For all of these configurations, we fixed the number of data ports 
to 4 and we used a 2-port, 32bit wide and 16-word deep register 
file as data queue. In this case, the MSAP (with four data ports) 
has an aggregate bandwidth of 4×200MHz×4bytes = 3.2 GB/sec. 
However, in general case, the bandwidth of MSAP is limited by 
the memory bandwidth. This result shows how configuration may 
affect heavily the cost of MSAP. 
The scalability of the number of data ports is also a key feature 
of MSAP. The synthesis results of several MSAPs with different 
numbers of ports are shown Table 3. For all of them, we used the 
configuration number 5 of Table 2. 
The MSAP compares favorably to classical DMA. The gate 
count (NAND2 equivalent) of a commercial DMA engine [5] (8 
channels, 32bit*4 words data queue per each port, with 
scatter/gathering feature) is about 82k. The gate count of the 
proposed data transfer engine (8 channels, 32bit*8 words data 
queue per each port, with scatter/gathering feature) is about 68k, so 
it is about 20% smaller than the commercial DMA. 
Port number MSAP w/o data queue 
1 
0.136 
2 
0.177 
4 
0.245 
8 
0.394 
16 
0.685 
Data 
queue 
0.026 
0.052 
0.103 
0.207 
0.413 
Total 
area 
0.161 
0.229 
0.349 
0.600 
1.099 
Table 3. Areas (mm2) of an MSAP about port number 
Second, to verify the scalability and flexibility of DMS, we 
conducted two other experiments. One is the design of an MPEG4 
[14] encoder that can encode QCIF image at 25 frames/sec by 
using 4 ARM7 processors. The other is the design of an MPEG4 
encoder for CIF image by using 16 ARM7 processors. 
MPEG4 Encoder
MPEG4 Encoder
MPEG4 Encoder
Q
Q
Q
DCT
DCT
DCT
Fc
Fc
Fc
ME
ME
ME
MC
MC
MC
Fn-1
Fn-1
Fn-1
IQ
IQ
IQ
IDCT
IDCT
IDCT
Fn
Fn
Fn
Net
Net
Net
ZIG
ZIG
ZIG
VLC
VLC
VLC
Xth processor
Xth processor
Xth processor
Px
Px
Px
M Global 
M Global 
M Global 
Memory
Memory
Memory
Net
Net
Net
Cam
Cam
Cam
Net
Net
NetNet
Cam
Cam
CamCam
M
M
MM
P1
P1
P1P1
Cam
Cam
Cam
Camera
Camera
Camera
Network 
Network 
Network 
Device
Device
Device
P3,P4
P3,P4
P3,P4
P2P2
P2P2
P2P2
P2
P2
P2
(a)
(a)
(b)
(b)
(b)
Figure 7. (a) Block diagram, (b) System-level architecture of an 
MPEG4 system for QCIF format 
Figure 7 (a) shows the Block diagram of an MPEG4 system. The 
MPEG4 encoder receives images from a camera, encodes the 
images and then sends the encoded bit-stream to network device, 
e.g. wireless LAN. Figure 7 (b) shows 
the system-level 
architecture of an MPEG4 system for QCIF format. Each node 
represents a subsystem and each edge represents an abstract 
channel. A camera writes a part of image from its buffer to the 
global memory. Each processor reads a part of image from the 
global memory and executes encoding algorithm (ME, MC, DCT, 
IDCT and so on). P1 makes a bit-stream from the results by 
executing VLC and sends it to network device. The structure of 
MPEG4 system for CIF format is hierarchical combination of 
MPEG4 system for QCIF format. 
Figure 8 shows the corresponding implementation as an MPSoC 
with the DMS architecture. 
Property 
# of MSAP 
# of port 
Area of DMS  
Area over DMAs+NIs  
Area over DMAs  
Aggregate bandwidth 
Latency of read transfer 
Latency of write transfer 
DMS for 
MPEG4 QCIF 
7 
18 
1.61 mm2 
45% 
86% 
7.2 GB/sec 
10 
6 
DMS for 
MPEG4 CIF 
22 
72 
5.93 mm2 
41% 
80% 
28.8 GB/sec 
10 
6 
Table 4. Experimental results of two DMS versions 
Table 4 summarizes the results obtained from the two DMS 
implementations. These results show clearly 
the bandwidth 
scalability and the area efficiency of the proposed DMS. 
 
 
 
 
 
 
Local bus 0
Local bus 0
Local bus 1
Local bus 1
m
m
Net
Net
Ctrl
Ctrl
m
m
Cam
Cam
Ctrl
Ctrl
M
M
LRA
LRA
MA
MA
MS
MS
MA
MA
MS
MS
MA
MA
RRA
RRA
Local bus 2
Local bus 2
m
m
P1
P1
Local bus 3
Local bus 3
Local bus 3
Local bus 4
Local bus 4
Local bus 4
Local bus 5
Local bus 5
Local bus 5
m
m
m
P2
P2
P2
m
m
m
P3
P3
P3
m
m
m
P4
P4
P4
MA
MA
MS
MS
RRA
RRA
LRA
LRA
MS
MS
RRA
RRA
LRA
LRA
LRA
LRA
LRA
LRA
MA
MA
MS
MS
MA
MA
MS
MS
MA
MA
MS
MS
RRA
RRA
RRA
RRA
RRA
RRA
DNW CNW
DNW CNW
DNW CNW
DNW CNW
DNW DNW DNW DNW DNW CNW
DNW DNW DNW DNW DNW CNW
DNW DNW DNW DNW DNW CNW
DNW DNW DNW DNW DNW CNW
DNW
DNW
DNW
DNW
CNW
CNW
DNW
DNW
DNW
DNW
CNW
CNW
DNW
DNW
DNW
DNW
CNW
CNW
Control Network
Control Network
Data Network
Data Network
6. Conclusion 
In this paper, we described a Distributed Memory Server (DMS) 
to handle massive data transfers between large numbers of 
distributed memories on MPSoC. It consists of high-performance 
and flexible Memory Service Access Points (MSAPs), data 
network and control network. The DMS provides 1) performanceefficient data transfer mechanism between subsystems of MPSoC, 
2) a scalable solution to handle a large application field, and 3) 
flexible designs to integrate with heterogeneous subsystems at lowcost. 
An MSAP, which is the main component of the DMS, receives 
data transfer requests from the attached subsystem of an MPSoC, 
schedules them and then executes the received data transfer 
requests concurrently. Synthesis results of different configurations 
show clearly the flexibility of the proposed architecture of MSAP. 
Compared to a conventional DMA engine of similar functionality, 
an MSAP reduce the area by about 20%. A typical instance of 
MSAP runs at 200Mhz, occupies 0.349 mm2 in a 0.18 µm 
technology, and has an aggregate bandwidth that scales up to 3.2 
GB/sec. 
The scalability and flexibility of the proposed DMS are 
illustrated through the implementation of an MPEG4 video 
encoder for QCIF and CIF formats. The experiments illustrate how 
DMS can be adapted to accommodate different SoC configuration 
requiring different data transfer bandwidths. The DMS for CIF 
requires 22 MSAPs and its aggregate bandwidth rises up to 28.8 
GB/sec. Its area is 55%~59% smaller than the combination of 
conventional DMAs [5] and network interfaces [8]. 
Figure 8. MPEG4 system for QCIF format using the DMS architecture
[5] ARM PrimeCell™ DMA Controller, 
http://www.arm.com/armtech/PrimeCell?OpenDocument 
[6] Dave Comisky et al, “A Scalable High-Performance DMA 
Architecture for DSP Applications,” in Proceedings of ICCD 
2000. 
[7] MemMax™ Memory Scheduler, 
http://www.sonicsinc.com/sonics/products/memmax  
[8] A. Radulescu et al, ""An Efficient On-Chip Network Interface 
Offering Guaranteed Services, Shared-Memory Abstraction, 
and Flexible Network Programming,"" in Proceedings of 
DATE'04. 
[9] P. Bhojwani et al. “Interfacing cores "
Design space exploration and prototyping for on-chip multimedia applications.,"Traditionally, design space exploration for systems-on-chip (SoCs) has focused on the computational aspects of the problem at hand. However, as the number of components on a single chip and their performance continue to increase, a shift from computation-bound to communication-bound design becomes mandatory. Towards this end, this paper presents a comprehensive evaluation of two communication architectures targeting multimedia applications. Specifically, we compare and contrast the network-on-chip (NoC) and point-to-point (P2P) communication architectures in terms of power, performance, and area. As the main contribution, we present complete P2P and NoC-based implementations of a real multimedia application (MPEG-2 encoder), and provide direct measurements using a FPGA prototype and actual video clips, rather than simulation and synthetic workload. From an experimental standpoint, we show that the NoC architecture scales very well in terms of area, performance, power and design effort, while the P2P architecture scales poorly on all accounts except performance","Design Space Exploration and Prototyping for On-chip 
Multimedia Applications
Hyung Gyu Lee1, Umit Y. Ogras2, Radu Marculescu2, Naehyuck Chang1
1School of Computer Science and Engineering
Seoul National University, Seoul, Korea
{hglee,naehyuck}@cselab.snu.ac.kr
2Department of Electrical and Computer Engineering
Carnegie Mellon University, Pittsburgh, PA, USA
{uogras,radum}@ece.cmu.edu
ABSTRACT
Traditionally, design space exploration for Systems-on-Chip
(SoCs) has focused on the computational aspects of the problem at
hand. However, as the number of components on a single chip and
their performance continue to increase, a shift from computationbound 
to communication-bound design becomes mandatory.
Towards this end, this paper presents a comprehensive evaluation
of two communication architectures targeting multimedia applications. Specifically, we compare and contrast the Network-on-Chip
(NoC) and Point-to-Point (P2P) communication architectures in
terms of power, performance, and area. As the main contribution,
we present complete P2P and NoC-based implementations of a real
multimedia application (MPEG-2 encoder), and provide direct
measurements using a FPGA prototype and actual video clips,
rather than simulation and synthetic workload. From an experimental standpoint, we show that the NoC architecture scales very
well in terms of area, performance, power and design effort, while
the P2P architecture scales poorly on all accounts except performance. 
Categories and Subject Descriptors
B.4.3[Interconnections (Subsystems)]: Topology (point-to-point,
networks-on-chip)
General Terms
Design, Measurement, Performance
Keywords
Networks-on-chip, Point-to-point, System-on-chip, MPEG-2
encoder, FPGA prototype
1. INTRODUCTION
The increasing number of IP cores that can be integrated on a single
chip enables implementation of complex applications using the
SoC approach. Since these applications exhibit huge communication demands, scalable communication architectures are needed for
efficient implementation of future systems.
Due to the lack of scalability, both in terms of power and performance, traditional bus-based communication architectures fail to
satisfy the tight requirements of future applications. On the other
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
DAC 2006, July 24-28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007...$5.00.
1
Input
Buffer (IB)
2
DCT &
Quant. (DQ)
3
VLE & Out.
Buf. (VB)
4
6
Motion
Comp. (MC)
Motion
Esti. (ME)
Inv. Quant. &
IDCT (IQ)
Reconst.
Frm.Buf (FB)
5
7
Figure 
1. MPEG-2 
encoder 
communication architecture.
implementation using P2P
hand, the Point-to-Point (P2P) communication architectures can
provide the utmost communication performance at the expense of
dedicated channels among all the communicating IP pairs. In contrast to these methods, the Network-on-Chip (NoC) approach
emerged as a promising solution to the on-chip communication
problems [1,2,3].
1.1.  Related Work 
While it is intuitively accepted that NoCs can provide scalable
communication with a small area overhead, this fact has not been
justified to date by concrete NoC-based implementations of real
applications. Theoretical studies rely mainly on simulation to justify such findings, so the network traffic used in such studies is
either synthetic or approximating a real application via welldefined traffic generators [4,5]. On the other hand, several papers
dealing with implementation issues in NoCs [6,7,8,9,14] do not
consider the target application but assume synthetic traffic patterns
instead. For instance, the authors in [6] present an MPEG-4 performance evaluation for a CDMA-based implementation of a NoC,
but their design mimics the MPEG-4 traffic using a random traffic
generator; this is clearly problematic given the complex nature of
multimedia traffic [10]. 
1.2.  Paper Contribution
To address the issues mentioned above, the contributions of this
paper are twofold: First, we provide a complete MPEG-2 encoder
implementation using both NoC and P2P communication architectures. Second, for several MPEG-2 implementations, we present
extensive comparisons involving area, performance, and power
consumption measurements using a FPGA prototype. 
In terms of broader impact, both contributions are relevant to the
larger class of embedded multimedia systems. Indeed, the MPEG-2
encoder has been selected as the driver application since it covers a
rich class of multimedia applications where similar considerations
apply from an implementation standpoint. For instance, JPEG,
Motion-JPEG and MPEG-1 encoders, can all be implemented
using similar architectures and set of IP cores. 
We also note that, due to the small number of point-to-point connections in its architecture, the MPEG-2 encoder lends itself to a
P2P implementation, as shown in Figure 1. Indeed, the link-to-node
ratio in the MPEG-2 implementation shown in Figure 1 is only 1.5,
while the same ratio is found to be 3.0 for a complete graph of same
size even when all the links are unidirectional. It should be noted
that for many applications where a subset of cores communicate
with all the remaining nodes, the overhead incurred by the dedicated channels of the P2P architecture will be significant. As a
result, the conclusions derived herein with respect to the benefits of
the NoC architecture compared to the P2P architecture are rather
conservative so they will shed light on a wide range of practical
scenarios. 
1.3.  Overall Approach
In the following, we first design an MPEG-2 encoder using a P2P
communication architecture. After that, the same set of IP cores is
used to implement the NoC-based version. We deliberately exclude
the bus-based implementation from this analysis for several reasons. From an implementation standpoint, a bus-based design
would clearly provide a much lower performance figure due to its
limited bandwidth capabilities. Moreover, the large capacitive load
for the bus drivers results in large delays and power consumption
[15]; this makes the bus-based solution inappropriate for MPEG-2
encoder. Also, it is more interesting to see how the NoC solution
compares to the P2P architecture, particularly since due to the high
level of customization, the P2P approach is very difficult to beat in
terms of performance and energy figures. Therefore, in what follows, our focus will remain on the P2P and NoC implementations. 
While NoCs gained recently a significant momentum in the
research community, there are no complete NoC-based implementations of real applications reported to date. To remedy this situation, this paper presents a complete MPEG-2 encoder design using
the NoC approach and compares it with a P2P architecture implementing the same application. As we will see later in the paper, our
NoC-based MPEG-2 implementation achieves 47 Frames/sec
encoding rate for a CIF frame of size 352×288, which is very close
to the rate achievable by a P2P communication which is 48
Frames/sec. Although these values are close, the real benefits of
using the NoC approach are observed when we analyze the scalability of these designs as a function of the number of cores. More
specifically, we replicate the motion estimation module, which performs the most computationaly expensive task, to exploiting the
data parallelism available in MPEG-2. As the total number of modules in the design increases, the area occupied by the P2P implementation grows abruptly. On 
the other hand, 
the NoC
implementation incurs only a modest area overhead, while keeping
up with the performance increase achieved by the P2P implementation for a similar increase in the design size.
1.4.  Paper Organization
The remaining part of this paper is organized as follows: Section 2
presents the details of the P2P and NoC-based implementations of
the MPEG-2 encoder. Detailed area, performance and power consumption comparisons are provided in sections 3, 4 and 5, respectively. Finally, our conclusions appear in Section 6. 
2. MPEG-2 ENCODER IMPLEMENTATION
The basic MPEG-2 implementation using the P2P approach is
depicted in Figure 1. It consists of 7 modules: 1) Input Buffer (IB),
2) DCT & Quantization (DQ), 3) Variable Length Encoder & Output Buffer (VB), 4) Motion Compensation (MC), 5) Inverse QuantiDQ
R1
IB
IQ
R2
VB
ME 1
FB
MC
Figure 2. NoC-based implementation of MPEG-2 encoder with one
ME module.
zation & Inverse DCT (IQ), 6) Motion Estimation (ME), and 7)
Reconstructed Frame Buffer (FB). In our implementation, each
intra (I) frame is followed by 3 predicted (P) frames. The P2P
architecture obviously enables the fastest possible communication
due to the existence of dedicated channels between all the communicating modules. On the other hand, the utilization of dedicated
channels is low, since most of the time the links are idle. For
instance, we measured an average utilization of the P2P links of
only 4.9% using our MPEG-2 encoder prototype. In contrast, using
a NoC communication architecture enables link sharing among the
communicating cores. For instance, the NoC implemented for this
encoder needs 8 links in the network as opposed 10 links needed
for the P2P version, as shown in Figure 2. Each of link can be
either bidirectional or unidirectional, depending on the functionality of the connected modules. Obviously, sharing links may cause
extra communication delay compared to the P2P implementation,
but performance degradation is negligible, as shown in Section 4.
2.1.  Design of the Processing Elements
The area occupied by the individual cores in the design is summarized in Table 1. The second column (labeled “w/o wrapper”)
shows the number of slices a core takes in a Xilinx XC2V4000
FPGA when implemented without any network interface1 (wrapper). However, before using the core in a real design, we need to
add a wrapper such that it can successfully communicate with the
other nodes in the network. The wrapper for the P2P communication architecture has a simple flow control and I/O buffers so it
takes only 116 slices to implement it. On the other hand, the NoC
interface has to perform packetize/depacketize operations; this
takes 189 slices. The third and forth columns in Table 1 show the
area of the cores (in terms of number of slices and BlockRAMs)
with all wrappers included. Although the basic wrapper for the P2P
architecture is smaller than its NoC counterpart, the cores instantiated for the P2P architecture have actually a larger area. This is due
to the fact that for the P2P architecture the modules have dedicated
Table 1. Area comparison for individual cores in MPEG-2
(# of slices and BRAMs in a Xilinx Virtex2 4000 FPGA).
Core
IB
DQ
IQ
FB
ME
MC
VB
w/o wrapper
74 slices (1BRAM)
2,527 (1)
3,873 (1)
803 (75)
956 (8)
480 (19)
961
In P2P 
394
2,868
4,082
1,092
1,346
756
1,196
In NoC
263
2,716
4,062
992
1,145
669
1,150
1.Communication interfaces of the modules both in P2P and NoC
implementations are referred as network interfaces (NI).
Port 1
Output
Output
Output
Controller
Controller
Controller
Input
Input
Input
Controller
Controller
Controller
Routing
Routing
Table
Table
r
r
r
t
t
t
l
l
l
l
l
l
e
e
e
u
u
u
o
o
o
p
p
p
r
r
r
t
t
t
t
t
t
u
u
u
n
n
n
o
o
o
O
O
O
C
C
C
r
r
r
e
e
e
2
t
r
o
P
l
l
l
t
t
t
l
l
l
r
r
r
t
t
t
u
u
u
o
o
o
p
p
p
n
n
n
n
n
n
o
o
o
I
I
I
C
C
C
C
C
C
I
I
I
o
o
o
n
n
n
n
n
n
p
p
p
o
o
o
u
u
u
t
t
t
r
r
r
l
l
l
t
t
t
l
l
l
e
e
e
4
4
t
t
r
r
o
o
P
P
r
r
r
C
C
C
O
O
O
o
o
o
n
n
n
u
u
u
t
t
t
r
r
r
t
t
t
o
o
o
p
p
p
u
u
u
e
e
e
l
l
l
l
l
l
t
t
t
r
r
r
Input
Input
Input
Controller
Controller
Controller
Output
Output
Output
Controller
Controller
Controller
Port 3
Port 3
Figure 3. Block diagram of the router used in the NoC implementation.
interfaces for each connection, while for the NoC implementation
each module has only one interface which connects it directly to the
router. 
2.2.  Router Design
Due to moderate buffer requirements and good performance, our
NoC uses wormhole routing. The block diagram of the router is
shown in Figure 3. The network channel width and flit size is set to
16 bits. Each packet in the network contains one block in the current frame (8×8×16 bits) divided into 64 flits. The router uses a priority-based scheduling. It takes only 4 cycles for the router to route
the header flit. Then, the remaining flits follow the header in a pipelined fashion. Finally, the FIFO buffers implemented at the output
ports of the router have a depth of 16 flits. 
Table 2. Area taken by routers in a NoC implementation.
Synthesis is performed for a Xilinx Virtex2 4000 FPGA.
Router type
3-port router
4-port router
5-port router
6-port router
# of slices
219
304
397
503
Device utilization
1.4%
1.8%
2.2%
2.8%
Based on the network topology, we use routers with 3, 4, 5 or 6
ports. The area occupied by the routers, as a function of the number
of ports, is summarized in Table 2. Although none of the routers is
optimized for area, their area overhead is smaller than the overhead
incurred by the dedicated links and network interfaces of the P2P
implementation; we discuss this in detail in Section 3. 
3. EVALUATION OF DESIGN AREA 
In this section, we compare the area occupied by the complete
MPEG-2 encoder implemented with the P2P and NoC communication architectures. Besides the absolute value of area of the baseline
designs in Figure 1 and Figure 2, we also analyze how the area
scales up with the increasing number of cores. For this reason, we
also implement a version of the MPEG-2 encoder which has 2 separate ME modules. 
As shown with dotted lines in Figure 4(a), adding one more ME
module to the P2P architecture requires 4 extra links and 8 network
interfaces. In addition to this, the IB, MC, FB and VB modules
have to be modified to allow the integration of the second ME
module. On the other hand, the impact of this additional core on the
NoC implementation is only local since we only need to add one
more link from the newly inserted module to a router and one extra
port inside the router (see Figure 4(b)). As a result, the effort of
adding additional modules in order to increase the design parallelIB
DQ
VB
MC
ME 1
ME 2
DQ
R1
IB
IQ
FB
(a)
ME 2
IQ
R2
VB
(b)
ME 1
FB
MC
Figure 4. Implementation of MPEG-2 encoder withe 2 MEs using
(a) P2P and (b) NoC communication architectures.
ism and the penalty in area are both much smaller for the NoC
design. 
To be more concrete, the area of the P2P implementation on our
FPGA prototype goes up from 11,587 to 13,501 slices resulting in a
16.5% increase. On the other hand, the area of the NoC design
increases from 11,790 to 12,929 slices, which is about a 9.7%
increase. Hence, the area for the NoC design with 2 ME modules is
smaller than its P2P counterpart. It is also interesting to analyze the
scaling effects for an arbitrary number of additional cores so we
provide next an analytical framework to estimate it. 
We analyze the area occupied by the logic components such as
cores, network interfaces and routers, and the area occupied by the
links, separately, because the interconnection mechanism of
FPGAs is quite different from that in real silicon implementation due to reconfiguration facility of FPGAs.
3.1.  Analytical Estimation of Area
The area occupied by the computation and communication
resources in the design can be estimated by using the area of the
individual modules, such as processing elements and all network
interfaces. In the following, A(.) denotes the area of its arguments,
=(
 denotes core i, while NC is the total number of
cores in the design. Finally, NIP2P and NINoC stand for the network
interfaces which correspond to the P2P and NoC designs, respectively. Using these notations, the area of the P2P implementation
can be found as:
ci i 1 … NC
)
,
,
⋅
+
i 1=
AP2P
A ci(
) 2NL A NIP2P
NC∑=
where NL is the number of links in the design. Similarly, the area of
the NoC-based design is expressed as:
NC∑=
NR∑+
A ci(
) NC A NINoC
ANoC
A Ri(
(1)
(2)
+
(
)
(
)
)
⋅
i 1=
i 1=
where Ri (i = 1,...,NR) denotes router i, while NR is the total
number of routers in the network.
Unlike the logic area, the interconnect area is difficult to estimate with simplified models, since it depends on the logic
placement and space complexity. Hence, instead of using such
models, we use an in-house communication-aware floorplanner
 
 
 
for both P2P and NoC implementations, and determine the total
length of all wires [16].
3.2.  Area Comparison of P2P and NoC Implementations
The area comparisons for the logic and wiring area of the P2P and
NoC implementations are shown in Figure 5. The logic area estimates (i.e. the P2P_E and NoC_E bars) for the designs with one
and two ME modules are within 4% of the measured values
(P2P_M and NoC_M). We also note that the measured values are
slightly smaller due to the optimization process during the synthesis of the complete design. For the MPEG-2 designs involving 4
and 8 ME modules, we use the estimated values as basis for comparison (i.e. P2P_E and NoC_E bars in Figure 5), since the total
design area is larger than the capacity of the target FPGA.   
As shown in Figure 5, starting with the designs involving 2 ME
modules, both the logic and wiring area of NoC implementation is
consistently smaller. More importantly, the difference in area
increases as the number of cores becomes larger. In general, it has
been shown that the P2P implementation scales as 
,
while NoC implementation scales as O(n) [11]. Hence, our experimental results are in agreement with the theoretical predictions. 
4. PERFORMANCE EVALUATION
In this section, we develop an analytical model to estimate the
throughput of the encoder. Then, we compare the performances of
the P2P and NoC designs using both this model and measured data.
4.1.  Analytical Estimation of Performance
Since both P2P and NoC implementations are pipelined, the
encoder throughput is determined by the latency of the critical path
on the data flow, as illustrated in Figure 6. More precisely, the critical path is given by:
(3)
where SC is set of computational nodes and communication
links on the critical path, and Ti is the latency of ith node or link.
From MPEG-2 functionality, it follows that for the I-frames, the
critical path is given by SC = {TIB→TDQ→TVB}. Similarly, the
critical path for the predicted (P) frames can be expressed as
SC = {TIB→TME→TMC→TFB→TMC→TIB→TDQ→TVB}. Since
the critical path for the P-frames is significantly longer, we consider only the performance analysis of the P-frame encoding. 
Among the modules on the critical path for the P-frame encoding,
the ME module requires the largest computational time. So, its
latency value directly determines the overall performance of the
system: 
(4)
where TME is the time required for motion estimation, and LData
is the time for receiving data, Ndata, from the FB module. We
note that TME is the same for both P2P and NoC designs, while
LData is different. More precisely, for the P2P architecture,
LData is the volume of data, NData, divided by the link bandwidth (W):
(5)
---------------For NoCs, on the other hand, we calculate the communication time
using the latency formula for wormhole routing [12]: 
(6)
where H is the hop count between source and destination, and
LR is the time it takes to route the header flit. The first term in
this equation gives the time it takes to route the header flit,
while the second term gives the latency of the remaining flits,
since they all follow the header flit in a pipelined manner.
4.2.  Performance Comparison of P2P and NoC 
Implementations
The throughput of the P2P implementation is measured as 48
Frames/sec for a CIF frame of size 352×288. The corresponding
NoC implementation achieves a throughput of 47 Frames/sec. As
explained in the previous section, the bottleneck module in both
designs is the ME module. For this reason, by duplicating this module, we expect a significant improvement in the throughput. The
encoder implementation with two ME modules shows that this is
indeed the case. Specifically, the P2P implementation with 2 ME
modules has a throughput of 90 Frames/sec, which is about 87.9%
improvement. Similarly, the throughput of the NoC design goes up
to 86 Frames/sec showing a comparable improvement. 
The accuracy of the performance estimates in Equation 5 and
Equation 6 was validated against measured data on the FPGA prototype. As shown in Figure 7, the estimated values (i.e. the P2P_E
and NoC_E bars) are in good agreement with the measured ones.
Again, due to complexity reasons, for designs with higher degree of
hardware redundancy (that is, the designs with 4 and 8 ME modules), we only provide the estimated values. We can clearly see that
NoC-based implementations perform close to the P2P implementation. However, one should note that beyond a certain degree of parallelism, the communication performance becomes the bottleneck
and the NoC performance settles down. It is possible to eventually
stretch the performance beyond this point by customizing the network topology [5,13]. 
    (a) Area occupied by logic
 (b) Area occupied by wires
Figure 5. Area comparisons of the MPEG-2 encoder implemented
using P2P and NoC architectures for increasing level of parallelism.
0
5,000
10,000
15,000
20,000
25,000
30,000
1
2
4
Degree of parallelism
8
#
o
f
s
i
l
s
e
c
P2P_E
P2P_M
NoC_E
NoC_M
14
12
10
8
6
4
2
0
1
2
4
Degree of parallelism
8
N
o
r
m
a
i
l
d
e
z
t
o
t
a
l
w
r
i
e
l
g
n
e
t
h
P2P
NoC
O n2 n
(
)
Tcritical max Ti{
} i SC∈
=
Figure 6. Critical paths for the I-Frame and P-Frame encoding are
shown by the dotted and dashed lines, respectively.
IB
DQ
IQ
FB
MC
ME
VB
I-Frame
P-Frame
2
3
4
5
6
7
8
1
9
Performance
1
TME LData
---------------------------------+
=
P2P NData
LData
W
=
NoC H L⋅ R
LData
NData W–
W
--------------------------+
=
 
 
 
 
 
P2P_E
P2P_M
NoC_E
NoC_M
c
e
s
/
s
k
c
o
l
b
o
r
c
a
m
f
o
#
180,000
150,000
120,000
90,000
60,000
30,000
0
1
2
4
Degree of parallelism
8
Figure 7. Performance comparison of the MPEG-2 encoder
implementations. P2P_E and NoC_E show the analytical estimations,
while P2P_M and NoC_M show the measurement.
5. ENERGY AND POWER CONSUMPTION 
EVALUATION
As battery-powered devices become popular, the energy consumption issues gain more importance. For this reason, this section evaluates energy and power consumptions for P2P and NoC
communication architectures.
The activity-based model, P = αCV2f, is the most commonly used
model for power estimation at all levels of abstractions. However,
as the complexity of systems increases, obtaining accurate C and α
values becomes more difficult. Therefore, we use a system-level
approach to reduce the simulation time and the complexity of the
work involved. 
5.1.  Analytical Estimation
The total system energy consumption can be divided into two components: the computational energy consumption, ECOMP, and the
communication energy consumption, ECOMM. In turn, the computational energy can be expressed as:
ECOMP
E ci(
)
(7)
NC
∑=
i 1=
where E(ci) denotes the energy consumption of any component ci,
and NC represents the total number of computational modules. The
communication energy, on the other hand, consists of link (L), network interface (NI), and router (R) energy consumption, namely: 
ECOMM
E Li(
)
(
E NIi
)
E Ri(
)
(8)
NL
∑=
i 1=
NNI
∑+
i 1=
NR
∑+
i 1=
where NL, NNi and NR represent the number of links, network interfaces and routers in the network, respectively. 
Depending on the operating mode, the IP cores are characterized by
different power consumption values. Therefore, we can analyze the
energy consumption of any individual module E(ci) as follows:
E ci(
)
(
πjPj
) t⋅
(9)
Link
NLP∑=
j 1=
NLP
where 
 is the number of distinct power states, πj is the probability that the module is in state j during execution time t, and Pj is
the power consumption which characterizes the jth power state1. 
Router
1. Here, the energy dissipation during transitions among different power modes is
neglected since this represents a small fraction of the energy consumption in regular
power states anyway. 
In order to achieve accurate results, we measure the power consumption of each individual IP using a cycle accurate energy measurement tool based on the technique presented in [17]. Then, these
individual power values are used to compute the power consumption of the entire system. 
As seen from Equation 9, having power consumption estimates is
critical for system energy characterization. Toward this end, we use
two levels (idle and active) of power states to characterize the individual computational nodes, as summarized in Table 3. We note
that the power consumption of the ME module dissipates less
power than that of the DQ and IQ modules. However, the ME module consumes much more energy than these modules because its
operation time is about ten times longer compared to that of the DQ
and IQ modules. 
For the communication components, we use a larger number of
power states as summarized in Table 4. To accurately compute the
link power consumption, we use an in-house communication-aware
floorplanner for both P2P and NoC implementations, and determine the length of all links. Three typical values (which are classified as shortest, middle and longest) are shown in Table 4. After
that, we measure the corresponding link power consumption in the
FPGA prototype by varying the link length. These measured values
are later used to estimate the link power consumption. Finally, we
characterize the power consumption of the router, as shown in
Table 4. The table also shows the router power consumption when
the number of active ports varies. 
Table 3. Power consumption (in mW@100MHz) for each
computational node in the network
Node
IB
DQ
IQ
FB
ME
MC
VB
Power in idle mode
60
353
420
247
133
108
210
Power in active mode
109
1,279
1,755
352
411
203
626
Table 4. Power consumption (in mW@100MHz) over the
communication channel 
Resource
Mode
Interface
Idle
Receive
Send
Receive + Send
Shortest
Middle
Longest
Idle
1 port
2 port
3 port
4 port
Power consumption
P2P
NoC
39
47
74
69
78
69
92
101
12
16
19
NA
NA
NA
NA
NA
121
176
190
216
252
 
 
 
P2P (energy)
NoC (energy)
P2P (percentage)
NoC (percentage)
P2P (power)
NoC (power)
P2P (percentage)
NoC (percentage)
)
k
c
o
l
b
o
r
c
a
m
/
J
u
(
y
g
r
e
n
E
80
60
40
20
0
1
2
4
Degree of parallelism
8
40
30
20
10
0
)
%
(
e
g
a
t
n
e
c
r
e
P
)
W
m
(
r
e
w
o
P
4,000
3,000
2,000
1,000
0
40
30
)
%
(
e
g
a
t
n
e
c
r
e
20
P
10
0
1
2
4
Degree of parallelism
8
(a)
(b)
Figure 8. (a) Energy consumption and (b) Power consumption (@
100MHz) as a function of the degree of parallelism for P2P and NoC
implementations.
5.2.  Energy Consumption Comparisons of P2P 
and NoC Implementations
We first obtain the utilization and operation modes of all the components in the design; this is done by dynamically profiling Verilog
simulations using actual video clips. Then, we measure the energy
consumption of each module separately; this is because the entire
MPEG-2 encoder is too big to fit the memory space of the energy
measurement tool we employ [17]. Finally, we use these values to
estimate the energy and power consumption of the entire design, as
explained in Section 5.1. Therefore, in the section we only provide
estimated values for the energy and power consumption, unlike the
area and performance comparisons reported before.
The power consumption of the computational modules is the same
for both P2P and NoC implementations. Therefore, we focus next
on the communication power consumption. Figure 8(a) shows that
the communication energy consumption for the NoC implementation is consistently smaller than the P2P counterpart for different
levels of parallelism. Likewise, we observe that the NoC design
looks better in term of power consumption. Since the P2P implementation has more interfaces and links than the NoC counterpart,
its power consumption is slightly larger than the power consumed
by the NoC even for the baseline implementation. Furthermore,
Figure 8(b) shows that the power consumption of the P2P architecture scales poorly as the degree of parallelism increases. Indeed,
increasing the degree of parallelism makes the power consumption
difference only bigger, since the P2P implementation requires a
significantly larger number of additional links and network interfaces. Our experiments demonstrate that the NoC implementation
consumes up to 42% less power compared to the P2P implementation for an implementation involving 8 ME modules. This corresponds to about 17% of the total power consumption which is quite
important when optimizing portable systems.
Another issue of interest is the proportion of the communication
energy consumption compared to the overall energy consumption.
For this reason, we plot the percentage of the communication
energy and power consumptions in Figure 8(a) and (b), respectively. For the baseline implementations, the communication power
consumption of the P2P and NoC designs represents 31% and 26%,
of the total power consumption, respectively. As we can see in
Figure 8(b), the percentage of communication power consumption
of the P2P implementation increases very fast with the increase in
the degree of parallelism. On the other hand, the percentage of
communication power in the NoC implementation increases much
slower. This means that the NoC architecture has good scalability
in terms of power as well as area and design complexity.
6. CONCLUSION
Integrating an increasingly large number of IP cores on the same
chip makes the design of the communication architecture of future
SoCs a challenging problem. As a result, design space exploration
with emphasis on the communication aspects becomes crucial. 
Towards this end, this paper presented a comprehensive evaluation
of the P2P and NoC communication architectures targeting multimedia applications. Through analytical prediction and direct measurements on a FPGA-based prototype, it has been shown that the
performance of the NoC implementation is very close to the P2P
implementation of similar size. Moreover, the scalability of the P2P
and NoC implementations was analyzed through duplicating the
bottleneck module in the MPEG-2 design. It has been observed that
NoC design scales very well in terms of area, performance, power
and overall design effort, while the P2P architecture scales poorly
on all accounts except performance. 
7. ACKNOWLEDGEMENTS
This work was supported in part by the International Research
Internship Program of the Korea Research Foundation (KRF) and
in part by Marco GSRC. 
8.  "
Statistical on-chip communication bus synthesis and voltage scaling under timing yield constraint.,"We propose a statistical approach for minimizing on-chip communication bus width and number of buses with reduced communication energy under timing yield constraint. The slack is exploited to maximize sharing of buses and to reduce energy by simultaneously scaling the voltage during the communication synthesis. Because of the diversity of applications to be run on a single SoC, there exists variability of data size to be transferred among the on-chip communicating modules. This variability of data size is modeled as a normally distributed random variable. The resulting synthesis problem is relaxed to the convex quadratic optimization problem and is solved efficiently using a convex optimization tool. The effectiveness of our approach is demonstrated by applying optimization to an automatically generated benchmark and a real-life application. By varying the value of timing yield constraint, a trade-off between minimization of buses and energy reduction is explored. The experimental results show the significant reduction of communication energy with the increasing timing yield. However, the timing yield offers a limitation to minimize the size of bus width and number of buses, if the yield is increased beyond a certain limit","Statistical On-Chip Communication Bus Synthesis and
Voltage Scaling Under Timing Yield Constraint
Sujan Pandey, Manfred Glesner
Institute of Microelectronics Systems
Darmstadt University of Technology, Darmstadt, Germany
{pandey,glesner}@mes.tu-darmstadt.de
ABSTRACT
We propose a statistical approach for minimizing on-chip communication bus width and number of buses with reduced communication energy under timing yield constraint. The slack is
exploited to maximize sharing of buses and to reduce energy
by simultaneously scaling the voltage during the communication synthesis. Because of the diversity of applications to be
run on a single SoC, there exists variability of data size to be
transferred among the on-chip communicating modules. This
variability of data size is modeled as a normally distributed
random variable. The resulting synthesis problem is relaxed
to the convex quadratic optimization problem and is solved
eﬃciently using a convex optimization tool. The eﬀectiveness
of our approach is demonstrated by applying optimization
to an automatically generated benchmark and a real-life application. By varying the value of timing yield constraint, a
trade-oﬀ between minimization of buses and energy reduction
is explored. The experimental results show the signiﬁcant reduction of communication energy with the increasing timing
yield. However, the timing yield oﬀers a limitation to minimize the size of bus width and number of buses, if the yield
is increased beyond a certain limit.
Categories & Sub ject Descriptors: C.3 [Special-Purpose
& Application-Based Systems]: Real-Time and Embedded.
General Terms: Algorithms, Design Aid.
Keywords: Communication Bus Synthesis, Voltage Scaling.
1.
INTRODUCTION
Due to the increasing trend on a system complexity, there is
a huge demand of communication placed by the on-chip communication traﬃc on the communication architecture. On the
other hand, the technology scaling trend shows that the interconnect wires account for a signiﬁcant fraction (up to 50%
[14]) of the total energy consumption in an integrated circuit,
and this fraction is only expected to grow in the future. Considering these trends, synthesizing an energy eﬃcient on-chip
communication bus is a challenging task to the system designers. The early work about communication bus synthesis
focused mainly on minimizing bus width [17],[19] protocol selection [6],[13], interface synthesis [18], and synthesis of single
global bus topology [9]. In [23] an automatic bus generation
for a MPSoC was proposed. They generate bus for a given
size of bus width considering real time constraint. In [22] a
method of communication synthesis based on the library elements and constraints graph was presented. Their approach
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2006, July 24–28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007 ...$5.00.
focus mainly to synthesize the communication topology for
the point-to-point communication architecture. In [26] a bus
model for communication in embedded systems with arbitrary
topologies was proposed, where point-to-point communication is a special case for the real time application. In [15] a
synthesis ﬂow which supports shared bus and point to point
connection templates was presented. In [21] a ﬂoorplan aware
automated bus based communication synthesis algorithm was
presented. All above approaches deal only with the real time
constraint and the size, however, they do not consider the
energy consumption of communication buses.
There has been already a signiﬁcant amount work done
in the area of system level approaches to reduce the energy
of embedded systems. Dynamic voltage scaling (DVS) and
adaptive body biasing (ABB) can be an option to reduce
the energy consumption [10],[16],[25]. In [4], DVS and ABB
techniques are used to reduce the energy for fat wires and
repeaters based communication bus. However, they assume
that bus width and topology of communication bus have been
already identiﬁed and given to them. Another possibility to
reduce communication energy is the usage of bus encoding
techniques [5]. In [11] proposed shared bus splitting, which
dynamically breaks down long, global buses into smaller, local
segments to improve the energy consumption of bus and to
meet dynamic data traﬃc demand. In [8] proposed a model to
estimate the switching activity of communication buses and
to compare the eﬀectiveness of power optimization techniques
at the system level.
All the above techniques do not take into account the voltage scaling for communication energy reduction during the
communication bus synthesis process. In this paper, we propose a method to synthesize the minimum size of bus width
and minimum number of buses with reduced energy under
the variability of data size to be transferred by each on-chip
communicating module. This variability of data size is due
to the diversity of applications to be run in a single embedded system and is modeled as a normally distributed random
variable. The voltage is scaled to reduce the communication
energy and to meet the desired timing yield during the communication synthesis. The results show that voltage scaling
can be used to reduce the communication energy, however, if
the voltage is scaled to the minimum level (by increasing the
timing yield constraint) the synthesized size of bus width and
number of buses will not be the minimum.
To our best knowledge, this is the ﬁrst attempt to solve the
statistical on-chip communication bus synthesis and voltage
scaling problem under timing yield constraint.
2. PRELIMINARIES
In this paper, we consider embedded systems which are
realized as multiprocessor systems-on-a-chip (SoC). Such a
system consists of several on-chip processing modules like
general-purpose processor, an application speciﬁc integrated
circuit (ASIC) or a ﬁeld-programmable gate array (FPGA).
These on-chip modules communicate with each other by transferring data through communication buses like shared buses
or point-to-point connection. We assume that Hw/Sw partim1
m3
m2
m4
J2
J3
J1
J4
c1
c2
c3
J1
J3
J2
J4
c1
c2
c3
C1
C3
C2
S
dl
C1
C3
C2
S
dl
w
w
w
w
w
w
(a)
(b)
(c)
(d)
t=0
t=0
t=1
t=1
t=1
t=4
t=7
t=7
t=2
t=4
D
i
r
r
e
v
D
i
r
r
e
v
D
i
r
r
e
v
D
i
r
r
e
v
J5
J5
w
w
Fig. 1: Architecture model.
(a) Target architecture with
mapped tasks. (b) Extended tasks graph. (c) Communication
task graph with ASAP scheduling of CLTIs for 16-bit wide
bus. (d) Communication task graph with ALAP scheduling
of CLTIs for 16-bit wide bus.
C1
C2
C3
C1
C2
C3
C1
C2
C3
C1
C2
C3
w
w
w
w
(a)
(c)
(b)
(d)
Amount of slack
CLTI with voltage scaling
CLTI w/o voltage scaling
T
A
L
A
P
T
A
S
A
P
t(ms)
87mW
87mW
87mW
174mW
174mW
174mW
39mW
87mW
174mW
t(ms)
t(ms)
t(ms)
3
2
4
5
87mW
56mW
56mW
6
1
3
2
4
5
6
1
3
2
4
5
6
1
3
2
4
5
6
1
7
7
7
7
w
w
w
w
Fig. 2: Scheduling of CLTIs and voltage scaling of on-chip
communication bus. (a) Scheduling of CLTIs for 16-bit wide
bus. (b) Scheduling of CLTIs for 32-bit wide bus. (c) Scheduling and voltage scaling of CLTIs for 16-bit wide bus.
(d)
Scheduling and voltage scaling of CLTIs for 32-bit wide.
tioning and mapping of tasks onto the appropriate modules of
SoCs have been done eﬃciently as shown in Fig. 1(a). Based
on these mapped tasks, a directed acyclic extended graph
GE (T , E ) is obtained to extract the data processing tasks τ
and the data communication tasks c of a given application. In
the extended graph, a node τ ∈ T represents the data processe ∈ E indicates data dependency between the tasks (i.e. coming task, which is mapped onto the on-chip module, while edge
munication). All the communications that take place over the
on-chip communication buses are captured by communication
task ci , as indicated by square in Fig. 1(b). If the tasks τi and
τj are mapped to the same module then there exist an edge
between them without the square. This indicates that the
tasks τi and τj do not communicate using an on-chip communication bus. The notation ci is a communication task, which
takes a certain time duration to transfer data from one module to another module by using an on-chip communication
bus. This time duration is called a communication lifetime
interval (CLTI), which shows how long time a task ci uses a
communication bus. Furthermore, each communication task
has its start time and deadline to ﬁnish the task. From the
extended graph GE (T , E ), a directed acyclic communication
task graph GC (C, Π) is obtained with the start node S and
tasks. In the communication task graph, a node c ∈ C is a
deadline node d l to schedule the CLTIs of the communication
communication task, while an edge π ∈ Π gives the dependency between the communication tasks. Fig. 1(c) depicts
the communication task graph with ASAP (as soon as possible) scheduling of CLTIs for 16-bit wide bus with deadline
7ms. An edge between two nodes ci and cj weighted with w
is the data processing time of a task τi , which gives an early
start time constraint for a successor cj to transfer data using
a communication bus. Fig. 1(d) depicts the ALAP (as late
as possible) scheduling of the CLTIs for 16-bit wide bus with
deadline 7ms. In Fig. 1(c) and (d), there is a diﬀerence in
ASAP and ALAP time for the node c2 . This diﬀerence between the ALAP and ASAP time of a communication task
is called slack. It measures how free we are to schedule the
communication task ci into diﬀerent time slots so as to maximize the sharing of communication buses and to minimize
bus energy consumption. This slack is a function of three
variables, which are size of data to be transferred, size of bus
width and voltages. Their relation can be written as,
slackc,r,V = tALAPc ,r ,V − tASAPc ,r ,V
(1)
where tALAPc ,r ,V can be expressed as,
tALAPc ,r ,V = dlc − CLT Ic,r,V
(2)
CLT Ic,r,V =
N Bc (ζ )
br
In above equations, dlc is a deadline to ﬁnish a task, CLT Ic,r,V
is the communication lifetime interval for a task c with a bus
of type r and supply voltage V , N Bc (ζ ) is a random number of data bit to be transferred by a task, br is the size of
bus width of bus type r and Tc is the time period of one clock
cycle. For the sake of clarity, we consider only the supply voltage scaling for the dynamic energy consumption. Nonetheless,
the leakage energy as well as Adaptive Body Biasing (ABB)
techniques [10],[16],[25],[4] can easily be incorporated into the
formulation without changing our general approach. The execution time of a communication task c with voltage Vi is given
by [16],
· Tc
(3)
Tc = κ
Vi
(Vi − Vth )α
where κ is a technology dependent constant, α is the saturation velocity (1.4 < α ≤ 2), Vi is the supply voltage, and Vth
is the threshold voltage. The dynamic energy consumption of
each task c is given by [16],
Ec = ατ · Cef f · V 2
· Tc
(5)
where, ατ is the switching activity of the communication tasks
and Cef f is the eﬀective switched capacitance for a data communication. The energy overhead, for switching from Vi to
Vj , is [16]
i,j = Cr (Vi − Vj )
(6)
where, Cr is the capacitance of the power rail. The time
overhead, for switching from Vi to Vj , is given by [16]
i,j = ρ|Vi − Vj |
(4)
i
εΔV
2
δΔV
(7)
where ρ is a constant.
3. MOTIVATIONAL EXAMPLE
In this section we give a motivation to scale voltage for
on-chip communication bus synthesis by simultaneously performing voltage scaling, bus selection, scheduling and binding of communication tasks and illustrate that the slack of a
communication task changes with the size of bus width and
voltage. This slack can be exploited to scale the voltage and
to share the communication bus, which ultimately increase
the system eﬃciency in terms of number of buses/size of bus
width and energy consumption. Consider a system that has
been partitioned and mapped onto the on-chip modules of a
SoC and the driver of each module is capable to scale the
supply voltage while transferring data from one module to
another module. As shown in Fig. 1(a) ﬁrst, module m2
executes task τ1 and its driver transfers data to m1 and m3
to execute tasks τ2 and τ3 , respectively. After receiving the
data from module m2 , module m1 executes task τ2 and its
driver transfers data to module m4 , which executes tasks τ4
and τ5 . The task τ5 has to be ﬁnished before the deadline
7
6
5
4
3
2
1
y
a
l
e
d
I
T
L
C
0.6 0.8
1 1.2 1.4 1.6 1.8
(a)
Voltage
60
20
Bus width
40
Scenario 1
Scenario 2
Scenario 3
Scenario 4
10
9
Tmax
8
l
y
a
e
d
I
T
L
C
7
6
5
4
3
Tmin
2
0.6
0.8
Vmin
1
1.2
(b)
1.4
Voltage
1.6
1.8
Vmax
Fig. 3: Delay as a function of bus width and voltage for different scenarios
of 7ms. The ASAP and ALAP scheduling of communication
task graph of above example with their start node and deadline node are shown in Fig. 1(c) and (d), respectively. Fig.
2(a) shows a scheduling of CLTIs with their ASAP and ALAP
time of all the communication tasks c1 , c2 and c3 , considering
16-bit wide bus and the nominal voltage settings (the highest
supply voltage = 1.8V and body bias voltage = 0V), i.e., all
the drivers run at their highest performance. This schedule
of communication tasks for a 16-bit wide bus gives a slack
(denoted by white rectangle) of 1 ms and needs two separate
buses to meet the time constraint of 7ms. From the given
power consumption at the nominal voltage as shown in Fig.
2(a), the total energy consumption of all the communication
tasks can be calculated as 3·87mW·2ms = 522μJ. Fig. 2(b)
shows the scheduling of the same communication tasks c1 , c2
and c3 for 32-bit wide bus and the nominal voltage. This
schedule gives the total slack of 4ms, which increases the mobility so that all the communication tasks can share a single
bus. The total energy consumption at the nominal voltage
can be calculated as, 169mW·3ms = 507μJ. In order to reduce the energy consumption, we scale the voltage to exploit
the slack of communication tasks so that all the tasks c share
the minimum number of bus as shown in Fig. 2(c) and (d). To
make the problem simple, we assume in this example that the
task processing time of each on-chip module is known to us,
i.e, the operating voltages of modules are known. Further, we
assume that the supply voltage of all the drivers can be varied
continuously in the ranges [0.6, 1.8]V. In Fig. 2(c), communication tasks c2 is scheduled with the supply voltage 1.4V to
exploit the slack of 1ms, while tasks c1 and c3 are scheduled
the communication tasks is 39mW·3ms+(87+87)mW·2ms =
with the nominal voltage. The total energy consumption of
465μJ, which is reduction in energy by 11% compared to the
energy at the nominal voltages of 16-bit wide bus.
In Fig.
2(d), the amount of slack is increased to 4ms by scheduling
the communication tasks for 32-bit wide bus. This slack is
exploited to scale the voltage and share the minimum number
of bus. Hence, the supply voltage of the communication tasks
c1 and c3 are scaled to 1.2V, while c2 is kept to the nominal
is calculated as 174mW·1ms+ 2·56mW·2 = 398μJ, which is
voltage to share a single bus. The total energy consumption
reduction in energy consumption by 24% compared to the
scheduling of Fig. 2(c).
In above example, optimal size of bus width, number of
buses and voltages were found for a ﬁxed size of data N Bc of
each task c. Due to the diversity of applications to be run is
a single embedded system, a task c can have a random size
of data N Bc (ζ ) to be transferred. For this scenario, above
synthesized bus and voltage may not be optimal because of
random data size of on-chip modules. Fig. 3(a) depicts CLT I
delay of a task c as a function of voltage and bus width considering one scenario. Fig. 3(b) shows change in CLT I delay
time and voltage constraints {Tmin ,Tmax } and {Vmin ,Vmax },
of a task c for diﬀerent size of data to be transferred with
respectively. In Fig. 3(b) diﬀerent size of data are plotted as
a scenario, which has a certain probability. We saw in above
example that the slack of task c can be exploited to share the
bus and to scale the voltage, however for the random data size
of each task c the size of slack is not deterministic. Hence,
we model the communication bus synthesis together with the
voltage scaling problem in a stochastic nonlinear programming and ﬁnd an energy eﬃcient optimal size of bus width
and number of buses under the timing yield constraint.
4. PROBLEM FORMULATION
We assume that a set of tasks have been partitioned and
mapped eﬃciently onto the appropriate modules of a SoC.
Each module mi processes tasks and a communication task
c transfers data to another module mj , which has the data
dependency. The transfer of data from one module to another module takes place via a communication bus and this
bus is driven by a driver which is capable to scale the voltage
during each data transfer. Due to the diversity of applications that run within a single SoC, the workload oﬀered on
the embedded system is not uniform over the time. This introduces the randomness on the size of data to be transferred
among the on-chip communication tasks. We model the size
of data bit to be transferred by a communication task c as a
random variable N Bc (ζ ) with a known probability distribution function. For each task c its deadline dlc , the distribution of random variable N Bc (ζ ) and the switched capacitance
Cef f are given. Based on the mapped tasks τ , a directed
acyclic extended graph GE (T , E ) is obtained as shown in Fig.
1. From the extended graph GE (T , E ), the communication
task graph GC (C, Π) is obtained with start node S and deadline node dl.
In the communication task graph GC (C, Π),
c ∈ C be a set of communicating tasks and their data depenΠ ⊆ (C × C ), consists of two-tuples (ci , cj ) where a successor
dency between the communication tasks is deﬁned by a set
cj depends on the results of the predecessor ci . This data
dependency between communication tasks is constrained by
a set Depn ⊆ (C × C × W ) consists of 3-tuples (ci , cj , w) such
that ∀i, j ∈ [1 . . . N ], (ci , cj )i(cid:2)=j ∈ Π|Π ⊆ C × C , a task cj can
start transferring data no earlier than w time units after the
completion of transferring data by ci .
We assume that the supply voltage Vdd and the body bias
voltage Vbs of each data processing task τi are known and provided to calculate the execution time of task(s) in a module.
Vbs of each communication task c ∈ C are unknown and to
Unlike this, the supply voltage V and the body bias voltage
be identiﬁed.
In this work for the sake of clarity, we consider only the supply voltage scaling however, adaptive body
biasing (ABB) can easily be incorporated in our approach
of communication bus synthesis. Each task c ∈ C can vary
its supply voltage V within a certain continuous or discrete
ranges.
5. STATISTICAL COMM. TASKS MODELING
The relation of the execution time CLT I with the size of
bus width br , supply voltage Vi , and a random size of data
N B (ζ ) of a communication task c are given by the Eqs. (3)
and (4). In this section, we assume that CLT I is inversely
proportional to Vi (Vth = 0, α = 2) to make the illustration of
our point simpler, however the drawn conclusions are valid in
general. After the simple algebraic manipulation of Eqs. (3)
and (4) we get,
CLT Ic,r,V = κ
N Bc (ζ )
br · Vi
We assume that η be the timing yield of all communication
tasks c. The yield constraint gives a limit to scale the voltage
for the slack exploitation. We further assume that the timing
yield is constrained by the range 0.5 < η ≤ 1. The overall
(8)
 
 
delay constraint of tasks c can be written as,
P (dlc − CLT Ic,r,V − δΔV
i,j ≥ 0) ≥ η
(cid:2)
c∈C
(9)
c∈C
(cid:2)
i,j ) ∼
This equation constraints the probability of all tasks c having
a delay less than the deadline of the tasks to be more than η .
We assume that the random data model of all communication
tasks c are normally distributed. Hence, the mean μ and σ of
CLT I as a function of br and Vi can be written as,
P ((dlc − CLT Ic,r,V − δΔV
N (μCLT I (N B ), σCLT I (N B ))) ≥ 0) ≥ η
which can be rewritten as
−μCLT I (N B ) − φ−1
(1 − η) σCLT I (N B ) ≤ 0
(11)
where φ−1 (·) is the inverse of the error function. Eqs. (11)
can be consider as a convex function under the condition that
η > 0.5 [12]. Since the target yield for a given path is always
much greater than 50%, this condition is easily satisﬁed.
(cid:2)
(10)
c∈C
5.1 Statistical Parameters Estimation
Applying the above random data model of each communication task to the stochastic programming, the optimal number
of bus(es) and size of bus width are obtained. After an algebraic manipulation of Eqs. (8) the statistical parameters of
voltage in terms of optimal size of bus width br (opt) can be
written as,
− 1
σ2
2 (c) (13)
N Bc (ζ )
ˆVi (μ, σ ) = κ
CLT Ic,r,V · br (opt)
(12)
where, N B ∼ N (μ1 (c), σ1 (c)) and CLT I ∼ N (μ2 (c), σ2 (c))
are normally distributed random variables. Eqs. (12) is the
quotient of two random variables, the statistical parameters
of voltage can be estimated as [20],
ˆμVi (c) ∼
μ1 (c)
ρ12 (c) · σ1 (c)σ2 (c) +
=
μ2 (c)
μ1 (c)
μ2
2 (c)
μ3
2 (c)
similarly, the standard deviation of voltage can be estimated
as,
ˆσVi (c) ∼
=
σ 2
1 (c)
μ2
1 (c)
μ1 (c)
μ2
1 (c)
μ2
2 (c)
μ3
2 (c)
where, μ2
2 (c) and μ3
2 (c) are the 2nd and 3rd central moments
of each communication task c, respectively. In Eqs. (13) and
(14), ρ12 is the cross correlation between two random variables. Since, CLT I and N B are the dependent variables their
cross correlation is equal to unity, i.e. ρ12=1. The moments
of random variables can be expressed as,
x = E {(x − μ)
n } =
(x − μ)
μn
2 (c) − 2ρ12
σ2
n · m(x)
σ1 (c)σ2 (c)
∞(cid:2)
(15)
(14)
+
−∞
where, function m(x) is a probability mass function and μ is
the mean of a random variable x. Similarly, mean of the slack
ˆμS lack can be estimated as,
E [S lack] = dlc −
κ
E [N B (ζ )] − tASAPc
(16)
bopt · Vi
6. BUS SYNTHESIS AND OPTIMIZATION
The stochastic nonlinear programming formulation for simultaneously voltage scaling, bus selection, scheduling, binding of communication tasks c is given as follows:
Minimize:
C ostr · br
(cid:2)
(17)
r∈R
Where, r ∈ R is a library of on-chip communication buses
type, for example, bus of 16, 20, 24, · · · , 128-bit wide. The
C ostr of bus type r is expressed in terms of bus width, like the
cost of 32-bit wide bus is double the cost of 16-bit wide bus.
The ob jective is to minimize total cost of buses by maximizing
the sharing of buses among the communication tasks.
sub ject to,
∀c ∈ C,
(cid:2)
r∈R
Ψ(cid:2)
Vmin(cid:2)
t=ASAPc
V =Vmax
Xc,t,r,V = 1
(18)
(19)
i,j )
Ψ = (ALAPc + dminc − CLT Ic,r,V − δΔV
The variables those need to be determined in the formulation
are the bus b of type r , and binary decision variable Xc,t,r,V .
task ci at time t ∈ {0, · · · , λ}, bus type r and with supply
The binary variable indicates scheduling of a communication
voltage V , respectively. Where the term δΔV
is the time
overhead delay due to switching of voltage from Vi to Vj .
Each communication task c ∈ C must be mapped to a
single bus type r , operating at a single time t, with supply
voltage V as expressed in the Eqs. (18). The communication task c ∈ C with ALAP time ALAPc cannot be executed
later than Ψ, when the data is transferred through a bus b
of type r with the communication lifetime interval of a task
CLT Ic,r,V . The term dminc is the minimum time to execute
the communication task c ∈ C to meet the deadline dlc .
We introduce a set Ω, which represents the set of all times
that any communication task could possibly start at,
{ASAPc , · · · , ALAPc}
(cid:3)
i,j
Ω =
(20)
c∈C
∀t ∈ Ω, ∀r ∈ R,
(cid:2)
c∈C
(cid:2)
Vmin(cid:2)
(t
(cid:2) ∈{t,··· ,t+dr −1}
∩{ASAPc ,··· ,ψ)
V =Vmax
,r,V ≤ br
Xc,t
(cid:2)
(21)
No communication bus b of type r can execute more than one
communication task at a time t with voltage V is expressed
in constraint Eqs. (21). The ﬁrst summation is over all the
communication tasks with bus type r and the second sum(cid:2)
mation is over a ”time window” covering all start time t
for
which the communication tasks could overlap.
∀ (c
(cid:2)
, c) ∈ Π, ∀ (c
(cid:2)
, c, w) ∈ Depn,
t · Xc,t,r,V ≥ (cid:2)
r∈R
(cid:2)(cid:2)
Ψ
t=
ASAP
c
(cid:2)
(22)
(cid:2)
Ψ(cid:2)
Vmin(cid:2)
ASAPc
Vmin(cid:2)
r∈R
t=
V =Vmax
(t + CLT Ic
i,j ) · Xc
,r,V + w + δΔV
(cid:2)
(cid:2)
,t,r,V
V =Vmax
c
(cid:2)
(cid:2)
Ψ
(23)
(cid:2) + dmin
(cid:2)
= (ALAPc
,r,V − δΔV
i,j )
− CLT Ic
The data dependency between the communication tasks is
expressed as in Eqs. (22). The term in the right hand side of
the equation expresses that the communication task c should
(cid:2)
(cid:2)
be executed only after the execution time Ψ
of task c
. The
delay w is a ﬁxed time delay between two communication
tasks to execute the data processing task τi .
V = {V1 , V2 · · · , Vn }
(24)
The slack of each communication task c ∈ C is exploited to
share a communication bus and reduce the total energy consumption by scaling the supply voltage V for each communication task from the nominal voltage. The possible discrete
voltages are constrained by the Eqs. (24).
∀ c ∈ C, P ((
(cid:2)
Ψ(cid:2)
r∈R
t=
ASAPc
Vmin(cid:2)
V =Vmax
(dlc − t − CLT Ic,r,V − δΔV
i,j ) · Xc,t,r,V ) ≥ 0) ≥ η
(25)
δΔV
The summation of deadline dlc , start time t, the data transfer
time (CLTI) and the delay overhead due to voltage switching
i,j of each communication task c ∈ C should be greater than
or equal to zero with a probability η for on-chip bus b of type
r , supply voltage V as shown in Eqs. (25).
The above communication bus optimization problem has
constraints, which are nonlinear and formalized as a convex
quadratic optimization problem. The problem of simultaneous communication bus synthesis and voltage scaling is similar to the discrete time-cost trade-oﬀ problem, which is known
to be a NP-complete [7]. We also propose a heuristic using
a linear relaxation method with limited number of discrete
voltages so that the problem can be optimized with a quasipolynomial time complexity [24]. Since the convex quadratic
constraint is a convex function for η > 50%, they guarantee
a globally optimal solution [12].
7. EXPERIMENTAL RESULTS
We evaluate the eﬀectiveness of the proposed techniques
using generated benchmark as well as a real-life application,
namely speech recognition system [1]. The automatically generated benchmark consists of 119 communication tasks c and
data to be transferred by all tasks c are normally distributed
with mean μc (N B )=128 bit. Diﬀerent level of variability in
data size N Bc (ζ ) were explored ranging from 2% to 30% of
3σ . The deadline dlc of each task c is deterministic and it is
diﬀerent for diﬀerent values of σc (N B ). The data processing
time w of each task τ are given for each pair of communication tasks. We took the maximum value of w of each task τ
assuming that the on-chip modules are capable to scale the
voltages for changing data traﬃc. Each communication task
c can scale the supply voltage ranging from 1.8V to 0.6V, to
meet the desired timing yield. The on-chip communication
buses are given as a library of buses with diﬀerent size of bus
width, which ranges from 16 to 128 bit wide with an increment of 4-bit. For the experiment purpose, we consider a
bus with 4mm in length and its corresponding single line capacitance for 0.07μm technology is 609f F . These technology
dependent parameters were adopted from [16].
The ﬁrst set of experiment was conducted to synthesize the
minimum size of bus width and number of buses with reduced
communication energy, using voltage scaling technique. We
performed simultaneous voltage scaling (continuous), bus selection, scheduling and binding of communication tasks c using the proposed algorithm. The algorithm was implemented
in C as a pre-processing model to interface with a convex
solver of MOSEK [2]. Table 1 shows the results of optimized
size of bus width and number of buses for the automatically
generated tasks c. The table compares the size of bus width
and number of buses br (opt), mean voltage ˆμV and mean slack
ˆμS lack for the diﬀerent timing yield η . The results show that
optimized size of bus width and number of buses change with
timing yield η . In column 2 and 5 of the table, the size of
br (opt) are constant for two diﬀerent values of η , however,
the mean voltage ˆμV and mean slack ˆμS lack decrease in column 6 and 7, respectively. This is because of increased in
timing yield of communication tasks from 79% to 89%.
In
column 8 of the table, there are two buses with diﬀerent bus
width for all values of 3σ . In this case, timing yield η of all
the tasks is set to 99%, so that voltage of all the tasks c are
scaled to the minimum possible value. This results very small
amount of slack of communication tasks. Note that higher
the amount of slack, more the mobility of communication
tasks c, which maximizes the sharing of communication bus.
Hence, at the timing yield of 99%, there is very less mobility
of communication tasks c, which results the overlap among
them so that two separate buses are needed to meet the realtime constraint. Fig. 4 depicts, the estimated cumulative
distribution function (CDF) of voltage for η = 79%, 89% and
br (opt) = 48 bit wide. The estimated voltages have Cauchy
distribution with mean ˆμV and ˆσV from Eqs. (13) and (14).
The results show that the mean of voltage ˆμV is high in case
of η = 79% than the value of η = 89%.
The second part of experiment was conducted on the CMU
Sphinx [1] for speech recognition, which consists of three main
components front end, training and recognition. The front
end includes series of data processing tasks such as the preemphasis, hamming window, FFT (fast fourier transformation), mel frequency ﬁlter, IFFT, cepstral mean normalization, feature extraction to generate the features from the
speech. The training takes as input a large number of speech
along with their transcriptions into phonemes to provide the
speech models for the phonemes. The recognition is based on
the HMM (hidden markov model) to decode the speech. We
use the American English lexicon consisting of 32 phonemes
and a database of 17 diﬀerent words (spelling out name of
month, numbers and digits). The length and number of phonemes in a speech depends on application to application. After the partitioning, the front end was mapped to the dedicated hardware of FFT and ﬁlters. The tasks training and
recognition were mapped to the one PowerPC processor. The
co-simulation was carried out in Seamless CVE [3] from Mentor Graphics. The resulting co-simulation traces were used
to capture the communication tasks c and obtained the communication task graph Gc (C, Π). We considered the speech
length that varies from 1.06 to 11.8 sec and depending on
length, the recognition time changes. To shorten the recognition time, the FFT was conﬁgured to 256, 512 and 1024 point
and burst size of 3 to 6. The data model of the communication task of the FFT N Bf f t (ζ ) was approximated as a normal
distribution, with mean μN Bf f t = 248 bit and standard deviation σN Bf f t = 44. While, the data model of the communication tasks of a processor was kept deterministic. Fig. 5 shows
the results of communication bus synthesis and mean energy
consumption for timing yield ranging from 79% to 99%. In
ing with possible voltages {0.6V, 1.0V, 1.2V, 1,4V, 1.6V}. A
this part of experiment, we performed discrete voltage scalconstant single bus of 48 bit wide (cost 37.5) was obtained
for η ranging from 79% to 88% in Fig. 5(b), while the mean
communication energy consumption was reduced upto 57%
in Fig. 5(a) by scaling the voltage. For the timing yield η
> 88%, the amount of slack is less, which oﬀers less mobility
of communication tasks c to share the same communication
bus. Hence, for the η > 88%, two buses of 24 and 32 bit wide
(cost 43.7) was obtained as shown in Fig. 5(b).
Summarizing the experiments, the trade-oﬀ between minimization of buses and energy reduction was explored by varying the timing yield, during the communication synthesis. We
have seen that increasing the timing yield η can help to reduce
the energy consumption, however, if the value of η increases
from a certain limit, the mobility of communication tasks will
reduce and results additional some more buses to meet the
real-time constraint. So, the timing yield η can be used as a
tuning factor to synthesize the minimum size of bus width and
minimum number of buses with the reduced communication
energy consumption.
8. CONC"
Behavior and communication co-optimization for systems with sequential communication media.,"In this paper we propose a new communication synthesis approach targeting systems with sequential communication media (SCM). Since SCMs require that the reading sequence and writing sequence must have the same order, different transmission orders may have a dramatic impact on the final performance. However, the problem of determining the best possible communication order for SCMs is not adequately addressed by prior work. The goal of our work is to consider behaviors in communication synthesis for SCM, detect appropriate transmission, order to optimize latency, automatically transform the behavior descriptions, and automatically generate driver routines and glue logics to access physical channels. Our algorithm, named SCOOP, successfully achieves these goals by behavior and communication co-optimization. Compared to the results without optimization, we can achieve an average 20% improvement in total latency on a set of real-life benchmarks","Behavior and Communication Co-Optimization  
for Systems with Sequential Communication Media 
Jason Cong, Yiping Fan, Guoling Han, Wei Jiang, Zhiru Zhang 
Computer Science Department  
University of California, Los Angeles 
Los Angeles, CA 90095, USA 
{cong, fanyp, leohgl, wjiang, zhiruz}@cs.ucla.edu 
ABSTRACT 
In this paper we propose a new communication synthesis approach 
targeting systems with sequential communication media (SCM). 
Since SCMs require that the reading sequence and writing sequence 
must have the same order, different transmission orders may have a 
dramatic impact on the final performance. However, the problem of 
determining the best possible communication order for SCMs is not 
adequately addressed by prior work. The goal of our work is to 
consider behaviors in communication synthesis for SCM, detect 
appropriate transmission order to optimize latency, automatically 
transform the behavior descriptions, and automatically generate 
driver routines and glue logics to access physical channels. Our 
algorithm, named SCOOP, successfully achieves these goals by 
behavior and communication co-optimization. Compared to the 
results without optimization, we can achieve an average 20% 
improvement in total latency on a set of real-life benchmarks.  
Categories and Subject Descriptors 
B.4.4 [Hardware] Input/Output and Data Commutations  
General Terms 
Algorithms, Performance, Experimentation 
Keywords 
Communication, FIFO, Optimization, Scheduling, Reordering 
1. INTRODUCTION 
With the rapid increase of complexity in system-on-a-ship (SoC) 
design, the synthesis community is moving from RTL (register 
transfer level) synthesis to a higher level of abstraction (e.g., 
behavioral-level and system-level synthesis). Two of the essential 
problems related to system-level design are partitioning and 
communication synthesis. The goal of partitioning is to distribute 
and parallelize the functionalities of a system to subsystems. 
Communication synthesis is another important sub-task for systemlevel synthesis [2]. Typical communication synthesis techniques 
adopt a top-down approach, including the following steps: (i) 
channel binding and network synthesis (e.g., [3][6][12]); (ii) 
protocol refinement (e.g., [4]); (iii) interface synthesis (e.g., 
[6][9][10][11]). The communication synthesis approach proposed by 
Yen in [3] handles the network topology generation. Their algorithm 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. 
DAC 2006, July 24–28, 2006, San Francisco, California, USA.. 
Copyright 2006 ACM 1-59593-381-6/06/0007…$5.00. 
can create new PEs and buses to meet the design time constraints. 
Some platform-based approaches such as Daveau’s work in [5], take 
a given communication library and solve channel binding, protocol 
refinement and interface generation in a more integrated way as a 
binding problem. In [7] Knudsen incorporates the communication 
protocol 
selection 
as 
a 
design 
parameter within 
the 
hardware/software partitioning.  
Most of the aforementioned approaches [3][4][6][8][9][10][11] 
consider communication synthesis as the final step of the cosynthesis systems, and the behavior of each subsystem is retained 
during communication synthesis. However, this type of approach 
may 
lose optimization opportunities, especially when SCMs 
(sequential communication media) are used to implement the 
communication channels. The transmission order of SCMs may 
have a dramatic impact on the performance of the entire system. 
According to our experiments on several real-life designs, the 
performance may be 2X better if the order is carefully optimized. A 
well-known example is the fast simplex link (FSL) [20] in Xilinx 
FPGAs. Buses could be also considered as an SCM with respect to 
each transaction from one specific master to a slave.  
An example is shown in Figure 1. Figure 1(a) shows the original C 
description of an application, which is a matrix multiplication 
algorithm. Suppose after the design exploration step, as shown in 
Figure 1(b), the system-level synthesis engine decomposes the 
system into two processes, one for generating arrays A and B, and 
the other for matrix multiplication. An abstract channel is introduced 
to transfer A and B. In Figure 1(c) the two processes are mapped to 
two processing elements (PE), and communicate data through a 
FIFO. A better order than the native row-based layout order is 
shown in Figure 1(d), which sends the two matrices in an interleave 
fashion, and calculates the product based on the data received. Our 
simulation result shows a 17% improvement in total latency with the 
new order.  
Our approach to communication synthesis mainly focuses on the 
following objectives: detect the optimal communication order and 
computation order for data communication on SCM to optimize total 
latency; 
transform 
the behavior description based on 
the 
computation order, and automatically generate drivers and glue 
logics. To our knowledge, this is the first work that integrates 
behavior transformation with communication optimization at the 
communication synthesis step.  
The remainder of our paper is organized as follows: Section 2 
formally defines our problem. An algorithm to solve this problem is 
explained in details in Section 3. Section 4 shows the experimental 
results and is followed by our conclusions in Section 5. 
 
 
for (int i=0; i<N; i++)  
for (int j=0; j <N; j++)  
S1: A[i][j] = …; 
for (int i=0; i<N; i++)  
for (int j=0; j <N; j++)  
S2: B[i][j] = …; 
for (int i=0; i<N; i++)  
for (int j=0; j<N; j++) { 
S3: C[i][j] = 0; 
for (int k=0; ... 
} 
(a) 
for (int i=0; i<N; i++)  
for (int j=0; j<N; 
j++)  
S1: A[i][j] = …; 
Channel c 
A[N][N], B[N][N] 
for (int i=0; i<N; i++)  
for (int j=0; j<N; j++) {
S3: ... 
Process 1 
Custom 
Logic 
PE1 
(b) 
FIFO 
(c) 
} 
Process 2
Processor 
PE2 
A 
A 
A 
A
B 
B 
B 
B 
Process 1 
Process 2 
C 
(d) 
Figure 1. (a) Original C description; (b) Process network model 
after partitioning; (c) Hardware implementation model;  
(d) A better order than the native row-based order. 
2. PROBLEM DESCRIPTION 
In our SCM communication optimization approach, we assume that 
a set of processes P = {pi | i = 1, 2... n} is given. Since FIFO is 
widely used in practice, in the following context we will not 
distinguish SCM and FIFO. The behavior for each process pi, is 
captured by a control data flow graph (CDFGi). A CDFG contains a 
set of basic blocks connected by control edges. Each basic block is a 
data flow graph, in which a node represents an operation and an 
edge represents a data dependency between two nodes.  
For each process pi, we assume that it is already allocated to one 
physical processing element PEi. The characteristics of each PEi, 
such as delay, area and power, can be obtained from the target 
platform specification. Design constraints such as resource 
constraints and timing constraints are associated to each process as 
well.  
Processes communicate via a set of abstract channels C. Each 
abstract channel ci is associated with a data set D = {d1, d2… dm} to 
be transferred from the producer process to the consumer process. 
As mentioned before, Figure 1 shows an example of a process 
network and physical channels.  
With the above notions, we can formulate the SCM communication 
synthesis problem as follows:  
Problem: Given a set of processes P connected by channels in C, 
and a set of data D = {d1, d2, …, dm} to be transmitted on each 
channel cj, find the optimal transmission order based on the CDFG 
of each process, such that the overall latency of the process network 
is minimized subject to the given design constraints and platform 
specification, and generate drivers and glue logics for each process 
automatically.  
This problem can be divided into three sub-problems: 
(i) Communication order detection: Given the CDFG model of each 
process, the data to be transmitted and the platform information, we 
detect the optimal transmission order to minimize the total latency.  
(ii) Code transformation: To enable the optimal communication 
reordering, we may also need to change the computation order in the 
appropriate behavioral models. These changes are carried out 
without violating the data dependency.  
(iii) Interface generation: We generate interface drivers and glue 
logics for given physical channels. 
3. SCOOP ALGORITHM 
This section introduces our overall design flow to solve the SCM 
optimization problem. First, we try to detect the optimal order. 
Based on that order, we then automatically transform the code and 
generate the interfaces. An indices compression step is performed to 
further reduce loop transformation overhead. Our algorithm is called 
SCOOP (SCM CO-OPtimizaiton).  
3.1 Communication Order Detection 
In this step we try to find a transmission order of data 
communication that leads to the minimum latency, with the freedom 
to change the order of computations in processes as well. In 
particular, we show that our problem can be transformed to the 
resource-constrained scheduling problem. The main steps of our 
communication order detection algorithm are outlined below. 
Step 1: Construct a global CDFG by merging the individual CDFGs 
of each process in the process network.  
Step 2: Change each data element d which is transmitted by SCMi to 
a special type of operation Ti. At most k number of Ti operations can 
be executed at any point of time for each SCMi, where k is the 
number of concurrent operations allowed on SCMi (typically, k 
equals one for a FIFO). We then set up the correct data dependencies 
by linking the definition and uses of d to Ti.  
Step 3: Solve a resource-constrained scheduling problem to 
optimize the total latency of the global CDFG.  
Figure 2(a) shows a simple process network with two processes 
communicating by FIFO. In this example, we are transmitting three 
elements. Using the original order (1, 2, 3), the final total latency is 
seven cycles, as shown in Figure 2(b). If we add the T-type 
operations and the appropriate data dependencies to obtain the 
global CDFG, we can reduce the totally latency to five cycles. The 
new schedule is shown in Figure 2(c) and the corresponding 
communication order is (1, 3, 2) which maximizes the overlap of 
computations and communications.  
Theorem: Solving the order detection problem is equivalent to 
solving the resource constrained scheduling problem on the global 
CDFG constructed in Step 2, and we can obtain the optimal solution 
if the algorithm used in Step 3 gives the optimal solution.  
Proof: Since we assume that each FIFO has a fixed transfer delay, it 
could be viewed as a special hardware resource. We could enforce 
the resource constraints in the scheduling problem as follows: There 
are |C| types of transmission resources T = {tr1, tr2 … tr|c|} where C 
is the set of available FIFOs in the process network. Hence, we are 
able to reduce the problem to the resource-constrained scheduling 
problem on the global CDFG constructed in Step 2. (cid:0) 
The scheduling problem with resource constraints is NP-complete in 
general. In this work we adopt a list-scheduling-based algorithm to 
solve our problem. List scheduling [13] is one of the most popular 
techniques for the resource-constrained scheduling. In our case, we 
combine the ALAP (as late as possible) and ASAP (as soon as 
possible) schedules to prioritize the operations. 
 
 
 
 
 
Process 1 
− 
− 
− 
− 
T1 
+ 
− 
− 
*
T3
T2
− 
T1 
− 
* 
T2 
− 
T3 
+ 
− 
* 
T1 
T2 
T3 
Process 2 
+ 
+ 
− 
(a) 
(b) 
(c)  
Figure 2. (a) Merged CDFG; (b) Scheduling result with order (1, 
2, 3); (c) Scheduling result with order (1, 3, 2). 
Note that the traditional list scheduling algorithm primarily works 
well on the data flow graphs. Nevertheless, we can further extend 
our algorithm to handle loop-intensive and data-intensive designs 
with control flows, which prevail in the multimedia processing 
domain. To apply our algorithm to general CDFGs, we try to 
collapse a CDFG C to a DFG D. For an if-then-else statement, our 
algorithm treats this structure as a non-decomposable operation in D, 
and takes the longest execution path as the latency. With a for loop, 
we cannot simply change the loop body into one operation since it 
may iterate multiple times. In one loop iteration, we change the loop 
body to a set of nodes in the new DFG D, and calculate indices for 
each array access. The iteration spaces in C are then fully expanded 
in D. Currently we do not perform any optimizations on more 
general loops (e.g. while loops). However, users may choose to 
restructure a while loop into a for loop if the iteration bound can be 
derived from the program.  
After the above transformations, the size of D may become much 
larger than the original CDFG. However, after the order detection 
step, we will use the code transformation techniques described in the 
following section to compress a set of nodes in D back into a loop 
structure.  
3.2 Code Transformation 
Once we obtain the optimal communication order, we need to make 
necessary changes to the original behaviors, as well as generate the 
drivers and the glue logics for those processes. 
For DFG cases, the code transformation and interface generation are 
quite straightforward. We dump the behavior of each process based 
on the computation order we obtained, and insert drivers and glue 
logics for each process. If the computation order is consistent with 
the data communication order, drivers and glue logics are inserted 
immediately when the data is ready to be read or written to the 
physical channel; otherwise, we should delay 
the 
interface 
generation for one element until all the elements, which should be 
transmitted earlier, have been processed. 
The main difficulties in dealing with for CDFG code transformation 
are the loops. Since the loop iteration space has been completely 
expanded in code detection, it is not feasible to dump the expanded 
code directly. Therefore, we use the iteration reordering technique to 
generate reconstructed loops. In the compiler domain, a great deal of 
literatures focus on loop iteration reordering [14][15]. Our approach 
does the loop transformations in the compile-time, with auxiliary 
memory space to store the reordered sequence for handling general 
loop transformations.  
A loop’s iteration space is a set of integer tuples with constraints 
indicating the loop bounds.  
J = {[j1, j2… jn] | lb1≤j1≤ub1 ∧…∧ lbn≤jn≤ubn} 
Each iteration space has a function f: J(cid:198)Z+ to map the iteration 
space to the logic time steps (Z+ denotes the positive integers). An 
iteration-reordering transformation is expressed with a mapping T 
that assigns each iteration vector ji in an original iteration space to ji’ 
in a new iteration space, so that f(ji)=f’(j’i). An intuitive way to 
implement iteration reordering is shown in Figure 3(b): a reordering 
array (RA) is generated for each loop, and at the beginning of each 
iteration, and we should read in the new iteration vector from the 
RAs. 
Before: 
After: 
{(0,0),(0,1),(0,2),(0,3),…..(4,0),(4,1),(4,2),(4,3),(4,4)} 
{(0,0),(1,0),(2,0),(3,0),…..(0,4),(1,4),(2,4),(3,4),(4,4)} 
(a) 
// Reordering arrays 
int RA_i = {0, 0, 0, 0,…4, 4, 4, 4} ;  
int RA_j = {0, 1, 2, 3,…1, 2, 3, 4};  
for (int i=0; i<N;i++) 
for (int j=0; j<N; j++) { 
int i’ = map_i(i); 
int j’ = map_j(j); 
A[i’][j’] = …; 
fifo_write(A[i’][j’]); 
}
(b) 
// After indices compression 
for (int i=0; i<N;i++) 
for (int j=0; j<N; j++) { 
int i’ = j; 
int j’ = i; 
A[i’][j’] = …; 
fifo_write(A[i’][j’]); 
} 
(c) 
Figure 3. (a) Iteration space before and after order detection; (b) 
Iteration reordering through reordering arrays; (c) 
Transformed code after indices compression. 
The overhead of above approach is a result of two factors: storage 
overhead introduced by RAs and computational overhead of 
memory access at the beginning of iterations. Pre-fetching can 
reduce the computational overhead if the target PE supports certain 
parallelism in execution. We also developed another technique to 
reduce storage size which is called indices compression. The 
problem is described as follows:  
Problem: Given two sets of m-tuples {J1, J2 …Jn} and {J’1, J’2,…, 
J’n} where each Ji (or Ji’) represents an indices vector of one 
iteration, find the minimum number of intervals [pi, qi], satisfying 
that within each interval there exists a (qi - pi + 1) × m matrix Mi, 
such that  
Ji*Mi=J’i,     for all pi≤i≤qi 
It is clear that if we can find a matrix Mi for each interval [pi, qi], we 
then can express the new iteration vector using a linear combination 
of old indices variables. In Figure 3(c), the total iteration vectors can 
be merged into one interval with Mi as a reverse matrix, the new 
code after indices compression can remove all those RAs. We solve 
this problem in a greedy but near-optimal way. We start at an 
interval with zero length, and the interval continues to grow as long 
as the above condition is satisfied. If the current interval cannot 
grow any more, a new interval is inserted. The condition test can be 
performed by solving linear equations. If the number of intervals is 
small, then we can transform the original loops into several loops. 
Otherwise, we will store the start position of intervals and their 
matrices Mi in RAs, and change the loop body to calculate reordered 
iteration vectors based on current interval. In the worst case, after 
these optimizations the overhead introduced by iteration reordering 
may still offset the performance gain by reordering. However, in 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
practice, the number of intervals we generated is reasonably small 
due to the regular patterns in the programs. 
4. EXPERIMENTAL RESULTS 
We implemented our SCOOP communication synthesis system in 
C++/Unix environments. The target communication architecture in 
this experiment is currently fixed to a two-process producerconsumer model. Our SCOOP algorithm works as an optimization 
pass in our platform-based system-level and behavior-level synthesis 
infrastructure [1], which can take C or SystemC as the input. The 
scheduler [17] inside our behavior-level synthesis system is used to 
solve the scheduling problem mentioned in Section 3.1. Without the 
SCM co-optimization, our system will transmit data, including 
arrays and scalars, based on their original program order, and each 
array is sent according to the memory layout. After the SCM cooptimization, we will insert drivers to access SCM based on the 
optimized order, as discussed before. We use the mathematics 
library LAPACK++ [18] to solve linear equations in indices 
compression. We generate the VHDL code using the RTL backend 
of the system-level synthesis system for both scenarios. To obtain 
the final latency, Modelsim [19] is used as the simulator, and we 
developed a FIFO module in VHDL which resembles the behaviors 
of the Xilinx FSL. 
Benchmarks can be divided into two categories. One set of 
benchmarks includes DCT1, DWT and Haar, which are all DFG 
examples. The DCT1 example 
is an unrolled version of 
chenDCT8x8, which does the row and column DCT transformation 
on an 8x8 data block. The DWT example is part of the JPEG2000 
program. The Haar example 
implements a simple Haar 
transformation in image processing. The comparison results on those 
benchmarks are shown in Table 1. We can see that SCOOP will 
improve about 10% on those three examples in terms of latency in 
cycles. 
Table 1 .Experimental results. 
Total latency (Cycle#) 
RAs Compression 
Trad. 
SCOOP Reduction Before 
After 
325 
290 
10.77% 
0 
0 
689 
617 
10.45% 
0 
0 
142 
134 
5.63% 
0 
0 
483 
419 
13.25% 
80 
64 
1903 
1084 
43.04% 
300 
0 
620 
420 
32.26% 
192 
0 
408 
339 
16.91% 
96 
20 
Designs 
DCT1 
DWT 
Haar 
DCT2 
Dot 
Masking 
Mat_mul 
Another set of benchmarks consists of CDFG examples, including 
DCT2, Dot, Masking, and Mat_mul. The DCT2 example is the 
CDFG version of chenDCT8x8. Mat_mul and Image masking are all 
from Mediabench [16]. The Dot production example implements a 
dot production algorithm. An average of 26% improvement in total 
latency can be achieved for those examples, as shown in Table 1. 
Intuitively our approach gets better results on CDFG cases because 
CDFG has more control dependencies on operations than DFG (e.g., 
the instructions in the loop body must be executed consecutively), 
thus our decision will have a bigger impact for CDFGs. As shown in 
Table 1, RA compression can dramatically reduce the storage size 
(in number of integer data entries) needed for iteration reordering. 
5. CONCLUSIONS AND FUTURE WORK 
In this paper the behavior-communication co-optimization problem 
is addressed for SCMs, and a two-step approach is developed to 
solve this problem. Our algorithm applies to both DFG and CDFG. 
Experimental results demonstrate the efficacy of our work. Our 
future work will focus on further reducing the overhead introduced 
by reordering and applying the SCOOP algorithm in our platformbased synthesis system for design space exploration. 
ACKNOWLEDGMENTS 
This research 
is supported by 
the Semiconductor Research 
Corporation, Gigascale Silicon Research Center, National Science 
Foundation, and grants from the Altera Corporation, Magma Design 
Automation, Inc., and Xilinx, Inc. under the California / MICRO 
program. 
"
Synthesis of high-performance packet processing pipelines.,"Packet editing is a fundamental building block of data communication systems such as switches and routers. Circuits that implement this function are critical and define the features of the system. We propose a high-level synthesis technique for a new model for representing packet editing functions. Experiments show our circuits achieve a throughput of up to 40Gb/s on a commercially available FPGA device, equal to state-of-the-art implementations","Synthesis of High-Performance
Packet Processing Pipelines
∗
Cristian Soviani
Columbia University, CS Dept.
New York, New York
Ilija Had \377zi ´c
Bell Labs, Lucent Tech.
Murray Hill, New Jersey
Stephen A. Edwards
Columbia University, CS Dept.
New York, New York
†
soviani@cs.columbia.edu
ihadzic@bell-labs.com
sedwards@cs.columbia.edu
ABSTRACT
Packet editing is a fundamental building block of data communication systems such as switches and routers. Circuits that implement
this function are critical and deﬁne the features of the system. We
propose a high-level synthesis technique for a new model for representing packet editing functions. Experiments show our circuits
achieve a throughput of up to 40Gb/s on a commercially available
F PGA device, equal to state-of-the-art implementations.
Categories and Subject Descriptors
B.6.3 [Hardware]: Logic Design—Design Aids
General Terms
Algorithms
Keywords
Packet processors, Networking, FPGAs, high-level synthesis
1.
INTRODUCTION
A packet switch (or router) is a basic building block of data communication networks. Its primary role is to forward packets based
on their content, speciﬁcally header data at the beginning of the
packets. As part of this forwarding operation, packets are classiﬁed, queued, modiﬁed, transmitted, or dropped.
The forwarding algorithms used in most switches are simple to
facilitate efﬁcient hardware implementation. However, they are tedious to code at the RT level because performance demands them
to be deeply pipelined circuits that operate on many bits in parallel
(e.g., 128 or 256) and interact with high-speed FI FOs. This makes
for complicated datapaths and controllers.
We propose a high-level synthesis technique for packet editing
processors. We target F PGAs, although A S ICs are also possible.
∗
This work was done while Soviani was at Bell Labs.
†Edwards and his group are supported by an NSF CAREER award,
gifts from Intel and Altera, and by the SRC.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2006, July 24–28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007 ...$5.00.
Switching
Fabric
Line Card
Ingress
Trafﬁc Manager
Egress
Trafﬁc Manager
Ingress
Packet Processor
Egress
Packet Processor
V LAN
pop
V LAN
push
M P L S
push
T T L
update
AR P
resolve
memory
lookup
memory
lookup
Figure 1: A switch with detail of a packet processing pipeline.
Packet editing is a pivotal function of most switches, which also
include trafﬁc managers, a switching fabric, and other components
(Figure 1). We propose a novel way to model algorithms that transform input into output packets and present a synthesis procedure to
translate these models into efﬁcient VHD L code. Our technique produces packet editing blocks that are easily connected in pipelines.
We produce circuits for F PGAs that can sustain 40 Gbps throughput on industrial examples, equal to state-of-the-art switches (a
10 Gbps A S IC was novel in 2003 [6]) and are vastly easier to write
and maintain than RT L descriptions.
2. PACKET PROCESSORS AND EDITING
Figure 1 is a block diagram of a packet switch consisting of line
cards (we only show one) connected to a switching fabric that transfers packets. We focus on designing the line cards, which provide
network interfaces, make forwarding and scheduling decisions, and,
most critically, modify packets according to their contents.
Our synthesis technique builds components in the ingress and
egress packet processors. A packet processor is a functional block
that transforms a stream of input packets into output packets. These
transformations consist of adding, removing, and modifying ﬁelds
in the packet header. In addition to headers deﬁned by network protocols, the switch may add its own control headers for internal use.
Packet processors perform complex tasks through a linear composition of simpler functions. This model has been used for software implementations on hosts [7] and on switches [4]. A less suitable architecture for hardware is a pool of task-speciﬁc threads that
process the same packet in parallel without moving the packet [1].
We use a unidirectional, linear pipeline model that simpliﬁes the
implementation without introducing major limitations. For example, the loops in Kohler’s IP router [4] only handle exceptions. We
would do this with a separate control processor.
While the logical ﬂow can fork and join, we implement only linear pipelines that can use ﬂags to emulate such behavior. Non-linear
pipelines are more complicated and would not improve throughput.
Restruct(node n, pending bits v, word size w)
clean-visit ← true if v is empty
if clean-visit and cache contains n then
return cache[n]
case type of node n of
≥ 1 bytes, one successor
output data :
if v is w ∗ 8 bits then
append n to v
put n in current word
(cid:4) ← build-node(v)
ﬁnished word
n
next word node
(cid:4)(cid:4) ← Restruct(successor of n, (), w)
n
Make n
the successor of n
else
(cid:4) ← Restruct(successor of n, v, w)
n
conditional :
(cid:4) = copy of the conditional n
n
for each successor s of n do
(cid:4)(cid:4) = Restruct(s, v, w)
n
Add n
as a successor of n
if clean-visit then
cache[n] ← n
return n
the restructured node for n
(cid:4)(cid:4)
(cid:4)
(cid:4)
(cid:4)(cid:4)
(cid:4)
(cid:4)
Figure 2: Steps in synthesizing a P EG model for the M P L S push module in Figure 1.
Figure 3: Structuring a packet map into
words
(a) The initial speciﬁcation
(b) Split into 64-bit words
(c) Read cycle indices and delay
bubbles added
If a packet needs to be dropped or forwarded to the control processor, we set ﬂags in the control header and perform the action at the
end of the pipeline. This guarantees every processing element sees
all the packets in the same order; packet reordering is usually done
in a trafﬁc manager, a topic beyond the scope of this paper.
Figure 1 shows a packet processing pipeline that edits the Virtual Local Area Network (V LAN) tag of an Ethernet packet [3] and
adds a Multi Protocol Label Switching (M P L S) label [8], based on
unique ﬂow identiﬁcation (FlowID). We assume a previous stage
has performed ﬂow classiﬁcation and prepended a control header
with a FlowID.
Both the V LAN push and M P L S push modules insert additional
headers after the Ethernet header, while the Time-To-Live (T T L)
update and Address Resolution Protocol (AR P) resolution modules
only modify existing packet ﬁelds. The V LAN pop module removes
a header. While this pipeline is simple, real switches just perform
more such operations, not more complicated ones.
Thus, packet processing amounts to adding, removing, and modifying ﬁelds. Even the ﬂow classiﬁcation stage, which often involves a complex search operation, ultimately produces a modiﬁed
header. We refer to these operations as packet editing; it is the fundamental building block of a packet processor.
In addition to the main pipeline, Figure 1 shows two memory
lookup blocks. These blocks store descriptors that deﬁne how to
edit the headers (e.g., how many M P L S labels to add). Here, the
FlowID is a index into descriptor memory. A memory lookup module is any component that takes selected ﬁelds and produces data
for a downstream processing element (e.g., I P address search, present
in all I P routers, is a form of generalized memory lookup). Flow
classiﬁcation is thus packet editing with memory lookups.
Modules that use memory lookup assume a previous pipeline
stage issued the request, which is processed in parallel to hide memory latency. We do not synthesize memory lookup blocks, but can
generate requests and consume results. Because our pipelines preserve packet order, simple FI FOs sufﬁce for memory interfaces.
Hence, we model packet processors as linear pipelines whose
elements have four types of ports: input from the previous stage,
output to the next, requests to memory, and memory results. Each
processing element must be capable of editing packets based on
their content and data from memory.
3. RELATED WORK
Kohler et al. [4] propose the C L ICK domain-speciﬁc language for
network applications. It organizes processing elements in a directed
dataﬂow graph. C L ICK speciﬁes the interface between elements to
facilitate their assembly. Although originally for software, Kulkarni
et al. [5] propose a hardware variant called C L I FF, which represents
its elements in Verilog. Schelle et al.’s [9] CU S P is similar. Brebner
et al. [1] add speculative multi-threaded execution.
Unlike C L I FF/CU S P, we synthesize our modules instead of assembling library components. C L ICK, furthermore, deﬁnes ﬁnegrained elements whose connection has substantial handshaking
overhead; our larger processors minimize this problem.
Our approach differs from classical high-level synthesis (c.f., De
Micheli [2]) in important ways. For example, we always use assoon-as-possible scheduling for speed; classical high-level synthesis considers others. Furthermore, most operations are smaller than
the muxes needed to share them, so we do not consider sharing.
Our technique differs most from classical high-level synthesis
in its choice of computational model. Rather than assume data are
stored and retrieved from memories, we assume data arrives and
departs a word at a time from FI FOs, thus our scheduling mostly
considers the clock cycle in which data arrive and can leave.
4. THE PACKET EDITING GRAPH
Although the behavior of a node in a packet editing pipeline
can be modeled at the RT level, doing so is awkward for deeply
pipelined circuits that operate on many bits in parallel. By contrast,
our Packet Editing Graph (P EG) model describes such nodes in an
implementation-independent way that is much easier to design and
modify, and it can be synthesized into efﬁcient circuitry.
Figure 2a shows a P EG for a simpliﬁed M P L S push module. The
M P L S protocol adds a label to the beginning of a packet that acts
as a shorthand for the I P header. When another switch receives the
packet, it uses separate rules to forward the packet. The module
in Figure 2a inserts up to three M P L S labels according to an inmemory descriptor. It also updates the label count (LC T) to reﬂect
any added labels, replaces the FlowID ﬁeld with the one from the
descriptor and updates various other ﬂags. Replacing the FlowID
maps a set of M P L S tunnels to the set of next-hop destinations.
A P EG is an acyclic, directed graph consisting of inputs (the
packet itself and data from a memory lookup block, drawn as rectangles in the top left section of Figure 2a); arithmetic and logical operators (the circular nodes in the middle of the ﬁgure); outputs (an output packet map, shown on the right side of the ﬁgure;
and data used to generate memory lookup requests, not shown in
this example) and the connections among those. In the ﬁgure, time
ﬂows from top to bottom and data ﬂows from left to right.
The packet map—the control-ﬂow graph on the right—is the
most novel aspect of a P EG. The bits of the output packet are assembled by following a downward path. A diamond-shaped node
is a conditional: control ﬂows to one of its successors depending
on the value of the predicate. Conditionals allow bits to be inserted
and deleted from the output packet. The ﬁnal node, marked with
dots, copies the remainder of the input packet to the output.
5. THE SYNTHESIS PROCEDURE
The challenge in synthesizing a circuit from a P EG is converting
the ﬂat, bit-level speciﬁcation into the sequential word-level implementation needed for performance. This is tricky because things
generally do not fall on word boundaries and some results may depend on bits that arrive later. Moreover, a P EG allows conditional
insertions and removals, so there is not always a simple mapping
between the word in which a byte is received and when it is sent.
Our synthesis procedure analyzes the P EG, establishes the necessary mapping, and builds a datapath and controller.
5.1 Wrappers and the Module Interface
We create synthesizable RT L by instantiating a manually-written
wrapper around a core synthesized from a P EG. The wrapper adapts
the core interface to the speciﬁc FI FO protocol.
Figure 5 illustrates a typical wrapper. Here, we do not show any
memory input/output ports, which are also handled by the wrapper.
They transfer exactly one word per packet, so the core sees the input
port as a parameter and the output port as a register.
Cores receive and send packets over a w-byte parallel interface
(w = 16 is typical). The module sees the input packet as a sequence
of w-byte words arriving on the idata port (Figure 5). Similarly,
the output is generated as a sequence of w-byte words on the odata
port. Three ﬂags on each port indicate packet boundaries: sop denotes the start, eop the end, and the mod signal indicates the number
of bytes in the ﬁnal word in a packet.
A core communicates with the wrapper through three more signals. rd and wr request data from the input and indicate when data
are written to the output. Suspend instructs the module to stall. The
wrapper in Figure 5 simply stalls the module when input data are
not available or when the output cannot accept new data.
5.2 Splitting Data into Words
Our synthesis procedure begins by dividing the input and output
packets on word boundaries using the procedure of Figure 3. Dividing the input packet is straightforward; reshaping the output packet
map is complicated because of conditionals. Figure 2b shows the
result of this on the M P L S example of Figure 2a.
We restructure the packet map so conditions are only checked at
the beginning of each word to guarantee that only complete words
are generated in every cycle (except the last, a special case). For
example, the > 0 condition in Figure 2a has been moved four bytes
earlier in Figure 2b and the intervening four bytes have been copied
to the two branches under the conditional to maintain I/O behavior.
The algorithm in Figure 3 recursively walks the packet map to
build a new one whose nodes are all w bytes long (the word size).
Each node is visited with a vector v that contains bits that are “pendFigure 5: A core module
and wrapper.
Figure 4: Controller synthesized
from the packet map in Figure 2c
(a)
(b)
Figure
6:
Scheduling
within a read cycle.
(cid:4)
ing” in the current word. Output nodes are added to this vector until
w ∗ 8 bits are accumulated, at which point a new output node is created by build-node, which assembles the saved bits in v. The algorithm handles conditionals by copying the condition to a new node
n
, which is placed at the beginning of the current word, and visiting the two successors under the conditional. The same v is copied
to each recursive call, effectively duplicating the rules for the bits
that appeared before the conditional in the current word.
This has the potential of generating an exponentially-large tree,
but in practice protocols are designed to avoid this. For example,
there are four paths in Figure 2b, but we ﬁnd they lead to only two
different states: the one- and three-label cases converge since they
require the same alignment; the zero and two cases are similar.
We handle reconvergence by maintaining a cache of nodes that
can be reused. If a node visit is “clean,” i.e., the pending vector is
empty, the cache is checked for an earlier visit that can be reused.
5.3 Assigning Read Cycle Indices
After splitting the packet map into words, we label each node
with the logical cycle in which its data are available. These indices
(black boxes in Figure 2c) are like clock cycles, but practically an
index may map to several clocks if the controller causes a stall.
The ﬁrst input word index is zero, the second is one, etc. The
rest are computed from causality: the index of a node is the highest
index of all its predecessors. Constant nodes and memory inputs,
assumed to be present in all cycles, are therefore ignored.
5.4 Scheduling
Once read cycle indices are assigned, we insert “bubbles” that
correspond roughly to pipeline stages. We draw these as black rectangles in Figure 2c. We insert them according to the following
rules: if two indices differ by k > 0, at least k bubbles are needed
between them; and any two output nodes in the packet map, even
with the same index, require at least one bubble between them.
In Figure 2c, two bubbles were inserted between the top-most
node and the ﬁrst output node because the difference between their
indices is two. This follows the ﬁrst rule. The ﬁrst word cannot
be output in cycle 0 because it depends on the ﬂags ﬁeld, which
becomes available in cycle 1. Following the second rule, bubbles
were also added after the ﬁrst conditional because these arcs are
between two output nodes. These bubbles are necessary because it
is not possible to write two words simultaneously, even though the
information for building the second word is available earlier.
To comply with the ﬁrst rule, exactly k bubbles are inserted on
any arc between nodes with different indices. It is harder to comply
with the second rule. Consider the example in Figure 6a. We can
output X , Y , both, or none, depending on conditions a and b. If
both a and b are true, we need two physical cycles; otherwise, one
cycle will sufﬁce. If both a and b are false, the output will idle for
one cycle, as the data to follow will not be available.
Following the second rule, we may insert a bubble in the two
positions in Figure 6a. But if we insert it under X , if a is true and
b is false, we spend two cycles instead of one. The solution is to
reshape the graph by duplicating the second condition (Figure 6b).
The bubble-insertion algorithm is straightforward. For each node
in the original graph, two copies are built: “empty” and “full,”
which handle control ﬂow when the current cycle has and has not
been used for data output respectively. For most nodes, only one
copy remains after a sweep that removes unconnected nodes.
5.5 Synthesizing the controller
Once read cycle indices and bubbles have been added, we synthesize the control machine (F SM). Its structure follows the packet
map. Bubbles in the packet map become states; we replace them
with registers, giving a one-hot encoding. Bubbles adjacent to the
leaves are special states that copy data until ieop, after which the
F SM returns to the initial state.
Figure 4 shows the packet map of Figure 2c translated into an
F SM . When an output node is encountered, the data are steered to
the output, and the owr signal is asserted. The second scheduling
rule ensures that at most one output node is found on any path between two states. For paths with no output nodes, owr remains deasserted. For each arc index increase, the ird signal is asserted to
read the next word from the buffer. The ﬁrst scheduling rule ensures that at most one word will be read between two states. For
paths with no index increase, ird remains deasserted.
5.6 Handling the end of a packet
All paths in the packet map ﬁnally reconverge to no more than w
w − 1 bytes. Our algorithm merges the end of each path to a comdifferent states, corresponding to no shifting necessary to shifting
mon REP state that is accompanied by an auxiliary align register
of size log2 (w). Any transition that leads to REP loads align.
The REP state performs two tasks. First, it aligns the data using
a multiplexer. In Figure 2c, align can take only two values, 0 and 4,
demanding a two-input multiplexer. Second, if eop is active, the
F SM can use align and imod to decide whether an extra cycle is
required for the last word and the input FI FO must stall.
5.7 Synthesizing the data path
Combinational nodes translate directly into combinational logic
to form the datapath. Bubbles become registers that guarantee any
node with read cycle index i has a valid value in the matching cycle.
A read cycle index may correspond to several clock cycles, so registers must be able to hold their values. We take a simple approach:
values are held when the present state equals the next state. The
data path can be stalled by asserting suspend signal (Section 5.1)
that causes the core module to hold all control and datapath registers. This is a brute-force solution that could be more clever: only
the output registers are compelled to stall; data could still propagate
within the process unless it would overwrite existing data.
Table 1: Synthesis results for selected modules
Module
Core size
LUTs
FFs
Delay
ns
Throughput
Gbps
MPLSpush
TTLEXPupdate
VLANﬁlter
VLANedit
PPPoEﬁlter
PPPoEterm
556
43
11
505
410
819
107
20
12
125
151
322
3.8
2.9
2.9
4.0
3.7
4.0
33
44
44
32
34
32
6. EXPERIMENTAL RESULTS
We synthesized some 128-bit wide modules from industrial designs for a Xilinx Virtex 4 xc4vlx40–ff668–10 F PGA. We generated
VHD L with our synthesis method and fed it to the Xilinx I S E tools
for RT L synthesis, placement, and routing.
Table 1 shows the size and performance of each module. We instantiate a module between two FI FO buffers, place and route the
circuit and report the delay of the longest register-to-register path;
based on this result we calculate the resulting packet processing
throughput. For area, we only report the size of the core modules with no pipelining (i.e., not including the wrapper circuitry)
in terms of the required number of ﬂip-ﬂop and lookup table primitives for the Virtex 4 device family. As only packet headers ﬂow
through the pipeline, we are able to sustain 40 Gbps throughput
with a realistic distribution of packet sizes, something only very
high-end switches currently achieve.
7. CONCLUSIONS
Establishing a strict formalism for describing packet editing operations (our packet editing graph) allowed us to construct a highperformance hardware synthesis procedure that can be used to create packet processors. The performance of circuits synthesized by
our procedure is comparable to the performance of circuits in stateof-the-art switches, while the design entry is done at a much higher
level of abstraction than the RT L usually used. The direct beneﬁt
is improved designer productivity and code maintainability. Experimental results on modules extracted from actual product-quality
designs suggest that our approach is viable.
8. "
Optimal link scheduling on improving best-effort and guaranteed services performance in network-on-chip systems.,"With the advent of the multiple IP-core based design using network on chip (NoC), it is possible to run multiple applications concurrently. For applications with hard deadline, guaranteed services (GS) are required to satisfy the deadline requirement. GS typically under-utilizes the network resources. To increase the resources utilization efficiency, GS applications are always complement with the best-effort services (BE). To allow more resource available for BE, the resource reservation for GS applications, which depends heavily on the scheduling of the computation and communication, needs to be optimized. In this paper we propose a new approach based on optimal link scheduling to judiciously schedule the packets on each of the links such that the maximum latency of the GS application is minimized with minimum network resources utilization. To further increase the performance, we propose a router architecture using a shared-buffer implementation scheme. The approach is formulated using integer linear programming (ILP). We applied our algorithm on real applications and experimental results show that significant improvement on the overall execution time and link utilization can be achieved","Optimal Link Scheduling on Improving Best-Effort and 
Guaranteed Services Performance in Network-on-Chip 
Systems*
Lap-Fai Leung  
Department of Electrical and Electronic Engineering 
Hong Kong University of Science and Technology 
Clear Water Bay, Hong Kong SAR, China 
Chi-Ying Tsui  
Department of Electrical and Electronic Engineering 
Hong Kong University of Science and Technology 
Clear Water Bay, Hong Kong SAR, China 
eefai@ee.ust.hk 
eetsui@ee.ust.hk 
ABSTRACT 
With the advent of the multiple IP-core based design using 
Network on Chip (NoC), it is possible to run multiple 
applications concurrently. For applications with hard deadline, 
guaranteed services (GS) are required to satisfy the deadline 
requirement. GS typically under-utilizes the network resources. 
To increase the resources utilization efficiency, GS applications 
are always complement with the best-effort services (BE). To 
allow more resource available for BE, the resource reservation 
for GS applications, which depends heavily on the scheduling of 
the computation and communication, needs to be optimized. In 
this paper we propose a new approach based on optimal link 
scheduling to judiciously schedule the packets on each of the 
links such that the maximum latency of the GS application is 
minimized with minimum network resources utilization. To 
further increase the performance, we propose a novel router 
architecture using a shared-buffer implementation scheme. The 
approach is formulated using integer linear programming (ILP). 
We applied our algorithm on real applications and experimental 
results show that significant improvement on the overall 
execution time and link utilization can be achieved. 
Categories and Subject Descriptors 
C.5.4 [COMPUTER SYSTEM IMPLEMENTATION]: VLSI 
Systems 
General Terms: Algorithms, Design, Performance 
Keywords: Network-on-Chip, Latency, Routing 
1. INTRODUCTION 
 Continuous improvement in process technology allows billions 
of transistors and hundreds of computation resources and 
processor cores to be put on a single chip in the very near future. 
System-on-chips (SoCs) with these capabilities require efficient 
communication architecture to offer scalable bandwidth and 
parallelism. Network-on-chip (NoC) has emerged to be a viable 
*This work was supported in part by Hong Kong RGC CERG
under Grant HKUST6256/04E. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC 2006, July 24–28, 2006, San Francisco, California, USA. 
Copyright 2006 ACM 1-59593-381-6/06/0007…$5.00. 
Tile
Router
S1 3
S1 4
S1 5
(1,4)
(2,4)
(3,4)
(4,4)
(1,3)
(1,2)
S9
S5
S1
S1 0
S1 1
(2,3)
(3,3)
(4,3)
(2,2)
(3,2)
S6
S2
S7
S3
(4,2)
(1,1)
(2,1)
(3,1)
(4,1)
Tile-based NoC architec ture
North input
Ad d r Deco d er
Ch an n el Co n tro ller
Bu ffer
Eas t input
Ad d r Deco d er
Ch an n el Co n tro ller
Cro ssbar
Arbit er
No r t h out put
Bu ffer
East o ut p ut
South input
Ad d r Deco d er
Ch an n el Co n tro ller
Bu ffer
C rossbar
Switch
Sout h o ut put
Wes t input
Ad d r Deco d er
Ch an n el Co n tro ller
Loc al input
Bu ffer
Ad d r Deco d er
Ch an n el Co n tro ller
Bu ffer
W est out put
Lo cal out put
Figure 1(a) Tile-based NoC architecture (b) On-chip router 
solution to provide the necessary communication links among 
different on-chip processing cores [1]. Instead of using 
dedicated wire or share bus, the computation resources are 
mapped to a tile-based structure and each tile is connected to its 
adjacent neighbors by interconnection links. The data are 
packetized and transferred between links through a router 
embedded within each tile (Figure 1(a)). A typical router [8] 
with uniform-sized buffer in each port is shown in Figure 1(b).  
 Multiple IP-core based design using NoC allows multiple 
applications running at the same time. Some of them can be 
applications with hard deadline 
requirement, 
such as 
video/audio applications, that need critical communication 
requirement while others may have soft real-time requirements 
and need only non-critical communication. Based on these 
different requirements, the communication scheduling can be 
divided into two types.  The guarantee service (GS), which 
caters for the hard deadline requirement while best effort (BE) is 
used for the other non-critical communication [13]. BE exploits 
the network resource that is unused by the GS. In particular, GS 
is always combined with BE in order to avoid the low resource 
utilization which is mainly due to the resource reservations for 
the worst-case scenarios. The Aethereal NoC design framework 
presented in [13] aims at providing a complete infrastructure for 
developing heterogeneous NoC with end to end quality of 
service guarantees. The network supports guaranteed throughput 
(GT) for real-time applications and best effort (BE) traffic for 
applications with soft or unspecified constraints. Specialized 
router architecture that supports BE and GS at the same time 
was also presented in [13].  
One of the main challenging issues of designing such kind of 
NoC is to allocate the network resources for the GS applications 
such that the hard deadline requirements are satisfied while still 
allowing maximum available network resources for 
the 
  
 
 
 
remaining BE applications. The network resource allocation 
problem includes the time-slot reservation of the communication 
link for the packets; assignment of priority to various packets 
transmitting on the same communication link; and decision on 
the time to produce and consume the packets. All the above 
factors (we collectively denote these as the link scheduling 
problem) significantly affect the performance of the entire 
system. At the same time, we want to minimize the overall 
execution time and hence the maximum latency of the GS 
applications. To satisfy the hard dead-line requirement under 
worst case situation, the network resource allocation and 
scheduling of the GS applications has to be done in the static 
design phase. In this paper, we focus on the optimal link 
scheduling at the static phase. The objective is to minimize the 
maximum latency of the GS applications while allowing as 
much as possible network resources for the BE applications.  
 On-Chip network is by far resources limited, e.g. limited by 
wiring, buffer, area and energy consumption. In standard 
computer networks, network routers use fairly large amount of 
buffer spaces. The network performance is drastically impacted 
by the amount of buffering resources available, especially when 
the traffic is congested [8]. In contrast, on-chip networks should 
use the least amount of buffer space due to the limited energy 
and area budget. Since 
the 
traffic characteristics varies 
significantly 
across 
different 
applications 
[8], 
the 
straightforward uniform distribution of buffering resources 
among all the ports of the router fails to guarantee QoS under 
tight buffer space budget [8]. In this paper, we increase the 
buffer space utilization and hence 
increase 
the system 
performance by using a shared buffer scheme.  
 Wormhole routing [7, 9] is commonly used because it requires 
less buffer usage to achieve minimal routing latency. Each 
packet is segmented into flits (flow control unit). The header flit 
sets up the routing paths at each router. The body flits will then 
follow the designated path and the tail flit releases the 
reservation of the link in the routing path. One major advantage 
for wormhole routing is that it does not require the router to 
store the whole packet before it is sent out. This drastically 
reduces the latency. However, one major problem is that 
contention arises when the down-stream link or the router of the 
header flit is occupied. Previous works [8,11] used the 
backpressure mechanism to regulate the contention and the 
packets are dispatched immediately once they are ready. Under 
this scheme, all the flits of the packets are held in their current 
buffers until the downstream link is available. The main 
drawback is that all the network resources in the routing path are 
wasted since they are held by the congested packet. This would 
reduce the network resource efficiency and consequently, 
increases the latency of the whole system and decrease the 
resource available for the BE applications. In this paper, we 
judiciously schedule the packets of the GS applications such 
that the contention is minimized and the links can be utilized as 
much as possible to overcome this drawback. We propose a 
static link scheduling algorithm based on Integer Linear 
Programming  (ILP) optimization To obtain the  optimum static 
packet schedule if the processor mapping, routing and the 
operation characteristics of the GS applications are given. The 
algorithm also considers the condition that only finite buffer 
space is available on the router when making the scheduling 
decision.  
2. RELATED WORK 
Wormhole routing is an ideal candidate switching technique for 
NoC. However, it suffers from deadlock when packets block 
each other in a circular fashion [7] and contention when the 
traffic is congested [10]. One way to solve the deadlock is to use 
virtual channels [9,14] but this requires a lot of buffer space and 
is not applicable to NoC design with tight area and power 
budget. Another way to solve the deadlock issue is using the 
deterministic Dimension-ordered routing (e.g. XY routing , oddeven routing) [9]. Under this scheme, packets are routed in one 
dimension first and then the other until reaching the destination. 
In this work we use the XY routing which is widely used in 
NoC design [8]. The works in [8,6] proposed stochastic 
communication models which was captured from the traditional 
marco network. Using this model, the transmission latency is 
unpredictable and hence the performance constraints like the 
deadline requirement can not be guaranteed. In this paper, we 
consider 
the worst-case characteristic of 
the given GS 
applications such that we are able to guarantee the deadline 
requirement under the worst-case. 
 Many works [10,7] have been proposed to address the various 
optimization problems on NoC. In [7], a mapping problem to 
satisfy bandwidth constraints on the links for regular tile-based 
Networks on Chip (NoC) architecture was tackled. In [10], a 
solution was proposed for the problem of mapping applications 
on regular NoCs for minimizing execution time and energy 
consumption. However, all the above works do not consider the 
effect of packet schedule and the computation timing on 
calculating the latency of the packets. Also, they do not consider 
the overall execution time and therefore, the hard deadline 
requirement of the applications can not be guaranteed.  
 There are many analytical models for wormhole-based network 
[10,15,12]. Adve and Vernon in [15] modeled a wormhole mesh 
network as a closed queuing network and use approximate mean 
value to calculate the latency under random traffic. However it 
assumed a single-flit buffer which makes the timing model 
restrictive to apply because the buffer size is usually arbitrary 
but finite in real applications. On the other hand, the works in 
[10,14] assumed infinite buffer which is not a valid assumption 
for real applications. Despite the assumption of infinite buffer 
size and the fact that they did not consider packet contention in 
the timing model, the work in [10] provided a detailed timing 
model for the wormhole mesh network.  
 The work in [8] proposed a buffer allocation scheme for the 
NoC router. The buffer size of each input port of the routers 
were determined according to the given traffic pattern subject to 
the total buffering space constraint of the whole system and the 
buffer space for each router can be different by a lot. This 
method may not be favorable for regular tile-based NoC design 
as it is easier to design if every router has the same amount of 
buffer space. Also for different applications, the traffic patterns 
are different. If a fixed buffering space distribution 
is 
determined in the design phase based on the traffic patterns of a 
set of applications, for other applications, this fixed buffer 
distribution may not be optimized. The work in [13] assumed 
infinite buffer size which is infeasible in real NoC design. In 
this work, we use wormhole routing with finite buffering space 
in the router.  An accurate timing model for wormhole routing 
which takes packet contention into consideration is proposed.  
ET2=10
ET3=5
T2
T3
ET1=10
T1
20
ET5=5
5
5
10
T5
5
10
T4
ET4=10
20
5
P4
T1:[0,10]
T7:[10,15]
10
T7
ET7=10
T1→ΤT4:[10,33]
T7→ΤT8:[34,44]
R4
T2:[0,10]
T2→ΤT6:[10,24]
T1→ΤT4:[12,35]
T7→ΤT8:[36,46]
T6
ET6=5
T8
ET8=5
Deadline=100
P5
R5
T3:[0,5]
T4:[34,39]
T8:[40,45]
T2→ΤT6:[12,26]
T1→ΤT4:[27,32]
T7→ΤT8:[33,38]
P3
T5:[41,46]
T6:[46,51]
T3→ΤT5:[9,19]
T2→ΤT6:[20,30]
T3→ΤT5:[31,41]
P6
T3→ΤT5:[5,15]
T3→ΤT5:[17,37]
T1→ΤT4:[29,34]
T7→ΤT8:[35,40]
R6
R3
T3→ΤT5:[7,17]
T2→ΤT6:[18,28]
T3→ΤT5:[29,39]
T1:[0,10]
T7:[10,15]
P4
P5
T2:[0,10]
T3:[0,5]
T4:[21,26]
T8:[27,32]
T1→ΤT4:[10,20]
T7→ΤT8:[34,44]
R4
T2→ΤT6:[10,35]
T1→ΤT4:[12,22]
T7→ΤT8:[36,46]
T1→ΤT4:[14,19]
T7→ΤT8:[20,25]
T2→ΤT6:[27,37]
R5
P3
T5:[30,35]
T6:[41,46]
T3→ΤT5:[9,19]
T3→ΤT5:[20,30]
T2→ΤT6:[31,41]
P6
T3→ΤT5:[5,15]
T3→ΤT5:[15,25]
T1→ΤT4:[16,21]
T7→ΤT8:[22,27]
R6
R3
T3→ΤT5:[7,17]
T3→ΤT5:[18,28]
T2→ΤT6:[29,39]
 (a)   a task flow graph G                 (b) NoC assignment NA1 with single flit buffer   (c) NoC assignment NA2 with single flit buffer 
Figure 2. A motivational example 
3. MOTIVATION EXAMPLE 
We show the importance of considering the link scheduling 
using the example in Figure 2.  Figure 2(a) shows the task flow 
graph G of a GS application. It contains eight tasks T1,..,T8  
which are mapped to a 3x2 NoC platform and XY routing path 
allocation [9] are used for determining the routing of the packets 
among different cores. Due to the space limit, we only show the 
relevant resources in the figure. Here the 8 tasks are mapped 
onto 4 processors P3 to P6. The execution time, ETi of task Ti’s 
and the communication message volume (in flits) between the 
tasks are shown in the graph G. We assume the router setup time 
of the header flit tr=1 time unit (tu) and the time to transmit a flit 
through a link tl=1tu also. We assume the packet size is variable 
and the messages between two tasks are divided into packets. 
For example, the message T3→T5 is divided into two packets 
with equal size of 10 flits. In this example, we assume all the 
messages, except the one for T3→T5, are transmitted in 1 packet. 
We denote the transmission time of the packets Ti→Tj on the 
links as Ti→Tj [start-time,end-time] and the execution time of 
task Ti on processors as Ti [start-time,end-time]. 
 Figure 2(b) shows the resulting packet schedule based on an assoon-as-possible schedule similar to the work proposed in [10] 
in which the packets are sent out once they are ready. We can 
see that the packet T2→T6 is waiting at router R6 because the 
down-stream link P6→P3 is occupied by packet T3→T5. Under 
the backpressure control scheme, the packet T2→T6 occupies the 
link P5→P6 longer than the normal transmission time. We can 
re-schedule the packets as shown in Figure 2(c). In this case, we 
can improve the link utilization by sending packets T1→T4 and 
T7→T8 earlier than T2→T6. By doing so we avoid contention in 
router R6 and the network resources can be maximally utilized. 
Figure 2(c) shows a 9.8% reduction in overall execution time 
than that of Figure 2(b). From this example, we can see that the 
static scheduling of the packet has a profound impact on the 
possible contention and hence on the overall execution time and 
also the link resource utilization.   
4. SYSTEM CHARACTERIZATION 
In this work, we target for NoC with heterogeneous processor 
elements (PEs). We assume the hard real-time GS applications 
are periodic, frame-based non-preemptive.  The application is 
represented by a communication dependence and computation 
graph (CDCG) similar to that in [10] which is a directed graph, 
G(V,E). The CDCG contains N periodic tasks 
VTi ∈ and each 
task Ti has its own worst-case-execution-time (WCET) Wi. We 
define the mapping function map(Ti) to denote which PE Ti is 
mapped to. The absolute release time and deadline of task Ti are 
Ri and Di, respectively. Each directed arc 
Eij ∈ between Ti 
E
and Tj in the CDCG characterizes the communication message. 
Each message Ei,j is divided into packets with variable packet 
size. PSijm is the size of the m-th packet Pijm of Ei,j. Also, we use 
Ζijm to denote the set of the links used in the routing path of the 
packet Pijm. Similarly, we have ΖFijm and ΖLijm to indicate the 
first and the last link used in the routing path of the packet Pijm. 
Suppose packet Pijm pass through a router from the link ek’ and 
directing to the link ek. Link order 
ijmℜ of packet Pijm , is a set 
that contains all the tuple (ek’,ek). Each packet Pijm on a link ek is 
characterized by the start-time ts(Pijm,k)) and end-time te(Pijm,k) 
of the packet using this particular link. Traffic congestion 
happens when 
two packets 
try 
to use 
the same 
link 
simultaneously. Here, we use first-come-first-served (FCFS) to 
arbitrate the packets which are going into the same router. A PE 
can only execute a single task at any time instance. Each PE Pq 
is connected to a router Rq and each router is connected to five 
links as shown in Figure 1(b). Each input port that connects to 
the input link ek has a buffer size bskq.  
5. SHARED BUFFER ARCHITECTURE 
In the router architecture shown in Figure 1b), the buffer of each 
input port is of the same size.  For non-uniform traffic, some of 
the input port of the router may be less congested and therefore, 
some of the buffer may be under-utilized. In this work, we 
extend the shared output buffer proposed in [12] and present a 
shared buffer architecture which allocates the buffering space 
for each input port according to the traffics of the applications, 
thus to maximally utilize the buffering space.  The buffer is 
implemented using multiple-port memory and Figure 3a) shows 
the proposed router architecture. Each input port has a pair of 
head and tail pointers (ptr) to indicate the beginning and the end 
of the corresponding buffer in the memory. The buffer size for 
each input port can be varied and is determined by solving the 
ILP formulation.  In contrast to the buffer implementation in [8] 
in which the buffer size for each input port is fixed once the 
chip is fabricated, our shared buffering scheme allows the buffer 
size of each input port to be changed when mapping another 
application onto the NoC.  
We use the example in Figure 3 to show how the shared 
buffer architecture works. Suppose 4 packets A,B,C and D with 
5 flits, 8 flits, 4 flits and 6 flits, respectively, reach the router at 
different time and packets A, B and D are going to output port 
W(West) while packet C goes to output port E(East). We 
assume packets B and C reach the router 1 time unit (tu) earlier 
than the packet A and 7 tu earlier than packet D. In this case, 
packets A and D are stored at the router and wait for the packet 
B to release the network resource. We assume the router setup 
time is 1 tu. Figure 3(b) shows the memory allocation at the 
   
 
Router Rq
Packet P ijm
Packet P i'j'm'
Link ek
Link ek'
Link ek'''
Link ek''
Link ek'
Link ek'''
P ijm
P i'j 'm'
Time
0
P i 'j 'm'
P i''j ''m''
P i jm
tr+tl
te(P i 'j 'm', k)
ta(P i""j""m"",k)
tr
ts(P i jm, k)
Idle
dW(Pi' j'm ' ,k ,Pi""j""m "",k )
tl
Pac ket P i ''j''m''
Link ek''
P i''j ''m''
ts(P ijm, k)
P i jm
P i ''j''m''
tr
te(P i""j""m"", k)
dW(P i""j""m"",k ,Pijm ,k )
ta(P i jm, k)
Idle
Link ek
Arriv e the router and waiting until P i'j'm' f inish tramissioin
ta(P i""j""m"",k)
Busy
Busy
Figure 4. Graphical representation of the schedule 
Cros sbar
Sw itch
Shared buffer memory
Nor th input (N)
A dd r Deco der
Ch ann el
Con tro ller
Eas t input (E)
A dd r Deco der
Ch ann el
Con tro ller
South input (S)
A dd r Deco der
Ch ann el
Con tro ller
Wes t input (W)
A dd r Deco der
Ch ann el
Con tro ller
Nor th output
Eas t output
South output
Wes t output
Packet A
Packet B
Packet C
Loc al input (L)
A dd r Deco der
Ch ann el
Con tro ller
Pac ket D
Loc al output
Crossbar arbiter
(a) 
Header flit of packet B
Memory allocation at time 0 ut
ptr N
ptr S ptr E
ptr W,ptr L
Header flit of packet C
Tail flit of packet B
Memory allocation at time 7 ut
ptr N
ptr S ptr E
ptr W,ptr L
Header flit of packet D
packet A
Free
(b)          (c) 
Figure 3. (a) Proposed router architecture and (b,c) memory 
allocation 
time when only packets B and C arrive at the router. At this 
moment, the memory contains the header flit of packets B and C. 
The memory allocation when packet D arrives at the router at 
time 7 tu is shown in Figure 3(c). In order to avoid the 
backpressure of the packets, traditional architecture using 
uniform buffer allocation requires totally 30 flits (6 flits X 5) to 
store the packets. The shared buffering space scheme requires 
only a buffer size of 8 flits. The shared buffer architecture 
requires an extra control circuit to direct the packets to the 
crossbar switch according to the pointers of each input port. 
However the increase in complexity is small comparing with the 
whole router design.  
The above problem is formulated as an integer linear 
programming (ILP) problem. The objective function is to 
minimize the overall execution time which is equivalent to 
minimize 
the end-time of 
the 
latest computational 
task. 
However, this is not a standard ILP formulation because 
function Max() is endogenous model type which is not allowed 
inside the objective function [3]. We transform the given CDCG 
by adding a zero-workload task, Tdummy, and connect all tasks to 
it. By doing so, the Tdummy is the only last task to be executed 
and the objective function is now transformed to 
(
)
)
(
(
)
dummy
transform
i
T
 .
Tte
Min
Tte
Max
Min
LMin
 .
i
.
⎯⎯⎯ →
⎞
⎯⎟
⎠
⎛
⎜
⎝
=
∀
      (1) 
5.1 ILP Formulation Considering Contention 
with Finite Buffer Size 
Marcon et. al. presented a timing model for the wormhole 
routing algorithm in [10]. However, they assumed the input 
buffer size is unbounded which cannot be implemented in a real 
NoC architecture. Since the input buffer size is unbounded, the 
total packet delay of the wormhole routing algorithm proposed 
in [10] does not take the packet contention into consideration at 
the design time. Here, we present a timing model that considers 
the packet contention with a finite input buffer size bskq. Let nijm 
be the number of flow control units (flits) of the packet Pijm and 
λ be the period of a clock cycle. Also, let tr be the number of 
cycles needed for taking a routing decision in the router and tι 
be the number of cycles needed to transmit a flit through a link.  
In the following discussion, Ti denotes the current task and Ti’ 
is the previous task that executed on the same PE with Ti. Also, 
let Tj depends on Ti but executed on a PE different from that 
executing Ti. In addition, we assume there is no communication 
cost for two tasks that are allocated on the same PE. We use 
Figure 4 to show the graphical representation of the variables 
that characterize the packets and how they are used in the timing 
model. In Figure 4, 3 packets Pijm, Pi’j’m’ and Pi”j”m” arrive at the 
router Rq in different input ports and all need to be sent to the 
downstream link ek’’’. We also assume Pi’j’m’ reaches Rq first 
such that Pijm and Pi”j”m” are needed to wait at the input port 
until packet Pi’j’m’ finishes the transmission and releases the 
resource of link ek’’’. The idle time of the links ek and ek” due to 
the fact that packets Pijm and Pi”j”m” are congested is also shown 
in Figure 4. The arrival time ta(Pijm) of the packet Pijm is the 
time that the first flit of the packet arrives at the router. The 
contention delay dW(Pi’j’m’,k,Pi”j”m”,k) is the time that packet 
Pi”j”m” needs to wait at the buffer until the previous packet Pi’j’m’ 
finishes the transmission and releases the resource ek. It is 
defined as the time difference between the end-time of Pi’j’m’ at 
ek and the packet arrival time ta(Pi”j”m”,k) as shown in Figure 4.  
 To find an optimal link scheduling, we need to know the relative 
order of the packets on a link before determining the packet 
transmission time. We use a binary variable o(Pijm,k, Pi’j’m’,k) to 
represent the relative order of two packets Pijm,k, Pi’j’m’,k using 
the same link ek. o(Pijm,k, Pi’j’m’,k) is given by: 
0
 if P
 is
scheduled 
earlier th
an P
Po
, P
  1
otherwise
The value of o(Pijm,k, Pi’j’m’,k) is related to the start-time and 
the end-time of the two packets using link  ek. If o(Pijm,k, Pi’j’m’,k) 
is 0, then ts(Pi’j’m’,k) should be larger than te(Pijm,k) and vice 
versa. These relations are captured by the following constraints 
where M is a very large number:  
 (
)
⎧
⎨
⎩
=
,kmji
'
'
'
i'j'm',k 
ijm,k
ijm,k
(
)
(
)
(
)
'
mji
'
'
'
ji
'
,kmji
'
'
'
,kmji
'
'
'
,
M
,
, P
E
,
Po
E
Po
EE
,
1
EE
,
,
Pts
2,1'
,...,
Pts
2,1'
,...,
,
Pte
ii
,',
Pte
ii
,',
k
ijm
k
ij
ijm,k
r
ijm,k
Z
e
M
Z
e
, P
e
N
j
j
t
∈∀
⋅
∈∀∈
−
∀
−
≤
∈
≤
∈
∀
+
  (2) 
(
)
(
)
(
)
)
(
'
mji
'
'
'
ji
'
,kmji
'
'
'
,kmji
'
'
'
,
,
,
k
ijm
k
ij
ijm,k
r
ijm,k
Z
e
Z
N
j
j
t
∈∀
∈∀∈
∀
∀
⋅
−
+
 (3) 
If o(Pijm,k, Pi’j’m’,k) is 0, eqn. (3) is trivially true and eqn (2) 
specifies the relationship between the corresponding start time 
and end time.  
The arrival time ta(Pijm) is calculated by adding the packet 
start-time at the uplink ek’  with the latency tl. Thus we have 
  
 
 
 
       
          
          
          
          
 
 
We carried out experiments using several real applications 
ranging from video processing applications 
to 
industrial 
applications. These include a computer numerical control 
applications (CNC) (62 tasks) [5], a generic Multimedia System 
(MMS) (40 tasks) [8], a Video Object Plane Decoder (16 tasks) 
[7] and four benchmarks applications obtained from E3S 
benchmark suites [4], namely networking (13 tasks), telecom 
(30 tasks), auto_indust (24 tasks) and consumer (12 tasks). For 
each benchmark, we manually mapped the tasks onto a 4x4 tilebased NoCs and used XY routing for the communication link 
routing. We evaluate the performance of the proposed ILP 
algorithm on both the overall execution time and the link 
utilization. We compare the results with the work proposed in 
[10] where packets are sent out to the router as soon as they are 
ready at the PE and backpressure mechanism is used when there 
is packet contention.  However, [10] assumes an infinite buffer 
size which is impossible in real implementation. We modified it 
by using a uniform buffer implementation of which the size of 
the buffer at every input port is the same. The ILP is 
implemented in the GAMS [3] environment and it is solved by 
Cplex [3]. 
  In this experiment, a message is divided into several packets 
with the packet size ranging from 16 flits to 64 flits and the flit 
size is 32 bits. We studied the effect of different traffic patterns 
on the performance. Here two traffic patterns, namely uniform 
and hotspot, were generated by using different task mappings. 
For uniform traffic pattern, a task is randomly mapped on the 
PE with equal probability. For hot spot traffic pattern generation, 
the tasks are restricted to be mapped on certain PEs such that a 
hotspot 
traffic 
is generated. In hotspot distribution, 
the 
probability that a packet is sent to some nodes (called hotspot 
nodes) follows a normal distribution with mean is four times of 
the other nodes.  Table 1 shows the improvement of the overall 
execution time and the link utilization over the work in [10] 
under different traffic patterns. Here the buffer budget for each 
router is 40 flits. We can see that the improvements varied with 
the applications because the traffic characteristics of the 
 Table 1. Results for real applications 
Improvement of overall 
execution (40 flits buffer 
size /router)
hotspot
uniform
11.7%
7.0%
7.2%
5.9%
Improvement of link 
utilization reduction (40 flits 
buffer size /router)
hotspot
uniform
14.3%
4.9%
12.7%
8.5%
CNC
MMS
Video Object 
Plane Decoder
Networking
Telecom
Auto_indust
Consumer
64
40
16
13
30
24
12
10.2%
15.3.%
30.0%
22.3%
17.2%
2.8%
11.8%
13.8%
9.1%
9.5%
7.8%
18.3%
20.9%
20.2%
6.8%
5.6%
3.3%
13.4%
14.5%
4.3%
Application
Number of 
tasks
(
)
(
)
ijm
k
ijm
k
ij
l
ijm,k
ijm,
Z
e
Z
e
E
E
Pts
N
2,1
,...,
Pta
i
,
j
t
∈∀
∈∀
∈
∀
∈
∀
+
=
'
'
,
,
,
   (4) 
The definition of the contention delay dW(Pijm,k,Pi’j’m’,k) of 
Pijm at link ek is given by:  
(
)
(
)
(
)
'
mj
'
'
'
'
'
,kmj
'
'
'
,
,
Pte
N
EE
,
,
,...,
, P
j
2,1'
,
P
ii
,',
i
k
ijm
k
j
i
ij
ijm
m'
i'j
i
ijm,k
W
Z
e
Z
e
Pta
E
j
d
∈∀
∈∀∈
∀
∈
∀
−
=
(5) 
Similar to eqn. (2) and (3), we use the binary variable of o(Pijm,k, 
Pi’j’m’,k) when calculating the contention delay dW(Pijm,k, Pi’j’m’,k) 
in order to specify the order between the packets. Thus we have 
Pd
, P
Pte
Pta
Po
, P
M
(6) 
ii
,
,'
j
j
2,1'
N
,
EE
,
e
,
Z
Pd
, P
Pte
Pta
ii
,',
j
2,1'
EEN
,
,
(
)
(
)
(
)
(
)
'
mji
'
'
'
ji
'
,kmji
'
'
'
'
,kmji
'
'
'
,
,...,
≤
,
k
ijm
k
ij
ijm,k
ijm
m'
i'j
ijm,k
W
e
, P
e
Z
Po
Z
E
∈∀
+
∈∀
−+
∈∀∈
∈
∀
∈
∀
⋅
−
≤
(
)
(
)
(
)
(
)
)
(
'
mji
'
'
'
ji
'
,kmji
'
'
'
'
,kmji
'
'
'
,
1
eE
,
,...,
,
k
ijm
k
ij
ijm,k
m'
i'j
ijm
ijm,k
W
Z
j
M
∈∀
∀
∈
∀
⋅
−
(7) 
 In order to avoid the backpressure control during packet 
contention, we need to make sure the buffer bskq of the router Rq 
should be able to store the flits coming to the input port k during 
contention time. Also, the sum of the buffer size of all the input 
ports is equal to the total buffer budget of each router 
(
)
'
mji
'
'
'
'
mji
'
'
'
'
'
ji
'
qk
'
,kmji
'
'
'
N
)
,...,
,
e
2,1'
,
(
e
,
,
bs
e
,
,
Pd
EE
,
,
,
ii
,',
Z
k
k
k
k
ijm
k
k
ij
l
ijm,k
W
e
e
t
Z
e
E
j
j
, P
ℜ∈
∈
∀
∈
λ
∀
⋅
∈
∀∈
∀
∀
⋅
≤
(8) 
                                               (9) 
∑
∀
=
k
q
kq
BS
bs
 The actual start-time of the packet Pijm on the link ek is larger 
than the arrival time plus the router setup time and the wait time. 
So we have  
Pts
Pta
ii
,
2,1'
(
)
(
)
(
)
'
mj
'
'
'
'
,kmj
'
'
'
,
,
,
d
EE
,
t
,
,...,
,
,'
i
k
ijm
k
j
i
ij
ijm,k
i
W
r
ijm,k
ijm,k
Z
e
Z
e
E
N
j
j
P
P
∈∀
∈∀
∈
∀
∈
∀
+
+
≥
(10) 
The end-time of the packet Pijm on the link ek is equal to 
Pte
=
Pts
+
t
⋅
ni
⋅
 λ
∀
i
,
j
∈
2,1
,...,
N
,
∀
E
∈
E
,
∈∀
e
Z
 In the above, we show how the communication characteristics 
on the applications are captured in the ILP formation. We also 
need to consider the scheduling and timing of the computation 
tasks when we determine the actual packet schedule. In hard real 
time applications, the task Ti can start execution after the ready 
time Ri and has to finish before the deadline Di in order to 
guarantee the performance constraints. The end-time te(Ti) is the 
start-time ts(Ti) plus the WCET Wi. Thus we have 
R
Tts
,
Tte
(
)
D
i
2,1
(
)
(
)
ijm
k
ij
jm
l
ijm,k
ijm,k
           (11) 
(
)
N
i
i
i
,...,
∈∀
2,1∈∀
i
≤
W
≤
       (12) 
 (13) 
Considering two tasks mapping to the same processor, we 
need to make sure that Ti start the execution only after its 
preceding task Ti’ finish the execution. We have 
(
)
(
)
N
T
ts
T
te
i
i
i
,...,
+
=
 (
)
(
)
)
(
)
(
,
,...,
2,1
'
'
i
i
i
i
T
map
T
map
N
i
Tte
Tts
=
∈∀
≥
   (14) 
 In the case that two dependent tasks Ti and Tj are executed on 
two different PEs, Ti sends the message to Tj when Ti finishes 
the execution and the transfer of the first packet of the message 
starts when the execution of Ti is finished.  When the last packet 
reaches the destination, task Tj can start the execution.     
    (
         (15) 
Pts
≥
Tte
∈∀
i
2,1
,...,
N
,
e
Z
Tts
Pte
∈∀
i
2,1
,...,
N
       (16) 
)
(
)
Fijm
k
i
k
ijm
∈∀
,
∈∀
e
,
(
)
(
)
Lijm
k
k
ijm
i
Z
≥
,
 The resulting schedule after solving 
the proposed ILP 
formulation does not need the backpressure control mechanism 
to regulate the packets during packet contention. It is because if 
a packet is congested at the router, the packet can actually be 
delayed to be sent out at the source in order to avoid the 
operation of backpressure. The time that the packet can start the 
transfer greatly depends on the buffer size available at the input 
port of the router. "
Prediction-based flow control for network-on-chip traffic.,"Networks-on-chip (NoC) architectures provide a scalable solution to on-chip communication problem but the bandwidth offered by NoCs can be utilized efficiently only in presence of effective flow control algorithms. Unfortunately, the flow control algorithms published to date for macronetworks, either rely on local information, or suffer from large communication overhead and unpredictable delays. Hence, using them in the NoC context is problematic at best. For this reason, we propose a predictive closed-loop flow control mechanism and make the following contributions: first, we develop traffic source and router models specifically targeted to NoCs. Then, we utilize these models to predict the cases of possible congestion in the network. Based on this information, the proposed scheme controls the packet injection rate at traffic sources in order to regulate the total number of packets in the network. Evaluations involving real and synthetic traffic patterns show that the proposed controller delivers a superior performance compared to the traditional switch-to-switch flow control algorithms","Prediction-based Flow Control for Network-on-Chip Traffic
Umit Y. Ogras and Radu Marculescu
Department of Electrical and Computer Engineering
Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: {uogras,radum}@ece.cmu.edu
ABSTRACT
Networks-on-Chip (NoC) architectures provide a scalable solution
to on-chip communication problem but the bandwidth offered by
NoCs can be utilized efficiently only in presence of effective flow
control algorithms. Unfortunately, the flow control algorithms published to date for macronetworks, either rely on local information,
or suffer from large communication overhead and unpredictable
delays. Hence, using them in the NoC context is problematic at
best. For this reason, we propose a predictive closed-loop flow control mechanism and make the following contributions: First, we
develop traffic source and router models specifically targeted to
NoCs. Then, we utilize these models to predict the cases of possible
congestion in the network. Based on this information, the proposed
scheme controls the packet injection rate at traffic sources in order
to regulate the total number of packets in the network. Evaluations
involving real and synthetic traffic patterns show that the proposed
controller delivers a superior performance compared to the traditional switch-to-switch flow control algorithms. 
Categories and Subject Descriptors
B.4 [Hardware]: Input/output and data communications.
General Terms
Algorithms, Performance, Design.
Keywords
Multi-processor systems, networks-on-chip, flow control, congestion control.
1. INTRODUCTION
Network-on-Chip (NoC) architectures have been proposed to
address the communication problems generated by the increasing
complexity of the single chip systems [1,4,9]. While the NoC
architectures offer substantial bandwidth and concurrent communication capability, their performance can significantly degrade in
the absence of an effective flow control mechanism. Such a control
algorithm tries to avoid resource starvation and congestion in the
network by regulating the flow of the packets which compete for
shared resources, such as links and buffers [2,5].
In the NoC domain, the term flow control was used almost exclusively in the context of switch-to-switch [4,7,8,11,18] or end-toend [16] transport protocols. These protocols provide a smooth
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
DAC 2006, July 24-28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007...$5.00.
traffic flow by avoiding buffer overflow and packet drops. However, the flow control can also regulate the packet population in the
network by restricting the packet injection to the network [2]1.
This is precisely the main objective of this paper. To the best of our
knowledge, this is the first study which addresses the congestion
control problem in the NoC domain.
1.1.  Overall approach
Switch-to-switch flow control algorithms, such as on-off, creditbased and ack/nack mechanisms, regulate the traffic flow locally
by exchanging control information between the neighboring routers. These approaches have a small communication overhead,
since they do not require explicit communication between source/
sink pairs. However, the switch-to-switch flow control does not
regulate the actual packet injection rate directly at the traffic
source level. Instead, it relies on a backpressure mechanism which
propagates the availability of the buffers in the downstream routers
to the traffic sources. Consequently, before the congestion information gets the chance to reach the traffic sources, the packets generated in the meantime can seriously congest the network.
End-to-end flow control algorithms, on the other hand, conserve
the number of packets in the network by regulating the packet
injection rate right at the source of messages. For example, in window-based algorithms, a traffic source can only send a limited
number of packets before the previously sent packets are removed
from the network. However, the major drawback of end-to-end
control algorithms is the large overhead incurred when sending the
feedback information [2]. Besides this, the unpredictable delay in
the feedback loop can cause unstable behavior as the link capacities increase [12]. Since this is very likely to happen in NoCs, such
algorithms are not directly applicable to regulate the NoC traffic.
Starting from these considerations, in this paper, we propose a predictive flow control algorithm which enjoys the simplicity of the
switch-to-switch algorithms, while directly controlling the traffic
sources, very much like the end-to-end algorithms. Towards this
end, we first present a novel router model based on state space representation, where the state of a router is given by the amount of
flits already stored in the input buffers. Using this model, each
router in the network predicts the availability of its input buffers in
a k-step ahead of time manner. These availability values are computed via an aggregation process using the current state of the
router, the packets currently processed by the router, and the availability of the immediate neighbors. Since all predictions are based
on the data the routers receive directly from their immediate neighbors, the computations are decentralized and no global data
exchange is required. Moreover, we note that the availability information computed at time n is obtained by aggregating the availability of the immediate neighbors at time n – 1. This information,
1. This function is also referred as congestion control. However, following the
convention in [2] and [6], we do not make such a distinction.
in turn, reflects the state of the routers situated two hops away, at
time n – 2, and so on so forth. Therefore, due to the aggregation
process the local predictions actually reflect the global view of the
network. Finally, the traffic sources utilize the availability of the
local router to control the packet generation process and avoid
excessive injection of packets in the network.
1.2.  Related work and paper contribution
From a flow control perspective [5,6], the majority of work presented in the NoC domain relies on the switch-to-switch flow control [4,7,8,11,16,18]; this is primarily due to the large overhead
incurred by the end-to-end flow control algorithms. The work presented in [16] employs the end-to-end flow control for guaranteed
service in addition to the basic link-level control. A comparison of
the overhead of flow control algorithms can be found in [15]. 
Congestion control is well studied for macronetworks [2,6,12,17].
In [12], the authors develop a decentralized control system, where
the sources adjust their traffic generation rates based on the feedback received from the bottleneck links. A predictive explicit-rate
control mechanism is presented in [17]. The authors consider a single bottleneck node and infinite buffering resources. The sources
adjust their traffic rates using the congestion information received
from the bottleneck node via control packets. 
Our approach is different from the previously mentioned work in a
number of ways. First, our technique is computationaly light since
it relies on local data transfers, similar to the basic switch-toswitch flow control. At the same time, due to the aggregation performed at each router, the information exchanged between the
switches actually reflects the global view of the network. Furthermore, since the predictions reflect the state of the network k steps
ahead in time, the packet sources across the network can sense a
possible congestion situation early on and then adapt in order to
avoid excessive packets injection to the network.
We note that, in real applications, the best effort (or non-real time)
and guaranteed service (or real time) traffic coexist. The proposed
framework is a convenient way to control the best effort traffic
such that it makes best use of network bandwidth without sacrificing the bandwidth allocated to the guaranteed service traffic.
1.3.  Paper organization 
Section 2 and Section 3 present the traffic source and router models, respectively. In Section 4, we propose a flow control mechanism for NoCs. Experimental results appear in Section 5. Finally,
Section 6 concludes the paper.
2.  SYSTEM AND TRAFFIC MODELS
2.1.  System model and basic assumptions
We assume that the network nodes are populated with processing
and storage elements (referred to as PEs) but we do not make any
assumption about the underlying network topology. The nodes
communicate by exchanging packets across the network. Since we
consider wormhole routing, the packets are divided into flits. The
length of a packet (S) is measured by the number of flits it contains. For convenience, the flit size is assumed to be equal to the
physical channel width (W). 
In order to avoid packet loss, a basic link-level ON-OFF flow control mechanism is implemented at the routers [5]. The proposed
predictive control technique works together with this link-level
mechanism to control directly the behavior of the traffic sources. 
2.2.  Traffic source model
Since the traffic injection rate into the network is the main knob for
source control, an accurate model of the input traffic is necessary.
Such a model will not only show how the input traffic can be handled by the actual design, but also describe its impact on the packet
delay in the network. Towards this end, we observe that the NoC
nodes can be in two different states:
OFF STATE: The PE is either processing data or waiting for new
data. While in this state, the PE does not generate traffic (hence the
name OFF) as shown in Figure 1. 
ON STATE: The PE injects packets to the network so the traffic
source and its corresponding state are referred to as ON. In this
state, the source injects packets in a bursty manner until the message is completely transmitted.
Let the discrete time stochastic process λ(t), t ∈ Z+ denote the
instantaneous flit injection rate at time t. The cumulative traffic
volume generated up to time t (denoted by V(t)) is given by:
Z +∈,
V t( )
=
V t
1–(
) λ t( ) V 0( )
+
,
=
0 t
(1)
In the ON state, the flit injection rate λ(t) is constant and equal to
channel width, i.e.  λON = W bits/sec. If a header flit is injected to
the network at time t0, one can see that λ(t0 + ∆) ≠ 0 for 0< ∆ < S,
where S is the packet size in flits. Similarly, when the PE is in the
OFF state, one can get an idea of how much longer the OFF state
will continue, given the amount of time already spent for processing and type of processing done by the PE. Therefore, the interarrival times are not memoryless, (i.e. not exponentially distributed) and so the flit injection process cannot be Poisson. Consequently, we need to modify the classical ON/OFF [13] model to
work for NoC traffic sources.
2.2.1. Distribution of tON
tO N
The duration of the ON state is determined by the size of the packets 
generated 
by 
the 
node 
and λON; 
specifically,
=
SW λO N
⁄
. While λON is constant, S depends on the particular packet (or packets) generated by the source after completing a certain task. In an NoC, the type of the tasks performed by
each PE and the size of the resulting message are typically known
at design time. For example, a DSP core implementing DCT/IDCT
operations in a multimedia chip, can only produce the cosine or
inverse cosine transforms of a fixed size data block. Hence, S can
take only certain discrete values, usually known at design time.
Note that, this is in stark contrast with a general purpose network,
where a node can generate a much wider range of messages. As
such, we model the probability mass function FON as: 
p tO N
(
i=
)
FO N t( )
p tO N
(
We can actually compute FON(t), since the communication volume
between the network nodes and λON are known at design time. 
∑
i
0=
t≤
)
=
(2)
=
t
2.2.2. Distribution of tOFF
The duration of the OFF state is the sum of two random variables.
The first is the processing time of the PE, tproc; this can take certain discrete values, based on the number of different tasks implemented by the PE. Therefore, tproc is a discrete random variable
with discrete probability mass function: 
The second component of tOFF is the waiting time (twait) for new
data, before the node cannot start processing. Unlike tON and tproc,
the waiting time twait can take a wide range of values as it depends
on the latency in the network. One can express the distribution of
tOFF as a function of the waiting time, p(twait ≤ t), as follows: 
(3)
2.3.  Predictive control of the traffic sources
Suppose that the ON states of several traffic sources overlap and
lead to congestion in the network. Consequently, starting at time t0,
the packets generated by source i cannot be delivered to their destinations. In this scenario, the source i will continue to inject packets
to the network until it senses congestion, let say at time t0+∆. The
number of flits injected during this time is given by:
where the first element in the tuple represents the total number of
flits that can be generated by the source, while BT is the available
buffering space along the path from source i to the congested
router. If the interval (t0, ∆] covers the ON period of the source, it
is likely that the source will continue to inject packets until it
senses the backpressure effect due to the buffer starvation. This, in
turn, can further increase the number of packets in the network and
hence make the congestion more severe. Since there are many
sources sharing the same network resources, it is extremely important to minimize ∆. 
∆ can be reduced by predicting the possible congestion before it
becomes severe and propagating this information to all traffic
sources. Since the availability in the routers may indicate congestion, the traffic sources can send a packet to the router only if its
availability of greater than zero. Otherwise, the traffic source can
delay the packet injection until the resource availability improves,
as illustrated in Figure 1. Delaying the packet injection effectively
regulates the total number of packets in the network, hence the
average packet latency. While the precise time for packet injection
is difficult (if not impossible) to find at design time, an online predictor can guide the packet generation at the source in order to utilize the network resources in the best possible way.   
3. STATE SPACE MODEL FOR NoC ROUTER
To obtain accurate predictions, we also need a good model for the
NoC router. Traditionally, the network research has been focused
on directly computing the router delay [3,14]. Unlike previous
work, our goal is to predict how many flits the router can accept
over the next k time steps. For this reason, the parameter of interest
is the occupancy of the router input buffers1. 
We propose a state space representation of a NoC router driven by
stochastic inputs, as shown in Figure 2. The state of the router at
time n is given by the number of flits in its input buffers; that is: 
]T
X n( )
=
[
x1 n( ) x2 n( ) … x, P n( )
,
,
(4)
where xP(n) is the state of the input port P (i.e. total number of flits
in all of the input buffers associated with port P) and ‘T’ denotes
the transposition operation. For instance, a router with d neighboring routers and one local PE connection has (d+1) ports. Hence,
X(n) is a (d+1)×1 vector.
The input received at port P, at time n, is denoted by uP(n). uP(n) is
equal to 1, if a flit is received at time n and is 0 otherwise. Similarly, the output from port P is represented by yP(n), where
yP(n) = 1 implies that a flit is transmitted to the downstream router,
at time n. Consequently, the input and output processes of the
router are given by the following P×1 vectors:
U n( )
=
[
u1 n( ) u 2 n( ) … u, P n( )
,
,
] T
Y n( )
=
[
y1 n( ) y2 n( ) … y, P n( )
,
,
]T
(5)
Next, we model how the flits are read from the input buffers.
oP(n) = 1 means that one flit is read from the input buffer at port P,
and the vector O(n) = [o1(n),...,oP(n)]T represents the outcome of
reading process from the input buffers. Note that this is different
from the outputs Y(n) of the router. The output of the input buffers
goes through the crossbar switch and then ends up at one of the
router output ports (Figure 2). 
As a result, the knowledge of either Y(n) or O(n) provides information about the other, given the connections in the crossbar switch.
As a result, the router can be described by an integrator where the
next state is determined by the current state, current input and current output processes, as follows:
X n
1+(
)
=
IP P× X n( ) U n( ) O n( )
+
–
, 
(6)
F p r o c t( )
p tp r o c
t≤
(
)
p tp r o c
i=
(
)
i
0=
t
∑
=
=
FO F F t( )
p tw a i t
–≤
t
t k
t p r o c
t k=
(
)
(
)P tp r o c
t k=
(
)
k
1=
n
∑=
V t( )
m i n
λ t( )
BT∑,
t
t 0=
t 0 ∆+
∑
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
=
tON
tON
λ(t)
λ(t)
t
t
tproc
tproc
OFF ON
OFF ONON
twait
twait
tproc
tproc
ON
ONON
OFF
OFF
…
…
ON
ONON
tON
tON
λcont(t)
λcont(t)
t
t
tproc
tproc
OFF ON
OFF ONON
tproc
tproc
OFF
OFF
…
…
∆
∆
twait
twait
twait
Figure 1. Illustration of the ON-OFF source model and the
control action. By delaying the start of the ON period, the
waiting time in the network can be reduced.
λ(t): Injection rate without controller
λcont(t): Injection rate
with controller
∆: Delay due to the control action
twait: Waiting time without controller
twait: Waiting time with
controller
tproc: Processing time at
PE
1. A similar model for the output buffers can be also developed. 
o2(n)
o2(n)
Switch
Switch
o4(n)
o4(n)
u1(n)
u1(n)
y1(n)
y1(n)
y3(n)
y3(n)
u3(n)
u3(n)
c4(n)
c4(n)
y4(n)
y4(n)
u4(n)
u4(n)
u2(n)
u2(n)
y2(n)
y2(n)
c2(n)
c2(n)
o3(n)
o3(n)
a1(n)
a1(n)
c1(n)
c1(n)
a4(n)
a4(n)
c3(n)
c3(n)
a3(n)
a3(n)
o1(n)
o1(n)
a2(n)
a2(n)
x3(n)
x3(n)
x2(n)
x2(n)
x1(n)
x1(n)
[
[
[
[
[
[
X
X
]Txxxx
]Txxxx
,
,
,
,
,
,
=
=
]TuuuuU
]TuuuuU
=
=
,
,
,
,
,
,
Y
Y
=
=
]Tyyyy
]Tyyyy
,
,
,
,
,
,
]TooooO
]TooooO
,
,
,
,
,
,
=
=
]TaaaaA
]TaaaaA
=
=
,
,
,
,
,
,
]TccccC
]TccccC
,
,
,
,
,
,
=
=
4
4
3
3
2
2
1
1
4
4
3
3
2
2
1
1
4
4
3
3
2
2
1
1
[
[
[
[
[
[
4
4
3
3
2
2
1
1
4
4
3
3
2
2
1
1
4
4
3
3
2
2
1
1
:  State
:  State
:  Input
:  Input
:  Output of the 
:  Output of the 
router
router
: Output of the 
: Output of the 
input ports
input ports
: Availability of 
: Availability of 
the input ports
the input ports
: Availability of 
: Availability of 
the neighbors
the neighbors
x4(n)
x4(n)
[
[
]Tbbbb
]Tbbbb
,
,
,
,
,
,
B
B
4
4
3
3
2
2
1
1
=
=
: Buffer space of 
: Buffer space of 
the input ports
the input ports
Figure 2. The state variables, inputs and outputs of a 4-port
router are shown. 
Router stability
The router described by Equation 6 becomes unstable (i.e. the state
grows unbounded), if the average arrival rate to the router is
greater than the rate at which the router can serve any given
packet. In practice, however, the input buffers are all finite. Hence,
in order to avoid packet loss, no more flits are accepted by the linklevel flow control, when the buffers are full. As a result, the router
model given in Equation 6 can be refined as: 
X n
1+(
)
=
IP P× X n( ) U n( )H n( ) O n( )
+
–
(7)
where H(n) = [h(b1-x1(n), h(b2-x2(n), ... ,h(bP-xP(n)]T. h(xi) is the
unit step function (i.e. h(xi)=0 if xi<0, and h(xi)=1 otherwise), and
b1 to bP represent the capacity of each input buffer. 
Finally, solving Equation 7 with respect to a known state X(n0),
gives the state at time n+n0 as 
X n
n0+(
) X n 0(
=
)
n
n 0+
∑+
j
=
n0
1+
(
U j( )H j( ) O j( )
–
)
(8)
Obviously, the router described by Equation 8 has a bounded
response. However, since such a control does not limit the source
injection directly, if the average arrival rate becomes larger than
the service rate of the router, the input buffers will remain full for
most of the time. This, in turn, results in blocked links and large
delays in the network. While one could regulate the traffic injection by an open loop controller [5], even under a light load, the
packets may experience congestion due to the overlaps between
the ON periods of the traffic sources. For instance, consider a 4×4
2D mesh network running hotspot traffic1. Although the traffic
load is kept low such that the input buffers of the most congested
router are empty more than 80% of time and the buffers become
full only about 1% of time (Figure 3(a)), about 18% of the packets
experience delays more than twice as large as the average delay.
The delay histogram is shown Figure 3(b). Such packets will not
only block the network resources, but also affect the other packets
as well. As a result, we cannot merely rely on such an open-loop
control scheme. In what follows, we show how the router model
presented in this section can be used to implement a predictive
flow controller which regulates the traffic injection to the network. 
4. PREDICTIVE FLOW CONTROLLER
Unpredictable delays in the feedback loop of flow control algorithms prevent the timely transmission of the congestion information and control signals. To mitigate this problem, we propose a
prediction-based control which relies on the traffic source and
router models developed in sections 2 and 3, respectively. These
models enable us to predict the availability of any router at a future
time step, as described next.
4.1.  Availability predictor
Xˆ n0
The optimal k-step predictor for network state, 
(
+
k n 0
)
, is
the conditional expectation at n0+k, given the state at time n0 [10]: 
Xˆ n0
+
k n0
=
E X n 0
k+
) X n0(
) U n 0(
(
)
[
(
,
)
]
Using Equation 8, we have:
Xˆ n 0
(
(
E U j( )H j( ) n0
[
] E O j( ) n0
–
[
) X n0(
=
k n 0
n 0+
(9)
+
)
]
)
k
∑+
j
=
n 0
1+
1. Under hotspot traffic, all nodes in the network receive packets with uniform
probability, while four randomly selected nodes receive some extra traffic.
(a)
(b)
Figure 3. (a) Buffer utilization and (b) delay histogram of a router.
E . n 0
E . X n 0(
where 
[
]
 stands for 
[
) U n 0(
,
)
]
 (for notational simplicity). To compute the k-step forward prediction, we need the
expected value of input and output processes, given the current
state and input. If sufficient processing power is available (e.g.
when the predictor is implemented in a data macro-network with
plenty of resources), then Equation 9 can be directly used to estimate the conditional mean values of the input and output processes
to predict the state at n0+k. However, for NoCs we have to keep
the area overhead as small as possible. For this reason, we use
Equation 9 to predict how many flits a given input port can accept,
over the following k steps, rather than dealing with the absolute
value of the state. 
We call the number of flits the input port P can accept, over the
next k steps, the availability of port P and denote it by aP(n0 ,k).
aP(n0 ,k) is simply the sum of the number of empty slots in the
buffer at time n0+k, and the number of flits that are expected to be
xˆ P n0
admitted 
in 
the following k steps (i.e. 
bP
–
(
k+
)
+
Σj
E uP j( ) h bP
[
(
–
xP j( )
) n0
]
). So the availability vector
A(n0,k) = [a1(n0,k),...,aP(n0,k)], can be found using Equation 9 as
k+
n 0
n 0
=
1+
k
n0+
j
]
)
)
=
=
n 0
1+
(10)
∑+
A n0 k,
(
B X n 0(
–
E O j( ) n0
[
where B = [b1,b2,...,bP,]T is the vector containing the depth for
each input buffer. Intuitively, B - X(n0) represents the availability
at time n0, while the last term is the expected number of flits that
will be read from the router. Since a new flit can be written to the
buffer for each flit being read, the sum of these terms gives the
availability for the interval [n0, k].
The expected value of the read process from the input buffers (the
last term in Equation 10) can be approximated using the router output Y(j) as follows:
j
k
k
]
=
=
n0
)
)
1+
n 0+
n 0+
∑
∑
E Y j( ) n 0
[
E O j( ) n 0
[
g1 1, n0(
g2 1, n0(
gP 1, n 0(
) … g1 P, n 0(
) … g2 P, n 0(
… … …
) … gP P, n0(
where the coefficients gi,k(n0) are determined based on the state of
the switch matrix and channel allocations in the router. If we let
G(n0) = {gi,k(n0)}, then Equation 10 can be written as:
E Y j( ) n0
[
B X n0(
–
A n 0 k,
(
G n0(
(11)
n 0+
1+
n 0
+
=
=
)
)
)
]
]
k
j
)
j
∑
n 0
=
1+
 
n0
=
k+
n 0
1+
Note that 
Σj
(
E Y j( ) n 0
[
]
)
 is the expected number of flits
transmitted by the router in the interval [n0, k]. However, this is
nothing but the availability of the immediate neighboring routers.
In other words, instead of predicting the number of flits transmitted over the next k steps, we aggregate the availability information
already predicted by the neighboring routers. As a result, the availability of a router is updated using the following equation: 
A n 0 k,
=
B X n0(
–
+
G n0(
)C n 0
1 k,–
(12)
(
)
)
(
)
where the vector C(n0-1, k) denotes the availability of the immediate neighbors predicted at time n0-1, as illustrated in Figure 2.
4.2.  Practical implementation of the predictor
(13)
T
] 1 P×
The predictor defined by Equation 12 represents an iterative process. The availability predicted at time n0 is a function of the availability of the neighboring routers predicted at time n0 –1. The
initial availability values are the sum of buffer capacities and the
prediction step, i.e.
A 0 k,(
)
=
B k 1 1 … 1
+
[
,
,
,
since all of the buffers are empty at time 0. 
The computation of the availability depends mainly on the coefficients gi,k, as illustrated in Figure 4. The update process consists of
distributing the availability of the neighboring routers to the input
ports as a function of the current connections in the crossbar
switch. For example, when processing the availability data from
port 1 (i.e. c1), we first check whether or not one of the input ports
is connected to port 1 through the crossbar switch. If this is true
(e.g. input port 2) and δ flits have been already sent over this connection, the remaining flits coming from the same input port will
use this connection. As a result, min(c1,S-δ) flits are allocated to
the input port 2, while the remaining slots are distributed among
the remaining ports. 
The coefficients gik determine this allocation process. In our
implementation, we do this distribution uniformly to minimize the
area overhead for the predictor. The allocation requires (P – 1)2 mbit full adders and P m-bit shifters, where m is the total number of
bits needed to express the maximum availability, i.e. log2(a1(0, k)).
For instance, in our implementation m = 4, and the area overhead
of the predictor is in the order of 1000 transistors. We estimate the
overhead of the predictor implemented for a 5-port wormhole
router with 16x16 bit input buffers to be about 10%. A more precise analysis of the implementation complexity for the predictor is
left as future work. 
e
e
h
h
f
f
t
t
t
t
m
m
t
t
s
s
r
r
e
e
o
o
r
r
u
u
o
o
a
a
r
r
a
a
g
g
d
d
n
n
r
r
y
y
o
o
b
b
h
h
b
b
g
g
a
a
e
e
a
a
n
n
v
v
i
i
l
l
i
i
t
t
i
i
l
l
i
i
i
i
A
A
c1
c1
c2
c2
c3
c3
cP
cP
g1,2
g1,2
g1,3
g1,3
M
M
g1,P
g1,P
Crossbar switch 
Crossbar switch 
connection 
connection 
information
information
g2,1
g2,1
g2,3
g2,3
M
M
M
M
g2,P
g2,P
M
M
a1
a1
a2
a2
a3
a3
m-bit adder
m-bit adder
m=4, in our 
m=4, in our 
implementation
implementation
(P-1)×(P-1) are 
(P-1)×(P-1) are 
needed since 
needed since 
gi,i=0 ∀i
gi,i=0 ∀i
gP,1
gP,1
gP,2
gP,2
gP,3
gP,3
M
M
L
L
L
L
L
L
M
M
L
L
Updated availability data to the 
Updated availability data to the 
neighboring routers
neighboring routers
aPL
aPL
Figure 4. Practical implementation of the predictor.
4.3.  Using prediction for network control
Each router in the network updates periodically its availability by
aggregating the data received from the immediate neighbors using
Equation 12. As a result, the availability of a local input port connected to a traffic source reflects the backpressure from all of the
downstream routers. As such, when a traffic source sees that the
input port connected to it has a zero availability, it delays the generation of new packets until the availability of the port becomes
greater than zero. Since the information exchange between the
neighboring routers is achieved by a small number (i.e. log2(aP(0,
k))) of dedicated control wires, the congestion prediction does not
experience long queuing delays. In order to guarantee the timely
transmission of the prediction to the traffic sources, k is selected to
match the diameter of the network.
5. EXPERIMENTAL RESULTS
Next, we demonstrate the effectiveness of the proposed flow control technique using an audio/video system complying with the
H263.1 standard, as well as synthetic benchmarks which are all
mapped to a 4×4 2D mesh network. The simulations are performed
using a custom cycle-accurate NoC simulator which implements a
basic ON/OFF switch-to-switch flow control, the ON/OFF traffic
sources and the proposed flow control scheme. The simulations are
repeated for a range of buffer sizes in routers and local PEs. The
results reported next are obtained for 4-flit input buffers in the
routers and 100-flit local memory in the host PE1.
5.1.  Audio/Video traffic
The audio/video system from [8] is first simulated using only the
switch-to-switch flow control. When the offered load is about half
of the maximum achievable throughput, the average and maximum
packet latencies in the network are found to be 149 and 897 cycles,
respectively. After that, the simulations are repeated with the proposed flow controller. As shown in Table 1, the average packet
latency becomes 47 cycles, while the maximum packet latency
drops to 466 cycles. This huge reduction in packet latencies is
mainly due to the reduced number of packets in the network. 
As mentioned before, unlike the switch-switch flow controller, the
proposed controller regulates the number of packets in the network
directly. As such, the number of packets in the network drops from
129 to 52 packets which is about a 2.5× reduction.
Switch-to-switch 
The proposed 
control only
flow control
149 cycles
47 cycles
897 cycles
466 cycles
94 packets
29 packets
129 packets
52 packets
Ave. latency
Max. latency
Ave. # of packets
Max. # of packets
Reduction 
(×)
3.2
1.9
3.2
2.5
Table 1: The reduction in the latency and number of packets in
the network due to the proposed flow control algorithm.
To better understand the effects of the controller, in Figure 5, we
further analyze the packet latencies. For the network without the
proposed flow controller, about 49% percent of the packets experience longer delays than the average delay (i.e 149 cycles). The
packets located at the tail of the distribution in Figure 5(a) are the
1. Note that the local memory in the host PE is not part of the router. The 100flit local buffer is used to emphasize that (i) its size is finite and (ii) PEs sense
the backpressure from the network for the switch-to-switch flow control.
 
 
 
 
 
 
 
 
 
 
main cause for this poor performance. The proposed technique prevents the packets that are likely to experience such long delays
from entering the network. Indeed, as depicted in Figure 5(b), the
latency histogram is pushed significantly towards left so about
91% of packets experience less than 100 cycles latency.   
since in this case the PE has to stall to avoid packet loss. Nevertheless, the proposed controller works efficiently for various buffer
sizes, as shown in Table 2. 
(a)
(b)
Figure 5. Histogram of the packet latencies without (a) and
with (b) the proposed flow controller.
5.2.  Synthetic traffic
Similar to Section 5.1, new experiments are performed for uniform
and hotspot traffic patterns. First, we compare the performance of
a 4×4 2D mesh network under hotspot traffic with and without the
proposed controller. The average packet latency in the network is
plotted as a function of the packet injection rate in Figure 6(a). We
observe that without the flow controller, the network become congested as the packet injection rate increases. The reason for this
behavior is uncovered in Figure 6(b). Indeed, in absence of a traffic controller, as the traffic injection rate increases, the number of
packets in the network grows at an increasing pace. The proposed
flow controller, on the other hand, effectively limits the number of
packets injected to the network, as depicted in Figure 6(b). This, in
turn, results in significant improvements in the average packet
latency. Finally, Figure 6 (a) and (b) demonstrate that the average
packet latency is proportional to the average number of packets in
the network and justify once more controlling the pac"
A multi-path routing strategy with guaranteed in-order packet delivery and fault-tolerance for networks on chip.,"In this work we present a multi-path routing strategy that guarantees in-order packet delivery for networks on chips (NoCs). We present a design methodology that uses the routing strategy to optimally spread the traffic in the NoC to minimize the network bandwidth needs and power consumption. We also integrate support for tolerance against transient and permanent failures in the NoC links in the methodology by utilizing spatial and temporal redundancy for transporting packets. Our experimental studies show large reduction in network bandwidth requirements (36.86% on average) and power consumption (30.51% on average) compared to single-path systems. The area overhead of the proposed scheme is small (a modest 5% increase in network area). Hence, it is practical to be used in the on-chip domain","A Multi-Path Routing Strategy with Guaranteed In-Order
Packet Delivery and Fault-Tolerance for Networks on Chip
Srinivasan Murali(cid:1) , David Atienza§ † , Luca Benini‡ , Giovanni De Micheli†
(cid:1)CSL/Stanford University, Stanford, USA, smurali@stanford.edu
§DACYA/UCM, Madrid, Spain, datienza@dacya.ucm.es
† LSI/EPFL, Lausanne, Switzerland, {david.atienza, giovanni.demicheli}@epﬂ.ch
‡DEIS/University of Bologna, Bologna, Italy, lbenini@deis.unibo.it
ABSTRACT
In this work we present a multi-path routing strategy that guarantees in-order packet delivery for Networks on Chips (NoCs). We
present a design methodology that uses the routing strategy to optimally spread the trafﬁc in the NoC to minimize the network bandwidth needs and power consumption. We also integrate support for
tolerance against transient and permanent failures in the NoC links
in the methodology by utilizing spatial and temporal redundancy
for transporting packets. Our experimental studies show large reduction in network bandwidth requirements (36.86% on average)
and power consumption (30.51% on average) compared to singlepath systems. The area overhead of the proposed scheme is small
(a modest 5% increase in network area). Hence, it is practical to be
used in the on-chip domain.
Categories and Subject Descriptors
C.3 [Special-purpose and Application-based Systems]: Real-time
and embedded systems; C.4 [Performance of Systems]: Design
studies
General Terms: Design, Measurement, Performance.
keywords: Systems on Chip, networks on chip, routing, multipath, fault-tolerance, re-order buffers, ﬂow control.
1.
INTRODUCTION
Scalable Networks on Chips (NoCs) are needed to provide high
bandwidth communication infrastructure for SoCs [1]-[3]. The use
of NoCs facilitate applying network error resiliency techniques to
tolerate transient and permanent errors in interconnects.
For routing packets in the NoC, either a single-path can be used
for all the packets from a source to a destination or multiple paths
can be utilized. When compared to single-path routing, the multipath routing scheme improves path diversity, thereby minimizing
network congestion and trafﬁc bottlenecks. Reducing the trafﬁc
bottlenecks leads to lower required NoC operating frequency, as
trafﬁc is spread evenly in the network. A reduced operating frequency translates to a lower power consumption in the NoC. AnPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2006, July 24–28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007 ...$5.00.
other important property of the multi-path routing strategy is its
spatial redundancy for transporting a packet in the on-chip network.
Many of today’s NoC architectures are based on single-path routing. This is because, with multi-path routing, packets can reach the
destination in an out-of-order fashion due to the difference in path
lengths or due to difference in congestion levels on the paths. The
re-order buffers needed at the receiver for ordering the packets have
large area and power overhead and deterministically choosing the
size of them is infeasible in practice. The re-order buffers, unless they have inﬁnite storage capacity, can be full for a particular
scenario and can no longer receive packets. This leads to dropping of packets to recover from the situation and requires end-toend ACK/NACK protocols for resuming the transaction. However,
such protocols have signiﬁcant overhead in terms of network resource usage and congestion [13]. Thus, they are not commonly
used in the NoC domain.
In this work we present a multi-path routing strategy with guaranteed in-order packet delivery (without packet dropping) for onchip networks. We present a method to split the application trafﬁc
across multiple paths to obtain a network with minimum power
consumption. We integrate reliability constraints in our multi-path
design methods to provide a reliable NoC operation with least increase in network trafﬁc. Experiments on several benchmarks show
large power savings for the proposed scheme when compared to traditional single-path schemes and multi-path schemes with re-order
buffers. The area overhead of the proposed scheme is very small
(only 5% increase in network area). Hence, it is usable in the onchip domain.
Many works on mapping of applications onto NoC architectures
have considered the routing problem during the NoC design phase
[5]-[8]. The adaptive routing schemes presented in [9] and [10], assume that the architectural support needed for such routing schemes
(such as packet re-order buffers) are available in the NoC. Several works in the multi-processor ﬁeld have focused on the design
of efﬁcient routing strategies [19]- [22]. Several research works
have focused on designing reliable NoC systems [11]-[18]. In [14],
fault-tolerant stochastic communication for NoCs is presented. The
use of non-intersecting paths for achieving fault-tolerant routing
has been utilized in many designs, such as the IBM Vulcan [19].
The use of temporal and spatial redundancy in NoCs to achieve
resilience from transient failures is presented in [18].
2. ROUTING WITH IN-ORDER DELIVERY
In this section, we present the conceptual idea of the multi-path
routing strategy with in-order packet delivery. For analysis purposes, we deﬁne the NoC topology and the trafﬁc ﬂow paths as:
Out−of−order packets
stalled
P4 P3
look up 
table
arbiter
waiting for packet P1
Packets
P2 − P1
crossbar
input buffers
output buffers
Figure 1: Switch design to support multi-path routing
D E FIN I T ION 1. The topology graph is a directed graph G(V , E )
with each vertex vk ∈ V representing a switch/Network Interface
(NI) in the topology and the directed link (or edge) el ∈ E representing a direct communication between two switches/NIs.
We represent the trafﬁc ﬂow between a pair of cores in the NoC
as a commodity i, with the source switch/NI of the commodity being si and the destination of the commodity being di . Let the total
number of commodities be I . The rate of trafﬁc transferred by commodity i is represented by ri .
D E FIN I T ION 2. Let the set SPi represent the set of all paths
for the commodity i, ∀i ∈ 1 · · · I . Let P j
∀j ∈ 1 · · · |SPi |. Thus P j
i be an element of SPi ,
i represents a single-path from the source
to destination for commodity i. Each path P j
i has a set of links.
We deﬁne a set of paths to be non-intersecting if the paths originate from the same source vertex but do not intersect each other
in the network, except at the destination vertex. Consider packets
that are routed on the two non-intersecting paths. Note that with
worm-hole ﬂow control [19], packets of a commodity on a particular path are in-order at all time instances. However, packets on
the two different paths can be out-of-order. Therefore, we need a
mechanism to re-order the packets at the path re-convergent nodes
to maintain the packet ordering.
To implement the re-ordering mechanism at network re-convergent
nodes, the following architectural changes to the switches/NIs of
the NoC are required (shown in Figure 1). Even though the methodology presented in this paper is general, for illustrative purposes we
assume that the component architectures are based on the design
presented in [4]. To support multi-path routing, individual packet
identiﬁers are used for packets belonging to a single commodity. At
the re-convergent switch, we use a look-up table to store the identiﬁer of the next packet to be received for the commodity. Initially
(when the NoC is reset), the identiﬁers in the look-up tables are set
to 1 for all the commodities. When packets arrive at the input of the
re-convergent switch, the identiﬁer of the packet is compared with
the corresponding look-up table entry. If the identiﬁers match, the
packet is granted arbitration and the look-up table identiﬁer value
for this commodity is incremented by 1. If the identiﬁers do not
match, it is an out-of-order packet and access to the output is not
granted by the arbiter, and it remains at the input buffer.
As the packets on a particular path are in-order, the mechanism
only stalls packets that would also be out-of-order if they reach
the switch. Due to the disjoint property of the paths reaching the
switch, the actual packet (matching the identiﬁer on the look-up
table) that needs to be received by the switch is on a different path.
As a result, such a stalling mechanism (integrated with credit-based
or on-off ﬂow control mechanisms [19]) does not lead to packet
dropping, which is encountered in traditional schemes when the
re-order buffers at the receivers are full.
3. MULTI-PATH TRAFFIC SPLITTING
From the set of non-intersecting paths for each commodity, we
need to determine the amount of ﬂow of each commodity across the
paths that minimizes congestion. Then, we can assign probability
values for each path of every commodity, based on the trafﬁc ﬂow
across that path for the commodity. At run time, we can choose the
path for each packet from the set of paths based on the probability
values assigned to them.
To achieve this trafﬁc splitting, we use a Linear Programming
(LP) based method to solve the corresponding multi-commodity
ﬂow problem. The objective of the LP is to minimize the maximum trafﬁc on each link of the NoC topology, satisfying the bandwidth constraints on the links and routing the trafﬁc of all the commodities in the NoC. Our LP is represented by the following set of
equations:
(1)
(2)
(3)
(4)
(5)
min:
t
X
f j
i = ri ,
∀i
i = f lowel ∀el
f j
s.t
X
∀j∈1···|SPi |
X
∀i
∀j,el ∈P j
i
f lowel ≤ bandwidthel ∀el
f lowel ≤ t ∀el ∈ P j
∀i, j
i ,
f j
i ≥ 0
(6)
In the objective function we use the variable t to represent the
maximum ﬂow on any link in the NoC (refer Equations 1, 5). Equation 2 represents the constraint that the NoC has to satisfy for the
trafﬁc ﬂow of each commodity, with the variable f j
i representing
the trafﬁc ﬂow on the path P j
i of commodity i. The ﬂow on each
link of the NoC and the bandwidth constraints are represented by
Equations 3 and 4.
Other objectives (such as minimizing the sum of trafﬁc ﬂow on
the links) and constraints (like latency constraints for each commodity) can also be used in the LP.
As an example, the latency constraints for each commodity can
be represented by the following equation:
X
i × lj ) /
(f j
X
i ≤ di
f j
(7)
∀j∈1···|SPi |
∀j∈1···|SPi |
where di is the hop delay constraint for commodity i and lj is the
hop delay of path j . Once the ﬂows on each path of a commodity
are obtained, we can order or assign probability values to the paths
based on the corresponding ﬂows.
4. ADDING FAULT-TOLERANCE SUPPORT
The errors that occur on the NoC links can be broadly classiﬁed into two categories:
transient and permanent errors. To recover from transient errors, error detection or correction schemes
can be utilized in the on-chip network [13]. Forward error correcting codes such as Hamming codes can be used to correct single-bit
errors at the receiving NI. However, the area-power overhead of
the encoders, decoders and control wires for such error correcting schemes increases rapidly with the number of bit errors to be
corrected. In practice, it is infeasible to apply forward error correcting codes to correct multi-bit errors [13]. To recover from such
multi-bit errors, switch-to-switch (link-level) or end-to-end error
detection and retransmission of data can be performed. This is applicable to normal data packets. However, control packets such as
interrupts carry critical information and need to meet real-time requirements. Using retransmission mechanisms can have signiﬁcant
latency penalty that would be unacceptable to meet the real-time
requirements of critical packets. Error resiliency for such critical
packets can be achieved by sending multiple copies of the packets
across one or more paths. At the receiving switch/NI, the error detection circuitry can check the packets for errors and can accept an
error free packet. When sending multiple copies of a packet, it is
important to achieve the required reliability level for packet delivery with minimum data replication. We formulate the mathematical
models for the reliability constraints and consider them in the LP
formulation presented in previous section, as follows:
D E FIN I T ION 3. Let the transient Bit-Error Rate (BER) encountered in crossing a path with maximum number of hops in the NoC
be βt . Let the bit-width of the link (equal to the ﬂit-width) be W .
We assume a single-bit error correcting Hamming code is used
to recover from single-bit errors in the critical packets and packet
duplication is used to recover from multi-bit errors. The probability
of having two or more errors in a ﬂit received at the receiving NI is
given by:
P( ≥ 2 errors) = γt =
WX
k = 2
k × β k
t × (1 − βt )W −k (8)
CW
When a ﬂit is transmitted nt times, the probability of having two
or more errors in all the ﬂits is given by:
θt = γ nt
t
(9)
As in earlier works ([11]-[13]), we assume that an undetected or
uncorrected error causes the entire system to crash. The objective is
to make sure that the packets received at the destination have a very
low probability of undetected/uncorrected errors, ensuring the system operates for a pre-determined Mean Time To Failure (MTTF)
of few years. The acceptable residual ﬂit error-rate, deﬁned as the
probability of one or more errors on a ﬂit that can be undetected by
the receiver, is given by the following equation:
E rrres = Tcycle/(M T T F × Nc × inj )
(10)
where Tcycle is the cycle time of the NoC, Nc is the number of
cores in the system and inj is the average ﬂit injection rate per
core. Each critical packet should be duplicated as many times as
necessary to make the θt value to be greater than the E rrres value,
i.e.:
≥ E rrres
θt = γ nt
i.e. nt ≥ ln(E rrres )/ln(γt )
t
The minimum number of times the critical packets should be
replicated to satisfy the reliability constraints is given by:
nt = (cid:5)ln(E rrres)/ln(γt )(cid:6)
(11)
To consider the replication mechanism in the LP, the trafﬁc rates
of the critical commodities are multiplied by nt and Equation 2 is
modiﬁed for such commodities as follows:
X
i = nt × ri ∀i, critical
f j
(12)
∀j∈1···|SPi |
To recover from permanent link failures, packets need to be sent
across multiple non-intersecting paths. The non-intersecting nature
of the paths makes sure that a link failure on one path does not affect
the packets that are transmitted on the other paths. The probability
of a path failure and the number of permanent path for each commodity (denoted by np ) can be obtained similar to the derivation of
nt .
Let the total number of paths for a commodity i be denoted by
ntot,i . Once the number of possible path failures is obtained, we
(ntot,i − np ) paths should be able to support the trafﬁc demands
have to model the system such that for each commodity, any set of
of the commodity. Thus, even when np paths fail, the set of other
paths would be able to handle the trafﬁc demands of the commodity
and proper system operation would be ensured. We add a set of
ntot,i !/(np !× (ntot,i−np )!) linear constraints in place of Equation
the fact that the trafﬁc on (ntot,i − np ) paths can handle the trafﬁc
2 for each commodity in the LP, with each constraint representing
demands of the commodity.
Thus the paths of each commodity can support the failure of np
paths for the commodity, provided more than np paths exist. When
we introduce these additional linear constraints, the impact on the
run-time of the LP is small (for our experiments, we did not observe
any noticeable delay in the run-time). This is due to the fact that
the number of paths available for each commodity is usually small
(less than 4 or 5) and hence only few tens of additional constraints
are introduced for each commodity.
5. SIMULATION RESULTS
The estimated power overhead (based on gate count and synthesis results for switches/NIs from [4]) at the switches/NIs to support the multi-path routing scheme for a 4 × 3 mesh network is
found to be 18.09 mW, which is around 5% of the base NoC power
consumption. For the power estimation, without loss of generality, we assume that 8 bits are used for representing the source and
destination addresses and 8-bit packet identiﬁers are utilized. The
power overhead accounts for the look-up tables and the combinational logic associated with multi-path routing scheme. The numbers assume a 500 MHz operating frequency for the network. The
estimated area overhead (from gate and memory cell count) for the
multi-path routing scheme is low (less than 5 % of the NoC component area). The maximum possible frequency estimate of the switch
design with support for the multi-path routing tables is above 500
MHz, with synthesis results based on the architecture from [4].
5.1 Comparisons with Single-Path Routing
The network power consumption for the various routing schemes:
dimension ordered (Dim), minimum path (Min) and our proposed
multi-path (Multi) strategy for different applications is presented
in Figure 2(a). The numbers are normalized with respect to the
power consumption of dimension-ordered routing. We use several
benchmark applications for comparison: Video Object Plane Decoder (VOPD - mapped onto 12 cores), MPEG decoder (MPEG
- 12 cores), Multi-Window Display application (MWD - 12 cores)
and Picture-in-Picture (PIP - 8 cores) application. By using the
proposed routing scheme, on average we obtain 33.5% and 27.52%
power savings compared to the dimension ordered and minimum
path routing, respectively.
The average packet latencies incurred for the MPEG NoC for the
different routing schemes is presented in Figure 2(b). The multin
o
i
t
p
m
u
s
n
o
C
r
e
w
o
P
d
e
z
i
l
a
m
r
o
N
1.5
1
0.5
0
VOPD MPEG MWD PIP
Applications
(a) Routing effects on applications
Dim
Min
Multi
)
s
n
300
n
i
(
y
c
n
e
200
t
a
L
t
100
e
k
c
a
P
Dim, Min
Multi
e
g
a
r
e
v
A
0
200
300
400
500
Network Frequency (in MHz)
(b) Average latency for MPEG NoC
n
o
i
t
p
m
u
s
n
o
C
r
e
w
o
P
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0% 
BER = 1e−5
BER <= 1e−6
20% 40% 60% 80% 100%
Percentage of Critical Traffic
(c) Effect of error resiliency
Figure 2: (a) Performance of routing schemes for MPEG. (b), (c) Effect of routing and fault-tolerance on NoC power consumption
path routing strategy results in reduced frequency requirements to
achieve the same latency as the single-path schemes for a large part
of the design space.
When compared to the multi-path routing scheme with re-order
buffers (10 packet buffers/receiver), the current scheme results in
28.25% reduction in network power consumption. The total run
time for applying our methodology (includes the run time for path
selection algorithms for all commodities and for solving the resulting LP) is less than few minutes for all the benchmarks, when running on a Sun workstation at 1 GHz.
5.2 Effect of Fault-Tolerance Support
The amount of power overhead incurred in achieving fault-tolerance
against temporary errors depends on the transient bit-error rate (βt )
of each link and the amount of data that is critical and needs replication. The effect of both factors on power consumption for the
MPEG decoder NoC is presented in Figure 2(c). The power consumption numbers are normalized with respect to the base NoC
power consumption (when no fault-tolerance support is provided).
As the amount of critical trafﬁc increases, the power overhead of
packet replication is signiﬁcant. Also, as the bit-error rate of the
NoC increases (higher BER value in the ﬁgure, which imply a
higher probability of bit-errors happening in the NoC), the amount
of power overhead increases. We found that for all BER values
lower than or equal to 1e-6, having a single duplicate for each
packet was sufﬁcient to provide the required MTTF of 5 years.
ure for each commodity of the MPEG NoC resulted in a 2.33×
Adding support for resiliency against a single-path permanent failincrease in power consumption of the base NoC.
6. ACKNOWLEDGMENTS
This work is supported by the US National Science Foundation (NSF, contract CCR-0305718) for Stanford University. It is
partially supported by the the Swiss National Science Foundation
(FNS, Grant 20021-109450/1), Spanish Government Research Grants
TIC 2002/0750 and TIN2005-5619, a Mobility Post-Doc Grant from
UCM for David Atienza, by the Semiconductor Research Corporation (SRC, project number 1188) and a grant by STMicroelectronics for University of Bologna.
7. CONCLUSIONS
The re-order buffers required in traditional multi-path schemes
have large area, power overhead and deterministically sizing them
is infeasible in practice. In this work, we have presented a multipath routing strategy that guarantees in-order packet delivery at the
receiver. We introduced a methodology to split the application trafﬁc across the multiple paths to obtain a network operation with
minimum power consumption. We also integrated with the methodology, the use of spatial and temporal redundancy to tolerate transient as well as permanent errors occurring on the NoC links. Our
method results in large NoC power savings for several SoC designs
when compared to traditional single-path systems.
8. "
DyXY - a proximity congestion-aware deadlock-free dynamic routing method for network on chip.,"A novel routing algorithm, namely dynamic XY (DyXY) routing, is proposed for NoCs to provide adaptive routing and ensure deadlock-free and livelock-free routing at the same time.A new router architecture is developed to support the routing algorithm.Analytical models based on queuing theory are developed for DyXY routing for a two-dimensional mesh NoC architecture,and analytical results match very well with the simulation results.It is observed that DyXY routing can achieve better performance compared with static XY routing and odd-even routing.","DyXY - A Proximity Congestion-Aware Deadlock-Free
Dynamic Routing Method for Network on Chip
Dept. of Electrical & Computer Engineering and Computer Science, University of Cincinnati
Cincinnati, OH, USA
Ming Li, Qing-An Zeng, Wen-Ben Jone
lim0@ececs.uc.edu, qzeng@ececs.uc.edu, wjone@ececs.uc.edu
ABSTRACT
A novel routing algorithm, namely dynamic XY (DyXY)
routing,
is proposed for NoCs to provide adaptive routing and ensure deadlock-free and livelock-free routing at
the same time. A new router architecture is developed to
support the routing algorithm. Analytical models based on
queuing theory are developed for DyXY routing for a twodimensional mesh NoC architecture, and analytical results
match very well with the simulation results. It is observed
that DyXY routing can achieve better performance compared with static XY routing and odd-even routing.
Categories and Sub ject Descriptors: B.4.3 [Hardware]:
Input/Output and Data Communication - Interconnections.
General Terms: Algorithms, Performance, Design.
Keywords: Network-on-Chip, Packet Routing, Queuing
Theory.
1.
INTRODUCTION
A layered architecture called Network on Chip (NoC) [1]
[2] has been proposed for global communication in complex
SoCs to meet the performance requirements. In NoCs, routing algorithms are used to determine the path of a packet
traversing from the source to the destination. Routing algorithms can be generally classiﬁed as deterministic routing
and adaptive routing. The former beneﬁts from its simplicity in router design; however, it is likely to suﬀer from
throughput degradation when the packet injection rate increases. The later determines routing paths based on the
congestion conditions in the network. The adaptiveness reduces the chance for packets to enter hot-spots or faulty
components, and hence reduces the blocking probability of
packets. Adaptiveness is an important factors for message
routing, and the other important requirement of a routing
algorithm is the freedom from dead lock and livelock.
Many routing algorithms dealing with networks with the
mesh architecture have been proposed for deadlock-free and
adaptiveness recently. In [3]-[5], virtual channels are introduced to assist the design of nonadaptive and adaptive routPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2006, July 24–28, 2006, San Francisco, California, USA.
Copyright 2006 ACM 1-59593-381-6/06/0007 ...$5.00.
ing algorithms for a variety of network architectures. In [6][9], routing algorithms that require no virtual channels have
been proposed for networks with the mesh architecture. A
static XY routing algorithm for two-dimensional meshes has
been presented in [6]. With static XY routing, a packet ﬁrst
traverses along the x dimension and then along the y dimension. This algorithm is deadlock-free but provides no
adaptiveness. The work in [7] proposed another algorithm
called the turn model, which is a partially adaptive routing
algorithm without virtual channels. In [8], a routing algorithm called odd-even turn was proposed based on the turn
model. It restricts some locations where turns can be taken
so that deadlock can be avoided. A routing scheme called
DyAD was proposed in [9]. This algorithm is the combination of a deterministic routing algorithm called oe-ﬁx, and
an adaptive routing algorithm called odd-even as proposed
in [8]. The router can switch between these two routing
modes based on the network’s congestion conditions.
In this paper, we propose a novel routing algorithm, namely
dynamic XY (DyXY) routing, which provides adaptive routing based on congestion conditions in the proximity, and
ensures deadlock-free and livelock-free routing at the same
time. The adaptiveness lies in making routing decisions
by monitoring congestion status in the proximity, and the
deadlock-free and livelock-free features are incorporated by
limiting a packet to traverse the network only following one
of the shortest paths between the source and the destination. The DyXY routing method can be supported by a
router architecture eﬃciently. Analytical models based on
queuing theory are developed for both XY routing (called
static XY in the following part of this paper) and DyXY
routing to evaluate their performance for a two-dimensional
mesh NoC architecture. Extensive simulation is done to validate the analytical models, and it is observed that the simulation results match very well with the analytical results.
To further evaluate the performance of DyXY, we compare
it with both static XY routing and odd-even routing under
diﬀerent traﬃc patterns, and it is shown that DyXY routing
can achieve the best performance.
2. DYXY ROUTING AND ROUTER
ARCHITECTURE
With the DyXY routing algorithm, each packet only travels along a shortest path between the source and the destination (this guarantees the deadlock-free feature of the routing
algorithm).
If there are multiple shortest paths available,
the routers will help the packet to choose one of them based
on the congestion condition of the network. The detailed
routing algorithm can be summarized as follows:
• Read the destination of an incoming packet.
• Compare addresses of the destination and the current router.
– If the destination is the local core of the current router,
send the packet to the local core;
– Else
∗ If the destination has the same x (or y) address as
the current router, send the packet to the neighboring router on the y-axis (or x-axis) towards the destination;
∗ Else, check the stress values of current router’s neighbors towards the destination, and send the packet to
the neighbor with the smallest stress value.
The stress value is a parameter representing the congestion condition of a router. Here, we use the ‘instant queue
length’ of each router (i.e., the number of occupied cells in
all input buﬀers) as the stress value, since it achieves the
best results among all kinds of average or ﬂow-control types
we have attempted. Each router stores instant stress values
for all neighbors, and each stress value is updated based on
an event-driven mechanism.
Figure 1: NoC interconnections under DyXY routing.
Figure 2: Router Architecture for DyXY routing.
The NoC system interconnection under DyXY routing is
shown in Figure 1, and the router architecture is shown
in Figure 2. Each router contains a set of ﬁrst-in ﬁrst-out
(FIFO) input buﬀers, an input arbiter, a history buﬀer, a
crossbar switch circuit, a controller, and four stress value
counters. The size of each input buﬀer is a design parameter.
In Figure 2, Din0 /Dout0 to Din4 /Dout4 represent the data
lines between a router and its local core, right router, up
router, left router and down router, respectively. Rin0 /Rout0
to Rin4 /Rout4 represent the request signal lines between a
router to its local core and all neighbor routers. Sin1 /Sout1
to Sin4 /Sout4 represent the input/output signal lines to update stress value between the local router and its neighbors.
At each clock cycle, the history buﬀer records the channels that have input requests. The input arbiter selects a
request from input buﬀers to process based on the FIFO
mechanism referring to records in the history buﬀer. The
main task of the controller is to determine the routing path
for incoming packets, based on the routing algorithm described above. Besides this, the controller also needs to send
signals to its neighbors for updating its stress value. When
there are new incoming packets from neighbors or the local core, the controller will inform neighbors to increase its
stress value. When the outgoing direction for a packet is
determined, the controller will set a request signal to the
local core or the corresponding neighbor router, and inform
all neighbors to decrease its stress value.
3. MODELING AND ANALYSIS
A NoC system can be modeled as a queuing network.
The cores generate packets and inject them into the routing
network. Each packet is queued in the input buﬀer of the
ﬁrst router, and then transmitted to the next router until it
reaches its destination.
3.1 Router Modeling and Analysis
One of the best indicators for a router’s performance is its
mean response time. In our analysis, we model each buﬀer in
a single router as a non-preemptive inﬁnite buﬀer. Although
each channel has a separate input buﬀer, the sequence to
process all requests is based on the FIFO mechanism. Hence,
all input buﬀers of a router can be modeled as a single FIFO
queue. Using the inﬁnite buﬀer model, we can estimate the
mean waiting time of a packet in each router, and thus can
use this information to estimate the required buﬀer size of
each router for a speciﬁc traﬃc load. To model and analyze
the mean response time of each router, we ﬁrstly analyze
the traﬃc load of each router.
A. Traﬃc load
work with U × V routers (cores). Router i (core i) has a
Assume that the NoC network is a two-dimensional netnetwork address (ix , iy ) which indicates its x and y coordinates, respectively. A packet enters router i due to one of
the following three reasons: 1) The packet from core i has
to be sent out from its local router (ﬁxed regardless of the
routing algorithm); 2) The packet whose destination is core
i has to go through its local router(varied with diﬀerent network communication patterns); 3) The packet needs to go
through router i to be passed to other routers (aﬀected by
both the network communication pattern and the routing
algorithm).
Assume that each core generates packets following Poisson
distribution with mean rate λ (λ is also called the average
packet injection rate for the NoC). The service time of each
router for all packets follows exponential distribution with
mean rate µ. Let λi be the mean packet arrival rate of router
i, λs d be the mean rate of packets from core s to core d and
Ps d i be the probability of a packet from core s to core d
via router i. The mean packet arrival rate of router i can be
calculated by
λi = λ +
U ×VX
U ×VX
s=1
d=1
λs dPs d i , for s (cid:1)= d.
(1)
The mean rate λs d of packets from core s to core d is
determined by the network communication pattern, and the
probability of a packet from core s to core d via router i
is determined by both the network communication pattern
and the routing algorithm. Here, we use an uniform network communication pattern to model the traﬃc load of
each router with both static XY and DyXY routing algorithms. With the uniform network communication pattern,
λs d can be calculated as
n
, for
λs d =
8<
:
λ
U × V − 1
1 ≤ s ≤ U × V
, and s (cid:1)= d.
1 ≤ d ≤ U × V
(2)
For static XY routing, the probability of a packet from core
s to core d via router i is given by
if iy = sy and ix ∈ [sx , dx ] (or [dx , sx ])
or ix = dx and iy ∈ [sy , dy ] (or ([dy , sy ]),
0, otherwise,
(3)
where z ∈ [l, h] denotes that z is a value between l and h.
Hence, the mean arrival rate of each router with static XY
routing can be calculated using Equations (1), (2) and (3).
For DyXY routing, the probability of a packet from core
s to core d via router i is given by
Ps d i =
1,
8>><
>>:
Ps d i =
1,
0,
P
if i = s or i = d,
if iy (cid:1)∈ [sy , dy ] (or [dy , sy ])
or ix (cid:1)∈ [sx , dx ] (or [dx , sx ]),
j∈ψ Ps d j Pj i , otherwise,
(4)
where ψ is the set of router i’s neighbors, which is located
in a packet’s possible routing paths from core s to core d,
immediately before router i. Further, Pj i is the probability
that router j forwards a packet to its neighbor router i with
destination core d, and it can be calculated as follows:
if iy (cid:1)∈ (jy , dy ] (or [dy , jy ))
or ix (cid:1)∈ (jx , dx ] (or [dx , jx )),
if ix = jx = dx , iy ∈ (jy , dy ] (or [dy , jy ))
or iy = jy = dy , ix ∈ (jx , dx ] (or [dx , jx )),
p, otherwise,
8>>><
>>>:
Pj i =
0,
1,
(5)
where p is a variable depending on congestion conditions of
the network. For a packet in router j whose destination is a
core in the right-up direction, the packet can be forwarded to
either router i (ix = jx , iy = jy + 1) or router k (kx = jx + 1,
is p, the probability to forward this packet to router k is 1−p.
ky = jy ). If the probability to forward this packet to router i
Since the DyXY routing algorithm chooses a path based on
each possible router’s stress value, the probability can be
estimated by p = Wk /(Wk + Wi ), where Wk (or Wi ) is the
mean waiting time of router k (or router i). Fortunately, Wk
equation Wk = λk /(µ− λk ). Combining these two equations
can be approximated using M/M/1 queue mean waiting time
with Equations (1), (2), (4) and (5), we can calculate the
value of p, and thus the mean arrival rate of each router
under DyXY routing can be calculated.
B. Mean response time
For static XY routing, the total traﬃc arrival process follows Poisson distribution, and hence a router can be modeled as a M/M/1 queue. The mean response time of router
i can be calculated using
E [T ri ] = 1/(µ − λi ).
(6)
where λi can be calculated using Equations (1), (2), and
(3). For the DyXY routing algorithm, since the traﬃc of
each router changes dynamically with network congestion
conditions, the real traﬃc distribution is not a Poisson distribution. The mean router response time in this case can
be estimated using a pair of upper bound and lower bound.
The real traﬃc distribution is an interrupted Poisson distribution, which is actually an optimization based on network congestion conditions, therefore, the real mean response time should be smaller than that calculated using the
mean arrival rate (λi ) and the Poisson distribution model.
Hence, the later one can be used as an upper bound of the
real mean response time. The lower bound can be estimated
by the mean response time with the minimum traﬃc at each
router. The minimum traﬃc of each router occurs when p
is set to 0 in Equation (5).
In this case, the total traﬃc
arrival process for each router follows Poisson distribution,
and the mean arrival rate of each router can be calculated
using Equations (1), (2), (4) and (5).
After calculating the mean response time of each router,
we can derive the mean waiting time of a packet in each
router by E [T ri ] − 1/µ. The mean buﬀer size required for
each router can be calculated using E [Wi ] × λi by Little’s
law [10]. The assignment of the buﬀer size to each channel of
a router can be determined based on the traﬃc load at each
diﬀerent direction of the router. The average mean response
time of all routers, E [T r ], can be calculated by
E [T r ] =
1
U × V
U ×VX
i=1
E [T ri ].
(7)
The performance of a router with ﬁnite buﬀer size α can
also be analyzed similarly with one more performance indicator (the blocking probability of packets) into consideration. The details are not presented here due to the space
limit.
3.2 System Modeling and Analysis
The performance of the entire system can be evaluated
by the average packet latency E [Latency ], which equals to
E [T r ] × N , where N is the average packet path length (i.e.,
average routing path length). E [T r ] can be derived directly
by Equation (7), and N depends on the speciﬁc communication pattern and routing algorithm employed.
With static XY routing, the length of a path traveled by
packets for a given pair of source and destination is a constant, which equals the shortest path length. For DyXY
routing, although the routing path is not static, it is always
a shortest path and hence the length is still the shortest
path length. Therefore, the average packet path length is
only aﬀected by the communication pattern. Without losing
the generality, we consider both uniform and non-uniform
communication patterns in this paper, and we choose Poisson distribution for the non-uniform communication pattern
since it is a widely used distribution for statistical analysis,
and can reﬂect real situations of a system. Once the communication pattern is ﬁxed, N can be easily derived. Due
to the space limit, the details are not presented here.
4. EXPERIMENTAL RESULTS
To evaluate the performance of the DyXY routing algorithm and verify the correctness of our analytical models,
we developed an event-driven simulator using C++ and designed three sets of experiments. In all these experiments,
the buﬀer depths were set to inﬁnite. To refelct the difference in packet lengths, the service time (not including
the waiting time) of each router for each packet was set to
a variable which follows a Poisson distribution (with mean
service rate µ equal to 1). More than 140,000 packets have
been injected into the network in each simulation, and the
NoC was warmed up for 20,000 packets before measuring
latencies.
from 3 × 3 to 9 × 9) with average packet injection rate λ
The ﬁrst set of experiments is based on NoCs (size varied
increasing from 0.1 to 0.3 under both the DyXY and static
XY routing algorithms. We have found that the simulation
results precisely match with the analytical results for both
routing algorithms. Further, the DyXY routing algorithm
achieves better balance in load distribution (for routers in
the center, edge, and corner) compared with the static XY
routing algorithm, and thus it can relieve the hot-spot problem when the network traﬃc is high. Results for a 3 × 3
NoC are shown in Figure 3. As we can see, the analytical
model for static XY routing can precisely evaluate the average mean response time for all routers. For DyXY routing,
the average mean response time for all routers can be eﬀectively estimated using the analytical lower bound and upper
bound.
Figure 3: Average mean response time for all routers in
3x3 NoC.
Figure 4: Average packet latency for 3x3 NoC with
Poisson distributed network communication pattern.
Since DyXY routing can balance the load distribution
among all routers much better than static XY routing, the
average mean response time for all routers is smaller with
DyXY routing than that with static XY routing. Further,
since the average packet path length is the same for both
routing algorithms, the average packet latency with DyXY
routing is also smaller than that with static XY routing.
To verify this, we conducted experiments by simulation for
both Poisson and uniform distribution network communicaFigure 5: Average packet latency for NoCs with uniform
network communication pattern.
tion patterns, and also compared the results with odd-even
routing. It can be observed that DyXY routing achieves the
best performance in average packet latency in both cases.
Results for one set of experiments are shown in Figure 4
(Poisson). The performance of a network with the Poisson
distribution based communication pattern is not sensitive
to the network size. However, the performance under the
uniform network communication pattern is aﬀected by the
network size. Therefore, our last set of experiments changed
the size of NoC from 3 × 3 to 9 × 9 with λ ﬁxed to 0.15. The
results are shown in Figure 5 (uniform). Obviously, it can
be seen that the system performs best with DyXY routing.
5. CONCLUSIONS
In this paper, we have proposed a novel routing algorithm,
namely dynamic XY (DyXY) routing, which provides adaptive routing based on congestion conditions in the proximity, and ensures deadlock-free and livelock-free routing at the
same time. The DyXY routing method can be supported by
a router architecture eﬃciently. Analytical models based on
queuing theory were developed for both static XY routing
and DyXY routing to evaluate their performance for twodimensional mesh NoC architectures. The accuracy of the
analytical models has been veriﬁed by extensive simulations.
It has been observed that DyXY routing can achieve better
performance than static XY routing and odd-even routing.
6. "
Fault-tolerant Routing for On-chip Network Without Using Virtual Channels.,"Thanks to its less design complexity, less power consumption and service time, to avoid using virtual channel has became a very attractive approach to building future reliable and massively parallel many-core systems. Furthermore, less area of the light-weight router decrease the probability of failure. To this end, by constructing an acyclic channel dependency graph that breaks all cycles and preserves connectivity of the network, we propose a new deadlock-free fault-tolerant adaptive routing without virtual channel. Extensive experiments of 8×8 2D-mesh network demonstrate 99.73% and 97.56% reliability under uniform random traffic when 10% and 20% of the links are failed.","Fault-tolerant Routing for On-chip Network Without Using
Virtual Channels
Pengju Ren, Qingxin Meng, Xiaowei Ren, and Nanning Zheng
Institute of Ar tiﬁcial Intelligence and Robotics, Xi’an Jiaotong University Shaanxi, 710049, China
(pengjuren, qxmeng, renxiaowei66)@gmail.com, nnzheng@mail.xjtu.edu.cn
downside of the components’ increased susceptibility to failure. Recently, Intel Corp announced that Broadwell’s 14nm
ABSTRACT
Thanks to its less design complexity, less power consumption
and service time, to avoid using virtual channel has became a
very attractive approach to building future reliable and massively parallel many-core systems. Furthermore, less area of
the light-weight router decrease the probability of failure.
To this end, by constructing an acyclic channel dependency
graph that breaks all cycles and preserves connectivity of
the network, we propose a new deadlock-free fault-tolerant
adaptive routing without virtual channel. Extensive experiments of 8x8 2D-mesh network demonstrate 99.73% and
97.56% reliability under uniform random traﬃc when 10%
and 20% of the links are failed.
Categories and Subject Descriptors
EDA2.1 [On-chip Communication and Networks-onchip]: On-chip communication network modeling and analysis
General Terms
Algortihm, Design, Performance
Keywords
Fault tolerance, Networks-on-Chip, Without virtual channels, Reliability
1.
INTRODUCTION
The ongoing miniaturization of semiconductor manufacturing technologies enable assembling hundreds to thousands of
processing cores on a single chip [3]. On the other hand, as
the barriers to SoC scaling have risen with each successive
node shrink, one of the most important obstacles is that,
transistors are approaching the limits of scaling, because
gate widths are nearing the molecular scale and the need
to ensure ever-higher levels of control over dopant distribution and voltage characteristics are slamming up against the
fundamental limits of physical laws [2]. It comes with the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers
or to redistribute to lists, requires prior speciﬁc permission and/or a fee.
Request permissions from Permissions@acm.org. DAC ’14, June 01 - 05
2014, San Francisco, CA, USA Copyright 2014 ACM 978-1-4503-27305/14/06$15.00. http://dx.doi.org/10.1145/2593069.2593141
ity of routing decisions and router microarchitecture. More
importantly, heavy-weighted router increases the power consumption and the probability of components failure. In order to solve these problems, we take the tight on-chip area
and power budget into consideration, and by exploring cyclebreaking and connectivity guaranteed(CBCG) algorithm to
the network’s CDG, then, we proposed an adaptive faulttolerant routing without using virtual channel. The remainder of the paper is organized as follows: the previous related works are introduced in Section 2, Section 3 presents
our methodology and fault-tolerant adaptive routing algorithm, experimental results and discussion are provided in
Section 5. Section 6 concludes this paper.
2. RELATED WORK AND CONTRIBUTION
Fault-tolerant scheme on on-chip network without virtual
channel has been studied. Glass and Ni [13] proposed onefault-tolerant routing derived from negative-ﬁrst routing algorithm without virtual channel with n-1 fault-tolerant degree for n dimensional meshes. However, the number of failure nodes that can be tolerated is small. Wu [21] introduced
a dimension-order based on Odd-Even turn model, but his
approach does not support failures of edge nodes.
In order to form rectangle or convex regions, block based
fault model [1] needs to deactivate healthy nodes, what is
more, the number of deactivation nodes is proportional to
the number of failure. Recently, Yusuke [11] proposed a
routing algorithm named Overlapped-Ring-Chain-Route to
reduce the number of deactivated nodes in the rectangle region, but still satisﬁed the computational capability of faultless nodes.
Prohibit certain turns is a convenient and powerful approach
to deadlock prevention. Glass and Ni [12] proposed 12 diﬀerent ways to break a link’s dependence and three unique turn
routing progress is indeﬁnitely inhibited. A network’s deadlocking properties can be depicted using DCDG by noting
the existence of cycles, in other words, if cycles are present in
the graph, the potential for deadlock exists. The widely used
deadlock avoidance approach is to disallow the appearance
of cycles in the network’s DCDG. According to Dally and
Seitz [8], a routing algorithm is deadlock free if the links can
be numbered and every message can only traverses links in a
strictly increasing (or decreasing) order. Therefore, to meet
our dual ob jectives of deadlock avoidance and preserving the
consistency, we need to ﬁnd a SETpturns of prohibited turns,
and the proper ordering of available nodes can be used for
routing, while guaranteeing at least one path could connect
every pair of nodes for a connected graph.
Firstly, we present an algorithm called CBCG as described
below, then give the analyses and properties of the CBCG
in the following:
Algorithm 1: Pseudocode of CBCG Algorithm
Input: An undirected graph ARCG(R, L)
Result: Sets of prohibited turns for each vertex
initialization;
SETncut is empty ;
/* Set of non-cut vertices */
SETlvertex is empty ;
/* Set of labeled vertices */
SETpturns is empty ; /* Set of prohibited turns */
SETaturns is empty ;
/* Set of al lowed turns */
label == 1;
while |R|! = 2 :
Calculate the SETncut of ARCG(R, L) ;
Select a non-cut vertex x from SETncut with
the minimal degree;
for turns (i, x, j ) in ARCG(R, L) :
Add (i, x, j ) in SETpturn
for turns (x, i, j ) and (i, j, x) in ARCG(R, L) :
Add (x, i, j ) and (i, j, x) in SETaturn
Remove vertex x from ARCG(R, L);
Label vertex x equal to label;
label + +
Label the remained two nodes |R| − 1 and |R|;
/*end while*/
return SETpturns and SETaturns
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
• Line 8: we can determine the set of cut vertices of
ARCG using Depth-First Search algorithm(DFS). Let
ARCGπ = (R, Lπ ) be the depth-ﬁrst tree of ARCG,
the root of ARCGπ is a cut vertex iﬀ it has at least
two children in ARCGπ ; if v is a non-root vertex of
ARCGπ , v is a cut vertex iﬀ v has a child u such that
there is no back edge from u or any descendant of u to
a proper ancestor of v . Detail implementation can be
found in Chapter 22 [6];
• Line 9-10:
It is note worthy that, there are possibly more than one node with the minimal degree in
SETncut , and diﬀerent ordering of the selected nodes
will produce diﬀerent SETpturns and SETaturns . The
inﬂuence will be discussed in Section 3.3;
• Line 11-15: note that at this stage of the algorithm
when node x is selected, all other undeleted nodes are
Figure 2: Example demostrating the CBCG algorithm. The fraction of prohibited turns is 20%.
unlabeled (they will be labeled later), therefore, turn
(i, x, j ) is prohibited iﬀ label(x) < label(i) and label(x)
< label(j ); but turn (x, i, j ) and (i, j, x) are allowed,
that label(x) < label(i) and label(x) < label(j ).
Example. Figure 2 demonstrates the operation of the CBCG
algorithm. The original connected and defective network is
shown in Figure 2(a), in which dashed line indicates fault
links (G, D), (D , E ) and (D , A) and fault router D .
It
takes 7 stages to complete the algorithm, because there are
8 available vertices in the graph. SETlvertex , SETpturns and
SETaturns are Ø after initialization. At the beginning, the
number of unlabeled vertices in ARCG is 8. Then, by using
DFS algorithm, we calculated the number of non-cut vertex
is 6 (the cut vertices B and H are heavily shaded). At this
step, the minimal degree non-cut vertex A is selected and
labeled ’1’, as shown in Fig 2(b), none of the turns is prohibited, since the degree of vertex A is 1; after that, vertex A is
removed and CBCG algorithm proceeds to stage two, then,
vertex G is selected and labeled with ’2’ shown in Figure 2;
In stage 3, vertex C is selected and labeled with ’3’, two
prohibited turns are denoted by dotted arcs in Figure 2(d),
i.e. (B , C, F ) and (F , C, B ). During the last stage, node I ,
F are labeled with ’7’, ’8’ and algorithm CBCG is ﬁnished.
All the prohibited turns and labeled vertices are shown in
Figure 2(e).
Two properties of the CBCG algorithm are listed here:
• CBCG is deadlock-free guaranteed.
• CBCG is connectivity guaranteed.
Proof of property 1: Assuming there is a cycle C in
ARCG, vertex i is with the minimum label label(i) in C .
Then there exists a turn (m, i, n), both label(m) and label(n)
are bigger than label(i), m, i, n ∈ C . Obviously, turn (m, i, n)
∈ SETpturns , which is forbidden and cycle C is non-existent.
Proof of property 2: The property can be expressed as:
for any two vertices x, y ∈ ARCG, there exists a path P =
(x, ..., y) that does not include turns from SETpturns . In the
ﬁrst stage, vertex L1 is selected, labeled with ’1’ and deleted
afterwards, therefore, SETlvertex has one element L1 , and
SETaturns has turns of the form (L1 , i, j ) and (i, j, L1 ). Because vertex L1 is the non-cut vertex in ARCG, according to
Figure 3: Examples of acyclic channel dependency graphs generated using CBCG algorithm. (a) The defect
network, fault node is 3; (b) CDG of the ﬂawless 3x3 2D mesh; (c) Acyclic DCDG the order of selected vertices is 0-6-2-1-74-8-5; (d) and (e) are other acyclic-DCDGs with the selected vertices orders are 6-0-2-1-8-7-5-4 and 6-0-7-8-4-5-1-2; (f ) is the
(c) with added dummy vertices and arcs.
line 9 in Algorithm 1, there still exists a path from any node
x to any node y if x, y ∈ ARCG \ SETlvertex ; Likewise,
if x = L1 or y = L1 , all the turns of the form (i, j, xory)
and (xory , i, j ) are allowed, So there exists a path from x,
x ∈ ARCG \ SETlvertex (or y , y ∈ ARCG \ SETlvertex ) to
y = L1 (or x = L1 ). Now, assume after stage n, vertex Ln is
a non-cut vertex and labeled with ’n’, there still exits a path
between any unlabeled vertex if x, y ∈ ARCG \ SETlvertex ;
Consider now x ∈ SETlvertex and is labeled with ’x’, while
y ∈ ARCG \ SETlvertex , or x, y ∈ SETlvertex are labeled
with ’x’ and ’y’, assuming ’x’<’y’, go back to stage X, at this
moment, y is unlabeled and all turns of the form (i, j, x) are
permitted; therefore, after prohibition of all turns (i, x, j ), x
and y are still connected.
3.3 Fault-tolerant Routing without Using Virtual Channel
After calculating all the prohibited turns using CBCG, we
can generate the acyclic DCDG by deleting edges ((i, j ), (j, k)),
for every forbidden turn (i, j, k) in SETpturns . Figure 3
demonstrates an example, Fig.3(a) is the same with Fig.2(a),
except for vertices are annotated in natural numbers rather
than alphabet due to convenience. According to previous
analysis, SETpturns = {(1, 2, 5), (5, 2, 1), (4, 7, 8), (8, 7, 4)}.
Fig.3(c) is generated by removing corresponding edges ((1, 2)
, (2, 5)), ((5, 2), (2, 1)), ((4, 7), (7, 8)) and ((8, 7), (7, 4)) from
original DCDG shown in Fig.3(b). The property 1 of CBCG
mentioned above can be veriﬁed by checking the existence
of cycles in the DCDG, also property 2 can be proved by
checking if there is a path between all node pairs.
As mentioned earlier, there might be multiple non-cut vertices with the minimal degree in every stage of CBCG,
and the diﬀerent ordering of selected vertices would constructs diﬀerent acyclic DCDGs as shown in Figure 3(c), (d)
and (e), and generates diﬀerent SETpturns correspondingly,
SETpturns={(1, 2, 5), (5, 2, 1), (5, 8, 7), (7, 8, 5)} and {(8, 7, 4),
(4, 7, 8), (1, 4, 5), (5, 4, 1)} for Figure 3(d) and (e). Obviously, it is worth noting that the qualities of network performance might be changed with diﬀerent underlying topology
of DCDGs.
Before making the routing decision for each ﬂow, we make
a temporary modiﬁcation to the acyclic DCDG by adding
dummy vertices Si and Di for ﬂow i with source Si and
destination Di , then we add directed arcs from Si to all vertices of the form (Si , x), and to Di from vertices of the form
(x, Di ). Dummy vertices and arcs are removed afterwards.
Figure 3(f ) shows an example of traﬃc ﬂow from vertex 1
to 7 (dummy verices 1 and 7, as well as added arcs are highlighted using red circle and blue dashed lines). Then, adaptive routing is applied, the route taken by a packet is determined dynamically according to available paths. For example, if node 1 wants to send a packet to node 7, it can choose
the path (1, (1, 4), (4, 7), 7) or (1, (1, 4), (4, 5), (5, 8), (8, 7), 7)
as is shown in Figure 3(f ).
4. EXTENSIONS FOR PROPOSED METHODOLOGY
We further present some potential extensions. It’s well known
that, messages transmitted by adaptive routing can reach
the destination in an out-of-order fashion due to the diﬀerence congestion level in each possible path. Therefore, for a
given application characteristic, oblivious routing can be explored based on the generated acyclic DCDG (ﬁgure 3(f )).
In-order message delivery and good load balance can be
Figure 4: Flit latency of diﬀerent fault-rate. Each
point is the average results of 10,000 simulations for 8x8
2D mesh. The results of 4x4 2D mesh exhibited the same
feature and we omit them here for brevity.
achieved in this way, as described in [15].
Routers are treaded either work well or fully out-of-service,
whereas a physical defect inside an input buﬀer or switch is
likely to aﬀect only part of the router’s functionality [16].
We can partially deactivate the defective router in order
to alleviate its negative impact on network performance by
leveraging the knowledge of detailed fault status. Therefore,
more ﬁne-grained fault diagnosis models can be explored to
make full utilization of the remaining functionality.
5. EXPERIMENTAL RESULTS
We used hornet, a highly conﬁgurable, cycle-accurate onchip network simulator [20] to evaluate the routing algorithm. We implemented 4x4 and 8x8 2D- mesh and torus
with diﬀerent number of broken links. The positions of the
fault links and nodes were randomly conﬁgured, and assuming the ratio of fault nodes and links is nearly 1:2 as in [1].
In order to get the performance results independent of the
relative distribution of the faults and the ordering of labeled
vertices, 10,000 simulations are performed for each fault-rate
case, with 200,000 warmup cycles and total 1,200,000 analyzed cycles, the traﬃc has a uniform random distribution
with packet length is 8 ﬂits.
According to our observation, no deadlock has appeared and
the disconnected network only presented when one of the
fault links (routers) happened to be the bridge (cut vertex )
of the network and created disjoint subnetworks. Our results
also exempliﬁed the properties of CBCG algorithm.
5.1 Performance in the Presence of Faults
Taking the ﬂawless network as the baseline for our discussion, we ﬁrstly measure network performance by means of
the saturation throughput and ﬂits latency obtained in the
presence of faults under uniform traﬃc. Figure 4 shows the
results of ﬂits latency, as can be seen, ﬂit latency is signiﬁcantly increased when the number of failure links rises. At
the packet injected rates below saturation, average ﬂit latency is increased by 8.24% when 5% faults is present, the
latency is increased by 131.36% when the fault rate is 40%.
Figure 5: Throughput of diﬀerent fault-rate.
Figure 6: Reliability with increasing fault-rate.
Figure 5 shows the network saturation throughput, the achieved
throughput is decreased by 5.15% when 5% faults is present
in the network, but it dropped to 46.5% when fault rate is
40%. It can be noticed that, saturation throughput drops
rapidly as the injected faults increases. This behavior is
caused by the fewer available links and routers for communication. The fact that the average distance of paths increases when routes around faults use non-minimal routing,
also contributes to this behavior. According to our observation, for 4x4 network, the ordering of selected labeled nodes
has signiﬁcant impact on network performance, however, for
8x8 network, the performance diﬀerence is diminished.
5.2 Reliability Analysis
Then, we evaluated the relationship between fault-rate and
the reliability of the network. As shown in Figure 6, 8x8
2D mesh network exhibit a reliability over 99.73% when 11
links are broken (10% out of 112 links), whereas for torus,
over 99.34% when 19 links are broken (15% out of 128 links).
And reliability curve drops rapidly as the number of faults
in the network increases. As can be seen, for 30% fault-rate,
roughly 90% of the fault cases are not tolerated. Smaller
4x4 networks has 97.74% for 2D mesh, and 99.49% for torus,
when fault-rate is 40%. Because with larger networks, the
probability of disjoint subgraphs increases when the number of faults rises, and node pairs belonging to diﬀerent subgraphs can no longer send or receive messages.
In these
cases, faults cannot be tolerated.
6. CONCLUSION
Resiliency to the decreasing transistor reliability will be a
necessary condition in future technology nodes. Moreover,
OCNs have a tight area and power budget, necessitating
simple router structures. To this end, we have presented
an adaptive fault-tolerant routing without virtual channel
for connected network. Deadlock freedom and preservation
of connectivity are achieved by constructing a connected
acyclic channel dependency graph. Our method loses eﬃcacy only when there are disjoint subgraphs caused by faults.
The main drawback of the proposed algorithm is that, the
forbidden turns are computed oﬄine and they need to be updated if new faults are detected and reﬂected in a modiﬁed
network topology. The network performance and reliability
can be improved when failures are pinpointed in a single
switch link of crossbar or in an input buﬀer when applying ﬁne-granularity diagnosis. Further more, adding virtual
channels is not required in our routing algorithm, but it can
be used to improve performance at the cost of increased
complexity of the router implementation.
7. ACKNOWLEDGMENTS
This research is partially funded by NSFC grant No.610303036,
China Postdoctoral Science Foundation No.2012M521777,
Specialized Research Fund for the Doctoral Program of Higher
Education of China No.20130201120024, Natural Science Basic Research Plan in Shaanxi Province of China No.2013JQ8029
and the Fundamental Research Funds for the Central Universities.
8. "
VIX - Virtual Input Crossbar for Efficient Switch Allocation.,"Separable allocators in on-chip routers perform switch allocation in two stages that often make uncoordinated decisions resulting in sub-optimal switch allocation. We propose Virtual Input Crossbars (VIX), where more than one virtual channel (VC) of an input port is connected to the crossbar. VIX improves switch allocation by allowing more than one input VC of an input port to transmit flits in the same cycle. Also, more input VCs can participate in the output arbitration, reducing the chances of uncoordinated decisions. VIX improves network throughput by more than 15% for the topologies studied without affecting the router critical path.","VIX: Virtual Input Crossbar for Efﬁcient Switch Allocation
Supriya Rao, Supreet Jeloka, Reetuparna Das, David Blaauw, Ronald Dreslinski, and Trevor Mudge
Depar tment of Computer Science and Engineering, University of Michigan
{supriyar, sjeloka, reetudas, blaauw, rdreslin, tnm}@umich.edu
one virtual channel for each input port. The winners compete in the
second phase, where the output arbiters choose one input port for each
output port. This complexity-effective design is widely assumed in
NoCs as it can meet the tight timing constraints of an on-chip router.
However, a separable allocator suffers from two problems that lead
to sub-optimal switch allocation. We consider an optimal switch allocator to be one that guarantees that an output port of a router is utilized
in a cycle as long as there is at least one input virtual channel requesting for it.
The ﬁrst problem is the sub-optimal matching problem. The input
arbiters, while selecting an input VC for each input port, fail to coordinate between each other. As a result, often two input ports may
end up choosing input VCs that request the same output port, when
that output port can serve only one input in a cycle. Had one of those
two input ports selected a VC with a different output port request, both
input ports could have been proﬁtably allocated to different output
ports.
Past work on improving switch allocation has focused on solving
the above sub-optimal matching problem. They employ iterative algorithms to allocate as many output ports as possible in a cycle [14, 21].
Unfortunately, these iterative solutions come at the cost of increased
delay and area, and may not be able to operate within the tight timing
constraints of the router.
The second problem that leads to sub-optimal switch allocation is
the input port constraint that dictates that only one VC in an input port
can transmit ﬂits in a cycle. As a result, when the only requestors for
two output ports are from one input port, it is guaranteed that one of
those outport ports would be left idle. Thus, even if we are to implement an optimal matching solution [8], we could still have sub-optimal
switch allocation, where one or more outports ports are left idle in
spite of some input VCs requesting them.
In this paper, we make a key observation that the root cause of the
above two main switch allocation problems in NoC routers lies in unnecessarily restricting VCs of an input port from access to the crossbar
switch. Instead of over-burdening the already complex switch allocation stage to improve its matching efﬁciency [14, 21], we propose the
Virtual Input Crossbar (VIX) that connects to more than one VC per
input port. While this increases the number of inputs to the crossbar,
it is feasible without increasing the router’s cycle time, as the crossbar
is not in a router’s critical path.
VIX addresses both problems in switch allocation we discussed earlier. As VIX connects to more than one VC per input port, it directly
addresses the input port constraint problem. VIX also improves the
matching efﬁciency. The probability that an output request from an
input VC gets killed in the ﬁrst phase of separable allocation reduces
as we expose more input VCs to the second phase. In one extreme, if
we connect all the input VCs of an input port to the VIX, we can not
only achieve optimal matching but also guarantee optimal switch allocation. In practice, we limit VIX to two virtual inputs per input port
in order to bound the complexity of crossbar within a router’s timing
constraints.
We study VIX and the different switch allocation techniques through
detailed cycle accurate performance simulations, circuit-level delay
analysis of routers and network power models. We model a 64 node
network-on-chip. We study statistical trafﬁc, and also 35 different
benchmarks with multiprogrammed workloads. We ﬁnd that VIX improves the effectiveness of separable allocators signiﬁcantly, making
them comparable or better than more expensive and slower allocators.
Our evaluations show that VIX based allocation improves network
throughput by 16% for a mesh topology. VIX improves network
throughput by 15% when compared to wavefront switch allocation,
while having 39% lower delay. We also study the applicability of
VIX to higher radix topologies. VIX improves network throughput by
ABSTRACT
Separable allocators in on-chip routers perform switch allocation in
two stages that often make uncoordinated decisions resulting in suboptimal switch allocation. We propose Virtual Input Crossbars (VIX),
where more than one virtual channel (VC) of an input port is connected to the crossbar. VIX improves switch allocation by allowing
more than one input VC of an input port to transmit ﬂits in the same
cycle. Also, more input VCs can participate in the output arbitration,
reducing the chances of uncoordinated decisions. VIX improves network throughput by more than 15% for the topologies studied without
affecting the router critical path.
Categories and Subject Descriptors
C.1.2 [PROCESSOR ARCHITECTURES]: Multiprocessors—Interconnection Architectures
General Terms
Design,Algorithms
Keywords
network-on-chip, throughput, switch-allocation
1.
INTRODUCTION
Processors with 10s of cores are already available commercially. A
packet-based on-chip network with a regular topology is an attractive
solution for connecting these large number of cores and the associated on-chip structures, as they are scalable with predictable electrical
properties [7]. An efﬁcient on-chip network design could have a signiﬁcant impact on a many-core processor’s performance and power
consumption.
Routers in an on-chip network play a central role in determining its
efﬁciency. A router’s crossbar (switch) connects a set of input ports to
its output ports. A key functionality of a router is to decide every cycle
which of its output ports gets allocated to which of its input ports. This
functionality is performed by the switch allocation unit of the router.
Thus, the switch allocator primarily determines the utilization of a
router’s output links and therefore has a signiﬁcant impact on overall
network performance.
Optimal switch allocation for an on-chip network without virtual
channels is easier to realize: each output port has an output arbiter
which chooses from a set of input ports that are requesting for it. However, optimal switch allocation for an on-chip network with virtual
channels (VCs) is considered to be a challenging problem, and there
has been signiﬁcant research to address it [14, 1, 21, 4, 15, 5].
The challenge in networks with VCs is that each input port contains
one or more input virtual channels that could each request for a different output port. However, in conventional router designs, since there is
only one input to the crossbar per input port, only one virtual channel
in an input port can transmit a packet in a cycle. To select an input VC
for each input port, conventional router designs employ an additional
set of input arbiters.
The above two phase switch allocator is referred to as the inputﬁrst separable allocator. In the ﬁrst phase, the input arbiters choose
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee. Request permissions
from Permissions@acm.org.
DAC 2014, June 1-5, 2014, San Francisco, California, USA.
Copyright is held by the owner/authors. Publication rights licensed to ACM.
ACM 978-1-4503-2730-5/14/06 ...$15.00.
15% for concentrated mesh topology and by 17% for ﬂattened butterﬂy topology. Higher performance from VIX can be used to reduce
network buffers. VIX can reduce the network buffers by 33% while
still improving network throughput by 10%. For application workloads, VIX improves system performance by 5% on average.
2. VIRTUAL INPUT CROSSBARS
Figure 1: Router Architecture.
2.1 Description
The routers are responsible for communicating packets between the
source and destination nodes connected by the on-chip network. Each
packet consists of several smaller units of ﬂow control called ﬂits. Every ﬂit travels hop by hop from one router to another unitl it reaches its
destination. A generic router architecture is shown in Figure 1. This
router has P input ports and P output ports. The Route Computation
unit, RC determines the output port for each packet. The Virtual channel Allocation (VA) stage decides the winner amongst all the packets
requesting access to the same output VC in the downstream router.
The Switch Allocation (SA) unit decides the winning ﬂit for each input port and each output port that can proceed to crossbar traversal.
Typically crossbars in routers have as many inputs as the number of
input ports. A multiplexer is used to connect one virtual channel from
v virtual channels in an input port to the corresponding input in the
crossbar. We can potentially increase the inputs to the crossbar to
provide more virtual inputs. For instance, the crossbar can have 2P
inputs. Thus each input port has 2 virtual inputs corresponding to it.
The virtual channels are partitioned into two virtual input sets. A multiplexer can be used to connect one virtual channel from v/2 virtual
channels to the corresponding virtual input of the crossbar as shown
in Figure 2.
2.2 Switch Allocation with VIX
Our goal is to provide high matching efﬁciency with complexity
comparable to separable allocators. VIX provide opportunities to expose more conﬂict free inputs during switch allocation and hence improve matching efﬁciency of separable allocators. The virtual inputs
provide two advantages. First, now more than one virtual channel can
transmit ﬂits from one input port to different output ports. Figure 4
illustrates this advantage for a 5-port mesh router with 4 virtual channels. There are two packets in the W est port of the router in V C 0 and
V C 2. The packet in V C 0 is requesting for Local port and the packet
in V C 2 is requesting for E ast port. In absence of virtual inputs only
E ast output port is allocated and one ﬂit is transferred. With virtual
inputs, its possible to transfer ﬂits from both V C 0 and V C 2 in the
same cycle. Thus both E ast and Local output ports are allocated and
two ﬂits are transferred.
The second advantage of virtual inputs is better matching efﬁciency
because more input requests are exposed for output arbitration. This
reduces the probability that different input arbiters pick the same output. Figure 5 illustrates this advantage. In absence of virtual inputs
(Figure 5 (a)) both W est and S outh input port arbiters pick E ast
output port. E ast output port can pick only one, which is W est, resulting in only one ﬂit transfer. In presence of virtual inputs (Figure 5
(b)), the input arbiters for each virtual input in S outh pick different
outputs, N orth and E ast. This leads to a better matching, resulting
in three ﬂit transfers.
Figure 2: Router Architecture with VIX
Figure 3: Switch Allocation
2.3 Virtual Channel Assignment with VIX
The Virtual Channel Allocation stage decides the winner among all
the packets requesting access to the same output VC at the downstream
router. Before Virtual Channel Allocation every input VC is assigned
an output VC. Typically the output VC with maximum number of free
ﬂit-buffers is assigned. VIX can be optimized by intelligent assignment of output virtual channels to input virtual channels. This is because virtual channels are now divided among virtual inputs to the
crossbar creating sub-groups of virtual channels, each sub-group connected to a speciﬁc virtual input to the crossbar. For example, for the
router shown in Figure 2, there are two sub-groups of virtual channels
at each port, and each sub-group has v/2 virtual channels. Packets
can be assigned to different sub-group of output virtual channels (and
hence different virtual inputs in the downstream router) based on the
direction of the output port at the downstream router. By using this dimension information, there will be fewer output port conﬂicts because
requests to different output ports will likely be generated from virtual
channels belonging to different virtual inputs. The input requests are
also load balanced among the sub-groups of virtual channels to make
sure that the different virtual inputs connected to the crossbar always
have requests. This exposes majority of the input requests at the crossbar and can help in better switch allocation. Load balancing the input
requests in combination with dimension information in the VIX architecture will help improve performance in adversarial trafﬁc patterns.
2.4
Implementation
Supporting VIX requires modest changes to the router architecture.
The datapath of the router needs to support extra connections between
input buffers and crossbar. The extra connections can be realized by
increasing the number of output multiplexors which route ﬂits from
virtual channels to the crossbars. The extra connections for a router
with two virtual inputs per input physical port, are shown in Figure 2.
The switch allocation unit also needs modiﬁcation to support VIX.
Figure 3 (a) shows the organization of switch allocation unit with
input-ﬁrst separable allocation for baseline router without VIX, with
v virtual channels, and P physical input and output ports. The input
arbitration is done in parallel by P input arbiters each of size v : 1.
The output arbitration is done in parallel by P output arbiters, each of
size P : 1. Figure 3 (b) shows the organization of switch allocation
Figure 4: Virtual input crossbars allow more than one ﬂit to be
transferred from each input port per cycle.
Figure 5: Virtual input crossbars expose more conﬂict free input
requests for output arbitration.
(a) 5-stage pipeline
Figure 6: Router pipeline stages
(b) 3-stage pipeline
unit with VIX. To realize two virtual inputs per physical input port, the
number of input arbiters are doubled. However each input arbiter is
now a v/2 : 1 arbiter and hence its complexity is reduced by half. The
number of output arbiter remains same as baseline, but the complexity
of each output arbiter is now doubled to 2P : 1.
The number of inputs of the crossbar are doubled to support a VIX
with two virtual inputs per input physical port. The size of crossbar is
increased from a P × P crossbar to a 2P × P crossbar as shown in
Figure 2. This can potentially effect the cycle time (and frequency) of
the router. In order to examine the impact of using larger crossbars on
router’s cycle time, we synthesized the various components of router’s
pipeline. The different stages of a router’s pipeline are shown in Figure 6. The Figure 6 (a) illustrates the stages of a 5-stage pipeline and
Figure 6 (b) illustrates the stages of an optimize 3-stage pipeline with
speculative switch allocation [19] and look ahead routing [9]. Several
studies have shown that Virtual channel Allocation (VA) or Switch Allocation (SA) is on the critical path of the router [19, 17, 18, 10, 4].
Thus we examine the delays of VA stage, SA stage, and crossbars to
study the impact of using larger crossbars for supporting VIX.
We synthesize the VA and SA stage using open-source NoC router
RTL code [4, 3]. Synthesis was done with Synopsis Design Compiler
using a commercial 45nm SOI technology library (1.0V, 25◦C and
topographical mode). Since crossbars are wire dominated structures,
we use SPICE modeling for more accurate estimation of crossbar delay. We model 128 bit matrix crossbars whose inputs are driven by a
ﬂip-ﬂop followed by a driving buffer. Each output gets latched into a
register. Tri-state buffers connect the input wires to the output wires,
with a one-hot enable signal. The enable signal is assumed to be driven
from the SA pipeline stage. Metal 3 and Metal 4 are used to model the
crossbar wires. Appropriate length wire models are used, assuming
2x spacing between wires, to avoid coupling. The driving buffer and
tri-state gates are optimally sized to reduce delay.
Design
Mesh
Mesh with VIX
CMesh
CMesh with VIX
FBﬂy
FBﬂy with VIX
Radix Xbar size VA Delay
5
5 x 5
300 ps
5
10 x 5
300 ps
8
8 x 8
340 ps
8
16 x 8
340 ps
10
10 x 10
360 ps
10
20 x 10
360 ps
SA Delay Xbar Delay
280 ps
167 ps
290 ps
205 ps
315 ps
205 ps
330 ps
289 ps
340 ps
238 ps
345 ps
359 ps
Table 1: Router pipeline stage delays.
The delays for analyzed components of the router are shown in Table 1. To study the wider applicability of VIX, we synthesized the
components for routers of three different topologies. The topologies
considered are mesh [10, 22], concentrated mesh [2] and ﬂattened butterﬂy [11]. Each topology requires a router with different radix1 , radix
5 for mesh, radix 8 for concentrated mesh (CMesh), and radix 10 for
ﬂattened butterﬂy (FBﬂy). The different topologies also require crossbars of different sizes as noted in Table 1. We observe that crossbar stage is not in the critical path of router’s pipeline. Our analysis
demonstrates that there is sufﬁcient slack in crossbar stage to support
larger crossbars required for VIX architecture. For a mesh router, the
delay of crossbar stage increases by 22%, while still remaining within
70% of the router’s cycle time. The slack in crossbar stage reduces for
higher radix ﬂattened butterﬂy topology, however the allocation stage
delays also increase proportionally for higher radix. Supporting VIX
for ﬂattened butterﬂy increases the delay of crossbar stage by 50%.
However the crossbar delay continues to be lower than VA delay, thus
VIX can be implemented for ﬂattened butterﬂy without increasing the
cycle time of router. But this also indicates that VIX architecture may
not scale to very high radices unless innovative high-radix switch architectures are utilized [20].
In summary, we conclude that we can implement VIX architecture
without degrading the frequency of baseline router architecture for the
three topologies considered.
3. METHODOLOGY
We use a cycle-accurate network-on-chip simulator for our analysis.
All routers use the three stage pipeline shown in Figure 6 (b). We
model deterministic dimension order routing algorithm, ﬁnite input
buffering, wormhole switching, and virtual-channel ﬂow control. For
each router, we assume a buffering of 6 virtual channels per port and
a buffer depth of 5 ﬂits per virtual channel. The datapath width of the
router is constant across all topologies and is equal to 128 bits.
We evaluate the different network conﬁgurations with uniform random statistical trafﬁc and application benchmarks. For applications,
we use a trace-driven, cycle-accurate manycore simulator with the
above network model integrated with core, cache and memory controller models. Table 2 provides the conﬁguration details of the components simulated.
We use a set of multiprogrammed application workloads comprising scientiﬁc, commercial, and desktop applications. We study 35
benchmarks, including SPEC CPU2006 benchmarks, and four commercial workloads traces (sap, tpcw,sjbb, sjas). The details of
how each multiprogrammed workload mix is derived from the different benchmarks are discussed in Section 4.7.
1Radix is equal to number of physical input/output ports of a router.
For network energy, each component of router such as, links, buffers
and switch is modelled through SPICE. Our models include energy
spent due to clocking and leakage energy. The activity factor of links,
buffers and switches were collected from cycle-accurate simulations
and integrated with component models to determine the overall network energy consumption.
Cores
L1 Caches
L2 Caches
Main Memory
64 cores , 2-way out-of-order, 2 GHz frequency
32 KB per-core, private, 4-way set associative,
64B blocks, 2-cycle latency, split I/D caches,
32 MSHRs
64 banks, 256KB per bank, shared, 16-way
set associative, 64B block size, 6-cycle latency,
32 MSHRs
8 on-chip memory controllers,
4 DDR channels each @16GB/s,
up to 16 outstanding requests per core,
80ns access latency
Table 2: Processor conﬁguration
4. EVALUATION
4.1 Network Conﬁgurations Studied
We evaluate the network performance for four different switch allocation schemes - Separable input-ﬁrst (IF), Wavefront (WF), Augmented Path (AP) and VIX. In the IF scheme, the input arbiters
choose one request per input port. Next, the output arbiters choose
one input request for each output port. This can lead to uncoordinated
decisions between the two arbiters. The inefﬁciency of separable allocators can be minimized by more complex iterative allocators [14]
or wavefront (WF) [21]. The WF allocator works on the principle of
iteratively allocating all conﬂict free input-output pairs along the diagonals. By exposing more conﬂict free inputs to the outputs, WF allocation achieves better allocation efﬁciency than separable allocators.
Better allocation comes at a cost of increased complexity and higher
cycle time [4, 15]. The WF allocator has 39% higher cycle time than
a separable allocator as shown in Table 3. However, the WF allocator
does not necessarily achieve maximum or ideal matching. Maximum
matching can achieved by the AP algorithm [8]. The complexity of
AP algorithms limit their applicability to NoC routers [4]. In the experiments conducted, all VIX conﬁgurations have two virtual inputs
per input port. All our experiments are performed for 64 node networks. For each of these conﬁgurations, we study the average packet
latency and average network throughput for each of the four switch
allocation techniques. To study the allocation techniques independent
of their implementation limitations, we assume equal cycle time for all
switch allocation techniques. The packet latency is reported in cycles
and network throughput in packets/cycle/node or f lits/cycle. The
routers have a 128 bit datapath width, and support 6 virtual channels
per input port. We evaluate the different network conﬁgurations with
uniform random statistical trafﬁc with a packet size of 512 bits (i.e. 4
ﬂits).
Separable Wavefront Augmented Path
280 ps
390 ps
Infeasible
Delay
Table 3: Delay of different switch allocation schemes.
4.2 Switch Allocation Efﬁciency
In this section, we study the switch allocation efﬁciency for a single
router. This study allows us to analyze the different switch allocation techniques in isolation, without second order effects of network
topology and interaction of a router with other routers. Packets are
injected at maximum injection rate into each port of the router. Figure 7 illustrates the throughput achieved by the router for different
radices and different allocation techniques. A single router of Radix5 can achieve at best 5 ﬂits/cycle and Radix-10 can achieve at best
10 ﬂits/cycle. We observe that the trends across different radix routers
are similar. The AP algorithm can provide above 30% higher throughput than separable IF for all radix conﬁgurations, while VIX provides
above 25% throughput improvement over IF for all radices evaluated.
Both AP and VIX achieve efﬁciency very close to ideal switch allocation. Ideal allocation is possible by allowing 6 virtual inputs per input
Figure 7: Switch allocation efﬁciency for single router.
port. Interestingly, some of these trends are reversed for network level
performance, as we shall see in the next section.
4.3 Network-Level Performance
In this section, we evaluate the network performance for a mesh
conﬁguration using a 64 node network.
Figure 8 shows the average packet latency and average network
throughput for mesh topology. We observe that VIX only provides
beneﬁts at high injection rates. Since, at low network load all the allocation schemes have nearly identical performance due to fewer output
port conﬂicts. At higher injection rates, we observe that VIX improves
network throughput by 16.2% and average packet latency by 36% over
the IF allocation method. VIX also outperforms the AP scheme. VIX
improves network throughput by 15.9% over the AP allocation.
The AP scheme improves the network throughput by a mere 0.3%
over IF for mesh topology. This is because the AP algorithm follows
a greedy approach, making optimal decisions locally while making
sub-optimal decisions at the network level, leading to high levels of
unfairness in the network. The extent of unfairness in the network is
proportional to the average number of hops in the network. To illustrate this fact, we evaluated the maximum/minimum throughput values of all the nodes in the network. Ideally, the maximum/minimum
throughput should be close to 1, as all network nodes are injecting
at equal injection rate. Figure 9 shows that this ratio is 6.4 for Mesh
using an AP algorithm, while it improves to 1.99 by using VIX.
Overall at the network level, VIX achieves higher network throughput and also has signiﬁcantly lower average packet latency at high
load. In addition, VIX also achieves maximum fairness at the network
level when compared to the other allocation schemes studied.
Figure 10: Network throughput achieved by Packet Chaining,
VIX and other allocation techniques.
4.4 Comparison to Packet Chaining
Recent work proposes the concept of Packet Chaining [15] for improving the efﬁciency of separable allocators. The key idea is to inherit
the allocation decisions made by the separable allocator in previous
cycles and then decide the new allocations for current cycle. Thus,
packets which want to communicate between same output-input pairs
are chained. Packet chaining provides higher beneﬁts for short packets
or single-ﬂit packets. By using past history and spreading the allocation over multiple cycles, Packet Chaining (PC) achieves some of the
properties of iterative allocators.
In our view, PC works by elimination. By preserving the allocations
from previous cycles, it eliminates many requests from the request matrix, thereby reducing the chances of uncoordinated decisions between
the input and output arbiters. Input ports which preserve connections
Figure 8: Network throughput (a) and packet latency (b) for a mesh topology.
Figure 9: Fairness for a mesh topology.
close to ideal VIX for mesh and concentrated mesh topologies. The
switch allocator has more opportunities for improving throughput in
ﬂattened butterﬂy because of its higher radix. Thus the gap between
1:2 VIX and ideal VIX is more pronounced for ﬂattened butterﬂy with
6 VCs.
We also observe that 1:2 VIX with 4 VCs achieves more than 10%
throughput improvement over a 6VC conﬁguration without VIX. Thus
VIX can be leveraged to reduce number of buffers, and save router
area/power. VIX can be used to reduce network buffers by 33% (from
6 VCs to 4 VCs) while still improving the network throughput by 10%.
4.7 Application-Level Performance
In this section, we analyze the effect of different allocation schemes
with real application workloads. We evaluate the multi-programmed
workloads Mix1 - Mix8 for our experimental analysis. Each workload consists of 6 unique applications. These applications are chosen
at random from a suite of 35 benchmarks. Table 4 lists the multiprogrammed workloads used for the study and the speedup observed
over the baseline scheme. We observe an average increase of 5% and
a maximum of 7% in system performance using VIX over the base
(IF) allocation scheme. VIX also shows an increase of upto 3.2% in
performance over the AP algorithm.
5. RELATED WORK
Several interesting switch allocation schemes have been proposed
for off-chip networks [1, 16]. We focus on switch allocation techniques which have been proposed for on-chip networks. We have already compared our approach extensively, both qualitatively and quantitatively, to existing switch allocation techniques such as separable
allocators, wavefront allocators [21], almost ideal matching based on
augmented path algorithms [8] and packet chaining [15].
Kumar et al. [13] proposed SPAROFLO switch allocation to improve efﬁciency of separable allocators. In SPAROFLO, the number
of input requests presented to output arbiters is varied dynamically
with network load. Similar to VIX, more than one request per input
port is selected for output arbitration at low and medium load. Since
there are no virtual inputs at the crossbar, only one request per input port can be granted. Thus, conﬂicts must be detected after output
arbitration. This is done by assigning a priority to the virtual channels during input arbitration. These conﬂicts limit the efﬁciency of
SPAROFLO when compared to VIX. In addition to varying dynamically the number of requests per input port, SPAROFLO prioritizes
older requests during output arbitration. Priority is also given to ﬂits
of same packet, so that all ﬂits of a packet stay together. These prioritization optimizations can be easily integrated with VIX.
Becker et al. [4] study the design space of virtual channel allocators
and switch allocators for NoCs. They also propose an optimization
for speculative switch allocation. In speculative switch allocation, the
winners from the non-speculative requests must be prioritized over
speculative requests. By pessimistically masking the speculative requests, their design removes the prioritization circuit from the critical
path of switch allocation. This optimization is orthogonal to our proposed techniques and is applicable to routers with VIX.
Recently Chang et al. [5] proposed TS-router. By predicting future requests to arrive at a router, TS-router maximizes the matching
of separable allocators across cycles. While this is an beneﬁcial approach, it does not solve the problem uncoordinated decisions between
the input and output arbitration stages. We believe both TS-router and
Figure 11: Network energy per bit.
from the previous cycle do not participate in input arbitration. Thus the
chances that many input arbiters pick the same output port is reduced.
On the other hand, VIX works by exposing more non-conﬂicting requests to output arbiters, and thereby reducing the chances of uncoordinated decisions between input and output arbiters. Both allocation
methods solve the problem with baseline separable allocators by taking very different approaches. In addition, VIX has the advantage of
allowing more than one virtual channel per input port to transmit ﬂits
in a given cycle.
To compare the two techniques quantitatively, we replicate the router
pipeline used by Michelogiannakis et al.
for Packet Chaining [15]
and simulate the S ameI nput, anyV C scheme. We evaluate a 8x8
mesh with uniform random trafﬁc, and single-ﬂit packets. Figure 10
shows the network throughput achieved by the different allocation
methods at maximum injection rate. We observe that PC improves
network throughout by 9%, whereas VIX improves network throughput by 16%. We conclude that for separable allocators, an approach
which exposes more non-conﬂicting input requests to output arbiters
provides better allocation than an approach which eliminates input requests and preserves prior connections.
4.5 Energy Consumption
Figure 11 shows the energy per bit for the mesh topology at an
injection rate of 0.1 packets/cycle/node. In VIX, the switch energy
increases due to an increase in the crossbar size. We observe that
the total network energy per bit increses by 4% for a Mesh network
due to a larger crossbar size. Note, VIX improves both network-level
throughput and application-level throughput signiﬁcantly. Tasks will
complete execution earlier, leaving the network and processor’s other
components idle for longer periods. Hence, leakage energy of network and processor’s others components can be saved. Also, energy
dissipated by clocking can be saved during the idle periods.
4.6
Increasing Virtual Inputs
In this section we study the impact of increasing virtual inputs.
Figure 12 shows network throughput for three conﬁgurations. First,
baseline which has no virtual inputs (no VIX). Second, VIX with two
virtual inputs per input port (1:2 VIX) and lastly, ideal VIX conﬁguration which has equal number of virtual inputs and virtual channels,
per input port. We study the impact of VIX for routers with both 4
VCs and 6 VCs per input port. The realistic 1:2 VIX provides significant throughput improvement for both 4 VCs (21% on average) and
6 VCs (16% on average). For routers with 4 VCs, we ﬁnd that VIX
with two virtual inputs (1:2 VIX) achieves nearly equal performance
as ideal VIX for all topologies. Thus, two virtual inputs sufﬁce for
routers with 4 VCs per input port. "
SHiFA - System-Level Hierarchy in Run-Time Fault-Aware Management of Many-Core Systems.,"A system-level approach to fault-aware resource management of many-core systems is proposed. The proposed approach, called SHiFA, is able to tolerate run-time faults at system level without any hardware overhead. In contrast to the existing system-level methods, network resources are also considered to be potentially faulty. Accordingly, applications are mapped onto healthy nodes of the system at run-time such that their interaction will not require the use of faulty elements. By utilizing the simple routing approach, results show 100% utilizability of PEs and 99.41% of successful mapping when up to 8 links are broken. SHiFA design is based on distributed operating systems, such that it is kept scalable for future many-core systems. A significant improvement in scalability properties is observed compared to the state-of-the-art distributed approaches.","SHiFA: System-Level Hierarchy in Run-Time Fault-Aware
Management of Many-Core Systems
Mohammad Fattah1 ,Maurizio Palesi2 , Pasi Liljeberg1 , Juha Plosila1 , Hannu Tenhunen1
1University of Turku, Turku, Finland.
2University of Enna, Kore, Italy.
{mofana, pakrli, juplos, hanten}@utu.ﬁ maurizio.palesi@unikore.it
ABSTRACT
A system-level approach to fault-aware resource management of
many-core systems is proposed. The proposed approach, called
SHiFA, is able to tolerate run-time faults at system level without
any hardware overhead.
In contrast to the existing system-level
methods, network resources are also considered to be potentially
faulty. Accordingly, applications are mapped onto healthy nodes of
the system at run-time such that their interaction will not require the
use of faulty elements. By utilizing the simple routing approach,
results show 100% utilizability of PEs and 99.41% of successful
mapping when up to 8 links are broken. SHiFA design is based
on distributed operating systems, such that it is kept scalable for
future many-core systems. A signiﬁcant improvement in scalability
properties is observed compared to the state-of-the-art distributed
approaches.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Fault tolerance
General Terms
Performance, Reliability
Keywords
application mapping, system-level design, hierarchical management
1.
INTRODUCTION
According to the International Technology Roadmap for Semiconductors [1], by 2020 Multi-Processor Systems-on-Chip (MPSoCs)
will integrate hundreds of processing elements (PEs) connected by
a Network on Chip [2] (NoC) based communication infrastructure.
This massive integration, enabled by aggressive scaling of transistors, exacerbates the reliability issues in multi-processor design.
Accordingly, future many-core systems require sophisticated faultaware approaches to tolerate their erroneous characteristics.
Integrated circuit failures are usually classiﬁed by the failure duration, namely, permanent and temporary [3]. Permanent faults
are those who continue to exist until the faulty components are repaired. While, temporary faults can be divided into transient and
intermittent. The former appears once and disappears, while the
latter occurs, vanishes, reappears; and so on. In this paper we focus
on the permanent faults, and unless otherwise stated, with the term
“fault"" we mean “permanent fault"".
Several methods have proposed architectural support to provide
run-time fault-tolerance to NoC-based systems. They mainly contribute to routing algorithms (e.g. [4–6]), topologies [7] or redundant routers [8]. Besides the hardware overhead of such methods,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers
or to redistribute to lists, requires prior speciﬁc permission and/or a fee.
Request permissions from Permissions@acm.org. DAC ’14, June 01 - 05
2014, San Francisco, CA, USA Copyright 2014 ACM 978-1-4503-27305/14/06...$15.00. http://dx.doi.org/10.1145/2593069.2593214
architecture-level approaches are limited to single- or a few faults
[4, 8], or lack 100% reliability (connectivity) for all fault scenarios
[5, 6]. As such, fault patterns may not always be kept transparent to
the run-time resource management of the operating systems (OS),
i.e. OS cannot completely rely on the network architecture.
On the other hand, run-time resource management, i.e. application mapping, is well recognized as an important challenge for
future many-core systems [9–11]. This is primarily because of
the extremely dynamic workload of such systems, where an unpredictable sequence of different applications with various requirements enter and leave the system at run-time. The growing complexity has encouraged several researchers to use distributed resource management methods [11–14].
There are several fault-aware resource management methods proposed in literature [15–19]. However, they assume failures to happen only in PEs while their associated router are still working. As
such, they make the unrealistic assumption that the healthy PEs
always maintain full connectivity. Thus, such system-level solutions are equivalent to those of fault-free mapping/migration methods where a faulty PE is equal to a busy/overloaded one. Whilst, as
aforementioned, the imperfectness of the underlying network may
not be always kept hidden from the OS perspective even by utilizing state-of-the-art architectural supports. Therefore, it is necessity
for run-time resource management approaches to adapt to run-time
failures.
Accordingly, we propose our novel, yet simple, System-level and
Hierarchical Fault-Aware (SHiFA) approach for run-time resource
management of many-core systems. Unlike other system-level methods, we consider failures of both PEs and network resources in
SHiFA. We construct the system-level design of SHiFA based on
distributed OS proposals [20, 21]; i.e. a light-weight kernel is running on the background of each PE, hosting tasks of different applications.
Current decentralized approaches [11–14] completely distribute
the resource management job over different nodes. As a result, they
lack a pervasive view of the system and might lead to disharmonic
and random behaviors [22]. In contrast, we structure SHiFA kernels
in three levels of hierarchy: (i) the system mobile master (MM), (ii)
the application managers (AM), and (iii) the basic kernels. As such,
MM works as a conductor which keeps holistic view of the system
and orchestrates between different AMs, which in turn, each AM
performs detailed jobs regarding its application of responsibility.
This degrades the random behavior of the system while keeping
advantages of distributed management approaches. In this paper
we make the following contributions:
• To the best of our knowledge, we propose the ﬁrst work that
takes the imperfect network into account for run-time fault-aware
management of NoC-based many-core systems (Section 4). Our
system-level design makes SHiFA independent of the underlying
network architecture while keeps the system faults transparent to
the applications. Accordingly, it imposes no hardware overhead
and can be adopted in the already manufactured platforms such as
Intel SCC [23].
• Hierarchical structure of our approach increases the utilization
of healthy nodes within the imperfect network, as motivated in Fig.
Figure 1: SHiFA case example: due to the crossed faulty node and XY
routing, none of the 5 nodes in the shaded area can be accessed by MM.
However, the selected AM can utilize 4 of them (asterisk ones) within
the same architectural limitations.
1. Our experiments show that SHiFA over 99.99% of PEs can be
utilized when up to 8 network links are broken (Section 5.1).
• We equip AMs with a powerful fault-aware mapping algorithm.
The algorithm ﬁnds a feasible allocation of the application tasks,
such that the communication between tasks does not violate the network limitations. The time complexity of the proposed algorithm
is independent of the network size, which is a signiﬁcant scalability property over existing distributed methods. Results demonstrate
99.41% of success rate, when up to 8 links are broken in the network (Section 5.2.1).
• As mentioned, MM keeps a holistic view of the system and
prevents random behavior of distributed managers. We integrate
smart hill climbing [9] with tabu search [24] to decrease the related
overheads compared to random choice of state-of-the-art methods.
Results show 5 times overhead reduction in average (Section 5.3).
• Last but not least, we discuss the use of an always healthy control network which is, implicitly or explicitly, assumed by most of
the existing methods (inc. SHiFA) in Section 6. We also suggest
alternative solutions regarding SHiFA characteristics.
2. RELATED WORK
The deﬁnition of techniques and methodologies for dealing with
on-chip fault-tolerant communication started well before the NoC
paradigm emerged. With the advent of NoC-based architectures,
however, the increase in the number of degree of freedom opened
new opportunities for improving the fault tolerance of the system
[25]. In the following we study related work handling the permanent faults occurring at run-time in NoC based many-core systems.
Architecture-level methods has been improved by operating toward several different directions; including network topologies [7],
network redundancy [8] and routing algorithms [4–6]. However,
architectural methods suffer from several shortcomings. First, they
usually impose noticeable hardware overhead, e.g. [8]. Second,
they might be limited to speciﬁc fault models. For instance, work in
[5] only supports broken links, while routers are assumed healthy,
relying on their Vicis [26] router architecture which imposes 42%
area overhead. Most importantly, such methods are limited in the
number of faults they cover; i.e. they fail in keeping whole system
functional because of a limited number of faults.
Complementary system-level methods are proposed to ensure resiliency while maintaining the required levels of system performance. Although silicon failures will defect both PEs and network
components, the following proposed system-level methods assume
failures only in PEs. Hence, their target problem is reduced to mapping/migration problem where each faulty PE is equivalent to an
unavailable/overloaded PE.
Ofﬂine task mapping techniques have been proposed in [16] and
[18]. The proposed techniques are based on extensive design-time
analysis of different single-PE fault scenarios to determine optimal
mappings. These mappings are stored in a table and are looked-up
at run-time to migrate tasks as and when faults occur. The work in
[16] also presents some run-time heuristics for optimal migration
of the task running on failed PE.
Chou and Marculescu, in their FARM [15] approach, propose a
run-time mapping algorithm which considers failures in PEs. Failed
PEs are not taken into account during application mapping, while
(a)
(b)
Figure 2: (a) Gaussian Elimination application with 9 tasks and 11
edges. (b) Its feasible mapping solution: All the tasks are mapped onto
accessible healthy nodes and none of their communications go through
the crossed faulty node.
during an application execution, spare-cores are used to migrate
tasks of broken PEs.
An adaptive fault-aware scheduling approach is presented by Bolchini et al. [19]. They use software-based triple modular redundancy (TMR) techniques to detect and tolerate faults occurring in
the PEs, and to dispatch application threads to the healthy PEs.
3. PRELIMINARIES
We design SHiFA according to message-passing paradigm both in
the application model and the system architecture. Accordingly,
each application is represented by a directed task graph Ap =
T G(T , E ). A vertex ti ∈ T represents an application task, and
an edge ei,j ∈ E stands for its communication with a destination
task tj . Task graph of Gaussian Elimination application [27] which
is extracted using TGG [28] is shown in Fig. 2(a).
In our message-passing many-core architecture, cores have access to their own local memory and communicate and synchronize
through exchanging messages. An architecture graph AG(N , L)
describes the set of network nodes (N ), connected together with
the set of links (L). Our SHiFA design is not limited to any speciﬁc
network architecture nor fault model; i.e. it can handle faults happening in links, routers, and PEs. Failures can be detected at runtime using an online testing method, e.g. []. Consequently, we denote the set of healthy elements by AGH (NH , LH ) ⊆ AG(N , L).
Reachability: Iff a data packet can be delivered from a node nsrc
to another ndst , regarding the current AGH , we call the destination
node to be reachable by the source node (denoted as nsrc
Accessibility: Iff a control packet can be delivered back from a
reachable node, we called it to be accessible from the source node
A→ ndst ). Assuming a fault-free control network
(denoted as nsrc
(Section 6), every reachable nodes is also accessible.
Territory: The set of healthy nodes that are accessible from a
given node is called its territory (denoted as T node ∈ NH ).
Mapping Problem: Given the current application (Ap) and runtime architecture (AGH ), a mapping algorithm must allocate tasks
to healthy nodes such that all communicating pairs are accessible.
Moreover, allocated nodes should be accessible from the point in
which the mapping algorithm is executed (AM node in SHiFA). In
other words, a feasible mapping is the one whose task allocation
and communication does not involve any faulty element:
R→ ndst ).
map : T → N : map(ti ) = nw,h ; ∀ti ∈ T , ∃nw,h ∈ NH ;
∀ei,j ∈ E , nti
A→ ntj ; ∀ti ∈ T , AM A→ nti
(1)
where nti /ntj is the node which the task ti /tj is mapped onto.
A feasible mapping of Gaussian elimination application onto the
example faulty network (with XY routing) is shown in Fig. 2(b).
In general, the feasibility of a mapping can be assessed at run-time
based on the topology and routing algorithm.
Compliance with current systems: As an example, we note that
our system model is very similar to that of Intel SCC [23] in which
cores (i) are connected through NoC, (ii) simultaneously boot Linux
kernels, and (iii) run message-passing applications.
4. SHIFA SCHEME
As mentioned, we deﬁne three different roles of kernels in the proposed hierarchy: (i) the system mobile master (MM), (ii) the application manager (AM), and (iii) the basic kernel. Regarding the
run-time circumstances, a kernel can switch to any role according
to SHiFA procedures. Note that SHiFA kernels are activated at runtime in arrival and termination of applications, and upon run-time
failures. Otherwise, they will not interfere in the performance of
applications.
Brieﬂy stated, upon arrival of an application into MM, it hands
the application over to an accessible node (AM). AM explores its
neighboring nodes and tries to ﬁnd a feasible mapping of the application, according to (1). AM then reports back to MM about its
success/failure in mapping. In case of success, the application is
executed on the allocated nodes. Otherwise, MM assigns another
AM until a feasible mapping is found. When a task execution is terminated on a PE, its basic kernel informs its associated AM, which
in turn, AM informs MM when all the tasks of its application are
terminated; i.e. the application execution is ﬁnished.
As a result of the proposed hierarchy, we can also utilize nodes
which are not necessarily in MM’s territory. That is, a node can be
utilized (called utilizable) in SHiFA, iff we can ﬁnd a node as AM
such that M M A→ AM and AM A→ ndst .
4.1 System Mobile Master
The mobile master works as conductor and keeps the holistic view
of the system. Accordingly, we deﬁne two major jobs for MM.
First, it signals kernels to promote as AMs. Second, it moves to
accessible nodes to retain its territory size.
4.1.1 AM Selection
Regarding the run-time conﬁguration of the system and the current fault pattern, an appropriate node must be selected to hand the
job of mapping over to it. The candidate node should provide (i)
a high likelihood of a feasible mapping (ii) in its close neighborhood. This is to (i) ﬁnd a feasible mapping quickly while (ii) it
is kept contiguous. Contiguous allocation reduces the number of
network elements incorporated in application execution. As a result, it reduces the expected number of infected applications upon
a run-time failure. Moreover, contiguous allocation decreases trafﬁc congestion and energy dissipation of network [9, 29] as well as
execution time of applications [30, 31].
Existing distributed resource management approaches [11–13] use
random-base node selection. That is, they send the mapping request to one or several random nodes, and each node will explore
its neighborhood to ﬁnd the required number of available nodes.
Such random exploration continues until one or several offers are
found [11, 12]. Nevertheless, random node selection can lead to
high time and communication overheads [9], as each kernel spends
time on generating several exploration packets to collect required
information.
We exploit smart hill climbing (SHiC) [9] integrated with tabu
search [24] for AM selection. Consequently, an AM is selected
using SHiC approach, while unsuccessful AMs are added to a tabu
list to prevent reselection. We empty out the tabu list whenever the
run-time conﬁguration of system changes; i.e. upon an application
termination, fault occurrence, etc.
The utilized AM selection works with the approximate shape of
the running applications, e.g. rectangle in mesh topology [9]. Hence,
we do not need to keep the availability status of all nodes in MM:
AMs report only shape of the obtained mapping back to MM, e.g.
rectangle corners (nx1,y1 , nx2,y2 ) in mesh topology. It is noteworthy that quality of mapping is in AM’s responsibilities, not MM’s.
As a result, MM stops looking for AMs upon a mapping success.
4.1.2 Mobility
MM reacts proactively against being failed or getting isolated. Accordingly, upon any run-time failure, or its prediction1 , MM migrates to an accessible node with the largest territory. This provides
1 e.g. by using software-based prediction methods [32].
MM with the widest options for AM selection which improves the
likelihood of mapping feasibility.
As mentioned, MM does not need to keep the availability status of
all the nodes and only knows about the approximate area of running
applications. This eases the housekeeping and facilitates an agile
migration for MM. For instance, in a mesh, it keeps only address
of 3 nodes for each running application:
the corresponding AM
and two corners of its approximate rectangle. A more sophisticated
exploration of migration scenarios and deﬁnition of a vice-MM role
(for unpredicted failures) is left as future work.
4.2 Application Manager
An application manager maps, migrates, and remaps the application of its responsibility. The task mapping is required once the
application is requested for execution. The task migration and application remapping is required in the case of prediction/occurrence
of run-time faults during the application execution. Note that this
paper focuses on the system-level process of the task migration,
while its implementation details are out of this paper scope.
4.2.1 Fault-Aware Mapping
AM is assigned to ﬁnd a feasible mapping for its application of
responsibility. In order to ﬁnd a feasible and contiguous mapping
of an application, we equip AMs with a fault-aware adaptation of
CASqA [31], as described in Algorithm 1.
Initially, AM maps a task of application (tf ) onto its own local
nodes ( (cid:101)N – line 9) which are in the ﬁrst square around AM (r = 1)3
node and marks connected tasks as M ET 2 (lines 2–5). Afterwards,
for each M ET task (tc ), we collect the set of available accessible
and with the smallest Manhattan Distance (M D = 1) from parent
of tc . Among the collected nodes, we select the one, if any, which
keeps the mapping feasible to allocate to tc (lines 10–15). MD increases once all M ET tasks are examined for feasible mappings
while current radius, r , increases once all possible MD values are
tried. In order to limit the possible dispersion, we limit the maximum value the current radius might have such that twice the application size of nodes will be explored:
(cid:106)(cid:108)(cid:112)2 × |T |(cid:109)
/2
(cid:107)
Rmax =
(2)
The above scenario continues until all the tasks are mapped (line
18). Note that the proposed algorithm is re-executed with picking
different tasks as tf (line 1) to increase the success rate. According
to the fault pattern around AM, however, there might be no accessible node for a given task which will keep the mapping feasible.
Consequently, all the loops will ﬁnish and the algorithm will return
a failure (line 19).
Note that the proposed algorithm considers a variety of different
allocations for a given task and maps it once there is no other task
could be mapped with better conditions; i.e. with smaller r and
M D values. This leads to a higher chance of ﬁnding a feasible
mapping (Section 5.2.1) along with a better mapping quality (Section 5.2.2) compared to related work [14, 29, 33–35].
As AM does not know the availability status of PEs, it needs to
ask them before considering them in the allocation. As such, when
an accessible node is explored for the ﬁrst time (within line 9), AM
sends a reservation request to it. The kernel of the destination node
will reply back to AM about its status and will book a place if it is
available. The destination node will remain reserved until end of
the mapping procedure. Finally, AM will release those nodes that
are not used in the mapping; e.g. all of them in the case of mapping
failure.
(|T |). The next one (line 6) repeats up to (cid:112)|T | times according to
Complexity Analysis: There are four main loops in the algorithm.
The ﬁrst one (line 1) is repeated according to the application size
(2). That is the same for the next loop (line 7). As the M ET set
2We deﬁne parent as the task newly MET tasks are met through.
3We denote each layer of square around AM as square with radius
(r) of the layer count.
Algorithm 1 Fault-aware mapping algorithm executed by AMs.
T G(T , E ): Task graph of the requested application, Ap.
AGH (NH , LH ): The set of healthy elements of the system.
Inputs:
Output:
map : T → NH .
Variables:
U NM , M ET and M AP : Sets of unmet, met and mapped tasks.
r : The current radius of the square around the AM .
Body:
1: foreach tf ∈ T do
2:
ntf ← nAM ;
3: M AP ← tf ;
U NM ← T − (M AP + M ET );
for r = 1 → Rmax do
4: M ET ← set of tasks connected to tf in T G;
for M D = 1 → 4 × r do
foreach tc ∈ M ET do
(cid:101)N ← Available nodes ∈ T AM in r with M D hop away from tc
if ∃nc ∈ (cid:101)N so that keeps the mapping feasible then
parent.
move tc from M ET to M AP ;
if there are tasks in U NM connected to tc then
move tasks connected to tc from U NM to M ET ;
ntc ← nc ;
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
M D ← 1;
if M AP = T then
18: end foreach tf ∈ T ;
return success;
19: return failure; =0
is a subset of tasks, the last loop can repeat up to |T | times. All in
all, this leads to a complexity of O(|T |3 ), which is independent of
the network size, making it suitable for future many-core systems.
Note that if there is no fault in AM region, the ﬁrst loop will run
once; reducing the complexity to O(|T |2 ).
4.2.2 Task Migration and Re-Mapping
Upon a run-time fault occurrence/prediction that inﬂicts the application execution, AM tries to move the interfered tasks so that
they can safely resume their execution. If the task migration cannot resolve the issue, a task re-mapping is initialized. In case of
re-mapping failure, MM will be involved, which either restarts or
migrates the application to a healthy region.
Recall that thanks to the proposed hierarchy, the fault information
does not need to be consistent in all kernels. A new fault will ﬁrst
raise the involved AMs. In case an AM fails in handling the local
failure or after handling it, MM is informed about the fault and the
taken actions.
5. RESULTS AND ANALYSIS
In this section we evaluate our proposed approach and compare it to
state-of-the-art. Accordingly, different set of experiments are performed on our in-house cycle-accurate SystemC many-core platform which utilizes a pruned version of Noxim [36], as its communication architecture. Each node consists of a router and a PE
connected to its local port. A light-weight kernel resides in the
background of each PE and switches between its three deﬁned roles
according to SHiFA strategies. Each PE also hosts a task and emulates the task behavior according to the task graph.
5.1 Accessibility of Nodes
In our ﬁrst analysis, we measure the portion of nodes which can potentially be accessed and utilized under SHiFA. A 12x12 mesh with
XY routing is studied, while the number of broken (bidirectional)
links is gradually increased from 1 to 8. For the sake of scalability analysis, we also considered the case where up to 10% of links
(27 of them) are broken. Moreover, we try to quantify the contribution of the mobility feature of master, as well as the use of an
enhanced architectures. Accordingly, we ran the same set of simulations when the system master resides in a ﬁxed node (FM), and
when the mesh exploits an Odd-Even routing (OE). In OE case,
when two outputs are granted, the healthy one is selected. Once
both granted outputs are healthy, a local priority-based selection is
Figure 3: Percentage of the accessible PEs with ﬁxed master (FM) and
mobile master (MM), using XY or odd-even (OE) routing algorithm.
The darker parts of each bar shows the percentage of PEs that can be
accessed directly from ﬁxed/mobile-master.
applied. The extracted results are shown in Fig. 3 where each point
of the graph is averaged over 100 different random fault-patterns.
As can be seen, 99.999% of nodes can be utilized, when up to
8 links are broken. This is slightly decreased (to 99.72% in MMXY case) when the broken links are increased over 10% of the total amount of links. Our system-level method is comparable to a
highly-resilient routing algorithm [5] which provides 99.99% reliability when 10% of links are broken.
It can be seen that the mobility feature and OE routing mainly
contribute to the territory of system master; i.e. they provide system
master with a larger set of nodes to be selected as candidate AMs.
5.2 Success Rate and Quality of Mapping
As the second study, we measure the success rate of our proposed
fault-aware mapping (Section 4.2.1) in ﬁnding a feasible mapping
for an application. Moreover, we compare the mapping quality with
state-of-the-art algorithms in our run-time simulation environment.
5.2.1 Mapping Success
A 12x12 mesh network is used, while the number of faulty links
is varied between 1 to 8. As before, both XY and OE routing algorithms, as well as the case where 10% of links are broken, are
considered. 100 different fault patterns are generated and examined
for each case. For each fault pattern, we have considered the mapping of 9 different real applications; such as, MPEG4 decoder [37],
Video Object Plane Decoder (VOPD) [38], Multi-Window Display
(MWD) [38], etc. Within each fault pattern, nodes in MM’s territory are promoted as AM one by one. Each AM is assigned to
map each application using the proposed algorithm. For the sake
of comparison, we also utilized the distributed D&C mapping algorithm [14], which is derived from NMAP [35]. As the original
D&C is not fault-aware, we changed the algorithm such that a swap
is applied if the number of feasible communications is increased.
Accordingly, each bar in Fig. 4 shows the percentage of successful mappings over all examined cases (# of fault patterns ×T MM ×
# of apps ).
In case of single-fault by using our proposed algorithm, an arbitrary AM selection can lead to 99.93% of successful mapping using OE routing algorithm. This reduces to 99.41%
when there are up to 8 faulty links. Results show that our algorithm
scales well when there are up to 10% of links broken; an arbitrary
AM selection for a given application leads to over 96% of mapping
success. Note that in a dynamic environment, it is the failure rate
which impacts the performance but the success rate. For instance,
using D&C algorithm, the probability of an arbitrary AM failure increases by 30 folds, when there are only 2 broken links. Moreover,
Fig. 4 shows the ratio of energy per bit of communication obtained
by SHiFA and D&C mappings; the proposed algorithm enhances
network energy dissipation at least by 60%.
5.2.2 Mapping Quality
We compared the quality of the obtained mappings with state-ofthe-art. As mentioned before, however, other system-level methods
consider a fault-free network. In order to enable a fair comparison,
the same assumption is made (only in this set of experiments). A
Figure 4: Success rates for arbitrary AM selection over SHiFA and
D&C algorithms. The line demonstrates the ratio between energy dissipation of obtained solutions.
12x12 mesh network is instantiated with two different fault ratios:
5% and 10%.
A random sequence of applications are entered into the system.
This sequence is kept ﬁxed in all experiments for the sake of fair
comparison. Applications are mapped onto the system using different mapping algorithms including NN[34] and CoNA[33]. Each
experiment is run for 1 million of simulation cycles. Note that
100% of the mappings are feasible due to the assumed healthy network. A normalized summery of extracted results are shown in
Table 1. Note that each cell is averaged over 10 different fault patterns. As can be seen, SHiFA mapping algorithm outperforms other
works regarding the network metrics.
5.3 Run-Time Overhead and Scalability
As mentioned before, MM perceives a holistic view of the system
due to the proposed hierarchy. Whereas, other distributed methods [11–13] lack such pervasive view of the system and explore
the system nodes randomly, which leads to high overhead in ﬁnding suitable resources. In order to evaluate the gained performance
and scalability, we extracted the average number of trials required
to ﬁnd a successful AM, using both SHiFA method and random
method of related work (Fig. 5). As can be seen, SHiFA improves
the related overhead compared to previous works around 5 times,
while scales over different fault populations and system sizes. More
precisely, SHiFA ﬁnds the suitable AM at most in the second try,
while random search might lead to up to 13 trials. Moreover, Fig.
5 shows the normalized execution time required for mapping algorithms. Comparison is done over D&C algorithm as a distributed
approach (which is shown to have improvements over other related
work).
6. DISCUSSION
Existing fault-tolerant work, specially those with no fault pattern
limitations, rely on a perfect oracle network that cannot have faults
to maintain connectivity when faults appear in the actual network.
That is, they require a guaranteed delivery of control packets in
order to keep the system resilient against run-time failures.
For instance, routing algorithms of [6] and [39] explicitly depend
on always healthy control network. Although the presented work
in [5] tends to work in a distributed manner, authors assume that
“the routers know when they need to invoke the algorithm"". This
breaks the assumed distribution and requires an always healthy control network to synchronize routers.
As another example, Chang et al. [8] reconﬁgure the spare routers
in a single point of management in both of their SARA and DAPA
algorithms. This requires an always healthy network which collects the fault information over the chip and distributes the reconﬁguration commands back to the routers. The same assumption is
Figure 5: Scalability results when (up) increasing the fault rate and
(down) increasing the system size. Bars represent of AM trials.
made by existing system-level approaches, as they initially assume
a fault-free network.
Similarly, SHiFA needs a guaranteed negotiation between kernels.
That is, the destination node must be not only reachable but also
accessible in most of the SHiFA procedures such as, AM selection,
MM migration, task allocation/migration, etc.
However, building a completely fault immune control network
will not be light-weight due to the imposed redundancies. Moreover, such assumption cannot be realistic regarding the mean time
to failure (MTTF) of nanometer scale silicons. For instance, the
MTTF of a 12x12 mesh network will be around 15 months, according to [6, 40].
An alternative solution is to route control packets through the
same– potentially– faulty network. Then, in order to guarantee the
delivery of control packets, it will require either a completely reliable and distributed routing algorithm or violating the turn-model
limitations when it is necessary. The former is not proposed yet,
while the later might lead to deadlock.
Our suggestion is to accept the deadlock probability as a price to
pay, and violate the routing restriction if and only if it is inevitable.
The main motivation is our observation that thanks to SHiFA characteristics, the deadlock probability will be extremely negligible.
Especially, when the trafﬁc of control packets is isolated; e.g. by
exploiting "
Disease Diagnosis-on-a-Chip - Large Scale Networks-on-Chip based Multicore Platform for Protein Folding Analysis.,"Protein folding is critical for many biological processes. In this work, we propose an NoC-based multi-core platform for protein folding computation. We first identify the speedup bottleneck for applying conventional genetic algorithm on a mesh-based multi-core platform. Then, we address this computation- and communication- intensive problem while taking into account both hardware and software aspects. Specifically, we group the processing cores into islands and propose an NoC-based multicore architecture for intra- and inter-island communication. The high scalability of the proposed platform allows us to integrate from 100 to 1200 cores for the folding computation. We then propose a genetic migration algorithm to take advantage of the massive parallel platform. Our simulation results show that the proposed platform offers near-linear speedup as the number of cores increases. We also report the hardware cost in area and power based on a 100-core FPGA prototype.","Disease Diagnosis-on-a-Chip: Large Scale
Networks-on-Chip based Multicore Platform for Protein
Folding Analysis
Yuankun Xue
Fudan University
Shanghai, P.R.China
Zhiliang Qian
HKUST
Kowloon, Hong Kong, China
yuankunxue11@fudan.edu.cn
qianzl@ust.hk
Paul Bogdan
University of Southern
California
Los Angeles,CA,USA
pbogdan@usc.edu
Chi-Ying Tsui
Fan Ye
Fudan University
Shanghai, P.R.China
fanye@fudan.edu.cn
HKUST
Kowloon, Hong Kong, China
eetsui@ust.hk
ABSTRACT
Protein folding is critical for many biological processes.
In this work, we propose an NoC-based multi-core platform for protein folding computation. We ﬁrst identify the
speedup bottleneck for applying conventional genetic algorithm on a mesh-based multi-core platform. Then, we address this computation- and communication- intensive problem while taking into account both hardware and software
aspects. Speciﬁcally, we group the processing cores into islands and propose an NoC-based multicore architecture for
intra- and inter-island communication. The high scalability
of the proposed platform allows us to integrate from 100 to
1200 cores for the folding computation. We then propose a
genetic migration algorithm to take advantage of the massive parallel platform. Our simulation results show that the
proposed platform oﬀers near-linear speedup as the number
of cores increases. We also report the hardware cost in area
and power based on a 100-core FPGA prototype.
Categories and Subject Descriptors
D.1.2 [Computer Systems Organization]: Multiple
Data Stream Architecture—Interconnection Architecture
Keywords
Network-on-Chip (NoC), Multi-core, Protein Folding
1.
INTRODUCTION
Proteins are large biological molecules formed by one or
more chains of amino acid residues; they perform a variety of
essential functions in life [5]. In order to be functional, proPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC ’14 June 1 - June 5, San Francisco, CA, USA
Copyright 2014 ACM 978-1-4503-2730-5/14/06 ...$15.00.
Figure 1: a)The folding structure of the Ubiquitin [13] protein
molecule. The atoms belong to the same amino acid residues are
grouped into the same color. b) The 3D HP-SC model representation of a protein with ﬁve amino acids
teins have to fold themselves to speciﬁc three-dimensional
structures known as native states (see the structure in Fig. 1a). Consequently, protein folding is fundamental for virtually all biological processes and deviation from native state
might cause diseases like cancer, Huntington’s and Parkinson’s diseases or allergies [2]. In the future, many diseases
will be diagnosed by examining the patients’ corresponding
protein structure. Towards this end, a comprehensive understanding of the protein native state is required [2]. Although
a large amount of proteins have been discovered, only a small
portion has their folding structures known due to the high
cost of unveiling the protein structure through biochemical
experimentation [14]. Therefore, protein models have been
proposed to enable computer simulations to predict the protein native state. For example, the Hydrophobic-Polar (HP)
model [14] classiﬁes the 20 standard amino acids into two
types (i.e., hydrophobic and polar as shown in Fig. 1-b) and
uses the energy landscape model [4] to predict the conformation with minimum free energy.
In general, the computational approach to search the
minimum energy state has been proven to be NP-complete [1].
Speciﬁcally, the evolutionary algorithms have been proposed
to explore the large folding space eﬃciently. Moreover, several high-performance computing platforms such as computer clusters [3] or FPGA-based [7] hardware accelerators have
been used to fulﬁll the massive calculation requirements in
the searching process.
In this work, we propose an application-speciﬁc on-chip
platform for the protein folding analysis. A parallel genetic
migration algorithm is implemented which takes advantage
of the high scalability provided by the hierarchical NoC [9].
Our proposed platform enables the integration of 100 (cid:24) 1200
cores and achieves from 33 to 310 speedup compared to the
single master and single slave processor based implementation. We also demonstrate the proposed architecture based
on a 100-core FPGA prototype.
The remainder of this paper is organized as follows: in
section 2, we review several related works and highlight the
paper contributions. In section 3, we present the proposed
NoC-based application-speciﬁc platform for the folding computation. Section 4 presents the simulation results. Finally,
in Section 5, we summarize our contributions and point out
some future directions.
2. RELATED WORK
There are two types of models for the protein folding
computation that are widely used: the lattice model [14]
and the full atomistic model [10]. For example, the Anton
machine [10] employs 512 ASICs to perform the molecular
dynamics (MD) simulations based on the atomistic model.
Although accurate, many folding processes of interest occur
on a timescale that is much longer than the full molecular
dynamics (MD) simulations can reach [6]. MD focuses on
the exploration of dynamic folding process while atomistic
models such as HP models can predict the native conformation of proteins in a faster and more eﬃcient way.
For
the lattice model of protein folding prediction, the minimum
energy conformation is usually found via Metropolis Monte
Carlo algorithm [8]. In [12], a genetic algorithm with natureinspired mutation and crossover operations is used to achieve
better conformation and faster convergence. However, the
algorithm is sequential and best suited for single CPU-based
machine. In[3], a parallel genetic algorithm is proposed to
apply the folding analysis on computer cluster systems. The
parallelism is obtained by using a master processor to divide
the workload among several slave processors. However, it is
observed in [3] that the speedup tends to saturate when the
number of processors increases (saturation happens for 50
slave cores). In [4], a hierarchical parallel genetic algorithm (HPGA) is proposed to improve the scalability. Multiple
coarse-grained islands are implemented to support sporadic
migrations between the populations. The algorithm is designed for a computer cluster and uses dedicated message
passing interface (MPI) for the inter-computer communication. Besides using computer clusters, there have been
several attempts at accelerating the folding computation by
using network of FPGAs [11] or a hybrid network of FPGA
and computers [7].
In this work, instead of concentrating on dynamic folding
process using ubiquitous full-atom model as Anton does, we
focus on the eﬃcient exploration of possible protein conformations and massive parallelism on large scale NoC, rather
than on cluster system like CPU/GPU based platform, to
enable faster platform and provide solutions of good quality
in ever-reduced time.Then, we propose an on-chip platform
for the folding computation. The system speedup is improved from the joint optimization in both the algorithm
and the hardware design. In summary, this work brings the
following new contributions:
1) We propose a hierarchical and highly scalable NoC
architecture which integrates from 100 to 1200 cores for
Figure 2: Mesh NoC based protein folding prediction platform
the folding computation. All designs have been implemented using fully synthesizable hardware description language.
Moreover, a 100-core FPGA platform is developed to demonstrate the speedup and scalability performance.
2) We propose a parallel genetic migration algorithm
which takes advantage of the massive parallel NoC platform
and achieves near linear speedup as the number of cores increases. We contrast our approach with prior solutions and
show the overall performance beneﬁts.
3. PROPOSED NOC-BASED PLATFORM
In this section, we ﬁrst present a mesh NoC based platform. Then, we discuss the speedup limitations of the platform. Two new hierarchical NoC architectures are proposed
to improve the system scalability. Finally, a genetic migration algorithm for the proposed NoC architecture is presented.
3.1 Mesh NoC-based platform design
Based on a set of feasible protein conformations, the genetic algorithm iteratively performs the following two procedures: 1) ﬁtness evaluation of each individual in the population; 2) population reproduction based on the returned
ﬁtness value.
In one generation, the population contains multiple individuals that are independent of each other.
Therefore, a straightforward way to achieve speedup is to
employ multiple processors and calculate the ﬁtness values
simultaneously.
Towards this end, a mesh NoC based multicore platform is proposed to accelerate the protein folding analysis
(shown in Fig. 2).
In this platform, slave processors are
symmetrically placed to balance the network traﬃc. When
the protein folding process begins, the target population is
initialized randomly by the master processor. A coordinate
scheme is developed which encodes the spatial structure of
a protein by using a 5-direction relative movements starting from the coordinate of the ﬁrst amino acid. Once the
population initialization is done, the master processor starts to send individual chromosome one by one to the slaves
through on-chip network. Upon receiving new individuals,
the slave processor performs the chromosome decoding and
the ﬁtness calculation. Speciﬁcally, the slave ﬁrst converts
the encoded chromosome conﬁguration back into 3D Cartesian coordinates. Then, an ob jective function similar to that
in [12] is used to compute the ﬁtness value. The calculated
ﬁtness values are then sent back to the master processor.
After collecting all ﬁtness values, the elitism-based selection
is performed in the master processor such that ﬁve conﬁgurations with the highest scores are kept intact and preserved
in the new generation. Finally, the genetic operators (e.g.,
mutation and crossover) are mounted to produce remaining
individuals in the new generation.
3.2 Motivation of multi-island platform design
The mesh NoC-based platform with a single master processor is simple to implement. However, it is observed that
the parallel speedup gain saturates very quickly as the number of slave processors increases above 25. This phenomenon
is due to the following two reasons:
1) Non-trivial master processing time: Let Tpg denote the overall time to produce a new generation. It is given
by: Tpg = Tdis + Tproc , where Tdis includes three parts: i)
the time required by the master processor to distribute individuals to all slave cores, ii) the time needed for the slave
processor to ﬁnish the ﬁtness calculation and iii) the time for
the network to return the calculated ﬁtness values. Moreover, we use Tproc to represent the time spent in the master
processor, i.e., the time needed to produce a new generation.
As the number of slave processors increases, more computing resources are available to calculate the ﬁtness values at
the same time. Therefore, Tdis is reduced. On the other
hand, Tproc remains unchanged since there is only one master processor to do the control and it cannot beneﬁt from
the increasing number of slave processors.
From our simulations, Tproc is trivial compared to Tdis
when using less than 25 slave processors. However, if more
slave processors are introduced in the system, Tproc dominates Tpg and limits the speedup enhancement.
2) Non-negligible communication cost: In addition
to the master processing time, the communication overhead
is also not negligible. In every cycle, the master processor
could only send one ﬂit out due to ﬁxed channel bandwidth
between master and slave processors in NoC. The other slave
processors have to wait before receiving the corresponding
individual. Formally, let tcom represent the time for a master processor to send an individual chromosome to a slave
processor. tcalc denotes the actual time for a slave processor
doing the ﬁtness computation. The eﬀective ﬁtness calculation time t
calc is then given by: t
n (cid:3) tcom is the time interval of a slave processor to receive
where n is the total number of processors in the network and
two chromosomes.
The derivation of t
calc , for larger networks shows that
it takes a longer time for a slave to receive an individual
before it can begin the calculation. In Fig. 3, we show the
t
calc breakdown of diﬀerent mesh sizes. As Fig. 3 indicates,
when the network size exceeds 10, tcom becomes comparable
to tcalc and limits the speedup.
An intuitive means of overcoming the speedup bottleneck
of the mesh NoC-based platform is the partitioning of the
single-master multi-slave design into several small islands.
Each island contains a number of slaves as well as a master
processor. However, from the point of view of the genetic
algorithm, the partitioned island with only a portion of the
global population could easily fall to the local minimal trap
due to the limited bio-diversity within each island. Then,
the genetic operator tends to always select a relatively small
calc = n (cid:3) tcom + tcalc ,
′
′
′
′
Figure 3: t
′
calc breakdown under diﬀerent network size
A partitioned island with a 5 (cid:2) 5 mesh interfaced with migration
Figure 4: a) Proposed multi-master multi-slave architecture, b)
oﬃce, c) Two proposed inter-island connection schemes (SRP and
EBC), d) Router mircoarchitecture
group of elites. Thus, the search easily ends up with immature convergence.
Consequently, population migrations between the islands are needed. In the next section, we present a hierarchical
NoC architecture with dynamic migrations between islands
to overcome these limitations.
3.3 Hierarchical multi-master architecture
In Fig. 4a-d, the overall architecture of the proposed
hierarchical NoC is shown.
In the proposed design, the
processors are grouped into several partitioned islands as
shown in Fig. 4a. The intra-island connectivity is oﬀered by
mesh-based NoC. While for the inter-island communication,
two connection schemes are proposed, namely the Swap-hub
roaming path (SRP) and Express broadcasting channel (EBC), respectively (shown in Fig. 4c).
1) EBC-based inter-island connection:
If all the
processor cores are integrated on a single chip, the simplest way to exchange information among the islands can
be implemented via broadcasting. The partitioned islands
are connected as a mesh network. Together with the lower
level meshes within each island, it forms a two-level hierarchical structure. An external broadcast center (BC) is introduced in the top level (i.e., the mesh of islands) to control
the whole migration process. An express broadcast channel (EBC) is designed as an ubiquitous link to connect the
Algorithm 1 Migration scheme on proposed architecture
3:
Input:
Set of initial individual chromosome C ; Set of slave processors S lavek ; Iteration number G; Number of individuals P ; Total number of slaves S ; Migration time interval
MGAP ; Static refresh interval RGAP ;
Output:
1: for all k 2 G do
The set of individual chromosome C ;
For all i 2 P , send C [i] to S lavek ; k = i mod(S ) to
2:
get ﬁtness F [i]
Find max ﬁtness Cmax in F and compare Cmax with
previous max ﬁtness,Pmax
If Cmax > Pmax ,replace Pmax with Cmax , send 2 individuals with highest ﬁtness to adjacent swap-hub and
reset mcnt . Else, increase counter mcnt and rcnt ;
If rcnt== RGAP , send 2 chromosome in C randomly
to swap-hub and reset rcnt
If mcnt== MGAP , send get 5 migrants from neighboring swap-hub randomly and reset mcnt
7: Mount genetic operator in C to produce N , replace C
with N
8: end for
9: return C ;
4:
5:
6:
with static and dynamic migrants injection process. The
details of the algorithm are presented in Algorithm 1. At
run time, together with the conventional genetic algorithm executed in each island, the proposed migration scheme
re-distributes the migrants across the entire network in a
ﬁtness-aware manner. The traﬃc ﬂows of the migrants are
provided by both the dynamic and static injection methods.
For the dynamic injection, the migrants in a swap-hub are
generated from its four adjacent islands whenever the bestever ﬁtness values get improved in these islands. In this way,
the islands containing continuously improved generations
will contribute more to the migrant pool in the swap-hub
and reshape the migrants’ distribution to a ﬁtness-inclined
form. In addition, the static migration injection method in
[4] is also implemented to disturb immature convergences.
4. EXPERIMENTS AND RESULTS
4.1 Simulation setup
In order to evaluate the proposed NoC-based multicore
platform, we implement the whole architecture in fully synthesizable verilog. All the simulations are then performed
using the Cadence NC-Verilog simulator. For the hardware
prototype, we have synthesized 100 processor cores together
with the NoC on a Xilinx Virtex-6 LX760 FPGA board. A
set of protein sequences with diﬀerent lengths taken from [4]
are used as our input benchmarks. Speciﬁcally, the tested
proteins consist of 9 protein sequences formed by 27 (cid:0) 48
amino acids that are widely adopted in the lattice protein
model.
In this section, we ﬁrst evaluate the performance
of the single master platform. Then, we present the experimental results for multi-master (islands) based architectures. Finally, the hardware overhead is reported based on
the FPGA prototype.
Figure 5: The swap-hub based inter-island connection
broadcast center and the islands within one cycle. When
a migration request is generated in an island after, the migration oﬃce within that island will forward this request
to the broadcasting center (BC). The BC is responsible for
arbitrating the requests among the conﬂict islands. Once
the request from an island Gi is granted, it is also sent to
the remaining islands (i.e., broadcasting). Upon receiving
this broadcast signal, all the other islands are obligated to
transfer their local migrant candidates to Gi through the
network. In Gi , an input buﬀer is used to hold the incoming
migrants from other islands. The migrants are then eligible to participate in the elite selection process in the master
processor and help to improve the quality of next generation.
2) SRP-based inter-island connection: If the multicore plaform is developed at the board-level, i.e., integrating
several chips on the same board, the bandwidth across the
chip boundaries will limit the application of broadcasting.
Therefore, we propose a hub-based network to bridge the
islands instead of using EBCs. As shown in Fig. 5-a, the
inter-island mesh in EBC is replaced by a more compact
network formed by swap-hubs. Each swap-hub connects up
to 4 islands to receive and send migrants. Fig. 5-b and 5-c
show the functional block and micro-architecture of the proposed swap-hub. As shown in Fig. 5-c, the swap-hub have
two crossbar switches and four FIFO queues. The ﬁrst crossbar is between the input ports and the FIFOs. The second
crossbar is implemented between FIFOs and output ports.
In this way, any input and output port of a swap hub can
be connected to one of the four FIFO queues at run time.
Based on the swap-hub topology, we propose a new migration scheme which allows the migrants to cross the entire
island network. Moreover, the migrants are refreshed based
on a ﬁtness-aware algorithm to prevent immature convergence to local minimal energy trap. To be more precise,
each swap-hub is designed to have three working states: i.e.,
SWAP, RENEW and MIG. Migrants are exchanged between
the neighboring swap-hub pairs during the SWAP state and
they are renewed by adjacent islands in the RENEW state.
When a migration request is asserted in any of the four islands, the swap-hub will enter the MIG state and randomly
send ﬁve out of eight candidate individuals to the request
island.
3.4 Proposed Migration algorithm
Thanks to the high scalability facilitated by the proposed
architecture, we can propose a combined migration scheme
4.2 Speedup of single master platform
Figure 6: Speed up gain of single master platform
Figure 7: Latency to distribute task to slaves
To evaluate the parallel speedup gain of the proposed single master platform, we have conducted simulations with
diﬀerent mesh network sizes ranging from 2(cid:2)2 to 7(cid:2)7 meshes. Given a pre-determined population size, the speedup
gain G(N ) is deﬁned as the ratio of the time needed to fulﬁll the computation on a single-slave platform (i.e., Ts ) over
the time needed on the proposed multi-slave platform (i.e.,
TN ).
In Fig. 6, we show the speedup gain (G(N )) in the
protein folding simulation of the Unger273d.1 molecular. As
shown in Fig. 6, the red line shows the overall speedup gain.
The blue line highlights the speedup of T dis (deﬁned in
Section 3.2). Both speedup gains tend to saturate beyond
25 slave cores because the time spent in the master (i.e.,
T proc) becomes much larger than T dis when the number of slave processors continues to increase. Therefore, the
master processor is the performance bottleneck that limits further speedup.
In Fig. 7, we explore the NoC design
that is suitable for the proposed platform. For NoC-based
protein folding computation, as analyzed in Section 3.2, we
can reduce tcom in order to improve the achievable parallelism within an island. Therefore, a large mesh size and
buﬀer depth are preferred to improve the network saturation and provide low-latency communication. On the other
hand, tcom is equal to the reciprocal of the injection rate of
the master processor and is aﬀected by the protein sequence
length. For a longer protein chain, it takes more time to
transmit an individual to the slave processor. Therefore,
the maximum packet injection rate of the master core is
bounded by the minimum protein length. There is no need
to further improve the NoC saturation beyond this rate. We
then explore the most suitable island size assuming a 4-ﬂit
buﬀer in each input port of the router. As shown in the
ﬁgure, it is observed only under 5 (cid:2) 5 mesh, a single masFigure 8: Multi-island Speedup
ter could send individual chromosome packets freely to its
slaves with an acceptable delay. For this island size, the
speedup gain is 7:6. In the following, the 5 (cid:2) 5 mesh NoC
speedup gain in terms of Tdis is about 15:36 and the overall
is always adopted as the communication backbone for the
multi-island implementation.
4.3 Evaluation of Multi-island platform
After identifying the speedup bottleneck of the singlemaster platform, we then evaluate the performance of proposed multi-island architecture. We ﬁrst show the verilog
simulation results with 2 to 48 islands (i.e., from 50 to 1200
cores). The speedup performance is shown in Fig. 8. As
shown in the ﬁgure, the speedup curve shows near-linear
property. As a result, for the 1200 core platform with 48islands, a 310X speedup is achieved.
Next, we compare the convergence of the proposed platforms against a single island implementation for 9 proteins
in the benchmark. For single island simulation, 2400 individual chromosomes are randomly initialized for a population. For multi-island platforms, we divide the workloads
into 48 islands with EBC- and SRP-based inter-island connection, respectively. Due to the signiﬁcant time required
by the verilog-based simulator, ten independent runs are
done with diﬀerent random seeds. we adjusted the key running parameters as crossover rate = 0:8, mutation rate =
0:1,migration gap = 40, static ref resh gap = 100. Table
1 summarizes the folding results. In Table 1, we report the
best-ever ﬁtness values (B est F itN orm) of the three platforms up to 2500 generations. The equivalent number of
H-H side-chain non-local contacts (H nC ), which is deﬁned
as a standard measure of goodness, is adopted to evaluate
the folding result [3]. From Table 1, it consistently shows
a better convergence for the multi-island based solution in
the same number of iterations. Also, it can be observed that
the single island implementation easily gets trapped in local
optimal solutions. The maximal number of H-H contacts
reported by [3] is included in the third column of Table 1,
which is obtained from a 31 computer cluster by running
more generations. Table 1 shows that the quality of the
proposed platform after 2500 generations is comparable to
the reference solution. Moreover, comparing the EBC and
SRP schemes, we can ﬁnd the SRP scheme outperforms EBC in terms of the ﬁtness quality because the migrants are
more likely to be limited in the neighboring islands in the
EBC scheme, while the SRP scheme successfully supports
Benchmark
Unger273d.1
Unger273d.2
Unger273d.3
Dill.1
Dill.4
Dill.5
S48.1
S48.2
S48.3
Table 1: Folding results for 9 proteins under diﬀerent architecture
Single Island
EBC
SRP
Best-Fit Norm Max HnC Best-Fit Norm Max HnC Best-Fit Norm Max HnC
0.893
9
0.954
11
1
12
0.916
9
0.949
10
1
12
0.873
9
0.96
11
1
12
0.94
19
0.94
19
1
20
0.91
32
0.951
35
1
36
0.917
10
0.953
11
1
13
0.92
27
0.926
27
1
29
0.78
21
0.92
26
1
29
0.92
24
0.95
25
1
26
n HnC ref
27
27
27
27
31
36
48
48
48
12
13
13
21
41
14
32
32
30
the global population migration among the cores.
4.3.1 Implementation overhead evaluation
We investigated the hardware complexity, timing and
estimated power consumption based on a 100-core FPGA
prototype. The prototype consists of 4 partitioned islands
connected by SRP scheme. For each island, a 5 (cid:2) 5 mesh is
employed. Table 2 summarizes the results. Due to the device
limitation, the proposed swap-hub-based architecture has a
maximum frequency of 100:56M H z and a power consumption of 4:892W . Overall, 80% of LUT and 17% of registers
are used for routability considerations. At this running frequency, it takes down to 5.08us for our baseline design with
single slave to produce a new conformation of given benchmarks. In the future, we will extend this work to a 1200 core
platform with on-board integration of multiple ASIC chips.
Table 2: FPGA implementation on Xilinx Virtex-6 LX760
FPGA resource
Registers as FF
Slice of LUTs
Block RAM/FIFO
Maximum Frequency
Dynamic Power
Summary
162017(17%)
394657(80%)
576(80%)
100.56M H z
4.892W
5. CONCLUSION
In this work, we have identiﬁed the speedup bottleneck of
the conventional genetic algorithm on a single-master platform and proposed two hierarchical architectures for on-chip
and inter-chip integration. A parallel migration scheme has
been proposed to take advantage of the NoC architecture.
Simulation results show an overall 310X speedup gain compared to single-master and single-slave design. Finally, we
have demonstrated a 100-core FPGA prototype and analyzed the hardware overhead. We believe that our work
opens the path towards large scale NoC design and enables
the future work of bioinformatics and bio-computing who
reevaluate NoC-based multicore platforms, especially using
conﬁgurable devices like FPGA or hybrid system, and develop solutions for improving quality and reducing costs.
6. ACKNOWLEDGEMENT
We are thankful to anonymous reviewers for their valuable feedback. This research was supported in part by University of Southern California.
7. "
CAP - Communication Aware Programming.,"Networks on Chip (NoC) come along with increased complexity from the implementation and management perspective. This leads to higher energy consumption and programming complexity of NoC architectures. This work introduces communication aware programming to address communication resource management and efficient programming of NoC architectures. A programming interface is introduced to express communication requirements at the language level. These requirements are evaluated by an operating system component, which configures the communication hardware accordingly. The proposed concept enables an intuitive use of NoC features like end-to-end connections and Direct Memory Access (DMA). The presented results show that communication aware programming can improve performance and energy consumption.","CAP: Communication Aware Programming
Jan Heisswolf(cid:2) , Aurang Zaib(cid:2) , Andreas Zwinkau(cid:2) , Sebastian Kobbe(cid:2) ,
Andreas Weichslgar tner† , Jürgen Teich† , Jörg Henkel(cid:2) ,
Gregor Snelting(cid:2) , Andreas Herkersdorf(cid:2) , Jürgen Becker(cid:2)
(cid:2)Karlsruhe Institute of Technology (KIT), Germany
(cid:2)Technische Universität München (TUM), Germany
†Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany
ABSTRACT
Networks on Chip (NoC) come along with increased complexity from the implementation and management perspective. This leads to higher energy consumption and programming complexity of NoC architectures.
This work introduces communication aware programming to
address communication resource management and eﬃcient
programming of NoC architectures. A programming interface is introduced to express communication requirements
at the language level. These requirements are evaluated by
an operating system component, which conﬁgures the communication hardware accordingly. The proposed concept
enables an intuitive use of NoC features like end-to-end connections and Direct Memory Access (DMA). The presented
results show that communication aware programming can
improve performance and energy consumption.
Categories and Subject Descriptors
D.1.3 [Programming Techniques]: Concurrent Programming—Distributed programming, Paral lel programming
General Terms
Design, Language, Performance
Keywords
communication, network on chip, many-core, X10, invasive
1.
INTRODUCTION
Physical limitations prevent designers from further increasing the performance of single cores. Consequently, parallel architectures have emerged as a ma jor choice for increasing computational performance. From the communication perspective, such parallel architectures lead to new
challenges. At the architectural level, limitations of busbased communication need to be overcome. Networks on
Chip (NoC) [2] have been presented as a scalable communication infrastructure for many-core architectures. Compared to bus-based interconnects, NoCs oﬀer a better scalability for two ma jor reasons: (1) Distributed communication. (2) Wire length and clock frequency are independent
of the number of compute nodes. However, deploying such
complex communication systems in modern architectures,
comes with several drawbacks: (1) NoCs have a noticeable
share in the overall power consumption, as analyzed for Intel’s Single-chip Cloud Computer (SCC) [15]. (2) On-chip
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
DAC ’14, June 01 - 05 2014, San Francisco, CA, USA
Copyright 2014 ACM 978-1-4503-2730-5/14/06 ...$15.00.
http://dx.doi.org/10.1145/2593069.2593103
networks are required to be managed by the Operating System (OS) or applications because of their distributed nature. (3) In contrast to buses, the distance between communicating nodes should be taken into account.
It is essential to address the above-mentioned drawbacks during
application development which may otherwise lead to semioptimal performance and higher power consumption. Therefore, eﬃcient mechanisms to manage and utilize NoC-based
communication infrastructures are required. However, such
strategies may also result in an increased software development complexity. Thus, we introduce communication aware
programming as an approach for developing parallel applications for NoC-based architectures. An easy to use constraint system is introduced. It enables to express the communication requirements of the application in an intuitive
way. These constraints are evaluated by the operating system during run-time, taking the current utilization of the
communication infrastructure into account. The high level
constraints expressed by the programmer are broken down
into low level control routines for the communication hardware. Afterward, the NoC hardware is conﬁgured according
to the requirements of the application. A feature-rich scalable NoC is introduced to enable eﬃcient communication.
The presented architecture is realized as an FPGA prototype and used for the detailed investigations. For power
analysis, ASIC synthesis results are presented.
The rest of this work is organized as follows: Section 2
summarizes related work. The general concept of Communication Aware Programming (CAP) is introduced in section 3. Section 4 introduces the language extensions and the
constraints system. In section 5 Compilation, OS support
and hardware management is discussed. Section 6 gives an
overview on the NoC hardware extensions and their implementation. The beneﬁts of the proposed concept are investigated in section 7 with respect to performance and power
consumption. The work is concluded in section 8.
2. RELATED WORK
Automated generation of eﬃcient parallel applications is
a huge research challenge. The MAPS framework [6] aims
at parallelizing C-application for MPSoCs. However, communication demands are not addressed because MAPS considers a transparent non-scalable bus-based crossbar architectures. SteamIt [23] is a language-based approach for development of streaming applications. In contrast to CAP,
StreamIt only addresses streaming applications and does not
take into account the characteristics of the underlying communication infrastructure. Another approach for the development of parallel streaming applications is presented in [7].
It targets the IBM cell architecture and its scratchpad memories. None of the previous discussed work addresses the
demands of scalable NoCs and their features.
Resource aware programming is addressed by Lorincz et
al. [17] for sensor networks. The so-called resource brokers are used to mediate between low-level physical resources
and higher-level application demands. Such sensor networks
have a fundamentally diﬀerent architecture and thus diﬀerent communication requirements as compared to a generalpurpose tiled many-core NoC architecture, addressed by our
approach. Invasive computing [22] proposes a novel computing paradigm, which supports resource aware programming
from application [21] as well as architecture perspective [14].
Along with these ideas this work focuses on communication
resources in the context of resource aware programming and
also addresses its realization for a NoC-based architecture.
3. CONCEPT
The concept of communication aware programming addresses Non-Uniform Memory Access (NUMA) architectures,
such as Intel’s SCC [15] or Tilera’s architectures [1]. A schematic representation of a tiled NoC-based NUMA architecture is shown in Figure 1(a). It consists of processing tiles
and memory tiles. The memory tiles enable access to oﬀchip memory (e.g. DDR memory). The internal structure
of a processing tile is shown in Figure 1(b). Hardware managed uniform caches with intra-tile coherency are used to
hide memory access latencies. An addressable single cycle
SRAM-based tile local memory or scratchpad enables computation with a low memory latency for a limited data set
size. The Network Adapter (NA) connects the tile internal
bus via the L2-cache to the NoC. The NoC enables communication between tiles, access of external memories and peripherals. The memory is arranged as a Partitioned Global
Address Space (PGAS). PGAS is realized through address
look-ups performed by the network adapter. Each memory
within the architecture can be addressed from each node.
Therefore, L2-cache misses result in transparent cache-line
fetching supported by the NoC. However, the concept is not
restricted to the shown mesh topology.
CPU
CPU
CPU
CPU
N
A
Memory
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
N
A
Memory
N
A
Memory
NoC
Router
NoC
Router
NoC
Router
CPU
CPU
CPU
CPU
N
A
Memory
NoC
Router
Memory
N
A
NoC
Router
CPU
CPU
CPU
CPU
N
A
Memory
NoC
Router
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
CPU
L
1
L
1
Core 
0
Core 
2
L2-Cache
N
A
Memory
N
A
Memory
N
A
Memory
NoC
Router
NoC
Router
NoC
Router
N
A
NoC
Router
L
1
L
1
Core 
1
Core 
3
Tile Local 
Memory
No
Rou
(a) Architecture overview
(b) Tile
Figure 1: (a) Tiled architecture with multi-core tiles
connected via a mesh-based NoC. (b) Each compute
tile has 4 cores, a local memory and caches.
3.1 Spatial data locality
For eﬃcient programming of NoC-based NUMA architectures, the programmer has to be aware of data locality and
access latencies. In general, the most frequently used program data should be located as close as possible to the processing element because of the following reasons: (1) The
application performance suﬀers from increased latency when
accessing distant data. (2) The utilization of a NoC-based
communication infrastructure increases linearly with the distance. (3) The dynamic power consumption for data access
is almost proportional to its distance. Caching can solve the
spatial data locality problem for small data sets which are
used exclusively. However, if a parallel program needs to
exchange data between tiles or if the data set is to large for
the cache, eﬃcient inter-tile communication is required.
A tile local memory, as shown in Figure 1(b), can be used
to explicitly store data close to the processor. However, the
limited size of this memory demands its eﬃcient utilization.
It could be used in an eﬃcient manner, if its management is
handled by the application developer itself. He has the exclusive knowledge about the most recently used data. However, user driven memory handling complicates the application development. Therefore, a tradeoﬀ between eﬃciency
at the language level and the communication awareness is
required. An eﬃcient prefetching methodology at the language level is applied to handle this trade-oﬀ (see section 4).
It enables to cache frequently used data in the tile, reducing
the number of tile external memory accesses.
3.2 Hints and constraints
For communication aware programming, the application
developer provides his knowledge about the communication
of the application to the OS. The OS in turn manages the underlying architecture to fulﬁll the application requirements
by taking into account the current architecture utilization.
The hints provided by the application developer can be used
by the OS for task mapping and for allocating communication resources. Therefore, the hardware is required to
support the resource allocation at the granularity of applications. The deﬁnition, run-time evaluation and hardware
conﬁguration based on communication constraints are detailed later.
4. PROGRAMING LANGUAGE
X10 [19] is a programming language which brings modern features to the ﬁeld of scientiﬁc computing by addressing parallelization from the start of the application development. This includes well-known advantages like type safety,
system modularity, partitioned global address space, generic
programming, and integrated concurrency. In addition, X10
also includes promising new features like dependent types
and transactional memory (via atomic and when). Since
this feature set is not available in other languages, e.g. C++,
we have used X10 as a programming language to realize resource aware applications in our framework. The concurrency and parallelism semantics of X10 are deﬁned in terms
of activities, which are lightweight threads. The activities
can run in parallel on diﬀerent processing cores and can not
be preempted by the OS.
4.1
Invade, Infect, Retreat & Claims
Invasive computing [21] addresses resource awareness with
respect to computation resources. The idea of invasive programming is taken as a basis to realize CAP. A simple invasive program is shown in Figure 2. The concept of allocating, utilizing and releasing resources is known from memory
allocation. Invasive computing generalizes the concept to invade, infect and retreat under speciﬁc resource constraints.
Constraints for invasion form a hierarchy [25], which are
applied to realize complex applications [5]. Communicationspeciﬁc constraints are an extension to the existing set of
constraints. Applications can specify a need for throughput or latency with respect to global shared memory, peer
activities, or parent activity, as detailed later.
4.2 Prefetching
For execution of applications like matrix multiplication or
image processing, where the data is too big for the tile local
memory, blocks of data should be cached instead. Those
blocks should be prefetched in parallel to the execution. This
can be done eﬃciently by using a DMA unit (see section 6.2).
1 val ilet = (id:IncarnationID) => {
do_something(id);
2
3 };
4 claim = Claim.invade(constraints)
5 claim.infect(ilet)
6 claim.retreat()
2
1 val claim = Claim.invade(
new PEQuantity(1) &&
new Type(PEType.RISC) &&
new ThroughputToMaster(128)
3
4
5 )
Figure 2: The basic idea of invasive programming:
The ilet function provides an action to perform in
parallel on all allocated processing elements; Invade
allocates resources under speciﬁc constraints keeping in view with other concurrent applications; Infect uses those resources by letting ilets (basically
compute kernels) run; Retreat releases allocated resources.
First, the amount of tile local memory must be speciﬁed
using the LocalMemory constraint. While memory in X10
is managed by a garbage collector, there are explicit alloc
and free methods for tile local memory to provide the application full freedom for cache management. An example
is given in Figure 3.
1 val loc = TileLocalMemory.alloc[int](cs);
2 val offset = id.ordinal * cs;
3 val future = data.fetch(offset, loc);
4 ... // do something else, while the data is
copied into tile local memory
5 val loc2 = future.force();
6 assert loc == loc2;
7 ... // use the tile local data in ‘loc‘
Figure 3:
The code example is showing the
prefetching in X10. For the programmer, the future
concept is used to model the background activity of
the DMA transfer. The force call synchronizes and
waits for the transfer to ﬁnish.
4.3 Example
As an example, we have considered a Picture in Picture
(PIP) task graph [3] as shown in Figure 4. The communication bandwidth requirements in this example are known.
Invasion of one edge and one node of such a graph is shown in
Figure 5 for C0 . In this way, arbitrary tree-form task graphs
with arbitrary bandwidth requirements can be constructed.
C0(128)
inp
mem
C4(64)
hs
C1(64)
vs
C2(64)
jug1
C3(64)
inp
mem
C5(64)
jug2
C6(64)
mem
C7(64)
op
disp
Figure 4: Picture in Picture (PIP) task graph with
varying bandwidth requirements.
In order to describe all forms of task-graphs, a more ﬂexible representation in the programming language is needed.
A very intuitive way to describe such a graph is to create a
Node ob ject for every task with constraints concerning the
processing element (e.g. PEType, PEQuantity) and deﬁne
the communication connections to its successors with the
corresponding bandwidth requirements. A task graph of the
PIP application is shown in Figure 6 as a constrained X10
representation. We have used PIP and other more complex
applications for our evaluations which are presented later in
this paper.
Figure 5: The code example shows the invasion of
RISC processing element with a throughput of 128
Mb/s to its master, which triggered the invasion.
1 val inp_mem = new Node(""inp_mem"")
2 val hs = new Node(""vs"");
3 val vs = new Node(""vs"");
4 val jug1 = new Node(""jug1"");
5 val inp_mem2 = new Node(""inp_mem2"")
6 val jug2 = new Node(""jug2"");
7 val mem = new Node(""mem"");
8 val op_disp = new Node(""op_disp"");
9 inp_mem.connect(hs, 128);
10 hs.connect(vs, 64);
11 vs.connect(jug1, 64);
12 jug1.connect(mem, 64);
13 inp_mem.connect(inp_mem2, 64);
14 inp_mem2.connect(jug2, 64);
15 jug2.connect(mem, 64);
16 mem.connect(op_disp, 64);
Figure 6: The code representing the task graph of
Figure 4 in X10. It can be converted to a series of
invade calls like in Figure 5.
5. RUN-TIME AND OPERATING SYSTEM
The adaptations of the language and the hardware, require support from the intermediate layers as well. We have
added an additional backend [4] to the X10 compiler and
adapted the run-time system according to language requirements and operating system capabilities. In the context of
CAP, the compiler and run-time system pass on the constraints from the application to the OS. The X10 at construct is the standard communication primitive and is implemented via OS mechanisms which exploit NoC features.
Inside the operating system, an agent-based resource management is introduced. Each application is represented by
an agent. This approach distributes the resource management overhead over the entire system to achieve the scalability required for future many-core architectures, as investigated in [16]. Basic OS functionality is provided by
OctoPOS [18]. The application uses the afore-mentioned
language features (i.e. constraints, local memory allocation
and the invade function call) to inform its agents about its
resource requirements. The agent is then responsible for allocating resources, i.e. assembling a hardware claim that
fulﬁlls the constraints speciﬁed by the application. The allocation of resources and the mapping of tasks to resources
themselves are complex operations, which are performed iteratively. However, the agents consider characteristic properties of the applications while allocating resources. The
agents pessimistically aim at allocating more resources than
necessary for successful mapping of the task graph. This
strategy avoids multiple iterations. The required communication bandwidth are considered and can be obtained from
the constraints provided by the application developer at the
language level. The agents use locally available resource
status information to perform the resource allocation. This
information consists of the available resources obtained from
other agents (e.g. resources that are currently allocated by
another application but not used or actually idle resources)
and the suitability of resources obtained from monitoring
information (e.g. NoC link monitoring) to evaluate the usefulness of resources for the invading application, as detailed
in [13]. Once enough resources have been allocated, an actual mapping of the task graph (obtained from the constraints) to the allocated resources is performed using the
heuristics presented in [20]. In this step, the guaranteed service connections of the NoC (see section 6.1) are set up for
the application to be mapped and the detailed communication constraints are realized. If it is not possible to map the
tasks to the allocated resources, the agent has to allocate
additional resources. Once the task graph has been successfully mapped, the idle cores among the allocated resources
are released and the claim is returned to the application.
Figure 7 summarizes the OS ﬂow.
invade
(Re-)allocate resources
Map tasks
success
Release unused resources
Return Claim
infeasible
Figure 7: Flowchart representing the iterative allocation and mapping process
The agents use the system messages provided by the NoC
to communicate and collect information (see section 6.3).
The individual actions of the agents are executed in the interrupt handler which is triggered on arrival of such a system
message by the NoC. This mechanism allows the agents to
virtually execute in parallel to the application running on
the computational resources and reduces the latency introduced by the distributed architecture.
6. COMMUNICATION HARDWARE
A wormhole packet switching meshed network with virtual channels (VC) [8] is used as a basis. Each physical link
is shared between a predeﬁned number of virtual channels.
Distributed XY routing is used to ensure scalability. A similar NoC is realized in Intel’s Single-chip Cloud Computer [15]
for inter-tile and main memory communication. It is used
later as a reference. However, diﬀerent features have to be
introduced in the NoC to support CAP as detailed now.
6.1 Resource allocation
One ma jor principle of CAP is the exclusive communication resource allocation for an application. This resource
allocation can be enabled by end-to-end connections which
allocate virtual channels exclusively. These connections enable hard guarantees and are thus called Guaranteed Service
(GS) connections. In the past, GS connections have been
mainly used to enable hard guarantees for real-time or safety
critical applications. In the context of CAP, such end-to-end
connections are used to improve performance and reduce
communication overhead on behalf of the application programmer. The proposed NoC realizes a fully decentralized
and scalable resource allocation scheme as detailed in [11].
All virtual channels can be either used by Best Eﬀort (BE)
communication or GS end-to-end connections. Best eﬀort
communication is done in form of packets, each containing
a header, (several) body and a tail ﬂit. To establish end-toend connections, a header ﬂit is injected in the start which
allocates a virtual channel at each link. After connection
setup, body ﬂits are used for communication. A tail ﬂit
is used to release the resources allocated by the end-to-end
connections. OS triggers an end-to-end connection setup
by conﬁguring the memory mapped registers inside the network adapter. If a connection between two tiles exist, it is
used transparently by the NA (e.g. access of tile-external
memories in case of cache misses). More details about this
communication concept are given in [10] and [11].
Figure 8 illustrates a data transmission example. Transmission 1 shows an end-to-end connection setup using a
header ﬂit. An established connection between two tiles
is shown by transmission 2. Due to the ﬂexible and distributed virtual channel allocation scheme, diﬀerent VCs are
allocated at diﬀerent links for transmission 2. Transmission
3 represents a BE packet. BE transmissions allocate communication resources (virtual channels) only for a short time
duration.
V
V
V
C
C
C
3
2
1
essing
le
Processing
Tile
V
V
V
C
C
C
3
2
1
Processing
Tile
V
V
V
C
C
C
3
2
1
Memory
N
A
NoC
Router
T
VC 1
VC 2
VC 3
N
A
N
A
NoC
Router
NoC
Router
H
essing
ile
VC 1
VC 2
VC 3
Memory
H
N
A
NoC
Router
Processing
Tile
Processing
Tile
N
A
N
A
NoC
Router
NoC
Router
1: Setup of GS-Connection
2: Established GS-Connection
3: BE-Packet
H
T
: Head Flit
: Tail Flit
Figure 8: Example of BE- and GS data transmissions within the NoC.
6.2 Data prefetching
Besides communication resource allocation, the CAP concept exploits spatial data locality controlled at the language
level by an advanced data prefetching methodology. It enables explicit control of the fast tile local memory by applications.
To move data between diﬀerent memories of the architecture, hardware support is incorporated. Therefore, a DMA
unit is realized in the network adapter of each tile. It can be
conﬁgured via memory mapped registers. The DMA support improves the performance by oﬄoading the processing
elements from the data transfer overhead. A DMA unit is
restricted to only push data from the local memory of the
tile to a remote memory location. Pull DMAs are emulated
by the OS using push DMA. The beneﬁts of hardware supported DMA transfers are investigated in section 7.2.
6.3 Additional hardware features
Two additional extensions of the NoC hardware are realized to support CAP. System messages are implemented
for fast OS-internal communication. A system message is
initiated by writing to memory mapped registers of the network adapter. At the receiving tile an interrupt is triggered
on arrival of a system message payload. This enables low
latency OS communication.
The second extension of the NoC for system software are
hardware communication monitors. These monitors observe
the communication for a given time period. The current implementation comprises monitors for the link utilization and
the virtual channel utilization, as detailed in [13]. The data
can be accessed and collected eﬃciently using NoC hardware
support [12]. The OS takes these monitoring information
into account during resource allocation and mapping phase.
h264 mms mpeg mwd
pip
vopd
0.7
0.8
0.9
1
R
e
l
a
t
i
e
v
B
d
n
a
w
i
d
t
h
R
u
q
e
i
e
r
m
e
n
s
t
e
r
e
f
e
r
[
1
n
e
c
]
CAP
(a) Communication bandwidth
0
200
400
600
800
1,000
0.3
0.4
0.5
0.6
0.7
BE Injection Rate (Flits/Cycle/Node)*1/1000
R
e
l
a
t
i
e
v
L
a
e
t
n
y
c
e
r
e
f
e
r
[
1
n
e
c
]
h264
mwd
mms
pip
mpeg
vopd
(b) Communication latency
h264 mms mpeg mwd
pip
vopd
10
20
30
40
24.21
21.16
30.23
41.72
20.64
19.02 19.82
29.02
11.68 11.25
9.8
15.84
C
o
m
m
n
u
i
a
c
t
i
n
o
P
o
w
r
e
[
P
a
d
t
a
i
n
W
m
]
CAP
(c) Power consumption
Figure 9: Communication bandwidth, latency and power consumption for six parallel multimedia applications
with CAP and without ().
7. RESULTS
The goal of communication aware programming is to increase the performance and eﬃciency of parallel applications
by improving their communication. Therefore, the impact
of communication resource allocation and data prefetching
is investigated in the following.
7.1 Simulation
A cycle accurate SystemC model of the NoC, presented
in section 6 is used. The instantiated meshed NoC has a
size of 10x10 nodes and uses distributed XY routing. The
routers have a four stage pipeline. Header ﬂits traverse all
of the stages, whereas body and tail ﬂits take only two of
the stages. This behavior reﬂects the real hardware implementation used for our FPGA prototype. Abstracted
behavioral models of processing cores and applications are
used for traﬃc generation. The communication constraints,
discussed in section 4, and the constraint evaluation and
mapping performed by the OS (see section 5) are modeled
abstract in the SystemC simulation framework. The used
task graphs are constrained according to their communication behavior. The mapping decisions and resource allocation are based on the constraints using nearest neighbor
mapping [20]. For the following investigations communication graphs of diﬀerent multimedia applications are used:
Video Ob ject Plan Decoding (VOPD, 12 cores), MPEG4
video decoding (14 cores), Picture-In-Picture (PIP, 8 cores),
and Multi-Windows Display (MWD, 14 cores), presented
in [3], as well as a H.264 CAVLC encoder (H.264, 16 cores)
and a Multimedia System (MMS, 25 cores), provided by
NoCTweak [24]. The reference used in the following is a
BE packet switching NoC with VCs, very similar to the one
realized in the Intel Single-chip Cloud Computer [15].
Figure 9(a) shows the bandwidth required for communication while executing the diﬀerent applications. All results
are relative to the reference. Compared to this reference,
the communication bandwidth can be reduced between 9.8%
for the MPEG decoder and 15.4% for the H.264 encoder.
This reduction is due to a reduced gross data rate resulting
from end-to-end connections. Figure 9(b) shows the communication latency of the applications under diﬀerent load
situations. To generate additional load, uniform random
traﬃc is injected by the nodes of the architecture that are
not used by the investigated application. The results show
that CAP can reduce the data transmission latency by 29%
to 64%. On average delay can be reduced by 47% compared to the reference. The reason for this signiﬁcant improvement is the pre-allocation of communication resources.
This pre-allocation enables low latency communication since
routing and VC allocation is performed in advance to data
transmission (two router pipeline stages are skipped). The
SystemVerilog hardware implementation used for the FPGA
prototype is now taken to estimate the power consumption.
An ASIC synthesis of the NoC was performed using a TSMC
45 nm general purpose standard cell library (tcbn45gsbwpwc )
with worst case operating conditions. For power estimation,
the toggle rates were derived from netlist simulation under
an idle router (Pidle ) is 7.94 mW. Pdata = Ptotal − Pidle is
application speciﬁc NoC load. The power consumption of
used in the following. Figure 9(c) shows the estimated power
consumption for data transmission over the NoC while executing the investigated applications. The results show that
communication aware programming can reduce the power
consumption for communication signiﬁcantly. For the H.264
encoder, PIP and MWD more than 50% of the power which
is directly related to data transmissions (Pdata ) can be saved.
This signiﬁcant reduction of the power consumption has two
reasons: (1) Due to the previous resource allocation, enabled
by end-to-end connections, data transmission is simpliﬁed
because routing and virtual channel allocation are only performed once while connection setup. (2) The protocol overhead of end-to-end connections is reduced and thus the gross
data rate is decreased.
7.2 Prototype
In addition to the simulation results, an FPGA-prototype
of the architecture presented in section 3 was realized. The
prototype has four processing tiles with one Leon3 RISC
core [9] per tile due to the limited amount of resources available on the used ML605 FPGA board. The tiles memory
hierarchy is same as shown in Figure 1(b). Each tile has a
tile local memory of 256 kB, 512 byte L1 data- and instruction cache and a 4 kB L2 cache. One of the four tiles has
a DDR3 memory attached to its internal bus. The DDR
memory is accessible from the other tiles via the NoC.
A parallel version of an integer matrix multiplication was
used to investigate the impact of CAP mechanisms on the
hardware prototype with respect to execution time of the
application, NoC-utilization and NoC power consumption.
The results are given in Figure 10. Five diﬀerent variants of
the matrix multiplication were investigated: Two versions
use BE communication, the other three versions use communication resource allocation by GS end-to-end connections. Each communication variant is investigated as DDR,
where the source matrices are located in the main memory
and PF, where the required parts of the source matrices are
prefetched from the DDR to the tile local memory according
to CAP principles. The GS DMA variant performs prefetching by the use of the hardware DMA unit located inside the
NA. The use of DMA is only investigated in combination
with end-to-end connections due to hardware requirements.
The BE DDR variant is used as a reference. It does not use
any CAP mechanism and could be realized on other architectures, such as the Intel SCC [15].
Figure 10(a) compares the speedup of the diﬀerent variants relative to a single core variant. The results show that
]
l
x
a
i
t
n
e
u
q
e
s
[
p
u
d
e
e
p
S
n
o
i
t
u
c
e
x
E
5
4
3
2
1
BE DDR
GS PF
BE PF
GS DMA
GS DDR
107
106
105
104
]
)
t
i
b
2
3
(
s
t
i
l
F
[
n
o
i
t
a
z
i
l
i
t
u
C
o
N
BE DDR
BE PF
GS DDR GS PF
GS DMA
]
W
m
n
i
a
t
a
d
P
[
r
e
w
o
P
n
o
i
t
a
c
i
n
u
m
m
o
C
BE DDR
BE PF
GS DDR GS PF
GS DMA
0.25
0.2
0.15
0.1
5 · 10
−2
0
32x32
64x64
96x96
128x128
32x32
64x64
96x96
128x128
32x32
64x64
96x96
128x128
(a) Application speedup
(b) NoC utilization
(c) NoC power consumption
Figure 10: Parallel matrix multiplication executed with diﬀerent settings on a 4 tile architecture prototype.
CAP mechanisms (prefetching and end-to-end connections) are compared against a reference (BE DDR).
prefetching has no beneﬁt with respect to execution time
for small matrix sizes due to the fact that all data ﬁt into
the L2-cache. A matrix with 64x64 elements even reaches
speedups higher than four. The reason is the increased overall cache size if four cores are used instead of one. If the matrix sizes become bigger, prefetching improves performance
signiﬁcantly. For a matrix of 128x128 elements, prefetching
introduced by CAP improves performance by 26% compared
to the reference (BE DDR). Figure 10(b) shows the NoC utilization caused by executing the ﬁve variants of the matrix
multiplication. To obtain these numbers, the NoC link monitors (see section 6.3) available on the prototype have been
used. As expected, the amount of communication increases
with the matrix size. For larger matrix sizes, the amount of
ﬂits can be reduced by 26% if resource allocation is used
(GS DDR). DMA prefetching (GS DMA) can reduce the
amount of communication by up to 96% compared to the
reference. The reason is the reduced main memory communication resulting from the use of the tile local memory.
Finally, the power consumption which is directly related to
data transmissions is analyzed for the ASIC implementation of the NoC, as detailed in section 7.1. Figure 10(c)
summarize"
Fort-NoCs - Mitigating the Threat of a Compromised NoC.,"In this paper, we uncover a novel and imminent threat to an emerging computing paradigm: MPSoCs built with 3rd party IP NoCs. We demonstrate that a compromised NoC (C-NoC) can enable a range of security attacks with an accomplice software component. To counteract these threats, we propose Fort-NoCs, a series of techniques that work together to provide protection from a C-NoC in an MPSoC. Fort-NoCs's foolproof protection disables covert backdoor activation, and reduces the chance of a successful side-channel attack by ""clouding"" the information obtained by an attacker. Compared to recently proposed techniques, Fort-NoCs offers a substantially better protection with lower overheads.","Fort-NoCs: Mitigating the Threat of a Compromised NoC
Dean Michael Ancajas Koushik Chakrabor ty Sanghamitra Roy
USU BRIDGE LAB, Electrical and Computer Engineering, Utah State University
dbancajas@gmail.com {koushik.chakrabor ty, sanghamitra.roy}@usu.edu
ABSTRACT
In this paper, we uncover a novel and imminent threat to
an emerging computing paradigm: MPSoCs built with 3rd
party IP NoCs. We demonstrate that a compromised NoC
(C-NoC) can enable a range of security attacks with an accomplice software component. To counteract these threats,
we propose Fort-NoCs, a series of techniques that work together to provide protection from a C-NoC in an MPSoC.
Fort-NoCs’s foolproof protection disables covert backdoor
activation, and reduces the chance of a successful side-channel
attack by ""clouding"" the information obtained by an attacker.
Compared to recently proposed techniques, Fort-NoCs offers
a substantially better protection with lower overheads.
1.
INTRODUCTION
Three emerging technology trends in the modern world
are conspiring to impose massive challenges in secure hardware design. First, the increasing integration of uncore components in a processor such as accelerators and IP cores with
different signaling protocols necessitates the need for an efﬁcient interconnect system. Figure 1 presents the evolution of
commodity multicore server processors. We see a transition
from bus-based crossbars connecting a few cores (e.g. Core
i7) to an on-chip network connecting many cores (e.g. Tilera,
Intel SCC [8, 25]). Second, as Multiprocessor-System-on-Chip
(MPSoC) development grows in complexity and cost, there is
an increasing emphasis on employing third party IP Networkon-Chip (NoC) blocks to connect these components for costreduction purposes [13, 15, 20, 21], also shown in Figure 1.
Even if all other SoC components are designed with the highest security standards, a malicious NoC that has access to
all nodes can wreak havoc to an otherwise secure system.
Lastly, recent trends of using MPSoCs in cloud computing
data centers, expose these systems to new threats as different
co-scheduled applications (secure and non-secure) are forced
to share the underlying hardware resources [24].
In this paper, we uncover a potent threat posed by a third
party NoC in an MpSoC cloud computing setup. We demonstrate that a malicious third party vendor can provide a compromised NoC by embedding a hardware trojan within the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
DAC ’14, June 01 - 05 2014, San Francisco, CA, USA
Copyright 2014 ACM 978-1-4503-2730-5/14/06...$15.00
http://dx.doi.org/10.1145/2593069.2593144.
IP block. Such a trojan can facilitate a range of possible attacks with an accomplice software. For example, an accomplice software component can establish a covert communication with the NoC to snoop on the ongoing data communication through the NoC, thereby stealing classiﬁed information. Many other types of attacks such as voluntary data
corruption or denial of service are also possible. A key challenge then is how to provide secure and reliable communication, when the underlying NoC is compromised.
In the
context of existing works (see Section 6), to the best of our
knowledge, this is the ﬁrst work to explicitly explore the threat
posed by a Compromised NoC (C-NoC).
We propose Fort-NoCs1—a three-layer security mechanism
for an MPSoC system with a potentially compromised communication platform. These security measures are introduced
in the SoC ﬁrmware that interfaces the processing element
(PE) with the network interface (NI) of the NoC. Fort-NoCs
comprises: (1) a lower layer Data Scrambling (DS) that creates
a stiff barrier for activation of a hardware trojan in the NoC;
(2) a middle layer Packet Certiﬁcation (PC) that breaks the
communication link between the untrusted NoC hardware
and its accomplice thread; and (3) a top layer Node Obfuscation (NObf) that decouples the source and destination nodes
of a communication to dramatically increase side-channel resilience. Combined together, our three-layer security mechanisms mitigate the threat of a C-NoC, with minimal powerperformance overhead.
We make the following contributions in this paper:
• We illustrate a new threat model stemming from a CNoC design. This model can be a potent security threat
as third party NoCs become more prominent in lowcost cloud computing on MPSoCs (Section 2).
• We show a detailed design of a C-NoC and evaluate its
design footprint and runtime performance overhead.
Our rigorous analysis demonstrates that it is possible
to realize this threat model with a minimal footprint,
clearly showcasing the potency of the threat model we
uncover in this paper (Section 3).
• We present Fort-NoCs, a holistic layered approach to
harden security on systems with a C-NoC: Node Obfuscation (NObf), Packet Certiﬁcation (PC) and Data Scrambling (DS). Our techniques play complementary roles in
hardware level protection by preventing the two-way
communication between software and the hardware CNoC, and introducing noise in the NoC data communication semantics with negligible overhead on performance and bandwidth of the NoC (Section 4).
1Fort-NoCs is a word play between NoCs and Fort Knox–a
security hardened structure housing US gold deposits.
Designer components
3rd-party IP components
Core 1 Core 2 Core 3 Core 4
Cache Cache Cache Cache
Crossbar Switch
Core
Core
Core
R
R
Core
Core
Core
R
R
Core
Core
Core
R
R
R
R
R
Core
Core
Core
Core
R
R
R
Core
Core
Core
Core
R
R
R
Core
Core
Core
Core
R
R
R
Core
Core
Core
Core
R
R
R
R
R
R
R
bus-based: Core i7, Barcelona
NoC-centric: Tilera, Intel SCC
NoC-centric + 3rd IP NoC
~2000: multicore
~2011: many-core
~2015: cloud scale MPSoC
Time
Figure 1: Cloud Chips Design Evolution.
(


(


 !
M""#$%$&')
*+&,+""P	
E
N	

Z
W
Y
Y
X
T
H
U
H
Figure 2: Compromised NoC snooping data messages between
programs A & B, and leaking to the accomplice program C.
• We analyze our secure design solutions by measuring
the associated power, area, and performance overheads.
Compared to the recently proposed state-of-the-art NoCMPU [18], our proposed Fort-NoCs offers compelling
advantages in power-performance overheads and threat
resilience from a C-NoC (Section 5).
2. THREAT OF A C-NOC
In this section, we outline the security threat posed by a
compromised NoC. We outline the threat model and brieﬂy
discuss the signiﬁcance of this threat in modern software and
hardware practices.
2.1 Threat Overview
Figure 2 shows a conceptual overview of one speciﬁc threat
posed by a compromised NoC: information leak. The security attack must have two complementary hardware and
software components. First, a hardware trojan implanted inside the NoC is responsible for facilitating an attack on the
on-chip communication network. Second, an accomplice application must secretly communicate with the hardware trojan to activate it and send commands. Based on these commands, the trojan can carry out a plethora of potent attacks,
such as information leak and denial of service.
Without loss of generality, we outline the sequence of phases
to realize information leaking on a compromised NoC.
1. Design Phase: In the design phase, a third party NoC IP
provider inserts a hardware trojan in the NoC. The trojan is able to detect and act upon commands, which are
inserted in the ﬂits communicated through the NoC.
Section 3 shows how to insert this trojan in a standard
NoC router with a minimal footprint.
2. Activation Phase: There are two speciﬁc steps in the activation phase. First, an accomplice thread (AcTh) is
scheduled on one of the on-chip processing elements
(PE). This step can be realized in a cloud computing
setup where a client thread is co-scheduled with other
client threads of the service provider. Second, the AcTh
establishes a covert communication channel with the
embedded hardware trojan in the NoC. For example,
the AcTh can write a covert message on its own address
space, and then cause eviction of the cache block containing that message. When a close-by NoC node receives data ﬂits containing that covert message, it sends
an acknowledgment message back to the AcTh. That
NoC node subsequently sends covert messages to other
NoC nodes in the network to activate them, while reporting the location of the AcTh. Once all activation
sequences are received, the trojan in each NoC node
will be waiting for further commands from the AcTh.
3. Operational Phase: During the operational phase, the
AcTh sends commands to initiate a malicious activity.
In the case of information steal, the AcTh may request
duplication of speciﬁc on-going data communication.
For example in Figure 2, an AcTh residing in the local
PE of NoC node X could send a command to the NoC
node W to leak the communication between nodes Y
and Z . The hardware trojan at NoC node W will then
duplicate all packets going in/out of its local PE and
send it to the AcTh at the PE of NoC node X .
4. Tear down Phase: In the tear down phase, the AcTh informs the compromised NoC to suspend its malicious
activity. Similar to activation, this operation can be realized by sending a previously agreed upon message
through a dirty cache block.
2.2 Threat Relevance
The setting of the security attack in this work is a cloud
computing system with many users running different programs on an MPSoC. This threat will be of growing importance as MPSoCs are poised to take over general purpose
processors in cloud computing hosting [13]. We also assume
that for promoting a cost-efﬁcient MPSoC design, the on-chip
interconnect is designed with a 3rd party NoC IP. This is a
very likely scenario as NoC IP blocks continue to ﬁnd their
way in many SoCs.
In fact, iSuppli, an independent market research ﬁrm, has determined that four out of the top
ﬁve Chinese fabless semiconductor OEM companies use the
FlexNoC interconnect from Arteris [21]. Consequently, Arteris has achieved a three-year 1002% sales growth through
IP licensing [20]. Given this trend, we have shown a unique
threat model where the NoC design is compromised. With
an accomplice software, such a C-NoC can engage in a wide
range of malicious activities such as information extraction,
denial of service or voluntary data corruption.
Now a question is can a third-party design a seemingly
innocuous but malicious NoC? To answer this question, we
must understand two central aspects of a C-NoC design. First,
we need to evaluate the footprint of such a design, in terms of
its power and area overhead. Second, we want to analyze the
runtime performance overhead on other applications, while
the NoC is engaged in malicious activities. Ideally, a designer
would like to keep these overheads low to increase the threat
potency. We now delve into these key issues.
3. EVALUATING A C-NOC
In this section, we go into more detail about the design
of a C-NoC (Section 3.1), and evaluate its design footprint
(Section 3.2) and the runtime overhead (Section 3.3).
3.1 Design Overview
Figure 3 shows a classic NoC 4-stage virtual-channel (VC)
input port 1
input port p
route compute
VC 1
VC v
Input Buffers
VC 1
VC v
Input Buffers
Trojan
VC allocator
switch allocator
output port 1
output port p
Figure 3: Compromised NoC with Hardware Trojan.
crossbar switch
router pipeline. The four stages are the input buffers/route
calculation, VC allocation, switch allocation and switch traversal. There are p ports with v virtual channels in each port. A
trojan hardware (HW) that duplicates incoming packets from
the local processing element can be inserted in each or one of
the ports as shown in the ﬁgure. The trojan taps the incoming links from the network interface (NI) and watches out for
covert signals from a possible accomplice thread.
The brain of a trojan is a state machine that has three major states: inactive, waiting and leaking, described in details in
Table 2. There are minor states in between the major states
to ensure that the major state changes are sanctioned by the
AcTh. To change the state of the trojan, a coded sequence of
ﬂits are sent by the AcTh. Although, accidental major state
changes are possible due to random trafﬁc, such an occurrence has an extremely low probability. We show in Equation 1 the probability that m consecutive n-bit sequences can
occur when having a random trafﬁc.
2n (cid:21)m
For m = 5 and n = 32, Pch ange ≈ 10−50 . Both variables can be
customized by the trojan creator according to their needs.
Pch ange = (cid:20) 1
(1)
3.2 Design Footprint
To evaluate the power and area overhead of adding a trojan in an NoC, we modify the RTL of an open-source NoC
Router [1]. We add the logic for the trojan described in this
section. We also assume that the router is used as part of
a mesh topology with 5-input/output ports (4 cardinal directions + 1 local) and 5 virtual channels in each port. We
synthesize the design with the TSMC 45nm library using the
Synopsys Design Compiler. Our results for power and area
overhead are shown in Table 1. Adding the trojan HW yields
only 4.62% and 0.28% overheads in area and power, respectively. This result demonstrates that the footprint added by
the trojan HW is really low and hard to detect.
Metric
Baseline with Trojan Overhead
Area (um2 )
Power (mw)
66400
705.56
69474
707.71
4.62%
0.28%
Table 1: Design footprint of the NoC Trojan Hardware.
3.3 Runtime Overhead
Evaluating the runtime overhead of our proposed model
is critical, as a designer would like to engage in covert activity with low overhead so that its activity remains undetected.
State # Description
Inactive
The trojan is inactive and is waiting for an accomplice thread to communicate.
Waiting An accomplice thread has established communication. The trojan is waiting for further commands.
Leaking The trojan is currently sending duplicate data
to an accomplice thread.
Table 2: States of the Trojan.
AcTh
Theft path
AcTh Accomplice Thread
T Trojan
AcTh
T
Figure 4: Trojan with two possible locations of AcTh. The
length of the theft path depends on where the AcTh is.
Therefore, we perform a rigorous analysis to simulate the effect of a covert theft on the network trafﬁc.
We use the gem5+garnet simulator [3] with PARSEC [2]
benchmarks to simulate execution on an 8x8 mesh topology.
We then add our trojan model that duplicates every packet
inserted at the target node and redirects it to the AcTh. Finally, we measure the average packet latency increase due
to the additional trafﬁc caused by duplicating packets. In a
large NoC, the relative position of the target node and the
malicious node, where the AcTh is scheduled, can vary substantially. Figure 4 shows two of the many possible theft
paths. Depending on this relative position, the overall impact on network latency can change substantially. Thus, we
analyze all possible theft paths across all benchmarks to evaluate the runtime overhead.
Figure 5 shows the probability distribution function (PDF)
of the average latency increase versus the number of possible theft paths. On an average, 83% of the theft paths have
a 0% overhead, 14% have between 0-5% performance overhead and only 0.25% of the theft paths have an overhead of
more than 10%. These results show that this type of a covert
theft can occur with a very small impact on the network latency when the longer theft paths are avoided. Given this result, it is clear that a compromised NoC poses a potent threat
to MPSoCs in the near future. We now outline our approach
to protect against this threat model.
4. MITIGATING THE THREAT OF A C-NOC
In this section, we describe our proposed design solutions
to prevent covert data theft by a compromised NoC. Our solutions introduce security measures in the SoC ﬁrmware that
interfaces the processing element with the network interface
(NI) of the NoC. We show in Figure 6 the design methodology used by SoC integrators. We assume two security levels:
a trusted in-house design team hired by the SoC integrator
and one or more untrusted 3rd IP vendor(s). Once the inhouse design team and the IP vendors agree on what protocol to be used, the design team can then independently develop its own trusted interface to the IP NoC chip. We assume the NIs use the Open-Core Protocol (OCP) [16] for interfacing on-chip components. Hence, Fort-NoCs’s techniques
do not depend on 3rd party vendors, including the NoC IP provider,
and are protected from malicious alterations by them.
blackscholes 
fluidanimate 
bodytrack 
swaptions 
canneal 
vips 
ferret 
x264 
100% 
)
s
h
t
a
P
t
f
e
h
T
f
o
%
(
n
o
i
t
u
b
i
r
t
s
i
D
90% 
80% 
70% 
60% 
50% 
40% 
30% 
20% 
10% 
0% 
-0.25% 0.00% 0.25% 0.50% 0.75% 1.00% 1.25% 1.50% 1.75% 2.00% >2.00% 
Average Packet Delay Increase 
Figure 5: Runtime Performance Overhead. The ﬁgure shows
overhead distribution across all possible theft paths.
Core
Core
In-house Design Team
OCP Interface
Request Response
IP Vendor
On-Chip Interconnection Network
 
 
 
 
 
any speciﬁc routing path, recovering meaningful sensitive
information from an application becomes almost impossible.
4.3.2 Implementation
NObf is implemented in the SoC ﬁrmware by initiating
a routine seamless migration of the running application to
another node in the system. Of course, there are various
PEs in the MPSoC, and care must be taken while performing these migrations so that matching PEs are selected when
a thread is migrated. To this end, SoC ﬁrmware maintains
the type of PEs, and uses such information for performing
NObf. The overhead of NObf comes from moving the architected state (processor registers) and cold cache effects. For
practical intervals between two migrations, our architectural
simulations show these effects to be low for our benchmarks.
More details on the implementation is given in Section A2.
4.3.3 Efﬁcacy Estimation
To estimate the efﬁcacy of our proposed NObf, we utilize
the recent work by Kocher et al. to transform this problem
into a signal-to-noise (SNR) estimation. Though noise injection to the signal does not provide theoretical security, it
increases the attacker ’s effort of extracting secret keys [19].
Kocher et al. has shown that decreasing the SNR of the sidechannel information can linearly or quadratically increase
the number of inputs required for a successful side-channel
analysis [12]. Hence, we analyze the disturbance introduced
by NObf to the original data in the network. Our experiment involves computing the amount of original data (i.e.
signal) communicated over a given routing path, compared
to unrelated data communicated over the same path. Given
a scheduling quanta Q, the time before NObf migrates a running application to a different node as P (termed as Singular
Vulnerability Period (SVP)), and the number of threads N , the
SNR is given by the Equation 2. The derivation of the formula can be found in Section A3.
SNRNOb f =
P × l Q
Q − P × l Q
PN m
PN m
(2)
5. EXPERIMENTAL RESULTS
In this section, we present the area, power and performance
overheads incurred by adding our proposed FortNoCs. We
have also observed scalable side channel resilience of our
proposed schemes (presented in detail in Section A6). Our
baseline is a traditional NoC without any security features.
We also compare Fort-NoCs against state-of-the-art related
works in preventing information leakage. We compare with
the NoC-MPU scheme by Porquet et al. that prevents unauthorized read and memory accesses at each NI [18]. Section
A5 outlines our implementation of the NoC-MPU.
5.1 Implementation
We evaluate our proposed designs on several metrics such
as power, area and performance. To obtain power and area
overheads, we use the open-source Stanford Verilog model of
a modern NoC router as our baseline [1]. We insert the trojan
on top of a virtual channel router to implement the functionality described in Section 3. The NoC router model that we
use has a 3-stage speculative pipeline composed of RC/VC
allocate, switch allocation and switch traversal. To evaluate
the performance overhead, we model DS, PC and NObf in
our gem5+garnet setup [3]. These modules are interfaced
during packet dispatch and reception. We then simulate realworld PARSEC benchmarks using full-system simulation.
5.2 Area and Power
Table 3 shows the overhead of adding security features in
a standard SoC OCP interface [7]. NObf is excluded from
the table as it doesn’t change the OCP interface. Fort-NoCs’s
schemes show lower overhead compared to the NoC-MPU.
Between Fort-NoCs’s two schemes, PC has about 15× lower
area overhead compared to DS. The large area overhead of
DS is due to the shifters needed to transform the data bits
during the packetization and depacketization stages. PC has
no power overhead as it needs minimal logic to attach a tag
on the unused ﬁeld of the head ﬂit. NoC-MPU has a large
overhead compared to Fort-NoCs because of the logic to check
if each access is authorized. This involves checking each entry of a content-addressable-memory (CAM) like the PLB to
see if a thread has read/write access to a memory location.
Metric
Baseline
PC
DS
NoC-MPU
Area (um2 )
Power (mw)
4917
1.705
+0.34% +9.57% +42.00%
+0%
+5.08% +64.29%
Table 3: Overhead of Security Schemes.
5.3 Performance
Fort-NoCs provides a three-layer security to a system with
a compromised communication platform. As such, we evaluate the incremental effect on the performance overhead of
adding each layer of security. This trade-off analysis will allow designers to conﬁgure Fort-NoCs based on the type of
security required by the application or design. For instance,
high-security domains can use Fort-NoCs’s three layers at
the cost of higher performance penalty while low-security
domains will probably need only 1 or 2 layers of security.
Figure 7 shows the performance overhead of Fort-NoCs.
We show the incremental overhead of adding each layer of
protection. All Fort-NoCs techniques have low overheads.
On an average, DS adds 3.8% overhead, PC adds 2%, while
NObf adds 0.01% increase in latency. The reason NObf adds
minimal overhead on an average is that some of the threads
experience degradation, while some experience improvement
due to application-dependent trafﬁc patterns. This correlation between thread location and performance was also observed by Missler and Jerger [14]. Between DS and PC, PC
has a smaller overhead because the size of the certiﬁcate to
be checked and decrypted is much smaller compared to the
size of the data ﬂits. Combining the ﬁrst two layers’ security
yields 5.8% overhead. Fort-NoCs’s overhead (3-layers) is at
5.9%. NoC-MPU’s overhead is larger (8.03%) because when
an access to the PLB misses, it needs to wait for a PLB reﬁll
before authenticating the current transaction. On an average,
Fort-NoCs’s 3-layer security system has 26% less overhead
compared to NoC-MPU.
6. RELATED WORK
A vast amount of recent works focus on hardware and software security, primarily to address a problem of growing importance. We focus our discussion on NoC security, which is
most relevant to our work.
b l a
c
k
s c
h
o l e s
o
d
b
y t r a
c
k
c
a
n
n
e
a l
f e r r e t
f l u i d
a
n i m a t e
a
s w
p t i o
n
s
v i p
s
x
2
6
4
a
v
e r a
g
e
P
e
r
e
c
n
t
e
g
a
O
e
v
r
h
a
e
d
0
5
10
15
DS DS_PC DS_PC_NObf
NMPU
Figure 7: Performance Overhead
The current state-of-the-art in NoC security revolves around
protecting information traveling in the network against side
channel, physical and software attacks. Table 4 presents a
high-level comparison of existing works on NoC security based
on four major factors. Similar to software protection mechanisms, many existing works provide access control by monitoring the memory addresses (e.g., Data Protection Unit (DPU)
proposed by Fiorin et al. [5], access control on memory banks
by Diguet et al. [4]). Some proposals aim to use encrypted
data transmission over the NoC (e.g., [6, 9]) or partition the
NoC into separate zones based on trust [23, 24]. In the industry, ARM has approached trusted computing through its
TrustZone platform [22]. The idea is to establish secure and
non-secure states throughout the chip. By changing the security mode of a component along with safety checks, information leaking from secure to non-secure areas are prevented.
Our work in this paper explores a new threat model orthogonal to these works. First, we assume that the hardware trojan is embedded in the NoC itself, whereas others
assume a trustworthy NoC. Second, we demonstrate that information can be extracted from the NoC without relying
on memory access (either through on-chip cache or off-chip
memory). Third, we assume the presence of an accomplice
software thread, which acts as the orchestrator of the attack.
The hardware-software coalition in our threat model is similar to the Illinois Malicious Processor (IMP) [11]. However,
the IMP does not consider the threat of a C-NoC.
Trojan Loca
ISb
Protc
TMd
Fort-NoCs
NoC
NoC
NI
S/W
DPU [5]
S/W
Mem
NI
—
KeyCore [6]
S/W
Mem
NI
—
surfNoC [24]
S/W
S/W
NoC —
AE [9]
S/W
Mem
NI
—
IMP [11]
µP
µP/Mem —
S/W
NoC-MPU [18]
S/W
Mem
NI
a The part of the system where a trojan is inserted.
b The part of the system where the information is stolen.
c The part of the system where a protection mechanism is
implemented to prevent an attack.
d The triggering mechanism in case of a hardware trojan.
Table 4: Comparison of Threats in NoCs.
—
7. CONCLUSIONS
NoCs designed by third-party entities may contain IP blocks
with hardware trojans. Such C-NoCs can coordinate an attack with an accomplice software thread to pose a potent
threat for the emerging computing platforms. We propose
Fort-NoCs, implemented by augmenting the SoC ﬁrmware,
that mitigates the threat of a C-NoC through a layered approach. Through a rigorous evaluation methodology, we demonstrate compelling advantages of Fort-NoCs over existing schemes.
Acknowledgments
We thank Jason Allred for the discussion on the initial version of this work. This work was supported in part by National Science Foundation grants CNS-1117425, CAREER-1253024, CCF-1318826 and donations from the Micron Foundation. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s)
and do not necessarily reﬂect the views of the NSF.
8. "
NoC-Sprinting - Interconnect for Fine-Grained Sprinting in the Dark Silicon Era.,"The rise of utilization wall limits the number of transistors that can be powered on in a single chip and results in a large region of dark silicon. While such phenomenon has led to disruptive innovation in computation, little work has been done for the Network-on-Chip (NoC) design. NoC not only directly influences the overall multi-core performance, but also consumes a significant portion of the total chip power. In this paper, we first reveal challenges and opportunities of designing power-efficient NoC in the dark silicon era. Then we propose NoC-Sprinting: based on the workload characteristics, it explores fine-grained sprinting that allows a chip to flexibly activate dark cores for instantaneous throughput improvement. In addition, it investigates topological/routing support and thermal-aware floorplanning for the sprinting process. Moreover, it builds an efficient network power-management scheme that can mitigate the dark silicon problems. Experiments on performance, power, and thermal analysis show that NoC-sprinting can provide tremendous speedup, increase sprinting duration, and meanwhile reduce the chip power significantly.","NoC-Sprinting: Interconnect for Fine-Grained Sprinting in the
Dark Silicon Era
Jia Zhan* , Yuan Xie* , Guangyu Sun‡
*The Pennsylvania State University, {juz145,yuanxie}@cse.psu.edu
‡Peking University, gsun@pku.edu.cn
ABSTRACT
The rise of utilization wall limits the number of transistors that can
be powered on in a single chip and results in a large region of dark
silicon. While such phenomenon has led to disruptive innovation
in computation, little work has been done for the Network-on-Chip
(NoC) design. NoC not only directly inﬂuences the overall multi-core
performance, but also consumes a signiﬁcant portion of the total chip
power.
In this paper, we ﬁrst reveal challenges and opportunities
of designing power-efﬁcient NoC in the dark silicon era. Then
we propose NoC-Sprinting: based on the workload characteristics,
it explores ﬁne-grained sprinting that allows a chip to ﬂexibly
activate dark cores for instantaneous throughput improvement.
In
addition, it investigates topological/routing support and thermal-aware
ﬂoorplanning for the sprinting process. Moreover, it builds an efﬁcient
network power-management scheme that can mitigate the dark silicon
problems. Experiments on performance, power, and thermal analysis
show that NoC-sprinting can provide tremendous speedup, increase
sprinting duration, and meanwhile reduce the chip power signiﬁcantly.
Categories and Subject Descriptors: C.2 [Computer-Communication
Networks]: Network Architecture and Design
General Terms: Performance, Design
Keywords: Network-on-Chip, Dark Silicon, Computational Sprinting
1 Introduction
The continuation of technology scaling leads to a utilization wall
challenge [21]:
to maintain a constant power envelope,
the
fraction of a silicon chip that can be operated at full frequency is
dropping exponentially with each generation of process technology.
Consequently, a large portion of silicon chips will become dark or dim
silicon, i.e., either idle or signiﬁcantly under-clocked. However, most
previous work focuses on energy-efﬁcient core/cache design while the
impact of on-chip interconnect is neglected. In fact, Network-on-chip
(NoC) plays a vital role in message passing and memory access that
directly inﬂuences the overall performance of many-core processors.
Moreover, network components dissipate 10% - 36% of total chip
power [8, 15, 20]. Therefore, how to design the interconnection network
is critical to tackle the challenges of multicore scaling in the dark
silicon age.
Recently, Raghavan et al. [17] proposed computational sprinting, in
which a chip improves its responsiveness to short-burst of computations
through temporarily exceeding its sustainable thermal design power
(TDP) budget.
All
the cores will be operated at
the highest
Zhan and Xie were supported in part by NSF 0905365 and by the Department
of Energy under Award Number DE - SC0005026. Sun was supported by NSFC
(No. 61202072), 863 Program of China (No. 2013AA013201), and AMD grant.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee. Request permissions
from Permissions@acm.org.
DAC ’14, June 01 - 05 2014, San Francisco, CA, USA
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2730-5/14/06 ...$15.00
http://dx.doi.org/10.1145/2593069.2593165.
frequency/voltage to provide instant throughput during sprinting, and
after that the chip must return to the single-core nominal operation
to cool down. While such mechanism sheds light upon how “dark”
cores can be utilized for transient performance enhancement, it exposes
two major design issues: First, the role of interconnect is neglected.
NoCs consume a signiﬁcant portion of chip power when all cores are
in sprinting mode. When switching back to the nominal mode, only
a single core is active. However, the network routers and links cannot
be completely powered down, otherwise a gated-off node would block
packet-forwarding and the access of the local but shared resources
(e.g., cache and directory). As a result, the ratio of network power
over chip power rises substantially and may even lead to higher NoC
power than that of the single active core. Second, the mode-switching
lacks ﬂexibility and only provides two options: nominal single-core
operation and maximum all-core sprinting. Depending on the workload
characteristics, an intermediate number of active cores may provide the
optimal performance speedup with less power dissipation.
To address these two issues, we propose ﬁne-grained sprinting,
in which the chip can selectively sprint to any intermediate stages
instead of directly activating all the cores in response to short-burst
computations. The optimum number of cores to be selected depends
on the application characteristics. Scalable applications may opt to a
large number of cores that can support highly parallel computation,
whereas other applications may mostly consist of sequential programs
and would rather execute on a small number of cores. Apparently,
ﬁne-grained sprinting can ﬂexibly adapt to a variety of workloads. In
addition, landing on intermediate sprinting stages can save chip power
and slow down the heating process by power-gating the remaining
inactive on-chip resources, which is capable of sustaining longer sprint
duration for better system performance.
Fine-grained sprinting opens up an opportunity to better utilize the
on-chip resources for power-efﬁciency, but it also poses challenges
on designing the interconnect backbone.
Inherently it incurs three
major concerns:
(1) how to form the topology which connects the
selected number of cores during sprinting when dark cores and active
cores co-exist?
(2) how to construct a thermal-aware ﬂoorplan
of
the on-chip resources (cores, caches, routers, etc.)
for such
sprinting-based multicores? (3) what would be an appropriate NoC
power-management scheme? To answer these questions, we propose a
topological sprinting mechanism with deadlock-free routing support,
and a fast heuristic ﬂoorplanning algorithm to address the thermal
problem during sprinting. Moreover, this sprinting scheme naturally
enables power gating on network resources in the dark silicon region.
In summary, we propose NoC-sprinting, which provides
topological/routing support for ﬁne-grained sprinting and employs
network power-gating techniques for combating dark silicon. Overall,
this paper makes the following contributions:
• Explores challenges and opportunities of designing NoC in the
dark silicon era, from the perspectives of both performance and power.
• Investigates the pitfalls of the conventional all-core sprinting
which fails to fulﬁll diverse workload characteristics, and proposes
ﬁne-grained sprinting for better power-efﬁciency.
• Proposes NoC support to enable ﬁne-grained sprinting, including
topology construction, routing, ﬂoorplanning, and power management.
• Conducts thermal analysis to evaluate how NoC-sprinting
correlates with the sprint duration.
2 Challenges and Opportunities
dynamic power in some cases.
Dark Silicon and Computational Sprinting. Conventionally in
multi-core scaling, the power gain due to the increase of transistor
count and speed can be offset by the scaling of supply voltage
and transistor capacitance. However,
in today’s deep sub-micron
technology, leakage power depletes the power budget. We cannot
scale threshold voltage without exponentially increasing leakage.
Consequently, we have to hold a constant supply voltage, and hence
produce a shortfall of energy budget to power a chip at its full
frequency. This gap accumulates through each generation and results
in an exponential increase of inactive chip resources — Dark Silicon.
Instead of shrinking the chip or sacriﬁcing transistor density,
computational sprinting [17] embraces dark silicon by leveraging the
extra transistors transiently when performance really counts. Special
phase change materials should be used as heat storage to support such
temporary intense sprinting, leveraging the property that temperature
stays constant during the melting phase of the material. Figure 1
demonstrates the nominal single-core operation as well as the sprint
mode for a 16-core system. The temperature rises from the ambient
environment when the sprint starts at ts print , and then extra thermal
energy is absorbed by the melting process of the phase change material,
which keeps the temperature at Tmelt . After the material is completely
melted,
the temperature rises again until Tmax where the system
terminates all but one core (tone ) to sustain the operation. The system
starts to cool after all work is done at tcool . Note that numbers in the
curve mark different sprint phases and will be analyzed in Section IV.
Figure 1: During nominal operation, only one core is active under the TDP constraint,
whereas the rest cores are dark silicon. In sprinting mode, all the cores will be activated to
provide instantaneous throughput.
Conventional computational sprinting still focuses on computation,
whereas the role of interconnect is neglected. Here we demonstrate two
key challenges that require careful consideration when designing NoC
for sprinting-based multicores in the dark silicon age.
NoC Power Gating. Power gating is an efﬁcient power-saving
technique by completely shutting down the idle cores to reduce leakage.
However, as more and more cores turn “dark”, the network components
such as routers and links also become under-utilized. As mentioned,
NoC dissipates 10% - 36% of total chip power [8, 15, 20]; additionally,
the more cores become dark, the larger the ratio of network power over
the total chip power. This observation also points out the ﬂaw of the
conventional computational sprinting which turns off all but one core
during nominal operation, while neglecting the impact of NoC.
To give a brief overview of network power, we simulate a classic
wormhole router with a network power tool DSENT [19]. The ﬂit width
is 128 bits. Each input port of a router comprises two virtual channels
(VC) and each VC is 4-ﬂit deep. The power value are estimated with
an average injection rate of 0.4 ﬂits/cycle. Figure 2 shows the router
power breakdown when varying the operating voltage (1v, 0.9v, 0.75v)
and frequency (2GHz, 1.5GHz, 1.0GHz) under 45 nm technology. We
can see that leakage power contributes a signiﬁcant portion to the total
network power. In addition, the ratio of leakage power increases as we
scale down the supply voltage and frequency, and even exceeds that of
Figure 2: Router power breakdown (dynamic power vs leakage power) when varying the
operating voltage and frequency.
Sprinting-based multicores activate a single core during nominal
operation whereas the rest are turned off. Figure 3 shows the chip
power breakdown when scaling the number of cores based on the
Niagara2 [16] processor. We evaluate the power dissipation with
McPAT [13] for cores, L2 caches, memory controllers (MC), NoC, and
others (PCIe controllers, etc.). We assume idle cores can be gated-off
(dark silicon) while other on-chip resources stay active or idle.
Figure 3: Chip power breakdown during nominal operation (single active core) in
sprinting-based multicores. The percentages denote the component power (core, cache,
NoC, MC, and others) over the total chip power.
As shown in Figure 3, NoC accounts for 18%, 26%, 35%, and 42%
of chip power respectively for 4-core, 8-core, 16-core, 32-core CMP
chips when they are operating at nominal mode. In contrast, the power
ratio for the single active core keeps decreasing as the “dark silicon""
grows. Therefore in this scenario, it is inappropriate to only measure
core power when power budget is the design limitation.
NoC power gating is heavily dependent on the trafﬁc.
In order
to beneﬁt from power gating, an adequate idle period (namely,
“break-even time"") of routers should be guaranteed to make sure they
are not frequently woken up and gated off. Recently researchers have
proposed various schemes [4, 5, 14, 18] to mitigate the latency overhead
caused by frequent router wake-up. However, these techniques do not
account for the underlying core status and will result in sub-optimal
power gating decisions.
Workloads-Dependent Sprinting. A straightforward sprinting
mechanism is to transiently activate all the dark cores at once. However,
this scheme fails to explore the sporadic workload parallelism and thus
may waste power without sufﬁcient performance gain, especially for
multi-threaded applications that have various scalability. Here we use
PARSEC 2.1 [2] as an example. We simulate CMP systems using
gem5 [3] and observe the performance speedup when varying the core
count . For clarity, Figure 4 selects a few results that can represent
different workload characteristics.
Figure 4: Execution time of running PARSEC benchmarks when increasing the number of
available cores.
The detailed evaluation methodology is described in Section IV.
As shown in Figure 4, some benchmarks (e.g. blackscholes and
bodytrack) achieve signiﬁcant performance speedup as the number
of cores increases.
In contrast, for freqmine, the execution time is
almost identical at different conﬁgurations, which implies its serial
program beneﬁts little from the extra cores.
In addition, there are
some applications (e.g.
vips and swaptions) that achieve obvious
speedup as the core count increases within a small range but then
slow down gradually, and further suffer from delay penalty after
exceeding a certain number. This is because adding more cores than
required by the application parallelism may incur signiﬁcant overheads
that may offset and even hurt performance. The overheads include
thread scheduling, synchronization, and long interconnect delay due
to the spread of computation resources. Therefore, for sprinting-based
multicores, activating all the dark cores is not a universal solution for
all applications.
3 Our Method: NoC-Sprinting
As illustrated above, an efﬁcient sprinting mechanism should be able to
provide different levels of parallelism desired by different applications.
Depending on workload characteristics, the optimal number of cores
required to provide maximal performance speedup varies. This also
raises challenges in designing a high performance and low power
interconnect to support the sprinting process.
3.1 Fine-Grained Sprinting
We ﬁrst propose ﬁne-grained sprinting, a ﬂexible sprint mechanism
that can activate a subset of network components to connect a certain
number of cores for different workloads.
Speciﬁcally, during execution, the CMP system may experience a
short burst of computation due to the abrupt ﬂuctuation of a running
program. As such, the system will quickly react to such intense
computation and determine the optimal number of cores that should be
offered for instantaneous responsiveness. Then the system will activate
the required number of cores while the others remain “dark"". There are
some existing work [6, 12] on adapting system conﬁgurations like core
count/frequency to meet runtime application requirements. Since our
focus is on how to design interconnect under such circumstances, we
assume that these application parallelism can be learnt in advance or
monitored during run-time execution.
3.2
Irregular Topological Sprinting and Deadlock-Free Routing
Under the nominal operation, only a single core (namely master core)
remains active. There are different choices of placement for the master
core. We list a few examples here, but real implementations should not
be limited by these mentioned conditions. Firstly it could be placed
in the center of the chip to reduce the transmission latency for thread
migration. Another example is to select the core running the OS as
the master core since it is always activated. The core next to the
memory controller is also a good candidate if the application generates
intensive memory accesses. Without loss of generality, we choose the
top-left corner node as the master node which is closest to the memory
controller, i.e., Node 0 as shown in Figure 5a.
(a) Logical connection of a 16-node
mesh network. The irregular topology
and convex DOR routing.
(b) Physical allocation for the original
network in (a). Only links for 4 nodes are
shown for clarity.
Figure 5: Topology, routing, and ﬂoorplan for ﬁne-grained sprinting
After the system transfers to the sprinting mode, a number of cores
will be activated and keep running for a short duration. Fine-grained
sprinting requires topological support from the following aspects:
• Pay-as-you-go: ﬁne-grained activation of any number of cores.
• Short communication delay between different nodes, especially to
the master node where the memory controller resides.
• Routing should be simple and deadlock-free which does not incur
signiﬁcant control complexity or hardware overhead.
To achieve these goals, we propose to start from the master node,
and connect other nodes to the network in ascending order of their
Euclidean distances to the master node. For example, the red nodes
in Figure 5a demonstrate the topology of a 8-core sprinting. Note
that we use Euclidean distances instead of Hamming distances here.
The latter may guarantee a shortest routing distance between the
newly-added node to the master node, but would generate longer
inter-node communication to other nodes. For example, both cases
would choose node 0, 1, and 4 as 3-core sprinting. But if 4-core
sprinting is triggered, the method with Hamming distance may possibly
choose node 2 whereas the method with Euclidean distance would
generate a better choice by accommodating node 5. Algorithm 1
generates the order of N nodes used for topological sprinting.
ALGOR ITHM 1 . Irregular Topological Sprinting
Result: A linked-list L of routers to be activated
Initialize: D[i] = 0, i = 0, 1, 2...N − 1. The coordinate for Rk is (xk , yk ).
for i ← 1 to N − 1 do
D[i] = (cid:112)(xi − x0 )2 + (yi − y0 )2 ;
end
Sort R[i](i = 0, 1...N − 1) in ascending order of D[i](i = 0, 1...N − 1) and put them
into a linked-list L. Break ties according to the order of indexes.
The ﬁne-grained sprinting process will generate an irregular network
topology to connect active cores. Meanwhile, it guarantees that chosen
nodes would form a convex set in the Euclidean space,
i.e.,
the
topology region contains all the line segments connecting any pair
of nodes inside it. Flich et al. [7] proposed a distributed routing
algorithm for irregular NoC topologies but their algorithm requires
twelve extra bits per switch. Adapted from their approach, we extend
the Dimension-Order-Routing (speciﬁcally, X-Y routing) algorithm for
such convex topologies (CDOR). Speciﬁcally, two connectivity bits
(Cw and Ce ) are leveraged to indicate whether a router is connected
to its western or eastern neighbors. As in conventional DOR, we
assume that X and Y coordinates of the ﬁnal destination are stored in
the packet header (Xd es and Yd es ), and each switch knows its X and
Y coordinates (through Xcur and Ycur registers at each switch). The
origin of the coordinate system is located at the top-left corner of the
2D mesh. Messages are routed from the current router to the destination
router, according to the offsets of coordinates and the two connectivity
bits per router. Figure 5a shows a routing path from the source to its
destination. The detailed routing algorithm is described in Algorithm 2.
Furthermore, Figure 6 depicts the routing logic design, which includes
two comparators per switch and the routing circuit for computing
the North port. The routing logic for other ports can be designed
similarly based on Algorithm 2. We implemented CDOR on behavioral
Verilog. Synthesized results using Synopsys Design Compiler (45nm
technology) show that it adds less than 2% area overhead compared to
a conventional DOR switch.
ALGOR ITHM 2 . Convex Dimension Order Routing (CDOR)
if Xd es > Xcur and Ce = 1 then
output_port = east;
else if Xd es < Xcur and Cw = 1 then
output_port = west;
else if Yd es > Ycur then
output_port = north;
else if Yd es < Ycur then
output_port = south;
else
end
output_pot = local;
DOR (such as deterministic X-Y routing) is deadlock-free because
some of the turns are eliminated such as SE, NW, NE, and SW (S, W,
(a) Two comparitors per switch
(b) Routing logic of the North port
Figure 6: Routing logic for convex DOR algorithm
N and E represent south, west, north and east, respectively). In our
CDOR, although the NE turn may happen, it is deadlock-free. For
example, as shown in Figure 5a, a NE turn happens at Node 5 but
this also indicates the east output port of its southern neighbor 9 is
not connected. Therefore a WN turn cannot happen and thus eliminate
a cycle that may cause a deadlock.
3.3 Thermal-Aware Floorplanning
The key design constraint of ﬁne-grained sprinting is the thermal
design power (TDP). The above topological sprinting process does
not consider thermal behavior to simplify control and routing logic.
Therefore, here we propose a design-time ﬂoorplanning algorithm that
can be seamlessly integrated with the topological sprinting process
while providing better thermal distribution to avoid hot spots. Thus, it
sustains a longer sprint duration by slowing down the heating process.
Consider a 4-core sprinting in a 16-node mesh network as shown in
Figure 5a. We may opt to choose the top-left four nodes for better
performance, but alternatively may prefer the four scattered corner
nodes from the thermal point of view. To overcome this dilemma, we
still maintain the original logic connectivity of the mesh network in
consistent of the topological sprinting process, but propose a heuristic
algorithm that reallocates the physical location of each node.
As shown in Algorithm 3, our ﬂoorplanning algorithm treats the
2D mesh network as a graph, and allocates the nodes based on the
list generated from Algorithm 1. In our annotations, G represents the
original logical network, S contains the set of nodes that have already
been explored in G, G(cid:48) represents the physical ﬂoorplan, and S(cid:48) is the
corresponding set of occupied nodes in G(cid:48) . At each iteration, it picks
up a node Rk in G − S, and maps it to a node in G(cid:48) − S(cid:48) that has the
maximum weighted sum of Euclidean distances to all the nodes in S(cid:48) for
the optimal thermal distribution. This process is described in Function
MaxWeightedDistance as in Algorithm 4. Note that the weight of a
distance is inversely proportional to the Hamming distance between Rk
and the node in S. The rationale behind this scheme is that, the longer
the Hamming distance in logical connectivity, the less chance these two
nodes would be selected together during sprinting and accumulate heat
dissipation, thus they can be placed closer in the physical ﬂoorplan.
ALGOR ITHM 3 . Thermal-aware heuristic ﬂoorplanning algorithm
Result: Positions for all nodes
Initialize: Original ﬂoorplan f : {R0 , R1 ... RN−1 }. Transformed ﬂoorplan f (cid:48) : {R(cid:48)
0 , R(cid:48)
... R(cid:48)
N−1 }. The coordinate for Rk or R(cid:48)
k is (xk , yk ).
Set S = φ , S(cid:48) ={R(cid:48)
0 ,R(cid:48)
1 ... R(cid:48)
N−1 }. Queue Q = φ . List L from Algorithm 1
Goal: Find the mapping Pos() from f to f (cid:48) .
Pos(R0 ) = 0(Master Node); Put R0 in S; Delete R(cid:48)
0 from S(cid:48) ;
Put all unexplored adjacent nodes of R0 into Q based on List L;
1
while Q (cid:54)= φ do
Pos(Rk ) = MaxWeightedDistance(S, S(cid:48) , Rk );
Rk = Q[0]; Delete Q[0] from Q;
Delete R(cid:48)
Pos(Rk ) from S(cid:48) ; Put Rk in S;
Put all unexplored adjacent nodes of Rk into Q based on List L;
end
The ﬂoorplanning algorithm frees the sprinting process and routing
algorithm from the thermal concern, i.e., only the logical connectivity
of mesh network needs to be considered during topological sprinting.
Figure 5b shows the ﬁnal ﬂoorplan of the physical network and
only links for four-core sprinting are shown for clarity. Note that
the ﬂoorplanning algorithm will increase the wiring complexity and
generate long links. A standard method of reducing delay of long
wires is to insert repeaters in the wire at regular intervals. Recently,
ALGOR ITHM 4 . MaxWeightedDistance(S, S(cid:48) , Rk )
Initialize: Sum = 0; Max = 0;
for every node R(cid:48)
i in S(cid:48) do
for every node R j in S do
wi j = 1/(|xk − x j | + |yk − y j |);
(xi − xPos(R j ) )2 + (yk − yPos(R j ) )2 ;
(cid:113)
di j =
si j = wi j ∗ di j ;
end
end
Sum = ∑ si j ;
if Sum > Max then
Max = Sum; Pos(Rk ) = i;
end
Return Pos(Rk );
Krishna et al. [11] have validated such clockless repeated wires that
allow multi-hop traversals to be completed in a single clock cycle.
3.4 Network Power Gating
With our proposed ﬁne-grained sprinting, the network power gating
scheme becomes straightforward.
Since the topological sprinting
algorithm activates a subset of routers and links to connect the active
cores, we gate off the other network components as shown in the shaded
nodes of Figure 5a. Moreover, the proposed CDOR algorithm routes
packets within the active network and thus avoids unnecessary wakeup
of intermediate routers for packet forwarding. This further increases
the idle period of the dark region for longer power gating.
However, we still need to consider the Last-Level-Cache (LLC)
architecture for network power gating. For private per-core LLC,
centralized shared LLC, or distributed shared LLC connected with
a separate network (NUCA), our power gating mechanism works
perfectly without the need for any further hardware support. However,
for tile-based multicores where each tile comprises of a shared bank
of LLC, there may be some packet accesses to dark nodes for cache
resources. Therefore, some complimentary techniques such as bypass
paths [4] can be leveraged to avoid completely isolating cache banks
from the network. We accommodate this method in our design.
4 Architectural Evaluation
We use the gem5 [3] full system simulator to setup a sprinting-based
to model a 4 × 4 mesh network and DSENT [19] for network power
multicore architecture with 16 ALPHA CPUs. We use Garnet [1]
analysis. The detailed system conﬁgurations are listed in Table 1.
Table 1: System and Interconnect conﬁguration
core count/freq.
16, 2GHz
topology
L1 I & D cache
private, 64KB
router pipeline
L2 cache
shared & tiled, 4MB
VC count
cacheline size
64B
buffer depth
memory
1GB DRAM
packet length
cache-coherency MESI protocol
ﬂit length
4 × 4 2D Mesh
classic ﬁve-stage
4 VCs per port
4 buffers per VC
5 ﬂits
16 bytes
We evaluate NoC-sprinting with multi-threaded workloads from
PARSEC [2] by assuming the chip can sustain computational sprinting
for one second in the worst case, which is consistent with [17]. Later we
will analyze how NoC-sprinting inﬂuences the sprint duration. We ﬁrst
start running the benchmarks in a simple mode and take checkpoints
when reaching the parallel portion of the program. Then, the simulation
restores from checkpoints and we record the execution time of running
a total of one billion instructions for each benchmark. In addition, we
construct synthetic trafﬁc for further network analysis.
4.1 Performance Evaluation
Here we
evaluate how NoC-sprinting improves
the
system
responsiveness.
In comparison,
one naive baseline design
(non-sprinting) is to always operate with one core under TDP
limit. Another extreme case (full-sprinting) is to activate all the 16
cores during sprinting. While the methods to predict the application
parallelism [6, 12] is beyond the scope of this paper, we conduct
off-line proﬁling on PARSEC to capture the internal parallelism of the
Figure 7: Execution time comparison with different sprint mechanisms.
benchmarks. Figure 7 shows the execution time of PARSEC workloads
with different sprinting schemes.
We can see that NoC-sprinting cuts down the execution time
substantially compared to non-sprinting.
It achieves 3.6x speedup
on average for all the applications.
In comparison,
full-sprinting
fails to provide the maximal speedup in some cases with an average
1.9x speedup. It is because increasing the active core count in some
programs would incur overheads that may outweigh achievable beneﬁts
after a saturating point is reached. These overheads come from OS
scheduling, synchronization, and long interconnect delay due to the
spread of computation resources.
4.2 Core Power Dissipation
Instead of waking up all
the dark cores for quick response,
NoC-sprinting provides better power-efﬁciency by allocating just
enough power to support the maximal performance speedup. Since
the triggered topology directly determines the number of cores to
be powered on, here we ﬁrst explore its impact on the core power
dissipation. Apart from full-sprinting, we also compare NoC-sprinting
with a naive ﬁne-grained sprinting scheme which does not employ any
power gating techniques, i.e. the system only cares about selecting the
optimal number of cores for actual execution and leaves the others idle.
Figure 8: Core power dissipation with different sprinting schemes. Here ﬁne-grained
sprinting does not include any power-gating schemes to the idle cores.
As shown in Figure 8, except for blackscholes and bodytrack which
achieve the optimal performance speedup in full-sprinting and hence
leave no space for power-gating, NoC-sprinting cuts down the most
power across all the other applications. Compared to full-sprinting,
ﬁne-grained sprinting saves 25.5% power even though power gating
is not applied. More promisingly, NoC-sprinting achieves 69.1% core
power saving on average for all applications.
4.3 Analysis of On-Chip Networks
NoC-sprinting provides customized topology, routing, ﬂoorplanning,
and efﬁcient power-gating support for ﬁne-grained sprinting. Therefore
in this subsection, we evaluate network performance and power to see
how the NoC behaves during the sprinting process.
Network Latency: Full-sprinting activates the entire network
and would possibly lose some performance speedup in the long
interconnect.
In contrast, NoC-sprinting uses a subset of routers
to directly connect
the active cores, which avoids unnecessary
network traversals in the dark nodes with the support of CDOR
routing algorithm. As an example, Figure 9 shows the average
network latency for running PARSEC with different sprinting schemes.
Apparently, NoC-sprinting shortens the communication latency for
most applications. Overall, it cuts down the network latency by 24.5%.
Figure 9: Comparisons of average network latency after
full-sprinting and NoC-sprinting
running PARSEC with
Network Power: As Figure 3 shows, network power becomes more
and more signiﬁcant as cores turn dark. Therefore, optimizing NoC
power dissipation becomes an urgent issue in order to combat the power
shortage in the dark silicon age.
Figure 10 shows the total network power consumption during the
sprint phase of running PARSEC. As we can see, NoC-sprinting
successfully cuts down the network power if an intermediate level of
sprinting is selected. On average, it saves 71.9% power compared to
full-sprinting. This is because NoC-sprinting can adapt the network
topology according to workload characteristics and only operates
on a subset of nodes.
In comparison,
full-sprinting activates a
fully-functional network and loses opportunities for power-gating.
Figure 10: Comparisons of total network power after running PARSEC with full-sprinting
and NoC-sprinting
More Analysis with Synthetic Trafﬁc: Furthermore, we construct
some synthetic trafﬁc on a network simulator booksim 2.0 [10] to test
NoC-sprinting under different trafﬁc scenarios. For full-sprinting, we
consider trafﬁc to be randomly mapped in the fully-functional network
and results are averaged over ten samples. We compare full-sprinting
with NoC-sprinting and observe the differences in performance and
power while varying the network load. As an example, Figure 11 shows
the results of 4-core and 8-core sprinting for a 16-core system under
uniform-random trafﬁc. There are a few key observations:
• As shown in Figure 11a and Figure 11c, NoC-sprinting cuts down
the average ﬂit latency by 45.1% and 16.1% before saturation for 4-core
and 8-core sprinting, respectively, because it uses a dedicated region
of network for more efﬁcient communication without traversing the
dark region. The latency beneﬁt drop"
darkNoC - Designing Energy-Efficient Network-on-Chip with Multi-Vt Cells for Dark Silicon.,"In this paper, we propose a novel NoC architecture, called darkNoC, where multiple layers of architecturally identical, but physically different routers are integrated, leveraging the extra transistors available due to dark silicon. Each layer is separately optimized for a particular voltage-frequency range by the adroit use of multi-Vt circuit optimization. At a given time, only one of the network layers is illuminated while all the other network layers are dark. We provide architectural support for seamless integration of multiple network layers, and a fast inter-layer switching mechanism without dropping in-network packets. Our experiments on a 4 × 4 mesh with multi-programmed real application workloads show that darkNoC improves energy-delay product by up to 56% compared to a traditional single layer NoC with state-of-the-art DVFS. This illustrates darkNoC can be used as an energy-efficient communication fabric in future dark silicon chips.","darkNoC: Designing Energy(cid:173)Efﬁcient Network(cid:173)on(cid:173)Chip
with Multi(cid:173)Vt Cells for Dark Silicon
Haseeb Bokhari†
Haris Javaid†∗
Muhammad Shaﬁque‡
Sri Parameswaran†
J ¨org Henkel‡
†
School of Computer Science and Engineering, University of New South Wales, Sydney, Australia
‡
Chair for Embedded Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany
{hbokhari, harisj, sridevan}@cse.unsw.edu.au, {muhammad.shaﬁque, henkel}@kit.edu
ABSTRACT
In this paper, we propose a novel NoC architecture, called darkNoC, where multiple layers of architecturally identical, but physically different routers are integrated, leveraging the extra transistors
available due to dark silicon . Each layer is separately optimized for
a particular voltage-frequency range by the adroit use of multi-Vt
circuit optimization. At a given time, only one of the network layers is illuminated while all the other network layers are dark. We
provide architectural support for seamless integration of multiple
network layers, and a fast inter-layer switching mechanism without
dropping in-network packets. Our experiments on a 4 × 4 mesh
with multi-programmed real application workloads show that darkNoC improves energy-delay product by up to 56% compared to a
traditional single layer NoC with state-of-the-art DVFS. This illustrates darkNoC can be used as an energy-efﬁcient communication
fabric in future dark silicon chips.
1.
INTRODUCTION
Network-on-Chips (NoCs) provide a scalable communication fabric for connecting large number of resources within a chip. NoC
can contribute signiﬁcantly to the total chip power, e.g., up to 18%
and 33% in Intel SCC [1] and RAW [2] architectures, respectively.
To address this issue, various power saving techniques for NoC
at system-, architecture- and circuit-level have emerged, among
which Dynamic Voltage and Frequency Scaling (DVFS) is very
prominent [3, 4]. System-level DVFS managers apply reduced
voltage and frequency (VF) levels in a NoC to save power. According to several recent studies [5, 6], the energy saving potential
of DVFS has been diminishing due to a number of reasons:
• With shrinking node size, the difference between nominal
voltage and the threshold voltage is decreasing. As the nominal V approaches threshold voltage, the transistor delay increases exponentially, resulting in huge performance penalties [5]. This limits the factor by which V can be scaled
down. Transistors with lower threshold voltages can be used
in the circuit to increase the gap between the nominal and
threshold voltages, but that results in an increased leakage
power.
22
40
35
40
25
40
LVt
Multi-Vt 
Optimization
Target 
Latency : 110
NVt
32
50
25
25
20
40
HVt
Figure 1: An example of multi-Vt circuit optimization.
With the help of the following NoC synthesis case study, we
show how the problem of diminishing returns of DVFS can be alleviated.
Motivational Example: Most fabrication foundries characterize cell libraries for various gate threshold voltage (Vt) values such
as normal Vt (NVt), Low Vt (LVt), and High Vt (HVt). LVt cells
can switch at a much faster speed than HVt cells. However, LVt
cells can be up to 5× leakier than their HVt counterparts. Modern CAD tools exploit the power-delay characteristics of multi-Vt
cell libraries and slacks in path delays to synthesize power efﬁcient
circuits [7]. A typical CAD tool optimizes the circuit by using LVt
cells on critical paths to meet the target latency, while inserting HVt
cells on the non-critical paths to reduce leakage power. For example, in Figure 1, a circle and its annotation represents the type of
cell and its delay. With multi-Vt circuit optimization, a mix of LVt,
NVt and HVt cells is used to meet the target latency rather than
just the NVt cells. Thus, the circuit on the right will have different intrinsic properties such as gate sizes, capacitances and leakage
power than the circuit on the left.
We exploited the multi-Vt circuit optimization available in CAD
tools to synthesize architecturally identical NoC routers for a set
of target VF levels: [1GHz, 0.9V], [750 MHz, 0.81V], [500 MHz,
0.81V] and [250 MHz, 0.72V]. Figure 2 reports the network power
for operation at [500 MHz, 0.81V] and [250 MHz, 0.72V] (details
of our experiments are presented in Section 4). We can observe that
for operation at [500 MHz, 0.81V], the NoC designed particularly
for [500 MHz, 0.81V] VF level is on average 35% and 16% more
power efﬁcient than applying DVFS on a NoC designed for [1GHz,
0.9V] and [750 MHz, 0.81V], respectively. Similarly, for operation
at [250 MHz, 0.72V], the NoC designed particularly for [250 MHz,
0.72V] VF level is on average 42% and 22.7% more power efﬁcient
• DVFS does not inﬂuence the intrinsic physical properties of
the circuit such as gate sizes, capacitances, etc. which are
ﬁxed at design time.
∗The author is now afﬁliated with Google Inc.
Ϳ
t

;
ƌ
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
DAC 2014, June 01 - 05 2014, San Francisco, CA, USA
Copyright c(cid:13) 2014 978-1-4503-2730-5/14/06 ...$15.00.
http://dx.doi.org/10.1145/2593069.2593117
Ğ
ǁ
Ž
W
Ŭ

ƌ
Ž
ǁ
ƚ
Ğ
E
ZŽƵƚĞƌĞƐŝŐŶĞĚΛϭ',ǌ
ZŽƵƚĞƌĞƐŝŐŶĞĚΛϳϱϬD,ǌ
ϱϬϬD,ǌ͕Ϭ͘ϴϭs
Ϳ
t
;
ƌ
Ğ
ǁ
Ž
W
Ŭ

ƌ
Ž
ǁ
ƚ
Ğ
E
ZŽƵƚĞƌĞƐ ŝŐŶĞĚΛϱϬϬD,ǌ
ZŽƵƚĞƌĞƐ ŝŐŶĞĚΛϮϱϬD,ǌ
ϮϱϬD,ǌ͕Ϭ͘ϳϮs
Ϭ͘Ϯϱ
Ϭ͘Ϯ
Ϭ͘ϭϱ
Ϭ͘ϭ
Ϭ͘Ϭϱ
Ϭ
Ϭ͘Ϭϱ
Ϭ͘ϭ
/Ŷ ũĞĐƚ ŝŽŶ ZĂƚĞ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
Ϭ
Ϭ͘ϬϮ
Ϭ͘Ϭϰ
Ϭ͘Ϭϲ
/ŶũĞĐƚ ŝŽŶ ZĂƚĞ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
Ϭ͘Ϭϴ
Ϭ͘ϱ
Ϭ͘ϰ
Ϭ͘ϯ
Ϭ͘Ϯ
Ϭ͘ϭ
Ϭ
Ϭ
Figure 2: NoC Power for Transpose Trafﬁc for a)(left)[500 MHz, 0.81V]
VF level, b)(right) [250 MHz, 0.72V] VF level
than DVFS applied to NoC designed for [1GHz, 0.9V] and [750
MHz, 0.81V], respectively. This observation shows that, unlike
traditional NoC with a single layer of routers, it may be beneﬁcial
in terms of power to have multiple layers of routers in a NoC such
that each layer is optimized for a particular VF level.
The provision of multiple layers of routers in a NoC may cost a
signiﬁcant amount of silicon; however, we can leverage dark silicon [8][9] to alleviate this overhead. Several researchers have proposed to leverage dark silicon to provide extra application-speciﬁc
accelerators [10], extra low-power processors [11], etc. for better
energy efﬁciency. In fact, the authors of [12] state that “in eight
years, we will be faced with designs that are 93.75% dark!”, and,
“we will spend exponentially increasing amounts of silicon area to
buy energy efﬁciency”. Therefore, even with the addition of extra accelerators [10] and processors [11], we believe that there will
still be dark silicon available which could be exploited for energyefﬁcient design of other system components such as NoC. Given
the availability of dark silicon, three research problems need to be
addressed for realization of a NoC with multiple network layers:
• What kind of architectural support is required to integrate
multiple VF-optimized network layers in a NoC?
• How many network layers (and their VF levels) should be
used for a set of applications under a dark silicon budget?
• How to enhance system-level DVFS managers to exploit the
provision of multiple VF-optimized network layers?
In this paper, we focus on the ﬁrst research problem. We propose a novel NoC architecture, where architecturally homogenous
routers that have been optimized speciﬁcally for particular VF ranges,
are seamlessly integrated at the architecture-level to complement
system-level DVFS managers, and exchange dark silicon for energy
efﬁciency in NoC. For the rest of the two problems, we provide our
initial insights from extensive experiments, and leave their comprehensive solutions as future work. In particular, our key contributions are:
• We propose a novel NoC architecture, named darkNoC, where
multiple network layers consisting of architecturally identical routers, but optimized to operate within different voltage
and frequency ranges during synthesis are used. Only one
network layer is illuminated (active) at a given time while
the rest of the network layers are dark (deactivated).
• We propose an efﬁcient hardware-based mechanism to transition from one network layer to another. Our network layer
switch-over mechanism preserves the lossless communication property of a packet-switched buffered NoC, and is transparent to the software. Further, we investigate and report the
factors that can potentially affect the time and energy overhead of switch-over mechanism.
• Finally, to illustrate the potential of our darkNoC architecture, we show how a state-of-the-art DVFS manager [13] is
deployed. Further, we compare our darkNoC architecture
having different number of network layers (due to dark silicon budget) with a traditional single network layer NoC,
where both the architectures have the same DVFS manager
(our results show savings of up to 56% in energy-delay product).
2. RELATED WORK
Various system-level DVFS techniques have been proposed for
NoC power management to strike a balance between performance
and power, such as at the granularity of communication links [14],
routers [3, 4] and regions [13]. Run-time power gating has also
been proposed to reduce leakage power of NoCs. Matsutani et.
al. [15] proposed an ultra ﬁne-grained power gating technique for
routers. Similar techniques at other granularities have been explored in [16, 17, 18]. All of these techniques are orthogonal to
our work and can be tweaked to exploit the provision of multiple
network layers in a NoC. In this paper, we use the DVFS technique
of [13] as the state-of-the-art for comparing our darkNoC architecture with the traditional single network layer NoC.
Pullini et.al. [19] investigated the advantages of using multi-Vt
circuit optimization in terms of performance and power for a single
layer NoC architecture. In contrast, we exploit the multi-Vt circuit
optimization in the context of multiple network layers and DVFS.
Various NoCs with multiple network layers have been proposed in
[20, 21, 22]. However, all of these works customized the architecture of the network layers or the routers, in contrast to our work
where we integrate architecturally homogenous yet VF-optimized
network layers. Our work is inspired by [23, 24], where the authors
exploited multi-Vt circuit optimization for designing processors in
the context of dark silicon. However, one cannot infer from their
work that multi-Vt circuit optimization will be beneﬁcial for NoCs,
and what should be the architecture of a NoC with multiple VFoptimized network layers.
In summary, to the best of our knowledge, this is the ﬁrst work
that proposes a NoC architecture which exploits multi-Vt circuit
optimization for multiple network layers in the context of systemlevel DVFS and dark silicon.
3. darkNoC ARCHITECTURE
We consider NoCs with predeﬁned topologies as shown in Figure 4(a). The NoC consists of n routers, which are divided into
regions. Each region has m routers, and separate voltage and frequency (VF) rails with their own VF regulators. The value of m
can vary from 1 to n, depending on the granularity of the regions.
For communication between VF regions, bi-synchronous links [25]
have to be used. However, in this work we assume that VF regions
can only communicate if they are operating at the same VF level.
A designer can opt to use the darkNoC architecture in any of the
VF regions based upon his/her requirements. The following paragraphs explain the darkNoC architecture from the perspective of a
single VF region.
3.1 darkNoC Layers
A region contains l number of network layers, where a network
layer consists of m routers as shown in Figure 4(b). At a given time,
only one of the network layers is illuminated (activated), while the
rest of the network layers remain dark (deactivated). When a network layer is illuminated, all of its routers are active, and thus, at
a given time, only m routers are active. Figure 3 shows an example of transitioning from network layer 0 to 3 in a region with 4
network layers.
Each network layer is optimized at design-time to operate in a
certain VF range. That is, multi-Vt circuit optimization of CAD
tools is used to optimize all the routers of a network layer for a
particular VF range. All the layers in a region are managed by a
hardware-based darkNoC Layer Manager (dLM ) as shown in Figure 4(b). The function of the dLM is to switch between network
layers when directed by the system-level DVFS manager.
3.2 darkNoC Routers Stack
Each node in Figure 4(b) represents a stack of l routers to realize
l network layers of the region. For example, for the 4 network layers in Figure 3, each routers stack has 4 routers and all the routers
marked 2 belong to the network layer 2. As shown in Figure 4(b),
all the routers in a stack share the same set of link wires with the
neighboring stacks of routers, which reduces veriﬁcation costs.
Each router in a stack has separate controls for power-gating and
enabling input/output ports. For example, a stack of two routers in
shown in Figure 4(c), where R0 P W R en and R0 en controls are
used to power-on/off and enable/disable input/output ports of router
0
1
0
1
0
1
0
1
0
1
0
1
3
2
3
2
3
2
3
2
3
2
3
2
Transition from 
network layer 0 to 3
0
3
0
3
1
2
1
2
0
3
0
3
1
2
1
2
0
3
0
3
1
2
1
2
0
3
0
3
1
2
1
2
0
3
0
3
1
2
1
2
0
3
0
3
1
2
1
2
Illuminated network layer
Dark network layer
Figure 3: An example darkNoC architecture with four network layers.
VF0
E Ž






























































E Ž
D
>
Ě
D
>
Ě
VF1
E Ž






























































E Ž
D
>
Ě
D
>
Ě
VF2
VF3
Ě>D
< EK  > z
Z DE' 
Z
/ Ͳ^ z E  , Z KEKh ^ >/ E<

Z
Ě>D
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
Ě Z D
Z ^
ĚZ D  Z
Z ^
Z Khd
Z
< EK 
Z Khd
^ ^d 
<
Z DE' 
Z
Y +
VDD 
X R0 
clock
X +
local port
Y R0_PWR_en
R1_PWR_en
R0_en
R1_en
Figure 4: darkNoC architecture: (a) (left) NoC divided into VF regions (b) (middle) a single VF region (c) (right) a stack of (two) VF optimized routers.
Algorithm 1: dLM Function
Algorithm 2: dRM Function
1 nodes = m; // number of routers in a region
// Phase 1
2 WaitForSwitchOverCommand();
3 for i=nodes to 0 do
4
SendDisableInjControlFlitTodRM (i);
5 WaitForNetworkFlush();
6 for i=nodes to 0 do
7
SendSwitchRouterControlFlitTodRM (i);
8 WaitForNetworkFlush();
9 for i=nodes to 0 do
10
SendEnableInjControlFlitTodRM (i);
0, respectively. Thus, a router in a stack is illuminated in two steps:
powering it on and enabling its input/output ports. The local port
of a routers stack is connected to a tile (which may consist of a
processor, memory, etc.) through a network interface (NI). The NI
has the capability to stop injection of packets from its tile, which
we exploit during the switching of network layers. For managing a
routers stack, we use a hardware-based darkRouter Manager (dRM )
as shown in Figure 4(b). The function of dRM is to switch between
routers when directed by the dLM.
3.3 darkNoC Layer Switch(cid:173)Over Mechanism
The switch-over between network layers is an important design
requirement for our darkNoC architecture. The main challenges
are: a) the lossless data communication property of packet-switched
buffered NoC should be preserved, b) the switch-over mechanism
should be transparent to software, and c) the switch-over mechanism should be efﬁcient in terms of time and energy overhead.
In our solution, the darkNoC Layer Manager (dLM ) and the darkNoC Router Managers (dRMs) autonomously coordinate with each
other to realize a switch-over mechanism with the aforementioned
requirements.
At a high level, the dLM ensures correct switch-over between
two network layers in the region. The dLM uses the injection channel of the NI of the routers stack it is attached to, and sends different
types of control ﬂits to dRMs through the NoC channels. Further,
the dLM gathers the buffer occupancy information from the routers
stacks using a single bit AND-gate network as shown in Figure4(b).
Similar AND-gate networks have been used in NoC for congestion
detection [26]. The dRM of a routers stack is attached to the ejection port of the NI, and receives control ﬂits from the dLM. Based
upon the control ﬂits from the dLM, a dRM can: (a) power-on/off
Rt
Ra
off
active
t1
switch-over 
powering-on
on
flushing
disable
t2 t3
enable active
on
off
t6
t5
t4
Figure 5: Timing diagram for switch-over from router Ra to Rt .
1 WaitForDisableInjControlFlitFromdLM ();
2 SwitchOnTargetRouter();
3 DisableInjectionFromNI();
4 if RouterLocatedAtRegionBorder then
5
SetStopFlagForNeighbourRouters();
6 WaitForRouterFlush();
7 SetEmptyFlag();
// Phase 2
8 WaitForSwitchRouterControlFlitFromdLM ();
9 DisableActiveRouterPorts();
10 EnableTargetRouterPorts();
11 PowerOffActiveRouter();
12 SetEmptyFlag();
// Phase 3
13 WaitForEnableInjControlFlitFromdLM ();
14 EnableInjectionFromNI();
15 if RouterLocatedAtRegionBorder then
ResetStopFlagForNeighbourRouters();
16
a router, (b) enable/diable input/output ports of a router, or (c) enable/disable injection of packets into the NI. Further, a dRM generates a single bit ﬂag by analyzing the buffer occupancies of its
router to indicate presence/absence of in-transit packets, which is
propagated through the AND-gate network to the dLM.
Details of dLM and dRM: Algorithms 1 and 2 report the functionalities of dLM and dRM respectively during switch-over of a
network layer. Figure 5 illustrates the switch-over mechanism for a
routers stack (other stacks similarly switch-over their routers in parallel). On detecting a new switch-over command (from a systemlevel DVFS manager), the dLM starts sending control ﬂits to all the
dRMs to initiate the switch-over mechanism (Algorithm 1, lines 2
– 4). A dRM enters into its ﬁrst phase on receiving the control ﬂits
from the dLM (Algorithm 2, lines 2-5; Figure 5@t1 ). The dRM :
(1) powers-on the target router, and (2) stops NI from injecting any
new packets. If the dRM is controlling one of the border routers,
then it also sets a ﬂag to stop neighboring routers of other regions
from injecting any new packets. The reason behind stopping injection of packets is to ensure that all the in-transit packets reach their
destination before the actual switch-over starts. Once the router
buffers are ﬂushed and no packets are injected by the neighboring
routers, the dRM sets the empty ﬂag which is propagated through
the AND-gate network. It is important to note that the poweringon of target router and ﬂushing of currently active router (Figure 5,
t1 − t3 ) takes place simultaneously to speed up the switch-over
mechanism.
Once the empty ﬂags from all the dRMs have been propagated
to the dLM through the AND-gate network, the dLM concludes
the ﬂushing of the currently active network layer. Then, it starts
another round of control ﬂits to all the dRMs to start the actual
switch-over of routers (Algorithm 1, lines 5 – 7). On receiving



Ϭ
ϱϬ
ϭϬϬ
ϭϱϬ
ϮϬϬ
ϮϱϬ
ϯϬϬ
Ϭ
Ϭ ͘ϭ
Ϭ ͘Ϯ
Ϭ ͘ϯ
Ɛ
ǁ
ŝ
Đ
ƚ
Ğ
ǀ
Ž
Ś
ƚ
ƌ

ŝ
ŵ
Ğ

;
Ŷ
Ɛ
Ϳ
/ŶũĞĐƚŝŽŶƌĂƚĞ ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
ϰп ϰDĞƐŚ
dZ
hZ
Ϭ
ϮϬϬ
ϰϬϬ
ϲϬϬ
ϴϬϬ
ϭϬϬϬ
ϭϮϬϬ
Ϭ
Ϭ ͘ϭ
Ϭ ͘Ϯ
Ϭ ͘ϯ
Ɛ
ǁ
ŝ
Đ
ƚ
Ğ
ǀ
Ž
Ś
ƚ
ƌ

ŝ
ŵ
Ğ

;
Ŷ
Ɛ
Ϳ
/ŶũĞĐƚŝŽŶƌĂƚĞ ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
ϴп ϴDĞƐŚ
dZ
hZ
Figure 6: Time overhead of switch-over mechanism for NoC running @
500MHz
Ϭ
Ϯ
ϰ
ϲ
ϴ
ϭϬ
ϭϮ
Ϭ
Ϭ ͘ϭ
Ϭ ͘Ϯ
Ϭ ͘ϯ
Ɛ
ǁ
ŝ
Đ
ƚ
Ğ
ǀ
Ž
Ś
ƌ

Ğ
Ŷ
Ğ
ƌ
Ǉ
Ő

;
Ŷ
Ϳ
:
/ŶũĞĐƚŝŽŶƌĂƚĞ ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
ϰп ϰDĞƐŚ
dZ
hZ
ϭϴϬ
ϭϲϬ
ϭϰϬ
ϭϮϬ
ϭϬϬ
ϴϬ
ϲϬ
ϰϬ
ϮϬ
Ϭ
Ϭ
Ϭ ͘ϭ
Ϭ ͘Ϯ
Ϭ ͘ϯ
Ɛ
ǁ
ŝ
Đ
ƚ
Ğ
ǀ
Ž
Ś
ƌ

Ğ
Ŷ
Ğ
ƌ
Ǉ
Ő

;
Ŷ
Ϳ
:
/ŶũĞĐƚŝŽŶƌĂƚĞ ;ĨůŝƚƐͬŶŽĚĞͬŶƐͿ
ϴп ϴDĞƐŚ
dZ
hZ
Figure 7: Energy overhead of switch-over from [500MHz,0.81V] to
[750MHz,0.81V] layer
the control ﬂit, a dRM enters into its second phase (Algorithm 2,
lines 8 – 12), where it: (1) disables the input/output ports of the active router (Figure 5, @t3 ), (2) enables the input/output ports of the
target router (Figure 5, @t4 ), and (3) powers-off the (previously)
active router (Figure 5, @t5 ). Note that the order of (1) and (2)
is essential to avoid short-circuit conditions. At the end, the dRM
sets the empty ﬂag, which is propagated through the AND-gate network.
Once the dLM detects (from the AND-gate network) that the network is empty again, it starts the last round of control ﬂits to all the
dRMs (Algorithm 1, lines 8 – 10). This control ﬂit dictates a dRM
to enters its third phase (Algorithm 2, lines 13 – 16), where the injection of packets into NI is enabled (Figure 5, @t6 ). Further, if the
dRM is controlling one of the border routers, then it enables injection of packets from the neighboring routers of other regions. With
this, the network layer switch-over mechanism is complete.
Time and energy overhead. Now, we present an analysis of
the time and energy overhead of switch-over mechanism in darkNoC architecture. For this analysis, we assume that the transition
of VF level is done after the switch-over from an active network
layer to a target network layer. We do not include the time and energy overhead of VF scaling because this overhead is present in the
traditional single layer NoC as well. The time overhead in clock
cycles of switch-over can be estimated as:
Tso = in-transit-packets(m, load) + no-load(m)
+control-ﬂits(m, load)
The in-transit-packets term represents the time to ﬂush the active
network layer which depends on the number of routers m and the
network load. The no-load term captures the time spent in switchover of all the routers which only depends on m, while the term
control-ﬂits represents the time spent by dLM in sending control
ﬂits to dRMs and is dependent on both m and the network load.
Since the whole switch-over mechanism occurs at the frequency of
the active network layer, it can be used to convert the switch-over
clock cycles to actual time.
The energy overhead of switch-over mechanism can be estimated
as:
Eso = power-on-energy(Lt , m) + control-ﬂits(Lt , La , m)
+leakage-power(Lt , La , m) × Tso
where La and Lt refer to the active and target network layers respectively. The ﬁrst term power-on-energy is the energy overhead
of powering-on a network layer, which depends on the type of the
target network layer and the number of routers m in it. During
Topology
Router Architecture
4×4 mesh
5 port router (4 neighbors + 1 local), 3
stage pipeline [LT+BW, RC+SA, ST]
8 ﬂit deep (no virtual channel)
Dimension Order XY
On/Off
64 bit data, 8 bit control
Matrix arbiter
Input Buffer
Routing
Link Flow Control
Link Width
Switch Arbiter
Table 1: NoC architectural details.
Frequency
Voltage
Area [µm2 ]
HVt Cells [%]
NVt Cells [%]
LVt Cells [%]
1 GHz 750 MHz 500 MHz 250 MHz
0.9 V 0.81 V
0.81 V
0.72 V
49200
46313
45750
45225
44.7
12.5
42.8
60.5
13.6
25.9
90.7
4.3
5.0
92.8
3.9
3.3
Table 2: Synthesis results for a single router.
a typical switch-over, the control ﬂits from the dLM use both the
active and target network layers, and their energy consumption is
captured in the control-ﬂits term. The last term includes the leakage
energy of the active and target layers, since both the layers consume
leakage power for almost the whole duration of the switch-over
mechanism.
We experimented with 4×4 and 8×8 mesh sizes, transpose random (TR) and uniform random (UR) trafﬁc patterns and varying
trafﬁc injection rates (details are in Section 4. The results for time
and energy overhead are presented in Figures 6 and 7 respectively.
Three trends are evident. The time and energy overhead: (1) increases with an increase in mesh size due to a higher number of
control ﬂits and hops, and longer delays in the AND-gate network,
(2) increases with an increase in the trafﬁc injection rates due to
longer ﬂushing times of in-transit packets and delays of control
ﬂits, and (3) heavily depends on the network load as trends are different for TR and UR trafﬁc patterns. It is important to note that the
time and energy overhead of our switch-over mechanism is not signiﬁcant compared to the total execution time of typical applications
and energy consumption of the NoC (see Section 4).
4. EVALUATION METHODOLOGY
4.1 NoC Synthesis
We use a packet-switched wormhole NoC architecture for experiments. The details of the NoC architecture are reported in Table 1. The proposed router is implemented in Verilog RTL, and synthesized using Synopsys Design Compiler for commerical TSMC
45nm libraries characterized for HVt, NVt and LVt cells. We enabled leakage and dynamic power optimization in Synopsys Design
Compiler, which automatically enabled multi-Vt optimization. We
explored the following four VF levels: [1GHz, 0.9V], [750MHz,
0.81V], [500 MHz, 0.81V] and [250 MHz, 0.72V], and the synthesis results are reported in the last four columns of Table 2.
We can observe that the target VF level signiﬁcantly affects the
type and number of cells used during optimization. For example,
the router optimized for [1GHz, 0.9V] contains 42.8% LVt cells,
which decreases to 5.0% and 3.3% for optimization at [500 MHz,
0.81V] and [250 MHz, 0.72V], respectively. Moreover, the cells
are sized according to the target VF level, which has resulted in
different silicon areas for different target VF levels (row 2). These
results corroborate our motivational example that it is worthwhile
to explore multi-Vt optimization in NoCs for energy efﬁciency.
4.2 Experimental Setup
MPSoC and Applications. We used a 16-node MPSoC laid as a
4×4 2D-mesh, with its architectural details in Table 3. Four VF levels are assumed for the NoC, whose architectural details are in Table 1. Note that the VF level of processors is not changed in our experiments. We used eight applications from mediabench suite and
created diverse multi-programmed application mixes (AM) whose
Topology
Processors
L1 I/D Caches
DRAM
NoC VF Levels
4 × 4 mesh
Tensilica in-order LX4 @ 1GHz
4KB, 2-way set associative, 16 byte line size
2 memory controllers, 40 ns latency
[1GHz, 0.9V], [750MHz, 0.81V], [500 MHz,
0.81V], [250 MHz, 0.72V]
Table 3: MPSoC architectural details.
AM-1 mpeg2enc, sha, mpeg2dec, g721enc
AM-2
h264enc, mpeg2dec, sha, jpeg dec
AM-3
jpeg enc, sha, g721enc, g721dec
AM-4
g721enc, mpeg2enc, g721dec, mpeg2dec
Table 4: Details of application mixes with four copies of each application.
Name
Conﬁguration
Relative
Chip Area
baselineNoC Traditional NoC with [1Ghz,
0.9V] layer
[1GHz, 0.9V] + [750MHz,
0.81V]
[1GHz, 0.9V] + [500MHz,
0.81V]
[1GHz, 0.9V] + [750MHz,
0.81V] + [500MHz, 0.81V]
1 ×
darkNoC1
1.13×
darkNoC2
1.13×
darkNoC3
1.26×
Table 5: NoC conﬁgurations used in experiments.
details are in Table 4. For an application mix, four copies of each
application in that mix are used. We also used synthetic trafﬁc injection models consisting of Uniform Random (UR), Bit Complement (BC) and Transpose (TR).
Simulation Setup. We used two step system simulation methodology where memory access trace of each application executing
on a processor is collected from Xtensa instruction set simulator.
These memory access traces are then simulated through a closedloop cycle-accurate NoC and DRAM simulator. Our NoC simulator also modeled different VF levels accurately for the NoC. We
also modeled the dLM, dRM and AND-gate network in the NoC
simulator. For application mixes, each processor was executed for
at least 2 million instructions. For synthetic trafﬁc injection models, the NoC was warmed up for 100,000 clock cycles, followed by
collection of statistics for 80,000 clock cycles. The injection rates
were measured in f lits/node/ns.
NoC Power Estimation. We used Synopsys PrimeTime tool
to analyze the netlist generated by Synopsys Design Compiler to
compute leakage and dynamic power of a NoC. We ﬁrst apply this
analysis to compute power values at the nominal VF level of the
NoC. Then, scaled VF conditions are applied to compute power
values at VF levels lower than the nominal VF level. These computed power values are used in the NoC simulator to compute the
total NoC power/energy consumption. This methodology is also
used for all the network layers in a darkNoC.
NoC Conﬁgurations. We used different NoC conﬁgurations in
our experiments, which are listed in Table 5. The baselineNoC
represents the traditional single layer NoC designed for the highest VF level, [1GHz, 0.9V]. For fairness of comparison, we synthesized baselineNoC with multi-Vt optimization. The darkNoC1
and darkNoC2 conﬁgurations have 2 VF-optimized network layers each, while darkNoC3 conﬁguration is a superset of darkNoC1
and darkNoC2. In Table 2, the difference in silicon area and distribution of multi-Vt cells between [500 MHz, 0.81V] and [250 MHz,
0.72V] optimized routers (columns 4 and 5) is small. This means
that two network layers optimized for [500 MHz, 0.81V] and [250
MHz, 0.72V] will have nearly identical properties, and thus similar
energy efﬁciencies. Therefore, a designer can use synthesis results
for early elimination of network layers in a darkNoC. That is why,
we did not include [250 MHz, 0.72V] optimized network layer in
our experiments.
The last column of Table 5 reports the relative chip area of NoC
conﬁgurations, measured as a sum of the area of the processors,
Ϭ ͘Ϭ
Ϭ ͘Ϯ
Ϭ ͘ϰ
Ϭ ͘ϲ
Ϭ ͘ϴ
ϭ ͘Ϭ
s&^ Ͳϭ
s&^ͲϮ
E
Ž
ƌ
ŵ
Ă
ŝ
ů
ǌ
Ě
Ğ


W
DͲϭ 
Ϭ ͘Ϭ
Ϭ ͘Ϯ
Ϭ ͘ϰ
Ϭ ͘ϲ
Ϭ ͘ϴ
ϭ ͘Ϭ
s&^Ͳϭ
s&^ ͲϮ
E
Ž
ƌ
ŵ
Ă
ŝ
ů
ǌ
Ě
Ğ


W
DͲϮ
Ϭ ͘Ϭ
Ϭ ͘Ϯ
Ϭ ͘ϰ
Ϭ ͘ϲ
Ϭ ͘ϴ
ϭ ͘Ϭ
s&^Ͳϭ
s&^ ͲϮ
E
Ž
ƌ
ŵ
Ă
ŝ
ů
ǌ
Ě
Ğ


W
DͲϯ
Ϭ ͘Ϭ
Ϭ ͘Ϯ
Ϭ ͘ϰ
Ϭ ͘ϲ
Ϭ ͘ϴ
ϭ ͘Ϭ
s&^ Ͳϭ
s&^ ͲϮ
E
Ž
ƌ
ŵ
Ă
ŝ
ů
ǌ
Ě
Ğ


W
DͲϰ
s&^Ͳϭ͗ ϭϱйWĞƌĨŽƌŵĂŶĐĞ ^ůĂĐŬ s&^ͲϮ͗ ϭϬйWĞƌĨŽƌŵĂŶĐĞ ^ůĂĐŬ
ĂƐĞůŝŶĞEŽ
ĚĂƌŬEŽϭ
ĚĂƌŬEŽϮ
ĚĂƌŬEŽϯ
Figure 8: NoC Energy-Delay Product (EDP) normalized w.r.t traditional
NoC operating at highest VF level.
caches, network layers, and the dLM and dRMs. A designer can
use darkNoC1 or darkNoC2 if he/she has dark silicon budget of
a single network layer. Likewise, he/she can use darkNoC3 if the
dark silicon budget permits inclusion of 2 network layers. Analyzing router micro-architecture in context of multi-Vth optimization
is our future work.
System-level DVFS Manager. We used the state-of-the-art memory latency based NoC DVFS technique introduced by Chen et
al. [13] for the baselineNoC. We used a control interval of 50K
clock cycles for the DVFS. For the darkNoC conﬁgurations, we
modiﬁed their technique as follows. If DVFS manager decides to
switch to i-th VF level and there is a network layer optimized for
i-th VF level, then the DVFS manager requests the dLM to switchover to that particular network layer. On the other hand, if there
is no network layer optimized for i-th VF level, then the DVFS
manager requests the dLM to switch-over to the network layer optimized for the closest yet higher VF level, and will scale VF of the
selected network layer. For example, in darkNoC1, if system-level
DVFS manager decides to operate NoC at [250 MHz, 0.72V], then
the [750 MHz, 0.81V] network layer will be scaled down to operate at [250 MHz, 0.72V] rather than the [1 GHz, 0.9V] network
layer. We created two ﬂavors of DVFS managers based upon application requirements: DVFS-1 with a target performance loss of
15% and DVFS-2 with a target performance loss of 10%. Note
that our NoC simulator included the time and energy overhead of
network layer switch-over for the darkNoC conﬁgurations.
4.3 Results and Discussion
Overall Trend in NoC Energy-Delay Product (EDP) Savings.
Figure 8 reports the savings in NoC EDP for the four application
mixes of Table 4, four NoC conﬁgurations of Table 5 and two
DVFS managers. Overall, darkNoC conﬁgurations provide signiﬁcant improvement in EDP over baselineNoC. For example, for AM2, the DVFS-1 reduced EDP by only 5% in baselineNoC. On the
other hand, EDP is reduced by 34% and 44% by DVFS-1 in darkNoC1 and darkNoC3, respectively. Likewise, for AM-3, DVFS-1
in baselineNoC saved 15% EDP, whereas DVFS-1 in darkNoC1
and darkNoC3 reduced EDP by 45% and 52%, respectively.
In
summary, darkNoC provides up to 56% EDP savings (AM-1, darkNoC3, DVFS-2) over baselineNoC. These results show that DVFS
alone can be ineffective in reducing NoC energy due to increasing
leakage power to dynamic power ratio. Thus, our darkNoC architecture can well-complement system-level DVFS managers.
Interplay of Applications, DVFS Manager and darkNoC Conﬁgurations. EDP savings vary signiﬁcantly across D"
Power-Aware NoCs through Routing and Topology Reconfiguration.,"With the advent of multicore processors and system-on-chip designs, intra-chip communication demands have exacerbated, leading to a growing adoption of scalable networks-on-chip (NoCs) as the interconnect fabric. Today, conventional NoC designs may consume up to 30% of the entire chip's power budget, in large part due to leakage power. In this work, we address this issue by proposing Panthre: our solution deploys power-gating to provide long intervals of uninterrupted sleep to selected units. Packets that would normally use power-gated components are steered away via topology and routing reconfiguration, while Panthre provides low-latency alternate paths to their destinations. The routing reconfiguration operates in a distributed fashion and guarantees that deadlock-free routes are available at all times. At runtime, Panthre adapts to the application's communication patterns by updating its power-gating decisions. It employs a feedback-based distributed mechanism to control the amount of sleeping components and of packets detours, so that performance degradation is kept at a minimum. Our design is flexible, providing a mechanism that designers can use to tradeoff power savings with performance, based on application's requirements. Our experiments on multi-programmed communication-light workloads from the SPEC CPU2006 suite show that Panthre reduces total network power consumption by 14.5% on average, with only a 1.8% degradation in performance, when all processor nodes are active. At times when 15-25% of the processor cores are communication-idle, Panthre enables leakage power savings of 36.9% on average, while still providing connected and deadlock-free routes for all other nodes.","Power-Aware NoCs through Routing and Topology Reconﬁguration
Ritesh Parikh, Reetuparna Das and Valeria Ber tacco
Depar tment of Computer Science and Engineering, University of Michigan
{parikh, reetudas, valeria}@umich.edu
ing periods of no activity are ineffective. The problem is two-fold:
i) even when lightly utilized, NoC components often do not observe
long idle-periods, failing even to compensate for the energy spent in
the power-gating event itself, and ii) packets that encounter sleeping
components in their paths accrue latencies due to wake-up delays.
Power-gating at a ﬁner granularity than entire routers [11] provides
more sleeping opportunities. However, it further worsens the problem of accumulated wakeup latencies, as it puts components to sleep
more aggressively. Early wakeup with lookahead routing was proposed to compensate for wakeup latency [10]. However, for a typical
2-stage pipeline router, lookahead can only hide a small fraction of
the wakeup latency, which is typically many cycles. In our evaluations with multi-programmed workloads, we have identiﬁed that such
conventional schemes often lead to signiﬁcant application slowdown.
A workload stressing only a portion of the network creates opportunities for power-gating the remaining, lightly-used portions of the network. However, deterministic routing algorithms provide ﬁxed routes
among source-destination pairs, and in practice do not allow for isolation of any network component. A possible solution is to use an adaptive routing algorithm and deﬂect packets toward active units when
they encounter a sleeping component on their regular path. However,
this approach requires additional resources to maintain deadlock freedom, which must be kept active at all times. Moreover, accruing multiple deﬂections leads to increased packet latency.
Our solution, called Panthre (for Power-aware NoC through Routing and Topology Reconﬁguration), overcomes these issues by modifying routing paths periodically so to exclude lightly used portions
of the topology. When Panthre determines that the set of powergated components must be updated, it executes route reconﬁguration
to avoid the new set of power-gated components, while providing
deadlock-free routes for all packets. This step eliminates deﬂections
and the need for dedicated resources to support deadlock-freedom.
Panthre leverages the rich set of alternate paths that are available in
NoC fabrics to keep trafﬁc away from sleeping components. It also
proactively adapts to application demands by power-gating only those
network components that are under-utilized. Based on our analysis,
Panthre may disable multiple router resources, adding up to 99% of
the router’s static power (Section 3.1).
Naturally, Panthre leads to an increase in trafﬁc on the links kept
active, by channeling trafﬁc away from sleeping components.
It is
therefore essential for Panthre that substantial low-usage links exist
in the NoC. To this end, we conducted a study, whose ﬁndings are
plotted in Figure 1. The plot shows the contribution of network links
to total network activity. Our testbed consisted of an 8x8 mesh CMP
running a network-light multiprogrammed mix of applications from
the SPEC CPU2006 suite. Links are sorted by increasing utilization
during the execution, and the plot on the right indicates what fraction
of network trafﬁc (Y axis) was carried out by a given fraction of sorted
links. The plot on the left is an enlargement of the contribution by the
bottom 30% of active links: only 10% of the trafﬁc travels through
the bottom 30% of used links. Beyond the 30 percentile of utilization,
this disparity is no longer obvious, thus, Panthre’s goal is to identify
and leverage the 30% least used links, so to maximize power savings
without a signiﬁcant load increase on active links.
ABSTRACT
With the advent of multicore processors and system-on-chip designs,
intra-chip communication demands have exacerbated, leading to a growing adoption of scalable networks-on-chip (NoCs) as the interconnect
fabric. Today, conventional NoC designs may consume up to 30%
of the entire chip’s power budget, in large part due to leakage power.
In this work, we address this issue by proposing Panthre: our solution deploys power-gating to provide long intervals of uninterrupted
sleep to selected units. Packets that would normally use power-gated
components are steered away via topology and routing reconﬁguration, while Panthre provides low-latency alternate paths to their destinations. The routing reconﬁguration operates in a distributed fashion
and guarantees that deadlock-free routes are available at all times. At
runtime, Panthre adapts to the application’s communication patterns
by updating its power-gating decisions. It employs a feedback-based
distributed mechanism to control the amount of sleeping components
and of packets detours, so that performance degradation is kept at a
minimum. Our design is ﬂexible, providing a mechanism that designers can use to tradeoff power savings with performance, based on application’s requirements.
Our experiments on multi-programmed communication-light workloads from the SPEC CPU2006 suite show that Panthre reduces total
network power consumption by 14.5% on average, with only a 1.8%
degradation in performance, when all processor nodes are active. At
times when 15-25% of the processor cores are communication-idle,
Panthre enables leakage power savings of 36.9% on average, while
still providing connected and deadlock-free routes for all other nodes.
Categories and Subject Descriptors
C.1.2 [PROCESSOR ARCHITECTURES]: Multiprocessors—Interconnection Architectures
General Terms
Design,Algorithms
Keywords
network-on-chip, power-gating, routing-reconﬁguration
1.
INTRODUCTION
Networks-on-chip (NoCs) have become increasingly widespread in
recent years due to the extensive integration of many components
in modern multicore processors and SoC designs. NoCs are scalable and ﬂexible, however, they are crippled by excessive power consumption [6]. Particularly problematic for NoC structures is leakage
power, which is dissipated regardless of communication activity or
lack thereof. At high network utilization, static power may comprise
more than 74% of the total NoC power at a 22nm technology node
[16], and this ﬁgure is expected to increase in future technology generations. At low network utilization, leakage power is an even higher
fraction of the total power budget for the NoC. With growing system
integration, larger and larger portions of the NoC will be only lightly
used at any point in time, with the lightly used set varying with each
application and even within a single application over time.
As the NoC is a distributed and shared resource, conventional powergating schemes [7] that opportunistically put components to sleep durPermission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee. Request permissions
from Permissions@acm.org.
DAC 2014, June 1-5, 2014, San Francisco, California, USA.
Copyright is held by the owner/authors. Publication rights licensed to ACM.
ACM 978-1-4503-2730-5/14/06 ...$15.00.
(cid:6)(cid:7)
(cid:11)
(cid:14)
(cid:4)
(cid:6)(cid:1)
(cid:2)
(cid:5)
(cid:13)
(cid:12)
(cid:11)
(cid:5)
(cid:10)
(cid:9)
(cid:2)
(cid:8)
(cid:7)
(cid:6)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:7)
(cid:1)
(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:14)(cid:17)(cid:16)(cid:13)(cid:15)(cid:18)(cid:11)(cid:10)(cid:11)(cid:19)(cid:18)
(cid:15)(cid:18)(cid:17)(cid:1)
(cid:15)(cid:16)(cid:17)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:11)
(cid:14)
(cid:4)
(cid:2)
(cid:5)
(cid:13)
(cid:12)
(cid:11)
(cid:5)
(cid:10)
(cid:9)
(cid:2)
(cid:8)
(cid:7)
(cid:6)
(cid:6)
(cid:1)
(cid:2)(cid:7)
(cid:7)(cid:1)
(cid:8)(cid:7)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:6)(cid:1)(cid:1) (cid:1)
(cid:1)
(cid:6)(cid:1)
(cid:2)(cid:1)
(cid:9)(cid:1)
(cid:6)(cid:4)(cid:5)(cid:8)(cid:3)(cid:7)(cid:10)(cid:19)(cid:2)(cid:10)(cid:6)(cid:2)(cid:9)(cid:7)(cid:19)(cid:20)(cid:12)(cid:2)(cid:21)(cid:1)(cid:2)(cid:22)(cid:2)(cid:23)(cid:2)(cid:9)(cid:10)(cid:24)(cid:2)(cid:3)(cid:10)(cid:2)(cid:13)(cid:7)(cid:25)(cid:13)(cid:2)(cid:26)(cid:12)(cid:5)(cid:25)(cid:14)
Figure 1: Fraction of trafﬁc load shared by the least utilized links. The
fraction of trafﬁc transferred by the bottom 30% of used links is small, thus
Panthre targets this pool for power-gating without burdening other links.
""
%
(
#
&
(cid:30)
$
’
)
(cid:26)
(cid:26)
(cid:25)
(cid:12)(cid:13)(cid:14)(cid:12)(cid:15)(cid:15)(cid:7)(cid:16)(cid:17)(cid:18)(cid:11)(cid:19)(cid:11)(cid:3)(cid:2)(cid:7)
(cid:17)(cid:2)(cid:8)(cid:11)(cid:2)(cid:17)
(cid:26)
(cid:26)
(cid:25)
(cid:16)(cid:11)(cid:19)(cid:9)(cid:22)(cid:11)(cid:29)(cid:10)(cid:9)(cid:17)(cid:16)(cid:7)
(cid:18)(cid:3)(cid:2)(cid:9)(cid:22)(cid:3)(cid:5)(cid:7)(cid:21)(cid:11)(cid:1)(cid:7)
(cid:9)(cid:28)(cid:22)(cid:17)(cid:19)(cid:28)(cid:3)(cid:5)(cid:16)(cid:7)
(cid:10)(cid:23)(cid:16)(cid:1)(cid:9)(cid:17)
(cid:5)(cid:11)(cid:2)(cid:20)(cid:7)(cid:1)(cid:18)(cid:9)(cid:11)(cid:21)(cid:11)(cid:9)(cid:6)(cid:7)
(cid:18)(cid:3)(cid:10)(cid:2)(cid:9)(cid:17)(cid:22)(cid:7)(cid:1)(cid:2)(cid:16)(cid:7)
(cid:18)(cid:3)(cid:4)(cid:23)(cid:1)(cid:22)(cid:1)(cid:9)(cid:3)(cid:22)(cid:7)(cid:24)(cid:25)(cid:26)(cid:26)(cid:27)
*(cid:1)(cid:2)(cid:9)(cid:28)(cid:22)(cid:17)(cid:7)(cid:17)+(cid:10)(cid:11)(cid:23)(cid:23)(cid:17)(cid:16)(cid:7)
(cid:22)(cid:3)(cid:10)(cid:9)(cid:17)(cid:22)(cid:19)
(cid:30)(cid:31)(cid:29)(cid:11)(cid:9)(cid:7) (cid:11)(cid:22)(cid:17)(cid:7)!(cid:3)(cid:22)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:6)(cid:7)
(cid:29)(cid:22)(cid:3)(cid:1)(cid:16)(cid:18)(cid:1)(cid:19)(cid:9)
(cid:18)(cid:3)(cid:2)(cid:8)(cid:17)(cid:19)(cid:9)(cid:11)(cid:3)(cid:2)(cid:14)
(cid:4)(cid:11)(cid:19)(cid:22)(cid:3)(cid:10)(cid:9)(cid:11)(cid:2)(cid:8)
(cid:25)(cid:26)(cid:26)
(cid:22)(cid:17)(cid:18)(cid:3)(cid:2)!,(cid:7) (cid:11)(cid:22)(cid:17)(cid:19)(cid:7)
(cid:9)(cid:3)(cid:7)(cid:2)(cid:17)(cid:11)(cid:8)(cid:28)(cid:29)(cid:3)(cid:22)(cid:19)
(cid:22)(cid:3)(cid:10)(cid:9)(cid:17)(cid:7)
(cid:18)(cid:3)(cid:4)(cid:23)(cid:10)(cid:9)(cid:17)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:6)(cid:7)
(cid:4)(cid:8)(cid:4)(cid:9)(cid:7)(cid:10)(cid:2)(cid:11)(cid:9)
(cid:25)(cid:26)(cid:26)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:6)(cid:7)!(cid:5)(cid:1)(cid:8)(cid:7)(cid:1)(cid:2)(cid:16)(cid:7)
(cid:22)(cid:3)(cid:10)(cid:9)(cid:17)(cid:22)(cid:7)
(cid:22)(cid:17)(cid:18)(cid:3)(cid:2)!(cid:11)(cid:8)(cid:10)(cid:22)(cid:1)(cid:9)(cid:11)(cid:3)(cid:2)(cid:7)
(cid:16)(cid:1)(cid:9)(cid:1)(cid:23)(cid:1)(cid:9)(cid:28)(cid:7)
 (cid:11)(cid:22)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:8)(cid:4)(cid:17)(cid:2)(cid:9)(cid:19)
Figure 2: Overview of Panthre. Panthre consists of four components at each
router: a) a usage activity count and compare (ACC) framework to identify
lightly used components, b) an ON/OFF decision engine that determines the set
of components to be power-gated, c) a Panthre-enabled route computation unit
that can execute in the background without interrupting regular NoC operation,
and d) an anomaly-based feedback algorithm that tracks application’s needs
dynamically and send updates to the ON/OFF engine.
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)
(cid:11)(cid:9)(cid:12)(cid:13)(cid:8)(cid:14)(cid:15)(cid:16)(cid:10)(cid:11)(cid:14)
Panthre deploys a simple and distributed framework for activity
collection and subsequent exclusion of low-usage components. Even
though frequent decisions to power-gate components are made locally
at each router, Panthre’s novel reconﬁguration solution ensures uninterrupted full connectivity and deadlock-freedom of the NoC topology globally at runtime. Panthre, by construction, is also free of
reconﬁguration-induced routing deadlock [9], thus eliminating the need
for costly deadlock-recovery protocols [2, 14]. Finally, it can reconﬁgure frequently without ever interrupting normal network operation.
In our evaluation with multiprogrammed benchmarks running on a
8x8 mesh network, Panthre was able to reduce NoC power consumption by 14.5% on average for communication-light workloads, while
causing less than a 2% application slowdown.
In contrast, powergating with lookahead [10, 11], leads to 9-11% application slowdown
if implemented at a router level, while causing as much as 20% performance loss if ﬁne-grained power-gating is applied. Finally, Panthre leakage power savings can be as much as 36.9% on average,
under the fair assumption that 10-16 nodes of a 64-node CMP are
communication-idle at any given time.
2. RELATED WORK
A number of power-gating schemes have been been proposed for
NoC components, operating at different levels of granularity: routers
[10], ports [11], VCs [12], buffers [8]. However, all such schemes suffer from accumulated wakeup times and excess energy spent in each
power-gating event. NoRD [2] elongates periods of router sleep by
steering light, sleep-interrupting trafﬁc to a low-power ring network
that is always kept active. However, NoRD requires additional VC resources for deadlock-freedom. In addition, NoRD requires substantial
dedicated hardware design and veriﬁcation effort. Router Parking [14]
proposes to completely switch off routers associated with idle cores by
leveraging route-reconﬁguration at a central node. Such a process requires dedicated channels to communicate with the central entity and
typically takes a long time, often requiring to suspend network operation. As a result, such centralized schemes provide little adaptivity and
can only be applied at a coarse-granularity (entire routers), and only
when communication patterns are known well in advance. Catnap [4]
proposes the use of multiple lightweight networks in CMPs, while
applying power-gating at the network granularity to keep the performance overhead low. However, Catnap is only applicable to CMPs
with high-bandwidth requirements, and unlike Panthre, it cannot be
utilized for ﬁne-grained power-gating.
Finally, route-reconﬁguration has been well studied in the literature
in the context of fault-tolerant routing. However, faults are rare occurrences, and therefore the majority of the algorithms designed for
fault tolerance are centralized and take signiﬁcant time and hardware
resources. A notable exception is Ariadne [1], which describes a very
quick (4K cycles for a 64 node network) and lightweight route reconﬁguration scheme (<2% area overhead). Ariadne, however, also requires suspending the NoC operation during reconﬁguration and can
lead to reconﬁguration-induced deadlocks. In our context, since we
frequently reconﬁgure the NoC routing, network suspension would be
detrimental. In contrast, our route reconﬁguration algorithm, although
inspired by Ariadne, provides deadlock-freedom throughout the reconﬁguration, with no interruption of the mainstream NoC activity.
(cid:16)(cid:6)(cid:7)(cid:17)(cid:7)(cid:15)(cid:6)(cid:18)(cid:19)(cid:20)(cid:21)(cid:11)(cid:2)(cid:7)(cid:4)(cid:14)(cid:21)(cid:3)(cid:15)(cid:8)(cid:4)(cid:22)(cid:9)(cid:23)(cid:9)(cid:24)(cid:25)(cid:16)(cid:6)(cid:7)(cid:17)(cid:7)(cid:15)(cid:6)(cid:18)(cid:11)(cid:2)(cid:7)(cid:4)(cid:14)(cid:21)(cid:3)(cid:15)(cid:8)(cid:4)(cid:22)(cid:9)(cid:23)(cid:9)(cid:16)(cid:6)(cid:7)(cid:17)(cid:7)(cid:15)(cid:6)(cid:18)(cid:10)(cid:4)(cid:14)(cid:6)(cid:5)(cid:22)(cid:9)(cid:26)(cid:9)(cid:27)(cid:28)(cid:29)(cid:30)(cid:9)(cid:23)(cid:9)(cid:31)(cid:28)(cid:29) !(cid:9)(cid:23)(cid:9)(cid:27)!
(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:9)(cid:12)(cid:7)(cid:5)(cid:14)
(cid:3)(cid:13)(cid:6)(cid:6)(cid:16)(cid:5)
(cid:5)(cid:7)(cid:13)(cid:14)(cid:16)(cid:9)(cid:18)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)(cid:16)
(cid:5)(cid:7)(cid:13)(cid:14)(cid:16)(cid:9)(cid:18)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)(cid:16)
(cid:15)
(cid:14)
(cid:5)
(cid:7)
(cid:12)
(cid:9)
(cid:14)
(cid:13)
(cid:12)
(cid:11)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)
(cid:7)(cid:13)(cid:14)(cid:12)(cid:13)(cid:14)(cid:9)(cid:12)(cid:7)(cid:5)(cid:14)
(cid:11)(cid:7)(cid:4)(cid:7)(cid:2)(cid:7)(cid:4)(cid:14)(cid:9)
(cid:3)(cid:6)(cid:15)(cid:8)(cid:6)(cid:13)(cid:4)
(cid:10)
(cid:9)
(cid:8)
(cid:7)
(cid:5)
(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)
(cid:18)(cid:7)(cid:11)(cid:14)(cid:4)(cid:18)(cid:14)(cid:19)(cid:9)
(cid:20)(cid:1)(cid:21)(cid:9)(cid:22)(cid:23)(cid:24)(cid:9)
(cid:4)(cid:25)(cid:25)(cid:7)(cid:18)(cid:4)(cid:14)(cid:7)(cid:5)(cid:15)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:5)(cid:10)(cid:1)(cid:4)(cid:6)(cid:5)
(cid:14)(cid:7)(cid:9)(cid:7)(cid:14)(cid:17)(cid:16)(cid:5)(cid:9)
(cid:5)(cid:7)(cid:13)(cid:14)(cid:16)(cid:5)(cid:15)
(cid:6)(cid:5)(cid:7)(cid:8)(cid:9)(cid:7)(cid:14)(cid:17)(cid:16)(cid:5)(cid:9)
(cid:5)(cid:7)(cid:13)(cid:14)(cid:16)(cid:5)(cid:15)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)
(cid:15)
(cid:14)
(cid:5)
(cid:7)
(cid:12)
(cid:9)
(cid:14)
(cid:13)
(cid:12)
(cid:14)
(cid:13)
(cid:7)
(cid:9)
(cid:7)
(cid:14)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)
(cid:18)(cid:7)(cid:11)(cid:14)(cid:4)(cid:18)(cid:14)(cid:9)
(cid:4)(cid:25)(cid:25)(cid:7)(cid:18)(cid:4)(cid:14)(cid:7)(cid:5)(cid:15)
(cid:11)(cid:10)(cid:12)(cid:13)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:5)(cid:10)(cid:1)(cid:4)(cid:6)(cid:5)
Figure 3: A datapath segment. 99% of the router leakage power is dissipated
in the 5 datapath segments of the baseline mesh router. Results from DSENT
at 22nm node.
3. PANTHRE DESIGN
Panthre consists of four components at each router: i) a component
usage collection framework (ACC), ii) a lightweight ON/OFF decision
engine that determines the set of links to power-gate based on local usage data, iii) a route compute module that updates the routing tables
using broadcasts after each decision event, without interrupting normal NoC operation, and iv) a feedback-based anomaly-detection and
management unit that communicates updates to the ON/OFF decision
engine so that Panthre can adapt dynamically to changing communication patterns in the application over time. The four components of
Panthre are highlighted in Figure 2. These four components are implemented using fast and lightweight distributed hardware, with minimal
information communicated globally via a few single-bit wires. The
lightweight distributed hardware allows Panthre to adjust to application communication needs very quickly and without ever interrupting
normal network operation.
3.1 Fine-Grained Power Gating
Panthre provides ﬁne-grained power-gating by allowing components
to be excluded at the granularity of a single unidirectional link. Upon
careful examination of routers’ datapath, we identiﬁed that poweringdown a unidirectional link between two routers is equivalent to powering down the corresponding crossbar contact and the output port at
the upstream router, the unidirectional link itself, and the input port,
input buffer and crossbar contacts at the downstream router. We call
this combined set of components, a datapath segment: it represents
the smallest granularity at which routing-based reconﬁguration can
be applied for the purpose of power-gating. The concept of a datapath segment is illustrated in Figure 3. Fortunately, crossbar, links and
buffers consume most of the leakage power in a router, and they can all
be powered off by our approach. DSENT [16] reports that 99% of the
leakage power consumption of our baseline mesh router synthesized at
22nm can be attributed to its 5 datapath segments. The remaining 1%
is attributed to shared units such as route computation and allocators.
Note that Panthre is unable to exclude the local datapath segment (the
one connecting to the local core) in absence of alternate paths for them
to connect to the NoC. Therefore, in the rest of this paper, we exclude
these local datapath segments from our computations, and assume that
orthogonal power-saving schemes are being deployed for them.
3.2 Execution Flow
Panthre operates in epoch-based execution, with power-gating decisions made at the beginning of each epoch for the entire epoch. Thus,
Panthre can ensure that power-gated components can be off for at
least one epoch-long interval. The distributed activity counter units
(ACC, Figure 2) periodically collect datapath segment usage statistics by means of simple counters: the data is then used to guide the
decision process for the next epoch. Panthre’s decision process is simple: all datapath-segments that experience activity below a threshold
(AT H ) are put to sleep. Thereafter, a route update process updates
routing tables to operate the network in the new conﬁguration. Note,
however, that different communication loads require different AT H
for Panthre to be effective. A ﬁxed value could lead to an excessive
or insufﬁcient number of power-gated segments. Therefore, we propose a threshold update algorithm that leverages feedback from the
distributed anomaly detection units. When the number of segments
power-gated is excessive, two types of anomalies may arise: i) a large
fraction of packets suffer long detours to their destinations, and ii)
congestion due to the increased load on active links.
Thus, we detect these anomalies locally at each router and broadcast them globally using single-bit wires. Each ON/OFF decision engine is equipped with logic to update the threshold value based on
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)
(cid:9)(cid:5)(cid:2)(cid:7)(cid:10)(cid:11)(cid:5)(cid:1)
* (cid:17)(cid:28)(cid:29)(cid:27)(cid:22)
!(cid:13)(cid:16)(cid:11)(cid:23)""(cid:13)(cid:20)(cid:27)(cid:28)(cid:19)(cid:13)(cid:28)(cid:16)(cid:15)(cid:20)(cid:28)(cid:24)(cid:23)(cid:20)(cid:24)(cid:29)(cid:19)(cid:13)(cid:22)(cid:28)(cid:16)(cid:15)(cid:16)(cid:19)(cid:20)(cid:15)(cid:13)(cid:11)(cid:13)(cid:23)""(cid:29)(cid:24)(cid:23)(cid:16)(cid:13)(cid:20)(cid:29)(cid:13)(cid:15)#(cid:24)(cid:20)(cid:23)"" (cid:29)(cid:30)(cid:30)(cid:13)(cid:20)#(cid:29)(cid:13)(cid:31)(cid:22)(cid:11)(cid:20)""(cid:13)(cid:15)(cid:16)(cid:17)(cid:18)(cid:16)(cid:19)(cid:20)(cid:15)
!(cid:13)(cid:11)(cid:14)(cid:14)(cid:13)(cid:23)(cid:29)(cid:19)(cid:30)(cid:24)(cid:17)(cid:27)(cid:28)(cid:11)(cid:20)(cid:24)(cid:29)(cid:19)(cid:15)(cid:13)(cid:30)(cid:29)(cid:14)(cid:14)(cid:29)#(cid:13)(cid:29)(cid:28)(cid:24)(cid:17)(cid:24)(cid:19)(cid:11)(cid:14)(cid:13)(cid:20)(cid:27)(cid:28)(cid:19)(cid:13)(cid:28)(cid:16)(cid:15)(cid:20)(cid:28)(cid:24)(cid:23)(cid:20)(cid:24)(cid:29)(cid:19)(cid:15)
!(cid:13)(cid:11)(cid:14)(cid:14)(cid:13)(cid:23)(cid:29)(cid:19)(cid:30)(cid:24)(cid:17)(cid:27)(cid:28)(cid:11)(cid:20)(cid:24)(cid:29)(cid:19)(cid:15)(cid:13)(cid:17)(cid:27)(cid:11)(cid:28)(cid:11)(cid:19)(cid:20)(cid:16)(cid:16)(cid:13)(cid:31)(cid:16)(cid:11)(cid:31)(cid:14)(cid:29)(cid:23)(cid:21) (cid:30)(cid:28)(cid:16)(cid:16)(cid:13)(cid:23)(cid:29)(cid:19)(cid:19)(cid:16)(cid:23)(cid:20)(cid:24)(cid:25)(cid:24)(cid:20)$
(cid:1)
(cid:3)
(cid:20)
(cid:29)
(cid:29)
(cid:28)
(cid:7)
(cid:2)
(cid:4)
(cid:20)(cid:27)(cid:28)(cid:19)
(cid:28)(cid:16)(cid:15)(cid:20)(cid:28)(cid:24)(cid:23) (cid:13)
(cid:20)(cid:24)(cid:29)(cid:19)
(cid:8)
(cid:5)
(cid:6)
(cid:9)
(cid:1)
(cid:3)
(cid:20)
(cid:29)
(cid:29)
(cid:28)
(cid:7)
(cid:2)
(cid:24)(cid:19)(cid:11)(cid:23)(cid:20)(cid:24)(cid:25)(cid:16)
(cid:4)
(cid:8)
(cid:5)
(cid:6)
(cid:9)
(cid:1)
(cid:3)
(cid:2)
(cid:15)(cid:11)(cid:18)(cid:16)(cid:13)(cid:13)(cid:20)(cid:27)(cid:28)(cid:19)(cid:13)
(cid:28)(cid:16)(cid:15)(cid:20)(cid:28)(cid:24)(cid:23)(cid:13)(cid:13) (cid:20)(cid:24)(cid:29)(cid:19)(cid:15)
(cid:4)
(cid:20)
(cid:29)
(cid:29)
(cid:28)
(cid:7)
(cid:8)
(cid:5)
(cid:6)
(cid:9)
(cid:10)(cid:11)(cid:12)(cid:13)(cid:11)(cid:14)(cid:14)(cid:13)(cid:15)(cid:16)(cid:17)(cid:18)(cid:16)(cid:19)(cid:20)(cid:15)(cid:13)(cid:21)(cid:16)(cid:22)(cid:20)(cid:13)(cid:11)(cid:23)(cid:20)(cid:24)(cid:25)(cid:16)(cid:26)(cid:13)
(cid:10)%(cid:12)(cid:13)(cid:15)(cid:16)(cid:17)(cid:18)(cid:16)(cid:19)(cid:20)(cid:15)(cid:13)(cid:29)(cid:19)(cid:14)$(cid:13)(cid:29)(cid:19)(cid:13)(cid:20)""(cid:16)(cid:13)
(cid:10)(cid:23)(cid:12)(cid:13)(cid:11)(cid:31)(cid:11)(cid:22)(cid:20)(cid:24)(cid:25)(cid:16)(cid:13)(cid:15)(cid:16)(cid:17)(cid:18)(cid:16)(cid:19)(cid:20)(cid:13)
(cid:11)(cid:14)(cid:14)(cid:13)(cid:20)(cid:27)(cid:28)(cid:19)(cid:13)(cid:28)(cid:16)(cid:15)(cid:20)(cid:28)(cid:24)(cid:23)(cid:20)(cid:24)(cid:29)(cid:19)(cid:15)(cid:13)(cid:16)(cid:19)(cid:30)(cid:29)(cid:28)(cid:23)(cid:16)(cid:31)
(cid:15)(cid:22)(cid:11)(cid:19)(cid:19)(cid:24)(cid:19)(cid:17)(cid:13)(cid:20)(cid:28)(cid:16)(cid:16)(cid:13)(cid:21)(cid:16)(cid:22)(cid:20)(cid:13)(cid:11)(cid:23)(cid:20)(cid:24)(cid:25)(cid:16)
&’(&))(cid:13)(cid:18)(cid:11)(cid:19)(cid:11)(cid:17)(cid:16)(cid:18)(cid:16)(cid:19)(cid:20)
Figure 4: Panthre’s reconﬁguration algorithm allows power-gating decisions
to be made independently at each router, without causing disconnection or
deadlock. a) Breadth-ﬁrst construction of the up*/down* spanning tree and
corresponding turn restrictions. Each turn restriction node (L-group), presents
a power-gating choice between datapath-segments. b) A minimally-connected
network conﬁguration degenerates into a spanning tree. c) A dynamicallyadapted NoC where low-usage datapath-segments are power-gated.
this information. Note that the global anomaly broadcast ensures that
AT H values are kept consistent throughout the NoC. This aspect, in
turn, guarantees that power-gating decisions are fair, tackling the least
used segments in the NoC. In addition, Panthre naturally provides the
ability to systematically trade-off performance for power savings by
adjusting criterion for the detection of these anomalies. With Panthre,
stable and power-efﬁcient conﬁgurations are typically attained 10-15
epochs (1 epoch is 10K cycles in our design) after a program phase
change, which is quick considering that application phases are up to
10s of millions of cycles.
3.3 Reconﬁguration Algorithm
A hallmark of Panthre is that all power-gating decisions can be
made independently at each router, while deadlock-freedom and connectivity among all nodes is still guaranteed throughout execution.
This allows for frequent reconﬁgurations, in the order of one reconﬁguration event per tens of thousands of cycles. In addition, Panthre
eliminates the need of any additional hardware to recover from pathological scenarios such as deadlock. This is a great advantage in terms
of silicon cost (and power), and it also limits the impact of reconﬁguration on performance. Panthre’s reconﬁguration is based on the
up*/down* routing algorithm, which breaks deadlocks by forbidding
certain through-router connections between non-local links (‘turns’).
Up*/down* routing works by organizing all network nodes on a spanning tree, starting from a root node of choice. Each node receives a
unique order based on its distance from the root, equidistant nodes are
ordered arbitrarily. Thereafter, all routes involving going ﬁrst away
from the root node (down-traversal) and then towards it (up-traversal)
are marked invalid. This ensures deadlock-freedom, as all dependency cycles involve at least one ‘down→up turn’(down-traversal followed by up-traversal). A breadth-ﬁrst construction of the spanning
tree rooted at node 0 is shown in Figure 4a. As an example, 1→4
is a down-traversal, while 4→3 is an up-traversal. Therefore, turn
1→4→3 must be marked invalid.
Note that a spanning tree, by deﬁnition, connects all the nodes in the
graph and it is acyclic. In this context, Panthre’s route-reconﬁguration
algorithm leverages the fact that turn restrictions are placed between
two links when only one of them can be part of the spanning tree.
Because of this, it is possible to power-gate one of the two datapath
segments connected to a disabled turn and still maintain full network
connectivity. Figure 5 illustrates the property just outlined: the left
portion of the ﬁgure shows a spanning tree construction, such that
nodes 0,1 and 3 are already on the spanning tree rooted at node R,
while node 2 is being added. It can be noted that either link 0-2 or
link 3-2 are sufﬁcient to connect to node 2. Since both are available,
there will be a turn restriction 0-2-3. A similar situation is shown on
the right side of the ﬁgure, where links 1-3 and 2-3 are sufﬁcient to
reach the to-be-added node 3, and the turn restriction is 1-2-3. The
middle part of the ﬁgure shows a more general case, where both node
2 and 3 are being added to the spanning tree, using links 0-2 and 1-3,
respectively. In this case, either the turn 0-2-3 or 1-3-2 must be disabled to break the cycle. Depending on the turn restriction placement,
this situation degenerates into the one shown on the left or the right.
In order to organize Panthre’s reconﬁguration process, we call any
two links connected by a disabled turn an L-group, as shown in Fig(cid:11)(cid:23)(cid:25)(cid:8)(cid:8)(cid:12)(cid:8)(cid:26)(cid:9)
(cid:5)(cid:7)(cid:10)(cid:10)(cid:9)(cid:26)(cid:7)(cid:14)(cid:27) (cid:5)(cid:15) (cid:9)
(cid:28)(cid:12)(cid:7)(cid:10)(cid:13)(cid:5)(cid:12)(cid:14)(cid:8)
(cid:21)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)
(cid:7)(cid:10)(cid:11)(cid:5)(cid:7)(cid:12)(cid:13)(cid:5)(cid:12)(cid:14)(cid:8) (cid:9)
(cid:13)(cid:15)(cid:14) (cid:12)(cid:13)(cid:10)
(cid:29)
(cid:30)
(cid:4)
(cid:2)
(cid:1)
(cid:3)
(cid:7)(cid:14)(cid:14) (cid:5)(cid:9)
(cid:21)
(cid:16)(cid:17)(cid:17)(cid:18)(cid:19)(cid:12)(cid:8)(cid:20)(cid:9)
(cid:13)(cid:15)(cid:14) (cid:12)(cid:13)(cid:10)(cid:11)(cid:9)
(cid:1)
(cid:3)
(cid:4)
(cid:2)
(cid:7)(cid:14)(cid:14) (cid:5)(cid:9)
(cid:21)
(cid:16)(cid:17)(cid:17)(cid:18)(cid:19)(cid:12)(cid:8)(cid:20)(cid:9)
(cid:13)(cid:15)(cid:14) (cid:12)(cid:13)(cid:10)(cid:11)(cid:9)
(cid:4)
(cid:2)
(cid:1)
(cid:3)
(cid:13)(cid:14)(cid:22)(cid:23)(cid:6) (cid:19)(cid:11)(cid:14)(cid:7)(cid:24)(cid:9)(cid:19)(cid:12)(cid:8)(cid:20)(cid:11)
(cid:13)(cid:15)(cid:14) (cid:12)(cid:13)(cid:10)(cid:9)(cid:19)(cid:12)(cid:8)(cid:20)(cid:11)
(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:5)(cid:7)(cid:12)(cid:13)(cid:5)(cid:12)(cid:14)(cid:8)
(cid:14)(cid:8) (cid:9)(cid:5)(cid:15)(cid:10) (cid:9)(cid:5)(cid:7)(cid:10)(cid:10)
(cid:5)(cid:14)(cid:9)(cid:31)(cid:10)(cid:9)(cid:25)(cid:28)(cid:28)(cid:10)(cid:28)
Figure 5: Each turn restriction provides an opportunity for power-gating
one bidirectional link. Panthre leverages this property to put low-utilization
links to sleep. Note how the property holds for any topology.
ure 4a. Therefore, a decision can be taken locally at each L-group, to
power-gate one of the two bi-directional datapath links, while the network would still be connected globally. Note that each bi-directional
links comprises two opposite unidirectional datapath segments (see
Section 3.1). In the extreme case when all L-groups decide to powerdown two datapath segments each, the topology will degenerate into a
spanning tree, as shown in Figure 4b. Panthre leverages a distributed
and adaptive datapath-segment ON/OFF decision engine that determines the power-gating decisions locally at each L-group, according
to application communication characteristics. An example network
conﬁguration produced using Panthre is shown in Figure 4c.
Even though many reconﬁguration algorithms [1, 14] replace deadlock free routing paths with another set of deadlock-free routing paths
after reconﬁguration, packets in-transit following the old routing paths
can cause deadlocks by interacting with packets following the new
routing paths. This is because paths valid in the old routing function
might be disabled in the new routing function, or vice-versa. To circumvent this issue, Panthre ensures that any newly developed routing
conﬁguration complies with the turn-restrictions that were determined
for an all-powered-ON NoC conﬁguration (e.g., Figure 4a), eliminating the possibility of reconﬁguration-induced deadlocks. Since Panthre only disables a link if it is part of a turn-restriction (L-group), the
corresponding turn would not be exercised, whether the corresponding link is enabled or disabled. Intuitively, if all L-groups maximally
power-gate, the network topology will degenerate into a "
Quality-of-Service for a High-Radix Switch.,"Communication in multi-processor systems-on-chip requires guaranteed throughput and latency. If the network is unaware of ongoing communication patterns, applications may not receive their necessary bandwidth or may suffer high network latencies. Many techniques have been proposed to provide quality-of-service (QoS) in the network by regulating network traffic; however, as network sizes have increased, the complexity of these techniques has grown as well, particularly in the case of multi-hop networks. In this paper, we propose an efficient QoS implementation for a single-stage, high-radix switch, which is readily scalable to 64 nodes. In addition to best effort and guaranteed throughput services, we implement a guaranteed latency traffic class with a latency bound. Our implementation allows systems significantly larger than most current multi-core chips to be implemented without the need for difficult and complex multi-hop QoS.","Quality-of-Service for a High-Radix Switch
Nilmini Abeyratne, Supreet Jeloka, Yiping Kang,
David Blaauw, Ronald G. Dreslinski, Reetuparna Das and Trevor Mudge
University of Michigan, Ann Arbor, MI 48109
ABSTRACT
Communication in multi-processor systems-on-chip requires
guaranteed throughput and latency. If the network is unaware of ongoing communication patterns, applications may
not receive their necessary bandwidth or may suﬀer high
network latencies. Many techniques have been proposed to
provide quality-of-service (QoS) in the network by regulating network traﬃc; however, as network sizes have increased,
the complexity of these techniques has grown as well, particularly in the case of multi-hop networks.
In this paper, we propose an eﬃcient QoS implementation for a single-stage, high-radix switch, which is readily
scalable to 64 nodes. In addition to best eﬀort and guaranteed throughput services, we implement a guaranteed latency traﬃc class with a latency bound. Our implementation allows systems signiﬁcantly larger than most current
multi-core chips to be implemented without the need for
diﬃcult and complex multi-hop QoS.
Categories and Subject Descriptors
C.1.2 [Processor Architectures]: Multiprocessors—Interconnection architectures
General Terms
Algorithms, Design
Keywords
network-on-chip, quality-of-service
1.
INTRODUCTION
A system-on-chip (SoC) with real-time deadlines (e.g. a
base station or an embedded system) consists of workloads
that have both bandwidth requirements and latency constraints. The on-chip network in an SoC is shared among
many nodes (cores/accelerators/IP blocks). Networks that
are application-unaware will treat all messages equally even
though some applications may require more bandwidth or
lower latency for their messages. Quality-of-service (QoS)
techniques can be added to networks to make them more
application-aware. QoS techniques regulate access to a shared
node, such as the memory controller, so that an application
can meet its needs without degrading the performance of
other applications.
Typically, QoS is implemented by controlling a variety of
behaviors such as the injection time of packets into the network, total number of packets injected, and speciﬁc use of
virtual channels and physical links [3, 6, 7, 8, 13, 18, 19]. As
the number of nodes
in systems continue to grow, a scalable interconnect becomes essential. Most recent work has
focused on multi-hop network-on-chips (NoCs) to accommodate this growing number of nodes. Implementing QoS
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
DAC ’14, June 01 - 05 2014, San Francisco, CA, USA
Copyright 2014 ACM 978-1-4503-2730-5/14/06 ...$15.00.
techniques in these NoCs, particularly to provide diﬀerentiated bandwidth and low latency services, is diﬃcult. Some
of these algorithms need information about traﬃc ﬂows to
be stored at each router in a multi-hop network, introducing
large amounts of buﬀering.
Recently, the Swizzle Switch [15, 16], a high-radix crossbar that can provide a point-to-point interconnect fabric for
many nodes, was introduced. While traditional crossbars
cannot scale to higher radices due to power and area complexities, the Swizzle Switch was demonstrated to scale to a
radix of 64 in 32nm technology at 1.5GHz [16]. This suggests that it can be used to build high-radix, single-crossbar
networks. We will show that it is possible to implement QoS
techniques within the Swizzle Switch with low complexity.
We augment the Swizzle Switch by implementing QoS
with three traﬃc classes: Best-Eﬀort (BE) class, Guaranteed Bandwidth (GB) class, and Guaranteed Latency (GL)
class, in that order of increasing priority. The BE class is
the default class in Swizzle Switch with least recently granted
(LRG) arbitration [15]. To maintain the bandwidth requirements of the GB class, we derive a mechanism from Virtual Clock [19], a well-known QoS algorithm designed for
packet switching networks. We eﬃciently integrate the Virtual Clock algorithm into the Swizzle Switch crossbar fabric
and perform switch arbitration in a single clock cycle. An
undesirable characteristic of Virtual Clock is that it couples
the network delay of a ﬂow with the bandwidth rate reserved
by that ﬂow. Packets from a ﬂow with a small reserved rate
have high network delay on average. Our interpretation of
the Virtual Clock algorithm in the Swizzle Switch improves
upon the original Virtual Clock algorithm by providing low
delays to small-bandwidth ﬂows.1 Finally, our GL class
is envisioned for sending infrequent, time-critical messages,
such as interrupts, that need to quickly pass through the
network. It is given the highest priority in the network, but
we put safeguards in place to prevent its abuse. We calculate
an upper bound in latency for GL class packets.
2. MOTIVATION AND BACKGROUND
2.1 Single-Crossbar Networks
A single-crossbar network is a single centralized crossbar
as the on-chip interconnect. Large crossbars were considered infeasible because their area and power grow quadratically with radix. However, the recent Swizzle Switch has
shown that crossbars can be designed more eﬃciently with
the help of advanced circuit technology [15]. The Swizzle
Switch crossbar reuses the wires of the output data bus to
also perform switch arbitration, hence saving both area and
energy. Single-crossbar networks provides beneﬁts such as
reduced network latency, dedicated input and output channels for each core, and uniform cache accesses. Furthermore,
adding support for QoS, which involves adding storage to
track ﬂow history and global knowledge of bandwidth capacity, can be more easily achieved in a single-switch than
in a multi-hop network.
1A ﬂow is a stream of packets that traverse the same route
from a source to a destination.
2.2 Quality-of-Service Algorithms
Several QoS approaches for networks have been proposed.
Static approaches such as Weighted Round Robin (WRR)
and Deﬁcit Weighted Round Robin (DWRR) can provide
strict bandwidth guarantees [17]. However, WRR and DWRR
lead to network underutilization as they do not distribute
leftover bandwidth equally to ﬂows with excess data or to
best-eﬀort ﬂows. Fair Queuing (FQ) and Weighted Fair
Queuing (WFQ) emulate bit-by-bit round robin (BR) service [2, 5, 12]. They compute ﬁnish times for packets, which
is the time that the packet would have been serviced had
the server been doing BR. However, computing ﬁnish times
in these schemes have O (N ) complexity. Globally Synchronized Frames (GSF) [8] is a frame-based approach that controls the number of packets injected into the network at the
source. It requires a global barrier network across all nodes,
which adds overhead and can be slow.
The QoS mechanism we chose to implement was inspired
by the Virtual Clock algorithm. Virtual Clock emulates time
division multiplexing (TDM). In a true TDM system, packets are serviced only in the time slots allocated to the source.
If the source has no packets to send, that time slot is wasted
and results in link underutilization. Unlike TDM, Virtual
Clock makes eﬃcient use of link capacity by redistributing
idle time slots to sources with excess demand.
A snippet of the Virtual Clock algorithm from [19] is
shown below. Each ﬂow has its own virtual time space kept
in a VirtualClock. Transmitting one packet marks the passing of one virtual time step, Vtick, which is the average
arrival time between packets from a ﬂow in real time clock
ticks. If the ﬂow sends packets according to its average rate,
its VirtualClock should approximately equal the real time
clock. In the presence of multiple ﬂows, the VirtualClock s
of all ﬂows are multiplexed to emulate a TDM system. Packets are stamped with the VirtualClock value of their ﬂows
at the time of their arrival and are transmitted according
to increasing time stamps.
If one ﬂow does not send any
packets for a while, its VirtualClock will fall behind. Then
with a sudden burst, that ﬂow can starve other ﬂows until
its VirtualClock value has caught up. To prevent ﬂows from
building up priority this way, their VirtualClock s are set to
at least the real time clock, as shown in step 1 in the algorithm below. Now, any burst of packets will be interleaved
with packets from other ﬂows.
Original Virtual Clock Algorithm from [19]
• Upon receiving each packet from f lowi ,
1. auxV C ← max(auxV C , real time)
2. auxV C ← (auxV C + V ticki )
3. stamp the packet with the auxV C value
• Transmit packets in the order of increasing stamp values.
Coupling of Bandwidth and Latency: A drawback of
the Virtual Clock algorithm is that it couples the reserved
rate of bandwidth to latency. Flows with high reserved rates
observe low average latency while ﬂows with low reserved
rates observe high average latency. The reason is that ﬂows
with high reserved rates have more packets to send; therefore, the switch must schedule these ﬂows for data transmission more frequently. This results in very low average
latency for packets. On the other hand, ﬂows with low reserved rates are scheduled less frequently, resulting in high
average latency for packets.
Previous work in the Swizzle Switch [14] had implemented
a 4-level message-based QoS arbitration scheme. Our implementation has three main diﬀerences from the 4-level QoS
implementation [14]. First, we allocate certain fractions of
bandwidth to each input and ensure that they receive at
least that much bandwidth. In the previous design inputs
could only assign a priority level to messages and could not
control how much bandwidth each priority level receives.
Second, the previous design used a ﬁxed-priority QoS mechanism (highest level messages are prioritized ﬁrst), which
could lead to starvation of messages in other levels. Third,
the previous design required two arbitration cycles, whereas
our entire arbitration (Virtual Clock arbitration + LRG arbitration) is within a single cycle. This is one of the new
contributions of our work.
3. SWIZZLE SWITCH WITH QOS
In this work, we implement QoS in a single-crossbar network by extending the Swizzle Switch architecture to support three diﬀerent traﬃc classes. We brieﬂy explain each
class and their priorities below.
Best-Eﬀort (BE) class is applicable to ﬂows that require neither guaranteed bandwidth nor guaranteed latency
services. This class has the lowest priority in the network.
Guaranteed Bandwidth (GB) class is applicable to
ﬂows that require a guaranteed bandwidth from the network but not a tight bound on network latency; therefore,
this traﬃc class has the second highest priority in the network. Bandwidth guarantees are maintained using the Virtual Clock algorithm.
Guaranteed Latency (GL) class is intended for timecritical packets that need to quickly traverse the network,
such as interrupts or watchdog timers. The GL class is maintained by giving it the highest priority in the system. GL
packets are serviced before any GB packets and may cause
a disruption in GB services. To ensure that the GL class
does not completely deny service to the GB class, a small
fraction of bandwidth is reserved for it.
3.1 Swizzle Switch-Virtual Clock
The GB class is achieved by integrating the Virtual Clock
algorithm into the Swizzle Switch , henceforth referred to
as Swizzle Switch -Virtual Clock (SSVC ). In a Swizzle Switch , each crosspoint is conﬁgured to transmit packets
of one particular ﬂow, (I ni ,Outo ). We added the following
components to each crosspoint to support QoS:
• A virtual clock counter (auxV C )
• Virtual clock thermometer code register
• Virtual clock increment value register (V tick)
• Replicated LRG arbitration logic
The virtual clock counter, auxV C , tracks the bandwidth
usage history of the ﬂow and is incremented by V tick each
time a packet is transmitted. During arbitration, auxV C
values of requesting inputs are compared. We modiﬁed the
switch arbitration circuitry to enable this comparison. The
modiﬁcations are explained in detail below and also in Figure 1 and Figure 2. In Figure 1, ﬁve inputs of an 8-input
switch is shown to be requesting some output M . To determine the winner, each input’s auxV C counters are used as
the priority values in an inhibit-based arbitration.
Swizzle Switch ’s Inhibit-Based Arbitration: The Swizzle Switch employs an inhibit-based arbitration mechanism.
In the beginning of the arbitration cycle, a subset of the output bus’ bitlines, which are repurposed to perform switch arbitration, are pre-charged. During the arbitration, requesting inputs with higher priorities discharge the bitlines that
they have priority over to inhibit inputs of lower priorities.
At the end of the arbitration cycle, each input senses just
one wire (the wire in the column with a ‘×’ in Figure 1(c)).
If other inputs have a priority bit ‘1’ associated with this
wire, then the ‘×’ wire had been discharged and this input
loses arbitration. Only a single input will remain with a still
charged wire; that input had the highest priority amongst
(a)
(b)
(c)
Figure 1: Example of a QoS-facilitated arbitration to some Output M . This example pertains to an 8-input switch
with 64-bit output channels. (a) Five inputs are requesting Output M . Each (InN,OutM) crosspoint contains a 12-bit auxV C
counter, from which three most signiﬁcant bits are used to create a thermometer code bit vector. The circuit in (b) uses two
adjacent thermometer code bits to make discharge decisions for every lane (which, in this example, is a set of 8 bitlines). (c)
Discharge decisions are then mapped to the output channel’s bitlines for inhibit-based arbitration.
the requesting inputs and is the winner of the arbitration.
The priorities can be updated in diﬀerent ways to implement diﬀerent arbitration policies [14]. We use least recently
granted (LRG) arbitration in SSVC .
Thermometer Code Creation: Before auxV C values
of requesting inputs can be compared with inhibit-based arbitration, they must be mapped to the output bus’ bitlines.
The auxV C counters (as seen in Figure 1(a)) are too large to
be directly mapped to a typical 64-bit or 128-bit data bus.
Therefore, we use some of the most signiﬁcant bits of the
auxV C value and create a thermometer code vector. Ties
between identical thermometer codes are broken with LRG
arbitration. The use of LRG arbitration creates a restriction on the size of the thermometer code vector because the
number of bitlines required for each LRG arbitration equals
the number of inputs. The thermometer code vector is updated by shifting it up by 1 each time the most signiﬁcant
bits of auxV C change (Figure 2).
Modiﬁed Inhibit-Based Arbitration: The SSVC arbitration has two ma jor components: 1) inputs with the
smallest thermometer code bit vector must defeat all inputs
with larger thermometer code bit vectors because in Virtual
Clock the smaller the auxVC the higher the priority; and
2) ties between any inputs with the same thermometer code
bit vector must be resolved with LRG. To achieve both components of the SSVC arbitration in a single clock cycle, we
created the small circuit in Figure 1(b). This circuit uses
two adjacent thermometer code bits to determine discharge
decisions for every set of 8 bitlines (or lane 2 ). For example,
to decide which bitlines to discharge in lane 4, the circuit
in 1(b) uses thermometer code bits T4 and T5. This circuit
is replicated for every lane, as can be seen in Figure 2. Finally, Figure 1(c) shows how these decisions are mapped to
the output channel’s bitlines. During the Swizzle Switch ’s
inhibit-based arbitration, each input will discharge the bitline where a ‘1’ appears in 1(c). At the end of the arbitration,
a single input will remain with its bitline still charged and
that is the winner of the SSVC arbitration.
As for the example in Figure 1(c), In0 senses in lane 6
because the three most signiﬁcant bits of its auxV C counter
Improving Latency Fairness: We explored two more
ways of managing ﬁnite counters: halving and resetting.
Both methods provide latency fairness across bandwidth
allocations by somewhat decoupling latency from reserved
bandwidth rates. The halving method divides all auxV C
registers by 2 when any one of them saturate. The auxV C
register is shifted down by 1 position and the top half of the
thermometer code is copied to the bottom half and then reset. The reset method resets all auxV C registers to 0 when
any one of them saturate. All thermometer codes are also
reset to zero. After implementing these two methods, we observed a further improvement in latency for ﬂows with low
reserved rates.
3.2 Guaranteed Latency Class
The SSVC mechanism is used only by the GB traﬃc class
to enforce bandwidth usage of ﬂows. It cannot be used for
the time-critical packets in the GL class because it couples
bandwidth and latency and provides very high latency to low
bandwidth ﬂows. Therefore, we decouple bandwidth from
latency by giving the GL traﬃc class the highest priority in
the system regardless of its bandwidth usage. Its arbitration
takes place in a separate GL Lane, leaving one fewer lane
for the GB class. Figure 1(b) was modiﬁed to Figure 3 to
support the GL traﬃc class. First, all ongoing GB arbitration are made to lose. Second, LRG arbitration selects one
input if there are several inputs sending a GL packet. At
the input ports, GL class packets should be buﬀered separately from GB class packets. Additional modiﬁcations to
the sense amp circuit will be required to correctly sense the
GL Lane.
Figure 3: Modiﬁed discharge decisions circuitry to
to support GL class arbitration. In the presence of
a GL request, all bitlines in GB class lanes will be
discharged.
3.3 Bandwidth Allocation To Trafﬁc Classes
The BE class has no reserved bandwidth allocations and
packets are serviced when neither GB nor GL packets are
present.
In the GB class, each individual input may request a fraction of the output channel’s bandwidth; therefore, there can be as many GB ﬂows per output as there are
inputs. For the GL class, the output reserves a small fraction of bandwidth for any GL packet injected from any input
to that output. Then, for each output channel, the sum of
bandwidth allocated to all GB ﬂows and the GL class should
be less than or equal to the total bandwidth capacity of the
output channel.
3.4 Guaranteed Latency Bound
As mentioned in Section 3.3, the output reserves a small
fraction of bandwidth for any GL packet injected from any
input to that output. A drawback of this approach is that
multiple inputs may want to send GL packets, and because
the GL class’ bandwidth allocation is shared among all inputs, one input may take away from another’s ability to send
a GL packet within a reasonable network latency. Thus, our
GL class is only applicable to types of time-critical messages
(1)
that are very infrequent. Also, we only allocate a small fraction of bandwidth as the GL class has absolute priority over
the GB class and may hinder the ability to maintain GB
services. The bandwidth usage of the GL class is tracked
by a counter similar to the auxVC counters of the GB class
and increments by a tick count proportional to the reserved
rate.
The following expression determines the maximum waiting time, τGL , for a buﬀered GL packet at the switch.
τGL ≤ lmax + NGL,o × (b + b/lmin )
lmax and lmin are the maximum and minimum length of
a packet; NGL,o is the number of inputs injecting into the
GL class to an output o; b is the buﬀer depth of GL class
buﬀers. The term lmax accounts for the waiting time for
channel release from a packet already holding the the channel. The term (NGL,o × b) accounts for the transmit latency
of buﬀered ﬂits, and the term (NGL,o × b/lmin ) accounts for
the arbitration latency of each buﬀered GL packet.
In addition to knowing the worst case latency bound, it
is also helpful to quantify the burst size allowed for some
input requiring a latency bound for its packets. For example,
if only one input is injecting to GL class and it expects a
latency bound of 100 cycles, then it should not send more
than 50 1-ﬂit packets in a burst. But if there are 8 inputs and
all of them expect a latency bound of 100 cycles, then each
should not send more than 12 1-ﬂit packets in a burst. We
generalize this logic to two equations. First, we arrange all
NGL,o inputs with GL packets to send in order from tightest
to loosest latency constraint {L1 , L2 ,...,LN }. The maximum
burst size in packets allowed for the input with the tightest
latency constraint L1 is
L1 − lmax
(2)
(3)
σ1 =
(lmax + 1) × NGL,o
Recursively, the maximum burst size in packets for the input
with the nth tightest latency (n > 1) constraint is
Ln − Ln−1
(lmax + 1) × (NGL,o − n)
The ﬂow with the Ln latency constraint can burst as many
ﬂits as the ﬂow with the Ln−1 latency constraint but has
to compete with the remaining NGL,o − n ﬂows with higher
latency constraints.
σn = σn−1 +
4. RESULTS
4.1 Evaluation Methodology
We wrote a custom, cycle-accurate simulator for the Swizzle Switch that modeled the SSVC mechanism in detail. To
verify the correctness of SSVC , we further modeled the behavior of each wire, multiplexer, and sense amp in a C++
program. We tested this program with all input combinations of thermometer code vectors and valid LRG states.
The arbitration decision of the level model was compared
to the arbitration decision of a true (non-coarse grained)
auxV C value comparison to verify that each decision was
correct.
The Swizzle Switch has been fabricated and tested in silicon [15] in 32nm industrial process. We have analyzed the
impact on area and delay of the Swizzle Switch before and
after adding the QoS logic. Wire delays were collected from
SPICE modeling.
4.2 Evaluating Guaranteed Bandwidth
First, we demonstrate the ability of SSVC to adhere to
reserved rates with Figure 4. Each input port has reserved
a fraction of the output port’s total bandwidth. We preallocated these fractions and pre-calculated each input ﬂow’s
Figure 5: The SSVC implementation improved the
packet latency for GB ﬂows with low bandwidth allocations (<10%).
reduce the number of unique thermometer code values in existence, on average. During arbitration, contention between
ﬂows with the same thermometer code value is resolved using LRG, which introduces more fairness. As can be seen in
the results, the reset to zero method has the least variance
in latency across bandwidth allocations. All three methods
were able to provide bandwidth to ﬂows on average within
−2% of their reserved rates.
4.4 Scalability
The accuracy of the SSVC technique increases with more
Vtick. Unlike the LRG policy, which distributes bandwidth
equally among inputs during congestion, the Virtual Clock
policy distributes the bandwidth according to the requested
rates, and guarantees that each input gets at least its requested rate. We simulated 200 combinations of reserved
rates and a variety of packet sizes and veriﬁed that in each
case SSVC is able to give ﬂows their requested rates. Throughput loss from the Swizzle Switch ’s arbitration cycle can be
mitigated by applying techniques such as Packet Chaining [10] to multiple small packets headed to the same destination.
Figure 4: Bandwidth received by ﬂows without and
with QoS. (a) Without QoS, the switch performs LRG
arbitration among the inputs. During congestion all ﬂows
receive an equal share of the available bandwidth. The maximum possible throughput is 0.89 ﬂits/cycle because this experiment uses 8-ﬂit packet sizes. (b) With QoS, all inputs
get at least their reserved rate of bandwidth during congestion. The reserved fractions, r , of all 8 inputs are 40%,
20%, 10%, 10%, 5%, 5%, 5%, and 5%. Details : 8 inputs, 1
output, 128-bit output channel, 8-ﬂit packets, 16-ﬂit buﬀers,
GB traﬃc only, 4 signiﬁcant bits of auxV C used for SSVC
arbitration.
4.3
Improving Latency and Latency Fairness
Next we analyze the latency of GB packets with respect
to the reserved rate of their ﬂows. Figure 5 shows the average latency experienced by packets given the percentage of
bandwidth allocated to a ﬂow out of the output channel’s
total bandwidth capacity. The original Virtual Clock algorithm tends to give low bandwidth ﬂows (<10%) very high
latency. This is because ﬂows with low reserved rates have
fewer packets to transmit; therefore they are scheduled less
frequently.
Our SSVC implementation greatly reduces the latency for
smaller allocations because the comparison of auxV C values
is more coarse-grained. The LRG arbitration, which acts as
a tie-breaker for multiple ﬂows with the same auxV C value,
adds some fairness across the ﬂows. However, the decrease
in latency for smaller allocations comes with a sacriﬁce: the
increase in latency for ﬂows with larger allocations. We reason that this increased latency might not aﬀect performance
because nodes or applications which might need large allocations of bandwidth may have other built-in latency tolerance
techniques, such as memory level parallelism.
The two other methods we explored for managing ﬁnite
counters, halving or resetting the auxV C , further decreased
the latency for ﬂows with very low allocations (< 5%), especially during bursty injection. Results are shown in Figure 5. The insight is that by halving or resetting auxV C , we
Table 1: SSVC storage requirements (in bytes) for
64x64 switch with 512-bit output buses.
Buﬀering/ BE
4 ﬂits, 64 bytes/ﬂit
Input
GB 4 ﬂits/out, 64 outs,
64 bytes/ﬂit
GL
4 ﬂits, 64 bytes/ﬂit
Total buﬀering for all 64 inputs
Per-crosspoint
auxVC(3+8 bits)
state
Thermometer (8 bits)
Vtick (8 bits)
LRG (63 bits)
Total storage for 4096 crosspoints
Total switch storage (input port buﬀering
+ crosspoint storage)
256
16,384
256
1,056 K
1.375
1
1
7.875
45 K
1,101 K
16×16 and 32×32) and bus widths (128-bit, 256-bits, and
512-bits). With the SSVC logic, the crosspoint area for the
128-bit channel increased by 2%, which is equivalent to the
area of a 131-bit channel. For 256-bit and 512-bit buses,
the crosspoint area is large enough to comfortably house the
SSVC logic without additional area overhead.
In addition, we calculated the impact on frequency. The
critical path is extended by the multiplexer before the sense
amp from Figure 2. The frequency slowdown is shown in
Table 2. The worst slowdown is 8.4% for the 256-bit channel,
8×8 conﬁguration.
Table 2: Frequency (in GHz) with and without
SSVC .
Radix
8x8
16x16
32x32
64x64
128
SSVC
3.95
3.15
2.30
SS
4.17
3.44
2.45
Channel Width
256
SSVC
3.18
2.35
1.54
0.93
SS
3.47
2.50
1.60
0.93
512
SSVC
2.35
1.55
0.92
0.51
SS
2.52
1.62
0.94
0.51
5. RELATED WORK
In addition to the techniques we discussed in 2.2, there are
several previously proposed techniques that oﬀer support for
GB and BE QoS classes. Æthereal [6] and Nostrum NoC [11]
both use TDM and circuit switching to provide guaranteed
bandwidth services. MANGO is a clockless NoC that provides GB using dedicated virtual channels [3]. A GB connection is established between two points in the network by
reserving virtual channels along the path. LOFT combines
locally synchronized frames and ﬂit-reservation, which uses
a look-ahead network to pre-schedule the data through each
router [13].
Some techniques also provided explicit GL services. SonicsMX [18] has both priority threads and bandwidth threads,
equivalent to our GL and GB classes, respectively. However,
SonicsMX does not provide guarantees such as maximum
network delay for its priority class. Credit-Controlled Static
Priority (CCSP) decouples latency from the allocated bandwidth rate by using a scheduler that assigns a static priority
among requesters [1].
Other works, such as QNoC [4] and CoQoS [9] implemented QoS management for multiple traﬃc classes based
on system requirements.
6. CONCLUSION
In this paper, we proposed a QoS implementation for a
single-stage, high-radix switch. We implemented three different traﬃc classes: Best-Eﬀort (BE), Guaranteed Bandwidth (GB), and Guaranteed Latency (GL). We explained
in detail the new SSVC circuit design for GB class that compares multiple priority values in a single clock cycle. The
SSVC mechanism adds no more than 2% area-overhead to
the Swizzle Switch crosspoint and incurs a frequency slowdown of at most 8.4%. Furthermore, our GL traﬃc class
decouples network delay from reserved rate of bandwidth
and oﬀers low network latency to critical messages. We provided equations that accurately predict the maximum latency bound for GL class packets.
7. ACKNOWLEDGEMENTS
We thank the anonymous reviewers for their input. This
work was supported by DARPA under agreement #HR001113-2-0006 and ARM.
8. "
Complementary communication path for energy efficient on-chip optical interconnects.,"Optical interconnects are considered to be one of the key solutions for future generation on-chip interconnects. However, energy efficiency is mainly limited by the losses incurred by the optical signals, which considerably reduces the optical power received by the photodetectors. In this paper we propose a differential transmission of the modulated signals, which contributes to improve the transmission of the optical signal power on the receiver side. With this approach, it is possible to reduce the input laser power and increase the energy efficiency of the optical communication. The approach is generic and can be applied to SWSR-, MWSR-, SWMR- and MWMR-like architectures.","Complementary Communication Path for Energy Efficient 
On-Chip Optical Interconnects 
Hui Li1, Sébastien Le Beux1*, Yvain Thonnart2, 3, Ian O'Connor1 
1Lyon Institute of Nanotechnology, INL-UMR5270, Ecole Centrale de Lyon, F-69134 Ecully, France 
2Univ. Grenoble Alpes, F-38000 Grenoble, France 
3CEA, LETI, MINATEC Campus, F-38054 Grenoble, France 
hui.li@doctorant.ec-lyon.fr, sebastien.le-beux@ec-lyon.fr, yvain.thonnart@cea.fr, ian.oconnor@ec-lyon.fr 
ABSTRACT 
Optical interconnects are considered to be one of the key solutions 
for future generation on-chip interconnects. However, energy 
efficiency is mainly limited by the losses incurred by the optical 
signals, which considerably reduces the optical power received by 
the photodetectors. In this paper we propose a differential 
transmission of the modulated signals, which contributes to 
improve the transmission of the optical signal power on the 
receiver side. With this approach, it is possible to reduce the input 
laser power and increase the energy efficiency of the optical 
communication. The approach is generic and can be applied to 
SWSR-, MWSR-, SWMR- and MWMR-like architectures. 
Categories and Subject Descriptors 
C.1.2 
[Processor Architectures]: Multiple Data Stream 
Architectures (Multiprocessors) – Interconnection architectures 
(e.g., common bus, multiport memory, crossbar switch) 
General Terms 
Design 
Keywords 
Optical interconnect, Differential transmission 
1. INTRODUCTION 
Technology scaling, increased die sizes and die stacking are 
still promising to increase the processing density of advanced 
CMOS circuits. Yet, in order to push further the many-core 
integration, chip-level communication between processing 
clusters needs a deep restructuring to fit in power budgets. Silicon 
photonics is seen as a key enabling technology for highbandwidth communication across large chips or interposers, with 
limited power consumption and low footprint. Optical networks 
on chip (ONoC) were proposed as a scalable solution to 
interconnect a large number of processing cores. However, chiplevel optical communication in silicon waveguides incurs more 
losses than fiber-optic communication (dB/cm vs. dB/km), which 
needs to carefully optimize the optical paths from the light   
*Corresponding author. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights 
for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a 
fee. Request permissions 
from 
Permissions@acm.org. 
DAC '15, June 07 - 11, 2015, San Francisco, CA, USA 
Copyright 2015 ACM 978-1-4503-3520-1/15/06…$15.00 
http://dx.doi.org/10.1145/2744769.2744810 
sources, so as 
to maintain high energy efficiency. A 
comprehensive work has been done on energy-efficient ONoC 
topologies, including MWMR- (Multiple Writer Multiple Reader 
[3]), MWSR- (Multiple Writer Single Reader [4]), SWMR- 
(Single Writer Multiple Reader [5]) and SWSR- (Single Writer 
Single Reader [8]) like architectures.  
A recurrent communication pattern in all these topologies is a 
serpentine-like structure, possibly opened at various points, 
crossing all the processing cores with one or more waveguides 
used to propagate modulated light on different wavelengths, with 
on-off keying modulation (light transmitted or absorbed). In this 
paper, evolving on this pattern, we propose to recover the 
absorbed light, to re-inject it in a complementary waveguide, and 
provide differential optical communication. With this approach, it 
is possible to reduce the input laser power and increase the energy 
efficiency of the optical communication. Our approach is generic 
and can be applied to a large panel of existing architectures. 
Section 2 of this paper presents the basic communication 
structures for the proposed approach and their integration. Section 
3 proposes an analytical model for the optical power budget 
evolution from the light sources to the optoelectronic receivers. 
And finally, Section 4 presents case studies on the application to 
MWMR, MWSR, SWMR, SWSR and generic architectures with 
the associated power savings. 
2. PROPOSED APPROACH 
In this section, we present an implementation of the proposed 
approach enabling the differential transmission of modulated data. 
The elementary structure is then introduced. 
2.1 Approach Overview 
Core0
Core1 (transmitting)
…01001110…
Core2
a)
inP
b)
c)
t
r
y
a
h
a
p
P
m
i
r
0
inP
y
r
a
t
n
e
h
m
a
e
p
p
l
t
m
o
C
0
Core3 (receiving)
VCC
R
TIA
comp
Vou t
…01001110…
Vref
GND
1,pS
0,pS
pS
1
gain
=
c
S
+
S
p
S
c
cS
0
0,cS
1,cS
cS
Photodetector
Photonic
representation
Electrical 
representation
Optical signal
data
crosstalk
pS
Transmission 
of  data ‘1 ’
Transmission 
of data  ‘0’
Active MR
ON state
OFF state
Modulation
state
OFF for data ‘1’
ON for data ‘0 ’
Figure 1. Transmission of data ‘1’ and ‘0’ through primary and complementary 
paths respectively. 
Figure 1-a illustrates our approach using a MWSR-like 
architecture, assuming 3 Writers, one waveguide and a single 
wavelength (i.e. one MR per core). In the example, Core1 
communicates with Core3. Hence, the MRs in Core0 and Core2 are 
turned OFF (i.e. at off-resonance) and the MR in Core1 is in the 
 
 
 
 
 
 
 
modulation state, i.e. it switches from OFF state to ON state (i.e. 
at resonance) to transmit data ‘1’ and ‘0’ respectively. 
2.1.1 Primary Path: Transmission of Data‘1’ 
Data ‘1’ leads to a MR in the OFF state to let the light pass 
through. Similarly to the related approaches, the light propagates 
through the so-called primary path: it crosses the modulator and 
continues propagating along the same waveguide until reaching 
the photodetector, as illustrated by the red arrow in Figure 1-a. 
The power of the optical signal decreases due to losses from 
waveguide propagation, crossed modulators and photodetector, as 
illustrated by the red line in Figure 1-b. The optical power 
received by the photodetector (Sp,1) must be high enough to ensure 
proper detection and photo-electronic conversion through the 
Trans-Impedance Amplifier (TIA). Conversely, the MR is turned 
ON for data ‘0’ and diverts most of the light from the primary 
path, leading to a low optical power Sp,0 at the photodetector. 
While the sensitivity of the photodetector (of the order of -20dBm 
[1]) is a key parameter for energy-efficient optical interconnect, 
the actual received power is usually higher to distinguish data ‘1’ 
(Sp,1) from data ‘0’ (Sp,0) and to ensure high enough signal-tonoise ratio (SNR) in the worst-case scenario. It is well known that 
optical interconnects are sensitive to crosstalk and variations in 
both the fabrication process and local temperature. We define Sp 
as the difference between the signal power received when 
transmitting ‘1’ and ‘0’ in the primary path. 
2.1.2 Complementary Path: Transmission of Data‘0’ 
When the MR is in the ON state to transmit data ‘0’ in the 
primary path, instead of “absorbing” the light as in related 
approaches, we propose to drop the light into a secondary path, 
the so-called complementary path, as illustrated by the blue arrow 
in Figure 1-a. In this example, it is composed of a waveguide 
propagating the light towards a second photodetector receiving 
Sc,0. This complementary structure allows 
the differential 
transmission of the data, which contributes to increasing the 
detected signal power and ultimately reduces the laser power 
consumption. The signal propagating in the complementary path 
experiences some power increase due to the crosstalk signals 
dropped from the primary path (i.e. dotted line arrows from Core0 
and Core2 in the figure). This positive effect is partially 
counteracted by the increase of Sc,1, the crosstalk level in the 
complementary path when a ‘1’ is transmitted (i.e. dashed red 
line), as illustrated in Figure 1-c. Sc is the difference between the 
signal power received 
in 
the complementary path when 
transmitting ‘0’ and ‘1’. This additional received power enables 
the value of the transmitted data to be better distinguished thanks 
to a higher dynamic of the signals. Getting Sp+Sc instead of Sp, it 
is thus possible to reduce the input laser power by a factor of 
Sp/(Sp+Sc), i.e. with an energy efficiency gain of Sc/(Sp+Sc). 
2.1.3 Micro-Architecture of the Receiver 
The addition of a complementary path to the conventional 
transmission structure creates the need for a modified receiver, 
which will benefit from the differential transmission to improve 
the optical signal quality and allow working at lower laser input 
power. To do so, the photocurrents provided by the photodiodes 
on the primary and complementary paths are substracted by a 
simple series connection of the photodiodes between supply and 
ground, the difference current being fed to a conventional TIA, as 
depicted in Figure 1-a. 
The analytical models presented hereafter consider a linear 
conversion from the photocurrents to the TIA output, which 
actually means the signal is considered to propagate without 
alteration from the optical subsystem to the electrical subsystem. 
2.2 Elementary Blocks for Signal Analysis 
signal
MR
λsignal (nm)
Pdrop
Pthrough
b)
T
s
n
a
r
m
i
s
s
i
n
o
(
%
n
P
i
)
0
100
λMR= λi
λsignal (nm)
Pdrop
P through
T
s
n
a
r
m
i
s
s
i
n
o
(
%
n
P
i
)
0
100
λMR=λi
ON : λres = λMR
OFF: λres = λMR+ ∆λ
c)
λi
a)
r1
k1
Pin
P through
)
λλϕ
(
,
d
)
λλϕ
(
,
MR
signal
t
λλϕ
(
,
Δ+MR
λ
)
signal
d
)
r
+ k
12
=
λ 2=
π
m
=
⋅−
α
exp(
)
(2
λ
res
res
res
res
m
λ
n
λπ
m
2
λ
a
Rnres
res
2
resλ
signal
res
signal
λλθ
(
,
)
=
a
Pdrop
Padd
k2
r2
λMR+∆λ
λMR+∆λ
λ
)
λλϕ
(
,
Δ+MR
signal
t
Figure 2. MR model: a) device geometry, signal transmission on through port 
and drop port in b) ON state and c) OFF state. 
Table 1: Transmission parameters.  
Parameter
r1, r2
k1, k2
R
Description
Self-coupling coefficient 
Cross-coupling coefficient 
MR radius
Effective refractive index of MR (varies 
with applied voltage, device geometry and 
ambient temperature) 
Resonant mode number of MR 
MR resonant wavelength  
Signal wavelength  
MR wavelength between ON and OFF 
states 
Power attenuation coefficient 
Single-pass amplitude transmission 
Single-pass phase shift 
180° bending loss under 40µm radius 
Unit
µm
nres
m
λres
λsignal
nm
nm
∆λ
α
a (λres)
dB/cm
θ(λsignal, λres)
Lb
dB
2
21
21
2
1
21
2
2
2
r
]
)]
+
)
rr
−
(2
a
λ
)
rr
cos[
λλθ
(
,
)
rr
cos[
λλθ
(
,
)]
+
([
a
λ
a
(
λ
)
r
(21
−
a
λ
)
λλϕ
(
,
res
res
signal
res
res
signal
res
res
res
signal
t
=
(1) 
2
21
21
2
2
2
1
]
)
([
a
λ
r
)]
a
(
λ
1)(
−
r
1)(
−
λλθ
cos[
(
,
)
(21
−
a
λ
)
+
)
λλϕ
(
,
rr
rr
res
res
signal
res
res
res
signal
d
=
(2) 
The improvements induced by the complementary path 
depend on the transmission of the optical signals in the through 
port and the drop port of a MR (Figure 2-a). A MR is 
characterized by a resonant wavelength (λres) in the ON state, i.e. 
the input signals at λsignal=λres are redirected to the drop port. In the 
OFF state, the resonant wavelength drifts by ∆λ, i.e. the input 
signals at λsignal≠λres will continue propagating in the same 
waveguide. Figure 2-b-c illustrates the transmissions on through 
tϕ and 
dϕ ) in ON and OFF states 
port and drop port (i.e. 
respectively. The actual transmissions depend on the device 
geometry (i.e. ring radius, R), the self(cross)-coupling coefficient 
(i.e. r1, r2, k1 and k2), the power attenuation coefficient α, and the 
single-pass phase shift θ(λsignal, λres). Table 1 summarizes the 
tϕ and 
parameters and Equations(1) and(2) give the transmission 
dϕ extracted from [2]. 
Figure 3 illustrates the elementary optical writing and reading 
structures, respectively named writer wj and reader rj. In the 
writer, the modulation of data ‘1’ leads to the propagation of the 
signal at λj on the through port (Figure 3-a). The losses of the 
signal are thus expected to be the same as in traditional 
architectures. If data ‘0’ is modulated, the signal at λj is strongly 
attenuated on the through port and is mainly redirected to the drop 
 
 
 
 
 
port. The signal then i) propagates on a waveguide following the 
backward-direction and ii) is redirected to the forward-direction 
using a U-shaped waveguide (Figure 3-b). In order to reduce the 
waveguide bending losses Lb, a 40µm radius is considered, which 
leads to -0.025dB losses based on equations in Figure 2-a (Figure 
3 is not at the right scale: the radius of the ring is around 1-10µm 
and is thus smaller than the U-shaped waveguide). 
a)
iλ
jλ
b)
c)
Inp
Outp
Outc
Inc
R=40µm
ON
jw
t λλϕ
(
,
)
j
i
b
j
i
d
( λλϕ
,
L)
jr
ON
Inp
Throughp
Dropp
Inc
Throughc
Dropc
t λλϕ
(
,
)
j
i
d λλϕ
(
,
)
j
i
d)
λλλϕ
(
,
Δ+j
)
i
t
b
j
i
d
λλλϕ
(
,
Δ+
L)
Outp
Inp
Outc
Inc
OFF
R=40µm
jw
OFF
jr
Inp
Throughp
Dropp
Inc
Throughc
Dropc
λλλϕ
(
,
Δ+j
)
i
t
λλλϕ
(
,
Δ+j
)
i
d
Figure 3. The writer wj includes an MR that a) let the signals at λj continue 
propagating on the primary path or b) drops the resonating signal toward the 
complementary path. The reader rj is symmetric and includes two MRs allowing 
to a) let signals continue propagating or d) filter them for photodetection. 
The aim of rj is to filter the same wavelength on both primary 
and complementary paths, as illustrated in Figure 3c-d. For this 
purpose, the traditional reader structure is duplicated and the 
resulting MR pair is controlled by a common signal (note that 
there are no MRs in the reader structure in Figure 1-a since a 
single wavelength was considered). These additional MRs lead to 
the actual cost of the proposed approach. In the result section, we 
will compare the reduction of the laser power to the increase of 
the area. 
3. ANALYTICAL MODEL 
3.1 General MWMR Model 
Figure 4 
illustrates 
the MWMR architecture we are 
considering. It is composed of NW Writers and NR Readers (the 
general case allows NW≠NR). Each waveguide pair of the network 
(a single pair in the figure) performs 2 round trips to successively 
cross the Writers and the Readers. CW lasers inject optical power 
in each primary path. We assume that NWL wavelengths are used 
to modulate the signal (i.e., λ0, λ1, …, λNWL-1). The modulation of 
the signals occurs in WS, the Writer part of the source core CoreS 
(note that WS is composed of w0, w1,…, wNWL-1), and experiences 
the detection in the RT, the Reader part of the target core CoreT 
(RT is composed of r0, r1,…, rNWL-1). The aim of the analytical 
model is to evaluate the laser power reduction achievable with the 
use of the complementary path. For this purpose, we distinguish 
the signal transmission in the set of all writers and the set of all 
receivers. 
3.1.1 Transmission in the Writers 
The losses experienced by the optical signal crossing the 
writers will depend on i) the modulated data dj (i.e. ‘0’ or ‘1’) and 
ii) the core position where the modulation takes place, WS. In 
order to distinguish the transmissions T of a signal at wavelength 
λj in the primary path and the complementary path, we define 
 and 
 respectively. In this notation, dj 
is the value of the modulated data (‘0’ or ‘1’) at λj, which is 
important to take into account since it determines the optical path. 
These transmissions allow the power level of the optical signal to 
be evaluated before entering the set of Readers (in the second 
waveguide round trip). For the sake of clarity, the formulas for Tw 
are given in Section 6. 
][
j
S dWpw
,
,
,
T
j
][
j
S dWcw
,
,
,
T
j
3.1.2 Transmission in the Readers 
Due to a symmetric micro-architecture and control circuitry of 
the reader structure, the signals in primary and complementary 
paths experience the same transmission to reach RT, and are 
expressed as 
. After crossing intermediate Reader 
cores (R0…RT-1), the signals at wavelength λj reach RT where they 
are dropped by the appropriate filters toward photodetectors. The 
formula for Tr is also given in Section 6. 
][,
j
jRr
,
T
T
3.1.3 Received Signal Power 
In order to estimate the gain induced by the complementary 
path, we define 
 and 
 as the signals 
modulated in WS and received by the jth photodetectors in RT on 
primary and complementary paths respectively. The four cases are 
formally defined as: 
S
=
=
S
S
=
=
S
j
T
S
djRWpS
,
,
,
,
j
T
S
dj
,
RWcS
,
,
,
][
Tj
⋅
][
jPj
⋅
][
⋅
⋅
][
Tj
][
jPj
][
][
Tj
⋅
][
jPj
⋅
][
⋅
⋅
][
Tj
][
jPj
][
where P in[j] is the laser power coupled into the waveguide at 
λj. 
 (resp. 
) corresponds to the difference 
between the received signal powers for dj=‘1’ and dj=‘0’ (resp. ‘0’ 
and ‘1’). From 
this, we obtain 
the signal 
transmission 
improvement for communication WS(cid:198)RT, which is defined as 
(4) 
jRr
,
,
0,
Wpw
,
,
0,
jRWp
,
,
,
T
T
T
T
in
T
S
T
S
jRr
,
,
1,
Wpw
,
,
1,
jRWp
,
,
,
in
T
S
T
S
jRr
,
,
0,
Wcw
,,
0,
jRWc
,
,
,
in
T
S
T
S
jRr
,
,
1,
Wcw
,,
1,
jRWc
,
,
,
in
T
S
T
S
(3-a)
(3-b)
(3-c)
(3-d)
jRWp
,
,
,
T
S
S
jRWc
,
,
,
T
SS
jRWc
,
,
,
jRWp
,
,
,
jRWc
,
,
,
T
S
T
S
T
S
S
S
S
+
Core0
r0
rj
CoreS
r0
rj
CoreT
r0
rj
CoreN-1
r0
rj
rNWL-1
rNW L-1
rNWL-1
rNWL-1
From
CW lasers
1−WLNλ
0
λ jλ
writing
OFF
reading
w0
wj
wNWL-1
w0
wj
wNWL-1
w0
wj
wNWL-1
w0
wj
wNWL-1
Figure 4. Generic MWMR architecture in which CoreS transmits data to CoreT. The light, generated from CW lasers propagates on the primary path, crosses the writer 
parts of a first set of cores until reaching CoreS for the modulation. The modulated signals then propagate on primary and complementary paths (data ‘1’ and ‘0’ 
respectively), cross a second set of writers, a set of readers until reaching the photodetectors of CoreT. 
 
              
 
 
 
 
 
By considering the worst case transmission improvement 
among the possible communication pairs, the laser power 
reduction is estimated. 
For the sake of clarity, the equations described above do not 
take into account the inter-channel crosstalk (which occurs due to 
the signals modulated at other wavelengths). We refer to Section 6 
for the general case in which inter-channel crosstalk is considered. 
All the analyses in Section 4 rely on the models taking into 
account the inter-channel crosstalk. 
3.2 Depopulated Models 
This MWMR model allows the gain of our approach to be 
applied in network architectures such as Flexishare [3]. In 
addition, the analytical model can be adapted to other types of 
network such as MWSR, SWMR and SWSR. The model in 
Section 3.1 is adapted based on the implementation of the 
complementary path structure in these networks, which is 
illustrated in Figure 5. 
a)
Core0
CoreS
CoreN-1
CoreT
w0
wj
wN WL-1
w0
wj
wNW L-1
w0
wj
wN WL-1
r0
rj
rNW L-1
b)
CoreS
Core0
CoreT
CoreN-1
w0
wj
wNW L-1
r0
rj
rNW L-1
r0
rj
rNW L-1
r0
rj
rNWL -1
c)
Core0
Core1
CoreN-2
CoreN-1
w0
wj
wNWL-1
r0
rj
rNW L-1
w0
wj
wNWL -1
r0
rj
rNW L-1
d)
Core0
Core1
Core2
CoreN-1
w0
wj
wNW L-1
r0
rj
rNW L-1
w0
wj
wN WL-1
r0
rj
rNW L-1
Figure 5. Implementation of the complementary path structure in ring-based a) 
MWSR, b) SWMR and c) SWSR like networks. Part d) illustrates its 
implementation on sectioned waveguides based networks. 
Figure 5-a illustrates a MWSR architecture which is obtained 
by considering a single target core per waveguide. In such 
network architecture, each core is reachable through a set of 
dedicated waveguides. Among others, MPNoC [5], Corona [4] 
and Clos [9] rely on MWSR. On the opposite, SWMR-like 
architectures (Figure 5-b) imply a single source core per 
waveguide, i.e. the arbitration occurs in the Readers. Networks 
such as Firefly [7], LumiNOC [12], ATAC unicast channel [6] 
implement SWMR. Finally, SWSR-like architecture is obtained 
by allocating, at design time, dedicated wavelengths, i.e. no 
arbitration is required. The immediate implementation of a SWSR 
network leads to a single writing core and a single reading core 
per waveguide. It 
thus 
implies dedicated waveguides for 
Core0(cid:198)Core1, Core0(cid:198)Core2, Core1(cid:198)Core0, etc. as illustrated in 
Figure 5-c. For all these scenarios (Figure 5-a-b-c), we have 
derived the analytical model from Section 6, taking into account 
the (possibly different) number of Writers (x) and Readers (y), i.e. 
which is denoted as xWyR. In Section 4, we evaluate the gain of 
the complementary path in these architectures. 
Our approach suits well with serpentine 
layout-based 
networks since it does not add complexity in the layout and does 
not introduce waveguide crossing. Hence, it can also be applied to 
networks relying on sectioned waveguides (Figure 5-d) such as 
ORNoC [8], Chameleon [10] and SUOR [11]. The improvement 
evaluation of the complementary path in these networks is 
remained as future work. While it could also be applied to other 
topologies such as Mesh, Torus [13] and Multi-Stage, it would 
need to duplicate all rings used as routers, and would also 
introduce many waveguide crossings that will, in turn, reduce the 
signal power on primary and complementary paths. For this 
reason, these topologies options are not considered. 
3.3 Simulation Model 
To investigate the dynamic behavior of our proposal, and 
analyze the optical power budget along the complete transmission 
path, compact models of the MRs and photodetectors were 
encoded in Verilog-A. The MR models are based on the equations 
of φt and φd as described previously, including parasitic 
resistances and capacitances for the electrical modulation, and by 
modeling the refractive index changes (real index and imaginary 
part for absorption) due 
to 
the modulation voltage. The 
photodiode models are based on a conventional diode equation 
including dark current along with a linear responsivity of the 
photocurrent (in A/W). 
4. CASE STUDY 
4.1 MWMR 
Modulation using MRs is usually done in two ways. One way 
is based on doping the ring to form a P-N junction at the ring 
level, such that reverse biasing will change the refractive index 
through carrier depletion. The other way is to leave the ring 
undoped, with P and N doping on either side, so as to create a P-IN junction, which will change the refractive index by carrier 
injection when forward biased. PN modulators are known to have 
a faster switching time [14] than PIN modulators for a given bias 
voltage, which makes them an interesting option for high-speed 
modulation. Nevertheless, the high absorption in the rings due to 
the cloud of carriers at the junction leads to lower transmission 
ratios toward the through and drop ports. 
Table 2. Parameters and values for the writer and the reader. 
Parameter 
Value 
2
2
r1
r2
α
∆λ
FSR
nMR
m
Lb
PN 
PIN 
0.992 
0.998
0.996 
0.998
15dB/cm 
2dB/cm
0.09nm (under 2.5V) 
0.16nm(under 2V)
~59nm (allowing 72 channels) 
~2.55 
17 
-0.025dB 
Table 2 presents the parameter values for both types of rings 
for transmission covering the complete C-band (around 1550nm). 
The 72 dense wavelength-division-multiplexing 
channels 
(DWDM) of the C-band require non-interference of higher 
resonance orders between rings. Therefore, a Free-Spectral-Range 
(FSR) above 58nm is targeted, resulting in resonance at order 17 
in rings of around 1.6μm of radius. The coupling coefficients of 
the rings to the waveguides are chosen in order to optimize the 
contrast between ‘1’ and ‘0’ at the through and drop ports. 
In order to minimize the losses in the set of receivers, PIN 
MRs are preferably used on the receiver side: indeed, they have 
lower losses, and are configured at the granularity of a packet, i.e. 
with a lower switching frequency than the modulators. The choice 
of writers, conversely, is not obvious, and will be studied in the 
next paragraph. We analyze the benefits of the complementary 
path in a 4W4R network with 4 wavelengths. 
The 4W4R network has been captured as a schematic in the 
Cadence Virtuoso editor, and the resulting netlist has been fed to 
Mentor Graphics Eldo simulator, using the compact models of the 
optical elements that were introduced in Section 3.3. Two 
different netlists were generated, using PN and PIN modulators. 
 
This allowed extracting the evolution of the optical power budget 
along the primary and complementary paths. The communication 
from C0 to C3 is presented in Figure 6. 
Sp,1
Sc ,1
Sp,0
Sc ,0
100%
80%
60%
40%
20%
0%
C3(Rt)
Src
C0(Ws)
C1
C2
C3
B2
B1
C0
C1
C2
Writers (PN)
backwards
cmpl. path
Readers (PIN)
O
p
t
i
a
c
l
o
p
w
e
r
i
(
n
%
o
f
n
P
i
)
PN writers
a)
100%
80%
60%
40%
20%
0%
C3(Rt)
Src
backwards
cmpl. path
O
p
t
i
a
c
l
o
p
w
e
r
i
(
n
%
o
f
n
P
i
)
Sp,1
Sc ,1
Sp,0
Sc ,0
PIN writers
b)
Writers (PIN)
Readers (PIN)
C0(Ws)
C1
C2
C3
B2
B1
C0
C1
C2
Figure 6. Evolution of the optical power budget along the primary and 
complementary paths for a) PN writers, and b) PIN writers. 
Using PN modulators as writers (Figure 6-a), the attenuation 
is relatively important, and the primary signal rapidly decreases 
along the writers chain. Conversely, the complementary signal 
passes by a single PN ring at Core0 where it is deflected to the 
complementary path. Hence, Sc,0 is eventually higher than Sp,1. 
Moreover, the optical leakage to the drop port in through mode is 
still important, and these contributions are added along the path, 
up to a level Sc,1. 
As this behavior is not acceptable, it is needed to consider PIN 
rings for modulation, which thanks to a higher shift in resonance 
(∆λ) with the modulation voltage allows for a much better 
transmission in through mode and reduced optical leakage to the 
drop port. Figure 6-b presents the results with PIN modulators for 
the same communication as before, which show a much better 
separation between ‘0’ and ‘1’ in both the primary and 
complementary paths. The drawback is that to compensate for the 
slower bandwidth of these PIN modulators, the modulation 
requires complex drivers with pre-emphasis. This leads to 
increased area and power consumption in the CMOS drivers [14]. 
However, this electrical power consumption increase remains 
negligible with respect to the optical power consumption of the 
lasers. 
We computed values Sp and Sc for all the communication 
pairs in 4W4R architecture. These values fit within 3% of the 
previous simulation. The difference is a cumulative error in 
through mode due to ring radius variation across WDM channels. 
C3(cid:198)C2 is the communication showing the smallest signal 
transmission improvement (44%); it occurs when the last Writer 
communicates with the last Reader. 
4.2 Large Scale MWMR-like Architectures 
We evaluate the impact of the complementary path on 
MWMR architectures including up to 16 Writers and 16 Readers. 
It is worth noticing that, assuming a total connectivity between the 
cores and a serpentine layout, using only (many) SWSR rings will 
lead to the same waveguide length as with only a (single) 
16W16R ring. In all our results, we assume 11dB propagation 
losses in each waveguide, which corresponds to a total 11cm wire 
length under typical 1dB/cm losses [3]. We also assume -20dBm 
[1] for the photodetectors sensitivity, i.e. the laser input power P in 
should be adjusted to maintain more than 10μW optical power on 
the photodetectors. Figure 7-a gives the possible laser power 
reduction given this constraint for the worst-case communication 
scenario. 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
-0%
-10%
-20%
-30%
-40%
-50%
-60%
-70%
-80%
-90%
-100%
1
3
5
7
9
11
13
15
Laser power reduction wrt. traditional method 
a)
MWMR
SWMR
MWSR
SWSR
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
+0%
+10%
+20%
+30%
+40%
+50%
+60%
+70%
+80%
+90%
+100%
1
3
5
7
9
11
13
15
Area cost wrt. traditional method 
b)
MWMR
SWMR
MWSR
SWSR
Figure 7. Impact of the complementary path on a) the laser power reduction 
and b) the area cost of the optical interconnect. 
Our approach leads to some area overhead: in addition to the 
MRs required to implement the primary path (Ap), additional MRs 
are required for the complementary path (Ac). Figure 7-b 
represents the area cost (i.e. Ac/Ap). 
Results show that the SWMR-like networks (purple color) 
allow around 40% laser power reduction independently from the 
number of Readers, as the losses are identical on both paths in the 
set of readers. However, the area cost considerably increases and 
reaches +90% for SW16R due to doubling the MRs in all readers. 
We observe the opposite trend for MWSR-like architectures (dark 
green color): the more the Writers, the higher the power 
improvement and 
the 
lower 
the area cost. Actually, 
the 
complementary path eventually brings more power than the 
primary path, as the waveguide leads to fewer losses than MRs. 
For 16WSR case, power is reduced by 68%, while area is only 
increased by 5% respectively. For MWMR-like networks (in red), 
power reduction is even more important while the area cost is 
constant. In a nutshell, our approach leads to the highest 
improvement 
for networks 
favoring writer 
aggregation. 
Experiments also demonstrate that, if we also bound the laser 
power possibly injected in the circuit (here 600μW), the 
complementary path allows increasing the scale of the network by 
detecting signals not detectable in the primary path only 
(highlighted in light green, for 10-16 cores). 
 
 
 
 
 
 
 
 
 
 
 
 
 
5. CONCLUSION 
Differential optical communication in ONoCs has been 
demonstrated to lead to large savings in laser power for most ringbased topologies, sometimes with gains of much more than the 
intuitive factor of 2, thanks to reduced losses along the 
complementary path. MWSR communication shows a power 
divided by 3 for only 5% overhead. For multiple-Writer schemes, 
the MR overhead is always below 50%. Actual area overhead is 
even smaller for 3D-stacked CMOS on photonics, as receiver 
signals are shared, and overhead is due to 3D-interconnections. 
The proposed approach has been checked against simulations with 
compact VerilogA models. Complete electro-optical design of 
specific receivers and possible noise sensitivity reduction will be 
investigated in future works. 
6. APPENDIX: ANALYTICAL MODEL 
In this appendix, we consider communication between Writer 
WS and Receiver RT. The received signal power on rj (i.e. the 
reader of RT at wavelength λj) depends on di and dj, the data 
modulated at λi (received power at λj is crosstalk) and λj 
respectively. They are defined by 
 and 
which are formalized in Equations 5-a and 5-b respectively. 
(5-a) 
j
T
S
djRWpS
,
,
,
,
j
T
S
djRWcS
,
,
,
,
][
iPi
⋅
)][
][
Ti
⋅
(
T
−
1
=
0
jRr
,
,
dWpw
,
,
,
djRWp
,
,
,
,
∑
= WL
T
i
S
j
T
S
N
i
in
S
][
iPi
⋅
)][
][
Ti
⋅
(
T
−
1
=
0
jRr
,
,
dWcw
,,
,
djRWc
,
,
,
,
∑
= WL
T
i
S
j
T
S
N
i
in
S
(5-b) 
They depend on: 
 (Eq. 6), transmission in the primary path of the 
writer parts, which is the same with that in traditional 
architectures. 
T
][
i
(Eq. 7), transmission in the complementary path 
of the writer parts. It is divided into: i) the transmission 
before the modulation, ii) the transmission from primary to 
complementary path where the modulation occurs and iii) the 
transmission after the modulation.   
 (Eq. 8), transmission in the reader parts to reach 
the reading core. The transmission of signals at λi across the 
intermediate Readers and dropped by RT is defined. 
In these equations, Lp is the propagation loss (-1dB/cm in the 
results section). lw,total[i] and lr,total[i] (resp. Nw,b[i] and Nr,b[i]) are 
the total waveguide length (resp. number of waveguide bends) 
experienced by signal at λi in Writers (in primary path) and 
Readers parts. Regarding the waveguide length experienced by 
signal at λi coupled from the primary path into the complementary 
path, we define it as lv[i], where v is the considered Writer (i.e. 
v=Ws when considering the modulating Writer). 
• 
][,
i
S dWpw
,
,
T
i
• 
S dWcw
,
,
,
i
• 
][,
i
jRr
,
T
T
+
λλλϕλλλϕλλλϕ
(
,
Δ+
))
(
Π
(
,
Δ+
))(
(
,
Δ+
)
L
Π
=
∑
∑
−
1
=
−
1
=
−
1
=
−
1
=
]]})
[(
[
)
(
{{
)
(
][
i
0
0
0
0
][
i
][
i
dWcw
,
,
,
bw
,
S
WL
WL
v
i
S
W
v
N
j
b
j
i
d
k
i
t
j
k
v
k
i
t
N
k
l
p
N
b
L
L
T
+
Δ⋅
+
Δ⋅
λλϕλ
))(
(
,
+
Π
λλλϕ
(
,
Δ+
))
Π∑
−
1
=
−
1
=
−
1
=
]]})
λ
)
λλϕ
(
,
(
[(
[
)
{(
0
0
0
][
i
WL
WL
Ws
N
j
b
j
j
i
d
k
k
i
t
j
k
Ws
k
i
t
N
k
l
p
L
d
d
L
]]}}
)
Δ⋅
λλλϕλλλϕλ
))(
Π
(
,
Δ+
))(
(
,
Δ+
)
λλϕ
(
,
(
λλλϕ
(
,
Δ+
))
[(
[
)
(
{
−
1
+
1
−
1
0
−
1
0
−
1
0
−
1
−
1
0
][
i
∑
∑
Wv
=
=
=
=
=
+
Π
Π
W
S
WL
WL
WL
v
N
N
j
b
j
i
d
k
i
t
j
k
k
k
i
t
N
k
v
k
i
t
N
k
l
p
L
d
L
Before Writer v
Before wj
Dropped by wj
Before modulation
Before Writer WS
Before wj"
On-chip interconnection network for accelerator-rich architectures.,"Modern processors have included hardware accelerators to provide high computation capability and low energy consumption. With specific hardware implementation, accelerators can improve performance and reduce energy consumption by orders of magnitude compared to general purpose cores. However, hardware accelerators cannot tolerate memory and communication latency through extensive multi-threading; this increases the demand for efficient memory interface and network-on-chip (NoC) designs. In this paper we explore the global management of NoCs in accelerator-rich architectures to provide predictable performance and energy efficiency. Accelerator memory accesses exhibit predictable patterns, creating highly utilized network paths. Leveraging these observations we propose reserving NoC paths based on the timing information from the global manager. We further maximize the benefit of paths reservation by regularizing the communication traffic through TLB buffering and hybrid-switching. The combined effect of these optimizations reduces the total execution time by 11.3% over a packet-switched mesh NoC and 8.5% over the EVC [18] and a previous hybrid-switched NoC [29].","On-chip Interconnection Network for
Accelerator-Rich Architectures
Jason Cong Michael Gill Yuchen Hao Glenn Reinman Bo Yuan
Center for Domain Speciﬁc Computing, UCLA
{cong, mgill, haoyc, reinman, boyuan}@cs.ucla.edu
ABSTRACT
Modern processors have included hardware accelerators to
provide high computation capability and low energy consumption. With speciﬁc hardware implementation, accelerators can improve performance and reduce energy consumption by orders of magnitude compared to general purpose
cores. However, hardware accelerators cannot tolerate memory and communication latency through extensive multithreading; this increases the demand for eﬃcient memory
interface and network-on-chip (NoC) designs.
In this paper we explore the global management of NoCs
in accelerator-rich architectures to provide predictable performance and energy eﬃciency. Accelerator memory accesses exhibit predictable patterns, creating highly utilized
network paths. Leveraging these observations we propose
reserving NoC paths based on the timing information from
the global manager. We further maximize the beneﬁt of
paths reservation by regularizing the communication traﬃc
through TLB buﬀering and hybrid-switching. The combined
eﬀect of these optimizations reduces the total execution time
by 11.3% over a packet-switched mesh NoC and 8.5% over
the EVC [18] and a previous hybrid-switched NoC [29].
1.
INTRODUCTION
Accelerators oﬀer orders-of-magnitude improvement in performance and energy eﬃciency as compared to general-purpose
cores. Driven by the need for energy eﬃciency, the industry has proposed incorporating accelerators other than
general-purpose cores into a die. These on-chip accelerators are application-speciﬁc implementations of a particular functionality, and can range from simple tasks (i.e. a
multiply-accumulate operation) to complex tasks (i.e. FFT,
encryption/decryption). An example of existing accelerator
architecture designs is the IBM wire-speed processor architecture [12], which features special-purpose dedicated accelerators optimized for security, XML parsing, compression,
etc. We believe that future computing servers will improve
their performance and power eﬃciency via extensive use of
accelerators.
Unlike GPUs, accelerators cannot hide memory latency
through extensive multithreading. The throughput of an
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Request permissions from Permissions@acm.org
DAC’15, June 07-11, 2015, San Francisco, CA, USA
Copyright is held by the owner/authors. Publication rights licensed to ACM.
ACM 978-1-4503-3520-1/15/06$15.00
http://dx.doi.org/10.1145/2744769.2744879.
accelerator is often bounded by the rate at which the accelerator is able to interact with the memory system. For this
reason, an open question is: How should the accelerators,
cores, buﬀers, L2 caches and memory controllers should be
interconnected for maximum eﬃciency? Existing interconnect designs for accelerator architectures use partial crossbar
switches [12, 10], bus [24] and packet-switched networks [3,
19] to transfer data between memory and accelerators. In
Larrabee [24], a wide bi-directional ring bus is used to limit
the communication latency. Bakhoda et al. [3] propose using
a packet-switched mesh interconnect to connect accelerators
and memory interfaces.
Unfortunately, as the number of on-chip accelerators increases, crossbar switches will consume intolerable area overhead due to the inability to scale. The ring bus solution
is not suﬃcient to avoid latency that reaches problematic
levels when the number of nodes increases beyond a certain
point. Even though the modern packet-switched network enables high bandwidth by sharing channels to multiple packet
ﬂows, it comes with long per-hop delays and a high energy
overhead.
To combat this problem, circuit-switched fabrics for CMPs
are explored [13, 27, 28]. Compared to packet-switched
networks, circuit-switched networks can signiﬁcantly lower
the communication latency, but suﬀer from the long setup
and teardown latency. In order to amortize the setup overhead, hybrid-switched networks are proposed, where packetswitched ﬂits and circuit-switched ﬂits are interleaved on the
same physical channels [16].
Our investigation shows that accelerator memory accesses
exhibit pairwise bulk transfer and streaming, creating highly
utilized but frequently changing paths between the accelerator and memory interfaces. Moreover, hardware accelerators feature well-deﬁned I/O patterns following ﬁxed timing,
allowing for better resource scheduling. Previous circuitswitched networks do not work well because the setup decisions are made locally and are unaware of the long-term
communication patterns or the characteristics of accelerators; this results in unnecessary setups/teardowns and infrequent use of circuit paths.
In the meantime, frequent
setup and teardown requests hurt the overall performance.
In this paper we propose the Hybrid network with Predictive Reservation (HPR) which globally manages NoC resources and reserves paths based on the timing of accelerator
memory accesses. The global accelerator manager identiﬁes
the location and timing of accelerator memory accesses, reserves paths and resolves conﬂicts in advance to improve
circuit utilization. Meanwhile, the circuit-switched paths
are reserved at the granularity of NoC cycles, maximizing
the beneﬁt of circuit-switched paths while minimizing the
interference caused to packet-switched traﬃc.
MC
MC
Processing
Engine
CPU / 
Accelerator
L1$
GAM
L2 Cache Bank
MC
Router
Tile
Memory 
Controller
Global Accelerator 
Manager
MC
GAM
MC
Figure 1: The overview of the accelerator-rich architecture
The key contributions of this work are:
• We analyze the common characteristics of accelerator communication patterns, which suggests opportunities and challenges in shared resource management.
• We extend the global accelerator manager to identify exact
locations and precise timing periods of accelerator memory
accesses, and predictively reserve a series of circuit-switched
paths before the accelerator starts to execute.
• To support the global circuit-switching decisions, we propose a new hybrid network design that provides predictable
performance for circuit-switched traﬃc, while minimizing
the interference caused to packet-switched traﬃc.
2. BASELINE ARCHITECTURE
In this section we describe our baseline accelerator architecture and on-chip interconnect. Fig. 1 illustrates the
overall chip layout of the accelerator-rich architecture [7].
This platform consists of cores (with private L1 caches), accelerators, shared L2 banks, memory controllers, the global
accelerator manager, and the on-chip interconnect. We assume a 2D mesh topology with memory controllers placed
in the corners, similar to the topology used in the Tilera
TILE64 [26] and Intel’s 80-core design [25], since it provides
a simple and scalable network.
Our on-chip accelerator architecture features three key
elements: 1) on-chip accelerators that implement complex
specialized ﬁxed functionalities, 2) buﬀer in NUCA [9], a
modiﬁed cache that enables allocation of buﬀers in the shared
last-level cache, 3) a global accelerator manager that assists
with dynamic load balancing and task scheduling.
Loosely Coupled Accelerators: The on-chip accelerators that we consider in this work are specialized hardware
computational units that are shared on an as-needed basis
among multiple cores. These accelerators can signiﬁcantly
improve performance and save energy. However, this massive increase in performance typically comes with an increase
in memory demand that is signiﬁcantly larger than generalpurpose cores. Fig. 2 shows the block diagram of loosely
coupled accelerators featuring a dedicated DMA-controller
and scratch-pad memory for local storage. The DMA engine is responsible for transferring data between the SPM
and shared L2 caches, and between SPMs in the scenario of
accelerator chaining.
Buﬀer in NUCA: Accelerators are designed to work with
private buﬀers. These buﬀers are introduced to meet two
goals: 1) bounding and reducing the ﬂuctuation in latency
Accelerator
Customized Datapath
DMA
SPM
Control Unit
Figure 2: The microarchitecture of the on-chip accelerator
Network Interface
between memory accesses, and 2) taking advantage of reuse
of data within the buﬀer. As the number of accelerators in
a system grows, the amount of dedicated buﬀer space devoted to these accelerators grows as well. In order to make
more eﬃcient use of chip resources, previous work provides
a mechanism to allocate these buﬀers in cache space [11, 9,
19]. When not used as buﬀers, this memory would instead
be used as regular cache. While there are a number of mechanisms for allocating buﬀers in cache, we chose the Buﬀer
in NUCA [9] scheme due to the consideration of spatial locality and the distributed nature of banked caches found in
many-core systems.
The Global Accelerator Manager (GAM): The GAM
fulﬁlls two primary roles: 1) it presents a single interface for
software to invoke accelerators, and 2) it oﬀers shared resource management including buﬀer allocation and dynamic
load balancing among multiple accelerators to allow collaboration on a single large task. To invoke an accelerator, a
program would ﬁrst write a description of the work to be
performed to a region of shared memory. This description
includes location of arguments, data layout, computation
to be performed, and the order in which necessary operations to be performed. By evaluating the task description,
the GAM splits the requested computation into a number
of ﬁxed-size data chunks to enable eﬃcient parallelism, prescreens common TLB misses using a shared TLB, and then
dispatches tasks to available accelerators [8].
3. NETWORK CHARACTERIZATION
Due to the pipelined hardware implementation, accelerators often exhibit predictable memory access patterns such
as bulk transfers and streaming. In this section we set out
to characterize the common accelerator memory access patterns that motivate our proposed NoC design.
We collect the number of streaming ﬂits between node
pairs and show the network traﬃc breakdown in Fig. 3.
Streaming ﬂits are deﬁned as consecutive ﬂits traveling between the same source/destination pair. We can observe
that the data streams account for a considerable fraction
of total on-chip data traﬃc – more than 70% of total ﬂits
transmitted – which suggests that eﬀective optimizations
will have a signiﬁcant impact on performance and eﬃciency.
We further investigate the memory access patterns of certain benchmarks to explore the potential optimization opportunities. Fig. 4(a) shows the L2 cache bank access traces
for Deblur from the medical imaging suite [6]. The Deblur accelerator takes in two 3D arrays, performs the Rician deconvolution and then outputs the 3D deconvoluted
array. The results are collected to show the input stage
of one accelerator execution using our simulation platform.
Memory data is mapped to cache lines at the memory page
granularity to allow for previous optimization techniques,
such as hardware prefetching and page coloring, to be easily
adapted. As we can see from this ﬁgure, the accelerator gen4. MECHANISM: HPR
We propose the hybrid network with predictive reservation (HPR) to exploit the accelerator memory access characteristics to improve the network performance. The goal
is to maximize the beneﬁt of circuit-switching data streams
while reducing the setup overhead and the interference with
packet-switched traﬃc. To achieve this goal, we propose
the global management to eﬀectively identify the timing of
setup and teardown of circuit-switching paths, and the timedivision multiplexing (TDM) based hybrid-switching to provide eﬃcient transmission with low overhead.
4.1 Global Management of Circuit-Switching
As described in Section 2, the global accelerator manager
(GAM) provides shared resource management and dynamic
load balancing. We extend the GAM to perform predictive reservation (PR) of NoC resources for accelerators to
improve the throughput of the network.
The GAM is able to obtain information on the accelerator
type and the read/write sets from the task description sent
by a core before assigning the task to the accelerator. Therefore, the latency of the accelerator can be easily obtained
and the possible read/write locations can be extracted by
performing address translations. Based on this information,
the communication pairs and the timing of communication
are known to the GAM without actually executing the task.
DMA
Memory
DMA
Computing
DMA
Memory
time
1
2
3
4
Figure 5: The service timeline of one accelerator execution
Taking advantage of this knowledge, the GAM is capable
of scheduling a series of circuit-switched paths at the granularity of time slot for the accelerator. As Fig. 5 shows, at
the beginning of processing the task, the DMA engine issues
requests to memory interfaces, according to the read set, to
fetch the data to the buﬀer. After these requests arrive at
the memory interface, responses will be generated after the
access latency, which can be estimated for cache and memory. Once all data requests are fulﬁlled, the accelerator will
start to read from the input buﬀer and write to the output buﬀer following the ﬁxed latency. Then results will be
written back to the memory. As we can tell from the above
process, the latency of each stage is either constant or can
be estimated, which suggests that by identifying the timing
of the four stages shown in Fig. 5, and in turn providing
predictable circuit-switching transmission to traﬃc in each
stage, unpredictable factors in the accelerator execution can
be largely reduced.
To prevent conﬂicts in path reserving, the GAM tracks the
current reservation status of each router in the network using
a global reservation table. It searches the table for routes
and conﬂicts before sending out the setup message. The
XY-routing scheme is adopted for simplicity.
If a conﬂict
is found in the table, the GAM will delay the targeted time
slots to the earliest available slots. Accordingly, future transactions will be delayed as an aftermath of the conﬂict. As
we try to make reserved circuit-switched paths as transient
as possible, the probability of conﬂicts is minimized, and the
penalty of delayed reservation is also negligible. With the
global management of circuit-switching, the result of setups
can be guaranteed beforehand. Thus, no ACK message is required, thereby reducing network traﬃc and setup latency
Figure 3: Network Traﬃc Breakdown. Results are collected
on the baseline architecture described in Section 2. Details
about the benchmarks and setups of the simulation platform
can be found in Section 5.
Figure 4: L2 cache bank access trace from the read phase
of Deblur and BlackScholes. A read to a cache bank is
depicted as a dot in the ﬁgure.
erates consecutive memory requests toward three L2 cache
banks. Since these requests have ideal spatial and temporal
locality, setting up a circuit-switched path would greatly improve the throughput. Based on our observations, however,
this is not always the case for accelerators targeting diﬀerent
domains.
Fig. 4(b) shows the access traces for BlackScholes from
the ﬁnancial analytics domain [4]. The ﬁgure clearly shows
that the accelerator reads from six diﬀerent banks that correspond to six input arrays deﬁned in the kernel. Unlike
Deblur, the BlackScholes accelerator only requests a small
amount of data from multiple cache banks. As a consequence, setting up circuit-switched paths for all destinations
will not help to improve the overall performance since each
path is not extensively used to amortize the setup overhead.
Although Deblur and BlackScholes have diﬀerent memory access patterns, both cases demonstrate that requests
are generated following predictable timings. Prior work also
proved that accelerator data streams have constant timing
intervals between consecutive requests [15].
Based on the above observations, we summarize that data
streams introduced by hardware accelerators often 1) account for a considerable portion of the total on-chip trafﬁc, 2) exhibit uninterrupted streams with diﬀerent lengths
towards multiple destinations, and 3) generate memory requests following constant timing. Previous optimizations on
packet-switched and hybrid-switched NoCs either add signiﬁcant per-hop delay to support streaming or fail to identify the pattern to improve circuit utilization.
In light of
this, we propose global management combined with hybridswitching to deliver eﬃcient and predictable NoC performance to accelerator-rich architectures.
of circuit-switched paths.
In the event that a TLB miss occurs during the read/write
session of an accelerator, the reserved circuit-switched paths
must be voided since they can no longer match the communication period, leading to performance degradation. To
combat this problem, we propose to translate the address
from the read/write set at the GAM, buﬀer corresponding
TLB entries and then send those entries to the accelerator alongside the task description. As a result, the address
translation requests at the accelerator side will always hit
in the local TLB so that the accelerator is able to execute
to completion without TLB misses.
In other words, the
TLB buﬀering mechanism eliminates the uncertainty in the
accelerator execution, providing the GAM with better estimations of communication timings.
Example 1 shows a case for circuit-switched paths reservation to summarize the proposed global management scheme.
Example 1 Circuit-switched Paths Reservation
1: The GAM receives a task description from a core:
2: Perform buﬀer allocation
3: Extract memory addresses from read/write sets
4: for all physical address obtained from the read set do
Locate the L2 $ bank and the Memory controller
Try to reserve circuit-switched paths using XY-routing between the buﬀer, L2 cache bank and memory controller
if conﬂicts found then
Delay the time window
5:
6:
7:
8:
9:
10:
end if
Update local reserved route record
11: end for
12: Reserve circuit-switching paths for both read and write session between the accelerator and the buﬀer
13: Do step 4 for the write set
14: Send the task description to the accelerator
4.2 Hybrid Network
We adopt a hybrid-switched router architecture similar
to [16, 29], as is shown in Fig. 6 and 7. To support the
2-stage hybrid-switched pipeline, the conventional 4-stage
virtual-channeled wormhole router is extended with circuitswitched latches, a reservation table, and demultiplexers.
Reservation Table
Routing Unit
Allocators
Crossbar
Ej
N
S
E
W
Inj
N
S
E
W
Figure 6: The hybrid-switched router architecture
BW
VA
SA
ST
LT
ST
LT
(a)
(b)
Figure 7: The packet-switched and circuit-switched pipeline
Once the setup decision is made by the GAM, a setup
ﬂit carrying the reservation information will traverse the
planned path. This setup ﬂit reserves consecutive time slots
for multiple future data streams sharing the same path (shown
in Fig. 8). To avoid multiple table entries occupied by the
same data stream, the reservation table is organized as each
entry corresponds to a future data stream – where a start
time, an end time, an input port and an output port are
recorded. The router uses the reservation table to conﬁgure
the switch in preparation for circuit-switching. By the completion of one circuit-switched session, the router recovers
to packet-switching mode, and the old table entry is freed
up to allow for new reservations.
StartTime EndTime InPort OutPort
StartTime EndTime InPort OutPort
200
208
1
3
201
209
1
3
Figure 8: An example of the reservation tables from two
neighboring routers
By the start of a reserved circuit-switched session, the
router needs to check the circuit ﬁeld of the incoming ﬂit
to determine if a data stream is arriving. Once a data
stream is conﬁrmed, the router will forward incoming ﬂits
directly to the switch that was already conﬁgured according
to the reservation. The packet-switched ﬂits in the buﬀer
are not allowed to perform virtual channel allocation (VA)
and switch allocation (SA) until the end of the session. If
the circuit ﬁeld is zero, this is a packet-switched ﬂit which
means no data stream matches the reservation. The reason
for a missed reservation can be speculative schedules of circuit paths (i.e., miss-predictions of the data locations). As
a result, the current circuit-switched session will be aborted
and packet-switched ﬂits are released to proceed to regular
pipeline stages.
In order for data streams to catch the reserved session,
the source and destination network interfaces are notiﬁed
of the reserved window as well. The source node will not
inject streaming ﬂits to the NoC until the reserved session
has taken place. This is essential for memory interfaces with
variable latency (due to contention, scheduling policies, etc.)
because by doing so, a dead line is set at the source node
to bound the unpredictable memory latency. As a consequence, uninterrupted streams can be generated to fully use
the reserved circuit-switched paths.
In summary, we propose HPR to eﬀectively identify the
timing of data streams and speculatively reserve circuitswitched paths to improve the throughput of the network.
Meanwhile, we design a hybrid network that allow the reserved circuit-switched paths to tolerate variable memory
latency and provide predictable performance with minimal
interference to the packet-switched traﬃc.
5. SIMULATION RESULTS
5.1 Experimental Methodologies
We extended the full-system cycle-accurate Simics [20]
and GEMS [21] simulation platform and modiﬁed GARNET [2] to model the baseline architecture described in Section 2 and the proposed HPR scheme illustrated in Section 4.
We use Orion 2.0 [17] to estimate the power and energy consumption of the NoC. We consider a 32-node mesh topology,
with one core, 30 on-chip accelerators and one GAM, with
parameters shown in Table 1.
The benchmarks used in our study are four benchmarks
from the medical imaging domain [6], two from the ﬁnancial
(a) Normalized execution time
(b) Normalized network latency
Figure 9: Overall simulation results
(c) Normalized energy
(a) Fraction of ﬂits covered by circuit-switched paths
(b) Network latency of control messages
Figure 10: Network traﬃc results
analytics domain [4], and three from the robotics domain [1].
We extract the computation-intensive kernel functions from
these benchmarks and implement them as on-chip accelerators. We used the high-level synthesis tool Vivado HLS from
Xilinx to generate the RTL of these computation-intensive
kernel functions, and used the Synopsys design compiler to
obtain ASIC characteristics of the accelerators.
Table 1: System parameters for the baseline architecture
NoC
Core
Coherence
protocol
L1 data &
inst cache
L2 cache
(static-NUCA)
Memory
4 × 8 mesh topology, XY routing,
wormhole switching, 4-stage router pipeline
4 Ultra-SPARC III-i cores @ 2GHz
MESI directory coherence
32KB for each core, 4-way set-associative,
64B cache block, 3 cycle access latency,
pseudo-LRU
2MB, 32 banks, each bank is 64 KB,
8-way set-associative, 64Byte cache block,
6-cycle access latency, pseudo-LRU
80GB/s bandwidth, 280-cycle access latency
We compare the HPR scheme against a baseline packetswitched NoC with four virtual channels, the express virtual
channel (EVC) [18] and a previously proposed TDM-based
hybrid-switched NoC for heterogeneous platforms [29] (denoted as HTDM below).
5.2 Experimental Results
Fig. 9(a) shows the comparison of normalized total execution time for each benchmarks against the baseline. HPR
reduced the total execution time by 11.3% over the baseline,
and 8.5% on average over EVC and HTDM. The largest reduction was seen for Deblur – about 20%. This is mainly attributable to high traﬃc levels between the accelerator and a
small number of locations in the benchmark, leading to high
circuit utilization. LPCIP, on the other hand, represents a
relatively small read/write set with dynamic memory accesses deﬁned during the execution. With GAM completely
oblivious to them, the dynamic accesses cannot be covered
by circuit-switched paths, and reserved paths also experience infrequent use due to unpredictable delay. As can be
seen from this ﬁgure, none of the evaluated schemes signiﬁcantly improve the performance.
Fig. 9(b) shows the normalized network latency for EVC
and HPR. EVC provides a consistent reduction in network
latency of about 15% since the mechanism aims to improve
the performance for all on-chip traﬃc and is oblivious to
the heterogeneity in network traﬃc. Meanwhile, the goal of
HPR is to provide eﬃcient and predictable transmission for
accelerator data streams. As a consequence, HPR reduces
latency for the most critical part of the traﬃc, resulting in
lower overall average network latency.
Fig. 9(c) shows the normalized network energy for the
diﬀerent benchmarks. Again, HPR shows energy reductions
over the compared schemes – a reduction of around 11% on
average going up to 16% for Deblur.
In terms of LPCIP,
whereas HTDM suﬀers from underutilized circuit-switched
paths due to short dynamic accesses, HPR saves setup energy on dynamic accesses and still gains from predictively
reserved circuit-switched paths.
In trying to study this gain further, we analyze the circuitswitching coverage of streaming ﬂits with respect to the total
number of on-chip streaming ﬂits (shown in Fig. 10(a)). As
shown in this ﬁgure, both HTDM and HPR provide relatively high coverage of streaming ﬂit. The reason that our
scheme achieves better performance is essentially twofold.
First, in setup of the same circuit-switched path, HTDM
requires the conﬁrmation of an ACK message before transmission, whereas our scheme eliminates the overhead by introducing global management. Second, our scheme also aims
to identify and circuit-switch control messages (e.g., memory
requests) which account for a small portion of ﬂits, but are
critical to the overall latency of memory access. In contrast,
previous schemes only target data movements. Fig. 10(b)
illustrates this eﬀect by showing the normalized average network latency for control messages. The largest reductions
were seen for Deblur, Swaptions and Robot, which receive
the largest reductions in execution time as well. This is because these reductions in network latency will in turn lead
to faster memory responses to the accelerator.
Acknowledgments
We thank the anonymous reviewers for their feedback. This
work was supported in part by C-FAR, one of the six SRC
STARnet Centers, sponsored by MARCO and DARPA, and
by NSF/Intel Innovation Transition (InTrans) Grant awarded
to the Center for Domain-Speciﬁc Computing (CDSC).
8. "
Bandwidth-efficient on-chip interconnect designs for GPGPUs.,"Modern computational workloads require abundant thread level parallelism (TLP), necessitating highly-parallel, many-core accelerators such as General Purpose Graphics Processing Units (GPGPUs). GPGPUs place a heavy demand on the on-chip interconnect between the many cores and a few memory controllers (MCs). Thus, traffic is highly asymmetric, impacting on-chip resource utilization and system performance. Here, we analyze the communication demands of typical GPGPU applications, and propose efficient Network-on-Chip (NoC) designs to meet those demands. We show that the proposed schemes improve performance by up to 64.7%. Compared to the best of class prior work, our VC monopolizing and partitioning schemes improve performance by 25%.","Bandwidth-Efﬁcient On-Chip Interconnect Designs
for GPGPUs ∗
Hyunjun Jang1 , Jinchun Kim2 , Paul Gratz2 , Ki Hwan Yum1 , and Eun Jung Kim1
1Department of Computer Science and Engineering, Texas A&M University, {hyunjun, yum, ejkim}@cse.tamu.edu
2Department of Electrical and Computer Engineering, Texas A&M University, {cienlux, pgratz}@tamu.edu
ABSTRACT
Modern computational workloads require abundant thread
level parallelism (TLP), necessitating highly-parallel, manycore accelerators such as General Purpose Graphics Processing Units (GPGPUs). GPGPUs place a heavy demand
on the on-chip interconnect between the many cores and
a few memory controllers (MCs). Thus, traﬃc is highly
asymmetric, impacting on-chip resource utilization and system performance. Here, we analyze the communication demands of typical GPGPU applications, and propose eﬃcient
Network-on-Chip (NoC) designs to meet those demands. We
show that the proposed schemes improve performance by up
to 64.7%. Compared to the best of class prior work, our
VC monopolizing and partitioning schemes improve performance by 25%.
Categories and Subject Descriptors
C.1.2 [Computer Systems Organization]: MultiprocessorsInterconnection architectures
Keywords
Network-on-Chip, Bandwidth, GPGPU
1.
INTRODUCTION
General Purpose Graphics Processing Units (GPGPUs)
have emerged as a cost-eﬀective approach for a wide range
of high performance computing workloads which have a high
thread level parallelism (TLP) [10]. GPGPUs are characterized by numerous programmable computational cores which
allow for thousands of simultaneous active threads to execute in parallel. The advent of parallel programming models, such as CUDA and OpenCL, makes it easier to program graphics/non-graphics applications, making GPGPUs
an excellent computing platform. The growing quantity of
parallelism and the fast scaling of GPGPUs have fueled an
increasing demand for performance-eﬃcient on-chip fabrics
ﬁnely tuned for GPGPU cores and memory systems [3, 11].
Ideally, the interconnect should minimize blocking by eﬃciently exploiting limited network resources such as virtual
channels (VCs) and physical channels (PCs) while ensuring deadlock freedom. Networks-on-Chip (NoCs) have been
useful in chip-multiprocessor (CMP) environments due to
their scalability and ﬂexibility. Although NoC design has
matured in this domain [9, 14], NoC design for GPGPUs is
still in its infancy. Only a handful of works have examined
the impact of NoC design in GPGPU systems [3, 11, 13, 15].
∗This work was partially supported by NSF CCF-1423433.
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are not 
made or distributed for proﬁt or commercial advantage and that copies bear 
this notice and the full citation on the ﬁrst p age. Copyrights for components 
of this work owned by others than ACM must be honored. Abstracting with 
credit is permitted. To copy otherwise, or republish, to post on servers or to 
redistribute to lists, requires prior speciﬁc permission and/or a f ee. Request 
permissions from Permissions@acm.org.
DAC ’15, June 07 - 11, 2015, San Francisco, CA, USA
Copyright 2015 ACM 978-1-4503-3520-1/15/06 ...$15.00
http://dx.doi.org/10.1145/2744769.2744803.
Unlike CMP systems, where traﬃc tends to be uniform
across the cores communicating with distributed on-chip
caches, the communication in GPGPUs is highly asymmetric, mainly between many compute cores and a few memory controllers (MCs). Thus the MCs often become hot
spots [3], leading to skewed usage of the NoC resources such
as wires and buﬀers. Speciﬁcally, heavy reply traﬃc from
MCs to cores potentially causes a network bottleneck, degrading the overall system performance. Therefore, when we
design a bandwidth-eﬃcient NoC, the asymmetry of its onchip traﬃc must be considered.
In prior work [3, 4, 11],
the on-chip network is partitioned into two independent,
equally divided (logical or physical) subnetworks between
diﬀerent types of packets to avoid cyclic dependencies that
might cause protocol deadlocks. Due to the asymmetric trafﬁc in GPGPUs skewed heavily towards reply packets, however, such partitioning can lead to imbalanced use of NoC
resources given in each subnetwork. Thus, it fails to maximize the system throughput, particularly for memory-bound
applications requiring a high network bandwidth to accommodate many data requests. The throughput-eﬀectiveness
is a crucial metric for improving the overall performance
in throughput-oriented architectures, thus designing a high
bandwidth NoC in GPGPUs is of primary importance. In
the GPGPU domain, this is the ﬁrst study evaluating and
analyzing the mutual impacts of diﬀerent MC placements
and routing algorithms on system-wide performance. We observe that the interference from disparate types of GPGPU
traﬃc can be avoided by adopting the bottom MC placement with proper routing algorithms, obviating the need of
physically partitioned networks.
The contributions of this work are as follows: First, we
quantitatively analyze the impact of network traﬃc patterns in GPGPUs with diﬀerent MC placements and dimension order routing algorithms. Then, motivated by this
detailed analysis, we propose VC monopolizing and partitioning schemes which dramatically improve NoC resource
utilization without causing protocol deadlocks. We also investigate the impact of XY, YX, and XY-YX routing algorithms under diverse MC placements. Simulation results
show the proposed NoC schemes improve overall GPGPU
performance by up to 64.7% over baseline. Compared to the
top performing prior work, our VC monopolizing and partitioning schemes achieve a performance gain of 25% with a
simple MC placement policy.
2. BACKGROUND
In this section, we describe the baseline GPGPU architecture and NoC router microarchitecture in detail.
2.1 Baseline GPGPU Architecture
A GPGPU consists of many simple cores called streaming
multiprocessors (SMs), each of which has a SIMT width of
8. The baseline GPGPU architecture consists of 56 SMs and
8 MCs as shown in Figure 1. Each SM is associated with
a private L1 data cache, read-only texture/constant caches,
and register ﬁles along with a low latency shared memory.
Figure 1: GPGPU NoC Layout and Router Microarchitecture. (The NoC layout consists of many SMs and a few MCs,
each of which contains an NoC router.)
Every MC is associated with a slice of the shared L2 cache
for faster access to the cached data. We assume write-back
polices for both L1 and L2 caches [4], and minimum L2 miss
latency is assumed to be 120 cycles. We assume a 2D mesh to
connect cores and MCs as in Figure 1 due to its advantages
of scalability, simplicity and regularity [3].
2.2 Baseline NoC Router Architecture
Figure 1 shows the baseline NoC router, which has 5 I/O
ports to connect the SMs to L2 cache and MCs in a GPGPU.
The router is similar to that used by Kumar et al. [12]
employing several features for latency reduction, including
speculation and lookahead routing. Each arriving ﬂit goes
through 2 pipeline stages in the router: routing computation (RC), VC allocation (VA), and switch arbitration (SA)
during the ﬁrst cycle, and switch traversal (ST) during the
second cycle. Each router has multiple VCs per input port
and uses ﬂit-based wormhole switching. Credit-based VC
ﬂow control is adopted to provide the backpressure from
downstream to upstream routers, which controls ﬂit transmission rate to avoid buﬀer overﬂows.
3. DESIGNING BANDWIDTH-EFFICIENT
NOCS IN GPGPUS
Here, we analyze the GPGPU workload NoC traﬃc characteristics and their impact on system behavior. Based on
this analysis, we propose VC monopolization and asymmetric VC partitioning to achieve higher eﬀective bandwidth.
3.1 GPGPU On-Chip Trafﬁc Analysis
3.1.1 Request and Reply Trafﬁc
Prior work shows on-chip data access patterns to be more
performance critical than data stream size in GPGPUs [7].
Further, these traﬃc patterns are inherently many-to-few (in
the request network, from the many cores to the few MCs)
and few-to-many (in the reply network, from the MCs back
to the cores) [3]. As shown in Figure 2 MC-to-core, the reply
network sees much heavier traﬃc loads than core-to-MC, the
request network. This is because the request network consists of many short packets (read requests) mapped into a
single ﬂit and fewer long packets (write requests) mapped
into 3∼5 ﬂits. The reply network consists of many long
packets (read reply) mapped into 5 ﬂits and relatively a few
short packets (write reply) mapped into a single ﬂit. Figure 3 shows that on average around 63% of packets are read
replies. Exceptionally, RAY, contains more request packets
than reply packets, due to a write demand in this application.
In general, the ratio between request and reply traﬃc can
Figure 2: Normalized Traﬃc Volumes Between Cores and
MCs.
Figure 3: Packet Type Distribution for GPGPU Benchmarks
(cid:48) · Ls
be derived as follows. Considering the overall injection rate
as λ at each node, we denote the ratio of read and write
requests by r and w, respectively, and the sum of r and w
equals one because request consists of only two types: read
and write. The length of each packet can be divided into
two groups: a short packet (Ls ) representing read request
and write reply, and a long packet (Ll ) including read reply
and write request. The amount of request traﬃc Trqs is the
sum of read and write requests and likewise, the amount of
reply traﬃc Trep is the sum of read and write replies.
Trqs = λ · r · Ls + λ · w · Ll
Trep = λ · r
(cid:48) · Ll + λ · w
(1)
where r (cid:48) and w (cid:48) denote ratios of replies for read (r) and
write (w) requests, respectively. Since a single request is
always followed by a single reply, the ratio between r and
w is identical to that of r (cid:48) and w (cid:48) . Here, the ratio of reply
to request (R) is derived by dividing Trep by Trqs . Thus,
according to Figure 2, R equals around two since the traﬃc
volume of reply packets is two times higher than that of
request packets.
Figure 4 illustrates the traﬃc, Trqs and Trep , in an (N xN )
mesh network with k MCs. In this ﬁgure, we take an example of N = k = 4, and XY routing with the bottom
MC placement. Each arrow represents the direction of trafﬁc and the associated coeﬃcient number denotes the link
utilization toward that direction. By multiplying the coeﬃcient with either Trqs or Trep , we can approximate the
amount of traﬃc towards a speciﬁc direction. For vertical
links, a coeﬃcient value is determined by the row location.
For example, each core located at the 1st row (i=1 ) uses its
south output port 4 times. If a core sends request packets
to N MCs at the bottom row, the south port of a router
associated with the core is utilized by N times. Thus, for a
core located at the ith row in an (N xN ) mesh, the request
coeﬃcient towards south direction becomes N · i. Similarly,
we can utilize the core’s column in deriving coeﬃcient values
for horizontal links. If a core j is located at the j th column
in the mesh network, N − j MCs are on the east side of this
(a) XY Request
(b) XY Reply
(c) XY Request + XY Reply
Figure 4: Network traﬃc example with XY routing. (Note
that request (a) and reply (b) traﬃc take diﬀerent paths, thus
traﬃc does not mix on horizontal and vertical links.)
core. To access these N − j MCs, all cores located from the
1st to the j th columns must use the east output port of core
j. Therefore, the coeﬃcient for east port of j th core is set
by j · (N − j ). In a similar way, we analyze the coeﬃcient
values for all directions as follows.
Csouth = N · i
Cnorth = N · (i − 1)
Ceast = j · (N − j )
Cwest = (N − j + 1) · (j − 1)
The quantitative analysis above proves three important
facts about the characteristics of GPGPU network traﬃc.
First, reply traﬃc is much heavier than request traﬃc as
empirically shown in Figures 2 and 3. Second, both request
and reply traﬃc get congested as they approach to the MCs
located at the bottom. The growing coeﬃcient values of
Csouth and Cnorth from Equation 2 support this argument.
Third, request and reply traﬃc do not get mixed on horizontal or vertical links as shown in Figures 4(a) and 4(b).
(2)
3.1.2 Memory Controller Placement
One way of alleviating traﬃc congestion is to move the
MCs. Diﬀerent MC placements help improve network latency and bandwidth by spreading the processor-memory
traﬃc, balancing the NoC load. While prior work on MC
placement shows the performance improvement to be gained
from MCs placement in CMPs [2], this work does not analyze how MC placement aﬀects the average hop count for
GPGPUs. Here, we conduct a detailed quantitative analysis
of GPGPU MC placement policies and show how distributed
MC location can improve the NoC eﬃciency in GPGPUs.
The most clear advantage of distributed MC placement
is the reduced average number of hops from cores to MCs,
as shown in Figure 5. Comparing with other MC placements, speciﬁcally, the diamond MC placement shown in
Figure 5: Diﬀerent MC Placements. Shaded tiles represent
MCs co-located with GPGPU cores
Havg =
=
N(cid:80)
i=1
Hvert + Hhori
N 2−N(cid:80)
j=1
Figure 5(d) allows more cores to access MCs with fewer hops.
Since we are using dimension order routing, there is only one
unique path from each core to a given MC. The row and column number of ith MC are assumed to be rowm,i and colm,i ,
respectively. In the same way, the row and column number
of j th core are rowc,j and colc,j . With this notation, for an
(N xN ) mesh network with N number of MCs and (N 2 − N )
(cid:80) (Vertical + Horizontal hops)
cores, the average number of hops from cores to MCs can be
estimated as follows:
(N 2 − N )N
Total number of paths
|rowm,i − rowc,j | + |colm,i − colc,j |
N 2 (N − 1)
Note that Equation 3 is a general form of the equation applicable to any MC placement. Hvert and Hhori represent
the aggregated number of hops for vertical and horizontal
directions. We summarize Hvert and Hhori of diﬀerent MC
placements in Table 1.
Based on Table 1, we ﬁnd that sorting the MC placement
diagrams in the order of decreasing average number of hops
yields the following order: bottom, edge, top-bottom, and diamond. This analysis also corresponds with results from prior
work [2], which reported the best performance improvement
with diamond MC placement. Although the diamond placement shows the least number of hops, we show that other
MC placement policies can outperform the diamond placement by adopting VC monopolizing and diﬀerent routing
algorithms in Section 4.2.
=
(3)
3.2 Proposed Design
3.2.1 VC Monopolizing and Asymmetric VC Partitioning
Request and reply packets in GPGPUs compete for NoC
resources such as VCs and PCs. When the resources are
na¨ıvely shared by both packets, avoiding protocol deadlock
requires that reply packets must not compete for the same
resources as request packets. To avoid this, prior studies [4, 3, 11] suggest partitioning NoCs equally into two
MC placement
Hvert
Hhori
Bottom
Edge
Top-Bottom
N 3 (N − 1)
2
N 2 (N − 1)2
2
N 2 (N − 1)2
2
N (N + 1)(N − 1)2
3
≈ N (N + 1)(N − 1)2
3
N (N + 1)(N − 1)2
3
Diamond
≈ N 2 (N + 1)(N − 2)
8
≈ N 2 (N + 1)(N − 2)
8
Table 1: The average number of vertical/horizontal hops
under diﬀerent MC placements in an (N xN ) mesh
parts for the diﬀerent types of traﬃc: one network carries
request packets and the other network reply packets. Creating two parallel physical networks [11] incurs signiﬁcant
hardware overheads due to the twofold increase in the number of routers and wire resources. To this overhead, we
employ a virtual network partitioning, where the network is
divided virtually by two separate sets of VCs dedicated for
request-reply traﬃc under one physical network.
However, when all MCs are located at the bottom, request and reply traﬃc are not overlapped with dimension
ordered routing as shown in Figure 4. Therefore, there is
no need to split networks to avoid protocol deadlock. Thus,
all the VCs can be ful ly monopolized by either request or
reply packets, providing more buﬀer resources for each type
of traﬃc, thus helping improve overall system performance.
On the other hand, VC monopolizing is not feasible when
VCs have mixed request and reply traﬃc, as shown in Figure 6(c). These mixed VCs must be partitioned into request
and reply packets to avoid protocol deadlock. In this case,
we propose asymmetric VC partitioning which assigns more
VCs to reply traﬃc. Since reply traﬃc generally requires
much more network bandwidth than request traﬃc, moving VC resources from the request to the reply improves
the overall system performance while maintaining the same
overall NoC area and power budget. The detailed evaluation
is described in Section 4.2.
3.2.2 Routing Algorithms
A routing algorithm is one of the critical factors in achieving bandwidth-eﬃcient NoC, inﬂuencing the amount of trafﬁc each link will carry. Routing contributes to the reduction in network contention (hot spots) when combined with
an appropriate MC placement. To ﬁnd the performanceoptimal combination of a routing algorithm and an MC
placement, we analyze the impact of diﬀerent dimension order routing algorithms (XY, YX, and XY-YX [2]) under
the diﬀerent MC placements shown in Figure 5. For example, under our baseline MC placement, bottom MC, shown
in Figure 5(a), XY routing incurs increased network contention mainly due to the high volume of reply traﬃc between MCs, thus degrading overall system performance. Alternatively, XY-YX routing which leads request packets to
follow XY routing, while reply packets to follow YX routing,
helps achieve signiﬁcant performance improvement because
heavy traﬃc between MCs due to reply packets is entirely
eliminated as shown in Figure 6. Since the request traﬃc
in YX routing still generates contention between MCs, the
performance improvement of YX routing is less than that of
XY-YX routing. However, the reply traﬃc in YX routing
does not cause any communication between MCs since the
(a) XY Request
(b) YX Reply
(c) XY-YX Routing
Figure 6: Network traﬃc example with XY-YX routing.
(Note, request/reply traﬃc is mixed on horizontal links.)
reply traﬃc always traverses to the Y direction ﬁrst. Therefore, XY-YX or YX routing is eﬀective in load-balancing
the processor-memory traﬃc in a 2D mesh topology with
the bottom MC placement scheme1 . On other MC placement schemes, we ﬁnd routing algorithms have little impact
on overall performance. Simulation results for diﬀerent MC
placements and routing algorithms are detailed in Section 4.
4. PERFORMANCE EVALUATION
In this section, we evaluate schemes proposed in Section 3
with the aim of developing a high performance NoC, optimized for use in GPGPUs. We also analyze simulation results in detail using a wide variety of GPGPU benchmarks.
4.1 Methodology
The NoC designs and MC placement schemes examined
here are implemented in GPGPU-Sim [5]. The simulator is
ﬂexible enough to capture the internal design of GPGPU
and our target architecture has similarities to NVIDIA’s
FermiGTX 480. Figure 1 shows the NoC router microarchitectures modeled in GPGPU-Sim. A 2D mesh network
is used to connect SMs, caches, and MCs. To prevent protocol deadlock, the baseline NoC (Table 2) is built with a
single physical network with two separate VCs for handling
request and reply traﬃc. We evaluate our schemes with a
wide range of GPGPU workloads such as CUDA SDK [1],
ISPASS [4], Rodinia [6], and MapReduce [8]. Each benchmark suite is structured to span a range of parallelism and
compute patterns, providing feature options that help identify architectural bottlenecks and ﬁne tune system designs.
4.2 Performance Analysis
1Note, we do not consider adaptive routing because of the
increased critical path delays in a router [2] and degraded
row buﬀer locality caused by not preserving the order of
request packets to MCs [15].
System Parameters
Shader Core
Memory Model
Interconnect
Virtual Channel
VC Depth
Warp Scheduler
MC placement
Shared Memory
L1 Inst. Cache
L1 Data Cache
L2 Cache
Min. L2 / DRAM Latency
Details
56 Cores, 1400 MHz, SIMT width = 8
8 MCs, 924 MHz
8 x 8 2D Mesh, 1400 MHz, XY Routing
2 VCs per Port
4
Greedy-then-oldest (GTO)
Bottom
48KB
2KB (4 sets/4 ways LRU)
16KB (32 sets/4 ways LRU)
64KB per MC (8-way LRU)
120 / 220 cycles
Table 2: System Conﬁguration
Figure 8: Speed-up with VC monopolized scheme (Normalized to XY routing with VC separated for each traﬃc)
Figure 7: Speed-up with routing algorithms (Normalized to
baseline XY)
Impact of Network Division. As described in Section 3.2.1,
we advocate for a single physical network with separate virtual networks for request and reply packets. To avoid protocol deadlock, we increase the number of VCs per port,
where diﬀerent types of packets traverse on-chip networks
via diﬀerent VCs.
It is noted that additional VCs employed to avoid a protocol deadlock can aﬀect the critical
path of a router since VC allocation is the bottleneck in the
router pipeline [11]. However, we observe that two separate VCs under a single physical network degrades system
performance less than 0.03% in geometric mean across 25
benchmarks. This observation leads us to use separate VCs
with a single physical network instead of two physical networks requiring more hardware resources.
Impact of Routing Algorithms. While maintaining the
same number of VCs with reduced network resources, we
observe that alternative routing algorithms can signiﬁcantly
improve the overall system performance. Figure 7 shows
the speed-up obtained with YX and XY-YX, normalized
against the baseline XY. YX and XY-YX with the bottom MC placement scheme achieve a speedup of 39.3% and
64.7%, respectively. As discussed in Section 3.2.2, the improvement mainly comes from mitigated traﬃc congestions
between MCs. The heavy reply traﬃc generated from MCs
is the main factor causing performance bottlenecks in NoCs.
In this context, XY-YX routing outperforms YX routing by
more than 25%. This is because unlike YX routing, XY-YX
completely removes the resource contention between MCs
by providing diﬀerent routing paths for request and reply
packets as illustrated in Figure 6.
VC Monopolizing. As illustrated in Figure 4, request and
reply packets never overlap with each other in any dimension
under XY or YX routing with the bottom MC placement,
allowing VC monopolization as described in Section 3.2.1.
Figure 8 shows the impact of VC monopolizing on system
performance under diﬀerent routing algorithms. Monopolized VCs lead XY and YX to achieve 43.8% and 88.9% (=
39.3% from YX + 49.6% from monopolization) of speed-up
in geometric mean, respectively. Note that unlike XY and
YX routing, XY-YX routing still requires separate VCs in
horizontal links to prevent protocol deadlock because different types of packets get potentially mixed while moving
Figure 9: Speed-up with diﬀerent MC placements with routing algorithms (PM: Partial Monopolizing, FM: Full Monopolizing, Normalized to bottom MC+XY routing)
along the horizontal links as illustrated in Figure 6. This
limits the number of VCs that can be monopolized (partial
monopolizing ) because only the VCs located in the vertical
links can be fully monopolized in XY-YX routing. Accordingly, partially monopolized XY-YX routing shows less performance improvement at 85.4% (= 64.7% from XY-YX +
20.7% from monopolization), compared to that of the fully
monopolized scheme with YX routing. Furthermore, across
diﬀerent MC placements (edge, top-bottom, and diamond ),
VC monopolizing is eﬀective in achieving better performance
improvement, as detailed below.
Impact of MC Placement Scheme. In our baseline, we
simply put all MCs at the bottom row in 8 × 8 2D mesh.
As the network traﬃc in GPGPUs gets skewed towards the
MCs in this scheme, the bottom MC placement causes high
network congestion near the MCs, thus degrading performance. One way to alleviating such traﬃc congestion is to
locate MCs sparsely across the network. Figure 9 shows the
impact of diﬀerent MC placements on overall system performance. Note that each MC placement is simulated with
three diﬀerent routing algorithms (XY, YX, and XY-YX). In
Figure 9, we pick the routing algorithm showing the highest
performance improvement for each MC placement scheme.
In the ﬁgure we see that MC location has a signiﬁcant impact on performance. This is because, with distributed MC
placements, request and reply packets are spread across multiple locations of the on-chip network rather than converging to the bottom row. Compared to the bottom MC placement, the average performance speedup is 37.3%, 64.4%, and
40.4% for the edge, diamond, and top-bottom placements, respectively. And when applying the VC monopolizing scheme
in combination with diﬀerent MC placements, additional
28.3%, 12%, and 47.3% performance improvement (65.6%,
76.4%, and 87.7% in total) are achieved with the edge, diamond, and top-bottom placements, respectively. Here it
is worthwhile to note that our baseline bottom MC placement combined with YX routing and fully monopolized VCs
shows the highest performance improvement (89.4%) and
even outperforms the prior top-performing work, diamond
MC placement, by 25% (= 89.4% - 64.4%), even though the
6. CONCLUSIONS
In this paper, we analyze the unique characteristics of onchip communication within GPGPUs under a wide range of
benchmark applications. We ﬁnd that the many-to-few and
few-to-many traﬃc patterns between cores and MCs create
a severe bottleneck, leading to the ineﬃcient use of NoC
resources in on-chip interconnects. We show the improved
system performance based on VC monopolizing and asymmetric VC partitioning under diverse MC placements and
dimension ordered routing algorithms.
7. "
DimNoC - a dim silicon approach towards power-efficient on-chip network.,"The diminishing momentum of Dennard scaling leads to the ever increasing power density of integrated circuits, and a decreasing portion of transistors on a chip that can be switched on simultaneously---a problem recently discovered and known as dark silicon. There has been innovative work to address the ""dark silicon"" problem in the fields of power-efficient core and cache system. However, dark silicon challenges with Network-on-Chip (NoC) are largely unexplored. To address this issue, we propose DimNoC, a ""dim silicon"" approach, which leverages drowsy SRAM and STT-RAM technologies to replace pure SRAM-based NoC buffers. Specifically, we propose two novel hybrid buffer architectures: 1) a Hierarchical Buffer (HB) architecture, which divides the input buffers into a hierarchy of levels with different memory technologies operating at various power states; 2) a Banked Buffer (BB) architecture, which organizes drowsy SRAM and STT-RAM into separate banks in order to hide the long write-latency of STT-RAM. Our experiments show that the proposed DimNoC can achieve 30.9% network energy saving, 20.3% energy-delay product (EDP) reduction, and 7.6% router area decrease compared with the baseline SRAM-based NoC design.","DimNoC: A Dim Silicon Approach towards Power-Efﬁcient
On-Chip Network
Jia Zhan† , Jin Ouyang§ , Fen Ge* , Jishen Zhao‡ , Yuan Xie†
†University of California Santa Barbara, §NVIDIA Corporation,
*Nanjing University of Aeronautics and Astronautics, ‡University of California Santa Cruz
† {jzhan, yuanxie}@ece.ucsb.edu, § jouyang@nvidia.com,
*gefen@nuaa.edu.cn, ‡ jishen.zhao@ucsc.edu
ABSTRACT
The diminishing momentum of Dennard scaling leads to the ever
increasing power density of integrated circuits, and a decreasing
portion of
transistors on a chip that can be switched on
simultaneously—a problem recently discovered and known as dark
silicon. There has been innovative work to address the “dark silicon""
problem in the ﬁelds of power-efﬁcient core and cache system.
However, dark silicon challenges with Network-on-Chip (NoC) are
largely unexplored. To address this issue, we propose DimNoC, a
“dim silicon"" approach, which leverages drowsy SRAM and STT-RAM
technologies to replace pure SRAM-based NoC buffers. Speciﬁcally,
we propose two novel hybrid buffer architectures: 1) a Hierarchical
Buffer (HB) architecture, which divides the input buffers into a
hierarchy of levels with different memory technologies operating at
various power states; 2) a Banked Buffer (BB) architecture, which
organizes drowsy SRAM and STT-RAM into separate banks in order
to hide the long write-latency of STT-RAM. Our experiments show
that the proposed DimNoC can achieve 30.9% network energy saving,
20.3% energy-delay product (EDP) reduction, and 7.6% router area
decrease compared with the baseline SRAM-based NoC design.
Categories and Subject Descriptors: C.2 [Computer-Communication
Networks]: Network Architecture and Design
General Terms: Performance, Design
Keywords: Network-on-Chip, Dark Silicon, STT-RAM
1
Introduction
Dennard scaling, which has offered near-constant chip power with
doubling number of transistors, has come to an end [8]. Computer
designers are seeking ways to stay on the performance curve without
exceeding the thermal design power
(TDP) by using emerging
many-core processors. Dynamic voltage/frequency scaling (DVFS) of
cores can mitigate the issue. However, many-core processors integrate
increasingly more transistors than those can remain powered-on
simultaneously. Recent studies refer to the fraction of chip which is
entirely powered-off as “dark silicon"" [27].
To address the challenges of many-core scaling in the dark
silicon era, most prior studies focus on core or memory sub-system
optimization, while dark-silicon-aware on-chip interconnect design has
This work is supported in part by NSF grant 1500848, 1461698, and
1213052. This work is part of the ASKS project (http://www.ece.ucsb.
edu/~yuanxie/projects/ASKS/).
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee. Request permissions
from Permissions@acm.org.
DAC ’15, June 07 - 11, 2015, San Francisco, CA, USA
Copyright 2015 ACM 978-1-4503-3520-1/15/06$15.00
http://dx.doi.org/10.1145/2744769.2744824.
not drawn much attention. Network-on-Chip (NoC) has a signiﬁcant
impact on the overall performance of many-core processors and
consumes a large portion of total power budget. Recent research and
industrial prototypes have validated that about 10%–36% [3, 10, 25] of
total chip power is consumed by NoC. Moreover, in the dark silicon
era, the majority of on-chip core/cache components may be put in the
dormant mode due to the TDP limit. Without further optimization, the
NoC components must remain active, because a gated-off router will
block packet-forwarding and the access to shared caches/directories.
As a result, the ratio of NoC power rises substantially among the
remaining on-chip active resources, e.g. 42% in a 32 core system [29].
Driven by these observations,
there have been some research
efforts to aggressively shut down parts of the NoC,
leveraging
conventional power-gating [4, 6, 16, 22, 23] techniques to reduce idle
power. However, NoC power-gating is heavily dependent on the
trafﬁc pattern.
In order to beneﬁt from power-gating, an adequate
idle period (break-even time) of a router is required to secure any
appreciable power saving. However, the major performance penalties
for waking up the power-gated blocks (in the range of 10∼20 cycles
[4, 6, 23] depending on the frequency) are undesirable. To alleviate
this drawback and achieve better power saving, we explore the
replacement of conventional NoC buffers by drowsy SRAM, which
uses similar circuitry in drowsy cache [9] to introduce an intermediate
sleep mode between power-on and power-gating state. In addition to
power management techniques, emerging non-volatile memory (NVM)
technologies enable new opportunities to enhance the power-efﬁciency
of NoC. Since buffer is the dominant leakage consumer in NoC, it
may be constituted by NVM whose leakage power is considerably
lower than conventional SRAM. Among alternative NVM technologies
(PCM, STT-RAM, ReRAM), STT-RAM is particularly promising to
replace NoC buffers because of its high cell density, non-volatility
feature, low leakage power consumption, and high endurance [11].
In this paper, we propose DimNoC which uses much
ﬁner-granularity power saving techniques rather than power-gating in
prior dark-silicon researches. To that end, our work can be viewed as
a dim-silicon approach dimming the power consumption with novel
buffer-management schemes. It introduces hybrid buffer architectures
which combine the beneﬁts of both drowsy SRAM and non-volatile
STT-RAM. The rational behind the hybrid organization is that, we can
use drowsy SRAM to reduce the wake-up penalty of every sleeping
block, and leverage the low-leakage property of STT-RAM to further
eliminate unnecessary power-gating operations, and thus, reduce the
number of costly wake-up operations. Moreover, the combination
of these two technologies mitigates the disadvantages of each other:
the long-latency write operations of STT-RAM can be reduced by
steering frequent write accesses to drowsy SRAM, and the hardware
overhead of drowsy circuit can be compensated by the area saving
from the high-density STT-RAM cells. Speciﬁcally, two novel hybrid
buffer architectures are proposed in this paper: 1) Hierarchical Buffer
(H B), in which each input buffer of the NoC router is organized as
multiple levels of storage, where each level is composed of a different
memory technology. Depending on the trafﬁc load, different levels are
activated successively. STT-RAM is designed to be lastly activated to
(a) Router power breakdown (dynamic power vs leakage power) when
varying the operating voltage and frequency.
Figure 2: Histogram of inter-arrival time for a router
(b) Dynamic power breakdown
(c) Leakage power breakdown
Figure 1: Dynamic and leakage power breakdown of a virtual-channel based router ((b) and
(c) are for 32nm and 0.9v case in (a)).
avoid frequent slow write operations. 2) Banked Buffer (BB), in which
drowsy SRAM buffers and STT-RAM buffers are logically interleaved
within each VC, but physically separated into multiple banks to hide
the long write latency of STT-RAM.
2 Challenges and Opportunities
Router Power Analysis. To better understand the power distribution
of a router, we simulate a classic wormhole router with DSENT [24]
and plot the power breakdown of both dynamic and leakage power
for different process technologies: 45nm, 32nm, and 22nm. We use a
frequency of 1GHz, and the supply voltages for different technologies
are, by default, 1.0V, 0.9V, and 0.8V, respectively. For sensitivity
studies, we vary the supply voltage around the default values. The
ﬂit width is set to be 128 bits. Each input port of a router comprises 2
virtual channels (VC) and each VC is 4-ﬂit deep. The power numbers
are estimated with an average injection rate of 0.3 ﬂits/cycle.
As shown in Figure 1a, for a certain process technology,
the
percentage of leakage power increases as the supply voltage reduces;
As technology scales from 45nm to 22nm, the ratio of leakage power
increases and substantially outweighs that of dynamic power. For
example, leakage power is around 63.4% of total power with a supply
voltage of 0.9V at 32nm technology. These results also reveal the
power crisis in the dark silicon era due to the increasing leakage
power. The breakdown of dynamic and leakage power consumption
for different router components are shown in Figure 1b and Figure 1c
(32nm, 0.9V), respectively.
In summary, Figure 1 reveals that: (1)
Leakage power dominates the power budget in the dark silicon era.
(2) Buffers become the primary power consumer for NoC.
The Limitation of Conventional Power Gating. In the dark silicon
background, as a signiﬁcant percent of transistors cannot be switched
on due to TDP limit, researchers started to explore power-gating on
NoC: completely shut down idle routers or links and then wake them
up when new packets arrive. Power-gating is implemented by inserting
appropriately sized transistor(s) with high threshold voltage between
Vd d and the logic block. Therefore, the entire router block can be
switched between on and off states by asserting and de-asserting the
sleep signal. However, applying power-gating on NoC has been
elusive, because frequently switching routers/links off and on will incur
not only signiﬁcant energy overhead, but also considerable wake-up
delay. The inter-arrival period of packets has to be long enough in
order to compensate for the overheads.
Therefore, the beneﬁt of applying power-gating on NoC is largely
trafﬁc-dependent. As a motivational example, we run the disparity
workload on a NoC-based sixteen-core system and conduct statistical
analysis on the intervals of consecutive ﬂit arrivals. As the histogram
in Figure 2 shows, some long inter-arrival periods exist in the network
which offers the opportunity of power-gating. Nevertheless, most
intervals are less than 10 cycles (the break-even time of routers is
approximately 10 cycles [4, 6, 23]) and among them a signiﬁcant
portion are less than 2 cycles.
Consequently,
in this example,
conventional power-gating techniques will be detrimental
to the
network performance because of frequent on-off state transitions of
network routers, and even severely offset the power saving gained from
long idle intervals.
Fine-Grained Power Management. Conventional power-gating
techniques shut down a router only when it
is completely idle,
which limit their application in skewed or heavily-loaded network
trafﬁc. Correspondingly, the whole router needs to be woken up for
a new arrival, which incurs substantial wake-up overhead. However,
depending on the trafﬁc, buffer utilization may vary over time (temporal
difference) and different VCs have different occupancies (spatial
difference). To test this hypothesis, using the same disparity
example, we proﬁle the buffer occupancies for two VCs in a physical
port, the result of which is presented in Figure 3. Within each VC, we
found that their utilization differ most of the time. Furthermore, when
comparing VC 0 with VC 1, their buffer utilizations differ at most time.
There are periods when VC 0 is busy while VC 1 is idle, and vice versa.
The variation of buffer utilization over time and space indicates
indicates that only a subset of buffers are required at most time. A
ﬁner-granularity power management of buffers can potentially save
signiﬁcant power. In contrast, prior proposals of bufferless NoC [18]
or elastic-buffered NoC [17] aggressively carve away nearly all buffers
from NoC to save leakage power (and area). Consequently, these
designs suffer from performance penalties under high load and do not
support VCs for multiple trafﬁc classes.
3 Our Method: DimNoC
Instead of completely shutting down the on-chip components in
response to power shortage, another option is to operate them
under-clocked, namely ""dim silicon"".
Conventional approaches
towards this direction are employing dynamic voltage and frequency
scaling (DVFS) on individual NoC routers/links [19, 28]. While these
approaches have shown some promising power saving, their feasibility
is unclear as the per-node voltage regulators incur non-negligible area
overhead and the re-timing latency leads to signiﬁcant performance
degradation.
In order to achieve a practical dim-silicon NoC (DimNoC) design,
our study focuses on optimizing buffer architecture—the primary
NoC power consumer—rather than adopting conventional DVFS in
individual routers. We leverage both drowsy SRAM and STT-RAM
techniques to design power-efﬁcient NoC buffers.
In particular, the
drowsy SRAM buffers introduce an intermediate power state between
power-on and power-gating to achieve better performance-power
trade-off; the STT-RAM buffers dissipate low leakage power, endure
frequent packet accesses, and consist of high-density cells to save area.
Furthermore, we integrate drowsy SRAM with STT-RAM to
simultaneously leverage the advantages of both memory technologies
to 1) reduce wake-up penalty of gated buffers using drowsy circuit, 2)
Figure 3: Buffer utilization for different VCs
avoid unnecessary power-gating and wake-up by utilizing low-leakage
STT-RAM, and 3) reduce the area budget with the dense STT-RAM
cells. As a result, our design can substantially reduce network power
and area with negligible performance overhead.
3.1 Drowsy SRAM Buffers
Our ﬁrst technique dims the network by introducing an intermediate
“drowsy"" state between “on"" and “off"", in which a router operates at
a low-power state which retains the data but does not allow for write
accesses. Since the voltage in the drowsy state is lower than the ""on""
state but higher than the ""off"" state, entry to and exit from the ""drowsy""
state are both more efﬁcient than the power-gated state.
Figure 4 shows our drowsy buffer, which is motivated by drowsy
cache [9]. The drowsy circuit includes a drowsy bit, a voltage
controller, and a word-line gating circuit. Depending on the state of
the drowsy bit, the voltage controller switches the operating voltage
between high (active) or low (drowsy) states. Additionally,
the
word-line gating circuit prevents access to the drowsy cells and avoids
data corruption. .
Figure 4: Circuit design for drowsy buffer. In the drowsy bit, the word line, bit lines, and
two pass transistors are not shown for simplicity.
Drowsy SRAM has shorter transition delay than power-gated
SRAM, despite consuming higher leakage power. As shown in Figure
2, there are a lot of short intervals that are less than the break-even time
(∼10 cycles) which cannot be leveraged by power-gating, but can be
used to enter the drowsy state whose wake-up latency is only 1 to 2
cycles [9].
3.2 STT-RAM Buffers
In addition to modifying SRAM buffer circuitry, we also propose to
adopt emerging non-volatile memory (NVM) technologies to replace
SRAM buffers. STT-RAM has the following advantages which make
it a good candidate to “dim"" NoC buffers in the dark silicon era.
3.2.1 Characteristics
STT-RAM relies on non-volatile, resistive information storage in a cell,
and thus exhibits near-zero leakage in the data array. Figure 5a shows
the structure of a 1T1J STT-RAM cell, which comprises of an access
transistor and a Magnetic Tunnel Junction (MTJ) for binary storage. An
MTJ contains two ferromagnetic layers (the reference layer and the free
layer) and one tunnel barrier layer (MgO). The directions of these two
layers determine the high/low resistance of the MTJ, which indicate the
“1""/“0"" state. The 1T1J cell size (about 6 - 50 F 2 ) is smaller compared
to typical 6T SRAM cells (about 120 - 200 F 2 ). As a result, for the
same capacity as SRAM, high-density STT-RAM cuts the buffer area
budget.
(a) 1T1J STT-RAM cell
(b) Power-gating STT-RAM
Figure 5:
(a) shows the 1T1J cell structure of the STT-RAM data array.
(b) applies
power-gating on STT-RAM buffers to further cut the leakage power of peripheral circuit.
The endurance of STT-RAM (1015 writes [11]) is signiﬁcantly
higher than other NVM (e.g.
109 writes for PCM). This makes
STT-RAM superior to other NVM in enduring frequent packet accesses
in the on-chip network.
3.2.2 Power-Gating STT-RAM
As the data array of STT-RAM has negligible leakage, the peripheral
circuit becomes the dominant leakage consumer in STT-RAM buffers.
Simulation results from NVsim [7] show that nearly half of STT-RAM
die area is occupied by peripheral circuitry, i.e., half of the chip is leaky.
Therefore, we propose to shut off the peripheral leakage power through
power-gating. Figure 5b shows the gating circuit, which uses a sleep
transistor inserted between the Vd d and the STT-RAM buffers to control
the on/off states.
3.2.3 Relaxing Non-volatility
STT-RAM achieves comparable read latency and energy as SRAM.
However, the write access time and the write energy of STT-RAM are
relatively higher than an SRAM write operation. With the advance of
technology, recent designs have demonstrated shorter write latency of
2 - 4 ns [5, 15, 20], which corresponds to 2 - 4 cycles for 1 GHz clock
frequency.
Even with more conservative STT-RAM design, the write overhead
can be mitigated by relaxing its non-volatility [13].
STT-RAM
generally has more than 10 years of retention time. This long retention
time is unnecessary for on-chip buffers because they only serve as
temporary storage for packets. Even under heavy network load, the
worst-case queuing time of packets is at the magnitude of microseconds
(µ s). Therefore, by relaxing the non-volatility of STT-RAM from years
to a few ms or even µ s, faster write speed and smaller write energy can
be obtained. It has been demonstrated by Jog et al. [13] that a 10ms
retention time of MTJ can be achieved at a switching time of 2ns with
61µ A write current.
Although the signiﬁcant reduction of STT-RAM write latency
enables integration of STT-RAM as NoC buffers, we do not
aggressively assume the write latency gap between STT-RAM and
SRAM can be completely eliminated.
Instead, as illustrated later,
we propose hybrid buffer designs to further reduce STT-RAM write
overhead and even seamlessly hide the long write latency through
buffer banking. A recent work [12] leverages STT-RAM in NoC
buffers, but only for performance purposes by designing larger buffers
with dense STT-RAM cells. Their design suffers from power overhead
in moderate or high network load.
3.3 Hybrid Buffer Architectures
In this subsection, we propose two novel hybrid buffer architectures to
take the advantages of both drowsy SRAM and non-volatile STT-RAM
for power-efﬁcient DimNoC design in the dark silicon era.
3.3.1 Hierarchical Buffer
To allow ﬁne-grained activation/power-gating of different VCs, we
divide the VCs into a hierarchy of levels and activate different levels
successively based on the network load. We employ drowsy SRAM
in lower-level VCs, which switch between power-on and drowsy state.
Being aware of the frequent short inter-arrival periods of packets, we
do not shut down the drowsy buffers in order to reserve resources
for instant wake-up. Correspondingly, higher-level VCs are made of
STT-RAM and will be activated lastly to accommodate heavy trafﬁc,
thus avoiding frequent costly write accesses. Moreover, power-gating
techniques will be applied on STT-RAM VCs to take advantage of the
long idle periods under very light trafﬁc.
(a) The Hierarchical Buffer (HB) architecture
(b) State transition
Figure 6: The Hierarchical Buffer (HB) architecture with four VCs as an example. VC 0
& 1 are drowsy VCs, and the rest are STT-RAM VCs. In (a), VC 0 is active, VC 1 is in
drowsy state, whereas VC 2 and 3 are power-gated. Diagram (b) depicts the state transitions
for different VCs.
Figure 6a depicts a Hierarchical Buffer (HB) design, where VC 0 and
1 are drowsy VCs, while VC 2 and 3 are STT-RAM VCs. Note that we
use four VCs here for illustration purposes, less or more number of
VCs are also applicable. We separate drowsy VCs and STT-RAM VCs
into multiple levels, and allow ﬁne-grained activation of different levels
in a hierarchical manner. For example, in Figure 6a, VC 0 (level 1) is
active, VC 1 (level 2) is in drowsy state, whereas the rest of STT-RAM
VCs (level 3) are power-gated.
Figure 6b shows the state transition diagram of different VCs in
response to variation of network trafﬁc. X, Y, and Z are used to
represent the status of VC 0, 1, and {2,3}, respectively. “1"" means
active whereas “0"" stands for inactive. Note that inactive means
drowsy for VC 0 and 1, and power-gated for VC 2 and 3. Initially,
assuming the trafﬁc load is light, only VC 0 is activated and the rest
of VCs are inactive (100). When the incoming trafﬁc exceeds a certain
threshold (T h1 ) of buffer occupancy, the remaining drowsy VC 1 will
be promptly activated (110). In case of further increase of network load
or abrupt arrival of burst packets that exceeds another threshold (T h2 ),
the STT-RAM VCs will be switched on (111). Reversely, when the
network trafﬁc decreases (T h3 ), we do not power-gate STT-RAM VCs
immediately to avoid unnecessary wake-up due to network ﬂuctuation,
but instead put VC 1 into drowsy state (101). Finally, if the trafﬁc
further decreases (T h4 ), the STT-RAM VCs will be shut down (100).
Also, 110⇒100 and 101⇒111 happen under network ﬂuctuation.
3.3.2 Banked Buffer
Apart from the VC-based partitioning that separates the drowsy SRAM
VCs from STT-RAM VCs, we propose a more ﬁne-grained hybrid
buffer architecture, in which drowsy SRAM and STT-RAM buffers
are logically interleaved within each VC, but physically organized as
separate banks. This design allows for simultaneous access to multiple
banks so as to hide the longer write latency of STT-RAM.
Figure 7a shows the logical view of the Banked Buffer (BB), where
STT-RAM buffers and drowsy SRAM buffers are organized in an
interleaved fashion within each VC. Figure 7b shows the physical
architecture inside a single VC. Speciﬁcally, each VC is separated into
(a) Logical view of the Banked Buffer (BB)
(b) Physical design of a banked VC
Figure 7: A Banked Buffer (BB) architecture where each VC consists of both STT-RAM
and drowsy SRAM in an interleaved fashion. Within each VC, the STT-RAM buffers and
drowsy SRAM buffers are separated into two banks.
two banks. One is the STT-RAM bank, and the other is the drowsy
SRAM bank. Then, the incoming ﬂits will go through a bank-selection
multiplexer and write into the appropriate bank. For example, assuming
a two-cycle write latency for STT-RAM, during cycle 0, ﬂit 1 will be
written into the STT-RAM bank. Subsequently at cycle 1, ﬂit 2 will be
directed into the drowsy SRAM bank and complete the write operation.
Meanwhile, ﬂit 1 will also complete its two-cycle write operation in the
STT-RAM bank at this cycle, and thus both ﬂits are readable at the next
clock cycle. Similarly, the following ﬂit 3 and ﬂit 4 will be transferred
into the STT-RAM bank and drowsy SRAM bank, respectively. In this
way, the long write latency of STT-RAM can be hidden. In general, for
a n-cycle STT-RAM write latency, we can divide the STT-RAM buffers
into n-1 banks to hide the long write latency.
4 Experiments
We use a Pin [21] based functional simulator to collect instruction
traces from applications. The traces are then fed into a cycle-level
trace-driven multicore simulator integrated with Garnet [1] and DSENT
[24] to evaluate the network performance and power with 32nm
technology. Detailed system conﬁgurations are listed in Table 1.
Table 1: System and Interconnect conﬁguration
core count
L1 I & D cache
L2 cache
cacheline size
frequency
16
private, 32KB
shared, 512KB/bank
64B
1GHz
topology
router pipeline
VC count
buffer depth
packet length
4 × 4 2D Mesh
four-stage
4 VCs per port
4 buffers per VC
5 ﬂits, 16B/ﬂit
We use NVsim [7], combined with statistics collected from
recent STT-RAM prototypes [14] to estimate the latency and energy
consumption of each SRAM and STT-RAM access. Furthermore, we
obtain the scaled latency and energy of accesses to drowsy SRAM [9]
and STT-RAM with relaxed non-volatility [13]. Table 2 shows the
parameters of various designs.
We evaluate our hybrid buffer designs with the San Diego Vision
Benchmark Suite [26]. Table 3 summarizes and compares various
buffer design techniques which are evaluated in our experiments. We
compare our design with a prior work that employs look-ahead routing
based scheme [16], and a recent NoC power-gating design called
Node-Router Decoupling (NoRD) [4].
4.1 Energy and Performance Evaluation
Energy Savings Because our primary goal is to conserve network
energy, we run different benchmarks with all buffer design techniques
listed in Table 3.
Figure 8 shows the results of total network
energy. Note that we assume the wake-up latency of conventional
power-gating,
look-ahead routing based power-gating, and drowsy
buffers are ten cycles [4], ﬁve cycles [16], and two cycles [9],
respectively. In addition, the write latency of STT-RAM is two cycles.
We also perform sensitivity study of the impact of longer STT-RAM
write-latencies and present the results in the later part of this section.
We make four important observations from the evaluation results:
• Compared with the baseline Al l _SRAM design, Al l _SRAM_PG,
which employs conventional power-gating techniques to turn off an
entire router whenever it is idle, indeed incurs 5.82% energy overhead
due to frequent router wake-up, averaged over all the benchmarks.
Table 2: 1KB SRAM and STT-RAM buffer conﬁgurations
SRAM
DrowsySRAM
STT-RAM (10yr)
STT-RAM (10ms)
cell size (F 2 )
146
146
50.67
45.60
write latency (cycle)
1
1
10
2
read latency (cycle)
1
1
1
1
write energy/bit (pJ)
0.049
0.049
0.534
0.286
read energy/bit (pJ)
0.063
0.063
0.153
0.082
leakage power (mW)
1.797
0.165
0.042
0.044
Table 3: Comparisons of different buffer design techniques
All_SRAM
All_SRAM_PG
All_SRAM_PG_LA
NoRD
All_DrowsySRAM
All_STTRAM
HB
BB
Baseline, which employs pure conventional SRAM based buffers
Buffers are designed with pure SRAM, and conventional power-gating technique is applied
Buffers are designed with pure SRAM, and look-ahead routing based power-gating technique is applied [16]
Buffers are designed with pure SRAM, and bypass paths are introduced to deliver packets without waking up gated routers [4]
Buffers are designed with pure drowsy SRAM
Buffers are designed with pure STT-RAM
Hierarchical Buffer: Lower-level VCs are pure SRAM based, and higher-level VCs are pure STT-RAM based
Banked Buffer: Drowsy SRAM and STT-RAMs are constructed into separate banks, and are accessed in an interleaved fashion in each VC
Figure 8: Energy comparisons for different buffer designs and power management schemes
Figure 9: Packet latency comparisons for different buffer designs and power management schemes
• Al l _SRAM_PG_LA [16] employs look-ahead routing to hide some
wake-up latency and achieves 11.9% energy saving over the baseline.
N oRD [4] provides bypass to transmit packets through gated routers
and thus increases the power-gating duration. It reduces the network
energy by 16.4%.
• Al l _DrowsySRAM put routers into low-power drowsy state instead
of completely shutting them down to further accelerate the wake-up
process. On average, it cuts down the energy by 18.2%.
• Al l _ST T RAM replaces SRAM by STT-RAM, and still achieves
energy savings compared with the baseline, indicating the beneﬁt of
low leakage outweighs the overhead of high write energy of STT-RAM.
However, the energy saving (17.1%) is not very signiﬁcant.
• To avoid unnecessary write operations to STT-RAM, H B uses
hybrid buffers and only activates the STT-RAM VCs at high load. As a
result, it achieves 30.9% energy savings on average. Alternatively, BB
accesses the drowsy SRAM bank and STT-RAM bank in an interleaved
fashion within each VC, which successfully hides the long write latency
of STT-RAM and achieves 26.3% energy saving on average.
Performance Overhead The proposed buffer designs achieve energy
savings by deactivating some of the network resources, which will
unavoidably sacriﬁce the network performance to some degree.
Figure 9 shows the average packet latencies when running different
benchmarks, including queuing latency in the network interface and
network latency from source nodes to destination nodes.
As expected, Al l _SRAM_PG leads to signiﬁcant network delay,
adding 44.9% on average across all the benchmarks. By mitigating the
wake-up delay, Al l _SRAM_PG_LA and Al l _DrowsySRAM amortize
the latency overhead to 22.3% and 8.8% on average, respectively.
N oRD reduces the number of wake-up operations through bypass but
still suffers from 18.6% performance overhead due to packet detours.
For Al l _ST T RAM , the average network latency overhead is only
9.4% without applying power-gating. H B and BB slightly increase
the overhead to 10.4% and 10.8%, respectively. This is because they
further apply ﬁne-grained power-gating on individual VCs.
4.2 Sensitivity Study on STT-RAM writes
The aforementioned STT-RAM buffer designs assume a two-cycle
write latency by sacriﬁcing the retention time of STT-RAM cells.
Here, we conduct sensitivity studies with different write latencies
of STT-RAM. Intuitively, when increasing the retention time of
STT-RAM, the write latency and the write energy of STT-RAM cells
increase. As a result, the performance overhead of Al l _ST T RAM and
H B will increase, and the corresponding energy savings will decrease.
For BB, it hides the long write latency of STT-RAM through banking.
However, the increase of hardware overhead and write energy overhead
of STT-RAM will still decrease its energy saving.
Therefore, we use energy-delay product (EDP) as a metric to
evaluate different buffer designs, where E and D refer to the network
energy and latency, respectively. Figure 10 shows the EDP results
when varying the write latency of STT-RAM, using disparity as an
example. As illustrated in Section 3.2.3, the write latency of STT-RAM
is between 2 to 4 cycles through relaxing of non-volatility.
5 Conclusion
In this work, we propose DimNoC to tackle the dark silicon
problem from the NoC’s perspective, which integrates drowsy SRAM
and STT-RAM to design buffers in a router. Two hybrid buffer
architectures, Hierarchical Buffer (HB) and Banked Buffer (BB), are
proposed with efﬁcient power-management strategies. Experimental
results on the San Diego Vision Benchmark Suite show that DimN oC
can achieve 30.9% energy savings on average, 20.3% network
energy-delay product (EDP) reduction, and 7.6% router area reduction.
6 "
Domain-wall memory buffer for low-energy NoCs.,"Networks-on-chip (NoCs) have become a leading energy consumer in modern multi-core processors, with a considerable portion of this energy originating from the large number of virtual channel (FIFO) buffers. While emerging memories have been considered for many architectural components such as caches, the asymmetric access properties and relatively small size of network-FIFOs compared to the required peripheral circuitry has led to few such replacements proposed for NoCs. In this paper, we propose control schemes that leverage the “shift-register” nature of spintronic domain-wall memory (DWM) to replace conventional memory buffers for the NoC. Our results indicate that the best shift-based scheme utilizes a dual-nanowire approach to ensure that reads and writes can be more effectively aligned with access ports for simultaneous access in the same cycle. Our approach provides a 2.93X speedup over a DWM buffer using a traditional FIFO memory control scheme with a 1.16X savings in energy. Compared to a SRAM-FIFO it exhibits an 8% message latency degradation versus a 56% energy reduction. The resulting approach achieves a 53% reduction in energy delay product compared to SRAM and a 42% reduction in energy delay product versus STT-MRAM.","Domain-Wall Memory Buffer for Low-Energy NoCs
Haifeng Xu
Electrical and Computer Eng.
University of Pittsburgh
hax6@pitt.edu
Alex K. Jones
Electrical and Computer Eng.
University of Pittsburgh
akjones@pitt.edu
Donald Kline, Jr.
Computer Engineering
University of Pittsburgh
dek61@pitt.edu
Rami Melhem
Computer Science
University of Pittsburgh
melhem@cs.pitt.edu
ABSTRACT
Networks-on-chip (NoCs) have become a leading energy consumer in modern multi-core processors, with a considerable
portion of this energy originating from the large number
of virtual channel (FIFO) buﬀers. While emerging memories have been considered for many architectural components
such as caches, the asymmetric access properties and relatively small size of network-FIFOs compared to the required
peripheral circuitry has led to few such replacements proposed for NoCs. In this paper, we propose control schemes
Figure 2: FIFO queue structure with DWM. (a) traditional circular buﬀer. (b) shift-register approach.
In addition to improvements which focus on emerging
memories, a substantial amount of research has been performed to reduce the number and optimize the usage of network buﬀers. For example, a network which actively adjusts
the number of available buﬀers through ﬂow control to save
energy has been designed [13]. Further, a completely buﬀerless high-performing NoC design has been realized [14] that
performs well for light traﬃc, but has challenges maintaining
high performance at medium and high network loads.
The leading scheme to reduce energy in NoC buﬀers while
maintaining the original buﬀer capacity is to replace a large
percentage of the FIFO’s SRAM with STT-MRAM [15].
This approach writes into SRAM and then lazily migrates
it to a reduced retention (i.e., a faster lower write eﬀort)
STT-MRAM [16, 17] when possible. An energy savings of
16% is demonstrated. In contrast, we demonstrate several
DWM designs for NoC FIFOs that replace the entire SRAM
buﬀer with spintronic memory with little (e.g., a single-ﬂit)
or no SRAM buﬀering required. We describe our DWMbased variable length queue designs in the next section.
3. DWM-MEMORY FIFO DESIGN
Traditional FIFO architectures utilize head and tail counters to implement a circular buﬀer, where the head and tail
pointers (track the next write and read locations, respectively) can wrap around the array. While this conﬁguration
naturally lends itself to array-based memory technologies
(e.g., SRAM), it does not naturally extend to DWM. DWM
has a non-uniform access time due to the shifting required
for alignment with an access port. In Figure 2, we present
two DWM-based queues where the queue is implemented as
a group of N simultaneously shifted Racetracks and N is
Figure 1: The DWM design.
ble eﬃciency.
In this paper we address that challenge by
making the following contributions:
• We create a DWM-based FIFO implementation based
on the head and tail pointer concept (circular buﬀer)
used by SRAMs for NoCs.
• We create a “shift-register” style FIFO, including a
dual-nanowire approach, that leverages the properties
of the DWM shifting operations for eﬃcient NoC buﬀer
implementation
• We provide evaluations of our DWM designs with benchmark traﬃc in a mesh for a 64-core CMP.
Our best DWM approach provides a 2.93X speedup over a
DWM circular buﬀer implementation with a 1.16X savings
in energy. Compared to a SRAM-FIFO it provides a 56%
energy reduction with an 8% latency degradation.
In our
full system performance experiments, this results in a 53%
reduction in energy delay product compared to SRAM and
a 42% reduction in energy delay product as compared to the
leading STT-MRAM FIFO scheme.
2. BACKGROUND AND RELATED WORK
DWM comprises an array of magnetic nanowires, where
each nanowire consists of many magnetic domains separated
by domain walls (DWs). Each domain has its own magnetization direction used in a similar manner to STT-MRAM.
For a horizontally-arranged planar strip (Figure 1), several
domains share one access point for read and write operations [4]. The DW motion is controlled by applying a short
current pulse on the head or tail of the nanowire in order
to align diﬀerent domains with the access point. Since the
storage elements and access devices in a DWM do not have
a one-to-one correspondence, a random access requires two
steps to complete: Step 1–shift the target magnetic domain
and align it to an access transistor; Step 2–apply an appropriate voltage/current to read or write the target bit. Intrinsically, the read operation from step 2 is the same as STTMRAM, however the write can be a shift in the orthogonal
dimension [3]. Thus, the tradeoﬀ for DWM is reduced leakage power over STT-MRAM (fewer access transistors per
bit) with increased dynamic power due to shifting domains.
Various forms of storage applications based on DWM have
been demonstrated, such as array integration [5], lower level
cache [6, 7], content addressable memory (CAM) design and
fabrication [8, 9], reconﬁgurable computing memory [10],
and a GPU register ﬁle [11].
Due to the growing percentage of power consumption in
many-core architectures contributed by the NoC, it has been
a signiﬁcant concern of many research groups to ﬁnd ways to
reduce power consumption and use high-density memories.
To the best of our knowledge, there is no prior work using
DWM to implement variable length FIFO queues suitable
for utilization in a NoC. However, a ﬁxed-length shift register, realized by perpendicular magnetic anisotropy (PMA)
technology, has been demonstrated [4, 12].
Figure 3: Linear buﬀer FSM. R=read, (R)=read
align RT, W=write, R+W=read and write,
Idle=neither read nor write.
[<<RC] and [>>RC]
also shift the read port left and right, respectively,
depicted in the RC Shift Register where the highlighted (‘1’) port is currently selected.
To address these ineﬃciencies we consider three approaches,
a linear buﬀer (LB) concept [Figure 2 (b)] that shifts data
through the Racetrack like a shift-chain, an increase in the
number of access points, and the introduction of temporary
SRAM storage to buﬀer reads or writes to move them oﬀ
the critical path. It has been demonstrated that a multiple
read port Racetrack does not detract signiﬁcantly from the
density achievable by the nanowire because of the small relative size of read ports [6, 18]. Thus, it is reasonable to add
additional read access points to increase performance at the
cost of some additional static power. These additional ports
are displayed in Figure 2 with dashed lines.
While the CB scheme uses traditional head/tail pointers,
the LB scheme requires a ﬁnite state machine (FSM) for
control as shown in Figure 3. To simplify the FSM, we
present the case where read access ports are separated by
one domain and assume that data must stay contiguous in
the Racetrack (i.e., there are no gaps between ﬂits). We
also assume that writes and shifts require half a cycle, and
reads require one cycle (see Section 5). Thus, there are four
states possible for the buﬀer (see Figure 4): RW-Aligned,
where the queue head is aligned with a read access point and
the tail pointer is aligned with the write access point, RAligned where the queue head is aligned with a read access
point but the tail pointer is not, W-Aligned where the tail
pointer is aligned with the write access port but the head is
not aligned with a read access point, and Unaligned where
neither the head nor tail is aligned with an access point. This
FSM can easily be expanded for a larger gap (more domains)
For LB only N domains are required as compared to the
2N-1 domains per Racetrack for CB. The overhead for this
scheme includes two bits of storage to represent four states,
and the same number of bits as the number of read heads
(1 hot, in a shift register) for the currently used read head
plus the same overhead for an additional SRAM buﬀer as
CB. Similarly, in the Dual racetrack scheme, N domains are
required per buﬀer (N/2 for each Racetrack). The overhead
for the Dual scheme includes two bits per racetrack to represent the four states, one bit per buﬀer to represent the valid
read head (which replaces the RC shift register, in the case
of the length of each racetrack in the case of four domains),
and two bits per buﬀer to indicate which racetrack is controlling the buﬀer reads and writes (i.e., the read and write
owner of the buﬀer, respectively, in Figure 5). The Dual
scheme does add additional peripheral circuitry to write to
and shift two half length racetracks, which is accounted for
in the energy calculations presented in the next section.
5. EXPERIMENTAL METHODOLOGY
To evaluate the Racetrack FIFO schemes, we implemented
CB, LB, and Dual schemes to serve as virtual channel buﬀers
in a NoC using the cycle-accurate HORNET multicore simulator [21] to compute both average ﬂit latency and energy
consumption.
In addition, peripheral circuitry power calculations for SRAM, STT-MRAM and diﬀerent Racetrack
FIFO schemes were analyzed using data from [3, 19] and a
modiﬁed version of NVSim [20]. Sniper [22] was used to generate workload traces of the PARSEC benchmark suite [23]
for the modiﬁed HORNET simulator. To estimate the full
system performance impact of the NoC, the HORNET generated latencies were then used in a second full system simulation in Sniper to determine performance impact via IPC.
The tests were performed on a simulated 64-core network
using one-queue-per-ﬂow o1-turn routing. In addition, each
ingress port connecting a core to its neighbor has eight virtual channels, each of which can store eight ﬂits. CB, LB,
and Dual were tested both with and without a single ﬂit
SRAM storage to buﬀer the queue’s head ﬂit. The Sniper
Figure 6: Flit latency of Racetrack buﬀer schemes normalized to SRAM.
Figure 7: Full system performance (IPC) for diﬀerent Racetrack buﬀer schemes normalized to SRAM.
within 2% of the all SRAM case, which is equivalent to the
overhead of reducing the number of SRAM VCs in half.
The impact of these latencies on full system performance
is shown in Figure 7 as IPC normalized to all SRAM buﬀers.
CB resulted in a dramatic 22% IPC degradation, while LB
also had a signiﬁcant 7.2% IPC reduction compared to SRAM.
Adding the SRAM head ﬂit storage, The LB+S and Dual
scheme were nearly indistinguishable, requiring a nominal
1.7% overhead over SRAM and were within 0.5% of the
SRAMHalf scheme. Dual+S was nearly indistinguishable
from SRAM.
The energy consumption of the buﬀers computed from the
information in Table 1 is shown in Figure 8 and normalized
to SRAM. As expected, SRAMHalf, STT, and STTHalf progressively reduce energy. Of the Racetrack schemes CB does
moderately worse than the other racetrack schemes despite
its reduced static power, while LB and Dual perform the
best. CB+S, LB+S, and Dual+S consume much higher energy due to the added static power from the SRAM, but
also because most ﬂits are both written to and read from
the Racetrack and SRAM buﬀer. The energy delay product,
reported in Figure 9, shows that CB is a poor choice, causing a more than 1.5X increase over SRAM in energy delay
product. LB provides a 25% reduction over SRAM which is
similar to STTHalf. However, Dual has the best result with
a more than 50% savings over SRAM. The added energy
and performance degradations of LB+S and Dual+S make
them less valuable for the energy/performance tradeoﬀ.
7. CONCLUSION
While the inherent composition of racetrack memory can
result in a signiﬁcant energy reduction from traditional SRAM,
a direct replacement of racetrack memory with well established SRAM FIFO techniques results in signiﬁcantly reduced performance (over 300% increase in message latency
for the CB design without an SRAM buﬀer, and 12.8% on
average with an additional SRAM buﬀer). That is why,
in order for racetrack memory to practically replace SRAM
in virtual channel buﬀers without a drastic drop in performance, more advanced schemes are necessary. One such
strategy that demonstrated its viability through this experiment was Dual, which on average across the Parsec benchmarks had a 41% improvement over the CB scheme in performance, as well as a 56% improvement over the SRAM
scheme in power. Further, Dual with an additional SRAM
buﬀer also had a very promising latency result, with only a
1.7% increase in message latency compared to SRAM.
The results of the energy delay product calculations revealed that the dual virtual queue scheme yields a 53% reduction over SRAM, and the Linear Buﬀer scheme provides
a 25.4% reduction over SRAM. Also, CB with an additional
SRAM buﬀer is slightly higher than the SRAMHalf conﬁguration in terms of energy delay product, and LB with an
SRAM buﬀer is also very similar to the SRAMHalf scheme
in energy delay product. When compared to 0.5ns read and
sub 1ns write STT-MRAM technologies, CB, CB with an
SRAM buﬀer, and LB with an SRAM buﬀer yield an increased energy delay product. In contrast, LB yields an energy delay product in between that of STT and STT-Half,
the Dual scheme with a buﬀer provides a reduction over the
STT-MRAM implementation by 11.7%, and Dual produces
a 41.5% reduction from the baseline STT-MRAM.
Due to the reduced number of read-write heads in DWM
as compared to SRAM or STT-MRAM, 2X the number of
DWM queues can occupy the same space as X STT-MRAM
queues. When comparing the Dual results to the conﬁgurations with half the number of queues for STT-MRAM or
SRAM, Dual with an SRAM buﬀer results in an increase
in IPC by 0.632% on average, a decrease in latency by
0.846% on average, and reductions in the energy-delay product by 17.6% and 1.5% when compared to SRAMHalf and
STT-MRAMHalf, respectively. When the buﬀer is removed
from the dual scheme, the energy-delay product improves
by 34.3%. However, this results in a message latency increase of 6.28%, which translates to an IPC reduction of
1.05%. Depending on the demand for performance, at a 1
GHz clock frequency, the Dual scheme with a buﬀer outperforms and reduces energy compared to the equivalent area
STT-MRAM and SRAM schemes. If the focus is more energy savings, the buﬀer can be removed to achieve a 46% and
Figure 8: NoC buﬀer energy results normalized to SRAM.
Figure 9: NoC buﬀer energy delay product normalized to SRAM.
36% energy-delay product reduction compared to the areaequivalent SRAM and STT-MRAM queues, respectively.
"
SuperNet - multimode interconnect architecture for manycore chips.,"Designers of the on-chip interconnect for manycore chips are faced with the dilemma of meeting performance, power and reliability requirements for different operational scenarios. In this paper, we propose a multimode on-chip interconnect called SuperNet. This interconnect can be configured to run in three different modes: energy efficient mode; performance mode; and, reliability mode. Our proposed interconnect is based on two parallel multi-vt optimized packet switched network-on-chip (NoC) meshes. We describe the circuit design techniques and architectural modifications required to realize such a multimode interconnect. Our evaluation with diverse set of applications show that the energy efficient mode can save on average 40% NoC power, whereas the performance mode can improve the core IPC by up to 13% on selected high MPKI applications. The reliability mode provides protection against soft errors in the router's data path through byte oriented SECDED codes that can correct up to 8 bit errors and detect up to 16 bit errors in a 64 bit flit, whereas the router's control path is protected through DMR lock step execution.","SuperNet: Multimode Interconnect Architecture for
Manycore Chips
Haseeb Bokhari†
Haris Javaid§ Muhammad Shaﬁque‡
Sri Parameswaran†
J ¨org Henkel‡
†School of Computer Science and Engineering, University of New South Wales, Sydney, Australia
§Google Inc., USA
‡Chair for Embedded Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany
{hbokhari, sridevan}@cse.unsw.edu.au, harisjavaid@google.com, {muhammad.shaﬁque,
henkel}@kit.edu
ABSTRACT
Designers of the on-chip interconnect for manycore chips are faced
with the dilemma of meeting performance, power and reliability
requirements for different operational scenarios. In this paper, we
propose a multimode on-chip interconnect called SU PERN E T. This
interconnect can be conﬁgured to run in three different modes: energy efﬁcient mode; performance mode; and, reliability mode. Our
proposed interconnect is based on two parallel multi-vt optimized
packet switched network-on-chip (NoC) meshes. We describe the
circuit design techniques and architectural modiﬁcations required to
realize such a multimode interconnect. Our evaluation with diverse
set of applications show that the energy efﬁcient mode can save
on average 40% NoC power, whereas the performance mode can
improve the core IPC by up to 13% on selected high MPKI applications. The reliability mode provides protection against soft errors
in the router’s data path through byte oriented SECDED codes that
can correct up to 8 bit errors and detect up to 16 bit errors in a 64
bit ﬂit, whereas the router’s control path is protected through DMR
lock step execution.
Categories and Subject Descriptors
C.1.2 [Processor Architecture:] Multiprocessors - Interconnection architectures
Keywords
Network-on-Chip, fault tolerance, performance, power optimization, multimode
1.
INTRODUCTION
The cost of designing a SoC has increased dramatically with
shirking node sizes as the limits of the laws of physics are reached [1,
2, 3]. The cost trend of chip design is shown in Figure 1. A signiﬁcant amount of ﬁnancial and human resources are required to design
and verify chips designed in smaller nodes. Therefore, according to
the laws of economy, the only way to achieve proﬁtability and beat
the initial design cost is to increase the number of units sold. To
increase the sales volume, a chip has to target a wider set of applications. This means the SoC has to fulﬁll the design requirements
for different usage scenarios. For example, designing a multimedia
SoC for battery powered smart phone requires a special emphasis on
low energy operation. On the other hand, using the same SoC for
home entertainment system requires special attention to high performance. A similar SoC used for engine management or a braking
system in a car has to operate reliably throughout the car’s lifetime.
This poses a serious challenge to designers who have to account for
various operational scenarios in a single chip.
It is important to reconsider the on-chip interconnect in context of such complex multipurpose SoC designs. Packet switched
Network-on-Chips (NoCs) are envisioned as a scalable interconnect to support the growing communication demand of SoC components. Like other on-chip components, it is essential to study the
effects of downscaling trends on NoCs. Although NoCs provide a
scalable communication backbone for 10s of on-chip components,
they also consume a considerable share of chip power(e.g., up to
18% and 33% in Intel SCC [4] and RAW [5] architectures, respectively). Research studies show that leakage power is the major share
of total NoC power and this share is expected to increase for smaller
nodes [6]. On the other hand, deep transistor integration adversely
affects the reliability of the digital circuits. A soft error in NoC
can have adverse effects such as partial disconnection of SoC component, erroneous execution of application or system level deadlock [7]. For maintaining high performance, NoCs are designed
either for worst case or average case scenarios. However, a design
approach for worst case delay can lead to wasted energy for less
demanding applications [8].
The above mentioned design problem becomes more complex
when NoCs have to be designed for multi-purpose SoCs. Traditionally NoCs are designed with a speciﬁc goal of either low power operation, high performance [8] or high reliability [9]. For example, a
NoC for high performance chip can be designed with a wider channel width and higher clock frequency to fulﬁll the requirement of
high bandwidth and low latency[8]. Such a high performance NoC
will be impractical for low power battery powered devices. Similarly, low power NoC can be designed with narrow channels and
lowest possible voltage guard-bands. However this NoC will neither be good enough for application with high bandwidth required
to support high performance applications, nor operate reliably under harsh operational environment. Therefore, to our best of knowledge, there is no single design proposal that covers all three possible
operational requirements discussed earlier.
A very naive solution to the above stated design problem is to
integrate a NoC for each possible design goal and use those NoCs
depending on the usage scenario. Such designs are not a suitable
choice for two reasons. The ﬁrst reason is that having multiple mutually exclusive NoCs will carry a signiﬁcantly higher silicon cost.
Despite the fact that a large amount of dark silicon is available in
future chips due to power budget limitations, NoCs still need to
compete with other on chip components such as application accelerators etc. for this dark silicon [10] area. The second reason is
that every on-chip component has to be extensively veriﬁed before
fabrication [1] and therefore naively adding more NoCs can potenPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
DAC’15, June 07 - 11, 2015, San Francisco, CA, USA
Copyright c(cid:13) 2015 978-1-4503-3520-1/15/06 ...$15.00.
http://dx.doi.org/10.1145/2744769.2744912
Figure 1: Hardware Design Cost Trend (Source: Semico Research)
tially increase the chip design cost.
In this paper, we show that
instead of such inefﬁcient NoC architecture, we can design a system with minimal number of NoCs that can be used to achieve the
same goal through the adroit reuse of same set of NoCs for different
operational modes.
Our solution to the above stated problem is the SU PERN ET NoC
architecture. The architecture consists of parallel architecturally
homogenous, but voltage-frequency(VF) optimized heterogeneous
NoCs. For energy efﬁcient mode, we leverage the two NoCs for dynamic voltage and frequency scaling (DVFS) by switching on only
one NoC depending on application requirements or static conﬁguration. In performance mode, we classify the network packets under
the category of critical and non-critical packets, and steer the packets accordingly into either low VF or high VF NoC. In the reliability
mode, the two NoCs work in dual lock step mode where one NoC
carries the actual data while the other NoC carries the error correction codes. This scheme improves the reliability by mitigating the
effects of soft errors in control and data path.
We make the following contributions in this paper:
• We introduce a novel multimode NoC SU PERN ET for future
multipurpose SoCs. SU PERN E T uses simple control interface
for switching between three different modes, i.e, low power
• We describe the circuit design techniques and architectural
mode, high performance mode and reliability mode.
innovation used to realize the SU PERN E T architecture. The
process of switching between modes is completely autonomous
• Using real application and cycle accurate simulation setup,
with minimal software intervention.
we explore the efﬁcacy of different operational modes of SU PERN E T.
The rest of the paper is organized as follows. In Section 2, we
provide a brief background of previous NoC proposals and compare them with the SU PERN E T architecture. Section 3 provides
details about the SU PERN E T architecture and the three operational
modes. In Section 4 we present our results from experiments with
SU PERN ET architecture. We conclude our paper in Section 5.
2. BACKGROUND AND RELATED WORK
Muliple NoCs: There are number of commercial and academic
chips that leverage multiple NoCs. Tilera’s iMesh architecture [11]
used ﬁve parallel NoCs for different class of trafﬁc, i.e, cache coherence messages, memory controller access, I/O port access, and
user space message passing. TRIPS [12] architecture use multiple
networks for operand and data forwarding. Therefore the reason for
using multiple NoCs in TRIPS and Tilera iMesh architecture is to
provide isolation between different classes of trafﬁc. Balfour and
Dally [13] showed that adding parallel NoCs can potentially reduce
the network latency and improves the bandwidth. Mishra et al. [8]
extended the idea by using two heterogeneous network, where one
network is used for bandwidth sensitive applications and the second network is used for latency sensitive applications. In contrast,
in performance mode for SU PERN ET, we divide the trafﬁc from all
the applications based on packet type instead of the application to
which the trafﬁc belongs. In addition to the performance improvement, SU PERN E T can also be used to provide reliability or energy
efﬁciency by using the same set of NoCs.
System Level Power Saving Techniques: Over the years various system level power management techniques have been introduced. DVFS for on-chip networks has been proposed at various
granularities — such as link level [14], router level [15, 16] and
region level
[17, 18]. However the advantage of DVFS scheme
has been reducing due to NoC’s high leakage power in smaller
nodes and diminishing room for reducing supply voltages [19]. To
improve the efﬁciency of DVFS schemes, Bokhari et al. [6] proposed integrating multiple NoCs, where each NoC is optimized using multi-vt circuit technique. At runtime, along with changing VF
levels, a different NoC is activated. We use a similar technique
for our energy-efﬁcient modes. However we also use these multiple NoCs for improving performance and reliability of the system.
Das et al. [20] proposed integrating four architecturally homogenous NoCs for the purpose of energy efﬁciency. Depending on network trafﬁc, a subset of these NoC are activated at runtime while
unused NoC are power gated. Both Das et al. [20] and Bokhari et
al. [6] did not address the issue of reliability and performance in
their architecture.
Fault Tolerance in NoC: Many researchers have studied the
topic of soft error detection and correction for NoCs1 . The soft
errors are caused by random logic ﬂip on input/output of a logic
gate. This leads to temporary glitches in NoC router’s data path
and/or control path [9]. BulletProof router [21] use Cyclic Redundancy Check (CRC) for detection and correction of soft errors in
data path and selective triple modular redundancy (TMR) for detecting faults in control paths of the routers. Prodromou et al. [7]
proposed a scheme based on hardware invariance checking to detect
faults in router control path. Park et al. [22] explored the use of hopby-hop error detection and correction by using Single Error Correction - Double Error Detection (SECDED) codes. In case of errors
which were not correctable, ﬂits are retransmitted from downstream
routers. Commercial NoC vendor, Arteris, has also introduced an
error resilience solution called FlexNoC Resilience Package [23]
that combines end-to-end error data correction and port checking.
Murali et al. [24] analyzed different error detection and correction
schemes and concluded that end-to-end fault detection/correction
and retransmission based on time-out is a good tradeoff between
performance and energy and/or area overhead. None of the techniques discussed above studied the soft error resilience in context
of reusing the existing multiple NoCs. In contrast to previous solutions, we use the additional NoC to transmit strong byte based
SECDED code for end-to-end error detection and correction, and
use the control path of the two NoCs for lock-step dual modulo redundancy (DMR) protection in the control path.
3. SuperNet ARCHITECTURE
3.1 Architecture
Our target architecture is a general purpose tiled multi-core chip
as shown in Figure 2, which closely matches commercial SoC chips
such as Intel Single Chip Cloud Computer(SCC) [4] architecture.
The processing core is a simple in-order core with private L1 Data
and Instruction caches. Cache load and store requests are injected
into the NoC by cores. Memory controllers are placed at the border
routers as shown in the Figure 2 to serve cache load and store requests. We target a N ×N Mesh topology based NoC architecture
in this paper as it is commonly used by commercial and academic
multi-core chips. Each core runs a different application therefore at
a given time, up to N 2 applications are executed in parallel.
3.1.1 Multiple VF Optimized NoCs
SU PERN ET architecture consists of two parallel NoCs that are
optimized for different VF levels using multi-vt optimization [6].
Commercial fabrication foundries such as TSMC and Global Foundaries
provide cell libraries with various gate threshold voltage (Vt). These
library packages contain cells with normal Vt (NVt), Low Vt (LVt),
and High Vt (HVt). Availability of heterogeneous Vt provides an
opportunity for designers to optimize the energy-delay characteristics of circuits below a given synthesis latency constraint. CAD
tools exploit the energy-delay characteristics of the cells by inserting low Vt cells (high leakage, faster switching) on circuit logic
paths with negative slack to meet the latency constraint and replacing normal cells with high Vt cells (low leakage, slower switching)
on paths with positive slack to save energy.
We assume that there are two voltage supplies, V 1 and V 2 available, where V 1 > V 2. These voltages are associated with two
frequencies f and f /2. At each node we integrate two parallel
routers as shown in Figure 2. Although both routers are architecturally homogeneous, they are optimized for a VF level using
multi-vt optimization. One of the routers is designed for VF level
[V 1,f ] whereas the other router is designed for [V 2,f /2]. Using
these routers, two parallel NoCs are realized as follows:
• NoC A: This NoC consists of routers designed for [V 1,f ].
NoC A can either be operated at [V 1,f ] or [V 2,f /2] depend• NoC B: This NoC consists of routers designed for [V 2,f /2].
ing on the operating mode.
NoC B can only be operated at [V 2,f /2].
3.1.2 Description of NoC Components
Fabric Manager: Fabric Manager(FM ) is responsible for managing all the NoC components in the MPSoC. The FM uses the
1We are limiting our discussion to mitigating soft errors. An
overview of fault tolerance schemes for NoCs in presented in [9]
Figure 2: Overview of MPSoC with SU PERNET Interconnect
Figure 3: Details of SU PERNET Components
NoC channels for communication with Local Managers and read
the response through 3 wire AND network as shown in Figure 3(a).
The FM can be implemented as a specialized hardware or it can be
a part of existing chip management ﬁrmware. FM autonomously
co-ordinate with Local Managers in the NoC to implement different operational modes. This eases the burden of writing software
routines to manage resources.
Local Manager: Local Managers(LMs) are responsible for managing the routers at mesh node and to forward information from local mesh node to FM. LM drives the power gating enable/disable
signals for each router. LM also enables and disables the trafﬁc
injection from local core’s port. LM monitors the buffer empty condition, application proﬁler status and error status from the lock-step
comparator (more on this in Sections 3.3, 3.4 & 3.5). This information is communicated back to the FM through a simple 3 wire AND
network as shown in Figure 3(a). LM s receives their commands
from FM through the NoC for setting up the operational mode.
Application Proﬁler Unit: Each core contains an Application
Proﬁler Unit. This proﬁler unit (marked PROF in Figure 2) keeps
track of vital information on application behavior. Each proﬁler unit
is augmented with two saturating counters, an instruction counter
and a L1 cache miss counter. The proﬁler unit resets the counters
after every preset control interval and saves the value from the previous interval. The counter information is used by FM to implement
either the Energy Efﬁcient Mode or the Performance Mode.
Error Detection and Correction Unit: Every Network Interface (NI) is augmented with an Error Detection and Correction Unit
(marked EDC in Figure 2) for use in the Reliability Mode. The EDC
unit is responsible for encoding the outgoing data from NI with forward error correction code (FEC) (Figure 3(b)), and detecting and
correcting any errors in the incoming data (Figure 3(c)). We use
SECDED codes for each 8-bit data which results in a 5-bit codeword. In our architecture each ﬂit contains 64-bits of data as shown
in Figure 3(d). Therefore for every 64-bit ﬂit, we generate a 40-bit
codeword. Similarly on the receiver side, a decoder checks and tries
to correct errors in the ﬂit data. Therefore, through EDC unit, we
can correct up to 8 single bit errors (as long as they occur in separate
data bytes) and detect up to 16 bit errors (maximum of two error in
each byte). For saving power, these units can be power gated by LM
in case the Reliability Mode is not activated.
Equivalence Checker: Equivalence Checker (blocks with ’=’
sign in Figure 2 and Figure 3(a)) are used for implementing the
Reliability Mode. The checker circuit asserts that the control bits
(Figure 3(d)) of the two NoC channels are equal. These checkers
are placed at two locations: (1) each of the ﬁve outgoing router
channels, (2) incoming channel of each Error Detection and Correction Unit. In case the control part of the channels are not equal,
the checker unit raise an error signal which is read by the LM.
3.2 Mode Selection
When the chip is switched on, FM by default initializes the SU P ERN ET in the normal mode (i.e, only NoC A is powered on with
[V 1,f ]). Depending on various factors such as power budget, reliability requirement etc., chip user can decide to activate a certain SU PERN ET mode. Therefore at runtime, the user can execute
the chip’s ﬁrmware or OS system call to write the mode change
command to FM ’s setting register. Once this happens, the FM autonomously implements the desired operational mode.
3.3 Energy Efﬁcient Mode
We base our Energy Efﬁcient Mode on the fact that not all applications need a low latency NoC. One important metric to measure the dependence of applications on NoC is cache misses per
kilo instructions (MPKI)[8]. Application with higher MPKI values
spend a major portion of time waiting for the cache load miss to
be served from memory controller through the NoC. The graph for
MPKIs for two applications h264enc and mpeg2enc are shown in
the Figure 4. In our experiments we found that the MPKI value
for h264enc is 40× the MPKI of mpeg2enc. Therefore, for an
MPSoC with h264enc application being executed, NoC has to be
clocked at a higher frequency. On the other hand, for low MPKI
applications such as mpeg2enc, NoC can be clocked at a lower frequency to save energy. Furthermore, the MPKI values can also ﬂuctuate depending on the application behavior. Therefore, depending
Figure 4: MPKI for h264enc and mpeg2enc
on application’s current MPKI, the NoC can be clocked at different
frequencies.
VF Selection Scheme: SU PERN E T implements a coarse grain
DVFS method with two VF levels. When the VF level [V 1,f ] is
used, NoC A is operational and NoC B is power gated. Whereas,
when the VF level [V 2,f /2] is used, NoC B is used and NoC A is
switched off. Bokhari et al. [6] showed that using NoC that is optimized for a speciﬁc VF level is more energy efﬁcient than using a
scaled VF on a NoC that has been designed for a higher VF level.
MPKIs are calculated for each application using instruction and
cache miss counters in proﬁling unit. If the MPKI of the application
falls below a pre-set threshold value, the proﬁling unit de-asserts the
HI-LOAD signal, which is sensed by LM and broadcasted to FM.
If all applications in the system de-assert the signal, FM decides to
lower the NoC frequency to f /2. The value of threshold can be
chosen to mark a trade-off between performance and power. On
the other hand, if any application in the system asserts HI-LOAD
signal (Figure 3(a)), the FM switches back to higher frequency (f ).
Another, possible mode that can be used by FM is to keep the NoC
running at low frequency irrespective of application characteristics
(i.e, static VF selection) in case of low power requirements, or if
the performance mode has been executed for too long and the chip
is too hot.
Mode Setup: To start the Energy Efﬁcient Mode, the FM sends
control ﬂits to each LM to stop any further transaction in the NoC
and informs the LM about initiation of the mode along with threshold value to be used. FM then waits for all existing transactions to
ﬁnish by monitoring the global BUFF-EMPTY signal (Figure 3(a)).
At the start of the mode, LM s switch on the router for NoC A.
VF Switching Procedure: Whenever the FM decides to switch
between VF levels, the currently active NoC has to be switched off
after ensuring that there are no in-ﬂight packet in the system. FM
starts the procedure by sending a S top N I command to all LM s
to stop any further injection of packets. The global BUFF-EMPTY
signal (Figure 3(a)) is raised when there are no more packets in the
NoC. Then FM sends the next batch of control ﬂits to LM to activate
the required NoC and power gate the existing NoC. FM sends the
ﬁnal batch of control ﬂits to LM s to resume normal operation. This
concludes the VF switch-over process.
3.4 Perfomance Mode
The Performance Mode for SU PERN E T is based on the fact that
not all packets in the NoC are critical to the performance of the application. We have classiﬁed the possible packet types in Table 1.
The LoadRequest packet is generated by the core in case a memory load instruction causes a cache miss. The memory controller
replies to the request by sending the required cache line through
LoadRequestReply packet. In case a cache line is evicted, core generates a StoreRequest packet for the memory controller. In response
to the StoreRequest , memory controller sends the StoreRequestAck packet as an acknowledgement. We assume an MPSoC with
an in-order core and therefore the core stalls in case of a cache
loads misses. Thus, it is critical to deliver the LoadRequest and
LoadRequestReply as quickly as possible. Whereas, the process of
sending the evicted cache line to the memory controller happens in
parallel to the execution and therefore the StoreRequest and StoreRequestAck packets are not critical for application performance.
When only one NoC is used, all packets compete for shared channel resources. Therefore due to contention, packets delivery can
be delayed. This delay can possibly affect the system performance
if delay for critical packets increase. The situation becomes even
worse when applications have high MPKIs.
Packet Steering Based on Criticality: We leverage the two
NoCs available in SU PERN E T to implement the performance mode.
FM switch on both NoC A(with [V 1,f ]) and NoC B at the same
time. At runtime, the cores inject LoadRequest packets in the NoC A
and StoreRequest in the NoC B. Whereas, the memory controller inject the LoadRequestReply packets in the NoC A and StoreRequestAck packet in the NoC B. The intuition behind the steering scheme
is to reduce the contention between critical and non-critical packLength
Critical?
Type
Load Request
1f lit
Load Request Reply (cache line size)/8 + 1f lits
(cache line size)/8 + 1f lits
1f lit
Store Request
Store Request Ack
yes
yes
no
no
Table 1: Packet Classiﬁcation
ets by physical separation. However, this scheme is implemented at
the cost of increased NoC power as compared to normal operation
because two NoCs are used instead of a single NoC. Moreover the
performance mode is only useful for applications where network
load is high due to a high application MPKI. Therefore, FM can enable or disable the performance mode based on power budget and
application characteristics.
Mode Setup: To start the Performance Mode, the FM sends control ﬂits to each LM to stop any further transaction in the NoC and
informs the LM about initiation of the mode. FM then waits for all
the existing transactions to ﬁnish by monitoring the global BUFFEMPTY signal (Figure 3(a)). In parallel, LM also switches on NoC
A and NoC B routers in case one of them was not already switched
on. FM sends control ﬁts to each LM in response to which NI is
enabled and conﬁgured to steer trafﬁc into a speciﬁc NoC based on
trafﬁc classiﬁcation.
3.5 Reliability Mode
The Reliability Mode for SU PERN ET enables the detection and
correction of soft errors in the NoC. The scheme can provide protection against soft errors in both data path and control path. In this
mode both NoCs are used at the same time. Reliability Mode use
NoC A and NoC B in parallel by operating them at [V 2, f /2].
Data Path Protection: Router’s data path is protected through
the use of strong single error correction, double error detection
codes (SECDED) in a forward error correction mode (FEC). The
Error Detection and Correction Unit (EDC) is used to generate a
(13,8) BCH code (8 bit data generate 5 bit code). In SU PERN E T
we only encode the data at the time of packet injection and only
try to check and correct the data at the receiver node. A more
error resilient approach is to perform error encoding and decoding at each router in the path of the packet. However this scheme
would increase the energy cost due to additional hardware at each
router. Moreover, per router encoding and decoding will increase
the router’s pipeline stages, which will eventually result in an increase in network latency and hence performance penalty. The actual packets are injected in NoC A and the error correction codes are
injected in NoC B simultaneously as shown in Figure 3(b).
Control Path Protection: In SU PERN ET, we employ the idea
of dual modular redundancy (DMR) and lock step processing to
protect the router control path against the soft errors. Both NoCs
are architecturally homogenous and are operated at the same frequency. Furthermore, during the setup of Reliability Mode, switch
allocators for all the routers are reset to a known state. Therefore,
routers from different NoCs at a given mesh node are expected to
generate the same output. Each NoC channel consists of control
bits and data bits as shown in Figure 3(d). We perform equivalence
check on control bit output on outgoing router channels in a given
direction as shown in Figure 2 and Figure 3(c). Any soft error in
control circuit in either of the routers will result in in-correct data
written on a port or no output at all. For example, an error in routing
logic will route the ﬂit to the wrong port. In case the equivalence
check fails, an error is raised for LM.
Mode Setup: To start the Reliability Mode, the FM sends control
ﬂits to each LM to stop any further transaction in the NoC. FM then
waits for all the existing transactions to ﬁnish by monitoring the
global BUFF-EMPTY signal (Figure 3(a)). Then, FM sends control
ﬂits to LM to switch-on the routers for NoC A(at VF [V 2,f /2]) and
NoC B. Last set of control ﬂits are sent to LM to reset the router
switch allocators to a known state and continue normal operation.
Runtime Fault Recovery: At runtime, whenever an non correctable error occurs in the data path or lock step router execution
fails, an error is raised by the LM for FM as shown in Figure 3(c).
In response to the error, FM sends control ﬂits to each LM to stop
any new transaction from the NI. Once the network is free of any
trafﬁc, FM sends control ﬂits to LM s to reset the switch arbiter to a
known state. FM sends the last set of control ﬂits to LM s to continue
normal operation with these conditions:
• If the core has a sent a LoadRequest packet and the memory
controller has not sent a reply through LoadRequestReply
• If the core has sent a S toreRequest packet and the memory
packet, the local NI again sends the LoadRequest (Table 1).
controller has not sent a reply through S toreRequestAck
packet, the S toreRequest packet is sent again.
Performance Overhead: The main performance overhead of
SU PERN ET’s Reliability Mode is due to the fact that NoC is operated at a reduced frequency of f /2 which causes increase in the
Topology
Router Architecture
Input Buffer
Routing
Link Flow Control
Link Width
Switch Arbiter
8×8 mesh
5 port router (4 neighbors + 1 local), 2 stage
pipeline [LT+BW, RC+SA+ST]
8 ﬂit deep (no virtual channel)
Dimension Order XY
Wormhole switching with On/Off ﬂow control
64 bit data
Matrix arbiter
Table 2: NoC architectural details.
Frequency
Voltage
Area [µm2 ]
HVt Cells [%]
NVt Cells [%]
LVt Cells [%]
1 GHz 500 MHz
0.9 V
0.81 V
33041
45
7
48
28976
82
6
12
Table 3: Synthesis results for a single router using Multi-Vt optimization.
ECC Decode ECC Encode Equivalence
Area [µm2 ]
1814
760
46
Table 4: Synthesis results for Error Detection & Correction
Units
Topology
Processors
L1 I/D Caches
Memory Controller
NoC VF Levels
8 × 8 mesh
Tensilica in-order LX4 @ 1GHz
16KB, 2-way set associative, 16 byte line size
4 memory controllers with perfect L2 cache
[1GHz, 0.9V], [500 MHz, 0.81V]
Table 5: MPSoC Conﬁguration
network latency. The performance penalty of increased NoC latency will be more signiﬁcant for applications that have high MPKIs. Furthermore, performance overhead is incurred when there
is a non-correctable fault (data path and/or control) which requires
retransmission from source.
4. EXPERIMENT AND RESULTS
4.1 Experiment Setup
NoC Synthesis: For this paper, we use a 2 stage pipelined wormhole switched router with 64 bit data channel width. Architectural
details of the NoC router used in our study are reported in Table 2.
In the ﬁrst stage, ﬂits travel on links and get written in the input
buffer. The second stage consists of route computation, switch
arbitration and switch transversal. The router description is written at the RTL (Verilog). We synthesized the RTL using Synopsys Design Compiler version H-2013.03-SP4. We used commercial
TSMC 45nm libraries that are characterized for Low Vt(LVt), Normal Vt(NVt) and High Vt(HVt). We enable leakage and dynamic
power optimization which automatically invoke multi-Vt optimization. We target two VF levels for the router design in this paper,
i.e, [1GHz, 0.9V] and [500MHz, 0.81V]. We report the synthesis
results for NoC in Table 3. We also synthesized the error detection
and correction hardware, results for which are shown in Table 4.
MPSoC Details: We target a 64 core general purpose MPSoC
organized in a 8×8 2D-mesh. Architectural details for the MPSoC
are included in Table 5. The processors are an in-order Tensilica
Xtensa core with private L1 instruction and data caches. The core
stalls when there is an outstanding cache load miss. The frequency
for the cores is ﬁxed at 1GHz. For the baseline system, we assume that there are 4 memory controllers placed at the boundaries
as shown in Figure 2. To stress the NoC, we assume that the memory controllers are augmented with perfect L2 caches. This technique has been used previously for testing NoC architectures [25,
26].
Benchmark Applications: We use a diverse set of benchmark
applications from MediaBench and SPEC2006 packages to evaluate
our proposed scheme. We created multi-programmed workloads
using the applications from these two suites by running a copy of
the selected application on each core. Each core executes 50 million
instructions on average for experimentation.
Simulator: We use an in-house cycle accurate simulator for our
study. The simulation is carried out in two steps: First, the applications are executed on Xtensa instruction set simulator which
generates time-stamped cache load/store miss traces. In full system simulation, these memory traces are replayed in a"
Network footprint reduction through data access and computation placement in NoC-based manycores.,"Targeting network-on-chip based manycores, we propose a novel compiler framework to optimize the network latencies experienced by off-chip data accesses in reaching the target memory controllers. Our framework consists of two main components: data access placement and computation placement. In the data access placement, we separate the data access nodes from the computation nodes, with the goal of minimizing the number of links that need to be visited by the request messages. In the computation placement, we introduce computation decomposition and select appropriate computation nodes, to reduce the amount of data sent in the response messages and also to minimize the number of communication links visited. We performed an experimental evaluation of our proposed approach, and the results show an average execution time improvement of 21.1%, while reducing the network latency by 67.3%.","Network Footprint Reduction through Data Access and
Computation Placement in NoC-Based Manycores
Jun Liu, Jagadish Kotra, Wei Ding and Mahmut Kandemir
Dept. of Computer Science and Engineering
The Pennsylvania State University
University Park, PA - 16802, U.S.A
{jxl1036, jbk5155, wzd109, kandemir}@cse.psu.edu
ABSTRACT
Targeting network-on-chip based manycores, we propose a
novel compiler framework to optimize the network latencies
experienced by oﬀ-chip data accesses in reaching the target
memory controllers. Our framework consists of two main
components: data access placement and computation placement. In the data access placement, we separate the data
access nodes from the computation nodes, with the goal of
minimizing the number of links that need to be visited by
the request messages. In the computation placement, we introduce computation decomposition and select appropriate
computation nodes, to reduce the amount of data sent in
the response messages and also to minimize the number of
communication links visited. We performed an experimental
evaluation of our proposed approach, and the results show
an average execution time improvement of 21.1%, while reducing the network latency by 67.3%.
Categories and Subject Descriptors
H.4 [NoC-Based Hardware]: Data and Computation Placement; D.2.8 [Computation Placement]: Metrics—performance measures, energy measures
General Terms
Design, Experimentation, Performance, Energy
Keywords
NoC Based manycores, Data and Computation placement
1.
INTRODUCTION
It is clear that processor performance is improving at a
much faster pace than memory performance. This makes
it very important to optimize memory system performance
using both hardware and software techniques. Most of the
compiler-based data access optimization schemes proposed
in the literature exclusively target cache behavior, largely
omitting oﬀ-chip memory accesses. While optimizing cache
behavior is of utmost importance, oﬀ-chip data accesses can
also play a critical role. This is particularly true for emerging
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, to republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee.Request
permissions from Permissions@acm.org.
DAC ’15, June 07-11, 2015, San Fransico, CA, USA
Copyright 2015 ACM 978-1-4503-3520-1/15/06 ...$15.00
http://dx.doi.org/10.1145/2744769.2744876.
on-chip
off-chip
other
100
80
60
40
20
0
e
i
m
w
n
T
o
n
d
o
k
a
u
e
c
r
e
B
x
E
i
t
Figure 1: Execution time breakdown of applications
running on a 4 × 8 on-chip network based multicore.
manycore architectures where parallel threads from dataintensive applications can issue a lot of memory requests,
creating contention not only on memory controllers, but also
on the on-chip network. The cost of an oﬀ-chip data access in
a manycore consists of two main components: (a) time spent
by the access in the network on-chip (NoC) to reach the target memory controller and (b) time spent in accessing the
memory itself including the queuing latency and the DRAM
access latency. In a large manycore system, the ﬁrst component can be very important and is the target of this paper.
For example, Figure 1 plots the execution time breakdown
for the multithreaded applications in our benchmark suite
[12] into three parts (on a 4× 8 multicore). The ﬁrst, marked
as “on-chip”, captures the on-chip component of an oﬀ-chip
memory access (i.e., the time the oﬀ-chip access spends in
the on-chip network). The second, “oﬀ-chip”, is the fraction
of time spent in accessing the oﬀ-chip memory itself, including the queuing latency in the memory controller, and the
DRAM access itself. Finally, the last part, called “other”, is
the fraction of time spent in computation as well as on-chip
cache accesses. One can observe that the time spent in the
on-chip network is quite signiﬁcant (averaging on 31.4%).
Motivated by this observation, this paper proposes and
evaluates a novel “compiler-directed” data access and computation placement strategy targeting on-chip network based
multicore architectures and programs composed using aﬃne
loop nests. Our speciﬁc contributions in this work include:
• We propose a data access placement strategy that minimizes the number of links visited by request messages. This
strategy assigns a separate “data access node” for each data
access to initiate the data request.
• We propose a computation placement strategy that minimizes the number of links visited by response messages.
This strategy determines, for each computation, a computation node that has the shortest average distance to the
related memory controllers.
• We propose a strategy that reduces the amount of data
sent in response messages. The idea behind this is to break
a computation into sub-computations (sub-operations), and
 
 
 
execute them directly on the data access nodes. Consequently, only the generated intermediate results from the
sub-operations will be sent to the computation core, causing less traﬃc on the on-chip network.
• We present results collected on a 32-core system, indicating that our optimization framework cuts, on average,
the network latency of oﬀ-chip data accesses by 67.3%, and
generates an execution time improvement of 21.1%.
Communication Link 
MC 
Core<0, 0> 
MC 
Core 
L1 Cache 
L2 Cache 
Router 
MC 
Core<i, j> 
MC 
Figure 2: Target NoC-based manycore architecture
(MC denotes a memory controller).
Note that our compiler-based work is complementary to
related OS [7] and architectural based studies [6].
2. BACKGROUND AND MOTIVATION
2.1 Architecture
based manycore system with (N × M ) nodes. Each node
As shown in Figure 2, we target a network-on-chip (NoC)
has a core and private L1 and L2 caches. To facilitate our
discussion, each node in this architecture is labeled by a
unique index (cid:2)i, j (cid:3) (e.g., the index of the upper leftmost
core is (cid:2)0, 0(cid:3)). Further, some nodes are attached memory
controllers (MCs) that manage oﬀ-chip memory accesses.
Each MC also has an index, which is the same of the index
of the node to which the MC is attached. When no confusion
occurs, we use the terms “core” and “node” interchangeably.
Any missed data request in L1 cache is forwarded to L2
cache. If it is missed in the L2 cache too, an oﬀ-chip data
request through a memory controller is initiated. Note that,
in this architecture, the oﬀ-chip data accesses can be very
expensive for two reasons: (a) overhead of traversing over
the on-chip network and (b) overhead of accessing a memory
bank. Our footprint reduction framework aims at optimizing
the ﬁrst overhead, while prior strategies to reduce the second
overhead [9] are orthogonal to our work.
Assuming a static XY-routing in the network1 , we start
with an important observation: for two cores that communicate with the same MC, the latencies of the messages can be
diﬀerent due to the diﬀerent number of network hops (distance) that need to be traversed [8]. Speciﬁcally, shorter the
distance that need to be traversed by a data access, better
from both the performance and power perspectives. We deﬁne the distance Dis((cid:2)i, j (cid:3), (cid:2)m, n(cid:3)) from core (cid:2)i, j (cid:3) to MC
(cid:2)m, n(cid:3) as the Manhattan Distance:
Dis((cid:2)i, j (cid:3), (cid:2)m, n(cid:3)) = |i − m| + |j − n|,
(1)
from core (cid:2)i, j (cid:3) to MC (cid:2)m, n(cid:3). We say that core (cid:2)i, j (cid:3) is
which captures the number of hops that need to be traversed
associated with MC (cid:2)m, n(cid:3) if Dis((cid:2)i, j (cid:3), (cid:2)m, n(cid:3)) is minimum
among all distances between core (cid:2)i, j (cid:3) and any MC. For
example, in Figure 2, core (cid:2)0, 1(cid:3) is associated with MC (cid:2)0, 0(cid:3).
since this reduces distance-to-data.
1There are two main reasons why we focus on XY-routing.
First, since we want to expose routing to compiler, it needs
to be static. Second, our preliminary experiments indicated
that dynamic routing in NoC can incur signiﬁcantly higher
energy consumption than static routing.
2.2 Footprints
The execution of a statement in a program usually consists of two steps. The ﬁrst step is to load the needed data
elements from the main memory or on-chip caches into the
registers. The second step is to perform the speciﬁed computation.
In the ﬁrst step, an oﬀ-chip access generates a
network footprint, i.e., message traﬃc on the on-chip network. There are two types of footprints. The ﬁrst type is
caused by data request messages from the cores to the MCs.
The second type is generated by the data messages (response
messages) carrying the requested data from the MCs. For
example, in Figure 3, core (cid:2)2, 2(cid:3) sends a request message to
MC (cid:2)0, 0(cid:3) and causes footprint (1). This MC sends back a
data message to core (cid:2)2, 2(cid:3) and generates footprint (2). We
deﬁne the footprint value F as follows:
F = L × W,
(2)
where L is the length of the footprint (the number of traversed links calculated using Eq. (1)), and W is the weight of
the footprint (the number of data elements in the message).
The network footprints directly aﬀect data access latencies,
network congestions and power consumption on the communication links/message buﬀers. Hence, it is critical to reduce
footprints on the on-chip network as much as possible.
2.3 Data Access Placement
This section introduces our ﬁrst optimization, called Data
Access Placement, to reduce the “request message footprints”.
Everything else being equal, one would prefer the sender
(core (cid:2)i, j (cid:3)) of the request message to be as close as possible
to the receiver (MC (cid:2)m, n(cid:3)). In a conventional execution,
for each data access, there is only one “data access node”
(i.e., the sender of the request message), which is the same
as the computation node (i.e., the node that will perform
the computation). A data access node may need to send
request messages to diﬀerent MCs, in order to fetch multiple data elements needed by the computation. The problem
is that, it is not possible to ﬁnd a data access node that is
close to all MCs. Our solution to this problem is to separate/decouple the data access and computation, and place
them into diﬀerent cores. Further, we can have “multiple”
data access nodes to issue diﬀerent request messages on behalf of the same computation node. The advantage of doing
so is that we can decide at which node to perform a speciﬁc data access, so that the network footprint of a data request message can be reduced independently. The response
data messages will be sent from the MCs to the computation node. Hence, the data access node actually performs a
data access for the computation node, not for itself. We introduce a new load operation for our NoC-based manycore
architecture (“slw dest, addr”). Speciﬁcally, this new load is
executed on the data access node and sends the request message to the MC based on target address addr . The response
message is then sent from the MC to the destination node
dest. Figure 3 shows the footprint of request message (1)
and data message (2) when not using an explicit data access
node (the computation node is (cid:2)2, 2(cid:3)) – this represents the
conventional data access in the manycore. In comparison,
Figure 4 illustrates the footprint when we employ (cid:2)0, 1(cid:3) as
the data access node, whereas the computation node is still
(cid:2)2, 2(cid:3). The new request footprint (1) is now sent from (cid:2)0, 1(cid:3)
to the MC, and the data read will be forwarded to (cid:2)2, 2(cid:3) (using our new load operation). Therefore, compared to Figure
3, Figure 4 reduces the data request footprint F from 4 to
1 
MC 
1 
Core<0, 1> 
MC 
2 
MC 
Core<2, 2> 
MC 
2 
MC 
Core<2, 2> 
2 
1 
2 
1 
1 
2 
Core<0, 0> 
1 
2 
Core<0, 3> 
Core<1, 2> 
MC 
MC 
MC 
Figure 3: Footprint example without any data access node.
the data access node ((cid:2)0, 1(cid:3)).
Figure 4: Footprint example with
Figure 5: Optimal data access placement for a simple example.
1, assuming that the weight W in Eq. (2) is 1.
To explain the general principle of how to select the data
access nodes, we consider the following program statement:
e = a + b + c + d. Assume, for the sake of illustration,
that a and b are mapped to MC (cid:2)0, 0(cid:3), and c and d are
mapped to MC (cid:2)0, 3(cid:3). Assume further that this computation
(statement) is to be performed on core (cid:2)1, 2(cid:3), that is, this
statement is scheduled to be executed on core (cid:2)1, 2(cid:3). For
this example, as shown in Figure 5, the optimal data access
node for variables a and b would be (cid:2)0, 0(cid:3), and for variables
c and d (cid:2)0, 3(cid:3). With this data access placement, the request
are attached to nodes (cid:2)0, 0(cid:3) and (cid:2)0, 3(cid:3). We also want to point
footprints (1) need not to go over any links since the MCs
out that, our data access placement does not aﬀect the data
message footprints (2). Generally, one can identify for each
data element (required by a computation) the MC to which
that data is mapped, and choose a data access node that is
close to that MC (reducing L in Eq. (2)).
2.4 Computation Placement
We now introduce another optimization, called Computation Placement, to reduce the “data message footprints”.
We use the same example statement (e = a + b + c + d) to
explain our strategy. As in the previous section, we assume
that a and b are accessed through MC (cid:2)0, 0(cid:3), c and d are
accessed through MC (cid:2)0, 3(cid:3), and the computation is originally placed on core (cid:2)1, 2(cid:3). We further assume that the data
access placement optimization has already been performed
and the data access nodes are determined to be (cid:2)0, 0(cid:3) and
(cid:2)0, 3(cid:3). The original data footprints of this example are depicted in Figure 6. In this ﬁgure, both a and b (footprints
tached to core (cid:2)0, 0(cid:3)) to the computation node (cid:2)1, 2(cid:3), and c
marked using (1)) need to go over 3 hops from the MC (atand d (footprints (2)) need to traverse 2 hops.
Our initial optimization is straightforward. The basic idea
is to select a computation node that has the smallest average
the computation node as(cid:2) 0, 2(cid:3) instead of (cid:2)1, 2(cid:3). The data
distance to the MCs. For the example above, we can choose
footprints with this computation node placement are shown
in Figure 7. Compared to Figure 6, a and b (footprints (1))
in Figure 7 need to traverse only 2 hops, and c and d (footprints (2)) need only 1 hop. The above optimization for
computation placement actually reduces the lengths of the
data footprints (L in Eq. (2)). Our next optimization, on
the other hand, aims at reducing the “weights” of the data
footprints (W in Eq. (2)). For example, e = a + b + c + d
Observing that both a and b are requested by core (cid:2)0, 0(cid:3),
can be rewritten ase = f + g , where f = a + b and g = c + d.
we can choose to perform the sub-computation (f = a + b)
directly on core (cid:2)0, 0(cid:3) as depicted in Figure 8. As a result,
we only need to send data element f from (cid:2)0, 0(cid:3) to (cid:2)0, 2(cid:3)
(footprint (2)). Even though the number of hops that need
to be traversed by the data is still the same, the number of
data elements to be sent has reduced from 2 (a and b) to 1
(f ), thereby reducing the footprint weight W . Similarly, we
can perform the sub-computation (g = c + d) directly in core
(cid:2)0, 3(cid:3) and send only one data element (g) from (cid:2)0, 3(cid:3) to core
(cid:2)0, 2(cid:3) (footprint (3)). In general, the basic idea of reducing
the “weights” of the data footprints is to ﬁrst identify the
“sub-operations” (sub-computations) of a computation that
access data from the same MC and decompose the original
computation accordingly (into these sub-operations). We
then execute these sub-operations directly on the data access nodes, and send only the generated intermediate results
(which usually contain less data) from the data access nodes
to the computation node.
3. OVERVIEW OF OUR FRAMEWORK
The input to our framework is a parallelized code (currently, we employ shared memory programming model for
the multicore architecture and can handle pthreads and OpenMP
codes). Our optimization scheme consists of two phases:
data access placement and computation placement, with the
goal of reducing request footprints and data footprints, respectively. The computation placement phase can be further divided into two sub-steps, computation decomposition
and computation (node) placement, in order to reduce the
amount of data in the response messages as well as the
number of links visited. The output of our framework is
an optimized parallel code where footprints of the request
and data messages are minimized. In this work, we target
at optimizing “aﬃne loop nests”2 , and employ the polyhedral
model [5] for “pre-process” these loops.
In the polyhedral
model model, each iteration (in an n-dimensional loop nest)
can be represented by an iteration vector (cid:2)i = (i1 , i2 , ..., in )T .
The set of computations (computation domain) can be represented as a polytope Dc of n dimensions as follows:
Dc : Dc
(cid:3)T ≥ (cid:2)0,
(cid:2)i (cid:2)n 1
(3)
where Dc is the inequality matrix of the domain, and (cid:2)n is
the vector of loop-independent parameters. To increase parallelism and improve data locality, the iteration space can be
divided into smaller blocks (tiles) [4, 11, 10], each of which
contains a subset of the iterations and is an “atomic” execution unit. All the necessary data needed for a tile must
be ready before its execution starts. Also, all the output
data will be available at the end of the tile execution. One
important constraint in tiling is that, after partitioning the
iteration space, the dependencies among the resulting tiles
2 In these types of loops, the loop bounds and array references are aﬃne functions of enclosing loop indices and loopindependent variables.
(cid:2)
1 
2 
2 
Core<1, 2> 
1 
2 
1 
2 
Core<0, 2> 
1 
2 
1 
3 
1 
1 
Core<0, 2> 
MC 
MC 
MC 
MC 
MC 
1 
MC 
Figure 6: Data footprints before
the computation placement optimization.
Figure 7: Data footprints after the
computation (node) placement optimization.
Figure 8: Data footprints after the computation decomposition optimization.
should form a “partial order”. In other words, a legal tiling
requires that no two tiles should be mutually dependent on
each other. Note that while an application-speciﬁc data access pattern can in theory create hard-to-predict bottlenecks
on the network, we did not observe that to happen in our
data-parallel applications i.e, network congestion was more
or less evenly distributed across the links.
4. TECHNICAL DETAILS
Recall that the execution of a statement in our approach
is divided into two phases: data access and computation.
Similarly, the execution of a tile can also be divided into such
two phases, which gives us the ﬂexibility to map the data
access and computation phases of a given tile to diﬀerent
cores. where each vertex represents a tile, and each edge
indicates a dependency relationship between two tiles. Our
compiler fetches the ready tiles from the Data Dependency
Graph (DDG) and maps the data accesses and computations
in these tiles to cores. Due to the dependency constraints
and resource limitations, it may not be possible to schedule
all the tiles at the same time step (scheduling slot). Instead,
we typically need multiple rounds to schedule the tiles. At
each round, the number of tiles to be handled equals to
either the number of currently ready tiles or the number of
cores in the target machine, whichever is smaller.
4.1 Data Access Placement
We need to consider the following factors when performing our data access placement optimization for a tiled code.
First, the data access nodes should be as close as possible to
the MCs, in order to reduce L in Eq. (2). Second, we would
like to balance the “data access workload” across all cores.
If this is not done, the nodes that are closest to the MCs
can be overloaded (become bottlenecks). Third, we need to
ensure that the diﬀerent data requests originating from a
given tile will arrive at the MCs at similar time steps, i.,
the lengths of diﬀerent request footprints should be similar.
Otherwise, some data requests could be excessively delayed,
which could in turn delay the computations in the tile.
We ﬁrst determine how all the data elements referred in
a tile are accessed through the MCs. To achieve this, we
ﬁrst need to locate the data block accessed by a given iteration tile. As discussed in Section 3, in the polyhedral
model, each iteration can be represented by an iteration vector (cid:2)i = (i1 , i2 , ..., in )T in an n-dimensional iteration space.
Similarly, each data element in an array can also be denoted
using a data vector (cid:2)d = (d1 , d2 , ..., dm )T in an m-dimensional
space. Each data reference to an array in a loop nest can
be written as: (cid:2)d = A(cid:2)i + (cid:2)o, where A is the access matrix (of
m × n) and (cid:2)o is the oﬀset vector (of m × 1). Therefore, given
any iteration of a tile, we can determine which parts of an
Iteration Tile 
Data Block 
Iteration Vector  i(cid:38)
Data Vector  d(cid:38)
(cid:2)
t
D
(cid:2)
(5)
(cid:2)
(cid:2)
Figure 9: Example mapping from an iteration tile
to a data block.
array are accessed based on the access matrix A and the
oﬀset vector (cid:2)o. Speciﬁcally, let us assume that the polytope
Dt for a tile is as follows:
Dt : Dt
(cid:3)T ≥ (cid:2)0.
(cid:2)i (cid:2)n 1
(4)
polytope Dt . For each data reference (cid:2)d, we have (cid:2)d = A(cid:2)i + (cid:2)o.
We next obtain the data points accessed by the iteration
We ﬁrst concatenate (cid:2)d to the iteration vector (cid:2)i in Eq. (4).
We then add columns of 0s as the coeﬃcients corresponding
to (cid:2)d in Dt to form D
t , and obtain the following expression:
(cid:3)T ≥ (cid:2)0,
(cid:2)i
(cid:2)d (cid:2)n 1
Using this expression and the equality deﬁning the data
access ( (cid:2)d = A(cid:2)i + (cid:2)o), we can obtain the polytopeD d of the
data block accessed by the tile. Figure 9 gives an example
mapping from iteration space to data space, indicated by an
arrow starting at the iteration tile and ending at the data
block. Each point on the left box represents an iteration
vector (cid:2)i, while each point on the right space denotes a data
vector (cid:2)d. A loop nest can be generated to enumerate all the
data points in Dd [2]. In the loop nest enumerating the data
elements of a data block, we introduce two loops to replace
the original innermost loop. The new second innermost loop
determines the starting address of a contiguous data chunk
that is mapped to a speciﬁc MC. The new innermost loop
iterates over all the data elements in the contiguous chunk
one by one and issues data requests to the MC. As can be
observed, by obtaining the data block accessed by a tile, we
only need to send one request for a speciﬁc data element,
even though it may be used multiple times in the tile.
Once we have determined the MCs for all the data elements of a tile, we merge the data accesses to the same MC
and schedule them together on the same data access node.
In addition, on a data access node, if several data elements
share the same cache line, we coalesce them into a single
access and issue only one oﬀ-chip request message. As mentioned earlier, we may not simply choose the core that is
closest to the MC, since we also want to maintain “data access load balance” across the cores. For this reason, we assign
weights Ai to the cores based on their distances (the number of hops) to the associated MCs. Speciﬁcally, we choose
Ai to be inversely proportional to the number of hops. This
strategy turns out to be the best tradeoﬀ between load balance
and overal l traﬃc reduction. Thus, the shorter the distance
from the core to the MC is, the larger is the weight of the
core. If two cores have the same number of hops to a given
(associated) MC, their weights will be the same. When we
select which core to issue the data request messages, we
need to check both a core’s weight and its current workload
Ri (i.e., the number of bytes to be accessed). Speciﬁcally,
assume that a core with weight Ai currently has been assigned workload Ri . For a data request, we check all the
cores associated with the MC, and keep those cores as can≤ σ ,
didate cores which satisfy the following condition: Ri
where σ is a “workload balancing threshold”. A delay in any
data access can lead to a delay of the entire computation of
the tile, which can in turn degrade the overall application
performance. Therefore, the lengths (L) of the footprints
generated by diﬀerent data accesses in a tile should be similar. Our strategy to achieve this is as follows. For each set
of data accesses Ai to an MC, we ﬁrst choose its best available core Di . Among all such cores, we identify the one with
longest request footprint M ax(L). Then, the data requests
to be issued by this core can be considered as the “bottleneck”. As a result, for the data accesses to other MCs, we
can choose any available core other than the current best
available one, as long as the length of the newly-generated
footprint is smaller than or equal to M ax(L). for scheduling
other tiles, while keeping similar footprints for diﬀerent data
requests coming from the current tile. We perform our data
placement for the ready tiles one-by-one based on the above
steps, until all the ready tiles are processed.
Ai
4.2 Computation Placement
To reduce data footprints, our computation placement step
should take into account the following factors. First, we
would like to reduce the distances between MCs and computation nodes, which directly aﬀects the data footprint
lengths (L in Eq.
(2)). Second, we want to reduce the
amount of data (W in Eq. (2)) transferred from the MCs to
the computation node, which can be achieved through computation decomposition. performed on the data access nodes
and only the generated intermediate results are sent. Third,
we want to balance the distances from diﬀerent MCs to the
computation node (in order to balance the corresponding
data arrival times). Finally, we want to balance the computation workload among all the cores.
4.2.1 Computation Decomposition
The idea behind computation decomposition is to extract
some sub-operations of a computation and execute them directly on the data access nodes. Such a sub-operation should
not depend on the rest of the computation, and all the data it
needs should be requested by the same data access node. We
refer to a sub-operation that satisﬁes these two constraints
as independent operation. Note that an independent operation may contain multiple operators.
When applying computation decomposition to a tile, the
decomposition of the statements in the loop body may not
always be the same. One reason is that the same reference
to an array can access diﬀerent MCs at diﬀerent iterations.
Consequently, for each iteration of a tile, we need to identify
the set of independent operations O as much as possible by
analyzing the relative priorities (precedences) of the operations, as well as the data access placement decisions. We
Table 1: Our default conﬁguration.
Cores/
Caches
Memory
On-chip
Networks
Processor: two-issue
L1: 16 KB (per node), 64 byte lines, 2 ways
L2: 256KB (per node), 64 byte lines, 16 ways
Number of Memory Controllers: 4
Total Capacity: 4GB
Size: 4 × 8 two-dimensional mesh
Delays: 16B links, 2-cycle pipeline
Routing: XY-routing
ﬁrst locate all the sub-operations that have the highest priority, and for each such sub-operation, we then check whether
its data are requested by the same data access node or not.
If so, we label this sub-operation as an independent operation and replace it with an intermediate variable in the ﬁnal
computation. We then schedule this sub-operation to be
performed directly on the data access node, and send only
the generated result to the computation node.
4.2.2 Computation Node Placement
As we have already pointed out, the computation node
should have “balanced distances” to the data sources. Note
that the data source here can be either a MC, or a data
access node that executes a sub-operation. To this end, we
take into account all the data sources of the current tile. For
each data source (cid:2)Dix , Diy (cid:3), we assign it a weight Gi which
is the amount of data to send to the computation node.
When the data source is a data access node that executes
a sub-operation, the generated intermediate data from the
sub-operation will be counted towards the weight.
Once the weight Gi is available for each data source (cid:2)Dix , Diy (cid:3),
we ﬁnd the center (cid:2)Cx , Cy (cid:3) of all the data sources using the
following formula:
(cid:2)Cx , Cy (cid:3) = (cid:2)
i=0 DixGi
i=0 DiyGi
(cid:4)N −1
(cid:4)N −1
(cid:3),
(6)
,
N
N
where N is the number of data sources.
If a data source
center node (cid:2)Cx , Cy (cid:3) will be closer to it. Consequently,
has more data to send compared to other ones, the selected
all the data sources. In case the center node (cid:2)Cx , Cy (cid:3) has
the center node will have balanced (similar) distances to
already been assigned to another tile, we check this center node’s available neighbors (i.e., those with one-hop distance). Among these cores (cid:2)C
y (cid:3), the one with the smallx , C
i=0 Gi (|C
x − Dix | + |C
y − Diy |) is selected as
est value of
the computation node. If none of the current neighboring
cores is available, the distant neighbors will be checked until
the computation node is decided. Therefore, in both data
access and computation placement, we have mechanisms to
maintain load balance (to prevent the nodes that are close
to MCs from being overloaded).
(cid:4)N
(cid:2)
(cid:2)
(cid:2)
(cid:2)
5. EXPERIMENTAL ANALYSIS
The compiler component of our approach is implemented
using the LLVM compiler infrastructure [1] and all the experiments presented below are carried out using the GEM5
tool-set [3]. We used a set of multi-threaded applications
[12]. The “baseline” against which we compare our placement schemes is the original parallel applications with default data access/computation behavior (without our footprint reduction optimizations).
Figure 10 plots the “percentage reductions” in oﬀ-chip
data access latency and total execution time, when both
the components of our approach (data access placement and
computation placement) are applied. The average latency
on the network for oﬀ-chip data accesses has been reduced
by 67.3% with our footprint reduction optimization. As a
network latency
execution time
e
n
g
o
a
i
t
t
n
c
u
e
c
d
r
e
e
R
P
80
60
40
20
0
i
n
o
e
m
i
t
c
u
T
d
e
n
R
o
i
e
t
g
u
a
c
t
e
n
x
e
E
c
n
r
e
i
P
35
30
25
20
15
10
5
0
Figure 10: Percentage reductions of network latency
and total execution time brought by our approach.
Figure 12: Sensitivity experiments with one parameter changed with others kept at their default values
given in Table 1.
f
w
o
n
s
g
o
n
d
v
k
a
a
S
e
i
r
B
data access placement
computation placement
100
80
60
40
20
0
i
n
o
e
i
t
m
i
t
c
u
T
d
e
n
R
o
e
u
g
c
a
e
t
n
x
E
e
c
n
r
e
i
P
optimized placement
ideal placement
35
25
15
5
-5
Figure 11: Breakdown of savings in execution time
Figure 13: Results with ideal placement.
result, it achieves an average of 21.1% reduction in total execution time. We observe that, even though the percentage
reductions in network latency for applications fmm and radix
are similar (66.6% vs 63.3%), the performance improvement
for fmm is much higher than that for radix (26% vs 12.2%).
This is because fmm is a highly memory-intensive application, whereas radix is not. Figure 11 quantiﬁes the “individual contributions” of data access placement and computation
placement components of our approach. One can observe
that, the computation placement component is responsible
for about 75.4% of our execution time improvements. The
reason is that, the size of a request message is generally much
smaller than the size of response message, which contains the
actual data to be accessed. As a result, the response footprints contribute more to the network latencies for oﬀ-chip
data accesses than the request footprints.
In each experiment presented in Figure 12 we only changed
one parameter and remaining parameters are kept at their
default values given in Table 1. From Figure 12, one "
A Lightweight Early Arbitration Method for Low-Latency Asynchronous 2D-Mesh NoC's.,"A new asynchronous low-latency interconnection network is introduced for a 2D mesh topology. The network-on-chip, named AEoLiAN, contains a fast lightweight monitoring network to notify the routers of incoming traffic, thereby allowing arbitration and channel allocation to be initiated in advance. In contrast, several recent synchronous early arbitration methods require significant resource overhead, including use of hybrid networks, or wide monitoring channels and additional VCs. The proposed approach has much smaller overhead, allowing a finer-grain router-by-router early arbitration, with monitoring and data advancing independently at different speeds. The new router was implemented in 45nm technology using a standard cell library. It had 52% lower area than a similar lightweight synchronous switch, xpipesLite, with no early arbitration capability. Network-level simulations were then performed on 6 diverse synthetic benchmarks in an 8×8 2D mesh network topology, and the performance of the new network was compared to an asynchronous baseline. Considerable improvements in system latency over all benchmarks for moderate traffic were obtained, ranging from 34.4-37.9%. Interestingly, the proposed acceleration technique also enabled throughput gains, ranging from 14.7-27.1% for the top 5 benchmarks. In addition, a zero-load end-to-end latency of only 4.9ns was observed, for the longest network path through 15 routers and 14 hops.","A Lightweight Early Arbitration Method
for Low-Latency Asynchronous 2D-Mesh NoC’s
Weiwei Jiang
Dept. of Computer Science
Columbia University
New York, NY 10027
wjiang@cs.columbia.edu
Kshitij Bhardwaj
Dept. of Computer Science
Columbia University
New York, NY 10027
kbhardwaj@cs.columbia.edu
Geoffray Lacourba
ARM Ltd.
Sophia Antipolis, France
geoffray.lacourba@gmail.com
Steven M. Nowick
Dept. of Computer Science
Columbia University
New York, NY 10027
nowick@cs.columbia.edu
Abstract. A new asynchronous low-latency interconnection network is introduced for a 2D mesh topology. The network-on-chip,
named AEoLiAN, contains a fast lightweight monitoring network to
notify the routers of incoming trafﬁc, thereby allowing arbitration
and channel allocation to be initiated in advance. In contrast, several recent synchronous early arbitration methods require signiﬁcant
resource overhead, including use of hybrid networks, or wide monitoring channels and additional VCs. The proposed approach has
much smaller overhead, allowing a ﬁner-grain router-by-router early
arbitration, with monitoring and data advancing independently at different speeds. The new router was implemented in 45nm technology
using a standard cell library.
It had 52% lower area than a similar lightweight synchronous switch, xpipesLite, with no early arbitration capability. Network-level simulations were then performed on
6 diverse synthetic benchmarks in an 8x8 2D mesh network topology, and the performance of the new network was compared to an
asynchronous baseline. Considerable improvements in system latency
over all benchmarks for moderate trafﬁc were obtained, ranging from
34.4-37.9%. Interestingly, the proposed acceleration technique also
enabled throughput gains, ranging from 14.7-27.1% for the top 5
benchmarks. In addition, a zero-load end-to-end latency of only 4.9ns
was observed, for the longest network path through 15 routers and 14
hops.
1.
Introduction
Over the last decade, networks-on-chip (NoC’s) are becoming the
de facto standard approach for on-chip communication, both for lowpower embedded systems and high-performance chip multiprocessors [5, 18]. These on-chip networks typically replace traditional ad
hoc bus-based communication with packet switching in a structured
network architecture, and can be targeted to a variety of cost functions (latency, throughput, power, fault tolerance, quality-of-service)
and parameters (network topology, routing strategies).
In recent years, there has been signiﬁcant interest in designing
asynchronous NoC’s. Since the NoC approach inherently separates
the communication infrastructure, and its timing, from processing elements, it is a natural match for an asynchronous paradigm: the asynchronous interconnect eliminates the need for global clock management across a large network, providing support for scalability and ondemand operation, while avoiding the complexity of instrumenting
complex distributed clock-gating. The increasing role of asynchrony
in system communication is highlighted in the recent International
Technology Roadmap for Semiconductors (ITRS).
A number of asynchronous and globally-asynchronous locally-synchronous (GALS) NoC’s have been proposed to support guaranteed
service [4], dynamic voltage and frequency scaling [2, 3], fault tolerance [12], and automated design ﬂows [9]. Recent commercial uptake
of asynchronous NoC’s, for specialized domains, includes: (i) Intel’s
FM5000/6000 series Ethernet switch chips, based on its 2011 acquisition of the asynchronous startup Fulcrum Microsystems [17]; (ii)
IBM’s recent TrueNorth chip (2014) for neuromorphic applications,
This work was partially supported by NSF Grant CCF-1219013.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
DAC ’15, June 07 - 11, 2015, San Francisco, CA, USA
Copyright 2015 ACM 978-1-4503-3520-1/15/06$15.00
http://dx.doi.org/10.1145/2744769.2744777
integrating 4096 neurosynaptic cores, modelling 1 million neurons
and 256 million synapses, using a fully-asynchronous NoC [7]; and
(iii) the P2012/STHORM processor from STMicroelectronics (201214), an advanced highly-customizable accelerator-based GALS system, which includes a fully-asynchronous NoC to connect 4 clusters, each with 16 synchronous processors [3]. STHORM’s asynchronous NoC is a key to enable signiﬁcantly improved performance
efﬁciency, i.e. performance per unit area and unit power, over several
recent Quadro and Nvidia commercial GPUs. These asynchronous
and GALS approaches allow modular design, with ﬂexible integration of different components running at varied rates. Also, substantial
power beneﬁts [7, 3, 11], and reductions in overall area [11], have
been demonstrated in direct comparison to synchronous networks.
These beneﬁts are obtained in several GALS systems even when considering the moderate synchronization overhead incurred [11, 3]. However, several of these designs, including all the above commercial examples, use both four-phase return-to-zero protocols with two complete round-trip channel communications for each transmission resulting in reduced throughput, and delay-insensitive data encoding,
resulting much lower coding efﬁciency (i.e. wider channel width, often doubled in size from dual-rail or 1-of-4 encodings) than single-rail
bundled data encoding which we use in this paper [4, 2, 17, 7].
The proposed work builds on a recent general-purpose asynchronous NoC router design presented in [9]. This 5-ported switch uses
a nearly entirely standard cell design, with low overhead and simple optimized control. Unlike many of the above asynchronous NoC
approaches, it uses a two-phase non-return-to-zero (i.e.
transitionsignalling) protocol, involving a single round-trip channel communication per data transmission, and single-rail bundled data encoding, thereby providing coding efﬁciency (i.e. data channel width)
nearly identical to a synchronous design. When compared to a highly
optimized synchronous switch, xpipes Lite [21], the asynchronous
switch demonstrated signiﬁcant beneﬁts: 71% reduction in switch
area, 60-85% reduction in overall power, and a 44% average reduction
in energy per ﬂit, in the same technology. Unlike a number of prior
asynchronous NoC’s, which have been targeted to low or moderate
throughput with high router node latency and area [4, 17], this switch
also exhibited high throughput and relatively low latency.
This paper uses the above approach as a starting point, and demonstrates a new strategy to obtain considerable system-level latency improvement, while retaining many of the existing beneﬁts. System
latency has now become a fundamental challenge and bottleneck in
NoC’s, especially those targeted to cache and memory interfaces in
high-performance multicore systems [19, 10], but also for performanceoriented embedded systems. While many early NoC’s used conservative 3- and 4-cycle latency router nodes and simple routing protocols [5], a variety of recent synchronous strategies have been proposed for latency acceleration [15, 22, 13, 1, 6, 16] (see Related
Work). While several demonstrate effectiveness in latency improvements, most require major hardware resource allocation to support
such acceleration on a per-clock cycle basis: multiple extra virtual
channels (VCs), hybrid conﬁguration networks requiring additional
planes, and wide control or monitoring channels.
Contribution. The contribution of this paper is a new NoC for latency acceleration, called AEoLiAN, Asynchronous Early-arbitration
approach for a Lightweight Accelerated Network. A low-overhead
monitoring network is introduced, for a 2D-mesh, which allows arbitration and channel allocation to be performed in advance of a packet
through the network. A ﬁne-grain protocol deﬁnes the internal coordination of the monitoring control with a node’s datapath, as well as
between monitoring units in adjacent nodes, providing safe synchronization in both lightly-loaded and contentious scenarios.
The new router design includes a major transformation of an earFigure 1: Asynchronous communication: (a) 4-phase RZ protocol;
(b) 2-phase NRZ protocol; (c) Single-rail ""bundled data""
lier lightweight non-accelerated baseline design [9]. In addition, it
introduces a fundamentally different early arbitration protocol from a
recent well-known approach, SMART NoC [13]. The latter requires
substantial extra network resources, including VCs and link resources,
to enable a complete multi-hop channel reservation for a packet in
the preceding clock cycle.
In contrast, our approach exploits ﬁnegrain asynchronous operation, which allows rapid “sub-cycle” preallocation decisions, on a hop-by-hop basis, resulting in lightweight
dynamic resource allocation as a packet advances. Interestingly, given
the decoupled and ﬁne-grain operation of the new asynchronous NoC,
only a narrow increase in channel width – 11 extra bits per channel –
is needed to support our monitoring network, without the need to accommodate worst-case notiﬁcation scenarios within each clock cycle
as in synchronous approaches.
The AEoLiAN network was implemented and simulated on 6 diverse synthetic benchmarks in an 8x8 2D mesh network topology.
Uniform improvements in system latency are obtained over all the
benchmarks and over a wide range of injection rates. Pre-allocation
was shown to be nearly completed in advance, with only slight overlap, with the arrival of packet headers. Interestingly, the acceleration
technique also provided signiﬁcant throughput gains for several of the
benchmarks, by reducing network congestion. A small number of additional experiments were also performed by perturbing packet size,
network dimension and link length. The new NoC shows almost identical, or slightly degraded latency improvements.
Related work. There has been much work on improving system latency in NoC’s. This research is divided roughly into two categories:
(i) acceleration techniques, and (ii) pre-allocation approaches.
Several acceleration techniques have been proposed, using a variety of strategies. An early single-cycle router in [18] speculatively
allocates the switch as soon as a packet arrives at the router without any arbitration, assuming no other packets complete for the same
output channel. However, it is only effective under congested scenarios where the input buffer contains adjacent ﬂits, otherwise it relies on prediction. Router bypassing techniques have been also proposed in two recent approaches [15, 16], which provide additional
dedicated high-speed VCs to enable single-cycle transmission. However, both networks are evaluated based on a default slow 3-cycle synchronous switch, unlike ours which uses a low-latency asynchronous
baseline. In addition, signiﬁcant restrictions are imposed in their approach: packets can only be accelerated within a small segment of
3-4 hops, then have to regain high-speed VCs for the next segment;
acceleration is not allowed across turns; and both networks require
packet priorities (either dynamically [15], or statically [16]). In more
recent work, WaveSync NoC [22] uses source-synchronous routing
in a multi-synchronous domain, where the source clock is propagated
on channels, which is a different focus from ours; it proposes acceleration techniques to avoid synchronization in non-congested router
nodes. As an emerging technique, a mm-wave hybrid NoC [6], based
on a small-worlds topology, uses wireless channels to connect a subset of long-range multi-hop links for acceleration, however it relies
the use of emerging technologies which are not yet fully available and
which require high-cost transceivers.
Closer to our work, a number of approaches use early information
and aim for rapid pre-allocation. The SMART NoC [13] uses clockless repeated links to pre-allocate channels for multiple-hops simultaneously, allowing packets to potentially traverse multiple routers in
one cycle. However, it requires wide monitoring channels with extra
VCs to obtain the full beneﬁts of the approach. In particular, it includes an example (Fig. 7 [13]) with 24 distinct smart links emanating
from each router, each with 2-4 address bits, and 12 VCs are proposed
to obtain maximal beneﬁts for its experimental results. Deja Vu [1]
uses a hybrid network and requires an expensive dedicated monitoring plane, with the same channel width as the data plane, to support
early arbitration; also, the environment must inject monitoring information into the network earlier than the data, unlike our proposed apFigure 2: Baseline Output Port Module (OPM) architecture [9]
proach. The approach of [14] uses “advanced bundles” to set switch
allocation one cycle in advance; however, it builds on a slow 3-cycle
baseline switch, and reported network-level performance had minimal latency improvement from 10-40% injected load and only modest improvements at other rates. Finally, a recent asynchronous NoC
with early arbitration [8], targets a simple topology (Mesh-of-Trees)
and routing scheme, i.e. a single path for each source-sink pair. In
contrast, the proposed network addresses a more challenging design
problem: a higher-radix switch with non-trivial routing, which can
be used in a common 2D-mesh topology, and includes fundamentally
new monitoring protocols and components.
2. Background
This section reviews background on asynchronous communication, as well as a recent asynchronous NoC 5-ported router design.
Asynchronous communication. Two common asynchronous handshaking protocols are used to synchronize a sender and receiver: (i) a
four-phase protocol (return-to-zero [RZ]) in Fig. 1(a), and (ii) a twophase protocol (non-return-to-zero [NRZ], also known as transitionsignalling) in Fig. 1(b).
In a four-phase protocol, the two signals
are asserted and de-asserted in turn, while in a two-phase protocol,
a single toggle occurs on each signal with no return-to-zero phase.
Fig. 1(c) shows a common asynchronous data encoding approach,
called single-rail bundled data. A standard synchronous-style data
channel is used, using a binary data encoding. One extra req wire is
added, serving as bundling or local strobe, which sends a clean transition to the receiver after all data bits are stable and valid. Interestingly,
the bundling scheme allows arbitrary glitches on the data channel, as
long as data becomes stable and valid before the req signal is transmitted. To enforce the one-sided timing constraint, a small matched
delay must be added, either an inverter chain or carefully replicated
portion of the critical path.
Basic asynchronous 5-ported switch [9]. A low-overhead asynchronous router node, with no latency acceleration, provides the baseline on which the new router is developed. The 5-ported router is
based on the xpipes Lite synchronous switch architecture [21]. It is
a VC-less design, using destination addressing and X-Y routing. The
router has two components: Input Port Modules (IPMs) and Output
Port Modules (OPMs). Five IPMs are connected through the crossbar to ﬁve OPMs, using a standard organization [5]. Each component is based on Mousetrap asynchronous pipelines [20], which use a
normally-transparent capture-pass protocol with single-level D-latch
registers. An IPM has a single input channel and four output channels, while an OPM has four input channels and one output channel.
The IPM computes the current node’s routing information, and propagates it to the designated OPM; input data is also broadcast 4-way to
all OPMs. The role of the OPM is to (i) use the routing information
to identify a valid request, (ii) resolve arbitration between competing
requests, and (iii) allocate the designated output channel.
Fig. 2 shows the microarchitecture of the OPM. An asynchronous
4-way mutual exclusion element (mutex) performs arbitration, and a
4-way MUX selects the appropriate input data from the crossbar. The
Tail Detector detects when a tail ﬂit is sent out. The right data register
uses a capture-pass protocol for ﬂow control, and other components
manage handshaking with crossbar and output channel, as well as reset of the internal components (see [9] for details).
3. Overview of the Approach
The section introduces the basic strategy of AEoLiAN, and highlights the key features that are distinct from a previous baseline asyntegration of monitoring capability and an entirely new monitoring/datapath control, along with new protocols for synchronization of the
monitoring and datapath. The basic structure and operation of the
Input Port Module (IPM ) and Output Port Module (OPM ) are ﬁrst
introduced, in turn. Each contains a lightweight monitoring control
and the datapath. Then the monitoring network is presented in detail,
including a more detailed view of the control components residing in
the IPM and OPM. The section closes with timing analysis.
4.1 Input Port Module Architecture
An Input Port Module, shown in Fig. 3, routes the packet to the
selected Output Port Module. It has a single input channel from the
upstream neighbor, and four output channels that connect to 4 different Output Port Modules through a crossbar. Each channel is now
augmented by a narrow monitoring sub-channel. Monitoring information typically arrives earlier than actual data, and initiates both early
arbitration and channel pre-allocation.
Structure. An Input Port Module is divided into two independent
blocks – the monitoring control on top, and the datapath at bottom.
The datapath consists of an input register, four Data Request Control units (one for each output port), and a Data Ack acknowledgment
generator for the input channel. The datapath allows a packet to enter
the IPM as soon as it arrives at the input channel, through the capturepass input register. The packet is then quickly broadcast to all OPMs,
with corresponding requests sent out by the four Data Request Controls. However, only the selected OPM will accept the data. All the
remaining requests will be cancelled.
The monitoring control is part of the entire monitoring network. It
consists of four Monitoring Req Controls on the right, each communicating with a different OPM, and a Monitoring Ack Control on the
left, for handshaking with the predecessor router. Both control units
use 4-phase handshaking protocols, and are initialized to the all-0 (i.e.
reset) state. In more detail, the Monitoring Req Control initiates early
arbitration as soon as the advance notiﬁcation arrives, which includes
the destination address and the lookahead information. It then asserts
the appropriate monitoring req to one of the OPMs, and also forwards
the packet destination along with the request. Only one of the control
units is activated at a time. The unit serves two purposes: (i) a FIFO
stage and ﬂow control for the monitoring network, to maintain the
monitoring information until the successor router safely stores it; and
(ii) synchronization with datapath, to keep the monitoring request asserted throughout the processing of the entire packet. The Monitoring
Ack Control simply merges the four monitoring reqouts and generates
the acknowledgment to the predecessor, allowing a new monitoring
token to arrive.
Operation. The basic operation of the IPM is illustrated by a friendly
scenario with a single packet and no contention. Without loss of
generality, this packet is assumed to be routed to OPM0.
Initially,
all handshaking signals are zeros for both monitoring and data channels. In particular, a 1-hot encoding is used for the lookahead routing address, Route-sel-in; this DI code serves as a bundling signal
for the single-rail Destination-in address. First, monitoring arrives
on the input channel before the actual data, with an asserted Routesel-in[0] signal. The Monitor Req Ctl0 immediately asserts Monitorreqout0 high, and passes it, along with the destination address, to
OPM0. Then acknowledgment is generated to the left. In friendly
case, the entire 4-phase monitoring handshaking completes quickly
on the left channel. In parallel, on the right channel, OPM0 acknowledges on Monitor-ackin0, indicating early arbitration and channel preallocation are completed, and monitoring information has also been
received by the downstream successor. Note that Monitor-reqout0 still
remains high. Next, the actual packet arrives. The header is broadcast
to all four OPMs, similar to the approach in [9]. Since the designated
channel is already pre-allocated in OPM0, the header is immediately
sent to the next router. Each body ﬂit is processed the same as the
header. Eventually the tail ﬂit arrives. After it is sent out, the OPM0
asserts Tail-Passed0 high, indicating the entire packet has been processed. At this point, Monitor-reqout0 ﬁnally goes low, allowing the
release of the arbiter and output channel. Finally, Monitor-ackin0 and
Tail-Passed0 are de-asserted low almost simultaneously, thereby completing the entire operation.
In a contentious case, with two back-to-back incoming packets, the
new monitoring token can arrive before Monitor-reqout is de-asserted
for the ﬁrst packet. However, the second request is not generated until
the previous Monitor-reqout is de-asserted, even if it would request a
different OPM, to avoid malfunction in the router.
Figure 3: Proposed Input Port Module (IPM) architecture
chronous NoC [9] with no early arbitration capability.
The new NoC consists of two sub-networks: a lightweight and
narrow monitoring network for fast forwarding the early notiﬁcation
of incoming data, and the datapath network for normal packet processing. Once a packet enters the network, its destination address in
the header ﬂit serves as the monitoring information, and is rapidly
advanced along the expected routing path through the monitoring network. Every router on the path performs early arbitration and channel
pre-allocation based on the receipt of this information. Typically, the
data only waits for a very short period in each router after it arrives,
before the channel is completely allocated. In contrast, in the earlier
NoC [9], arbitration is not allowed to start until the actual data arrives
at each router, resulting in a long data wait time.
Lookahead routing is combined with the monitoring network, to
further speed up its forward latency. A non-classical approach is proposed, which has higher parallelism to improve performance. There
are two key distinctions. First, in a classical scheme [18], a packet
must arrive at the current node before it initiates computation of lookahead routing for the successor node, while in the proposed approach
the computation is initiated earlier – just after the advance monitoring arrives. Second, the classical approach begins lookahead computation after switch allocation in the current node, while the proposed
approach begins the computation in parallel with early arbitration. As
a result, the above new protocol enhancements provide signiﬁcantly
accelerated performance.
In case of contention, early arbitration is performed on a routerby-router basis. That is, if the monitoring loses the early arbitration
at a certain router node, it is not allowed to further propagate, until it
ﬁnally wins. The corresponding data packet also has to wait until the
monitoring wins the arbitration and allocates the channel.
In both cases, the monitoring proceeds as an arbitration ""wave"",
by winning early arbitration at each router on the expected path. In
other words, the monitoring network decides the processing order for
input data streams in advance, and the actual data simply follows and
replays the pre-determined order. This ﬁne-grain “router-by-router”
early arbitration mechanism enables a simple design, which eliminates the need for over-provisioning the network as is required in
some recent synchronous approaches, which use extra VCs to support
multi-hop single-cycle transmissions, or expensive hybrid multi-plane
networks. Instead, the ﬁne granularity of asynchronous operation (in
this paper, a router node can forward monitoring in little more than
200 ps), allows “sub-cycle” pre-allocation decisions on a hop-by-hop
basis, using limited channel resources.
Different handshaking protocols are used in the monitoring network vs. datapath. Data channels retain a 2-phase NRZ handshaking
protocol with single-rail bundled data encoding, as in [9], to allow a
high processing rate. On the other hand, the monitoring channels only
operate on a per-packet basis, and use a 4-phase RZ protocol, with the
packet destination encoded using single-rail bundled data. The use of
the 4-phase protocol simpliﬁes the monitoring control, without sacriﬁcing performance of the entire NoC in most of cases.
4. The New Router Node Design
The new asynchronous 5-port router builds on our earlier lightweight design with no early arbitration capability [9], including the inarrive almost simultaneously. Lookahead routing information will be
computed in parallel for both. Based on the arbiter result, the winner
processes the packet following the same procedure as in single-packet
case. The losing channel, though it completes lookahead computation, does not send out this information.
Its data packet must also
wait, as its channel is not allocated. Eventually, the ﬁrst packet is
processed and releases the mutex. The losing channel then immediately continues its operation by winning the arbitration, allocating the
channel, and forwarding lookahead information.
4.3 The Monitoring Network
In this section, the operation of the monitoring network are presented in detail. Unlike the earlier baseline, monitoring and control
functions are seamlessly merged.
System-level operation. When a packet enters the network, a corresponding monitoring token is generated at the source node, and
injected into the monitoring network. The token contains the destination address of the packet, and the lookahead routing information
computed by each router on the path. Considering a single sourcedestination path, the monitoring network is effectively a linear FIFO,
with extra synchronization with datapath. A monitoring token has to
be kept until (i) the next FIFO stage (i.e. the successor router) safely
stores it, and (ii) the corresponding packet is entirely processed by
the current router. The monitoring FIFO can only store one token
per router, only half capacity of the datapath. However, because each
packet only requires a single monitoring token, the capacity of the
monitoring network is sufﬁcient in the case of multi-ﬂit packets. By
having fewer number of pipeline stages, the monitoring network is
optimized for area and latency, without sacriﬁcing performance.
Node-level operation. The monitoring operation inside a single router
is now considered. In particular, it involves the interaction between
IPMs and OPMs across the crossbar. The monitoring channel connecting an IPM and an OPM effectively uses a modiﬁed 4-phase
handshaking protocol. After the IPM sends out a request, the OPM
has to acknowledge three separate events: (i) early arbitration is resolved and channel pre-allocation is done; (ii) the successor router
has received and safely stored the monitoring token; and (iii) the
corresponding packet has been processed and the monitoring token
becomes stale. Event (i) and (ii) are acknowledged using a merged
monitor-ack signal, while event (iii) is indicated by Tail-passed. After both acknowledgments are received, which can be in either order,
the monitor-reqout is reset, followed by monitor-ack and Tail-passed
de-asserted almost simultaneously. Then, a new monitoring token is
allowed to be sent out, if one is pending.
4.4 Local Input and Output Port Modules
The previous subsections are focused on a typical IPM and OPM.
In a 5-port router, only four of the ﬁve IPMs/OPMs are typical, and
the remaining one IPM/OPM needs to communicate with the network
interface – namely the local IPM/OPM. The local IPM generates the
monitoring information for each packet entering the network, and
serves as the starting point of its entire monitoring path. The local
OPM terminates the monitoring from further propagation, as there is
no successor router. Detailed structure and operation for local modules are not included due to space limitations.
Figure 4: Proposed Output Port Module (OPM) architecture
4.2 Output Port Module Architecture
An Output Port Module, shown in Fig. 4, arbitrates between multiple incoming data streams that try to access the associated output
channel. By symmetry, it has four input channels and one output channel, with each channel containing a monitoring sub-channel. Upon
receiving early monitoring requests from the IPM, the OPM starts arbitration and channel allocation, well in advance.
Structure. The OPM also consists of a monitoring control and datapath, which synchronize with each other. The structure of the datapath
is similar to [9], as described in the Background section.
The new monitoring control contains three important components:
(i) the Packet Route Pre-Computation Unit computes the lookahead routing information for the successor router node. The unit supports parallel computation for all the incoming monitoring channels,
and select one of them based on the results of early arbitration. The
routing information calculation is done in parallel with arbitration,
and therefore its latency cost is entirely removed from the critical path.
In the proposed design, a simple X-Y routing is used. However, other
types of routing algorithms can also be supported.
(ii) the Monitor Output Control quickly forwards the lookahead
routing information from the Packet Route Pre-Computation Unit to
the monitoring output channel. It is also a decoupling unit, enforcing
monitoring ﬂow control, which delays sending out a monitoring token
until the monitoring output channel completes its previous 4-phase
communication with the successor router node, Effectively, it allows
the router to prepare the next monitoring input, in case the output
channel incurs a long communication delay.
(iii) the Monitor Ack Control is simply a de-mux to route a handshaking acknowledgment from the downstream router back to the correct IPM.
Operation. A similar single-packet friendly case is illustrated for basic operation. The packet is assumed to arrive from IPM0. First, monitoring arrives from IPM0, with an asserted bundling request Monitorreqin0, which is kept high throughout the processing of the packet.
Two operations are initiated simultaneously: early arbitration and lookahead routing computation. The mutex resolution and the completion
of pre-computation are synchronized, before the monitoring information is sent out through the Monitor Output Channel Control. At this
point, latch L1 is opened, i.e.
the output channel is pre-allocated.
In friendly case, the monitoring output channel on the right responds
quickly, and completes the entire 4-phase handshaking without any
other synchronization. When the actual packet arrives, the header ﬂit
is sent out as soon as L1 opens, through the capture-pass output data
register. The channel remains allocated; each body ﬂit is processed
exactly the same as the header ﬂit. Finally, after the tail ﬂit is placed
onto the output channel. Tail-Passed0 is asserted high. In turn, IPM0
de-asserts Monitor-Reqin0 low, and releases the mutex. The entire
operation completes with de-assertion of Tail-Passed0 and Monitorackout0.
In case of contention, two monitoring requests from different IPMs
4.5 Timing Analysis
The new added monitoring network is carefully designed to contain only a few one-sided timing constraints, shown below, where both
racing paths involve a global channel or intra-node crossbar communication. The remaining constraints involve a fast local path vs. a
slower global channel or crossbar communication, and can be simply satisﬁed. The remaining timing constraints in the datapath must
still be satisﬁed; several of these constraints are discussed in [9], for
parts of the datapath that are invariant between the baseline and new
designs. Note that single-rail bundled data asynchronous NoC’s have
been demonstrated as robust and correctly implemented, including at
post-layout levels, in several recent designs [11, 9].
Input Bundling. A standard bundling timing constraint has to be
satisﬁed f"
ANN Based Admission Control for On-Chip Networks.,"We propose an admission control method in Network-on-Chip (NoC) with a centralized Artificial Neural Network (ANN) admission controller, which can improve system performance by predicting the most appropriate injection rate of each node via the network performance information. In the online control process, a data preprocessing unit is applied to simplify the ANN architecture and make the prediction results more accurate. Based on the preprocessed information, the ANN predictor determines the control strategy and broadcasts it to each node where the admission control will be applied. Compared to the previous work, our method builds up a high-fidelity model between the network status and the injection rate regulation. The full-system simulation results show that our proposed method can enhance application performance by 17.8% on average and up to 23.8%.",
An Energy-Efficient Network-on-Chip Design using Reinforcement Learning.,"The design space for energy-efficient Network-on-Chips (NoCs) has expanded significantly comprising a number of techniques. The simultaneous application of these techniques to yield maximum energy efficiency requires the monitoring of a large number of system parameters which often results in substantial engineering efforts and complicated control policies. This motivates us to explore the use of reinforcement learning (RL) approach that automatically learns an optimal control policy to improve NoC energy efficiency. First, we deploy power-gating (PG) and dynamic voltage and frequency scaling (DVFS) to simultaneously reduce both static and dynamic power. Second, we use RL to automatically explore the dynamic interactions among PG, DVFS, and system parameters, learn the critical system parameters contained in the router and cache, and eventually evolve optimal per-router control policies that significantly improve energy efficiency. Moreover, we introduce an artificial neural network (ANN) to efficiently implement the large state-action table required by RL. simulation results using parsec benchmark show that the proposed rl approach improves power consumption by 26%, while improving system performance by 7%, as compared to a combined PG and DVFS design without RL. additionally, the ann design yields 67% area reduction, as compared to a conventional RL implementation.",
Lightweight Mitigation of Hardware Trojan Attacks in NoC-based Manycore Computing.,"Data-snooping is a serious security threat in NoC fabrics that can lead to theft of sensitive information from applications executing on manycore processors. Hardware Trojans (HTs) covertly embedded in NoC components can carry out such snooping attacks. In this paper, we first describe a low-overhead snooping invalidation module (SIM) to prevent malicious data replication by HTs in NoCs. We then devise a snooping detection module (THANOS) to also detect malicious applications that utilize such HTs. Experimental analysis shows that unlike state-of-the-art mechanisms, SIM and THANOS not only mitigate snooping attacks but also improve NoC performance by 48.4% in the presence of these attacks, with a minimal ~2.15% area and ~5.5% power overhead.",
Sparse 3-D NoCs with Inductive Coupling.,"Wireless interconnects based on inductive coupling technology are compelling propositions for designing 3-D integrated chips. This work addresses the heat dissipation problem on such systems. Although effective cooling technologies have been proposed for systems designed based on Through Silicon Via (TSV), their application to systems that use inductive coupling is problematic because of increased wireless-communication distance. For this reason, we propose two methods for designing sparse 3-D chips layouts and Networks on Chip (NoCs) based on inductive coupling. The first method computes an optimized 3-D chip layout and then generates a randomized network topology for this layout. The second method uses a standard stack chip layout with a standard network topology as a starting point, and then deterministically transforms it into either a “staircase” or a “checkerboard” layout. We quantitatively compare the designs produced by these two methods in terms of network and application performance. Our main finding is that the first method produces designs that ultimately lead to higher parallel application performance, as demonstrated for nine OpenMP applications in the NAS Parallel Benchmarks.",
Surf-Bless - A Confined-interference Routing for Energy-Efficient Communication in NoCs.,"In this paper, we address the problem of how to achieve energy-efficient confined-interference communication on a bufferless NoC taking advantage of the low power consumption of such NoC. We propose a novel routing approach called Surfing on a Bufferless NoC (Surf-Bless) where packets are assigned to domains and Surf-Bless guarantees that interference between packets is confined within a domain, i.e., there is no interference between packets assigned to different domains. By experiments, we show that our Surf-Bless routing approach is effective in supporting confined-interference communication and consumes much less energy than the related approaches.",
Effect of Distributed Directories in Mesh Interconnects.,"Recent manycore processors are kept coherent using scalable distributed directories. A paramount example is the Xeon Phi Knights Landing. It features 38 tiles packed in a single die, organized into a 2D mesh. Before accessing remote data, tiles need to query the distributed directory. The effect of this coherence traffic is poorly understood. We show that the apparent UMA behavior results from the degradation of the peak performance. We develop ways to optimize the coherence traffic, the core-to-core-affinity, and the scheduling of a set of tasks on the mesh, leveraging the unique characteristics of processor units stemming from process variations.",
DCFNoC - A Delayed Conflict-Free Time Division Multiplexing Network on Chip.,The adoption of many-cores in safety-critical systems requires real-time capable networks on chip (NoC). In this paper we propose a new time-predictable NoC design paradigm where contention within the network is eliminated. This new paradigm builds on the Channel Dependency Graph (CDG) and guarantees by design the absence of contention. Our delayed conflict-free NoC (DCFNoC) is able to naturally inject messages using a TDM period equal to the optimal theoretical bound and without the need of using a computationally demanding offline process. Results show that DCFNoC guarantees time predictability with very low implementation cost.,
O-Router - an optical routing framework for low power on-chip silicon nano-photonic integration.,"In this work, we present a new optical routing framework, O-Router for future low-power on-chip optical interconnect integration utilizing silicon compatible nano-photonic devices. We formulate the optical layer routing problem as the minimization of total on-chip optical modulator cost (laser power consumption) with Integer Linear Programming technique under various detection constraints. Key techniques for variable number reduction and routing speedup are also explored and utilized. O-Router is tested on optical netlist benchmarks modified from top global nets of ISPD98/08 routing benchmarks. O-Router experimental results are compared with conventional minimum spanning tree algorithm, demonstrating an average of over 50% improvement in terms of total on-chip optical layer power reduction.","O-Router: An Optical Routing Framework for Low Power
On-chip Silicon Nano-Photonic Integration
Duo Ding, Yilin Zhang, Haiyu Huang, Ray T. Chen and David Z. Pan
ECE Dept. Univ. of Texas at Austin, Austin, TX 78712
{ ding, yzhang1, dpan }@cerc.utexas.edu, haiyu@mail.utexas.edu, chen@ece.utexas.edu
17.2
ABSTRACT
In this work, we present a new optical routing framework,
O-Router for future low-power on-chip optical interconnect
integration utilizing silicon compatible nano-photonic devices. We formulate the optical layer routing problem as
the minimization of total on-chip optical modulator cost
(laser power consumption) with Integer Linear Programming technique under various detection constraints. Key
techniques for variable number reduction and routing speedup are also explored and utilized. O-Router is tested on
optical netlist benchmarks modiﬁed from top global nets
of ISPD98/08 routing benchmarks. O-Router experimental results are compared with conventional minimum spanning tree algorithm, demonstrating an average of over 50%
improvement in terms of total on-chip optical layer power
reduction.
Categories and Subject Descriptors
B.7.2 [Hardware, Integrated Circuit]: Design Aids
General Terms
Algorithms, Design, Performance
Keywords
Optical Routing, Low Power Nanophotonic Integration, Integer Linear Programming
1.
INTRODUCTION
As raised in the International Technology Roadmap for
Semiconductors [8], silicon system complexity rockets exponentially due to increasing transistor counts, fueled by
smaller feature sizes and increasing demands for higher integration / performances with lower costs. Consequently,
interconnect design becomes more and more important for
DSM VLSI as technology further scales down, among which
on-chip optical interconnect is a potential quantum leap towards next-generation technology. Ever since its ﬁrst introduction by Goodman in [5], the concept of on-chip optical interconnect has attracted more and more attention
over the years in industry (e.g., [9, 15]) as well as academia
(e.g., [3,12,14]), with ma jor focus on device fabrication level.
As analyzed and pro jected in [2], on-chip optical interconnect outperforms traditional copper interconnect in power,
throughput and delay with apparent gain below 22nm technology node starting from 2016.
As one of the most promising device level break-through
for on-chip optical integration, silicon compatible nano photonic devices (e.g., [11, 15]) take advantage of optical properties of a signal, characterizing great resilience in terms
of small delay, low power and high throughput when compared with traditional copper interconnection. Advances in
device level improvements of silicon nano photonics (such as
photonic crystal structures in [7, 16]) have also been demonstrated. In recent years, low RF power optical modulators
operating at a few Gbps speed have been successfully demonstrated [6,7], with compact footprint for potential large scale
on-chip integration. Compact photodetecters with up to
50Gbps processing rate have also been demonstrated (such
as Germanium-on-Insulator photodetecter in [10]). With
a proper collection of current Silicon nano-photonic devices
and some extended pro jections / assumptions based on [2,8],
there can be exciting CAD synthesis explorations in interconnect planning for optical on-chip integration.
As a related work, [13] studied timing driven and congestion driven on-chip optical routing CAD algorithms under
3-D system-on-package scenario. Yet the routing geometry
in [13] was formulated in a very simple manner: point-topoint straight connection, which also means there is at least
1 optical modulator inserted at each pin and Steiner point in
the netlist. There are 3 ma jor issues with such an approach:
First,
it neglects the laser power consumption of optical
modulators. Since each modulator requires a laser source for
electrical-to-optical data conversion, this approach results in
a very power consuming chip; Second, it neglects the photonenergy loss constraint on optical interconnect; consequently,
there could be pins whose received photon-energy drop below the photo-detectors’ detection threshold, leading to inevitable malfunction after optical-to-electrical data converElectrical
Logic Cell
Driver
Optical
Modulator
On-chip
metal layers
transmitter
receiver
Optical Waveguide
Optical layer
laser
Electrical
Logic Cell
Amplifier
Photon
Detector
Figure 1: Block diagram for electrical/optical and optical/electrical data conversions
264
sion. Third, optical routing has very diﬀerent characteristics
compared with conventional electrical (copper) interconnect
routing, therefore, special routing geometry must be developed to tackle optical interconnect planning problems. In
other words, total laser power consumption (proportional to
number of modulators inserted) and the constraints for successful optical-to-electrical detection must both be addressed
properly for optimized optical routing geometry.
In this work, we present O-Router, an optical routing
framework that takes into consideration of various constraints
and ﬂexibilities that silicon nano-photonics device libraries
and optical waveguide models shall impose on the future onchip optical interconnect. O-Router is driven by low power
on-chip silicon nano-photonic integration.
The rest of the paper is organized as follows: Section 2 introduces some preliminaries regarding optical and electrical
data conversions and silicon photonics, followed by a motivational example and a summary list of key contributions
of this paper. Section 3 describes our Optical Interconnect
Library built for O-Router ; Section 4 focuses on the optical
routing Integer Linear Programming (ILP) problem formulation and speed-up techniques, followed by experimental
results in Section 5. Section 6 concludes the paper by a
brief summary and some potential future work.
2. PRELIMINARIES AND MOTIVATION
As shown in Fig. 1, on the transmitter’s side, the electric
signal from the driver (electrical layer) amplitude modulates
the light source from the laser inside an optical modulator,
and then send the modulated optical signal onto optical interconnect (optical waveguide on optical layer); on the receiver end, a photo-detector detects the photons from the
waveguide and converts it into electric signal (back to electrical layer); an ampliﬁer may be needed if this signal drives
a high fan-out net on electric layers.
Bending loss
r
R
Bending loss
Line Propagation loss
coupling loss
Optical   interconnect
Optical   coupler
Figure 2: Sources of loss for on-chip optical routing
2.1 Optical Waveguide Routing
As aforementioned, optical routing has unique characteristics when compared with traditional copper routing. Manhattan (X/Y) routing based algorithms are not favored on
optical layer because of the huge amount of loss caused by
the sharp wire turnings along the data path, unless some
special structures be inserted; yet these structures are usually costly in fabrication and/or bulky in footprint size, etc.
O-Router performs gridless optical routing with waveguide couplings/crossings on a single layer. As a result, routing geometry becomes very ﬂexible, with diﬀerent geometries and penalties according to their respective optical interconnect loss. In order to further explore optical routing
geometry, we deﬁne the following 3 types of losses (with dB
unit) on an optical interconnect path in equations 1- 5.
Lloss = α · lengthpath
(1)
265
p1-3
p2-1
p1-1
p1-2
p1-3
via
M1
M2
p2-1
p1-1
p1-2
(a)
pin2-1
(b)
pin2-1
Metal Layer
pin1-1
pin1-2
pin1-3
Metal Layer
pin1-1
pin1-2
pin1-3
silicon wire
Optical Layer
Si
SOI
silicon wire
Optical Layer
Si
SOI
P2-1
P1-1
P1-3
P1-2
(c)
P2-1
P1-3
P1-1
P1-2
(d)
P2-1
P1-3
P2-1
P1-3
P1-1
P1-2
(e)
P1-1
P1-2
(f)
- electrical buffer
- optical modulator
- photo-detector
- electrical via
- silicon wires
- optical coupler
Figure 3: Motivational example for electrical routing v.s
optical routing
Bloss = β · θ · r
−η
Closs = γ · N umcouplers
Ploss = Lloss + Bloss
T otalloss = Ploss + Closs
(2)
(3)
(4)
(5)
As shown in Fig. 2, Lloss is straight line waveguide loss,
it is proportional to the length of optical interconnect, with
a coeﬃcient α; Bloss is the bending loss, since waveguide
cross-section width is negligible compared to the bending
radius in O-Router, we assume Bloss to be proportional to
the degree of the optical interconnect (silicon waveguide)
arc angle θ, and inversely proportional to the radius r of
the interconnect, with an index η ; Closs is the coupling loss,
proportional to the number of couplers (crossings) on the
interconnect, with a coeﬃcient γ . All related coeﬃcients
are determined by our Optical Interconnect Library, which is
built for O-Router and will be explained further in Section 3.
2.2 Motivational Example
In this section, we brieﬂy explore the diﬀerent trade-oﬀs
for optical routing. As shown in Figure. 3, there are 2 nets
to be routed on a chip, noted as pini-j, meaning it is the jth
member of net i ; Fig. 3(a) and (b) shows two alternatives for
conventional routing on electrical layer with buﬀers and/or
metal via inserted to alleviate the timing penalty caused by
the long wires across the chip. Buﬀers are inserted since
RC delay increases quadratically with electrical wire length.
Yet buﬀer insertion is not all-powerful technique. Generally speaking, cross-chip timing critical nets are tough to ﬁx
thus impose great diﬃculty to VLSI design timing closure.
As technology further scales down and system integration
level rockets, issues with electrical interconnect will get more
severe.
Fig. 3(c)-(f ) show 4 possible routing geometries for the
2 nets on optical layer, according to our optical routing.
Routing geometry (c) requires a total of 2 optical modulators: 1 inserted at P1-1, 1 inserted at P2-1, while for (d), 1
extra modulator will be inserted at P1-2, in order to drive
P1-3, since sharp turning at P1-2 is either too lossy or too
costly to ﬁx other than using an extra modulator. In (e) and
(f ), optical coupler is introduced for coupling optical signal
across 2 wires, with certain amount of loss. In these 2 cases,
couplers can be employed either because doing so results in
less amount of loss than taking detours as in (c) and (d),
or because taking detours results in more coupling loss with
other nets on chip, etc.
We can learn that geometries (c)(e) result in least among
of modulating power among (c)-(f ), yet optical interconnect
bending loss: Bloss is also introduced, as well as the coupling loss: Closs (in (e)) so that the constraint for successful
detection at P1-3 may be violated due to too much loss on
interconnect. To optimally pick the best routing geometry
from the (c)-(f ) 4 cases is the motivation of O-Router.
O-Router targets at ﬁnding optimal optical routing geometry to minimize total modulating power, sub ject to various
constraints imposed by the device characterizations.
2.3 Main Contributions of This Paper
Main novelty and contributions of O-Router are as follows:
• Based on extensive data collection and road-mapping,
we pro ject the technology trends of on-chip silicon
nano-photoincs and build OIL: an Optical Interconnect Library characterized for low-power on-chip integration/synthesis.
• For the ﬁrst time, we formulate the optical routing
problem by taking into considerations of various detection constraints and ﬂexibilities that OIL imposes
on the future optical interconnect.
• Under gridless single layer optical routing with couplings/crossings, the solution space is theoretically inﬁnite. To reduce solution space without losing optimality, we put a set of constraints on the waveguide
routing rules and formulate the optical routing problem with Integer Linear Programming.
• We also propose several key techniques to speed-up the
optical routing framework under ILP formulation.
3. OPTICAL INTERCONNECT LIBRARY
To support our O-Router framework, we ﬁrst build an
Optical Interconnect Library (OIL), which includes a MachZehnder optical modulator [6], a photo-detector from [10], a
fully simulated optical coupler using Rsoft [1], and a set of
optical interconnect (silicon waveguide) model. For details
regarding OIL, please refer to [4].
3.1 Optical Modulator and Photo-detector
Based on some related research (e.g., [2, 8, 12]), we pro ject
current OIL parameters towards next generation technology, which essentially enables better on-chip integration for
nanophotonic devices.
In Table 1, there are 2 sizes of modulators included, one
is a normal modulator; the other is ModulatorX: a large
modulator with 10X driving power, which will be inserted
into a net that suﬀers greatly from power losses in order to
Port A
Port C
Coupler
high
Port B
Case (a)
Case (b)
Case (c)
Case (d)
low
Figure 4: Working mechanism of optical coupler in O-Router
Port D
Table 1: Ma jor OIL components with high level parameters.
throughput
length
width
modul1 >10Gps <50um <10um
modulX >10Gps <50um <50um
detector >10Gps <10um <10um
coupler >10Gps <50um <5um
driving
power
1X
10X
loss
<10%
guarantee successful detection. Since the power consumption of ModulatorX is much larger than normal modulator, its usage will be penalized with a constant coeﬃcient
M P owpenalty , details in Section 4.
Optical Layer
silicon wire
Si
d
w
Si
l
SOI     n = 1.46
Silicon Substrate     N = 3.46
Figure 5: OIL on-chip optical waveguide model
3.2 Optical Coupler and Interconnect Model
The working principle of optical coupler is shown by Fig. 4.
There are 4 ports from A to D for each coupler, and the parallel double interconnect region is the arm region. Optical
signals will be cross coupled in the arm region. From the
4 simulation cases, we can verify that PortA=PortD and
PortB=PortC always satisfy, as if there is wire connection
between A-D and B-C. Optical couplers allow us to make
full use of the optical layer routing space, making non-planar
netlists routable on a single silicon layer. In case (b)(c) in
Fig. 4, there is slight loss for high optical logic after the
coupler, as is formulated by Closs .
As shown in Fig. 5, the optical waveguide model included
in OIL has a reﬂective index of 3.46, coated on top of a
2um thick SOI layer (reﬂective index < 1.46). The crosssection width of the silicon wire w=0.5um, cross-section
height l=0.22um, wire spacing d between 0.5um and 3.0um.
d should be set properly to avoid wire cross-talk.
4. O-ROUTER FORMULATION AND ALGORITHM
Given the pin locations of certain placed netlist for optical routing, O-Router seeks optimal routing solution with
Integer Linear Programming to minimize total modulating
power, meanwhile satisfying various detection constraints
according to established OIL parameters. This section is
divided into three parts: First is the optical netlist mapping. This is when suitable optical netlist benchmarks for
O-Router are constructed. Second part is the core ILP formulation, followed by routing speed-up techniques in the
third part.
4.1 Optical Netlist Mapping
Given an electrical layer netlist after placement, the goal
of this step is to prepare an optical netlist that makes most
use of optical layer resource to ﬁx top timing critical nets
(i.e., longest) in electrical layer. For our ILP formulation,
the resulting optical netlist of this stage consists of only 2,
3 and 4 pin nets. It takes place in 3 phases:
Phase 1: Pre-select top timing critical nets from electrical layer to map onto optical layer. Shown in Fig. 6(a)(b),
non-timing critical nets 1/2/3 are not selected.
Phase 2: Cluster within each pre-selected net for routing
266
non- timing cr itical nets
(a)
(b)
(c)
(d)
timing cr itical nets
net_4_cluster1
pin4_4
pin5_1
pin5_2
net_1
net_2
net_3
pin4_3
pin4_2
pin4_1
pin4_4
pin4_5
pin4_6
pin6_1
pin6_2
pin6_4
pin6_3
pin6_5
pin6_1
pin6_2
pin7_2
pin7_1
pin7_3
pin8_1
pin8_2
timing cr itical nets
timing cr itical nets
Figure 6: Illustrations for optical netlist mapping
X1_0
X0_0
X0_1
X0_2
X2_0
No. mod = 2
No. mod=1 No. mod = 1
No. mod = 1
No. mod = 3
X1_1
No. mod = 2
X1_2
No. mod = 2
X1_3
No. mod = 1
X1_4
No. mod = 1
X1_5
No. mod = 1
X2_1
No. mod = 3
X2_2
No. mod = 3
X2_3
No. mod = 3
X2_4
No. mod = 3
X2_5
No. mod = 3
X2_6
No. mod = 3
X2_7
No. mod = 3
X2_8
No. mod = 3
X2_9
No. mod = 3
X2_10
No. mod = 3
X2_11
No. mod = 3
X2_12
No. mod = 3
X2_13
No. mod = 3
X2_14
No. mod = 3
X2_15
No. mod = 3
X2_16
No. mod = 2
X2_17
No. mod = 2
X2_18
No. mod = 2
X2_19
No. mod = 2
X2_20
No. mod = 2
X2_21
No. mod = 2
No. mod = 2
X2_22
No. mod = 2
X2_23
No. mod = 2
X2_24
No. mod = 2
X2_25
No. mod = 2
X2_26
No. mod = 2
X2_27
X2_28
No. mod = 2
X2_29
No. mod = 2
X2_30
No. mod = 2
X2_31
No. mod = 2
X2_32
No. mod = 2
X2_33
No. mod = 2
Figure 7: A list of optical routing geometries (represented by integer variables) for 2, 3 and 4 pin nets
eﬃciency enhancement. Since optical routing is most eﬀective dealing with global interconnect, we map a single pin
from each local pin cluster onto the optical layer and leave
the remaining pins to electrical layer.
As shown from Fig. 6(b) to Fig. 6(c), the net 4 cluster1
is represented by pin4 4 on optical layer. With this phase,
the pin number for each optical net becomes very small.
For O-Router, we manage to keep each optical net size to
below 5 pins. Practically, nets with more than 4 pins can
be decomposed into a set of 2/3/4 pin nets, as illustrated
in (c)-(d), where a 5-pin net6 is decomposed into two 2-pin
nets and a 3-pin net.
Phase 3: For intersected 2-pin nets in the netlist from
Phase2, expand them to have 2 more integer variables if
and only if they can avoid crossing each other by taking
an arc detour, meanwhile the detour does not cut a third
net. This step further expands the feasible solution space
for 2-pin nets.
4.2 Integer Linear Programming Formulation
For the original ILP formulation, we enumerate all routing geometries for the 2-pin, 3-pin and 4-pin nets, shown in
Each Xij is an integer variable, where i ∈ net space, j ∈
Fig. 7(a concave shape 4-pin net is shown as an example).
sol space(net i). When Xij = 1, the corresponding routing
geometry from Fig. 7 will be adopted, as part of the ﬁnal
routing solution space. Number of modulators in each Xij is
also recorded; OIL will return the actual modulating power
based on this number and the ij index.
The ILP formulation is as follows in Equation 6- 15, with
all terms and variables explained in Table 2. The ob jective
function is the total power required to drive all the on-chip
optical modulators for our optical interconnect framework.
The ILP solver will minimize the ob jective function, sub ject
to constraints imposed from Eq. 7 to Eq. 15. In Eq. 6, the
ﬁrst term M P owX ij is total modulating power consumption for routing geometry Xij using 1X modulators, while
the second term (M P owpenalty − P0 ) · Mij · Nij is for penalizing the usage of 10X driving power ModulatorX: if Mij is
to replace all Modulator1(cid:3)s in geometryX ij to meet the con1(hard constraint violation), then ModulatorX will be used
straint (P0 is the laser power consumption of Modulator1).
min{j∈sol space(i)(cid:2)
i∈net space()
[M P owX ij ·Xij +(M P owpenalty−P0 )·Mij ·Nij ]}
s.t
(6)
∀i, m ∈ net space, i (cid:4)= m, j ∈ sol space(i), n ∈ sol space(m) :
PlossX ij
· Xij + netlossX ij
PlossX ij
= LlossX ij
n∈sol space(m)(cid:2)
m∈net space
≤ loss thX ij + pow · Nij · Mij
(7)
+ BlossX ij
(8)
net lossX ij =
ClossX ij mn
· Xij mn
(9)
ClossX ij mn
= γij mn · cross num < Xij , Xmn >
(10)
Xij + Xmn ≤ 1 + Xij mn
(11)
(1 − Xij ) + (1 − Xmn ) ≤ 2 − Xij mn
Xij = 1, Xij = 0, 1
(12)
(cid:2)
j∈sol space(i)
(13)
Xij mn = 0, 1
(14)
Mij = 0, 1
(15)
Constraint Eq. 7 is set for each routing geometry Xij ,
such that its total loss (propagation loss Ploss and coupling
loss Closs ) is bounded by an upper bound of loss threshold: loss thxij , once the upper bound of loss is exceeded, it
means the photo-detection requirements in routing geometry Xij are violated. If among all feasible Xij , some of such
constraint is inevitably violated, then ModulatorX will be
inserted into the corresponding geometry Xij and replace
existing 1X modulators. Constraint Eq. 10 explicitly maps
the crossing number of a net into corresponding coupling
loss using OIL.
For ILP formulation of the calculation of optical interconnect coupling number, we introduced the cross-term integer
267
Algorithm 1 ILP based Optical Routing for low power chip
Require: mapped optical netlist benchmark
invoke optical netlist parser; link OIL
while i ∈ net space do
while j ∈ sol space(i) do
calculate (LlossX ij ,BlossX ij ,M P owX ij ,M P owpenalty ,etc.)
while m ∈ net space, m (cid:3)= i do
while n ∈ sol space(m) do
calculate (ClossX ij mn , constraint coeﬃcients,etc.)
end while
end while
end while
end while
generate glpk syntax ﬁle; invoke glpk ILP solver – minimize
return optical routing for minimum modulating power
Algorithm 3 ILP cross-term variable reduction via merging
Require: mapped optical netlist benchmark
while i ∈ net space do
while j ∈ sol space(i) do
while m ∈ net space, m (cid:3)= i do
while n ∈ sol space(m) do
if i > m then
swap (i,j) with (m,n) in Xij mn ; calculate crossterm constraint coeﬃcients; update glpk syntax ﬁle
end if
end while
end while
end while
end while
return reduced set of cross-terms
Algorithm 2 ILP variable number reduction via trimming
Require: mapped optical netlist benchmark
while i ∈ net space do
while j ∈ sol space(i) do
calculate (LlossX ij ,BlossX ij )
≥ thresholdX ij then
exclude Xij ; update data structures
end if
end while
end while
return trimmed set of routing geometries for each net
if LlossX ij + BlossX ij
Algorithm 4 bounding box for Closs computation speed-up
Require: mapped optical netlist benchmark
while i ∈ net space do
generate bounding box matrix [ ][ ]
while m ∈ net space, m (cid:3)= i do
if bounding box matrix[i, m] == 1 then
calculate Clossij mn ; update glpk syntax ﬁle
end if
end while
end while
return optical routing for minimum modulating power
variables: Xij mn . Numerically, it is the product of term Xij
and Xmn . Since variable multiplications are not supported
by ILP solver, we add the constraint pair Eq. 11- Eq. 12. Integer constraints Eq. 11 and Eq. 12 bound the Xij mn term
so that it always equals the product of its two corresponding
routing geometries. Equality constraint Eq. 13 makes sure
that the ILP solver eventually picks only 1 routing geometry
out of each net for the ﬁnal optimal solution. For further
details please see Table 2 and Algorithm 1.
M P owpenalty
Table 2: Descriptions for ILP involved terms and variables.
Name
Description
net space()
set of nets for an optical netlist
sol space(i)
set of possible routing geometries for net i
M P owX ij
total modulator power consumption of
routing geometry Xij
power consumption penalty for using each
ModulatorX. Set to 10 times of P0
power consumption of Modulator1
least number of optical modulators used
for geometry Xij
coupling loss power between routing
geometry Xij and Xmn
propagation loss power on silicon wires of Xij
integer variable. Xij = 1 means to accept
the j th routing geometry of net i
integer variable. Mij = 1 means to insert
modulatorX into j th routing geometry of net i
numerically equals to Xij · Xmn
integer variable.
loss threshold for O-E conversion for Xij
more driving power each ModulatorX brings
than Modulator1
coupling loss coeﬃcient returned by OIL
dependent on geometry Xij and Xmn
loss thX ij
pow
PlossX ij
Xij
Mij
Xij mn
P0
Nij
ClossX ij
γij mn
4.3 Variable Reduction and Speed-up Techniques
Apparently, a direct implementation of Algorithm 1 will
result in very large number of variables as well as tremendous among of computations, especially for large optical
netlists. Here we propose some useful techniques to speedup O-Router.
268
4.3.1 Variable Trimming/Merging
Variable trimming procedure ﬁrst scans through the Xij
list and calculate bending loss Bloss and line propagation
loss Lloss for each Xij .
If the loss of Xij itself becomes
unbearable, then such a routing geometry is dumped before invoking ILP. Variable trimming procedure successfully
trims oﬀ the infeasible integer variables in Fig. 7 and greatly
reduces the variable set. Solution optimality will not be
harmed with careful choice of the threshold value. Details
about this procedure are shown Algorithm 2.
Variable merging procedure runs in parallel with variable
trimming procedure. As described in Algorithm 3, it cuts
the cross-variable set to half of original size, and the idea is
amazingly simple:
Xij · Xmn = Xij mn
Xmn · Xij = Xmn ij
Xij mn = Xmn ij
(16)
(17)
(18)
Essentially, Xij and Xmn generate non-zero constraint coeﬃcients only when both of them are adopted, which means
the product equality holds in Eq. 16 and 17, consequently,
Eq. 18 also holds, so we can rename half of the cross-term
variables to the other half, since they are identical. Details
about this procedure are shown in Algorithm 3.
4.3.2 Bounding Box Elimination for Speed-up
net_ i
net_ j
net_ k
Bounding-Box k
Bounding-Box i
whole chip
Bounding-Box j
Bounding_Box_Matrix
net_ i
net_ j
net_ i
net_ k
net_ j
net_ k
0
0
1
Figure 8: Illustration of Bounding Box Elimination
Table 3: Performance comparisons between O-Router and Minimum Spanning Tree algorithm.
Net number
Pin number
Pin/net ratio
MST-routing (normalized power)
O-Router (normalized power)
Improvement
photo-detection threshold : 55%
photo-detection threshold: 75%
ibm01
ibm02
ibm03
ibm04
ibm01
ibm02
ibm03
ibm04
5
20
50
137
5
20
50
137
15
50
155
391
15
50
155
391
3
2.5
3.1
2.85
3
2.5
3.1
2.85
3.5
6
35.66
305.13
3.5
12.75
39
306.25
1
2.88
10.75
57.75
2.13
5.38
16.5
100.25
71.40% 52.00% 69.90% 81.10% 39.10% 57.80% 57.70% 67.30%
The introduction of Bounding Box contributes to computation speed-up of O-Router. Bounding Box of net i is
deﬁned as the rectangle that bounds all the pins of net i. It
is deﬁned by 4 values as in Eq. 19:
Bounding Boxi = (min(X ), max(X ), min(Y ), max(Y ))
where X ∈ x axis{neti }, Y ∈ y axis{neti }
(19)
(20)
As illustrated in Fig. 8, a Bounding Box Matrix will be
generated in pre-scanning stage; for any pair of nets with
non-overlapping bounding boxes, a 0 is recorded, otherwise,
1 is written; In Fig. 8, net j and net k have potential crossings, thus only them will be processed for constructing coupling loss constraints. This procedure worth the eﬀorts because a general algorithm for calculating Closs is much more
complicated than min/max value search. Further details
for bounding box elimination procedure are shown in Algorithm 4.
With all 3 speed-up procedures, the original ILP formulation in Section 4.2 is modiﬁed, implemented and tested.
5. EXPERIMENTAL RESULTS
Simulations are carried out according to the aforementioned 3 steps in Section 4, and original electrical benchmarks come from ISPD98/08 routing benchmarks. ibm01-04
are the ﬁnal 4 optical netlists benchmarks, listed as in Table 3. Due to considerations of silicon wire spacing/low coupling noise communication, the sizes of the optical netlists
are kept from small to medium, and the optical layer pin
density is kept from low to medium. As a baseline for ORouter, Minimum Spanning Tree (MST) routing algorithm
is implemented on ibm01-04. Both O-Router framework
and MST algorithm are repeated on ibm01-04 for 2 diﬀerent
photo-detection threshold values: 55% and 75%. Such percentages signify the photo-detection power threshold for received signals at the end of optical interconnect. Therefore,
75% threshold photo-detectors impose stricter detection requirements on O-Router framework. In Table 3, the simulated power consumptions are normalized by the amount
of power reported by O-Router on ibm01, under photodetection threshold of 55%. For 55% threshold, O-Router
achieves above 50% of power reduction compared to MST
baseline, with a max of 81.1% on ibm04. For the 75% threshold, O-Router reports slightly less power reductions due to
higher detection requirements; still an average of above 50%
reduction, with a max of 67.3% of power reduction on ibm04.
6. CONCLUSION
In this paper, we present the ﬁrst optical routing framework, O-Router for low power on-chip integration of silicon
nano-photonics with consideration of various detection constraints. Based on ILP formulation with several variable
reduction techniques for routing speed-ups, O-Router utilizes Optical Interconnect Library, which is an established
collection of some silicon compatible on-chip nano-photonics
devices and optical interconnect models, with key parameters pro jected for future technologies based on optical interconnect roadmap. Experimental results show promising improvements compared with traditional Minimum Spanning
Tree routing algorithm. We expect to see a lot of future
works along this direction as new nano-photonics devices
are introduced for the ultimate global optical and electrical
interconnect co-synthesis and planning.
7. ACKNOWLEDGMENT
This work is supported in part by Texas Norman Hackerman Advanced Research Program.
8. "
Spectrum - a hybrid nanophotonic-electric on-chip network.,"On many-core chip designs, short, often-multicast, latency-critical messages, used extensively in high-level coherence and synchronization protocols, often become the bottleneck of parallel performance scaling. This paper presents Spectrum, a hybrid nanophotonic-electric on-chip network that optimizes both throughput and latency. Spectrum's novel planar nanophotonic subnetwork broadcasts latency-critical messages through a wavelength-division multiplexed (WDM) two-dimensional waveguide. Spectrum's throughput-optimized packet-switching electrical subnetwork handles high bandwidth traffic. Overall, Spectrum delivers an almost ideal CMOS-compatible interconnection network for many-core systems.","	 
    
  ! "" 
 #$   #$ %& !& ""' (& )  * 
 +$   , -,   + -$. , #$ +$ /'$ 0& 122234 
 ! ,      /'$ ,  0  32526 /..

78$.$.. .,  .$ $.'& & 8. $8$..
35.1

0  +9
 +
   $$ $ ,$ 
$$$ $ :$'  '   $
7 $ ,    ,  ,
 $. +$  $$    
    7$    
. ;$ '   $ 
$$  $$$   ''$ 
: <(!#= $ '. ;$ 
7 $  $ $  
 ,>. '  '$  $  #
   ,  $$$.
	 
  		 .1.? @)$$ 

$A	 # !  
$ <#$$$= 
- 
$.

 	 !$ ),.
	 $ $ $  
.
     ! 
( ,  $ $ $ ' 
, , $$$  , ,  $ $ 
$ $  $    ,
  $$$. #'   $ ,
 $  $  $  $ $>
 ,  ' $$  . 
$  $ 
  $   &   $ 
 ,    $$$.
9   $  
 ,$ $   '   
$ ,   ' ;$  $
<.. $ $$     =  
    	
   
   . #  $ ,  $ $
<.. +9-) @1A +-B4 @?A  -;$ 32 +C$ 
 @5A=. +$ $    ' $
 
 $$ $D , : 
 
  -;$ 32  ,,$ 5?2 E0F$ $ .
(  $  $ ' :
    :$$  '  $
,    ,, $   
. 
$   ,  $$ $ $$ 
$$  $,,$ $  '  , $  
$$. +$  ,  $$$ $ 
$  $7   $$$   
   , .
   $   $>  , 
 $$$  $ $$$   $7 
+$  $ $     ""  $ ""23?66G2 
      "" ,  <""=  
HB2?5B2?2   7 9$ "" ,  ! ) ,
  <9""!)= H?22G2225235.
 $  $  $  $ @4A. 0$
 $ $$$  $   $$ , 
$ $$$   $$   $
. +  $ $    $$ $
. ($ $  $$ $  ' ,   $
 ,     ,> $ $ ,> 
'       $$ 
   ,>  , '$ ,
  $   @GA.
$ $      '
 :  $$     @BA.
%$ $$ '  $    $ 
$ '$  . !$ 
. @KA , 5!   $$  >$
 $$ <-= ,  -$.  
. @3A $   ,    $ 
, & $  , $ ,  
$$. M  . @6A '$  $$ $ , 
   , $$$. +
$   $ $ $ $$ ,  
$    .   . @12A $
    , , ??  . - $ 
  $ $     
$   . %$  . @11A $
  
  7 
 $$$ , , 1B . + $ 
$ '$ (!#   $$$ ,  ' 
 . )  . @1?A : $ , 
 $ $   . #$ , $ $ ,$
     >$ ,  $. 
$$$
 $   ,  $   $
$$   # $.
- $   $'     
	  I  , $  
$$$. 
$  $  $ $$ ,  
  ,> '      .
#'  , ,  $ $  
 :$ # $$. - $  ,   ,
 ,  $$    $, # $$.
( $  '    
 . +    $$
  .  $  , $	
1= 
 $> 7 $ 
   $$ 5! $ .  
  $   ' $ .
?= 
 $$   $  (' !'
$ #: <(!#= '  ?! - '. +$ 
,> $$  $ , $ $$$ $
$ $ $    $7 $.
5= 
 7  . ( ' $$$, ,
  $$       #
 .
   ! ! ""#!  $%!&'(
-    $  $   9
$ $ $  . +$   '  $
  ..  . +$ $ $$$$  $>$
,  $$    :$  
, $ '  $$.
575
	
	
	


		

		








	

!	
""



	
	




	
	





 
	

	


	





	






!	
""



	


	

	


<=  .
<= + '.
<= $ ,      ,I $$$.
	


#



"" 1. .
<= #$ , $
  $
$.
<= )$  .
"" ?. 


$ $  "" 1<=     $ 
;$  $ $$$ , $$ 
$  '$. +$ $  $    $
  "" 1<=.
+ '  # $$$  , 
$  $$$   $ 
,  $ $  '   $
  $ # . + ' $   - ' 
    "" 1<=. ( $    , 
 $ $    $, ,  $ 
$  '   $. +$ $  I 
  $    '  
 '  $$$   ' $$$, , $
$.
+  ,$ :  $'
$ , 
$   '$   $$$ 
  $F' $    '. +
'$  $ $    $$   $'
$ >$ @15A $ @14A  (!# :$ @1GA.
E $  $  >$ $  
      $.  
 $$  $       7  
7 $   :$$' $$ 
    , ' $$'$.
  ) %	 
			
 *

	
-   $ - ,  ?! '. - '
 $ $$ ,   $   
'  @1BA   $ 1G $. 
$$$ .. 
2.K 0F     $  $$  '
 : > < $= $$.
- ' $$  $$ ,  $ $$
 $  :  , $ $. +$ $
$   $$  ,,$ ,  7
F  : .  ,,$ $$  $ 
  $ : ,,$   $ :. +
: , , $  $ <   81GG2
=  $ : < 
 81GG2 = $  , 
$    '   $. +$  :
,,   $    >  $ $
   , $ ,,  ,,$ $. - 
, $    $ 512     ,  1 
 ,     ,  1   , $.
+ $  ,  ' $   
<  = , 
    $  ,  - ,.
+ 
  $   $$   
$ , $I 
 $$   $ $I . +
 $$ ,$  $  ,  $
  , $ $$ ,  $$$. + $$$ 
  $ $ ,     $I $  $$
$     $. +   $$  
,  $  ,$   <""-0=  ,$$  $
   $,     $  
   $, ,  $. 
 $   ,
  $$   12  
   ,  -  $
$  "" ?<=. + $      
$   $ ,  	

 @1KA ,,  $ $
  "" ?<=.
+    ,  $$$ $   
    $ ,  $ . E'
  ,   , $ $$$ ,I  
    $. - $  $$    
 <09= ,  $ I ,  $$$. +
$$   @13A I$     
I $     ,  '    $$
$. 
$$       $ ' 
$$ <  $ =  I   , 
    <E$$ = $ ,$	
	    	  
   

<1=


  $  
      	  $  $
 $     
 	 

. +$   $,
$$  
 $$' ,  '  $  . - 
$ $  $ $$ $    
$' E$     $  $$
$   $  $$ , $$$ ,>$ $
  $$    $      
. + ,  & $$   $ 
 $  $ $$. + $ ""$ 
I  '$
<?=
 
 	
	





 + $   $$$   	   $ ,
 $  '  $' 	 $  '
  $  $   $   ' 
 $  $ , $  '   $  : ,
,. 
$$   $   
,, $  $ ,   , $  G3 0 
,  : 2.K   $. +  $ '$
$    $  $$ . ""   
 , $ $      $ 
  '  , $. +$     B 0 ,
 ;$ '. + ' $$ $$  ,  $
$ '$ ' 1B 0   ' < 1B 0 N G3 0
576
	
		
	



	

	


		
	

	





 !	
	""#
	$
		

	
		 !
%	


'

	(
		
&	


"" 5. 
 $   $ .
 B 0 $$$   B 0 '   52 0= 
$ $     '  I   .
!  $   '   -%.
+ $ $ $   O $ <1G52 
1B?G =. 
  $ ,  $  
    O $  ' ,
   . 
 1G22   2.1   
 $,>     $ , $   
   12 E7 $ ,   ' ,  $ $ >:
 $7.     $  $ , 
'$ , 12 EF$ .
% 
	
+ $ '$  $     $ 
   . +  $ $  $  $
, $ # '$     $  
 ,,  $.
%+,  		 )$  $$    $
 $ ,,  $   $     $ 
  , @BA. (  :  
> $$    '$  ,  '
  . +$ $$     122 ( 
'  $    ,  $$.
%),  - +$  ,  '
$$       $. + $$ ,  $
' $ : 1.G 0F $   .
%., / 
  !  "" 1<=  
 ' $  ' , '$    $ 
   $ '  . ('
$ $ , $  >$$  $$ @16A $ 
>. + '    $  ,   
$   $ ,I    C :
 $  , $ .

, ' $ 
$'  $ ,
$     $   "" 5. "" 5<=
 "" 5<= $    $. (  
$ $      '   ' 
  '   $    $
  $$$   ?!  '. ( $ 
 $ <$=   $  $$.
9 '$   $ ' $  
 ,>$  :  $   
,$ , G   @?2A.  '$    '
  $. ""  '$   $ $ $ $
   ,  12 ,"".
 (- 	

:  $ ' $. )$  $ $ 
   (!# :$    
 $ @?1A. E  $ $  $,$
  $   $   $$  , 
#  $.
+, 0 ( $  $ 
 ,   :    ,  '$ 
   ' ,   ,   , 
 $. +  , P  . @16A $ $  
,   '  $ , 1.G . 0    
, I $  ,,$  $$$     
  '     , $$. 0 $'
$ $ $$ $  1<=   '   $
    , $ ..    (!# 
,  $ $. +  , $     
$  $ :     . 
 B4 :
  '   ,     . + $$ , $ 
: $ $   G 0.
),   	
 ""!  
 
	

1 + E  $  @??A  $'$ $ 
'  $$    #    
>$   . +  ,   $
>   $ > <+=  '$ 
' $$   # $  ,. + + $$$ , 
$ '   , $$ $  )# $$.
    &2&(  2 $%!&'(
 $$$ ,    $ $
. + $ $ $  ' $ 
. - '$  ,    $
 $  $ . -  
  $ $ $$ 5! $  
'    $  '
& $$ $   $  7
 $$ .
  ( 	

"" 4<= $$    ,  $
 $  $$$ , 
$ 
,,$ $$ $$     . +
 $  $$ $ <.. ?  
,$=  $ & $   
,. /    $  $ ,
    $> $  $ ,  
$   ,$$  $  $ ,
 7. 9 $$    
$ ' ,$      :$ , 
: F    : $ @?5A 
:$$ ' $ @?4A   $ @?GA . ' 
' , $ I$      
   ' $ ,,$  $> 
' '   .
;$  $ I    $ $ 
  $$   $$ $  
$. "" 4<= $$   $. - 
 $  $$$  $$   $ 
 ,      
,  $  .   ,> 
.. '   $$$ $   >'$
 . 
 , , ;$   $ $
$ , 5! . 9' 5! - 
 $  
 '  $ , ?!
-   ' ,> $ ,  $$
 @?BA @?KA @?3A.  $ , , 5! 
 $  $  $ <   , $ ,
$= @?BA @?3A. 
$ $  "" 4<=   $$$
,     $   '  
& $$ $   $.
% !
*
3& 


  ,    , $ ,C   
  $    ,   (!# .
+$ $ $$  ,> $  $
  ;$ .
 $ I   $   $
    ,   . +$
577









	
!


""
#""$""%








 


"" &''&



+""
(
*
)

(
*
* * * ) * ) ) *
+""
(
*
)

(
*
* * * ) * ) ) *
+""
(
*
)

(
*
* * * ) * ) ) *




,,,"" 4.
<=   . <= 9 $. <= 
  :.
$   '$    $ ,$
    $  ,  
    $$ $. 0 '
 (!#    ?! ' $ 
, $  $     '$.
;$   $ '$   
, '$     $. +$  $$$
 $$$ ,  '$      
$   $        $ 
,I  ,       .
   ,    $  '  $
 $$ , $ $$$  $  $ ,$ $
$$. "" 4<= $$  : ,  $ 
$. +  $ $$     $$
 I  . +$   $$  
,  '$    . 
,  $$$ 
 $ $   $  $ 
 .   $ $  $ $  $
    .
+  , $$      
$$   '   ' $  $ 
 . "" : ,     

 $  <      =   $$ 
<=   ! $$  <=. 
$
  $$ $    $ $$   
$ ,  $  . -,    
$$ $    ,     $
  $   '  $  $ 9 ,  ,  
$ $. "" : ,  
   !  ,
$$   $     $    ,
      $  $ 9 , 9 
   !;$
 $.
+$   $   $  

,, ,$ ,  $$. + >$ $ $  
 ,   $    $  
$ $      $   , 
  $. +$ $ I$    $ 
    ,      
'  $ ,  $  ,   $  $. +
 ,    $  ' :  
, $ $$$. 
   '  '  
   , $ . +   $ 
  $ $  $   $  
$$  $ $    $.   
$   $  '  $  $$
   $   '$  $$ I
   .   $      ,  
'     '$ $>   9  
 ,  ,  $    '     
 ;$     $   $$   .
-    ,>  $ $ 
,>   $ '   . + 
,> ' $ $$    $ $  
 $      $ .

$  $  $ '$  
 , $$   $$ $ 
,> $. (  $  $ $$ 
$$ $ #-    ;$ 
  $ $  $ > 
  $ $ $ $  
  @4A.
 & 2$  !
+$
$
'$
 

$

  . (  ;$
,   $ $  
'$ $ $>   $  
:  $ $ , .
   ""	
+ $  ,  $ $ .. BG  
# . ( $  , > 

  1B $$  , $ !9
#
$. 
 1B  #)   ? E7 $ $$  
   1  $I $ .   $ I 
 
 ?1?B4 $$  ' B4M0 1   '
?GBM0 ?     .
+    $  $ +# BG 
  .   $ :  
 .  E  $ )-    :
    . + '    $ 
 $$      $   , $ 
$   $$ <>$ $  :$=
 .  $       ,  $
$   ,      $  ' 
 ,    , $ $.
+ ,I ,   $ $  
   $ ,$.
  	   Q   ,  ' 
$  ,   $,. 
$$   , 1?,"" 
   $    .
  	  $$$ ,      <@?6A=.

    .
  	 + , C.   
 	 	 9'  	   <@?6A=.
  	   ,   '  $  
 $    .
+   $ : ?42$. +  
$ $  4 E7  $ ,$ $   . -
    $     $  . +
 ,  $    ,  $ 
   & '  ,I$.
 '$ 5!   '   $$
 $ !9
# .  ,  , !9
# $ $
     !9
# $    B4 ,.


578
+
0  ""-E/9
+ -
 $
+  '
4EF$
 , '$
0$ $
 
) $ $7
 $
9 ,I.
+
9
"" 
0,,$  . 
# 
1   
B4M0 ? B4 
?   
?GBM0 1B 1?3 
? $$ 
3 $
# $$ 
14 $
"" $7
B4 $
?E7
4 ?$
!$
$
?G <G  ' =


  #B4 $
 0
 20
 40
 60
 80
 100
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
E
E
S
p
L
H
e
ctru
m
2
L
m
i
s
s
l
a
t
y
c
n
e
serialization
cholesky fft
fmm
lu
memory
L2 data
mpgenc ocean radix SPECJbb
protocol
TPC-H TPC-W waternsq
<= ? $$   $.
 0
 10
 20
 30
 40
 50
 60
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
S
E
p
e
L
ctru
m
P
o
w
e
r
/
W
link
cholesky fft
fmm
lu
EO/OE
mpgenc ocean
loss
radix
broadcast
radiate
SPECJbb TPC-H TPC-W waternsq
routers
<= ) $ <	 ,,  $  $  =.
"" G. $     .
/$ 
+- @52A '$ G.5  $$  !9
#
$  $     BG $$   
' ?#0     1?3#0   ?G.3 EF$
  . # $$  $ $   14
$ $ !9
# $$  '$   .
( '   '  
$  $$  '$ ,    
   $  $   
$$. + - $$  > ,   
  . +  $$$ ,  $ (!#
 $ $  $ #-   
 '  ,,  $.
 ,> $   $  #G ,$$
$ @51A  $' )
? @5?A  
)0 @55A
 $   '   	 

      . ( $
$  $$ $  ,  
$' $	   @54A   @5GA. 
 $ $  1B $   $ $
 $$ $  : .
% 	
 4  * & 
-	
+$ : '$  , ,  $
$         '  $ ,
,,$    : $ <$  -%=.
(   $
   
'$. + >$     $  ?! $
$ $  >'$   < ' 
 $  $ '$   '$=.
 ,$  $  $. + $ 
 ,   $ I  $
$$'  7 I$    ,
$$    ,, $$   $$
, $$ $$  $   '$
$ $7.  $   , $ , 
 $  $$    
 $  . 0 
  $  $ $ #-  
  $  $ $$   $$   '
 $$ $.
"" G<= $$  ' ?  $$  < 
= ,       '$ $
 5! $.  $ $  , $	 $7
 ,     $$  ?  
 $$    $ . + >$ 
$   $   ? $$    $  $
       $. +$ $
$$    $>    $
 <.. 33.2R ' '   K?.KR '
' =      ?  $$  $ <.
B1.2R ' '   51.2R ' ' =.
+$ $$ $    ,,' $ 
  $$    $ $$ 
 '$. +$ $$ $ $  $ 
  $"
Exploring serial vertical interconnects for 3D ICs.,"Three-dimensional integrated circuits (3D ICs) offer a promising solution to overcome the on-chip communication bottleneck and improve performance over traditional two-dimensional (2D) ICs. Long interconnects can be replaced by much shorter vertical through silicon via (TSV) interconnects in 3D ICs. This enables faster and more power efficient inter-core communication across multiple silicon layers. However, 3D IC technology also faces challenges due to higher power densities and routing congestion due to TSV pads distributed on each layer. In this paper, serialization of vertical TSV interconnects in 3D ICs is proposed as one way to address these challenges. Such serialization reduces the interconnect TSV footprint on each layer. This can lead to a better thermal TSV distribution resulting in lower peak temperatures, as well as more efficient core layout across multiple layers due to the reduced congestion. Experiments with several 3D multi-core benchmarks indicate clear benefits of serialization. For instance, a 4:1 serialization of TSV interconnects can save more than 70% of TSV area footprint at a negligible performance and power overhead at the 65 nm technology node.","Exploring Serial Vertical Interconnects for 3D ICs 
Sudeep Pasricha 
Department of Electrical and Computer Engineering 
Colorado State University, Fort Collins, CO 
sudeep@engr.colostate.edu 
35.2
ABSTRACT 
Three-dimensional integrated circuits (3D ICs) offer a promising 
solution to overcome the on-chip communication bottleneck and 
improve performance over traditional two-dimensional (2D) ICs. 
Long interconnects can be replaced by much shorter vertical 
through silicon via (TSV) interconnects in 3D ICs. This enables 
faster and more power efficient inter-core communication across 
multiple silicon layers. However, 3D IC technology also faces 
challenges due to higher power densities and routing congestion 
due to TSV pads distributed on each layer. In this paper, 
serialization of vertical TSV interconnects in 3D ICs is proposed as 
one way to address these challenges. Such serialization reduces the 
interconnect TSV footprint on each layer. This can lead to a better 
thermal TSV distribution resulting in lower peak temperatures, as 
well as more efficient core layout across multiple layers due to the 
reduced congestion. Experiments with several 3D multi-core 
benchmarks indicate clear benefits of serialization. For instance, a 
4:1 serialization of TSV interconnects can save more than 70% of 
TSV area footprint at a negligible performance and power 
overhead at the 65nm technology node. 
Categories and Subject Descriptors:  
B.7.1 [Integrated Circuits]: Types and Design Styles—Advanced 
technologies, VLSI; 
General Terms: Performance, Design 
Keywords: 3D ICs, Serial Interconnect, Networks on Chip, VLSI 
1. MOTIVATION 
In recent years, the rapid scaling of semiconductor technology has 
led to more and more processing cores and memories being 
integrated on a single chip. These highly 
integrated chip 
multiprocessors (CMPs) have provided the high performance 
needed for supporting complex emerging applications particularly 
in the multimedia and networking domains. However, these planar 
CMP architectures are now facing fundamental challenges due to 
on-chip interconnects not scaling well with technology [1][9]. 
Interconnect delay has increased significantly compared to gate 
delay in ultra deep submicron (UDSM) technologies as a result of 
increased crosstalk coupling noise and parasitic resistivity [2]. 
According to the International Roadmap for Semiconductors 
(ITRS) [3], delay on global interconnects has become a major 
source of performance bottlenecks and is one of the semiconductor 
industry’s topmost challenges. 
One promising solution to overcome the interconnect bottleneck 
and continue the pace of growth of CMP systems is the use of 
three-dimensional (3D) integration, in which multiple active device 
layers are vertically stacked and interconnected [4]-[7]. Such 3D 
ICs not only allow more cores to be integrated on a chip, but also 
provide potential performance advances, as each core can access a 
greater number of nearest neighbors, and thus has a higher 
supportable communication bandwidth. Most importantly, since 
the inter-layer distance is small, there is an opportunity to replace 
long global interconnects (~several mm) between communicating 
cores in a horizontal plane, by stacking the cores on adjacent layers 
and connecting them with shorter vertical interconnects. Since wire 
delay depends on the square of the wire length (or has a linear 
dependence if repeaters are used), this results in a reduction in 
inter-core latency. Wire length reduction in 3D ICs also translates 
into lower power dissipation in interconnects and repeaters.   
Figure 1. 3D IC with three layers 
A number of different 3D IC manufacturing technologies have 
been explored in recent years, including transistor stacking, die-onwafer stacking, chip stacking, and wafer stacking. Out of these, 
wafer stacking is one of most promising high-performance yet 
inexpensive implementation technology for 3D ICs [7], and the 
focus of this paper. Figure 1 shows an example of such a 3D IC 
with three active silicon layers. Each layer can have potentially 
multiple processing cores (IPs) and memories (MEM). Inter-layer 
communication is facilitated by vertical through silicon via (TSV) 
interconnects. A major concern in the adoption of 3D ICs is the 
increased power densities that can result from placing a core on top 
of another core. Since high peak temperatures due to increasing 
power densities can cause catastrophic IC failure and this is 
already a major concern in 2D architectures, the move to 3D will 
accentuate the thermal problem. Strategically placed thermal TSVs 
(Figure 1) can help establish a thermal path from the core of a chip 
to the heat sink and are one possible solution for cooling 3D ICs. 
Traditional 2D network-on-chip (NoC) topologies such as mesh, 
torus and butterfly will most likely be extended to create scalable 
networks that can handle intra- and inter-layer communication 
requirements in 3D ICs [8]-[10]. As the number of cores increases 
in each layer to support rising application complexity, the amount 
of communication between layers is also expected to increase. This 
will lead to an increase in the number of interconnect TSVs. Since 
each interconnect TSV requires a pad for bonding to a wafer layer, 
this will lead to an interesting scenario where the area footprint of 
interconnect TSVs in each layer can no longer be ignored.  
581
 
 
 
 
 
 
 
 
 
 
As an example, consider a 3D NoC with a hundred 64-bit vertical 
TSV links between layers (for CMPs with hundreds of cores, such 
a large number of vertical links is to be expected). Assuming TSV 
pad dimensions of 10μm×10μm and a pitch of 16μm, the TSVs 
will take up an area of approximately 1.6mm2 in each layer, which 
is equivalent to the size of a computation core! Unlike a 
computation core however, these interconnect TSVs are spread out 
(uniformly or non-uniformly) in each layer, which will make 
floorplanning and routing extremely challenging. Furthermore, the 
need to use thermal TSVs (which can take 10-20% of total chip 
area [11][12]) to create thermal-efficient 3D ICs will lead to an 
even greater TSV footprint on each layer. This will further 
complicate efficient chip layout, and overall performance.  
In this paper, serialization of TSV interconnects is proposed to 
overcome the abovementioned challenge for 3D ICs. Serialization 
of TSV interconnects will have the benefit of reducing the number 
of TSV interconnects and line drivers, which in turn will reduce 
the TSV interconnect area footprint in each layer. This will make it 
easier to obtain a more efficient chip layout. Additionally, since 
TSV density 
is 
limited by fabrication cost factors, fewer 
interconnect TSVs can make way for more thermal TSVs, which 
will lead to more thermal-efficient IC designs. To the best of the 
authors’ knowledge, this is the first work to explore the impact of 
using serial vertical interconnects in 3D ICs. Experimental results 
with several CMP applications indicate that using serial TSV 
interconnects can significantly reduce TSV area footprint, at a 
negligible performance and power overhead.  
2. RELATED WORK 
In the last few years, there has been a growing interest in 3D ICs 
from academia and 
industry as a means 
to alleviate 
the 
interconnect bottleneck problem currently facing 2D ICs. IBM [4] 
[5] and Tezzaron 
[6] have 
recently presented promising 
preliminary results and test chips with 3D IC technology. Here, 
research on 3D ICs and on-chip serialization is briefly reviewed. 
Several researchers have proposed thermal-aware floorplanning 
techniques for 3D ICs [12]-[16]. In particular, [14] proposed 
inserting thermal vias to reduce temperature hotspots during 
floorplanning. A few researchers have explored interconnect 
architecture design for 3D ICs [8][10][17]. In [8], 2D mesh and 2D 
folded torus topologies were compared with 3D mesh and 3D 
stacked mesh topologies. It was shown that 3D NoCs have more 
complex switches but offer better performance and lower energy 
for communication. In [10] a hybrid bus-NoC 3D interconnect 
architecture was proposed. In [17] circuit level models for TSVs 
were explored. Some recent work has looked at decomposing cores 
(processors [18][19], NoC routers [20], and on-chip cache [21]) 
into the third dimension which allows reducing wire latency at the 
intra-core level, as opposed to the inter-core level. 
Serialization has been explored for long parallel on-chip global 
interconnects as a way to overcome UDSM artifacts such as 
crosstalk, wiring congestion, skew and high power dissipation 
[22]-[30]. In [22][23] it was shown that serial links can reduce 
communication area and power dissipation for not only long global 
links but also for shorter links in future technologies. In [24][25] a 
serial bus architecture was proposed to reduce on-chip bus energy. 
In [26]-[28] high speed ring oscillators and shift registers for 
communication serialization were described. In [29][30] fast 
asynchronous serial links were explored. None of the above works 
have explored the impact of using serial vertical interconnects in 
3D ICs. As will be shown later, vertical TSV serialization is a 
powerful means of reducing TSV area footprint, and thus 
improving 3D IC cost, routability, and thermal efficiency. 
582
Figure 2. 3D TSV bundle schematic 
3. VERTICAL TSV OVERVIEW 
 3.1 Interconnect TSVs 
Vertical interconnects implemented as through silicon vias (TSVs) 
provide the highest interconnect bandwidth in 3D ICs, compared to 
wire bonding, peripheral vertical interconnects, and solder ball 
arrays. To be useful in 3D CMP communication (in NoCs for 
instance), TSV interconnects should not be used in isolation, but 
rather as a group or bundle. Figure 2 shows a bundle of 9 TSVs 
placed in a 3×3 grid structure, in the bulk-silicon manufacturing 
technology [17]. Each TSV has a length L, width W, and an oxide 
coating of thickness tox around it in the bulk-silicon. Pads on the 
wafer surfaces are needed to bond to the vertical TSVs, using 
mechanical thermo-compression [31]. Typically, the pads tend to 
be larger than via cross-section to account for the oxide coating. 
For instance, for a via cross section of 4μm×4μm, and an oxide 
thickness of 1μm, the pad thickness has to be at least 5μm×5μm. 
Interconnect TSVs have inherent resistance, capacitance and 
inductance, and can be modeled as RLC interconnects. As 
discussed in [7], the impact of inductance on delay and power 
ction of via length l, crossdissipation for frequencies of a few GHz can be ignored for 
vertical TSV interconnects. In such a scenario, TSV interconnect 
resistance can be described as
 a fun
section ı and resistivity ȡ:    
ఘൈ௟ఙ                                      (1) 
                                      R = 
For example, a copper TSV with a 4μm×4μm cross section has a 
resistance of about 1.18mȍ/μm in the 130nm technology node. 
The skin effect for TSVs at frequencies of a few GHz is negligible. 
Since TSVs are 
interconnected using metal bonding, an 
appropriate contact resistance must also be considered. In this 
work, a contact resistance of 100mȍ per layer is considered [31].  
The capacitance of TSVs must account for coupling between 
adjacent TSVs in a bundle. The following capacitance matrix is 
used for this purpose: 
             ܥ෤ ൌ ൮ ܥͳǡͳ െܥͳǡͳ
െܥʹǡͳ
ܥͳǡͳ
െܥ݊ǡͳ െܥ݊ǡʹ
ǥ
ǥ
ǥ  െ ܥͳǡ݊
ǥ  ǥ
ǥ  െܥͳǡ݊
ǥ ܥ݊ǡ݊ ൲              (2) 
where the elements outside of the diagonal represent inter-via 
( ܥ௜ ǡ଴ not explicitly shown in the matrix) plus the coupling 
coupling (with inverted signs), while the elements along the 
diagonal are the sum of the capacitances towards the ground plane 
capacit
can es:  
           ܥ௜௜ ൌ ܥ௜ ǡ଴ ൅ ܥ௜ ǡଵ ൅  ǥ ൅ ܥ௜ ǡ௜ିଵ ൅ ܥ௜ ǡ௜ାଵ ൅  ǥ ൅ ܥ௜ ǡ௡           (3) 
The capacitance for TSVs in bulk-silicon was extrapolated from 
extraction results in [17] for TSV bundle densities corresponding 
to commonly used on-chip bus/link sizes (32, 64, 128 bits). 
3.2 Thermal TSVs 
In addition to acting as vertical interconnects, TSVs can also be 
 
 
 
  
 
 
 
 
 
 
 
used in a non-electrical capacity to conduct heat and alleviate hot 
spots in 3D ICs [12][14][33]. The idea of using thermal TSVs to 
overcome thermal problems was first utilized in the design of 
packaging and printed circuit boards (PCBs). Lee et al. [34] 
studied arrangements of thermal vias in the packaging of multichip 
modules (MCMs) and found that as the size of thermal via islands 
increased, more heat removal was achieved but less space was 
available for routing. This observation holds for 3D ICs as well, 
where thermal problems are greater than in 2D ICs because of the 
many dielectric layers. Thermal TSVs create efficient thermal 
conduits and greatly reduce chip temperatures, but have been 
shown to take up to 10-20% of total chip area to achieve a 
reduction in maximal chip temperature of up to 47% [11][12].  
3.3 TSV Reliability Issues 
Since TSV fabrication technology is not yet mature, the reliability 
of TSV interconnects is expected to be a limiting factor for 3D IC 
performance and yield for the near future [7]. Unsuccessful wafer 
alignment prior to and during the wafer bonding process is one of 
the primary mechanisms of failure for TSVs. To improve yield, 
hardware redundancy is often used. A simple and effective way to 
add redundancy and improve yield is to use double pads [35]. 
Since misalignments are caused by the unavoidable shift of 
bonding pads with respect to their nominal position, using large 
square pads twice as wide as standard pads can improve 
misalignment tolerance by an order of magnitude [31][36]. 
4. TSV SERIALIZATION SCHEME 
To reduce the number of interconnect TSVs in 3D ICs, a shiftregister based serialization scheme is proposed, similar to [26]-[28]. 
A single serial line is used to communicate both data and control 
signals between the source and destination nodes. A frame of data 
transmitted on the serial line using this scheme consists of n+2 bits, 
which includes a start bit (‘1’), n bits of data, and a stop bit (‘0’). 
Figure 3(a) shows the block diagram of the transmitter (or 
serializer) at the source. When there is no transmission, the output 
of the flip-flop is zero, the ring oscillator and shift registers are 
disabled, and the n+2 bit counter is in the reset state, with a ‘1’ in 
its least significant bit output (r0). When a word becomes available 
for transfer in the transmission buffer, the R-S flip-flop is enabled, 
thereby enabling the ring oscillator, which generates a local clock 
signal and can oscillate above 2 GHz to provide high transmission 
bandwidth. At the first positive edge of this clock, an n+2 bit data 
frame is loaded in the shift register. In the next n+1 cycles, the 
shift register shifts out the data frame bit by bit. The stop bit is 
eventually transferred on the serial line after n+2 cycles, and r0 
becomes ‘1’. At this time, if the transmission buffer is empty, the 
ring oscillator and shift registers are disabled, and the serial line 
goes into its idle state. Otherwise, the next data word is loaded into 
the shift register from the transmission buffer on the next positive 
clock edge. Thus data transmission continues without interruption. 
Figure 3(b) shows the block diagram of the receiver (or deserializer) at the destination. The R-S flip-flop in the receiver is 
activated when a low-to-high transition is detected on the input 
serial line (the ‘low’ corresponds to the stop bit of the previous 
frame, while the ‘high’ corresponds to the start bit of the current 
frame). After being activated, the flip-flop enables the receiver ring 
oscillator (which has a circuit similar to the transmitter ring 
oscillator) and the ring counter. The n-bit data word is read bit by 
bit from the serial line into a shift register, in the next n clock 
cycles. Thus, after n clock cycles, the n bit data will be available 
on the parallel output lines, while the least significant bit output of 
the ring counter (r0) becomes ‘1’ to indicate data word availability 
at the output. With the assertion of r0, the R-S flip-flop is also 
583
reset, disabling the ring oscillator. At this point the receiver is 
ready to start receiving the next data frame. Note that in case of a 
slight mismatch between the frequencies of the transmitter and 
receiver ring oscillators, correct operation can be ensured by 
adding a small delay in the clock path of the receiver shift register. 
The preceding discussion assumed n:1 serialization, where n data 
bits are transmitted on one serial line (i.e., a serialization degree of 
n). If wider links are used, this scheme can be easily extended. For 
instance, consider the scenario where 4n data bits need to be 
transmitted on four serial lines. In such a case, the number of shift 
registers in the transmitter must be increased from 1 to 4. However 
the control circuitry (flip-flop, ring oscillator, ring counter) can be 
reused among the multiple shift registers and remains unchanged. 
At the destination, every serial line must have a separate receiver 
to eliminate jitter and mismatch between parallel lines.  
Note that it is possible to modify the proposed scheme by using an 
additional strobe line for synchronization, similar to [26]. This 
would reduce the number of bits transferred on the serial data line, 
and the sizes of the shift register and ring counter (which must be 
rewired to disable the oscillator after the desired number of clock 
cycles have been generated) from n+2 to n. However, for the case 
of TSV serialization, the overhead of the extra TSV line and pads 
are prohibitive enough to overshadow the very slight improvement 
in energy and performance that may be obtained for this case. 
(a) Transmitter 
(b) Receiver 
Figure 3. TSV Serialization scheme 
5. EXPERIMENTS 
To explore the impact of using the proposed serialization scheme 
for vertical TSV interconnects in 3D ICs, several experiments were 
conducted. First the area overhead of using the proposed scheme 
was explored. Subsequently, the power and performance impact of 
using the serialization scheme was explored, in the context of 
several CMP applications. 
5.1 Impact on Area  
The goal of the first experiment was to quantify the impact of 
using serialization on TSV area footprint. The serialization scheme 
was implemented at the RTL level and synthesized down to a gate 
 
 
 
 
 
 
 
 
level net-list using Synopsys Design Compiler [38]. The synthesis 
was performed for the 130, 90, and 65nm TSMC standard cell 
libraries. This enabled a determination of the area overhead of the 
transmitter and receiver logic used in the serialization scheme.  
(a) 
(b) 
(c) 
(d) 
Figure 4. % area saving with serialization for 130–65 nm (a) 
8μm TSV pitch, 32-bit link, (b) 8μm TSV pitch, 64-bit link, (c) 
16μm TSV pitch, 32-bit link, (d) 16μm TSV pitch 64-bit link 
Figure 4(a) and (b) show the area savings when using the proposed 
serialization scheme for a 4μm×4μm interconnect TSV cross 
section with 1μm oxide thickness (i.e., 5μm×5μm TSV pads) and 
an 8μm pitch. Figure 4(a) shows the area savings for a 32-bit link. 
The x-axis shows the number of wires in the link with varying 
degrees of serialization and the y-axis shows the percentage 
savings in area compared to the base case (32 wires) with no 
serialization used. It can be seen that the proposed serialization 
scheme can significantly reduce the area footprint of TSVs in 3D 
ICs. Even a 4:1 serialization can result in as much as a 55-70% 
area savings. As technology scales, the savings in area increase, as 
can be seen with the curves for the 130, 90, and 65nm 
implementations. This is a result of lower area footprint of the 
transmitter and receiver logic with shrinking sizes of gates with 
technology. A similar analysis for area saving is presented for a 
64-bit link in Figure 4(b).  
An interesting observation from both Figure 4(a) and (b) is that the 
584
saving in area reaches a critical point beyond which serialization 
does not lead to further area savings. For instance, in Figure 4(b), 
the most area saving for a 130nm implementation is achieved with 
four wires. Any further serialization (i.e., reduction to 2 or 1 wires) 
causes a reduction in area savings. Similarly, for 90 and 65nm 
implementations, the most area saving is obtained for a two wire 
solution. This phenomenon is a result of the overhead of the 
transmission and receiver circuitry taking up more area as the level 
of serialization is increased. Serialization beyond a critical point is 
thus not beneficial for area savings. 
As discussed in Section 3.3, often the TSV pad area is increased to 
ensure fault tolerance, since misalignment of wafers is not 
uncommon in 3D ICs. To explore area savings due to serialization 
under this phenomenon, TSV pad dimensions and pitch were 
increased to 10μm×10μm and 16μm respectively, while TSV 
dimensions were kept the same. Figure 4(c) and (d) show the area 
savings for 32-bit and 64-bit links when the proposed serialization 
scheme. The savings in area are more in this case, as compared to 
the previous non fault-tolerant case. For instance with a 4:1 
serialization, 
the area savings are greater 
than 70% for 
implementations across 130-65nm libraries. For a TSV footprint of 
1.6 mm2 in each layer of a 3D CMP design (as explained in 
Section 1), the results show an area saving ranging from 45-95% 
depending upon the level of serialization. Such a reduction in area 
has significant benefits for reducing routing congestion and 
fabrication cost, and potentially 
improving 
thermal TSV 
distribution that can lead to lower peak chip temperatures.  
5.2 Impact on CMP Power and Performance  
Of course the area savings due to serialization come at a cost: a 
reduction in performance. A serialization degree of n theoretically 
reduces link throughput by a factor of n. There is additionally also 
an impact on the power dissipation of the communication fabric 
due to the additional serialization transmitter and receiver circuitry 
used. The next set of experiments attempt to quantify the power 
and performance impact of using the proposed serialization scheme 
for multi-core CMP designs implemented in 3D ICs.  
5.2.1 Experimental Setup 
Six applications from the well known SPLASH-2 benchmark suite 
(Barnes, Water-NSq, FFT, Cholesky, Ocean, Raytrace) [39] were 
selected, then parallelized and implemented on multiple cores that 
were mapped to a 3D IC. The die size was assumed to be 
2cm×2cm. The cores were connected with a packet switched 2D 
mesh NoC communication fabric with 64-bit wide vertical and 
planar links, clocked at a frequency of 1 GHz. Table 1 summarizes 
the implementation details of the CMP applications, such as 
number of cores (including processors and on-chip memories), the 
number of (64-bit) vertical links and the number of layers on 
which the cores were mapped in the 3D IC implementation.  
Table 1. 3D CMP Implementation Details 
CMP 
applications 
Barnes 
Water-NSq 
FFT 
Cholesky 
Ocean 
Raytrace 
Description 
Galaxy evolution 
Forces/potentials of 
H2O molecules 
FFT kernel 
Cholesky 
factorization kernel 
Ocean movements 
3-D ray tracing 
Cores Vert. 
links 
16 
18 
32 
38 
44 
76 
88 
112 
20 
54 
66 
82 
Layers 
2 
2 
2 
4 
4 
4 
The 64-bit vertical TSV links were implemented as vertical buses 
with an interface at each router. There are two reasons for using 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
vertical buses instead of vertical packet switched links. Firstly, 
vertical packet switched links would require the addition of two 
more ports and links (up and down) to each router, which would 
increase its complexity. In contrast, a vertical bus requires the 
addition of only a single new port to a router. Secondly, the 
distance between layers is relatively small (~20-100μm) in 3D ICs 
compared to inter-router distance in 2D ICs (~1000μm or more). 
As a consequence, multi-hop and router delay for vertical packet 
switched links would dominate the vertical propagation time and 
reduce performance [10]. Therefore a vertical bus with a dynamic 
TDMA arbitration to support programmable quality of service 
(QoS) is considered instead of vertical packet switched links.  
The CMP applications were modeled in SystemC [40] using a fast 
and accurate transaction-based bus cycle accurate (T-BCA) 
modeling abstraction [41]. The cores were modeled at the 
behavioral level granularity, while the inter-core communication 
was modeled at a cycle accurate granularity. Each of the 
applications was simulated with testbench traffic (~few hours) to 
estimate performance of the implementations. A high level 
simulated annealing floorplanner for 2D ICs [32] was used to 
create a thermal-aware layout of the CMP application on the 3D-IC, 
and Manhattan distance based wire routing estimates were used to 
determine wire lengths. The wire length information, together with 
application traffic profiles obtained from simulation were plugged 
into an on-chip communication architecture power estimation 
framework [37] to determine link and communication-centric logic 
component (routers, NoC interfaces) power dissipation for the 
target technology library.  
degradation in application performance (as discussed in the next 
subsection). It can be seen that serialization causes a slight increase 
in power dissipation. This is an interesting result because a lot of 
previous work on serialization for planar interconnects has 
indicated the potential for power savings with serialization 
[22][24][25]. The reason for the reduction in power dissipation in 
those schemes is because of two reasons: (i) the schemes allow 
aggressive reduction in crosstalk capacitance due to greater 
freedom with wire spacing and sizing after serialization reduces 
the number of wires, and (ii) the length of the wires considered are 
long enough for the saving in crosstalk to mitigate power 
dissipation in the serialization circuitry. However, for the proposed 
vertical TSV serialization case, while there is a reduction in 
crosstalk capacitance with a reduction in number of TSVs, the 
width and spacing (pitch) of the remaining TSVs is not altered. In 
addition, the length of the TSVs is much smaller, which limits the 
savings due to reduction in crosstalk capacitance. Thus, the power 
dissipation overhead of the serialization transmitter and receiver 
circuitry dominates, leading to an increase in power dissipation.  
Note that the power dissipation for the 4:1 serialization case is less 
that for the 2:1 case, because of the greater sharing of resources in 
the transmitter and receiver circuitry (Section 4), as well as the 
reduction in switching power on the fewer wires for the 4:1 
serialization case. The power dissipation overhead of serialization 
decreases with technology scaling, and is almost negligible for the 
65 nm library. Further scaling below 65 nm will potentially lead to 
the wire power dissipation dominating the power dissipation of the 
serialization circuits. 
In such a scenario, serialization of 
interconnect TSVs will lead to a reduction in power dissipation.     
(a) 
(b) 
Figure 5. % power change for CMP applications for 130–65nm 
nodes when using (a) 2:1 serialization (64 to 32 bit links), and 
(b) 4:1 serialization (64 to 16 bit links) 
5.2.2 Impact on Power Dissipation 
Figure 5 shows the percentage change in power dissipation of the 
on-chip communication architecture fabric when the proposed 
serialization scheme 
is used, 
for 
the CMP applications 
implemented in the 130, 90, and 65 nm technology libraries. 
Figure 5(a) shows the case of 2:1 serialization (64-bit links reduced 
to 32-bit links) while figure 4(b) shows the case of 4:1 serialization 
(64-bit 
links reduced 
to 16-bit 
links). Higher degrees of 
serialization are not as important because they can cause a notable 
585
Figure 6. % performance change with varying degrees of 
serialization 
for 64-bit 
links 
in a 65nm 
technology 
implementation for CMP applications (1 GHz clk frequency) 
5.2.3 Impact on Performance 
Serialization of links can save area, but only at the cost of 
performance. Figure 6 shows the percentage reduction in overall 
application performance (y-axis; measured in number of cycles to 
finish application execution) for the CMP applications, with 
varying degrees of serialization (represented using number of 
wires; x-axis). It can be seen that as the degree of serialization 
increases, and the number of wires in the links are reduced from 
the original value of 64, 
the performance degrades. This 
performance degradation is low for low degrees of serialization, 
but can become high for higher degrees of serialization. For 
instance, the performance degradation is about 1.7% on average 
for a 4:1 serialization (16 wires) but reaches around 16.1% for a 
64:1 serialization (1 wire). The exact value of performance 
degradation depends on the frequency of vertical transfers and the 
number of TSV interconnects, which varies across the CMP 
applications. It is clear however that lower degrees of serialization 
such as 2:1 and 4:1 are more practical because of their low 
performance and power overhead and an appreciable area saving. 
 
 
 
 
 
 
 
 
 
One advantageous consequence of serialization is that it reduces 
crosstalk capacitance, which in turn reduces propagation delay. 
This can enable higher clock frequencies on the wires. Figure 7 
shows 
the percentage performance overhead 
for 
the 4:1 
serialization case, when TSV interconnects are clocked at higher 
clock frequencies than the baseline 1GHz, for the 65 nm 
technology node. The higher clock frequencies can improve 
performance, but only to a limited degree. Even without such 
frequency increase however, serialization has clear benefits for 3D 
IC technology. For instance, 4:1 serialization of 64-bit links can 
save more than 70% of the TSV area footprint on each layer, at the 
minimal cost of 0.06% power and 1.86% performance overhead on 
average for CMP applications. This is a strong motivation for 
considering serialization of TSV links in emerging 3D CMP 
architectures. 
Figure 7. % performance change with 4:1 serialization (64 to 
16 bit links) for CMP applications for 65 nm technology node, 
with scaled TSV clock frequencies 
6. CONCLUSION 
Vertical interconnect TSVs in 3D ICs take up significant chip area, 
and can cause routing congestion because of their typically spreadout distribution. In this paper, the use of serialized vertical 
interconnects in 3D ICs was explored to reduce the area footprint 
of interconnect TSVs. A shift register based serialization scheme 
was proposed and it was shown how varying the degree of 
serialization can result in significant savings in TSV area on each 
layer of a 3D IC. It was also demonstrated through experiments on 
several 3D CMP architectures that the proposed serialization 
scheme can significantly reduce interconnect TSV area footprint, 
at a nominal power and performance overhead. The extra space 
made available on each layer due to serialization can be used for 
better core placement and routing, as well as more efficient 
thermal TSV insertion for temperature management. Future work 
will quantify the temperature reduction due to serialization, as well 
as explore other serialization schemes for 3D ICs. 
7. "
No cache-coherence - a single-cycle ring interconnection for multi-core L1-NUCA sharing on 3D chips.,"Consistent with the trend towards the use of many cores in SOC and 3D Chip techniques, this paper proposes a ""single-cycle ring"" interconnection (SC_Ring) with ultra-low latency and minimal complexity. The proposed SC_Ring allows multiple single-cycle transactions in parallel. The main features of the circuit-switched design include a set of 3-ported circuit-switched routers (4~16) and a performance/timing effective arbiter. The arbiter, called ""BTPC"", features single-cycle arbitration and routing-control by means of the novel Binary-Tree paths convergence and path-prediction mechanisms, to provide a highly reduced time complexity. By combining this with the integration of 3D chips, the proposed ring-based interconnection offers several advantages for hierarchical clustering in future many-core systems, in terms of cost, latency, and power reductions. Moreover, based on the proposed SC_Ring, this work realizes a ""level-1 non-uniform cache architecture"" (L1-NUCA) for fast data communication without cache-coherency in facilitating multithreading/multi-core as a case study. Finally, experimental results show that our approach yields promising performance.","35.3
No Cache-Coherence: A Single-Cycle Ring Interconnection 
for Multi-Core L1-NUCA Sharing on 3D Chips 
Shu-Hsuan Chou, Chien-Chih Chen, Chi-Neng Wen, Yi-Chao Chan,                                    
Tien-Fu Chen, Chao-Ching Wang and Jinn-Shyan Wang 
Dept. of CSIE and EE, National Chung Cheng University, Taiwan, R.O.C. 
{csh93, ccchi96m, wcn93, cyc95m, chen}@cs.ccu.edu.tw,  
92ccwang@vlsi.ee.ccu.edu.tw, ieegsw@ccu.edu.tw 
ABSTRACT 
Consistent with the trend towards the use of many cores in SOC 
and 3D Chip techniques, this paper proposes a ""single-cycle 
ring"" interconnection (SC_Ring) with ultra-low latency and 
minimal complexity. The proposed SC_Ring allows multiple 
single-cycle transactions in parallel. The main features of the 
circuit-switched design include a set of 3-ported circuitswitched routers (4~16) and a performance/timing effective 
arbiter. The arbiter, called ""BTPC"", features single-cycle arbitration and routing-control by means of the novel Binary-Tree 
paths convergence and path-prediction mechanisms, to provide 
a highly reduced time complexity. By combining this with the 
integration of 3D chips, the proposed ring-based interconnection offers several advantages for hierarchical clustering in 
future many-core systems, in terms of cost, latency, and power 
reductions. Moreover, based on the proposed SC_Ring, this 
work realizes a ""level-1 non-uniform cache architecture"" (L1NUCA) for fast data communication without cache-coherency in 
facilitating multithreading/multi-core as a case study. Finally, 
experimental results show that our approach yields promising 
performance. 
Categories and Subject Descriptors 
B.4.3 [Interconnections (Subsystems)]: Topology, C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]: Interconnection architectures, C.1.4 [Parallel Architectures]: Distributed 
architectures  
General Terms 
Design, Management, Performance 
Keywords 
Ring interconnection, Single-cycle transactions, Arbitration, 
Level-1 non-uniform cache architecture, Memory structure, 
Multi-core, NOC, SOC 
1. INTRODUCTION 
The SOC design trend has recently experienced a movement 
from multi-core (4~16) to many-core (16~256) design. The concepts of communication-centric interconnections and memory 
structure designs greatly impact the success of many-core systems. A scalable and efficient interconnection can be characterized by its topology, routing strategy, highly regular wiring 
strategy, and method of flow-control [1, 2, 3]. Resource limitations, in terms of area and power, are the major constraints on 
network on-chip (NoC) designs [1]. Packet and circuit-switched 
NoCs offer significant differences 
in memory 
latency, 
area/power, frequency, and design complexity [1, 3]. Currently, 
issues of reliability, hot spots, circuit variation, and low power 
problems are more important than frequency issues. Hence, 
several recently proposed works still stick on circuit-switching, 
but the disadvantages of these approaches, including unacceptable wire delay, requiring an efficient arbiter, and so on, [3] become more significant as the systems are scaled up. 
Compared to mesh, the ring topology requires fewer connections, simpler router design, and easier routing-path control [2, 
4]. A ring interconnection with a circuit-switched strategy may 
be sufficiently performance and power effective to be competitive, although it is comparably simpler. However, problems of 
wire delay and efficient arbitration limit the maximum number 
of transactions. Fortunately, the development of 3D die-stacking 
techniques (4~5 layers) has the potential to offer significant 
reductions in the number of wires, through 3D placement and 
routing [5, 6, 7]. The distance between two device layers is 
10~20 ȝm, and the wire delay is negligible (~8 ps) [5]. Moreover, IBM reduces the pitch to a state-of-the-art 0.2 x 0.2 ȝm2 [6], 
which may greatly increase the capacity for inter-wave wires. 
Furthermore, several architecture designs have already been 
discussed for integrating all the components on a 3D chip [5, 6, 
7], including NoCs, cache, memory, critical paths in the processor, and so on. 
The non-uniform cache architecture (NUCA) is a wellknown option for the multi-core memory structure. In general, 
L2 caches are organized as a L2-NUCA with dynamic datamigration and private/shared mechanisms [8, 9] for improving 
the cache utilization and lower the miss rate of the multi-core 
sharing. Moreover, the L2-NUCA also stands to benefit from the 
no cache-coherency property [8, 9], because the many-core 
cache-coherency can cost much latency and power. L1 caches, 
which are sensitive to the processor clock rate, are usually organized by a centralized coherent bus, instead of a L1-NUCA. 
However, a bus is not scalable to support a large-scale multicore. 
In order to achieve ultra-low latency and minimal complexity for on-chip networks, this work proposes a ""single-cycle"" ring 
(SC_Ring) interconnection, using the circuit-switched strategy 
(no buffers) and the ring topology (minimum wires). By singlecycle latency, we mean that the proposed SC_Ring allows data 
transmission between two nodes to be performed in one cycle, 
and it allows multiple single-cycle transactions in parallel. The 
587
 
 
SC_Ring is simply constructed by a set of 3-port circuitswitched routers and a performance/timing effective arbiter. A 
variable number of routers (4~16) are connected by one clockwise and one counterclockwise ring-paths, for various applications to extend flexibility and scalability. Moreover, the number 
of ring-paths can be determined application-specific.  However, 
several critical design challenges require further examination, 
including the wire delay, effective arbitration for maximum 
parallelism, and the timing complexity of arbitration. Section 2 
illustrates some approaches to the design challenges by means of 
binary-tree paths convergence (BTPC) arbitration flow, pathprediction, and 3D placement. 
Based on the proposed SC_Ring interconnection, with low 
latency and high bandwidth, this paper also proposes a ""L1 nonuniform cache architecture"" (L1-NUCA) for fast data communication and no cache-coherency in facilitating multithreading/multi-core (especially in streaming processing [4]), as a case 
study in Section 3. In our experiments, synthesis results (technology node 90 nm) of the proposed SC_Ring interconnection 
by different wire loads model show benefits to placing SC_Ring 
on the 3D chip with the performance/timing effective arbiter 
(900MHz/8-router ring, 600 MHz/16-router ring). In the performance evaluation of the SC_Ring, the average parallel transactions, average memory latency, and power consumption are 
estimated by random and multithreading benchmarks. Moreover, 
some performance and power results are used to compare with 
the proposed L1-NUCA and the original cache-coherent model. 
2. PROPOSED SINGLE-CYCLE RING 
Figure 1. Conceptual view of the single-cycle ring (SC_Ring). 
Figure 1 illustrates the organization of the proposed ring 
(SC_Ring) interconnection and its design challenges. Due to the 
circuit-switched design, the wire delay problem seems almost 
intractable. Fortunately, with the development of the 3D chip [5, 
6, 7], the wire length (delay) and lots of buffer insertion 
(cost/power) are greatly reduced shown in Subsection 2.3. The 
routing path arbitration on rings is critical, and finding the optimal solution can be proved to be an instance of the NPcomplete problem of 0/1 Knapsack. This work proposes an efficient ""Binary-Tree Paths Convergence"" (BTPC) arbitration flow 
with only O(log2N) time complexity (N = number of routers), 
and still giving good arbitration performance. Figure 1(a) shows 
the 3-ported circuit-switched router, which is configured by the 
BTPC routing-path arbiter. Each router supports a set of 
read/write ports for the processor core or IP (namely the tile) 
connection. In Figure 1(b)(c), the request/status signals from 
tiles are provided for arbitration, and the BTPC arbiter delivers 
588
the routing configuration and grant signals after the computation. 
Figure 1(d)(e) presents a single-cycle (SC) transaction on the 
clockwise ring, and another SC transaction on the counterclockwise ring. In the best case, it allows N SC transactions to 
take place concurrently on the proposed ring interconnection. 
2.1 BTPC Arbitration for Routing-Paths 
2.1.1 Transform into vector-based ring-paths 
 A transaction from Tile 0 to Tile 2 (Tile 0ĺ2 on 8 routers 
ring) can generally be represented in binary-encode notation as 
""3’b000ĺ3’b010"", but we employ the bit-vector notation instead as 8’b00000110, as shown in Figure 2. Although the cost 
of bit-vector notation is more than the binary-encode, the bitvector not only implies several transactions on the ring-path, but 
the operation of arbitration also becomes quite simple. For example, we can see that Tile 0ĺ2 (8’b00000110) and Tile 1ĺ3 
(8’b00001100) conflict in the ring-path, by applying the operation ""AND"" to their bit-vectors[2]. In another example, Tile 
0ĺ2 (8’b00000110) and Tile 4ĺ6 (8’b01100000) can be 
merged as Tile 0ĺ2, 4ĺ6 (8’b01100110) by the operation 
""OR"". As the proposed SC_Ring interconnection has both 
clockwise and counterclockwise ring-paths, a pair of N-bit (N = 
number of routers) bit-vectors can be represented as the two 
reversed ring-paths, and the transactions transformed to either 
bit-vector. In Figure 2, the transaction (Tile 0ĺ2) is transformed 
into 8’b00000110 as the clockwise ring-paths, and the transaction (Tile 3ĺ1) is transformed into 8’b00000110 as the counterclockwise ring-paths. 
Simple rules for keeping traversal distance < half-ring: 
¾ ""Clockwise"", if dst_id > src_id & distance < N/2. 
¾ ""Counterclockwise, Cross"", if dst_id>src_id & distance>N/2. 
¾ ""Counterclockwise"", if dst_id < src_id & distance < N/2. 
¾ ""Clockwise, Cross"", if dst_id < src_id & distance > N/2. 
( If dst_id(D) > src_id(S), distance= D-S; else, distance= S-D;) 
Under the half-ring constraint on the transaction traversal 
distance, simple rules are provided for the vector-based ringpath transformation. Comparing the source’s tile_id (src_id) and 
the destination’s tile_id (dst_id) is the primary way to decide 
between clockwise and counterclockwise. However, if the distance between two nodes is larger than N/2, it should be transformed into the reversed ring-path (namely ""cross"", as Tile 6ĺ0) 
because the distance on the reversed ring-path must be smaller 
than N/2. Therefore, the half-ring constraint is guaranteed. The 
proposed transformation rules show several compare/minus 
operations, but they can be simplified to only a few levels of 
gates to improve the timing.  
Figure 2. Example of vector-based ring-paths (8-router ring). 
Another important design strategy is to reserve the previous 
routing-path, so the next transaction will have a good opportunity to access the same destination. In Figure 2, we define the true 
request (T) and no request but inherit the last (L) and transform 
them both into bit-vector. If there is no request, the bit-vector 
will be inherited by the last-request (ex. Tile 6ĺ0 (L)), but the 
fake transaction will have lower priority to not affect true transactions. 
 
 
2.1.2 Binary-tree paths convergence (BTPC) 
The problem of arbitration of ring routing-paths can be identified with the NP-Complete problem of 0/1 Knapsack, because 
the best solution is to maximize the transferring transactions, 
subject to limited routing resources. The number of transactions 
transferred is regarded as the value in 0/1 Knapsack, and the 
limited routing resources corresponds to the constraint of total 
weight. Thus, an optimal arbitration result is unfeasible, but our 
approach provides an approximated solution that is efficient and 
feasible in the hardware implementation. Two approximate solutions, ""Shortest-path first"" and ""Priority first"", are discussed as 
comparisons for the ring routing-path arbitration. At first, the 
""Shortest-path first"" always picks up the shortest transferring 
distance transaction and marks it as a permitted routing-paths if 
it does not encounter any conflicts (step-by-step). It is a greedy 
algorithm to take the minimum routing resources to permit as 
many transactions as possible. However, it is unfeasible to find 
the shortest path for the hardware in run-time (time complexity 
O(N2), where N is the number of routers in the ring-path). It also 
has a serious deadlock problem. In the ""Priority first"" strategy, 
the transactions are picked up according to their priority and 
marked as permitted routing-paths if they do not encounter conflicts (step-by-step). The algorithm is deadlock-free by the 
round-robin priorities, but it is also hardware unfeasible (time 
complexity O(N2)). 
Figure 3. The O(log2N) arbiter by BTPC mechanism.         
(BTPC arbitration example for 8 routers clockwise ring) 
We propose a ""binary-tree paths convergence"" (BTPC) algorithm, which provides efficient arbitration with low time complexity O(log2N). In Figure 3, showing the arbitration for the 
clockwise ring, 
the main 
idea 
is 
that 
two 
transactions 
(represented as bit-vectors for ring-path) in two neighboring 
routers are compared to see if they conflict, and merged. After 
the first level convergence, there are only 4 bit-vectors (8/2) for 
next-level convergence, and some transactions are abandoned. 
Finally, it only costs three levels of convergence, and the last 
ring-path bit-vector indicates the permitted transactions. The 
algorithm uses a comparison-based binary-tree, and substantially 
reduces the time complexity to O(log2N). However, it will abandon a sub-tree while the routing-paths have conflicts, and it will 
also sacrifice some un-conflicting routing-paths, especially in 
abandoning sub-trees at higher levels. For this reason, two design strategies are proposed to minimize the negative impact. 
In the first design strategy, comparing/merging two neighboring bit-vectors will find conflicts of routing-paths in lower 
convergence level, because there is a greater opportunity for 
conflict between neighboring routers on the single direction ring 
(e.g., the clockwise ring). In the second design strategy, the 
589
variable į is defined for the number of true transactions (T) in 
the bit-vector as shown in Figure 3, and the arbiter will abandon 
the sub-tree with smaller į for more permitted transactions, the 
so-called ""local optimization"" strategy. Figure 3(a) shows two 
neighboring bit-vectors detecting the routing-path with no conflict and merging, and the operations are quite simple. The bit[6] 
of two neighboring bit-vectors checks whether they conflict 
(""V4[6] & V5[6] == 0""), and the ""OR"" operand defines the 
merging of two bit-vectors. Figure 3(b) shows how the conflict 
is detected by checking ""V2[4] & V3[4] != 0"", and the arbiter 
abandons the right sub-tree. Because the bit-vector (L) of router3 is not truly a request, but inherited the last request in router3 
(the fake transaction) that is mentioned in Subsection 2.1.1. 
Therefore, the fake transaction will have a low priority for arbitration. Therefore, it not only enables the arbitration of true 
transactions, but also helps with the prediction of routing-paths 
for most single-cycle transactions. Figure 3(c) shows how the 
conflict in the final convergence level is detected, and the algorithm abandons the left sub-tree, which has smaller į (į of left 
sub-tree (1) < į of right sub-tree (2)) according to the ""local 
optimization"" strategy. 
However, the ""local optimization"" strategy may also lead to 
deadlock. Hence, we also support the starvation detection feature, and then the BTPC arbiter will set a larger value of į to 
prevent transactions with higher priority from starvation. In 
figure 4, our experiment shows an average of 6.3 transactions 
transferred by successive random patterns on the proposed 16 
routers of the SC_Ring interconnection (clockwise and counterclockwise) if a tile only communicates with its 8 neighbors (Locality-8). However, it shows only an average of 3.9 transactions 
transferred if each tile communicates with all tiles randomly 
(Locality-16). In addition, it also shows the arbitration efficiency, 
the permitted transaction count of BTPC is near to the permitted 
count of two approximation solutions whose can’t be hardware 
implementation. The BTPC algorithm plays an important role in 
reducing the time complexity of ring routing-paths arbitration to 
O(log2N), and some synthesis results demonstrate the promised 
performance in Figure 9. 
Figure 4. Arbitration efficiency v.s. communication locality. 
2.1.3 Remove conflicts between rings 
After the binary-tree paths convergence (BTPC), the result is 
a number of bit-vectors for routing configuration by a number of 
ring-paths. The BTPC removes ring routing-path conflicts and 
intra-ring destination conflicts. However, destination conflicts 
between rings (inter-ring) should also be resolved. Therefore, 
the final step of the BTPC arbitration flow checks destination 
conflicts between rings and selects one by priority. Finally, the 
routing configuration is generated. 
2.2 Path-Prediction Mechanism 
The time complexity of the BTPC arbitration flow is shown, 
including transforming the vector-paths (O(1)),  BTPC arbitration (O(log2N)), and removing conflicts (O(1)). In general, one 
data transaction can be divided into two processing stages, 
namely the arbitration and routing stages. Usually, the two stag 
 
es would be pipelined to maintain frequency without sacrificing 
performance. Fortunately, many applications sequentially access 
just one memory segment at a time. Accordingly, the prediction 
strategy speculates that the subsequent transaction(s) may be 
similar to these previous transactions. Our proposed approach 
provides a prediction mechanism to combine these steps into a 
single stage to achieve efficient single-cycle transaction performance. Figure 5 shows a single-cycle transaction block diagram 
that maintains frequency. In brief, the path-prediction predicts 
the subsequent-cycle routing paths by assessing the status of 
current or previous requests. The predicted routing paths will be 
prepared for the subsequent cycle, and these will then be used to 
configure the routers by the ""pipe"". Of course, the prediction is 
not always correct, and it should be compared with the real requests and with the status of tiles in the context of destination 
fulfillment procedures (the ""Check"" module) during the routing 
stage. In summary, the path-prediction and routing stages can 
process in parallel to reduce the critical path. 
Figure 5. A pipelined arbitration for single-cycle transaction. 
Figure 6 and 7 show experiments of average transaction latency on 16-router SC_Ring by random and multimedia workloads, including 1 cycle memory access latency. By increasing 
injection rate, the memory latency increases rapidly if each tile 
communicates with all tiles randomly (Locality-16). Thus, the 
communication locality is an important factor for average latency reduction. A small case study on 16-router SC_Ring interconnection indicates that due to a low injection rate, the data 
communication is fast, excepting H.264. However, an overall 
better results can be obtained if we perform optimized placement of threads onto processors by considering communication 
locality (5.77(cid:1064)4.86). 
Figure 6. Average latency and power estimations. 
Figure 7. Average latency on multimedia workloads. 
2.3 Placement on 3D Chips 
To further overcome long wire delays, we may employ 
three-dimensional (3D) designs 
to support 
the proposed 
SC_Ring interconnection, where multiple device layers are 
stacked together. Given the thermal and process limitations of 
3D chips [5, 6, 7], we only propose 4 device layers for a maximum of 16 routers in the SC_Ring interconnection. Figure 8(a) 
shows the 8-router SC_Ring placement on a 4-layer stack, and 
each layer has 2 routers connecting 2 tiles. For reference, in the 
3D stacking technique in technique node 70 nm [5], the distance 
between 2 layers is 10~20 ȝm, and the wire delay is about 8 ps. 
The wire delay is negligible, and the maximum transferring 
distance on the 8-router SC_Ring is estimated to be less than 
100 ȝm with less than a 100 ps delay. As regards the inter-wave 
wire area, there are 172b signals (128b data, adr, control) between two routers by one ring. Figure 8(b) shows the view of the 
BTPC arbiter placement, with 22b signals between the arbiter 
and each router. Hence, there are total (172*4+22*4) d2d paths 
between two layers. From a current reference [6], it only costs 
about 2,850 ȝm2 by via pitch-1 ȝm. Therefore, this shows that 
the inter-wave wire area cost is acceptably small. 
Figure 8. Side view of the 3D chip (4 layers) with the SC_Ring. 
Figure 9. Timing/Power by different **wire load models (90 nm). 
While the wire delay is negligible in a 3D chip, the frequency of the proposed SC_Ring interconnection is dominated by 
arbitration and MUXs in each router. Figure 9 shows timing/power evaluations of the proposed SC_Ring by different 
wire load models simulating different wire delay models, and 
the results show that larger wire delays will cause lower frequencies, larger area 20~25%(cid:1061) (buffer insertion), and larger 
power consumption 20~70%(cid:1061) (signal driven). Moreover, it also 
shows that the BTPC routing-path arbiter is timing efficient, and 
the proposed SC_Ring interconnection is quite suitable to integrate into a 3D chip. The experiment shows that a circuitswitched 16-router SC_Ring can achieve up to a frequency of 
630 MHz (90 nm), which still include the design features of 
multiple single-cycle transactions. 
3. L1-NUCA FOR NO COHERENCY 
Given a single-cycle ring, we further propose the ""L1 NonUniform Cache Architecture"" (L1-NUCA) to realize the no 
cache-coherency system. The key point of the approach is to 
distribute the shared address space to different L1 data caches, 
depending on the locations of the data. As shown in Figure 10, 
each processor has its own data cache, and these caches are connected via a SC_Ring, constructing the proposed L1-NUCA. 
The proposed MMU (to add fields for identifying memory location) helps to differentiate private (such as stack accesses), 
shared, and non-cacheable address regions without extra latency 
(gray is a private cache-line, and black is a shared cache-line). 
Figure 10(a)(b) shows the core accessing private and shared 
address space in its L1 D$. Figure 10(c) illustrates how a remote 
shared cache is accessed by searching the memory location in 
the MMU stage, and the cases of non-cacheable in 10(d) and 
cache-miss will be passed to L2-NUCA. 
590
 
 
 
 
 
Figure 11. Page-based hardware mechanism for                 
fast searching memory locations with no extra latency. 
3.2 Distributed Shared Strategy 
By the proposed distributed control strategy for shared 
memory management, several distributed control behaviors 
complete the efficient L1-NUCA. In Figure 11, the mapping 
controller of each core operates distributed control by broadcasting commands on the SC_Ring. Figure 12(a) describes a shared 
memory location change. Core0 receives the location change 
command from SC_Ring, and updates the sub-page (0x81400) 
located in the 7th L1 D$, if the sub-page exists in Core0’s MMU. 
When a core (Core3) allocates a new page, it will broadcast to 
inquire about the new page’s location on SC_Ring. If none of 
the cores responds, the page’s location remains in Core3, as 
shown in Figure 12(b). On the other hand, in this case when 
there are replies giving the page location in 1st, 7th, 12th, 12th 
L1 D$, the core will update them to MMU.  
Figure 10. Conceptual view of the no cache-coherency system.                      
(a) Private cache access, (b) Local shared cache access,                   
(c) Remote shared cache access, (d) Non-cacheable or cache-miss. 
In our proposed no cache-coherency system, multi-cores 
share the L1-NUCA in the same cluster, and multiple clusters 
share the L2-NUCA. Hence, by the low latency/high bandwidth 
SC_Ring interconnection, the multi-cores in a given cluster 
feature fast data communication and no cache-coherency, especially for applications with streaming processing. In addition, 
the inter-cluster communication will go though the L2-NUCA 
via the non-cacheable address space, and the address space partition will be set by multithreading programs. 
3.1 Private, Shared, and Non-Cacheable 
In order to search distributed shared memory locations in L1 
D$ more quickly, this work proposes a page-based hardware 
mechanism by adding fields to the entries in the TLB and the 
smart address space mapping controller. Therefore, we can insert the information of the distributed shared memory locations 
into each page entry of MMU by looking ahead along the shard 
mapping, and the searching process is hidden by processing the 
virtual address translation in parallel without extra latency. Each 
page consists of four sub-pages, which can be located at four 
different cores. Upon a new page, the mapping controller will 
check if the page is located in a private, shared, or noncacheable address region, and determine to which L1 D$ the 
location maps by handshaking with other cores if it is shared. 
Afterwards, the page will be updated to the MMU of this core 
with the physical address and address partition information. The 
state field in Figure 11 indicates the state of the corresponding 
memory location as private (01), shared (10), or non-cacheable 
(11). The other four fields identify where four sub-pages are 
locate in which L1 D$. The example in Figure 11 illustrates that 
the second entry of MMU is a shared page, which is divided into 
four sub-pages, located at the 7th, 7th, 10th, and 0th L1 D$ respectively. In general, the granularity of the sub-pages determines the amount of shared data per processor, and will be the 
basic unit in cache migration. A large unit of sub-pages may 
lead to unnecessary remote shared accesses due to false sharing. 
However, a lower granularity of sub-pages will require more 
sub-page fields entered in each MMU (4 KB per page and 1 KB 
per sub-page in Figure 11). Fortunately, the cost is negligible 
(each memory location identifier is 4b).  
The mapping controller (Mapping Ctrl.) of each core is responsible for the management of distributed shared memory 
locations, and it contains private/shared region registers for address space partition and records the access frequencies of 
shared pages for data-migration. In particular, a distributed control strategy is provided for the distributed shared management, 
in which the mapping controller of each core handshakes with 
other cores to update its location information in case of new 
page allocation, data-migration, and memory location changes.  
Figure 12. The management of distributed shared mapping. 
3.3 Attracting Sub-pages by Migration 
Although the SC_Ring is supported, the remote shared 
access still pays some overhead (minimum 2~3 cycles). The 
behavior of data migration is necessary to keep data localized 
within L1 D$ for performance/power effectiveness, and the 
migration range is 1 KB (sub-page) in our proposed L1-NUCA 
prototype. In the example in Figure 12(c), when Core0 detects 
frequent access to a remote shared sub-page (ex. 0x81400, in 7th 
L1 D$) from the record of access frequency, it will determine to 
migrate the sub-page into its own L1 D$. At first, Core0 broadcasts to lock the sub-page and change its location commands to 
all cores in the cluster, and then the sub-page data migrates from 
the 7th to 0th L1 D$. When the migration finishes, Core0 will 
broadcast to unlock the sub-page to complete the migration 
process. When a task is done by multiple threads, the address 
partition mapping may change for the new task. Thus, the multithreading scheduling handler should flush all L1 D$ in the 
591
 
 
 
cluster and reset the MMU by new private/shared regions before 
restarting. 
4. EXPERIMENTS 
In the proposed single cycle ring (SC_Ring) experiment, we 
implemented a cycle-accurate model connected with many-core 
simulator mcore similar to [11] and a configurable RTL model 
for synthesis and power evaluation in technology node 90 nm. 
Table I shows the target system parameters. This work evaluated 
by a parallel simulator in the context of the SPLASH-2, Parsec 
benchmarks. This experiment constructs 16 cores with intracluster L1 cache system as MESI snooping cache-coherency 
protocol, static and dynamic L1-NUCA, and there are some 
latency/power results for comparison. In Figure 4, 6, 7, 9, there 
are several SC_Ring experimental results, which including the 
efficiency of BTPC arbiter, average transfer latency by the injection rate, small case studies for multimedia workloads, and timing/power evaluations by different wire load models to show 
assumed results in 3D chips. In overall, the SC_Ring performs 
ultra-low latency and minimal complexity for on-chip networks. 
We focus on the shared memory access events. Table I gives 
those parameters, where the migration size is 1KB (sub-page) on 
frequently remote accessing and the L2 cache latency is average 
20 cycles. The power model is composed of cache bank (CACTI 
5.3 [10]), SC_Ring interconnection, and snooping bus by hspice 
tools. 
Table I. Target System Parameters. 
2.5
2.0
1.5
1.0
0.5
0
Cache-miss
Remote-hit
Local-hit
S-NUCA
97%
$ Coherency
D-NUCA
32%
30%
fft
mp3d
ocean
waternsquared
waterspatial
fluidanimate
Figure 13. Comparisons of shared access latency. 
Figure 13 shows the shared access latency (normalized by $ 
coherency) of three L1 $ system (Static-NUCA, DynamicNUCA), and the latency is primarily contributed by cache-miss, 
remote hit, and local hit. In mp3d, the result shows access latency reducing (35%(cid:1062)), because there are lots of data overlapping 
and communication between threads (low cache-miss rate and 
fast data communication). However, the other benchmarks are 
lacking of the streaming property, and their shared data address 
space is less overlapped. Hence, the L1-NUCA will not perform 
better and pay for the latency of remote shared access contrarily. 
Moreover, the S-NUCA with no data-migration has high ratio of 
remote shared access to cause high latency, especially in waternsquared, water-spatial, and fludanimate. The D-NUCA with 
proposed data-migration mechanism eliminates the penalty 
greatly shown in Figure 13. Comparing average power consumption, the cache snooping protocol sends commands to 
snooping bus and broadcasts to all tag banks in each core for 
checking. Our power model count is about 680mW (40mW for 
tag banks per core and 40mW when snooping bus activating). In 
592
L1-NUCA, the remote shared access will cost extra power by 
transactions on SC_Ring without broadcasting. One transaction 
on SC_Ring costs only 20mW. Figure shows significant power 
saving in mp3d (43%). However, the S-NUCA consumes large 
power in water-spatial because of too many remote shared 
access. Therefore, the unnecessary remote shared access can be 
avoided. 
Figure 14. Comparisons of shared access power. 
5. CONCLUSION 
We propose a SC_Ring interconnection with ultra-low latency and minimal complexity. By combining this with the integration of 3D chips, the proposed ring-based interconnection 
offers several advantages for hierarchical clustering in future 
many-core systems, in terms of cost, latency, and power reductions. Moreover, by SC_Ring supporting, this work realizes L1NUCA in facilitating multithreading/multi-core but also shows 
great reduction of shared access latency and power consumption 
in coherent multithreading programs. 
"
NoC topology synthesis for supporting shutdown of voltage islands in SoCs.,"In many systems on chips (SoCs), the cores are clustered in to voltage islands. When cores in an island are unused, the entire island can be shutdown to reduce the leakage power consumption. However, today, the interconnect architecture is a bottleneck in allowing the shutdown of the islands. In this paper, we present a synthesis approach to obtain customized application-specific networks on chips (NoCs) that can support the shutdown of voltage islands. Our results on realistic SoC benchmarks show that the resulting NoC designs only have a negligible overhead in SoC active power consumption (average of 3%) and area (average of 0.5%) to support the shutdown of islands. The shutdown support provided can lead to a significant leakage and hence total power savings.","46.5
NoC Topology Synthesis for Supporting Shutdown of Voltage Islands in SoCs
Ciprian Seiculescu(cid:2) , Srinivasan Murali§ (cid:2) , Luca Benini‡ , Giovanni De Micheli(cid:2)
(cid:2) LSI, EPFL, Lausanne, Switzerland,{ciprian.seiculescu, giovanni.demicheli}@epﬂ.ch
§ iNoCs, Lausanne, Switzerland, murali@inocs.com
‡ DEIS, Univerity of Bologna, Bologna, Italy, luca.benini@unibo.it
ABSTRACT
In many Systems on Chips (SoCs), the cores are clustered in to
voltage islands. When cores in an island are unused, the entire island can be shutdown to reduce the leakage power consumption.
However, today, the interconnect architecture is a bottleneck in allowing the shutdown of the islands.
In this paper, we present a
synthesis approach to obtain customized application-speciﬁc Networks on Chips (NoCs) that can support the shutdown of voltage
islands. Our results on realistic SoC benchmarks show that the resulting NoC designs only have a negligible overhead in SoC active
power consumption (average of 3%) and area (average of 0.5%) to
support the shutdown of islands. The shutdown support provided
can lead to a signiﬁcant leakage and hence total power savings.
Categories and Subject Descriptors
B.4.3 [INPUT/OUTPUT AND DATA COMMUNICATIONS]:
Interconnections (Subsystems)—topology
General Terms
Design
Keywords
NoC, voltage islands, shutdown, leakage power, topology
1.
INTRODUCTION
Power management is a challenge for modern Systems on Chips
(SoCs), as many of them are destined for the embedded market
and have to operate with low power consumption. With technology
scaling, the leakage power consumption is increasing rapidly as a
fraction of the total power consumption. In fact, leakage power can
be responsible for 40% or more of the total system power [6].
In order to reduce the leakage power consumption, cores that
are not used by an application can be shutdown or placed in sleep
mode, while the other cores can be operational. For example, power
gating using sleep transistors is a popular way to shutdown cores
[6]. To achieve power gating, the sleep transistors are added between the actual ground lines and the circuit ground (also called
the virtual ground) [6], which are turned off in the sleep mode to
cut-off the leakage path. Due to routing restrictions, separate VDD
and ground lines cannot be used for each core. Instead, cores are
grouped in to Voltage Islands (VIs), with cores in an island using the
same VDD and ground lines [5]-[8]. When all the cores in an island
are unused for an application, the entire island can be shutdown.
For example, the IBM fabrication processes CU-08, CU-65HP and
CU-45HP all support the partitioning of chips into multiple VIs and
power gating of the VIs [4].
In today’s SoCs, the interconnect architecture is a bottleneck
in allowing the shutdown of the islands. There are several approaches presented to synthesize application-speciﬁc Networks on
Chips (NoCs) [12]-[15]. However, none of them consider the issue of shutdown of VIs. These approaches cannot be directly extended to design NoCs for SoCs with voltage islands. If the NoC
is designed using such approaches, either the whole NoC should be
placed in a separate VI or the islands cannot be shutdown.
Placing the entire NoC in a separate VI is not a feasible solution.
The NoC switches are usually spread across the chip, connecting
the different cores. If the entire NoC is in the same island, it is
difﬁcult to route the VDD and ground lines for the NoC across the
chip. On the other hand, if all the NoC switches are physically
clustered and placed in the center of the chip, then long wires are
needed to connect all the cores to the NoC island. Thus, the routing congestion would be enormous and the solution is not scalable.
Moreover, additional resources for routing the additional voltage
and ground lines may not even be available in the design. If the
switches are spread across the different VIs and if a VI needs to
be shutdown, then packets between cores on the other VIs that use
the switches in this VI cannot be transmitted. This will prevent the
shutdown of the entire island.
The concept of voltage island should be considered during the
NoC topology synthesis phase itself. In this work, we present a
synthesis approach to determine the best NoC topology points that
are tailored to meet the application performance constraints, minimizing power consumption and supporting the ability to shutdown
voltage islands. To the best of our knowledge, this is the ﬁrst work
that addresses custom NoC topology synthesis for supporting the
shutdown of voltage islands.
2. RELATED WORK
Power gating of designs has been widely applied in many SoCs
[5]. In [5], the authors present the importance of partitioning cores
in to voltage islands for power reduction. Several methods have
been presented to achieve shutdown of islands [5]-[8]. Any of these
methods can be used in conjunction with our topology synthesis
process to achieve the actual shutdown of cores.
A description of the NoC paradigm with the related beneﬁts and
issues is presented in [1]-[3]. Many works have been presented on
synthesizing bus based systems [16]-[18]. In [9]-[11], algorithms
for mapping application to regular NoC topologies are presented.
In [12]-[15], methods for designing application speciﬁc NoCs are
described. However, none of these works address the issue of sup822
porting shutdown of voltage islands on the chip.
In [24] an architecture for Globally Asynchronous Locally Synchronous (GALS) NoC is presented. In [23] the authors present a
physical implementation of multi-synchronous NoC. In [22], the
authors present a methodology to partition a NoC into multiple
voltage islands. This work is complementary to ours, as we present
a methodology to design a custom NoC topology with VIs. In [19],
the authors present an approach to design NoCs with voltage islands. However, the designs produced by the method do not support
the shutdown of the islands.
In [20], the authors present approaches to route packets even
when parts of the NoC have failed. A similar approach can be used
for handling NoC components that have been shutdown. However,
such methods do not guarantee the availability of paths when elements are shutdown. Moreover, mechanisms for re-routing and
re-transmission can have a large area-power overhead on the NoC
[21] and are difﬁcult to design and verify.
3. PROBLEM DESCRIPTION
In this section, we describe the architectural features of the NoC
and the synthesis problem.
3.1 Architecture Description
An example of the architecture for which our custom NoC synthesis algorithm is designed is presented in Figure 1. The cores of
the design are assigned to different VIs, which is given as an input
to our method. The cores in a VI have the same operating voltage
(same power and ground lines), but could have different operating
frequencies. In order to have a scalable solution, we build NoC
systems, where cores in a VI are connected to switches in the same
VI. We follow an approach similar to the GALS approach, where
the NoC components in a VI are synchronous and operate at the
same frequency. Having a locally synchronous design eases the
integration of the NoC with standard back-end placement&routing
tools and industrial ﬂows. Moreover, if the different switches in a
VI operate at different frequencies, power and latency hungry synchronizers are needed to connect them.
The cores are connected to the NoC switches by means of Network Interfaces (NIs) that convert the protocol of the cores to that
of the network. The NIs also perform clock frequency conversion,
if the cores are running at different frequencies than the switches
in the VI. When a switch in one VI is to be connected to a switch
in another VI, we use a bi-synchronous FIFO to connect them together. The FIFO takes care of the voltage and frequency conversion across the islands. The frequency conversion is needed because the switches in the different VIs could be operating at different frequencies. Even if they are operating at the same frequency,
the clock tree is usually built separately for each VI. Thus, there
may be a clock skew between the two synchronous islands. We
use over the cell routing with unpipelined links to connect switches
across different VIs, as the wires could be routed on top of other
VIs.
3.2 Synthesis Problem
In our synthesis procedure, we generate switches in each VI to
connect the cores in the VI. Optionally, our method can explore
solutions where a separate NoC VI can be created. We take the
availability of power and ground lines for the intermediate VI as an
input, and our method will use the intermediate island, only if the
resources are available. Our method produces topologies such that
a trafﬁc ﬂow across two different VIs can be routed in two ways: (i)
the ﬂow can go either directly from a switch in the VI containing
the source core to another switch in the VI containing the destination core, or (ii) it can go through a switch which is placed in the
intermediate NoC VI, if the VI is available. The switches in the intermediate VI are never shutdown. The method will automatically
explore both alternatives and choose the best one for meeting the
application constraints.
The objective of our synthesis method is to determine the number
of switches needed in each VI, the size of the switches, their operating frequency and routing paths across the switches, such that
application constraints are satisﬁed and VIs can be shutdown, if
needed. Our method determines if an intermediate NoC VI needs
to be used to connect the switches in the different VIs and if so, the
number of switches in the intermediate island, their sizes, frequency
of operation, connectivity and paths. The synthesis method can be
plugged in our design ﬂow presented in [15] in order to generate
fully implementable NoCs
Our method produces several design points that meet the application constraints with different switch counts, with each point having different power and performance values. The designer can then
choose the best design point from the trade-off curves obtained.
4. TOPOLOGY SYNTHESIS APPROACH
The synthesis algorithm is explained in detail in this section.
From the input speciﬁcations, we construct the VI communication
graph deﬁned as follows:
is a directed graph, each vertex vi ∈ V represents a core in the VI
D E FIN I T ION 1. A VI Communication Graph (VCG(V , E , isl))
denoted by isl and the directed edge (vi , vj ) representing the communication between the cores vi and vj . The bandwidth of trafﬁc
ﬂow from cores vi to vj is represented by bwi,j and the latency constraint for the ﬂow is represented by lati,j . The weight of the edge
(ei , ej ), deﬁned by ei,j , is set to a combination of the bandwidth
and the latency constraints of the trafﬁc ﬂow from core vi to vj :
hi,j = α × bwi,j /max_bw + (1 − α) × min_lat/lati,j , where
max_bw is the maximum bandwidth value over all ﬂows, min_lat
is the tightest latency constraint over all ﬂows and α is a weight
parameter.
The value of the weight parameter α can be set experimentally
or obtained as an input from the user, depending on the importance
of performance and power consumption objectives.
Algorithm 1 Core-to-switch connectivity
1: Determine the frequency at which the NoC will operate in each
VI and max_sw_sizej , ∀j ∈ [1 · · · NV I ]
2: min_swj = |V CG(V , E , j )|/max_sw_sizej , ∀j
else
if i + min_swj < |Vj | then
k = i + min_swj
k = |Vj |
3: {Vary number of switches in each VI}
4: for i = 1 tomax ∀j∈1···NV I |Vj | do
5:
for j = 1 to NV F do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
Perform k min-cut partitions of V CG(V , E , j ).
end for
{Vary number of switches in intermediate NoC VI}
for k = 0 to max∀j∈1···NV I do
Compute least cost paths for inter-switch ﬂows. Choose
ﬂows in bandwidth order and ﬁnd the paths.
16:
If paths found for all ﬂows save design point
17:
end for
18: end for
The algorithm for topology synthesis is presented in Algorithm
1. In the ﬁrst step of the algorithm, the frequencies of operation
823
Comunnication based partitioning
Logical partitionning
100
80
60
40
20
)
W
m
(
n
o
i
t
p
m
u
s
n
o
c
r
e
w
o
P
Comunnication based partitioning
Logical partitionning
7
6.5
6
5.5
5
4.5
4
3.5
)
s
e
l
c
y
c
(
y
c
n
e
t
a
l
e
g
a
r
e
v
A
0
1
2
3
4
5
Island count
6
7
26
3
1
2
3
4
5
Island count
6
7
26
Figure 1: Example Input
Figure 2: VI count vs. power
Figure 3: VI count vs. latency
of the switches in each of the islands are determined. In our NoC
design, a core is connected to only one switch, through a NI. The
links connecting the NI and the switch determine the frequency at
which the NoC elements have to run in an island. The bandwidth
available on a link is a product of the link data width and the frequency. In our synthesis procedure, without loss of generality, we
ﬁx the data width of the NoC links to a user-deﬁned value. Please
note that it could be varied in a range and more design points could
be explored, which does not affect the algorithm steps. For a ﬁxed
data width, the frequency of the switches in an island is determined
by the link that has to carry the highest bandwidth from or to a core
in the island.
A larger switch will have a longer critical path in the crossbar
and therefore will have to operate at a smaller frequency. The frequency at which a switch has to operate determines the maximum
size (number of inputs and outputs) of the switch that can be allowed, denoted as max_sw_sizej . As the switches in the different
VIs can operate at different frequencies, the maximum switch size
is different for the different VIs. Once the maximum switch sizes
are determined, based on the number of cores in each VI, the minimum number of switches required for each island is determined in
step 2 of the algorithm. Let NV I denote the total number of VIs in
the design. In steps 4 to 10 of the algorithm, the number of switches
in each island is varied from the minimum value (computed in step
2) to the maximum number of cores in the island. In step 11, for
the current switch count of the VI, that many min-cut partitions of
the VCG corresponding to the VI are obtained. Cores in a partition
share the same switch. As min-cut partitioning is used, cores that
communicate heavily or that have tighter latency constraints would
be connected to the same switch, thereby reducing the power consumption and latency.
At this point, the connectivity of the cores with the different
NoC switches is obtained. We still need to connect the switches
together and ﬁnd paths for the inter-switch trafﬁc ﬂows.
If the
switches from a VI are directly connected to the switches on the
other VIs, then several switch-to-switch links would be needed.
This may lead to large switch sizes, which may lead to violation
of the max_sw_sizej constraint. By using switches in an intermediate NoC island, the number of switch-to-switch links can be
reduced. These switches act as indirect switches, as they are not
directly connected to the cores, but only connect other switches. If
the design constraints permit the usage of another VI, then we explore the solution space (step 14) with varying number of switches
on the intermediate NoC VI.
For each combination of direct and indirect switches, the cost of
opening links is calculated and the minimum cost paths are chosen
for all the ﬂows (step 15). The trafﬁc ﬂows are ordered based on
the bandwidth values and the paths for each ﬂow in the order is
computed. The cost of using a link is a linear combination of the
power consumption increase in opening a new link or reusing an
existing link and the latency constraint of the ﬂow. When opening
links, we ensure that the links are either established directly across
the switches in the source and destination VIs or to the switches in
the intermediate NoC island.
If for all the ﬂows, paths that do not violate the latency constraints are found, then the design point is saved. Finally, for each
valid design point, the NoC components are inserted on the ﬂoorplan and the wire lengths, wire power and delay are calculated. The
time complexity of our algorithm is O(V 2E 2 ln(V )), however in
practice the algorithm runs quite fast as the input graphs typically
are not fully connected.
5. EXPERIMENTAL RESULTS
Experiments are performed using the power, area and latency
models for the NoC components based on the architecture from
[25]. The models are built for 65nm technology node. We extended the library with models for the bi-synchronous voltage and
frequency converters.
When the NoC has to be designed to support power gating of
islands, there is an additional overhead on the dynamic power consumption of the NoC. To study the impact of the overhead and to
see the impact of different core assignment to islands and different number of islands, we consider a case-study on a realistic SoC
benchmark. The SoC design is used for mobile communication and
multimedia applications. The benchmark has 26 cores, consisting
of several processors, DSPs, caches, DMA controller, integrated
memory, video decoder engines and a multitude of peripheral I/O
ports.
We consider two ways of assigning the cores to different VIs.
One way, designated as logical partitioning, is based on the functionality of the cores. For example, shared memories are placed
in the same VI, as they have the same functionality and therefore
are expected to operate at the same frequency and voltage. The island with the shared memories is also expected not to be shutdown,
since memories are shared and should be accessible at any time
and this is another reason to cluster them in the same VI. Similar
reasoning was used to partition all the cores for the case of logical
partitioning. Another way we considered for partitioning is based
on communication and is called communication based partitioning. In this case, cores that have high bandwidth communication
with one another will be placed in the same VI. Please note that
the assignment of cores to the VIs is an input to our synthesis algorithm.
In Figure 2, we show how the dynamic power consumption of the
NoC varies when the cores are partitioned in to different number of
VIs. The power consumption values comprise the consumption on
824
 
 
 
 
 
 
 
 
Figure 4: Topology example
Figure 5: Floorplan example
switches, links and the synchronizers. In the x-axis, the ﬁrst point
(1 island) is actually a design point with all the cores in the same
island, which is the reference point. The last point on the graph
corresponds to 26 VIs, which is the point when each core is in its
own island. It can be seen that in the case of logical partitioning,
we have to pay a some overhead in NoC dynamic power, as there
are more high bandwidth ﬂows that have to go across islands. In the
case of the communication based partitioning, the NoC consumes
less power than the reference point with 1 island, as the NoC can
run at a slower frequency in some of the islands. In this case most
of the high bandwidth ﬂows are inside an island, so the power overhead is less. In Figure 3, we show the average packet latencies for
the different design points. The latency quoted is the number of cycles needed to transfer a single chunk of the packet from the output
of the source NI until the input of the destination NI under zeroload conditions. When packets cross the islands, a 4 cycle delay is
incurred on the voltage-frequency converters. Thus, with increasing number of islands, the latencies increase. A topology for the
6 VI logic partitioning case is shown in Figure 4 and a ﬂoorplan
example is presented in Figure 5.
We studied the support of voltage islands on a variety of SoC
benchmarks. For the different SoC benchmarks, we found that the
topologies synthesized to support multiple VIs incur a 3% overhead on the total system’s dynamic power. We found that the area
overhead is also negligible, with less than 0.5% increase in the total
SoC area. In many SoCs, the shutdown of cores can lead to large
reduction in leakage power, leading to even 25% or more reduction
in overall system power [6]. Thus, compared to the power savings
achieved, the penalty incurred in the NoC design is negligible.
The exploration of the design points for all the benchmark took
only a few hours on a 2 GHz Linux machine. To be noted that the
synthesis process is only run once at design time and therefor the
computational time required by the algorithm is negligible.
6. CONCLUSIONS
Stand by and leakage power consumption of the SoC is becoming a large fraction of the total power consumption. Clustering of
cores in to voltage islands and shutdown of unused islands is an effective way to reduce the leakage power consumption. The system
interconnect has to be designed to ensure proper operation when
shutting down voltage islands. In this paper, we presented an approach to synthesize application speciﬁc Networks on Chip (NoC)
interconnects that can effectively support shutdown of islands. The
topologies synthesized by our methods have negligible power and
area overhead (3% power and 0.5% area, on average), in order to
support shutdown. The presented approach also allows the design
space exploration of NoCs with different power-performance values that meet the application constraints.
7. ACKNOWLEDGMENT
We would like to acknowledge the ﬁnancial contribution of CTI
under project 10046.2 PFNM-NM and the ARTIST-DESIGN Network of Excellence.
8. "
Dynamic thread and data mapping for NoC based CMPs.,"Thread mapping and data mapping are two important problems in the context of NoC (network-on-chip) based CMPs (chip multiprocessors). While a compiler can determine suitable mappings for data and threads, such static mappings may not work well for multithreaded applications that go through different execution phases during their execution, each phase with potentially different data access patterns than others. Instead, a dynamic mapping strategy, if its overheads can be kept low, may be a more promising option. In this work, we present dynamic (runtime) thread and data mappings for NoC based CMPs. The goal of these mappings is to reduce the distance between the location of the core that requests data and the core whose local memory contains that requested data. In our experiments, we evaluate our proposed thread mapping and data mapping in isolation as well as in an integrated manner.","Dynamic Thread and Data Mapping for NoC Based CMPs
48.1
Mahmut Kandemir
Pennsylvania State University
University Park, PA
kandemir@cse.psu.edu
Ozcan Ozturk
Bilkent University
Turkey
ozturk@cs.bilkent.edu.tr
Sai P. Muralidhara
Pennsylvania State University
University Park, PA
smuralid@cse.psu.edu
ABSTRACT
Thread mapping and data mapping are two important problems in the context of NoC (network-on-chip) based CMPs
(chip multiprocessors). While a compiler can determine suitable mappings for data and threads, such static mappings may
not work well for multithreaded applications that go through
diﬀerent execution phases during their execution, each phase
with potentially diﬀerent data access patterns than others. Instead, a dynamic mapping strategy, if its overheads can be kept
low, may be a more promising option. In this work, we present
dynamic (runtime) thread and data mappings for NoC based
CMPs. The goal of these mappings is to reduce the distance
between the location of the core that requests data and the
core whose local memory contains that requested data. In our
experiments, we evaluate our proposed thread mapping and
data mapping in isolation as well as in an integrated manner.
Categories and Subject Descriptors
D.3.4 [Programming Languages]: Processors-compilers
General Terms
Algorithms, Performance
Keywords
Thread, data, dynamic, mapping, NoC, CMP
1.
INTRODUCTION
It is clear today that large, complex uniprocessors are no
longer scaling performance-wise. This is because there is only
a limited amount of parallelism that can be extracted from
a single thread of execution, and even extracting this limited
parallelism requires sophisticated superscalar instruction fetch,
dependence check and instruction execution mechanisms. Unfortunately, it is not possible to continuously increase clock
frequency of these architectures as it is well known that, beyond a point, power dissipation becomes a ma jor bottleneck.
In addition to these constraints, with extremely large numbers
of transistors available on today’s microprocessor chips, it is
too costly to design and debug ever-larger and ever-complex
processors in every two or three years. All these trends clearly
point that chip multiprocessors (CMPs) are in very good position to become the next generation mainstream computer
architecture. The fact that there are many applications from
diverse set of application domains that can take advantage
of thread-level parallelism (e.g., embedded multi-media codes,
data-intensive simulation programs) further motivates for CMPs.
Since future technologies oﬀer the promise of being able to
integrate billions of transistors on a chip, the prospects of having hundreds to thousands of cores on a single chip along with
an underlying memory hierarchy and an interconnection system is entirely feasible. Once the number of cores on one chip
passes some technology and architecture dependent threshold,
point-to-point buses will no longer be a suﬃcient interconnect
structure. Thus, it is reasonable to assume that future CMPs
will employ an NoC (network-on-chip) in order to be able to
handle the required communications between the cores in a
scalable, ﬂexible, programmable, and reliable fashion.
Clearly, assigning (mapping) threads and data manipulated
by threads (of a multithreaded application) to CMP nodes is
a challenging task for extracting maximum performance. One
option is to assign threads and data to CMP nodes statically
at compile-time and not to change this assignment throughout
the execution of the application. While an optimizing compiler
can certainly perform thread-data assignments, it is not possible to capture dynamic modulations in data access patterns
when such static mappings are employed. Instead, a dynamic
(runtime) mapping strategy, if its overheads can be kept low,
may be more promising for data-intensive applications which
go through multiple phases during execution.
Data and computation mapping for generic multiprocessors
have been widely studied. From the data mapping angle, previously proposed approaches range from generic data partitioning to techniques speciﬁcally target NoC based systems.
Balasundaram et al [3] propose a static performance estimator
to guide data partitioning decisions for generic data partitioning. In [12], authors present a data layout selection tool that
generates data layout speciﬁcations of distributed-memory machines automatically. Kim et al [13] introduce a dynamic and
static algorithms to partition the cache among the threads.
Panda et al [16] implement a data partitioning algorithm in
embedded processor-based systems. In the multi-core domain,
Guz et al [8] propose Nahalal, an architecture which partitions
data according to its usage as shared versus private data, and
locates the private data close to each processor. Kandemir [10]
present an interprocessor data reuse vector based data locality optimization scheme for CMPs. Jin et al [9] implement a
page-level data to L2 cache slice mapping for CMPs. Chishti
et al [6] propose a data mapping scheme which utilizes replication and capacity allocation to neighboring processors within
a CMP.
Pop and Kumar [17] use multi-threaded processors as resources in NoC based CMPs. They map the concurrent applications to multi-threaded processors of the CMP oﬀ-line.
Chen et al [5] compare two thread scheduling techniques on
CMPs, namely, Parallel Depth First and Work Stealing.
In
852
[2], authors propose a scheduling method for real-time systems
targeted on CMPs. Chou et al [7] propose a run-time strategy
for allocating the application tasks to platform resources in an
NoC. Nuzzo et al [20] present task allocation strategies based
on bin-packing algorithms. Kempf et al [11] propose and evaluate a task mapping framework in the context of heterogeneous
chip multiprocessors.
In this work, our goal is to reduce data access latency by
reducing the distance between the core that requests the data
and the memory that holds the requested data. For this purpose, we ﬁrst present an application-speciﬁc, dynamic thread
assignment strategy for NoC based CMP systems.
In this
strategy, the job of thread-to-core assignment is given to a
helper thread which carries out this task on behalf of the
application and in parallel with the execution of application
threads. Consequently, the resulting thread mappings are customized for an application and can change at runtime to adapt
dynamic data sharing patterns exhibited by the threads of the
application. We implemented this thread mapping strategy
using a simulation platform and compared it to a static mapping scheme. The results from our experimental evaluation
show that application-speciﬁc dynamic thread mapping can
bring signiﬁcant improvements in performance for all CMP
sizes and applications we ran. More speciﬁcally, we achieve an
average execution latency saving of 16.3% over static mapping.
Secondly, we present a dynamic data-to-core mapping scheme,
also implemented using a helper thread. The results collected
show that dynamic data mapping alone brings an improvement of 8.2% on average. More importantly, our integrated
thread-data mapping scheme, which alternates between thread
mapping and data mapping in successive mapping periods
(epochs), takes these savings (over the static mapping) to
29.1% when all the applications are considered. Our results
clearly demonstrate that to achieve the best performance both
thread mapping and data mapping should be considered together. The results also indicate that the integrated scheme
achieves consistent savings under diﬀerent thread counts and
CMP sizes.
2. EXECUTION MODEL
We assume an NoC based CMP architecture in which each
node has a processor core, a private L1 cache (instruction
and data) and a portion of the shared on-chip main memory space. Note that, while the on-chip memory space is distributed across CMP nodes, the entire memory space is logically shared, i.e., a core can access the memory attached to
any node. It should be emphasized however the distance between the node of the requesting core and the node that holds
the requested data element in its memory can make signiﬁcant diﬀerence in performance (in data access latency), and
in fact, our main goal in this work is to reduce this distance
through dynamic thread mapping and data mapping across
CMP nodes.
We assume that, when requested by an application, operating system (OS) gives a set of nodes (called al location in
this paper) to the requesting application and the application
has full control in assigning its threads and data anywhere
within its allocation. We further assume that the allocation
for an application does not change during its execution, until it ﬁnishes. We also assume the existence of two types of
migration support in this architecture: thread migration and
data migration. The former moves a thread from one location in the allocation to another, whereas the latter moves a
data block from one location to another within the allocation.
While it is conceptually possible to modify an application code
(i.e., thread bodies) to insert explicit ”thread move” and ”data
move” instructions, this can have signiﬁcant degradation in
performance of the application. Therefore, in this work, we
explore a diﬀerent option which employs helper threads to migrate threads and data blocks across CMP nodes. Speciﬁcally,
we use two helper threads, which are created at the time the
Thread
Mapping
Data
Mapping
Statistics 
Collection
Solver
Execution
Thread
Scheduling
Figure 1: Thread migrations, data migrations and
thread schedulings. Time ﬂows from left to right.
application has been initiated. One of these threads moves
application threads across the CMP nodes at runtime and the
other moves data blocks. Note that a helper thread can execute in parallel with application threads. The overall goal of
these helper threads is to reduce the distance between the core
that requests a data block and the memory that holds the requested data block. While we focus on a two-dimensional (2D)
mesh-based NoC (i.e., CMP nodes form a 2D mesh), our approach is applicable to other types of on-chip communication
network topologies as well, as long as the target topology is
exposed to our scheme. In the rest of this paper, we use the
terms ”mapping” and ”migration” interchangeably. Basically,
what we mean by ”migrating a thread/data” is to ”re-map” it
from one location to another.
Figure 1 depicts how thread migrations, data migrations
and thread schedulings take place on the timeline. Typically,
thread scheduling (performed by the process scheduler in that
node) is invoked much more frequently; in comparison, data
and thread migrations (carried out by helper threads as explained above) are less frequent and they alternate (in our
integrated scheme)—i.e., a set of thread migrations followed
by a set of data migrations, which in turn are followed by a set
of thread migrations, and so on. In each thread (or data) migration interval, the ﬁrst half of the interval is used to collect
runtime statistics, which indicate the frequency of accesses to
each data block from each core. In the second half, an ILP
(integer linear program) solver is invoked and the new thread
(or data) mapping is determined and implemented. Note that
the new mappings take place only after the ILP solver returns
a solution, and at each step, the number of threads (or data
blocks) to be migrated is dictated by the changes in the data
access patterns of the threads between the previous interval
and current interval.
In the following two sections, we describe the thread migration and data migration components of our approach. As
mentioned above, in our integrated scheme, thread migrations
and data migrations interleave at an (mapping) interval granularity. That is, at the end of the ﬁrst interval (see Figure 1),
threads are migrated; at the end of the second interval, data
blocks are migrated; at the end of the third, threads are migrated again, and so on. For deciding both thread and data
migrations (at each interval), we use the ILP based approaches
explained below.
3. DYNAMIC THREAD MAPPING
We now present our ILP based formulation of dynamic thread
mapping targeting 2D mesh NoC based CMP architecture.
Our goal is to assign these threads to the CMP nodes such
that the overall distance-to-data is reduced across all threads.
To achieve this, our approach tries to place the threads that
share a lot of data between them in close-by locations (cores)
in the CMP.
853
Constant Deﬁnition
X × Y
T
G(T )
E (T )
V (T )
we
Number of CMP nodes
Number of threads
Graph representing thread aﬃnities
Edges in graph G(T )
Vertices in graph G(T )
Weight of edge e or degree of aﬃnity
between the corresponding threads
Unit cost of data access in the CMP
Ccomm
Table 1: The constant terms used in our dynamic
thread mapping formulation. These are either architecture speciﬁc or program speciﬁc. G(T ) is built by
collecting statistics during execution at each interval.
An integer linear program, or ILP for short, tries to solve a
linear ob jective function via linear functional constraints along
with integer solution variables. In 0-1 ILP, however, each solution variable is restricted to be either 0 or 1 [15]. Table 1
gives the constant terms used in our ILP formulation of the
thread mapping problem. Although choice of the ILP tool is
orthogonal to the focus of this paper, we formulated our ILP
problem using a commercial tool, Xpress-MP [1]. Recall that
in our execution model, the ILP solver is invoked by the helper
thread. For the sake of our formulation, we view the CMP as
Assume that the target CMP is composed of X × Y nodes,
a 2D grid, and assign the threads to the nodes of this 2D grid.
where X denotes the number of nodes on the x-axis and Y denotes the number of nodes on the y-axis. We denote a speciﬁc
node in this grid using (x, y).
We use 0-1 integer variables R to indicate where the threads
are mapped (once our solver returns a solution). More specifically,• Ri,j,k : indicates whether thread i is running on (j, k).
The distance between two threads is captured using Dt1 ,t2 .
Speciﬁcally, we have:
• Dt1 ,t2 : indicates the distance between threads t1 and t2 .
We next give our constraints based on these deﬁnitions. A
thread denoted by t can be assigned to a unique node in the
CMP:
XX
YX
j=1
k=1
Rt,j,k = 1,
∀t.
this term with the absolute distance between the two nodes
expression, the ﬁrst term given with (Rt1 ,j1 ,k1 + Rt2 ,j2 ,k2 − 1)
given with (j1 , k1 ) and (j2 , k2 ). As can be observed from this
is going to be either 0 or negative if threads t1 and t2 are not
running on the indicated CMP nodes. However, our ILP solver
will pick the maximum value among all the possible Dt1 ,t2 in
order to satisfy all the constraints. Hence, we will have the
distance in variable Dt1 ,t2 .
Our helper thread employs a weighted directed graph to indicate the data sharing volume between threads. In this graph,
each vertex represents a thread in the program and an edge
between two vertices indicates the data sharing between the
corresponding threads. An edge is annotated with a weight
which is multiplication of the amount of data shared between
the threads and the total number of references made to these
shared data elements by the two threads. Note that this weight
in a sense represents the aﬃnity between the two threads.
Clearly, two threads with high aﬃnity should be as close to
each other as possible in the 2D grid. In other words, placing
two threads with high aﬃnity in locations (on the CMP) far
away from each other can incur high access latencies to data
which should be avoided.
graph representation G(T ), where G(T ) = V (T ) ∪ E (T ) and
In mathematical terms, we map the given threads, T , to its
E (T ) ⊆ V (T ) × V (T ).
As stated above, each edge carries a weight value between
the vertices of the graph. Since we have at this point covered
mapping constraints and distance constraints, we can now give
our ob jective function. Our ob jective function actually targets minimizing the distance between threads of high aﬃnity
and does not consider speciﬁc optimization problems such as
power and performance. However, this formulation, if desired,
can easily be converted to serve such speciﬁc goals. We express our ob jective function using the aforementioned graph
structure as an input. More speciﬁcally, the cost can be captured, for each edge e in G(T ), using the distance between
the two threads designated by the vertices connected by this
edge. This distance is multiplied with the weight of the corresponding edge along with the unit data access cost, which is a
constant. We can express this as follows:
(1)
C omm =
we × Dev1 ,ev2 × Ccomm .
(4)
X
∀e∈E (T )
We also need to make sure that only one thread is mapped
to a node (x, y) (this assumption can be dropped if there are
more threads than CMP nodes, or can be limited by an upper
bound; in our implementation, we have a parameter that limits
the number of threads per node, which is set to 4 in our default
conﬁguration):
TX
i=1
Ri,x,y ≤ 1,
∀x, y .
(2)
We use the Manhattan Distance to capture the cost of the
placement of threads that share data. Manhattan Distance
is given as the distance between two points measured along
axes at right angles. In a plane with p1 at (x1 , y1 ) and p2 at
(x2 , y2 ), it is |x1 − x2 | + |y1 − y2 |. We assume this distance to
be the cost of the relative placements (on the CMP) of thread
t1 and thread t2 . To capture the Manhattan Distance, we use
the variable, Dt1 ,t2 and employ the following constraints:
Dt1 ,t2 ≥ (Rt1 ,j1 ,k1 + Rt2 ,j2 ,k2 − 1) × (|j1 − j2 | + |k1 − k2 |) (3)
∀t1 , t2 , j1 , j2 , k1 , k2 .
Using the above constraint, we capture the distance between
thread t1 and thread t2 . Note that, Rt1 ,j1 ,k1 is 1 if t1 is running
on CMP node (j1 , k1 ), whereas Rt2 ,j2 ,k2 is 1 if t2 is running on
CMP node (j2 , k2 ). The role of the ﬁrst term in the above constraint is to perform a logical ‘and’ on Rt1 ,j1 ,k1 and Rt2 ,j2 ,k2 .
More speciﬁcally, if both of these 0-1 variables are equal to 1,
then the ﬁrst term in the constraint is equal to 1. We multiply
In the above expression, we assume that Ccomm is the unit
cost of data access within the NoC. We multiply this with the
volume of the data access, which is speciﬁed as we , i.e., the
weight of the edge between two threads. Distance, Dev1 ,ev2 ,
captured in our previous constraints, is used to obtain the
overall cost of placement of two threads. It is to be noted that
ev1 and ev2 are the two vertices connected by edge e which
actually represent the two threads. Based on these constraints,
we can express our ob jective function as min C omm.
4. DYNAMIC DATA MAPPING
Our goal in this section is to present an ILP formulation of
the problem of minimizing data access latencies by determining
the optimal placement of data blocks in the CMP. A data block
in this section corresponds to a set of consecutive cache lines.
Each dataset is assumed to be divided into blocks of equal size,
which is the granularity of migration in our scheme across the
memories attached to CMP nodes. Unless stated otherwise,
we allow at most one copy of each data block to exist within
the cumulative on-chip memory space (data cache can have its
composed of X × Y nodes, and we represent a speciﬁc node
own copy). As previously stated, we assume a 2D grid CMP
in this 2D grid using (x, y). Table 2 gives the constant terms
used in our ILP formulation of the data mapping problem.
We use A to indicate where a data block is assigned within
the CMP (once the ILP solver return a solution). More specifically,
854
Constant Deﬁnition
X × Y
D
lm
BS ize
Number of nodes
Number of data blocks
Size of a local memory of a core
Size of a data block
Number of accesses to data block i by processor (j, k)
Cost of moving a data block to an adjacent core
Cost of accessing a data block from the local memory
Cof f −chip Cost of bringing a data block from oﬀ-chip memory
C Ti,j,k
Ctr
Cacc
Table 2: The constant terms used in our dynamic data
mapping formulation. These are either architecture
speciﬁc or program speciﬁc. The values of C Ti,j,k are
obtained by collecting statistics at runtime.
• Ai,j,k : indicates whether data block i is in core (j, k).
Similarly, M is used in our formulation to identify whether a
data block is in oﬀ-chip memory. Observe that the cumulative
on-chip memory space may not be suﬃcient, in general, to
store all the data blocks manipulated by the multithreaded
program.
• Mi : indicates whether data block i is in oﬀ-chip memory.
The distances between a core and a data block is captured
using Di,j,k . Speciﬁcally:
indicates the distance between data block i and
• Di,j,k :
core (j, k).
Based on these deﬁnitions, we can start describing our constraints. A data block given by d needs to be assigned to a
unique core or oﬀ-chip memory:
Md +
PX
QX
j=1
k=1
Ad,j,k = 1,
∀j, k .
(5)
As before, the Manhattan Distance is a factor in determining
the cost when data block d is accessed by processor core (x, y).
This is also referred to as the data access cost in this section,
and is the metric whose value we want to minimize. We want
to remind the reader that a data block can be shared across
multiple cores. To capture the Manhattan Distance, we use
variables Dd,x,y and accommodate the following constraints:
Ad,j,k × (|x − j | + |y − k |)∀j, k .
YX
(6)
Dd,x,y ≥ XX
j=1
k=1
In the above constraint, we capture the distance of data block
d to the corresponding core denoted using (x, y). Note that,
Ad,j,k is used to indicate the location of the data block and
we take the vertical and horizontal distances with the core
(x, y). As can be seen from this expression, in the case of data
not being stored in the on-chip memory, the distance value
given with Dd,x,y will be returned as 0. However, as we will
explain in more detail, the oﬀ-chip memory access costs will
be captured separately.
We also need to make sure that the local memory size (capacity), which is given by lm , is not exceeded. This can be
captured using the following constraint:
Ai,x,y × BS ize ≤ lm∀x, y .
DX
(7)
i=1
block and the location of the core accessing it multiplied with
the cost of moving this data block within the chip, Ctr . On
top of this cost, we have the local access cost which is given by
Cacc . This will then be multiplied by the frequency of these
accesses. As a result, we have:
T Con−chip =
DX
XX
YX
((Di,j,k × Ctr + Cacc ) × C Ti,j,k ). (8)
i=1
j=1
k=1
In a similar fashion, we can compute the oﬀ-chip memory access cost using the following expression:
T Cof f −chip =
DX
XX
YX
((Mi × Cof f −chip + Cacc ) × C Ti,j,k ).
i=1
j=1
k=1
(9)
In the above expression, we assume that Cof f −chip captures
the cost of bringing the data block to the local memory of the
requesting core. That is, we assume that each core can access
the oﬀ-chip memory with uniform latency. Alternatively, if the
accesses are not uniform for some reason, one can introduce
individual oﬀ-chip access costs for each core such as Cx,y for
oﬀ-chip access cost of core (x, y). Thus, our overall ob jective
function can be written as:
min
T C = T Con−chip + T Cof f −chip .
(10)
Within each interval (at the end of which we want to migrate
data blocks), the helper thread collects access statistics for
each data block, build the ILP formulation summarized above,
and invokes the ILP solver to solve it. Based on this solution,
the required data block migrations are performed by the helper
thread.
Data Replication. Recall that so far in our ILP formulation, we assumed that there is only one copy of each data
block, either on-chip memory or oﬀ-chip memory. We can
expand the scope of our formulation by allowing data block
replication within the local memories if this helps to reduce
the total data access costs. We replicate a data block only if
the block is read-only (to avoid coherence-related issues). In
order to reﬂect these changes on our baseline formulation, we
modify the constraint given in Expression 5 as follows:
Md +
XX
YX
j=1
k=1
Ad,j,k ≥ 1,
∀j, k .
(11)
This modiﬁed constraint enables us have multiple copies of
the data block in diﬀerent local memories, i.e., for a speciﬁc
data block d, we can have multiple Ad,j,k = 1. However, this
requires additional modiﬁcations such as selecting the closest
data block in case of multiple data blocks residing within the
local memories. In order identify the closest data block, we
deﬁne an additional 0-1 variable, Nd,j,k,l,m . More speciﬁcally:
indicates whether core (j, k) can access the
closest data block i from core (l, m).
A core (j, k) can only access a data block d if it resides in
the said core (x, y), therefore:
• Ni,j,k,l,m :
Nd,j,k,x,y ≤ Ad,x,y ,
∀j, k .
(12)
This expression sums up the total amount of space required
to keep the assigned data blocks within the local memory of
core (x, y). BS ize indicates the data block size being used
and may vary depending on the implementation and the data
block granularity. Although it is possible to add a constraint
to check the overall on-chip memory size, this is not necessary
as we do it for individual local memories.
We next discuss our ob jective function. For clarity reasons
we give our ob jective function in two main components. First,
on-chip data access cost can be captured, for each data block
access, using the distance between the location of that data
Also, we have to make sure that there is one and only one
closest data block d for a core (j, k). Note that, if the data
block is not located within local memories, then the closest
location is the oﬀ-chip memory, which can be expressed as
Md . In mathematical terms:
Md +
XX
YX
l=1
m=1
Nd,j,k,l,m = 1,
∀j, k.
(13)
In addition to these constraints, we have to modify Expres855
NoC topology
Core (fetch,issue,retire width)
L1 cache
On-chip memory
Scheduling frequency
Mapping frequency
Data block size
5 × 5 2D mesh
(4,2,2)
16KB per node, 2-way, 2 cycle
256KB per node, banked, 10 cycle
50 msec
500 msec
512 bytes
Program Application
Domain
Graphics
General
Graphics
Graphics
Simulation
radiosity
radix
raytrace
volrend
water
Dataset
Size (MB)
6.44
5.68
4.96
7.26
5.17
Execution
Latency (sec)
7.21
6.17
5.89
8.73
5.37
Table 3: Our simulation parameters and their default
values.
Table 4: Benchmark codes used in our evaluation. The
last column gives the execution latency values for the
baseline static (thread and data) mapping scheme.
sion 6 accordingly:
Dd,x,y ≥ XX
l=1
YX
m=1
Nd,x,y,l,m × (|x − l| + |y − m|)∀l, m.
(14)
As explained earlier, we only allow a certain number of copies
for a given data block, which is τ . This requirement can be
enforced using:
XX
YX
j=1
k=1
Ad,j,k ≤ τ ,
∀d.
(15)
In the above expression, the total number of copies of data
block d stored within the on-chip memory space is captured
with the sum. Finally, our ob jective function give earlier remains the same since both T Con−chip and T Cof f −chip are the
same as in our original ILP formulation.
5. EXPERIMENTAL EVALUATION
The baseline thread and data mapping used in our experiments is a static one, where a compiler determines appropriate data-thread mapping statically and these mappings do
not change during the course of execution. Note that, this
static scheme is in fact derived from the ILP based formulations explained in Sections 3 and 4. Speciﬁcally, we analyze
the entire application and record the number of accesses made
by threads to data blocks and the aﬃnities between thread
pairs (for the entire execution). Once these counts have been
obtained, we ﬁrst execute our thread mapping scheme and
then our data mapping scheme. Note that, this baseline static
mapping scheme can also be seen as a speciﬁc instance of the
dynamic mapping scheme where the mapping interval is set
to the entire execution period. In the results presented in the
next subsection, all values are given as normalized with respect
to this static scheme. In addition to this static scheme, we conducted experiments with the following dynamic schemes:
• TM. In this scheme, data mapping is decided statically by
the compiler (as in the static mapping case) and only dynamic
thread mapping is applied at runtime.
• DM. This is in a sense the opposite of scheme TM. In
this scheme, thread mapping is ﬁxed at compile time (as in
the static mapping case) and only dynamic data mapping is
exercised at runtime.
• TM+DM (C). This is our integrated scheme which exercises both data and thread mapping at runtime. The initial
data and thread mappings are decided by the compiler. And at
runtime, we use data and thread mappings in alternate fashion
as explained earlier.
• TM+DM (R). This is similar to the previous scheme except that the initial thread and data mappings are random,
that is, threads and data blocks are assigned to the CMP
nodes randomly at compile time but we employ the integrated
scheme at runtime.
All these mapping schemes are implemented on top of SIMICS [14] and have been tested using the platform summarized
in Table 3. Recall that the target architecture is a shared memory based one, i.e., although each core has a portion of the onchip memory space (as its local memory), all on-chip memory
space are accessible (albeit with diﬀerent costs) by all cores.
Table 4 on the other hand lists the benchmark codes used in
this study. We selected ﬁve parallel benchmark codes from
the Splash-2 benchmark suite [19] and executed them under
all the mapping schemes discussed above. For each benchmark
code in our experimental suite, 100 threads are generated and
mapped onto 25 CMP nodes (we later present results with different number of threads and diﬀerent number of CMP nodes).
Finally, in all the experiments we made, the default node-level
thread scheduling scheme is round-robin (clearly, if there is
only one thread per node, no scheduling is needed).
Figure 2 gives the normalized execution latencies under different mapping policies. Our ﬁrst observation is that while
both TM and DM generate better results than the static threaddata mapping, the integrated scheme generates the most savings for all the benchmarks. The second observation is that
the diﬀerence between TM+DM (C) and TM+DM (R) is not
signiﬁcant, meaning that initial mapping may not matter too
much if the dynamic re-mapping is activated at runtime. We
also see that TM performs better than DM. This is mainly
because, in general, a data block can be shared by more than
one thread and positioning threads with respect to that data
turns out to be easier than ﬁnding a good location for that
data. The average execution latency improvements brought
by TM, DM, TM+DM (C) and TM+DM (R) over the static
scheme are 16.3%, 8.2%, 29.1% and 26.7%, respectively.
The sensitivity of these mapping schemes to the number
of threads is presented in Figure 3. Each bar in this plot
represents the average (normalized) execution latency across
all ﬁve benchmarks. Recall that the default number of threads
was 100 for our benchmarks. We see from these results that the
integrated scheme consistently generates good results for all
the thread counts considered. The results are a bit higher with
larger thread counts as it gives more ﬂexibility to our mapping
scheme. Figure 4 on the other hand gives the sensitivity of the
mapping schemes with respect to the number of nodes.
In
addition to our default 5 × 5 mesh, we experimented with 4 ×
4 and 6 × 6 mesh sizes (and the number of threads is ﬁxed at
100 in this experiment). The average execution latency results
are presented in Figure 4 indicate that all dynamic mapping
schemes achieve slightly better results with larger meshes as
larger meshes give more ﬂexibility to a dynamic approach in
moving data and threads across CMP nodes.
We next study the sensitivity of our dynamic mapping schemes
to the mapping frequency. Recall from Table 3 that the mapping frequency used in experiments so far was 500 msec. The
results with mapping frequencies of 100 msec, 200 msec, 800
msec, and 1000 msec are shown in Figure 5. Maybe the most
important observation from these results is that working with
very low or very high frequencies may not be the best option.
Though the results change from one benchmark to another, we
see that, for each benchmark, there is an ideal value (among
the frequency values tested) that generates the maximum improvement in execution latency.
So far in our experiments no data block is replicated across
CMP nodes, i.e., each data block has exactly one copy in the
on-chip memory space. Figure 6 plots the results when each
data block is replicated n times (x-axis), where 1 ≤ n ≤ 5.
Note that, when n is 1, we have no replication. We note
that, as may be expected, the DM scheme takes advant"
Cost-driven 3D integration with interconnect layers.,"The ever increasing die area of Chip Multiprocessors (CMPs) affects manufacturing yield, resulting in higher manufacture cost. Meanwhile, network-on-chip (NoC) has emerged as a promising and scalable solution for interconnecting the cores in CMPs, however it consumes significant portion of the total die area. In this paper, we propose to decouple the interconnect fabric from computing and storage layers, forming a separate layer called Interconnect Service Layer (ISL), in the context of three-dimensional (3D) chip integration. Such decoupling helps reduce the die area for each layer in 3D stacking. ISL itself can integrate multiple superimposed interconnect topologies. More importantly, ISL can be designed, manufactured, and tested as a separate Intellectual Property (IP) component, which supports multiple designs in the computing and storage layers. The resulting methodology also helps support different manufacturing volume in each die of 3D to reduce the overall manufacturing cost. We demonstrate the proposed methodology with an ISL design example and compare to its 2D and 3D counterparts without ISL support. The results show that 3D design with ISL not only provides significant cost reduction, but also achieves power-performance improvement thanks to the efficient usage of ISL.","Cost-driven 3D Integration with Interconnect Layers ∗
10.2
†
Xiaoxia Wu
, Guangyu Sun,
Xiangyu Dong, Reetuparna Das,
Yuan Xie, Chita Das
Pennsylvania State University
University Park, PA 16802
{xwu}@cse.psu.edu
ABSTRACT
The ever increasing die area of Chip Multiprocessors (CMPs)
aﬀects manufacturing yield, resulting in higher manufacture
cost. Meanwhile, network-on-chip (NoC) has emerged as
a promising and scalable solution for interconnecting the
cores in CMPs, however it consumes signiﬁcant portion of
the total die area.
In this paper, we propose to decouple
the interconnect fabric from computing and storage layers,
forming a separate layer called Interconnect Service Layer
(ISL), in the context of three-dimensional (3D) chip integration. Such decoupling helps reduce the die area for each
layer in 3D stacking. ISL itself can integrate multiple superimposed interconnect topologies. More importantly, ISL can
be designed, manufactured, and tested as a separate Intellectual Property (IP) component, which supports multiple
designs in the computing and storage layers. The resulting
methodology also helps support diﬀerent manufacturing volume in each die of 3D to reduce the overall manufacturing
cost. We demonstrate the proposed methodology with an
ISL design example and compare to its 2D and 3D counterparts without ISL support. The results show that 3D design
with ISL not only provides signiﬁcant cost reduction, but
also achieves power-performance improvement thanks to the
eﬃcient usage of ISL.
Categories and Subject Descriptors
C.0 [Computer Systems Organization]: GENERAL—
Systems speciﬁcation methodology
General Terms
Design
Keywords
Three-dimensional Integrated Circuit, Network-on-Chip, Interconnect Service Layer
We thank Lixin Zhang and Steve Vander Wiel for their
guidance and support. This work was supported in part by
NSF 0905365, 0903432, 0702617, and SRC grants.
Xiaoxia Wu is now with Qualcomm, San Diego, CA 92121.
†
∗
Jian Li
IBM Austin Research Laboratory
Austin, TX 78758
{jianli}@us.ibm.com
1.
INTRODUCTION
The diminishing return of endeavors to increase clock frequencies and exploit instruction level parallelism in a single processor have led to the advent of chip multiprocessors
(CMPs). As the number of cores in CMPs increases aiming for higher computation throughput, die size gradually
increases as well. Consequently, the manufacturing yield
suﬀers, which leads to higher manufacturing cost. Meanwhile, networks-on-chip (NoC) has emerged as a promising
and scalable solution for interconnecting the cores in CMPs.
A rich collection of NoC literature exist. Nonetheless,
challenges for future many-core CMP design remain. For
example, current NoC designs lack ﬂexible support for costeﬀective power-performance improvement of future manycore CMPs. First, it consumes much chip area and power
with increased number of cores in CMPs, which makes the
chip bigger, more power hungry, and constraining the number of cores (computing) and the capacity of on-chip cache
memory (storage) [9]. Second, the current interconnect fabric is typically ﬁxed for one chip design, since it is integrated
with processors and cache memory within one single die.
Reuse of the interconnect fabric for future chip generations
is diﬃcult, resulting in both design and manufacturing overhead.
On the other hand, three-dimensional (3D) integration
technology has become a promising means to mitigate the
power-performance related problems in conventional 2D chips,
such as dominant interconnect delay and power consumption [7, 17]. In 3D ICs that are based on TSV technology,
multiple active device layers are stacked together (through
wafer stacking or die stacking) with direct vertical TSV interconnects [17]. One important beneﬁt of 3D ICs is that it
provides the opportunity of stacking dies with diﬀerent technologies, processes, and vendors [17]. 3D technology also
reduces the die area in each layer and may provide cost eﬃciency beneﬁt [8].
Putting these together, we propose to decouple the interconnect fabric from the computing (core) and storage (cache)
layers as a separate layer, called Interconnect Service Layer
(ISL), in 3D stacking. This decoupling can provide reduced
manufacture cost since each layer has smaller die area in
3D. It can oﬀer more reliable and ﬂexible interconnect layer
compared to its traditional 2D counterparts. The decoupled
ISL has the real estate for more than one on-chip network,
e.g., it can support multiple on-chip networks in a single die
such as mesh, ring, hierarchical topologies, etc. With ISL,
the constraints on the router area and link bandwidth in
2D can be relaxed. It also supports diﬀerent manufacture
volume for each die in 3D to reduce the overall cost. For
example, our proposed ISL can be manufactured with much
larger volume than the other computing and storage layers.
Then it can be bonded to those with varied designs, such as
150
10.2
diﬀerent number of cores and storage capacity.
This paper makes the following contributions:
1) We map computing, communication and storage (including cache) functions to diﬀerent layers in 3D stacking.
Particularly, we extract communication functions as a single
layer called ISL for ﬂexible 3D integration.
2) The ISL can be designed, manufactured and tested as
a separate IP component. This allows designs of more ﬂexible and reconﬁgurable interconnect fabric. Particularly, we
propose an architecture with multiple superimposed heterogeneous networks for ISL. The ISL can potentially consist
of M multiple networks, each network providing a separate
degree of ﬂexibility and communication capacity. One or
more of these M networks can be active simultaneously at
runtime.
3) We evaluate one speciﬁc example of ISL and demonstrate the cost beneﬁt and performance improvement. The
evaluation results show that the cost reduction of our proposed architecture could be up to 40% compared to 2D case.
The performance improvements are 21% and 6.5% in average
compared to 2D and 3D without ISL design, respectively.
4) We extend existing 3D cost models by modeling the
number of TSVs for power delivery, diﬀerentiating cost models between diﬀerent functional layers, and addressing product volume factor of each layer in 3D integration.
2.
INTERCONNECT SERVICE LAYER
Typical 3D stacking architectures include logic-to-logic stacking [4] and logic-to-cache/memory stacking [16]. Figure 1 (a)
illustrates a logic-to-cache structure. In this structure, the
cache layer is stacked to the computing (processor) layer,
while the interconnect network is integrated in computing
and cache layers. Figure 1(b) illustrates the proposed 3D
stacking structure with interconnect service layer (ISL). The
interconnect layer consists of routers and links to connect
the computing layer and cache layer. Since the whole layer
is dedicated for the interconnect it has room to support multiple networks of diﬀerent granularity and topologies, such
as mesh, ring, hierarchical topology, etc.
Cache layer
y
Through
Silicon
Vi
Vias
(TSVs)
Compute layer
T diti
Traditional Cache Stacking
l C h St
ki
(a)
Cache layer
Interconnect layer
Compute layer
Our proposed architecture
d
(b)
Figure 1: (a) Logic-cache 3D stacking. (b) 3D stacking with interconnect service layer.
2.1 Advantages
A separate interconnect service layer (ISL) decouples the
active logic components (computing and storage) from interconnect fabric, reducing greatly the pre-fabrication and
post-fabrication veriﬁcation complexity, hence cost. The
ISL can be designed, manufactured and tested as a separate IP component. This allows network architects to design
more ﬂexible, adaptive and reconﬁgurable interconnect fabric. We propose multiple superimposed heterogeneous networks in ISL. The ISL can potentially consist of M multiple
networks, each providing a separate degree of ﬂexibility. One
or more of these M networks can be active simultaneously at
runtime. We enumerate the beneﬁts of ISL in the following:
Latency. One or more of the M multiple networks can be
optimized for providing low latency for latency critical applications. For example, concentrated and richly connected
topologies (e.g., Flattened Butterﬂy and Hierarchical) are
oriented best towards providing low latency.
Bandwidth. One or more of M multiple networks can be
optimized for providing high bandwidth. For example, a ﬂat
topology with many routers and many buﬀers can provide
high throughput. The throughput provided by mesh can be
2X higher than butterﬂy or hierarchical topology.
Power. One or more of M multiple networks can be optimized for low power to meet the stringent power constraints
imposed by computing and storage layers. The power eﬃciency of a network can be programmed by using diﬀerent
frequency domains, as well as using small data path widths
for routers and channels.
2.2 ISL Example
In this section, we describe one speciﬁc example of the
ISL design with two meshes of diﬀerent granularity. We
assume a range of CMP designs with up to 36 processor
cores. Figure 2 illustrates a possible conﬁguration for the
proposed 3D stacking architecture. There are two superimposed meshes in the interconnect layer: 6x6 ﬁne-grained
mesh and 3x3 coarse-grained mesh, which are highlighted
in dark and light, respectively. The reason we choose mesh
topologies is because of its simplicity and its scalability for
global network [6]. In a 36-core design, each router in the 6x6
ﬁne-grained mesh is connected to one core. Four cores are in
a cluster in the 3x3 coarse-grained mesh. A bus, cross-bar
or point-to-point interface can be used to connect the two
meshes. Each mesh is self-suﬃcient and supports typical
communication traﬃc. With this interconnect layer, we can
stack a processor layer with 36 small cores (6x6) and their
caches, 9 big cores (3x3) with their caches, 3 big cores and
24 small cores and their caches, etc, all of which ﬂexibly use
the same interconnect layer for chip stacking.
3x3coarseͲgrainedmesh
6x6fineͲgrainedmesh
BusorxͲbarorpointͲtoͲpoint
p
p
interfacebetweenthetwomeshes
3: Cache layers
3: Cache layers
2: Interconnect layer
1 P
1: Processor layer
l
Routers
WiresWires
Figure 2: An ISL example for 3D chip designs of up
to 36 (6x6) cores.
The two meshes can be used coordinately to improve the
performance. For 6x6 multi-core integration, 3x3 coarsegrained mesh provides fast-path cross-chip interconnect. For
example, if there is communication between upper left corner core and lower right corner core, 10 hops are needed if
no 3x3 coarse-grained mesh is integrated. With the coarsegrained mesh, only 6 worst-case hops are needed. Figure 3
shows an elaboration on supporting 3x3 cores with 6x6 cache
banks. Each core has its corresponding 4 cache banks. The
ﬁne-grained interconnect fabric supports local data communication between neighbor cores/caches, whether or not the
cores belong to the same cluster under a router in the coarsegrained mesh. The coarse-grained interconnect supports faster
global data communication between cores/caches that are
further apart. For example, if there is communication between Core 0 and Core 8, there are fewer hops and higher
link bandwidth between them.
2.3 Architecture
In this section, we present the design of the ISL such as
the router design, the TSVs connections, and the routing
151
10.2
CorelayerwithL1s
Cachelayer
Core0
Core1
Core2
Core3
Core4
Core5
Core6
Core7
Core8
0.0
0.2
3 0
3.0
3.2
6.0
6.2
0.1
0.3
3 1
3.1
3.3
6.1
6.3
1.0
1.2
4 0
4.0
4.2
7.0
7.2
1.1
1.3
4 1
4.1
4.3
7.1
7.3
2.0
2.2
5 0
5.0
5.2
8.0
8.2
2.1
2.3
5 1
5.1
5.3
8.1
8.3
Figure 3: Elaboration on supporting 3x3 cores with
6x6 cache banks.
scheme to support the superimposed mesh topology.
2.3.1 Router Microarchitecture
The key concept is to use NoC routers for communications
within the interconnect layer, while using a speciﬁc through
silicon bus (TSB) for communications among diﬀerent layers. Figure 4 illustrates an example of the structure. There
are 4 cores located in the core layer, 4 routers in the interconnect layer, and 16 cache banks in the cache layer and all
layers are connected by through silicon bus (TSB) which is
implemented with TSVs. This interconnect style has the advantage of short connections provided by 3D integrations. It
has been reported the vertical latency of traversing a 20-layer
stack is only 12ps [12], thus the latency of TSB is negligible
compared to the latency of 2D NoC routers. Consequently,
it is feasible to have single-hop vertical communications by
utilizing TSBs. In addition, hybridization of 2D NoC routers
with TSBs requires one (instead of two) additional ports on
each NoC router, because TSB can move data both upward
and downward [10].
Cache Bank
Router
Router
Through
Silicon
Bus
Bus
Core
Through
Silicon Bus
(TSB)
Layer3
Vertical Hop
Layer2
Layer1
R
R
R
R
Horizontal Hop
Figure 4: Elaboration of TSBs and routers in a setup
of 2x2 cores with 4x4 cache banks.
2.3.2 Routing
The interconnect service layer provides routing both within
each superimposed network and between them. The need
for routing between the independent networks is to facilitate ﬂexibility and improve utilization of network resources.
Intra-network communication can be supported by simple
baseline routing schemes. Simple extension to default dimensionordered-routing can be implemented if the network supports
a regular topology. For example, our previous example has
the mesh topology, which we evaluate in this paper.
If
any one of the networks has irregular topology, applicationspeciﬁc architecture or ﬂow-based topology mapping for guaranteed services, the router needs to support table-based routing and arbitration. To enable inter-network communication, special inter-network input/output ports are provided
at speciﬁc routers of each independent network which connect it to other networks. Thus each independent superimposed network has fewer designated “interface routers”
which connect it to separate networks. To avoid possibility of deadlock at interface routers, egress and ingress traﬃc
have dedicated virtual channels in these routers. In addition
to operating the superimposed networks independently, the
interface router’s routing tables can also be programmed for
fusion of multiple networks into a larger monolithic network.
We skip the rich details of this design space due to limited
space.
3. 3D COST MODEL
In 3D integration, the manufacturing cost reduction may
come from smaller die area of each layer as well as reduced
number of metals for routing. The number of metals is predicted by a 3D routability model, which is based on the wire
length distribution [8]. As described in [8], when a large 2D
chip is partitioned into multiple smaller dies in 3D, the cost
could be reduced due to fewer number of metal layers needed
for each smaller die although extra bonding cost is needed
in 3D stacking.
Our cost model is based on a few prior art, particularly the
one by Dong et al. [8] with several improvements: (1) It models the number of TSVs for power delivery; (2) It diﬀerentiates cost models for diﬀerent layers (logic/cache/interconnect);
3) It adds mask cost, design cost, addresses (product) volume factor of each layer, and addresses (production) time
factor of each layer.
It is important to estimate the number of TSVs and its
impact on the die area in 3D stacking since the area overhead
caused by TSVs could be signiﬁcant depending on the TSV
pitch and the design. The TSVs modeled in [8] includes
only signal TSVs while the TSVs used for power delivery
is not considered. However, for example, the power grid
distribution via TSVs in 3D is important to leverage on-chip
power density. Therefore, we take into account the TSVs for
power delivery.
The estimation on the number of TSVs for power delivery is based on voltage drop caused by TSVs. Assume the
allowed maximum voltage drop is d% and the resistance of
one single TSV is Rtsv , the number of TSVs is estimated by
Ntsvp = Rtsv /(d% ∗ V /(P /V )) = Rtsv ∗ P /0.01 ∗ d ∗ V 2
where Ntsvp is the minimum number of TSVs needed for
power delivery, P is the total power consumption, V is the
power supply voltage. Based on the power and the voltage,
the total resistance caused by TSVs can be estimated. Since
all the resistance of TSVs are connected in parallel, the number of TSVs needed is obtained from the total resistance and
the resistance of one single TSV.
In addition to adding TSVs for power, we also distinguish
the cost for diﬀerent layers such as logic layer (core and
interconnect) and cache layer. The ﬁrst reason is that cache
layer may use fewer number of metal layers than the logic
layer due to its regularity. Second, diﬀerent layers may not
have the same die area so that it is necessary to diﬀerentiate
the cost for each single layer.
Another improvement is that we address (product) volume factor of each layer. In our proposed architecture, the
interconnect layer “ISL” can be reused in diﬀerent 3D designs because it is designed, manufactured and tested as a
separate IP component and it could provide multiple superimposed heterogeneous networks. This allows ISL to be
stacked with diﬀerent functional units (diﬀerent number of
cores) and various capacity of storage depending on diﬀerent
applications/designs to reduce the total cost. After considering volume, the cost of each die (layer) is deﬁned as follows [14]: C = N RE /V olume + C ostdie , where NRE stands
for non-recurring engineering,
including mask and design
cost, C ostdie is calculated using the model from [8], which
mainly considers the die cost including the wafer cost, the
wafer yield, the defect density, and the extra cost caused
by 3D bonding. We estimate the design cost and the mask
cost based on [5], in which these two metrics at diﬀerent
technology nodes are provided.
152
10.2
The overview of our 3D cost model is shown in Fig. 5,
with our improvements indicated in the three ovals in the
right side. Similar to [8], the key factor of the die cost
model is the die area. We assume that the wafer cost, the
wafer yield, and the defect density are constant for a speciﬁc foundry using a speciﬁc technology node. The extra
fabrication steps required by 3D integrations consist of TSV
forming, thinning, and bonding. The entire 3D cost model
also depends on the volume of each single die and some design options, such as Die-to-Wafer/Wafer-to-Wafer bonding,
Face-to-Face/Face-to-Back bonding, and Known-Good-Die
cost in addition to the wafer cost model and the bonding
cost model.
Wafer Cost 
Model
Bonding Cost 
Model
F2F / F2B 
Bonding
Die Cost
Die Yield
Bonding Cost
Bonding Yield
W2W / D2W 
Stacking
KGD Test Cost
3-D Cost 
Model
3D Chip Cost
TSVs estimation 
for clock/power
Cost for logic/
interconnect/
arrays
Volume 
information for 
each die
Figure 5: Overview of the proposed 3D cost model.
4. METHODOLOGY
System Conﬁguration: Our baseline conﬁguration is a
36-core in-order processor using the Ultra SPARC-III ISA.
We use McPAT [11], an integrated power, area, and timing
modeling framework, to estimate the area of the cores in
45nm technology. The area of one core is estimated to be
6.8mm2 . By using CACTI [15], we further obtain that one
cache layer ﬁts to approximately 36MB SRAM L2 cache, assume the cache layer has similar area to the core layer. The
conﬁgurations are detailed in Table 1. We use the Simics
toolset [13] for performance simulations. We also evaluate
9-core processor with 9MB L2 cache conﬁguration for the
performance using diﬀerent network topologies. The parameters will be described in Section 5.
Table 1: System conﬁguration.
Processor
L1
L2
Memory
Router lat.
36-core, in order, 2GHz
32KB DL1/IL1 per core, 128B, 2-way, 2 cycles
36MB shared cache, 1MB per bank, 8-way, 10 cycles
400 cycles latency, 16MB large page
5 cycles
Workloads: We use a set of workloads from SpecOMP2001[1]
and PARSEC [3]. For each benchmark, we fast forward the
benchmark to the program phase of interest and warm up
the caches, then 3 billions of cycles are simulated in detailed
mode. The instruction throughput of all the cores are used
as the metric of performance.
5. EXPERIMENTS AND RESULTS
In this section, we evaluate ISL, with the example of superimposed mesh networks (Section 2), by comparison against
its 2D counterpart and a 3D design without ISL, in chip
area, cost, and performance.
5.1 Area estimation
In 3D stacking, multiple layers may or may not have the
same die area. As we mentioned in Section 3, the total cost
of 3D stacking is related to the area of each layer and we
distinguish the cost for diﬀerent layers. For our ISL design,
we can distribute the routers aligned with the cores (one
router is vertically below one core). Or, we can minimize
the area of this layer by centralizing/shrinking the routers
with redistribution interconnect layer in both core layer and
cache layer, if the two mesh networks consume less real estate
than other two layers. In the latter, one extra metal may be
added to the core and cache layers for the horizontal routing.
The area of ISL includes router area and link area for two
mesh topologies, the interface area and link area to connect
two networks, and TSV area occupied for signal and power
delivery.
We estimate the router area Arouter by using McPAT [11]
based on 45nm technology.
In order to support layer-tolayer communication in 3D stacking, hybridization of 2D
NoC routers with Through Silicon Buses (TSB, which consists of many TSVs) requires one additional link on each
NoC router, because TSB can move data both upward and
downward [10]. We feed related parameters into McPAT and
obtain the router area 0.93mm2 . The area of the link depends on the wire width and space as well as wire length,
which can be changed in ISL depending on the area of routers
and the interface design between two mesh topologies.
In our ISL example, there are 45 signal TSBs (6x6 + 3x3
= 45) pierce through all three layers. We provide one TSB
for each core in the structure. Consider the range of TSV
pitch size of 1 − 100μm, the area of one TSB AT SB for a
1024-bit TSB (one cache line) would consume diﬀerent area
overhead depending on the TSV pitch. Note that we assume face-to-back bonding in our 3D design so that TSVs
are formed by punching through the silicon substrate, resulting in extra area. Table 2 summarizes the area for a single
TSB (1024 TSVs) depending on diﬀerent TSV pitch. If the
TSV pitch is smaller than 10μm, the TSB area overhead may
be negligible. However, when the TSV pitch is larger than
60μm, the area of one TSB is even comparable to the area
of one core. We assume the maximum TSV pitch in our
cost evaluation is 40μm. The estimation of TSVs for the
power delivery is based on the model proposed in Section 3.
Assume P is 100W , supply voltage is 1V olt, the resistance
of one single TSV is 40mΩ [2], and the voltage drop is 1%,
then the number of TSVs needed is 400. Considering larger
TSV resistance and higher power consumption, the number
of TSVs for power delivery is increased but we expect it will
not exceed 2 TSBs used for signals (2048). To summarize, we
consider the area overhead caused by TSVs to be 47 TSBs.
Table 2: The area of one TSB (1024 TSVs).
TSV pitch(μm)
TSB area(mm2
)
1
0.001
10
0.103
20
0.411
40
1.645
60
3.699
100
10.28
We use a bus to connect the two meshes, due to its high
bandwidth and low latency for local communications [6]. If
we assume the link/bus can be routed over the routers then
the minimum area for the ISL is only the area of routers plus
some routing overhead. Nonetheless, a range of ISL area is
evaluated in the cost analysis.
5.2 Cost analysis
5.2.1 Die cost without volume analysis
In this section, we analyze the die cost (without volume
consideration) for ISL design with diﬀerent area consumption and compare it against its 2D counterpart and a 3D
153
10.2
286.09, 313.61 for these 6 cases. We ﬁnd that 3D with ISL is
most cost eﬃcient if TSV area overhead is reasonable but the
exact boundary for TSV pitch depends on the design itself.
Therefore, it is essential to take the TSV area, including signal and power delivery etc, into account, when we evaluate
a 3D design at its early stage.
5.2.2 Volume analysis
NoͲreuse
Reuse
600
600
500
400
300
)
.
u
.
a
(

t
t
s
o
200C
100
0
1
2
5
10
20
30
40
50
Volume(million)
Figure 7: Volume analysis for non-reuse and reuse.
In our ISL architecture design, the ISL could be stacked
with diﬀerent functional units (e.g., diﬀerent number of cores)
and various capacity of storage (diﬀerent capacity of L2
Cache). With die reuse, the total cost for multiple applications could be reduced. Assume we have two 3D designs,
each with 3 layers. The ﬁrst design has 36 cores as layer 1,
interconnect layer as layer 2, 36M L2 Cache as layer 3. The
second design has 9 cores as layer 1, interconnect layer as
layer 2, 9M L2 Cache as layer 3. If no reuse is enabled, the
total cost for these two designs is calculated as
C osttotal =
6(cid:2)
i=1
(N REi/V olumei ) +
2(cid:2)
j=1
3Dj
There are total 6 layers in these two designs and each layer
has its own NRE cost and volume. With the reuse, since the
NRE part of the total cost is reduced, i.e., two ISL layers
use the same design, thus only one design cost and mask
cost need to be considered. Here we provide an example to
illustrate the volume impact on the total cost for these two
applications.
We obtain 3Dj from our cost model without volume consideration. For volume related cost at 45nm technology, the
design cost and mask cost are either from or scaled from [5].
Note that we assume 3D design does not introduce extra design cost compared to 2D once the 3D design ﬂow is mature.
Fig. 7 illustrates the cost comparison between the non-reuse
and reuse cases. The result shows that the reuse is very cost
eﬀective when the volume is low. However, when the volume
is very high, the NRE cost is not dominant then the cost difference is reduced. Note that if the cache layer can also be
reused (have the same capacity for these two applications)
then the cost reduction will be improved.
5.3 Performance result
We ﬁrst examine the performance of the example ISL architecture with 36 cores. The cache size is estimated based
on the area of cache layer using Cacti [15]. For 36-core system, we assume there is a 36-bank shared L2 cache with
36MB capacity. Each core is connected to a cache bank
using TSB. The cache controller is integrated with the processor layer. The ﬂoorplans for these three cases are shown
in Fig. 8 (only 4 cores are illustrated for the simplicity).
Fig. 8(a) illustrates 2D conﬁguration, in which one core and
one cache bank is placed together and connected to other
design without ISL. In our ISL example design, the area of
the core and cache layer is 228mm2 without TSV area overhead. If the TSV area overhead is considered then the maximum extra area caused by TSVs is 75mm2 (40μm pitch),
which is 1/4 of the total die area of processor die. The area
of ISL may be changed depending on the router design, the
interface design between two mesh networks, and the TSV
area overhead.
In order to perform comprehensive analysis, the ISL area is varied from 60mm2 to 228mm2 without
TSV overhead. We also evaluate TSV area overhead with
diﬀerent TSV pitches. Three TSV area overhead cases are
evaluated: 0mm2 , 30mm2 , and 75mm2 . The default number of metal layers is 9, 6, and 6 for core layer, cache layer,
and ISL, respectively. When ISL area is smaller than core
and cache layers, one extra metal layer is needed in core
and cache layers for horizontal routing to connect to TSBs.
Fig. 6 illustrates the cost comparison for diﬀerent ISL area
and TSV area overhead. The X-axis represents the ISL area
for diﬀerent routers and interface design. We observe that
the total cost increases with the increased ISL area. When
the ISL area is smaller than the core and cache layers one
extra metal is needed for core and cache layer for the routing; however, the cost reduction caused by reduced ISL area
oﬀsets the extra metal cost. We also observe that the extra cost caused by TSV area overhead could be signiﬁcant
if TSV pitch is large. This indicates that it is important
to take the TSV area into account when we evaluate 3D at
early design stage.
NoTSVoverhead
300
30mm^2TSVoverhead
75mm^2TSVoverhead
250
200
150
)
.
u
.
a
(

t
t
s
o
100C
50
0
60
100
140
ISLarea(mm2)
180
230
Figure 6: Cost comparison for diﬀerent ISL area and
TSV area overhead.
We also evaluate the cost for 2D design and 3D design
without ISL. For 2D design, we place 36 cores, 36 L2 cache
banks, and ﬁne-grained mesh network in one layer. Without
loosing generality, the area of the mesh network also varies
depending on the router design and the link area. The 36
cores and 36 cache banks consume 460mm2 . We assume
the total area ranges from 510 to 600mm2 (510, 540, 570,
600 with 9 metals), the total cost is 258.49, 289.80, 322.53,
and 359.61 for these four cases, respectively. We see that
even with smallest network area, the cost of 2D design is
much higher than that of 3D with ISL (maximum area) or
comparable to 3D with ISL with maximum TSV area overhead. The cost reduction of 3D with ISL (maximum area) is
about 40% compared to 2D even with the smallest interconnect area if no TSV area overhead is considered. However, if
maximum TSV area overhead is considered in 3D, the cost
is about 6% higher than 2D with smallest network area.
For 3D design without ISL, the routers are integrated with
cache layer (or computing layer) so that there are two layers
in total. Similarly, the area of these two layers is changed
from 280 to 400mm2 (6 cases: 280, 300, 320, 340, 360 and
400) depending on the router design and TSV area overhead. The total cost is 164.84, 184.58, 206.57, 229.8, 258.47,
154
10.2
Core
Cache Bank
(a)
Router
Cache Bank
Router
Router
Core
Coarse-grained
Router
Fine grained
Fine-grained
Router
(b)
(c)
Figure 8: System conﬁgurations for 2D, 3DB, and
3DI cases.
cache/bank groups by routers. Fig. 8(b) shows 3D baseline
(3DB), in which the ﬁne-grained mesh network is integrated
with core layer. Fig. 8(c) shows 3D design with ISL (3DI), in
which both coarse-grained and ﬁne-grained mesh networks
are integrated in ISL. In 3DI, the coarse-grained mesh reduces the number of hops for global communication. In 2D
case and 3DB cases, there is only ﬁne-grained mesh network
to support the communication. In 2D case, the link latency
is larger due to longer link between two routers. Fig. 9 illustrates the IPC comparison of these three cases, normalized
to 2D case. While it varies between applications, the result
shows that 3DI achieves 21% and 6.5% average performance
improvement over 2D and 3DB, respectively.
2D
3DB
3DI
1 6
1.6
1.4
1.2
1
0.8
0.6
0.6
0.4
0.2
0
Figure 9: Performance comparison among 2D, 3DB,
and 3DI cases.
Coarse
Coarse+Fine
1 2
1.2
1.1
1
0.9
0 8
0.8
0.7
0.6
Figure 10: Performance comparison between multiple networks and coarse-grained network.
We also evaluate the performance of the example ISL architecture with 9 cores, and compare the proposed superimposed (coarse+ﬁne grained) networks with coarse-grained
mesh network. For 9-core system, we assume there is a 36bank shared L2 cache with 9MB capacity. The performance
comparison is shown in Fig. 10. The result shows that ISL
with superimposed (coarse+ﬁne grained) networks has 5.5%
performance improvement than the coarse-grained network.
In summary, we show that our ISL design improves performance by eﬀectively using these two mesh topologies for
diﬀerent system conﬁgurations.
Finally, we have shown the proposed ISL design improve
performance by supporting the selection of networks for ef155
ﬁcient data communication, e.g., reducing hop counts. As
a result, the interconnect power consumption is reduced accordingly.
In addition, the thermal is expected not a big
concern with reduced power consumption. Due to limited
space, we skip the power and thermal evaluation results in
this paper.
6. CONCLUSION
We have demonstrated a methodology that decouples the
interconnect fabric from computing and storage layers, forming a single layer called ISL, in emerging three-dimensional
chip integration technology. This decoupling can reduce
manufacture cost thanks to smaller die area for each layer in
3D. It also supports diﬀerent manufacture volume for each
die in 3D to reduce the overall chip cost. As an example, we
have proposed to superimpose multiple networks in the interconnect service layer for ﬂexible 3D integration. We have extended the state-of-the-art 3D cost model in this work. Our
evaluation shows that a 3D design with ISL not only provides
signiﬁcant cost beneﬁts but also achieves performance-power
improvement, compared to its conve"
A multilayer nanophotonic interconnection network for on-chip many-core communications.,"Multi-core chips or chip multiprocessors (CMPs) are becoming the de facto architecture for scaling up performance and taking advantage of the increasing transistor count on the chip within reasonable power consumption levels. The projected increase in the number of cores in future CMPs is putting stringent demands on the design of the on-chip network (or network-on-chip, NOC). Nanophotonic interconnects have recently emerged as a viable alternate technology solution for the design of NOC because of their higher communication bandwidth, much reduced power consumption and wiring simplification. Several photonic NOC approaches have recently been proposed. A common feature of almost all of these approaches is the integration of the entire optical network onto a single silicon waveguide layer. However, keeping the entire network on a single layer has a serious implication for power losses and design complexity due to the large amount of waveguide crossings. In this paper, we propose MPNOC: a multilayer photonic networks-on-chip. MPNOC combines the recent advances in silicon photonics and three-dimensional (3D) stacking technology with architectural innovations in an integrated architecture that provides ample bandwidth, low latency, and energy efficient on-chip communications for future CMPs. Simulation results show MPNOC can achieve 81.92 TFLOP/s peak bandwidth and an energy savings up to 23% compared to other proposed planar photonic NOC architectures.","A Multilayer Nanophotonic Interconnection Network for
On-Chip Many-core Communications
10.3
Xiang Zhang and Ahmed Louri
Depar tment of Electrical and Computer Engineering, The University of Arizona
1230 E Speedway Blvd.,Tucson, AZ, USA, 85721
{zxkidd, louri}@email.arizona.edu
ABSTRACT
Keywords
Multi-core chips or chip multiprocessors (CMPs) are becoming the de facto architecture for scaling up performance and
taking advantage of the increasing transistor count on the
chip within reasonable power consumption levels. The projected increase in the number of cores in future CMPs is
putting stringent demands on the design of the on-chip network (or network-on-chip, NOC). Nanophotonic interconnects have recently emerged as a viable alternate technology
solution for the design of NOC because of their higher communication bandwidth, much reduced power consumption
and wiring simpliﬁcation. Several photonic NOC approaches
have recently been proposed. A common feature of almost
all of these approaches is the integration of the entire optical network onto a single silicon waveguide layer. However,
keeping the entire network on a single layer has a serious implication for power losses and design complexity due to the
large amount of waveguide crossings. In this paper, we propose MPNOC: a multilayer photonic networks-on-chip. MPNOC combines the recent advances in silicon photonics and
three-dimensional (3D) stacking technology with architectural innovations in an integrated architecture that provides
ample bandwidth, low latency, and energy eﬃcient on-chip
communications for future CMPs. Simulation results show
MPNOC can achieve 81.92 TFLOP/s peak bandwidth and
an energy savings up to 23% compared to other proposed
planar photonic NOC architectures.
Categories and Subject Descriptors
B.4.3 [Hardware]: Interconections—Topology
; C.1.2 [Computer Systems Organization]: Multiprocessors—Interconnection architectures
General Terms
Design, Performance
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2010, June 13-18, 2010, Anaheim, California, USA
Copyright 2010 ACM 978-1-4503-0002-5/10/06 ...$10.00.
Silicon photonics, Interconnection networks, 3D, CMP
1.
INTRODUCTION
The ITRS Semiconductor roadmap [1] predicts that CMOS
feature sizes will shrink from 45nm to sub-22nm regime
within the next 5 years. Additionally, it has been pro jected
that by 2017 [25], up to 256 general-purpose cores can be put
on a single die. The proliferation of multiple cores on the
same chip heralded the advent of a communication-centric
system wherein the design of the on-chip network connecting various modules, namely the cores, the cache banks, the
memory units, and the I/O devices has become extremely
critical [3].
Nanophotonic interconnects are under serious consideration for providing the communication needs of future CMPs
especially for long metallic wires [14, 19, 6]. Silicon waveguides can propagate end to end signals 70% faster than optimized and repeated global wires.[9]. A number of 2D planar
nanophotonic on-chip interconnects have been proposed recently[20, 4, 7, 25, 24, 11, 10]. However, the design of planar
nanophotonic on-chip networks is proving to be very challenging and may not be scalable due to power consumption
and wiring complexity. For a large scale on-chip interconnect
system, signal paths will have a large amount of waveguide
crossings. This results in a signiﬁcant optical signal power
loss and back-reﬂection due to the changes in refractive index at the crossing points[9].
Recently, the semiconductor industry has proposed 3D
stacking technology as the next growth engine for performance improvement [5]. The emerging 3D stacking technology has provided new design dimension for on-chip networks.
Several 3D metallic-based interconnection network designs
have been proposed and have shown a tangible improvement in performance and power savings over 2D interconnections[21, 13, 26]. A prevalent way to connect these layers
vertically is using through silicon vias(TSVs)[22]. The pitch
of these vertial vias is very small (4μm∼10μm), and can be
further reduced to 1μm [18]. The delay of these vertical
lines is generally very small, only 20ps over a 20-layer stack.
Unfortunately, 3-D metallic-based interconnection networks,
still inherent the fundamental physical limits of electrical
signaling and this will be compounded by the thermal and
power challenges of 3-D stacking technologies.
In this paper, we leverage the advantages of two emerging
technologies, namely, silicon photonics and 3D stacking with
architectural innovations to design a high bandwidth, low
latency, energy-eﬃcient on-chip network called MPNOC: a
156
10.3
Figure 1: A typical on-chip nanophotonic link
multilayer photonic networks-on-chip. The proposed architecture targets 256 cores CMPs and 22nm CMOS technology. On the architecture side, MPNOC provides a global
crossbar-like connectivity with much improved power eﬃciency and performance.
The remainder of the paper is organized as follows.
In
Section 2, we review the recent advances in 3-D silicon photonics as they apply to MPNOC. We provide a detailed description of the proposed MPNOC in Section 3. We evaluate
its performance in Section 4, and we conclude the paper in
Section 5.
2.
3-D SILICON PHOTONICS
A silicon photonic integrated circuit requires a laser source,
a modulator and its driver circuit, medium (Si waveguide),
a photodetector and on-chip oﬀ-chip interface (coupler) as
shown in Figure 1. External laser source generates light
with multiple wavelengths. Light is carried by the optical
ﬁber and coupled to the silicon waveguide. The waveguide
passes through an array of microring modulators. Each ring
is tuned to a diﬀerent wavelength to modulate the intensity
of the light of that wavelength. At the receiver end, an array
of tuned microring Ge-doped detectors absorbs the light and
converts signal back to electrical domain.
Recent advances in silicon photonics have opened up the
door to design 3D on-chip nanophotonic interconnects. Jalali
group at UCLA has fabricated a SIMOX (Separation by IMplantation of OXygen) 3-D sculpting to stack optical devices
in multiple layers[15]. Lipson group at Cornell has successfully buried active optical ring modulators in polycrystalline
silicon[23]. Another interesting device is optical vias (Interlayer coupler) as shown in Figure 2. The basic function
is to couple light from one silicon layer to another. According to[4, 9], interlayer coupler introduces a 1dB optical
power loss, while each optical waveguide crossing undergoes
a 0.05dB loss. If we stack the connections in 3-D as opposed
to keeping all the waveguides in 2-D we can realize a very
signiﬁcant energy savings. For example, if we consider one
hundred crossing points per waveguide for a 2-D implementation, then using a 3-D, where the waveguides are stacked
vertically, we can realize an about 60% optical laser power
savings.
3. MPNOC ARCHITECTURE
The proposed architecture comprises 256 cores in 64 tiles
on a 400 mm2 3D IC. As shown in Figure 3, 256 cores are
mapped on an 8x8 network with a concentration factor of
four. Since the performance and energy of the electrical
interconnects are suﬃcient for short links (<2.5mm), small
degree of concentration will signiﬁcantly reduce system complexity[3]. Each tile is comprised of four cores. Each core
has a private L1 cache, and four cores in the same tile share
Figure 2: Illustration of an optical via built using a
microring resonator. (a) ’ON’ state (b) ’OFF’ state
(c) Power loss comparison between optical via and
planar waveguide with various number of waveguide
crossings
a L2 cache. The bottom layer, adjacent to the heat sink,
contains cores and local caches. One or more high level
caches and memory layers in the middle provide the bulk of
on-chip storage. The upper part of the chip contains four
optical layers implementing the decomposed optical crossbar as will be described later. Silicon photonic devices, such
as planar waveguides, couplers, microring resonators, and
Ge photodectors, are combined to provide the photonic infrastructure for intra-chip and chip-to-chip communications.
Inter-layer communications are realized by TSVs. As discussed in the previous section, such vertical wires occupy
very small area and can transmit the signals from top layer
to bottom layer in less than one clock cycle.
Figure 3: Proposed 256-core 3-D Chip Layout
In MPNOC, waveguides that have the potential to cross
each other are laid out on diﬀerent optical layers (cloverleaf intersection). Ring resonators are interleaved on different layers to minimize potential temperature variation.
The proposed architecture takes the advantage of the unique
properties of nanophotonic interconnects for global communication channels and switching capabilities of electronics
at the router level. Such hybrid combination reduces the
power dissipation on long inter-router communication while
electrical switching provides ﬂow control to regulate traﬃc
and prevent buﬀer overﬂow.
In the proposed 3-D layout, we divide tiles into four clusters based on their physical location. Each cluster contains
16 tiles. Unlike the global 64x64 optical crossbar design in
[25] and the hierarchical architecture in [20], MPNOC consists of 16 decomposed optical crossbar slices mapped on
157
10.3
Figure 4: Optical MWSR bus implementation, deposited silicon ring resonators are courtesy of[23]
four optical layers. Each slice is a 16x16 optical crossbar
connecting all tiles from one cluster to another (Inter-cluster
communication), or all tiles from same clusters (Intra-cluster
communication). Figure 4 shows the implementation of each
slice in the decomposed optical crossbar.
It is composed
of a few Multiple-Write-Single-Read (MWSR) nanophotonic
channels, which require much less power than Single-WriteMultiple-Read (SWMR) channels described in [20]. Token
slot [25] is adopted to improve the arbitration eﬃciency (up
to 100%)for the channel. Each wavelength in the waveguide operates at 10Gb/s. In MPNOC, we consider a 256 bit
per phit size to achieve a 2.56Tb/s bandwidth, a 4 waveguide bundle with 64 wavelengths in each waveguide is required for each crossbar channel. Considering the total number of optical channels on the chip, MPNOC can achieve
81.92TFLOPS peak performance (81.92TB/s bandwidth).
Since each optical crossbar channel has multiple senders
and a single receiver, we deﬁne each optical channel as the
home channel for the receiver. A source tile sends packets to
a destination tile by modulating the light on the home channel of the destination tile. Oﬀ-chip laser source generates
128 continuous wavelengths, Λ = λ0 , λ1 , λ2 , ..., λ127 . We divide these wavelengths into two groups. Figure 5 shows the
detailed optical device ﬂoorplan of optical layer 1. The detailed decomposition and slicing of optical crossbar on four
optical layers is shown in Figure 6.
Figure 5: Plan view of optical layer one
Inter-cluster communication: The upper part of the
chip in Figure 5 shows the waveguide bundle for inter-cluster
communications between Cluster 0 and Cluster 1. Blue
wavelengths (λ0 , λ1 , ..., λ63 ) and green wavelengths (λ64 , λ65 ,
158
Figure 6: The decomposition, slicing and mapping
of the optical crossbar. Color lines of the crossbar
represent valid part of optical crossbar on each layer.
The lines with same color mean they share the physical waveguides on each layer.
..., λ127 ) are injected into both ends of the waveguide bundle.
The waveguide bundle contains 32 crossbar channels (16 in
each direction and physically 64 waveguides) corresponding
to each tile of Cluster 0 and Cluster 1. Here the blue wavelengths are assigned as the communication channels for the
tiles from Cluster 0 to Cluster 1, and the green wavelengths
are assigned as the reversed communication channels (from
Cluster 1 to Cluster 0). When the blue wavelengths are coupled onto the waveguide bundle, tiles of Cluster 0 will arbitrate and enable portion of the blue modulators to transmit
the packets on these wavelengths. The blue microring photodiodes in the tiles of Cluster 1 will be passively tuned to
resonant wavelengths and detect the signals on their home
channel.
Intra-cluster communication:
Intra-cluster communications are very similar to inter-cluster communications.
The diﬀerence is that there are 64 wavelengths on each
waveguide of the waveguide bundle for intra-cluster communications as opposed to 128 wavelengths for inter-cluster
communications. This results in 50% reduction in the number of ring resonators required for intra-cluster communications. Consequently, there is considerable power savings for
intra-cluster communications.
Each tile contains an electrical router as shown in Figure 7.
The electrical router provides the proper interface to local
cores/caches, on-chip nanophotonic interconnects and onchip/oﬀ-chip memory/IO devices. In addition to having the
same features as those routers in electrical NOC, the routers
of MPNOC should provide interface to receive/generate tokens from/to optical waveguides. Tokens are used for optical
crossbar arbitration and ﬂow control.
Arbitration and Flow Control: Our proposed token
slot arbitration is slightly diﬀerent from[25], where only the
destination tile can inject one-bit token every clock cycle.
Such a modiﬁcation can add ﬂow control mechanism for
the architecture without extra hardware overhead. Tokens
are transmitted in arbitration waveguide through piggybacking. Tokens will be generated from the receiver end when
10.3
in Table 2. They all implemented with a concentration degree of 4. We evaluate MPNOC architecture with two other
crossbar-like architectures, CORONA[25] and FIREFLY[20]
and one electrical architecture(CMESH)[3] using dimensionordered routing (DoR). We assume token slot for both MPNOC and CORONA to pipeline the arbitration process to
increase the eﬃciency. Multiple requests can be sent from
four local cores to optical channels to increase the arbitration eﬃciency. We use Fly Src routing algorithm for Fireﬂy
architectures, which operates intra-cluster communications
by electrical mesh link ﬁrst and then operates inter-cluster
communication through optical crossbar.
Table 2: Evaluated Architectures
VC# Description
1
Multilayer optical crossbar
1
a single layer optical crossbar
1
a hierachical architecture,
local electrical mesh link,
several global optical crossbars
Concentrated mesh network
8
NAME
MPNOC
CORONA
FIREFLY
Routing
Token Slot
Token Slot
Src Fly
CMESH
DoR
4.2 Performance
We simulate four architectures on seven sythetic traﬃc
traces[8],
including both random uniform traﬃc patterns
and permutation patterns, such as bit-complement(bitcomp),
bit-reversal(bitrev), transpose, tornado, neighbor and prefect shuﬄe. Figure 8 shows the throughput and average
network latency per packet for the uniform traﬃc. The proposed architecture outperforms all the other networks on the
uniform traﬃc. It improves zero load latency by 28%, 43%,
and 51% as compared to CORONA, FIREFLY, and CMESH
respectively. We observe MPNOC exceeds the throughput
to 2.4x and 2x compared to Corona and Fireﬂy.
The throughput study for all traﬃc traces is shown in
Figure 9. The maximum throughput is normalized to 1.
We observe MPNOC can achieve 100% throughput in bitreversal traﬃc, because there is no contention in the network. MPNOC outperforms FIREFLY and CMESH in most
of the traﬃc patterns and has the same throughput in bitcomp, transpose and shuﬄe as CORONA. While CMESH
and FIREFLY has a better performance in neighbor trafﬁc because such traﬃc pattern exploits the spatial locality
and these two use electrical links for local traﬃc, MPNOC
and Corona has a better performance in global traﬃc (nonneighbor) traﬃc patterns, where nanophotonic crossbar can
dramatically reduce the hop counts and traverse from source
tile to destination tile within one hop. On average, MPNOC
provides a 55%, 109% and 233% improvement in throughput
Figure 7: (a) On-chip Network Router architecture
for MPNOC, (b) An example of 2-ﬂit packet from
source tile to destination tile, assuming the optical
transversal latency is 3 clock cycles
there are enough buﬀers left in the input port. The receiver
should reserve enough buﬀers for the worst case optical token round-trip latency, 12 clocks for inter-cluster communications and 8 for intra-cluster communications. Since the token is one-bit, it only carries the information whether there
is an available buﬀer at the receiver. As a result, when a
source router captures the token, it will have the privilege
to send one ﬂit (assume ﬂit size = phit size) to the corresponding destination. Successive transmissions depend on
whether the successive tokens are captured. Since there are
four optical input/output pairs for each router, their tokens
are maintained separately and send to diﬀerent arbitration
waveguides on diﬀerent layers.
4. EVALUATION AND SIMULATION
In this section, we evaluate our proposed architecture and
compare the performance, power-eﬃciency, device requirement and area against alternative architectures.
4.1 Simulation Setup
We ﬁrst describe the simulation setup of the proposed
architecture. A cycle-accurate simulator was modiﬁed and
developed based on Booksim simulator[8] to support optical networks. The packet injection rate was varied from 0.1
to 0.9 of the network capacity. Since the delay of Optical/Electrical (O/E) and Electrical/Optical (E/O) conversion can be reduced to less than 100ps each, the total optical transmissions latency is determined by physical location
of source/destination pair and two additional clock cycles
for the conversion delay. Our simulation model includes the
pipeline model, router arbitration and contentions, ﬂow control and other overhead. The simulator is warmed up under load without taking measurements until steady-state is
reached. An aggressive single cycle electrical router[17] is
applied in each tile and the ﬂit transversal time is 1 cycle
from the local core to electrical router. A detailed simulation
conﬁguration is shown in Table 1.
Table 1: Simulation Conﬁgurations
Concentration (# cores of per router)
Buﬀer per input port
Phit size (Flit size)
Packet size
Vdd
CPU Frequency
4
64 ﬂits
256 bits
1 ﬂit
1.0V
5GHz
All the evaluated architectures are 256-core systems listed
Figure 8: (a) Load-latency curve for uniform; (b)
Load-throughput curve for uniform traﬃc
159
Figure 9: Simulation results showing normalized
saturation throughput for seven traﬃc patterns
compared to CORONA, FIREFLY and CMESH on seven
traﬃc patterns.
4.3 Energy Comparison
The energy consumption of a nanophotonic interconnection network can be divided into two parts, electrical energy
and optical energy. Optical energy consists of the oﬀ-chip
laser energy and on-chip microring resonator heating energy.
4.3.1 Electrical Energy Model
Ee = Elink + Erouter + EO/E ,E /O
(1)
Electrical power includes the energy of link, router and
back-end circuit for optical transmitter and receiver. We
use ORION 2.0[12] model and modiﬁed some parameters
for 22nm technology according to [1]. We assume the injection rate of the electrical link is 0.1. The energy of electrical link include both planar links and vertical links (TSVs).
The length of electrical planar links in Fireﬂy and CMesh
is determined to be 20mm/8=2.5mm. The energy for planar link is conservatively obtained as 0.15pJ/bit under lowswing voltage level. The length of vertical links is very small.
For a 10-layer chip, the vertical via is determined as ∼100200μm[18], which is much less than planar links. As a result,
the power consumption of vertical links is very small. We
neglect it when we calculate our electrical link power model.
For the electrical router power, we assume a 8x8 router consumes 0.30pJ/bit/hop and a 5x5 router with the same buﬀer
size requires 0.22pJ/bit/hop. For each optical transmitted
bit, we need to provide electrical back end circuit for transmitter end and receiver end. We assume the O/E and E/O
converter energy is 100fJ/b, as predicted in [16].
4.3.2 Optical Energy Model
Plaser = Prx + Closs + Ms
(2)
The optical power budget is the result of the laser power
and the power dissipation for the microring resonators. The
laser power budget is determined by Equation (2). Plaser is
the laser power requirement, Prx is the receiver sensitivity,
Closs is the channel losses and Ms is the system margin.
The ring power comes from the static power:
fabrication
error trimming and the heating power to keep the ring resonators in the resonance region, the dynamic power: direct
data modulation power.
In order to perform an accurate
comparison with the other two optical architectures, we use
the same optical device parameters and loss values from provided in [2, 4], as listed in Table 3.
10.3
Table 3: Laser and Ring Power Budget
Component
Laser eﬃciency
Coupler (Fiber to Waveguide)
Waveguide
Splitter
Non-Linearity
Ring Insertion & scattering
Ring drop
Waveguide Crossings
Photo Detector
Ring Heating
Ring Modulating
Receiver Sensitivity
Value Unit
5
dB
1
dB
1
dB/cm
0.2
dB
1
dB
1e-2 - 1e-4
dB
1.5
dB
0.05
dB
0.1
dB
26
μW/ring
500
μW/ring
-26
dBm
4.3.3 Synthetic Workload Energy Comparison
Table 4: Power parameters of four architectures
Electrical link
Router
O/E, E/O
Optical channel loss
Optical power per λ
Laser requirement
Ring heating
CORONA
0.22pJ/b
100fJ/b
-25.2dB
0.81mW
13.6W
26W
FIREFLY MPNOC
0.15pJ/b
0.30pJ/b
0.22pJ/b
100fJ/b
100fJ/b
-17.6dB
-16.0dB1
0.14mW 0.10mW
2.4W
6.1W
6.5W
27.5W
CMESH
0.15pJ/b
0.30pJ/b
Figure 10: Average per-bit energy consumption
Based on the energy model discussed in the previous section, we calculate the energy parameters of four architectures as shown in Table 4. We test uniform traﬃc with 0.1
injection rate to the four architectures and obtain energy
per-bit comparison shown in Figure 10. Althrough Fireﬂy has 1
4 as much as the rings in CORONA and MPNOC,
which results in 1
4 energy consumption per bit on ring heatings, it still consumes more energy per bit than MPNOC and
CORONA because of the energy consumption overhead on
routers and electrical links. In general, MPNOC saves 6.5%,
23.1%, 36.1% energy per bit compared to CORONA, FIREFLY, and CMESH respectively.
It should be noted that
when the network injection rate increases, MPNOC becomes
much more energy eﬃcient than other three architectures.
4.3.4 Optical Device Requirement
In Figure 11(a), the contour line is the optical link power
per wavelength budget in mWatts. The power budget of
MPNOC requires further improvement of the ring devices,
while FIREFLY requires the further improvement on waveguide propagation loss and CORONA requires both parame1
-16.0dB for inter-cluster comm. and -14.4dB for intra-cluster comm.
160
10.3
Figure 11: (a) Optical link power per wavelength, (b)Optical Laser Power requirement
[11] A. Joshi and et al. Silicon-photonic clos networks for global
on-chip communication. In NOCS ’09, pages 124–133, 2009.
[12] A. Kahng and et al. Orion 2.0: A fast and accurate noc
power and area model for early-stage design space
exploration. In DATE, pages 423–428, April 2009.
[13] J. Kim and et al. A novel dimensionally-decomposed router
for on-chip communication in 3d architectures. SIGARCH
Comput. Archit. News, 35(2):138–149, 2007.
[14] N. Kirman and et al. Leveraging optical technology in
future bus-based chip multiprocessors. In MICRO 39, pages
492–503, 2006.
[15] P. Koonath and B. Jalali. Multilayer 3-d photonics in
silicon. Opt. Express, 15(20):12686–12691, 2007.
[16] A. Krishnamoorthy and et al. Computer systems based on
silicon photonic interconnects. Proceedings of the IEEE,
97(7):1337–1361, July 2009.
[17] A. Kumar and et al. A 4.6tbits/s 3.6ghz single-cycle noc
router with a novel switch allocator in 65nm cmos. In
ICCD ’07, October 2007.
[18] G. H. Loh. 3d-stacked memory architectures for multi-core
processors. SIGARCH Comput. Archit. News,
36(3):453–464, 2008.
[19] D. Miller. Device requirements for optical interconnects to
silicon chips. Proceedings of the IEEE, 97(7):1166–1185,
July 2009.
[20] Y. Pan and et al. Fireﬂy: illuminating future
network-on-chip with nanophotonics. In ISCA ’09, pages
429–440, Austin, TX, USA, 2009.
[21] D. Park and et al. Mira: A multi-layered on-chip
interconnect router architecture. In ISCA ’08, pages
251–261, 2008.
[22] S. Pasricha. Exploring serial vertical interconnects for 3d
ics. In DAC ’09, pages 581–586, 2009.
[23] K. Preston and et al. Deposited silicon high-speed
integratedelectro-optic modulator. Opt. Express,
17(7):5118–5124, 2009.
[24] A. Shacham and et al. On the design of a photonic
network-on-chip. In NOCS ’07, pages 53–64, 2007.
[25] D. Vantrease and et al. Corona: System implications of
emerging nanophotonic technology. In ISCA ’08, pages
153–164, Beijing, China, 2008.
[26] Y. Xu and et al. A low-radix and low-diameter 3d
interconnection network design. In HPCA ’09, pages 30–42,
2009.
ters. In Figure 11(b), we show optical laser power contour in
Watts. The total laser power of MPNOC can be limited to
2W with the waveguide propagation loss to 0.3dB/cm and
oﬀ resonance ring loss to 0.0003dB.
5. CONCLUSIONS
Recent advances in silicon photonics and 3D stacking technology have motivated us to explore multilayer nanophotonic interconnects to meet the performance and power requirements of future many-core CMPs. To this end, we
propose MPNOC: a power-eﬃcient multilayer nanophotonic
network design for on-chip interconnects. MPNOC can achieve
81.92 TFLOP/s peak performances with reasonable power
consumption. Simulation results show the 3D MPNOC approach outperforms 2D photonic designs both for performance and energy savings.
6. ACKNOWLEDGEMENT
This research was funded in part by NSF grants CCR0538945, ECCS-0725765 and CCF-0953398. We would like
to thank the detailed feekback from reviewers.
7. "
Virtual channels vs. multiple physical networks - a comparative analysis.,"Packet-switched networks-on-chip (NoC) have been proposed as an efficient communication infrastructure for multi-core architectures. Adding virtual channels to a NoC helps to avoid deadlock and optimize the bandwidth of the physical channels in exchange for a more complex design of the routers. Another, possibly alternative, approach is to build multiple parallel physical networks (multiplanes) with smaller channels and simpler router organizations. We present a comparative analysis of these two approaches based on analytical models and on a comprehensive set of experimental results including both synthesized hardware implementations and system-level simulations.","Virtual Channels vs. Multiple Physical Networks: a Comparative Analysis
Young Jin Yoon, Nicola Concer, Michele Petracca, Luca Carloni
Depar tment of Computer Science, Columbia University
{youngjin, concer, petracca, luca}@cs.columbia.edu
10.4
ABSTRACT
Packet-switched networks-on-chip (NoC) have been proposed as an
efﬁcient communication infrastructure for multi-core architectures.
Adding virtual channels to a NoC helps to avoid deadlock and optimize the bandwidth of the physical channels in exchange for a
more complex design of the routers. Another, possibly alternative,
approach is to build multiple parallel physical networks (multiplanes) with smaller channels and simpler router organizations.
We present a comparative analysis of these two approaches based
on analytical models and on a comprehensive set of experimental
results including both synthesized hardware implementations and
system-level simulations.
Core 
Proc. Unit 
NI 
Core 
Proc. Unit 
NI 
Core 
Proc. Unit 
NI 
Core 
Proc. Unit 
NI 
Router 
(a)
Categories and Subject Descriptors
C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]:
Interconnection architectures
Design, Performance
General Terms
Keywords
Channel Slicing, Virtual Channel, Network-on-Chip.
1.
INTRODUCTION
Packet-switched networks-on-chip (NoC) have been proposed
as an alternative solution to standard bus-based interconnects to
address the global communication demands of future chip-multiprocessors (CMP) and system-on-chip (SoC). While these communication demands continue to grow as more cores are integrated on
a chip, the on-chip power-dissipation budget is expected to remain
very limited due to packaging constraints. Hence, the challenge is
not only to design NoCs that can deliver high-bandwidth at low latency for inter-core communication, but also to make sure that this
is done in a very power-efﬁcient way [1].
Virtual channels (VC) have been proposed as a buffer-management ﬂow control that extends worm-hole ﬂow control (WH) by
associating more than one logical channel to each physical I/O port
of the router [2]. This is obtained through the partitioning of the
storage resources to enable selectively buffering of the incoming
worms so that they do not interfere during the forwarding process.
VC can be used to avoid routing deadlock, improve performance
under congestion [3], and separate different classes of trafﬁc to
avoid protocol deadlock [4]. Both WH and VC ﬂow controls are
appealing for NoC design because they require less buffering space
in the routers. Indeed, in comparison with macro-level networks,
NoCs must be designed while considering that, in a chip, buffers
are generally more expensive resources than wires in terms of area
and power. VC ﬂow control aims at improving performance by investing in more ﬂit buffering space to better exploit the available
channel bandwidth. On the other hand, the use of multiple physical
networks on the same chip has been proposed to improve performance and keep trafﬁc classes separate. For instance, in the RAW
processor four separate and independent NoCs are used: two NoCs
are statically routed and two are dynamically routed [5].
B 
v=1, p=1 
Q 
VC0 
VC1 
B 
B 
MP0 
MP1 
v=2, p=1 
Q/v 
(b)
B/p 
B/p 
Q 
v=1 p=2 
Figure 1: VC NoC (a) vs. MP NoC (b). Relative storage allocation for reference, VC, and MP NoC (c).
(c)
Contributions. We present a comparative analysis in terms of
network performance, area occupation and power consumption of
using virtual-channels versus multiple physical planes. For a fair
comparison, we explore the design space as we vary the ﬂit width
B , the queue depth Q, the number of planes p and the number of
virtual channels v while keeping the aggregate input-port storage
and channel bandwidth constant. Fig. 1 compares the basic architectural organization of a multi-plane (MP) NoC with p = 2 with an
equivalent VC NoC with v = 2. Note that the MP NoC is obtained
by partitioning equally the same number of wires of the VC NoC
across the two planes. The count of the total number of FFs in the
routers’ input queues is the same for both architectures but in the
VC router the storage is spread into different virtual queues while
in the MP case the storage is partitioned among smaller, simpler,
and independent WH-routers (Fig. 1(c)). In the sequel we study the
cases of MP NoCs with p = 2 and p = 4 and compare them with a
single-plane NoC without virtual channels as well as single-plane
NoCs with v = 2 and v = 4. Our analysis includes: (a) the RTL
design, logic synthesis and technology mapping of various routers
across three technology process generations (b) an extensive set of
system-level simulations with synthetic trafﬁc patterns, and (c) ISA
simulations of a 16-core CMP running PAR S EC benchmarks.
Related Work. Balfour and Dally present a comprehensive comparative analysis of NoC topologies and architectures in [6], where
they also discuss the idea of duplicating certain NoC topologies,
such as Mesh and CMesh, to improve the system performance.
Carara et al. also propose to replicate the physical networks taking
advantage of the abundance of wires between routers and compared
this solution to the VCs approach [7]. Our work differs from these
analyses because instead of duplicating the NoC we actually partition (or slice [2]) it in a number of sub-networks while keeping the
overall amount of wire and buffering resources constant. Noh et al.
propose a multi-plane-based design for a VC-enabled router [8]:
the internal crossbar switch is replaced with a number of parallel
crossbars (planes) that increase the ﬂit-transfer rate between input
and output queues. This results in a router with a simpler design
which performs better than a single-plane router with a larger number of VCs. Differently from our study, Noh et al. maintain the
ﬂit-width constant as they scale the number of additional lanes.
162
10.4
Routing 
g 
V
VC state 
e 
e 
B 
Buffer[Q] 1..v 
B
er[Q] 1 v
input portM 
ortM
Routing 
g 
V
VC state 
e 
B 
Buffer[Q] 1..v 
fer[Q] 1 v
input port1 
VC allocator1..M 
Output Arbiter 
ut Ar
Switch 
Switc
Allocator 
Switching 
Fabric 
(Crossbar) 
B 
output portM 
B 
output port1 
Terms 
Definitions 
B 
Q 
S 
p 
v 
flit width of single-plane NoC 
size of router input buffer 
input storage per port (=B × Q) 
number of physical channels 
number of virtual channels 
(a)
(b)
(c)
Figure 2: Block diagrams of VC (left) and MP routers (center), NoC parameters used in our comparative study (right).
Q
Tech [nm]
p = 1
p = 2
p = 4
p = 1, v = 2
p = 1, v = 4
90
1.39
1.40
2
65
1
1.05
1.13
N/A
N/A
45
90
1.07
1.21
1.36
1.32
2.03
4
65
1
1.16
1.23
2.42
N/A
45
90
1.06
1.16
1.92
1.21
1.56
1.68
2.51
8
65
1
1.10
1.21
1.32
2.33
45
90
1.04
1.22
1.36
2.37
1.00
1.12
1.01
1.53
16
65
1
1.00
1.18
0.87
1.55
45
90
1.05
1.14
0.88
1.53
1.00
1.16
0.79
1.06
32
65
1
1.04
1.15
0.69
1.02
45
1.03
1.15
0.67
1.01
Table 1: Power dissipation ratio (w.r.t. 1-plane WH reference NoC) for different technologies and values of p, v with B = 128.
2. ANALYTICAL MODEL
Fig. 2(a) shows the block diagram of a classic 5-port VC router
for a 2-D Mesh network. Each I/O port is connected to a physical channel that has a data parallelism of B bits, which matches
the ﬂit size. In a VC-router with v virtual channels each input port
is equipped with: (1) a routing logic block; (2) a set of v queue
buffers; and (3) a VC control block that holds the state needed to
coordinate the handling of the ﬂits of the various packets. Each
queue buffer can store up to Q ﬂits. In a VC-router, a set of VC allocators arbitrates the matching between input and output VCs. A
switching fabric that forwards ﬂits from the I/O ports is conﬁgured
dynamically based on input routing and output arbitration.
Fig. 2(b) shows the block diagram of a MP router that can be
used on each plane of a multi-plane 2-D Mesh NoC (MP-router).
Its structure is simpler than the VC-router because it implements
the basic WH ﬂow control with a single queue per each input port
and hence does not need VC allocators.
The table in Fig. 2(c) reports the parameters of our model. We
compare the two router architectures across different NoCs by keeping the amount of storage installed on the interconnect constant. We
ﬁrst deﬁne a reference NoC architecture based on WH ﬂow-control
routers with ﬂit-width B and input-queue depth Q. The input storage at each port is S = B × Q bits. This can be seen either a
VC-router with one virtual channel (v = 1) or a MP-router for a
single-plane NoC (p = 1). Then, we vary the number v of virtual
channels and number p of planes by partitioning the available storage S according the following rules: (1) if v > 1, the queue length
of a virtual channel is QV C = Q/v and BV C = B ; (2) if p >1,
the ﬂit width Bi of each plane is constrained by
case of uniform partitioning Bi = B/p, ∀i) and QM P = Q. These
rules enforce SM P = SV C which in the following is the conﬁguration that we consider unless differently stated (competitive sizing).
The most common ﬂow-control on router-to-router links in a
NoC is credit-based ﬂow control, which uses credits to allow the
upstream router to keep track of the storage availability in the input queue of the downstream router. In order to guarantee minimal
zero-load latency, this ﬂow control imposes a constraint on the minimum size of the router input queue, which should be at least equal
to the round-trip-time (RTT) of one credit on the link [9]. In the
best case, the RTT is equal to 2 clock cycles, thus Qmin = 2 (minimum sizing constraint) [2]. This translates in an aggregate amount
of storage across the p routers of a node in the MP NoC that is
i=1 Bi = 2 × B . Instead, the minimal
storage for a VC-router is SminV C = 2 × v × B because every
virtual channel requires a minimum-size queue.
In summary minimum-sizing allows reducing the number of bits
equal to SminM P = 2 × Pp
Pp
i=1 Bi = B (in
in the router input queues to the minimum value for a minimal
zero-load latency. Since the trafﬁc load on a NoC is often limited,
longer queues do not bring relevant beneﬁts in terms of offered
throughput while having larger area and power overheads. Indeed,
for those applications where the NoC is not required to achieve
high throughput but only to provide connectivity among the on-chip
cores, minimum-sizing can bring interesting advantages in terms of
power reduction.
3. SYNTHESIS-BASED COMPARISON
We used logic synthesis to implement the two NoC architectures introduced in the previous section. For the RTL designs we
took advantage of the NOC EMU LATOR (NOC EM) [10]. We performed the synthesis using Synopsys Design Compiler with industrial standard-cell libraries for three different technology processes
(90nm, 65nm, and 45nm), and with target clock frequencies set to
500M hz , 1Ghz , and 2Ghz , respectively. We analyzed the power
dissipation of the synthesized designs with Synopsys Primetime PX,
assuming at each input port a uniformly distribution of the data
bits and a trafﬁc load of 0.4 ﬂits/clock-cycle (roughly the saturaurations of 2 × 2 NoCs1 obtained by varying the values of the
tion throughput of a 2D-Mesh). We synthesized various conﬁgparameters of the table in Fig. 2(c). Speciﬁcally, we considered
v ∈ {1, 2, 4} virtual channels versus p ∈ {1, 2, 4} planes with
queue lengths Q ∈ {2, 4, 8, 16, 32}. The WH router for the reference single-plane architecture has ﬂit-width B and queue length Q.
Each plane in a multi-plane NoC has an MP-router with ﬂit width
BM P = B/p and input-queue length QM P = Q. In a VC-router,
instead, the ﬂit width is BV C = B , while a portion of the input
queue of QV C = Q/v is reserved to each virtual channel. We report results for B = 128 bits. The trends are similar for B = 64
and B = 256.
Table 1 reports the power dissipation of each NoC router architecture normalized to the reference architecture (p = 1). For Q ≤ 8
the management of VCs costs 32 − 142% in terms of power, while
the overhead for having MPs is only 4 − 56%, and sometimes negligible when having just p = 2. For Q ≥ 16 the efﬁciency in
area translates in a power efﬁciency, which is actually ampliﬁed as
power savings are obtained also in a Q = 16, v = 2 conﬁguration.
The trends for area occupation are similar to those observed for
the power analysis. Note that the results on both area occupation
and power dissipation are independent from the technology pro1Notice that since the results presented in this section are given in terms of relative
numbers they are independent from the network size. Hence, the analysis of a 2 × 2
NoC is sufﬁcient to expose the main trends in the comparison of the alternative NoC
conﬁgurations, which for instance are valid also for larger n × n 2D-Mesh NoCs.
163



















	




























Figure 3: Throughput improvement ratio (TIR) for 2D Mesh.
cess. For short queues, Q ≤ 8, the management of virtual channels
causes substantial area overhead (59 − 136%), while the area overhead when implementing multiple separate planes is signiﬁcantly
smaller (8 − 36%). For long queues, Q ≥ 16, the partitioning of
an input queue into shorter queues to support multiple virtual channels delivers efﬁciency in terms of occupied area. E.g. for Q = 32,
splitting the storage with v = 2 virtual channels delivers almost a
20% reduction in area.
In summary, when the total amount of storage in terms of sequential elements (ﬂip-ﬂops) that can be assigned to each router is
small, it is convenient to build a MP NoC instead of an equivalent
VC NoC while the opposite is true if there is room for more storage.
We also collected data on the critical path of the router for each
conﬁguration. This depends inversely on B and v . In a MP NoC to
reduce B by a factor of p ∈ {2, 4} leads to a delay improvement
of 1-10%. Further, a MP NoC with p = 2 planes can run at a clock
frequency 15-25% higher than an NoC with v = 2 VCs, while a
MP NoC with p = 4 planes can run at a clock frequency that is up
to 25-35% faster than an NoC with v = 4 VCs. 2
4. SYSTEM-LEVEL ANALYSIS
We developed an event-driven simulator with detailed models of
routers and NIs and high-level models for the cores that generate
and consume the network trafﬁc. Each NI is connected to one or
4 × 4 Mesh and four synthetic trafﬁc patterns: Uniform Random
more routers depending on the number of NoC planes. We used a
Trafﬁc (URT), Tornado, Transpose, and 4-HotSpot [2, 9]. We set
the ﬂit width of the single-plane reference NoC to B = 256 while
partitioning the MP NoCs with uniform width (e.g. Bi = B/p).
We run the simulation with the offered load close to saturation.
Both VCs and MPs improve the system throughput3 . The improvement depends on the application and the amount of buffering
installed on the router’s input queues. With a MP-NoC different
packets assigned to different planes can be processed in parallel
during a single clock cycle. Moreover MPs can be used to improve
the performance also when the available storage is reduced to the
minimum (i.e. S = 2B ), whereas VCs can not be implemented because the virtual queues would become shorter than the minimum
sizing constraint imposed by the ﬂow control.
Fig. 3 shows the throughput improvement ratio T I R as function of the number of given VCs and MPs for competitive sizing:
T hV C . A value T I R > 0 indicates that VCs perform
better than MPs, whereas a value T I R < 0 indicates that MPs
perform better. The performance of the two NoCs varies notably
as function of the application and available storage. In the case of
random trafﬁcs, such as URT and 4HS where packets incur many
channel contentions, VCs outperform MPs by offering a up to 20%.
However, when we tested the 4HS trafﬁc in a 8 × 8 Mesh, therefore
reducing the randomness of the trafﬁc we noticed that MPs achieve
similar throughput values as VCs. In Tornado and Transpose instead the trafﬁc ﬂows are very regular as each core communicates
with only one single other core deﬁned by the trafﬁc pattern itself.
Here MPs exploit the reduced contention probability and outperform VCs by up to 30%. As the storage increases though VCs
can store multiple ﬂits in their queues reducing the contentions and
hence improving their performance as in the case of Tornado.
T I R = 1 − T hM P
2Note that in the following system-level simulation section we conservatively assume
the same clock frequency for all NoC conﬁgurations.
3Measured considering the bundle of MPs on each channel when p >1.
164
10.4
(a)
(b)
Figure 4: Target system: (a) logical design of a node and (b)
topology of the 16-CMP with 4 memory controllers.
Processors
L1 Caches
L2 Caches
Directory Caches
Memory
16 in-order 1-way SPARC cores
Split IL1 and DL1 with 16KB per core, 4-way
set associative, 64B line size,and 1 cycle access
time.
1 MB per core with 4-way set associative, 64B
line size, and 3 cycle access time.
256kB per memory controller with 4-way set
associative, 4 cycle access time.
4 memory controllers on chip, 275-cycle
DRAM access + on-chip delay
Table 2: Basic parameters of the target 16-CMP system.
Initially the improvement of throughput comes at the expense of
serialization latency: using p MPs, each NI has a channel width
b that is p times narrower than the reference conﬁguration. As a
consequence, each packet traveling on a MP network is made of p
times more ﬂits than in the single-plane NoC, e.g. a packet of 1 Kbit
is composed by 4 ﬂits when b = 256 bits or 8 ﬂits when b = 128
bits. As the average load increases, however, MP NoCs can better
handle the higher trafﬁc volume, thus reducing the overall system
latency and raising its maximum throughput. These trends have
been observed in all of the trafﬁc patterns that we analyzed.
5. CASE STUDY: SHARED-MEMORY CMP
We completed full-system simulation of a 16-core shared-memory chip-multiprocessor (CMP) similar to the one used by Peh et
al. [4]. We used Virtutech Simics [11] with the GEMS toolset [12],
augmented with GARN E T, a cycle-accurate model for packet-switched NoC that supports pipelined routers with either WH or VC ﬂow
controls [13]. We extended GARN E T to accommodate the modeling of heterogeneous multi-plane NoCs with different ﬂit sizes per
plane and to support on-chip directory caches. We run simulations
with eight benchmarks from the PAR S EC suite.: with the simsmall
input dataset [14].
Target System. We assume that the 16-core CMP is designed
with a 45nm technology and runs at 2Ghz . Each core is connected
to a node of a 2D-mesh NoC through a network interface as illustrated in Fig 4(a). The NoC provides support for communication
with the off-chip DRAM memory through four memory controllers
as illustrated in Fig 4(b). Cache access latency were characterized
using CACTI [15] and cache coherence is based on the MOESI
directory protocol [16]. Each memory controller has a 256kB directory cache, where each blocks consists of a 16-bit vector matching the number of private L2-caches in the CMP. The bandwidth of
DRAMs, off-chip links, and memory controllers was assumed to
be ideal, i.e. high enough to support all outstanding requests. The
basic simulation parameters are summarized in Table 2.
Network-on-Chip Conﬁgurations. Cache coherence protocols
are characterized by a number of functionally-dependent data and
control messages. The MOESI cache-coherence implementation
in GEMS, has four classes of messages that are exchanged among
the private L2-caches and the memory controllers (Table 3): Data
Request (REQ), Request Forward (FWD), Data Transfer (DATA),
and Write Back (WB). Causality dependencies across messages of
different classes may cause message-dependent, or protocol deadlock [17]. A common way to guarantee the absence of messagedependent deadlock is to introduce an ordering in the use of the

	






10.4
Message
Class
REQ
FWD
DATA
DATA
WB
From → To
Size
(bits)
Cache →Mem
Mem → Cache
64
64
Mem → Cache
Cache → Cache
576
576
Cache → Mem {64,576}
M P assignment
Plane ID
(ﬂits)
0
8
1
8
2
18
2
18
3
{8,18}
Table 3: Plane assignments for M P4 and M P16 .
network resources. From a NoC design viewpoint this translates
in assigning a separate set of channels and queues to each message
type. Therefore, we use VCs or planes to isolate different message
classes. The baseline virtual-channel NoC (V C 64
4 ) assigns to each
message class a distinct virtual channel for a total of v = 4 virtualchannels. The ﬂit width, which also corresponds to the channel
parallelism, is BV C = 64 bits. For each virtual channel the router
has an input queue of size QV C = 4 and, therefore, the total buffer
storage per input port is 16 ﬂits.
As possible alternative implementations to the baseline VC NoC
we consider two MP NoC conﬁgurations, called M P4 and M P16 ,
each with p = 4. All the NoCs use 5-stage pipelined routers with
credit-based ﬂow control. The MP NoC conﬁgurations differ for
the sizing of the router input queues: M P4 has QM P4 = 4 (minimum sizing) while M P16 has QM P16 = 16 (competitive sizing).
For both multi-plane conﬁgurations we partitioned the 64 bits
channel parallelism of V C 64
4 as follows: B0 = B1 = 8 bits for
Plane 0 and 1, B2 = 32 bits for Plane 2, and B3 = 16 bits for
Plane 3. Table 3 reports the plan assignment for each message class
together with the message size expressed both in bits and in the
number of ﬂits that are necessary when this message is transferred
on a MP-NoC. For example, a DATA message, which consists of
a cache line of 512 bits and an address of 64 bits, is transmitted
as a worm of 18 ﬂits on Plane 2, whose ﬂit width is B2 = 32.
Notice that the same message incurs a much smaller serialization
latency when transmitted as a sequence of 9 ﬂits on the VC-NoC
4 , whose ﬂit width is BV C = 64 bits 4 . Similarly, a REQ
message, which consists of 64 bits, requires 8 ﬂits to be transmitted
on Plane 0 of either M P4 or M P16 , but only 1 ﬂit on V C 64
4 .
Experimental Results. Fig. 5 reports the average ﬂit latency
measured while running the eight PAR S EC workloads on the 16core CMP for the two MP NoC conﬁgurations. The values are
normalized with respect to the corresponding values for the VC
NoC conﬁguration. The latency is measured from the ﬂit generation to its arrival at the destination and includes the serialization
latency (the ﬂits are queued right after identifying the coherent status of L2-cache block). Consequently, it is not surprising that both
MP NoCs present a performance loss with respect to the baseline
VC NoC. On the other hand, the MP NoCs offer an interesting
power/performance trade-off. Under competitive sizing, M P16
offers a 18% average power saving in exchange for a 14% average performance loss across all benchmarks. Under minimum sizing constraints, instead, M P4 reaches an average power saving of
about 70% at the cost of a average performance loss of 32%.
V C 64
6. CONCLUSIONS
We compared virtual channels (VC) with physical multi-plane
(MP) networks-on-chip in terms of system-level performance, gatelevel area occupation, and power dissipation. First we showed
that, independently from the technology process, MP are an efﬁcient solution when the amount of storage resources at the input
ports of each router is limited.
Instead, VC give interesting advantages when it is possible to equip the router inputs with longer
queues that can be efﬁciently partitioned among the virtual channels. Then, we showed that the choice of implementing VC rather
than MP can also be driven by the characteristics of the trafﬁc over
the NoC. More irregular trafﬁc patterns take advantage of deeper
4Notice that GARN E T does not model the head/tail ﬂits of a worm.











Figure 5: Average ﬂit latency.
input queues in the routers, thus working best on a VC NoC. When
the trafﬁc is more regular, the collisions are less frequent and, therefore, shorter queues can be deployed in the routers, thereby making MP a good design choice. Finally, we performed a comparative analysis running a suite of real benchmark applications on
an instruction-set-architecture for an hypothetical 16-core CMP.
Here we showed that as long as the amount of storage resources
on the NoC is comparable, the two solutions offer similar power
efﬁciency. However, MP permits to reduce the queue lengths without compromising functionality, which leads to implementations
with higher performance-per-watt efﬁciency when a NoC with long
buffering queues would be over-provisioned with respect to the offered load. In summary, to have multiple physical networks appear
as an interesting design choice for NoC that need to satisfy low performance requirements with low amount of available resources, but
with more stringent needs in terms of power savings. Virtual channels, instead, are a better solution for NoCs in case of high resource
availability and performance needs.
Acknowledgments. This work was partially supported by the
Gigascale Systems Research Center, one of six research centers
funded under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation entity.
7. "
An efficient dynamically reconfigurable on-chip network architecture.,"In this paper, we present a reconfigurable architecture for NoCs on which arbitrary application-specific topologies can be implemented. The proposed NoC can dynamically tailor its topology to the traffic pattern of different applications at run-time. The run-time topology construction mechanism involves monitoring the network traffic and changing the inter-node connections in order to reduce the number of intermediate routers between the source and destination nodes of heavy communication flows. This mechanism should also preserve the NoC connectivity. In this paper, we first introduce the proposed reconfigurable topology and then address the problem of run-time topology reconfiguration. Experimental results show that this architecture effectively improves the NoC power and performance over the existing conventional architectures.","An Efficient Dynamically Reconfigurable On-chip Network 
Architecture  
10.5
Mehdi Modarressi1, Hamid Sarbazi-Azad1,2, Arash Tavakkol2 
1Computer Eng. Dept., Sharif University of Technology, Tehran, Iran 
2IPM School of Computer Science, Tehran, Iran  
modarressi@ce.sharif.edu, azad@sharif.edu, arasht@ipm.ir 
ABSTRACT 
In this paper, we present a reconfigurable architecture for NoCs 
on which arbitrary application-specific 
topologies can be 
implemented. The proposed NoC can dynamically tailor its 
topology to the traffic pattern of different applications at run-time. 
The 
run-time 
topology construction mechanism 
involves 
monitoring the network traffic and changing the inter-node 
connections in order to reduce the number of intermediate routers 
between 
the 
source 
and destination nodes of heavy 
communication flows. This mechanism should also preserve the 
NoC connectivity. In this paper, we first introduce the proposed 
reconfigurable topology and then address the problem of run-time 
topology reconfiguration. Experimental results show that this 
architecture effectively improves the NoC power and performance 
over the existing conventional architectures.   
Categories and Subject Descriptors 
B.4.3 [INPUT/OUTPUT AND DATA COMMUNICATIONS]: 
Interconnections (Subsystems)—topology 
General Terms 
Design 
Keywords 
NoC, performance, power, reconfigurable, topology. 
1. INTRODUCTION 
Application-specific optimization is one of the most effective 
approaches 
to 
improve 
the power/performance metrics of 
Network-on-Chips (NoC) [1]. Customizing the network topology 
for a given application is an important application-specific NoC 
optimization method which dramatically affects the power 
consumption and average latency of NoCs, when running that 
application. Like other application-specific optimization methods, 
existing topology customization methods generate a customized 
topology and mapping for a single target application, provided 
that the application and its traffic characteristics are known at the 
design time [2]. However, several different applications may be 
integrated onto single complex multicore System-on-Chips (SoC). 
In addition, NoC-based Chip Multiprocessors (CMPs) are 
generally applied to run different applications which may not be 
known at design-time. 
Generally, an NoC that is designed to run exactly one 
application does not necessarily meet the design constraints of 
some other application. Furthermore, the traffic pattern of a single 
application may vary significantly over time. 
In this paper, we address the need for dynamic topology 
adaption by introducing a reconfigurable NoC architecture which 
enables the network topology to dynamically adapt to the 
communication pattern of the currently running application.  
The reconfiguration of the proposed architecture is achieved 
by designing bypass-paths 
in 
the network which allows 
dynamically 
changing 
the 
inter-node 
connections 
and 
implementing the topology in which the number of intermediate 
routers between the source and destination nodes of high volume 
communication flows is reduced. This can lead to considerable 
performance improvement since the power/latency of the router’s 
pipeline stages has a significant contribution to the total NoC 
power/latency.  
Our early evaluation results using a static design-time 
topology generation method showed the effectiveness of this 
architecture in reducing NoC power consumption [3]. In this 
work, we show the ability of our reconfigurable NoC architecture 
in handling dynamic traffic patterns by developing a run-time 
topology 
reconfiguration mechanism. To guarantee 
the 
connectivity of the network when the topology is reconfigured, 
the connections of a subset of the NoC links are not allowed to 
change. This set of links, called fixed sub-network (or Fnet), is 
used to provide permanent connections among nodes according to 
a given topology (mesh, in this work). The second subset of links, 
called reconfigurable sub-network (or Rnet), on the other hand, is 
devoted to establish application-specific connections between the 
nodes with high-volume communication demands. The Rnet is 
still a packet-switched network, but its topology is customized for 
the traffic flows traveling over it.  
The NoC links and resources are divided between the two 
sub-networks using Spatial-Division Multiplexing (SDM). Unlike 
Time-Division Multiplexing (TDM), where at each time slot, all 
the wires of a link are dedicated to transmission of data from a 
single source, the SDM technique allocates a sub-set of the link 
wires to a given circuit for the whole connection lifetime.  
Improving the power and performance metrics of packetswitched NoCs by integrating a second switching mechanism has 
been addressed in several previous works [4][5][6]. For example, 
the long-range links in [6] are constructed to provide shortcut path 
between the source and destination nodes of high-volume 
communication flows at design-time. The reconfigurability is, 
however, the key advantage of our work over physical long-range 
links as it allows the shortcut paths to be constructed dynamically 
based on the current traffic pattern at run-time. Besides, being 
established over conventional NoCs, our architecture still benefits 
from 
the predictability and 
reusability of 
regular NoC 
architectures. 
166
 
 
The rest of the paper is organized as follows. Section 2 
introduces the proposed NoC architecture. A dynamic scheme for 
topology reconfiguration is then presented in Section 3.  Section 4 
presents the evaluation results, and finally, Section 5 concludes 
the paper. 
2. THE PROPOSED NOC  
The system under consideration is composed of m×n nodes 
arranged as a 2D mesh network. In 
the proposed NoC 
architecture, however, the routers are not connected directly to 
each other, but connected through simple switch boxes, called 
configuration switches (see Figure 1). Each square in Figure 1 
represents a network node which is composed of a processing 
element and a router, whereas each circle represents a 
configuration switch. Figure 1.a shows the internal structure of a 
configuration switch. It consists of some simple transistor 
switches that can establish connections between incoming and 
outgoing links. In this figure, for the sake of simplicity, only a 
single wire connection is depicted between each two ports of a 
configuration switch. However, 
there are 
two connections 
between each two ports of a configuration switch in order to route 
the incoming and outgoing sub-links of bidirectional links 
independently. Actually, the internal connections are implemented 
by a multiplexer at each output port of the switch. We refer 
interested readers to [3] and [7] for more details of this 
architecture, including its architectural features, area overhead, 
and reconfiguration delay. 
As mentioned in Section 1, in this work, we propose to 
partition the NoC into two parallel sub-networks using the SDM 
scheme to benefit from the decreased average packet path length 
(hop count) of 
the 
reconfigurable sub-network, whereas 
preserving the network connectivity via the fixed sub-network.  
An in-depth analysis of the effect of SDM on the NoC 
latency and throughput in [8] shows that splitting the links into 
two sub-links increases the throughput considerably (as the path 
diversity of SDM reduces the head of line blocking) with some 
negative effect on the average message latency (as the number of 
flits of a packet is increased). Thus, SDM is a suitable bandwidth 
multiplexing scheme for our NoC, since in addition to throughput, 
by selectively employing the sub-networks, we can also improve 
the power and latency of the NoC.  
Figure 2 illustrates the partitioning scheme, where network 
resources (links, buffers, crossbar, and configuration switches) in 
an N-bit wide NoC are divided into 2 parallel N/2-bit subnetworks. The first set of sub-links (the upper part) form Fnet and 
the other part is devoted to Rnet. The total buffering space in a 
router remains the same as that in the original conventional NoC. 
The crossbar switch structure is the same as in a conventional 
packet-switched router, but the switch allocator (arbiter) is 
divided into two separate Fnet and Rnet allocators.  
Although the proposed NoC is not restricted to a specific 
switching scheme, the NoC routers, in this study, adopt a 
conventional wormhole switching mechanism. However, the route 
computation and switch allocation schemes are adapted to the 
specific features of the proposed architecture.  
Rnet employs a table-based routing scheme by which Rentpackets (packets belonging to a communication flow for which a 
path is constructed in Rnet) are routed along the paths determined 
during the topology reconfiguration procedure. 
10.5
Figure 1.  The reconfigurable NoC architecture 
{
t
r
o
p
{ {
N
t
u
p
n
i
r
o
t
c
e
v
n
o
i
t
i
t
r
a
P
{
t
u
p
t
u
o
t
u
p
t
u
o
N
S
t
u
p
t
u
o
W
t
u
p
u
o
t
E
{
Figure 2. The proposed router architecture 
Fnet, on the other hand, is used by the remaining NoC 
packets. To improve the resource utilization, Fnet-packets can use 
free cycles of the Rnet links in order to bypass some intermediate 
nodes and shorten the distance to the destination nodes.  
To route an Fnet-packet, the router first checks if there exists 
an Rnet long link originating from the current node destined to 
some node along the route towards the packet’s destination. As a 
packet is only allowed to use the shortest paths, routing logic 
checks the (at most two) Rnet output ports along the shortest paths 
toward the destination for long links. Between the two Rnet ports, 
the one which allows more intermediate nodes to be skipped over 
is selected. In parallel, the default routing algorithm (x-y routing) 
is also executed to select an Fnet output port. The header then 
requests for the selected Fnet and Rnet (both, if any) output ports 
in the switch allocation stage of the router pipeline. If the flit wins 
more than one ports at the same cycle, it selects the Rnet port. 
Fnet-packets may switch between Rnet and Fnet several times to 
reach to their destinations. Switching between Rnet and Fnet is 
done by the multiplexers between the input buffers and the 
crossbar input ports. In the current implementation, deadlock 
freedom on Rnet is guaranteed using the well-known escape 
channel method [9] which involves using some extra virtual 
channels per port. 
Although this section assumes that the link-width is equally 
divided between the Fnet and Rnet sub-networks, this architecture 
is capable to assign different number of wires to each subnetwork. This can be achieved by using a bit-vector register, 
called the partition vector (PV), which determines the bit-width 
assigned to each sub-network. Setting a bit in PV to 1 indicates 
that the corresponding bit in the NoC link is assigned to Fnet, 
otherwise it belongs to Rnet. PV is used in the network interface 
of each node to determine the flit width of packets. Setting the flit 
width to an appropriate value can be easily carried out by shift 
registers. Clearly, the flit width should be long enough to 
guarantee that the routing and other required information can be 
embedded into a single flit (the header flit). Moreover, PV is used 
at the crossbar to direct the flits to appropriate output port wires 
167
 
  
 
 
 
 
 
 
 
 
10.5
(Figure 2), according to the decision made by each arbiter ( the 
Fnet and Rnet allocators).  
Reconfigurability is the key point of the proposed partitioning 
method, in that the link-width of Fnet and Rnet can be changed at 
run-time by simply changing the bits of the PV register. The 
network can vary from a fully customized to a fully fixedtopology network, based on the current traffic pattern. 
3. RECONFIGURATION ALGORITHM  
This section proposes a topology reconfiguration method which 
dynamically changes the Rnet connections in response to a change 
in the current on-chip communication pattern. Here, it is assumed 
that each task is non-migratory and already mapped onto some 
node. The whole procedure relies on monitoring the traffic 
generated by each node 
in order 
to detect high-volume 
communication flows. A mechanism then selects the best Rnet 
path for the detected flows. The path must include only the routers 
and configuration switches along the shortest paths between the 
source and destination nodes of the requesting flow.  
Finding a new topology using this procedure may take a few clock 
cycles. However, this procedure is done in parallel with normal 
network operation and packets do not wait for the new Rnet 
configuration to be completed; they continue traveling on the 
current Fnet and Rnet configuration. Therefore, this setup latency 
does not degrade the network performance.  
Traffic Monitoring. Traffic monitoring is simply done at each 
node by storing the number and the destination node addresses of 
the packets the node sends. The weight of each flow is an m-bit 
value which is set to the m most-significant bits of the register 
holding the traffic volume, multiplied by the traffic flow length 
(defined as the Manhattan distance between the source and 
destination nodes of the flow); this is because as the length of a 
flow increases, the contribution of the flow in the entire on-chip 
traffic increases. 
Reconfiguration Policy. Once 
the weights are updated 
periodically at some specific times, each node sends the weight of 
its flows weighting higher than a threshold to a global arbiter. The 
threshold of each node is defined as the average communication 
volume of the flows originating from it. The arbiter then sorts the 
communication flows in order of their communication volume and 
tries to build a path with minimum cost for each flow in the order. 
We calculate the cost of a path as the cumulative cost of the 
routers and configuration switches it includes. According to the 
power and latency analysis presented in [3], we assign a cost of 1 
to a configuration-switch and a cost of 5 to a router. 
Initially, in the topology selection algorithm, it is assumed that the 
internal connections of all configuration switches are unconfigured. Finding a route may involve configuring the switches 
which are not yet configured in order to bypass some intermediate 
routers and make a shorter connection between two given nodes. 
The algorithm can configure 
the un-configured 
internal 
connections of the configuration switches, but not the connections 
that have already been configured at previous iterations of the 
algorithm. 
 Path Selection Algorithm. We employ a simple and tiny set-up 
network (or control network) along with the main data network to 
configure the Rnet paths. The size of the setup network is the 
same as the size of the main data network. Each node in the setup 
network corresponds to a node (router or configuration switch) in 
the data network with the same address. Having a small bit-width 
and a simple internal structure, the area of the setup network is 
negligible, compared to the main data-network. It also consumes 
negligible power due to its infrequent activity and small bit-width. 
To calculate the minimum cost for an Rnet path for a flow, the 
cost of the path is propagated from the source node toward the 
destination node, along all of the shortest paths. In this way, 
initially, the source node sends its cost as the initial partial-path 
cost to its neighboring nodes along the shortest path towards the 
destination. Each node (router or configuration switch) receives 
the partial path cost (the cost of the path from the source node to 
the current node), increases it by its cost, and propagates the result 
to the next nodes towards the destination, until the final cost is 
received at the destination node. If the current node is a router, it 
sends the cost to its neighboring configuration switches along the 
shortest paths. Similarly, configuration switches send the updated 
cost to appropriate neighboring routers or configuration switches. 
However, the current configuration switch may be already 
configured in previous steps of the algorithm, in such a way that 
the port through which the cost reaches the switch is connected to 
some output port. In this case, the cost should be sent along the 
direction determined by the current switch configuration, provided 
that the direction is along the shortest path towards the 
destination. Starting from the source node, the cost values are 
forwarded to the destination node one hop per cycle. The 
intermediate nodes may receive two costs from the neighboring 
nodes at the same cycle. In this case, each node selects and 
propagates the smaller cost and ignores the other one. 
A node also ignores a received cost, if the link between the 
current node and the node from which the cost is received does 
not have enough bandwidth 
to accept 
the 
requesting 
communication flow.  
Each node keeps the track of the path with the minimum cost by 
storing the direction from which the minimum cost is received. 
Once the final cost reaches the destination node, indicating that an 
Rnet path with the minimum possible cost is found for the flow, 
the destination node sends back an acknowledgement signal 
toward the source node along this path. The algorithm then 
updates the state of the according configuration switches within 
the path. Afterwards, the algorithm continues with the next flow, 
until all requesting flows are serviced. The new configuration is 
then loaded into the network by setting the routing tables and 
configuration switches. A flag is also set at the source node of the 
flows for which an Rnet path is constructed to direct the traffic of 
the corresponding flow through Rnet. 
4. EXPERIMENTAL RESULTS 
4.1 Synthetic Traffic  
Since realistic CMP workloads generally exhibit a high degree of 
temporal and spatial communication locality [10], we use a 
synthetic traffic with similar characteristics in this section. To this 
goal, we use the synthetic traffic patterns presented in [10], to 
evaluate the effects of the presented architecture on the CMP 
workloads. In the first pattern, the 1-hot flow traffic pattern, each 
node sends 80% of the generated packets to exactly one 
destination node and the remaining 20% packets to other 
randomly chosen nodes. In the other traffic pattern, the 3-hot flow 
traffic pattern, each node sends 20% of the generated packets 
equally randomly to the network nodes and the remaining 80% are 
sent to three specific nodes. The hot destination nodes of a source 
node, defined as the destinations to which the source sends most 
of its packets, are selected randomly from the network nodes. 
In the simulations, the randomly-selected hot destination nodes of 
each source node are changed every 200,000 cycles and the 
reconfiguration procedure is initiated every 100,000 cycles.  
168
Reconf. NoC
Conv. noC
latency, the proposed architecture outperforms the conventional 
NoC by 19%, on average. Following the same trend, it also 
decreases the on-chip communication energy by 12%, on average. 
10.5
y
c
n
e
t
a
L
t
e
k
c
a
P
e
g
a
r
e
v
A
80
60
40
20
0
0
Reconf. NoC
Conv. noC
0 .01
0 .02
0 .03
Injection Rate
y
c
n
e
t
a
L
t
e
k
c
a
P
e
g
a
r
e
v
A
80
60
40
20
0
0
0.02
0.04
0.06
Injection Rate
20
15
10
5
0
)
%
(
n
o
i
t
c
u
d
e
R
y
g
r
e
n
E
Figure 4. The reduction in average packet latency (left) and energy 
consumption (right) of the proposed NoC over the conventional NoC 
5. CONCLUSIONS 
We proposed a dynamically reconfigurable architecture for NoCs 
on which arbitrary application-specific 
topologies can be 
implemented. The reconfigurability of the proposed NoC allows it 
to dynamically tailor its topology to the traffic pattern of different 
applications at run-time. The network connectivity in our NoC is 
guaranteed by partitioning the entire links into two sets and 
keeping the connection of the links of one of the sets fixed. 
Simulation results showed that the proposed NoC architecture 
consumes less power and reduces the average communication 
latency, compared to a conventional NoC.  
6. "
Networks on Chips - from research to products.,"Research on Networks on Chips (NoCs) has spanned over a decade and its results are now visible in some products. Thus the seminal idea of using networking technology to address the chip-level interconnect problem has been shown to be correct. Moreover, as technology scales down in geometry and chips scale up in complexity, NoCs become the essential element to achieve the desired levels of performance and quality of service while curbing power consumption levels. Design and timing closure can only be achieved by a sophisticated set of tools that address NoC synthesis, optimization and validation.","20.1
Networks on Chips:
from Research to Products
G. De Micheli(cid:2) , C. Seiculescu(cid:2) , S. Murali§ (cid:2) , L. Benini‡ , F. Angiolini§ , A. Pullini§
(cid:2) LSI, EPFL, Lausanne, Switzerland,{ciprian.seiculescu, giovanni.demicheli}@epﬂ.ch
§ iNoCs, Lausanne, Switzerland, {murali, angiolini, pullini}@inocs.com
‡ DEIS, University of Bologna, Bologna, Italy, luca.benini@unibo.it
ABSTRACT
Research on Networks on Chips (NoCs) has spanned over a decade
and its results are now visible in some products. Thus the seminal
idea of using networking technology to address the chip-level interconnect problem has been shown to be correct. Moreover, as technology scales down in geometry and chips scale up in complexity,
NoCs become the essential element to achieve the desired levels of
performance and quality of service while curbing power consumption levels. Design and timing closure can only be achieved by a
sophisticated set of tools that address NoC synthesis, optimization
and validation.
Categories and Subject Descriptors
B.4.3 [INPUT/OUTPUT AND DATA COMMUNICATIONS]:
Interconnections (Subsystems)—topology
General Terms
Design
Keywords
Network on Chip, NoC, System on Chip, SoC
1.
INTRODUCTION
A key challenge in the design of multi-core chips is the choice
of a scalable system-level interconnect. This issue surfaces in both
general-purpose multi-cores, i.e., Chip Multi-Processors (CMPs),
as well as in heterogeneous, application-speciﬁc Multi-Processor
Systems on Chips (MPSoCs). A similar challenge affects the design of the next generation of Field-Programmable Gate Arrays
(FPGAs).
The importance of interconnects for system performance is growing with technology scaling. An increasingly large number of onchip cores with continuously improving performance can be found
in designs such as TI’s OMAP [30], Inﬁneon XMM/X-Gold [31]
or ST’s Nomadik [29]: a mobile phone SoC nowadays comprises
several tens to hundreds of components that need to be connected
together. Thus, application bandwidth requirements have also been
increasing steadily. Furthermore, at the physical level, with technology scaling, gate delays decrease while global wire delays do
not. Thus, in current advanced technologies the delay on the wires
has an increasingly signiﬁcant impact on system performance.
For a long while, bus-based solutions have been widely used to
connect components inside chips. As the number of components
and their complexity scales up, the complexity of the bus system
also increases. Bus architectures have evolved signiﬁcantly, with
designers migrating from a single shared-bus topology to bridged
buses and to multilayer buses. Today a complex SoC can have several levels of bus hierarchy. The protocol complexity has also increased, with support for burst, outstanding, out-of order transactions, just to give some examples.
One of the most critical design issues with bus-based systems is
that it is getting increasingly hard to achieve design closure. The
use of several levels of bus hierarchies, interconnected by means of
bridges and crossbars in an ad hoc manner is difﬁcult to design and
even more challenging to verify. Moreover, it is hard to predict the
length and the delay of the wires of the bus within the architectural
design phase. Accurate wire delay estimates are possible only at
the end of the physical design (placement and routing) phase, and
any delay violations lead to costly design iterations.
The use of networking principles to connect components alleviates some of the major issues with bus-based systems. There are
several advantages in adopting a Network on Chip (NoC) paradigm
for chip-level interconnects. First and foremost, the modularity
of NoCs is a key asset in supporting scalability from the ground
up, in particular in terms of performance. Physical-design-aware
NoC components enable large-scale System on Chip (SoC) design
with more predictable (and possibly guaranteed) performance. Second, NoCs can be tuned to support speciﬁc applications on SoCs.
Whereas macroscopic networks (e.g., LANs, WANs) emphasize
general-purpose communication schemes and modularity, in NoCs
these constraints are less restrictive because, in most cases, onchip communication is known at design time. Thus NoC implementations may be optimized, e.g., by merging or separating data
and control trafﬁc, or using ad-hoc bus widths and ﬂow control
schemes. This ﬂexibility enables CAD tools to explore the power/
performance design space and provide designers with effective solutions. Finally, the distributed nature of NoC infrastructures can
be effectively leveraged to enhance system-level reliability. For
example, NoCs can locally handle at run-time the correction of
timing failures induced by variability and/or other signal integrity
issues. Moreover, reconﬁgurable NoCs can support component redundancy in a transparent fashion, thus being an essential technology for designing highly-dependable systems. The key challenge in
bringing networking means to silicon is to suitably adapt the principles to the chip medium, and to achieve a low-latency interconnect
with low power consumption and area overhead.
300
20.1
Many SoCs have been realized with NoC-based interconnects in
the recent years. Examples range from a multitude of designs based
on the ARM AMBA Network Interconnect [32], which closely resembles a NoC due to their multi-layered nature, to Intel’s Teraﬂop
Research Chip (also called it Polaris), a multi-core processor [37],
and the new Single-chip Cloud Computer [38]. Tilera markets the
TILE-Gx [27], a 100 core processor, which is the commercial spinoff of research done on the RAW architecture [39] at MIT. Other
research SoCs include LETI’s FAUST [25] and KAIST’s BONE
series [41]. Whereas some of these chips will be reviewed in Section 5, it is important to stress that NoCs are present in commercial
products and that top semiconductor manufacturers have invested
in NoC design methods and tools.
Electronic Design Automation (EDA) tools for NoC design and
optimization have been the object of intensive research in the last
decade. A few providers of NoCs have shown to support a ﬂow
and to yield cost-effective designs. As NoCs become mainstream,
the related tools will be an important value-added part to standard
design ﬂows. Indeed, one of the biggest advantages of NoCs is the
ease of design, optimization and veriﬁcation, and thus faster design
times that NoC design automation can bring. With the use of appropriate tool ﬂows, the NoC operating frequency can be predicted
accurately already during architectural design, accounting for the
impact of ﬂoorplan on wire lengths. Thus, fewer (or no) iterations
are needed across the architectural and physical design phase for
convergence. The NoC can be ﬁnely tuned and optimized, and its
power consumption can be evaluated and reduced. Other advantages include scalability to newer SoC platforms, ability to support
varied application Quality-of-Service (QoS) constraints, better support for voltage and frequency islands, etc..
2. HISTORY OF NOCS
Design issues in macro-networks (LAN, WAN, Internet) have
received broad attention in the last twenty ﬁve years. In the last
decade, the design of chip-to-chip interconnection networks for
parallel processing has also received considerable focus. However,
the challenges encountered in the design of on-chip networks for
SoCs is quite different from the design of such macroscopic networks. Some major differences are the following three.
(1)The
communication between the various cores can be statically analyzed for many SoCs, so that the NoC can be tailored for the particular application behavior. In the case of macroscopic networks, it is
impossible to obtain an upfront knowledge of the trafﬁc patterns of
all the users. (2) The design objectives and constraints are different.
As many SoCs are used in mobile and hand-held devices, having a
network with minimum power consumption becomes an important
design objective. Many SoCs also need to respond in real-time for
certain inputs, for which the NoC has to support different criticality
levels for the different trafﬁc streams. Area and latency constraints
are also much more stringent for NoCs.
(3) The design process
should also consider VLSI issues, such as the structure (ﬂoorplan
requirements) and wiring complexity of the resulting interconnect.
The use of packet-switched networks to connect components inside a computing system was advocated ﬁrst by Seitz and co-workers
[1]. Nevertheless, the concept of Networks on Chip was pioneered
by Greiner within the Scalable Programmable Integrated Network
(SPIN) project [3] and elaborated in its various facets by Benini et
al. [2], and Dally et al. [5], [7] in the early part of this decade.
Thereafter, there has been a ﬂurry of contributions addressing the
different design issues of NoCs. We present a non-exhaustive summary of some important contributions, while referring the author to
[4] for a detailed description of the literature.
Early examples of chip realizations involving NoCs or precursors
include the following. The Maia heterogeneous signal processing
architecture [6] is fully instance-speciﬁc and uses circuit switching
to route data. The RAW architecture [39] uses a mesh topology
to support general-purpose parallel multiprocessing and introduces
the notion of exposing the wiring architecture to the compiler. The
SPIN project described in [3] is an early example of a NoC architecture, with the use of a regular, fat-tree-based network.
Many NoC architectures have been proposed. A large fraction of
these are natively synchronous, such as ×pipes [44], NOSTRUM
[10], Spidergon [22]; others are conceived to support asynchronous
operation, such as Mango [26], FAUST [25] and ANOC [23]. Some
architectures were presented to achieve predictable QoS behavior,
using special hardware mechanisms, such as Æthereal [21], QNoC
[20]. We show some examples of NoC architectures in Section 3.
Several research groups have focused on design automation for
NoCs. The issues include routing strategy development, topology
synthesis, QoS achievement, buffer sizing. Initial works on topology design focused on mapping cores onto regular topologies [8],
[9]. The research and development of ×pipesCompiler
[45] and
followup tools [11] addressed both the support of heterogeneous
NoC topologies, tailored to the application trafﬁc requirements,
and the corresponding design tool ﬂow. It strongly differentiated
from earlier approaches that were targeting only standard topologies, such as meshes, as these do not map well to SoCs that are
usually heterogeneous in nature. In this work, both a parametrized
library ×pipes and a NoC hardware compiler were presented to address the problem of synthesis and optimization for heterogeneous
NoCs by means of highly-conﬁgurable network building blocks,
customizable at instantiation time for a speciﬁc application domain.
This work showed the advantages of customizing NoC resources,
such as switches, and the need to consider the ﬂoorplan of the chip
when designing the NoC.
While the need for custom topologies became evident at this
point, synthesizing them automatically to meet application speciﬁcations considering physical design issues still remained a challenge. Topology synthesis needs to account for several, often conﬂicting, objectives such as reducing latency and power consumption. It should also consider a variety of network operational issues.
For example, the synthesized topologies should be free of routing and message-dependent deadlocks. Moreover, the wire lengths
should also be considered when calculating the power and delay
values of topologies. This motivated the work of SunFloor [11],
where a synthesis tool was presented to design application speciﬁc
custom topologies. The tool has several key features, including a
method to gradually drive the synthesis process to meet conﬂicting objectives and constraints and the ability to explore a large design space, producing several design points with different powerperformance values. Another highlight of this work and subsequent
developments [12] is the use of incremental ﬂoorplanning. The tool
takes an early ﬂoorplan of the SoC (without the interconnect) as an
input, which is used to guide the synthesis process. Once a topology is designed, the tool inserts the NoC components in the best
positions in the ﬂoorplan, while marginally perturbing the initial
ﬂoorplan input. This incremental ﬂoorplan is then fed as an input
to standard placement and routing tools. This approach captures accurately wire delays and power values of the NoC during topology
synthesis.
Around the same time, several other works also tackled different
aspects of the application speciﬁc custom topology synthesis [13][19]. Some groups have established complete tool ﬂow for NoCs,
that include CAD tools to design the NoCs, covering from architecture to physical design issues. Æthereal [21] and Netchip [42]
are examples of such complete tool ﬂows.
301
20.1
3. NOC ARCHITECTURES
Many NoC architectures have been proposed in literature.
In
on-chip networks, the wiring limitations are less strict than in multiprocessor chip-to-chip networks. For this reason, simple switches
with little buffering and reduced complexity are the most suitable.
Complex switches that aim at maximizing link utilization are not
needed for most NoCs and some of the initial NoC architectures
overlooked this trend, which is dominant in recent implementations. The subject of NoC architecture is extensive and we refer
the reader to [4] for a detailed coverage. We report here on modular NoC architectures that can be built out of a simple (parametrizable) library, and that can be thus be considered in connection with
design automation.
A modular NoC architecture usually consists of at least three
basic elements:
• Network Interfaces (NIs)
• Switches
• Links
The main role of the Network Interfaces is to convert the bus protocol that is used by the Processing Elements (PEs) to the network protocol used by the switches. An NI is needed to connect
each IP core to the NoC. NIs convert transaction requests/responses
into packets and vice versa. Packets are then serialized into a sequence of FLow control unITS (ﬂits) before transmission, to decrease the physical wire parallelism requirements. While there
are no standards NoC protocols for intra-network communication,
many NoCs support standard protocols (e.g., OCP, AHB, AXI, Wishbone, OPB, PLB) at the outer edge, to connect the PEs to NIs. This
enables existing IPs to be connected easily to the network and provides greater ﬂexibility in connecting IPs using different protocols
to the same system interconnect.
Switches are the backbone of the network. Their main function
is to route packets from source to destination. They provide arbitrary connectivity between several inputs and several outputs and
allow for implementation of different topologies in order to provide
connectivity for many PE. Switches provide buffering resources
to lower congestion and improve performance. The buffers could
be placed at the input ports (input-queued router), output ports
(output-queued router) or at both places. In many NoCs with regular topologies where one or few cores are connected to a switch, the
functionality of the NI is integrated in the switch itself. Links abstract the connectivity between NIs and switches and between the
switches themselves. Links can represent more than just physical
wires as they can provide pipelining in order to achieve the required
timing.
ponents, based on ×pipes [44], is presented in Figure 1. This liAs an example, a brief description of the basic NoC library combrary incorporates features that have been successful in many NoC
designs. In ×pipes, two separate NIs are deﬁned, an initiator and a
target one, respectively associated with system masters and system
slaves. A master/slave device will require an NI of each type to be
attached to it. The interface among IP cores and NIs is point-topoint as deﬁned by the Open Core Protocol OCP 2.0 [46] speciﬁcation, guaranteeing maximum re-usability. NI Look-Up Tables
(LUTs) specify the path that packets will follow in the network
to reach their destination (source routing). ×pipes switches use
wormhole switching, as most common in NoCs, but support two
variations of ﬂow control. If ACK/NACK ﬂow control is used then
output buffers are required, as ﬂits have to be retransmitted until the
downstream router has sufﬁcient capacity to store and accept them.
302
Figure 1: ×pipes building blocks: (a) switch, (b) NI, (c) link
If ON/OFF ﬂow control is used, backpressure from the downstream
switch stalls the transmission until the there is sufﬁcient buffering
capacity. In this case, output buffers can be omitted. In any case,
the arbiter is required to resolve conﬂicts between packets when
they require access to the same physical link.
Other arbitration and routing schemes have been developed in order to offer support for predictable communication behavior. The
Æthereal NoC design framework presented in [21] aims at providing a complete infrastructure for developing heterogeneous NoC
with end-to-end quality of service guarantees. The network supports guaranteed throughput (GT) for real time applications and
best effort (BE) trafﬁc for timing unconstrained applications. The
architecture offers so-called GT connections which provide bandwidth and latency guarantees on that connection. In order to provide bandwidth and latency guarantees, it uses a Time Division
Multiple Access (TDMA) mechanism to divide time in multiple
time slots, and then assigns each GT connection a number of slots.
The result is a slot-table in each NI, stating which GT connection is
allowed to enter the network at which time-slot. For trafﬁc that has
no real-time requirements, Æthereal implements Best-Effort connections.
4. PHYSICAL IMPLEMENTATION
One of the main strengths of NoCs is their promise to simplify
back-end design closure. For this reason, it is essential to assess
and optimize the interplay of NoC technology with physical design
processes.
4.1 Structured Wiring
NoCs have been improving on-chip wiring in two distinct ways.
First, the packetization paradigm enables easily the implementation of communication serialization. A typical on-chip bus requires
around 100 to 200 wires: 32 or 64 bits of write data, 32 or 64 bits
of read data, 32 bits of address, plus control signals. On the other
hand, a NoC sends packets, and can do so by splitting them over
multiple cycles in ﬂits. Therefore, it does not, in principle, have
constraints over how many wires need to be deployed in parallel. By deploying highly serialized links, routing can be simpliﬁed,
while area and crosstalk can be minimized.
In practice, a lower
bound is set by performance needs. Published NoC research shows
that some implementations [21] have gone for a ﬁxed ﬂit width and
packet structure, whereby the number of wires is much more manageable than in buses, e.g. 32; some others [44] even allow for
20.1
Figure 2: Study on 65nm, 32-bit switch scalability. Routers up
to 10x10: 85% row utilization or more; 14x14 to 22x22: 70%
to 50% row utilization; 26x26 and above: DRC violations to
tackle manually even at 50% row utilization
complete ﬂexibility, letting designers choose their favorite performance/wiring tradeoff.
A second contribution of NoCs to a tidier wiring implementation
is through wire segmentation. As NoC wires are laid point-to-point,
as opposed to being multipoint nets in buses, it is possible to optimize NoC topologies to constrain maximum wire lengths. This
is done either by choosing highly regular topologies, e.g. meshes,
or by suitable NoC topology synthesis. Furthermore, as shown in
Section 3, links can be explicitly segmented to further break critical
paths. This is simpler on a NoC than on a bus, where most speciﬁcations, including AMBA AHB [33], implicitly assume singlecycle communication among masters and slaves.
4.2 Routability
Bus-based architectures have been extended with components
such as crossbars, as e.g. in Multilayer AHB [33], whereby fullyconnected data lanes allow for parallel communication among a
plurality of masters and slaves. Crossbars are successful at providing non-blocking access and minimizing arbitration delays. Unfortunately, if the inputs and outputs of the crossbars are 100- to
200-wires wide as in buses, crossbars may exhibit serious physical
wire routability issues. Due to this, commercial tools [34] often
constrain the maximum crossbar size to 8x8 or less. NoCs permit wire serialization, largely obviating the issue. Figure 2, based
on [43], shows that NoC switches of radix 10x10 can be efﬁciently
designed, and even much larger switches are still feasible, though
at an area and frequency cost. Alternatively, smaller NoC routers
can be chosen, completely solving routability concerns.
4.3 Synchronization Schemes
To tackle the increasing challenges of global clock distribution
in large chips, including the power cost and variability concerns,
a variety of Globally Asynchronous Locally Synchronous (GALS)
chip design paradigms have been proposed. NoCs offer a natural backbone for the implementation of such approaches. This is
because packet-switching networks (i) are distributed, (ii) natively
provide ways to tackle heterogeneity, including in timing, and (iii)
natively decouple transaction injection and transaction transport
times. Among others, fully asynchronous communication [35] and
pausible clocking [24] have been proposed and demonstrated. By
incorporating all necessary timing adaptation features natively in
the on-chip communication framework, designs can converge more
quickly and easily, strengthening the “plug&play” view of system
composability.
Figure 3: 3D IC with NoC for communication
4.4 3D Integration Extensions
3D chip stacking is increasingly touted [47] as a way to pursue
Moore’s Law and “More than Moore” visions of heterogeneous,
multi-functional products including MEMS and bio-interfaces. Nevertheless, to be successful, 3D integration still has to solve some
shortcomings, such as the yield of vertical connections, the area
overhead, and the complexity of system design and veriﬁcation.
NoCs are an ideal ﬁt to 3D design paradigms because they represent a ﬂexible, scalable, distributed backbone. Figure 3 shows a
chip where iNoCs [36] technology has successfully met the constraints of 3D design. For example, area and yield have been optimized by suitably serializing vertical links, to minimize the number
of required vertical vias. Veriﬁcation has been automated by leveraging built-in link testing facilities. 3D system integration has also
been made easier by the ﬂexibility of NoC routing tables, easily
enabling either 2D-only operation (in testing mode) or 3D-capable
communication. The NoC also hides the 3D clocking challenges
by natively handling synchronization among layers.
5. NOC CHIP EXAMPLES
Many SoCs for multimedia and wireless connectivity applications fabricated in 45nm technology have either a proprietary NoC
or NoC IP from a third party IP provider. In this section, we show
some chip-level implementations of NoCs. We provide small casestudies on the use of NoCs and their implementations.
ARM is commercializing the AMBA Network Interconnect [32].
The IP library provides crossbars and bridges that can be assembled
to construct a hierarchical bus interconnect that closely resembles
an NoC, because the topology of the interconnect can be designed
to match the ﬂoorplan and buffering can be added to provide stalls.
NoCs are being actively used in CMPs, both in academia and in
real products. The Tilera TILE-Gx processor [27] has 100 cores
integrated onto a chip, with the cores connected by a 2D mesh network. A NoC-based interconnect from Arteris [28] has been implemented to connect the components on the TI OMAP platform.
Several research prototypes for CMPs, such as the TRIPS processor [48], Smart Memories [49], use a NoC to connect the cores
together. The Intel Teraﬂops [37], a prototype 80-core processor,
also uses a mesh network to connect the cores. A block diagram depicting a core and the 5-port router is shown in Figure 4. Each core
consists of two programmable ﬂoating point units and a ﬁve-port
router. The routers are connected in a 2D mesh topology. In order
303
20.1
Figure 4: A single core and a 5-port router of the Intel 80 core
processor
Figure 5: A multicore NoC using the BONE architecture
to avoid the communication overhead in maintaining coherency,
the system does not use cache coherency and instead, data is transferred using message passing. The aggregate bandwidth supported
by the chip at 3.16 GHz operating speed is around 1.62 Terabits/s.
The GALS based ANoC and the multi-synchronous DSPIN NoC
have been implemented in two demonstrator chips as system interconnect for the FAUST application [25]. The FAUST (Flexible Architecture of Uniﬁed Systems for Telecom) demonstrator is a SoC
platform with advanced telecommunication capabilities. The implemented topology is a quasi-mesh as on some routers connect
more than one core. In the receiver matrix - which consists of only
of 10 cores - the aggregate required bandwidth is 10.6 Gbits/s to
maintain real time communication.
Several chips have been fabricated by the BONE NoC group,
including the design of a NoC based parallel processor for visual
attention engine [40] and an object recognition processor [41]. The
chip layout of a memory centric NoC for a homogeneous MPSoC
designed by the team is shown in Figure 5. The design consists of
8 dual port memories, crossbar switches and ten RISC processors.
They are connected in a hierarchical star topology. The dual-port
SRAMs are assigned dynamically to the RISC processors that are
exchanging data. The crossbars act as a non-blocking medium to
connect the RISC processors and the SRAMs. The architecture
supports ﬂexible mapping of tasks to processors, thereby providing
better performance than a conventional 2D mesh-based CMP.
304
Figure 6: A NoC design ﬂow from iNoCs
6. NOC DESIGN TOOL FLOW
Designing an efﬁcient NoC architecture, while satisfying the application performance constraints, is a complex process. The design issues include topology synthesis, ﬁnding routes for trafﬁc
ﬂows, setting architectural parameters (such as frequency of operation, link width), verifying performance, building simulation and
emulation models [42].
In order to handle the design complexity and meet the tight time-to-market constraints, it is important to
automate most of these NoC design phases. To achieve design closure, the different phases should also be integrated in a seamless
manner.
Some companies like: Arteris [28], iNoCs [36], Silistix [35] are
providing design automation tools for NoCs . As an example, we
describe here the tool ﬂow from iNoCs, also shown in Figure 6 [36].
The tool ﬂow takes the application architecture and application
constraints as inputs. The architecture speciﬁcations include the
type of core (master or slave), the kind of protocol supported. The
application communication constraints include the average bandwidth of communication between the different cores, average latency constraints, hard QoS constraints on bandwidth and latency,
type of transaction, trafﬁc shape. This information is obtained by
application proﬁling or from the designer’s estimates. The tool ﬂow
also optionally takes the ﬂoorplan of the SoC without the interconnect as an input. The ﬂoorplan is an estimate of the position
of each core, depending on the I/O constraints and the communication among cores. Instead of a ﬂoorplan, a simpler metric can
be used, such as the relative distance between the blocks. Finally,
the NoC components are characterized with the target technology
library to compute the area, power and maximum operating frequency of the routers, NIs and links. All this information is fed
into the design toolchain. Based on the speciﬁcations, the topology
synthesis tool builds several topologies with different switch counts
and architectural parameters. The topologies are designed to meet
the application and architecture constraints, with each design point
having different power, area and performance values. From the set
of all Pareto optimal points, the designer can then choose a NoC
instance. Then, the RTL of the topology is automatically generated. The tools also generate simulation models (high level as well
as RTL) with trafﬁc generators that can be used to validate the runtime behavior of the system.
If an input ﬂoorplan is provided, the synthesis tool can use the
knowledge of the positions of the cores to design topologies with
shorter wire lengths. Moreover, accurate delay and power consumption of the wires can be obtained and used during the synthesis process itself. The tool also produces an output ﬂoorplan for
the topology point, with the NoC components placed at the ideal lo20.1
cations, in order to minimize power consumption and delay. Then,
the RTL and simulation models of the topology are generated. The
tool ﬂow supports several features, such as providing hard real-time
guarantees if needed, without additional hardware support. It also
supports the concept of voltage islands, where cores in an island
operate at the same frequency and voltage, while cores in different
islands can operate at different frequencies and voltages.
7. CONCLUSIONS
Network on Chip technology has been established as the preferred way of realizing system-level interconnect for most (if not
all) high-end SoC products for nomadic and multimedia applications fabricated with the 45nm node. These SoCs embody either a
proprietary NoC or a NoC from a third-party IP provider. While
looking at current and future designs of complex SoCs in 45nm
technology and beyond, NoCs are essential components to achieve
performance and design closure. At the same time, there is a growing interest of using NoCs in complex FPGA designs as well.
The EDA infrastructure for NoC design is provided by either inhouse design groups or by a few commercial companies. This infrastructure comprises both IP blocks and tools for optimizing and
integrating the IP with the rest of the design. Needless to say, the
EDA infrastructure has shown to be essential for realizing existing
large-scale SoCs with NoC-based interconnects.
Future directions of NoC research and applications lead along
two avenues. The former is related to 3-Dimensional chip stacking, with 3D NoCs providing a modular and ﬂexible interconnect
means that can also obviate for vertical connection failures and engineering changes. The second avenue deals with other structured
interconnect means, including optical NoCs as well as networks integrating chips and the environment with RF means. In all these
cases, key features are the separation of computation and communication via a NoC as well as the NoC modularity and support for
scalability.
8. ACKNOWLEDGMENT
We would like to acknowledge the ﬁnancial contribution of CTI
under project 10046.2 PFNM-NM, the ARTIST-DESIGN Network
of Excellence and the EU FP7 project NaNoC (248972).
9. "
"The aethereal network on chip after ten years - goals, evolution, lessons, and future.","The goals for the Æthereal network on silicon, as it was then called, were set in 2000 and its concepts were defined early 2001. Ten years on, what has been achieved? Did we meet the goals, and what is left of the concepts? In this paper we answer those questions, and evaluate different implementations, based on a new performance: cost analysis. We discuss and reflect on our experiences, and conclude with open issues and future directions.","
20.2
The Aethereal Network on Chip after Ten Years:
Goals, Evolution, Lessons, and Future
(Invited Paper)
Kees Goossens
Eindhoven University of Technology
k.g.w.goossens@tue.nl
Andreas Hansson
University of Twente
a.hansson@utwente.nl
ABSTRACT
The goals for the Æthereal network on silicon, as it was
then called, were set in 2000 and its concepts were deﬁned
early 2001. Ten years on, what has been achieved? Did
we meet the goals, and what is left of the concepts? In
this paper we answer those questions, and evaluate diﬀerent
implementations, based on a new performance:cost analysis.
We discuss and reﬂect on our experiences, and conclude with
open issues and future directions.
Categories and Subject Descriptors
B.7 [Hardware]: Intregrated Circuits
General Terms
Performance
Keywords
Network on chip, rate control, circuit switching
1. APPLICATION DOMAIN AND GOALS
Work on the Æthereal network on chip (NOC) started at
Philips Research, for systems on chip (SOC) in the consumerelectronics (CE) domain, in particular digital TV (DTV)
and set-top boxes (STB). These systems include applications
such as audio and video decoding and improvement that
have real-time requirements and high computational performance at low cost (high bandwidth (Gb/s) to area (mm2 ) ratio). Moreover, unlike for general-purpose computing, consumers do not tolerate CE devices misbehaving or crashing.
Hence CE SOCs have to be robust, in the sense that a faulty
IP or application, must not cause the entire system to break.
DTV and STB SOC architectures were characterised by
the use of a single external SDRAM for all inter-IP sharedmemory communication [5]. All IPs, most of which were
hardware accelerators, had dedicated wires to the SDRAM,
leading to many global wires and wire congestion at the
SDRAM interface.
IPs required a guaranteed minimum
bandwidth and maximum latency to ensure an end-to-end

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC'10, June 13-18, 2010, Anaheim, California, USA  
Copyright 2010 ACM 978-1-4503-0002-5 /10/06...$10.00 

306
SOC bandwidth and latency for video and audio processing. In contrast, the CPU’s average latency and bandwidth
requirements were strict, but not real time.
With this background, several trends were emerging in
2000, which we took as our problem statement:
1. Physical, back-end problems, in particular diﬃcult timing closure and congestion of many long global wires, introduction of multiple synchronous regions and GALS.
2. Logical, scalability problems, in particular tri-state and
broadcast busses with a single global arbiter, and bottlenecks in the SOC architecture such as a single shared SDRAM
for all inter-IP communication.
3.
Increasing cost of designing ;
integration of working
components often did not lead to a working system, both at
the physical level (e.g. resizing bus tri-state buﬀer strenghts)
and at the logical level (e.g. resizing depths of decoupling
FIFOs between IPs). Moreover, analysing or determining
the real-time performance of SOCs was time-consuming and
complex. Finally, post-silicon SOC validation and debug
were taking an increasing percentage of the design cycle.
Note that low power was absent from the list of requirements, as DTV and STB applications are tethered.
2. CONCEPTS
We discuss Æthereal’s concepts in turn. We use a multihop interconnect to solve back-end and scalability problems.
Next, real-time applications and the cost of designing are
addressed by guaranteed performance with a corresponding
notion of time. The consumer-electronics domain requires
low cost, for which we introduced best-eﬀort traﬃc.
2.1 Guaranteed Performance
To oﬀer real-time performance (guaranteed service, GS),
e.g. a maximum latency or a guaranteed minimum bandwidth over a ﬁnite interval, resource budgets must be reserved and enforced. A connection with its associated minimum bandwidth and maximum latency is the NOC equivalent of a resource budget. Real-time NOCs have essentially
two options: non-blocking routers with rate control [16], and
(virtual)-circuit switching.
Rate Control. Non-blocking routers with a rate-controlled
service discipline [16] are the ﬁrst way to achieve real-time
guarantees. Routers are non-blocking, i.e. when packets
arrive at a router, they can be routed directly to the appropriate output links without switching conﬂicts on the
crossbar. Queueing occurs only at the output ports of the
router. With these assumptions, only links are arbitrated,
and a connection can be modelled as traversing a number
of queueing servers, with each server modeling the output
20.2
link of a router. Many diﬀerent arbitration policies can be
used, with diﬀerent characteristics in terms of latency, buﬀer
sizes, utilisation, fair use of slack, etc. [16]. Note, however,
that priorities alone oﬀer guarantees to only one circuit, and
controlling the rate is essential. Traditionally, N 2 output
buﬀers and a N×N 2 crossbar are used. Alternatively, N
output buﬀers with a N×N crossbar can be used, with both
running at N times the link frequency. But this is not feasible in NOCs, where the router and link speeds are the same.
Mango [2] is a rate-controlled NOC, but uses virtual-circuit
buﬀering rather than the traditional output buﬀering.
Note that rate-controlled service disciplines require that
all routers act as independent servers. A packet cannot,
therefore, block a link (or router crossbar) for other packets.
As a result, either store-and-forward or virtual-cut-through
switching must be used. Or, if virtual channels are used
for guaranteed services, as is often proposed, as many channels as circuits must be used, in addition to a non-blocking
crossbar. This then coincides exactly with Mango’s virtualcircuit buﬀering with a non-blocking crossbar.
Circuit Switching. SOCBus [15] was the ﬁrst pure
circuit-switching NOC. No resources, i.e. wires between and
inside routers and any (pipeline) buﬀers, are shared between
connections. Once a connection has been established, realtime guarantees are trivially achieved. However, pure circuit
switching does not reduce the number of global wires.
Traditionally, frequency multiplexing (FDM) and/or time
multiplexing (TDM) have been used to address this. Note
that even with FDM or TDM data never contend for resources in the network: they never wait and no buﬀers are
required. Optical NOCs [3] use FDM, and Æthereal and
Nostrum [10] use TDM. Since the ﬂight time of photons is
neglible and because they are hard to buﬀer, FDM circuitswitching NOCs are not pipelined.
In TDM NOCs, however, pipeline buﬀers on links or in
routers are essential to ensure high operating speeds. Hence
a connection does not use the same time slot for its entire path; instead it increments at every hop. Figure 1 illustrates how multiple connections are mapped on a TDM
NOC. Since only one datum is guaranteed to arrive in every
time slot at every input, and since it leaves in the next time
slot, routers only require a pipeline buﬀer at each input. The
table in each router indicates to which input each output is
connected, for each slot (allowing multicast.) Since circuit
switching is used and routers know where to switch incoming data to, guaranteed-service (GS) data required no routing headers. Later versions of Æthereal, described below,
employ virtual -circuit switching where packets with routing
headers tell routers where they should go.
Thus links are shared (reducing the number of global wires),
and routers are small (N minimal input buﬀers, minimal
N × N crossbar). Routers are also fast: no arbitration is
required, as contention is absent since all communication is
statically scheduled. But TDM requires that all routers have
a global notion of time, to keep their TDM slots aligned.
2.2 Notion of Time
TDM (and FDM) (virtual)-circuit switching treat the entire NOC as a single shared resource with a single arbiter. In
other words, packets wait only at the ingress network interfaces (NI) until their TDM slot, after which they progress
without contention to the egress NIs, with minimal latency.
This diﬀers from rate-controlled (and other) NOCs, where
a
b
b
R1
i0
o0
o1
i1
i3 o3
o2
i2
R3
o1
c
i0
o2
R2
i0
a
o2
i3
o3
c
o2 o3
i3
i0
i0
T2
o2
i0
i1
T1
o1 o2
i0
i0
T3
Figure 1: Contention-free routing.
each router independently arbitrates incoming packets. Arbiters at each router in a packet’s path are not aligned, and
a packet may incur the worst-case delay at every hop.
Æthereal has a global notion of time because all routers
are always in the same TDM slot. Our main innovation was
to implement the single global arbiter and the global notion
of time in a distributed manner. The NOC is deﬁned as
a dataﬂow graph, where routers are single-rate actors. On
reset, all routers produce a token on all outputs. Following this, in an inﬁnite loop they all read a token on all inputs, increment the slot, and switch each token to an output
according to the switching table. (Note the absence of contention.) Tokens are either empty or full (corresponding to a
circuit datum, or a virtual-circuit packet). This model has
been implemented synchronously, mesochronously [8], and
asynchronously [4]. In fact, since no contention occurs, the
asynchronous router requires a synchroniser to wait for all
incoming tokens, but no asynchronous arbiter for the crossbar. Since all routers only advance to the next slot when
their neighbours have, the NOC is logically synchronous and
runs at the speed of the slowest router.
This model of time is suitable for analytical performance
analysis, as well as allocation/synthesis, i.e. computing paths,
slots, and buﬀer sizes for given communication requirements
(bandwidth and latency, use cases), see Section 3.4
2.3 Low Cost and Best Effort
Minimal area cost was a foremost concern from the outset. Although this was achieved by TDM circuit switching,
we made the classic mistake of confusing utilisation with efﬁciency. TDM is not work conserving, i.e.
slots may be
empty even though data is waiting in the NIs. We therefore
included lower-priority best-eﬀort (BE) traﬃc in the NOC
that used the unallocated and unused slots. The NOC logically consisted of a GS NOC and a BE NOC (input buﬀers,
round-robin arbitration) that only shared the links. However, BE traﬃc increased the GS ﬂit size of one word to the
ﬂit size (and latency) of three words for BE. However, utilisation is not relevant, but the performance:cost ratio is. And
the addition of BE traﬃc signiﬁcantly worsened the latter.
Since RAM or ﬂip-ﬂop based BE buﬀers occupied up to
80% of the router, we designed a dedicated hardware FIFO [14]
that reversed this ratio.
In the NIs it implemented the
(virtual)-circuit buﬀers, and also served as the clock-domain
crossing between IP and NOC clock domains.
2.4 Routing, Flow Control and Deadlock
The NIs were strictly decoupled in a kernel and a shell,
corresponding to the network and transport layers of the
protocol stack. Shells serialise distributed-shared-memory
307
20.2
protocols, such as AXI, to a streaming protocol accepted by
the kernel, which may also be used directly by IPs.
Input buﬀering is used for both GS (one phyt) and BE
(a number of ﬂits) in routers, which is independent from
the number of circuits passing through.
(Virtual)-circuit
buﬀering is used in both ingress and egress NIs, i.e. a request
and response buﬀer per connection.
Absence of contention for GS traﬃc entails that no linklevel ﬂow control is required, but end-to-end (NI-NI) ﬂow
control per connection is essential. We opted for creditbased ﬂow control because it results in smaller buﬀers than
other schemes, although it requires higher bandwidth. Routing deadlock does not occur due to absence of contention.
End-to-end ﬂow control avoids all deadlocks, including message and higher-protocol deadlocks; we therefore have a combined request/response NOC [7]. Stalled IPs cannot, therefore, negatively impact other IPs, increasing robustness. BE
traﬃc uses link-level ﬂow control, and is also connectionbased to avoid all deadlocks. Special BE setup and teardown packets were used to create and remove connections,
concurrently and pipelined from any NI.
In conclusion, the need for real-time performance led to
TDM circuit switching; a focus on low cost led to the inclusion of best-eﬀort traﬃc; and the robustness requirement led
to use of end-to-end ﬂow control. Our main innovation was
the use of a single global TDM arbiter that was implemented
through distributed handshaking between routers.
3. EVOLUTION AND CURRENT STATUS
Given the basic concepts, a decade of continuing research
resulted in a multi-processor NOC-based platform called
CompSOC [6] with an accompanying design ﬂow. We ﬁrst
discuss the architecture changes, then the design ﬂow.
3.1 Use Cases and Composability
Robustness was one of the goals of Æthereal, which translated in GS connections and end-to-end ﬂow control. Quite
soon, however, the concept of use case, a set of concurrently
running applications, became important.
In many SOCs,
multiple applications can run together, and can be switched
on or oﬀ dynamically, often under user control. Use cases
have two important repercussions. First, assuming that all
applications are always active is unduly pessimistic and expensive. Hence the mapping of connections on the NOC
should take applications and use cases into account.
Second, the notion of robustness was reﬁned to composability, i.e. absence of interference between applications.
Within an application, tasks exchange data and are hence
dependendent on each other. Between applications, however, interference should be avoided. Only in this way can
they 1) be developed, veriﬁed, and debugged independently
(e.g. by independent software vendors, ISV), and 2) can they
be integrated in a larger system without any unexpected
side eﬀects to application or system. Our notion of composability separates the resource sharing (scheduling) within
applications, which may be real-time or not, from that between applications, which must be independent. This reﬁnes
Kopetz’s time-triggered approach [9], and enables dynamic
scheduling and the use of slack within applications.
In essence, use cases and composability both require a virtual platform per application. Users of resources, including
tasks on processors, connections in a NOC, and buﬀers in
memories, are given a resource budget. A virtual platform
is the collection of resources and budgets of an application
(a set of tasks, connections, and buﬀers). For a predictable
(real-time) platform, only minimum budgets have to be deﬁned. For a composable platform, the budgets must be
constant, and the times they are handed out to an application are independent of other applications. Intuitively, using
TDM on all resources (independently) is the simplest way to
achieve this: slots allocated to an application are ﬁxed and
independent of others, and unallocated or unused slots go
to waste. Although Æthereal’s GS connections are therefore
composable, memories (especially SDRAM) [1], and processors require more work.
3.2 Best Effort, Low Latency, and Cost
Our approach for GS did not change. However, the inclusion of BE traﬃc was a mistake, and it is no longer (by
default) used in the NOC for three reasons. 1) (our) best
eﬀort does not oﬀer low latency in a loaded NOC; 2) the
performance:cost ratio is much worse than for GS traﬃc ; 3)
use of BE quickly breaks composability between applications.
GS traﬃc such as video streams, tends to have strict minimum bandwidth (and jitter) constraints, but is relatively
latency tolerant. Processor traﬃc, especially cache misses,
on the other hand, is latency critical, with a focus on average rather than minimum latency and bandwidth. Intuitively, low-latency traﬃc should use BE connections. Unfortunately, this is not the case, because BE traﬃc has a lower,
not higher, priority than GS traﬃc. However, perhaps more
important, the performance of BE depends strongly on the
NOC xload and slack. Since in DTV and STB applications
GS reserves and uses up to 60% of the NOC capacity, BE
performs rather poorly and does not oﬀer a low latency.
GS vs. GS+BE Performance:Cost Trade-Off
Consider the raw performance:cost ratio of a 8x8 GS+BE
router with 4-ﬂit BE buﬀers of 0.6 GHz / 0.07 mm2 (65 nm)
versus that of a GS-only router of 1.9 GHz : 0.022 mm2 . The
diﬀerence is a factor of 10, which allows us to replace, at no
additional cost, a BE connection with a given required average bandwidth by a GS connection guaranteeing 10 times
the required bandwidth. Only if the diﬀerence between average and worst-case required bandwidth is more than a factor
of 10, or if statistical multiplexing between diﬀerent connections can be relied upon, then perhaps a case can be made for
BE. In this simple example we have not taken into account
eﬀects such as (absence of ) contention, or the maximum
load of a BE NOC (around 50%) versus that of a GS NOC
(closer to 100%). Figure 2(a) shows a frequency:area:power
trade-oﬀ for 5x5 GS routers and GS+BE routers (with input buﬀers of 4 or 8 ﬂits), both with and without clock gating, for a 65 nm low voltage library. Similarly, GS NI and
GS+BE NI with 16 TDM slots and 8 ports ﬁgures (without
connection buﬀers) are shown.
(We use frequency rather
than raw link bandwidth as performance metric to more easily compare routers and NIs. In Figure 2(b), described in
Section 4.2, we extend the experiment to nett bandwidth.)
Since GS routers contain little buﬀering, clock gating makes
not much of a diﬀerence. GS+BE routers do contain much
buﬀering (additional 5x4x3 or 5x8x3 words for BE), and
they are at least twice as large and power consuming as GS
routers. All buﬀers are based on registers, and use of our
hardware FIFOs will reduce both area and power, especially
for GS+BE components. In all cases the clock uses between
14 and 22 percent of the power.
308
20.2
GS+BE router
(8 flits, gating)
GS+BE router
(8 flits, no gating)
GS+BE router
(4 flits, no gating)
GS+BE router
(4 flits, gating)
GS router
(no gating)
GS router
(gating)
60
40
20
)
W
m
n
i
y
g
r
e
n
e
(
t
s
o
c
0
3000
2000
performance
(freq. in MHz)
1000
6
4
2
cost (area in µm2)
0
0
8
x 104
)
2
m
m
n
i
a
e
r
a
(
t
s
o
c
14
12
10
8
6
4
2
0
0
reference real
STB design
reference real
automotive design
2D mesh (incl. buf.)
2D mesh (excl. buf.)
point−point (incl. buf.)
point−point (excl. buf.)
STB
STB
synthetic
synthetic
synthetic
synthetic
automotive
STB
STB
0.5
1
1.5
2
2.5
performance (effective cumulative bandwidth in b/s) 
x 1011
b/s)
Figure 2: Left: performance:area:energy trade-oﬀ for router and NI. Right: performance:cost trade-oﬀ for GS NOCs.
Best Effort and Composability
Assume two applications, one best-eﬀort by independent
software vendor (ISV) one, and another real-time application by another ISV. BE traﬃc uses the unallocated or unused slots in the NOC. The unused GS slots belong to a different application the one that the BE connection belongs
to. Hence the BE application is inﬂuenced by the absence
or presence of the real-time application. The BE connection (application) receives more or less bandwidth depending on the use of the NOC by the GS connection (application). Note that extra slots can cause an application to
miss its deadline if it is not performance monotonic, through
scheduling anomalies. At a more mundane level, it complicates debugging the BE application, because its behaviour
depends on the other real-time application, the sources or
even executable of which the ISV may not have at its disposal, because it is supplied by another ISV.
Related to the removal of BE traﬃc, we moved to virtualcircuit switching using packets, which allows deletion of the
slot tables from routers, making them much smaller. Arbitrary topologies can be used, without routing restrictions.
To set up and tear down connections, only NIs are now
programmed, using memory-mapped IO on GS connections.
Adding or removing connections is composable, i.e. does not
aﬀect other active connections.
3.3 Protocol Stack, Busses, and Clocking
Although the division of the NI in the transport-level shell
and network-level kernel was good ﬁrst start, we further split
the shell in a local bus, a simpler shell, and a clock-domain
crossing block. The local master bus essentially demultiplexes distributed-shared-memory requests, and enforces the
right order on the responses. The shared-memory requests
and responses are (de)serialised to a streaming protocol by
shells. Slave busses are similar, except that they multiplex
incoming requests, and hence require arbitration. Our design ﬂow, described below, generates the hardware, but also
computes the conﬁguration (i.e. address maps, arbiter settings) for all those components, based on end-to-end (i.e.
master-bus-NOC-bus-slave) requirements.
Regarding clocking, busses and shells operate on the IP
clock, and they are connected by clock-domain-crossing blocks;
routers operate mesochronously on the NOC clock [8].
3.4 Design Flow
The NOC architecture is only half the work: during the
past ten years most of the eﬀort on Æthereal was spent on
the development of a design ﬂow. Based on the speciﬁcation of IP blocks and their (multi-use-case) communication
requirements, it automates the following tasks:
1. Dimensioning and instantiation of hardware, i.e. generating a NOC topology, and then (constrained) optimising
of the NIs, local busses and their decoders and arbiters, and
binding of IPs to these, as well as the sizes of buﬀers in NIs.
2. For all use cases, computing the run-time-programmable
conﬁguration of hardware, i.e. paths, TDM slots, address
maps for master busses, and arbiter settings for slave busses.
3. Generating drivers to (re)program the NOC with a
conﬁguration, at run time, and with real-time constraints.
Applications (i.e.
their connections) can be composably
started and stopped independently at run time.
4. Generation of TLM SystemC models and RTL implementation, including traﬃc generators, testbench, and performance monitors.
5. Automatic inclusion of a test & debug infrastructure,
e.g. distributed monitors, event propagation for cross-triggering,
and transaction-based stepping/stopping control [12].
Our design ﬂow is fully automated for ASIC and FPGA
(synthesis, compilation, loading, etc.). It is unique in allowing constraints on both bandwidth and latency, from which
NI buﬀer sizes and arbiter settings for shared-slave busses
are computed. Performances is guaranteed for connections
(end to end: master-NOC-slave-NOC-master), and even for
entire applications running on CompSOC platform, using
dataﬂow modeling [11, 6].
4. REFLECTIONS AND LESSONS
In this section we reﬂect on our experiences; in particular,
“selling points,” scalability, TDM, and admission control.
4.1 (Unique) Selling Points
309
 
 
 
 
 
 
20.2
Æthereal was conceived for the DTV and STB domain,
where, ironically, it turned out to be the hardest to compete with the incumbent architecture [5]. An interconnect
with a single slave (i.e.
the external SDRAM controller)
for many masters (CPU, VLIWs, and many hardware accelerators) is naturally a circuit-switching tree with a single arbiter at the root. However, in the end, a NOC was
found to be superior for several reasons. First, with the
concept of use cases, applications could be switched on and
oﬀ individually. Previously, each combination of applications would be modelled as a separate application of which
there were hundreds, and which required a global reset to
start or stop. Second, the use of a protocol stack allowed
more eﬃcient (serialised and pipelined) link-level protocols,
which reduced the number of long global wires. This alleviated the SOC back-end problems such as timing closure.
Third, since the circuit switching operated at the granularity of large SDRAM transactions the NIs required much
distributed buﬀering, leading to many SRAM instances. By
using a NOC, the interconnect transport granularity was reduced to ﬂits. Hence buﬀering of SDRAM bursts could be
concentrated at the SDRAM interface, which allowed the
use of fewer, larger (and hence more area eﬃcient) on-chip
SRAMs. Finally, the SOC architecture started to include
multiple slaves, e.g. multiple SDRAM interfaces and chipto-chip links. Rather than duplicate multiple tree-like interconnects, a NOC naturally allowed balancing of traﬃc over
multiple SDRAMs, either within or between use cases.
NXP’s automotive infotainment SOCs had very diﬀerent
architectures, with multiple embedded memories, and multiple data and control busses with bridges. These were a
natural candidate for a replacement by a NOC, although
back-end problems were not as severe as for the DTV and
STB SOCs. Instead composability was the compelling reason for our NOC, since multiple applications from independent (software) vendors were integrated in a single SOC.
Composability aims to ease application integration, and ensure application/SOC robustness and stability, which are
essential in the automotive domain.
In both cases, it is not feasible to introduce composability
or predictability in the entire SOC in one go. However, since
the NOC integrates the IPs, composability can already be
achieved if they are not shared by diﬀerent applications.
This is usually the case for hardware accelerators, and often
feasible for processors (especially DSPs and VLIWs). Only
the oﬀ-chip SDRAM is almost always shared, and must be
made composable and predictable too [1].
4.2 Scalability
NOCs are claimed to be scalable, which here we deﬁne
as: the performance:cost ratio of the NOC is constant, i.e.
cost depends linearly on the requirements on bandwidth and
latency. This encompasses adding slaves, increasing the offered load, and to some extent the diversity in requirements
(ratios of bandwidths) of diﬀerent connections. The performance:cost ratio of a GS NOC versus a GS+BE NOC
was discussed in Section 3.2. Here we concentrate on the
scalability of the GS NOC.
First, observe that our GS routers are independent, in
terms of performance (frequency) and cost (area, notably
buﬀers), of the number of connections passing through them.
This contrasts with rate-controlled routers, where virtualcircuit buﬀering or output buﬀers are used. The size of the
latter depends on the frame size of the guarantees, which is
comparable to the slot table size, and indirectly the number
of connections (more on this below). As mentioned previously, this is not the case for NIs, which use virtual-circuit
buﬀering (also to avoid deadlock, with end-to-end ﬂow control). Hence the total area cost of all NIs depends on the
number of connections. Their performance does not, however, since the connection buﬀers are not in the critical path.
The total area cost of a NOC hence depends on the topology
(i.e. the number of routers, and packetisation and scheduling parts of NIs) plus a (NI buﬀering) part that depends
linearly on the number of connections. (A minor complication is that each NI requires a connection for programming.)
Assuming that NI buﬀers are dimensioned for maximum
performance (cf. end-to-end ﬂow control), and that NOC
clients always accept data as soon as it is oﬀered, the bandwidth of a GS NOC scales linearly with its number of routers,
as there is no contention. However, this supposes that all
TDM slots are used, which depends on the IP port to NI
binding and on the NOC topology. The worst performance
(equal to that of a single link) occurs when all connections
are routed over a single link in the NOC. Conversely, the
best performance (the sum of all links) is obtained when
each connection, consisting of two IP ports, is implemented
with a dedicated source NI and destination NI, connected
without intervening routers. These two extremes also illustrate that the number of TDM slots reﬂect the (design-time)
contention of the given use case and topology, i.e. more slots
are required when more (diverse) connections share a link.
The GS NOC saturates at 100% load, unlike a BE NOC
which eﬀectively saturates much earlier.
Scalability Experiment
The scalability of (any) NOC depends to a large extent on
the scalability of the required traﬃc: if all traﬃc converges
on a single slave, then no NOC will scale. The reference use
case used here is a reasonable intermediate, and contains
70 GS connections, with 32 masters and 32 slaves. 10% of
the connections have a maximum con/divergence degree of
10 (i.e. creating bottlenecks); the rest has a maximum degree of 3. Each connection has a burst size between 8 and 32
bytes, and a random nett required bandwidth between 35 and
205 MB/s. cumulative total of 8046 MB/s is available on the
AXI write data groups. The design ﬂow computes the required raw bandwidth requirements (which is 24 GB/s in total), taking into account AXI byte masks and addresses, and
NOC packetisation and end-to-end ﬂow control credits. To
vary the requirements we uniformly scale the bandwidths of
all connections between 0.2 and 3.8 times the reference, with
a maximum cumulative nett required bandwidth of 30 GB/s.
Given the use case, our design ﬂow ﬁnds the smallest NOC
of a given topology (point-to-point or mesh), and a valid
conﬁguration (paths, slots), and with the required buﬀers.
Real requirements for a set-top box SOC and automotive
infotainment SOC are similarly scaled (0.2-3.4 and 0.2-4.4).
Cost is the area (estimated by the design ﬂow) in 90 nm of a
NOC running at 500 MHz with 32 TDM slots, and includes
local busses, NIs, and routers. Importantly, it includes the
additional capacity (routers, buﬀers, TDM slots) required
due to less-than-perfect mapping, routing, and slot allocation, but also more slots for low latency, etc.
Figure 2(b) shows the cost of a NOC as we increase the
required performance.
It illustrates that sharing wires is
beneﬁcial, as the mesh topology is signiﬁcantly cheaper that
310
20.2
the point-to-point topology. However, they converge under
heavy loads, as links are progressively more shared (making
NI buﬀers larger), and/or requiring more links and routers.
The former eﬀect dominates, as shown by the increasing
percentage of buﬀer area. For both topologies, however, the
performance:cost ratio is remarkably constant, showing that
our GS NOC is indeed scalable. The use of static source
routing or TDM is no impediment to scalability.
4.3 Time-Division Multiplexing
Our choice of TDM has often been contested, for several reasons. First, TDM inversely couples latency and rate,
i.e. a low latency is obtained only at the expense of a high
bandwidth. Hence, in essence, low latency means more (lessshared) wires, rather than fewer wires shared with prioritybased rate-control, such as Mango [2]. Second, TDM is interpreted to imply a synchronous design. As discussed previously, mesochronous and asynchronous implementations are
also possible, with the limitation that slot tokens keep part
of routers or NIs alive, even when no data is sent.
In our opinion these disadvantages are more than oﬀset
by the advantages, namely: for any number of slots TDM is
cheap and fast, i.e. has no arbitration in the routers, and NI
arbitration can be pipelined. It also attains 100% maximum
eﬀective bandwidth, and the restrictions on the distribution
of budgets it can oﬀer are less severe (limited by the number of slots vs. by the number of priorities and precision
of rates). Most important, however, is that TDM is composable, both in steady state, and during (re)conﬁguration:
connections that are not reconﬁgured do not experience (transient) interference. (Rate-controlled) priority-based approaches
fail on all these points. Note that TDM is overly composable,
in the sense that it also disallows the use of slack between
connections of the same application. Ideally a two-level arbitration scheme is used; TDM between applications and
a work-conserving scheduler within an application. However, this requires at least a virtual-circuit buﬀer per application [13] and a more complex arbiter.
4.4 Admission Control
Admission control, or the complexity of checking whether
a connection can be accommodated on a given NOC, varies
for NOCs that oﬀer guaranteed services. For pure circuit
switching it only requires computing a free path between
source and destination. For unbuﬀered frequency-divisionmultiplexing (FDM) circuit switching, admission control requires both (a single) frequency assignment "
The evolution of SOC interconnect and how NOC fits within it.,"In this paper, we describe “The Evolution of SOC Interconnect and How NOC Fits Within It” which is being presented during the “A Decade of NOC Research-Where Do We Stand?” session at DAC 2010. The presentation looks at the features that have helped shape the development of on-chip interconnect solutions. The evolution of NoC is considered alongside the evolution of the AMBA on-chip communication architecture, and consideration is given to the techniques that are considered most important.","20.3
The Evolution of SOC Interconnect and 
How NOC Fits Within It 
Bruce Mathewson 
ARM 
Cambridge, UK 
+44 1223 400400 
Bruce.Mathewson@arm.com 
ABSTRACT 
In this paper, we describe “The Evolution of SOC Interconnect 
and How NOC Fits Within It” which is being presented during the 
“A Decade of NOC Research - Where Do We Stand?” session at 
DAC 2010. 
The presentation looks at the features that have helped shape the 
development of on-chip interconnect solutions. The evolution of 
NoC is considered alongside the evolution of the AMBA on-chip 
communication architecture, and consideration is given to the 
techniques that are considered most important. 
Categories and Subject Descriptors 
B.4.3 [Input/Output and data communications]: Interconnections 
(Subsystems). 
General Terms 
Performance, Design. 
Keywords 
Network on chip, AMBA AXI 
1. INTRODUCTION 
The term Network-on-Chip, or NoC, has become commonplace in 
discussions around on-chip interconnect solutions. However, the 
single term, NoC, actually has many implications for different 
system designers. 
The term NoC can be used to describe an interconnect solution 
that has a number of different key features including narrow 
connection links; Quality of Service capability; a regular matrix 
style structure; asynchronous clock domains; and dynamic routing 
capability. 
2. EVOLUTION OF AMBA 
While NoC development has continued over the past decade, so 
has the evolution of AMBA. 
AMBA has become 
the de-facto standard 
for on-chip 
communication. The specification has been downloaded by over 
30,000 engineers worldwide and is estimated to be implemented 
in billions of SoC devices. 
The first AMBA specifications, ASB and APB, were released in 
1995. Four years later AHB was added to the family of 
specifications, providing a more friendly interface for traditional 
ASIC design styles. 
In 2003, probably the most significant step was made in the 
AMBA roadmap, with the addition of AXI. 
AXI provides a high performance, high speed interface and 
interconnect solution that uses a simple uni-directional channel 
based interface. The nature of the interface allows simple cross 
domain crossing and support for register slices which ensures 
timing closure can be achieved in even the most complex of SoC 
system designs. 
The support for burst based addressing, multiple outstanding 
transactions and out-of-order responses make it particularly 
applicable for connection between on-chip components and a high 
performance off-chip memory controller. 
3. NoC FEATURES 
3.1 Introduction 
The term “Network-on-Chip” or NoC is used frequently, but can 
imply several different sets of features that are relevant to the onchip interconnect environment. 
Each of these features can have differing levels of relevance for 
any given SoC design team, which will depend on the system 
requirements and design goals. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC’10, June 13–18, 2010, Anaheim, California, USA. 
Copyright 2010 ACM 978-1-4503-0002-5/10/06…$10.00. 
3.2 Quality of Service 
A common feature associated with NoC is “Quality of Service” or 
QoS. Again QoS can have different meanings for different design 
teams. 
Some consider QoS to be the ability to build composeable systems 
with guarantees on particular performance aspects, some care 
312
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
20.3
A common set of trade-offs is observed. Increased latency and 
reduced predictability are offset against a better average 
utilization of the wires and reduced overall wire count. 
In any single design, there is unlikely to be a single best choice of 
approach. Some communications paths will benefit from the 
narrow communication links, whilst for other communication 
paths the cost of the increased latency can be too high. 
4. ADDITIONAL CONSIDERATIONS 
Whilst NoC techniques have brought many useful features to the 
world of SoC interconnect there is a key feature that is often 
missing. 
This is the consideration of caches and buffers in the memory 
hierarchy of a system. 
It is no longer appropriate to think only in terms of a single level 1 
cache close to a processor and the next level in the memory 
hierarchy is the bulk off-chip DDR memory. 
Complex SoC designs can include several levels of memory 
hierarchy and information regarding the handling of different 
transactions by each of the cache levels is needed to ensure 
correct operation of the overall system. 
Several 
factors 
influence 
the best choice of caching 
characteristics. The 
location 
in 
the system hierarchy of 
components that are communicating is a primary consideration, 
but also the traffic flows of the components involved will help 
determine the most appropriate style of caching. 
5. CONCLUSION 
NoC research over the past decade has aided the development of a 
number of techniques that are relevant to the future of on-chip 
interconnects. It has also explored some additional avenues that 
may not see large scale uptake in the future. 
Future systems-on-chip are growing ever more complex and are 
likely to see the further adoption of those techniques that have 
proven successful and also the introduction of new features and 
techniques, such as those associated with handling an increasingly 
complex memory hierarchy. 
6. "
Automated modeling and emulation of interconnect designs for many-core chip multiprocessors.,"Simulation of new multi- and many-core systems is becoming an increasingly large bottleneck in the design process. This paper presents the ACME design automation tool flow that facilitates the hardware emulation of newly proposed large multi-core interconnection networks on FPGAs to mitigate the slowdowns of single threaded event driven simulation. The tool is aimed at computer and network architects who have knowledge of digital design but may not be comfortable with hardware description languages and synthesis flows. ACME uses a graphical entry that allows a mix of hardware components with software algorithms written in C, each with a user defined latency and throughput in terms of system cycles. ACME automatically generates a cycle accurate hardware emulator as a Xilinx Platform Studio project, which integrates synthesized hardware blocks with embedded soft-core processors that execute the C code. Our results demonstrate that for 16-core and 64-core cycle accurate packet switching networks, the FPGA-based emulation is faster than Simics-based software simulation by 2.5x and 14.6x, respectively.","27.1
Automated Modeling and Emulation of Interconnect
Designs for Many-Core Chip Multiprocessors∗
Colin J. Ihrig, Rami Melhem, and Alex K. Jones
University of Pittsburgh
cji3@pitt.edu, melhem@cs.pitt.edu, akjones@ece.pitt.edu
ABSTRACT
Simulation of new multi- and many-core systems is becoming an
increasingly large bottleneck in the design process. This paper
presents the ACME design automation tool ﬂow that facilitates the
hardware emulation of newly proposed large multi-core interconnection networks on FPGAs to mitigate the slowdowns of single
threaded event driven simulation. The tool is aimed at computer
and network architects who have knowledge of digital design but
may not be comfortable with hardware description languages and
synthesis ﬂows. ACME uses a graphical entry that allows a mix
of hardware components with software algorithms written in C,
each with a user deﬁned latency and throughput in terms of system
cycles. ACME automatically generates a cycle accurate hardware
emulator as a Xilinx Platform Studio project, which integrates synthesized hardware blocks with embedded soft-core processors that
execute the C code. Our results demonstrate that for 16-core and
64-core cycle accurate packet switching networks, the FPGA-based
emulation is faster than Simics-based software simulation by 2.5x
and 14.6x, respectively.
Categories and Subject Descriptors: J.6 [COMPUTER-AIDED
ENGINEERING]: Computer-aided design (CAD)
General Terms: Design, Experimentation, Veriﬁcation
Keywords: emulation, simulation, multi-core, many-core, interconnection network
1.
INTRODUCTION
Issues such as die yield, high power density, and heat dissipation
are causing a migration from large, high-clock speed uniprocessors
towards systems with multiple smaller, more modest performance
and power efﬁcient processor cores. Unfortunately, current simulators cannot keep up with this change in computer architecture because their performance does not scale well to a larger number of
processors [1]. Current full system simulators such as Virtutech’s
Simics [2] incur a slowdown of 3-4 orders of magnitude [3] where
in some cases, one minute of simulated execution time can take
days of real time. This results in over a month of simulation in order to run the SPEC2k benchmark suite [4, 5]. As more cores are
added to a system, this slowdown increases exponentially [3].
∗
This work is partially supported by NSF award CCF-0702452
Another disadvantage of these software simulation systems is
that they are complicated and difﬁcult to modify for simulation of
new interconnect strategies or cache organizations. In many cases,
commercial simulators, while more robust, provide limited application programming interfaces (APIs), leaving the user to write their
design from scratch. In open source simulators, the source code is
often very complex, cryptic, and potentially ﬁlled with bugs, making it hard to modify. In both cases, the learning curve and development time can be steep.
To replace simulation, we propose a hardware emulation system
to test and evaluate new cache and interconnect design proposals.
An overview of this system is shown in Figure 1. Processor cores
are executed either natively on an actual processor or through traditional software simulation.
In many cases, multiple cores can
be simulated on multi-core processors, natively. Rather than simulating the processors’ on-board cache and interconnect (for multicore processors), memory accesses are captured using interrupts
and sent to a Field Programmable Gate Array (FPGA) for emulation. The FPGA conducts a cycle accurate emulation of the network
and returns the system time when the transaction completes.
Core
Core
Core
Core
Core
Core
Cache & 
network 
emulation
Core
Core
Core
Core
Core
Core
Interconnect
Library
Cache
Library
(
b
u
s
)
S
l
e
e
c
t
o
r
PPC
i386
ARM
Sparc
Core
Figure 1: Emulation system overview.
An FPGA is a natural emulation engine for caches and interconnect structures because they contain many small on-board memories, a wide and ﬂexible interconnect, and a sea of logic for implementing routers and other control logic. Hardware acceleration
using FPGAs has been shown to provide speedups of more than 10x
over software for application speciﬁc tasks [6]. Standard interconnect designs and cache organizations are provided as libraries for
building systems. However, for new interconnect and cache strategies this system requires a tool to translate these concepts onto the
FPGA-based emulation system.
This paper introduces the Architecture Compiler for Model Em431
 
27.1
ulation (ACME). ACME is a tool ﬂow designed for rapid prototyping and emulation of large systems. In particular, ACME takes a
graphical description of a design and automatically generates a reconﬁgurable system-on-a-chip model to emulate the system using
an FPGA. This allows architects to design their interconnects using
similar diagrams to which they are accustomed to presenting their
ideas, and to translate these concepts quickly into a cycle accurate
hardware emulation system.
Ptolemy serves as a graphical entry tool for software simulation
of a design [7]. Figure 2 shows a conceptual model of the emulation environment where each switch is emulated using a combination of FPGA logic and soft core processors such as the MicroBlaze [8], PicoBlaze [8], or Nios [9]. Basic logic such as multiplexers, pipelines, buffers, and simple control are directly instantiated in the FPGA logic, while more complicated logic such as
switch arbitration is described in C and executed on the soft-core
processors. ACME is different from traditional hardware generation which typically optimizes designs for speed, area, power, etc.
ACME models the latency and throughput of the emulated hardware in terms of system cycles as speciﬁed by the user for each
component. For simulations requiring detailed/cycle accurate results from the network-on-chip, the network component dominates
the time of the system simulation. Thus, this paper studies the results of emulating only the interconnect component in hardware
while simulating the remainder of the system in software.
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Interface to 
CPU Core
Switch 
Logic
Softcore 
CPU
Figure 2: FPGA layout for emulation.
The remainder of this paper is organized as follows: Section 2
presents previous work related to the simulation of multi-core systems and networks, as well as design automation targeting FPGAs.
In Section 3, the ACME automation ﬂow is described in detail.
An example case study of a 4x4 mesh interconnection network is
discussed in Section 4 including results scaled to an 8x8 mesh. Finally, conclusions and future directions of this work are presented
in Section 5.
2. RELATED WORK
A great deal of research effort has been placed into addressing
the issue of simulating large many-core systems. Cho et al. introduced a many-core processor simulation system called Two-Phase
Trace-driven Simulation (TPTS) [10]. TPTS breaks simulations up
into two separate phases — a trace generation phase and a trace
simulation phase. The idea behind TPTS is to only incur the overhead of many events once during the trace generation phase.
The SuperESCalar Simulator (SESC) is a multi-core MIPS-based
simulator that attempts to provide fast simulations for large numbers of processors [11]. SESC decouples the functional simulation
from the timing simulation by executing the code for each core using the MIPS INTerpreter (MINT) [12]. The timing simulation is
conducted from the functional simulation output to model everything from out of order execution to cache coherence trafﬁc. SESC
is fast but not cycle accurate, and makes many questionable assumptions (such as unlimited buffers for its mesh interconnect).
When correcting some of these assumptions, the simulations slow
down signiﬁcantly.
In [5] and [4] the SMARTS framework attempts to reduce simulation times by only sampling various simulation points of execution. Once a simulation point is reached, a detailed simulation
is performed for a speciﬁed time window. After the window has
elapsed, the simulator will proceed to the next simulation point.
In [13] this technique is improved upon by analyzing the program
ﬁrst in order to determine simulation points which are most representative of program behavior.
The Research Accelerator for Multiple Processors (RAMP) project is a multi-university project aimed at creating new computer architecture research tools by using FPGAs to emulate parallel computer systems [14]. The RAMP Blue is a prototype system for
multi-core emulation using FPGAs [15]. The system is comprised
of 768-1,008 MicroBlaze soft core processors running in 64-84 Xilinx Virtex-II Pro 70 FPGAs on 16-21 BEE2 boards. It also consists
of a software architecture which utilizes gcc, uClinux, and Uniﬁed
Parallel C (UPC) [16].
In contrast to these projects, the ACME tool provides a graphical design interface for multi-core network design. Unlike software
simulators such as SESC, Simics, and SMARTS, the ACME concept accelerates cycle accurate simulation through hardware emulation. Unlike RAMP, ACME attempts to retain a hardware target independent, cycle accurate simulation through special hardware structures that guarantee a particular latency and throughput
in the system. Finally, ACME provides a high-level design environment to build its emulation, which particularly compared to
RAMP avoids the need to (1) develop using cumbersome hardware
languages and (2) manually integrate hardware and software components. ACME also does not attempt to emulate the entire system
including the processor cores as in RAMP. However, ACME could
be conﬁgured to target RAMP hardware.
3. ACME TOOL FLOW
This section describes the ACME tool ﬂow from graphical entry
in Ptolemy to full FPGA design generation. The system works in
a synchronous fashion, where each element of the design proceeds
according to an emulated target clock. The target clock is different from the host clock of the FPGA. A hardware barrier ensures
that all of the components have completed the appropriate amount
of work as dictated by the target clock, at which point it allows
execution to continue to the next target clock cycle. This method
allows soft-core processors and hardware components, each with
potentially different clock domains to operate in parallel but also
remain in sync and generate cycle-accurate results. This is further
discussed in Section 3.1.
A high level view of this ﬂow from Ptolemy to FPGA synthesis
is shown in Figure 3 and will be referenced throughout this section.
In the chart, lightly shaded regions are provided by various third
parties, while the darker segments of the ﬂow were developed by
the authors.
The graphic design and simulation environment shown in the
ﬂow as Ptolemy II is based on the Ptolemy II design environment [17].
432
27.1
ACME
Actor
Generator
Java
Actor
Library
VHDL
Actor
Library
Xilinx
Component
Library
Ptolemy 
II
ACME
Front End
ACME
Xilinx
Back End
Xilinx
Platform
Studio
FPGA
XML
SOC
Model
XPS
Project
Bit
Stream
Figure 3: ACME tool ﬂow.
Ptolemy focuses on modeling and simulation of concurrent, realtime, embedded systems [18]. ACME utilizes the discrete-event
mode of Ptolemy, as discrete-event simulation is widely used for
modeling digital circuits [7].
Ptolemy models are comprised of two types of components. Atomic
components, or actors, represent the lowest level designs in the system, and are implemented in Java. Basic hardware components for
building network switches such as multiplexers and buffers are provided in the Java Actor Library. These actor libraries are then used
to create hierarchal, or composite, components which can be stored
in an XML ﬁle.
The ACME tool starts with a library of VHDL implementations
of the relevant Ptolemy actors shown in VHDL Actor Library. Based
on this library, the ACME Front End can read the composite model
XML ﬁle and generate a synthesizable VHDL description of the
model which can be run on the FPGA. In the event that a desired
actor is not available in the library, the user graphically designs a
black box of the new actor by adding ports and port types. The
ACME Actor Generator then generates a Java and VHDL actor
skeleton from the black box for which the user describes the necessary Java and VHDL behavior.
In some cases it may not be practical to design a hardware implementation of a component of the network design. For example, it
might be easier to design a switch arbiter in C rather than constructing one from hardware blocks. Figure 4 shows an extension to the
actor generation from Figure 3. The VHDL Based Actor Generator follows the original ﬂow. But to allow C code to be integrated
into the system we add the Processor Based Actor Generator. This
allows the user to generate a “processor actor” and write their behavioral code in C, while the tool automatically generates wrapper
code using the Java Native Interface (JNI) to integrate it into a Java
actor skeleton. The JNI is a framework which allows Java code to
inter-operate with native (platform speciﬁc) code written in another
language such as C/C++ or assembly language [19]. The same C
code can be executed on a soft-core processor in the FPGA design.
Java
Actor
Java
Actor
Library
Library
XML
Graphical
Actor
Entry
Processor
Based
Actor
Generator
C 
Compiler
Java     
Native 
Interface
JNI
C Files
Java
Actor
VHDL
Based
Actor
Generator
VHDL
Actor
VHDL
Actor
Library
Figure 4: Expanded actor generation ﬂow.
When the design contains processor actors, the ACME Front End
generates both VHDL and an SoC model for the system, which
includes the connections between the hardware and soft-core processors included in the design. The ACME Xilinx Back End, takes
the SoC model and VHDL from the front end and generates a Xilinx Platform Studio Project consisting of Microblaze processors
and the appropriate code to interface with the VHDL. The project can then be synthesized and run on a Xilinx FPGA. In the
Xilinx back end, the processors and hardware communicate using
software accessible registers (SARs). SARs are registers used by
the custom logic which can also be accessed by a processor via
memory-mapped I/O.
ACME currently supports VHDL and Xilinx targets. However,
other target languages such as Verilog [20] or SystemC [21] and
embedded design environments like Altera SOPC Builder and Actel SmartDesign tools are planned for support.
3.1 Emulation Augmentation
As previously explained, ACME has the capability of generating
standalone hardware as well as SoC implementations for large systems, provided that all of the low level actors are deﬁned in the code
generation library. However, ACME was also designed with the intention of modeling timing constructs such as latency and throughput in hardware.
In the emulation environment (the FPGA), it is necessary to deﬁne two basic units of time. The ﬁrst unit, the host clock, is the
FPGA’s clock frequency for the design. The second time unit, δ ,
is the emulation clock frequency. In the ACME system, δ is used
to denote a target cycle. This allows for the synthesis of timing
constructs, as well as more effective decoupling of the design from
the host FPGA. The additional hardware generated for timing constructs is described in Section 3.1.1.
The timing model becomes more complex in SoC designs, as
they may include an arbitrary number of processors. The addition
of these processors effectively creates a synchronization problem,
an issue present in most parallel computing models. A common
way for achieving synchronization is the use of barriers. Barriers
serve as points in execution where no single processor can proceed
until all other processors are ready. ACME’s hardware based barrier system is discussed in Section 3.1.2.
3.1.1 Modeling Latency and Throughput
ACME provides the ability to specify latency and throughput parameters independent of a component’s functionality. This allows
multiple copies of a component to be tested simultaneously with
different latency and throughput parameters. This is also beneﬁcial
because it requires no effort by the system designer or library writer
to generate performance speciﬁc hardware directly.
In ACME’s latency model, α, deﬁnes how many δ (target cycles)
are required for the input of a component to propagate through to
the output. By default α = 0 for all components.
The throughput metric, β , speciﬁes how many δ must elapse between changes at the inputs of a component. In the ACME model,
if a new input value arrives in fewer than β cycles, then the old inbe reset to zero. β has a default value of zero, and β ≤ α.
put value will be overwritten and the number of elapsed cycles will
In ACME generated VHDL, latency and throughput are modeled
using variations of the circuit shown in Figure 5. The circuit in
Figure 5 has β = 3 and α = 4. Latency is easily implemented as
shift registers of length α− β . In Figure 5 the latency is represented
by register C. This is equivalent to a shift register of length one.
The throughput is represented by the entire circuit, with the exception of register C, which is used to model latency. If β < 2 then
this additional logic is removed, leaving only the latency register(s).
433
27.1
==
D
Clock
“10”
D
Q
>> 1
Shift
Reg.
D
Q
Enable
Reg. A
D
Q
Enable
Reg. B
Q
D
Q
Reg. C
Figure 5: Latency and throughput circuit.
If β ≥ 2 then the latency is tracked by a bit string of length β − 1.
This bit string is right shifted by one place each cycle. When the
least signiﬁcant bit becomes a ‘1’ then β cycles have elapsed and
the input value is guaranteed to appear at the output. In Figure 5,
the constant value of “10” is used to reset the β bit string.
A comparator tests the equivalence of the input and output of register A. The arrival of a new input value is indicated by a nonequivalence, which signals the bit string to reset. This causes the old
input to be erased and the new value to be stored. However, if the
old and new values are equivalent then the bit string is right shifted
by one position. Eventually, the least signiﬁcant bit of the bit string
will become a ‘1’ and the new value is passed to register B.
3.1.2 Hardware Barrier Synchronization
In designs comprised solely of logic, it is perfectly acceptable
to use the host clock to serve as the target clock, δ . However, the
inclusion of processors, and more speciﬁcally software programs,
which can take many clock cycles to execute, necessitates a method
of decoupling δ from the FPGA clock.
In a typical SoC, processors communicate with custom logic via
SARs.
In an example system, processors will write data to the
SARs and signal the logic to perform some set of computations.
The logic will then write the results back to the SARs and notify the processors of completion, at which point the processors
can continue execution with the new data. Synchronizing the processors with the hardware is difﬁcult due to factors such as nondeterminism in software and the use of potentially different clock
frequencies. Barriers are a common synchronization construct used
in many parallel programming paradigms. Barrier synchronization
is also fairly simple to implement in hardware, making it a natural
ﬁt for a FPGA based system.
In [22], a simple hardware barrier circuit is described. A hardware barrier for P processors is composed of P single bit registers
connected to zero detect logic. Once the barrier point is reached,
each processor sets one of the P registers to zero. The zero detect
logic will assert a barrier signal once all of the processors set their
corresponding bits to zero. Once the barrier is cleared, all of the
processors can reset their bits and proceed with execution.
The barrier circuit used in ACME designs extends the circuit
of [22]. In the original design, it was sufﬁcient for each processor to know only that the barrier had been reached. However, in
the emulation system the processors must reach the barrier point,
set the barrier, notify the custom logic to execute for exactly one δ ,
and then reset the barrier.
The adapted circuit shown in Figure 6 utilizes both zero detect
logic and one detect logic, while using the barrier output as the
clock signal for the custom logic. Once the barrier is set by the processors, the zero detect logic asserts the barrier output, which acts
as the rising clock edge to the custom hardware. Simultaneously,
the processors are notiﬁed that the barrier has been set and can begin resetting their signals. Once the barrier has been reset, the one
detect logic will unassert the barrier signal, creating a falling edge
on the clock signal to the custom logic. The barrier ensures not
only that all of the processors stay in sync, but also that the hardware executes for exactly one δ , where δ is dictated by the software execution and is independent of the FPGA clock frequency.
Similar to the hardware blocks, processors can mimic the latency
and throughput by adding a circuit like Figure 5 to the output connections of the processor that connect to the custom hardware (not
shown in Figure 6).
μP0
μP1
...
μPn
Logic
R0
R1
...
Rn
One
Detect
Zero
Detect
Controller
Figure 6: Hardware barrier logic.
4. CASE STUDY
In this section the power of the ACME tool is demonstrated by
implementing a 4x4 packet switching mesh network using round
robin arbitration, and comparing the performance of the FPGA emulator to a commercial, cycle-accurate software simulator. The top
level design ﬁgure is too dense to be included in the paper, however, Figure 7 shows the schematic of a single switch point in the
mesh. On the top left, there are ﬁve buffers corresponding to each
cardinal direction and a local core connection. These buffers are
implemented as atomic actors in Ptolemy, and are realized in the
FPGA using custom logic. Just below the buffers is a processorbased round robin controller. The controller receives packets from
the buffers and conﬁgures the crossbar appropriately. The crossbar is the actor on the right and is a composite actor containing a
network of multiplexers.
The crossbar composite actor is expanded in Figure 8. The crossbar contains ﬁve data input ports, ﬁve control signal input ports,
ﬁve data output ports, and ﬁve 4:1 multiplexers. The data input
and output ports represent the North, South, East, West, and local
ports of an interconnect switch. The local connection is routed to
the external processor for the local core via the core interface hardware. Each input port is routable, via the multiplexers, to any other
output port. During ACME synthesis, the circuit from Figure 5 is
appended to the output of the VHDL representation of each actor to
provide the appropriate latency and throughput values as speciﬁed
in the Ptolemy design.
The control signals to the multiplexers are controlled by an actor
that implements a simple round robin arbitration protocol in the
following manner: In the example crossbar it is possible for every
input port’s data to be passed through to an output port. However,
if two or more input ports wish to send to the same output port then
contention occurs. The round robin policy gives each input port a
priority for sending data. In the event of contention, the input port
with the higher priority is allowed to send its data while the other
port(s) must wait until the next cycle. After each cycle, each input
port receives a higher priority and the input port that previously had
the highest priority is demoted to the lowest priority.
434
27.1
Figure 8: Example crossbar interconnect.
Figure 7: Switch point composite design.
Ptolemy actors contain the following three most important methods, init(), prefire(), and fire(). init() initializes
the actor to a particular start state, prefire() looks at messages
received by the actor and decides if the actor needs to take any action (e.g. call fire()), and fire() executes the behavior of the
system. As processor actors require the behavior to be written in
C, the JNI is used to create C versions of the functions init(),
prefire(), and fire().
Figure 9 shows the jniinit() method which initializes the
round robin policy to assign the highest priority to the North input
port. The South, East, West, and local ports are each assigned a
progressively lower priority. In this example, the jniprefire()
method does not perform any special checks and always returns
true. The jnifire() method, shown in Figure 10, determines
the ports that will be allowed to send data based on priority and
any conﬂicts that exist. First, the highest priority queue is found by
checking the priority array in the for loop. Each queue is accessed
in priority order, and the destination of each queue’s message is
checked using the get_buff_dest(queue) method.
If that
destination has not been used by another message (checked in array
q), then a connection from that input port to the requested output
port is scheduled using the schedule(src,dest) method. As
the queues are checked, the priority is updated by increasing the
priority by one with a modulo ﬁve to wrap around back to zero
when necessary.
The full system was implemented using the ACME design ﬂow
and in Simics [2] using a cycle accurate network simulator. Only
the actual interconnection network was emulated in hardware. The
remainder of the system (cores and caches) was simulated in Simics.
The systems contained 16 and 64 cores operating at a frequency of
2 GHz, and a network clocked at 1 GHz. The cores have a private
L1 cache and a non-uniform cache access distributed shared L2
cache. The simulation assumes a switch latency of two cycles with
a throughput of one message per cycle. The arbitration using round
robin scheduling was set to one cycle. To test the system, the Raytrace benchmark from the SPLASH-2 multi-threaded benchmark
suite [23] was run.
To allow for reasonable simulation testing times, between 2,500
JNIEXPORT void JNICALL Java_..._jniinit
(JNIEnv *env, jclass clazz )
// assuming NORTH=0,SOUTH=1, to LOCAL=4
priority[NORTH] = 4;
priority[SOUTH] = 3;
priority[EAST]
= 2;
priority[WEST]
= 1;
priority[LOCAL] = 0; }
Figure 9: Example crossbar init method.
JNIEXPORT void JNICALL Java_..._jnifire
(JNIEnv *env, jclass clazz )
{
{
bool q[5] = {false,false,false,false,false};
int start,i,dest;
for(i=0; i<5; i++)
if (priority[i]=4){
start = i;
break; }
do {
priority[i]=(priority[i]+1)%5;
dest=get_buff_dest(queue[i]);
if(!q[dest]) {
schedule(i, dest);
q[dest] = true;
}
i=i+1%5;
} while(i!=start); }
Figure 10: Example crossbar ﬁre method for round robin.
and 5,000 target cycles from the parallel section of the application
were run. Messages were provided to the ACME design through
traces generated by Simics. For the 16-core (4x4) mesh, a 1,000
message segment of the Raytrace benchmark was run, resulting in
approximately 2,600 target cycles of executing. For the 64-core
(8x8) mesh, a 5,000 message segment was run, requiring approximately 4,000 target cycles. The execution times of the network
simulator are shown in Table 1. Ptolemy refers to running the design within the Ptolemy II framework. Simics refers to the time of
executing the network simulator connected to a Simics simulation.
Finally, FPGA is the interconnect emulation time. The Ptolemy and
Simics simulations were run on a Intel Xeon 8 core machine (dualquad core “Clovertown” processors) operating at 2.3 GHz with 8
GB of RAM. The emulation was executed on a Xilinx Virtex 5
110T operating at 100 MHz. Speedups of emulation over the other
two implementations are shown in Table 2.
435
27.1
Table 1: Runtimes for Raytrace on Ptolemy, Simics, and the
FPGA.
Ptolemy II
Simics
FPGA
4x4 mesh
8x8 mesh
2.6s
0.04s
0.018s
17.7s
0.40s
0.027s
Table 2: Speedup of FPGA over Ptolemy and Simics executing
a segment of Raytrace.
Ptolemy II
Simics
4x4 mesh
8x8 mesh
146x
2.25x
649x
14.7x
The speedups of the FPGA are only about 2x over Simics for
16-core experiments. However, this speedup increases to nearly
15x for 64-cores. The improvement is partially due to the FPGA
being able to emulate each switch entirely in parallel compared to
sequential execution in Simics. With 4x the number of components, the FPGA should have only been about 9x faster than Simics.
However, the slowdown is typically super-linear as more messages
are handled in the system per cycle. While the parallel components of the FPGA do not slow much, an event driven system like
Simics slows considerably, creating limits in the number of cores
that can be considered. The biggest limitation for the FPGA is area
consumption, but projects like RAMP have shown that this can be
overcome successfully by combining multiple FPGAs.
5. CONCLUSIONS AND FUTURE WORK
This paper has presented a tool ﬂow for the creation and emulation of interconnection networks for multi-core systems. The
Ptolemy modeling infrastructure is used to design these systems.
From there, the ACME tool ﬂow is able to generate a description
of the system, which is then synthesized using commercial FPGA
tools. Additionally, the components used in the system can be
parameterized in terms of latency and throughput. Sophisticated
component interfaces are easily created graphically, while their actual implementations are written in C code and integrated into the
Ptolemy simulation engine via the JNI.
Benchmarking tests demonstrate that the ACME-based approach
can generate a viable network-on-chip emulator with a substantial
speedup (more than 10x) over a software system, particularly as the
number of processor cores scales beyond 16. Future directions of
this work include the creation of large ACME component libraries,
integration of shared cache hierarchies into the FPGA emulation,
targeting of multi-FPGA systems, and automated integration with
software-based simulators such as Simics. Further studies are also
needed to determine the ideal partitioning of the systems between
software simulation and hardware emulation in order to minimize
the potential bottleneck of host to FPGA communication.
6. "
Trace-driven optimization of networks-on-chip configurations.,"Networks-on-chip (NoCs) are becoming increasingly important in general-purpose and application-specific multi-core designs. Although uniform router configurations are appropriate for general-purpose NoCs, router configurations for application-specific NoCs can be non-uniformly optimized to application-specific traffic characteristics. In this paper, we specifically consider the problem of virtual channel (VC) allocation in application-specific NoCs. Prior solutions to this problem have been average-rate driven. However, average-rate models are poor representations of real application traffic, and can lead to designs that are poorly matched to the application. We propose an alternate trace-driven paradigm in which configuration of NoCs is driven by application traces. We propose two simple greedy trace-driven VC allocation schemes. Compared to uniform allocation, we observe up to 51% reduction in the number of VCs under a given average packet latency constraint, or up to 74% reduction in average packet latency with same number of VCs. Our results suggest that average-rate driven methods cannot effectively select appropriate links for VC allocation because they fail to consider the impact of traffic bursts. As a case study, we compare our proposed approach with an existing average-rate driven method and observe up to 35% reduction in the number of VCs for a given target latency.","27.2
Trace-Driven Optimization of Networks-on-Chip
Conﬁgurations
Andrew B. Kahng, Bill Lin, Kambiz Samadi and Rohit Sunkam Ramanujam
ECE Depar tment, University of California San Diego, La Jolla, CA
Email: {abk,billlin,ksamadi,rsunkamr}@ucsd.edu
ABSTRACT
Networks-on-chip (NoCs) are becoming increasingly important in
general-purpose and application-speciﬁc multi-core designs. Although uniform router conﬁgurations are appropriate for generalpurpose NoCs, router conﬁgurations for application-speciﬁc NoCs
can be non-uniformly optimized to application-speciﬁc trafﬁc characteristics. In this paper, we speciﬁcally consider the problem of
virtual channel (VC) allocation in application-speciﬁc NoCs. Prior
solutions to this problem have been average-rate driven. However,
average-rate models are poor representations of real application
trafﬁc, and can lead to designs that are poorly matched to the application. We propose an alternate trace-driven paradigm in which
conﬁguration of NoCs is driven by application traces. We propose
two simple greedy trace-driven VC allocation schemes. Compared
to uniform allocation, we observe up to 51% reduction in the number of VCs under a given average packet latency constraint, or up to
74% reduction in average packet latency with same number of VCs.
Our results suggest that average-rate driven methods cannot effectively select appropriate links for VC allocation because they fail to
consider the impact of trafﬁc bursts. As a case study, we compare
our proposed approach with an existing average-rate driven method
[9] and observe up to 35% reduction in the number of VCs for a
given target latency.
Categories and Subject Descriptors
C.2 [COMPUTER-COMMUNICATION NETWORKS]: Network Architecture and Design
General Terms
Algorithms, Design, and Performance
Keywords
Networks-on-Chip, virtual channel, and greedy heuristics
1.
INTRODUCTION
Diminishing returns in the performance of uniprocessor architectures have led to multi-core chips. In these designs, networks-onchip (NoCs) are emerging as the interconnection fabric of choice.
With the increasing number of on-chip cores, the need for a scalable
and high-bandwidth communication fabric becomes more evident
[5, 17]. NoCs can be designed for general-purpose multi-core processors [13, 25] or application-speciﬁc multi-core systems-on-chip
(SoCs) [21, 29]. The challenges are different in each case. Since
general-purpose multi-core processors are designed to run a wide
range of applications, the application trafﬁc characteristics are inherently unknown a priori. Hence, the conﬁgurations of on-chip
routers, e.g., with respect to the number of virtual channels (VCs),
are typically uniform across all routers in the design. On the other
hand, since application-speciﬁc multi-core SoCs are designed to
implement speciﬁc functions efﬁciently, the conﬁguration of each
router in the network can be non-uniformly optimized to the trafﬁc
characteristics of the particular application.
With power being a ﬁrst-order design constraint, network resources must be carefully allocated to minimize overall power
with no or minimum loss in performance. Among all router resources, input buffers consume a signiﬁcant (e.g., 28% [7]) portion
of the total communication power. Hence, minimizing the number of buffers is important for reducing router power consumption.
Buffers are used in routers to house incoming ﬂits1 that cannot be
immediately forwarded to the output links because of contention.
Earlier router architectures used packet-level ﬂow control [6, 8]
whereby buffer and channel resources are allocated at the level
of packets. Packet-level ﬂow control requires large input buffers
to store entire packets, and also increases contention latency as
a packet must wait for the previous packet to be transmitted before it can acquire a channel. This problem was alleviated with
the introduction of wormhole routers [4] which operate at ﬂit-level
granularity. However, wormhole routers without virtual channels
are prone to head-of-line blocking, which signiﬁcantly limits router
performance. Virtual channels provide alternate paths for incoming
packets to bypass a blocked packet, and hence can improve router
performance. However, performance is improved at the expense of
extra power consumption and area overhead.
Though the problem of NoC conﬁguration for applicationspeciﬁc multi-core SoCs is not new, prior approaches [3, 8, 9] have
been average-rate driven in that the trafﬁc characteristics have been
modeled with average data rates. Unfortunately, average-rate models are poor representations of actual trafﬁc characteristics of real
applications. For example, consider the actual trafﬁc behavior of
two real applications from the PARSEC [2] benchmark suite shown
in Figure 1. As shown in these two applications, the actual trafﬁc behavior tends to be very bursty with substantial ﬂuctuations
over time. Therefore, average-rate driven approaches may be misled by average trafﬁc characteristics, which may lead to poor design
choices that are not well matched to the actual trafﬁc characteristics.
In this paper, we propose an alternate paradigm based on a tracedriven approach that uses actual application trafﬁc traces to drive
the optimization of NoC conﬁgurations.To illustrate the beneﬁt of
a trace-driven approach, in this paper we speciﬁcally consider the
problem of application-speciﬁc VC allocation.
In particular, we
propose two greedy trace-driven heuristics in which we use PopNet [23], a cycle-accurate ﬂit-level on-chip network simulator, to
constantly evaluate the impact of adding or deleting a VC on average packet latency of a given application. To decouple the VC
allocation and the buffer space allocation problems we assume that
1A ﬂit is a ﬁxed-sized portion of a packetized message.
437
27.2
multiplexing the physical channel, and thus mitigate the head-ofline blocking problem in wormhole routers. Huang et al. [9] propose a queueing-based algorithm for VC allocation. Given a network topology, a mapping of tasks to the NoC, and a deterministic
routing algorithm, they determine the number of VCs allocated to
each link. The proposed algorithm proceeds in a greedy fashion: it
increases the number of VCs for a given link only if this reduces the
average packet latency. Similarly, Al Faruque et al. [1] propose a
two-step approach to minimize the number of virtual channels under certain quality of service criteria. In their approach, they ﬁrst
minimize the number of VCs during mapping of communication
tasks; then, based on the assumption that packet arrivals follow a
Poisson distribution, they estimate the size of each VC for each
output port. A signiﬁcant shortcoming of this approach is that it relies on Markovian assumptions for multimedia applications which
are contradicted by, e.g., the experimental results in [28].
All of the above approaches are static: buffer sizes (including
number of VCs) are determined at design time based on analysis
of different trafﬁc patterns. In contrast, buffer space can also be
allocated dynamically, i.e., at runtime [6, 18, 24]. Dynamically allocated multi-queue [24] uses linked lists to allocate VCs to each
port. However, a three-cycle delay at every ﬂit arrival/departure,
to update the logical pointer to the free list, makes this approach
unsuitable for high-performance designs. Fully-connected circular buffers [18] use registers to selectively shift ﬂits within buffers.
However, this method requires a P 2×P crossbar instead of the conventional P × P crossbar, where P is the number of ports. It also
requires existing ﬂits to be shifted when new ﬂits arrive. Hence,
this approach has signiﬁcant latency and power overheads. Finally,
Nicopolous et al. [19] propose ViChaR, a dynamic VC allocator, in
which the number of VCs and the buffer depth per VC are dynamically adjusted based on the trafﬁc load.
In summary, VC allocation is an important issue in optimization
of NoCs conﬁgurations as it directly affects the overall performance
of the network and consumes a major portion of router power and
area. There are two existing VC allocation approaches: (1) dynamic
and (2) static. Dynamic allocation of VCs at runtime seems more
desirable for general-purpose and reconﬁgurable design platforms
executing different workloads. However, allocating VCs at design
time is more desirable for application-speciﬁc NoCs running a speciﬁc application or a limited class of applications. Previous VC
allocation approaches are all average-rate driven whereas in this paper, we propose two simple yet effective trace-driven VC allocation
heuristics that signiﬁcantly reduce the number of VCs while matching the latency attributes of a uniform VC allocation scheme. As far
as the authors know, there is no existing trace-driven VC allocation
approach; however, we note that similar trace-driven approaches
have been used to optimize traditional bus-based communication
architectures [20, 22].
3. PROBLEM FORMULATION
In a typical design of virtual channel routers, a ﬁxed amount
of hardware resources (i.e., queues) is set aside to implement the
VC buffers. With power being a ﬁrst-order design constraint,
these resources must be allocated efﬁciently such that certain
performance criteria are satisﬁed while keeping the number of
VCs to a minimum. When the application is known, as discussed
above, the concept of a trace-speciﬁc resource allocation becomes
relevant. We formulate the trace-driven VC allocation problem as
follows.
Given:• Application communication trace, Ctrace
• Network topology, T (P , L), where P is the set of processing
• Deterministic routing algorithm, R
elements and L is the set of physical links
• Target latency, Dtarget
Determine:
• A mapping nV C from the set of links L to a set of positive
integers, i.e., nV C : L → Z + , where for any l ∈ L, nV C (l)
gives the number of VCs associated with link l, such that
l∈L nV C (l)) is minimized while average packet latency
P
(

































	



     

	
Figure 1: Communication trafﬁc between two arbitrary nodes
for two different applications.
each VC has a ﬁxed buffer length which is larger than the average
packet size of the application. Our approach is simple, but is shown
to be quite powerful in achieving more efﬁcient VC resource allocation. Although “trace-driven” suggests high runtime complexity
(cf. Subsection 3.3), we believe that in practice ample computing
resources are available to enhance any given application-speciﬁc
NoC conﬁguration using our techniques.
The contributions of our work are listed below.
• We propose two greedy VC allocation algorithms based on
our proposed trace-driven optimization paradigm.
• We evaluate our proposed algorithms on a set of real applications. In particular, we evaluate our methods on the recently
developed PARSEC benchmark suite [2], which contains
multi-threaded programs that are representative of emerging
• In comparison with uniform conﬁgurations, we observe up
workloads.
to 51% reduction in the number of VCs needed to achieve
the same performance, and up to 74% reduction in average
• We also compare our proposed approach with an existing
packet latency with the same number of VCs.
average-rate driven VC allocation method [9]. In this comparison, we achieve up to 35% reduction in the number of
VCs, which demonstrates the beneﬁts of a trace-driven approach over average-rate driven approaches.
The remainder of this paper is organized as follows. In Section
2 we contrast against prior related work. Section 3 describes our
trace-driven non-uniform VC allocation problem formulation. In
Section 4 we describe our experimental setup and testcases, and
present our results. Finally, Section 5 concludes the paper.
2. RELATED WORK
Input buffers affect the overall performance of interconnection
networks and consume a major portion of router power and area [7,
10, 12, 15]. Hence, determining the optimal buffer size that maximizes performance with little or no impact on power is of utmost
importance. Several buffer sizing methods have been proposed in
the literature. Hu et al. [8] propose a probabilistic approach to size
the input buffers of the intermediate nodes of the network. The main
objective is to minimize the average packet latency of all communications occurring in the network while keeping total buffer area
under a certain budget. In their approach, the authors of [8] use
packets as atomic units of storage (i.e., store-and-forward); however, packet-level ﬂow control mechanisms are not efﬁcient due to
their negative impact on latency and area.
Chandra et al. [3] propose a sizing algorithm based on production and consumption rates of packets. A major disadvantage of this
work is the use of synthetic trafﬁc models which can potentially impact the relevance of solutions. In a different approach, Manolache
et al. [14] propose a trafﬁc shaping methodology in which packets are delayed at the source to avoid contention between different
ﬂows, and hence reduce the total amount of buffer space required
at intermediate nodes.
In recent works, virtual channel (VC) routers have gained prominence. Virtual channels allow packets to pass blocked packets by
438



	

27.2
F1 and F2 share links A → B and B → C , both of which initially
approach, consider the example of Figure 3, where two trafﬁc ﬂows
have only one VC (A, B , and C are network nodes). F1 turns west
and F2 turns east at node C . In this case, adding a VC to either link
A → B or link B → C may not have a signiﬁcant impact on average packet latency of ﬂows F1 and F2 because the other link with
added to both links A → B and B → C , the average packet lajust one VC becomes the bottleneck. On the other hand, if VCs are
tency of the ﬂows may be signiﬁcantly reduced. This is because if
one of the two ﬂows (F1 or F2 ) is blocked at node C due to congestion further downstream, the other ﬂow can still use the shared links
A → B and B → C . The greedy VC addition approach may fail to
realize the beneﬁts of these combined additions and not pick either
of the links as candidates for VC allocation at a given iteration. To
overcome this drawback, we propose another greedy VC allocation
heuristic based on deletion, instead of addition, of VCs.







	
Figure 3: An example illustrating the drawback of greedy addition heuristic.
3.2 Greedy Deletion VC Allocation
V C
V C
vector, ncurrent
Our greedy VC deletion algorithm is shown in Figure 4. The
approach is structurally similar to greedy addition, except for the
fact that we delete a VC, instead of adding one, at the end of each
iteration. The algorithm starts with a given initial VC conﬁguration
for the network2 , with number of VCs equal to ninitial
(Line 1).
At the beginning of each iteration, the current VC conﬁguration
, is saved into a new vector, nnew
V C (Line 6). Next,
single VC-deletion perturbations of the current VC conﬁguration
are generated by removing a VC from each link in the network that
has more than one VC (Lines 7-8). The impact of each VC removal
on the average packet latency is assessed and the one resulting in
best latency is selected for the next iteration (Lines 12-13).
The stopping condition for the algorithm is qualitatively different from that of the addition approach. We generally expect the
average packet latency to increase at every iteration as VCs are removed. However, on rare occasions during the algorithm execution
we may encounter a conﬁguration with fewer VCs that has comparable or slightly lower packet latency than one with a higher number of VCs. This may be because the link for which a VC was
removed was not the bottleneck link responsible for increased average packet latency. Hence, instead of stopping the algorithm as
soon as the average packet latency exceeds Dtarget , it is better to
continue exploring conﬁgurations even after exceeding the target
latency value, and then return the minimum VC conﬁguration that
satisﬁes the latency constraint. The algorithm automatically stops
once a wormhole conﬁguration is reached and there are no more
VCs left to be deleted.
The greedy deletion approach can overcome the drawback of
greedy addition depicted in Figure 3. In that example, if the initial conﬁguration had two VCs for links A → B and B → C ,
deletion of a VC on either of these links would expose the link as a
bottleneck. At a given iteration, a VC is deleted from one of these
links only if the impact of the deletion on packet latency is less than
the impact of deleting VCs from any other link in the network.
Figure 5 shows the average packet latency of each intermediate conﬁguration using the greedy VC addition and deletion approaches for two different traces, ﬂuidanimate and vips from the
PARSEC benchmark suite [2]. The results presented are for a mesh
2 In this paper, without the loss of generality, we assume that the
algorithm starts with a given uniform VC conﬁguration.
using routing algorithm R, D(nV C , R), is within a target
latency constraint Dtarget
• Minimize
P
Objective:
l∈L nV C (l)
• D(nV C , R) ≤ Dtarget
Subject to:
In the next two subsections, we propose two greedy heuristics,
whose performance will be discussed in Section 4.
3.1 Greedy Addition VC Allocation
Our ﬁrst VC allocation heuristic is shown in Figure 2. The algorithm initializes every network link, l, with one VC (Lines 1-3);
this is equivalent to wormhole routing. Hence, the total number of
VCs in the network, NV C , is initialized to the total number of links,
NL (Line 5). Then, the algorithm proceeds in a greedy fashion: in
each iteration, the performance of all the NL possible perturbations
of the current VC conﬁguration, ncurrent
, are evaluated simultaneously. Each perturbation consists of adding exactly one VC to one
link (Line 9). The average packet latencies of all perturbed VC conﬁgurations are examined, and the conﬁguration with the smallest
average packet latency, nbest
V C , is chosen (Line 12). Subsequently,
V C is set as the starting conﬁguration of the next iteration. The
algorithm stops if either the total number of allocated VCs exceeds
the VC budget, budgetV C , in which case the VC budget needs to
be increased to achieve the target latency, or a conﬁguration with
better performance than the target average packet latency, Dtarget ,
is found. Runtime analysis of our proposed heuristics is explained
in Subsection 3.3.
nbest
V C
Algorithm: Greedy Addition
Input: Application communication trace, Ctrace ; Network topology,
T (P, L); Deterministic routing algorithm, R; Target latency, Dtarget
Output: Vector nV C , which contains the number of VCs associated
with each link l ∈ L
1. for i = 1 toN L
2. ncurrent
V C
(l) = 1;
3. end for
4. nbest
V C
= ncurrent
V C
;
5. NV C = NL ;
6. while (NV C ≤ budgetV C )
7.
8.
9.
10.
for l = 1 toN L
nnew
V C
nnew
V C
;
= ncurrent
V C
(l) = ncurrent
V C
(l) + 1;
run trace simulation on nnew
V C and record D(nnew
V C , R);
11. end for
12. ﬁnd nbest
V C ;
13. ncurrent
= nbest
V C ;
V C
V C , R) ≤ Dtarget )
14. if (D(nbest
15.
break;
16. end if
17. NV C ++;
18. end while
Figure 2: Greedy addition heuristic.
Despite the effectiveness of the addition approach as shown in
Section 4.2, it has an inherent disadvantage, namely, it bases its decision at each iteration on the impact (on average packet latency)
of adding only a single VC. To demonstrate the drawback of this
439
27.2
	
	
	

	































         
	
Figure 5: Performance of addition and deletion VC allocation
heuristics on ﬂuidanimate and vips traces.
4. EVALUATION
In this section, we ﬁrst describe our experimental setup and then
present the results of our proposed VC allocation heuristics. Finally, we assess the impact of our approach on network power and
area.
4.1 Experimental Setup
We use PopNet [23], a ﬂit-level, cycle-accurate on-chip network
simulator, to determine the average packet latency (AP L) of a nonuniform VC conﬁguration for a given trafﬁc trace. PopNet models a
typical four-stage on-chip router pipeline comprising of route selection and VC allocation in the ﬁrst pipeline stage followed by switch
arbitration, switch traversal and link traversal in the remaining three
stages. The head ﬂit of a packet traverses all four pipeline stages
while the body ﬂits bypass the ﬁrst stage and inherit the output port
and output VC reserved by the head ﬂit. The number of virtual
channels at the input ports can be individually conﬁgured to implement any non-uniform VC conﬁguration at a router. The routing
algorithm is always ﬁxed to be dimension-ordered routing where
packets are ﬁrst routed along the X dimension, and then along the
Y dimension. The latency of a packet is measured as the delay between the time the header ﬂit is injected into the network and the
time the tail ﬂit is consumed at the destination. The AP L value
reported by PopNet is the average latency over all packets in the
input trafﬁc trace.
Table 1: Processor conﬁguration for trace generation
Cores
Private L1 cache
Shared L2
Memory latency
Network
Packet sizes
16
32KB
1MB distributed over 16 banks
170 cycles
4×4 mesh
8B data packets, 72B control packets
To evaluate our VC allocation heuristics, we use seven different
benchmarks from the PARSEC benchmark suite, namely, blackscholes, canneal, ferret, ﬂuidanimate, swaptions, vips and x264 [2].
These benchmarks are multithreaded programs, representative of
emerging future workloads and encompassing diverse application
domains. The network trafﬁc traces were generated by running
these programs on Virtutech Simics [30], a full system simulator,
and capturing the program’s memory trace. The GEMS toolset [16]
running on top of Simics was used to perform accurate timing simulation. We simulate a 16-core tiled CMP architecture arranged in
a 4 × 4 mesh, with parameters shown in Table 1. Each tile consists
of an in-order SPARC core with private L1 and shared L2 cache.
DRAM is attached to the chip using four memory controllers that
are at the corners of the mesh. In all seven traces, every node in
the mesh both sends and receives packets. Also, all 64 links of the
4 × 4 mesh are used. Hence, for the addition heuristic we start with
a wormhole conﬁguration with exactly 64 VCs (1 VC/link) and set
network with 16 nodes and 64 links (details of the setup are explained in Section 4). In general, the average packet latency decreases as VCs are increased in the addition algorithm, and decreases as VCs are removed in the deletion algorithm. However,
the change in packet latency is much smoother in the case of deletion of VCs, compared to addition. This is because adding a VC at
a single link may not have a signiﬁcant impact on average packet latency unless the added VC relieves congestion along the entire path
of a trafﬁc ﬂow, as illustrated in the Figure 3 example. This explains
the stepwise nature of the curve for VC addition: there are periods
of little improvement followed by steep descents. The intermediate
solutions in the deletion approach are of a slightly higher quality
because the deletion heuristic is better at detecting bottleneck links.
Algorithm: Greedy Deletion
Input: Application communication trace, Ctrace ; Network topology,
T (P, L); Deterministic routing algorithm, R; Target latency, Dtarget ;
Initial VC conﬁguration, ninitial
Output: Vector nV C , which contains the number of VCs associated
with each link l ∈ L
V C
;
1. ncurrent
= ninitial
V C
V C
2. nbest
= ncurrent
V C
V C
l∈L
P
(l);
4. while (NV C ≥ budgetV C )
ncurrent
V C
3. NV C =
;
;
nnew
= ncurrent
V C
V C
if (ncurrent
V C
nnew
(l) = ncurrent
V C
V C
> 1)
(l) − 1;
run trace simulation on nnew
V C and record D(nnew
V C , R);
for l = 1 toN L
5.
6.
7.
8.
9.
10.
11.
12. ﬁnd nbest
end if
end for
V C ;
13. ncurrent
= nbest
V C ;
V C
V C , R) ≤ Dtarget )
14. if (D(nbest
16. end if
15.
break;
17. NV C −−;
18. end while
Figure 4: Greedy deletion heuristic.
3.3 Runtime Analysis
Let m be the number of VCs added to (by greedy addition) or
deleted from (by greedy deletion) an initial VC conﬁguration. The
runtime of the two proposed heuristics, Theuristic , for any given
input trace is
Theuristic = m ∗ NL ∗ T (trace simulation)
(1)
where T (trace simulation) is the average time to run trace simulation on all VC conﬁgurations explored by the algorithm. The
above expression for runtime holds if the performance of each perturbation of the current VC conﬁguration is evaluated sequentially.
However, the code for the two heuristics can be easily parallelized
by evaluating all the perturbations in parallel. This results in an
improved runtime of
Theuristic = m ∗ T (trace simulation)max
(2)
Here, T (trace simulation)max represents the average of the maximum runtimes of trace simulation at each iteration, where the maximum is taken over the runtimes of all perturbations in the iteration.
440














27.2
trace-driven VC allocation. Another outlook would be to quantify
the performance beneﬁts while keeping the number of VCs ﬁxed.
With respect to this metric, we see in Figures 8 and 9 that with a
constraint of 128 VCs, the average packet latency of greedy deletion
is better than that of an uniform-2VC conﬁguration by 32% for the
ﬂuidanimate benchmark and by 74% for the vips benchmark. So,
trace-driven non-uniform VC allocation can potentially be used to
either reduce power within a given performance constraint or to
improve performance within a given power constraint.
Finally, to assess the impact of our proposed approach on network power and area, we use ORION 2.0 [10]. The ORION 2.0
router model assumes the same number of VCs at every port in the
router. However, we need to compute the router power for nonuniform VC conﬁgurations. Hence, we ﬁrst estimate the power
overhead of adding a single VC to all router ports. This is done
by computing the router power for uniform conﬁgurations with
one, two, and three VCs per port and averaging the power difference between the uniform-1VC (uniform-2VC) and uniform-2VC
(uniform-3VC) conﬁgurations. This gives an estimate of the power
overhead of adding one VC to all router ports. This value is divided
by the number of router ports to determine the overhead of adding
a single VC to just one port. We use a similar approach to ﬁnd
the area overhead of adding a single VC to one router port. The
number of VCs saved by our trace-driven heuristics compared to
uniform conﬁgurations is then multiplied by the power (area) overhead of adding a single VC to a router port to estimate our power
(area) savings. From our experiments, we observe that our approach
can reduce the network power by up to 7% and 14% compared
to the uniform-2VC and uniform-3VC conﬁgurations, respectively,
while achieving the same performance. Similarly, we achieve up
to 9% and 16% area reduction compared to the uniform-2VC and
uniform-3VC conﬁgurations, respectively.
Figure 6: Performance of addition and deletion VC allocation
methods versus the uniform-2VC conﬁguration.
Figure 7: Performance of addition and deletion VC allocation
methods versus the uniform-3VC conﬁguration.
5. CONCLUSIONS
In this paper, we have proposed a trace-driven approach to the
optimization of NoC conﬁgurations. We have speciﬁcally considered the problem of application-speciﬁc VC allocation, which seeks
our VC budget to 256 VCs, equivalent to an uniform conﬁguration
with 4 VCs/link. In the case of deletion, we start with a uniform
conﬁguration with 4 VCs/link and delete one VC at every iteration
until we reach a wormhole conﬁguration. Each iteration of the addition and deletion heuristics creates at most 64 perturbations which
are run in parallel on a computational grid.
4.2 Experimental Results and Discussions
In this subsection, we present the results of our proposed VC
allocation heuristics with respect to average packet latency reduction and reduction in the number of VCs. We compare our greedy
VC addition and deletion approaches with uniform VC allocation
scheme and our reimplementation of the average-rate driven VC
allocation algorithm of [9] on the seven benchmarks from the PARSEC suite. Figures 6 and 7 show that our algorithms can reduce the
number of VCs required to achieve the same target latency as uniform conﬁgurations with two VCs per port and three VCs per port,
respectively. We also show that our algorithms can achieve higher
reductions in the number of VCs compared to existing average-rate
driven approaches.
Figure 6 shows the number of allocated VCs using our greedy addition and deletion algorithms to achieve the same average packet
latency as that of a uniform conﬁguration with two VCs per port
(uniform-2VC). We observe that our addition and deletion heuristics can achieve up to 36% and 34% reduction in number of allocated VCs, respectively. On an average, both our techniques reduce
the number of VCs by around 21% across all benchmarks. Figure 7
shows the number of VCs required using our algorithms to achieve
the same average packet latency as an uniform conﬁguration with
three VCs per port (uniform-3VC). Uniform-3VC achieves lower
average packet latency than uniform-2VC and so more VCs need to
be allocated even using our algorithms to achieve the same target
latency. However, we see high reductions of up to 51% for greedy
deletion and 48% for greedy addition, compared to the 192 VCs
required by the uniform conﬁguration. Even on an average, high
reductions of 41% and 31% were observed for the greedy deletion and addition techniques, respectively. These reductions over
uniform VC allocation are mainly seen because our VC allocation
schemes are able to fully exploit the extra degree of freedom offered by non-uniform VC allocation, using complete knowledge of
the application trace. Greedy deletion reduces the number of VCs
over addition by around 25% on average when the latency target is
the average packet latency of uniform-3VC. It performs comparably
to greedy addition when the target latency is the packet latency of
uniform-2VC. Thus, of the two heuristics, greedy deletion yields
slightly better solutions for the reasons discussed in Section 3.2
above.
Figures 6 and 7 also compare our algorithms to the solutions obtained using an average-rate driven technique [9] for VC allocation. Our addition and deletion algorithms outperform the averagerate driven approach by 19% and 20%, respectively, over all traces
when the target latency is set to be the latency of uniform-2VC and
by 25% and 35%, respectively, when the target latency is the latency for uniform-3VC. The average-rate driven approach reduces
the number of VCs over the uniform conﬁgurations for only two
of the seven benchmarks. We believe that this is due to the fact
that the average-rate characteristics of an application trace are not a
very accurate representation of the actual trafﬁc, which is bursty in
nature for most traces with wide variations about the average rate
(recall Figure 1). Our trace-driven techniques accurately comprehend the effects of these trafﬁc bursts and can lead to signiﬁcantly
better solutions.
Next, in Figures 8 and 9 we highlight the potential beneﬁts of
one of our VC allocation algorithms (greedy deletion) on two of
the trafﬁc benchmarks, namely, ﬂuidanimate and vips. In our experiments, we set a VC budget of 256 VCs or 4 VCs per link. We
observe that an uniform conﬁguration with 4 VCs per port has the
best average packet latency among all uniform conﬁgurations possible within the VC budget. With our greedy deletion approach, we
are able to achieve the same latency as that of an uniform conﬁguration with 4 VCs per port, with 50% and 42% reduction in the
number of VCs for the ﬂuidanimate and vips benchmarks, respectively.
Measuring the reduction in the number of VCs to achieve a given
performance criteria shows the potential power and area beneﬁts of
441
27.2
[4] W. J. Dally and C. L. Seitz, “The Torus Routing Chip”, Journal of
Distributed Computing, 1(3), 1986, pp. 187–196.
[5] W. J. Dally and B. Towles, “Route Packets, Not Wires: On-Chip
Interconnection Networks”, Proc. DAC, 2001, pp. 684–689.
[6] W. J. Dally and B. Towles, Principles and Practices of
Interconnection Networks, Morgan Kaufmann, 2004.
[7] Y. Hoskote, S. Vangal, A. Singh, N. Borkar and S. Borkar, “A 5-GHz
Mesh Interconnect for a Teraﬂops Processor”, IEEE MICRO, 2007,
pp. 51–61.
[8] J. Hu, U. Y. Ogras and R. Marculescu, “System-Level Buffer
Allocation for Application-Speciﬁc Networks"
ACES - application-specific cycle elimination and splitting for deadlock-free routing on irregular network-on-chip.,"Application-specific Network-on-Chip (NoC) in MPSoC designs often requires irregular topology to optimize power and performance. However, efficient deadlock-free routing, which avoids restricting critical routes and also does not significantly increase power for irregular NoC, has remained an open problem until now. In this paper an application-specific cycle elimination and splitting (ACES) method is presented for this problem. Based on the application-specific communication patterns, we propose a scalable algorithm using global optimization to eliminate as much channel dependency cycles as possible while ensuring shortest paths between heavily communicated nodes, and split only the remaining small set of cycles (if any). Experimental results show that compared to prior work, ACES can either reduce the NoC power by 11%~35% while maintaining approximately the same network performance, or improve the network performance by 10%~36% with slight NoC power overhead (-5%~7%) on a wide range of examples.","27.3
ACES: Application-Specific Cycle Elimination and Splitting 
for Deadlock-Free Routing on Irregular Network-on-Chip 
Jason Cong, Chunyue Liu, Glenn Reinman 
Computer Science Department, University of California, Los Angeles 
Los Angeles, CA 90095, USA 
{cong, liucy, reinman}@cs.ucla.edu 
ABSTRACT 
Application-specific Network-on-Chip (NoC) in MPSoC designs 
often requires 
irregular 
topology 
to optimize power and 
performance. However, efficient deadlock-free routing, which 
avoids restricting critical routes and also does not significantly 
increase power for irregular NoC, has remained an open problem 
until now. In this paper an application-specific cycle elimination 
and splitting (ACES) method is presented for this problem. Based 
on the application-specific communication patterns, we propose a 
scalable algorithm using global optimization to eliminate as much 
channel dependency cycles as possible while ensuring shortest 
paths between heavily communicated nodes, and split only the 
remaining small set of cycles (if any). Experimental results show 
that compared to prior work, ACES can either reduce the NoC 
power by 11%~35% while maintaining approximately the same 
network performance, or improve the network performance by 
10%~36% with slight NoC power overhead (-5%(cid:794)7%) on a wide 
range of examples.   
Categories and Subject Descriptors 
B.4.3 [Interconnections (Subsystems)]: Topology 
General Terms 
Algorithms, Design 
Keywords 
Application-specific Network-on-Chip, Deadlock-free routing 
1. INTRODUCTION 
As we shrink IC feature sizes to nanoscale, interconnect delay 
and power consumption emerge as the dominant factors in the 
optimization of Multi-Processor System-on-Chip (MPSoC). As 
the number of cores on a single die grows, more interconnect 
bandwidth will be 
required 
for on-chip communication. 
Therefore, the Network-on-Chip (NoC) design is proposed to 
solve these challenges because of its intrinsic scalability and 
predictability. 
Application-specific NoC offers the opportunity to optimize the 
NoC for the target problem domain and does not necessarily 
conform to regular topologies. In general, the selection of the 
topology can have a dramatic impact on the overall performance, 
area and power. Recent work in application-specific NoC 
topology synthesis has shown significant power savings compared 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC’10, June 13–18, 2004, Anaheim, California, USA. 
Copyright 2010 ACM 978-1-4503-0002-5 /10/06…$10.00. 
to a regular NoC [1][2]. Alternatively, the performance of regular 
topologies can be significantly improved with minimal impact on 
area and energy consumption by adding application-specific longrange links [3] or radio frequency interconnections (RF-I) [4][5]. 
Clearly, the evolution of application-specific NoCs is leading to 
irregular topologies. However, efficient deadlock-free routing for 
NoC with irregular topologies remains an open problem.  
There are two main approaches to dealing with deadlock in 
irregular NoCs. The first class of approaches is based on the 
theory of [6]. It divides the NoC into two virtual networks (VN): 
one fully-adaptive (no routing restrictions) and another that is 
deadlock-free (with routing restrictions). Network packets are 
routed in the full-adaptive VN at first and will be moved to the 
deadlock-free VN when there are no available resources in the 
full-adaptive VN. One recent example is the deadlock detection 
and recovery method used in [4] and [5]. However, since most 
application-specific MPSoC in the embedded system domain are 
power-critical, the power overhead of introducing two virtual 
channels for each physical channel is significant.  
The second class of approaches handles the deadlock-free 
routing problem in irregular NoCs by imposing routing restriction, 
such as the turn prohibition algorithm [7] and south-last routing 
[3]. They restrict routings without considering the applicationspecific communication patterns; hence they may increase the 
routing distance between heavily communicated nodes. The recent 
work in [8] first proposes to remove dependencies based on the 
application communication requirement. It uses a greedy heuristic 
and may exhaustively enumerate all possible combinations of 
channel dependency cycles and thus cannot scale to large designs. 
In fact, sometimes it is impossible to make the channel 
dependency graph acyclic without disconnecting the network with 
unidirectional links. This complication is not considered in [8]. 
Moreover, [8] only considers whether there is communication 
between two nodes or not, without consideration of the data size 
transferred, which may lead to sub-optimal solutions.   
Therefore, it is necessary to find an optimal trade-off point 
between power and performance (i.e., between these two types of 
approaches) for deadlock-free routing in irregular NoCs. We want 
something that avoids restricting critical routes in the NoC, but 
that also does not significantly increase NoC power. In this paper 
we propose an application-specific cycle elimination and splitting 
(ACES) method for this problem. We first develop a scalable 
algorithm using global optimization to eliminate as many channel 
dependency cycles as possible with the guarantee of network 
reachability, based on the application-specific communication 
patterns, and then only split the remaining small set of cycles (if 
any) using virtual channel splitting. Network performance is 
maintained by ensuring plentiful shortest paths between heavily 
communicated nodes. Moreover, with the possible existence of 
split channels, an routing table construction and encoding method 
is developed to minimize the hardware overhead of ACES. 
443
 
 
2. DEFINITIONS AND FRAMEWORK 
We first present some definitions relevant to the application and 
the NoC architecture that will be used in the rest of the paper.  
Definition 1: An application characterization graph APCG(C, 
E) is a directed graph, where each vertex ci(cid:281)C represents an IP 
core and each edge eci,cj(cid:281)E characterizes the communication from 
ci to cj. Each eci,cj is tagged with vci,cj which characterizes the 
communication volume (size of transferred data) from ci to cj.  
Definition 2: A NoC topology graph TG(R, Ch) is a directed 
graph where each vertex ri(cid:281)R represents a router. Each directed 
edge chi,j(cid:281)Ch represents a physical unidirectional channel that 
connects an output port of ri  to an input port of rj.  
Here we assume the mapping of the IP cores in a given 
APCG(C, E) to the routers of a given TG(R, Ch) has already been 
done. Figure 1(a) shows an example of a TG upon which an 
APCG is mapped. We use this as a working example in this 
section. The modeled NoC is a 3×3 baseline mesh overlaid with 
one shortcut from node A to node L. The different communication 
volumes between nodes are denoted by dashed lines.  
Definition 3: Given a topology graph TG(R, Ch) and a routing 
function P, a channel dependency graph CDG(Ch, D) is a directed 
graph. Each vertex chi(cid:281)Ch is a physical channel and each edge 
di,j(cid:281)D is a channel dependency from chi to chj, i.e., a network 
packet is allowed by the routing function P to go from chi to chj. 
According to the theory of [6], a routing function P for a TG is 
deadlock-free if the CDG is acyclic. Figure 1(b) illustrates the 
corresponding CDG of the TG in Figure 1(a) without any routing 
restrictions. A node AB denotes the physical channel from router 
A to B. The injection queue si and the ejection queue ti of a router 
ri are denoted in unfilled circles with their channel dependencies 
denoted by dashed lines. In the rest of the paper, a shortest path 
from router ri to router rj denotes the shortest path from si to tj.  
ACES breaks the cycles in the CDG based on the weight of the 
channel dependency edges. To appropriately guide the algorithm 
to remove the less-used edges and ensure a large number of 
shortest paths between heavily communicated nodes, an edge is 
weighted by both the number of shortest paths passing through it 
and the communication volume (size of transferred data). We 
introduce the usage probability to capture this property. Here, we 
use ĭ(ri, rj) to denote the set of shortest paths in CDG from router 
ri to rj and  A(d, ri, rj) to denote the number of shortest paths in 
CDG from ri to rj that go through edge d.  
Definition 4: Given a CDG(Ch, D), the usage probability of an 
edge d(cid:281)D for the application APCG(C, E) is: 
r r
,
i
j
,
 s.t. 
d
∈Φ
(
r r
,
i
j
)
c
i
∈
IP r
(
),
c
j
i
∈
IP r
(
)
j
where, IP(ri) and IP(rj) are sets of IPs mapped on router ri and rj, 
respectively; vci,,cj is the communication volume from vertex ci to 
cj (defined in Definition 1).  
With this weighting function, heavily communicating nodes 
will make all the edges of their shortest paths have larger weights 
and thus are less likely to be removed; i.e., the more two nodes 
communicate, the shorter their distance will be and the more 
shortest paths they will have. We introduce the applicationspecific weights (usage probability) for a channel dependency 
graph CDG(Ch, D) to construct an application- specific channel 
dependency graph (ASCDG). Figure 1(c) gives the ASCDG based 
on the APCG in Figure 1(a) and CDG in Figure 1(b). Figure 1(c) 
shows that the usage probability of different channel dependency 
edges varies considerably. This motivates the application-specific 
Pr d
( )
=
¦
¦
v
c c
,
i
j
A d r r
( ,
,
i
) / |
j
Φ
( ,
r r
i
) |
j
27.3
nature of ACES. Without consideration of the application 
communication pattern, an acyclic CDG generated by the southlast routing algorithm is given in Figure 1(d), where the use of the 
shortcut is restricted with the forbidden southbound dependency 
from AL to LH/LF. The shortcut from A to L is augmented to 
optimize the topology for the application, but by applying southlast routing, the use of the shortcut is severely restricted. Figure 
1(e) shows the acyclic CDG generated by ACES, where only the 
channel dependency edges that are never or rarely used are 
removed. It should be noted 
that reachability should be 
guaranteed while breaking cycles in the ASCDG; i.e., for each pair 
of communicating nodes of the application, there is at least one 
directed path from the source node to the destination node.  
(a)                                               (b) 
15.5 34
16
52.7
21
66.3
7.2
26.3
4
1
39
26.3
34
52.7
(c)                                                 (d) 
BA
AB
AL
CB
BC
AD
DA
BE
EB
CF
FC
ED
DE
FE
EF
DG
GD
EH
HE
FL
LF
HG
LH
GH
HL
(e)                                               (f) 
Figure 1.  Example of (a) an APCG mapped on a topology graph 
TG, (b) CDG, (c) ASCDG, (d) acyclic CDG by south-last routing, 
(e) acyclic CDG by the ACES. (f) Overall framework of ACES 
An overview of the ACES framework is shown in Figure 1(f). 
The framework takes the APCG and TG as the inputs. A CDG is 
constructed based on 
the given TG without any routing 
restrictions. From the APCG and CDG, an initial ASCDG is 
constructed by weighting the channel dependencies with the 
444
 
 
  
 
 
application-specific communication patterns. Then 
the RGMWFES algorithm (Section 3) is performed to remove channel 
dependencies that are not part of frequent communication routes. 
If the algorithm finishes with an acyclic ASCDG, virtual channel 
splitting is bypassed. Otherwise, it must be the case that the 
channel dependencies in the remaining cycles are kept to maintain 
the network reachability; thus, virtual channel splitting is used to 
break these remaining cycles. Furthermore, since applicationspecific NoC typically use routing tables to guide the routers to 
route the network packets [10], we construct and encode the 
routing tables based on the final acyclic ASCDG, to minimize the 
hardware overhead of ACES (Section 4). At the run time, the 
generated routing table will be loaded into the routers at the 
beginning of the application. Note that ACES is not restricted to 
routing-table-based method, its generated ASCDG can also be 
implemented in the logic-based routing method (such as the 
routing bits in LBDR proposed in [9]). To leverage the plentiful 
shortest paths between heavily communicated nodes in the 
ASCDG preserved by ACES, we use adaptive routing to 
dynamically balance the network traffic. It should be noted that, if 
deterministic load-balanced routing algorithms (e.g., [10]) are 
used, with plentiful shortest-path choices between heavily 
communicated nodes provided by ACES, these algorithms can 
also balance the load on each edge more easily; however, this is 
beyond the scope of this paper.  
3. RG-MWFES ALGORITHM 
Making the ASCDG acyclic is similar to the minimum weight 
feedback edge set (MWFES) problem on a directed graph, which 
is known to be NP-Complete [11]. Although good approximation 
methods are proposed in [12], our problem is more difficult here 
due to the requirement to guarantee ASCDG reachability. 
Although the existing MWFES algorithms can guarantee that the 
directed graph is connected after the cycle elimination, they can 
not guarantee the existence of a directed path between every 
application communication. Thus some source nodes may not 
reach their destination nodes in the ASCDG after applying 
existing MWFES algorithms. To overcome this problem, we 
introduce a new reachability guaranteed minimum weighted 
feedback edge set (RG-MWFES) problem in this paper as follows. 
Given: An ASCDG(Ch, D), 
Goal: Under the constraint of maintaining the reachabilitiy of 
ASCDG(Ch, D), remove edges to minimize the number of vertices 
that still get involved in cycles in the resulting ASCDG and the 
total weight of the removed edges. 
The first goal is to minimize the vertices involved in cycles, 
since the physical channels represented by them will be split in 
the subsequent stage, resulting increased power consumption. In 
the meantime, we want to minimize the total weight of the 
removed edges in order to maintain network performance, since 
the weight of an edge characterizes the number of shortest paths 
and the communication volume that go through that edge. 
We choose the MWFES algorithm in [12] as a starting point to 
develop 
approximation 
algorithms with 
this 
additional 
reachability constraint, since [12] represents a recent global 
optimization approach to MWFES problem on a directed graph. 
The idea of [12] is to decrease the weight of all the edges in any 
cycle it finds by the weight of the edge which will be removed 
from that cycle. Thus the more cycles an edge belongs to, the 
more likely its weight will be reduced and the more likely it will 
be removed in subsequent steps. Since more edges may be 
removed than necessary, a final edge add-back stage is performed 
to minimize the total weight of the removed edges while avoiding 
27.3
re-introducing cycles. However, like other MWFES algorithms, 
[12] does not guarantee the reachability. To overcome this 
problem, we introduce an edge lock scheme in our RG-MWFES 
algorithm; i.e., if the removal of an edge will violate the ASCDG 
reachability, it is locked so that it can not be removed in the 
future. Here, we call this edge a Critical Edge. Critical edge 
checking is performed in the step of updating edge weight: given 
an edge d, if all the paths in ASCDG from a source router ri to a 
destination router rj of an application communication go through 
edge d, then d is a critical edge. It will be preserved and marked 
locked. However, if all the edges in a detected cycle are locked, 
the edge with the least weight in that cycle is temporarily 
removed. If it remains critical after the final edge add-back stage, 
it will be added back and handled by virtual channel splitting.  
It should be noted that the edge weights are changed after each 
removal of a feedback edge, since the removal of an edge will 
increase the usage probability of other edges in the ASCDG. 
Moreover, the removal of an edge will make some other edges 
critical. Therefore, we can not fix the edge weight as the 
algorithm progresses. Instead, the edge weight needs to be 
updated at each iteration. However, since the weight of an edge is 
also reduced when another edge in the same cycle is removed, 
which is the key idea of the baseline MWFES algorithm [12] to 
obtain a globally optimized solution, it is not sufficient to use only 
one weight variable to keep track of these two kinds of weight 
update. In the RG-MWFES algorithm, for each edge d, we 
introduce two terms for the edge weight: base weight wb(d) and 
dynamic weight adjustment wa(d). The base weight wb(d) is 
initiated by the weight obtained from the initial ASCDG (initial 
weight). Each time that an edge is removed from the cycle, the 
wb(d) of all the other edges in this cycle will be reduced by the 
weight of the removed edge,  which is the same as [12]. The wa(d) 
is initiated as 0 and is handled as follows: each time that we 
update the edge weights of all the remaining edges in the graph 
based on the new usage probability when some edges are 
removed, we calculate the new weight, and record the difference 
between this new weight and the initial weight as wa(d). When we 
want to find the minimum weight edge in a cycle, the sum of these 
two terms will be used to denote the current weight w(d) of an 
edge d. 
In the edge add-back step, the earlier an edge tries to be added 
back, the more likely it can be successfully added back without reintroducing cycles. Since the first goal of our algorithm is to 
minimize the vertices that still get involved in cycles after the 
algorithm is completed, we first add back all the removed locked 
edges in decreasing order of the total size of the cycles they once 
belonged to (at the time of removal) if their additions do not 
introduce any cycle. Then we add back the previously removed 
unlocked edges if they do not re-introduce cycles in the 
decreasing order of the weights. Thus the total weight of the 
removed edges is minimized, which is the second goal of our 
algorithm. After adding back the previously removed edges, some 
of the remaining removed locked edges may no longer be critical. 
Therefore we check the criticality of the remaining removed 
locked edges. If they are no longer critical, they will be treated as 
unlocked edges so that we do not need to add them back. 
Otherwise, the removed locked edges that remain will finally be 
added back and be handled by virtual channel splitting.  
In sum, the RG-MWFES algorithm has three innovations: (i) 
network reachability 
is guaranteed, (ii) edge weights and 
criticality are properly and efficiently updated, and (iii) the 
algorithm 
simultaneously considers both network power 
445
27.3
into a pair of VCs: high VC and low VC (shown in Figure 3). 
There are only channel dependencies in the same level of VCs, 
except at the breaking edge, where there is only channel 
dependency from high VC to the low VC. The high VC holds all 
the shortest paths of the original channel while the low VC may 
not. Different VCs in the same input channels may have different 
routing decisions for the same destination, which makes the above 
encoding method impossible, and may result in 2MN2kL space.  
Figure 3. Virtual channel splitting to break a cycle 
We overcome this problem by proposing an efficient routing 
table construction method. To ensure that different input channels 
use the same VC of an output channel when the output channel is 
located on the shortest path to one destination, we make the 
following rule: if both the low and high VCs are located in the 
shortest path from router ri to rj, the low VC will be selected (to 
leave the high channel available for other routes). This rule also 
maximizes the use of both low and high VCs. 
From the above rule, we can see that for a pair of split VCs, if 
both have equal distance to the destination, the packet towards 
that destination must be located in the low VC; otherwise, the 
packet must be located in the high VC. Then, given a destination, 
we will know the location of the packet in the input channel; i.e., 
although the two VCs in an input channel may have different 
routing decisions to the same destination, the routing decision is 
fixed for this input channel. Thus, for each pair of split VCs, only 
one routing table entry is required. 
Figure 4. Router architecture for support of ACES 
Figure 4 shows a 5-port adaptive router with the shaded 
modules augmented by ACES. In the output-channel availability 
table (OAT), a 0 means that the output-channel is not available. In 
the routing table, a 1-bit prefix is added to each output channel 
index (0 means low VC and 1 means high VC; the non-split 
channels are considered low VC). A filter is added and its 
schematic diagram is presented in Figure 4. If a candidate output 
channel is the same with one element of the forbidden register, the 
filter will output a 0 to invalidate the corresponding availability 
signal from OAT. Results from Cacti 5.3 show that by routing 
table encoding, the routing table area can be reduced by 66%, 
power can be reduced by 43% and accessing time can be reduced 
by 13% compared to that of a channel-specific routing table. [10] 
446
Figure 2. Pseudo-code of the RG-MWFES algorithm 
consumption (by minimizing the need for channel splitting) and 
performance (by maximizing application-specific frequently used 
communication paths). Based on our experimental results 
(detailed in Section 5), for small NoCs the algorithm can break all 
cycles, while for large NoCs (more than 500 cycles in the original 
ASCDG) the algorithm can complete with only 4~5 cycles 
remaining. The pseudo-code of the RG-MWFES algorithm is 
shown in Figure 2. The complexity of the RG-MWFES algorithm 
is O(|Ch |*|D|3), given an ASCDG(Ch, D). 
4. ROUTER ARCHITECTURE  
The impact of ACES on the router architecture is twofold. On 
one hand it reduces the number of virtual channels (VC) 
compared to the virtual-network-based methods, which is the 
target of ACES. However, on the other hand, since ACES is based 
on channel-specific routing (i.e., each channel of the router may 
have different routing decisions for the same destination), a 
straightforward implementation will require a larger memory for 
the routing table than a routing algorithm where every input 
channel of the router has the same routing decision for the same 
destination (referred to as router-specific routing, and used in the 
fully-adaptive routing of the virtual-network-based methods). This 
may negate the power savings we obtained from reducing the 
number of VCs. Assuming an M-port router and the fact that each 
channel index can be represented by L bits, each routing table 
entry will contain kL bits, where k is a parameter related to the 
NoC adaptiveness. For a channel-specific routing, each input 
channel will require NkL space, and the routing table size will be 
MNkL for one router and MN2kL for the whole NoC. However, a 
router-specific routing table only requires N2kL space. 
Without virtual channel splitting, an effective routing table 
encoding method is as follows. All of the routing entries for the 
input channels in a router to the same destination are combined 
into a single entry so that it contains all the possible output 
channels for these input channels (as in router-specific routing). 
For each input channel, a small register called the forbidden 
register is used to store the index of its forbidden output channels 
(corresponding to the removed edges from the ASCDG by RGMWFES algorithm). Then at the routing computation stage, the 
router first reads the candidate output channels from the routing 
table and filters them with the forbidden register associated with 
the input channel. The channel-specific lookup table is avoided.  
However, virtual channel splitting complicates this method. It 
breaks the cycles in CDG by splitting each channel along a cycle 
 
 
 
27.3
shows that the typical critical path of a router is VC allocation 
(15-20 FO4), while the access time of the encoded router-specific 
routing-table is only 0.16ns in 32nm. Thus, the introduced logic 
(<3 FO4) will not impact the clock cycle. Orion 2.0 [14] reports 
that the area of the above router is 285,028 um2, and the synthesis 
results of the augmented logic is only 167 um2, which is less than 
0.1% of the total router area. 
5. EXPERIMENT RESULTS 
5.1 Experiment Platform 
The proposed ACES framework is implemented in C++. To 
accurately capture the NoC performance, we leverage the Garnet 
[13] network simulator and implement the dynamic routing table 
look-up method and virtual channel splitting scheme onto it. To 
accurately capture the NoC power, Orion 2.0 [14] is used to 
obtain the data of router dynamic/leakage energy and area with 
various router configurations and links.  
We evaluate the ACES method on a platform of a regular mesh 
overlaid with RF-I shortcuts [5]. Because shortcuts can be 
allocated arbitrarily on 
the mesh, most existing 
irregular 
topologies can be generalized to it. Different benchmarks have 
different optimal shortcut allocations (via [5]), yielding different 
irregular topologies. ACES does not rely on the underlying mesh 
to achieve deadlock-free routing. For the power estimation of the 
RF-I, we adopt the method described in [5], which shows a 10×10 
mesh (2cm×2cm chip) with a single cycle packet traversal delay 
across all RF-I shortcuts at 2GHz frequency. Thus in this paper, 
we assume that the delay to travel all the shortcuts is one cycle. 
We compare the ACES method with two other commonly used 
methods for deadlock-free routing in irregular topology: deadlock 
detection and recovery (DDR) [5] (based on the theory of [6]) and 
south-last routing (SLR) [3]. SLR is used in the dead-free VN of 
DDR instead of using XY routing in [5] to achieve the best 
performance of DDR. As discussed in Section 1, DDR sacrifices 
power for performance, while SLR sacrifices performance for 
power. We will show that ACES can make a good trade-off 
between these two endpoints. We do not compare ACES to [8] as 
it can not scale to the size of most of our evaluated networks. The 
number of VCs for each port is 1 for SLR, 2 for DDR, 1 for nonsplit channel in ACES, 2 for split channel in ACES. Wormhole 
switching and adaptive routing are used. Each flit is 8 bytes and 
each packet can have 36, 12 or 4 flits based on the message type. 
5.2 Benchmarks 
To explore the interconnect demand of future multithreaded 
applications, we generate probabilistic traces to represent a variety 
of communication patterns as in [5]. The first pattern is hotspot, in 
which there is one component generating a disproportionate 
amount of traffic. This can be exhibited by caches holding 
frequently used synchronization variables or a master/worker 
paradigm. The second set of patterns, uniDF and biDF, clusters 
components into groups which are functionally laid out in a 
dataflow-like fashion. Components are biased to communicate 
with components within their group and with components in 
groups that neighbor them on either one side (uniDF) or both 
sides (biDF). These patterns would be seen in data decomposition 
or a functional decomposition into a pipelined pattern. The 
probabilistic trace files are generated for a 10×10 mesh overlaid 
with 16 shortcuts. Both low and high injection rate are tested for 
each trace (e.g., hotspot_L/H). 
We also explored some real-life applications: three kernels and 
four applications from the SPLASH benchmark suite [15], an 
447
MPEG-4 decoder [16], two embedded system applications, 
auto_industry and telecom, from the E3S benchmark suite [17].  
For SPLASH benchmarks, the NoC is configured as a 10×10 
mesh overlaid with eight shortcuts. Memory controllers are placed 
at chip corners. The largest network packets are between memory 
controllers and L2 cache banks; thus, we surround the memory 
controller with eight L2 banks (256KB, 8-way set associative, 
128B block size) in each 3×3 corner to reduce the distance 
traveled by these messages. The other 64 nodes are tiles with one 
processor and one local L1 I/D cache (8KB, 4-way set associative, 
32B block size). We collect network message injection traces of 
these benchmarks on a 64-core SPARC processor using Simics 
[18], and then execute these traces on Garnet.  
A parallel version of the MPEG-4 decoder is executed on an 
MPSoC with seven processors with local L1 I/D Cache (16KB, 4way set associative, 16B block size), four L2 cache banks (1MB,  
16-way set associative, 32B block size) and five memory 
controllers. The NoC is configured as a 4×4 mesh overlaid with 
one shortcut. In order to maintain the manually performed task 
partition, we leverage MC-Sim [19] to run the MPEG-4 decoder 
and collected network message injection traces with two frames 
decoded, and then executed these traces on Garnet.  
For auto_industry and telecom, the tasks are mapped onto a 4×4 
mesh NoC overlaid with one shortcut. Simulation annealing is 
used to minimize the distance of the heavily communicated task 
nodes. Networks message injection traces are developed based on 
the communication pattern described in their task graphs. 
5.3 Comparison Results 
Figure 5 shows the NoC performance comparison results in 
terms of average network latency. To make a clear demonstration, 
the bars in the figure depict the normalized value to DDR and the 
absolute values are shown above the corresponding bars. By 
restricting the routing without considering the application-specific 
communication pattern, SLR consistently performs the worst 
among the three methods. Compared to DDR, ACES only 
introduces 1.9% degradation on average. In particular, when the 
injection rate goes high in hotspot and biDF, ACES outperforms 
DDR since many packets move to deadlock-free VN in DDR. In 
relatively small size NoC (i.e., the MPEG4, auto_industry and 
telecom), ACES performs almost the same as DDR.  
Figure 6 shows the NoC power results. Compared to DDR, 
ACES can achieve a significant power reduction (11%~35%) 
since only a small set (0%~16%) of the network channels need to 
be split (The number above the third bar of each benchmark 
shows the percentage of channels that are split by ACES). 
Compared to SLR,  although a small set of channels are split, by 
reducing the distance between the most frequently communicated 
nodes, ACES also reduces the dynamic power, and thus has power 
similar to SLR. 
Overall, ACES provides the best trade-off of NoC performance 
and power. It improves the performance by 10% ~36% with a 
power overhead of -5%~7% compared to SLR. It reduces the 
power by 7%~35% on average while maintaining approximately 
the same network performance compared to DDR.  
6. CONCLUSIONS 
An application-specific cycle elimination and splitting (ACES) 
method is proposed in this paper to provide an efficient deadlockrouting method in application-specific NoC designs. For powercritical designs where only 1~2 VCs can be used for each routerport, ACES can achieve the best power-performance trade-off. It 
should be noted that, for performance-critical designs with 
plentiful VCs for each router-port, ACES can still be used to 
optimize the deadlock-free virtual network (VN) based on [6], by 
ensuring plentiful shortest-paths between heavily communicated 
nodes, which is important because latency in the deadlock-free 
VN is crucial to the whole NoC latency when the network traffic 
is heavy since packets moved to the deadlock-free VN can not go 
back to the fully-adaptive VN to avoid livelock. 
Future work includes extending ACES to the fault-tolerant NoC 
designs since regular NoC with faults can also be considered as 
irregular networks. Moreover, the scheme to dynamically and 
incrementally adjust the routing table to the communication 
variation is also an interesting topic for future work. 
7. ACKNOWLEDGEMENTS 
This research is partially supported by the SRC Contract 2009TJ-1984, the NSF Expeditions in Computing Award CCF0926127 and the NSF grant CCF-0903541. We also thank Mishali 
Naik and Karthik Gururaj for interesting discussi"
NTPT - on the end-to-end traffic prediction in the on-chip networks.,"Power and thermal distribution are critical issues in chip multiprocessors (CMPs). Most previous studies focus on cores and on-chip memory subsystems and discuss how to reduce their power and control thermal distribution by using dynamic voltage/frequency scaling. However, the on-chip interconnection network, or network-on-chip (NoC), is also an important source of power consumption and heat generation. Particularly, the traffic flowing through the NoC affects directly its power and thermal distribution. Unfortunately, very few works discuss the dynamism of NoC. A key technique for NoC management is to capture its traffic patterns and predict future behaviors. In this paper, we propose a table-driven predictor called Network Traffic Prediction Table (NTPT) for recording and predicting traffic in NoC. The most unique feature of NTPT is its ability to predict end-to-end traffic, rather than switch-to-switch traffic. Thus, more application behaviors can be captured and monitored. Evaluations on Tilera's TILE64 show that NTPT has very high prediction accuracy. Analyses also show that it incurs a low area overhead and is very feasible.","27.4
NTPT: On the End-to-End Trafﬁc Prediction
in the On-Chip Networks
Yoshi Shih-Chieh Huang† , Kaven Chun-Kai Chou† , Chung-Ta King† , Shau-Yin Tseng‡
†Depar tment of Computer Science, National Tsing Hua University, Hsinchu, Taiwan
‡SoC Technology Center, Industrial Technology Research Institute, Hsinchu, Taiwan
† {yoshi, kavenc, king}@cs.nthu.edu.tw, ‡ tseng@itri.org.tw
ABSTRACT
Power and thermal distribution are critical issues in chip
multiprocessors (CMPs). Most previous studies focus on
cores and on-chip memory subsystems and discuss how to
reduce their power and control thermal distribution by using
dynamic voltage/frequency scaling. However, the on-chip
interconnection network, or network-on-chip (NoC), is also
an important source of power consumption and heat generation. Particularly, the traﬃc ﬂowing through the NoC aﬀects
directly its power and thermal distribution. Unfortunately,
very few works discuss the dynamism of NoC. A key technique for NoC management is to capture its traﬃc patterns
and predict future behaviors. In this paper, we propose a
table-driven predictor called Network Traﬃc Prediction Table (NTPT) for recording and predicting traﬃc in NoC. The
most unique feature of NTPT is its ability to predict end-toend traﬃc, rather than switch-to-switch traﬃc. Thus, more
application behaviors can be captured and monitored. Evaluations on Tilera’s TILE64 show that NTPT has very high
prediction accuracy. Analyses also show that it incurs a low
area overhead and is very feasible.
Categories and Subject Descriptors
B.4 [Input/output and data communications]: Processors
General Terms
Performance, Design
Keywords
Network-on-chip, Many-core, End-to-End Traﬃc Prediction
1.
INTRODUCTION
With the progress of VLSI technology, the number of cores
on a chip multiprocessor (CMP) keeps increasing. To interconnect the many cores on the chip, perhaps to the thousands, a Network-on-Chip (NoC) is essential. For example,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2010, June 13-18, 2010, Anaheim, California, USA.
Copyright 2010 ACM ACM 978-1-4503-0002-5 /10/06 ...$10.00.
Tilera’s TILE64 uses a 2D mesh network to interconnect 64
general-purpose tiles with supports for explicit tile-to-tile
communication. As NoC becomes one of the most critical
components on a CMP, its design and behavior aﬀect every
aspect of the CMP, from design complexity and chip area
to run-time performance, power consumption, and thermal
distribution. Particularly, NoC is becoming a ma jor source
of power consumption and heat generation on chip [4, 8].
When packets go through a switch of the NoC, power will
be consumed by the switch and heat will be generated. If
it is possible to predict the traﬃc ﬂowing through a speciﬁc switch in the following time interval, we can adjust its
power mode accordingly [7] to save power while maintaining
the performance.
Predicting traﬃc for NoC is challenging, especially from
the perspective of each individual switch. For one thing,
traﬃc generated by one core may be consumed by a core at
the other end of the NoC, going through several switches in
between. The intermediate switches must know the end-toend communications to correctly predict the through traﬃc
for the next time interval. This in turn requires knowledge of
the running application. Previous works on switch-to-switch
traﬃc prediction [6] are limited in that they do not know any
application information and make predictions based only on
the states of the switches, e.g., the buﬀer status. The predictions are at most passive and reactive with a local view.
It is diﬃcult to anticipate changes in traﬃc behaviors, such
as phase changes in the application or bursts in the traﬃc,
and react accordingly in time. In this paper, we focus on
the end-to-end (E2E) traﬃc of the NoC.
Since most prediction techniques are based on past history, given that there is no knowledge of future behavior,
another challenge in predicting traﬃc of NoC is to capture
the past traﬃc behaviors with the minimal resources. In this
work, we propose to use a small table in the network interface (NI) of each tile to record the traﬃc and help prediction.
The proposed Network Traﬃc Prediction Table (NTPT) can
predict the end-to-end traﬃc in a NoC. Inspired by the 2level branch predictor [10], we use a local predictor and a
global predictor. In the local predictor, prediction is based
on the last value. On the other hand, the global predictor
uses a table to record transmission patterns, which are then
used to index the prediction value.
The main challenge of the design for NTPT is to reduce
the size of the tables while maintaining high prediction accuracy. We will discuss in detail how this issue is addressed in
our design. Our discussions are based on the TILE64 architecture, which supports explicit tile-to-tile communication
449
27.4
4. SYSTEM DESIGN
4.1 The Design of NTPT
NTPT is a two-level table designed for predicting the trafﬁc of NoC in the next time interval based on the history.
The design is inspired by the branch prediction table.
In
this subsection, we ﬁrst introduce the notations used and
then describe the design of NTPT.
There are two predictors supported by the NTPT, local
predictor and global predictor. The local predictor makes
predictions based on the last activity saved in counteri,j ,
where i and j are the source and destination task, respectively. The local predictor is usually accurate if the running
application has a stable and consistent behavior. On the
other hand, the global predictor makes predictions based
on the communication history pattern.
It makes diﬀerent
predictions for each tracked communication pattern. NTPT
has a selector to decide which predictor to use at runtime.
Let historyi,j be the record of the communication pattern between tasks i and j in NTPT. Let tablei,j be the
global prediction table, in which each entry is indexed by
historyi,j and contains a prediction value. The prediction
value records the amount of data transmitted when this pattern was encountered last time. If the same pattern appears
again, the recorded value is used as the prediction value. Let
Δ denote the sampling period. The capacity of each link is
W bits/s.
After introducing the notations, we are now ready to describe the design of NTPT by illustrating the mechanisms
for monitoring and predicting the traﬃc produced by a task.
Consider a task i and the task j with which i may communicate, i.e., j ∈ commi . The prediction function can be deﬁned
as follows:
(cid:2)
pt (i, j ) =
ct−1 (i, j ),
tablei,j (historyi,j ),
s(i, j ) = 0;
s(i, j ) = 1.
Recall that pt (i, j ) is the predicted traﬃc from task i to
task j at the t-th interval. The prediction either comes from
the local predictor or the global predictor, determined by a
selector function s. The selector function can be designed
according to the system requirements. In this paper, we use
a 2-bit saturating counter.
Each entry in the NTPT has a counter for recording the
total size of transmitted data in the current time interval, a
shift register for tracking the history of traﬃc pattern, and
a ﬂag indicating whether there is an outgoing traﬃc in the
current time interval. The traﬃc pattern is updated with
the latest traﬃc behavior (i.e., the transmission ﬂag) at the
end of each time interval. The traﬃc pattern is used to index
a global history table. The global history table stores the
prediction for each diﬀerent traﬃc pattern.
When task i generates an outgoing traﬃc to task j, the
NTPT records the size of the transmitted data and the destination, and sets the transmission ﬂag indicating that there
is an outgoing traﬃc in the current time interval. At the end
of the current time interval, the NTPT shifts the transmission ﬂag into the traﬃc pattern shift register and makes a
traﬃc prediction for the next time interval either by the local
predictor or the global predictor. The prediction for the size
of the data to be transmitted is a pure last-value prediction,
i.e., the size is the same as in the last time interval.
Besides, the resolution for the size of the transmitted data
through the iLib library. Traﬃc from implicit communications such as the shared L2-cache and coherence traﬃc are
not considered. The proposed technique is very general and
should be applicable to other CMP architectures. With the
traﬃc prediction from NTPT, we can perform various operations to improve CMP performance and power consumption:
• End-to-end ﬂow control by controlling the injection
rate.
• Setting appropriate power modes of the switches.
• Modeling thermal distribution of a chip taking both
cores and switches into consideration.
• Optimizing inter-core communications at runtime.
This paper is organized as follows. Section 2 discusses
related works. In Section 3, we give a formal deﬁnition of
our problem. In Section 4, we introduce the design of our
traﬃc prediction scheme and several techniques for reducing
the size of the tables. Section 5 presents evaluations of our
design in terms of prediction accuracy and eﬀects of design
parameters. Finally, conclusions of the paper and suggestions for future works are given.
2. RELATED WORKS
A general strategy for predicting the future is to capture the past behaviors. Most existing works rely on simple
counters for capturing past behaviors, due to the simplicity and low area overhead of such counters.
In [3], counters and tables are used to identify whether running tasks
are computation- or memory-bound. Tables have also been
used widely in branch prediction and cache prefetching [5].
The tables can capture both temporal and spatial behaviors
for accurate future predictions. Unfortunately, even though
prediction tables have been applied successfully and widely
to predict branches and cache accesses, there are very few
works on predicting traﬃc in NoC. In [6], information exchanged between adjacent switches is used to predict the
pressure of the traﬃc and make ﬂow control decisions. The
prediction is from the switch perspective and does not take
account of the application behaviors. As far as we know,
NTPT is the ﬁrst work to address end-to-end traﬃc prediction for NoC from the perspective of applications.
3. PROBLEM FORMULATION
mesh topology. The size of the mesh network is M ×M . Note
For simplicity of discussion, we assume an NoC with a 2D
that our solution is general and independent of the underlying topology. We also assume that each core runs one task.
Thus, the terms task and core will be used interchangeably
in the following discussions. Again, this assumption can be
easily relaxed by duplicating the NTPT. The application
running on the CMP can be represented as a task graph,
denoted as T . The task graph consists of a set of tasks, each
runs on one core. Let commi denote the set of tasks which
task i may communicate to, i.e., commi = {j |i → j }. Let
pt (i, j ) be the traﬃc predicted by NTPT to ﬂow from task i
to j at the t -th time interval, where i, j ∈ T and j ∈ commi .
Note that this is end-to-end traﬃc. Let ct (i, j ) be the actual traﬃc from task i to j at the t-th time interval. The
goal of traﬃc prediction is to make the diﬀerence between
pt (i, j ) and ct (i, j ) as small as possible, i.e., the prediction
is as accurate as possible.
450
27.4
can be set by the quantizer G bits/unit according to the
is to set G to W . In this way, (cid:4)ct (i, j )/(G · Δ)(cid:5) is either 0 or
application. It follows that the most coarse-grained record
1 for some task i and j at t-th time period, indicating the
link either transmits or not at all. When we are going to
do ﬁne-grained prediction, we can set a smaller G for ﬁner
adjustments.
4.2 Practical Implementation Details
NTPT can be implemented as a two-level hierarchical table. The ﬁrst-level table is indexed by the id of the destination core. Each entry contains the number of packets
transmitted most recently and the recent traﬃc patterns.
The second-level table is indexed by the history of the traﬃc
patterns, and each entry contains the predicted transmission
rate.
Due to space limitation, the technical details for reducing
the table size and estimating the area overhead are omitted
here. Main techniques used are listed as follows and interested readers are referred to the technical report [2].
(1)
Reducing the size of the ﬁrst-level table, (2) Sharing the
second-level table, (3) Quantizing the values in the secondlevel table, (4) Quantizing the size of transmitted data, and
(5) Entry replacement in the second-level table.
Assume that the NoC has a topology of 5 × 5 mesh. Each
network interface thus needs to maintain a table with 52
entries. After applying all the reduction strategies above
and using LRU replacement in the 2nd-level table, we can
show that the table can be reduced to 56 bytes.
5. EVALUATION
In this section, we evaluate the accuracy of our NTPT
traﬃc prediction method using diﬀerent time interval settings, and compare our method with local and global predictors.
5.1 Methodology
We use Tilera’s TILE64 as the evaluation platform [1]. For
each NTPT in the system, we use a dedicated tile to simulate its behavior, such as tracking outgoing traﬃc and updating the communication pattern register. In other words,
if we are going to evaluate NTPT on a CMP with n tiles,
we will use n extra tiles in TILE64 for simulating the behavior of corresponding NTPTs. We deﬁne WORKER as
the set containing the tiles which are running the application and UPDATER as the set containing the tiles which
are simulating the behavior of NTPT. The relation between
the tiles in WORKER and the tiles in UPDATER is deﬁned as follows: uti ∈ UPDATER is updating the NTPT of
wtj ∈ WORKER, if i = j .
Our evaluation uses a modiﬁed blocked LU decomposition
kernel from the SPLASH-2 suite [9] as the benchmark. The
blocked LU decomposition kernel from SPLASH-2 is a dataparallel shared-memory program. For our evaluation, we
made two modiﬁcations to the benchmark. First, in the initialization stage, we make each tile in WORKER to allocate
and cache its own portion of the data memory space. Second, when wti needs data which belongs to wtj , we use the
iLib tile-to-tile message passing API to transmit the data
from wtj to wti . Meanwhile, a notiﬁcation containing the
information of this transmission will be sent to utj for NTPT
simulation.
The number of arrays in the LU decomposition kernel is
451
ϭϬϬй
ϵϬй
ϴϬй
ϳϬй
ϲϬй
ϱϬй
ϰϬй
ϯϬй
ϮϬй
ϭϬй
Ϭй
ϭϬϬϬ ϵϬϬ ϴϱϬ ϴϬϬ ϳϱϬ ϳϬϬ ϲϱϬ ϲϬϬ ϱϱϬ ϱϬϬ ϰϱϬ ϰϬϬ ϯϱϬ ϯϬϬ ϮϱϬ ϮϬϬ ϭϱϬ ϭϬϬ ϱϬ
hƉĚĂƚĞdŝŵĞ/ŶƚĞƌǀĂů;DŝůůŝŽŶĐǇĐůĞƐͿ
'ůŽďĂůWƌĞĚŝĐƚŽƌ
>ŽĐĂůWƌĞĚŝĐƚŽƌ
Figure 1: The ratio of using global predictor and
local predictor under diﬀerent time intervals.
set as 50, and each of them is a 512 × 512 64-bit ﬂoating
is separated into 1024 16 × 16 subarrays and dispatched to
point array generated randomly at initialization. Each array
16 tiles for parallel processing.
5.2 The Impact of the NTPT Update Time Interval
Update time interval setting is a critical factor aﬀecting
the prediction accuracy and the applicability of the prediction results.
In this experiment, we try to ﬁnd a suitable
time interval for our benchmark program. A suitable time
interval should be able to capture the communication behavior embedded in the application logics and predict the
network traﬃc with acceptable accuracy. We have tested
the time interval settings ranging from 1,000,000,000 cycles
to 50,000,000 cycles, as shown in Figure 1.
While using larger time interval settings (over 800,000,000
cycles), the prediction error rate is relatively low, but the
global predictor is rarely selected. Although we can get a
high prediction accuracy by using large time interval settings, the low global predictor usage means that NTPT does
not observe much variation in the communication behavior.
This indicates that it misses the dynamism of the NoC trafﬁc and thus the prediction is not applicable. Similar results
can be observed if applying relatively small time interval
settings (under 50,000,000 cycles).
For our benchmark, with the time interval settings ranging from 100,000,000 cycles to 400,000,000 cycles, the global
predictor is frequently selected. This indicates that NTPT
observes a varying communication behavior, and the prediction can tell us whether the NoC is busy or not. In summary,
the time interval settings ranging from 100,000,000 cycles to
400,000,000 cycles allow NTPT to capture the communication behavior inherited in our benchmark program.
5.3 Comparison between Local and Global Predictors
In this subsection, we evaluate the performance of our
NTPT-based predictor by comparing with a local predictor and a global predictor. The prediction error rates of
the three diﬀerent predictors with diﬀerent update interval
settings are shown in Figure 2. The local predictor (diamond dotted line) suﬀers from high error rates using update
27.4
Ğ
ƚ
Ă
Z

ƌ
Ž
ƌ
ƌ

Ŷ
Ž
ŝ
ƚ
Đ
ŝ
Ě
Ğ
ƌ
W
ϱϬ͘ϬϬй
ϰϱ͘ϬϬй
ϰϬ͘ϬϬй
ϯϱ͘ϬϬй
ϯϬ͘ϬϬй
Ϯϱ͘ϬϬй
ϮϬ͘ϬϬй
ϭϱ͘ϬϬй
ϭϬ͘ϬϬй
ϱ͘ϬϬй
Ϭ͘ϬϬй
ϭϬϬ
ϮϬϬ
ϯϬϬ
ϰϬϬ
ϱϬϬ
ϲϬϬ
ϳϬϬ
ϴϬϬ
ϵϬϬ
ϭϬϬϬ
hƉĚĂƚĞdŝŵĞ/ŶƚĞƌǀĂů;DŝůůŝŽŶǇĐůĞƐͿ
>ŽĐĂůWƌĞĚŝĐƚŽƌ
'ůŽďĂůWƌĞĚŝĐƚŽƌ
EdWdͲďĂƐĞĚWƌĞĚŝĐƚŽƌ
Figure 2: Prediction error rate of the three types of
predictors.
time intervals ranging from 150,000,000 to 650,000,000 cycles, because the communication traﬃc with these time interval settings varies widely as monitored by NTPT. While
using larger time intervals, the local predictor has a good
prediction accuracy since the communication patterns are
almost the same, again as shown in each NTPT update.
On the other hand, the global predictor (square dotted
line) performs well in all the evaluated update time intervals. The global predictor can work as a local predictor when
the two entries of 00000000 and 11111111 have been ﬁlled
in the 2nd-level table, which stand for the communication
patterns of no transmission and transmit at al l times. This
results in high accuracy predictions in low-variance communication patterns. The global predictor, by its nature, is also
capable of predicting accurately in high-variance communication patterns.
However, the global predictor uses more memory space
than the local predictor. The accuracy of the global predictor is determined mainly by the number of patterns that
can be tracked in the 2nd-level table. Currently, we have
not limited the size of the 2nd-level table, and therefore all
the patterns can be tracked and used to make predictions.
But in a practical implementation, the 2nd-level table may
not be large enough to keep all the patterns throughout the
application execution. The accuracy will be aﬀected greatly
by the replacement policy. When a longer global history is
needed, the number of total tracked patterns (the diﬀerence
between the global predictor and the NTPT-based predictor) will increase as Table 1 shows.
To summarize, the NTPT-based predictor (triangle dotted line) adaptively selects a local predictor in low-variance
traﬃc and a global predictor for high-variance traﬃc. Moreover, the experiments show that the NTPT uses fewer entries than the global predictor and thus incurs a lower space
overhead.
6. CONCLUSIONS
In this paper, we propose a two-level table design called
Network Traﬃc Prediction Table (NTPT) for predicting endto-end traﬃc of the NoC. The design is introduced and the
area overhead is analyzed. For evaluation, we port the LU
decomposition in SPLASH-2 with NTPT simulation to the
TILE64 platform. The prediction accuracy of the proposed
Table 1: Total patterns tracked by pure global
predictor and NTPT-based predictor by running
a modiﬁed SPLASH-2 blocked LU decomposition
benchmark with 16 tiles and 600,000,000 update
time interval.
Global History Length Global Predictor NTPT
4
92
26
6
132
37
8
218
60
10
233
75
12
275
87
14
346
95
16
407
167
NTPT method is then evaluated based on diﬀerent time interval settings and by comparing with pure local and global
prediction. The results show that NTPT performs well. In
the future, we will apply NTPT to adjusting the link frequencies for reducing the power consumption in NoC.
Acknowledgements
This work is funded by the Industrial Technology Research
Institute and National Science Council grant NSC 98-2220E-007-019.
7. "
Application-aware NoC design for efficient SDRAM access.,"In this paper, we propose an application-aware networks-on-chip (NoC) design for efficient SDRAM access. In order to provide short latency for priority memory requests with few penalties, a packet is split into several short packets which then are scheduled by the proposed flow controller in a router. Moreover, our NoC design further improves memory performance by matching application access granularity to SDRAM access granularity. Experimental results show that our application-aware NoC design improves on average 32.7% memory latency for latency-sensitive cores and on average 3.4% memory utilization compared to.","Application-Aware NoC Design for Efficient SDRAM Access 
Wooyoung Jang and David Z. Pan 
Department of Electrical and Computer Engineering 
University of Texas at Austin 
wyjang@cerc.utexas.edu, dpan@ece.utexas.edu 
27.5
ABSTRACT 
In this paper, we propose an application-aware networks-on-chip 
(NoC) design for efficient SDRAM access. In order to provide 
short latency for priority memory requests with few penalties, a 
packet is split into several short packets which then are scheduled 
by the proposed flow controller in a router. Moreover, our NoC 
design further improves memory performance by matching 
application access granularity to SDRAM access granularity. 
Experimental results show that our application-aware NoC design 
improves on average 32.7% memory latency for latency-sensitive 
cores and on average 3.4% memory utilization compared to [1]. 
Categories and Subject Descriptors 
C.2.1 
[COMPUTER-COMMUNICATION NETWORKS]: 
Network Architecture and Design - Packet-switching networks. 
General Terms 
Algorithms, Performance, Design. 
Keywords 
NoC, on-chip communication, flow control, router, memory, QoS 
1. INTRODUCTION 
In many-core processors based on networks-on-chip (NoC), 
memory quality-of-service (QoS) becomes one of the most 
important issues since both memory and on-chip network are 
critical shared resources. However, the improvement of memory 
performance aided by a memory subsystem independently 
working with an on-chip network is severely limited. Therefore, 
memory-aware NoC design has attracted great attentions.  
Many researchers have developed various QoS approaches [25] for NoC. However, they are not optimized for memory requests 
that cause the longest latency. Recently, the responsibility for 
high memory performance has been shared not only with memory 
subsystems but also with on-chip networks. In [1], each NoC 
router instead of memory subsystems schedules memory requests 
for a best-effort memory service. As a result, since memory 
requests arrive at a memory subsystem in the order more friendly 
to SDRAM operations, average memory latency and utilization 
(defined as the number of clock cycles used for data transfer 
divided by the number of total clock cycles) greatly improve with 
lower NoC design cost. However, as different applications 
generate their specific memory requests with various latency 
constraints and packet lengths, [1] needs to support various 
priority services and match application access granularity to 
SDRAM access granularity.  
In this paper, we propose an application-aware NoC design for 
efficient SDRAM access. Our key motivations are two-fold. First, 
different applications request various SDRAM latencies. For 
example, 
demand memory 
requests 
generated 
by 
a 
microprocessor are commonly served as a priority packet since 
the microprocessor may halt until the demand memory request is 
served. However, since the priority packet is served first by 
network routers which do not consider SDRAM operations, there 
is strong possibility to meet bank conflict and data contention 
which makes memory performance worse. In addition, a long 
best-effort packet prevents a priority packet being served fast. 
Therefore, a priority service which is efficient in accessing 
SDRAM should be provided and long best-effort packets should 
be split to short packets and then served. Second, different 
applications request various lengths of SDRAM data whereas 
DDR I/II SDRAM always generates fixed-length data. Even if 
DDR III SDRAM can generate variable-length data, it has few 
advantages due to long tCCD (CAS to CAS delay time) [6]. As a 
result, if the length of data requested by applications is neither the 
same as the length of data served by SDRAM nor a multiple of 
the length of data served by SDRAM, unnecessary data may be 
accessed and then thrown away. Therefore, the access granularity 
mismatch problem should be considered. Based on 
these 
motivations, the major novelty and contribution of this paper 
include the following. 
y A guaranteed SDRAM service (GSS) flow controller is 
proposed for applications sensitive to SDRAM latency, which 
provides various priority services with few penalties. 
y An SDRAM access granularity matching (SAGM) NoC 
design is proposed. Based on SDRAM access granularity, a 
packet is split to short fixed-length packets and then scheduled by 
our GSS flow controller. In addition, our memory subsystem uses 
a partially open-page and an auto-precharge (AP) mode.  
y We show that our application-aware NoC design significantly 
improves not only total memory utilization but also memory 
latency for a priority packet.  
2. PROBLEM DESCRIPTION 
2.1 Priority SDRAM Service in NoC 
A microprocessor commonly generates a demand request and a 
prefetch request. A demand request should be served as soon as 
possible since a microprocessor may stall until a service of the 
demand request is received. On the other hand, a prefetch request 
does not need to be served with a priority since it may be not 
promptly used. Memory requests of multimedia processors and 
peripherals are also handled similarly to a prefetch request.  
Fig. 1(b) and (c) show two different approaches as to how to 
treat a priority request with respect to others, where two demand 
requests, two prefetch requests and two requests by a video 
processor are filled in input buffers of an NoC router as shown in 
Fig. 1(a). BA means a bank address and all requests are read 
operations. In addition, RAs (Row Addresses) of all requests are 
different except for prefetch 2 and request 2. 
A memory scheduler in Fig. 1(b) regards a priority memory 
request to have the same priority as others and then schedules 
memory requests to avoid bank conflict and data contention and 
to encourage row-buffer hit and bank interleaving [1]. As a result, 
whereas all memory requests are successively executed with no 
bank conflict, the execution of demand 2 is considerably delayed. 
On the other hand, in Fig. 1(c), demand memory requests are 
executed with a priority. However, since demand 2 accesses the 
453
 
 
 
 
dem 1: BA 1
pref 1: BA 2
req 1: BA 3
pref 2: BA 4
req 2: BA 4
dem 2: BA 1
BI
BI
dem 1
pref 1
req 1
BI
RBH
pref 2
req 2
BI (Bank Interleaving)
dem 2
CPU halts
longer
(b) Priority-equal scheduling (best effort service)
bank conflict
BI
BI
dem 2
pref 1
req 1
BI
RBH
pref 2
req 2
(c) Priority-first scheduling
BI
BI
BI
BI
dem 1
pref 1
req 1
dem 2
increased
excution cycles
RBH (Row-Buffer Hit)
pref 2
req 2
frist
come
dem 1
(a) Input buffer of router
or memory subsystem
(d) Our approach:
      hybrid of priority-equal and priority-first scheduling
Figure 1: Examples of scheduling memory requests 
same bank as demand 1 access with different RA, bank conflict 
happens. It makes any data not delivered while a row buffer of 
bank 1 becomes idle and is filled with data of demand 2. 
Consequently, total execution time of six requests is longer.  
We propose a hybrid scheduling algorithm which achieves the 
same memory utilization as priority-equal scheduling and the 
same memory latency for demand requests as priority-first 
scheduling as shown in Fig. 1(d). The scheduling is performed by 
a flow controller in each NoC router which is similar to [1]. 
Moreover, we consider a long best-effort packet. In winnertake-all bandwidth allocation, it causes a priority packet to be 
severely delayed. The reason is that if a long best-effort packet is 
already scheduled, a priority packet should wait until the long 
best-effort packet finishes being delivered. To solve this problem, 
packets are split to short fixed-length packets and then scheduled. 
A length of short packets is determined by SDRAM access 
granularity. Consequently, priority packets can get more 
opportunities to get a channel.  
2.2 SDRAM Access Granularity Mismatch 
SDRAMs transfer or receive fixed-length data (= number of 
data bit ൈ burst length) per read/write. DDR I SDRAM has burst 
length (BL) 2, BL4 and BL8 modes and DDR II/III SDRAM has 
BL4 and BL8 modes. Especially, DDR III SDRAM has an 
additional selectable BL4/BL8 on-the-fly (OTF) mode. For 
example, if SDRAM with 16-bit data bus is set to a BL8 mode, it 
always generates 16 bytes per read/write. On the other hand, 
applications or cores request various data lengths to SDRAM. For 
example, H.264 [7] decoders request 4, 8 or 16 bytes per row for 
motion compensation to SDRAM. If H.264 decoder reads 4 or 8 
bytes, the rest of data are thrown away.  
Simple solutions are to reduce the number of data bit or to use 
short BL. If the number of data bit is changed to 4 bits, there is no 
useless data. However, the entire system does not have enough 
memory bandwidth. Therefore, we use short BL to match access 
granularity. That is, SDRAM is set to a short BL mode and a 
packet is split to short packets based on the BL, which is also 
related to Section 2.1. To support a short BL mode, our memory 
subsystem operates with a partially open-page and AP mode.  
3. APPLICATION-AWARE NOC DESIGN 
3.1 GSS Flow Controller  
In this section, we propose a GSS flow controller providing 
various priority services and achieving similar memory utilization 
to a best-effort scheduler. Let h(n) be a packet, which is already 
allocated a channel by our GSS flow control at the nth arbitration. 
Let hi(n+1) be any packet i of all completing packets, H(n+1) that 
may be allocated the same channel as h(n) by our flow controller 
at the (n+1)th scheduling. The packets, h(n)  and hi(n+1) contain 
an address and a command to access SDRAM, denoted by (RAn, 
BAn, R/Wn) and (RAn+1,i, BAn+1,i, R/Wn+1,i), respectively, where 
the notations are (row address, bank address, read/write 
27.5
Algorithm 1 Flow Control for GSS 
if new packet hk(n+1) comes in router then 
for hi(n+1) א H(n+1) do 
ti ĸ ti+1; 
if hk(n+1) is priory packet, hi(n+1) is best-effort packet and 
BAn+1,i=BAn+1,k then  
 hi(n+1) is except from H(n+1); 
if hk(n+1) is priority packet then 
tk ĸ 2 to 5; // PCT for priority packet 
else 
tk ĸ 1; // for best-effort packet 
if h(n) is done then 
    for hi(n+1) א H(n+1) do 
Ti(ti) in Fig. 2 ĸ hi(n+1); 
Ti(0) in Fig. 2 ĸ hi(n+1); 
    if SPPCT = ׎ then 
for hi(n+1) א H(n+1)  do 
ti ĸ ti+1; 
go to line 11; 
1:  
2: 
3: 
4: 
5: 
6: 
7: 
8: 
9: 
10:
11:
12:
13:
14:
15:
16:
17:
command). Thus, bank conflict, bank interleaving and row-buffer 
hit are defined as (BAn=BAn+1,i and RAnRAn+1,i), (BAnBAn+1,i) 
and (BAn= BAn+1,i and RAn=RAn+1,i), respectively. Based on these 
notations and definitions, algorithm 1 shows how our flow 
controller works for the GSS, which consists of two parts.  
First, any packet (i) is given a token (ti) depending on its input 
order and priority (line 1-9). Let a new packet, hk(n+1) come in a 
router. All old packets are given to one additional token (line 3). 
Then, if the new packet has a priority, old best-effort packets 
accessing the same bank as the priority packet are except from 
H(n+1). It means that best-effort packets which access the same 
bank as that of the priority packet are not scheduled until the 
priority packet is scheduled. Then, the new packet gets an initial 
token. If it is a best-effort packet, one token is given (line 9). 
Otherwise, any two to five tokens are given (line 9), called PCT 
(Priority Control Token). If one token is given to the priority 
packet, it is a priority-equal scheduler and if five tokens are given 
to the priority packet, it is a priority-first scheduler.  
Second, when h(n) finishes being delivered, competing packets, 
hi(n+1) in a router are scheduled (line 10-17). They are input to 
Fig. 2 according to the number of token a packet has. That is, if 
any packet has 1, 2, 3, 4 or 5 tokens, the packet is input to Ti(1), 
Ti(2), Ti(3), Ti(4) or Ti(5), respectively (line 12). All packets are 
also input to Ti(0) in line 15. If there is no packet passing the filter 
(line 14), all packets are given one additional token (line 16) and 
then filtered again (line 17).  Finally, if there are some packets 
passing 
the filter, one among 
the packets 
is output 
to 
SPPCT(Scheduled Packet). If PCT is n in line 7, SPn is used in Fig. 
2 where To(ti) is the filtered output of Ti(ti). SPn=A?B?C means A 
is chosen if A is not 0. If A is 0 and B is not 0, B is selected. 
Finally, if both A and B are 0 and C is not 0, C is chosen.  
In our NoC design, normal packets are not scheduled by our 
GSS flow controller. That is, our GSS flow controller for memory 
packets and a conventional flow controller for other packets are 
parallelly performed. Then, two resulting packets are scheduled 
by a 2-input conventional flow controller. Therefore, other normal 
packets are not delayed by our flow controller. 
Ti(5)
Ti(4)
Ti(3) Ti(2)
Ti(1)
Ti(0)
Bank conflict
& read to
write access?
Read to write access?
no
Write to read acess?
no
Read to write access?
no
Write to read acess?
no
Row-buffer hit?
yes
no
Bank conflict?
no
SP5 = To(5)&P ? To(0) ? To(5) ? To(4) ? To(3) ? To(2) ? To(1); // PCT=5
SP4 = To(5)&P ? To(4)&P ? To(0) ? To(5) ? To(4) ? To(3) ? To(2) ? To(1); // PCT=4
SP3 = To(5)&P ? To(4)&P ? To(3)&P ? To(0) ? To(5) ? To(4) ? To(3) ? To(2) ? To(1); // PCT=3
SP2 = To(5)&P ? To(4)&P ? To(3)&P ? To(2)&P ? To(0) ? To(5) ? To(4) ? To(3) ? To(2) ? To(1); // PTC=2
Figure 2:  Filtering packets for SDRAM scheduling 
454
 
27.5
3.2 SAGM NoC Design  
In NoC designs, it is useful to split a packet into shorter packets 
since on-chip network resources can be more efficiently reserved. 
In the newest video system, a length of packets requested by a 
video encoder/decoder gets shorter whereas a length of packets 
requested by a video enhancer gets longer. A long best-effort 
packet causes a priority packet to be further delayed since a 
priority packet wait until the long best-effort packet finishes 
transferring. On the other hand, a short packet causes SDRAM 
utilization to be deteriorated since most SDRAMs always 
generates longer fixed-length data than the short packet. Thus, the 
optimal packet length can greatly improve memory performance.  
We split a packet to shorter packets considering an SDRAM 
access granularity. Since our routers communicate through OCP 
(Open Core Protocol) [8] or AMBA protocol [9] widely used, 
packets consist of only body flits. Instead, information in head 
and tail flits is included in additional controls and address buses. 
Therefore, network loads do not increase whereas the number of a 
core or a different router interconnected to each router is limited.  
DDR I/II SDRAM always transfers/receives fixed-length data 
per read/write operation. Most memory subsystems prefer a BL8 
mode in DDR I/II SDRAM since BL2 and BL4 modes can cause 
command efficiency to be worse critically. As shown in Fig. 3, let 
a PRE command for BA1 and a CAS command for BA2 issued to 
SDRAM at the same time. In Fig. 3(a), the PRE command is 
performed earlier than the CAS command. Consequently, data of 
Packet 2 are written with some delays. In Fig. 3(b), the CAS 
command is performed earlier than PRE command. Consequently, 
the bank 1 gets idle with some delays. Fortunately, SDRAM can 
omit a PRE command if a CAS command is executed with AP. 
Consequently, the PRE command and the CAS command are not 
delayed as shown in Fig. 3(c).  
Under this operation, it is useful that a BL (granularity) of besteffort packets is 2 and a BL mode in DDR I/II SDRAM is set to 4. 
Now that DDR III SDRAM has a selectable BL4/BL8 OTF mode, 
it is useful that a BL (granularity) of packet is 4. For example, if a 
BL of any packet is 9, it is split to five packets which BLs are 2, 2, 
2, 2 and 1 for DDR I/II SDRAM and three packets which BLs are 
4, 4 and 1 for DDR III SDRAM. It is efficient not only to match 
the access granularity but also to serve a priority packet faster in 
winner-take-all bandwidth allocation. If the length of best-effort 
packet is 9, a priority packet waits until all 9 BLs of the packet 
are transferred. On the other hand, if it is split to the short packets, 
a priority packet just waits until 2, 2 and 4 BLs of the short packet 
are transferred in DDR I, II and III SDRAM, respectively. 
To implement this idea, we make a core generate short packets 
and the last packet has a tag to execute AP. Since the relation of 
 DDR II SDRAM @200MHz (BL4 mode)
clock
packet
command
data
command
data
read, 64 bits
BA1
write, 128 bits
BA2
tRCD
RAS
BA1
RAS
BA2
CAS
BA1
tRP
PRE
BA1
CAS
BA2
CL
R
1
R
2
R
3
R
4
(a) Delay of CAS  command (BA2)
RAS
BA1
CAS
BA2
W
1
W
2
W
3
W
4
W
1
W
2
W
3
W
4
RAS
BA1
RAS
BA2
CAS
BA1
CAS
BA2
PRE
BA1
CAS
BA2
RAS
BA1
R
1
R
2
R
3
R
4
W
1
W
2
W
3
W
4
W
1
W
2
W
3
W
4
(b) Delay of PRE command (BA1)
command
RAS
BA1
RAS
BA2
CAS
BA1
CAS
BA2
CAS
BA2
RAS
BA1
data
autoprecharge
R
1
R
2
R
3
R
4
W
1
W
2
W
3
W
4
W
1
W
2
W
3
W
4
(c) No delay of command
Figure 3:  SDRAM Operations when BL=4 
split packets is row-buffer hit, there is no loss of memory 
utilization. Our router proposed in Section 3.1 prefers row-buffer 
hit to bank interleaving even if both cause no loss of memory 
utilization. Therefore, if best-effort packets split do not meet any 
priority packet, they are delivered successively.  
Fig. 4 is our memory subsystem only with a memory command 
generator. It makes DDR SDRAM work for a partially open-page 
mode. A bank keeps activating (open-page) after an access of 
packets without any tag indicating the last packet. However, if a 
bank is accessed by a packet with a tag, the bank is deactivated 
(closed-page) by AP. In addition, when a priority packet is bank 
conflict with a previous best-effort packet, the bank may be 
closed even if the previous best-effort packet has no tag.  
A packet that is input to our SDRAM command generator is 
decoded to extract SDRAM access information such as BA, RA, 
CA (Column Address), a length of data and a type of command. 
Then, they are stored in a PRE buffer. A PRE buffer issues a PRE 
command only when a priority packet meets bank conflict with an 
open bank accessed. Then, the packet is stored to a RAS buffer. A 
RAS buffer issues a RAS command if a packet does not have the 
relation of a row-buffer hit with an open bank accessed. Then, the 
packet is stored in a CAS buffer. A CAS buffer always issues a 
read/write command. If a tag is attached to a packet, a command 
is executed with AP. Finally, an SDRAM command controller 
schedules all PRE, RAS and CAS commands and generates 
SDRAM interface signals.  
Input
Output
PRE buffer
RAS buffer
CAS buffer
SDRAM
command
controller
Control
Address
Data
Figure 4:  SDRAM Command Generator 
4. EXPERIMENTAL RESULTS 
Our application-aware NoC design is implemented in a Verilog 
hardware description language, where DDR I/II SDRAM are set 
to a BL4 mode and DDR III SDRAM is set to a selectable BL4/ 
BL8 OTF mode. It is compared to [1] and conventional NoC 
including a round-robin based NoC router and a full memory 
subsystem, called CONV. A full memory subsystem employs a 
design concept from Sonics’ MemMax [10] and Denali’s 
Databahn [11] which are an SDRAM scheduler and an SDRAM 
command generator, respectively. Moreover, a conventional NoC 
design and [1] use a BL8 mode in a memory subsystem.  
All NoC designs are applied to a Blu-ray [7], a single DTV [1] 
and a dual DTV model, which consist of 9, 9 and 16 subsystems, 
respectively. They are mapped to a 3ൈ3, 3ൈ3 and 4ൈ4 mesh grid 
by [12], respectively. All simulations run for one million cycles. 
4.1 No Priority Memory Request 
Our application-aware NoC design is experimented when there 
is no priority packet. Our NoC design is implemented to two 
versions. One is that a GSS flow controller is just employed and 
the other is that both GSS flow controller and SAGM NoC design 
are employed, called GSS and GSS+SAGM, respectively. Table 1 
shows their memory performance.  
Our GSS flow controller achieves slightly better average 
memory utilization and latency than [1]. On the other hand, the 
GSS flow controller shows slightly worse latency of demand 
packets compared to [1]. However, the latency of demand packets 
is not important since demand packets are not assigned to a 
priority packet. Our NoC design employing a GSS flow controller 
and an SAGM design further improves memory performance. 
455
 
27.5
Table 1: Comparison on Benchmarks without Priority Memory Request 
Memory utilization 
 [1] 
GSS 
0.763 
0.771 
0.691 
0.717 
0.592 
0.600 
0.737 
0.766 
0.673 
0.715 
0.554 
0.577 
0.707 
0.708 
0.627 
0.627 
0.559 
0.531 
0.656 
0.668 
1.000 
1.018 
Avg. latency of all packets (cycle) 
Avg. latency of demand packets (cycle)
GSS+
GSS+
GSS+
SAGM CONV
 [1] 
GSS 
SAGM CONV 
 [1] 
GSS 
SAGM
0.774 
121 
81 
74 
69 
111 
63 
65 
60 
0.761 
157 
109 
101 
86 
153 
91 
89 
74 
0.619 
216 
134 
140 
131 
216 
113 
124 
113 
0.776 
144 
101 
86 
71 
140 
80 
74 
61 
0.756 
173 
120 
108 
91 
171 
96 
94 
77 
0.596 
244 
154 
143 
140 
248 
126 
127 
119 
0.712 
154 
104 
89 
80 
128 
73 
67 
57 
0.682 
246 
149 
141 
115 
196 
107 
104 
85 
0.547 
364 
191 
195 
184 
266 
133 
144 
128 
0.691 
202 
127 
120 
107 
181 
98 
99 
86 
1.054 
1.591 
1.000 
0.942 
0.846 
1.847 
1.000 
1.007 
0.878 
CONV 
0.755 
0.651 
0.505 
0.717 
0.625 
0.463 
0.696 
0.555 
0.426 
0.599 
0.914 
Table 2: Comparison on Benchmarks with Priority Memory Request 
Bench
mark 
DDR 
DRAM 
Blu-ray 
Single 
DTV 
Dual 
DTV 
133MHza 
266MHzb 
533MHzc 
166MHza 
333MHzb 
667MHzc 
200MHza 
400MHzb 
800MHzc 
Average 
Ratiod 
Bench
mark 
Blu-ray 
DDR 
DRAM 
Memory utilization 
Avg. latency of all packets (cycle) 
CONV 
[1]+ 
GSS+
CONV 
[1]+ 
GSS+
GSS 
GSS 
+PFS 
PFS 
SAGM
+PFS 
PFS 
SAGM
133MHza 
0.729 
0.742 
0.77 
0.774 
141 
106 
77 
72 
266MHzb 
0.612 
0.621 
0.699 
0.75 
176 
134 
112 
96 
533MHzc 
0.454 
0.517 
0.561 
0.608 
248 
166 
151 
138 
166MHza 
0.676 
0.699 
0.755 
0.779 
163 
124 
96 
76 
333MHzb 
0.58 
0.613 
0.684 
0.738 
192 
143 
116 
107 
667MHzc 
0.387 
0.489 
0.534 
0.559 
309 
182 
158 
151 
200MHza 
0.655 
0.675 
0.7 
0.709 
183 
124 
103 
80 
400MHzb 
0.521 
0.577 
0.608 
0.657 
280 
178 
153 
127 
800MHzc 
0.405 
0.481 
0.518 
0.53 
389 
252 
210 
207 
Average 
0.558 
0.602 
0.648 
0.678 
231 
157 
131 
117 
Ratiod 
0.85 
0.917 
0.987 
1.034 
1.821 
1.233 
1.029 
0.922 
a DDR I SDRAM   b DDR II SDRAM   c DDR III SDRAM  dRatio is based on the SDRAM-aware NoC design [1] 
Single 
DTV 
Dual 
DTV 
Avg. latency of demand packets (cycle)
CONV 
[1]+ 
GSS+
GSS 
+PFS 
PFS 
SAGM
97 
59 
42 
38 
123 
73 
72 
60 
179 
88 
98 
90 
105 
64 
57 
41 
128 
74 
72 
66 
213 
94 
98 
95 
131 
62 
55 
36 
156 
81 
78 
68 
198 
104 
101 
99 
148 
78 
75 
66 
1.508 
0.793 
0.763 
0.672 
Our NoC design 
is synthesized by DesignVision from 
Synopsys with an industrial process technology library. The gate 
count of our NoC design is about 6.2% smaller than [1] in a 3×3 
mesh platform. The reason is that a PRE buffer in our SDRAM 
command generator is smaller than [1] and eight counters per 
router needed in [1] is not used. 
4.2 Priority Memory Request 
Our application-aware NoC design is tested when a demand 
packet is assigned to a priority packet. We implement a 
conventional NoC design and [1] with a priority-first scheduler, 
called CONV+PFS and [1]+PFS, respectively. Table 2 shows 
their memory performance, where the performance ratio is based 
on [1].  
Our application-aware NoC design proves more merits when 
there is a priority packet on NoC. [1]+PFS improves on average 
20.7% latency of priority packets compared to [1]. However, total 
memory utilization and latency are 8.3% and 23.3% worse than 
[1]. On the other hand, our GSS flow controller improves on 
average 23.7% latency of priority packets compared to [1]. Total 
memory utilization and latency are just 1.7% and 2.9% worse 
than [1]. Therefore, our GSS flow controller has fewer penalties 
to support a priority service. 
Our (GSS+SAGM)-algorithm improves not only 32.7% latency 
of priority packets but also 3.4% memory utilization and 7.8% 
memory latency compared to [1]. It also improves 12.7% memory 
utilization, 25.2% latency of all packets and 15.2% latency of 
priority packet on average compared to [1]+PFS.  
5. CONCLUSION 
We propose an application-aware NoC design for efficient 
SDRAM access, which includes a flow controller for GSS and an 
NoC design for SAGM. It greatly improves latency of priority 
memory requests, memory utilization and latency of all packets in 
several industrial video systems. In conclusion, our NoC design 
provides more opportunity for bandwidth-hungry SoC designs 
with a guaranteed priority service. 
6. "
Network on chip design and optimization using specialized influence models.,"In this study, we propose the use of specialized influence models to capture the dynamic behavior of a Network-on-Chip (NoC). Our goal is to construct a versatile modeling framework that will help in the development and analysis of distributed and adaptive features for NoCs. As an application testbench, we use this framework to construct a design methodology for dynamic voltage and frequency scaling (DVFS). We also point out similarities of the proposed model with backpressure mechanisms that could be potentially exploited toward enhanced models for estimation and optimization of NoCs.","Network on Chip Design and Optimization Using
Specialized Inﬂuence Models∗
38.4
Cristinel Ababei
Electrical and Computer Engineering Depar tment
Nor th Dakota State University, Fargo, ND 58108
cristinel.ababei@ndsu.edu
ABSTRACT
In this study, we propose the use of specialized inﬂuence
models to capture the dynamic behavior of a Network-onChip (NoC). Our goal is to construct a versatile modeling
framework that will help in the development and analysis
of distributed and adaptive features for NoCs. As an application testbench, we use this framework to construct a
design methodology for dynamic voltage and frequency scaling (DVFS). We also point out similarities of the proposed
model with backpressure mechanisms that could be potentially exploited toward enhanced models for estimation and
optimization of NoCs.
Categories and Subject Descriptors
B.7.1 [Integrated circuits]: Types and design styles—advanced technologies
General Terms
Algorithms, Design, Performance, Optimization
Keywords
Network on Chip, Inﬂuence model, VFI design style
1. THE INFLUENCE MODEL
The inﬂuence model strips away the speciﬁc details of
component models and oﬀers a simple and analytically tractable
model of the dynamics of systems comprised of n interacting
Markov chains [1]. Chains are associated with vertex sites
(or nodes) and they may diﬀer in their order and structure.
Each site i assumes one of a ﬁnite number mi of possible statuses at each discrete time instant. The model evolves by
updating the statuses of all sites in discrete time. Each site i
changes its status from si [k ] at timek to si [k+1] at time k+1
in accordance to a probability vector pT
i [k + 1] = sT
j [k ]Aj i ,
where sj [k ] is the status of the determining neighboring site
j and Aij is a ﬁxed row-stochastic mj × mi matrix. The
determining site j is selected with probability dij . Probabilities dij deﬁne the network graph associated with the inﬂuence model and they are assembled to form the network
inﬂuence matrix D = [dij ]. The evolution of the inﬂuence
model can be written collectively for all n sites in compact
form as [1]:
pT [k + 1] = pT [k ]H
(1)
This work was supported by the ECE Dept. at NDSU.
∗
where p[k ] is the matrix formed by stacking si [k ] of all n
sites, and H is a matrix whose entry (i, j ) is given by the
product dj iAij , and is generally not a stochastic matrix.
The inﬂuence model with Aij being the same for all n sites
is referred to as the homogeneous inﬂuence model. An example of a network graph associated with a 3 × 3 mesh NoC
topology is shown in Fig.1. The interaction between sites
is captured by the matrix D, while the determining site j
impacts how the current site i changes its status via matrix
Aij . The evolution of the model leads to a natural partitioning (or islanding) of all the nodes. This will be discussed in
the next section.
Figure 1: The network graph associated with a 3 × 3
mesh NoC, as a graphical depiction of the inﬂuence
model. The right hand-side shows the expanded
view of one site, with statuses Low, Medium, and High
usage of the buﬀers of the associated router.
We propose to employ the inﬂuence model in order to
build a NoC modeling framework that is simple enough, and
yet that oﬀers meaningful ways to track the spatially and
temporarily correlated behaviors of the network. Using this
as a basis, we discuss dynamic voltage and frequency scaling
(DVFS) as a pilot application for the proposed model.
2. DYNAMIC VOLTAGE AND FREQUENCY
SCALING
Globally asynchronous locally synchronous (GALS) represents a promising design paradigm to address the globalinterconnect delay problem. It also ﬁts well with the voltagefrequency islands (VFI) design technique introduced for achieving ﬁne-grain system-level power management. In the context of NoCs [2], a recent study showed that VFI can oﬀer
signiﬁcant energy savings [3]: this is achieved by doing centralized static VFI partitioning and voltage/frequency assignment, and then online dynamic voltage and frequency
scaling around the solution found in the ﬁrst step. We propose to take this idea one step further and design a decentralized (i.e., distributed) dynamic VFI partitioning methodology. This will enhance the adaptive and self-healing capabilities of NoC based systems to address changing environmental conditions or to implement fault-tolerant mechanisms. Our idea is to implement a distributed algorithm, based on the homogeneous inﬂuence model
from the previous section, which shall dynamically
625
38.4
perform VFI islanding in response to changes in application traﬃc or of the application itself.
To build the specialized inﬂuence model, one can place
traﬃc monitors on each of the physical channels of the NoC
and use their reading to compute probabilities dij .
Intuitively, the probability dij should be a proportional measure
of the amount of traﬃc between two adjacent routers. Inside
our model, that will mean that from among all neighbors of
a router i, the one that sends the largest amount of traﬃc,
say router j , should have a higher determining impact on
the status of router i.
In other words, if a router is surrounded by routers with increased traﬃc (i.e., higher buﬀer
utilization, status High in Fig.1), its status will likely be inﬂuenced toward having a lot of traﬃc too. Whenever a new
application is implemented by the NoC based system, the
main steps to do distributed VFI partitioning are:
1. The specialized inﬂuence model is initially reset, and then
evolved for a speciﬁed time interval - given by an adaptive
stopping strategy.
2. The model evolution leads to a network islanding that
represents the basis for VFI partitioning (see Fig.2.a).
3. Voltages and frequencies are assigned to islands so that
the application is geared toward an energy-saving mode
while satisfying performance constraints1 (see Fig.2.b).
(a)
(b)
Figure 2: (a) Statuses of the inﬂuence model after
the evolution time interval. (b) NoC VFI partitioning with voltage/frequency assignments.
3. FURTHER CONSIDERATIONS
i mi
The advantage of the inﬂuence model over a master discrete time Markov chain is that the inﬂuence matrix H has a
(cid:2)
(cid:3)
far smaller dimension
i mi compared with the size
of the associated state-transition matrix G of the master
chain. However, G and H are intimately related, and it is
this relationship that can be exploited in order to convert
computations involving G to reduced-order computations involving only H . This allows one to make inferences about
the limiting behavior of the master chain through the study
of the much smaller inﬂuence graph.
Even though we used DVFS as a pilot application to
present our ideas, the proposed model may be used to design other system level optimization algorithms. For example, the islanding achieved by the model may be used
to design task migration techniques in order to implement
thermal management policies or to facilitate fault tolerant
mechanisms. Such policies and mechanisms will improve the
long-term reliability of the NoC. Due to its distributed nature, the inﬂuence model can accommodate router failures.
If for example, a router suﬀers a permanent failure, then,
1We assume a rich availability of mixed clock/mixed voltage FIFO queues (used for communication across diﬀerent
islands), though in practice one would limit their number in
the interest of silicon area.
626
the inﬂuence model can update itself by removing the corresponding arcs from the inﬂuence network (Fig.1) and by
updating the local dij probabilities.
The status of a site i at time step k + 1 is determined by
the status of neighboring site j from time step k , which at
its turn had been determined by the state of a neighboring
site, say u, from time step k − 1, and so on. We note an
analogy between this recursive relationship and the backpressure mechanism in NoCs. This suggests that the proposed model could incorporate predictive explicit-rate control mechanisms in order to adjust source traﬃc rates [4] for
congestion optimization and to facilitate guaranteed quality
of service [5].
The status occupancy probabilities for each site at any
time is determined based only on the individual status occupancy probabilities of itself and of all the neighbors at the
previous time step (or from knowledge of the initial state of
the network) [1].
Information about occupancy probabilities may be employed in designing for example applicationspeciﬁc static buﬀer allocation (a primary concern during
NoC synthesis) algorithms. Also, by dynamically tuning
the probabilities dij , the model could account for the impact of congestion, as opposed to static models that assume
known packet delay when traversing network routers (which
becomes inaccurate and can break performance constraints
guarantees).
Finally, we would like to point out that the inﬂuence
model has further ramiﬁcations in other application domains,
including multicasting in ad-hoc networks, self-classiﬁcation
for autonomous vehicles, and distributed decision-making
[6].
4. CONCLUSIONS
We proposed a novel modeling framework for NoC design
based on the inﬂuence model. This framework was used
to develop a design methodology for GALS and VFI based
design style for NoCs. We hope that the inﬂuence model, as
a generalization of prior stochastic models, will facilitate the
development of distributed and adaptive features for NoCs.
This ﬁts the vision of network-based computational models
outlined by [7].
5. "
Crosstalk noise and bit error rate analysis for optical network-on-chip.,"Crosstalk noise is an intrinsic characteristic of photonic devices used by optical networks-on-chip (ONoCs) as well as a potential issue. For the first time, this paper analyzed and modeled the crosstalk noise, signal-to-noise ratio (SNR), and bit error rate (BER) of optical routers and ONoCs. The analytical models for crosstalk noise, minimum SNR, and maximum BER in mesh-based ONoCs are presented. An automated crosstalk analyzer for optical routers is developed. We find that crosstalk noise significantly limits the scalability of ONoCs. For example, due to crosstalk noise, the maximum BER is 10
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-3</sup>
 on the 8 × 8 mesh-based ONoC using an optimized crossbar-based optical router. To achieve the BER of 10
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-9</sup>
 for reliable transmissions, the maximum ONoC size is 6 × 6. A novel compact high-SNR optical router is proposed to improve the maximum ONoC size to 8 × 8.","Crosstalk Noise and Bit Error Rate Analysis for Optical Network-on-Chip
Yiyuan Xie1, Mahdi Nikdast1, Jiang Xu1, Wei Zhang2, Qi Li1, Xiaowen Wu1, Yaoyao Ye1, Xuan Wang1, Weichen Liu1 
1. ECE, Hong Kong University of Science and Technology, 2. School of Computer Engineering, Nanyang Technological University 
{eeyiyuan, mnikdast, jiang.xu}@ust.hk, zhangwei@ntu.edu.sg
39.5
ABSTRACT 
Crosstalk noise is an intrinsic characteristic of photonic devices 
used by optical networks-on-chip (ONoCs) as well as a potential 
issue. For the first time, this paper analyzed and modeled the 
crosstalk noise, signal-to-noise ratio (SNR), and bit error rate 
(BER) of optical routers and ONoCs. The analytical models for 
crosstalk noise, minimum SNR, and maximum BER in meshbased ONoCs are presented. An automated crosstalk analyzer 
for optical routers is developed. We find that crosstalk noise 
significantly limits the scalability of ONoCs. For example, due 
to crosstalk noise, the maximum BER is 10-3 on the 8×8 meshbased ONoC using an optimized crossbar-based optical router. 
To achieve the BER of 10-9 for reliable transmissions, the 
maximum ONoC size is 6×6. A novel compact high-SNR optical 
router is proposed to improve the maximum ONoC size to 8×8. 
Categories and Subject Descriptors: B.7.0 [General]; C.5.4 
[VLSI Systems]; C.1.2 [Processor Architectures]: Multiple Data 
Stream Architectures (Multiprocessors) - Interconnection 
architectures, parallel processors. 
General Terms: Measurement, Performance, Design 
Keywords: optical network-on-chip, crosstalk, SNR, BER 
1. INTRODUCTION 
     On-chip optical communication and integration technologies 
propose an attractive solution for MPSoC on-chip communication. 
The introduction of photonics to on-chip global interconnect 
structures can potentially capitalize on the unique advantages of 
optical communication. Several optical networks-on-chip (ONoC) 
architectures and optical routers are proposed based on optical 
waveguides and microresonators, and showed ultra-high capacity 
and low-power consumption [1][2][3]. 
     Crosstalk noise is an intrinsic characteristic of photonic devices 
[4][5]. It is a potential issue in optical networks-on-chip (ONoCs). 
Crosstalk noise is the undesirable coupling among optical signals. 
During crosstalk, a small portion of the power from one optical 
signal is directed to another optical signal and becomes noise. 
Large crosstalk noise will lower signal-to-noise ratio (SNR) of an 
ONoC and cause high bit error rate (BER). BER is an important 
consideration in any communication system including ONoCs. It is 
defined as the percentage of bits that have errors relative to the total 
number of bits received during a transmission. For reliable data 
transmissions, BER is required to be lower than 10-9. BER is 
directly affected by SNR, which is the ratio between signal power 
level and noise power level in ONoCs. 
     For the first time, this paper analyzed and modeled the crosstalk 
noise, SNR, and BER of optical routers and ONoCs. The analytical 
models for crosstalk noise, minimum SNR, and maximum BER in 
mesh-based ONoCs are presented. An automated crosstalk analyzer 
for optical routers is developed. We find that crosstalk noise 
significantly limits the scalability of ONoCs. For example, while 
using crossbar-based optical router, the minimum SNR in the 8×8 
mesh-based ONoC is only 14dB, and its maximum BER is 10-3. To 
achieve a BER of 10-9, the maximum size of mesh-based ONoCs is 
6×6 using the optimized crossbar-based optical router. We 
 Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, or republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. 
DAC’10, June 13-18, 2010, Anaheim, California, USA 
Copyright 2010 ACM 978-1-4503-0002-5 /10/06...$10.00 
*This work is partially supported by HKUST PDF and HK RGC 
proposed a novel compact high-SNR optical router, called Crux, to 
improve the maximum size of mesh-based ONoCs to 8×8. 
2. CROSSTALK IN OPTICAL ROUTERS  
     Optical routers are the key components to build many types of 
ONoCs. They establish and maintain optical paths from sources to 
destinations for optical signals which carry payload data. The 
physical limitations imposed by integration necessitate a compact 
high-SNR optical router design while maintaining the level of 
functionality in ONoCs. A detailed analysis of optical routers paves 
the way for the network-level analysis in the next section. 
2.1 Crosstalk in Optical Switching Elements 
     Optical 
routers are composed of microresonators and 
waveguides. In optical routers, waveguides and microresonators 
form two types of basic optical switching elements – parallel 
switching element and crossing switching elements (Figure 1). 
When two optical waveguides cross each other, they form a 
waveguide crossing (Figure 1a). In this paper, the radius of the 
microresonator based on silicon-on-insulator (SOI) technology is 
5μm. The rectangular cross section of both ring and waveguide is 
500nm wide and 250nm high. The basic optical switching elements 
can be powered on (the ON state) and off (the OFF state) by 
changing the voltage applied on the microresonators. The input 
optical signal with wavelength λS will propagate from the input 
port to the through port when the basic optical switching elements 
are in the OFF state (Figure 1b and d). When the basic optical 
switching elements are in the ON state, the optical signal will be 
coupled into the microresonators and then directed to the drop port 
(Figure 1c and e). Compared with the parallel switching element, 
the crossing switching element has a waveguide crossing which 
introduces non-negligible crossing insertion loss, LC. The insertion 
loss per crossing is very small. When a large number of crossings 
are used in ONoCs, the total insertion loss is significant. When a 
waveguide bends, it also incurs the bending loss, LB. 
Figure 1. Basic optical switching elements (a) Waveguide 
crossing (b) Parallel switching element in OFF state (c) Parallel 
switching element in ON state (d) Crossing switching element in 
OFF state (e) Crossing switching element in ON state 
     Crosstalk is an intrinsic characteristic of microresonators and 
waveguide crossings. Crosstalk noise is caused by the undesirable 
coupling among optical signals when they pass microresonators and 
waveguide crossings. During crosstalk, a small portion of the 
power of one optical signal is directed to another optical signal and 
becomes noise. A waveguide crossing is an optical resonant cavity 
with four ports. Since the imperfect resonant cavity cannot prevent 
all the resonant modes that are excited from the input port from 
decaying into Out2 and Out3, there will be crosstalk on the 
undesired output ports. If PI is the power of the input optical signal, 
the output powers at Out1, Out2 and Out3 ports are PO1=LCPI and 
657
 
39.5
The injection/ejection port is connected to a local processor core 
through an optical/electronic (O/E) interface. These five ports are 
aligned to their intended directions, and the input and output of 
each port are also properly aligned to ensure that no extra crossing 
or waveguide bending is required when multiple routers are 
connected to form mesh-based ONoCs.  
     Crux takes the advantages of the parallel switching element to 
minimize insertion loss and crosstalk. Different from the optimized 
crossbar, when optical signals travel in one dimension, Crux can 
passively route them and does not require to power on any 
microresonator. Only when optical signals use the injection/ejection 
port or make a turn from one dimension to the other, routers need to 
power on one microresonator. In Crux, the maximum number of 
waveguide crossings between the ports is five. These properties 
reduce not only the insertion loss but also the crosstalk noise 
caused by waveguide crossings. Moreover, regardless of the 
network size, the mesh-based ONoCs based on Crux only need to 
power on at most three microresonators to inject, turn and eject an 
optical signal between any pair of processor cores. 
     We developed an automated crosstalk analyzer for optical 
routers based on the analysis of the basic optical switching 
elements and waveguide crossings in the last section. The analyzer 
can explore the crosstalk noise, insertion loss, and SNR of arbitrary 
optical router architectures. We implemented it in C. An optical 
router architecture is described as a list of the basic switching 
elements, waveguide crossings, and their connections in an input 
file. A technology file includes all the parameters in Table 1 and 2. 
The analyzer helped us to optimize the crossbar-based optical 
router and design Crux.  
PO2=PO3=K1PI, where LC is the power loss per crossing and K1 is 
the crosstalk coefficient per crossing. PO2 and PO3 will become 
crosstalk noise when they are mixed with other optical signals. 
     The two basic optical switching elements also suffer from 
crosstalk. When an input optical signal goes through the basic 
optical switching elements, crosstalk noise will be generated in the 
other ports. When the parallel switching element is in the OFF state, 
the output powers at the through and drop ports can be calculated as 
PT=LP1PI and PD=K2PI. While the parallel switching element is in 
the ON state, the output powers at the through and drop ports can 
be calculated as PT=K3PI and PD=LP2PI. The output powers of the 
crossing switching element in the OFF state can be calculated as 
PT=LC1PI, PD=(K2+L2
P1K1)PI, and PA=K1LP1PI. When it is in the 
ON state, the output powers can be expressed as PD=LC2PI, 
PT=LCK3PI, and PA=K1K3PI. PT, PD and PA are the output powers of 
the through, drop and add ports, respectively. LP1 is the power loss, 
and K2 is the crosstalk coefficient of the parallel switching element 
in the OFF state. LP2 and K3 are the power loss and crosstalk 
coefficient when the parallel switching element is in the ON state. 
LC1 is the power loss of the crossing switching element in the OFF 
state. LC2 is the power loss of crossing switching element in the ON 
state. When the crossing switching element is powered on, its 
crosstalk model is the same as the parallel switching element. The 
values of the losses and crosstalk coefficients are shown in Table 1 
and 2 [4][5]. 
LC 
-0.12dB 
Table 1. Optical power losses 
LC2 
LB 
LP1
LC1 
LP2
-0.125dB 
-0.5dB 
-0.005dB/90Û 
-0.005dB -0.5dB
K1 
-40dB 
Table 2. Crosstalk coefficients 
K2 
-45dB 
K3
-25dB
2.2 Optimized Crossbar and Crux Router 
     5×5 optical routers are required for mesh-based ONoCs. We 
optimized the crossbar-based optical router to minimize the 
insertion loss and crosstalk (Figure 2a). The optimized optical 
crossbar router uses dimension-order routing algorithm, which is a 
minimal path routing algorithm and is free of deadlock and livelock. 
The dimension-order routing algorithm 
is a 
low-complexity 
distributed routing algorithm and does not require any routing table. 
These make it particularly suitable for mesh-based ONoCs, which 
require both low latency and low cost. Since the turns from the Y to 
X dimensions and the U turns are not required, the number of 
microresonators can also be reduced to 16. 
Figure 3. Maximum crosstalk noise at each port 
North
Ejection
Control 
unit
Injection
West
East
Control 
unit
North
Injection
West
East
Ejection
South
(a)
Optical 
terminator
Optical 
waveguide
Light 
direction
Micro 
resonator
Metal
wire
South
(b)
Figure 2. 5×5 optical routers (a) The optimized optical crossbar 
router (b) Crux router 
     Based on the two basic switching elements, we developed a new 
optical router, called Crux, to further minimize the insertion loss, 
crosstalk, and the number of microresonators (Figure 2b). Crux 
router only uses 12 microresonators to implement the strictly nonblocking 5×5 routing function required by the dimension-order 
routing algorithm. Both Crux and the optimized crossbar include a 
switching fabric and control unit. The routers have five 
bidirectional ports -- injection/ejection, east, south, west and north. 
Figure 4. Minimum SNR of each signal path 
     Using the automated crosstalk analyzer, we analyzed the 
crosstalk noise and SNR for the optical routers. Figure 3 shows the 
maximum crosstalk noise at each output port when the optical 
power at each input port is 0dBm. The average maximum crosstalk 
noise in Crux is lower than in the optimized crossbar. Crux has 
lower crosstalk noise at the north, east, and west ports than the 
optimized crossbar. Although Crux has higher crosstalk noise at the 
ejection and south ports than the optimized crossbar, its SNR is 
better because optical signals encounter significantly less loss. 
     We analyzed the SNR of the optimized crossbar and Crux. 
Figure 4 shows the minimum SNR of each input port and output 
port combination. SNR is defined by equation (1), where Ps is the 
optical power of the signal and Pn is the power of the crosstalk 
noise. On average, the minimum SNR of Crux is 8dB higher than 
658
 
 
 
the optimized crossbar. The average minimum SNR of Crux is 
40dB, while the average minimum SNR of the optimized crossbar 
is 32dB. The Crux has significantly better SNR for optical signals 
from north to ejection, injection to west, east to north, and west to 
south. In ONoCs, optical signals will encounter insertion loss at 
neighbor routers before reaching an optical router. Since Crux has 
low insertion loss, the ONoCs based on Crux will have higher 
signal power at each router input ports compared with the ONoCs 
based on the optimized crossbar. This will make Crux to have even 
higher SNR in ONoCs as we can see in the next section. 
P
=
P
s
n
SNR
(1)
3. SNR AND BER OF MESH-BASED ONOCS 
     BER of an ONoC is directly influenced by its SNR. Since the 
more routers an optical signal passes, the more insertion loss it will 
suffer and the more crosstalk noise will be accumulated, the 
minimum SNR link between processor cores in an ONoC will cross 
a large number of optical routers. Although, the longest optical 
links, which traverse the largest number of optical routers in an 
ONoC, suffer the largest insertion loss, they may not be the 
minimum SNR link, because the crosstalk noise accumulated at 
each router on a link is also decided by the power levels of other 
input optical signals crossing the same router. This shows that the 
minimum SNR link in an ONoC should also be crossed by a large 
number of other high-power links.  
Figure 5. The minimum SNR links of mesh-based ONoCs using 
(a) optimized crossbar (b) Crux router 
     As an example, we systematically analyzed the SNR of meshbased ONoCs. A mesh-based ONoC connects processor cores with 
M×N optical routers and uses 
the dimension-order routing 
algorithm. The following SNR and BER analysis is based on the 
router-level analysis in the last section. Our analysis finds one of 
the second or third longest links is the minimum SNR link in meshbased ONoCs (Figure 5). Due to the different crosstalk and 
insertion loss properties of the optimized crossbar and Crux, the 
minimum SNR links of ONoCs based on them are the different 
second and third longest links. The minimum SNR link in the 
ONoC using the optimized crossbar is from processor core (2,1) to 
processor core (M,N-1). The minimum SNR link in the ONoC 
using Crux is from processor core (1,N) to processor core (M,2).  
     The two minimum SNR links are used as examples to illustrate 
our analysis process. On the minimum SNR link, the insertion loss 
of an optical router (i, j) at the X dimension, LX,i,j, and at the Y 
dimension, LY,i,j, for ONoCs using the optimized crossbar and Crux 
can be expressed by Equation (2) to (5). In the equations, subscripts 
a and b denote the optimized crossbar and Crux respectively. On 
the right sides of the equations, the second subscripts denote the 
router input ports, and the third subscripts denote the router output 
ports. Based on Equation (2) to (5), the insertion loss suffered by 
the minimum SNR link can be expressed by Equation (6) and (7). 
In the equations, the five terms on the right side represent 
respectively the insertion losses from the source processor to the 
first optical router, on the X dimension section of the link, at the 
optical router where the link turns from the X to Y dimension, on 
the Y dimension section of the link, and at the optical router 
connected to the destination processor. 
a In E
,
,
a X i j
,
, ,
a W E
,
,
2,
j
2, 2
=
1
≤ ≤ −
j N
2
L
L
i
i
L
=
=
­
¯
L
L
L
L
L
= ®
(2)
°
­
®
°¯
­
®
°
¯
­
°
®
°
¯
−
i
,1
Nj
=
≤≤
i
3,1
Nj
=
= −
i
2,
j N
1
3
≤ ≤
i M
−
1,
j N
= −
i M j N
=
,
= −
1
=
=
=
1
WEb
,
,
WInb
,
,
iXb
,
,,
L
j
(3)
a W S
,
,
a Y i j
,
, ,
a N S
,
,
a N Ej
,
,
1
L
°=
(4)
i
=
,1
j
=
2
≤≤
−
2
Mi
,1
jMi
=
,
=
2
=
=
2
EjNb
,
,
SNb
,
,
SEb
,
,
iYb
,
,,
L
L
L
j
L
j
(5)
(
)
(
)
min
a SNR
,
a In E
,
,
a W E
,
,
a W S
,
,
a N S
,
,
a S Ej
,
,
−
3
−
3
−
3
−
2
L
L
=
L
=
L
N
N
L
L
L
L
M
M
L
L
L
L
+
+
+
+
+
+
+
+
(6) 
(7)
     Crosstalk noise will be introduced by both the optical signal on 
the minimum SNR link and the optical signals on other links 
crossing the minimum SNR link through optical routers. The power 
of the crosstalk noise introduced at the optical router j on the X 
dimension section of the minimum SNR link, NX, is shown in 
Equation (8) and (9). The introduced noise power at the optical 
router i on the Y dimension section of the link, NY, is shown in 
Equation (10) and (11). In the equations, Pin is the optical input 
power at the injection port. The total crosstalk noise can be 
calculated by summing the crosstalk noise from each router on the 
link, while the total insertion loss of the link is given by Equation 
(6) and (7). 
(
)
(
)
EjNb
,
,
SNb
,
,
SEb
,
,
WEb
,
,
WInb
,
,
SNR
b
min
,
­
°
°
°°
®
°
°
°
°
¯
−≤≤
Nj
kLkLLP
(
+
LkP
+
+
kLkLLPL
(
+
L
L
LkP
+
L
LL
=
+
kLkLL
(
+
=
2
2)
)
1
)
2
2
C
1
2
C
1
SNa
,
,
SIna
,
,
1
EWa
,
,
3
4
BC
NSaNIna
,
,
,
,
2
2
C
1
2
EjEaWIna
,
,
,
,
1
EWa
,
,
2
BC
WIna
,
,
1
2
EjEaWIna
,
,
,
,
2
2
C
1
2
C
1
SNa
,
,
SIna
,
,
jXa
,
,
PL
L
LL
L
L
L
j
kLLP
L
L
L
L
N
B
C
in
in
B
C
in
in
B
C
in
B
C
in
(8)
­
°
®
°
¯
Nj
=
3)
−≤≤
Nj
1
+
LkP
+
=
kP
(
0
1
2
p
1
2
EjWb
,
,
2
B
EInb
,
,
2
2
EInb
,
,
4
2
CCB
1
jXb
,
,
kL
L
L
L
LLLL
N
in
in
p
(9)
­
°
°
°
°
°
®
°
°
°
°
°
¯
Mi
=
L
LL
L
LLL
kLPkLLP
+
−
≤≤
Mi
+
+
+
=
+
+
+
+
+
=
L
L
L
LL
L
kL
3)1
kP
(
kL
kLP
(
L
i
kL
kLP
(
L
kP
L
LL
L
L
kL
kLP
(
L
L
L
kP
L
LL
L
N
3
C
in
C
C
in
B
WEa
,
,
4
C
WIna
,
,
2
p
1
in
CB
C
EIna
,
,
2
C
C
in
C
EjEa
,
,
WIna
,
,
2
C
C
in
NEa
,
,
4
B
C
WIna
,
,
in
SWa
,
,
3
4
C
B
NEaWIna
,
,
,
,
2
C
C
in
C
EjNa
,
,
SIna
,
,
in
B
SWa
,
,
2
C
WIna
,
,
iYa
,
,
1
1
11
2
2
2
1
2
1
2
1
2
1
2
1
1
2
1
2
1
1
1
)
2
)
)
(10)
­
°
°
°
°
°°
®
°
°
°
°
°
°
¯
2
Mi
=
−
≤≤
Mi
+
+
L
L
LLLL
+
+
+
+
=
+
L
LLLL
L
+
LL
+
+
+
=
kLP
L
L
L
kL
kPL
(
kL
kLP
(
L
L
L
kL
kP
(
i
kL
kLLP
(
kL
kPL
(
kL
kP
(
N
3
C
1
in
p
1
EjWb
,
,
EInb
,
,
2
P
1
inEInb
,
,
2
P
1
C
in
p
1
WEb
,
,
WInb
,
,
2
P
1
in
p
2
CCB
EInb
,
,
2
C
C
C
in
EjWb
,
,
2
PB
1
EInb
,
,
2
P
1
inEInb
,
,
2
P
1
in
p
2
CCB
EInb
,
,
iYb
,
,
0
1
)
)
)
1
)
)
)
2
1
2
1
2
1
1
2
2
1
2
1
2
1
1
2
1
2
2
1
(11)
     Based on Equation (1) and (6) to (11), the minimum SNR of 
mesh-based ONoCs with M×N optical routers can be shown 
(Equation (12) and (13)). Equation (12) is derived from Equation 
(1), (6), (8) and (10) and shows the minimum SNR of the ONoCs 
using the optimized crossbars. Equation (13) is based on (1), (7), (9) 
and (11) and shows the minimum SNR of the ONoCs using Crux 
routers.  
659
39.5
                                        
 
 
                               
 
                                 
 
                            
 
                               
 
 
 
 
 
  
 
 
 
     
 
     BER is the percentage of bits that have errors among the total 
number of bits received during a transmission. ONoC with a high 
BER will require error correction coding and even retransmissions 
if errors can be found but not corrected. For optical signals using 
non-return-to-zero (NRZ) line code, ONoCs can be modeled as onoff keying (OOK) systems. The BER and the SNR relation of the 
OOK system is defined in equation (14) [6]. Based on Equation 
(12) to (14), the maximum BER of mesh-based ONoCs can be 
calculated. 
1 SNR
−
4
2
e
BER
=
(14)
4. SIMULATIONS AND ANALYSIS 
     We studied the relationship between the network size and the 
crosstalk noise, SNR, and BER of the mesh-based ONoCs using the 
optimized crossbar and Crux. Numerical simulations are performed 
using MATLAB. Figure 6a shows the power of optical signal and 
crosstalk noise at the destination on the minimum SNR link of the 
ONoCs using the optimized crossbar. We find that as the network 
size increases, the optical signal power received by the destination 
processor core drops quickly, and the crosstalk noise power 
increases relatively slow. This is due to the large insertion loss of 
the optimized crossbar, which not only attenuates optical signals 
but also the crosstalk noise. When the ONoC size is larger than 
14×14, the optical signal power is smaller than the crosstalk noise 
power. For instance, when the ONoC size is 16×16 and Pin is 0dBm, 
the signal power is -39.3dBm and the crosstalk noise power is 31.4dBm.  
          (a) Optimized crossbar                    (b) Crux router 
Figure 6. Signal and crosstalk noise power in the M×N ONoCs 
     Figure 6b shows the optical signal power and crosstalk noise 
power reached the destination processor core on the minimum SNR 
link of the ONoCs using Crux routers. It shows that as the ONoC 
size increases, the optical signal power decreases and the crosstalk 
noise power slowly increases. Compared with the ONoCs using the 
optimized crossbar, the optical signal power decreases much slower 
in the ONoCs using Crux router. For example, when the ONoC size 
is 16×16, the optical signal power is -20.9dBm and the crosstalk 
noise power is -28.2dBm. The optical signal power in the ONoC 
using Crux is 18.4dB higher than in the ONoC using the optimized 
crossbar. This is due to the smaller insertion loss in Crux than in 
the optimized crossbar. Although Crux causes less crosstalk noise, 
its small insertion loss gives less attenuation to the crosstalk noise 
from neighboring routers.  
     The minimum SNR in the MxN ONoCs using the optimized 
crossbar and Crux is shown Figure 7. It shows that as the ONoC 
size increases, minimum SNR will decrease. However, the 
¹                              
minimum SNR in the ONoCs using Crux routers decreases 
significantly slower than in the ONoCs using the optimized 
crossbar. The minimum SNR in the ONoCs using Crux is always 
higher than in the ONoCs using the optimized crossbar, and the 
difference is significantly larger than those shown in Figure 4, 
which compares the SNRs of stand-alone routers. For example, 
when the size of the ONoCs using the optimized crossbar is 8×8, 
the minimum SNR is 14dB and when the size is 16×16, the 
minimum SNR will decrease to -7.9dB, which indicates that the 
crosstalk noise power is higher than the optical signal power. 
Figure 7. Minimum SNR and maximum BER in M×N ONoCs 
     Figure 7b shows the maximum BER of MxN ONoCs using the 
optimized crossbar and Crux routers. BER increases exponentially 
as SNR decreases. As the ONoC size increases, the maximum BER 
also increases. For example, when the size of ONoCs using the 
optimized crossbar is 8×8, the maximum BER is 10-3, and the 
number increases alarmingly to 10-0.3 when the size is 16×16. To 
achieve the BER of 10-9 for reliable transmissions, the ONoCs 
using the optimized crossbar should not be larger than 6×6. The 
maximum BER in the ONoCs using Crux is always lower than in 
the ONoCs using the optimized crossbar. To achieve the BER of 
10-9, the size of ONoCs using Crux can increase to 8×8, which is 
considerably larger than the maximum size of the ONoCs using the 
optimized crossbar. 
5. CONCLUSION 
     Crosstalk noise is an intrinsic characteristic of photonic devices 
used by ONoCs. Although it is very small at device level, our 
analysis shows that crosstalk has a significant impact on the BER 
of ONoCs at system level. The minimum SNR and maximum BER 
are the limiting factors of ONoC design. For the first time, we 
present the analytical models of the minimum SNR and maximum 
BER in mesh-based ONoCs. We showed that a properly designed 
optical router, such as Crux, could significantly improve SNR and 
the scalability of ONoCs. 
"
Mapping on multi/many-core systems - survey of current and emerging trends.,"The reliance on multi/many-core systems to satisfy the high performance requirement of complex embedded software applications is increasing. This necessitates the need to realize efficient mapping methodologies for such complex computing platforms. This paper provides an extensive survey and categorization of state-of-the-art mapping methodologies and highlights the emerging trends for multi/many-core systems. The methodologies aim at optimizing system's resource usage, performance, power consumption, temperature distribution and reliability for varying application models. The methodologies perform design-time and run-time optimization for static and dynamic workload scenarios, respectively. These optimizations are necessary to fulfill the end-user demands. Comparison of the methodologies based on their optimization aim has been provided. The trend followed by the methodologies and open research challenges have also been discussed.","Mapping on Multi/Many-core Systems: Survey of Current
and Emerging Trends
Amit Kumar Singh1 , Muhammad Shaﬁque2 , Akash Kumar1 , J¨org Henkel2
1 Department of Electrical and Computer Engineering, National University of Singapore, Singapore
2 Chair for Embedded Systems (CES), Karlsruhe Institute of Technology, Karlsruhe, Germany
1 {eleaks,akash}@nus.edu.sg, 2 {muhammad.shaﬁque,henkel}@kit.edu
ABSTRACT
The reliance on multi/many-core systems to satisfy the high
performance requirement of complex embedded software applications is increasing. This necessitates the need to realize eﬃcient mapping methodologies for such complex computing platforms. This paper provides an extensive survey
and categorization of state-of-the-art mapping methodologies and highlights the emerging trends for multi/many-core
systems. The methodologies aim at optimizing system’s resource usage, performance, power consumption, temperature distribution and reliability for varying application models. The methodologies perform design-time and run-time
optimization for static and dynamic workload scenarios, respectively. These optimizations are necessary to fulﬁll the
end-user demands. Comparison of the methodologies based
on their optimization aim has been provided. The trend
followed by the methodologies and open research challenges
have also been discussed.
Categories and Subject Descriptors
C.3 [Special-purpose and application-based systems]:
Real-time systems and embedded systems
General Terms
Algorithms, Design, Performance, Reliability
Keywords
Multiprocessor Systems-on-Chip, embedded systems, application mapping
1.
INTRODUCTION
The maximum operational frequency of a single-core processor has hit the roof due to power dissipation and radio frequency eﬀects. This has forced chip manufacturers to limit
the maximum frequency of the processor and shifting towards designing chips with multiple cores operating at lower
frequencies [42] [10]. Moreover, the performance demands of
modern complex embedded applications have increased substantially which cannot be satisﬁed by simply increasing the
frequency of a single-core processor or by customization of
the processor. Instead, there is a need of multiple processors that can cohesively communicate and provide increased
parallelism. The underlying concept is to consider applications as conglomeration of many small tasks which can be
eﬃciently distributed on multiple processors in order to execute them in parallel and thereby meeting the increased
performance demands [3] [46].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC’13 May 29 - June 07 2013, Austin, TX, USA.
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
t0
GPP
t1
t3
t6
t9
ASIC1
t4
t7
DSP
t2
t5
t8
ASIC2
t2,t4,t7
DSP
t0,t1,t3
GPP
DSP
GPP
t5,t8
ASIC2
t6,t9
ASIC1
ASIC2
GPP
DSP
Figure 1: Application Mapping on Many-core System.
With the technological advancement and increasing performance demands, the number of cores in the same chip
area has grown exponentially and diﬀerent types of cores
have been integrated. As nanotechnology evolves, it will
become feasible to integrate thousands of cores on the same
chip [10]. The large number of cores needs to employ Networkon-Chip (NoC) based interconnection infrastructure for efﬁciency and scalability [33] [7]. The distinct features of
diﬀerent types of cores can be exploited to meet the functional and non-functional requirements. This makes heterogeneous multi/many-core systems (consisting of diﬀerent
types of cores) a formidable computing alternative where
applications witness large improvement over their homogeneous (consisting of identical cores) counterpart.
In order to map applications on multi/many-core systems,
the applications need to be partitioned (parallelized) into
multiple tasks that can be executed concurrently on diﬀerent cores. An example of partitioned application is shown
as Application Task Graph in Fig. 1. The application is
partitioned into ten tasks (t0 ,t1 ,...,t9 ). The partitioning job
can be furnished by state-of-the-art application parallelization tools [14] [53] and manual analysis, which involves ﬁnding the tasks, adding synchronization and inter-task communication in the tasks, management of the memory hierarchy communication and checking of the parallelized code
to ensure for correct functionality [59].
In case of heterogeneous platforms, a task binding process that speciﬁes the
core types on them the task can be mapped along with the
cost of mapping is required [84]. The binding process analyses the implementation costs (e.g., performance, power and
resource utilization) of each task on diﬀerent supported core
types such as general purpose processor (GPP), digital signal
processor (DSP) and coarse grain re-conﬁgurable hardware.
Mapping application tasks on multi/many-core system involves assignment and ordering of the tasks and their communications onto the platform resources in view of some
optimization criteria such as energy consumption and compute performance. Fig. 1 shows mapping of tasks and their
communications on part of a many-core system. The communicating tasks are mapped on the same core or close to
each other in order to optimize for the communication delay
and energy. The optimization is necessary to satisfy performance constraints of the applications. This necessitates
the need to develop eﬃcient mapping methodologies that
take application model, platform model, constraints (e.g.,
compute performance and power), performance model of
inter-process communication (e.g., execution time and energy consumption) and estimate of the worst case execution
time (WCET) of the process implementations on diﬀerent
cores (e.g., GPP, DSP, ASIC) as input and provide mappings
that satisfy the constraints.
1.1 Mapping Problem and Challenges
Mapping and scheduling problem is similar to Quadratic
Assignment Problem, a well-known NP-hard problem [28].
Therefore, ﬁnding optimal solution satisfying all the given
constraints is very diﬃcult and time consuming. Thus, heuristics based on the application domain knowledge need to be
employed to ﬁnd a nearly optimal solution.
Furthermore, the user demands (e.g., performance and
power constraints) for each application need to be fulﬁlled.
This necessitates the need to ﬁnd optimal mapping solutions
for each use-case1 to be supported into the system. The
optimal solutions can be explored by advance design-time
analysis and then can be used at run-time. However, explosion in the number of use-cases with the increasing number
of applications make the analysis unfeasible. For n applications, the analysis needs to be performed for 2n use-cases.
Additionally, such analysis cannot deal with dynamic scenarios such as run-time changing standards and addition of
new applications. Run-time management is required to handle such dynamism albeit optimal mapping solutions are not
found.
The application mapping problem has been identiﬁed as one of the most urgent problem to be solved
for implementing embedded systems [57] [60]. This
problem is being addressed by several researchers who communicate their views through various forums. A series of
dedicated workshops on mapping of applications onto multicore systems have been started to move beyond state-of-theart. The mapping methodologies are developed by targeting
speciﬁc application domain (e.g., multimedia and networking) for the most promising multi-core system.
1.2 Classiﬁcation of Mapping Methodologies
There could be a number of taxonomies to classify the
mapping methodologies, like target architecture based, optimization criteria based, workload based, etc. Broadly, the
methodologies can be classiﬁed based on workload scenarios and other taxonomies can be included at some hierarchy in the classiﬁcation as shown in Fig. 2. For static
and dynamic workload scenarios, the mapping methodologies perform optimization at design-time and run-time
respectively, which has led them to classify as design-time
and run-time methodologies respectively. The methodologies target either homogeneous or heterogeneous multicore systems. The run-time mapping requires a platform
manager that handles mapping of tasks at run-time.
In
addition to mapping, the manager is also responsible for
task scheduling [51], resource control, conﬁguration control
and task migration at run-time. The manager may employ
centralized management, distributed management or
mixture of centralized and distributed management. In centralized management, one core of the platform is used as the
manager that handles the mapping process. For distributed
management, the platform is divided into regions (clusters)
and one core in each cluster manages the mapping process
inside the cluster. The cluster managers communicate with
each other through a global manager to ﬁnd the best cluster
for mapping an application.
Design-time mapping methodologies are suitable for static
workload scenarios where a predeﬁned set of applications
1Combination of simultaneously active applications.
Mapping 
Methodologies
Design-time
(For Static Workload)
Run-time
(For Dynamic Workload)
Homogeneous 
Architecture
Heterogeneous 
Architecture
Homogeneous 
Architecture
Heterogeneous 
Architecture
Centralized 
Management
Distributed 
Management
Centralized + Distributed  
Management
Figure 2: A Taxonomy of Mapping Methodologies.
with known computation and communication behavior and
a static platform are considered. They are unable to handle
dynamism in applications incurred at run-time (e.g., multimedia and networking applications). Examples of such dynamism could be adding a new application into the system at
run-time. Since applications are often added to the platform
at run-time (for example, downloading a Java application in
a mobile-phone at run-time), workload variation takes place.
We witness the need of run-time mapping methodologies to
handle such dynamic workloads.
The run-time mapping methodologies face the challenge to
map tasks of new applications on the platform resources to
satisfy their performance requirements while keeping accurate knowledge of resource occupancy. After mapping tasks,
task migration can also be used to revise placement of some
of the already executing tasks if the user requirement is
changed or a new application has entered into the system.
This paper performs an in-depth survey of the mapping
methodologies reported in literature based on the earlier
mentioned taxonomy. The methodologies have been analyzed to highlight their strengths and weaknesses. The trend
followed by the methodologies over the decade has been
observed and reported, which provides an insight for the
emerging mapping methodologies. Despite being signiﬁcant
advancement in the development of mapping methodologies,
some open issues that need to be addressed in the future are
highlighted by analyzing the methodologies reported in the
literature.
Paper Organization: Section 2 discusses mapping methodologies that perform design-time optimization while targeting homogeneous or heterogeneous architectures. In Section
3, run-time mapping methodologies are analyzed and elaborated. Section 4 provides the upcoming trends and open
research challenges. Finally, we conclude the paper highlighting the important points of mapping methodologies in
Section 5.
2. DESIGN-TIME MAPPING
Design-time mapping methodologies have a global view of
the system which facilitates in making better decision for
using the system resources. As optimization is performed
at design-time, the methodologies can use more thorough
system information to make decisions. Thus, a better quality of mapping may be achieved as compared to the runtime mapping methodologies that are restricted normally
to a local view where only the neighborhood of the task
mapping is considered. Most of the mapping methodologies
reported in the literature fall under design-time mapping.
These methodologies target either homogeneous or heterogeneous architectures.
Table 1 classiﬁes recent works in design-time mapping and
shows the target architecture and optimization goal of the
mapping methodologies proposed by diﬀerent authors. The
target architecture (Arch.) is either homogeneous (Hom.)
Table 1: Classiﬁcation of design-time mapping
methodologies.
Author
Arch.
Orsila et al. [68]
Hom.
Ruggiero et al. [74]
Hom.
Satish et al. [76]
Hom.
Bonﬁetti et al. [9]
Hom.
Lin et al. [50]
Hom.
Murali et al. [65]
Hom.
Rhee et al. [73]
Hom.
Chen et al. [16]
Hom.
Hu et al. [36] [37]
Hom.
Marcon et al. [55] [56] Hom.
Ascia et al. [5]
Hom.
Meyer et al. [62]
Hom.
Thiele et al. [89]
Hom.
Thiele et al. [88]
Het.
Choi et al. [18]
Het.
Che et al. [15]
Het.
Castrillon et al. [13]
Het.
Manolache et al. [54]
Het.
Javaid et al. [41]
Het.
Wu et al. [94]
Het.
Zhu et al. [100]
Het.
Hartman et al. [30]
Het.
Optimization Goal
Execution time
Execution time
Execution time
Mapping time & quality
Throughput, Resource utilization,
Energy consumption
Energy consumption
Energy consumption
Energy consumption, Execution time
Energy consumption, Execution time
Energy consumption, Execution time
Reliability
Reliability, Temperature
Execution time
Execution time
Execution time
Execution time
Exploration time, Accuracy
Exploration time, Accuracy
Energy consumption
Reliability
Reliability
or heterogeneous (Het.). The methodologies aim at optimizing for diﬀerence performance metrics in order to fulﬁll
the varying user demands.
Compute Performance: Optimizing for the compute
performance is of paramount importance in order to meet
the timing deadlines or to minimize the time taken to ﬁnish
some jobs. The compute performance may refer to total execution time, latency, delay, period, throughput, exploration
time, etc., which are related to timing information.
Diﬀerent well established search approaches have been extensively used to develop design-time mapping methodologies in order to ﬁnd optimal or near-optimal placement of
tasks on platform cores towards improving the compute performance. For example, Simulated Annealing (SA) is used
in [68] [50], Genetic Algorithm (GA) in [18], Tabu Search
in [54] and Integer Linear Programming (ILP) in [41]. Orsila
et al. [68] optimize execution time and memory consumption by claiming that traditional approaches only focus on
the execution time. Lin et al. [50] attempt to ﬁnd mapping
of tasks such that the overall system throughput is maximized. They show an improvement of 20% in the system
throughput. Choi et al. [18] propose a GA-based technique
for eﬃciently executing Synchronous Dataﬂow (SDF) applications on a multi-core system where each core has a limited
size of scratchpad memory (SPM). Manolache et al. [54] address the problem of task mapping in the context of multiprocessor applications with stochastic execution times and
in the presence of constraints on the percentage of missed
deadlines. Javaid et al. [41] propose a methodology consisting of ILP formulation to explore eﬃcient mappings. These
search based approaches provide eﬃcient mapping solutions,
but they have high computational costs for large scale problems such as applications with large number of tasks.
Diﬀerent pruning strategies have been incorporated to
prune the search space, thereby reducing the computational
costs. Ruggiero et al. [74] combine Integer Programming
with Constraint programming to speed up the executions.
They target bus-based architectures that are not scalable
and thus the approach enforces scalability issues. Satish et
al. [76] propose a decomposition based approach to speed
up constraint optimization. They optimize for the schedule
length or make-span. Bonﬁetti et al. [9] propose an approach
for throughput-maximal mapping of SDF applications. The
approach speeds-up the computation and enhances the efﬁciency of the search by jump-starting with a high-quality
bound and quickly tightening it. Thiele et al. [88] propose
a mapping framework called Distributed Operation Layer
(DOL), which optimizes for computation and communication time. They integrate an analytic performance analysis
strategy into DOL to alleviate the modeling and analysis
of systems. Che et al. [15] consider the number of software
pipeline stages to map streaming applications on SPM-based
embedded multi-core system. The proposed method scales
well over a wide range of cores and SPMs. Castrillon et
al. [13] propose an algorithm that directly addresses mapping of tasks and their communications. The algorithm is
executed repeatedly to compute mappings for real-time applications speciﬁed as Kahn Process Network (KPN). These
approaches provide mapping solutions in lesser time than
the approaches performing extensive or complete search, but
might miss high quality mapping solutions due to pruning of
the search space.
Energy Consumption: Optimizing for the energy consumption of modern embedded systems (e.g., mobile phones,
tablets) is important as they are usually operated by standalone power supply like battery. The optimization needs
to be performed during the system design and operation in
order to increase the operational time.
Murali et al. [65] present a methodology that handles mapping of multiple use-cases while satisfying their performance
constraints. The methodology shows a power savings of
54%. Rhee et al. [73] propose an ILP based approach that
optimally maps cores onto mesh architecture in order to minimize energy consumption or NoC congestion. The approach
achieves 81% energy savings for random benchmarks. Chen
et al. [16] propose a multi-step mapping methodology where
optimization is performed in diﬀerent steps of the mapping
process. Wu et al. [94] introduce a GA based approach that
use Dynamic Voltage Scaling (DVS) to reduce the energy
consumption by up to 51%. These methodologies show signiﬁcant energy savings.
Some methodologies that perform optimization for both
energy consumption and compute performance are introduced in [37], [56] and [5]. Hu et al. [37] propose a mapping methodology that reduces power consumption by decreasing the energy consumption in communication while
guaranteeing the required performance. They methodology
provides an energy savings of 51%. Marcon et al. [56] extend the work in [37] and propose a technique called Communication Dependence and Computation Model (CDCM).
In addition to communication volume as considered in [37],
timing of the communication has been considered. Execution time is reduced by 98% while achieving a signiﬁcant
amount of energy savings. Ascia et al. [5] present a GA
based approach that explore Pareto mappings eﬃciently and
accurately while optimizing for performance and energy consumption. The aforementioned methodologies optimize for
compute performance and energy consumption, but do not
take reliability of the system into account during that optimization. Therefore, the provided mapping solutions might
lead to reduced lifetime of the system.
Reliability: Design-time mapping methodologies targeting lifetime improvement of multi-core systems are proposed
in [62], [89], [100] and [30]. Meyer et al. [62] propose an approach to eﬀectively and eﬃciently allocate execution and
storage slack in order to jointly optimize system lifetime
and cost. Thiele et al. [89] propose a thermal-aware system
analysis method that produces mappings with lower peak
temperature of the system, leading to reliable system design.
Zhu et al. [100] exploit redundancy and temperature-aware
design planning to produce reliable and compact multi-core
systems. Hartman et al. [30] propose a lifetime-aware task
mapping methodology that produces mappings with higher
lifetimes. These methodologies take preventive measures by
performing reliability-aware mapping in order to reduce occurrence of faults in the systems. Such preventive measures
increase lifetime of systems.
2.1
Issues and Limitations of Design-time
Methodologies
Most of the design-time methodologies adopt search based
approaches (e.g., GA, ILP, SA) that incur high computational costs. Thus, the evaluation time might not be acceptable for large scale problems. However, they provide eﬃcient
mapping solutions for small scale systems within acceptable
time. The evaluation time can be reduced by eﬃcient pruning of the search space, but at the risk of missing the high
quality mapping solutions. The reliability-aware design-time
methodologies increase the system lifetime but they cannot
overcome the faults incurred in the system.
Further, as the design-time methodologies ﬁnd placement
of tasks at design-time, they are not suitable for run-time
varying workloads in the systems and run-time changing environments. Such dynamic workload scenarios require remapping/run-time mapping of applications. Even if these
mapping methodologies are inadequate for the dynamic workload scenarios, they might be useful to ﬁnd the initial task
placement, or be optimized to be working at run-time.
3. RUN-TIME MAPPING
In contrast to the design-time mapping, run-time mapping
needs to account for the time taken to map each task as it
contributes to overall application execution time. Furthermore, the tasks are mapped one by one, unlike the static case
where all the tasks are mapped at once by looking globally at
the system. Therefore, typically greedy heuristic algorithms
are used for eﬃcient run-time mapping in order to optimize
performance metrics such as energy consumption, communication latency, execution time, etc. The run-time mapping
has several requirements, advantages and issues & research
challenges for diﬀerent available mapping alternatives.
Requirement: The run-time mapping caters for dynamically workload scenarios where mapping of one or more already running applications may need to be reconsidered in
case of following requirements:
• Insertion of a new application into the system, which
needs resources from the already executing applica• Modifying parameters of a running application.
tions.
• Killing a running application in order to free it’s occu• Changing performance requirements of a running appied resources.
plication. This might need extra resources for perform• When current mapping is not suﬃciently optimal, it
ing extra functionality.
requires (re-)mapping.
Advantages: In addition to the suitability for dynamic
workload scenarios, run-time mapping also oﬀers a number
of advantages. Some of them are as follows:
• Adaptability to the available resources : The available
resources vary over time as the applications of the dy• Ability to enable unforeseeable upgrades : It is possible
namic workload scenario enter at run-time.
to upgrade the system for new applications or changing standards that are not known at design-time, even
• Ability to avoid defective parts of a SoC : If one or more
after the delivery of the system to the end-user.
processing cores are not working properly after production of a SoC, then the defective cores can be disabled
before the mapping process. Aging can lead to defective cores that are unforeseeable at design-time.
Mapping Alternatives: At run-time, mapping of new
applications to be supported onto a platform can be handled
either by performing all the processing at the same time, i.e.
on-the-ﬂy processing or by using previously analyzed (DSE)
General Purpose Processor 
(GPP)
Reconfigurable Area 
(RA)
MPEG4 decoder
1
p
p
A
i
l
a
c
t
i
e
S
n
o
t
M
u
l
t
i
c
o
r
A
e
c
r
h
i
t
c
e
t
u
r
e
MC
IDCT
IQ
VLD
2376
2376
1
1
1
1
1
1
d1
d2
d3
d4
2
VLD
IDCT
RC
FD
MC
1
1
1
1
99
1
1
1
1
1
1
1
1
1
1
1
99
99
99
H.263 decoder
JPEG decoder
R
GPP
R
R
R
R
R
R
R
R
R
R
R
ACC
GPP
GPP
GPP
GPP
GPP
GPP
RA
RA
GPP
GPP
R
ACC
R
R
GPP
RA
Design-time DSE
(compute intensive analysis)
Mappings using 
different number of PEs
Allocate Tasks 
to PEs
Application
User demands
Accelerator 
(ACC)
col-conv
1
IDCT
1
IQ
6
VLD
1
6
1
1
1
e1
e2
e4
e6
2
IZZ
1
reorder
1
1
1
e3
e5
Mapping using 
DSE results
On-the-fly Mapping 
(all the processing is 
performed at run-time)
Current System Status
Architecture 
Description
Figure 3: On-the-ﬂy and Hybrid Mapping Flow [81].
results as shown in Fig. 3. The run-time platform manager
handles the mapping of applications by taking the updated
resources’ status (Current System Status ) into account.
For on-the-ﬂy mapping, eﬃcient heuristics are required to
assign new arriving tasks on the platform resources. These
heuristics cannot guarantee for schedulability, i.e., for strict
timing deadlines due to lack of any prior analysis and limited compute power at run-time. However, these heuristics
are platform independent since they do not use any platform
speciﬁc analysis results computed in advance. Such heuristics lend well to map unknown applications (not available at
design-time) on any platform.
For mapping using previously analyzed (DSE) results, the
applications to be supported on a platform should be known
at design-time.
In such cases, light-weight heuristics are
required to select the most eﬃcient mappings for each application from the design-time (oﬄine) analyzed mappings
stored on the system (Mappings using diﬀerent number of
PEs ). The selection is done sub ject to available system resources (extracted from Current System Status ) and desired
performance (User demands ). The selected mapping is used
to conﬁgure the platform. Compute intensive analysis is
performed at design-time (Design-time DSE ), facilitating
for light-weight run-time platform manager that can conﬁgure the applications eﬃciently. In DSE, application and
architecture description are taken as input and a number
of mappings are produced. Such mapping methodologies
have been referred to as hybrid mapping as they take the
advantages of both design-time and run-time. The hybrid
approach maps applications more eﬃciently than on-the-ﬂy
heuristics. However, ﬂexibility in these approaches is limited, since all potential applications must be known in entirety at design-time and analysis results will be applicable
only to the analyzed platform. Therefore, design-time analysis needs to be repeated when the application set or platform changes. Further, storing analysis results introduces
additional memory overhead.
3.1 On-the-ﬂy Mapping
Recent works on on-the-ﬂy mapping are classiﬁed according to the proposed taxonomy and are listed in Table 2.
The table reveals the target architecture (Arch.), control
mechanism and optimization goal of the mapping methodologies. The methodologies target homogeneous (Hom.) or
 
 
Table 2:
Classiﬁcation of on-the-ﬂy mapping
methodologies
Author
Hong et al. [35]
Sho jaei et al. [80]
Moreira et al. [63]
Chou et al. [19]
Mehran et al. [61]
Briao et al. [11]
Qi et al. [72]
Chou et al. [20]
Coskun et al. [23, 24]
Peter et al. [70]
Theocharides et al. [87]
Feng et al. [92]
Ahmed et al. [1]
Huang et al. [38]
Liang et al. [17]
Nollet et al. [67]
Carvalho et al. [12]
Smit et al. [84]
Braak et al. [86]
Schranzhofer et al. [78]
Singh et al. [83]
Hartman et al. [31]
Faruque et al. [3]
Kobbe et al. [46]
Ebi et al. [27]
Arch. Control
Manager
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Distr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Centr.
Distr.
Distr.
HDistr.
Hom.
Hom.
Hom.
Hom.
Hom.
Hom.
Hom.
Hom.
Hom.
Hom.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Het.
Optimization Goal
Execution time
Execution time, Solution quality
Execution time, Resource utilization
Energy consumption, Communication cost
Energy consumption, Mapping time
Energy consumption, Execution time
Reliability, Energy consumption
Reliability, Energy consumption, Throughput
Reliability, Temperature
Execution Time
Execution time
Execution time
Execution time
Execution time, Resource utilization
Execution time, Resource utilization
Mapping time & quality
Communication overhead
Energy consumption, QoS for applications
Energy consumption, Execution time
Energy consumption, Execution time
Energy consumption, Communication overhead
Reliability
Execution time, Mapping time, Traﬃc
Execution time, Traﬃc
Execution time, Temperature
heterogeneous (Het.) multi-core systems depending upon
the requirement of applications. For controlling the system,
a centralized (Centr.), distributed (Distr.) or mix of centralized and distributed, i.e. hierarchical distributed (HDistr.)
resource management strategy is used to allocate tasks on
the resources at run-time. The methodologies aim at optimizing for diﬀerence performance metrics.
Compute Performance: The compute performance optimization relates to the timing optimization such as overall
execution time and mapping time. Hong et al. [35] change
the thread-to-processor mapping at run-time based on the
workload variation in order to optimize the performance.
An improvement of 29% is achieved in the overall execution
time. In [80], the presented heuristic oﬀers additional advantage to trade-oﬀ execution time versus solution quality.
Moreira et al. [63] present a methodology that ﬁrst assigns
tasks to virtual cores (VCs) aiming to minimize total number
of VCs and total bandwidth used while meeting the timing
constraints. Thereafter, the VCs are mapped to real cores.
Theocharides et al. [87] demonstrate a system-level biddingbased task mapping methodology that provides signiﬁcant
performance improvements when compared to a round robin
allocation. Feng et al. [92] perform workload variation aware
mapping to optimize the system performance. Ahmed et
al. [1] perform adaptive resource management to maintain
QoS requirement of application. Huang et al. [38] introduce
self-adaptability to the run-time task allocation to achieve
high system performance. The adaptability is obtained by
dynamically adjusting a set of key parameters based on current resource utilization. Liang et al. [17] take the advantage of shared multi-core reconﬁgurable fabric to optimize
the performance. Nollet et al. [67] describe a run-time task
assignment heuristic for eﬃciently mapping the tasks in a
multi-core system containing FPGA fabric tiles. With the
presence of FPGA fabric tiles, the heuristic is capable of
managing a conﬁguration hierarchy and improves the task
assignment success rate and quality of solutions. Carvalho
et al. [12] present heuristics where tasks are mapped according to the communication requests and the load in the NoC
links. Such consideration reduces the communication overhead, leading to reduced execution time.
The above mentioned methodologies to optimize compute
performance use centralized management (CM) approach.
The CM approach for large systems (many-core systems
(consist of thousands of cores)) faces the following problems:
1) single point of failure, 2) large volume of monitoringtraﬃc by the CM, 3) high computational cost to calculate
mapping inside CM and 4) bottleneck around the CM as
every core sends its status to the CM after every instance
of mapping [3]. Thus, the CM becomes a hot spot. This
necessitates the need of distributed management in order to
reduce the monitoring traﬃc and computational eﬀort.
For distributed mapping, the entire system is partitioned
into multiple clusters. The resources within each cluster
are managed by an individual cluster manager (agent) that
communicates with a global platform manager in order to
eﬃciently map an application inside the cluster. The distributed mapping methodology is better than the state-ofthe-art run-time mapping methodologies using Centralized
Manager (CM) approach when many-core systems are targeted. Peter et al. [70] present a heuristic algorithm that
is distributed over the processor cores, facilitating its applicability to systems of random size. However, as each core
can be considered as a cluster, resource management will
become diﬃcult due to linear increment in the number of
clusters with the system size. Eﬃcient distributed application mapping methodologies targeting large architectures
such as 32×32 and 32×64 systems are presented in [3], [46]
and [27]. Faruque et al. [3] consider static applications and
focuses on communication, whereas Kobbe et al. [46] consider malleable applications. Ebi et al. [27] consider hierarchical distributed management that targets to trade-oﬀ the
eﬀectiveness of a centralize approach using global knowledge
with the scalability of a fully distributed one.
Energy Consumption: Chou et al. [19] propose a methodology that incorporates the user behavior information in the
resource allocation process; that allows system to better respond "
Proactive circuit allocation in multiplane NoCs.,"This work explores a method for efficient pre-allocation of circuits in network-on-chip (NoC) to reduce communication latency and improve performance. Circuit pre-allocation eliminates the time cost of circuit establishment by using request messages to reserve the circuits for their anticipated reply messages. Requests reserve circuits in a priority order rather than for a particular time slot, avoiding delays or blocking even if the newly requested circuits conflict with previously reserved ones. Benchmark simulations show speedup in execution time of up to 16%, with an average of 8% for communication sensitive benchmarks, over a leading proposal in pre-configuring circuits.","Proactive Circuit Allocation in Multiplane NoCs∗
Ahmed Abousamra
University of Pittsburgh
Depar tment of Computer
Science
Pittsburgh, PA, USA
abousamra@cs.pitt.edu
Alex K. Jones
University of Pittsburgh
Depar tment of Electrical and
Computer Engineering
Pittsburgh, PA, USA
akjones@ece.pitt.edu
Rami Melhem
University of Pittsburgh
Depar tment of Computer
Science
Pittsburgh, PA, USA
melhem@cs.pitt.edu
ABSTRACT
This work explores a method for eﬃcient pre-allocation of
circuits in network-on-chip (NoC) to reduce communication
latency and improve performance. Circuit pre-allocation
eliminates the time cost of circuit establishment by using
request messages to reserve the circuits for their anticipated
reply messages. Requests reserve circuits in a priority order rather than for a particular time slot, avoiding delays
or blocking even if the newly requested circuits conﬂict with
previously reserved ones. Benchmark simulations show speedup
in execution time of up to 16%, with an average of 8% for
communication sensitive benchmarks, over a leading proposal in pre-conﬁguring circuits.
1.
INTRODUCTION
The scaling of semiconductor technology enables packing
many cores on a single chip. For example, Intel integrates
48 cores on a single chip [8], and Tilera produces chips with
up to a 100 cores on a single chip [19]. Communication
between the chip components is carried out by the NoC.
The communication latency has a signiﬁcant eﬀect on chip
multiprocessor (CMP) performance; this eﬀect continues to
increase as the technology scales down enabling even more
cores on a single chip [13].
In general purpose chips, the NoC must support full connectivity between the chip components to accommodate general programming/application. Full connectivity can be achieved
through packet-switching, in which packets are examined
at each NoC router and appropriately routed to their destinations. However, the overhead of making routing decisions increases communication latency. Conversely, circuitswitching reduces communication latency since packets avoid
the routing overhead by traveling on pre-conﬁgured paths
to their destinations. Unfortunately, limited resources allow only a subset of all possible circuits to simultaneously
exist. Thus, hybrid NoCs have been proposed to support
∗This work is supported,
1064976.
in part, by NSF award CCFPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC’13, May 29 - June 07 2013, Austin, TX, USA.
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
full connectivity through packet switching, while speeding
up communication whenever possible by conﬁguring paths
on which packets bypass routers’ pipelines, which is often
referred to as circuit-switching in the NoC literature [11, 9,
1, 2, 14] and in the rest of this paper.
Establishing a circuit between two nodes incurs time overhead since a control message must ﬁrst be sent to reserve or
conﬁgure the circuit. Such overhead can decrease the beneﬁt
of circuit switching unless it is reduced or eliminated. Diﬀerent methods have been proposed for reducing this overhead:
(1) Amortizing the overhead over many circuit re-uses [9];
circuits are conﬁgured on-demand and kept in place until
they are removed to allow the establishment of conﬂicting
circuits. Without circuit re-use, the overhead can become
too expensive.
(2) Time-Division-Multiplexing (TDM) [16, 6, 7]: removes
the circuit setup overhead by enforcing a static schedule for
realizing circuits. Although the NoC design is simpliﬁed,
the NoC may suﬀer from underutilization and may not be
suitable for general purpose CMPs, where communication
requirements may not be known a priori.
(3) Hiding circuit setup overhead: Flit reservation ﬂow
control [15] performs accurate time-based reservations of the
buﬀers and ports of the routers that a ﬂit will pass through,
but requires a dedicated faster plane to carry the reservation ﬂits that are sent ahead of the corresponding message
ﬂits for which reservations are made. Li et al. [12] also propose time-based circuit reservations. As soon as a clean
data request is received at a node, a circuit reservation is
injected into the NoC to reserve the circuit for the data
message, optimistically assuming that the request will hit in
the cache and assuming a ﬁxed cache latency. However, the
proposal is conceptual and missing the necessary details to
handle uncertainty in such time-based reservations. D´ej`a Vu
Switching [1] amortizes circuit setup overhead for data messages carrying requested cache lines through early sending
of circuit reservations once a cache hit is detected. Instead
of time-based reservations, circuit reservations are queued
and realized on a ﬁrst-come-ﬁrst-serve (FCFS) basis, which
enables reservations to proceed unblocked to their destinations. While the early sending of FCFS reservations helps
hide the circuit conﬁguration overhead, in a CMP optimized
for performance this advantage may be insuﬃcient. If the
on-chip cache is suﬃciently fast, circuit reservations may not
have a large enough lead time on their corresponding data
messages, particularly in higher diameter NoCs.
In this work we explore proactive circuit allocation to completely remove the overhead of circuit conﬁguration for anticipated reply messages without making the assumption that
circuit reservations travel on a plane that is clocked at a
speed which is higher than that of the data plane [15]. In
our proposal data request messages reserve the circuits for
their anticipated reply data messages. In this setting accurate time-based reservations as in [15] are impractical, since
at the time that a request is reserving a circuit, we cannot
be certain of the actual time at which the reply message will
be injected in the NoC as other network traﬃc may cause
unforeseen delays. Moreover, simple FCFS reservations [1]
can under-utilize the NoC by delaying the realization of circuits for data messages that have already arrived, as we explain later. Rather, our proposal combines the ideas of both
queued and time-based circuit reservations; reservations are
still queued but instead of an FCFS ordering for realizing
circuits, reservations are ordered based on estimates of circuit utilization times.
The remainder of the paper is organized as follows. Section 2 describes the proposed circuit pre-allocation scheme.
Section 3 discusses handling the cases when circuit preallocation is not possible. Section 4 describes the simulation environment and results. Related work is presented in
Section 5. Finally, Section 6 concludes the paper.
2. PROACTIVE CIRCUIT ALLOCATION
In this section we describe the proposed proactive circuit
allocation scheme. We start by describing the network architecture, then how data requests reserve circuits, and ﬁnally
how circuits are realized.
2.1 Network Architecture
The interconnect is composed of two planes1 organized in
a regular two dimensional mesh topology, where every router
is connected with its four neighboring routers via bidirectional point-to-point links and with a single processor tile
via the local port. One plane is packet-switched while the
other is circuit-switched. Control and coherency messages
such as data access requests (e.g.
read and exclusive requests), invalidation messages, and acknowledgments travel
on the packet-switched plane, which is referred to as the
control plane. Data messages carrying cache lines, whether
replies to data requests or write-back messages of modiﬁed
cache lines, travel on the circuit switched plane, which is
referred to as the data plane.
Data request messages travel on the control plane making
circuit reservations at the corresponding data plane routers
for their anticipated data reply messages, while data plane
routers inform their corresponding control plane routers of
space availability in the circuit reservation buﬀers.
2.2 Reserving Circuits
The purpose of circuit pre-allocation by request messages
is to remove the circuit conﬁguration overhead. To be able
to reserve circuits for their replies, a request and its reply
should travel the same path but in opposite directions ; hence
we refer to the circuits reserved by requests as reverse or
backward circuits. To avoid delaying request messages if they
attempt to reserve previously reserved ports, routers support
storing and realizing multiple reverse circuit reservations.
However, the order of realizing reverse circuits cannot be
FCFS since it can poorly utilize the interconnect resources
1The interconnect may be composed of more than two planes
but in this work we assume it is composed of two.
as it may delay the realization of circuits even when their
data messages are ready.
For example, in Fig. 1 the data request ReqA is traveling
to a far node, RN , and reserves a circuit, CA , at routers R1
and R2 , for its anticipated reply. On the other hand, ReqB
is traveling to a near node, R2 , and reserves a circuit CB also
at routers R1 and R2 immediately after ReqA . In this example, ReqB arrives at R2 much earlier than the time at which
ReqA arrives at RN . Assuming both requests hit in the
cache, ReplyB , the reply to ReqB , becomes ready much earlier than ReplyA , the reply to ReqA . However, with FCFS
ordering, circuit CA would be realized before CB , thus delaying the ready message ReplyB . Conversely, if circuits are
realized based on their expected utilization times, CB would
be realized before CA , and ReplyB would not suﬀer unnecessary delay. The proposed circuit pre-allocation improves
the circuits realization order using approximate predictions
of the arrival times of reply messages as described next.
Figure 1: Example showing that realizing reverse
circuits in a FCFS order can result in poor utilization of the NoC resources (See Section 2.2)
Approximate Time-Based Circuit Reservation.
Consider the following example. Router R1 sends a data
request to RN and this request has to traverse 10 routers on
the path to RN . Assume that a hop takes 3 cycles on the
packet-switched control plane. Assume that the request will
hit in the cache and that it takes 5 cycles to read the cache
line. On the circuit-switched data plane communication latency is 1 cycle per hop. Thus, assuming the request and
reply face no delays, the minimum duration of the roundtrip since sending the request and until receiving the ﬁrst
ﬂit of the reply is: the request travel time + cache processing time of the request + the reply travel time = 3x10
+ 5 + 1x10 = 45 cycles. Assume that R1 sends the request at cycle 100. Then the request reserves the circuit at
R1 with expected utilization cycle = 145, and on the next
router, R2 , the request reserves the circuit with expected
utilization cycle = 144 and so on, until it reaches RN and
reserves the circuit with expected utilization cycle = 136.
Essentially, the request carries the estimate, c, of the cycle
number at which the circuit is expected to be utilized at the
next router, R, where the circuit will be reserved. After the
circuit reservation is successfully added to R, the request’s
carried estimate is decreased by one to become c = c − 1,
and the request message advances to the next router on the
path to the request’s destination.
In the example, the expected circuit utilization cycle is
based on the minimum time for the round-trip that starts
with injecting the request and ends with receiving the reply
message. Unfortunately, the three components that make up
the round-trip time: request travel time, request processing
time, and reply travel time, will not always take the minimum time, nor can they be precisely determined. The travel
time of the request and reply messages may be aﬀected by
other traﬃc in the NoC. Similarly, the processing time may
vary depending on whether the cache can process the request immediately, whether the request hits or misses in the
cache, the cache may forward the request to the requested
cache line’s owner, or the cache may even reply with a negative acknowledgment indicating that the request should be
retried.
Since determining the round-trip precisely is not possible, the next best thing is to estimate how long a roundtrip would take, and include with the circuit’s reservation at
each router the circuit’s estimated utilization cycle at that
router. Routers would then realize circuits in ascending order of their estimated circuit utilization cycles, which need
not exactly coincide with the actual cycles that the reply
messages traverse the routers as long as the traversal order
is preserved.
An intuitive way to estimate the round-trip time from R1
to RN is to assume it is similar to the observed round-trip
time when R1 last sent a request to RN . However, large variability in request processing times can adversely aﬀect the
round-trip estimation. Better estimates can be derived by
averaging or using the median of previously observed round
trip times2 . Next we describe how reservations are ordered
and realized.
2.3 Realizing Reserved Circuits
When a circuit is reserved at a router, the expected utilization cycle (EUC ) of the circuit is included in the reservation.
Each port – whether input or output – has a separate reservation buﬀer (RB ) to store its circuit reservations. Rather
than realizing circuits in the order they were added to the
reservation buﬀers, routers realize circuits in ascending order of their expected utilization cycles. Speciﬁcally, each
port maintains a pointer, pmin to the reservation, resmin ,
having the earliest EU C . When a new reservation, resnew
is added to RB , its expected utilization cycle, EU Cnew , is
compared to EU Cmin , the EUC of resmin , and pmin is updated if necessary (Fig. 2). Circuits are realized by matching
the reservations pointed to by pmin pointers in each of the
RB s of the input and output ports, as the following example
demonstrates.
Figure 2: Checking if the new reverse reservation
has the earliest EUC among existing reservations.
Consider for example that at some router two data re2Estimating round-trip times is discussed in Section A in
the appendix.
quest messages, r1 and r2 , reserve the crossbar connections:
west-east (i.e., west output port and east input port) and
south-east, respectively, such that the EU C of r1 ’s reservation is earlier than that of r2 ’s. Assume both reservations
become the ones with the earliest EU C s in the RB s of the
west and south input ports (Fig. 3). Because the EU C of
r1 ’s reservation is earlier than that of r2 ’s, the east output
port realizes r1 ’s reservation before r2 ’s, i.e., the west-east
crossbar connection gets realized before the south-east.
Figure 3: Example: Realizing circuit reservations in
ascending order of their EU C s. The west-east connection is realized before the south-east connection.
Once a circuit’s connection is realized at a router, the
connection remains active until the tail ﬂit of the message
traveling on the circuit traverses the crossbar, at which time
the input and output ports of the connection become free to
participate in realizing subsequent circuit reservations. Correct routing requires that each node injects data messages in
the data plane in ascending order of the their circuit reservations’ EU C s. Further, the EU C s of any two circuit reservations ensure a consistent realization order of the circuits
in all the ports they share on their paths3 .
Note that since EUCs are only estimates that may not
coincide with the cycles at which packets traverse routers,
and since there is always the chance that a new reservation
having an earlier EUC than all reservations in a port’s RB
may be added4 , circuits are realized only after a packet is
incoming to an input port, which can be detected through a
look-ahead signal: each output port matched during switch
allocation signals its corresponding input port on the next
router that a packet is incoming. Once a circuit is realized,
its input and output ports update their pmin pointers to
point to the next reservation with the earliest EUC.
3. HANDLING CASES WHEN CIRCUIT
PRE-ALLOCATION IS NOT POSSIBLE
There are cases when circuit pre-allocation is not possible.
For example, write-back messages sent upon evicting a dirty
cache line are not preceded by a request, hence there are
no pre-allocated circuits for such messages. Additionally,
data request messages may not always reserve circuits. For
example, when sending a data request if there is not a good5
estimate for when the reply data message will arrive at the
requester, it may be better not to pre-allocate a circuit.
3Section D in the appendix discusses ensuring consistent
ordering of realizing circuits.
4Section B in the appendix discusses ensuring correct routing even when a new circuit with an early EU C is reserved,
while Section C discusses deadlock prevention.
5Section A in the appendix discusses the quality of roundtrip estimates.
There are also cases when a circuit is partially or completely reserved but should be removed. For example, when
a request misses in the cache, the requested cache line is
fetched from the oﬀ-chip memory, which takes a relatively
long time. If this request’s circuit is kept until the line is
fetched, it can delay the realization of other circuits, which
hurts performance; instead a message should be dispatched
in place of the data reply message to utilize and remove the
circuit. Another example is a reservation conﬂict4 , which
– although rare – may occur while reserving a new circuit.
If not handled, a reservation conﬂict can cause miss-routing
of already in-ﬂight data messages; thus the partial reservation of the new circuit need to be removed. In all the above
cases the data messages still need to be sent, and because
we design the data plane to be circuit-switched, we choose
to fall back to FCFS [1] to reserve forward circuits. In this
section we explain how the reverse and forward reservations
are simultaneously supported in the NoC.
There are two main distinctions between reverse and FCFS [1]
reservations: the direction of reserving the circuit and the
order of circuit realization. These distinctions require the reverse and FCFS reservations be separated and require that
the packets traveling on these two types of reserved circuits
be separated as well. I.e., each port maintains future reverse
and FCFS reservations in separate buﬀers, and two virtual
channels (VCs) are required on the data plane, one for packets traveling on reverse circuits and the other for packets
traveling on FCFS circuits. Conﬁguring the crossbar of a
data plane router is based on the result of matching either:
reverse reservations having the earliest EUCs in the RBs of
the input and output ports (Section 2.3), or the heads of
FCFS reservation queues of input and output ports – since
the FCFS reservations are already queued in their order of
realization. To improve the quality of matching, in each cycle separate matching of the reverse and FCFS reservations
is carried out with priority given to the decisions of one of
them based on a particular arbitration policy such as round
robin. Fig. 4 shows the architecture of the control and data
plane routers which support both kinds of reservations. The
top router depicts the control plane router which is packetswitched, and communicates to the data plane router reverse
and FCFS circuit reservations made by data request and
FCFS circuit reservation messages, respectively. The bottom router depicts the data plane router connected to the
control plane router at the same node. It is circuit-switched
and has reservation buﬀers for both reverse and FCFS circuits, and has two VC ﬂit buﬀers at each input port, one for
the packets traveling on reverse circuits and one for packets
traveling on FCFS circuits.
4. EVALUATION
We evaluate the proposed proactive circuit allocation (PRO)
scheme through simulations of benchmarks from the SPLASH2 [20], PARSEC [3], and Specjbb [18] suites using the functional simulator Simics [17]. We assume a 16-core CMP
with 3 GHz UltraSPARC III in-order cores with instruction
issue width of three. Each core has private 16 KB L1 data
and instruction caches with an access latency of one cycle.
The CMP has a distributed shared L2 with 1MB per core.
Cache lines are 64 bytes, and each is composed of eight 8byte words. Cache coherency is maintained with the MESI
protocol. Similar to [1], a stalled instruction waiting for an
L1 miss to be satisﬁed is able to execute once the critical
Figure 4: Diagrams of the control and data plane’s
routers with support for both FCFS and reverse
reservations.
word is received, which is sent as the ﬁrst word in the data
reply packet. We assume the cache is optimized for fast access. From Cacti [4], at 3 GHz and 32nm technology the
access cycles of the L2 tag and data arrays are two and four
cycles, respectively, for a 1MB L2 per tile partitioned into
two banks. The NoC’s topology is a 2D mesh.
We evaluate a CMP with the PRO NoC against CMPs
with:
(1) a purely packet-switched NoC (PKT), (2) the
state-of-the-art FCFS circuit reservations NoC [1], and (3)
a zero-overhead Ideal NoC. Each of the evaluated NoCs is
composed of two planes: a control plane that carries control
and cache coherency messages, and a data plane that carries data messages. The control plane is packet-switched in
all four NoCs, while the data plane is only packet-switched
in the PKT NoC and circuit-switched in the other three
NoCs. In the data plane of the Ideal NoC all possible circuits are assumed to simultaneously exist, such that all the
circuit-switched ﬂits experience only one-cycle per hop without suﬀering any network delays due to contention. Below,
we describe the conﬁguration of the simulated NoCs.
Packet-Switching and Message Sizes We simulate a
three cycle pipeline for packet-switched routers. In general,
messages on the control plane are one ﬂit long, while messages on the data plane are ﬁve ﬂits long. For PRO, data
request messages may be composed of either one or two ﬂits.
If the request will reserve a circuit for its reply, the request
message is composed of two ﬂits due to the additional space
required to carry the circuit’s EU C ; otherwise it is composed of one ﬂit.
Virtual Channels The control plane has four virtual
channels (VCs). Control plane routers have a FIFO buﬀer
for two packets per VC per input port. The data plane of
PKT, FCFS, and the Ideal NoCs, each has only one channel
for data messages, while the data plane of the proposed PRO
NoC has two VCs, one for the messages traveling on reverse
circuits and one for the messages traveling on FCFS circuits.
The routers of the data plane have a FIFO buﬀer for two
data packets per input port. In the case of PRO, the FIFO
buﬀer of each VC can hold one data packet.
Circuit Reservation Buﬀers In the PRO NoC, each
router port has two circuit reservation buﬀers, one for the
reverse and one for the FCFS reservations. The buﬀers can
hold 12 reverse reservations and 5 FCFS reservations, per
port. In the FCFS NoC, each port has only one buﬀer for
FCFS reservations; we set its size to 17, the total number of
reservations a port on the PRO NoC can store.
Estimating Round-Trip Time At the requesting node,
we estimate the round-trip time by computing the median
of the last observed three round-trip times for the request
message’s destination. However, large estimates tend to be
inaccurate which hurt performance6 . To reduce such inaccurate estimates, we allow a data request message to reserve a
circuit only if the estimate is at most X times the minimum
round-trip time. After experimenting with the design space,
we chose to set X = 2.
4.1 Performance Evaluation
We simulate the parallel section of each benchmark. First,
we compare the average latency of satisfying an L1 miss that
hits in the L2, or simply the average L2 hit latency, which is
essentially the average round-trip time for sending a request
that hits in the L2 and receiving its reply. Fig. 5 shows
the average L2 hit latency of the three CMPs: (1) with the
FCFS NoC; (2) with the PRO NoC; and (3) with the Ideal
NoC. The results displayed in all the ﬁgures are relative to
the CMP with the purely packet-switched NoC (PKT). With
the FCFS NoC there is only a modest improvement in the
L2 hit latency, while with the PRO NoC we see a signiﬁcant improvement for almost all the benchmarks except for
a couple of benchmarks (the contiguous version of LU and
Water Spatial did not beneﬁt from the PRO NoC).
Figure 6:
Identiﬁcation of communication sensitive benchmarks by examining the execution time
speedup using the Ideal NoC (the Y-axis starts at
1.0)
in Fig. 7. The speedups of the communication sensitive
benchmarks are displayed on the right side of the chart. The
system with FCFS achieves an average speedup of only 2%
over the system with PKT. The system with PRO achieves
up to 16% speedup (Raytrace and Specjbb), with an average
of 8% over the system with FCFS, and an average of 10%
over the system with PKT. On the left side of the chart the
speedups of the communication insensitive benchmarks are
displayed. With FCFS there is almost no speedup, while
with PRO there is a nominal speedup (2%, on average).
Figure 5: Average L2 hit latency normalized to the
purely packet-switched system (the Y-axis starts at
0.7).
Since the execution time of each benchmark may not be
sensitive to the communication latency over the NoC, we examine the execution time speedup achievable with the Ideal
NoC (Fig. 6) and classify the benchmarks into two groups:
communication sensitive with a speedup of at least 4% and
communication insensitive with a speedup of less than 4%.
Based on this classiﬁcation, we compare the execution
time speedup achievable with the FCFS and PRO NoCs
6Section A in the appendix discusses estimating round-trip
times.
Figure 7: Execution time speedup of CMPs with
the FCFS and PRO NoCs (the Y-axis starts at 1.0).
Communication sensitive benchmarks are displayed
on the right of the chart.
Fig. 8 shows how much of the potential execution time
speedup achievable with the Ideal NoC that the systems with
the FCFS and PRO NoCs achieve. The CMP with FCFS
gains only between 1% to 24%, with an average of 12%,
compared with the ideal case, while the CMP with PRO
gains much more; between 40% and 89%, with an average
of 68%.
4.1.1 FCFS Circuits as a Fallback
As mentioned in Section 3, there are situations that require releasing reverse circuit reservations.
In Fig. 9 we
examine the percentage of released circuits relative to the
number of circuit reservations. We ﬁnd that the ma jority
of circuits are released due to processing times that exceed
a threshold (for example, upon a cache miss to the oﬀ-chip
memory), which can reach more than 25% for several applications. The percentage of circuits released due to potential
on the same plane and speedup communication by using local router information to let packets bypass router pipelines
whenever possible, which is orthogonal to our work and can
be incorporated in the control plane in our design.
6. CONCLUSION
Circuit-switching is eﬀective in speeding up communication when the overhead of setting up circuits is reduced or
amortized with re-use of circuits. This work proposes a
proactive scheme for circuit allocation to completely hide
the circuit setup overhead for reply messages by having the
request messages reserve the circuits for their anticipated
replies. Reserving circuits by requests requires using timebased reservations to avoid holding NoC resources unnecessarily idle which under-utilizes the NoC. However, variability in network traﬃc conditions and request processing
times make it impossible to use accurate time-based reservations. Hence, we use approximate time-based reservations
by estimating the round-trip time from the time when a
request is sent and until its reply is received. We demonstrate the beneﬁt of our design through simulations of parallel benchmarks. For a CMP with a fast on-chip cache,
while the state-of-the-art circuit-switched NoC using FCFS
circuit reservations achieves execution time speedup of up
to 3% and an average of about 2% over a purely packetswitched NoC, our proposed scheme enables execution time
speedup of up to 16% and an average of about 10% over
the purely packet-switched NoC; outperforming the stateof-the-art FCFS reservations scheme by up to 16% and an
average of 8%.
7. "
A heterogeneous multiple network-on-chip design - an application-aware approach.,"Current network-on-chip designs in chip-multiprocessors are agnostic to application requirements and hence are provisioned for the general case, leading to wasted energy and performance. We observe that applications can generally be classified as either network bandwidth-sensitive or latency-sensitive. We propose the use of two separate networks on chip, where one network is optimized for bandwidth and the other for latency, and the steering of applications to the appropriate network. We further observe that not all bandwidth (latency) sensitive applications are equally sensitive to network bandwidth (latency). Hence, within each network, we prioritize packets based on the relative sensitivity of the applications they belong to. We introduce two metrics, network episode height and length, as proxies to estimate bandwidth and latency sensitivity, to classify and rank applications. Our evaluations show that the resulting heterogeneous two-network design can provide significant energy savings and performance improvements across a variety of workloads compared to a single one-size-fits-all single network and homogeneous multiple networks.","A Heterogeneous Multiple Network-On-Chip Design:
An Application-Aware Approach
Asit K. Mishra
Intel Corporation
Hillsboro, OR 97124, USA
asit.k.mishra@intel.com
Onur Mutlu
Carnegie Mellon University
Pittsburgh, PA 15213, USA
onur@cmu.edu
Chita R. Das
The Pennsylvania State University
University Park, PA 16802, USA
das@cse.psu.edu
ABSTRACT
Current network-on-chip designs in chip-multiprocessors are agnostic to
application requirements and hence are provisioned for the general case,
leading to wasted energy and performance. We observe that applications can generally be classiﬁed as either network bandwidth-sensitive or
latency-sensitive. We propose the use of two separate networks on chip,
where one network is optimized for bandwidth and the other for latency,
and the steering of applications to the appropriate network. We further
observe that not all bandwidth (latency) sensitive applications are equally
sensitive to network bandwidth (latency). Hence, within each network, we
prioritize packets based on the relative sensitivity of the applications they
belong to. We introduce two metrics, network episode height and length,
as proxies to estimate bandwidth and latency sensitivity, to classify and
rank applications. Our evaluations show that the resulting heterogeneous
two-network design can provide signiﬁcant energy savings and performance improvements across a variety of workloads compared to a single
one-size-ﬁts-all single network and homogeneous multiple networks.
Categories and Subject Descriptors
C.1.2 [Computer Systems Organization]: Multiprocessors; Interconnection architectures
Keywords
Heterogeneity, On-chip Networks, QoS, Packet Scheduling
1.
INTRODUCTION
Network-on-Chips (NoCs) are envisioned to be a scalable communication substrate for building multicore systems, which are expected to execute a large number of different applications and threads concurrently to
maximize system performance. A NoC is a critical shared resource among
these concurrently-executing applications, signiﬁcantly affecting each application’s performance, system performance, and energy efﬁciency. Traditionally, NoCs have been designed in a monolithic, one-size-ﬁts-all manner, agnostic to the needs of different access patterns and application characteristics. Two common solutions are to design a single NoC for (1)
the common-case, or average-case, application behavior or (2) the nearworst case application behavior, by over-provisioning the design as much
as possible to maximize network bandwidth and to minimize network latency. However, applications have widely different demands from the network, e.g., some require low latency, some high bandwidth, some both,
and some neither. As a result, both design choices are suboptimal in either performance or energy efﬁciency. The “average-case” network design
cannot provide good performance for applications that require more than
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC ’13, May 29 - June 07 2013, Austin, TX, USA.
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
the supported bandwidth or that beneﬁt from lower latency. Both network designs, especially the “over-provisioned” design, are power- and
energy-inefﬁcient for applications that do not need high bandwidth or low
latency. Hence, monolithic, one-size-ﬁts-all NoC designs are suboptimal
from performance and energy standpoints.
Ideally, we would like a NoC design that can provide just the right
amount of bandwidth and latency for an application such that the application’s performance requirements are satisﬁed (or its performance maximized), while the system’s energy consumption is minimized. This can be
achieved by dedicating each application its own NoC that is dynamically
customized for the application’s bandwidth and latency requirements. Unfortunately, such a design would not only be very costly in terms of die
area, but also requires innovations to dynamically change the network
bandwidth and latency across a wide range. Instead, if we can categorize
applications into a small number of classes based on similarity in resource
requirements, and design multiple networks that can efﬁciently execute
each class of applications, then we can potentially have a cost-efﬁcient
network design that can adapt itself to application requirements.
Building upon this insight and drawing inspiration from the embedded
and ASIC designs where a single network is customized for a single application, this paper proposes a new approach to designing an on-chip interconnect that can satisfy the diverse performance requirements of generalpurpose applications in an energy-efﬁcient manner. We observe that applications can be divided into two general classes in terms of their requirements from the network: bandwidth-sensitive and latency-sensitive. Two
different NoC designs, each of which is customized for high bandwidth or
low latency, can, respectively, satisfy requirements of the two classes in a
more power-efﬁcient manner than a monolithic single network. We, therefore, propose designing two separate heterogeneous networks on a chip,
dynamically monitoring executing applications’ bandwidth and latency
sensitivity, and steering/injecting network packets of each application to
the appropriate network based on whether the application is deemed to be
bandwidth-sensitive or latency-sensitive. We show that such a heterogeneous design can achieve better performance and energy efﬁciency than
current average-case one-size-ﬁts-all NoC designs.
To this end, based on extensive application proﬁling, we ﬁrst show that
a high-bandwidth, low-frequency network is best suited for bandwidthsensitive applications and a low-latency, high-frequency network is best
for latency-sensitive applications. Next, to steer packets into a particular
network, we identify a packet’s sensitivity to network latency or bandwidth. For this, we propose a new packet classiﬁcation scheme that is
based on an application’s intrinsic network requirements. We introduce
two new metrics, network episode length and height, to dynamically identify the communication requirements (latency and bandwidth sensitivity)
of applications. Observing that not all applications are equally sensitive to
latency or bandwidth, we propose a ﬁne-grained prioritization mechanism
for applications within the bandwidth and latency optimized networks.
Thus, our mechanism consists of ﬁrst dynamically classifying an application as latency or bandwidth sensitive, then steering it into the appropriate
network and, ﬁnally within each network prioritizing each application’s
packets based on its relative potential to improve overall system performance and reduce energy.
Our evaluations on a 64-core 2D mesh architecture, considering 9 design alternatives with 36 diverse applications, show that our heterogeneous two-network NoC design consisting of a 64b link-width latency!""
#""
$""
%""
&""
’""
(""
)""
+
+
*
,
""
.
""
0
/
*
1
/
""
*
3
2
""
,
4
5
7
6
3
""
*
8
/
3
6
4
""
7
/
9
4
:
""
*
6
9
2
""
&
(
$
;
""
:
:
7
""
<
+
/
=
*
""
1
6
>
1
>
""
?
,
@
8
""
8
>
7
9
A
""
*
1
4
*
/
""
9
,
?
:
""
;
9
9
3
/
""
4
.
?
9
""
4
5
8
8
""
4
+
*
""
6
*
*
B
,
""
4
6
;
+
B
""
8
C
?
+
""
,
8
9
""
4
5
*
4
""
4
+
>
,
B
""
:
*
:
4
1
""
>
9
3
6
1
""
3
7
9
4
""
9
:
""
0
D
E
F
""
""
>
6
/
9
""
G
1
8
&
(
>
""
?
,
""
4
A
6
""
H
(&8"",?6A4""
#$I8"",?6A4""
$’(8"",?6A4""
’#$8"",?6A4""
J>.341""
J#KLMD""
N?7;341""
J#KLMD""
Figure 1: Instruction throughput (IT) scaling of applications with increase in network bandwidth.
!""#$
!""%$
!""&$
!""’$
(""!$
(""($
,
*
*
)
+
$
$
/
.
)
0
.
$
)
2
1
$
+
3
4
6
5
2
$
)
7
.
2
5
3
$
6
.
8
3
9
$
)
5
8
1
$
<
#
;
:
$
9
9
6
$
=
*
.
>
)
$
0
5
?
0
?
$
@
+
A
7
$
7
?
6
8
B
$
)
0
3
)
.
$
8
+
@
9
$
:
8
8
2
.
$
3
@
8
$
3
4
7
7
$
3
*
)
$
5
)
)
C
+
$
3
5
:
*
C
$
7
D
@
*
$
+
7
8
$
3
4
)
3
$
3
*
?
+
C
$
9
)
9
3
0
$
?
8
2
5
0
$
2
6
8
3
$
8
9
$
/
E
F
G
$
?
5
.
8
$
""
0
;
?
$
H
2
9
>
9
+
$
.
,
?
0
2
$
I
.
;H9>9+2$.?,02.$
<H9>9+2$.?,02.$
#H9>9+2$.?,02.$
Figure 2: Instruction throughput (IT) scaling of applications with increase in router latency.
optimized network and a 256b link-width bandwidth-optimized network,
provides 5%/3% weighted/instruction throughput improvement and 31%
energy reduction over an iso-resource (320b link-width) monolithic network design. When compared to a baseline 256b link-width monolithic
network, our proposed design provides 18%/12% weighted/ instruction
throughput improvement and 16% energy reduction.
2. COMMUNICATION CHARACTERIZATION
We provide observations that highlight the intrinsic heterogeneity in
network demand across applications. These observations form the motivation for an application-aware NoC design. We start by looking at two
fundamental parameters: network channel bandwidth and latency.
Impact of channel bandwidth on performance scaling: Channel or
link bandwidth is a critical design parameter that affects network latency,
throughput and energy/power. To study the sensitivity of an application to
variation in link bandwidth, we use a 64-core chip-multiprocessor (CMP)
on an 8x8 mesh network, where both cores and network clocked at the
same frequency, and run a copy of the same application on all nodes on the
network. Table 1 shows the system conﬁguration. We chose applications
from commercial, SPEC CPU2006, SPLASH and SPEC OMP suites. We
analyze scenarios where we double the bandwidth starting with 64-bit
links up to 512-bit links. Figure 1 shows the results of this analysis for
30 of the 36 applications in our benchmark suite (6 non-network-sensitive
applications are omitted to reduce clutter in the plots). In this ﬁgure, the
applications are shown on the X-axis in order of their increasing L1MPKI
(L1 misses per 1000 instructions). The Y-axis shows the average instruction throughput when normalized to the instruction throughput of the 64b
network.
Observations from this analysis are: (1) Of the 30 applications shown,
performance of 12 applications (the rightmost 12 in the ﬁgure after swim)
scales with increase in channel bandwidth. For these applications, an 8X
increase in bandwidth results in at least a 2X increase in performance.
We call these applications bandwidth-sensitive applications. (2) The other
18 applications (all applications to the left of and including swim), show
very little to no performance improvement with increase in network bandwidth. (3) Even for bandwidth-sensitive applications, not all applications’
performance scales equally with increase in bandwidth. For example,
while omnet, gems and mcf show more than 5X performance improvement for 8X bandwidth increase, applications like xalan, soplex and
cacts show only 3X improvement for the same bandwidth increase. (4)
L1MPKI is not necessarily a good predictor of bandwidth-sensitivity of
applications. Intuitively, applications that have high L1MPKI would inject more packets into the network, and hence would beneﬁt more from a
higher-bandwidth network. But this intuition does not hold entirely true.
For instance, bzip, despite having a higher L1MPKI than xalan, is less
performance-sensitive to bandwidth than xalan. Thus, we need a better
metric to identify bandwidth-sensitive applications.
Impact of network latency on performance scaling: Next, we analyze
the impact of network/router latency on the instruction throughput of these
applications. For this, we add an extra pipeline latency of 2 and 4 cycles to
each router (in the form of dummy pipeline stages) on top of the baseline
router’s 2-cycle latency. The cores and the network are clocked at the
same frequency for this analysis. Increasing the pipeline stages at each
router mimics additional contention in the routers when compared to the
baseline network. Figure 2 shows the results for this analysis, where the
channel bandwidth is 128b (although the observation from this analysis
holds true for other channel bandwidths as well).
Our observations are the following: (1) Bandwidth-sensitive applications (the rightmost 12 applications) are not very responsive to increase
in network/router latency. On average, for a 3X increase in per-hop latency, there is only 7% degradation in application performance (instruction throughput) for these applications, i.e., an extra 4 cycle latency per
router is tolerated by these applications. (2) On the other hand, for all
applications to the left of and including swim, there is about 25% performance degradation when the router latency increases from 2-cycles to
6-cycles. We call these latency-sensitive applications. (3) L1MPKI is not
a perfect indicator of latency-sensitivity (hmmer, despite having a higher
L1MPKI than h264, does not show proportional performance improvement with reduction in router latency).
Application-level implications on network design: The above analysis
suggests that a single monolithic network is not the best option for various application demands. Therefore, an alternative approach to designing
an on-chip interconnect is to have multiple networks, each of which is
specialized for common application requirements, and dynamically steer
requests of each application to the network that matches the application’s
requirements. Based on Figures 1 and 2, a wide and low-frequency network is suitable for bandwidth-sensitive applications, while a narrow and
high-frequency network suitable for latency-sensitive ones. To improve
the performance of the latency-sensitive applications, a network architect
can reduce the router pipeline latency from 2-cycles (our baseline) to a single cycle, while keeping the frequency constant, or increase the network
frequency (to reduce network latency). Although there are proposals that
advocate for single-cycle routers [10, 13, 12, 6], such designs often involve
speculation, which increases complexity and can be ineffective at high or
adverse load conditions, and require relatively sophisticated arbiters that
are not necessarily energy efﬁcient. Hence, while single-cycle routers are
feasible, in this paper, we use frequency as a knob to reduce the network
latency. According to our analysis, increasing the frequency of the network from 1 GHz to 3 GHz (3 times the core frequency) leads to less
than 1.5% increase in energy for latency-sensitive applications (results for
energy with frequency scaling are omitted for brevity).
Designing latency- and bandwidth-customized networks is the ﬁrst step
in achieving customization in the network. We also need a runtime mechanism to classify applications into one of the two categories: latency or
bandwidth sensitive. In addition, since not all applications are equally sensitive to bandwidth or latency, we would like a mechanism that ranks the
applications within each category in a more ﬁne-grained manner within
the latency- and bandwidth-customized networks. The next section discusses how we perform application classiﬁcation.
3. DYNAMIC CLASSIFICATION OF APPLICATIONS
i
t
t
s
t
k
r
o
w
t
e
N
t
s
u
O
h
g
e
h
i
e
k
c
a
P
e
d
o
s
p
E
i
g
n
d
n
a
Episode length 
Network episode Compute episode 
The goal of dynamically identifying an application’s
sensitivity to latency
or bandwidth is to
enable the local network interface (NI)
Figure 3: Network and compute episodes.
at each router to steer packets into a network that has been optimized for
either latency or bandwidth. We propose two new metrics, called network
episode length and episode height, that effectively capture the network
latency and bandwidth demands of an application.
Episode length and height: During its life cycle, an application alternates between two kinds of episodes (shown in Figure 3): (1) network
episode, where the application has at least one packet (to L2 cache or
to DRAM) in the network, and (2) compute episode, where there are no
outstanding cache/memory requests by the thread. During the network
phase, there may be multiple outstanding packets from the application in
the network owing to various techniques that exploit memory-level parallelism (MLP) [5, 14]. During this network phase, the core is likely stalled,
waiting for L2 and memory requests to be serviced. Because of this, the
instruction throughput of the core is low. During the compute episode,
however, the instruction throughput is high [9]. We characterize a network episode by its length and height. Length is the number of cycles
the episode lasts starting from when the ﬁrst packet is injected into the
network until there are no more outstanding packets belonging to that
episode. Height is the average number of packets (L1 misses) injected
by the application during the network episode. To compute height, the
core hosting the application keeps track of the number of outstanding L1
misses (when there is at least 1 L1 miss) in the re-order buffer on a percycle basis.
A short episode height suggests that the application has low MLP. In
other words, the latencies of the requests are not overlapped, and as a result
the application’s progress is sensitive to network latency. The application
also does not require signiﬁcant bandwidth because it has a small number
of outstanding requests at any given time. On the other hand, a tall episode
height suggests that the application has a large number of requests in the
network, and the network latency of the packets are overlapped. This
indicates that the application likely needs signiﬁcant bandwidth from the
network to make progress while its progress is less sensitive to network
latency. Hence, we use the network episode height of an application as
the main indicator of latency or bandwidth sensitivity of an application.
A short episode length (on average) suggests that the application is less
network intensive. Contrast this with an application that has a long episode
length, i.e., more network intensive. An equal amount of network delay
would slow down the former application more than it does the latter application. As a result, the former application with a short average episode
length is likely to be more sensitive to network latency, and therefore
would likely beneﬁt from being prioritized over another application with
a longer average episode length. Note that this observation is similar to
Kim et al.’s for memory controllers [9], which showed that prioritizing an
application with shorter memory episodes over a one with longer memory
episodes is likely to improve performance.
We compute running averages of the episode height and length to keep
track of these metrics at runtime. We quantize episode height as tall,
medium or short and episode length as long, medium and short. This
allows us to perform ﬁne-grained dynamic application (or, application
phase) classiﬁcation based on episode length and height. Figures 4 and 5
show these metrics for 30 applications in our benchmark suite. Note that,
these ﬁgures show the average metrics for an entire application and that
there are intra-application latency/bandwidth sensitive phases that our dynamic scheme (Section 4) captures. Based on Figures 1 and 2, we classify
all applications whose average episode length and height are shorter than
sjbb’s episode length and height, respectively, to be short in length and
height (shaded black in the ﬁgures). Applications whose average episode
height is larger than sjbb’s episode height but lower than 7 (empirically
chosen) are classiﬁed as having medium (shaded blue in the ﬁgures) and
the remaining as having tall episode heights (shaded with hatches in Figure 4). Empirically, a cut-off of 10K cycles is chosen to classify applications as having medium vs. long episode length.
Classiﬁcation and ranking of applications: Figure 6 (left) shows the
classiﬁcation of applications based on their episode height and length. The
ﬁgure also shows the bandwidth-sensitive and latency-sensitive applications based on such a classiﬁcation. We use this ﬁne-grained classiﬁcation
to steer applications to the two networks as well as rank applications for
prioritization within a network.
Episode height, as a general principle, is used to identify the bandwidth/latency sensitivity of applications. Applications with tall height are
considered bandwidth sensitive (due to high MLP, which leads to high
bandwidth demand as well as high latency tolerance), and steered to the
bandwidth-optimized network. Applications with low height are latency
sensitive (due to low MLP, which leads to low latency tolerance for each
packet) and are steered to the latency-optimized network. Figure 6 shows
the resulting classiﬁcation of applications. Note that applications whose
episode height is medium but episode length is short are classiﬁed as
latency-sensitive as they have neither short nor tall episode heights, but
demand less from the network due to short episodes.
Episode length, for the most part, is used to determine the ranking of
applications within each network. The general principle is to prioritize
applications with shorter network episode length over others because delaying such applications in the network has much more of an effect on their
performance (slowdown) than delaying applications with long episode
lengths (since the latter set of applications are already slow due to long
network episodes anyway). Figure 6 (right) shows the resulting ranking
of the applications in their respective networks based on relative episode
length and height.
4. DESIGN DETAILS
Since we use a canonical 2D network in our study, instead of discussing
the standard router and network designs, we focus on the design aspects
for supporting our classiﬁcation and prioritization schemes in this section.
Computing episode characteristics: To ﬁlter out short-term ﬂuctuations
in episode height/length, and adapt our techniques to handle long-term
trafﬁc characteristics, we use running averages of the metrics. On every
L1 miss, the NI computes the running average of episode height/length.
To compute episode height, the outstanding L1 miss count is obtained
from the miss-status handling registers (MSHRs). Counting the number
of cycles (using an M -bit counter) the L1 MSHRs are occupied gives the
information to compute episode length. This M -bit counter is reset every
batching interval, B . We use the notion of batching [3, 15] to prevent
starvation due to ranking of applications in each network, as done in [3,
15, 2] (more information on this is in Section S.3).
When the NI of a router receives a packet, it: (1) updates the episode
height/length metric for the current application phase and (2) decides which
network this packet is to be steered to, based on the classiﬁcation scheme
(Section 3). Thus, the episode metrics are computed per phase of an application. All packets belonging to a particular phase are steered into the
network optimized for either latency or bandwidth. Note that each application’s rank and network sensitivity are decided at runtime (although the
classiﬁcation analysis we provided in Section 3 was for the whole application). This helps our scheme to capture within-application variation in latency and bandwidth sensitivity. No central coordination is required in our
technique to decide a uniform central ranking across all the applications
in the system, which was needed in past works that ranked applications
for prioritization [3, 2]. Once a packet’s rank is decided, it is consistently
prioritized with that rank across the entire network until it reaches its destination.
The NI tags the transmitted packet with its rank (2-bits) and its batch-id
(3-bits). At each router, the priority bits in the header-ﬂit are utilized by
the priority arbiters in a router to allocate the virtual channels (VCs) and
the switch. To prevent priority inversion due to VCs in routers, where a
packet belonging to an older batch or higher rank is queued behind a lower
ranked packet, we use atomic buffers [16].
Customized network design choices: As mentioned earlier, we opt for
a high-frequency but low link-width network for the latency-sensitive applications and a high-bandwidth network operating at the same frequency
as the cores for the bandwidth-sensitive applications. We use a 2-stage
baseline router and increase the router frequency up to 3 times for the
latency-sensitive network. High-frequency routers can be designed by a
combination of both micro-architecture and circuit optimizations as shown
 
 
 
 
 
!""
#!!!""
$!!!""
%!!!""
&!!!""
’!!!""
(!!!""
,
*
*
)
+
""
""
/
.
)
0
.
""
)
2
1
""
+
3
4
6
5
2
""
)
7
.
2
5
3
""
6
.
8
3
9
""
)
5
8
1
""
&
(
$
:
""
9
9
6
""
;
*
.
<
)
""
0
5
=
0
=
""
>
+
?
7
""
7
=
6
8
@
""
0
3
)
)
.
""
8
+
>
9
""
:
8
8
2
.
""
3
8
>
""
3
4
7
7
""
*
)
3
""
5
)
)
A
+
""
3
A
5
:
*
""
7
B
>
*
""
+
7
8
""
3
4
3
)
""
3
A
*
=
+
""
3
0
9
)
9
""
=
8
2
5
0
""
2
6
8
3
""
8
9
""
/
C
6
;
""
D
*
2
>
3
2
1
=
+
""
6
5
2
0
:
>
E
""
5
""
2
9
<
9
+
""
F
3
#!G""
#HG"" !D%I"" !D&I""
Figure 4: Average episode length (in cycles) across applications.
!""
#""
$""
%""
&""
’!""
’#""
’$""
+
)
)
(
*
""
,
""
.
(
/
""
(
1
0
""
*
2
3
5
4
1
""
(
6
1
4
2
""
5
7
2
8
""
(
4
7
0
""
$
%
#
9
""
8
8
5
""
:
)
;
(
""
/
4
<
/
<
""
=
*
>
6
""
6
<
5
7
?
""
/
2
(
(
""
7
*
=
8
""
9
7
7
1
""
2
7
,
=
""
2
3
6
6
""
)
(
2
""
4
(
(
@
*
""
2
@
4
9
)
""
6
A
=
)
""
*
6
7
""
2
3
2
(
""
2
@
)
<
*
""
2
/
8
(
8
""
<
7
1
4
/
""
1
5
7
2
""
7
8
""
.
B
5
:
""
C
)
1
=
2
9
5
1
9
1
0
<
""
=
/
D
""
1
4
/
,
<
1
?
8
(
)
?
""
""
E
2
/
Figure 5: Average episode height (in packets) across applications.
Ranking 
Length 
Long 
Medium 
Short 
Height 
Tall 
Rank-4 
Rank-2 
Rank-1 
Medium 
Rank-3 
Rank-2 
Rank-2 
Short 
Rank-4 
Rank-3 
Rank-1 
Classification 
Length 
Long 
Medium 
Short 
Height 
Tall 
gems, mcf 
sphinx,lbm, 
cactus, xalan 
sjeng, tonto 
Medium 
omnetpp, 
apsi 
ocean, sjbb, 
sap, bzip, sjas, 
soplex, tpc 
applu, perl, barnes, 
gromacs, namd, calculix, 
gcc, povray, h264,  
gobmk, hmmer, astar 
Short 
leslie 
art, libq, milc, 
swim  
wrf, deal 
Bandwidth-sensitive 
Latency-sensitive 
Figure 6: Application classiﬁcation and ranking based on episode length and height.
by previous works [10, 11] as well as industrial prototypes [18, 7]. In
canonical router designs, the arbitration and crossbar traversal stages are
the bottleneck stages in terms of critical path [11]. In our design, since
the latency-optimized network has only 3 VCs per physical channel and a
narrow link width (64b), our analysis (based on synthesis of the RTL of
VC and switch allocation and crossbar stages) shows that it is feasible to
clock the routers in this network at a higher frequency.
Network bandwidth depends on link-width and frequency. A designer
could think of increasing the frequency of the bandwidth-customized network to increase the total network bandwidth. However, increasing the
frequency of the wider 256b network would adversely affect the power
of this network. Hence, in our network designs, we only increase the
frequency of the narrow 64b link-width network whose power envelope
is 43% lower than the wider network.
Increasing the frequency of the
latency-customized network also increases this network’s bandwidth, but
since we steer only latency-sensitive applications (which are agnostic to
bandwidth increase) into this network, the performance improvement of
these applications is primarily due to network latency reduction.
The design space for optimizing network latency or bandwidth is large,
and not possible to cover in this paper. Although we articulate frequency
as a knob to improve latency in the latency-optimized network, a designer could use different topologies, ﬂow-control, and arbitration mechanisms as various knobs to improve latency. The motivation of this paper
is to demonstrate that heterogeneous multiple networks (customized for
latency and bandwidth separately) provides a better design than a monolithic network or homogeneous multiple networks. The 64b 3X-frequency
network for the latency-sensitive applications and 256b 1X-frequency network for the bandwidth-sensitive applications are only two design points
to demonstrate the concept.
5. EVALUATION METHODOLOGY
Design scenarios: Starting with a monolithic network, we show the beneﬁts of having two networks, each customized for either bandwidth or
latency. We also show the beneﬁts of our scheme compared to an isoresource single network (with the same total bandwidth). Following are
the nine design scenarios we evaluate on our experimental platform:
❶ 1N-128: This design has a single homogeneous 128b link network.
We assume this to be our starting point. Starting with this monolithic
network, we increase its bandwidth to create a bandwidth-optimized network, and reduce its bandwidth (and increase its frequency) to design a
latency-optimized network. ❷ 1N-256: This conﬁguration has a single
homogeneous network with 256b links. We chose this as our baseline network. Starting with this network, we ﬁrst design a homogeneous multiple
network design (where each network has equal bandwidth, 2N-128x128)
and then customize one network for latency-sensitive applications and the
other network for bandwidth-sensitive applications. ❸ 2N-128x128: This
design has two parallel networks, each with 128b link width. The buffer
resources in each network is half that of the 1N-128 network and each
of the networks operate at the same frequency as the cores. Packets are
steered into each network with a probability of 0.5, i.e., there is load balancing across the networks. ❹ 1N-512: This design has a single network
with 512b link width. We call this a high-bandwidth conﬁguration and
analyze it to see how our proposal fares compared to a very high bandwidth network. ❺ 2N-64x256-ST: In this design, there are two parallel
networks, one with 64b link width and the other with 256b link width.
The buffering resources in each network is half that of a single network,
so that the total buffering resources are constant across this design and
a design that has a single network. Further, in this conﬁguration, the
bandwidth-sensitive packets are steered (hence, the annotation ST) into
the 256b network and the latency-sensitive packets are steered into the 64b
network. Each network in this conﬁguration is clocked at the frequency
of the cores. ❻2N-64x256-ST+RK(no FS): This design is the same as
the previous network except that the network also prioritizes applications
based on their ranks (hence, the annotation RK) at every cycle in a router.
❼2N-64x256-ST+RK(FS): This design is same as the previous conﬁguration except that the 64b network is clocked at 3X the frequency of cores.
The 256b network is still clocked at the core frequency. This conﬁguration
is analyzed to see the beneﬁts of frequency scaling (hence, the annotation FS) the latency-optimized network. ❽1N-320(no FS): In this design,
there is a single network with 320b (=64b+256b) links. The network operates at the frequency of the core. This conﬁguration is iso-bandwidth
with all our 64x256 networks and is analyzed to see the beneﬁts of our
proposal over an equivalent conﬁguration. ❾1N-320(FS): This design is
similar to the above design, except that the network is now clocked at 3X
the core frequency. This design is analyzed to see the effectiveness of our
scheme over a scheme that is iso-resource as well as over-clocked to help
latency-sensitive applications.
Experimental setup: Our proposals are evaluated on an instruction-tracedriven, cycle-level x86 CMP simulator. Table 1 provides the conﬁguration
of our baseline, which contains 64 cores in a 2D, 8x8 mesh NoC. The network connects the cores, shared L2 cache banks, and memory controllers
(these all stay constant across all evaluated designs). A data packet consists of 1024b (=cache line size) and is decomposed into ﬂits depending upon the link width in each design. Since wiring resources on die
are abundant [1, 19], when simulating parallel networks, we assume the
 
 
 
 
 
 
 
 
 
 
-#6*(&7)1-+’01)
8&6$&10*9-#"":
):&#2,)
5)
5)
3""’0#*4)*+,’-.&/0$)#0’%-12)
!""#$%&$’()*+,’-.&/0$)#0’%-12)
$""
(#’""
(#&""
(#%""
(#$""
(""
!#’""
!#&""
!#%""
!#$""
!""
(
)
*
’
$
(
""
(
)
*
&
+
$
""
$
)
*
’
$
(
,
’
$
(
""
(
)
*
$
(
+
""
$
)
*
&
+
$
,
%
&
*
.
""
$
)
*
&
+
$
,
%
&
*
1
0
/
.
2
5
4
3
""
""
6
$
)
*
&
+
$
,
%
&
*
1
0
/
.
2
5
""
6
(
)
*
!
$
7
2
5
4
3
""
""
6
(
)
*
!
$
7
2
""
5
""
6
)
4
8
9
:
<
;
=
>
3
>
?
>
""
8
A
@
""
(a) Energy consumption of different networks
(#&""
(#%""
(#$""
(""
!#’""
!#&""
!#%""
!#$""
!""
(
)
*
’
$
(
""
(
)
*
&
+
$
""
$
)
*
’
$
(
,
’
$
(
""
(
)
*
$
(
+
""
$
)
*
&
+
$
,
%
&
*
.
""
$
)
*
&
+
$
,
%
&
*
1
0
/
.
2
5
4
3
""
""
6
$
)
*
&
+
$
,
%
&
*
1
0
/
.
2
5
""
6
(
)
*
!
$
7
2
5
4
3
""
""
6
(
)
*
!
$
7
2
""
5
""
6
8
9
:
2
""
4
3
;
<
""
=
(
4
""
)
*
>
3
’
$
(
""
""
6
#"
Designing energy-efficient NoC for real-time embedded systems through slack optimization.,"Hard real-time embedded systems impose a strict latency requirement on interconnection subsystems. In the case of network-on-chip (NoC), this means each packet of a traffic stream has to be delivered within a time interval. In addition, with the increasing complexity of NoC, it consumes a significant portion of total chip power, which boosts the power footprint of such chips. In this work, we propose a methodology to minimize the energy consumption of NoC without violating the prespecified latency deadlines of real-time applications. First, we develop a formal approach based on network calculus to obtain the worst-case delay bound of all packets, from which we derive a safe estimate of the number of cycles that a packet can be further delayed in the network without violating its deadline- the worst-case slack. With this information, we then develop an optimization algorithm that trades the slacks for lower NoC energy. Our algorithm recognizes the distribution of slacks for different traffic streams, and assigns different voltages and frequencies to different routers to achieve NoC energy-efficiency, while meeting the deadlines for all packets.","Designing Energy-Efﬁcient NoC for Real-Time Embedded
Systems through Slack Optimization
Jia Zhan* , Nikolay Stoimenov† , Jin Ouyang‡ , Lothar Thiele† ,
Vijaykrishnan Narayanan* , Yuan Xie* , §
*The Pennsylvania State University, {juz145,vijay,yuanxie}@cse.psu.edu
†Computer Engineering and Networks Laboratory, ETH Zurich, {stoimenov,thiele}@tik.ee.ethz.ch
‡NVIDIA, jouyang@nvidia.com
§AMD Research, yuan.xie@amd.com
ABSTRACT
Hard real-time embedded systems impose a strict latency requirement on interconnection subsystems. In the case of networkon-chip (NoC), this means each packet of a traﬃc stream has
to be delivered within a time interval. In addition, with the
increasing complexity of NoC, it consumes a signiﬁcant portion of total chip power, which boosts the power footprint of
such chips. In this work, we propose a methodology to minimize the energy consumption of NoC without violating the prespeciﬁed latency deadlines of real-time applications. First, we
develop a formal approach based on network calculus to obtain
the worst-case delay bound of all packets, from which we derive
a safe estimate of the number of cycles that a packet can be
further delayed in the network without violating its deadline—
the worst-case slack. With this information, we then develop
an optimization algorithm that trades the slacks for lower NoC
energy. Our algorithm recognizes the distribution of slacks for
diﬀerent traﬃc streams, and assigns diﬀerent voltages and frequencies to diﬀerent routers to achieve NoC energy-eﬃciency,
while meeting the deadlines for all packets.
Categories and Sub ject Descriptors
C.2 [Computer-Communication Networks]: Network Architecture and Design
General Terms
Algorithms, Design
Keywords
Network-on-Chip, Network calculus, Voltage-frequency scaling
1 Introduction
Contemporary embedded systems and SoCs feature an increasing number of processing elements (PE) and other components,
a sign that interconnection will play a more vital role in these
chips. Network-on-chips (NoC) is a promising design paradigm
for future many-core chips as found by many previous researches
[1, 7]. However, the fundamental challenge of using NoCs in
many-core embedded systems is that these systems often have
This work is in part supported by NSF CCF-0903432, CNS0905365, and SRC grant.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC ’13, May 29 - June 07 2013, Austin, TX, USA
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
very limited resources and stringent processing latency requirements, which places very diﬀerent constraints than generalpurpose processors on NoC design. There are two ma jor differences between embedded systems and general-purpose processors: 1) General-purpose processors are often designed to
achieve a high aggregate throughput, and therefore the NoCs
for them are allocated suﬃcient resources to sustain the peak
performance. In contrast, embedded systems are designed to
provide just enough performance to accommodate speciﬁc tasks.
Thrift is a virtue in designing NoC for those systems, in order
for power and area reduction. 2) General-purpose processors
care about the overall progress of all tasks running on all cores.
In contrast, embedded systems often provide certain guarantees
for individual tasks’ progress. In the so-called hard real-time
embedded systems, to provide certain quality-of-service (QoS),
each task has an associated maximum allowed communication
delay. Reﬂected on NoC, each network packet needs to be delivered to the destination before a dead line ; otherwise the corresponding task may not be able to deliver the required qualityof-service, and even causes catastrophic outcomes.
One way to address the conﬂicting requirements of energy
and latency is to leverage the inherent heterogeneity in NoC
traﬃcs, and use voltage frequency scaling (VFS) to improve the
energy-eﬃciency of NoC. A lot of previous work [15, 19, 21] have
adopted DVFS to reduce the energy consumption of NoC while
still providing high throughput. Heterogeneity can also be utilized to improve the eﬃciency of NoC. Das et al. [9] was the
ﬁrst to propose the idea of network slack , which refers to the
number of cycles that a packet can be delayed in the network
without aﬀecting execution time. In their work, packets with
smaller slacks (those more likely to impact execution time) are
prioritized. This slack-based approach improves the throughput of all running tasks. However, the above researches are
still focused on designing NoC for general-purpose processors,
and aimed at improving the overall throughput. For example,
the estimated slack proposed by Das et al. [9] does not consider precise deadlines on individual packets and only serves as
a hint in assigning priorities to packets. There is no guarantee
that a packet will arrive in time before it is needed. Therefore, these approaches cannot be applied to NoC in embedded
systems where violating deadlines could be disastrous.
Unlike previous work, we focus on improving NoC energyeﬃciency in hard real-time embedded systems by leveraging the
heterogeneity in NoC traﬃcs. We propose a design methodology that provides just enough power to NoC in order to meet
the latency requirements (deadlines) of all traﬃc streams. Inspired by Das et al.’s work [9], we ﬁrst calculate the worst-case
slacks for packets of diﬀerent streams. Then an energy optimization algorithm is proposed that leverages the slacks to
allocate diﬀerentiated resources (energy) to diﬀerent portions
of the network. Diﬀerent from their work, the slack calculation must be precise and conservative in order to guarantee the
timing-correctness of real-time systems. To solve this problem,
we adopt network calculus [3, 4] to predict the worst-case latency of diﬀerent packets, from which the worst-case slacks can
be obtained. Leveraging these slacks, we can progressively reduce the voltages and frequencies of individual routers in the
network to reduce energy consumption while still meeting all
• We develop a formal method based on network calculus to
deadlines. To summarize, the contributions of this work are:
obtain the worst-case slacks of packets in the NoC for hard realtime embedded systems. We improve over previous work [18]
by taking virtual channels and heterogeneous router frequencies
into consideration.
• We propose an eﬀective algorithm that trades slacks for energyeﬃciency of NoC, and thus minimizes the total communication
energy while still maintaining timing-correctness. This algorithm adjusts energy and performance by applying voltage and
frequency scaling (VFS) to individual routers in the network.
Finally, the voltage-frequency assignments computed using
the proposed approach are static, which ﬁts into a design category where a large number of real-time embedded systems
fall into. While not considered here, the framework devised
in this work can be applied in dynamically reconﬁgured NoCs
by periodically performing voltage/frequency scaling based on
run-time network states.
2 Worst-Case Delay Analysis
2.1 Router Architecture
Most of the state-of-the-art NoC researches assume a baseline wormhole router that achieves high energy-eﬃciency [7].
To provide guaranteed services in NoC, researchers extended
the baseline router architecture to either pre-allocate switching
time slots for critical packets [10, 11], or preserve a virtual channel for each traﬃc stream [2, 20]. Pre-allocating switching time
slots eliminates run-time contentions altogether, while preserving virtual channels only prevents head-of-line blocking and still
needs proper arbitration schemes for performance guarantee.
In this paper, we assume a baseline wormhole router architecture with ﬁve router stages (1. BW: Buﬀer Write; 2. RC:
Routing Computation; 3. VA: Virtual Channel Allocation; 4.
SA: Switch Allocation; 5. ST: Switch Traversal). Recently
NoC router architectures with fewer stages were also proposed.
However, changing the number of router stages aﬀects only the
initial latency in our analysis (refer to details below), and therefore our approach can be applied to router architectures with
fewer pipeline stages. In addition, we assume that each traﬃc
stream uses a dedicated virtual channel throughout the network, which is in line with the designs proposed by [2, 20]. We
do not opt for pre-allocating time-slots for each packet [10, 11],
because this approach eliminates the ﬂexibility of scaling voltage and frequency to reduce energy consumption.
In this section, the detailed analytic models for the two types
of router pipeline stalls are presented, and the worst-case packet
delay bound is derived from these models. Table 1 summarizes
symbols used in our modeling and analysis.
2.2 Principles of Network Calculus
Network calculus [3] is a theory of deterministic queuing systems for communication networks. In particular, this approach
is based on three important concepts:
Arrival Curve: If A[s, t) denotes the number of packets
(here we deﬁne a packet as a ﬁxed-length basic unit in network
traﬃcs; variable-length packets can be viewed as a sequence of
ﬁxed-length packets) that arrive in the time interval [s, t), then
we say the ﬂow A is constrained by an arrival curve α if and
only if for all s < t:
A[s, t) ≤ α(t − s)
(1)
Table 1: Symbols used for modeling and analysis
i
βRi
dworst
βR(cid:48)
A[s, t)
C [s, t]
Symbols Description
α
arrival curve
β
service curve
overall service curve of router Ri
ideal service curve of router Ri without back-pressure
the number of packets that arrive during [s, t)
the number of packets that can be processed during [s, t]
worst-case packet delay
number of slots assigned to ﬂow i in the scheduling model
VC buﬀer size
router frequency scaling factor
deadline constraint for ﬂow i
min-plus convolution e.g. a⊗b = min0≤s≤t {a(s)+b(t−s)}
inﬁmum e.g. a ∧ b = min{a, b}
burst delay function δT = +∞ if t > T , else 0
aﬃne arrival curve γr,b (t) = rt + b if t > 0, else 0
rate-latency function λ[t − T ]+ = λ(t − T ) if t > T , else 0
sub-additive closure f = δ0 ∧ f ∧ (f ⊗ f ) ∧ (f ⊗ f ⊗ f ) ∧ ...
si
B
η
Di
γr,b
βλ,T
⊗
∧
δT
f
Service Curve: If C [s, t) denotes the number of packets
that can be processed by a router or a whole network over the
time interval [s, t), and C is bounded by a service curve β if
and only if for all s < t:
C [s, t) ≥ β (t − s)
Delay Bound: Assume a packet stream, constrained by an
arrival curve α, traverse a system that oﬀers a service curve β .
Then the worst-case packet delay dworst can be bounded as:
dworst ≤ sup
{inf {τ ≥ 0 : α(t) ≤ β (t + τ )}}
(3)
(2)
t≥0
An example is shown in Figs. 1a and 1b for a single router,
where we show an aﬃne arrival curve γr,b , deﬁned by: γr,b (t) =
rt + b for t > 0, and γr,b = 0 otherwise, and a rate-latency
service curve βλ,T , deﬁned by: βλ,T (t) = λ[t − T ]+ = λ(t − T )
for t > T , and βλ,T = 0 otherwise. The arrival curve γr,b
implies that the source can send at most b packets at once, but
no more than r packets/cycle in the long run, while the service
curve βλ,T implies a pipeline delay T for a packet to traverse
a router and an average service rate of λ packets/cycle. As
shown in Fig. 1b, the worst-case delay bound d is the maximum
horizontal distance between arrival curve and service curve.
When extended to multiple interconnected components, as
shown in Fig. 1c, the end-to-end packet delay becomes more
unpredictable. Fig. 1d is the worst-case delay analysis for one
ﬂow f2 . As we can see, arrival curve α remains as the given
injection pattern, while service curve is now the concatenation
of all the routers that f2 traverses from source to destination,
which can be calculated through server concatenation [3]. For
instance, the concatenation of two routers with service curve
βR1 and βR2 is:
βR{1,2} = βR1 ⊗ βR2 = min
{βR1 (s) + βR2 (t − s)}
(4)
0≤s≤t
So far we only consider the case where routers with inﬁnite
buﬀers provide service to a single ﬂow. In reality, the router
is designed with a ﬁnite buﬀer size that exerts back-pressure,
and many ﬂows may share routers in NoC experiencing reduced
service quality. Both factors introduce additional stalls (ﬂowcontrol stall and switch-contention stall).
In the rest of this
section, we consider the additional stalls resulted from backpressure and resource sharing in the worst-case delay analysis.
2.3 Flow-Control Stall
With credit-based ﬂow control [8], the upstream router keeps
a count of the number of free buﬀers in each virtual channel
downstream. No packets will be forwarded if their intended
(a) single router
(b) delay bound for a router
(a)
(b)
Figure 3: Analysis of switch contention
(c) multiple ﬂows
(d) delay bound for a ﬂow
Figure 1: Delay bound from network calculus
buﬀers are full, until the downstream buﬀer forwards a packet
and sends a credit back to the upstream router. Here we adapt
Chang et al.’s work [5] to derive the worst-case latency bound
under the back-pressure of credit-based ﬂow control. For simplicity, we consider two adjacent routers R1 and R2 in our
demonstration, and the results can be easily applied to the
case when more routers are involved. Fig. 2 shows a graphical
view of the two-router case.
1
(6)
βR(cid:48)
1 ⊗ δs−si
the corresponding total cycle length is s = (cid:80)
The length of individual slot si (i = 1, 2) assigned for ﬂow fi
is proportional to the relevant priorities of incoming ﬂows and
should only take values that are multiples of a cycle length, and
i si . Assume the
full ideal service curve of the router is βR(cid:48)
1 , then the partial
ideal service curve for fi , as shown in Fig. 3b, is proportional
to the slot distribution:
βR(cid:48)
i =
si
s
where δs−si is the delay bound when stream fi just missed its
slots in the worst case and has to wait for the next round.
As for stream fi , (5) can be re-written as:
i = βR(cid:48)
i ⊗ (IB ⊗ (β
Then by plugging Equation (6) into (7) we can derive the
allocated service curve for a speciﬁc stream at each router it
traversed, and the concatenated service curve according to (4).
Finally, the worst-case delay bound can be obtained by applying the principles of network calculus. In this paper, we assume
a round-robin arbiter for every router in the network, which
implies that the priorities of each ﬂow are proportional to their
arrival rates to a speciﬁc router. While not considered here,
such results can be obtained in a similar way for other scheduling policies like Fixed Priority (FP), Rate Monotonic (RM),
and Earliest Deadline First (EDF).
i ⊗ βR2
R(cid:48)1
i
βR1
1
))
(7)
Figure 2: Analysis of credit-based ﬂow control
Let αin be the generic input process to router R1 while α be
the eﬀective input process to the internal crossbar of the router,
which is the outcome of both αin and back-pressure. αout is
the output process of router R2 . Suppose the overall service
curve βR2 of R2 seen by R1 is known, and the ideal service
curve (without back-pressure) of R1 is βR(cid:48)
1 (provided by the
crossbar), then according to [5], the overall service curve βR1
of R1 considering back-pressure is given as:
βR1 = βR(cid:48)
1 ⊗ (IB ⊗ (βR(cid:48)1 ⊗ βR2 ))
(5)
where B is the buﬀer size, and IB is deﬁned as IB (t) = ∞
for t > 0 and IB (0) = B . The horizontal bar is the operation
for sub-additive closure. In this way, we can derive the service
curve of each router and then recursively concatenate them
based on Equation (4) from destination to source to get the
concatenated service curve for all routers along a ﬂow’s path.
2.4 Switch-Contention Stall
Packet stall can happen at switch allocation stage, when all
front packets in diﬀerent virtual channels compete for the same
crossbar input or output port. Here we model a generic switch
arbiter that allocates time slots to diﬀerent input ports according to their priorities. In Fig. 3a, we show an example where
two ﬂows arrive at a router and compete for the same output
link to illustrate service curves experienced by each ﬂow.
3 Slack Optimization for Saving Energy
Applying the methodology in the previous section, we are able
to bound the worst-case packet delay for individual application streams, hence obtaining worst-case slack —the time interval between the delay bound and its pre-speciﬁed transmission
deadline, which indicates the number of cycles that a packet can
be further delayed in the network. Based on this idea, we propose an energy-aware voltage and frequency scaling approach to
optimize network energy-eﬃciency under deadline constraints.
3.1 Frequency and Voltage Scaling
The existence of packet slack implies that we can still achieve
the required performance while lowering the operating frequency
of some routers instead of making them run at a homogeneous
high speed. The supply voltage, in the meantime, can be reduced together with the frequency to reduce energy.
Voltage-frequency islands (VFI) [14] have been adopted for
achieving ﬁne-grain system-level power management. A ﬁne
granularity partitioning could assume that each module in the
design belongs to a diﬀerent island [16] for best ﬂexibility, or
ﬁnd the optimum partitioning via island merging [17] for energy
savings. Here we are doing static voltage-frequency assignment
and model these routers to be able to run at its own voltage and
frequency. For completeness, we also explore the energy gain
of operating all routers at homogeneous voltage and frequency.
In order for the network routers to operate at diﬀerent frequencies, they should communicate in a Globally Asynchronous
Locally Synchronous (GALS) mechanism. To address synchronization latency, we adopt the fast synchronizer proposed by
Dally et al. [6] which adds only half cycle of synchronization
delay that can be well absorbed in the buﬀer write stage.
Note that if we assign a lower frequency to a router, its service
curve is aﬀected accordingly. Speciﬁcally, the packet service
rate will decrease and the time it takes to traverse a router
will increase. For example, for a router Rk with rate-latency
service curve βRk
λ,T (t) = λ[t − T ]+ as discussed in Section II,
when its operating frequency is scaled by a factor η , its new
service curve is modeled as:
ηλ,T /η (t) = ηλ[t − T /η ]+
Then the worst-case packet delay should be updated to check
if there is remaining slack time for further optimization. Our
energy optimization algorithm is described below in detail.
βRk
(8)
3.2 Energy-Aware Heuristic Search Algorithm
There are two straightforward ways to trade slack for energy
savings. One is to simply scale down all the routers simultaneously by the same factor, which will keep them running at
homogeneous frequency and voltage. The other is through exhaustive search to ﬁnd out the optimum assignment. However,
the former approach is not ﬂexible enough for adjustment, because the degree to which the network speed can be reduced
is limited by the packet ﬂow with minimum slack. The latter
is time-consuming and not scalable for a large network with
multiple voltage-frequency levels.
Therefore, we propose an energy-aware heuristic search (EHS)
algorithm, which can ﬁnd an eﬃcient solution leveraging network heterogeneity and avoiding exhaustive search at the same
time. Speciﬁcally, we abstract a NoC energy model and integrate it into our worst-case delay analysis framework to automatically generate the frequency-voltage assignments.
3.2.1 Energy Models
The set of nodes in the network is denoted by T = {0, 1...N − 1}.
The supply voltage-frequency pairs of each node i ∈ T are given
by (Vi , fi ). Then the sum of dynamic and static energy consumption associated with node i is:
E (Vi , fi ) = Ed (Vi , fi ) + Es (Vi , fi )
(9)
The dynamic energy part can be calculated through:
Ed (Vi , fi ) = Mi ∗ Ep (Vi , fi )
where Mi is the total number of packets that traverse node
i during execution, and Ep (Vi , fi ) is the energy consumption
when a packet traverses node i:
(10)
Ep (Vi , fi ) = Ebuf f er + Eswitch + Elink
(11)
where Ebuf f er , Eswitch , and Elink represent the energy dissipated at input buﬀers, switch and link and are found experimentally using ORION 2.0 [13].
The static energy Es (Vi , fi ) part is deﬁned as:
Es (Vi , fi ) = Ps (Vi , fi ) ∗ t
where t is the system execution time, while Ps (Vi , fi ) is static
power for node i and can be obtained as:
Ps (Vi , fi ) = I i
static ∗ Vi
(12)
(13)
where I i
static is the leakage current for node i and can also be
extracted from ORION 2.0 [13].
Thus, combining Equation (9), (10) and (12), the total NoC
energy consumption for an application can be expressed as:
N −1(cid:88)
i=0
E =
(Mi ∗ Ep (Vi , fi ) + Ps (Vi , fi ) ∗ t)
(14)
For an N -node NoC with k voltage-frequency levels for each
node, the algorithm complexity of our EHS algorithm is (k −
1) ∗ N 2 logN , compared to kN for exhaustive search.
3.2.2 Algorithm Description
If node i is scaled from the current voltage-frequency level
(V k
i , f k
i ) to the next lower level (V k+1
), then energy reduction can be expressed as:
, f k+1
i
i
N(cid:88)
i=1
i
i
))
(15)
∆Ei =
, f k+1
i
, f k+1
i
(Mi ∗ (Ep (V k
i ) − Ep (V k+1
i , f k
+Ps (V k
i , f k
i ) ∗ tk − Ps (V k+1
) ∗ tk+1 )
Under deterministic routing, the only undetermined variable is
the system execution time tk . Assume the set of application
streams is denoted by S = {s1 ...sm }. The number of packets
injected by sj is MSj with average injection rate rsj . Then tk
(cid:8)Msj /rsj
can be approximated from the slowest stream:
This is because the end-to-end packet delay is negligible compared to t, especially when the packet number is large.
At the same time, the service curve βRi is modiﬁed based
on Equation (8) if scaling i and the new stream delay dsj is
calculated via the worst-case delay analysis in section II. The
accumulated slack cost after scaling is represented as:
t = max
sj ∈S
(cid:9)
(16)
∆di =
∆dsj
(17)
(cid:88)
sj ∈S
where ∆dsj is the reduced slack for stream sj .
Our heuristic search algorithm uses ∆di /∆Ei as a measure
of the slack cost and the energy gain if we adjust the voltagefrequency level of a router i. The pseudo-code below outlines
this algorithm. Speciﬁcally, the algorithm iterates through all
routers in the network. At each iteration, it generates a list
containing the slack costs and the energy gains for all routers
in the network, picks the router i with lowest ∆di /∆Ei without
causing deadline violations, and steps down the router’s voltage
and frequency. Finally, the algorithm terminates when there is
no router in the network of which the voltage and the frequency
can be further reduced without causing timing violations.
Algorithm 1: Energy-aware heuristic search algorithm
/* worst-case delay analysis
Result: Frequency-voltage assignment for all nodes
Initialize: Flag = 0;
while Flag == 0 do
for i ← 0 to N − 1 do
Flag = 1;
if fi is at its lowest level then
continue;
else
Calculate dsj (∀sj ∈ S ), ∆di and ∆Ei if scaling
down fi by one level, and insert them into a list L
as one element ;
end
end
Sort L in ascending order of ∆di /∆Ei , associated with the
original index i;
for each entry in the list L do
if dsj < Dsj (∀sj ∈ S ) then
Scale down fi by one level; Flag = 0; break;
end
end
end
*/
4 Experiments
4.1 Experimental Setup
We implemented a cycle-accurate network simulator based on
the booksim 2.0 simulator [12], with dynamic and leakage power
numbers extracted from ORION 2.0 [13] .
We also analyze the timing behavior of some video applications and characterize the arrival curves for packet streams.
Table 2 shows the simulator and benchmark conﬁgurations.
Table 2: Simulation Parameters
Topology
Size
VC #
Buﬀer depth
Frame
Period
Throughput
JPEG size
Baseline Network Conﬁguration
2D mesh
Phit width
128bits
4×4=16
Frequency
2GHz
3
Voltage
1.5v
4 ﬂits
Routing
dimension-order
Video Application Conﬁguration
MJPEG
352×240
PiP (HR)
704×576
PiP (LR)
Frame
90,000
PE service
226.57
307.2KB Macroblock #
1584
8Kb
Rate: 25 frames/s Rate: 12.5 frames/s
Deadline Constraint (cycle)
D2 = 95
D3 = 50
D1 = 50
Motion-JPEG (MJPEG) decoder: The MJPEG decoder
is a video codec in which each video frame is compressed as a
JPEG image. The video of 352 × 240 pixels is spilt into JPEG
image size of 8 Kb. The maximum throughput is 307.2 KB per
invocation with a period of 90,000 cycles.
Picture-in-picture (PiP): We use two sets of video clips:
Regular clips with moderate to high motion content and clips
displaying still images. These two sets characterize the two
streams high-resolution (HR) and low-resolution (LR). Incoming streams have the same frame resolution of 704 × 576 pixels
but will be down-scaled for LR, and each frame consists of 1584
macroblocks. Frames are read at a constant rate of 25 frames/s
for HR and 12.5 frames/s for LR. The service oﬀered by a processing element is 226.57 macroblocks/ms.
A JPEG image or a macroblock is treated as a packet, and
we derive arrival curves for the three packet streams:
MJPEG stream f1 : α1 (t) = 0.218t + 3.0
PiP HR stream f2 : α2 (t) = 0.175t + 13.109
PiP LR stream f3 : α3 (t) = 0.086t + 4.37
(18a)
(18b)
(18c)
And the baseline service curve is shown in (19) for a generic
wormhole NoC router that can process one packet per cycle
with a total pipeline length of ﬁve cycles.
Baseline router: β (t) = [1.0 × t − 5]+
are mapped in a 4×4 mesh network shown in Fig. 4a with deterAs a case study, we consider that the three application streams
ministic routing. Fig. 4b shows the resource sharing, including
feedback loops in stream f1 as an example. A detailed network
conﬁguration is shown in Table 2.
(19)
4.2 Experimental Results
We use the methodology in Section II to analyze the worst-case
latency in our case study, and results are shown in Fig. 5.
At the same time, we run simulation to get the maximum
packet latency for individual streams. A comparison between
the calculated worst-case delay bound (solid lines) and the simulated maximum packet latency (dashed lines) when varying
virtual channel buﬀer size B is shown in Fig. 6a. We can see
that the calculated delay bounds are fairly tight.
Apart from the case study with three applications streams,
we duplicate each of the three sample streams to generate more
streams and map them on the NoC platform to form diﬀerent
traﬃc scenarios. The whole process is conducted randomly.
(a)
(b)
Figure 4: Case study: Three video streams
Figure 5: Delay bounds for three ﬂows
(a) Delay comparison for
three streams
(b) Average diﬀerence for three,
ﬁve and eight streams
Figure 6: Calculated (Cal.) vs simulated (Sim.) delay bound
Here we consider running another ﬁve streams (Two MJPEG,
two PiP HR, one PiP LR) and eight streams (Three MJPEG,
three PiP HR, two PiP LR). Similarly, we do worst-case delay analysis to derive the delay bound and run simulation to
get the maximum packet latency. Due to space limitation, we
only show the diﬀerences between calculated delay bounds and
simulated results as the ratio with respect to simulated results,
when buﬀer size varies from 3 to 7, as shown in Fig. 6b.
We ﬁnd that the average diﬀerence is 17.2% while the ratio
is generally decreasing as VC size increases. This is because
with a small VC size, the eﬀect of back-pressure is more salient
and the results from our analytical model is more pessimistic.
With VC size increased, the eﬀect of back-pressure is smoothed
out, resulting in a tighter delay estimate that converges with
the simulation results.
Furthermore, we perform the proposed EHS algorithm to reduce the energy of routers, assuming that three discrete frequencies are available (1.0, 1.5 and 2.0 GHz), and the minimum required voltage is 0.8, 1.2 and 1.5 volts in 45nm CMOS,
respectively. As a comparison, we also evaluate homogeneous
scaling (H omo) in which case the frequency-voltage level of the
routers in the whole network is scaled together. Results of the
worst-case delay bounds and the normalized total network energy consumption are shown in Fig. 7, where we refer to the
j th duplicate of the ith sample stream as f j
i .
5 Conclusion
In this paper, a formal analysis based on network calculus is
adopted to obtain the worst-case slacks of packets in the NoC
for hard real-time embedded systems, and used to trade slacks
for energy savings by applying diﬀerent voltages and frequencies to individual routers. Experimental results show that our
worst-case delay analysis can derive a upper bound for packet
latency, and our energy-aware heuristic search algorithm can
eﬀectively ﬁnd the frequency-voltage assignment that can reduce network energy signiﬁcantly under variable slack ratios.
6 "
RISO - relaxed network-on-chip isolation for cloud processors.,"Cloud service providers use workload consolidation technique in many-core cloud processors to optimize system utilization and augment performance for ever extending scale-out workloads. Performance isolation usually has to be enforced for the consolidated workloads sharing the same many-core resources. Networks-on-chip (NoC) serves as a major shared resource, also needs to be isolated to avoid violating performance isolation. Prior work uses strict network isolation to fulfill performance isolation. However, strict network isolation either results in low consolidation density, or complex routing mechanisms which indicates prohibitive high hardware cost and large latency. In view of this limitation, we propose a novel NoC isolation strategy for many-core cloud processors, called relaxed isolation (RISO). It permits underutilized links to be shared by multiple applications, at the same time keeps the aggregated traffic in check to enforce performance isolation. The experimental results show that the consolidation density is improved more than 12% in comparison with previous strict isolation scheme, meanwhile reducing network latency by 38.4% on average.","RISO: Relaxed Network-on-Chip Isolation
for Cloud Processors
Hang Lu†‡ , Guihai Yan† , Yinhe Han†‡ , Binzhang Fu† and Xiaowei Li†‡
†State Key Laboratory of Computer Architecture, Institute of Computing Technology,
Chinese Academy of Sciences, Beijing, China
‡University of Chinese Academy of Sciences, Beijing, China
{luhang, guihai_yan, yinhes, fubinzhang, lxw}@ict.ac.cn
ABSTRACT
Cloud service providers use workload consolidation technique
in many-core cloud processors to optimize system utilization
and augment performance for ever extending scale-out workloads. Performance isolation usually has to be enforced for
the consolidated workloads sharing the same many-core resources. Networks-on-chip (NoC) serves as a ma jor shared
resource, also needs to be isolated to avoid violating performance isolation. Prior work uses strict network isolation to
fulﬁll performance isolation. However, strict network isolation either results in low consolidation density, or complex
routing mechanisms which indicates prohibitive high hardware cost and large latency. In view of this limitation, we
propose a novel NoC isolation strategy for many-core cloud
processors, called relaxed isolation (RISO). It permits underutilized links to be shared by multiple applications, at the
same time keeps the aggregated traﬃc in check to enforce
performance isolation. The experimental results show that
the consolidation density is improved more than 12% in comparison with previous strict isolation scheme, meanwhile reducing network latency by 38.4% on average.
Categories and Subject Descriptors
C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]: Interconnection architectures; C.1.4 [Parallel
Architectures]: Distributed architectures; D.4.1 [Process
Management]: Concurrency, Threads
General Terms
Performance, Design, Algorithms
Keywords
Networks-on-Chip, Cloud Computing, Cloud Processors, Workload Consolidation, Performance Isolation, Relaxed Isolation
1.
INTRODUCTION
Cloud computing has emerged as a fundamental platform
to deploy increasing on-line services like web searching, social networks and so forth. To tackle expanded power and
space consumption brought by the growing cloud infrastructure, the processor with tens even hundreds of cores is believed to play a critical role in the coming cloud computing
era, or simply called many-core “cloud processors”. Intel’s 48core “Single-chip Cloud Computer” [1] and Tilera’s “TILE-Gx
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC ’13, May 29 - June 07, 2013, Austin, Texas, USA.
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
Link
Core
Switch
Link
Figure 1: Two traditional schemes of strict isolation
3000 Series” [2] are two representatives. The cloud processor chips can accommodate abundant parallelism for concurrent scale-out workloads, which demand various amounts
of computational resources and will be scheduled back and
forth by the data center software [3][4]. For such cloud computing platform, one key optimization is to reduce the TCO
(total cost of ownership) to avoid low hardware utilization
by aggressive “workload consolidation” technique [5][6].
In
such computing paradigm, performance isolation between
scale-out workloads has to be enforced to provide controllable
QoS and priority-based services which are critical for cloud
computing service providers, such as Google and Amazon.
Performance isolation imposes two orthogonal requirements:
1) Computation isolation, i.e. the computing cores, and the
associate cache, memory bandwidth etc., should not be preempted by other workloads in uninterruptable computing sessions [6][7]. 2) Communication isolation, or network isolation, i.e. the on-chip network (NoC) traﬃc of diﬀerent workloads should not block each other because the performance
of many parallel applications are highly sensitive to network
latency [8]. Our work focuses on communication isolation,
the same important but less extensively studied topic than
computation isolation.
Communication isolation strategies involve making tradeoﬀs between the regularity of network topology and complexity of routing mechanism. Usually, a regular topology, e.g.
rectangular-shaped network, has more eﬃcient routing algorithm, but lower server consolidation density, hence less hardware utilization. By contrast, enabling the network isolation
to support more ﬂexible topologies, thereby achieving high
consolidation density, will inevitably complicate the routing
mechanism. This tradeoﬀ can be further explained with the
following example.
Figure 1(a) shows the principle of regular-shaped isolation.
The 16-core processor accommodates one application (App1)
mapped into a rectangular-shaped region. However, the consolidation density is poor to meet the regular shape requirement: there are seven routers which have to be idle although
the incoming application (App2) only needs a ﬁve-router network. To relax this limitation, a viable solution is to enable
the network isolation to support irregular shapes through implementing ﬂexible routing mechanisms such as Up*/Down*
[9] or table-based routing [10], as Figure 1(b) shows. However, the cost of employing those complex routing can hardly
be justiﬁed for cloud processors given the prohibitive TCO
and sporadic performance variations.
Previous work fails to resolve the above contradiction between consolidation density and complexity of routing mechanisms. The reason is that those schemes follow the concept
of “Strict Isolation”, i.e. resorting to strict physical isolation to achieve performance isolation. However, enforcing
such strict rule is quite conservative and often leads to overdesign. The on-chip routers and links are often heavily underutilized, especially those on the application region boundaries. By judiciously sharing some routers and links, we can
still achieve performance isolation without nailing down to
strict isolation. This paper thereby proposes the concept of
“Relaxed Isolation (RISO)” for many-core cloud processors. Following this concept, we can achieve the consolidation
density of schemes supporting irregular topology, but employ
the same eﬃcient routing mechanism as in schemes using regular topology. In particular, this paper makes the following
contributions:
∙ We propose relaxed NoC isolation scheme enforcing workload performance isolation. We ﬁnd the traditional strict isolation is highly conservative for performance isolation, which
impairs the consolidation density. RISO does not resort to
physical isolation, but permits conditional network resource
sharing without violating performance isolation.
∙ We propose an application mapping algorithm to maximal ly exploit the potential of RISO. This algorithm is fully
with respect to performance isolation by preventing overlaid
traﬃc exceeding a safe threshold. Also, irregular-shaped regions, which are wasted in previous work to compromise with
routing complexity, are also taking into consideration to further improve consolidation density.
The rest of this paper is organized as follows. Section II
describes the motivation of RISO by further specifying the
limitation of traditional strict isolation schemes. Section III
presents the key algorithms to implement RISO. Section IV
shows experiment setup and results, followed by related work
in Section V and conclusion in Section VI.
2. MOTIVATION
2.1 “Strict Isolation"" Degrading Consolidation
Density
Improving consolidation density is an eﬀective approach to
reduce the TCO of data centers for cloud service providers. In the future, the eﬀectiveness of consolidating multiple
workloads into a single many-core cloud processor is just
like today’s sever consolidation across racks in data centers [5]. However, enforcing performance isolation would be
more challenging in cloud processors because the on-chip network is much less ﬂexible than its traditional oﬀ-chip counterpart. In particular, the routing mechanism for large-scale
many-core system usually follows dimensional order routing
(DOR) which is table-less and tailored for on-chip networks
[11]. DOR imposes network topology constraints for every
workload. By the doctrine of “strict isolation”, the consolidation density will be degraded. For example, Figure 2(a)
ple, App1, App2, ⋅ ⋅ ⋅, and App5 have already been mapped
shows dynamic application mapping over time. In this examinto the system at that time. The remaining free cores constitute a contiguous but irregular region. Supposing a tencore workload, App6, is waiting to be served. However, the
DOR topology constraint renders this application fail to be
mapped because a regular rectangle shape cannot be found
under “strict isolation”, although there are still 16 free cores
which is more than App6 required. Furthermore, the block
of App6 probably prevents the subsequent applications, for
example App7, from execution, which further degrades the
consolidation density.
2.2 Breaking Strict Isolation — RISO
We propose Relaxed Isolation (RISO) to tackle the limitation of strict isolation. We ﬁnd that using strict isolation in
on-chip networks to ensure communication isolation is highly
Figure 2: Application mapping on strict isolation and
RISO
0%~25%
25%~50%
50%~75%
75%~100%
s
k
n
L
i
f
o
.
m
u
N
100
90
80
70
60
50
40
30
20
10
0
Figure 3: Link utilization distribution
conservative. Since our ultimate goal is performance isolation, the App6 can be mapped into the irregular regions as
long as the aggregated traﬃc on the “overlapped” routers and
links won’t degrade the latency of each other. As Figure 2(b)
shows, by permitting the router and link sharing — RISO,
both App6 and subsequent App7 can be served without delay, hence improving the consolidation density.
The rationale behind RISO is to exploit under-utilized
routers and links. We ﬁnd that low NoC utilization is not
uncommon in reality, which suﬃciently justiﬁes the concept
of RISO. The utilization (� ) of a link within � link cycles is
calculated by Eq.(1) [12].
∑�
{
� =
�=0 �(�)
�
�(�) =
0
Link is id le at cycle i ;
1 Otherwise.
(1)
We survey the link utilization of a set of workloads. The
result is shown in Figure 3, represented by “histgram”. The
link utilization is divided into four ranks: 0∽25%, 25∽50%,
50∽75%, 75∽100%. The result shows that the lowest rank,
0%∽25%, dominates in all applications. Very few links can
reach up to the second rank, without mention the third and
forth ranks. Hence, such severely low utilization should provide a unique opportunity for RISO.
RISO won’t violate communication isolation as long as the
aggregated traﬃc is kept below a certain threshold, called
“congestion point” in this paper, as Figure 4 shows. We ﬁnd
that network latency starts to increase only when the link
utilization increases beyond the congestion point. This trend
applies to various traﬃc patterns. Experimental study shows
the congestion point is application-independent and at the
link utilization around 65%, which agrees with prior work
[13]. Given that link utilization is usually much less than
25% in reality, we can safely conclude that the minority of
link sharing in RISO won’t cause obvious latency increase,
 
 
Figure 5: RISO supported shapes
Figure 4: Link utilization and latency under various
traﬃc patterns
therefore keeping the performance isolation intact.
In this paper, we conﬁne our scope to many-core cloud
processors connected with “Mesh” networks, which has been
proved with superior scalability. But the concept of relaxed
isolation is applicable to other types of on-chip networks.
3. APPLICATION MAPPING ALGORITHM
BASED ON RISO
Mapping application into a many-core system is to assign
the application a speciﬁed number of physical cores whose
network is organized to the routing-allowed topology, meanwhile without violating the criteria of performance isolation.
The basic mapping procedure can be divided into two steps:
1) Search for the physical core groups (regions) which meet
the topology requirement under the condition of relaxed NoC
isolation, instead of strict isolation; 2) Verify the performance
isolation criteria by checking for the utilization of shared links
carrying the overlapped traﬃc. The following will delve into
the details of the proposed mapping algorithm.
3.1 Topology Representation
For some applications with intensive intra-application communications, the performance and power consumption can
be topology-speciﬁc [14][15]. We therefore assume each application to be mapped has a preferred topology, which serves
as an input to our mapping algorithm. An application’s
preferred topology incorporates two unique characteristics:
physical shape and threads organization. The proposed algorithm ﬁrst searches for the candidate regions in NoC that
can accommodate the preferred physical shape. To maximize
the consolidation density, the mapping algorithm should be
capable to handle not only regular shape, i.e. rectangle, but
also various irregular shapes ignored in previous works [16].
basic types: “L”, “⊢”, “⊏”, as Figure 5 shows, with various
We abstract those irregular shapes into the following three
rotations and mirrors.
horizontal vector, � [ℎ1 , ℎ2 , ℎ3 , ⋅ ⋅ ⋅ ., ℎ� ], and vertical vector,
To represent the physical shapes in Figure 5, we deﬁne
� [�1 , �2 , �3 , ⋅ ⋅ ⋅ , �� ], where ℎ� and �� is the number of cores
in the �th row and column respectively in the preferred shape.
� applies to shapes that exhibit complete contiguity in horizontal dimension. For instance, Figure 6(a) shows a “L”
shape: the 1st and 2nd row each requires 4 cores; the 3rd
and 4th row each requires 2 cores and every row is contiguous; hence this shape is represented by the horizontal vector
� [4, 4, 2, 2]. However, for the shape that is non-contiguous
horizontally but contiguous vertically, we use a vertical vector as Figure. 6(b) shows. Its corresponding vertical vector
is � [4, 4, 2, 2, 2, 4, 4]. The other irregular shapes both � and
� cannot describe are beyond the scope of this paper.
For the multiple threads of an incoming application, we use
the tuple ⟨�� , �� ⟩ to indicate each thread and its corresponding position in the physical shape. Speciﬁcally, parameter ��
Figure 6: Shapes represented by horizontal (� ) or
vertical vector (� ), and thread organization
and �� means that the �th thread is located at the � th position, as Figure 6 shows. We use a tuple set � = {⟨�� , �� ⟩} to
represent all threads of an application and their positions in
the physical shape.
3.2 Problem Formulation
Based on the above speciﬁcation, the problem can be formulated as follows:
∙ Given: 1) the NoC topology, � (� , � ), where � and �
indicates the set of free and busy cores, respectively; 2) the
traﬃc matrix, �������� , whose elements denote the historical communication volume between thread pairs of various
applications; 3) the preferred topology, denoted by � or �
and tuple set � ; 4) the link utilization threshold ��������
under which performance isolation can be enforced.
∙ Determine: 1) the mapping of every < �� , �� > tuple from
� to � , < �� , �� >: � → � . After mapping, relevant position
of threads remains the same as in preferred topology; 2) the
shared link set �, and link utilization � of every shared link
� ∈ �;∙ With respect to the constraint: ∀� ∈ �, �� < �������� .
3.3 Algorithm in Detail
Given the inputs and constraints, the mapping algorithm
solves the problem in two steps: 1)Topology searching, represented in Algorithm 1; 2) Performance veriﬁcation, described
in Algorithm 2.
Step 1: Topology Searching: The algorithm will ﬁrstly
search the NoC for proper candidate shapes to serve the incoming application. If the number of free cores in � is fewer
than that the application requires (number of < �� , �� > tuples in � ), the searching process returns directly with a failure. Otherwise, it tries to ﬁnd the candidate shapes speciﬁed
by the � or � . The detail of topology searching is described
in Algorithm 1. Line 3 through line 13 are responsible for
searching the target shape denoted by � (horizontal vector
in this example). The algorithm starts searching � column
by column (line 5) to satisfy every element in � . If it ﬁnds
a busy node (line 8), the algorithm starts searching from another node in � . As long as every element in � is satisﬁed,
the target topology is found (line 4 to 11). Otherwise, if the
algorithm has traversed all nodes in set � but still does not
ﬁnd shape identical to � (line 13), the algorithm returns with
a failure. Algorithm 1 aims at shapes indicated by the horizontal vector, and for the shapes indicated by vertical vector
(� ), the overall process is the same, except that it searches
� row by row to satisfy every element in � (line 5). Note
that Algorithm 1 is not limited to searching ’L’ shape, and
to further boost consolidation density, it is also applicable to
Algorithm 1 Topology Searching
Algorithm 2 Performance Veriﬁcation
2:
3:
4:
5:
Input: Requested shape: � or � ; NoC topology: � (�, � );
Output: � ���� or � ��� ����
1: if � then
for each � ���(���, ���) ∈ � do
ℎ��� = length(� [ℎ1 , ℎ2 , . . . , ℎ� , . . . , ℎ� ]);
for � = 0;�<ℎ��� ;� + + do
if {[� ���(��� + �, ���), � ���(��� + �, ��� + ℎ� )]} ⊆�
then
continue;
else
break; //there is a � ��� ∈ � , start from another
� ��� ∈ �
end if
end for
return � ����; //the shape denoted by � is found in �
end for
return � ��� ����; //no shape is identical to � in �
14: end if
9:
10:
11:
12:
13:
6:
7:
8:
Core
Core
Core
Core
Core
Core
Core
Core
Core
Core
Core
Core
Core
M
running
Core
Core
Core
Core
Core
=
(cid:167)
(cid:168)
(cid:168)
(cid:168)
(cid:168)
(cid:169)
(6,5)(4,8)
(cid:22) (cid:22) (cid:22) (cid:22)
(cid:22)
(cid:22)
(cid:22)
(cid:22)
(cid:22) (cid:22) (cid:22) (cid:22)
R
R
R
R
(6,6)(4,8)
(6,5)(5,8)
(6,6)(5,8)
(cid:183)
(cid:184)
(cid:184)
(cid:184)
(cid:184)
(cid:185)
node(6,5);  node(6,6); node(6,7)
node(6,8);  node(5,8); node(4,8)
Figure 7: Shared link set identiﬁcation
other shapes shown in Figure 5.
Step 2: Performance Veriﬁcation: After successfully ﬁnding the target topology, we need to verify the criteria of performance isolation, represented by the constraint that the
aggregated link utilization of every shared link �� will not
exceed �������� . Firstly, we need to ﬁgure out the shared
link set �. As an example, Figure 7 shows the scenario after mapping topology in Figure 6(a). Every node has a coordinate marked by the horizontal and vertical location in
NoC. A link is denoted by the coordinates of associate nodes
as < (�� ��� , �� ��� ), (��� , ��� ) >. For example in Figure 7,
there are three shared links in DOR routing mechanism. The
links can be presented as < (6, 7), (6, 8) >, < (7, 7), (7, 8) >
and < (7, 8), (6, 8) >, respectively.
Secondly, to calculate the aggregated traﬃc on a shared
link, we divide the associate nodes of the shared link into
two sets:
����� ��� and ����� ���, deﬁned by Eq. 2.
In
DOR, a shared link will only carry the traﬃc generated from
nodes in ����� ��� and terminated at nodes in ����� ���. By
looking up into the traﬃc matrix �������� , we can get the
aggregated traﬃc volume on the shared link.
����� ��� : {� ���(�, �) ∣ � = �� ���&&� ≤ �� ��� }
����� ��� : {� ���(�, �) ∣ � ≤ ���&&� = ��� }
Algorithm 2 shows in detail the veriﬁcation process. Line 3
through line 7 determines the aggregated traﬃc volume on
each shared link. � is calculated based on Eq. 1 in line
8. Line 10 indicates that if the � of any shared link exceeds
�������� , this candidate topology is not valid and must search
another candidate from Step 1. It is valid only if all shared
links are satisﬁed, as line 13 describes.
Note that our algorithm takes the “ﬁrst-ﬁt” principle [16].
That is if multiple topology candidates can pass the Step 2,
we take the ﬁrst one.
{
(2)
3.4 Trafﬁc Prediction
As many prior NoC ﬂow management solutions, RISO relies on accurate traﬃc prediction. At each scheduling interval, the operating system, based on the traﬃc history,
predicts the traﬃc distribution and thereby identiﬁes the
Input: the traﬃc matrix, �������� , shared link set, �; time interval,
����, link utilization threshold, �������� ;
Output: The validity, ��������;
1: int ��� = 0;
2: for each shared link �(< (�� ��� , �� ��� ), (��� , ��� ) >) ∈ � do
for each ����(�, �) ∈ ����� ��� of � do
for each ����(�, �) ∈ ����� ��� of � do
��� += �(�,�)(�,�) ; //sum up the values in traﬃc matrix
end for
end for
� = ���
���� ;
if � > �������� then
�������� = � ����; //congestion will happen after mapping
end if
12: end for
13: �������� = ����; //� of every shared link is lower than �������� ,
performance isolation is ensured
3:
4:
5:
6:
7:
8:
9:
10:
11:
sharable links to implement RISO. Most prior work uses liner predictor to fulﬁll this purpose. We ﬁnd that although
liner predictor is capable for gradually-changed traﬃc patterns, it’s highly unreliable to cope with bursty traﬃc patterns which, if fail to predict, can jeopardize the performance
isolation. Therefore, we take a “conservative” approach in
traﬃc prediction: for bursty traﬃc, since the traﬃc volume
changes sharply, we just exclude the associated links from
sharing. This may slightly degrade the consolidation density,
but the performance isolation is well guaranteed. Speciﬁcally, we modiﬁed last value predictor (LVP) [17] to handle both
bursty and non-bursty scenarios. The prediction function is
implemented as Eq. 3.
����������� =
∣< 10%;
ℎ2
+∞ ��ℎ������.
∣ (ℎ2−ℎ1 )
ℎ1
(3)
{
∑�
�=1 ��
The predictor stores two most recent traﬃc volumes as ℎ1 , ℎ2
for every source-destination pair.
If the two values diﬀer
by assigning a +∞ to the ﬁnal prediction value; otherwise
sharply, we exclude the associated links from sharable links
we still follow the LVP. We ﬁnd that setting the bar to 10%
works well.
4. EVALUATION
4.1 Experimental Setup
4.1.1 Metric for Consolidation Density
We use system utilization (������� ) [16] as a metric for
consolidation density evaluation. For a � -node system during � period of time, ������� is deﬁned by Eq. 4.
������� =
� × �
where �� is the busy time of node � over � period of time. A
high system utilization means high consolidation density.
System utilization depends on the “����” condition [16]
which is deﬁned by Eq. 5.
(4)
���� =
� × �
� × �
where � is average requested resources (i.e. cores in this
paper) of all applications; � is average inter-arrival time between consecutive applications; � is average application running time. ���� below 1 means application arrival rate is
lower than departure rate; otherwise the system will be overloaded and improving system utilization will be critical. The
values of these parameters used in the experiment are listed
in Table 1.
(5)
4.1.2 Performance Simulation Setup
We modiﬁed Booksim2.0 [18] to evaluate performance after using the proposed application mapping algorithm. The
Table 1: Parameters of system utilization evaluation
Parameter
Value
Topology
Scheduling mechanism
�
�
distribution of requested Num. of cores
Num. of consolidated workloads
���� range
Mesh (32*32 and 16*16)
FCFS
64
2000 (in cycles)
uniform
10000 (per experiment)
[0.1∽1.6], step by 0.1
baseline NoC topology is a 8x8 mesh. The router is conﬁgured with a two-stage pipeline plus one cycle for link traversal. We use two virtual channels and each has an eight ﬂit
buﬀer. The congestion threshold (�������� ) is set at 65%, in
accordance with the result shown in Figure 4.
4.1.3 Workloads
For consolidation density evaluation, we map 10000 consolidated workloads at each ���� condition. We log the �������
in real time. This measurement is repeated ten times at each
����. The ﬁnal ������� result is the average of the ten measurements.
For performance evaluation, we run application traces [8]
and each application acts as a workload. Application traces
are obtained from GEMS [19], a full system simulator. The
detailed conﬁguration is shown in Table 2.
Table 2: Full system simulator conﬁguration
Parameter
Value
Cloud Processor
Coherence Protocol
L1 I/D Cache
L1 Cache Access Latency
Private L2 Cache
L2 Cache Access Latency
Main Memory Latency
16 in-order cores
MOESI
32KB (2-way associative)
1 cycle
256KB (4-way associative)
8 cycles
90 cycles
4.1.4 Baselines Compared
We compare our scheme with two previous representative
schemes: the ﬁrst scheme employs eﬃcient routing but at
the expense of lower consolidation density [20], denoted by
“Regularity-oriented” scheme. Clearly, “Regularity-oriented”
scheme ideally enforces performance isolation. The second
scheme takes the opposite, i.e. emphasizing the density, but
paying for more complex routing mechanisms [16], denoted
by “Density-oriented” scheme.
In the experiment we will show that our scheme, “RISO”,
can also preserve performance isolation as “Regularity-oriented”
scheme provides, and meanwhile achieves the consolidation
density of “Density-oriented” scheme, but with much better
performance.
4.2 Result 1: Consolidation Density
Figure 8 shows the system utilization from underload to
overload scenarios. The result shows that RISO improves
the system utilization by up to 12% higher than Regularityoriented scheme in the overload condition. Surprisingly, RISO
performs almost equally well to Density-oriented scheme (within 0.1%). Even though RISO cannot exploit all irregular regions due to the link utilization constraint, but it can deal
with some unique regions such as “⊏” which cannot be supported in “Density-oriented” scheme, which makes our scheme
match density-oriented scheme in consolidation density.
Moreover, our scheme uses DOR as the underlying routing
algorithm which is more eﬃcient and cost-eﬀective regarding
network performance, as the following results show.
4.3 Result 2: Performance
4.3.1 Performance Isolation Analysis
Regularity-oriented schemes are regarded for ideal performance isolation by enforcing strict physical isolation, as described in Section II. Hence, we compared our relaxed isolation scheme to the regularity-oriented scheme to verify the capability of performance isolation. We run application traces
Figure 8: System utilization for 16x16 and 32x32
mesh
RISO
Regularity-oriented
)
l
s
e
c
y
c
(
y
c
n
e
t
a
L
t
e
k
c
a
P
e
g
a
r
e
v
A
25
20
15
10
5
0
Figure 9: Performance comparison between routingoriented scheme and RISO
under RISO and regularity-oriented condition respectively.
The results are shown in Figure 9. RISO won’t degrade the
latency compared to the regularity-oriented scheme. This is
because we put a hard constraint for each shared link, as
Section III describes. Therefore, we can safely conclude that
RISO preserves performance isolation, but with higher consolidation density as shown in Figure 8.
4.3.2 Network Latency Analysis
Density-oriented scheme is notorious for traﬃc latency, although it can enforce performance isolation and provide high
consolidation density. Hence, we show how much latency can
be improved with RISO. We run various combinations of two
application traces under relaxed and density-oriented isolation respectively. Note that RISO uses eﬃcient DOR, while
density-oriented scheme uses the most favorable Up*/Down*
[9] routing mechanism.
Figure 10 shows the comparison results for 25 groups of
consolidated applications. Clearly, our scheme wins for all. The latency, if using density-oriented scheme, will degrade
over 100% for some groups such as blackscholes_bodytrack
(114%) and swaptions_bodytrack (101%), which are usually the mix of computation intensive and memory intensive applications. Such combination usually renders more
under-utilized links which RISO can take advantage of while
density-oriented scheme cannot. The average latency improvement is 38.4% for 25 groups of consolidated applications.
5. RELATED WORK
The concept of RISO is similar to the topology virtualization techniques proposed in [21] and [22], in which NoC
is reconﬁgured and the software always observes a logically
regular-shaped topology when faults are happening. However, our work targets the mapping of the arbitrary-shaped
topology required by an application into the appropriate region to improve consolidation density.
Quality of service (QoS) in NoC level is introduced to fairly
allocate on-chip resources according to speciﬁc service poli 
 
 
)
l
s
e
c
y
c
(
y
c
n
e
a
L
t
t
e
k
c
a
P
e
g
a
r
e
v
A
RISO
Density-oriented
45
40
35
30
25
20
15
10
5
0
Figure 10: Latency comparison between densityoriented scheme and RISO
cies. For instance, Brand et al [13] uses link utilization as
the congestion measure, and proposed that the bandwidth
and latency will be guaranteed for best-eﬀort communication service if the contribution to link utilization by diﬀerent
workloads stays below a certain threshold.
In [23], virtual point-to-point links are used to ensure reliable communication if the threads of an application are distributed into
disjoint NoC regions. Our work is orthogonal to them.
Performance isolation technique is highly required to achieve
controllable QoS in NoC level, and is ﬁrstly introduced in
[24]. It clariﬁes the basic items needed to solve in this area.
Particularly, the tradeoﬀ between regularity of topology and
complexity of routing is the most important in that it relates
directly to the network performance and power consumption.
Some proposed techniques follow the strict NoC isolation
strategy using rectangular shapes for performance isolation,
such as [20]. These methods are restrained by the maximum
number of consolidated workloads, which will degrade the
consolidation density. Unlike those regular shaped performance isolation methods, Solheim et al.
[16] proposed an
irregular-shaped isolation based on complex routing mechanism. This method also follows strict isolation between
workloads and to some extent improves consolidation density compared with rectangle based isolation. However, its
routing mechanism is less eﬃcient and exhibits substantial
degradation with respect to network performance. To the
best of our knowledge, the proposed relaxed isolation scheme
is the ﬁrst work that simultaneously takes the advantage of
regularity of topology and eﬃcient routing mechanisms in
network isolation.
6. CONCLUSION
This paper proposes relaxed isolation (RISO) strategy to
enforce performance isolation constraint in workload consolidation. Unlike the traditional strict isolation strategy such
as regularity-oriented and density-oriented approach, RISO
allows under utilized links to be shared by multiple applications, as long as the aggregated contribution to link utilization is lower than the congestion threshold. Compared
with regularity-oriented approach, RISO supports more ﬂexible topologies and can greatly improve consolidation density.
RISO does not complicate the routing mechanism required
by density-oriented approach; it also uses eﬃcient and costeﬀective DOR routing mechanism and hence yields higher
network performance. In other words, RISO strikes better
tradeoﬀ between regularity of topology and complexity of
routing algorithm in implementing network isolation. We
therefore believe that RISO is a promising scheme for workload consolidation in many-core cloud processors.
7. ACKNOWLEDGEMENT
This work is supported in part by National Basic Research
Program of China (973) under grant No. 2011CB302503,
in part by NSFC under grant No.
(61202056, 61100016,
61076037, 60921002). Correspondence should be addressed
to Prof. Yinhe Han. We also thank Prof. Ninghui Sun and
Prof. Lixin Zhang for their support.
8. "
Smart hill climbing for agile dynamic mapping in many-core systems.,"Stochastic hill climbing algorithm is adapted to rapidly find the appropriate start node in the application mapping of network-based many-core systems. Due to highly dynamic and unpredictable workload of such systems, an agile run-time task allocation scheme is required. The scheme is desired to map the tasks of an incoming application at run-time onto an optimum contiguous area of the available nodes. Contiguous and unfragmented area mapping is to settle the communicating tasks in close proximity. Hence, the power dissipation, the congestion between different applications, and the latency of the system will be significantly reduced. To find an optimum region, we first propose an approximate model that quickly estimates the available area around a given node. Then the stochastic hill climbing algorithm is used as a search heuristic to find a node that has the required number of available nodes around it. Presented agile climber takes the steps using an adapted version of hill climbing algorithm named Smart Hill Climbing, SHiC, which takes the runtime status of the system into account. Finally, the application mapping is performed starting from the selected first node. Experiments show significant gain in the mapping contiguousness which results in better network latency and power dissipation, compared to state-of-the-art works.","Smart Hill Climbing for Agile Dynamic Mapping in ManyCore Systems 
Mohammad Fattah, Masoud Daneshtalab, Pasi Liljeberg, Juha Plosila 
Department of Information Technology, University of Turku, Turku, Finland 
{mofana, masdan, pakrli, juplos}@utu.fi 
ABSTRACT 
Stochastic hill climbing algorithm is adapted to rapidly find the 
appropriate start node in the application mapping of networkbased many-core systems. Due 
to highly dynamic and 
unpredictable workload of such systems, an agile run-time task 
allocation scheme is required. The scheme is desired to map the 
tasks of an incoming application at run-time onto an optimum 
contiguous area of the available nodes. Contiguous and unfragmented area mapping is to settle the communicating tasks in 
close proximity. Hence, the power dissipation, the congestion 
between different applications, and the latency of the system will 
be significantly reduced. To find an optimum region, we first 
propose an approximate model that quickly estimates the available 
area around a given node. Then the stochastic hill climbing 
algorithm is used as a search heuristic to find a node that has the 
required number of available nodes around it. Presented agile 
climber takes the steps using an adapted version of hill climbing 
algorithm named Smart Hill Climbing, SHiC, which takes the runtime status of the system into account. Finally, the application 
mapping is performed starting from the selected first node. 
Experiments show significant gain in the mapping contiguousness 
which results in better network latency and power dissipation, 
compared to state-of-the-art works.   
Categories and Subject Descriptors: D.4.7 [Operating 
Systems]: Organization and Design – Real-time systems and 
embedded systems. 
General Terms: Algorithms, Management, Performance, 
Design. 
Keywords: On-chip many-core systems, Application mapping, 
Task allocation, AI algorithms, Hill climbing. 
1. INTRODUCTION 
The Future Multi-Processor Systems-on-Chip (MPSoCs) are 
likely to have tens or hundreds of resources connected together. 
Networks on chip [1] (NoCs) have emerged as a promising 
solution for communication infrastructure of such systems. NoCs 
provide a regular platform for connecting the system resources 
and makes the communication architecture scalable and flexible 
compared to traditional bus or hierarchical bus architectures.  
Many-core systems will feature an extremely dynamic workload 
where an unpredictable sequence of different applications enter 
and leave the system at run-time. In order to handle the featured 
dynamic nature, a run-time system manager is required to 
efficiently map an incoming application onto the system resources 
[2]–[4]. Applications are modeled as a set of communicating 
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
to republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee. 
DAC ’13, May 29 - June 07 2013, Austin, TX, USA. 
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00 
tasks, and the mapping function of the central manager (CM) of 
the system decides on the appropriate node for each task.  
The system performance is significantly influenced by the 
utilized mapping approach. For example, assume a dispersed 
application mapping where tasks of an application are mapped 
onto a distant and fragmented set of nodes. This increases the 
average hop count within tasks of an application and places tasks 
of different applications between each other. Accordingly, the 
power dissipation and congestion probability (latency) of the 
network will dramatically increase. On the other hand, consider a 
convex and contiguous application mapping where tasks of an 
application are placed on relatively close nodes without 
fragmentation. This will reduce the average distance between 
mapped tasks and isolates the communication of different 
applications from each other which decreases the congestion 
probability between them. 
To have an efficient mapping, it is desired to place an application 
on a near convex and contiguous set of nodes where the remaining 
set of available nodes is also kept contiguous. Finding a convex 
region of nodes is a polynomial, O(n3), problem [5]. However, 
considering more aspects such as near-convexity condition, 
remaining nodes contiguity, etc. turns it into clustering problems 
[6] with deterministic algorithms of NP-hard complexity. Indeed, 
these are not tolerable time complexities regarding the growing 
size and dynamic nature of the systems. 
In this work, we propose an algorithm with significantly 
decreased complexity in order to find the appropriate area for a 
given application. The task allocation is then efficiently carried 
out through our CoNA method [4]. Briefly stated, CoNA starts 
from a first node and attempts to map the application tasks onto a 
set of contiguous nodes around it. Thus, the job of finding the 
appropriate area is to select a first node which has the required 
number of contiguous and near convex available nodes around it 
and leads to least fragmentation of remaining nodes. For this, we 
first propose an approximate model to estimate the number of the 
available (contiguous and near convex) nodes around any given 
node. Then, hill climbing search heuristic is adapted in order to 
find the optimum first node rapidly among all the available nodes. 
The proposed Smart Hill Climbing, SHiC, is equipped with a level 
of intelligence in which the steps are not taken fully stochastic. 
The proposed near optimal (non-deterministic) solution tackles 
the problem in O(√n), n is the given network size, in the best case 
and O(n2) in the worst case; which is an impractical case. 
The rest of the paper is organized as follows: In Section 2 we 
present related works and motivate the impact of the optimum 
area (first node) selection. Section 3 formally defines the mapping 
problem and different metrics to evaluate the mapping results.  
Section 4 describes our SHiC heuristic in the first node selection. 
The simulation details along with the experimental results are 
presented and discussed in Section 5. Finally, Section 6 concludes 
the paper. 
2. RELATED WORK AND MOTIVATION 
There are several works dealing with dynamic management of 
workload in multi- and many-core systems. In this Section we 
explore different first node or generally the mapping area 
selection methods used by different works. We also motivate the 
importance of the selection method by presenting several 
examples.  
 
 
 
Figure 1. Possible scenarios resulting in the area dispersion. (a) Initial configuration of the system and the first node for the next 
application in different methods. (b) Mapped area (green hatched area) dispersion in NN, INC and CoNA methods. (c) 
Contiguous mapping of DistRM, STDM and VIP-supported approaches, however they lead to fragmentation of remaining nodes. 
Carvalho et al. [2], [7] present different heuristics, such as 
failure a new random node is chosen until several times. Both 
Nearest Neighbor (NN), Best Neighbor (BN), etc. In all of their 
methods impose huge negotiation traffic on the network in order 
heuristics, a clustering mechanism for the first node selection is 
to compensate the applied randomness.  
considered. A set of sparse nodes (cluster nodes) are assumed to 
Asadinia et al. [11], in their VIP-supported approach, find all 
select the first node of the mapping algorithm among them; i.e. a 
contiguous regions of free nodes, and select the smallest region 
mapping solution can exist only when a free cluster node exists. 
with the size greater than or equal to the application size. Then, 
This is to assure some amount of available nodes around the 
they start mapping from one of the selected region nodes with the 
chosen first node. 
maximum number of neighbors. This is to guarantee the 
Chou et al. [3], in their incremental (INC) approach, break down 
contiguity of the mapped region and select a first node with 
the mapping problem into two steps: the region selection, and the 
maximum available nodes around it. 
task allocation. In the region selection step, they start from the 
As another possible scenario, let us assume that in the system 
closest node to the CM and include it in the region. Then, they 
status of Figure 1 (a), the node (4, 7) is chosen by VIP as it is one 
iteratively add nodes to the selected region trying to keep both the 
of the nodes with maximum number of neighbors, DistRM or 
selected region and remaining nodes contiguous. Afterward, in the 
STDM after several random node generations. The selected first 
task allocation step, application tasks are mapped inside the 
node has enough resources around it for the proposed application 
selected region. 
with 12 tasks and, as shown in Figure 1 (c), a contiguous set of 
As an advanced approach, CoNA [4] selects the closest node to 
nodes are allocated to the application tasks. However, the 
the CM with all its four neighbors available. Thus a minimum 
remaining set of available nodes is dispersed and the future 
number of available nodes are assured. Then, task graph is 
applications may suffer from the area fragmentation of the 
traversed in breath-first order and tasks are mapped onto the 
available nodes. 
neighboring of their parents in which a smaller square is formed. 
Note that optimum first node selection is not only a function of 
There are lots of possible scenarios in which NN, INC and CoNA 
the current state of the system but also the size of the application. 
methods would easily result in area fragmentation. Figure 1 (a) 
For instance, CoNA, NN and INC approaches could have worked 
and (b) depict one of those scenarios. Notice that the cluster nodes 
optimally, in the described scenario, if the size of the application 
of the NN algorithm are indicated by asterisks. The current status 
had been less than or equal to 8. Moreover, the area selection 
of the system is shown in Figure 1 (a), where four applications are 
method not only has to be agile but also must avoid random trial 
running on the system after entrance and exit of several 
and error approaches. This 
is 
to minimize 
time and 
applications. As indicated in the Figure 1 (a), the selected first 
communication penalties imposed onto the system. 
node for the next application will be nodes (1, 1) or (0, 1) for 
In this work, we propose an algorithm for optimum first node 
CoNA and NN or INC algorithms, respectively. Now, an 
selection in order to achieve an agile and efficient mapping of 
application with 12 tasks enters the system while there are only 8 
applications. It uses the general knowledge of currently running 
contiguous nodes available regarding the selected first node. 
applications and imposes no additional traffic on the network. 
Accordingly, a fragmented area of nodes (the hatched area in 
Figure 1 (b)) will be chosen for the application. Consequently, as 
can be seen, the communication paths between dispersed nodes 
will be stretched and can be congested by communications of 
other existing applications (1 to 3) .  
As a decentralized mapping algorithm for 
tree-structured 
applications, Weichslgartner et al. [8] suggest three different 
methods for the first node selection. Two of them need prior 
knowledge about all incoming applications, while the one which 
is more dynamic, the farthest-away algorithm, leads to inefficient 
results as it does not consider the size of applications. 
Both the decentralized DistRM [9] and STDM [10] approaches, 
start mapping from a random node. The randomly chosen node in 
DistRM starts exploring its neighboring nodes as well as distant 
nodes in the system to find the node with enough surrounding 
resources to start allocation. To this end, DistRM utilizes a 
negotiation algorithm in cooperation with a proposed distributed 
directory service. On the other hand, the randomly chosen node in 
STDM tries to map the incoming application around it. In case of 
3. DEFINITIONS 
In the following, we present formal definitions of an application, 
the NoC architecture, and the mapping problem. In order to 
reduce the problem size and simplify the analysis, we consider a 
homogenous mesh-based NoC in our definitions and experiments. 
Moreover, we define several evaluation metrics as assessment 
tools to compare different algorithms. The metrics weigh the 
resulted mapping of an application. Later in Section 5, we 
evaluate effect of our first node selection method, SHiC, on the 
network performance using these metrics along with extracted 
results of the network simulation. 
denoted as a task graph Ap =TG(T, E). Each vertex ti ∈ T 
Mapping algorithms try to allocate system resources, connected 
together through an on-chip network, to tasks of a requested 
application in an optimal way. 
Each application in the system is represented by a directed graph 
3.1 Problem Definition 
*
*
*
*
*
RAN
or VIP
*
*
*
App 1
*
App 2
*
INC
CM
*
*
NN or 
CoNA
*
*
*
App 3
App 4
*
X 0
1
2
3
4
(a)
5
6
7
8
0
1
2
3
4
5
6
7
8
Y
*
*
*
*
*
*
*
*
App 1
*
App 2
*
*
CM
*
*
*
*
App 3
App 4
*
X 0
1
2
3
4
(b)
5
6
7
8
0
1
2
3
4
5
6
7
8
Y
*
*
*
*
*
*
*
*
App 1
*
App 2
*
*
CM
*
*
*
*
App 3
App 4
*
X 0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
Y
(c)
 
represents one task of the application Ap, while the edge ei,j ∈ E 
stands for a communication between the source task ti, and the 
destination task tj. Task graph of an example application with 6 
tasks is shown in Figure 2. The amount of data transferred from a 
XY routing (Figure 3 (a)). The AG contains a set of nodes nx,y ∈ 
N, connected together through communication links lk ∈ L. Each 
task ti to tj of edge ei,j is indicated on the edge as wi,j.  
An architecture graph AG(N, L) describes the communication 
infrastructure, which is a simple M×M 2D-mesh NoC with the 
node nx,y contains a 5-port router rx,y connected to the local 
processing element pex,y by its local port.  
Mapping of an application onto the NoC-based multi-core 
system is defined as a one-to-one mapping function from the set 
of application tasks T, to the set of NoC nodes N:  
map: T→ N, s.t. map(ti ) = nx,y; (cid:1482)(cid:1872)(cid:3036) ∈ (cid:1846), (cid:1484)(cid:1866)(cid:3051) ,(cid:3052) ∈ (cid:1840)
(1)
Based on the definition, a mapping function is started if and only 
if there are enough available nodes to map onto them. Figure 3 (a) 
illustrates a possible mapping of the application in Figure 2, onto 
the described NoC platform. For simplicity, we denote a node 
where a task ti is mapped onto as nti and the packet corresponding 
to the edge ei,j as pcki,j; i.e. the packet sent from nti to ntj. The set 
of running applications on the system is also denoted by APPS 
which changes at run-time due to the system’s dynamic nature. 
Note that the cardinality of a set shows the number of elements 
in that set, while the cardinality of a number means the absolute 
value of that number; e.g. |APPS| means number of running 
applications, while |-4| equals the number 4. 
3.2 Evaluation Metrics 
3.2.1 Average Weighted Manhattan Distance 
(cid:1827)(cid:1849)(cid:1839)(cid:1830)(cid:3040)(cid:3028)(cid:3043)((cid:3002)(cid:3291) ) = ∑
(cid:1482)(cid:3032)(cid:3284),(cid:3285)∈(cid:3006)
The dissipated energy related to a packet delivery is a function of 
both the packet size, and the path the packet traverses [12]. As a 
metric 
to evaluate 
the power consumption of a mapped 
application, Average Weighted Manhattan Distance (AWMD) is 
the sum product of MD and corresponding weight of 
 (2)
communicating nodes, averaged by the total communication 
volume of the application: 
In other words, the AWMD of a mapped application determines 
the number of hops that each bit of the application data traverses 
in the network. The AWMD value in Figure 3 (a), for instance, is 
63/35 = 1.8, which means each bit will dissipate 1.8 times of 
energy unit despite the networking overheads. 
 (cid:1875)(cid:3036) ,(cid:3037) (cid:3400) (cid:1839)(cid:1830)((cid:1866)(cid:1872)(cid:3036) , (cid:1866)(cid:1872)(cid:3037) )
∑ (cid:1875)(cid:3036) ,(cid:3037)
3.2.2 Mapped Region Dispersion 
The network performance is highly correlated to the network 
congestion [13], [14]. Congestion increases the network latency 
dramatically [15] and also increases the network dynamic power 
consumption considerably [16]. This is why there are plenty of 
works aiming to diminish the network congestion in different 
aspects, like routing [16], [17] . 
Two types of congestion can be defined from the perspective of 
dynamic application mapping: external and internal congestions. 
External congestion occurs when a network channel is contented 
n0,M-1
n1,M-1
(ni,j,ni -1,j)
r i,j
pei,j
ni,j
t4
t5
t3
t0
n0,0
t1
t2
nM-1,1
nM-1,0
(a)
(b)
(c)
Figure 2. (a) NoC-based platform, with an application mapped 
onto it (the highlighted region.). (b) An area of 24 nodes with 
minimum MRD value; and (c) the same number of nodes with 
the least NMRD value. 
by the packets of different applications; while the internal 
congestion is related to the packets of the same application.  
Hence, the internal congestion probability is related to the utilized 
mapping algorithm [4], and thus it is out of this paper scope. 
As aforementioned, 
to decrease 
the external congestion 
probability, the mapped area of an application should be as 
convex as possible and minimally fragmented. Several works, e.g. 
[3], [4], considered the average pairwise MD of the allocated 
nodes as a metric to assess the Mapped Region Dispersion: 
(cid:1839)(cid:1844)(cid:1830)(cid:3040)(cid:3028)(cid:3043)((cid:3002)(cid:3291) ) = ∑
(cid:3047)(cid:3284) ,(cid:3047)(cid:3285)∈(cid:3021) (cid:3435)|(cid:3021) |(cid:2870) (cid:3439) 
(cid:1839)(cid:1830)((cid:1866)(cid:1872)(cid:3036) , (cid:1866)(cid:1872)(cid:3037) )
Distant node allocation will increase the MRD for the obtained 
mapping; i.e. more external congestion probability. The increased 
congestion probability is originated from communication of tasks 
of different applications that are mapped among each other.  
The area with the smallest MRD is almost circular [18]. 
Regarding the mesh topology of the network, however, a circular 
region will generate irregularity in remaining available nodes and 
more area fragmentation in long term. This is shown in Figure 3 
(b) for allocation of 24 nodes with the least MRD value. On the 
other hand, a rectangular allocation forms regular regions, 
decreases 
applications overlap 
and 
thus 
isolates 
their 
communications. Thus, the best mapped area would be square 
(shown in Figure 3 (c)) as it is the rectangle with the smallest 
MRD. It can be shown that the MRD of a square with |T| nodes 
(3)
will be: MRDSQ(|T|)= (cid:2870)(cid:3400)(cid:3493)|(cid:3021) |(cid:2871)
(4)
Thus the Normalized MRD metric is defined which assesses the 
squareness of the mapped region independent of the size of the 
application: 
(cid:1840)(cid:1839)(cid:1844)(cid:1830)(cid:3040)(cid:3028)(cid:3043)((cid:3002)(cid:3291) ) = 1 (cid:3397) (cid:3627)(cid:1839)(cid:1844)(cid:1830)(cid:3040)(cid:3028)(cid:3043)((cid:3002)(cid:3043)) (cid:3398) (cid:1839)(cid:1844)(cid:1830)(cid:3020)(cid:3018)(|(cid:3021) |) (cid:3627)
(cid:1839)(cid:1844)(cid:1830)(cid:3020)(cid:3018)(|(cid:3021) |)
The NMRD value of 1 means a squared area. NMRD increases as 
the mapped area is getting more fragmented and less similar to a 
square shape. For example, the NMRD of the dispersed mapped 
area in Figure 1(b) is 1.86 while it is 1.01 in the rectangle region 
of Figure 1(c). Moreover, as the MRD of the first area is almost 
twice that of the second one (4.3 vs. 2.3); the AWMD will get 
almost doubled using the same mapping strategy. Thus, dispersed 
allocation not only increases the congestion probability but also 
results in more power dissipation of the network. This shows the 
importance of this work in finding the optimum area for an 
application. 
(5)
Figure 3. An application with 6 tasks and 7 edges. 
4. SMART FIRST NODE SELECTION 
Based on the motivational examples and analysis presented in 
previous sections, the optimum area for an application mapping is 
 
a (i) contiguous and (ii) square region of available nodes. CoNA 
[4] attempts to form a contiguous area around its first node while 
favoring square shapes. Given that, appropriate selection of the 
mapping first node is the important step towards the contiguity of 
the allocated area. Accordingly, the appropriate first node is the 
one with the required number of available nodes in a square shape 
around it while the minimum fragmentation of remaining nodes 
occurs after the mapping. 
In this section, we first define our approximate model which 
estimates the number of available nodes in a square shape around 
a given node. Then the proposed model is utilized in the adapted 
hill climbing search hubristic to find the appropriate first node in 
an agile and smart manner. 
4.1 Square Factor 
The square factor of a given node, SF(ni,j), is the estimated 
number of contiguous, almost square-shaped, available nodes 
around that node. Hence, the appropriate first node for mapping of 
an application would be the node with the SF equal to the 
application size. 
Each running (already mapped) application in the system is 
modeled as a rectangle defined by its left-down and right-up 
corner nodes. Regarding the modeled rectangle of a running 
application, there might be (1) some nodes within the rectangle 
which do not belong to the application and/or (2) some nodes of 
application which are excluded from the rectangle. The rectangle 
of each application is modeled such that minimizing the number 
of these two types of nodes. The rectangle models of running 
applications 1 to 3 of Figure 1 are shown in Figure 4 (a). For 
instance, the rectangle of the application 1 excludes one node 
(n2,2) of the application and the rectangle of the application 3 
includes one node (n3,1) which is not part of the application. 
However, they are the best fit rectangles according to the included 
and excluded nodes. 
To calculate the SF(ni,j), we first find the largest square centered 
on ni,j, SQmax = (ni,j, rmax), where it fits within the mesh limits and 
has no conflict with other rectangles (running applications) of the 
system. This is shown in Figure 4 (b) for the node n7,1 which is the 
first node of the application 4. In addition to the SQmax area, there 
might be also some more nodes beyond the square borders not 
belonging to system rectangles, as marked with asterisk in Figure 
4(b). These nodes are counted in order to prevent available nodes 
from being isolated while keeping the mapped area close to the 
square shape. The square factor of a given node ni,j is calculated 
by summing up the area of the SQmax with the available nodes 
beyond the square borders. For instance, the SF(n7,1) will be the 
square area, 9, summed up with marked nodes, 5, which is 14. 
During the SF calculation of a node, the algorithm also calculates 
a direction, called open direction (openDir), which indicates one 
of the eight neighbors of the node estimated to have a larger SF. 
The openDir is towards that side of SQmax where there is the 
maximum number of nodes beyond it. In the example shown in 
Figure 4 (b), there are 3 asterisked nodes beyond the up side of the 
SQmax versus 1 in the left side (corners are not counted in). Thus 
the openDir would be upward, meaning the upper neighbor is 
probably holding a larger square factor. When there is more than 
one side with the maximum number of nodes beyond them, the 
openDir is then towards their vector addition. An openDir of 
value zero means no specific direction is predicted to result in a 
larger square factor. The climber function of the hill climbing 
heuristic uses the computed open direction to provide smartness in 
taking the steps. 
Note that there can be available nodes inside an application 
rectangle; e.g. n3,1 is inside the rectangle of the application 3. In 
such cases the square factor is evaluated by counting the available 
neighbors of the node, and the open direction is toward the side 
exiting from the rectangle. For instance, SF(n3,1) = 3 because two 
of the node’s neighbors are available (n2,0 and n2,1); the open 
direction is leftward to exit from the rectangle of the application. 
Moreover, the system CM is always assumed as a rectangle of one 
node. 
Figure 4. (a) Applications are modeled as rectangles in our 
approach. (b) Square factor calculation of the node n7,1 before 
application 4 being mapped. 
As the algorithm for the SF calculation explores the distance 
between the given node and all rectangles (running applications, 
APPS) of the system to find the SQmax, the SF calculation has a 
linear time complexity of O (|APPS|). 
4.2 SHiC: Smart Hill Climbing 
To find the appropriate first node, one can calculate the SF, 
O(|APPS|), for all M×M nodes of the system, and select the one 
with the best SF regarding the application size (exhaustive 
search). However, this will take O(M2|APPS|) time which is not a 
tolerable complexity for the future many-core systems with 
hundreds of nodes. Instead, SHiC starts from a randomly selected 
node and walks smartly through the network nodes by means of 
the defined open direction to reach the optimum node. This 
significantly reduces the amount of traversed nodes, resulting in 
an agile mapping algorithm. The pseudo code of our proposed 
algorithm is shown in Figure 5.  
The original hill climbing heuristic starts from a node and moves 
in the direction of the increasing value (of the SF) to the uphill 
(max. SF) [19]. However, the appropriate first node is not the one 
on the uphill but the one in a specific height; i.e. the node with the 
SF equal to the application size. Hence, SHiC moves in the 
direction of the optimum SF instead of the maximum one. SHiC 
looks for the node with the optimum SF according to a preference 
Inputs: Size (|T|) of the requested application Ap, current set of running 
applications APPS. 
Output: chosen first node:  nfn. 
(1) repeat 2+√|APPS| times 
(2)     ncur ← choose a random available node; 
(3)     iter ← 0 ; 
(4)     yawAngle ← 0 ; 
(5)     while (iter < (M/2) AND SF(ncur) ≠ |T|) 
(6)         if openDir(ncur) = (0,0) then 
(7)             moveDir ← a random direction; 
(8)         else 
(9)             moveDir ← (SF(ncur) < |T| ) ? openDir (ncur): -openDir(ncur); 
(10)             divert moveDir by (yawAngle); 
(11)         end if 
(12)         nnext ← move in moveDir; 
(13)         if nnext is available AND preferred to ncur then 
(14)             ncur ← nnext ; 
(15)             yawAngle ← 0 ; 
(16)         else 
(17)             yawAngle ← a random in opposite side ; 
(18)         end if 
(19)     end while 
(20)     if ncur is preferred to nfn then 
(21)         nfn ← ncur ; 
(22)     end if 
(23) end repeat 
Figure 5. Smart Hill Climbing, SHiC, pseudocode.
(6)
function. Standing on a node ncur, a step to the next node nnext is 
called to be a preferred step when: 
(SF(ncur)  < |T| AND SF(nnext) > SF(ncur)) OR 
(SF(nnext) ≥ |T| AND SF(nnext) < SF(ncur)) 
By means of the defined preference function, SHiC first looks for 
the node with the smallest SF value which is larger than or equal 
to the application size. Otherwise, the node with the largest SF 
value is preferred. Note that, when there are two nodes with equal 
SF, the one closer to mesh corner is preferred to decrease the 
incurred defragmentation of remaining nodes. 
The original hill climbing heuristic examines all possible 
neighbors to find the best choice, which will increase the 
execution time significantly. As an alternative, stochastic (firstchoice) hill climbing generates random moves until one is 
generated that is preferred to the current one [19]. Moreover, 
SHiC uses the calculated openDir to impart smartness to the steps 
it takes (lines 6 to 11). Standing on the current node, the climber 
first moves according to the openDir. If the taken step is not 
preferred (line 17), the climber yaws and chooses a random 
direction on the opposite side of the yaw angle for the next move. 
The climber takes M/2 steps (line 5) as it is the required number 
of steps to start from a node in the mesh corner and reach the 
center node of the mesh. 
On the other hand, the hill climbing heuristic might get stuck on 
local optimums. For example, the climber might stuck to the node 
n1,1 of the Figure 1, while looking for a node with the SF equal to 
12. As a solution, we use the random restart approach in which the 
proposed stochastic hill climbing approach is executed several 
times starting from different randomly chosen nodes [19]. The 
algorithm is repeated (2+√|APPS|) times (line 1) as the number of 
the fragmented regions in the system is related to the number of 
running applications. 
Finally, the best found node is passed to the mapping algorithm 
as the first node. The SHiC outer loop is executed O(√|APPS|) 
times, the while loop is executed O(M) times and SF(nnext) is 
calculated in each iteration in O(|APPS|). Thus, the time 
complexity of the SHiC will be O(M×|APPS|3/2). This is 
significantly faster than the exhaustive search, and gets equal 
when |APPS| is equal to the mesh size; i.e. one application per 
node which is an impractical case. 
5. RESULTS AND ANALYSIS 
In this section, we assess the impact of the SHiC method on 
improving the mapping results. Several set of applications with 4 
to 35 
tasks are generated using TGG 
[20] where 
the 
communication volumes (wi,j) are randomly distributed between 2 
to 16 flits of data. Experiments are performed on our in-house 
cycle-accurate SystemC many-core platform which utilizes a 
pruned version of Noxim [21], as its communication architecture. 
Different mapping and first node selection methods are evaluated 
over the network size varying from 8×8 to 20×20 nodes. These 
sizes are according to the current industry trends [22], [23] as well 
as the future many-core systems. 
A random sequence of applications is entered into the scheduler 
FIFO according to the desired rate, λ. The sequence is kept fixed 
in all experiments for the sake of fair comparison. Applications 
are scheduled based on First Come First Serve (FCFS) policy and 
the maximum possible scheduling rate is called λfull. An allocation 
request for the scheduled application is sent to the CM of the 
platform residing in the node n0,0. The first node is selected using 
our SHiC method as well as other approaches. Then the 
Table 1. Extracted Results for Different Methods 
Mapping / first node 
CoNA 
/ INC 
CoNA 
/ NN 
CoNA 
/ CoNA 
CoNA 
/ SHiC 
CoNA 
/ Exh. SF 
INC 
/ INC 
INC 
/ SHiC 
Lavg 
42 
43 
41 
40 
39 
52 
42 
Ext. Cong. 
7.71 
7.65 
5.63 
3.96 
3.55 
11.51 
4.02 
NMRD 
1.26 
1.20 
1.09 
1.00 
0.97 
1.27 
1.02 
AWMD 
1.29 
1.23 
1.06 
1.00 
0.96 
1.49 
1.04 
application is mapped according to the desired mapping algorithm 
and tasks are allocated to the system nodes regarding the mapping 
result. Nodes emulate the behavior of their allocated task and 
inform the CM upon the task termination. Hence, the CM is kept 
updated about the status of the nodes at run-time without using 
any monitoring facilities. In order to have a holistic view of the 
results and enable real case comparisons, each set of experiments 
are performed over 10 million cycles where hundreds of 
applications enter and leave the system. 
5.1 Square Factor Accuracy and SHiC 
Success 
In our first study, we assess the accuracy of the square factor and 
SHiC success in finding the optimum node. We run several 
experiments while different combinations of first node selection 
approaches and mapping algorithms are set. In order to assess the 
square factor accuracy, in one of the experiments we perform 
exhaustive search to select the node with the optimum SF as the 
first node. The network size is set to 16×16 and applications enter 
the system with 0.8λfull rate. Table I shows the average latency of 
the network (Lavg), the percentage of packets delivered by external 
congestion (Ext. Cong.), and normalized values of NMRD and 
AWMD metrics for different experiments. 
As can be seen, the best results belong to exhaustive search on 
the SF values. This shows the accuracy of the proposed model. 
Results demonstrate that SHiC has been successful in finding the 
optimum node,"
HCI-tolerant NoC router microarchitecture.,"The trend towards massive parallel computing has necessitated the need for an On-Chip communication framework that can scale well with the increasing number of cores. At the same time, technology scaling has made transistors susceptible to a multitude of reliability issues (NBTI, HCI, TDDB). In this work, we propose an HCI-Tolerant microarchitecture for an NoC Router by manipulating the switching activity around the circuit. We find that most of the switching activity (the primary cause of HCI degradation) are only concentrated in a few parts of the circuit, severely degrading some portions more than others. Our techniques increase the lifetime of an NoC router by balancing this switching activity. Compared to an NoC without any reliability techniques, our best schemes improve the switching activity distribution, clock cycle degradation, system performance and energy delay product per flit by 19%, 26%, 11% and 17%, respectively, on an average.","HCI-Tolerant NoC Router Microarchitecture
Dean Michael Ancajas James McCabe Nickerson Koushik Chakrabor ty Sanghamitra Roy
USU BRIDGE LAB, Electrical and Computer Engineering, Utah State University
{dbancajas, jmnickerson}@gmail.com {koushik.chakrabor ty, sanghamitra.roy}@usu.edu
ABSTRACT
The trend towards massive parallel computing has necessitated the need for an On-Chip communication framework
that can scale well with the increasing number of cores.
At the same time, technology scaling has made transistors
susceptible to a multitude of reliability issues (NBTI, HCI,
TDDB). In this work, we propose an HCI-Tolerant microarchitecture for an NoC Router by manipulating the switching
activity around the circuit. We ﬁnd that most of the switching activity (the primary cause of HCI degradation) are only
concentrated in a few parts of the circuit, severely degrading
some portions more than others. Our techniques increase
the lifetime of an NoC router by balancing this switching
activity. Compared to an NoC without any reliability techniques, our best schemes improve the switching activity distribution, clock cycle degradation, system performance and
energy delay product per ﬂit by 19%, 26%, 11% and 17%,
respectively, on an average.
1.
INTRODUCTION
In the forthcoming era of many-core computing, fueled
by the tremendous growth in on-chip resources from technology scaling, Network-on-Chip (NoC) architectures have
emerged as the design of choice for on-chip communication.
On the other hand, rapid technology scaling has severely undermined the device level reliability, forcing the chip designers to critically consider long term sustainability in system
design. While a large body of recent works targets on-chip
computing resources (processing cores), many-core systems
must consider reliability and sustainability of NoCs. Various aging mechanisms such as Negative Bias Temperature
Instability (NBTI), Hot Carrier Injection (HCI), Time Dependent Dielectric Breakdown (TDDB), and Electromigration play a ma jor role in degrading performance characteristics of NoCs over time. Such a performance degradation
can have a massive system level impact in NoCs, and may
ultimately shorten the chip lifetime prematurely [2, 3].
To extend the period of fault-free execution, few recent
works have addressed aging challenges in NoCs by mitigating
NBTI or Electromigration. For example, Fu et al. propose
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC ’13, May 29 - June 07 2013, Austin, TX, USA.
Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.
techniques to mitigate NBTI aging in NoCs by balancing the
duty cycle in the Virtual Allocator circuits [10]. Bhardwa j et
al. propose aging-aware adaptive routing to throttle NBTI
and Electromigration degradation [3]. NBTI is a critical
but recoverable device aging mechanism. In contrast, HCI
is an unrecoverable aging phenomena [15], which aﬀects the
components due to their dependence on switching activity
[17]. Due to aggressive transistor scaling, the thinner gate
dielectric in CMOS transistors increases the probability of
HCI degradation.
In fact, HCI can account for a ma jor
component of aging in a 10-year product lifetime [19]. To the
best of our knowledge, none of the existing works consider
HCI aging in the NoC architecture.
In this work, we perform a holistic cross-layer analysis of
HCI degradation in the NoC router microarchitecture. We
focus on the crossbar structure of the router microarchitecture, due to its profound signiﬁcance in dictating the router
frequency [16]. Combining application level traﬃc proﬁle
with bit level logic analysis, we ﬁnd that the crossbar structure is highly vulnerable to HCI aging. Due to the data
communication patterns in many-core applications, we observe that a ma jority of gate-level switching activities are
restricted to a small portion of the entire crossbar circuit
topology, resulting in a large HCI degradation. To throttle HCI aging in the crossbar, we propose a series of lowoverhead techniques that evenly distribute the switching activity in the crossbar, without aﬀecting the architecture level
routing latency and bandwidth.
We make the following contributions in this paper.
• We develop a cross-layer framework for HCI aging analysis of an NoC router. Our framework combines application traces, RTL gate-level simulation of a crossbar
circuit, logic analysis, and HSPICE simulation of HCI
degradation eﬀect (Sections 3.2 and S1).
• We analyze the switching activity of the crossbar, a
ma jor circuit in an NoC router using real-world applications and ﬁnd that only a small group of gates
account for most of the switching activity. On an average for PARSEC benchmarks, only 25% of the gates
account for more than 75% of the switching activity,
severely damaging some gates while leaving others unscathed (Section 3).
• We propose four schemes using low overhead techniques to evenly distribute the switching activity and
minimize HCI degradation (Section 4). Our four schemes
are: Bit Cruising that distributes the high activity bits
around the channel; Distributed Cycle Mode that exploits idle cycles in the NoC; Crossbar Lane Switching
that manipulates the port in the crossbar by utilizing
the virtual channels; and a combination of Bit-Cruising
and Crossbar Lane Switching.
• We present a holistic evaluation of our proposals spanning full-system simulation down to RTL and gatelevel HSPICE simulation (Section 6). Our best schemes
improve the switching activity distribution by up to
31% (ave: 19%). We also see a maximum of 30%
(ave: 26%) improvement in the clock cycle degradation, while the system performance degradation is reduced by up to 17.6% (ave: 11%) compared to the
baseline scheme. The Energy Delay Product per Flit
is improved by up to 27% (ave: 17%).
2. BACKGROUND
In this section, we introduce our HCI model that correlates threshold voltage degradation with the time spent by
a transistor in stress.
HCI occurs when a carrier overcomes the potential barrier
between silicon and the gate oxide and leaves the channel. A
portion of the carriers (hole/electrons) that leave the channel are deposited into forbidden regions in the transistor
such as the gate oxide. Throughout a transistor’s lifetime,
these deposited carriers change the conductive properties
of the transistor and ultimately lead to degradation of the
threshold voltage (Vth ), drain saturation current (Ion) and
transconductance (∆gm ).
The HCI eﬀect on the transistor parameters described
above can be modeled as a power-law with respect to the
stress time (t) [6, 22]. We only discuss the Vth model as the
one for Ion is similar. The model for ∆gm can be seen in [6].
∆Vth = A · tn
(1)
where A and n are technology dependent parameters. Parameter n has been widely accepted as ∼0.5 over a wide
range of processes [1]. Parameter t is the time the transistor
is under stress, while A is derived as:
A =
q
Cox
KpCox (VGS − Vth · e
Eox
E0 e
ϕit
qλEm
(2)
All relevant parameters in Equation 2 can be obtained from
Wenping et al. [23]. The stress time of a transistor is derived
from the transition density and the pertinent transitions,
since not all inputs that cause switching have a signiﬁcant
contribution to HCI aging [13]. We give a brief background
of how we estimate pertinent transitions in our framework
in Section S1.1.
3. MOTIVATION
In this section, we motivate the need for HCI-aware design of components in an NoC router. We ﬁrst discuss ma jor
reliability concerns in the datapath of an NoC router. We
then explain our framework for holistic HCI aging analysis
of the NoC crossbar. Lastly, we discuss our results, demonstrating the need for HCI-aware techniques in the design of
resilient NoC routers.
3.1 HCI Degradation in the NoC Crossbar
Massively parallel programs running in the many-core use
the NoC as an interconnect fabric due to scalability demands. Processors communicate with each other through
messages sent as packets in the NoC. Since on-chip wiring
Circuit
Logic Depth
# of gates
Crossbar Switch
64-bit ALU
Address Generator
Issue Queue Logic
4
46
43
33
5760
4728
491
189
Table 1: Logic Depth of Various Modules
is abundant, a lot of these packets that were previously sent
over narrow oﬀ-chip buses now cannot fully utilize the whole
channel bandwidth available. Coupled with the fact that
most data sent through the network are narrow width [8],
this trend leads to uneven sensitization of transistors, eventually causing unbalanced HCI degradation across the channel.
The crossbar switch is at the heart of the communication
infrastructure in an NoC router1 , largely dictating the cycle time [16]. There are three critical reliability issues in
an NoC crossbar. First, the gate level activity in a crossbar is only concentrated in a very few bits of the channel
width, due to the bit patterns being sent. This asymmetry causes unbalanced HCI degradation. Second, since most
upper bit transistors do not switch and only maintain their
values, they can undergo NBTI degradation. Third, since
the crossbar is a wide circuit with a shallow logic depth (Table 1), minor delay variations caused by both HCI and NBTI
will have a profound eﬀect on its overall critical path delay.
3.2 Aging Analysis Framework for the NoC
Crossbar
Figure 1 shows the methodology we employ in assessing
HCI degradation in the crossbar circuit. Our cross-layer approach comprises system level simulation of 16-thread parallel programs and their gate-level HCI degradation in a
crossbar circuit. Since HCI depends on switching activity,
we acquire the switching activity of each gate by capturing
cycle-by-cycle actual data values traversing the crossbar. We
then evaluate its overall degradation eﬀect for each transistor in the circuit using our model discussed in Section 2.
However, using real-world applications to assess gate-level
degradation is a computationally intensive task. As such,
we have adopted several important steps to eﬃciently avoid
long simulation times, while still providing a holistic analysis
of HCI aging eﬀect.
First, we pick multiple sample points in diﬀerent phases
of execution of the program. The sample points are chosen
according to traﬃc intensity in the NoC. Each sample phase
contains about 1 million ﬂits. Second, we run our simulation
setup (Section 5) and take the traces of data traﬃc at the
speciﬁed points. Third, we feed these data traces to an
Open Source RTL Verilog model of a 16-core NoC and gather
cycle-by-cycle inputs in the crossbar circuit. Lastly, we use
our novel HCI Aging Analyzer Framework (Section S1) to
analyze degradation in the circuit.
3.3 Results
3.3.1 Logic Depth Analysis
Table 1 shows the results for the logic depth analysis we
perform on ma jor circuits from NoC and processor systems.
We analyzed the crossbar switch from an NoC, the Arithmetic and Logic Unit (ALU), the memory address generator
1We provide an NoC primer on Section S3.
100
80
60
40
20
blackscholes
canneal
dedup
ferret
fluidanimate
swaptions
y
t
i
v
i
t
c
a
g
n
h
c
i
t
i
w
s
f
o
%
Figure 1: HCI Aging Analysis Framework
and the issue queue selector of a Fabscalar core [7]. Among
all these modules, the crossbar has the shallowest logic depth
that can be 10× lower than the other circuits. This characteristic makes it more susceptible to aging as there is little chance that a diﬀerent signal path can hide the delay
incurred by degraded transistors. Thus, we need to implement eﬃcient aging mitigation techniques in the crossbar
data path.
3.3.2 HCI Degradation Results
Figure 2 shows the switching activity data in the crossbar circuit. The x-axis shows the percentage of gates while
the y-axis shows its accumulated switching activity as a percentage of the total activity. Ideally, a 1:1 ratio between the
percentile gates and the switching activity is optimal for HCI
aging (i.e. a straight line with a 45◦ slope). However, it can
be seen that on an average, only 25% of the gates account
for 75% of the total switching activity. This large asymmetry leads to unbalanced HCI degradation between diﬀerent
parts of the circuit and can accelerate failure of NoCs before
their rated lifetime.
We also show the corresponding clock cycle degradation of
an NoC router (22nm, 7 years) in Figure 3 as a result of the
unbalanced HCI degradation. From this data, swaptions experiences the most clock cycle degradation at 10.51%, while
canneal has the least at 8.99%. We can also verify this trend
from Figure 2, where swaptions (left most curve) has the
most concentrated switching activity among all programs.
Both the results above show that the inherent imbalance
in switching activity caused by data patterns sent over the
network causes non-uniform HCI aging in the crossbar circuit. This asymmetrical aging causes some path delays to
increase disproportionately and will eventually lead to premature router failure.
In the succeeding sections, we will
discuss our proposed designs that primarily shift the switching activity from one part of a circuit to another in order to
slow down HCI degradation and balance aging impact.
4. DESIGN OVERVIEW
In this section, we discuss our proposed techniques for mitigating HCI eﬀect in the router crossbar. Our techniques aim
to balance HCI degradation by distributing the switching
activity. We explore four techniques in the router microarchitecture: Bit Cruising (BC) ; Distributed Cycle Mode
(DCM) ; Crossbar Lane Switching (CLS) ; and BCCLS that
is a combination of schemes BC and CLS. Apart from DCM,
0
0
10
20
30
40
50
60
70
Percentile(cid:0)(cid:0)#(cid:0)(cid:0)of(cid:0)(cid:0)Gates
80
90
100
Figure 2: Cumulative Distribution Function of the Switching
Activity vs Gate Count
n
o
i
t
a
d
a
r
g
e
D
l
e
c
y
C
%
12
11
10
9
8
7
6
5
bla c k s c h ole s
c a n n e al
d e d u p
f e rr et
at e
fluid a ni m
s w a ptio n s
Figure 3: Clock Cycle Degradation of a 22nm NoC Router due
to HCI after 7 years
all of our schemes involve minimal modiﬁcations at the frontend of the router and do not aﬀect the critical path of the
pipeline (crossbar traversal).
4.1 Bit Cruising (BC)
Bit Cruising swaps the diﬀerent portions of the data being
transmitted in the crossbar. This technique is largely motivated by two properties of the programs. First, most data
traversing the NoC do not occupy the full channel width
of the network because most data in the cache line are aggregated at the lower bits. In some cases, all data bits are
actually zero. Recent works have also exploited this characteristic by compressing ﬂits and sending only those that
have important data [8]. Second, control requests, while being sent as a single ﬂit also do not store information in the
most signiﬁcant portions of the channel as the routing information can ﬁt in the ﬁrst few bytes of the whole channel. In
our setup, the control ﬂit only utilizes 25% of the channel
width, leaving the remaining 75% constant. Together, these
two characteristics radically lower the switching activity in
certain bits while emphasizing others.
To prevent this asymmetry in HCI degradation, the data
being sent across the network must be such that the switching activity across the channel is distributed. By passing
diﬀerent data values each time a gate is used, it will balance
the switching activity and hence also uniformly degrade all
gates. This is the primary working philosophy of Bit Cruising, where highly changing bits are being cruised around the
channel. The Bit Cruiser circuit is situated in the Network
Interface (NI) and does not add any overhead in the
critical path of the pipeline of an NoC. We explain in
detail the functionality and the circuit implementation of
the BC circuit in Section S2.
4.2 Distributed Cycle Mode (DCM)
The Distributed Cycle Mode aims to balance out degrada 
 
 
 
 
tion of transistors by latching an input value in the crossbar
during idle times such that unswitched transistors in previous cycles will transition and experience equivalent aging.
As such, it does not relieve any HCI aging compared to our
other schemes but can be beneﬁcial as equally aged transistors have smaller leakage power. The DCM mode can also
be coupled with NBTI recovery schemes such as [21]. We
explain the DCM mode in more detail in Section S4.
4.3 Crossbar Lane Switching (CLS)
Our two previous techniques focused on distributing the
switching activity across an entire channel of an input port
to balance HCI degradation. However, another asymmetrical degradation also occurs in the crossbar lanes that are
immune to techniques applied in the channel level.
This type of asymmetric degradation arises when some
input-output pairs are used more than others. We demonstrate this occurrence with an example in Figure 4 where
there are two paths (p0 and p1) that both use the same
East output port. For instance, if path p0 is used more than
p1, then the transistors along the path p0 will be sensitized
more and hence, experience more HCI degradation.
Our third technique, CLS, is also situated at the frontend of the router pipeline and aims to balance the usage of
the crossbar lanes2 . In the canonical router model, an input
port directly forwards ﬂits to the output ports by establishing a physical connection between the two via the crossbar
switch. As such, ﬂits coming from the same input port will
always use the same crossbar lane to connect to diﬀerent
output ports. However, the introduction of Input Buﬀers
(IB) and Virtual Channels (VC) in modern router architectures decouples this one-to-one association because the ﬂits
are ﬁrst stored in the IB before being transmitted to the
output ports. With trivial modiﬁcations in the VC allocator and the Route Calculation part of the pipeline, we can
control which crossbar lane an input port will utilize at any
given time.
This new allocation and routing policy will now cause the
crossbar circuit to use a diﬀerent path and activation circuit,
but still send the same data as if it were coming from the
original input port. Thus, we preserve the correctness of the
ﬂit and the route. Similar to the Bit Cruising technique’s
cruise setting, CLS will need a knob input to indicate the
new mapping between input ports and crossbar lanes. We
expand on this and explain the required overheads in implementing CLS in Section S5.
4.4 Bit Cruising and Crossbar Lane Switching (BCCLS)
Our last technique is a combination of the BC and CLS
schemes. BCCLS combines both the beneﬁt of switching distribution inside a channel (BC scheme) and the distribution
of activity across many channels (CLS scheme). The implementation of BCCLS comes naturally because both BC
and CLS tackle diﬀerent portions of the router circuit. BC
reshuﬄes the data sent through the network while CLS effectively changes the port a ﬂit is coming from by modifying
the VC allocation and route calculation.
5. METHODOLOGY
2 a lane is the path taken by an input port to the output
port
North
South
East
West
Figure 4: East Section of A Crossbar Switch. CLS works on the
inter-lane3 (by changing the path of the data) level while BC
works only on the intra-lane level (by changing the bit ordering
within a path).
In this section, we discuss our simulation infrastructure
that combines multiple tools across diﬀerent abstraction layers. Our methodology can be broadly classiﬁed into three
categories: Architectural Setup, RTL and Switching Activity Simulation and HCI degradation analysis using SPICE.
5.1 Architectural Setup
Our simulation setup is composed of a 16-node mesh system arranged in a 4×4 grid. Each node in the system is
composed of 1 processor, 1 L1 Cache and a slice of a systemshared L2 cache. Each router in the system has seven sets of
input and output ports including the ones for the processor
and caches. The ﬂit size is conﬁgured at 16-bytes (128 bits).
A single control request ﬁts in a single ﬂit while data ﬂits
needed to transfer a 64-byte cache line are sent in ﬁve (4
data + 1 control) consecutive ﬂits. Each processor’s L1 and
L2 cache sizes are 64kB and 512kB, respectively.
5.2 RTL and Switching Activity Simulation
The ﬁrst step in obtaining an accurate switching activity is
to produce real-word data vectors from standard benchmark
programs as inputs to the RTL circuits. We use the PARSEC [4] benchmark suite (large inputs) running on gem5 [5]
to collect data traces. We collect data traces for the four
center most routers in a 16-node mesh.
After the traces are taken, we implement a trace feeder
through a Verilog VPI based functional veriﬁcation framework called Teal [18]. This module allows us to easily obtain
cycle-by-cycle values in any sub-module of the router such
as the crossbar.
5.3 HCI Degradation Analysis
Using the outputs from the previous step, our logic analysis tool is then used to obtain the transition densities of each
transistor (Figure 1). We post-process all the results in our
HAAF (Section S1) to calculate Vth degradation and simulate them in HSPICE to obtain clock cycle degradation data
for all paths and for diﬀerent benchmarks. In all our analysis, we use the 22nm [24] technology and an aging period of
7 years.
6. RESULTS
In this section we present the eﬀectiveness of our schemes
across diﬀerent metrics.
6.1 Comparative Schemes and Evaluation Metrics
We compare the following ﬁve schemes:
y
t
i
v
i
t
c
a
g
n
h
c
i
t
i
w
s
f
o
%
100
80
60
40
20
blackscholes
canneal
dedup
ferret
fluidanimate
swaptions
baseline-ave
0
0
10
20
30
40
50
60
70
Percentile(cid:0)#(cid:0)of(cid:0)Gates
80
90
100
Figure 6: CDF for BCCLS scheme.
• BASE: Baseline conﬁguration where the system is unmodiﬁed.
• BC: Bit Cruising scheme, the channel is divided into
four segments and a bit cruiser circuit is placed between the NI and the router.
• DCM: Distributed Cycle Mode technique presented
in Section S4.
• CLS: Crossbar Lane Switching scheme discussed in
Section 4.3.
• BCCLS: Combination of BC and CLS schemes.
We evaluate all these schemes in terms of switching activity distribution through Cumulative Distribution Function
(CDF) plots, clock frequency degradation, Energy-Delay Product Per Flit (EDPPF) and System Performance. The circuits used to facilitate all these schemes (except DCM) are
added in the front-end of the pipeline without aﬀecting the
actual crossbar circuit, and as such do not incur any additional timing overhead in the critical path3 .
6.2 Switching Activity Distribution
We show the CDF plots (Figures 5 and 6) of the switching activity distribution of all schemes. The average of the
baseline scheme is superimposed in each ﬁgure. All of our
schemes outperform the baseline by having a lower value
(y-axis) at any percentile point. Hence, it is evident that
our schemes achieved their aim of distributing the switching activity. At an evaluation point of 20 percentile, our
best performing scheme (BCCLS in Fig. 6) shows 31% less
switching activity compared to the baseline.
6.3 Clock Cycle Degradation
Figure 7(a) shows the cycle degradation for the NoC router
at the end of a 7 year aging period using the ASU 22nm
predictive technology model [24] operating at 1 Ghz. On an
average, the base scheme degrades the clock cycle by 9.4%.
Our schemes improve this degradation by 20.6%, 0%, 12%
and 25.5% for BC, DCM, CLS and BCCLS, respectively.
Combining both BC and CLS schemes results in the least
amount of clock cycle degradation while DCM provides no
improvement from the baseline. As HCI is an unrecoverable
degradation [15], any damage done during normal operation cannot be rectiﬁed. The diﬀerence between DCM and
all other schemes is that it is reactive while the others are
proactive (preventing aging beforehand). However, DCM
improves other aspects of the circuit such as the EDPPF
which will be discussed next.
6.4 Energy Delay Product Per Flit (EDPPF)
3DCM’s cycle degradation is taken without the timing of
the additional multiplexers as we want to show the timing
degradation in the crossbar circuit only across all schemes.
We show in Figure 7(b) the EDPPF of all schemes. The
base scheme is shown as a line at 100%. Most schemes
have lower EDPPF compared to the baseline except for
some outliers. For the BC scheme, dedup and ferret have
larger EDPPFs while for CLS, swaptions has a slightly larger
EDPPF than the baseline. Upon further investigation, although BC has helped achieve less degradation and a more
distributed switching activity, its dynamic switching activity
for benchmarks dedup and ferret are actually 63% and 30%
more compared to the average of all other programs. This
unusual activity increase is due to the workload-dependent
bit patterns being sent across the network. For swaptions,
the switching activity for the benchmark is unusually high
in all schemes except for BC.
Even though DCM does not provide any improvement in
the clock cycle, it provides consistent reduction in EDPPF.
This reduction is because optimally aged transistors have
higher threshold voltages and will have lesser leakage power.
Leakage power cannot be ignored in small technologies such
as the one we are using (22nm). On an average, DCM improves the EDPPF by 18% compared to the baseline.
6.5 System Performance
Figure 7(c) shows the overall system performance impact
of all schemes relative to the baseline. DCM shows no improvement because it has the same clock degradation as
the baseline. On an average, performance degradation is
reduced by 9.3%, 8% and 11% for BC, CLS and BCCLS
schemes. Maximum is 17.6% for the BCCLS scheme running ferret. Overall, the system performance improvement
is less than the clock cycle degradation improvement due
to the sublinear dependence of clock frequency and performance.
7. RELATED WORK
The aggressive scaling in CMOS technology has made reliability a primary design constraint in modern computing
systems. While there has been a wide scope of studies tackling diﬀerent reliability issues (NBTI, TDDB, HCI) in processing elements [11, 21], there is only a limited number of
works which address wear-out mitigation in the on-chip communication infrastructure of such systems. Bhardwa j et al.
implemented a dynamic routing algorithm to equalize NBTI
and electromigration aging across the on-chip network [3].
Fu et al. created new virtual channel allocation and routing
algorithms in order to improve process variation and NBTI
eﬀects in key components of the router [10]. Park et al.,
Fick et al. and Kim et al. explored fault tolerant NoC architectures by decoupling modules and having redundancies
in order to recover from intermittent errors in the network
or provide graceful degradation [9] [14] [20].
Most of the studies mentioned above focus on recovering
from intermittent errors or minimizing NBTI eﬀect on storage elements by balancing the duty cycle. On the contrary,
our work focuses on HCI, an unrecoverable aging phenomena that aﬀects combinational components. HCI mitigation
presents a diﬀerent set of challenges because of its dependence on the switching activity of transistors, as opposed to
NBTI which depends only on the input bias. To the best of
our knowledge, our study is the ﬁrst work to tackle HCI in
an NoC router microarchitecture.
 
 
 
30
40
50
60
70
Percentile(cid:0)(cid:0)#(cid:0)(cid:0)of(cid:0)(cid:0)Gates
0
10
20
80
90
100
%
o
f
s
w
t
i
c
a
g
n
h
c
i
i
t
v
t
i
y
0
20
40
60
80
100
blackscholes
canneal
dedup
ferret
fluidanimate
swaptions
baseline-ave
(a) Bit Cruising
30
40
50
60
70
Percentile(cid:0)(cid:0)#(cid:0)(cid:0)of(cid:0)(cid:0)Gates
0
10
20
80
90
100
%
o
f
s
w
t
i
c
a
g
n
h
c
i
i
t
v
t
i
y
0
20
40
60
80
100
blackscholes
canneal
dedup
ferret
fluidanimate
swaptions
baseline-ave
(b) DCM
30
40
50
60
70
Percentile(cid:0)(cid:0)#(cid:0)(cid:0)of(cid:0)(cid:0)Gates
0
10
20
80
90
100
%
o
f
s
w
t
i
c
a
g
n
h
c
i
i
t
v
t
i
y
0
20
40
60
80
100
blackscholes
canneal
dedup
ferret
fluidanimate
swaptions
baseline-ave
(c) Crossbar Lane Switching
Figure 5: Cumulative Distribution Graph of BC, DCM and CLS schemes. Solid line in each graph is the baseline average. (Lower
is better.)
b l a
c
k
s
c
h
o l e
s
c
a
n
n
e
a l
d
e
d
u
p
f e rr e t
fl u i d
a
n i m
a t e
s
w
a
p ti o
n
s
a
v
e r a
g
e
P
e
r
e
g
a
n
e
c
t
D
g
e
r
n
o
100
a
d
a
i
t
60
70
80
90
BC
DCM
CLS
BCCLS
(a) Cycle Degradation
b l a
c
k
s
c
h
o l e
s
c
a
n
n
e
a l
d
e
d
u
p
f e rr e t
fl u i d
a
n i m
a t e
s
w
a
p ti o
n
s
a
v
e r a
g
e
P
e
r
e
g
a
n
e
c
t
(cid:0)
(cid:0)
D
g
e
r
a
d
a
i
t
n
o
60
80
100
120
140
160
BC
DCM
CLS
BCCLS
(b) EDPPF
b l a
c
k
s
c
h
o l e
s
c
a
n
n
e
a l
d
e
d
u
p
f e rr e t
fl u i d
a
n i m
a t e
s
w
a
p ti o
n
s
a
v
e
P
e
r
e
g
a
n
e
c
t
(cid:0)
(cid:0)
D
g
e
r
a
100
d
a
i
t
n
o
60
70
80
90
BC
DCM
CLS
BCCLS
(c) System Performance Degradation
Figure 7: Router Cycle time Degradation, Energy Delay Product Per Flit and System Performance Degradation comparison. Solid
line indicates baseline. (Lower is better).
8. CONCLUSION
In this paper, we ﬁnd out that Network On Chip architectures running parallel programs produce communication
patterns that lead to unbalanced HCI degradation through
asymmetrical gate switching activity. We exploit this property and present four novel proposals in HCI mitigation in
the crossbar circuit, a ma jor component in an NoC router
which dictates the operating frequency of the network. Overall, our schemes distribute the switching activity, improve
the clock cycle degradation, energy delay product per ﬂit
and system performance.
Acknowledgments
This work was supported in part by National Science Foundation grants CNS-1117425 and CAREER-1253024, and donation from the Micron Foundation.
9. "
Fault and energy-aware communication mapping with guaranteed latency for applications implemented on NoC.,"As feature sizes shrink, transient failures of on-chip network links become a critical problem. At the same time, many applications require guarantees on both message arrival probability and response time. We address the problem of transient link failures by means of temporally and spatially redundant transmission of messages, such that designer-imposed message arrival probabilities are guaranteed. Response time minimisation is achieved by a heuristic that statically assigns multiple copies of each message to network links, intelligently combining temporal and spatial redundancy. Concerns regarding energy consumption are addressed in two ways. Firstly, we reduce the total amount of transmitted messages, and, secondly, we minimise the application response time such that the resulted time slack can be exploited for energy savings through voltage reduction. The advantages of the proposed approach are guaranteed message arrival probability and guaranteed worst case application response time.","Fault and Energy-Aware Communication Mapping with
Guaranteed Latency for Applications Implemented on NoC
∗
Link ¨oping University, Sweden
Sorin Manolache
sorma@ida.liu.se
Link ¨oping University, Sweden
Petru Eles
petel@ida.liu.se
Zebo Peng
Link ¨oping University, Sweden
zebpe@ida.liu.se
ABSTRACT
As feature sizes shrink, transient failures of on-chip network links become a critical problem. At the same time, many applications require
guarantees on both message arrival probability and response time. We
address the problem of transient link failures by means of temporally
and spatially redundant transmission of messages, such that designerimposed message arrival probabilities are guaranteed. Response time
minimisation is achieved by a heuristic that statically assigns multiple
copies of each message to network links, intelligently combining temporal and spatial redundancy. Concerns regarding energy consumption are addressed in two ways. Firstly, we reduce the total amount
of transmitted messages, and, secondly, we minimise the application
response time such that the resulted time slack can be exploited for
energy savings through voltage reduction. The advantages of the proposed approach are guaranteed message arrival probability and guaranteed worst case application response time.
Categories and Subject Descriptors
B.4.4 [Hardware Input/Output and Data Communications]: Performance Analysis and Design Aids; B.4.5 [Hardware Input/Output
and Data Communications]: Reliability, Testing, and Fault-Tolerance
Algorithms, Performance
General Terms
1.
INTRODUCTION
Several authors [2, 5] have proposed network-on-chip (NoC) architectures as replacements to bus-based designs in order to reduce
design, veriﬁcation and test complexity and to ease the power management problem. One of the main problems for such highly integrated complex systems is the increasing rate of failures of the communication lines. In this paper we concentrate our discussion on how
to handle transient failures of on-chip network links in the context of
time and energy constrained applications implemented on NoC. The
reliability of network nodes is guaranteed by speciﬁc methods which
are outside the scope of this paper.
In general, 100% reliable communication cannot be achieved in
the presence of transient failures, except under assumptions such as
no multiple simultaneous faults or at most n bit ﬂips, which are unrealistic in the context of complex NoC. Hence, we are forced to tolerate occasional errors, provided that they occur with a rate below an
imposed threshold. With shrinking feature size, the on-chip interconnects have become a performance bottleneck [4]. Thus, the selection
of message routes has a signiﬁcant impact on the responsiveness of
applications implemented on the NoC. The energy consumption of
wires has been reported to account for about 40% of the total energy
consumed by the chip [9]. This is a strong incentive to consider the
communication energy in addition to guaranteeing required levels of
message transmission reliability.
In this paper, we address all of the three stringent problems identiﬁed above: link reliability, latency, and energy consumption. We
propose a solution for the following problem: Given an NoC architecture with a speciﬁc fault model for its network links and given an
application with required message arrival probabilities and imposed
deadlines, ﬁnd a mapping of messages to network links such that the
∗
The authors thank Prof. Radu Marculescu for inviting Sorin
Manolache to visit his research group at CMU. The ideas presented
in this paper originated during that visit.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2005, June 13–17, 2005, Anaheim, California, USA.
Copyright 2005 ACM 1-59593-058-2/05/0006 ...$5.00.
imposed message arrival probability and deadline constraints are satisﬁed at reduced energy costs.
In order to cope with the unreliability of on-chip network links, we
propose a way to combine spatially and temporally redundant message transmission. Our approach to communication energy reduction
is to minimise the application latency at almost no energy overhead
by intelligently mapping the redundant message copies to network
links. The resulting time slack is exploited for energy minimisation
by means of voltage reduction on network nodes and links.
At on-chip bus level, Bertozzi et al. [3] address the problem of
energy-efﬁcient reliable on-chip communication. They analyse the
trade-off between consumed energy, transmission latency and error
codes, while considering the energy and the chip area of the encoders/
decoders. While Bertozzi et al. address the problem at link level,
in this paper we address the problem at application level, considering time-constrained multi-hop transmission of messages sharing the
links of an NoC.
At system level, Dumitras and Marculescu [6] have proposed stochastic communication as a way to deal with permanent and transient
faults of network links and nodes. Their method has the advantage of
simplicity, low implementation overhead, and high robustness w.r.t.
faults. The selection of links and of the number of redundant copies
to be sent on the links is stochastically done at runtime by the network routers. Therefore, the transmission latency is unpredictable
and, hence, it cannot be guaranteed. More importantly, stochastic
communication is very wasteful in terms of energy [10].
Pirretti et al. [12] report signiﬁcant energy savings relative to Dumitras’ and Marculescu’s approach, while still keeping the low implementation overhead of non-deterministic routing. An incoming
packet is forwarded to exactly one outgoing link. This link is randomly chosen according to pre-assigned probabilities that depend on
the message source and destination. However, due to the stochastic
character of transmission paths and link congestion, neither Dumitras and Marculescu, nor Pirretti et al. can provide guarantees on the
transmission latency.
Our approach differs in the sense that we deterministically select
at design time the links to be used by each message and the number of
copies to be sent on each link. Thus, we are able to guarantee not only
message arrival probabilities, but also worst-case message transmission times. Additionally, by carefully balancing temporal and spatial
communication redundancy, we are able to minimise the application
latency and, by this, also the consumed energy.
The rest of the paper is structured as follows. The next section
presents the models of architecture, communication and application
and gives the problem formulation. Section 3 outlines our solution to
the formulated problem, while Sections 4, 5 and 6 address different
aspects of our approach. Section 7 presents experimental results and
the last section draws the conclusions.
2. SYSTEM MODEL AND PROBLEM FORMULATION
2.1 Hardware model
columns, numbered from 0 to W − 1 and to H − 1 respectively. The
The NoC is modelled as a 2D array of cores arranged in rows and
core on the row x and column y is identiﬁed as Px,y , where 0 ≤ x < W
and 0 ≤ y < H . Core Px,y is connected to cores Px,y+1 (north), Px+1,y
(east), Px,y−1 (south), and Px−1,y (west) if these cores exist. The link
connecting core Px,y to core Px,y+1 is denoted by Lx,y,N or by Lx,y+1,S ,
where the ﬁrst two indexes denote one end of the link, while the third
index shows the direction of the link. Each link is characterised by
the time and energy it needs to transmit a bit of information.
2.2 Application model
is a directed acyclic graph (V, E ⊂ V × V ), where each vertex Vi corThe application is modelled as a set of task graphs. A task graph
responds to a task τi and each edge e = (Vi ,V j ) ∈ E models a data dependency between tasks τi and τ j . Each task is mapped on one core
and different tasks may be mapped on the same core. Let m(τi )denote
the core task τi is mapped onto.
Each task τi is characterised by its period πi and its worst-case
execution time ci when executed on core m(τi ). Tasks belonging to
the same task graph have the same period. Task τi is said to arrive
every πi time units. Its response time (a.k.a. latency) is given by the
time interval between its arrival and its completion.
Each task graph is characterised by a deadline. The task graph
deadline is met if all tasks belonging to the task graph have completed
their execution by the time of the graph deadline. The task graph
response time is given by the maximum response time of its tasks,
while the application response time (or latency) is given by the largest
response time of the task graphs. In addition to task graph deadlines,
every task may have its own deadline δi .
Each edge e = (Vi ,V j ) ∈ E is characterised by bi, j , the largest
amount of bits that is transmitted by task τi to task τ j each time they
are instantiated. The transmission is assumed to be ready to commence as soon as task τi has ﬁnished its execution. Task τ j cannot
start its execution before receiving the data items sent by all its predecessor tasks τk .
The execution of tasks mapped on the same core is scheduled
based on pre-assigned priorities. This execution is preemptive.
2.3 Communication model
The time needed for the communication between two tasks mapped
on the same core is assumed to be part of the worst-case execution
time of the sender. Inter-core communication is packet-based, i.e. the
data sent by a task is chopped at the source core in packets and then
sent on the links along a predetermined route. At the destination core,
the packets are assembled and the message is delivered to the receiving task. Each message is characterised by its length (bits) bi, j , by the
size of the packets it is chopped into, and by a priority for solving link
contention. The packet transmission on a link is non-preemptive.
2.3.1 Fault model
Links may fail temporarily due to transient faults. If a data packet
is sent on the link during the time the link is in the failed state, the
data is scrambled. We assume that the cores have the ability to detect
incoming scrambled packets. Scrambled packets are dropped and are
not forwarded further. Several copies of the same packet may be sent
on the network links. In order for a message to be successfully received, at least one copy of every packet of the message has to reach
the destination core unscrambled. Otherwise, the message is said to
be lost. The link failure events are assumed mutually independent.
The probability that a packet of b bits is conveyed successfully by a
link is denoted αb .
Let us consider an arbitrary pair of communicating tasks, τi → τ j .
The two tasks have the same period π, as they belong to the same task
graph. Let Si, j (t ) denote the number of messages that were sent by
task τi and were received unscrambled by task τ j in the time interval
[0, t ). The expected fraction of successfully transmitted messages for
the task pair τi → τ j , called message arrival probability and denoted
MAPi, j , is given by limt→∞ Si, j (t )
(cid:6)t /π(cid:7) .
2.3.2 Message communication support
In order to introduce the notion of communication support we will
use the example in Figure 4(a). The squares in the ﬁgure represent
cores and the thick undirected lines connecting them represent the
network links. The circles inside each square denote tasks that are
mapped on the core represented by the square. The solid arrows connecting the circles represent the data dependence among tasks. The
dashed arrows show how the data communication between the tasks
is mapped on the network links. Thus, the messages between task
τ1 and τ2 are conveyed on the link L0,0,E . The message τ1 → τ3
is sent in multiple copies. Thus, one copy circulates on the route
P0,0 → P1,0 → P1,1 traversing the links L0,0,E and L1,0,N , while two
P0,0 → P0,1 → P1,1 traversing the links L0,0,N and L0,1,E .
more copies (shown as the duplicate arrow) circulate on the route
τi → τ j can be formalised as a set of tuples Ci, j = {(L, n) : L is a link, n ∈
In general, the mapping of the communication between two tasks
N}, where n indicates how many copies of the same packet are conveyed by the corresponding link L. We will call the set Ci, j the communication support (CS) of τi → τ j . In our example, the two com(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
for each pair of communicating tasks τi → τ j
ﬁnd a set of candidate CSs s.t. MAPi, j ≥ Bi, j (Sec. 4)
end for
(sol , min rt ) =explore the space of candidate CSs (Sec. 6)
using response time calculation (Sec. 5)
for driving the exploration
if min rt > deadline then
return “no solution”
else
(cid:9) =voltage freq reduction(sol ) (according to [1])
sol
return sol
end if
Figure 1: Approach outline
munication supports are C1,2 = {(L0,0,E , 1)} and C1,3 = {(L0,0,E , 1),
(cid:9)
(L1,0,N , 1), (L0,0,N , 2), (L0,1,E , 2)}.
Let us assume that a message of bi, j bits is supported by a particular CS. The message arrival probability (MAP) and the expected
communication energy (ECE) for the message can be computed as
functions of αb , the probability that a packet is successfully conveyed
across a link, and of the energy-per-bit values Ebit of the links of the
CS, and the number bi, j of transmitted bits. As opposed to the message transmission time that can be affected by the interference from
other transmitted messages, the MAP and ECE can be precisely computed for each message in isolation given the CS supporting it. The
MAP is computed from probabilities to reach each core on the way.
The latter are obtained using simple probability theory as shown by
us elsewhere [10]. The ECE is a sum of expected energies consumed
by each link of the CS. This energy is proportional to the probability
to reach the start point of the link multiplied to the number of times
the message is conveyed on the link.
2.4 Problem formulation
The input to the problem consists of
• The hardware model, i.e.
the size of the NoC, and, for each
link, the energy-per-bit, the bandwidth, and the probability of a
packet to be successfully conveyed by the link;
• The application model, i.e. the task graphs, their deadlines, the
mapping of tasks to cores, the task periods, deadlines, worstcase execution times, priorities and the amounts of data to be
• The communication model, i.e.
transmitted between communicating tasks;
the packet size and message
priority for each message (alternatively, our approach can automatically assign message priorities according to the message
• The lower bounds Bi, j imposed on the message arrival probabilcriticality);
ity MAPi, j , which is the expected fraction of successfully transmitted messages, for each pair of communicating tasks τi → τ j .
The problem is formulated as follows: Given the input described
nicating tasks τi → τ j such that:
above, ﬁnd the communication support Ci, j for each pair of commu• all message arrival probabilities MAPi, j satisfy MAPi, j ≥ Bi, j ,
• the communication energy is minimised, and
• all deadlines are met
3. APPROACH OUTLINE
The outline of our approach to solve the problem is shown in Figure 1. First, for each pair of communicating tasks (message), we ﬁnd a
set of candidate communication supports (line 2, see Section 4), such
that the lower bound constraint on the message arrival probability is
satisﬁed. Second, the space of candidate communication supports is
explored in order to ﬁnd sol , the selection of communication supports
that result in the shortest application response time min rt (line 4, see
Section 6). The worst-case response time of each explored solution
is determined by the response time calculation function that drives
the design space exploration (line 5, see Section 5). If no solutions
are found that satisfy the response time constraint, the application is
deemed impossible to implement with the given resources (line 7).
Otherwise, the solution with the minimum application response time
among the found solutions is selected. Voltage reduction is performed
on the selected solution in order to decrease the overall system energy
consumption (line 9), and the modiﬁed solution is returned (line 10).
The next section discusses the construction of the set of candidate
communication supports for an arbitrary pair of communicating tasks.
Section 5 describes how the response time calculation is performed,
while Section 6 outlines how the preferred communication supports
representing the ﬁnal solution are selected.
)
t
n
e
s
E
C
E
f
o
o
t
r
l
m
e
a
b
n
o
u
n
o
d
p
e
o
i
t
r
t
r
P
(
c
e
p
x
E
 700
 680
 660
 640
 620
 600
 580
 560
 540
 520
 0.96
 0.965
 0.97
SRD=2, GRD=10
SRD=1, GRD=10
SRD=2, GRD=11
SRD=2, GRD=12
SRD=2, GRD=13
for each pair of communicating tasks τi → τ j
Determine N1 and N2 , the minimum GRDs of CSs of
SRD 1 and 2 respectively, such that the MAP constraint
on τi → τ j is satisﬁed
Add all CSs with SRD 1 and with GRD N1 and
set of CS candidates of τi → τ j
all CSs with SRD 2 and with GRD N2 to the
end for
Figure 2: Construction of candidate CS set
(1)
(2)
(3)
(4)
s
t
i
b
 0.975
 0.98
 0.985
Message arrival probability
 0.99
 0.995
 1
Figure 3: Energy-efﬁciency of CSs of SRD 1 and 2
4. COMMUNICATION SUPPORT CANDIDATES
This section describes how to construct a set of candidate communication supports for a pair of communicating tasks. First we introduce the notions of path, coverage, and spatial, temporal, and general
redundancy degree of a CS.
A path of length n connecting a source core with a destination core
is an ordered sequence of n links, such that the end point of the it h link
coincides with the start point of the i + 1t h link, and the start point of
the ﬁrst link is the source core and the end point of the last link is the
destination core. We consider only loop-free paths. A path belongs to
a CS if all its links belong to the CS. A link of a CS is covered by a
path if it belongs to the path.
The spatial redundancy degree (SRD) of a CS is given by the minimum number of distinct paths belonging to the CS that cover all the
links of the CS. For the example in Figure 4(a), the SRD of C1,2 is 1,
as C1,2 contains only one path, (L0,0,E ). The SRD of C1,3 is 2, as the
four links of C1,3 can be covered only by the paths (L0,0,E , L1,0,N ) and
(L0,0,N , L0,1,E ).
The temporal redundancy degree (TRD) of a link is given by the
number of redundant copies to be sent on the link. The TRD of a
CS is given by the maximum TRD of its links. For the example in
Figure 4(a), the TRD of C1,2 is 1 and the TRD of C1,3 is 2 (because
two redundant copies are sent on links L0,0,N and L0,1,E ).
The general redundancy degree (GRD) of a CS is given by the
sum of temporal redundancy degrees of all its links. For the example
in Figure 4(a), the GRDs of C1,2 and C1,3 are 1 and 6.
It is important to use CSs of minimal GRD because the ECE of
a message is strongly dependent on the GRD of the CS supporting
it. To illustrate, we constructed all CSs of SRD 2 and GRD 10–13
a 4 × 4 NoC. We also constructed all CSs of SRD 1 and GRD 10.
for a message sent from the lower-left core to the upper-right core of
For each of the constructed CS, we computed their MAP and ECE. In
Figure 3, we plotted all resulting (MAP, ECE ) pairs. We observe that
the ECE of CSs of the same GRD do not differ signiﬁcantly among
them, while the ECE difference may account to more than 10% for
CSs of different GRD.
The algorithm for the candidate set construction proceeds as shown
in Figure 2. Candidate CSs with SRD of only 1 and 2 are used. The
justiﬁcation for this choice is given by us elsewhere [10] due to space
limitations. Also, a more detailed explanation of how to determine N1
and N2 , the minimum GRDs of CSs of SRD 1 and 2 respectively, can
be found there. For self-containment, we mention brieﬂy that N1 and
N2 are obtained by progressively increasing the GRD until the CS satisﬁes the MAP constraint. The redundant copies must be uniformly
distributed over the links of the CS. Additionally, in the case of CSs
with SRD 2, when increasing the GRD, links should be added to the
CS such that many path intersection points are obtained and that they
are close to each other.
5. RESPONSE TIME CALCULATION
In order to guarantee that tasks meet their deadlines, when no message is lost, response times have to be determined in the worst case.
τ
3
P1,0
P1,1
P0,0
τ
1
P0,1
τ
2
τ
1
τ
τ
2
3
τ
4 τ5
τ
6 τ8 τ9
τ
7
τ
10
τ11
τ 2
τ3
τ
τ
1
1
(a)
(b)
Figure 4: Application modelling for response time analysis
in Section 2.3.2. The two CSs are C1,2 = {(L0,0,E , 1)} and C1,3 =
Let us consider the example depicted in Figure 4(a) and described
{(L0,0,E , 1), (L1,0,N , 1), (L0,0,N , 2), (L0,1,E , 2)}. Packet sizes are such
that message τ1 → τ2 is chopped into 2 packets, while message τ1 →
τ3 ﬁts into a single packet.
Based on the application graph, its mapping and the communication supports, we construct a task graph as shown in Figure 4(b). Each
link L is regarded as a processor PL , and each packet transmission on
link L is regarded as a non-preemptive task executed on processor PL .
The shadings of the circles denote the processors (links) on which the
tasks (packets) are mapped. Tasks τ4 and τ5 represent the ﬁrst and
the second packet of the message τ1 → τ2 . They are both dependent
on task τ1 as the two packets are generated when task τ1 completes
its execution, while task τ2 is dependent on both task τ4 and τ5 as it
can start only after it has received the entire message, i.e. both packets, from task τ1 . Both tasks τ4 and τ5 are mapped on the “processor”
corresponding to the link L0,0,E . Task τ6 represents the packet of the
message τ1 → τ3 that is sent on link L0,0,E and task τ7 represents
the same packet once it reaches link L1,0,N . Tasks τ8 and τ9 are the
two copies of the packet of the message τ1 → τ3 that are sent on link
We are interested in the worst-case scenario w.r.t. response times.
In the worst case, all copies of packets get scrambled except the latest
packet. Therefore, the copies to be sent by a core on its outgoing
links have to wait until the last of the copies arriving on incoming
links of the core has reached the core. For example, tasks τ10 and τ11 ,
modelling the two copies of the message τ1 → τ3 that are sent on the
link L0,1,E , depend on both τ8 and τ9 , the two copies on link L0,0,N .
Also, task τ3 depends on all three copies, τ7 , arriving on link L1,0,N ,
and τ10 and τ11 , arriving on link L0,1,E .
The modiﬁed model, as shown in Figure 4(b), is analysed using
the dynamic offset based schedulability analysis proposed by Palencia
and Harbour [11]. The analysis calculates the worst-case response
times and jitters for all tasks.
L0,0,N .
6. SELECTION OF COMMUNICATION
SUPPORTS
As shown in Section 4 (see also line 2 in Figure 1), we have determined the most promising (low energy, low number of messages) set
of CSs for each transmitted message in the application. All those CSs
guarantee the requested MAP. As the last step of our approach (line
4 in Figure 1) we have to select one particular CS for each message,
such that the application response time is minimised. The response
time for each candidate solution is calculated as outlined in Section 5
(line 5 in Figure 1).
The solution space is explored with a Tabu Search based heuristic [7]. Given a certain solution alternative, a new one is generated
by performing a “move”. A move means picking a pair of communicating tasks and selecting a new communication support for it. For
each candidate solution, the application response time has to be calculated. An approach exploring all candidate moves would be too time
consuming for our problem. Therefore, we only explore “promising”
moves. Thus,
1. we look at messages with large jitters as they have a higher
chance to improve their transmission latency by having assigned
2. for a certain message τi → τ j , we consider only those candidate
a new CS;
CSs that would decrease the amount of interference of messages of higher priority than τi → τ j . (By this we remove message from overloaded links.)
7. EXPERIMENTAL RESULTS
We report on three sets of experiments that we ran in order to assess the quality of our approach. The ﬁrst set investigates the application latency as a function of the number of tasks. 340 applications
 
 
 
 
 
 
 800
 1000
 1200
 1400
 1600
 1800
 2000
 2200
 2400
 30
 40
 50
 60
Number of tasks
 70
 80
R
e
s
n
o
p
s
e
i
t
m
e
Only temporal redund.
Temporal and spatial redund.
(a) Application latency vs.
number of tasks
 800
 0.94
 0.96
 0.98
Imposed message arrival probability
 1000
 1200
 1400
 1600
 1800
 2000
 1
R
e
s
o
p
s
e
i
t
m
e
[
s
b
a
r
t
c
a
t
i
t
m
e
n
u
t
i
s
]
Only temporal redundancy
Temporal and spatial redundancy
(b) Application latency vs.
imposed MAP
 10
 12
 14
 16
 18
 20
 22
 24
 1
 1.5
 2
 2.5
 3
 3.5
Amount of communication per
time unit [bits / abstract time unit]
 4
R
a
e
l
i
t
e
v
l
y
c
n
e
a
t
r
c
u
d
e
i
t
n
o
[
%
]
4x4 NoC
5x5 NoC
6x6 NoC
(c) Application latency
vs NoC size and communication load
Figure 5: Experimental results
of 16 to 80 tasks were randomly generated. The applications are executed by a 4 × 4 NoC. The probability that a link successfully conveys
ing CSs with SRD of 1 and 2. The slack that resulted in the second
case was exploited for energy reduction. We have used the algorithm
a data packet is 0.97, and the imposed lower bound on the message
by Andrei et al. [1] for calculating the voltage levels for which to run
arrival probability is 0.99. For each application, we ran our commuthe application. For our energy models, we considered a 70nm CMOS
nication mapping tool twice. In the ﬁrst run, we consider CSs of SRD
fabrication process. The resulted energy consumption is depicted in
1, i.e. packets are retransmitted on the same, unique path. In the secFigure 5(d). The energy reduction ranges from 20% to 13%.
ond run, we consider CSs of SRD 1 and 2, as described in Section 4.
Finally, we applied our approach to a multimedia application [8],
Figure 5(a) depicts the averaged results. The approach that uses both
namely an image encoder implementing the H263 algorithm. The apspatially and temporally redundant CSs leads to shorter application
6 DSPs, 2 CPUs, 4 ASICs, and 2 memory cores (organised as a 4 × 4
plication is composed of 24 tasks running on a platform consisting of
latencies than the approach that just re-sends on the same path.
The second experiment investigates the dependency of latency on
NoC with one unused core). The approach combining spatially and
the imposed message arrival probability. 20 applications, each of 40
temporally redundant message transmission obtained a 25% response
tasks, were randomly generated. We considered the same hardware
time reduction relative to the approach deploying only temporal replatform as in the ﬁrst experiment. For each application, we considdundancy. The energy savings after voltage reduction amounted to
ered 17 different lower bounds on MAP, ranging from 0.94 to 0.9966.
20%.
The averaged results are shown in Figure 5(b). For low bounds on
MAP, such as 0.94, almost no transmission redundancy is required to
satisfy the MAP constraint. Therefore, the approach combining spatially and temporally redundant communication fares only marginally
better than the approach that uses only temporal redundancy. However, for higher bounds on the MAP, the approach that combines spatially and temporally redundant transmission (as shown in Section 4)
has the edge. In the case of bounds on the MAP larger than 0.9992,
spatial redundancy cannot satisfy the constraint anymore, and therefore the temporally redundant transmission becomes dominant and
the approach combining spatial and temporal redundancy does not
lead to signiﬁcant latency reductions anymore.
The third experiment has a double purpose. First, it investigates
the dependency of latency reduction on the size of the NoC. Second, it
investigates latency reduction as a function of the communication load
(bits/time unit). 20 applications of 40, 62 and 90 tasks were randomly
generated. The applications with 40 tasks run on a 4 × 4 NoC, those
with 62 tasks run on a 5 × 5 NoC and those with 90 tasks run on a
6 × 6 NoC. For each application, we considered communication loads
of 1–4 bits/time unit. The averaged latency reductions when using the
optimal combination of spatial and temporal redundancy, compared to
purely temporal redundancy, are depicted in Figure 5(c). We observe
that for low communication loads, the latency reduction is similar for
all three architectures, around 22%. However, at loads higher than 3.4
the relatively small number of links of the 4 × 4 NoC get congested
and response times grow unboundedly. This, however, is not the case
NoC of 6 × 6 and 12% for 5 × 5.
with the larger NoCs. Latency reduction for a load of 4 is 22% for a
The presented experiments have shown that, using an optimal combination of temporal and spatial redundancy for message mapping,
signiﬁcant reduction of latency can be obtained while guaranteeing
message arrival probability at the same time. It is important to notice that the latency reduction is obtained without energy penalty, as
shown in Section 4. This means that for a class of applications using the proposed approach it will be possible to meet the imposed
deadlines, which otherwise would not be possible without changing
the underlying NoC architecture. However, the proposed approach
gives also the opportunity to further reduce the energy consumed by
the application. If the obtained application response time is smaller
than the imposed one, the resulting slack can be exploited by running
the application at reduced voltage. In order to illustrate this, we have
performed another set of experiments.
Applications of 16 to 60 tasks running on a 4 × 4 NoC were randomly generated. For each application we ran our message mapping
approach twice, once using CSs with SRD of only 1, and second us 0.01
 15 20 25 30 35 40 45 50 55 60
Number of tasks
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
E
e
n
r
y
g
u
s
n
o
c
m
p
i
t
n
o
[
J
]
Only temporal redund.
Temporal and spatial redund.
(d) Energy consumption vs.
number of tasks
8. CONCLUSIONS
This paper has presented an approach to reliable, low-energy onchip communication for time-constrained applications implemented
on NoC. The contribution is manifold. First, we show how to generate supports for message communication in order to meet the message
arrival probability constraint and to minimise communication energy.
Second, we give a heuristic for selecting most promising communication supports with respect to application responsiveness and energy.
Third, we model the fault-tolerant application for response time analysis. Finally, we present experiments demonstrating the proposed approach.
9. "
A low latency router supporting adaptivity for on-chip interconnects.,"The increased deployment of system-on-chip designs has drawn attention to the limitations of on-chip interconnects. As a potential solution to these limitations, networks-on-chip (NoC) have been proposed. The NoC routing algorithm significantly influences the performance and energy consumption of the chip. We propose a router architecture which utilizes adaptive routing while maintaining low latency. The two-stage pipelined architecture uses look ahead routing, speculative allocation, and optimal output path selection concurrently. The routing algorithm benefits from congestion-aware flow control, making better routing decisions. We simulate and evaluate the proposed architecture in terms of network latency and energy consumption. Our results indicate that the architecture is effective in balancing the performance and energy of NoC designs.","A Low Latency Router Supporting Adaptivity for On-Chip
Interconnects ∗
Jongman Kim, Dongkook Park, T. Theocharides, N. Vijaykrishnan and Chita R. Das
Depar tment of Computer Science and Engineering
The Pennsylvania State University
University Park, PA 16802
{jmkim, dpark, theochar, vijay, das}@cse.psu.edu
ABSTRACT
The increased deployment of System-on-Chip designs has drawn
attention to the limitations of on-chip interconnects. As a potential
solution to these limitations, Networks-on -Chip (NoC) have been
proposed. The NoC routing algorithm signiﬁcantly inﬂuences the
performance and energy consumption of the chip. We propose a
router architecture which utilizes adaptive routing while maintaining low latency. The two-stage pipelined architecture uses look
ahead routing, speculative allocation, and optimal output path selection concurrently. The routing algorithm beneﬁts from congestionaware ﬂow control, making better routing decisions. We simulate
and evaluate the proposed architecture in terms of network latency
and energy consumption. Our results indicate that the architecture
is effective in balancing the performance and energy of NoC designs.
Categories and Subject Descriptors: B.4[I/O and Data Communications] Interconnections(Subsystems):B.8[Perfromance and Reliability] Performance Analysis and Design Aids
General Terms: Design, Performance.
Keywords: Networks-On-Chip, Adaptive Routing, Interconnection Networks.
1.
INTRODUCTION
With the growing complexity of System-on-Chip (SoC) architectures, the on-chip interconnects are becoming a critical bottleneck in meeting performance and power consumption budgets of
the chip design. The ICCAD 2004 Keynote Speaker [17] emphasized the need for an interconnect centric design by illustrating that
in a 65nm chip design, up to 77% of the delay is due to interconnects. Packet-based on chip communication networks [10, 5, 4]
(a.k.a network-on-chip (NoC) designs) have been proposed to address the challenges of increasing interconnect complexity.
∗
This research was supported in part by NSF grants CCR-0093085,
CCR-0098149, CCR-0208734, CCF-0429631, EIA-0202007, and
MARCO/DARPA GSRC:PAS.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2005, June 13–17, 2005, Anaheim, California, USA
Copyright 2005 ACM 1-59593-058-2/05/0006 ...$5.00.
The design of NoC imposes several interesting challenges as
compared to traditional off-chip networks. The resource limitations - area and power limitations - are major constraints inﬂuencing NoC designs. Early NoC designs used dimension order routing, due to its simplicity and deadlock avoidance. However, traditional networks enjoy complicated routing algorithms and protocols, which provide adaptivity to various trafﬁc topologies, handling congestion as it evolves in the network. The challenge in
using adaptive routing in NoC designs, is to limit the overhead in
implementing such a design.
In this work, we present a low-latency two-stage router architecture suitable for NoC designs. The router architecture uses a
speculative strategy based on lookahead information obtained from
neighboring routers, in providing routing adaptation. A key aspect
of the proposed design is its low latency feature that makes the
lookahead information more representative than possible in many
existing router architectures with higher latencies. Further, the
router employs a pre-selection mechanism for the output channels
that helps to reduce the complexity of the crossbar switch design.
We evaluated the proposed router architecture by using it in 2D
mesh and torus NoC topologies, and performing cycle-accurate
simulation of the entire NoC design using various workloads. The
experimental results reveal that the proposed architecture results in
lower latency than when using a deeper pipeline router. This results
from the more up to date congestion information used by the proposed low-latency router. We also demonstrate that the adaptivity
provides better performance in comparison to deterministic routing for various workloads. We also evaluate our design from an
energy standpoint, as we designed and laid out the router components, and obtained both dynamic and leakage energy consumption
of the router. Our results indicate that for non-uniform trafﬁc, our
adaptive routing algorithm consumes less energy than dimension
order routing, due to the decrease in the overall network latency.
This paper is organized as follows. First, we give a short background of existing work in Section 2. We present the proposed
router architecture and the algorithm in Section 3, and we evaluate our architecture in Section 4. Finally we conclude our paper in
Section 5.
2. RELATED WORK
The quest for high performance and energy efﬁcient NoC architectures has been the focus of many researchers. Fine-tuning
a system into maximizing system performance and minimizing energy consumption includes multiple trade-offs that have to be explored. As with all digital systems, energy consumption and system
performance tend to be contradictory forces in the design space
of on-chip networks. Router architectures have dominated early
NoC research, and the ﬁrst NoC designs [5, 9] proposed the use
of simplistic routers, with deterministic routing algorithms. Gradually researchers have explored multiple router implementations,
and ongoing research such as [12, 2, 7, 3] explores implementations
where pipelined router architectures utilize virtual channels and arbitration schemes to achieve Quality of Service (QoS) and highbandwidth on-chip communication. Mullins, et. al. [14] propose a
single stage router with a doubly speculative pipeline to minimize
deterministic routing latency. Among the disadvantages however of
such approach, is an increased contention probability for the crossbar switch, given the single-stage switching. The results given in
[14] do not seem to take contention into consideration. Additionally, emphasis on the intra-router delay does not imply adaptivity
to the network congestion. As such, a better approach should combine both low intra-routing latency, and adaptivity to the network
trafﬁc.
Under non-uniform trafﬁc, or application-speciﬁc (i.e. real time
multimedia) trafﬁc, deterministic routing might not be able to react to congestion due to network bursts, and consequently results
in an increase in network delay. Adaptive routing algorithms employed in traditional networks as a solution to congestion avoidance
are more suitable for NoC implementations. As a result, adaptive
routing algorithms have recently surfaced for NoC platforms. Such
examples include thermal aware routing [18], where hotspots are
avoided by using a minimal-path adaptive routing function, and a
mixed-mode router architecture [11] which combines both adaptive and deterministic modules, and employs a congestion control
mechanism. Both these adaptive schemes do not use up-to-date
congestion information to make the routing decision. A preferred
method is to use real time congestion information about the destination node, and concurrently utilize adaptive routing to handle
ﬂuctuation. A disadvantage, however, that adaptive routing algorithms might suffer is the increased hardware complexity, and possibly higher power consumption. Energy consumption is a primary
design constraint, and power driven router design and power models for NoC platforms, have been investigated in [19, 8]. Energy
consumption depends on the number of hops a packet travels prior
to reaching its destination, We believe that by reducing the overall
network latency and increasing the throughput, the minor energy
penalty paid when migrating from deterministic to adaptive routing
is nulliﬁed by the performance of adaptive routing in non-uniform
trafﬁc. The motivation for our work, therefore, focuses on supporting adaptivity, while maintaining low latency and low energy
consumption.
3. PROPOSED ROUTER MODEL
The NoC latency impacts the performance of many on-chip applications. Minimization of message latency by optimizing the
intra-node delay and utilizing organized wiring layout with regular
topologies has been targeted in NoC designs. The proposed router,
designed with this objective, consists of a two-stage pipelined model
with look ahead routing and speculative path selection [6, 16]. In
this section, we present a customized router architecture that can
support deterministic, and adaptive routing in 2-D mesh and torus
on-chip networks.
3.1 Proposed Router Architecture
A typical state-of-the-art, wormhole-switched, virtual channel
(VC) ﬂow control router consists of four major modules:
routing control (RC), VC allocation (VA), switch allocation (SA), and
switch transfer (ST). In addition, it may have an extra stage at
each input port for buffering and synchronization of arriving ﬂits.
Pipelined router architectures typically arrange each module as a
pipeline stage. It is possible to reduce the critical path latency by
reducing the pipelined stages through look-ahead routing [14]. Figure 1(a) illustrates the logical modules of our two-stage pipelined
router incorporating look ahead routing and speculative allocation.
The ﬁrst stage of the router performs look ahead routing decision
for the next hop, pre-selection of an optimal channel for the incoming packet (header), VA and SA in parallel. The actual ﬂit
transfer is essentially split in two ”stages”, the preliminary semiswitch traversal through the VC selection (ST1) and the decomposed crossbar traversal (ST2). In contrast to the prior router architectures, the proposed model incorporates a pre-selection (PS)
unit as shown in Figure 1. The PS unit uses the current switch
state and network status information to decide a physical channel
(PC) from among the possible paths, computed during the previous stage look-ahead decision. Thus, when a header ﬂit arrives at a
router (stage i), the RC unit decides a possible set of output PCs for
the next hop (i+1). The possible paths depend on the destination
tag and the routing algorithm employed. Instead of using a routing
table for path selection, we use a hardware control that computes a
3-bit direction vector, called Virtual Channel ID (VCID), (based on
the four destination quadrants (NE, NW, SE, and SW) and four directions (N, E, S and W)), which can be sent along with the header,
for deciding an optimal path using the PS unit in the next hop. The
VA and SA are then performed concurrently for the selected output,
before the the transfer of ﬂits across the crossbar.
The detailed design of the router is shown in Figure 2. We describe its functionality with respect to a 2-D mesh or torus. The
router has ﬁve inputs, marked for convenience inputs from the four
directions and one from the local PE. It has four sets of VCs, called
path sets; one set for possible traversal in each of the four quadrants; NE, SE, NW, SW. Each path set has three groups of VCs to
hold ﬂits from possible directions from the previous router.
Header
Information
Arbiter (VA/SA)
RC
PS
VA
SA
  ST1
ST2
RC (Routing Computation), PS (Pre-selection),
VA (Virtual Channel Allocation), SA (Switch
Allocation), ST1 (Switch Traversal to Path
Candidate), ST2 (Crossbar Switch Traversal)
N*
E
S
W
NE* SE
SW NW
*NE (North East Quadrant), N (Output port for North)
(a) Router Pipeline
(b) Direction Vector Table in PS
Figure 1: The Two-Stage Pipelined Router
This grouping is customized for a 2-D interconnect. For example, a ﬂit will traverse in the NE quadrant only if is coming from
the west, south or the PE itself. Similarly, it will traverse the NW
quadrant if the entry point is from south, east or the local PE. Thus,
based on this grouping, we have 3VCs per PC. Note that it is possible to provide more VCs per group if there is adequate on-chip
buffering capacity, and the VA selects one of the VCs in a group.
The MUX in each path set selects one of the VCs for crossbar
arbitration. In the mean time, the PS generates the pre-selection
enable signals to the arbiter based on the credit update, and congestion status information of the neighboring routers. The arbiter,
in turn, handles the crossbar allocation.
tailored routing, we use a 4 × 4 decomposed crossbar with half the
Another novelty of this router is that since we are using topology
connections of a full crossbar as shown in Figure 2. Usually a 5 × 5
crossbar is used for 2-D networks with one of the ports assigned
VCID
(Direction Vector)
Smart
Demux
For North-East (NE)
Path Set
W_NE
From West (W)
From North (N)
For South-East (SE)
From PE
Flit_in
Credit_out
S_NE
PE_NE
N_SE
W_SE
PE_SE
S_NW
E_NW
PE_NW
*Decomposed
Crossbar
(4x4)
Mux
Scheduling
Forward NE
Forward SE
Forward SW
Forward NW
Eject
Arbiter
(VA/SA)
Pre-selection
Enables
Pre-selection
Function
(Congestion-Look-Ahead)
Output VC Resv_State
Credit, Status of Pending Msg Queues
*Decomposed Crossbar
North
East
South
West
Figure 2: Proposed Router Architecture
to the local PE. In our architecture, a ﬂit destined for the local PE,
does not traverse the crossbar. Utilizing the look-ahead routing information, it is ejected after the DEMUX. Thus, the ﬂits save two
cycles at the destination node by avoiding the switch allocation and
switch traversal. This provides a signiﬁcant advantage for nearestneighbor trafﬁc, and can take advantage of NoC mapping which
places frequently communicating PEs close to each other [13].
The decomposed crossbar offers two advantages for on-chip design. First, it needs less silicon and second, it should consume
less energy compared to a full crossbar.
In addition, because of
less number of connections, the output contention probability is reduced. This, in turn, should help in reducing the mis-speculation of
the VA and SA stages.
3.2 Pre-selection Function
The pre-selection function, as outlined earlier, is responsible for
selecting the channel for a packet. It maintains a direction vector
table for the path selection and works one cycle ahead of the ﬂit
arrival. The direction vector table, as shown in Figure 1 (b), selects the optimal path for each of the four quadrant path sets. As an
example, if a ﬂit is in the W NE VC, the PS decides whether it will
use the N or E direction and inputs this enable signal to the arbiter.
The PS logic uses the congestion look-ahead information and crossbar status to determine the best path, and also immediately updates
the credit priorities into the arbitration. The congestion look-ahead
scheme provides adaptive routing decisions with the best VC (or
set of VCs) and path selection from the corresponding candidates
based on the congestion information of neighboring nodes.
3.3 Adaptive Routing Algorithm
An adaptive routing algorithm either needs a routing table or
hardware logic to provide alternate paths. Our proposed router can
support deadlock-free fully adaptive routing for 2-D mesh and torus
networks using hardware logic. Note that a ﬂit can use any minimal
path in one of the four quadrants, as discussed in the router model
in Section 3.1. The ﬁnal selection of an optimal channel is done
by the PS module. It can be easily proved that the adaptive routing
is deadlock free for a 2-D mesh because the four path sets, shown
in Figure 2 are not involved in cyclic dependency. Whenever two
ﬂits from two quadrants are selected to traverse in the same direction (for example a ﬂit from NE and a ﬂit from SE contend for an
east channel), they use separate VC in the NE and SE path sets,
thereby avoiding channel dependency.
For a 2-D torus network, we use an additional VC to support
deterministic and fully adaptive routing similarly to other adaptive
routing algorithms in torus. One of the VCs in each subset is used
for crossing dimensions in ascending order, and the other VC is
used for descending order, which is possible for wrap-around links
in a torus. Note that prior adaptive routing schemes need at least 3
VCs per PC to support adaptivity.
Although an adaptive routing algorithm helps in achieving better performance speciﬁcally under non-uniform trafﬁc patterns, the
underlying path selection function has a direct impact on performance. Most adaptive routing algorithms take little account of temporal trafﬁc variation and other possible trafﬁc interferences.
The proposed adaptive routing algorithm, utilizes look-ahead congestion detection for the next router’s output links using creditbased system. The PS module keeps track of credits for each neighboring router on the candidate paths. Neighboring routers send
credits indicating congestion (VC state), with the amount of free
buffer space available for each VC. In addition, time between successive transmissions to neighboring routers is kept minimal due
to the low latency of our architecture, and consequently our proposed architecture captures congestion trafﬁc in short spurts and
reacts accordingly. This helps in capturing congestion ﬂuctuation
and picking the right channel from amongst the available candidate
channels.
3.4 Contention Probabilities
To illustrate the beneﬁts of our decomposed crossbar architecture, we compare the contention probabilities of a full crossbar
versus our decomposed crossbar design. We derive the contention
probability as a function of the offered load λ, where λ is the probability that a ﬂit arrives at an arbitrary time slot for each input port
[15], for a generic (NxN) full crossbar and an (N-1xN-1) decomposed crossbar, assuming uniform distribution. Let λf , λP E denote
load directed to an output port and ejection channel respectively.
Let Ps be the probability that a port is busy serving a request. Let
Qn be the probability that an input port has n ﬂits for a speciﬁc
output port at the time of request. Qn can be solved by discrete
time Markov-chain (DTMC) as follows:
Q0 = 1 − λf
Ps
, Q0 P E = 1 − λP E
Ps
The contention probabilities, Pcon f ull for the full crossbar and
Pcon dec the decomposed crossbar, can be formulated as:
Full Crossbar:
N −1X
„
«
(1 − Q0 )j QN −1−j
0
Q0 P E
Pcon f ull =
N −1X
j=2
+
j=2
„
1
j
N − 1
j
«
1
j
N − 1
j − 1
(1 − Q0 )j−1QN −j
0
(1 − Q0 P E )
Decomposed Crossbar:
«
„
N −1
2X
j=2
1
j
N −1
2
j
Pcon dec =
(1 − Q0 )j Q
N −1
2 −j
0
The contention probability is shown in Figure 3. It is evident that
the decomposed crossbar outperforms the full crossbar. The tree
structure of the decomposed crossbar (see Figure 2) provides ﬂits
with pre-selected path sets towards the destination, thus reducing
contention.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
t
i
l
i
b
a
b
o
r
P
n
o
i
t
n
e
t
n
o
C
0
0.1
Full Crossbar
Decomposed Crossbar
0.2
0.3
0.4
0.5
0.6
0.7
0.8
(λ
f) Offered Load per Input Port for An Output Port
0.9
Figure 3: Comparison of Contention Probability
4. PERFORMANCE EVALUATION
In this section, we present simulation-based performance evaluation of our architecture in terms of network latency and energy
consumption, under various trafﬁc patterns. We describe our experimental methodology, and detail the procedure followed in evaluation of our proposed architecture.
4.1 NoC Simulator
To evaluate our architecture, we have developed an architectural
level cycle-accurate NoC simulator using C, and we used simulation models for various trafﬁc patterns. The simulator inputs are
parameterizable allowing us to specify parameters such as network
size and topology, routing algorithm, the number of VCs per PC,
buffer depth, PE injection rate, ﬂit size, and number of ﬂits per
packet. Our simulator breaks down the router architecture into
individual components, modeling each component separately and
combining the models in unison to model the entire router and
subsequently the entire network architecture. We measure network latency from the time a packet is created to the network interface of the PE, to the time its last ﬂit arrives at the destination
PE, including network queuing time. We assume that link propagation happens within a single clock cycle.
In addition to the
network-speciﬁc parameters, our simulator accepts hardware parameters such as power consumption (leakage and dynamic) and
clock period for each component, and utilizes these parameters to
provide results regarding the network energy consumption.
4.2 Energy Estimation
Energy consumption for on-chip networks consumes a large chunk
of the total chip energy. While no absolute numbers are given, researchers [19] estimate the NoC energy consumption to approximately 45% of the total chip power. The generic estimation in
[19] relates the energy consumption for each packet to the number of hop traversals per ﬂit per packet, times the energy consumed
for each ﬂit per router. We took this estimation a step further, decomposing the router into individual components which we laid
out in 70nm technology, using a power supply voltage of 1V, and
200MHz clock frequency. We used the Berkeley Predictive Technology model [1], and measured the average dynamic power using
50% switching activity on the inputs. Additionally, we measured
the leakage power of each component when no input activity is
recorded. We imported these values into our architectural level
cycle-accurate NoC simulator and simulated all individual components in unison to estimate both dynamic and leakage power in
routing a ﬂit. Doing so we were able to identify the individual
utilization rates of each component, thus accurately modeling the
leakage and dynamic power consumption.
A particular detail which we emphasized was to distinguish between each ﬂit type (header, data, tail) so as to correctly model the
power consumed by each ﬂit type. For instance, a header ﬂit will
be accessed by all components, where as a data ﬂit will simply stay
in the buffer until propagated to the output port.
4.3 Experimental Setup
We evaluated our architecture using two 8x8 networks, one using 2D torus and the other using 2D mesh topologies. We use 4
ﬂits per packet and wormhole routing, with 4-ﬂit deep buffers per
VC. We use 3 VCs per PC for 2D mesh and 6VCs per PC for 2D
torus (as required by our algorithm). Every simulation initiates a
warm-up phase of 20,000 packets. Thereafter, we inject 1,200,000
packets and simulate our architecture until all the packets are received at the destination nodes. We log the simulation results and
gather network performance metrics based on our simulations for
various trafﬁc patterns. For packet injection rates, we used three
workload traces: self similar web trafﬁc, uniform and MPEG-2
video multimedia traces. We simulated each trace using four different trafﬁc permutations: normal-random, transpose, tornado and
bit-complement [6].
4.4 Simulation Results
First we compare our proposed 2-stage router architecture to a 3stage pipelined router, in order to evaluate the average network latency. In the 3-stage router, we implement both a dimension-order
(DOR) algorithm, as well as our adaptive routing algorithm. We
separate the crossbar arbitration from the routing decision stage,
resulting in three stages in the router pipeline when using our adaptive algorithm. A direct effect of this is a full crossbar switch instead of the decomposed crossbar our 2-stage architecture employs.
We compare both routers (2-stage vs. 3-stage) using both DOR and
our adaptive algorithm, and show the results in Figure 6. We partition the average latency in three parts: contention free transfer
latency, blocking time, and queuing delay in source node. In both
zero load latency, and congestion avoidance, the 2-stage router per 
forms better than the 3 stage. Figure 6 shows also the impact of
adaptivity, indicating poor performance when compared to DOR
under uniform random trafﬁc. Spatial trafﬁc evenness beneﬁts load
balancing in DOR, even if we improve adaptivity. In the presence
of non-uniform trafﬁc however, the adaptive algorithm outperforms
the DOR algorithm as shown in Figure 7 for transpose (TP), selfsimilar(SS) and Multimedia(MM) trafﬁc patterns. As bursty trafﬁc
is more representative of a practical on-chip communication environment, we expect our adaptive algorithm to be very useful.
An additional NoC property is spatial locality. PEs that communicate often with each other, are usually placed in proximity to
each other. In our architecture, when a packet arrives its destination router, instead of traversing the crossbar to the PE, the VC
selection mechanism ejects the packet into the PE immediately, bypassing the router traversal as shown in Figure 4. We also show a
3-stage router without the bypass modiﬁcation in Figure 4, to illustrate the difference. Even though in Figure 4 we assume no blocking inside the router, as the injection rate increases, the blocking
latency inside the router increases as well and the bypassing technique results in signiﬁcant beneﬁts. We simulated the 2-stage and
3-stage router architectures for nearest-neighbor trafﬁc in order to
investigate the latency, which we show in Figure 5, illustrating that
early ejection and intra-router latency reduction jointly decrease the
overall network latency. The increased hardware complexity in implementing the adaptive routing algorithm results in a higher power
consumption than when implementing the simpler, DOR algorithm.
However, the latency reduction when using adaptive algorithm for
non-uniform, more practical trafﬁc environments, more than compensates for this power increase, reducing the overall energy. Figure 8(a)shows the energy consumption per packet for uniform and
transpose trafﬁc, when comparing adaptive routing vs. DOR. In
Figure 8(b), we also see that adaptive routing is beneﬁcial when
we consider the Energy-Delay Product for non-uniform trafﬁc.
5. CONCLUSIONS
We present a low-latency router architecture suitable for on-chip
networks which utilizes an adaptive routing algorithm. The architecture uses a novel path selection scheme resulting in a 2-stage
switching operation with a decomposed crossbar switch. We evaluate our architecture using various trafﬁc patterns, and we show
that under non-uniform trafﬁc our architecture reduces the overall
network latency by a signiﬁcant factor. Due to the low latency of
our architecture, congestion information about each router’s neighboring routers is swiftly updated, resulting in almost real-time updates. We also show that the energy consumption when using adaptive routing is also reduced due to the reduction in the network laPacket
Injection
(1cycle+Tq)
Router
Traversal
(2cycles)
Link
Traversal
(1cycle)
Tq : Queueing
        Delay
Packet
Ejection
2 stage bypass
PE
Bypass
Link
PE
Router
Router
Packet
Injection
(1cycle+Tq)
Router
Traversal
(3cycles)
Link
Traversal
(1cycle)
3 stage non-bypass
PE
Router
Traversal
(3cycles)
Normal
Ejection
Packet
Ejection
PE
Router
Router
Figure 4: Latency Analysis of Nearest-Neighbor Trafﬁc
)
l
e
c
y
c
(
t
y
c
n
e
a
L
e
g
a
r
e
v
A
13
12
11
10
9
8
7
6
5
Decomposed CB(2stage)
Full CB (3stage)
10
15
20
25
30
35
40
Injection Rate (% flits/node/cycle)
Figure 5: Comparison of Nearest-Neighbor Trafﬁc
tency. Results also show a better performance in the presence of
nearest-neighbor trafﬁc, a particular beneﬁt for NoC designs which
emphasize spatial locality. In the future we are planning to explore
architectural optimizations in reducing the latency to a single stage
thus obtaining better congestion information, and introduce more
features in our adaptive algorithm implementation. In particular,
we can expand the lookahead information for the destination router
from VC state only, to include the link state and other parameters
such as hotspots and permanent faults in the links and routers in
order to provide hotspot avoidance and fault tolerance.
6. "
Floorplan-aware automated synthesis of bus-based communication architectures.,"As system-on-chip (SoC) designs become more complex, it is becoming harder to design communication architectures to handle the ever increasing volumes of inter-component communication. Manual traversal of the vast communication design space to synthesize a communication architecture that meets performance requirements becomes infeasible. In this paper, we address this problem by proposing an automated approach for synthesizing cost-effective, bus-based communication architectures that satisfy the performance constraints in a design. Our synthesis flow also incorporates a high-level floorplanning and wire delay estimation engine to evaluate the feasibility of the synthesized bus architecture and detect timing violations early in the design flow. We present case studies of network communication SoC subsystems for which we synthesized bus architectures, detected timing violations and generated core placements in a matter of hours instead of several days it took for a manual effort.","Floorplan-Aware Automated Synthesis of Bus-based 
Communication Architectures 
Sudeep Pasricha†, Nikil Dutt†, Elaheh Bozorgzadeh†, Mohamed Ben-Romdhane‡ 
34.3 
               †Center for Embedded Computer Systems 
                      University of California, Irvine, CA  
                     {sudeep, dutt, eli}@cecs.uci.edu 
                       ‡Conexant Systems Inc. 
    Newport Beach, CA 
                             m.benromdhane@conexant.com 
ABSTRACT 
As System-on-Chip (SoC) designs become more complex, it is becoming 
harder to design communication architectures to handle the ever increasing 
volumes of inter-component communication. Manual traversal of the vast 
communication design space to synthesize a communication architecture 
that meets performance requirements becomes infeasible. In this paper, we 
address this problem by proposing an automated approach for synthesizing 
cost-effective, bus-based communication architectures that satisfy the 
performance constraints in a design. Our synthesis flow also incorporates a 
high-level floorplanning and wire delay estimation engine to evaluate the 
feasibility of the synthesized bus architecture and detect timing violations 
early 
in 
the design flow. We present case studies of network 
communication SoC subsystems 
for which we synthesized bus 
architectures, detected timing violations and generated core placements in a 
matter of hours instead of several days it took for a manual effort. 
Categories and Subject Descriptors: 
Applications]: Computer-Aided Design  
General Terms: Design, Performance 
Keywords: Communication Synthesis, Systems-on-Chip 
J.6.1 
[Computer 
1. INTRODUCTION 
Improvements in process technology have led to more and more 
functionality being integrated onto a single chip, which has in turn resulted 
in a sharp increase in the amount of overall on-chip communication 
volumes between the integrated components. In such highly integrated 
systems, on-chip communication 
is expected 
to become a major 
performance bottleneck [1]. Already, increasingly demanding performance 
requirements from the next generation of multimedia, broadband and 
network applications are making interconnect design a challenging 
proposition.  
Bus-based communication architectures [2-4] remain a popular choice for 
handling on-chip communication in SoC designs today, because they are 
simple to design and take up very little area. However, selecting and 
reconfiguring standard bus-based communication architectures such as 
AMBA [2] and CoreConnect [3], to meet application specific performance 
requirements, is a very time consuming process. This is due to the large 
exploration space created by customizable bus topologies, arbitration 
protocols, DMA burst sizes, data bus widths, bus clock speeds and buffer 
sizes, all of which significantly impact system performance [5][12][26]. 
To counter 
the challenge of ever 
increasing on-chip bandwidth 
requirements and a vast communication exploration space, early planning 
of the interconnect architecture at the system level must become an integral 
part of a SoC design process. However the complex interplay between 
communication architecture parameters is hard to analyze effectively even 
at the system level. Previous research in this area (discussed in Section 2) 
has been limited to identifying and automating the exploration of small 
subsets of this design space. Very often, designers end up evaluating the 
communication design space by creating simulation models annotated with 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. 
DAC 2005, June 13–17, 2005, Anaheim, California, USA. 
Copyright 2005 ACM 1-59593-058-2/05/0006…$5.00. 
565
detail based on experience, and manually iterating through different design 
configurations. Such an effort remains time consuming and produces 
systems which are generally overdesigned for the application at hand. 
To address this problem, we propose a bus architecture synthesis approach 
in this paper, which automates the generation of a cost effective 
communication architecture for a SoC. We make use of SystemC [23] to 
quickly capture components at the behavioral system-level and automate 
the bus architecture synthesis for the design. The novelty of our approach is 
in the ability to automatically satisfy performance constraints and detect 
bus cycle 
time violations, while synthesizing a feasible, 
low-cost 
configuration of a standard bus-based communication architecture (such as 
[2]) which is commonly used in SoC designs. Our approach synthesizes the 
bus topology, as well as values for bus architecture parameters such as 
arbitration priority orderings, data bus widths, bus clock speeds and DMA 
burst sizes. Additionally, we make use of a high-level floorplanning engine 
to generate estimates of core placements on the chip. Typically, once the 
system architecture is frozen, it takes several months before a floorplan of 
the design becomes available. Violations of timing constraints, detected at 
this late stage, can require changes in the architecture which can severely 
impact time-to-market. Our high-level floorplanning and wire delay 
estimation engines detect these timing violations early in the design flow at 
the system level, where architectural modifications and tradeoff analysis 
can be performed quickly and efficiently to eliminate such violations. To 
demonstrate the usefulness of our approach, we present case studies of 
network communication SoC subsystems, used for data packet processing 
and forwarding. Compared to a manual effort which took several days and 
produced overdesigned systems, our automated flow synthesized low-cost 
bus architectures, detected timing violations and generated core placements 
which satisfied performance constraints for the SoC subsystems in a matter 
of a few hours. 
2. RELATED WORK 
There is already a significant body of research in the area of bus 
architecture synthesis. Early work was aimed at minimizing bus width [6], 
interface synthesis and simple synchronization protocol selection [7] and 
topology generation for simple busses without arbitration [8]. Ryu et al. [9] 
performed studies to find optimal bus topologies for a SoC design. Pinto et 
al. [10] proposed an algorithm for constraint-driven topology synthesis 
under the assumption that relative positions of components were fixed. 
Lyonnard et al. [11] proposed a synthesis flow which supported shared bus 
and point to point connection templates. These templates have to be 
parameterized manually, which makes the process time consuming. Lahiri 
et al. [12] designed communication architectures after exploring different 
solutions using fast performance simulation. However, they assumed the 
bus topology to be given. Shin et al. [13] used a genetic algorithm for 
automating the generation of bus architecture parameters to meet 
performance requirements. However, they do not focus on bus topology 
synthesis. Our approach differs from these existing approaches in the way 
we automate the synthesis of not only the bus topology, but also the 
generation of values for bus architecture parameters, while also satisfying 
performance constraints. 
A key component of our synthesis flow is the integrated floorplanner. 
There have been other approaches in the past which have made use of a 
floorplanning tool [14-18] in a synthesis flow, but for different reasons. 
Bergamaschi et al. [18] and Thepayasuwan et al. [14] used the floorplanner 
to generate an early core placement estimate. Drinic et al. [15] used the 
floorplanner to determine feasibility of the synthesized design by 
comparing estimates of wire length with an upper bound on wire length. 
 
 
 
       
 
 
 
 
 
 
 
However an upper bound on wire length has the disadvantage of not 
accounting for varying capacitive loads of the components. Hu et al. [16] 
also used the floorplanner to estimate wire length, which they used to 
calculate energy consumption in point to point networks. Dick et al. [17] 
invoked the floorplanner repeatedly in their custom bus topology synthesis 
approach to obtain global wiring delays and ensure that real time deadlines 
were met. Unlike existing approaches, the floorplanner in our approach is 
used to identify bus cycle time violations and verify the feasibility of the 
synthesized bus architecture early in the design flow. We believe that this 
step will become increasingly important in the deep submicron era as clock 
speeds increase and lengthy propagation delays cause violations of timing 
constraints that will need to be detected and corrected early in the design 
flow.  
3. AUTOMATED BUS SYNTHESIS 
This section describes our approach for automated bus architecture 
synthesis. Section 3.1 formulates 
the problem and presents our 
assumptions. Section 3.2 discusses the simulation engine while Section 3.3 
describes communication parameter constraints, which guide the bus 
synthesis process. Section 3.4 gives an overview of our floorplan and wire 
delay calculation engines used for detecting timing violations in the design. 
Finally, Section 3.5 presents our automated bus architecture synthesis 
approach in detail.   
3.1 Problem Formulation 
We are given a SoC having several components (IPs) that need to 
communicate with each other. The standard bus-based communication 
architecture (e.g. AMBA [2], CoreConnect [3]) which determines the pins 
at the IP interface and for which the bus topology and communication 
parameter values must be synthesized, is also specified. It is assumed that 
hardware software partitioning has taken place and that the appropriate 
functionality has been mapped onto hardware and software. The IPs are 
assumed to be standard “black box” library components which cannot be 
modified during the bus synthesis process, except for the memory 
components.  
MEM1MEM1
CPU1
CPU1
S3S3
M2M2
360 Mbps
S1S1
MEM2MEM2
S2S2
M3M3
MEM3MEM3
Figure 1. Communication Throughput Graph (CTG) 
Typically, SoC designs need to satisfy performance constraints which are 
generally dependent on the nature of the application. The throughput of 
communication between components is a good measure of the performance 
of a system [8]. We assume that we are given one or more throughput 
constraints for the system that need to be satisfied. These constraints can 
involve communication between two or more IPs. Figure 1 shows a 
Communication Throughput Graph (CTG) which is a directed graph, 
where each vertex v represents a component in the system, and an edge aij 
connects components i and j that need to communicate with each other. 
Each vertex v contains information about the component it represents, such 
as its area, dimensions (fixed width/height or aspect ratio), capacitive loads 
on output pins and which bus type it can be connected to – a main high 
bandwidth bus like AHB [2], a peripheral low bandwidth bus like APB [2] 
or both. An edge aij is associated with a throughput constraint τ(aij) if it lies 
within a throughput constraint path (TCP). Figure 1 shows a TCP 
involving CPU1, MEM1, S1 and M2 components, where the rate of data 
packets streaming out of M2 must not fall below 360 Mbps. A TCP, in 
general, has a single master for which data throughput must be maintained 
and other masters, slaves and memories which are in the critical path that 
impacts the maintenance of the throughput. 
The problem then is to generate a bus topology, and determine bus 
architecture parameter values for 
the selected standard bus-based 
communication architecture, while ensuring that all throughput constraints 
in the system are satisfied. In addition, we want to consider layout 
information of the chip to detect bus cycle time violations, early in the 
design flow. Finally, the synthesized bus architecture must be cost 
effective, having the least number of busses, and the lowest values for bus 
widths and speeds, while still satisfying all constraints.  
3.2 Simulation Engine 
Since communication behavior is characterized by unpredictability due to 
dynamic bus requests from cores, bus contention etc., a simulation based 
approach is necessary for accurate performance estimation. In our synthesis 
flow, we capture behavioral models of components and bus architectures in 
SystemC [23], and keep them in an IP library database. Since we were 
concerned about the speed of simulation, we chose a fast transaction-based, 
bus cycle accurate modeling abstraction, which averaged simulation speeds 
of 150–200 Kcycles/sec [5], while running embedded software applications 
on processor ISS models. 
3.3 Communication Parameter Constraints 
The exploration space for a typical SoC bus-based communication 
architecture such as AMBA [2] consists of combinations of bus topology 
configurations with communication parameter values for arbitration 
schemes, data bus widths, bus clock speeds and DMA burst sizes. If we 
allow these parameters to have any arbitrary values, an incredibly vast 
design space is created. The time required to simulate through all possible 
system configurations searching for one which satisfies every design 
constraint would become unreasonably large, even with the fast simulation 
engine. More importantly, once we manage to find such a system 
configuration, there would be no guarantee that the values generated for the 
communication parameters would be practically feasible. To ensure that 
our synthesis approach generates a realistic communication architecture 
configuration, we allow the designer to specify a Communication 
Parameter Constraint set (Ψ). These constraints are in the form of a 
discrete set of valid values for the communication parameters to be 
synthesized. A major motivation to allow this constraint specification is 
that it allows the designer to bias the synthesis process based on knowledge 
of the design and the technology being targeted. For instance, a designer 
might decide that the synthesized design should only have data busses with 
16, 32 or 64 bit widths, because the IPs in the design cannot support larger 
widths effectively. Or a designer might set the allowable bus clock 
frequency to multiples of 33 MHz, with a maximum speed of 166 MHz, 
based on the operation frequency of the cores in the system and past 
experience of the clock generation mechanism. Such knowledge about the 
design is not a prerequisite for using our synthesis framework. As long as 
Ψ is populated with any discrete set of values for the parameters, our 
framework will attempt 
to synthesize a 
feasible communication 
architecture. However, informed decisions can greatly reduce the time 
taken for synthesis and help the designer generate a more practical system.  
3.4 Floorplanning and Delay Estimation Engines 
The floorplanning stage in a typical design flow arranges arbitrarily shaped, 
but usually rectangular blocks representing circuit partitions, into a nonoverlapping placement while minimizing a cost function, which is usually 
some 
linear combination of die area and 
total wirelength. Our 
floorplanning engine is adapted from the simulated annealing based 
floorplanner proposed in [19]. The input to the floorplanner is a list of 
components and their interconnections in the system. Each component has 
an area associated with it (obtained from RTL synthesis). Dimensions in 
the form of width and height (for “hard” components) or aspect ratio (for 
“soft” components) are also required for each component. Additionally, 
maximum die size and fixed locations for hard macros can also be specified 
as inputs. Given these inputs, our floorplanner minimizes the cost function 
               Cost = w1.Area +w2.BusWL +w3.TotalWL            ... (1) 
where Area is the area of the chip, BusWL is the wire length corresponding 
to wires connecting components on a bus, TotalWL is total wire length for all 
connections on the chip (including inter-bus connections) and w1, w2, w3 are 
adjustable weights which are used to bias the solution. The floorplanner 
outputs a non overlapping placement of components from which the wire 
lengths can be calculated by using half-perimeter of the minimum 
bounding box containing all terminals of a wire (HPWL) [20].  
566
 
 
 
 
 
 
 
 
 
 
 
 
567
Ck
Ck-1
lk
C2
C1
l2
Rd
l1
(a)
CL
C0
Rd
l
(b)
∑ ∑
=
1
j
=
i
1
= k
j
j
i
L
.
C
l
l
C
∑
=
1
CC
−
= k
j
L
j
O
C
(c)
Figure 2. Transforming multiple pin net into a two pin net  
Once the wire lengths have been calculated, the delay estimation engine is 
invoked. The wire delay is calculated based on formulations proposed in 
[21]. The inputs to this stage are the wire lengths from the floorplanner and 
the capacitive loads (CL) of component output pins (obtained from RTL 
synthesis). We can simplify 
the multiple pin problem (which 
is 
representative of a bus line) depicted in Figure 2(a) to a two pin problem 
shown in Figure 2(b). Then the delay for a wire of length l, with optimal 
wire sizing (OWS) [21], is given as 



.
l
lcrcR
cR
l2
α
lW
(
α
α
l
(
α
l
W
CR
T
fa
d
fd
2
1
2
2
1
o
d
)
)



 and W(x) is Lambert’s W function 
+
+
+
+
=
        … (2) 
where 
a
1
rc
1=α
4
, 
L
d
a
2
rc
CR
1=α
2
defined as the value of w which satisfies wew=x. Rd is the resistance of the 
driver, l is the wire length, CO and CL are capacitive loads which are 
calculated as shown in Figure 2(c) and the rest of the parameters are 
dependent on the process technology used – r is the sheet resistance in 
Ω/sq, ca is unit area capacitance in fF/µm2 and cf is unit fringing 
capacitance in fF/µm (defined to be the sum of fringing and coupling 
capacitances). The values for these technology dependent parameters are 
listed in Table 1, and have been calculated from [22].  
Table 1. Parameters based on NTRS 97 
Tech (µm) 
r 
ca 
cf 
0.18 
0.068 
0.060 
0.064 
0.15 
0.073 
0.054 
0.054 
0.13 
0.081 
0.046 
0.043 
The delay estimation engine is ultimately used to check for bus cycle time 
violations in the design. This is illustrated through an example. Figure 3 
shows a floorplan for a system where IP1 and IP2 are connected to the 
same bus as ASIC1, Mem4, ARM, VIC and DMA, and the bus has a speed 
of 333 Mhz. This implies that the bus cycle time is 3 ns. For a 0.13 µm 
process and a driver resistance value Rd of 0.4 kΩ, the floorplanner finds a 
wire length of 9.9 mm between pins connecting the two IPs to the bus, with 
CL = 2.936 pF and CO = 0.988 pF for the wire. The wire delay, obtained by 
inserting these values in (2), is found to be 3.5 ns. This violates the clock 
cycle time constraint of 3 ns, and we thus conclude that the bus architecture 
is not feasible. In this way, our floorplanning and delay estimation engines 
determine if a synthesized design is feasible or not. As we will show later, 
our synthesis flow attempts to automatically eliminate such violations once 
they are detected.  
IP1
IP2
Figure 3. Example floorplan 
3.5 Synthesis Approach 
We now describe our automated synthesis approach in detail. Figure 4 
gives a high level overview of the flow. The inputs to the flow include a 
Communication Throughput Graph (CTG), a 
target communication 
architecture (e.g. AMBA), a set of Communication Parameter Constraints 
(Ψ) and a library of behavioral IP models. The general idea is to first 
perform preprocessing transformations on the CTG to improve the 
performance of the entire system (preprocess) and then map all the 
components from the CTG to a simple bus topology. Then, we iteratively 
select a Throughput Constraint Path (TCP), starting from the TCP with the 
largest constraint and moving in descending order, from set Ω (which is a 
superset of all TCPs in the system) and search the communication 
parameter space for a suitable parameter configuration (explore_params) 
and possibly perform topology mutations if needed (mutate_topology) till 
all TCP constraints are satisfied. Once all TCP constraints are satisfied, we 
optimize the design (optimize_design) to lower the cost of the system and 
avoid possible timing violations. Next we invoke the floorplanning and 
delay estimation engines to detect bus cycle time violations. If timing 
violations are detected, we update Ω and repeat the topology mutation and 
parameter exploration phase, or proceed to output the synthesized system 
and floorplan if there is no violation. 
CTGCTG
comm
comm
arch.
arch.
constraint
constraint
Set (Ψ)
Set (Ψ)
preprocess
preprocess
simple bus 
simple bus 
mapping
mapping
explore_params
explore_params
TCP 
TCP 
met?
met?
yes
mutate_topology
mutate_topology
optimize_design
optimize_design
output synthesized 
output synthesized 
communication arch
communication arch
yes
IP 
IP 
library
library
Select unsatisfied 
Select unsatisfied 
TCP from Ω
TCP from Ω
Ω empty?
Ω empty?
Run floorplanner
Run floorplanner
and delay estimator
and delay estimator
Ω still
Ω still
empty?
empty?
no
no
yes
no
Figure 4. Automated Synthesis Flow 
Figure 5 shows the pseudo code for the preprocess stage. In the first step 
we map the components in the CTG from the behavioral IP library database 
to a simple, protocol-independent, transaction-level simulation model in 
SystemC [24] having a virtual channel for every edge in the graph. This 
model has no contention since there are no shared channels and also 
because we assume infinite ports at IP interfaces. The purpose of this step 
is to obtain, through simulation, a memory usage profile (Step 2). Once we 
have obtained this profile, we attempt to split those memory nodes for 
which different masters access non-overlapping regions (Step 3). Finally 
we merge local slave nodes with their master nodes to reduce contention 
and loading on shared busses (Step 4). Note that we perform Step 3 before 
Step 4 because it allows us to generate local memories which can then be 
merged with their corresponding masters.  
Step 1:  Map CTG to protocol-independent TLM 
Step 2:  
Simulate design 
Generate usage profile for memory modules 
Step 3:  
Split memory nodes wherever applicable 
Step 4:  Merge local memory/slave nodes with master nodes 
Figure 5. preprocess procedure 
After the preprocessing stage, all the components in the enhanced CTG and 
the selected bus architecture are mapped from the IP library database to the 
fast transaction-based bus cycle-accurate simulation model (Section 3.2) 
with a simple bus topology – a single shared main and a single shared 
peripheral bus. As mentioned earlier, every node in a CTG has information 
relating to the type of bus it can be connected to, which guides the mapping 
process. Once the simple topology is created, we select the largest 
unsatisfied TCP constraint from set Ω and search for a suitable combination 
of communication parameter values to satisfy the constraint in the 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
explore_params stage. Figure 6 gives the pseudo code for this procedure. 
explore_params searches for a suitable combination of parameter values 
which satisfies the TCP constraint under consideration, for the current 
topology. The parameter values are bounded by the constraint set Ψ 
specified by the designer. However, the exploration space arising from the 
combinations of the bounded values can still be very large. In the interest 
of achieving practical running times, we set the bus widths and speeds to 
the maximum allowed values set by the designer in Ψ (Step 1). We then 
select a combination of a valid arbitration priority ordering and DMA burst 
size, and proceed to simulate the design (Steps 2, 3). The best result 
configuration in Step 3 is the combination of parameters for which the least 
number of TCP constraints are violated and the throughput for the TCP 
being considered is the highest. The set of valid arbitration priorities is 
governed by the following rules: (a) priorities of masters in TCPs with 
larger throughput constraints are always greater than priorities of masters in 
TCPs with lower throughput constraints, (b) once a TCP constraint is 
satisfied, the relative arbitration priority ordering for masters in the TCP is 
updated (Step 5) and not changed anymore and (c) only combinations of 
priority orderings within the TCP under consideration need to be explored 
if the previous two rules are followed. These three rules reduce the large 
arbitration space and make it more manageable. The set of valid DMA 
burst sizes is governed by the following rule: (a) once a TCP constraint is 
satisfied, only those DMA burst size values which did not violate the 
satisfied TCP constraint are considered for subsequent TCPs. Thus, as TCP 
constraints are satisfied, the set of valid DMA burst size values shrinks, 
reducing the DMA burst size exploration space. Figure 6 shows how once a 
TCP constraint is satisfied, we simulate the design for different DMA burst 
size values to generate an updated set of allowed DMA burst sizes (Step 6) 
which will be used for subsequent TCP explorations. 
Step 3:  
Step 4:  
Step 1:  
Step 2: 
Set bus speed, bus width to maximum allowed in set Ψ 
Select a combination of valid arbitration priority ordering and 
valid DMA burst size. Exit if all valid combinations exhausted  
Simulate design 
Update best result configuration 
If TCP constraint not satisfied or previously satisfied TCP 
constraint  violated, goto Step 2  
Step 5:  Update Ω and arbitration priority ordering for masters in TCP 
Step 6: 
Simulate design for remaining DMA burst sizes and update 
allowed DMA burst size set. 
Figure 6. explore_params procedure 
If the TCP constraint is not satisfied for any combination of communication 
parameter values, we attempt to change the communication topology in the 
mutate_topology stage. Figure 7 shows the pseudo code for this procedure. 
To meet TCP constraints, we need to eliminate conflict on shared busses, 
and this can be done by creating new busses and migrating IPs, from the 
TCP being considered, iteratively to the new bus till the conflict is 
resolved. In mutate_topology, we first choose an unselected master in the 
current TCP, create a new bus and migrate the master to the new bus (Step 
2). In subsequent iterations of mutate_topology, we migrate the slaves in 
the current TCP to the new bus (Step 3). Once all slaves in the current TCP 
have been considered for migration and the TCP is still not satisfied, we 
check for unselected masters in the current TCP (Step 4). If there are still 
unselected masters remaining, we undo all slave migrations since the last 
master migration, mark the slaves in the TCP as being unselected and 
migrate a randomly chosen previously unselected master to the new bus 
(Step 4a). In subsequent iterations we again migrate the slaves to the new 
bus (Step 3). After all masters and slaves in the current TCP have been 
moved to the new bus or at least considered for migration, it is possible that 
the TCP constraint is still not met. In that case, we mark all the master and 
slave IPs in the TCP as unselected, randomly select a master on the 
previously created bus and permanently assign it to that bus, create another 
bus and starting from a randomly selected master, iteratively migrate IPs to 
that bus (Step 4b). In this way, new busses are created till enough 
bandwidth is available to satisfy the TCP throughput constraint. Note that if 
a 
topology mutation causes 
the best 
result configuration 
from 
explore_params to violate any previously satisfied TCP constraints, we 
undo the mutation (Step 1). Otherwise we keep the mutation, even if it 
deteriorates current TCP performance slightly. This allows us to take into 
account the effect of local minima in the exploration phase. 
568
Step 1:  
Step 2: 
Step3: 
Step 4: 
Step 4a: 
Step 4b: 
Step 5: 
Step 6: 
If previous mutation caused any satisfied TCP constraint to be 
violated, undo mutation 
If (no master in current TCP selected yet)  
Create new bus 
Goto Step 6 
If (TCP master selected) and (unselected TCP slaves remain) 
Goto Step 5 
If (all TCP slaves already selected) 
    If (unselected TCP master remains) 
        Undo all slave migrations since last master migration 
        Mark slaves in TCP as unselected 
        Goto Step 6  
   If (all TCP masters already selected) 
        Mark slaves in TCP as unselected 
        Mark all masters in TCP as unselected  
        Randomly select master on last created bus, permanently 
        assign master to that bus 
       Create new bus 
       Goto Step 6 
Randomly select an unselected slave. 
Migrate to new bus. Exit 
Randomly select an unselected master. 
Migrate to new bus. Exit 
Figure 7. mutate_topology procedure 
Once all the TCP constraints are satisfied, we arrive at the optimize_design 
stage. The pseudo code for this stage is given in Figure 8. The purpose of 
this stage is to reduce the ‘pessimistic’ high values we selected for bus 
widths and bus clock speeds, to reduce the cost of the final system. Here 
we iteratively consider each bus in the system and attempt to lower the 
value for bus width (Step 2) and bus clock speed (Step 4), without violating 
any TCP constraints. Reducing the bus speed in this stage also helps 
prevent physical timing violations since it lengthens the bus cycle time.  
Step 1:  
Select previously unselected bus from generated bus arch. 
Step 2:  Reduce data bus width to next lower value 
Simulate design 
If (TCP constraint violation), undo, else goto step 2 
Reduce bus speed to next lower value 
Simulate design 
If (TCP constraint violation), undo, else goto step 4 
If all busses examined, exit, else goto step 1 
Step 3:  
Step 4: 
Step 5:  
Step 6: 
Figure 8. optimize_design procedure 
Next we pass the optimized system through our floorplanning and wire 
delay estimator engine. If a timing violation is detected, the set Ω is 
updated with TCPs which have components on the busses with violations, 
and we again go back and attempt to select appropriate parameter value 
combinations and a different topology which resolves the timing violations. 
Changing the topology by migrating IPs to a new bus, in particular, reduces 
capacitive loading and consequently wire delay on the bus, which is one of 
the primary causes of timing violation in a design (Section 3.4). Finally, 
after any violations have been resolved and all TCP constraints satisfied, 
we output the final synthesized bus topology, parameter values for bus 
speeds, data bus widths, DMA burst size and arbitration priority ordering, 
along with the feasible floorplan. 
4. CASE STUDIES 
We applied our automated bus-based communication architecture synthesis 
approach on 
two 
industrial strength designs 
from 
the network 
communication domain. In the first case study, we selected a network 
communication SoC subsystem used for fast data packet processing and 
forwarding. Figure 9 shows the CTG for this system. There are two data 
manipulation related TCP constraints that must be satisfied in this system. 
The first TCP involves the encryption engine and includes the ARM926, 
ASIC1, RAM3 and EXT_IF blocks. The EXT_IF block fetches data and 
stores it in RAM3. The ASIC1 and ARM926 blocks fetch non overlapping 
sections of the data, process them and store them back in RAM3, from 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
         
 
 
 
 
 
 
 
 
 
 
 
 
 
569
where the EXT_IF block fetches and streams them out at a minimum rate 
of 200 Mbps. The second TCP involves the USB subsystem. Data packets 
received at the USB are routed to RAM1. The ARM926 reads this data, 
processes it and stores it back to RAM1 from where the DMA engine 
transfers it to SDRAM_IF, which streams it out at a minimum rate of 480 
Mbps. There is also a third subsystem which involves the SWITCH, RAM2 
and ARM926 components. However, this is a very low priority data path 
which has no data rate constraint from the designer, and therefore we do 
not classify it as another TCP to be satisfied. 
ARM926
ARM926
ASIC1
ASIC1
ITCITC
UART
UART
ROMROM
USB 2.0
USB 2.0
DMADMA
SDRAM 
SDRAM 
IF
IF
RTCRTC
TIMER
TIMER
RAM1
RAM1
RAM3
RAM3
EXT 
EXT 
IF
IF
SWITCH
SWITCH
RAM2
RAM2
Figure 9. Network Communication Subsystem 
Table 2 shows the Communication Parameter Constraint set (Ψ) for this 
case study. The target communication architecture for the automated 
synthesis is the AMBA2 high performance AHB bus and a l"
FLEXBUS - a high-performance system-on-chip communication architecture with a dynamically configurable topology.,"In this paper, we describe FLEXBUS, a flexible, high-performance onchip communication architecture featuring a dynamically configurable topology. FLEXBUS is designed to detect run-time variations in communication traffic characteristics, and efficiently adapt the topology of the communication architecture, both at the system-level, through dynamic bridge by-pass, as well as at the component-level, using component re-mapping. We describe the FLEXBUS architecture in detail and present techniques for its run-time configuration based on the characteristics of the on-chip communication traffic. The techniques underlying FLEXBUS can be used in the context of a variety of on-chip communication architectures. In particular, we demonstrate its application to AMBA AHB, a popular commercial on-chip bus. Detailed experiments conducted on the FLEXBUS architecture using a commercial design flow, and its application to an IEEE 802.11 MAC processor design, demonstrate that it can provide significant performance gains as compared to conventional architectures (up to 31.5% in our experiments), with negligible hardware overhead.","FLEXBUS: A High-Performance System-on-Chip
Communication Architecture with a Dynamically
Conﬁgurable Topology ∗
Krishna Sekar
Dept. of ECE
UC San Diego, CA 92093
ksekar@ece.ucsd.edu
ABSTRACT
Kanishka Lahiri
NEC Laboratories America
Princeton, NJ 08540
klahiri@nec-labs.com
In this paper, we describe F L E XBU S, a ﬂexible, high-performance onchip communication architecture featuring a dynamically conﬁgurable
topology. F L E XBU S is designed to detect run-time variations in communication trafﬁc characteristics, and efﬁciently adapt the topology of
the communication architecture, both at the system-level, through dynamic bridge by-pass, as well as at the component-level, using component re-mapping. We describe the F L E XBU S architecture in detail and
present techniques for its run-time conﬁguration based on the characteristics of the on-chip communication trafﬁc. The techniques underlying F L E XBU S can be used in the context of a variety of on-chip
communication architectures. In particular, we demonstrate its application to AMBA AHB, a popular commercial on-chip bus. Detailed
experiments conducted on the F L E XBU S architecture using a commercial design ﬂow, and its application to an IEEE 802.11 MAC processor
design, demonstrate that it can provide signiﬁcant performance gains
as compared to conventional architectures (up to 31.5% in our experiments), with negligible hardware overhead.
Categories and Subject Descriptors: B.4.3 [Input/Output and Data
Communications]: Interconnections (Subsystems)
General Terms: Algorithms, Design, Performance
Keywords: Communication architectures, On-chip bus
1.
INTRODUCTION
The on-chip communication architecture has emerged as a critical determinant of overall performance in complex System-on-Chip
(SoC) designs, which makes it crucial to customize it to the characteristics of the trafﬁc generated by the application. Since application
trafﬁc can exhibit signiﬁcant diversity, conﬁgurable communication
architectures that can be easily adapted to an application’s requirements are desirable.
As shown in this paper, complex SoC designs can beneﬁt from communication architectures that can be dynamically conﬁgured in order
to adapt to changing trafﬁc characteristics. Most state-of-the-art communication architectures provide limited customization opportunities
through a static (design time) conﬁguration of architectural parameters, and as such, lack the ﬂexibility to provide high performance in
cases where the trafﬁc proﬁles exhibit signiﬁcant dynamic variation.
In this paper, we describe F L E XBU S, a ﬂexible communication architecture that detects run-time variations in system-wide communication trafﬁc characteristics, and efﬁciently adapts the logical connectivity of the communication architecture and the components connected
to it. This is achieved using two novel techniques, bridge by-pass,
and component re-mapping, which address conﬁgurability at the system and component levels, respectively. These techniques provide opThis work was supported by the UC Discovery Grant (grant# com02-10123), the Center
for Wireless Communications (UC San Diego), and NEC Laboratories America.
∗
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2005, June 13–17, 2005, Anaheim, California, USA.
Copyright 2005 ACM 1-59593-058-2/05/0006 ...$5.00.
Anand Raghunathan
NEC Laboratories America
Princeton, NJ 08540
anand@nec-labs.com
Sujit Dey
Dept. of ECE
UC San Diego, CA 92093
dey@ece.ucsd.edu
portunities for dynamically controlling the spatial allocation of communication architecture resources among different SoC components,
a capability, which if properly exploited, can yield substantial performance gains. We also describe techniques for choosing optimized
F L E XBU S conﬁgurations under time-varying trafﬁc proﬁles. We have
conducted detailed experiments on F L E XBU S using a commercial design ﬂow to analyze its area and performance under a wide variety of
system-level trafﬁc proﬁles, and applied it to an IEEE 802.11 MAC
processor design. The results demonstrate that F L E XBU S provides up
to 31.5% performance gains compared to conventional architectures,
with negligible hardware overhead.
1.1 Related Work
The increasing importance of on-chip communication has resulted
in numerous advances in communication architecture topology and
protocol design [1]-[5]. However, these architectures are based on
ﬁxed topologies, and therefore, are not fully optimized for trafﬁc with
time-varying characteristics. The use of sophisticated communication
protocols [3], [6] and techniques for customizing these protocols, both
statically and dynamically [7], [8], [9], have been proposed. Protocol
and topology customization are complementary, and hence, may be
combined to yield large performance gains. A number of automatic
approaches have been proposed to statically optimize the communication architecture topology [10], [11]. While many of these techniques
aim at exploiting application characteristics, they do not adequately
address dynamic variations in communication trafﬁc proﬁles.
2. BACKGROUND
Communication architecture topologies can range from a single
shared bus, to which all the system components are connected, to a
network of bus segments interconnected by bridges. Component mapping refers to the association between components and bus segments.
Components can be either masters (e.g., CPUs, DSPs), which can initiate transactions (reads/writes), or slaves, (e.g., memories, peripherals),
which only respond to transactions initiated by a master. Communication protocols specify conventions for the data transfer (e.g., arbitration policies, burst transfer modes). Bridges are specialized components that enable transactions between masters and slaves located on
different bus segments. “Cross-bridge” transactions execute as follows: the transaction request from the master, once granted by the ﬁrst
bus, is registered by the bridge’s slave interface. The bridge forwards
the transaction to it’s master interface, which then requests access to
the second bus. On being granted, the bridge executes the transaction
with the destination slave and returns the response to it’s slave interface, which ﬁnally returns the response to the original master.
3. MOTIVATIONAL EXAMPLE: IEEE 802.11
MAC PROCESSOR
In this section, we use an IEEE 802.11 MAC processor design as
an example to illustrate the concepts behind the FLEXBUS architecture. The functional speciﬁcation of the IEEE 802.11 MAC processor consists of a set of communicating tasks, shown in Figure 1(a)
(details of the 802.11 MAC protocol are available in [12]). For outgoing frames, the LLC task receives frames from the Logical Link
Control Layer. The Wired Equivalent Privacy (WEP) task and the Integrity Checksum Vector (ICV) task work in conjunction to encrypt
and compute a checksum of the frame, respectively. The HDR task
From 
Logical Link 
Ctrl
LLC
WEP_
INIT
WEP_
ENCRYPT
HDR
FCS
PLI
TKIP
ICV
ICV
MAC_
CTRL
To carrier
sense
WEP
WEP
AHB I/F 
AHB I/F 
FCS 
FCS 
AHB I/F 
AHB I/F 
ARM946ES
ARM946ES
AHB I/F 
AHB I/F 
AHB 
AHB 
Arbiter
Arbiter
AHB I/F 
AHB I/F 
AHB
AHB
To
PHY
Frame
Frame
Data
Data
Control
Control
Signals
Signals
(a)
(b)
(b)
LLC
LLC
AHB I/F 
AHB I/F 
Frame 
Frame 
Buf
Buf
AHB I/F 
AHB I/F 
Key 
Key 
Buffer
Buffer
AHB I/F 
AHB I/F 
PLI
PLI
To 
To 
PHY
PHY
WEP
AHB_1 
Arbiter
ARM946ES
ARM946ES
FCS 
AHB_2 
Arbiter
AHB I/F 
AHB I/F 
AHB I/F 
AHB_1 
LLC
(c)
AHB I/F 
Frame 
Buf_1
AHB I/F 
Bridge
AHB I/F 
AHB I/F 
AHB I/F 
AHB I/F 
AHB_2 
AHB I/F 
AHB I/F 
Key 
Key 
Buffer
Buffer
AHB I/F 
Frame 
Buf_2
AHB I/F 
PLI
Figure 1: IEEE 802.11 MAC processor:(a) functional speciﬁcation, mapping to a (b) single shared, (c) multiple bus architecture
generates the MAC header. The Frame Check Sequence (FCS) task
computes a CRC-32 checksum over the encrypted frame and header.
MAC CTRL implements the CSMA/CA algorithm, determines frame
transmit times, and signals the Physical Layer Interface (PLI) task to
transmit the frames. The Temporal Key Integrity Protocol (TKIP) generates a sequence of encryption keys dynamically. If TKIP is disabled,
the key is statically conﬁgured.
Figure 1(b) shows the set of components to which the above tasks
are mapped in our design. An embedded processor (the ARM946ES)
implements the MAC CTRL, HDR, and TKIP tasks, while dedicated
hardware units implement the LLC, WEP, FCS, and PLI tasks. In the
next two sub-sections, we focus on only the tasks that result in the
majority of the communication trafﬁc: WEP, FCS, and TKIP.
3.1 Statically Conﬁgured Topologies
Example 1: We ﬁrst consider an architecture where a single AMBA
AHB bus segment [2] integrates all the system components (Figure 1(b)). The MAC frames are stored in Frame buf, and processed
in a pipelined manner: the WEP component encrypts a MAC frame,
and then signals the FCS component to start computing a checksum on
the encrypted frame, while it starts encrypting the next frame. When
the keys are statically conﬁgured (the TKIP task is disabled), the onchip communication trafﬁc is largely due to the WEP and FCS components. In such a scenario, simultaneous attempts by the WEP and
FCS hardware to access the system bus lead to a large number of bus
conﬂicts, resulting in signiﬁcant performance loss. Experiments indicate that the maximum data rate under this architecture is 188 Mbps.
When TKIP is enabled, additional trafﬁc between the processor and
the key buffer further degrades the data rate to 158 Mbps.
This bus topology can fail to provide high performance by failing
to exploit parallelism in the communication transactions. This can be
addressed by using an architecture that uses multiple bus segments.
Example 2: Figure 1(c) presents a version of the MAC processor architecture which consists of two AHB segments connected by
a bridge. The WEP component reads frame data from memory
Frame buf1, encrypts it, and then transfers the encrypted frame
into Frame buf2. The FCS component processes the frame from
Frame buf2, while WEP starts processing the next frame from
Frame buf1. We ﬁrst assume that the keys are statically conﬁgured. The parallelism offered by the multiple bus architecture enables the FCS and WEP tasks to process frame data stored in their
local frame buffers concurrently. However, the frame copy phase
from Frame buf1 to Frame buf2 incurs large latencies due to the
complex nature of cross-bridge transactions (Section 2). Experiments
show that the data rate achieved by this architecture is 201 Mbps, only
a 7% improvement over the single shared bus. When the TKIP task is
enabled, the improvement over the shared bus is 11% (176 Mbps).
In summary, we observe that when the proportion of cross-bridge
trafﬁc is low, the multiple bus architecture performs well, whereas at
other times, the single shared bus architecture is superior.
3.2 Dynamic Topology Conﬁguration
We next consider the execution of the IEEE 802.11 MAC processor
under two variants of the F L E XBU S architecture.
Example 3: We ﬁrst assume that the keys are statically conﬁgured.
Under F L E XBU S, the architecture operates in a multiple bus mode
during intervals that exhibit localized communications, and hence enables concurrent processing of the WEP and FCS tasks. In intervals
that require low latency communications between components located
on different bus segments, a dynamic bridge by-pass mechanism temporarily transforms the architecture into a single shared bus topology.
The measured data rate under this architecture was 248 Mbps, a 23%
improvement over the best statically conﬁgured architecture.
The bridge by-pass mechanism provides a technique to make
coarse-grained (system level) changes to the communication architecture topology. However, at times, the ability to make more ﬁnegrained (component level) changes to the topology is also important,
as illustrated next.
Example 4: When the TKIP task is enabled, the F L E XBU S architecture with dynamic bridge by-pass achieves a data rate of 208 Mbps,
which is 18% better than the best conventional architecture. We next
consider the application of a dynamic component re-mapping capability. Using this technique, while the overall architecture remains in the
multiple bus conﬁguration, the mapping of the slave Frame buf2 is
dynamically switched between the two buses. It is mapped to AHB 2
as long as the FCS and WEP components are processing frame data,
and to AHB 1 at times when the most recently encrypted frame needs
to be efﬁciently transferred from Frame buf1 by the WEP task. Under this architecture, we observed a maximum data rate of 224 Mbps,
a 27% improvement over the best static architecture.
In summary, the above illustrations establish that by recognizing
dynamic variations in the spatial distribution of communication transactions, and correspondingly adapting the communication architecture topology (both at the system and component levels), large performance gains can be achieved. In the next two sections, we describe
how this principle is realized in the F L E XBU S architecture.
4. FLEXBUS ARCHITECTURE
F L E XBU S consists of a dynamically conﬁgurable communication
architecture topology. The techniques underlying F L E XBU S are independent of speciﬁc communication protocols, and hence can be applied to a variety of on-chip communication architectures.
In our
work, we demonstrate its application to the AMBA AHB [2], a popular
commercial on-chip bus. F L E XBU S provides two different topology
customization opportunities: (i) dynamic bridge by-pass (Section 4.1),
which enables system level customization through run-time fusing and
splitting of bus segments, and (ii) dynamic component re-mapping
(Section 4.2), which enables component level customization through
run-time switching of components from one bus segment to another.
The key technical challenges in developing F L E XBU S are: (i) maintaining compatibility with existing on-chip bus standards; (ii) minimizing timing impact; (iii) minimizing logic and wiring complexity
(hardware overhead); (iv) providing low reconﬁguration penalty. The
rest of this section provides details on how F L E XBU S provisions for
dynamic conﬁgurability keeping the above goals in mind.
4.1 Dynamic Bridge By-pass
Figure 2 shows the hardware required to support dynamic bridge
by-pass for a system consisting of two AMBA AHB bus segments,
connected by a bridge. AHB1 (primary bus) has two masters and one
slave, and AHB2 (secondary bus) has one master and one slave.
Operating Modes: The system can be operated in (i) multiple bus
mode and (ii) single shared bus mode, by disabling or enabling bridge
by-pass, respectively, via the conﬁg select signal. In the multiple bus
mode, the signals shown by the dotted arrows are inactive. The two
bus segments operate concurrently, with each arbiter resolving conﬂicts among the masters in its own bus segment. Transactions between
masters on AHB1 and slaves on AHB2 go through the bridge using the
conventions described in Section 2. In the single shared bus mode, the
bridge is “by-passed” by setting conﬁg select = 1, which activates the
signals shown by the dotted arrows. In this mode, the inputs of the
bridge master and slave interfaces are directly routed to the outputs,
by-passing the internal bridge logic (using multiplexers). This allows
reconfigure
OK
Reconfiguration 
Unit
reconfigure
OK
config_select
busreq_AHB2
lock_AHB2
grant_AHB2
Decoder1
select
BRG
Arbiter1
Arbiter2 
(Virtual 
Master)
busreq_BRG
lock_BRG
grant_BRG
busreq_M3
lock_M3
grant_M3
Decoder2
select_S2
address, 
control, wdata
ready, 
response, rdata
AHB1
address, 
control, wdata
Slv I/F Mst I/F
Bridge
ready
ready, 
response, rdata
AHB2
S2
M3
select_S1
grant_M2
busreq_M2
lock_M2
grant_M1
busreq_M1
lock_M1
M1
M2
S1
Figure 2: Dynamic bridge by-pass capability in F L E XBU S
transaction requests from masters on AHB1 to slaves on AHB2 (and
the corresponding slave responses) to reach within one clock cycle.
Arbitration: In the multiple bus mode, more than one master may
have transactions executing in parallel, whereas in the single bus
mode, only one master can be granted access to F L E XBU S at any given
time. Clearly, the arbitration mechanisms of the multiple bus mode
need to be adapted for the single bus mode. For this, we use a distributed arbitration mechanism in the single bus mode, in which one
of the arbiters behaves as a virtual master that is regulated by the other
arbiter (Figure 2). On receiving one or more bus requests from masters
on AHB2, Arbiter2 immediately sends a bus request to Arbiter1 using
the busreq AHB2 and lock AHB2 signals. Arbiter1 arbitrates among
the received bus requests from AHB1 masters as well as the virtual
master. In parallel, in order to reduce arbitration latency, Arbiter2 arbitrates among its received bus requests. However, it grants AHB2 to
its selected master only when it receives the grant AHB2 signal from
Arbiter1. This guarantees that only one master is granted access to
F L E XBU S when in the single bus mode. Finally, the ready bus signal,
which in the AMBA AHB bus protocol indicates the state of the bus to
all the components, is routed from AHB1 to AHB2 to ensure correct
system operation in the single bus mode.
Address Decoding: The address decoders on the two bus segments
require no change to enable dynamic bridge by-pass.
Run-time Conﬁguration: The Reconﬁguration Unit (Figure 2) is responsible for selecting the bus conﬁguration at run-time (using policies such as described in Section 5), and for ensuring correctness of
system operation while switching between the two conﬁgurations. It
initiates bus reconﬁguration by asserting the reconﬁgure signal to the
arbiters, which then terminate their current transactions, deassert all
grant signals, and assert the OK signal. On receiving the OK signal from both the arbiters, the Reconﬁguration Unit toggles the conﬁg select signal. The worst case overhead of bus reconﬁguration for
the two bus segment AMBA bus system is 17 cycles (assuming the bus
is not locked and all slaves have single cycle response).
4.2 Dynamic Component Re-mapping
Figure 3 shows a two bus segment AHB architecture, wherein master M2 and slave S2 can be dynamically re-mapped to AHB1 or AHB2.
Operating Modes: The mapping of M2 and S2 is selected by the signals, conﬁg select M2 and conﬁg select S2, respectively. The signals
of a re-mappable master or slave are connected to both AHB1 and
AHB2. However, the switch boxes, SWITCH M and SWITCH S, activate the signals to and from only one of the buses, depending on the
conﬁguration. Note that, only a subset of the master and slave signals
require to be switched in the AMBA AHB protocol.
Arbitration: The arbiters require no change, as master requests are
only sent to the arbiter on the bus to which they are currently mapped.
Address Decoding: The address decoders on the two bus segments
are reconﬁgured to generate the correct select signal for the remappable slave. For example, when S2 is mapped to AHB1, on observing an address belonging to S2 on the address bus, Decoder1 asserts the select S2 signal, while Decoder2 asserts the select BRG signal. The situation is reversed when S2 is mapped to AHB2. This is
done by switching the mapping of S2’s address space between BRG
and S2 in the decoder, based on the conﬁg select S2 signal.
Remap 
Unit
config_select_M2
M2
SWITCH_M
lock_M2
busreq_M2
grant_M2
lock_M2
busreq_M2
grant_M2
busreq_BRG
lock_BRG
grant_BRG
busreq_BRG
lock_BRG
grant_BRG
grant_M1
busreq_M1
lock_M1
M1
S1
Arbiter1
address, 
control, wdata
ready, 
response, rdata
select_S1
Decoder1
select
BRG
AHB1
select_S2
config_select_S2
Arbiter2
address, 
control, wdata
ready, 
response, rdata
grant_M3
busreq_M3
lock_M3
M3
S3
Decoder2
select_S3
AHB2
select
BRG
select_S2
SWITCH_S
Mst
I/F
Mst
I/F
Slv
I/F
Slv
I/F
BRG
S2
Figure 3: Dynamic component re-mapping capability in F L E XBU S
Run-time Conﬁguration: The master and slave conﬁguration select
signals are generated by the Remap Unit. In order to ensure correct
operation of the system during re-mapping, the Remap Unit monitors
the master’s busreq and slave’s select S2 signals to determine if they
are currently active on the bus. If not, the corresponding conﬁg select
signal is toggled. The rest of the bus operates without interruption.
5. DYNAMIC CONFIGURATION POLICIES
The problem of dynamically choosing the optimum conﬁguration
of the F L E XBU S architecture based on an observation of the characteristics of the communication trafﬁc can be addressed using several
approaches. In the past, similar problems in the domain of dynamic
power management have been addressed using stochastic policies [13]
and history-based heuristic techniques [14]. In our work, we examine
a history-based technique in detail.
Let us consider a F L E XBU S system featuring dynamic bridge bypass between two bus segments, BUS1 and BUS2. Let NBU S1 , NBU S2
and NBRG represent the number of local transactions on BUS1, number of local transactions on BUS2 and number of transactions between
the two bus segments, respectively, during an observation interval. A
transaction refers to one bus access (e.g., a burst of 5 beats constitutes
gle bus mode, TSingl e , is (NBU S1 + NBU S2 + NBRG ) × CL × tSB , where
5 transactions). The time taken to process this trafﬁc under the sinCL is the average number of cycles for a local bus transaction, and
tSB is the clock period in the single bus mode, since all transactions
are on the same bus. Similarly, the time taken under the multiple
bus mode, TMul t i pl e , is approximated by max(NBU S1 , NBU S2 ) × CL ×
tMB + NBRG × CB × tMB , where CB is the average number of cycles
for a cross-bridge transaction, and tMB is the clock time period in the
multiple bus mode. If TSingl e > TMul t i pl e , then the single bus mode is
preferred, else the multiple bus mode is better. At run-time, the system
observes the two bus segments over a time period T P and records the
number of bus transactions of each type. At the end of the time period,
it selects the new conﬁguration based on the above criterion.
The choice of an appropriate conﬁguration time period, T P, is crucial. Smaller time periods enable the policy to be more responsive to
variations in the trafﬁc characteristics. However, if the trafﬁc proﬁle
changes rapidly, this might lead to excessive oscillations between the
conﬁgurations, thus potentially degrading the performance due to the
reconﬁguration overhead. Therefore, we propose the use of an adaptive time period, T P, which is selected as follows. Let C denote the
number of conﬁgurations of the bus over τ clock cycles. If C/τ > λ1 ,
then the time period, T P, is doubled, if C/τ < λ2 , then T P is halved,
else it is unchanged. λ1 and λ2 represent two thresholds, and depend
on the reconﬁguration overhead. In our experiments, we observed an
average reconﬁguration time of 10 cycles, for which a τ value of 250
cycles, a λ1 value of 0.0025, and a λ2 value of 0.001 proved effective. In general, these parameters should be carefully set, based on an
analysis of the trafﬁc characteristics of the application.
Similar policies can also be designed for dynamic component remapping by monitoring, for each re-mappable master and slave, the
fraction of trafﬁc to or from components on the different bus segments.
Bus Architecture
Single Shared Bus
Multiple Bus
FLEXBUS (single bus mode)
FLEXBUS (multiple bus mode)
Area   (sq. mm)
82.12
84.27
82.66
Delay (ns)
4.59
3.79
4.72
3.93
Frequency (MHz)
218
264
212
254
Figure 4: F L E XBU S hardware implementation results
6. EXPERIMENTAL RESULTS
In this section, we present experimental studies that evaluate the
hardware implementation of F L E XBU S, followed by an analysis of its
performance using (a) synthetic trafﬁc proﬁles, and (b) trafﬁc generated by the IEEE 802.11 MAC processor.
F L E XBU S was implemented by enhancing reference AMBA AHB
RT-level implementations available in the Synopsys Designware library [15]. For the synthetic workload experiments, we used a system consisting of eight masters and eight slaves, equipped with programmable VERA bus-functional models [15] for trafﬁc generation.
The MAC processor was implemented using an instruction set model
for the ARM processor, and Verilog for the remaining hardware. Performance results were obtained using ModelSim [16] simulations. For
accurate chip-level area and delay comparison, we generated ﬂoorplans of all the systems [17] for a commercial 0.13µm technology [18].
The ﬂoorplanner was modiﬁed to report global wirelengths for different communication architectures. Global wire delays were calculated
assuming delay optimal buffer insertion [19] and Metal 6 wiring. The
designs were annotated with these wire delays and Synopsys Design
Compiler [20] was used for delay estimation.
The area and timing analysis methodology described above was applied to the example eight master and eight slave system under (i)
F L E XBU S with dynamic bridge by-pass, (ii) single shared bus, and (iii)
multiple bus architectures. Figure 4 shows the results of these studies.
For the F L E XBU S, the critical path delay in the multiple bus mode is
smaller than in the single bus mode since many long paths of the single
bus mode are false paths in the multiple bus mode. The static multiple
bus architecture has smaller delay than the single shared bus due to
shorter wirelengths and lesser bus loading. F L E XBU S incurs a small
delay penalty (on average 3.2%) compared to the statically conﬁgured
architectures, due to extra wiring and logic delay.
To analyze F L E XBU S’s performance under synthetic trafﬁc proﬁles,
we generated trafﬁc proﬁles using a two-state Markov model, where
each state corresponds to either local trafﬁc (i.e., trafﬁc on the same
bus segment) or cross-bridge trafﬁc (i.e., trafﬁc between different bus
segments). Varying the state transition probabilities allowed us to vary
the granularity with which the two trafﬁc types are interleaved. Figure 5(a) shows a representative trafﬁc proﬁle consisting of a mix of
local and cross-bridge trafﬁc. Figure 5(b) shows the run-time conﬁguration decisions taken by the policy (Section 5). Figure 5(c) plots
the cumulative latency for the different architectures. We observe that
F L E XBU S successfully adapts to frequent changes in the trafﬁc characteristics, achieving signiﬁcant performance beneﬁts over the static
single shared bus (21.3%) and multiple bus (17.5%) architectures.
Finally, we examine the performance of F L E XBU S and conventional architectures for the IEEE 802.11 MAC processor described
in Section 3. We considered two variants of F L E XBU S, featuring
(i) dynamic bridge by-pass, and (ii) dynamic component re-mapping
(Frame buf2 is the re-mappable slave). The policy described in Section 5 is used to select the bus conﬁguration. All the buses were operated at 200 MHz. Table 1 shows the average time taken to process a
single frame (of size 1 KB) under the different bus architectures. From
the table, we see that the times required by both variants of F L E XBU S
are signiﬁcantly smaller compared to the conventional architectures.
The data rate increase due to F L E XBU S over the single shared bus is
31.5% and over the multiple bus is 23%. The table also shows the
upper bound on performance, obtained using an ideal reconﬁgurable
bus (no reconﬁguration overhead) with an ideal reconﬁguration policy
(full knowledge of future bus trafﬁc). We observe that for this system,
F L E XBU S and its associated policies perform close to the ideal case.
7. CONCLUSIONS
In this paper, we presented F L E XBU S, a novel dynamically conﬁgurable on-chip communication architecture, and described techniques
for efﬁciently adapting F L E XBU S at run-time. In future work, we plan
to extend F L E XBU S to more complex communication architectures.
Cross 
Bridge
Traffic
Local 
Traffic
0
Single 
Bus 
Mode
Multiple 
Bus 
Mode
0
35
30
25
20
15
10
5
0
)
s
u
(
y
c
n
e
t
a
L
e
v
i
t
l
a
u
m
u
C
0
500
1000
1500
2000
2500
3000
Amount of Data Generated (Bytes)
(a) Example trafﬁc proﬁle
500
1000
1500
2000
2500
3000
Amount of Data Transferred (Bytes)
(b) Bus conﬁgurations selected by the policy
Single Shared Bus
Multiple Bus
FLEXBUS
500
1000
1500
2000
2500
3000
Amount of Data Transferred (Bytes)
(c) Cumulative latency under different bus architectures
Figure 5: F L E XBU S performance under synthetic trafﬁc proﬁles
Table 1: Performance of the IEEE 802.11 MAC processor under
different communication architectures
Bus Architecture
Single Shared Bus
Multiple Bus
FLEXBUS (Bridge By-pass)
FLEXBUS (Comp. Re-mapping)
Ideally Reconfigurable Bus
Computation Time (ns)
42480
26905
27025
27010
26905
Data Transfer Time (ns)
12800
5290
5270
5120
Total Time (ns)
42480
39705
32315
32280
32025
8. "
A low-cost conflict-free NoC for GPGPUs.,"As integrated circuits are limited by hardware resources, reducing cost while maintaining the performance becomes especially important. In this article, we propose a conflict-free NoC (cfNoC) for the GPGPU request network. The cfNoC eliminates (i) conflicts among different columns by deploying an exclusive subnet for each column, and (ii) conflicts inside the same column by using a token-based mechanism. The elimination of conflicts allows cfNoC to exploit different subnet channel widths to maintain the performance while reducing cost. Compared with a baseline mesh with 1 VC, our work reduces request network area by 22.6% and power by 23.7%. With 2 VCs, cfNoC achieves more than 37.6% power reduction compared to the baseline and the checkboard placement/routing (CP) design. All these benefits do not cause any performance loss.","A Low-Cost Conﬂict-Free NoC for GPGPUs∗
Xia Zhao1,2 , Sheng Ma2†
, Yuxi Liu1,3 , Lieven Eeckhout1 , and Zhiying Wang2
1ELIS Department, Ghent University, {xia.zhao, lieven.eeckhout}@ugent.be
2College of Computer, National University of Defense Technology, {masheng, zywang}@nudt.edu.cn
3School of EECS, Peking University, {yuxi.liu}@pku.edu.cn
ABSTRACT
As integrated circuits are limited by hardware resources,
reducing cost while maintaining the performance becomes
especially important. In this article, we propose a conﬂictfree NoC (cfNoC) for the GPGPU request network. The
cfNoC eliminates (i) conﬂicts among diﬀerent columns by
deploying an exclusive subnet for each column, and (ii) conﬂicts inside the same column by using a token-based mechanism. The elimination of conﬂicts allows cfNoC to exploit
diﬀerent subnet channel widths to maintain the performance
while reducing cost. Compared with a baseline mesh with
1 VC, our work reduces request network area by 22.6% and
power by 23.7%. With 2 VCs, cfNoC achieves more than
37.6% power reduction compared to the baseline and the
checkboard placement/routing (CP) design. All these beneﬁts do not cause any performance loss.
CCS Concepts
•Computer systems organization → Interconnection architectures;
Keywords
GPGPU, NoC, Low-Cost, Conﬂict-Free
1.
Introduction
General-Purpose Graphics Processing Units (GPGPUs)
are gaining popularity for computing as the data-parallel
paradigm allows thousands of active threads to execute in
parallel. To continuously improve thread-level parallelism,
more and more processing units are integrated in GPGPUs
and a scalable Network-on-Chip (NoC) plays an important
role for achieving eﬃcient on-chip communication [4, 12, 14].
NoC cost is a key focus point for chip designers. Several prototype chips show that NoCs consume a substantial
portion of the system area and power [6, 10]. A large portion of hardware cost comes from the components related to
conﬂict processing. Since NoC resources, such as crossbars
and channels, are shared by all packets, diﬀerent packets
∗
This work is supported in part by the National Natural Science Foundation of China (no.61572508, 61272144, 61303065),
National High-Tech Research and Development Plan of China
(no.2012AA010905), the European Research Council under the European Community’s Seventh Framework Programme (FP7/20072013)/ERC grant agreement no.259295.
Corresponding Author
†
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC ’16, June 05-09, 2016, Austin, TX, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4236-0/16/06. . . $15.00
DOI: http://dx.doi.org/10.1145/2897937.2897963
Figure 1: NoC power comparison between two networks, a
8×8 baseline mesh network with 1VC.
will compete for the same resources, resulting in conﬂicts.
Traditional NoCs typically use input buﬀers, a virtual channel (VC) allocator and switch allocator to process conﬂicts.
These conﬂict processing related components not only complicate the router architecture and increase the latency, they
also induce a signiﬁcant area and power cost [13, 21].
Recent research proposes low-cost router architectures [15],
leverages power-gating [8], or uses emerging memory technologies in input buﬀers [21] to reduce the network area/power
cost. These low-cost NoC designs are fairly mature for chipmultiprocessor (CMP) systems. Unfortunately, the NoC design for GPGPUs is still in its infancy and fully utilizing
GPGPU characteristics can bring large beneﬁts.
The GPGPU has unique traﬃc patterns, called manyto-few-to-many, where the communication only exists between compute nodes and memory nodes [4]. The GPGPU
NoC consists of a request and a reply network. The request
network transfers request packets, including write and read
requests from many compute nodes to few memory nodes,
while the reply network transfers write and read replies in
the opposite direction. The GPGPU NoC can be optimized
by leveraging this unique traﬃc pattern. Previous designs
use a simpliﬁed crossbar to reduce cost [4] or use an asymmetric VC partition to improve performance [12]. However, these designs still rely on the traditional router components to process conﬂicts and this incurs signiﬁcant costs.
To eliminate conﬂicts, an existing design assigns each memory node a channel-sliced network [14]. However, it focuses
on the reply network and ignores the request network.
We observe that the request network is more suitable to
be simpliﬁed to reduce cost without performance loss. Since
a signiﬁcant portion of memory accesses are read requests
and they carry short data blocks, the request network experiences less traﬃc [12] and is often not the bottleneck [14];
hence simplifying will not bring many performance losses.
Moreover, as static power represents a large portion of the
total NoC power even at high activity factors [21], the request network consumes almost the same power as the reply
network, as shown in Fig. 1. Unfortunately, in contrast to
the reply network, many compute nodes send packets to few
memory nodes in the request network. Eliminating conﬂicts
through the NoC. The memory controllers may exploit a
top-bottom placement [12] or a checkboard placement [4].
Our design supports both types of placement, but we focus
on the checkboard placement since previous research shows
that this placement achieves good performance [3, 4]. The
GPGPU has two traﬃc classes, request and reply traﬃc.
Therefore, two separate networks, the request and the reply
network, are used to avoid protocol-level deadlock [4, 14].
The buﬀerless NoC removes router buﬀers to optimize
power and area [17]. Kim et al. [9] leverage the buﬀerless design into the GPGPU NoC by proposing clumsy ﬂow control,
which reduces the amount of packet deﬂection by throttling
network traﬃc. Although the buﬀerless design removes input buﬀers to reduce cost, the conﬂicts still exist, and complex schemes, such as packet dropping or deﬂection routing,
are used to process conﬂicts. These schemes may signiﬁcantly degrade performance and reduce the beneﬁts of buﬀerless
design [16]. By leveraging GPGPU traﬃc characteristics to
eliminate conﬂicts, our proposed cfNoC does not need packet dropping or deﬂection routing.
It signiﬁcantly reduces
hardware cost while maintaining performance.
Bakhoda et al. [4] propose the checkboard placement/
routing (CP) design which exploits a checkboard routing and
reduces the area by simplifying the crossbar. The bandwidtheﬃcient NoC design [12] combines reply and request network
into one physical network and uses asymmetric VC partitions to assign more VCs to reply packets as the reply traﬃc
generally requires much more bandwidth. Compared to our
cfNoC, these designs do not eliminate conﬂicts, so the input
buﬀers, VC allocator and switch allocator are still needed to
process conﬂicts, which induces huge hardware overheads.
Based on the characteristics in the reply network, Kim et
al. [14] propose the DA2mesh network. Only memory nodes
inject packets in the reply network; DA2mesh is conﬂict-free
by assigning each memory node a dedicated channel-sliced
network. However, it does not consider the request network.
In fact, the request network is more complicated because
more compute nodes inject packets; it is nearly impossible
to assign each compute node a channel-sliced network.
3. The cfNoC Architecture
cfNoC simpliﬁes the router by eliminating conﬂicts while
maintaining performance. To achieve this, cfNoC exploits
three approaches. First, we propose the column-independent
token mesh design to eliminate conﬂicts and to simplify the
router. Second, we design the end-to-end ﬂow control to
solve the overﬂow problem at ejection ports. Third, we use
a multiple-register-group structure to compensate the performance loss caused by the token-based mechanism.
3.1 System overview
We construct cfNoC on the basis of the mesh network.
The cfNoC has several exclusive networks (Enetwork), and
each Enetwork is exclusively used by compute nodes of one
column. In other words, the number of Enetworks equals to
the number of columns in the network.
Fig. 3 shows the cfNoC architecture for a 4×4 network.
The cfNoC consists of four Enetworks. Each Enetwork consists of a column-independent token mesh network and a
backpressure network. The column-independent token mesh
network is composed of an extended mesh subnet and a token network. Take Enetwork0 as an example. The ﬁrst
column of compute nodes uses Enetwork0. The extended
Figure 2: The baseline architecture of GPGPU NoC. White
nodes are compute nodes, and gray nodes are memory nodes.
between so many compute nodes is much more diﬃcult.
In this paper, we propose a conﬂict-free NoC (cfNoC) design for the GPGPU request network. More speciﬁcally,
cfNoC consists of three approaches. First, we propose a
column-independent token mesh design to eliminate conﬂicts.
The column-independent token mesh design uses exclusive
subnets to eliminate conﬂicts among diﬀerent columns and
uses a token-based mechanism to eliminate conﬂicts in the
same column. This conﬂict-free network removes traditional
conﬂict processing related components to simplify the router
architecture. By simplifying the router, cfNoC can choose
diﬀerent subnet channel widths to maintain the performance
while reducing hardware cost. However, the removal of the
buﬀered ﬂow control causes ejection overﬂow at memory
nodes and using tokens to eliminate conﬂicts exacerbates
the Head-of-Line (HoL) blocking at the injection network
interface (NI). Second, we design low-cost end-to-end ﬂow
control to make compute nodes aware of the status of memory nodes. The compute node will not send packets when
the memory node’s ejection buﬀer is near overﬂow. Third,
we design a simple multiple-register-group structure for the
ma jority of read packets. Each register group stores a short
read packet. When the compute node cannot send the head
packet in the injection queue, it can send read packets stored
in register groups to mitigate HoL blocking.
In summary, this work makes the following contributions:
• To the best of our knowledge, this work is ﬁrst to focus
on low-cost request network in GPGPUs. We notice
that the request network is suitable for simpliﬁcation.
• We propose the cfNoC design which uses the columnindependent token mesh to eliminate conﬂicts, exploits
the end-to-end ﬂow control to avoid ejection overﬂow,
and constructs the multiple-register-group structure to
mitigate HoL blocking.
• Due to the use of simpliﬁed routers, cfNoC can choose
diﬀerent subnet channel widths to maintain the performance while reducing hardware cost. Evaluation
results show that compared to the baseline mesh with
1 VC, our work reduces request network area by 22.6%
and reduces power by 23.7%. Compared to the baseline and checkboard placement/routing (CP) with 2
VCs, cfNoC achieves more than 37.6% power reduction. All beneﬁts do not cause any performance loss.
2. Background
As shown in Fig. 2, the GPGPU has a large number
of compute nodes which communicate with memory nodes
Figure 3: The overall structure of cfNoC design for a 4×4
mesh. White nodes are compute nodes, and gray nodes are
memory nodes. Light nodes are the nodes that do not use
this Enetwork.
mesh subnet0 transfers request packets from compute nodes
at the ﬁrst column to four memory nodes. The token ring
network0 passes tokens between compute nodes in the ﬁrst
column and we set four tokens which represent four memory
nodes respectively. The token prevents compute nodes at
the same column from sending packets to the same memory
node simultaneously. The backpressure network0 broadcasts
the status of memory nodes to compute nodes in the ﬁrst
column. The status indicates the occupancy of the ejection
buﬀer. We next describe the structure of these networks.
3.2 Column independent token mesh
This subsection proposes the column independent token
mesh to eliminate conﬂicts to achieve a simpliﬁed router.
The column independent token mesh consists of two structures, an extended mesh subnet and a token ring network.
The extended mesh avoids conﬂicts among compute nodes
across diﬀerent columns. The token ring network avoids
conﬂicts among compute nodes within the same column.
The mesh network is extended into multiple independent
extended subnets like previous work [8] and each column of
compute nodes uses an extended mesh subnet to connect
with memory nodes. As shown in Fig. 3a, the ﬁrst column
of compute nodes uses the extended mesh subnet0. Diﬀerent columns are only allowed to send packets through their
private extended subnet, so there is no conﬂict among them.
We use a token-based mechanism to avoid conﬂicts within the same column. It prevents compute nodes within the
same column from sending packets to the same memory node
simultaneously. For example, in Fig. 3b, there are four tokens which represent the four memory nodes respectively.
When a compute node, C0, C1 or C2, wants to send packets
to a memory node, it must get the corresponding token from
the token network0 and hold it during the sending process.
Thus, other compute nodes cannot send packets to the memory node until the current sender releases the token. After
injecting the tail ﬂit, the sender will release the token after
an additional cycle to oﬀer the tail ﬂit enough time to arrive
at the destination without any conﬂict. As shown later, this
conﬂict-free NoC allows the use of a simpliﬁed router which
achieves a single-cycle per-hop latency.
We now prove that cfNoC eliminates conﬂicts. By using a
Figure 4: The cfNoC router architecture.
dedicated extended mesh subnet for each column, there are
no conﬂicts between columns. When compute nodes at the
same column send packets to diﬀerent memory nodes with
XY routing, their routing paths have no shared segments
and hence there are no conﬂicts. For example, in Fig. 3a,
C1 sends packets to M2 and C2 sends packets to M1. Their
routing paths (the red lines and green lines) have no shared
segments. We only need to prove that by using the tokenbased mechanism, there is no conﬂict when compute nodes
in the same column send packets to the same memory node.
Proof. Suppose compute nodes C1 (x, y1 ) and C2 (x, y2 )
send packets to the memory node M1 (xm , ym ). If node C1
holds the token and begins to inject a packet which has
m ﬂits at cycle (0), its ﬂits will arrive at the destination
between cycle (|xm − x| + |ym − y1 |) and cycle (|xm − x| +
|ym − y1 | + m). After C1 releases the token, C2 will get
it no earlier than cycle (m + 1 + |y2 − y1 |), and the head
ﬂit of its packet would arrive at the destination at cycle
(m + 1 + |y2 − y1 | + |xm − x| + |ym − y2 |). Because |y2 − y1 |+
|ym − y2 | + 1 > |ym − y1 |, the tail ﬂit of packet from C1 will
arrive at the destination earlier than the head ﬂit of packet
from C2. Thus, there is no conﬂict.
Our proposed design achieves a conﬂict-free network, which
enables the design of the simpliﬁed router architecture as
shown in Fig. 4. This router removes input buﬀers, the VC
allocator and switch allocator from the traditional router.
Besides, our design also simpliﬁes the switch architecture to
remove useless turns, such as y-to-x, in cfNoC. The critical
path of our router consists of the register-read, a 2-to-1 multiplexer, a 3-to-1 multiplexer, the link traversal and ﬁnally
the register-write; a single-cycle per-hop latency is achieved.
3.3 End-to-end ﬂow control
In each Enetwork, cfNoC must handle the overﬂow where
a ﬂit arrives at the destination with a full ejection buﬀer.
The end-to-end ﬂow control prevents compute nodes from
sending packets to the memory node when its ejection buﬀer
is near overﬂow. As mentioned in Section 3.2, the sender
must get the corresponding token before sending packets.
When the senders get the overﬂow status of a memory node,
they are not allowed to acquire the corresponding token.
If a sender has held the token and is sending packets to
the memory node, it will stop sending and keep holding the
token. After getting the free status, the senders are allowed
to get the corresponding token again. The sender which
holds the token will continue sending the remaining ﬂits.
The backpressure network in each Enetwork broadcasts
the status of memory nodes to compute nodes. Fig. 5 shows
the backpressure network0 from the Fig. 3c in more details.
For each memory node, we connect several 1-bit registers
into a chain to broadcast status (‘1’ is overﬂow status and
‘0’ is free status) to the compute nodes in the ﬁrst column.
Table 1: Baseline conﬁguration of GPGPU-sim.
Parameters
Number of clusters
Number of Core/Cluster
Number of Threads/Core
L1 Data Cache/Core
Memory Controllers
L2 Cache/MC
Memory Scheduler
Topology
Routing Function
Channel Width
Router Microarchitecture
Buffer Configuration
Value
56
1
1536
16KB
8
64KB
FR-FCFS
2D Mesh
XY routing
128bits
4-stage router
1VC 4flit/VC
Packet Size
8byte/Read 136byte/Write
Injection/Ejection buffers
18Flit
Table 2: Conﬁguration of the evaluation NoCs.
Design
Description
Baseline
2D mesh network with 128bit channel width, 18flit ejection buffer
cfNoC16
REQ: 8 Enetworks with 16bit channel width, 132flit ejection buffer
cfNoC24
REQ: 8 Enetworks with 24bit channel width, 88flit ejection buffer
hancement. Fig. 6b shows the new architecture. In C2, the
read packets, such as Packet, B and C are preferentially inserted into the register groups. When there is no available
token for the head packet, Packet A, in the FIFO injection
queue, C2 will check tokens for packets in the register groups. Packet B’s destination is M1 and its corresponding token
is available. C2 will get the token and send Packet B to M1.
We set four groups and the cost of these registers is small.
3.5 Scalability Discussion
At last, we discuss the scalability of cfNoC. As we put
diﬀerent memory nodes on diﬀerent columns to avoid conﬂicts, it will not work if the number of memory nodes is
larger than the number of columns. In this case, we can put
two or more memory nodes on the same column and use one
token to represent these memory nodes.
In this way, the
token can still assure that there is no conﬂict although it
may limit the performance. Besides, due to pin limitations,
it is uncommon that the number of memory nodes is larger
than the number of columns. For example, TILE64 and Intel SCC exploit a 8×8 and 6×4 mesh network, respectively,
and both have 4 memory controllers [11, 20].
4. Evaluation
We modify GPGPU-sim v3.2.2 [5] to evaluate cfNoC. The
conﬁguration of GPGPU-sim is listed in Table 1 which is
similar to previous work [9, 14]. An 8×8 mesh network is
used as the baseline. CfNoC constructs the request network
by using eight Enetworks and keeps the reply network unchanged. To study various bandwidth settings for cfNoC,
we use cfNoC16 and cfNoC24 to represent diﬀerent subnet
channel widths of the Enetwork. A summary of the diﬀerent designs is shown in Table 2. The total bandwidth of
cfNoC16 is 128 bits, which is the same as for the baseline
design. The bandwidth of cfNoC24 is 1.5X the bandwidth of
cfNoC16, and cfNoC24 uses a shallower injection buﬀer and
a shallower ejection buﬀer to keep the NI area unchanged.
The used workloads are listed in Table 3 along with their
injection rates, and the percentage of write packets. We
use DSENT [18] to evaluate area and power overheads with
32nm technology.
4.1 Evaluation with 1 VC
Fig. 7a shows the performance comparison with results
normalized to the baseline design. Although cfNoC16 keeps
Figure 5: The backpressure network0.
(a) One injection queue.
(b) One injection queue (IQ)
and Two register groups.
Figure 6: The use of multiple register groups.
Take M2 as an example, when its buﬀer usage in Enetwork0
is near the threshold, M2 sets the nearest register to ‘1’
and the status will be broadcast to the compute nodes after
several cycles. When its buﬀer usage is below the threshold,
M2 sets the free status.
In the extreme case, the sender at the left bottom corner keeps sending packets to the receiver at the top right
corner. Considering the delay of the status transferring, to
cover this extreme case, the threshold value is equal to the
ejection buﬀer amount minus four times the node count per
dimension. To enhance performance, we use a deep ejection
buﬀer and evaluate its cost comprehensively in Section 4.
3.4 Multiple-register-group structure
The token-based mechanism exacerbates HoL blocking at
the sender’s NI. For example, in Fig. 6a, C2 wants to send
Packet A which locates at the head of injection queue to
M2. However, as C0 is holding the corresponding token, the
injection process in C2 is blocked even though some packets
behind the queue head, such as Packet B, are sent to other
available memory nodes. To solve this problem, we design
a simple multiple-register-group structure. In addition to a
normal FIFO injection queue, the NI conﬁgures several register groups, and each group can store a read packet. Unlike
conventional VCs which can store variable length packets,
the multiple-register-group structure is designed speciﬁcally
for read requests of small sizes. Thus, it has a low ﬁxed
hardware overhead. As the read packets are the ma jority of
request packets, our design gets suﬃcient performance enTable 3: Benchmarks.
Benchmark
Write Percentage
Injection Rate
(Flits per Cycle)
AES[5]
BACKPROP[7]
BFS[7]
CUTCP[2]
FFT[2]
GAUSSIAN[7]
HOTSPOT[7]
KMEANS[7]
LPS[5]
LUD[7]
MM[2]
MONTECARLO[1]
MRI-Q[2]
NN[7]
NW[7]
PATHFINDER[7]
REDUCTION[1]
SPMV[2]
STO[5]
0.229
0.554
0.318
0.092
0.666
0.250
0.239
0.013
0.373
0.258
0.103
0.019
0.283
0.029
0.288
0.077
0.001
0.029
0.550
0.020
0.073
0.023
0.0014
0.112
0.0001
0.027
0.0038
0.047
0.004
0.004
0.013
0.001
0.004
0.001
0.017
0.011
0.013
0.012
(a) IPC Comparison.
(b) NoC Area Comparison.
(c) NoC Power Comparison.
(d) NoC Energy Comparison.
Figure 7: Performance and cost comparison with 1 VC.
the bandwidth unchanged and simpliﬁes the router architecture by eliminating conﬂicts, it achieves similar performance compared to the baseline. After increasing subnet
channel width in cfNoC24, for some applications with high
percentages of write packets and high injection rates such
as FFT, its performance increases signiﬁcantly. However, as
the request network is not on the critical path for performance [14], cfNoC24 increases the performance by 2.3% on
average.
Fig. 7b shows the area cost of the request network. The
total area of cfNoC16 is 22.6% smaller than for the baseline design. The main contributors for this area reduction
are the input buﬀers, allocators, and crossbar. Input buﬀers
and allocators are all removed in cfNoC16 since there are
no conﬂicts. Using smaller crossbars (16-bit) can theoretically reduce the total crossbar area by 7/8 according to the
area formula O(p2w2 ) where p is the port count and w is
the channel width [4]. The NI area of cfNoC16 is larger
than for the baseline, because memory nodes use 132-ﬂit
buﬀers, and compute nodes exploit multiple-register-group
structures. Yet, since there are few memory nodes and the
register cost is low, the NI area only increases to 19.5% of
the area of the baseline. The token network and the backpressure network of cfNoC16 are both constructed by using
simple registers and wires and they only consume 1.8% and
1.2% of the total area of the baseline, respectively.
Compared to cfNoC16, since the token network and the
backpressure network have no relation with the subnet channel width, using 1.5X bandwidth in cfNoC24 only increases
its crossbar area and link area. The cfNoC24 increases its
link area to 150% of the link area of cfNoC16. This causes
its area to be 27.4% higher than the cfNoC16 as the link
area occupies a large portion of the total area.
We compare the power and energy consumption of the
request network in Fig. 7c and Fig. 7d. Total power consumption consists of the static and dynamic power. As
the static power occupies a large portion even for high activity factors and the static power is dominated by input
buﬀers [21], using 1.5X bandwidth in cfNoC24 does not incur large power overheads. Considering beneﬁts of removing input buﬀers and extra power costs of our NI design,
cfNoC16 and cfNoC24 achieve a 23.7% and 18.6% power reduction compared to the baseline, respectively. Considering
all workloads, the energy reduction of cfNoC16 and cfNoC24
equals 23.9% and 20.6%, respectively.
4.2 Evaluation with 2 VCs
In this subsection, we compare cfNoC to CP [4], which uses half-routers to reduce cost. As CP needs 2 VCs to avoid
deadlock, we use 2 VCs and set 4 ﬂits per VC. We also compare cfNoC with the baseline design of 2 VCs. Although CP
can be used in the request network and the reply network,
we only exploit CP in the request network, since we focus on
the request network in this article and the baseline/cfNoC
can also exploit CP in their reply networks.
Fig. 8a shows the performance results. Compared to the
baseline, CP has a slight performance loss due to the checkboard routing. For some applications such as FFT, cfNoC16
degrades performance. As these applications have high percentages of write packets and high injection rates, they rely
on 2 VCs in the request network to maximize performance.
In some cases, the unbalanced traﬃc in diﬀerent Enetworks
and the token-based mechanism also slightly degrades performance. Although cfNoC16 only degrades performance
by 4.6% on average, it still loses its energy beneﬁts as the
performance loss may increase the whole chip energy. Forinput buﬀers occupy a larger portion of the NoC power [21].
To maintain performance, cfNoC16 and cfNoC24 are the
best choices to replace the designs with 1VC and 2VCs, respectively.
If necessary, the energy advantages still allow
cfNoC to exploit much higher channel width and this more
suits the dark silicon era where the energy eﬃciency becomes
more and more important [19].
5. Conclusions
The GPGPU request network is not the performance bottleneck and it consumes almost the same hardware cost as
the reply network. It is therefore more suitable to be simpliﬁed to reduce hardware cost without inducing performance
loss. In this paper, we propose a conﬂict-free NoC (cfNoC)
for request packets in GPGPUs. Due to the use of the simpliﬁed router, cfNoC can choose diﬀerent subnet channel
widths to maintain the performance while reducing hardware cost. We use two diﬀerent subnet channel widths in the
evaluation and mark them as cfNoC16 and cfNoC24, respectively. Compared to the baseline of 1 VC, cfNoC16 maintains the performance while only using 76.3% power and
consuming 77.4% area. For 2VCs in the baseline and CP,
cfNoC24 can still keep the performance unchanged and reduce power consumption by 40.1% and 37.6%, respectively.
6. "
Notifying memories - a case-study on data-flow applications with NoC interfaces implementation.,"NoC-based architectures overcome the limitations of traditional buses by exploiting parallelism and offer large bandwidths. NoC adoption also increases communication latency, which is especially penalising for data-flow applications (DF). We introduce the notifying memories (NM) concept to reduce this overhead. Our original approach eliminates useless memory requests. This paper demonstrates NM in the context of video coding applications implemented with dynamic DF. We have conducted cycle accurate systemC simulation of the NoC on an MPEG4 decoder to evaluate NM efficiency. The results show significant reductions in terms of latency (78%), injection rate (60%), and power savings (49%) along with throughput improvement (16%).","Notifying Memories: a case-study on Data-Flow
Applications with NoC Interfaces Implementation
Kevin J. M. Mar tin1 , Mostafa Rizk1 , Mar tha Johanna Sepulveda2 , Jean-Philippe Diguet1
1Univ. Bretagne-Sud, CNRS UMR 6285, Lab-STICC, F-56100 Lorient, France
2 Institute for Security in Information Technology, Technical University of Munich, Germany
{kevin.mar tin,mostafa.rizk,jean-philippe.diguet}@univ-ubs.fr,
johanna.sepulveda@tum.de
ABSTRACT
NoC-based architectures overcome the limitations of traditional buses by exploiting parallelism and oﬀer large bandwidths. NoC adoption also increases communication latency, which is especially penalising for data-ﬂow applications (DF). We introduce the notifying memories (NM) concept to reduce this overhead. Our original approach eliminates useless memory requests. This paper demonstrates
NM in the context of video coding applications implemented
with dynamic DF. We have conducted cycle accurate systemC simulation of the NoC on an MPEG4 decoder to evaluate NM eﬃciency. The results show signiﬁcant reductions
in terms of latency (78%), injection rate (60%), and power
savings (49%) along with throughput improvement (16%).
CCS Concepts
•Networks → Network on chip; •Theory of computation → Streaming models; •Hardware → Buses and highspeed links;
Keywords
NoC based architecture; data-ﬂow; memory
1.
INTRODUCTION
Considering the evolution towards manycore and multiprocessor architectures in HPC and embedded systems, the
interconnect network has a strong impact on performance
and energy eﬃciency. On the software side, we observe a renewed interest for dataﬂow (DF) programming models that
oﬀer clear design guidelines to deal with application complexity and scalability.
It allows for explicitly specifying
both spatial and temporal parallelism of an application. DF
programming is also a very convenient approach to manage the evolution of standards based on a large set of reused
functions and to generate correct-by-construction code. One
of the side eﬀects of data-ﬂow applications is the overhead of
memory accesses. DF actors (data-ﬂow nodes) must check
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC ’16, June 05-09, 2016, Austin, TX, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4236-0/16/06. . . $15.00
DOI: http://dx.doi.org/10.1145/2897937.2898051
ﬁring rules including availability of input data and output
buﬀer space. This penalty is even more important for dynamic actors, which are unavoidable in real life applications.
Networks-on-Chips (NoCs) implement concurrent communications, increasing the bandwidth and taking advantage of
parallelism at the actor level. However, NoCs also increase
communication latency, penalising DF applications which
rely on a large number of requests to memories for ﬁring
rule validation. Some recent work has addressed the problem of NoC latency. It relies on path or time slot allocation
and processor or buﬀer mapping to minimise read and write
delays, but it is still based on the fact that memories are
slaves [4]. The idea of smart or active memory aims also
to reduce the number of transactions and improve memory access eﬃciency, by moving part of computations to the
memory side [1, 11].
In this work, we propose to address the question with a totally new approach that transforms memories into masters,
able to initiate transfers by means of notiﬁcations when data
is ready, so to get rid of useless accesses. We call this concept Notifying Memories (NM). The immediate consequence
of a new balance between memory and processors is the reduction of the number of transactions and injection rates.
It provides memories with notiﬁcation and processors with
listening mechanisms, which are conceptually close to the
observer design pattern, used in software engineering. They
are implemented in the network interface (NI). Our solution
is thus independent from memory, processor architectures
and NoC parameters. Moreover, the notifying memories
perfectly match with the DF models. In real-life DF applications, both static and dynamic actors are required. The
uncertainty of computing due to data-dependency prevents
from any static scheduling. We need to demonstrate the efﬁciency of the approach with real application execution, so
we prove the concept and the implementation with cycle accurate SystemC simulation. We have chosen an MPEG4-SP
decoder and related benchmark videos to show how we can
signiﬁcantly improve throughput, latencies, injection rate
and so power consumption.
The contributions of this paper are:
• The Notifying Memories concept, which includes a notiﬁer in the NI of the memories and a listener in the
NI of the processors.
• A model and an implementation of the notifying memory in SystemC. It includes the introduction of a new
kind of packet, the notiﬁcation packet.
• Experimental results running an MPEG4-SP decoder
for diﬀerent video sequences that demonstrate the interest of the concept in terms of performance, injection
rate and power consumption.
Section 2 presents the related work. Section 3 introduces
the initial situation and a motivational example. Section 4
reminds brieﬂy the observer design pattern and presents in
details the notifying memories. Section 5 shows the results
and Section 6 concludes this paper.
2. RELATED WORK
Many approaches rely on the same idea of the one we
propose: reducing the number of transactions to save bandwidth for useful communications by trimming conﬂicts. An
active memory processor is introduced in [11]. It is a processor that resides inside the memory controller to process
data on the memory side. As the computation on a set of
data is done directly inside the memory, it reduces the number of transactions in the NoC. In [1], the concept of smart
memory is presented. It is a modular reconﬁgurable architecture that can be adapted to better match the application
requirements. The goal is to propose a target that can efﬁciently execute applications speciﬁed with a wide range of
programming models, from the stream programming model
to speculative and random programming model. The work
in that domain mainly focuses on improving the eﬃciency of
memory access [11]. By means of intelligent cache, memory
and protocol controllers, this kind of work breaks the passivity of the memories. Our contribution follows the same
ob jective but is compliant with any memory controller. Our
idea is to program memories so that they can trigger a notiﬁcation according to some events. These events can be application speciﬁc or related to the programming model. The
DF models of computation (MoC) are based on a paradigm
that makes our approach particularly eﬃcient.
Executing DF applications onto an NoC-based architecture is an active topic and gained renewed interest with the
advent of parallel architectures [5]. Executing a DF application, naturally parallel, onto a parallel architecture seems
obvious but the problem is actually more challenging [8].
One commercial example is the MPPA programmed with
sigmaC [7]. The approaches in the DF domain propose to
extend a baseline MoC to support architectural features, to
adopt a speciﬁc scheduling policy, or to tune the architecture to ﬁt with the programming model. Our approach is
complementary to existing works and is compliant with any
DF MoC and any scheduling policy.
3. MOTIVATIONAL EXAMPLE
Figure 1 presents the actor model of the Caltrop Actor
Language (CAL). An actor can contain several actions. An
action is executed (ﬁred ) when a set of conditions is satisﬁed.
This so-called ﬁring rule usually consists of checking that
the number of tokens in the input FIFO is greater than the
number needed to compute, and that the output FIFO is
empty enough to store the produced tokens. In this paper,
we propose the FIFO to notify directly the processor about
its content. This idea is inspired from the observer design
pattern, used in software engineering.
We use as a case study an MPEG-4 Simple Proﬁle decoder
(MPEG4-SP) speciﬁed in RVC-CAL [6]. This decoder is
speciﬁed with heterogeneous MoC and contains up do 40%
Figure 1: CAL actor model [3]
i f
( n u m T o k e n s _ F I F O _ I N 1 >= 6 4
&& n u m T o k e n s _ F I F O _ I N 2 >= 1 ) {
i f ( S I Z E _ F I F O _ O U T −
n u m T o k e n s F I F O _ O U T > 6 4 ) {
/ / f i r e a c t i o n
} }
Figure 2: Structure
of a SW FIFO generated by ORCC
Listing 1: Example of a ﬁring
rule
of dynamic actors [9]. The ORCC tool is used for compiling and software synthesis [6]. Our work relies on the
C-backend that generates C code for multi-core platforms.
Since the number of actors (41) is greater than the number
of processing cores, an actor scheduler is required. Diﬀerent policies have been proposed, one of the most eﬃcient
one remains the round-robin (the default in ORCC). They
all have the same ma jor drawback related to DF principles,
which is the ineﬃcient polling that leads to useless accesses
to the memory when a scheduling attempt is not successful.
Figure 2 presents the structure of the SW FIFO generated
by ORCC [6].
It is composed of ﬁve parts:
i) size of the
FIFO; ii) FIFO content, where memory allocation is done
according to the FIFO size and size of data type; iii) number of readers, since one actor can write in a FIFO but there
might be several readers; iv) index of readers, each reader
has its own index; and v) index of writer. Synchronisation is
handled through indexes. The diﬀerence between the reader
index and writer index determines the number of tokens inside the FIFO. When multiple readers occur, the minimum
index is used. It might happen that one slow reader blocks
the other readers. Many papers deal with FIFO sizing and
FIFO handling but it is out of the scope of this paper. Given
this FIFO structure, the processor that executes an actor
has to read the values of the diﬀerent indexes in order to
determine the number of tokens in the FIFO. This leads to
a set of memory requests for each FIFO. Taking the example of the ﬁring rule given in Listing 1, to compute numTokens_FIFO_IN1, the number of tokens in the ﬁrst FIFO, the
processor emits two requests, one for the index of the writer,
and one for the index of the reader (one for each reader actually). For the second FIFO, the processor emits another set
of requests. Then, in order to check for the output FIFO,
other requests are emitted on the NoC. In C language, if
the ﬁrst condition is not satisﬁed, the whole test is stopped.
The worst case occurs when the input conditions are satisﬁed but not the output condition, which would lead to six
transactions for no action ﬁring. Of course, next scheduling
attempt will test again these conditions although the conditions on the input FIFO are satisﬁed. It has to be noticed
that true conditions cannot become false on the next trial.
Our contribution also relies on this property.
Table 1: Unsuccessful scheduling by the MPEG4-SP
decoder for diﬀerent video sequences and formats
Video
Useless
Empty
Full
Sequence Format attempt input FIFO output FIFO
Akiyo
CIF
42.7%
63.7%
36.3%
Parkjoy
720p
21.3%
90.8%
9.2%
Foreman
CIF
34.8%
90.7%
9.3%
Coastguard
CIF
27.8%
98.4%
1.6%
Stefan
CIF
25.9%
83.3%
16.7%
Bridge far
QCIF
23.8%
38.4%
61.6%
Ice
4CIF
45.6%
70.4%
29.6%
We have carried out some experiments using the C backend of ORCC and executed the MPEG4-SP decoder on a
desktop computer. We have traced the number of ﬁrings of
each actor during one scheduling attempt. We have counted
the number of zero ﬁrings, i.e. no action could be ﬁred, out of
the total number of scheduling. Table 1 shows the percentage of unsuccessful scheduling for diﬀerent video sequences
from [10]. There are two reasons why no action can be ﬁred:
1) one of the input FIFO is empty (i.e. does not contain
enough tokens); or 2) one of the output FIFO is full (i.e.
does not contain enough space). Table 1 also shows the distribution between empty and full FIFO. These results show
that at least 20% of scheduling attempts are unsuccessful.
Although the lack of tokens in the input FIFO seams to be
the ma jor reason, the disparity among the diﬀerent video
sequences prevents from drawing any clear conclusion. This
observation motivates the integration of mechanisms able
to monitor the FIFO status and to emit notiﬁcations. This
mechanism can be integrated in the memory NI, close to the
FIFO implementation.
This paper focuses on how to delete useless memory requests, independently from the processors, memories, NoC
parameters and scheduling policy1 . The main issue is to
stop the polling on the NoC that: 1) is useless when no action can be ﬁred; and 2) consumes bandwidth that would
be useful for eﬀective transactions. The current situation is
that memories are sub jected to processor requests. The idea
is to give new capabilities to memories, so that, they can inform the concerned processors that their (FIFO) content has
changed.
4. THE NOTIFYING MEMORIES
Our idea is inspired from the observer design pattern,
widely used in software engineering. Before detailing the
implementation in the NoC of the observer pattern, a brief
description of the software pattern is given.
4.1 The observer design pattern
The observer design pattern deﬁnes a one-to-many dependency between objects so that when one object changes state,
al l its dependents are notiﬁed and updated automatical ly [2].
That behaviour is exactly what we expect when the content
of a FIFO changes: notify the concerned processors that execute the actors connected to that FIFO. Figure 3 shows
the UML class diagram of the observer design pattern. The
sub ject is the element to watch. The observer is the element that should react whenever a change occurs from the
sub ject. The sub ject notiﬁes a change to their observers by
means of a method.
1 although combining our approach with a new scheduling
policy is relevant
Figure 3: UML Diagram of Observer Design Pattern
for software implementation
Figure 4: The structure of the used NoC adopting
the notifying memories concept
4.2 NoC Implementation
Porting the observer design pattern to our context, the
memory is the sub ject and the processor is the observer.
There are two elements to be added in the NoC platform:
the notiﬁer and the listener. In order to remain compliant
with any existing processor or memory and to be independent from NoC parameters such as topology, router buﬀer
depth and routing policy, we have decided to integrate these
elements into the NI of the NoC. Besides, we need a master
component that can send packets through the network and
a component that can monitor requests, the NI is the only
component to oﬀer such features.
Figure 4 illustrates the structure of the NoC used to demonstrate the notifying memory concept. The NoC is a 4 × 4
mesh-based network which interconnects 28 IP cores (13 processing and 15 memory nodes). It is based on usual wormhole packet switching mode, deterministic XY routing algorithm to avoid deadlocks, and ﬂow control policy without
virtual channels. The routers have one arbiter per port and
one buﬀer per input port. Our approach is actually generic
and can be applied to NoC with diﬀerent features. For instance, N-ﬂit buﬀers can be used to improve performance
at the cost of more memory, in that case all transactions
including notiﬁcations will take advantage of it. The backend part of the NI is typical, it includes a packet maker and
packet un-maker to assemble and disassemble the packets, a
scheduler/priority manager to synchronize packet transmission and reception. The modiﬁcations lie in the front-end
of the network interface by either implementing the notiﬁer
or the listener. The notiﬁer is implemented in the NI of a
memory. The listener is added in the NI of a processor. The
structures of the additional components are detailed in the
following subsections.
4.2.1 Notiﬁer
The notiﬁer is a hardware module that transmits the status of all FIFOs allocated in a node accommodating memory. For each FIFO, the notiﬁer generates a notiﬁcation sigAlgorithm 1 Checking ﬁring rules satisfaction
Input: k the number of FIFOs, nf the number of readers of FIFO f
for all f ∈ {1, ..., k} do
if If [W ] modiﬁed then
for all i ∈ {1, ..., nf } do
Tf [Ri ] = If [W ] − If [Ri ]
if Tf [Ri ] ≥ Cf [Ri ] then
Sf [Ri ] = true
end if
end for
for all i ∈ {1, ..., nf } do
end if
if If [Ri ] modiﬁed then
F Sf = If [W ] − max
if F Sf ≥ Cf [W ] then
Sf [W ] = true
end if
end if
end for
end for
i∈{1,...,n}(If [Ri ])
Figure 5: The notiﬁer architecture
Figure 6: The composition of notiﬁcation packet
sponding registers of FSB. Later, the notiﬁer compares the
updated values in FSB with their relative ﬁring rules conditions saved in RCB. If a value in FSB equals or exceeds its
nal that is passed to the FIFO’s writer whenever it contains
enough space to save new tokens, or to the FIFO’s readers
whenever enough tokens are available. The notiﬁer architecture is illustrated in Figure 5. It is basically composed of a
comparator, 2 subtractors, a maximum ﬁnder, an and-gate,
an inverter, and set of registers classiﬁed in seven banks: FIFOs indexes bank (FIB ), FIFOs status bank (FSB ), ﬁring
rule conditions bank (RCB ), ﬁring rule satisfaction bank
(RSB ), notiﬁcation history bank (NHB ), actors ID bank
(AIDB ) and actors locations bank (ALB ). The notiﬁer functionality can be divided into three phases: the conﬁguration
phase, the checking phase, and the notiﬁcation phase.
Conﬁguration phase.
Once the system is launched, the manager processor provides the mapping data to all notiﬁers. A packet is sent
to each memory node specifying the number of involved FIFOs in the notiﬁer scope. Also, it speciﬁes for each involved
FIFO the locations (ID of processing nodes) of its writer and
readers as well as their ﬁring rule conditions and the IDs of
their corresponding actors. The registers of ALB, FCB, and
AIDB store this information respectively to be used in the
next two phases. The registers of the other banks are reset.
Checking phase.
When a node receives a new packet modifying one of the
FIFOs’ indexes, the packet un-maker provides the notiﬁer
by the FIFO address (f ), and its modiﬁed writing index
(If [W ]) or ith reading index (If [Ri ]). The notiﬁer stores the
new received index in its speciﬁc register in FIB. If the writing index of FIFO f has changed, the notiﬁer recomputes
the number of available tokens for all its readers (Tf [Ri ]).
If one of its reading indexes is modiﬁed, the notiﬁer recomputes the available free space (F Sf ) in FIFO f .
The notiﬁer assigns the computed values to their correFigure 7: The listener architecture
listener architecture is illustrated in Figure 7. It is composed
of a look-up table (LUT), an adder, and set of registers classiﬁed in two banks: the ﬁring rule status bank (RSB ) and
the notiﬁcation data bank (NDB ). The listener functionality
is divided into two phases: the conﬁguration phase and the
execution phase.
Conﬁguration phase.
The listener beneﬁts from the mapping information provided to the processor at system boot. It acquires the speciﬁcation of the mapped actors (number, identiﬁcation number, number of ﬁring rules) to set its conﬁguration. The
look up table (LUT) is conﬁgured such that the starting address speciﬁed to store notiﬁcations of each mapped actor is
retrieved at the LUT output when the actor identiﬁcation
number is provided at the LUT input.
Execution phase.
When a node receives a notiﬁcation packet, the payload
is provided to the listener. The actor ID is given to the
LUT input; whereas the ﬁring rule ID is used as an oﬀset
to determine the storage index j . Accordingly, the listener
stores the notiﬁcation data (free space or available tokens)
in the j th register of NDB and set the j th bit of RSB to
Table 3: Notiﬁcation memory gain for decoding 10
frames of diﬀerent video sequences
Video
SequenceFormat
Bridgefar QCIF
bus
CIF
grandma QCIF
foreman
CIF
ice
4CIF
ThroughputLatency
Injection Switch Flits
rate
conﬂictsnumber
+15.53% -73,96% -45,80% -71,38% -54,22%
+2.84% -73,79% -53,40% -72,90% -54,73%
+16.79% -68,96% -60,78% -85,50% -67,36%
+14.26% -78,41% -46,81% -72,86% -54,39%
+15.41% -78,44% -50,53% -75,33% -58,16%
and dynamic power of the NoC components relying on the
switching activity traced by the SystemC simulation of 10
decoded frames of the ice-4CIF video sequence. The results
show that the NoC adopting notifying memories saves up
to 49.1% of power consumption compared to the reference
NoC. Besides, the power overhead of the interfaces of the
proposed NoC presents a modest value of 16.3%. Regarding
the area, the proposed NoC presents an overhead of 12.4%,
when compared to reference NoC.
6. CONCLUSIONS AND FUTURE WORK
This paper presents the notifying memories, a hardware
implementation based on the observer design pattern, to enhance communication of data-ﬂow application that are deployed on a multi-processor platform. The notifying memories are baseline memories extended by a notiﬁer implemented into the NI of the NoC. The notifying memories send
a new kind of packet, the notiﬁcation packet that is caught
by a listener, a module implemented in the NI of the processor. The notifying memory concept raises new opportunities
and new challenges. At the hardware level, an exploration
is needed to ﬁnd out the size of the components: number
of possible notiﬁcations, size of the listener. Another challenge is to make it conﬁgurable or programmable for other
programming models. At the software level, the compiler
should detect the ﬁring rules to appropriately separate the
code onto the notiﬁer and the listener. Finally, new scheduling policies could take advantage of notiﬁcations.
7. "
Quest for high-performance bufferless NoCs with single-cycle express paths and self-learning throttling.,"Router buffers are the main reason for the Network-on-Chip's (NoC) scalable bandwidth, but consumes significant area and power. The SCEPTER bufferless NoC sets up single-cycle virtual express paths dynamically, allowing packets to traverse non-minimal paths without latency penalty. Using prioritization, bypassing, and throttling mechanisms, we maximize opportunities to use these paths while pushing bandwidth. For 64 and 256 nodes, we achieve 62% lower latency, 1.3× higher throughput, and 35% lower starvation over a baseline bufferless NoC for synthetic traffic. Full-system 36-core simulations show a 19% lower runtime, on-par performance to a buffered network, with 36% lower area, 33% lower power.","(cid:84)a(cid:98)le (cid:49). (cid:83)C(cid:69)P(cid:84)(cid:69)(cid:82) (cid:78)et(cid:119)or(cid:107) Parameters
(cid:78)oC (cid:84)opolog(cid:121)
(cid:54)(cid:2)(cid:54)(cid:44) (cid:56)(cid:2)(cid:56)(cid:44) (cid:49)(cid:54)(cid:2)(cid:49)(cid:54) mes(cid:104)
(cid:82)o(cid:117)ti(cid:110)g (cid:77)i(cid:110)imal (cid:65)(cid:100)apti(cid:118)e(cid:44) (cid:78)o(cid:110)(cid:45)mi(cid:110)imal priorit(cid:121)
(cid:70)lit Prioriti(cid:122)atio(cid:110) Desti(cid:110)atio(cid:110) Proximit(cid:121)
(cid:82)e(cid:113)(cid:117)est Prioriti(cid:122)atio(cid:110)
(cid:70)lit (cid:62) (cid:83)(cid:83)(cid:82) (cid:62) (cid:78)IC
HPCmax (cid:56)
(cid:84)ime (cid:87)i(cid:110)(cid:100)o(cid:119) (cid:40)(cid:76)(cid:41) (cid:49)(cid:50) (cid:40)(cid:54)(cid:2)(cid:54)(cid:41)(cid:44) (cid:49)(cid:54) (cid:40)(cid:56)(cid:2)(cid:56)(cid:41)(cid:44) (cid:51)(cid:50) (cid:40)(cid:49)(cid:54)(cid:2)(cid:49)(cid:54)(cid:41)
I(cid:110)(cid:106)e(cid:99)tio(cid:110) Q(cid:117)e(cid:117)e (cid:84)(cid:104)res(cid:104)ol(cid:100) (cid:40)IQ(cid:84)(cid:41) (cid:50)(cid:48) (cid:40)(cid:56) (cid:2)(cid:56)(cid:41)(cid:44) (cid:52)(cid:48) (cid:40)(cid:49)(cid:54)(cid:2)(cid:49)(cid:54)(cid:41)
(cid:76)ear(cid:110)i(cid:110)g (cid:82)ate (cid:40)α(cid:41)
(cid:40)γ
× (cid:56)
×(cid:56)
Q(cid:91)s, a(cid:93) (cid:61) (cid:40)(cid:49)− α(cid:41)∗ Q(cid:91)s, a(cid:93) (cid:43)α ∗ (cid:40)r (cid:43)γ ∗ max(cid:40)Q
(cid:48) (cid:91)s
(cid:48)
(cid:48) (cid:93)
, a
s(cid:48) (cid:105)
α is t(cid:104)e lear(cid:110)i(cid:110)g rate(cid:44)γ is t(cid:104)e
(cid:100)is(cid:99)o(cid:117)(cid:110)t rate(cid:44)r is t(cid:104)e o(cid:98)ser(cid:118)e(cid:100) re(cid:119)ar(cid:100)(cid:44) a(cid:110)(cid:100)max(cid:40)Q(cid:48) (cid:91)s (cid:48) , a(cid:48) (cid:93)
s(cid:48) (cid:46)
a(cid:44) t(cid:104)at (cid:104)as t(cid:104)e largest Q
(cid:118)al(cid:117)e(cid:44) i(cid:46)e(cid:46) pote(cid:110)tial (cid:102)or maxim(cid:117)m re(cid:119)ar(cid:100)(cid:46) (cid:84)(cid:104)e (cid:118)al(cid:117)e(cid:45)iteratio(cid:110) o(cid:102)
(cid:102)×(cid:99)
(cid:97) ∗(cid:105)
× (cid:104)
× (cid:104)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:119)
(cid:105)
(cid:108)
(cid:108)
(cid:116)
(cid:97)
(cid:107)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:109)
(cid:97)
(cid:120)
(cid:105)
(cid:109)
(cid:117)
(cid:109)
(cid:77)
(cid:97)
(cid:110)
(cid:104)
(cid:97)
(cid:116)
(cid:116)
(cid:97)
(cid:110)
(cid:100)
(cid:105)
(cid:115)
(cid:116)
(cid:97)
(cid:110)
(cid:99)
(cid:101)
(cid:111)
(cid:102)
(cid:49)
(cid:54)
(cid:104)
(cid:111)
(cid:112)
(cid:115)
(cid:116)
(cid:111)
(cid:101)
(cid:110)
(cid:115)
(cid:117)
(cid:114)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:103)
(cid:108)
(cid:111)
(cid:98)
(cid:97)
(cid:108)
(cid:105)
(cid:110)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:105)
(cid:115)
(cid:112)
(cid:114)
(cid:111)
(cid:112)
(cid:97)
(cid:103)
(cid:97)
(cid:116)
(cid:101)
(cid:100)
(cid:116)
(cid:111)
(cid:97)
(cid:108)
(cid:108)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:115)
(cid:46)
(cid:85)
(cid:115)
(cid:105)
(cid:110)
(cid:103)
(cid:115)
(cid:121)
(cid:110)
(cid:99)
(cid:104)
(cid:114)
(cid:111)
(cid:110)
(cid:105)
(cid:122)
(cid:101)
(cid:100)
(cid:116)
(cid:105)
(cid:109)
(cid:101)
(cid:119)
(cid:105)
(cid:110)
(cid:100)
(cid:111)
(cid:119)
(cid:115)
(cid:111)
(cid:102)
(cid:108)
(cid:101)
(cid:110)
(cid:103)
(cid:116)
(cid:104)
(cid:76)
(cid:44)
(cid:101)
(cid:97)
(cid:99)
(cid:104)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:119)
(cid:105)
(cid:108)
(cid:108)
(cid:112)
(cid:114)
(cid:111)
(cid:112)
(cid:97)
(cid:103)
(cid:97)
(cid:116)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:105)
(cid:110)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:97)
(cid:116)
(cid:116)
(cid:104)
(cid:101)
(cid:98)
(cid:101)
(cid:103)
(cid:105)
(cid:110)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:111)
(cid:102)
(cid:97)
(cid:116)
(cid:105)
(cid:109)
(cid:101)
(cid:119)
(cid:105)
(cid:110)
(cid:100)
(cid:111)
(cid:119)
(cid:46)
(cid:70)
(cid:111)
(cid:114)
(cid:97)
(cid:110)
(cid:56)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:116)
(cid:105)
(cid:109)
(cid:101)
(cid:119)
(cid:105)
(cid:110)
(cid:45)
(cid:100)
(cid:111)
(cid:119)
(cid:108)
(cid:101)
(cid:110)
(cid:103)
(cid:116)
(cid:104)
(cid:44)
(cid:76)
(cid:99)
(cid:97)
(cid:110)
(cid:98)
(cid:101)
(cid:115)
(cid:101)
(cid:116)
(cid:97)
(cid:116)
(cid:49)
(cid:54)
(cid:99)
(cid:121)
(cid:99)
(cid:108)
(cid:101)
(cid:115)
(cid:97)
(cid:115)
(cid:111)
(cid:110)
(cid:101)
(cid:104)
(cid:111)
(cid:112)
(cid:105)
(cid:110)
(cid:116)
(cid:104)
(cid:105)
(cid:115)
(cid:99)
(cid:111)
(cid:110)
(cid:103)
(cid:101)
(cid:115)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:102)
(cid:101)
(cid:101)
(cid:100)
(cid:98)
(cid:97)
(cid:99)
(cid:107)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:105)
(cid:110)
(cid:99)
(cid:117)
(cid:114)
(cid:115)
(cid:97)
(cid:108)
(cid:97)
(cid:116)
(cid:101)
(cid:110)
(cid:99)
(cid:121)
(cid:111)
(cid:102)
(cid:111)
(cid:110)
(cid:101)
(cid:99)
(cid:121)
(cid:99)
(cid:108)
(cid:101)
(cid:46)
(cid:69)
(cid:118)
(cid:101)
(cid:114)
(cid:121)
(cid:76)
(cid:99)
(cid:121)
(cid:99)
(cid:108)
(cid:101)
(cid:115)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:102)
(cid:101)
(cid:101)
(cid:100)
(cid:98)
(cid:97)
(cid:99)
(cid:107)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:112)
(cid:114)
(cid:111)
(cid:118)
(cid:105)
(cid:100)
(cid:101)
(cid:115)
(cid:97)
(cid:118)
(cid:105)
(cid:101)
(cid:119)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:103)
(cid:108)
(cid:111)
(cid:98)
(cid:97)
(cid:108)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:109)
(cid:111)
(cid:115)
(cid:116)
(cid:115)
(cid:116)
(cid:114)
(cid:97)
(cid:105)
(cid:103)
(cid:104)
(cid:116)
(cid:102)
(cid:111)
(cid:114)
(cid:119)
(cid:97)
(cid:114)
(cid:100)
(cid:97)
(cid:112)
(cid:112)
(cid:114)
(cid:111)
(cid:97)
(cid:99)
(cid:104)
(cid:44)
(cid:114)
(cid:101)
(cid:102)
(cid:101)
(cid:114)
(cid:114)
(cid:101)
(cid:100)
(cid:116)
(cid:111)
(cid:97)
(cid:115)
(cid:79)
(cid:78)
(cid:47)
(cid:79)
(cid:70)
(cid:70)
(cid:84)
(cid:104)
(cid:114)
(cid:111)
(cid:116)
(cid:116)
(cid:108)
(cid:105)
(cid:110)
(cid:103)
(cid:44)
(cid:105)
(cid:115)
(cid:116)
(cid:111)
(cid:115)
(cid:116)
(cid:111)
(cid:112)
(cid:110)
(cid:111)
(cid:110)
(cid:45)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:101)
(cid:100)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:115)
(cid:102)
(cid:114)
(cid:111)
(cid:109)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:110)
(cid:103)
(cid:97)
(cid:110)
(cid:100)
(cid:97)
(cid:108)
(cid:108)
(cid:111)
(cid:119)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:101)
(cid:100)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:115)
(cid:116)
(cid:111)
(cid:99)
(cid:111)
(cid:110)
(cid:116)
(cid:105)
(cid:110)
(cid:117)
(cid:101)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:110)
(cid:103)
(cid:105)
(cid:110)
(cid:116)
(cid:111)
(cid:116)
(cid:104)
(cid:101)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:46)
(cid:72)
(cid:111)
(cid:119)
(cid:101)
(cid:118)
(cid:101)
(cid:114)
(cid:44)
(cid:105)
(cid:116)
(cid:108)
(cid:101)
(cid:97)
(cid:100)
(cid:115)
(cid:116)
(cid:111)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:100)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:105)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:111)
(cid:112)
(cid:112)
(cid:101)
(cid:100)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:115)
(cid:46)
(cid:41)
(cid:41)
(cid:40)
(cid:49)
(cid:41)
(cid:81)
(cid:45)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:46)
(cid:81)
(cid:45)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:105)
(cid:115)
(cid:97)
(cid:114)
(cid:101)
(cid:105)
(cid:110)
(cid:102)
(cid:111)
(cid:114)
(cid:99)
(cid:101)
(cid:109)
(cid:101)
(cid:110)
(cid:116)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:116)
(cid:101)
(cid:99)
(cid:104)
(cid:110)
(cid:105)
(cid:113)
(cid:117)
(cid:101)
(cid:119)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:97)
(cid:103)
(cid:101)
(cid:110)
(cid:116)
(cid:47)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:115)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:111)
(cid:112)
(cid:116)
(cid:105)
(cid:109)
(cid:97)
(cid:108)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:101)
(cid:108)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:112)
(cid:111)
(cid:108)
(cid:105)
(cid:99)
(cid:121)
(cid:46)
(cid:66)
(cid:97)
(cid:115)
(cid:101)
(cid:100)
(cid:111)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:101)
(cid:110)
(cid:118)
(cid:105)
(cid:114)
(cid:111)
(cid:110)
(cid:109)
(cid:101)
(cid:110)
(cid:116)
(cid:105)
(cid:110)
(cid:116)
(cid:101)
(cid:114)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:45)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:97)
(cid:108)
(cid:103)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:104)
(cid:109)
(cid:119)
(cid:105)
(cid:108)
(cid:108)
(cid:44)
(cid:111)
(cid:118)
(cid:101)
(cid:114)
(cid:116)
(cid:105)
(cid:109)
(cid:101)
(cid:44)
(cid:100)
(cid:101)
(cid:116)
(cid:101)
(cid:114)
(cid:109)
(cid:105)
(cid:110)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:98)
(cid:101)
(cid:115)
(cid:116)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:116)
(cid:111)
(cid:116)
(cid:97)
(cid:107)
(cid:101)
(cid:44)
(cid:103)
(cid:105)
(cid:118)
(cid:101)
(cid:110)
(cid:97)
(cid:110)
(cid:101)
(cid:110)
(cid:118)
(cid:105)
(cid:114)
(cid:111)
(cid:110)
(cid:45)
(cid:109)
(cid:101)
(cid:110)
(cid:116)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:46)
(cid:69)
(cid:97)
(cid:99)
(cid:104)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:109)
(cid:97)
(cid:105)
(cid:110)
(cid:116)
(cid:97)
(cid:105)
(cid:110)
(cid:115)
(cid:97)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:111)
(cid:102)
(cid:81)
(cid:91)
(cid:83)
(cid:44)
(cid:65)
(cid:93)
(cid:44)
(cid:119)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:83)
(cid:105)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:101)
(cid:116)
(cid:111)
(cid:102)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:97)
(cid:110)
(cid:100)
(cid:65)
(cid:105)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:115)
(cid:101)
(cid:116)
(cid:111)
(cid:102)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:46)
(cid:87)
(cid:101)
(cid:109)
(cid:97)
(cid:105)
(cid:110)
(cid:116)
(cid:97)
(cid:105)
(cid:110)
(cid:97)
(cid:81)
(cid:45)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:111)
(cid:102)
(cid:56)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:97)
(cid:110)
(cid:100)
(cid:53)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:97)
(cid:116)
(cid:101)
(cid:97)
(cid:99)
(cid:104)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:105)
(cid:110)
(cid:116)
(cid:101)
(cid:114)
(cid:102)
(cid:97)
(cid:99)
(cid:101)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:99)
(cid:111)
(cid:114)
(cid:114)
(cid:101)
(cid:115)
(cid:112)
(cid:111)
(cid:110)
(cid:100)
(cid:116)
(cid:111)
(cid:116)
(cid:104)
(cid:101)
(cid:99)
(cid:104)
(cid:97)
(cid:110)
(cid:103)
(cid:101)
(cid:105)
(cid:110)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:97)
(cid:99)
(cid:114)
(cid:111)
(cid:115)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:97)
(cid:110)
(cid:100)
(cid:97)
(cid:108)
(cid:115)
(cid:111)
(cid:97)
(cid:99)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:102)
(cid:111)
(cid:114)
(cid:97)
(cid:110)
(cid:121)
(cid:99)
(cid:104)
(cid:97)
(cid:110)
(cid:103)
(cid:101)
(cid:105)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:108)
(cid:111)
(cid:99)
(cid:97)
(cid:108)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:39)
(cid:115)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:117)
(cid:115)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:2)
(cid:118)
(cid:101)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:99)
(cid:111)
(cid:114)
(cid:114)
(cid:101)
(cid:115)
(cid:112)
(cid:111)
(cid:110)
(cid:100)
(cid:116)
(cid:111)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:105)
(cid:110)
(cid:103)
(cid:44)
(cid:100)
(cid:101)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:105)
(cid:110)
(cid:103)
(cid:44)
(cid:111)
(cid:114)
(cid:114)
(cid:101)
(cid:116)
(cid:97)
(cid:105)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:116)
(cid:104)
(cid:101)
(cid:116)
(cid:104)
(cid:114)
(cid:111)
(cid:116)
(cid:116)
(cid:108)
(cid:101)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:46)
(cid:73)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:116)
(cid:104)
(cid:114)
(cid:111)
(cid:116)
(cid:116)
(cid:108)
(cid:101)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:105)
(cid:115)
(cid:116)
(cid:111)
(cid:98)
(cid:101)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:100)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:97)
(cid:114)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:112)
(cid:111)
(cid:115)
(cid:115)
(cid:105)
(cid:98)
(cid:108)
(cid:101)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:44)
(cid:101)
(cid:105)
(cid:116)
(cid:104)
(cid:101)
(cid:114)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:115)
(cid:108)
(cid:111)
(cid:119)
(cid:108)
(cid:121)
(cid:119)
(cid:105)
(cid:116)
(cid:104)
(cid:97)
(cid:115)
(cid:109)
(cid:97)
(cid:108)
(cid:108)
(cid:101)
(cid:114)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:109)
(cid:101)
(cid:110)
(cid:116)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:40)
(cid:105)
(cid:110)
(cid:111)
(cid:117)
(cid:114)
(cid:99)
(cid:97)
(cid:115)
(cid:101)
(cid:105)
(cid:116)
(cid:105)
(cid:115)
(cid:49)
(cid:41)
(cid:44)
(cid:111)
(cid:114)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:113)
(cid:117)
(cid:105)
(cid:99)
(cid:107)
(cid:108)
(cid:121)
(cid:119)
(cid:105)
(cid:116)
(cid:104)
(cid:97)
(cid:108)
(cid:97)
(cid:114)
(cid:103)
(cid:101)
(cid:114)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:109)
(cid:101)
(cid:110)
(cid:116)
(cid:40)
(cid:105)
(cid:110)
(cid:111)
(cid:117)
(cid:114)
(cid:99)
(cid:97)
(cid:115)
(cid:101)
(cid:105)
(cid:116)
(cid:105)
(cid:115)
(cid:50)
(cid:41)
(cid:46)
(cid:84)
(cid:104)
(cid:117)
(cid:115)
(cid:44)
(cid:97)
(cid:110)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:116)
(cid:104)
(cid:97)
(cid:116)
(cid:105)
(cid:110)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:116)
(cid:104)
(cid:114)
(cid:111)
(cid:116)
(cid:116)
(cid:108)
(cid:101)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:98)
(cid:121)
(cid:111)
(cid:110)
(cid:101)
(cid:44)
(cid:119)
(cid:111)
(cid:117)
(cid:108)
(cid:100)
(cid:115)
(cid:116)
(cid:111)
(cid:112)
(cid:111)
(cid:110)
(cid:101)
(cid:3)
(cid:105)
(cid:116)
(cid:102)
(cid:114)
(cid:111)
(cid:109)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:110)
(cid:103)
(cid:105)
(cid:110)
(cid:116)
(cid:111)
(cid:116)
(cid:104)
(cid:101)
(cid:78)
(cid:111)
(cid:67)
(cid:105)
(cid:110)
(cid:97)
(cid:116)
(cid:101)
(cid:110)
(cid:45)
(cid:99)
(cid:121)
(cid:99)
(cid:108)
(cid:101)
(cid:116)
(cid:105)
(cid:109)
(cid:101)
(cid:105)
(cid:110)
(cid:116)
(cid:101)
(cid:114)
(cid:118)
(cid:97)
(cid:108)
(cid:46)
(cid:65)
(cid:102)
(cid:116)
(cid:101)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:99)
(cid:117)
(cid:114)
(cid:114)
(cid:101)
(cid:110)
(cid:116)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:111)
(cid:98)
(cid:115)
(cid:101)
(cid:114)
(cid:118)
(cid:101)
(cid:100)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:45)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:105)
(cid:115)
(cid:117)
(cid:112)
(cid:100)
(cid:97)
(cid:116)
(cid:101)
(cid:100)
(cid:97)
(cid:99)
(cid:99)
(cid:111)
(cid:114)
(cid:100)
(cid:105)
(cid:110)
(cid:103)
(cid:116)
(cid:111)
(cid:69)
(cid:113)
(cid:117)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:49)
(cid:44)
(cid:119)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:41)
(cid:105)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:109)
(cid:97)
(cid:120)
(cid:105)
(cid:109)
(cid:117)
(cid:109)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:105)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:45)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:102)
(cid:111)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:99)
(cid:117)
(cid:114)
(cid:114)
(cid:101)
(cid:110)
(cid:116)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:44)
(cid:84)
(cid:104)
(cid:101)
(cid:103)
(cid:108)
(cid:111)
(cid:98)
(cid:97)
(cid:108)
(cid:99)
(cid:111)
(cid:110)
(cid:103)
(cid:101)
(cid:115)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:118)
(cid:101)
(cid:99)
(cid:116)
(cid:111)
(cid:114)
(cid:116)
(cid:114)
(cid:105)
(cid:103)
(cid:103)
(cid:101)
(cid:114)
(cid:115)
(cid:97)
(cid:110)
(cid:117)
(cid:112)
(cid:100)
(cid:97)
(cid:116)
(cid:101)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:45)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:101)
(cid:110)
(cid:116)
(cid:114)
(cid:121)
(cid:44)
(cid:119)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:108)
(cid:101)
(cid:97)
(cid:114)
(cid:110)
(cid:105)
(cid:110)
(cid:103)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:105)
(cid:110)
(cid:100)
(cid:105)
(cid:99)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:104)
(cid:111)
(cid:119)
(cid:113)
(cid:117)
(cid:105)
(cid:99)
(cid:107)
(cid:108)
(cid:121)
(cid:110)
(cid:101)
(cid:119)
(cid:99)
(cid:111)
(cid:110)
(cid:103)
(cid:101)
(cid:115)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:105)
(cid:110)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:105)
(cid:115)
(cid:102)
(cid:97)
(cid:99)
(cid:116)
(cid:111)
(cid:114)
(cid:101)
(cid:100)
(cid:105)
(cid:110)
(cid:116)
(cid:111)
(cid:116)
(cid:104)
(cid:101)
(cid:117)
(cid:112)
(cid:100)
(cid:97)
(cid:116)
(cid:101)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:102)
(cid:117)
(cid:116)
(cid:117)
(cid:114)
(cid:101)
(cid:114)
(cid:101)
(cid:119)
(cid:97)
(cid:114)
(cid:100)
(cid:112)
(cid:111)
(cid:116)
(cid:101)
(cid:110)
(cid:116)
(cid:105)
(cid:97)
(cid:108)
(cid:105)
(cid:115)
(cid:99)
(cid:111)
(cid:109)
(cid:112)
(cid:117)
(cid:116)
(cid:101)
(cid:100)
(cid:98)
(cid:121)
(cid:102)
(cid:97)
(cid:99)
(cid:116)
(cid:111)
(cid:114)
(cid:105)
(cid:110)
(cid:103)
(cid:116)
(cid:104)
(cid:101)
(cid:109)
(cid:97)
(cid:120)
(cid:105)
(cid:109)
(cid:117)
(cid:109)
(cid:81)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:102)
(cid:111)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:99)
(cid:117)
(cid:114)
(cid:114)
(cid:101)
(cid:110)
(cid:116)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:105)
(cid:110)
(cid:116)
(cid:111)
(cid:116)
(cid:104)
(cid:101)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:39)
(cid:115)
(cid:81)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:117)
(cid:112)
(cid:100)
(cid:97)
(cid:116)
(cid:101)
(cid:44)
(cid:119)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:100)
(cid:105)
(cid:115)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:105)
(cid:110)
(cid:100)
(cid:105)
(cid:99)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:105)
(cid:109)
(cid:112)
(cid:111)
(cid:114)
(cid:116)
(cid:97)
(cid:110)
(cid:99)
(cid:101)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:102)
(cid:117)
(cid:116)
(cid:117)
(cid:114)
(cid:101)
(cid:114)
(cid:101)
(cid:119)
(cid:97)
(cid:114)
(cid:100)
(cid:46)
(cid:70)
(cid:111)
(cid:114)
(cid:97)
(cid:110)
(cid:111)
(cid:98)
(cid:115)
(cid:101)
(cid:114)
(cid:118)
(cid:101)
(cid:100)
(cid:115)
(cid:116)
(cid:97)
(cid:114)
(cid:118)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:116)
(cid:97)
(cid:116)
(cid:101)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:116)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:105)
(cid:115)
(cid:99)
(cid:104)
(cid:101)
(cid:99)
(cid:107)
(cid:101)
(cid:100)
(cid:116)
(cid:111)
(cid:2)
(cid:110)
(cid:100)
(cid:116)
(cid:104)
(cid:101)
(cid:97)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:44)
(cid:48)
(cid:46)
(cid:49)
(cid:68)
(cid:105)
(cid:115)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:82)
(cid:97)
(cid:116)
(cid:101)
(cid:41)
(cid:48)
(cid:46)
(cid:49)
(cid:116)
(cid:104)
(cid:101)
(cid:81)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:101)
(cid:105)
(cid:115)
(cid:99)
(cid:111)
(cid:110)
(cid:116)
(cid:105)
(cid:110)
(cid:117)
(cid:97)
(cid:108)
(cid:108)
(cid:121)
(cid:112)
(cid:101)
(cid:114)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:101)
(cid:100)
(cid:97)
(cid:110)
(cid:100)
(cid:101)
(cid:97)
(cid:99)
(cid:104)
(cid:110)
(cid:111)
(cid:100)
(cid:101)
(cid:39)
(cid:115)
(cid:116)
(cid:104)
(cid:114)
(cid:111)
(cid:116)
(cid:116)
(cid:108)
(cid:101)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:99)
(cid:111)
(cid:110)
(cid:118)
(cid:101)
(cid:114)
(cid:103)
(cid:101)
(cid:115)
(cid:46)
(cid:52)
(cid:46)
(cid:65)
(cid:82)
(cid:67)
(cid:72)
(cid:73)
(cid:84)
(cid:69)
(cid:67)
(cid:84)
(cid:85)
(cid:82)
(cid:69)
(cid:65)
(cid:78)
(cid:65)
(cid:76)
(cid:89)
(cid:83)
(cid:73)
(cid:83)
(cid:87)
(cid:101)
(cid:101)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:97)
(cid:116)
(cid:101)
(cid:83)
(cid:67)
(cid:69)
(cid:80)
(cid:84)
(cid:69)
(cid:82)
(cid:97)
(cid:110)
(cid:100)
(cid:98)
(cid:97)
(cid:115)
(cid:101)
(cid:108)
(cid:105)
(cid:110)
(cid:101)
(cid:97)
(cid:114)
(cid:99)
(cid:104)
(cid:105)
(cid:116)
(cid:101)
(cid:99)
(cid:116)
(cid:117)
(cid:114)
(cid:101)
(cid:115)
(cid:119)
(cid:105)
(cid:116)
(cid:104)
(cid:116)
(cid:104)
(cid:101)
(cid:103)
(cid:101)
(cid:109)
(cid:53)
(cid:91)
(cid:50)
(cid:93)
(cid:99)
(cid:121)
(cid:99)
(cid:108)
(cid:101)
(cid:45)
(cid:97)
(cid:99)
(cid:99)
(cid:117)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:105)
(cid:109)
(cid:117)
(cid:108)
(cid:97)
(cid:116)
(cid:111)
(cid:114)
(cid:97)
(cid:110)
(cid:100)
(cid:116)
(cid:104)
(cid:101)
(cid:71)
(cid:65)
(cid:82)
(cid:78)
(cid:69)
(cid:84)
(cid:91)
(cid:49)
(cid:93)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:109)
(cid:111)
(cid:100)
(cid:101)
(cid:108)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:101)
(cid:118)
(cid:97)
(cid:108)
(cid:117)
(cid:97)
(cid:116)
(cid:101)
(cid:100)
(cid:115)
(cid:121)
(cid:115)
(cid:116)
(cid:101)
(cid:109)
(cid:112)
(cid:97)
(cid:114)
(cid:97)
(cid:109)
(cid:101)
(cid:116)
(cid:101)
(cid:114)
(cid:115)
(cid:101)
(cid:116)
(cid:116)
(cid:105)
(cid:110)
(cid:103)
(cid:115)
(cid:97)
(cid:114)
(cid:101)
(cid:115)
(cid:104)
(cid:111)
(cid:119)
(cid:110)
(cid:105)
(cid:110)
(cid:84)
(cid:97)
(cid:98)
(cid:108)
(cid:101)
(cid:49)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:98)
(cid:97)
(cid:115)
(cid:101)
(cid:45)
(cid:108)
(cid:105)
(cid:110)
(cid:101)
(cid:115)
(cid:102)
(cid:111)
(cid:114)
(cid:112)
(cid:101)
(cid:114)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:97)
(cid:110)
(cid:99)
(cid:101)
(cid:99)
(cid:111)
(cid:109)
(cid:112)
(cid:97)
(cid:114)
(cid:105)
(cid:115)
(cid:111)
(cid:110)
(cid:97)
(cid:114)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:66)
(cid:76)
(cid:69)
(cid:83)
(cid:83)
(cid:78)
(cid:111)
(cid:67)
(cid:97)
(cid:110)
(cid:100)
(cid:83)
(cid:77)
(cid:65)
(cid:82)
(cid:84)
(cid:118)
(cid:105)
(cid:114)
(cid:116)
(cid:117)
(cid:97)
(cid:108)
(cid:45)
(cid:99)
(cid:104)
(cid:97)
(cid:110)
(cid:110)
(cid:101)
(cid:108)
(cid:98)
(cid:117)
(cid:102)
(cid:102)
(cid:101)
(cid:114)
(cid:101)
(cid:100)
(cid:78)
(cid:111)
(cid:67)
(cid:44)
(cid:100)
(cid:101)
(cid:115)
(cid:99)
(cid:114)
(cid:105)
(cid:98)
(cid:101)
(cid:100)
(cid:105)
(cid:110)
(cid:100)
(cid:101)
(cid:112)
(cid:116)
(cid:104)
(cid:105)
(cid:110)
(cid:83)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:50)
(cid:46)
(cid:52)
(cid:46)
(cid:49)
(cid:68)
(cid:101)
(cid:115)
(cid:105)
(cid:103)
(cid:110)
(cid:83)
(cid:112)
(cid:97)
(cid:99)
(cid:101)
(cid:69)
(cid:120)
(cid:112)
(cid:108)
(cid:111)
(cid:114)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:87)
(cid:101)
(cid:112)
(cid:101)
(cid:114)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:97)
(cid:100)
(cid:101)
(cid:115)
(cid:105)
(cid:103)
(cid:110)
(cid:115)
(cid:119)
(cid:101)
(cid:101)
(cid:112)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:113)
(cid:117)
(cid:101)
(cid:115)
(cid:116)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:44)
(cid:3)
(cid:105)
(cid:116)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:44)
(cid:111)
(cid:112)
(cid:112)
(cid:111)
(cid:114)
(cid:116)
(cid:117)
(cid:110)
(cid:105)
(cid:115)
(cid:116)
(cid:105)
(cid:99)
(cid:98)
(cid:121)
(cid:112)
(cid:97)
(cid:115)
(cid:115)
(cid:105)
(cid:110)
(cid:103)
(cid:97)
(cid:110)
(cid:100)
(cid:97)
(cid:100)
(cid:97)
(cid:112)
(cid:116)
(cid:105)
(cid:118)
(cid:101)
(cid:114)
(cid:111)
(cid:117)
(cid:116)
(cid:105)
(cid:110)
(cid:103)
(cid:117)
(cid:115)
(cid:105)
(cid:110)
(cid:103)
(cid:115)
(cid:121)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:116)
(cid:105)
(cid:99)
(cid:116)
(cid:114)
(cid:97)
(cid:102)
(cid:2)
(cid:99)
(cid:112)
(cid:97)
(cid:116)
(cid:116)
(cid:101)
(cid:114)
(cid:110)
(cid:115)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:108)
(cid:97)
(cid:116)
(cid:99)
(cid:104)
(cid:101)
(cid:100)
(cid:3)
(cid:105)
(cid:116)
(cid:115)
(cid:104)
(cid:97)
(cid:118)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:104)
(cid:105)
(cid:103)
(cid:104)
(cid:101)
(cid:115)
(cid:116)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:121)
(cid:44)
(cid:98)
(cid:117)
(cid:116)
(cid:119)
(cid:101)
(cid:99)
(cid:104)
(cid:97)
(cid:114)
(cid:97)
(cid:99)
(cid:116)
(cid:101)
(cid:114)
(cid:105)
(cid:122)
(cid:101)
(cid:116)
(cid:104)
(cid:101)
(cid:101)
(cid:102)
(cid:102)
(cid:101)
(cid:99)
(cid:116)
(cid:115)
(cid:111)
(cid:102)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:105)
(cid:110)
(cid:103)
(cid:101)
(cid:105)
(cid:116)
(cid:104)
(cid:101)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:83)
(cid:83)
(cid:82)
(cid:115)
(cid:111)
(cid:114)
(cid:78)
(cid:73)
(cid:67)
(cid:39)
(cid:115)
(cid:108)
(cid:111)
(cid:99)
(cid:97)
(cid:108)
(cid:3)
(cid:105)
(cid:116)
(cid:46)
(cid:70)
(cid:105)
(cid:103)
(cid:117)
(cid:114)
(cid:101)
(cid:51)
(cid:98)
(cid:105)
(cid:115)
(cid:97)
(cid:99)
(cid:111)
(cid:109)
(cid:112)
(cid:97)
(cid:114)
(cid:105)
(cid:115)
(cid:111)
(cid:110)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:97)
(cid:118)
(cid:101)
(cid:114)
(cid:97)
(cid:103)
(cid:101)
(cid:108)
(cid:97)
(cid:116)
(cid:101)
(cid:110)
(cid:99)
(cid:121)
(cid:111)
(cid:102)
(cid:83)
(cid:67)
(cid:69)
(cid:80)
(cid:84)
(cid:69)
(cid:82)
(cid:45)
(cid:83)
(cid:83)
(cid:82)
(cid:115)
(cid:40)
(cid:83)
(cid:83)
(cid:82)
(cid:115)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:101)
(cid:100)
(cid:111)
(cid:118)
(cid:101)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:108)
(cid:111)
(cid:99)
(cid:97)
(cid:108)
(cid:3)
(cid:105)
(cid:116)
(cid:41)
(cid:116)
(cid:111)
(cid:83)
(cid:67)
(cid:69)
(cid:80)
(cid:84)
(cid:69)
(cid:82)
(cid:45)
(cid:76)
(cid:79)
(cid:67)
(cid:65)
(cid:76)
(cid:40)
(cid:108)
(cid:111)
(cid:99)
(cid:97)
(cid:108)
(cid:3)
(cid:105)
(cid:116)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:101)
(cid:100)
(cid:111)
(cid:118)
(cid:101)
(cid:114)
(cid:83)
(cid:83)
(cid:82)
(cid:115)
(cid:41)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:83)
(cid:67)
(cid:69)
(cid:80)
(cid:84)
(cid:69)
(cid:82)
(cid:45)
(cid:76)
(cid:79)
(cid:67)
(cid:65)
(cid:76)
(cid:111)
(cid:112)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:97)
(cid:116)
(cid:117)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:115)
(cid:109)
(cid:111)
(cid:114)
(cid:101)
(cid:113)
(cid:117)
(cid:105)
(cid:99)
(cid:107)
(cid:108)
(cid:121)
(cid:97)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:110)
(cid:101)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:107)
(cid:105)
(cid:115)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:101)
(cid:100)
(cid:119)
(cid:105)
(cid:116)
(cid:104)
(cid:109)
(cid:111)
(cid:114)
(cid:101)
(cid:3)
(cid:105)
(cid:116)
(cid:115)
(cid:44)
(cid:97)
(cid:110)
(cid:100)
(cid:116)
(cid:104)
(cid:101)
(cid:83)
(cid:83)
(cid:82)
(cid:115)
(cid:102)
(cid:97)
(cid:105)
(cid:108)
(cid:100)
(cid:117)
(cid:101)
(cid:116)
(cid:111)
(cid:99)
(cid:111)
(cid:110)
(cid:103)
(cid:101)
(cid:115)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:46)
(cid:70)
(cid:105)
(cid:103)
(cid:117)
(cid:114)
(cid:101)
(cid:51)
(cid:97)
(cid:115)
(cid:104)
(cid:111)
(cid:119)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:97)
(cid:118)
(cid:101)
(cid:114)
(cid:97)
(cid:103)
(cid:101)
(cid:108)
(cid:97)
(cid:116)
(cid:101)
(cid:110)
(cid:99)
(cid:121)
(cid:97)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:114)
(cid:97)
(cid:116)
(cid:101)
(cid:105)
(cid:115)
(cid:105)
(cid:110)
(cid:45)
(cid:99)
(cid:114)
(cid:101)
(cid:97)
(cid:115)
(cid:101)
(cid:100)
(cid:102)
(cid:111)
(cid:114)
(cid:117)
(cid:110)
(cid:105)
(cid:102)
(cid:111)
(cid:114)
(cid:109)
(cid:114)
(cid:97)
(cid:110)
(cid:100)
(cid:111)
(cid:109)
(cid:116)
(cid:114)
(cid:97)
(cid:102)
(cid:2)
(cid:99)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:68)
(cid:101)
(cid:3)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(cid:97)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:99)
(cid:111)
(cid:110)
(cid:115)
(cid:105)
(cid:100)
(cid:101)
(cid:114)
(cid:115)
(cid:97)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:119)
(cid:105)
(cid:116)
(cid:104)
(cid:105)
(cid:110)
(cid:116)
(cid:104)
(cid:101)
(cid:3)
(cid:105)
(cid:116)
(cid:116)
(cid:104)
(cid:97)
(cid:116)
(cid:99)
(cid:111)
(cid:110)
(cid:116)
(cid:97)
(cid:105)
(cid:110)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:110)
(cid:117)
(cid:109)
(cid:98)
(cid:101)
(cid:114)
(cid:111)
(cid:102)
(cid:100)
(cid:101)
(cid:3)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:116)
(cid:104)
(cid:101)
(cid:3)
(cid:105)
(cid:116)
(cid:115)
(cid:104)
(cid:97)
(cid:115)
(cid:101)
(cid:110)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:101)
(cid:114)
(cid:101)
(cid:100)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:104)
(cid:105)
(cid:103)
(cid:104)
(cid:101)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:100)
(cid:101)
(cid:3)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:99)
(cid:111)
(cid:117)
(cid:110)
(cid:116)
(cid:44)
(cid:116)
(cid:104)
(cid:101)
(cid:104)
(cid:105)
(cid:103)
(cid:104)
(cid:101)
(cid:114)
(cid:116)
(cid:104)
(cid:101)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:121)
(cid:46)
(cid:84)
(cid:104)
(cid:101)
(cid:65)
(cid:103)
(cid:101)
(cid:112)
(cid:114)
(cid:105)
(cid:111)
(cid:114)
(cid:105)
(cid:116)
(cid:105)
(cid:122)
(ci"
DISCO - a low overhead in-network data compressor for energy-efficient chip multi-processors.,"Data compression has been proposed to increase the utility of on-chip memory space or Network-on-Chip (NoC) bandwidth in energy-efficient processors. However, such techniques usually add additional compression and decompression latency to the critical path of memory access, which is one of the major factors limiting their application to processors. In contrast to prior work that deals with either cache compression or network compression separately, this study proposes a unified on-chip DIStributed data COmpressor, DISCO, to enable near-zero latency cache/NoC compression for chip multi-processors (CMPs) adopting Non-Uniform Cache Access (NUCA). DISCO integrates data compressors into NoC routers and seeks opportunity to overlap the de/compression latency with the NoC queuing delay through a coordinated NoC scheduling and cache compression mechanism With the support of DISCO that unifies the solutions of on-chip data compression, it is shown in evaluation that DISCO significantly boosts the efficiency of on-chip data caching and data moving.","(cid:3)
(cid:3)
(cid:3)
(cid:39)(cid:44)(cid:54)(cid:38)(cid:50)(cid:29)(cid:3)(cid:36)(cid:3)(cid:47)(cid:82)(cid:90)(cid:3)(cid:50)(cid:89)(cid:72)(cid:85)(cid:75)(cid:72)(cid:68)(cid:71)(cid:3)(cid:44)(cid:81)(cid:16)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)
(cid:40)(cid:81)(cid:72)(cid:85)(cid:74)(cid:92)(cid:16)(cid:40)(cid:73)(cid:73)(cid:76)(cid:70)(cid:76)(cid:72)(cid:81)(cid:87)(cid:3)(cid:38)(cid:75)(cid:76)(cid:83)(cid:3)(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:51)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:3)
(cid:60)(cid:76)(cid:81)(cid:74)(cid:3)(cid:58)(cid:68)(cid:81)(cid:74)(cid:15)(cid:3)(cid:60)(cid:76)(cid:81)(cid:75)(cid:72)(cid:3)(cid:43)(cid:68)(cid:81)(cid:15)(cid:3)(cid:45)(cid:88)(cid:81)(cid:3)(cid:61)(cid:75)(cid:82)(cid:88)(cid:15)(cid:3)(cid:43)(cid:88)(cid:68)(cid:90)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:59)(cid:76)(cid:68)(cid:82)(cid:90)(cid:72)(cid:76)(cid:3)(cid:47)(cid:76)(cid:3)
(cid:54)(cid:87)(cid:68)(cid:87)(cid:72)(cid:3)(cid:46)(cid:72)(cid:92)(cid:3)(cid:47)(cid:68)(cid:69)(cid:82)(cid:85)(cid:68)(cid:87)(cid:82)(cid:85)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:85)(cid:3)(cid:36)(cid:85)(cid:70)(cid:75)(cid:76)(cid:87)(cid:72)(cid:70)(cid:87)(cid:88)(cid:85)(cid:72)(cid:3)
(cid:44)(cid:81)(cid:86)(cid:87)(cid:76)(cid:87)(cid:88)(cid:87)(cid:72)(cid:3)(cid:82)(cid:73)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:55)(cid:72)(cid:70)(cid:75)(cid:81)(cid:82)(cid:79)(cid:82)(cid:74)(cid:92)(cid:15)(cid:3)(cid:38)(cid:75)(cid:76)(cid:81)(cid:72)(cid:86)(cid:72)(cid:3)(cid:36)(cid:70)(cid:68)(cid:71)(cid:72)(cid:80)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:54)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:72)(cid:86)(cid:15)(cid:3)(cid:37)(cid:72)(cid:76)(cid:77)(cid:76)(cid:81)(cid:74)(cid:15)(cid:3)(cid:51)(cid:17)(cid:53)(cid:17)(cid:3)(cid:38)(cid:75)(cid:76)(cid:81)(cid:68)(cid:3)
(cid:94)(cid:90)(cid:68)(cid:81)(cid:74)(cid:92)(cid:76)(cid:81)(cid:74)(cid:21)(cid:19)(cid:19)(cid:28)(cid:15)(cid:3)(cid:92)(cid:76)(cid:81)(cid:75)(cid:72)(cid:86)(cid:15)(cid:3)(cid:93)(cid:75)(cid:82)(cid:88)(cid:77)(cid:88)(cid:81)(cid:15)(cid:3)(cid:79)(cid:76)(cid:75)(cid:88)(cid:68)(cid:90)(cid:72)(cid:76)(cid:15)(cid:3)(cid:79)(cid:91)(cid:90)(cid:96)(cid:35)(cid:76)(cid:70)(cid:87)(cid:17)(cid:68)(cid:70)(cid:17)(cid:70)(cid:81)(cid:3)
(cid:36)(cid:37)(cid:54)(cid:55)(cid:53)(cid:36)(cid:38)(cid:55)(cid:3)(cid:3)
Data compression has been proposed to increase the utility of on-chip 
memory space or Network-on-Chip (NoC) bandwidth in energyefficient processors. However, such techniques usually add additional 
compression and decompression latency to the critical path of memory 
access, which is one of the major factors limiting their application to 
processors. In contrast to prior work that deals with either cache 
compression or network compression separately, this study proposes a 
unified on-chip DIStributed data COmpressor, DISCO, to enable nearzero latency cache/NoC compression for chip multi-processors (CMPs) 
adopting Non-Uniform Cache Access (NUCA). DISCO integrates data 
compressors into NoC routers and seeks opportunity to overlap the 
de/compression latency with the NoC queuing delay through a 
coordinated NoC scheduling and cache compression mechanism. With 
the support of DISCO that unifies the solutions of on-chip data 
compression, it is shown in evaluation that DISCO significantly boosts 
the efficiency of on-chip data caching and data moving. 
(cid:20)(cid:17)(cid:3) (cid:44)(cid:49)(cid:55)(cid:53)(cid:50)(cid:39)(cid:56)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3)
(cid:36)(cid:86)(cid:3)(cid:80)(cid:88)(cid:79)(cid:87)(cid:76)(cid:70)(cid:82)(cid:85)(cid:72)(cid:3)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:86)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:74)(cid:85)(cid:82)(cid:90)(cid:76)(cid:81)(cid:74)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:83)(cid:88)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:88)(cid:85)(cid:72)(cid:3)(cid:82)(cid:81)(cid:3)(cid:82)(cid:81)(cid:16)(cid:70)(cid:75)(cid:76)(cid:83)(cid:3)(cid:86)(cid:87)(cid:82)(cid:85)(cid:68)(cid:74)(cid:72)(cid:17)(cid:3)(cid:36)(cid:3)(cid:79)(cid:68)(cid:85)(cid:74)(cid:72)(cid:3)(cid:82)(cid:81)(cid:16)(cid:70)(cid:75)(cid:76)(cid:83)(cid:3)(cid:47)(cid:68)(cid:86)(cid:87)(cid:3)(cid:47)(cid:72)(cid:89)(cid:72)(cid:79)(cid:3)
(cid:38)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:11)(cid:47)(cid:47)(cid:38)(cid:12)(cid:3) (cid:76)(cid:86)(cid:3) (cid:72)(cid:86)(cid:86)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3) (cid:87)(cid:82)(cid:3) (cid:69)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:74)(cid:68)(cid:83)(cid:3) (cid:69)(cid:72)(cid:87)(cid:90)(cid:72)(cid:72)(cid:81)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:86)(cid:87)(cid:85)(cid:72)(cid:81)(cid:74)(cid:87)(cid:75)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:82)(cid:73)(cid:73)(cid:16)(cid:70)(cid:75)(cid:76)(cid:83)(cid:3)(cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)(cid:3)(cid:69)(cid:68)(cid:81)(cid:71)(cid:90)(cid:76)(cid:71)(cid:87)(cid:75)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:69)(cid:76)(cid:74)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:82)(cid:85)(cid:3)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:16)
(cid:82)(cid:88)(cid:87)(cid:3) (cid:90)(cid:82)(cid:85)(cid:78)(cid:79)(cid:82)(cid:68)(cid:71)(cid:86)(cid:3) (cid:62)(cid:20) (cid:64)(cid:17)(cid:3) (cid:43)(cid:82)(cid:90)(cid:72)(cid:89)(cid:72)(cid:85)(cid:15)(cid:3) (cid:68)(cid:3) (cid:79)(cid:68)(cid:85)(cid:74)(cid:72)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:73)(cid:82)(cid:82)(cid:87)(cid:83)(cid:85)(cid:76)(cid:81)(cid:87)(cid:3) (cid:75)(cid:68)(cid:86)(cid:3) (cid:86)(cid:88)(cid:69)(cid:86)(cid:87)(cid:68)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)
(cid:76)(cid:80)(cid:83)(cid:68)(cid:70)(cid:87)(cid:86)(cid:3) (cid:82)(cid:81)(cid:3) (cid:71)(cid:76)(cid:72)(cid:3) (cid:68)(cid:85)(cid:72)(cid:68)(cid:15)(cid:3) (cid:83)(cid:82)(cid:90)(cid:72)(cid:85)(cid:3) (cid:70)(cid:82)(cid:81)(cid:86)(cid:88)(cid:80)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3) (cid:70)(cid:75)(cid:76)(cid:83)(cid:3) (cid:92)(cid:76)(cid:72)(cid:79)(cid:71)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:82)(cid:83)(cid:83)(cid:82)(cid:85)(cid:87)(cid:88)(cid:81)(cid:76)(cid:87)(cid:92)(cid:3)
(cid:70)(cid:82)(cid:86)(cid:87)(cid:3)(cid:87)(cid:82)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:74)(cid:85)(cid:68)(cid:87)(cid:72)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:85)(cid:72)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:86)(cid:17)(cid:3)(cid:53)(cid:72)(cid:70)(cid:72)(cid:81)(cid:87)(cid:79)(cid:92)(cid:15)(cid:3)(cid:85)(cid:72)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:72)(cid:85)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)
(cid:79)(cid:82)(cid:82)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3) (cid:73)(cid:82)(cid:85)(cid:90)(cid:68)(cid:85)(cid:71)(cid:3) (cid:87)(cid:82)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:87)(cid:82)(cid:3) (cid:76)(cid:81)(cid:70)(cid:85)(cid:72)(cid:68)(cid:86)(cid:72)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:86)(cid:83)(cid:68)(cid:70)(cid:72)(cid:3)
(cid:88)(cid:87)(cid:76)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:17)(cid:3)(cid:37)(cid:92)(cid:3)(cid:72)(cid:91)(cid:83)(cid:79)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:85)(cid:72)(cid:71)(cid:88)(cid:81)(cid:71)(cid:68)(cid:81)(cid:70)(cid:92)(cid:3)(cid:76)(cid:81)(cid:86)(cid:76)(cid:71)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:15)(cid:3)(cid:68)(cid:3)(cid:89)(cid:68)(cid:85)(cid:76)(cid:72)(cid:87)(cid:92)(cid:3)
(cid:82)(cid:73)(cid:3) (cid:71)(cid:68)(cid:87)(cid:68)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:87)(cid:72)(cid:70)(cid:75)(cid:81)(cid:76)(cid:84)(cid:88)(cid:72)(cid:86)(cid:3) (cid:75)(cid:68)(cid:89)(cid:72)(cid:3) (cid:69)(cid:72)(cid:72)(cid:81)(cid:3) (cid:83)(cid:85)(cid:82)(cid:83)(cid:82)(cid:86)(cid:72)(cid:71)(cid:3) (cid:76)(cid:81)(cid:3) (cid:87)(cid:75)(cid:72)(cid:86)(cid:72)(cid:3) (cid:71)(cid:68)(cid:92)(cid:86)(cid:3) (cid:87)(cid:82)(cid:3)
(cid:85)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:80)(cid:76)(cid:86)(cid:86)(cid:72)(cid:86)(cid:3)(cid:82)(cid:85)(cid:3)(cid:86)(cid:68)(cid:89)(cid:72)(cid:3)(cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)(cid:3)(cid:69)(cid:68)(cid:81)(cid:71)(cid:90)(cid:76)(cid:71)(cid:87)(cid:75)(cid:3)(cid:76)(cid:81)(cid:3)(cid:71)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:86)(cid:70)(cid:72)(cid:81)(cid:68)(cid:85)(cid:76)(cid:82)(cid:86)(cid:17)(cid:3)(cid:3)
(cid:55)(cid:75)(cid:72)(cid:85)(cid:72)(cid:3) (cid:68)(cid:85)(cid:72)(cid:3) (cid:73)(cid:85)(cid:72)(cid:84)(cid:88)(cid:72)(cid:81)(cid:87)(cid:3) (cid:83)(cid:68)(cid:87)(cid:87)(cid:72)(cid:85)(cid:81)(cid:3) (cid:69)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:87)(cid:72)(cid:70)(cid:75)(cid:81)(cid:76)(cid:84)(cid:88)(cid:72)(cid:86)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)
(cid:71)(cid:72)(cid:79)(cid:87)(cid:68)(cid:16)(cid:69)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3) (cid:86)(cid:70)(cid:75)(cid:72)(cid:80)(cid:72)(cid:86)(cid:3) (cid:68)(cid:86)(cid:3) (cid:76)(cid:87)(cid:3) (cid:70)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3) (cid:76)(cid:81)(cid:3) (cid:83)(cid:85)(cid:76)(cid:82)(cid:85)(cid:3) (cid:90)(cid:82)(cid:85)(cid:78)(cid:3) (cid:62)(cid:21) (cid:64)(cid:3) (cid:62)(cid:22) (cid:64)(cid:3) (cid:62)(cid:23) (cid:64)(cid:3) (cid:62)(cid:24)(cid:64)(cid:17)(cid:3)
(cid:36)(cid:79)(cid:87)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:3) (cid:76)(cid:81)(cid:3) (cid:71)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3) (cid:76)(cid:80)(cid:83)(cid:79)(cid:72)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:86)(cid:72)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:3) (cid:68)(cid:85)(cid:72)(cid:3)
(cid:86)(cid:88)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3) (cid:87)(cid:82)(cid:3) (cid:86)(cid:72)(cid:89)(cid:72)(cid:85)(cid:68)(cid:79)(cid:3) (cid:70)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3) (cid:90)(cid:72)(cid:68)(cid:78)(cid:81)(cid:72)(cid:86)(cid:86)(cid:72)(cid:86)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:75)(cid:76)(cid:81)(cid:71)(cid:72)(cid:85)(cid:3) (cid:87)(cid:75)(cid:72)(cid:80)(cid:3) (cid:73)(cid:85)(cid:82)(cid:80)(cid:3) (cid:69)(cid:72)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:72)(cid:71)(cid:3) (cid:87)(cid:82)(cid:3) (cid:70)(cid:82)(cid:80)(cid:80)(cid:82)(cid:71)(cid:76)(cid:87)(cid:92)(cid:3) (cid:80)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:17)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3) (cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:3) (cid:73)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3) (cid:76)(cid:86)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3)
(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:16)(cid:68)(cid:81)(cid:71)(cid:16)(cid:71)(cid:72)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3) (cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:71)(cid:3) (cid:69)(cid:92)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:17)(cid:3)
(cid:54)(cid:82)(cid:80)(cid:72)(cid:87)(cid:76)(cid:80)(cid:72)(cid:86)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:18)(cid:71)(cid:72)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3) (cid:79)(cid:76)(cid:72)(cid:86)(cid:3) (cid:76)(cid:81)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:70)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)
(cid:83)(cid:68)(cid:87)(cid:75)(cid:3) (cid:82)(cid:73)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:68)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:76)(cid:87)(cid:3) (cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:79)(cid:92)(cid:3) (cid:71)(cid:72)(cid:74)(cid:85)(cid:68)(cid:71)(cid:72)(cid:86)(cid:3) (cid:90)(cid:82)(cid:85)(cid:78)(cid:79)(cid:82)(cid:68)(cid:71)(cid:3) (cid:83)(cid:72)(cid:85)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:81)(cid:70)(cid:72)(cid:17)(cid:3)
(cid:40)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3) (cid:76)(cid:81)(cid:3) (cid:38)(cid:75)(cid:76)(cid:83)(cid:3) (cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:51)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:3) (cid:11)(cid:38)(cid:48)(cid:51)(cid:86)(cid:12)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:70)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:3) (cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:72)(cid:71)(cid:3)
(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:69)(cid:68)(cid:81)(cid:78)(cid:86)(cid:3) (cid:87)(cid:82)(cid:3) (cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:3) (cid:89)(cid:76)(cid:68)(cid:3) (cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:16)(cid:82)(cid:81)(cid:16)(cid:38)(cid:75)(cid:76)(cid:83)(cid:3) (cid:11)(cid:49)(cid:82)(cid:38)(cid:12)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:68)(cid:71)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:16)(cid:68)(cid:81)(cid:71)(cid:16)(cid:71)(cid:72)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:49)(cid:82)(cid:38)(cid:3)
(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3) (cid:90)(cid:76)(cid:79)(cid:79)(cid:3)
(cid:72)(cid:79)(cid:82)(cid:81)(cid:74)(cid:68)(cid:87)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:70)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:68)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3)(cid:83)(cid:68)(cid:87)(cid:75)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:83)(cid:85)(cid:82)(cid:69)(cid:79)(cid:72)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:86)(cid:82)(cid:3)(cid:70)(cid:85)(cid:76)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)
(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:16)(cid:86)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3) (cid:82)(cid:81)(cid:16)(cid:70)(cid:75)(cid:76)(cid:83)(cid:3) (cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)(cid:3) (cid:86)(cid:88)(cid:69)(cid:86)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:15)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:76)(cid:87)(cid:3) (cid:69)(cid:72)(cid:70)(cid:82)(cid:80)(cid:72)(cid:86)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3)
(cid:74)(cid:85)(cid:72)(cid:68)(cid:87)(cid:72)(cid:86)(cid:87)(cid:3) (cid:73)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:79)(cid:76)(cid:80)(cid:76)(cid:87)(cid:86)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:68)(cid:83)(cid:83)(cid:79)(cid:76)(cid:70)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:82)(cid:73)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:51)(cid:72)(cid:85)(cid:80)(cid:76)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)(cid:87)(cid:82)(cid:3)(cid:80)(cid:68)(cid:78)(cid:72)(cid:3)(cid:71)(cid:76)(cid:74)(cid:76)(cid:87)(cid:68)(cid:79)(cid:3)(cid:82)(cid:85)(cid:3)(cid:75)(cid:68)(cid:85)(cid:71)(cid:3)(cid:70)(cid:82)(cid:83)(cid:76)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:79)(cid:79)(cid:3)(cid:82)(cid:85)(cid:3)(cid:83)(cid:68)(cid:85)(cid:87)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:83)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)
(cid:82)(cid:85)(cid:3) (cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:85)(cid:82)(cid:82)(cid:80)(cid:3) (cid:88)(cid:86)(cid:72)(cid:3) (cid:76)(cid:86)(cid:3) (cid:74)(cid:85)(cid:68)(cid:81)(cid:87)(cid:72)(cid:71)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:82)(cid:88)(cid:87)(cid:3) (cid:73)(cid:72)(cid:72)(cid:3) (cid:83)(cid:85)(cid:82)(cid:89)(cid:76)(cid:71)(cid:72)(cid:71)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:70)(cid:82)(cid:83)(cid:76)(cid:72)(cid:86)(cid:3) (cid:68)(cid:85)(cid:72)(cid:3) (cid:81)(cid:82)(cid:87)(cid:3) (cid:80)(cid:68)(cid:71)(cid:72)(cid:3) (cid:82)(cid:85)(cid:3)
(cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:72)(cid:71)(cid:3) (cid:73)(cid:82)(cid:85)(cid:3)(cid:83)(cid:85)(cid:82)(cid:73)(cid:76)(cid:87)(cid:3)(cid:82)(cid:85)(cid:3)(cid:70)(cid:82)(cid:80)(cid:80)(cid:72)(cid:85)(cid:70)(cid:76)(cid:68)(cid:79)(cid:3)(cid:68)(cid:71)(cid:89)(cid:68)(cid:81)(cid:87)(cid:68)(cid:74)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:70)(cid:82)(cid:83)(cid:76)(cid:72)(cid:86)(cid:3)(cid:69)(cid:72)(cid:68)(cid:85)(cid:3) (cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:81)(cid:82)(cid:87)(cid:76)(cid:70)(cid:72)(cid:3)
(cid:68)(cid:81)(cid:71)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:73)(cid:88)(cid:79)(cid:79)(cid:3)(cid:70)(cid:76)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:81)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:3)(cid:83)(cid:68)(cid:74)(cid:72)(cid:17)(cid:3)(cid:38)(cid:82)(cid:83)(cid:92)(cid:85)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)(cid:3) (cid:73)(cid:82)(cid:85)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3) (cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)
(cid:82)(cid:90)(cid:81)(cid:72)(cid:71)(cid:3) (cid:69)(cid:92)(cid:3) (cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:86)(cid:3) (cid:87)(cid:75)(cid:68)(cid:81)(cid:3) (cid:36)(cid:38)(cid:48)(cid:3) (cid:80)(cid:88)(cid:86)(cid:87)(cid:3) (cid:69)(cid:72)(cid:3) (cid:75)(cid:82)(cid:81)(cid:82)(cid:85)(cid:72)(cid:71)(cid:17)(cid:3) (cid:36)(cid:69)(cid:86)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3) (cid:70)(cid:85)(cid:72)(cid:71)(cid:76)(cid:87)(cid:3) (cid:76)(cid:86)(cid:3)
(cid:83)(cid:72)(cid:85)(cid:80)(cid:76)(cid:87)(cid:87)(cid:72)(cid:71)(cid:17)(cid:3)(cid:55)(cid:82)(cid:3)(cid:70)(cid:82)(cid:83)(cid:92)(cid:3)(cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:90)(cid:76)(cid:86)(cid:72)(cid:15)(cid:3)(cid:82)(cid:85)(cid:3)(cid:85)(cid:72)(cid:83)(cid:88)(cid:69)(cid:79)(cid:76)(cid:86)(cid:75)(cid:15)(cid:3)(cid:87)(cid:82)(cid:3)(cid:83)(cid:82)(cid:86)(cid:87)(cid:3)(cid:82)(cid:81)(cid:3)(cid:86)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)(cid:86)(cid:3)(cid:82)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:85)(cid:72)(cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:72)(cid:3)
(cid:87)(cid:82)(cid:3)(cid:79)(cid:76)(cid:86)(cid:87)(cid:86)(cid:15)(cid:3)(cid:85)(cid:72)(cid:84)(cid:88)(cid:76)(cid:85)(cid:72)(cid:86)(cid:3)(cid:83)(cid:85)(cid:76)(cid:82)(cid:85)(cid:3)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:3)(cid:83)(cid:72)(cid:85)(cid:80)(cid:76)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)(cid:68)(cid:81)(cid:71)(cid:18)(cid:82)(cid:85)(cid:3)(cid:68)(cid:3)(cid:73)(cid:72)(cid:72)(cid:17)(cid:3)
DAC '16,(cid:3)(cid:45)(cid:88)(cid:81)(cid:72)(cid:3)(cid:19)(cid:24)(cid:16)(cid:19)(cid:28)(cid:15)(cid:3)(cid:21)(cid:19)(cid:20)(cid:25)(cid:15)(cid:3)(cid:36)(cid:88)(cid:86)(cid:87)(cid:76)(cid:81)(cid:15)(cid:3)(cid:55)(cid:59)(cid:15)(cid:3)(cid:56)(cid:54)(cid:36)(cid:3)
(cid:139)(cid:3)(cid:21)(cid:19)(cid:20)(cid:25)(cid:3)(cid:36)(cid:38)(cid:48)(cid:17)(cid:3)(cid:44)(cid:54)(cid:37)(cid:49)(cid:3)(cid:28)(cid:26)(cid:27)(cid:16)(cid:20)(cid:16)(cid:23)(cid:24)(cid:19)(cid:22)(cid:16)(cid:23)(cid:21)(cid:22)(cid:25)(cid:16)(cid:19)(cid:18)(cid:20)(cid:25)(cid:18)(cid:19)(cid:25)(cid:3)(cid:7)(cid:20)(cid:24)(cid:17)(cid:19)(cid:19)(cid:3)
(cid:39)(cid:50)(cid:44)(cid:29)(cid:3)(cid:75)(cid:87)(cid:87)(cid:83)(cid:29)(cid:18)(cid:18)(cid:71)(cid:91)(cid:17)(cid:71)(cid:82)(cid:76)(cid:17)(cid:82)(cid:85)(cid:74)(cid:18)(cid:20)(cid:19)(cid:17)(cid:20)(cid:20)(cid:23)(cid:24)(cid:18)(cid:21)(cid:27)(cid:28)(cid:26)(cid:28)(cid:22)(cid:26)(cid:17)(cid:21)(cid:27)(cid:28)(cid:27)(cid:19)(cid:19)(cid:26)(cid:3)
(cid:87)(cid:72)(cid:70)(cid:75)(cid:81)(cid:76)(cid:84)(cid:88)(cid:72)(cid:86)(cid:3)(cid:62)(cid:24)(cid:64)(cid:17)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:73)(cid:68)(cid:70)(cid:87)(cid:82)(cid:85)(cid:3)(cid:76)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:75)(cid:68)(cid:85)(cid:71)(cid:90)(cid:68)(cid:85)(cid:72)(cid:3)(cid:70)(cid:82)(cid:86)(cid:87)(cid:17)(cid:3)(cid:41)(cid:82)(cid:85)(cid:3)(cid:87)(cid:92)(cid:83)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:38)(cid:48)(cid:51)(cid:86)(cid:3)
(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:49)(cid:82)(cid:81)(cid:16)(cid:56)(cid:81)(cid:76)(cid:73)(cid:82)(cid:85)(cid:80)(cid:3)(cid:38)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:36)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3) (cid:11)(cid:49)(cid:56)(cid:38)(cid:36)(cid:12)(cid:3) (cid:68)(cid:86)(cid:3) (cid:76)(cid:79)(cid:79)(cid:88)(cid:86)(cid:87)(cid:85)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3) (cid:76)(cid:81)(cid:3)(cid:41)(cid:76)(cid:74)(cid:17)(cid:3)(cid:20)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3)
(cid:70)(cid:82)(cid:86)(cid:87)(cid:3)(cid:82)(cid:73)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:76)(cid:81)(cid:16)(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:85)(cid:76)(cid:86)(cid:72)(cid:86)(cid:3) (cid:79)(cid:76)(cid:81)(cid:72)(cid:68)(cid:85)(cid:79)(cid:92)(cid:3)(cid:90)(cid:75)(cid:72)(cid:81)(cid:3)(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:69)(cid:68)(cid:81)(cid:78)(cid:3)(cid:70)(cid:82)(cid:88)(cid:81)(cid:87)(cid:3)
(cid:76)(cid:81)(cid:72)(cid:89)(cid:76)(cid:87)(cid:68)(cid:69)(cid:79)(cid:92)(cid:3) (cid:76)(cid:81)(cid:70)(cid:85)(cid:72)(cid:68)(cid:86)(cid:72)(cid:86)(cid:15)(cid:3) (cid:72)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3) (cid:73)(cid:82)(cid:85)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3) (cid:83)(cid:68)(cid:87)(cid:87)(cid:72)(cid:85)(cid:81)(cid:3) (cid:87)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3) (cid:82)(cid:85)(cid:3)
(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:85)(cid:92)(cid:3)(cid:86)(cid:87)(cid:82)(cid:85)(cid:68)(cid:74)(cid:72)(cid:3)(cid:62)(cid:22)(cid:64)(cid:3)(cid:62)(cid:23)(cid:64)(cid:17)(cid:3)(cid:3)
(cid:44)(cid:81)(cid:3) (cid:87)(cid:75)(cid:76)(cid:86)(cid:3) (cid:90)(cid:82)(cid:85)(cid:78)(cid:15)(cid:3) (cid:90)(cid:72)(cid:3) (cid:83)(cid:85)(cid:82)(cid:83)(cid:82)(cid:86)(cid:72)(cid:3) (cid:68)(cid:3) (cid:39)(cid:44)(cid:54)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:72)(cid:71)(cid:3) (cid:76)(cid:81)(cid:16)(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3) (cid:71)(cid:68)(cid:87)(cid:68)(cid:3)
(cid:38)(cid:50)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:11)(cid:39)(cid:44)(cid:54)(cid:38)(cid:50)(cid:12)(cid:3)(cid:68)(cid:86)(cid:3)(cid:68)(cid:3)(cid:86)(cid:88)(cid:69)(cid:86)(cid:87)(cid:76)(cid:87)(cid:88)(cid:87)(cid:72)(cid:3)(cid:87)(cid:82)(cid:3)(cid:68)(cid:89)(cid:82)(cid:76)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:90)(cid:72)(cid:68)(cid:78)(cid:81)(cid:72)(cid:86)(cid:86)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:92)(cid:83)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)
(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:17)(cid:3) (cid:44)(cid:81)(cid:86)(cid:87)(cid:72)(cid:68)(cid:71)(cid:3) (cid:82)(cid:73)(cid:3) (cid:68)(cid:87)(cid:87)(cid:68)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3) (cid:87)(cid:82)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:82)(cid:85)(cid:3) (cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:15)(cid:3)
(cid:39)(cid:44)(cid:54)(cid:38)(cid:50)(cid:3)(cid:80)(cid:72)(cid:85)(cid:74)(cid:72)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)(cid:76)(cid:81)(cid:87)(cid:82)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:85)(cid:82)(cid:88)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:49)(cid:82)(cid:38)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:68)(cid:79)(cid:79)(cid:82)(cid:90)(cid:86)(cid:3)
(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:81)(cid:72)(cid:73)(cid:76)(cid:87)(cid:3)(cid:68)(cid:79)(cid:79)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:3)(cid:70)(cid:68)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:80)(cid:82)(cid:89)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:70)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3)
(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:82)(cid:70)(cid:70)(cid:88)(cid:85)(cid:3)(cid:82)(cid:81)(cid:3)(cid:70)(cid:75)(cid:76)(cid:83)(cid:17)(cid:3)
(cid:55)(cid:75)(cid:72)(cid:3)(cid:83)(cid:85)(cid:76)(cid:82)(cid:85)(cid:76)(cid:72)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:82)(cid:83)(cid:83)(cid:82)(cid:85)(cid:87)(cid:88)(cid:81)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:76)(cid:81)(cid:16)(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3)
(cid:38)(cid:82)(cid:81)(cid:89)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:15)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:3) (cid:73)(cid:82)(cid:70)(cid:88)(cid:86)(cid:3) (cid:82)(cid:81)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:15)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)
(cid:85)(cid:68)(cid:85)(cid:72)(cid:79)(cid:92)(cid:3) (cid:71)(cid:76)(cid:86)(cid:70)(cid:88)(cid:86)(cid:86)(cid:3) (cid:76)(cid:87)(cid:86)(cid:3) (cid:76)(cid:81)(cid:87)(cid:72)(cid:74)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3) (cid:86)(cid:87)(cid:85)(cid:68)(cid:87)(cid:72)(cid:74)(cid:92)(cid:3) (cid:76)(cid:81)(cid:87)(cid:82)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:38)(cid:48)(cid:51)(cid:86)(cid:17)(cid:3) (cid:58)(cid:72)(cid:3) (cid:73)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3) (cid:87)(cid:75)(cid:68)(cid:87)(cid:3)
(cid:80)(cid:72)(cid:85)(cid:74)(cid:76)(cid:81)(cid:74)(cid:3) (cid:71)(cid:68)(cid:87)(cid:68)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3) (cid:76)(cid:81)(cid:87)(cid:82)(cid:3) (cid:49)(cid:82)(cid:38)(cid:3) (cid:70)(cid:68)(cid:81)(cid:3) (cid:70)(cid:82)(cid:81)(cid:86)(cid:83)(cid:76)(cid:70)(cid:88)(cid:82)(cid:88)(cid:86)(cid:79)(cid:92)(cid:3) (cid:80)(cid:76)(cid:87)(cid:76)(cid:74)(cid:68)(cid:87)(cid:72)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3)
(cid:82)(cid:89)(cid:72)(cid:85)(cid:75)(cid:72)(cid:68)(cid:71)(cid:3) (cid:82)(cid:73)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3) (cid:68)(cid:79)(cid:86)(cid:82)(cid:3) (cid:68)(cid:70)(cid:75)(cid:76)(cid:72)(cid:89)(cid:72)(cid:3) (cid:80)(cid:88)(cid:79)(cid:87)(cid:76)(cid:83)(cid:79)(cid:72)(cid:3) (cid:85)(cid:72)(cid:90)(cid:68)(cid:85)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:68)(cid:71)(cid:89)(cid:68)(cid:81)(cid:87)(cid:68)(cid:74)(cid:72)(cid:86)(cid:15)(cid:3)(cid:80)(cid:68)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:76)(cid:87)(cid:3)(cid:80)(cid:82)(cid:85)(cid:72)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:3)(cid:68)(cid:71)(cid:82)(cid:83)(cid:87)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:85)(cid:72)(cid:68)(cid:79)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:86)(cid:17)(cid:3)
(cid:41)(cid:76)(cid:85)(cid:86)(cid:87)(cid:15)(cid:3) (cid:76)(cid:81)(cid:16)(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:82)(cid:85)(cid:3) (cid:70)(cid:68)(cid:81)(cid:3) (cid:86)(cid:88)(cid:69)(cid:86)(cid:87)(cid:68)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3) (cid:85)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:68)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:3)
(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)(cid:3) (cid:70)(cid:68)(cid:88)(cid:86)(cid:72)(cid:71)(cid:3) (cid:69)(cid:92)(cid:3) (cid:87)(cid:85)(cid:68)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3) (cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:82)(cid:85)(cid:3) (cid:72)(cid:89)(cid:72)(cid:81)(cid:3) (cid:70)(cid:82)(cid:80)(cid:83)(cid:79)(cid:72)(cid:87)(cid:72)(cid:79)(cid:92)(cid:3)
(cid:85)(cid:72)(cid:80)(cid:82)(cid:89)(cid:72)(cid:3)(cid:76)(cid:87)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:73)(cid:82)(cid:85)(cid:80)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:83)(cid:72)(cid:81)(cid:68)(cid:79)(cid:87)(cid:92)(cid:3)(cid:76)(cid:80)(cid:83)(cid:82)(cid:86)(cid:72)(cid:71)(cid:3)(cid:82)(cid:81)(cid:3)(cid:70)(cid:68)(cid:70)(cid:75)(cid:72)(cid:3)(cid:68)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)(cid:17)(cid:3)(cid:41)(cid:82)(cid:85)(cid:3)(cid:72)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:3)
(cid:76)(cid:81)(cid:3)(cid:41)(cid:76)(cid:74)(cid:17)(cid:3)(cid:20)(cid:15)(cid:3) (cid:87)(cid:75)(cid:72)(cid:3) (cid:87)(cid:76)(cid:80)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:68)(cid:87)(cid:75)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:"
Achieving lightweight multicast in asynchronous networks-on-chip using local speculation.,"We propose a lightweight parallel multicast targeting an asynchronous NoC with a variant Mesh-of-Trees topology. A novel strategy, local speculation, is introduced, where a subset of switches are speculative and always broadcast. These switches are surrounded by non-speculative switches, which throttle any redundant packets, restricting these packets to small regions. Speculative switches have simplified designs, thereby improving network performance. A hybrid network architecture is proposed to mix the speculative and non-speculative switches. For multicast benchmarks, significant performance improvements with small power savings are obtained by the new approach over a tree-based non-speculative approach. Interestingly, similar improvements are also shown for unicast. Finally, another benefit is to reduce the address field size in multicast packets.","Achieving Lightweight Multicast in Asynchronous
Networks-on-Chip Using Local Speculation
Kshitij Bhardwaj
Dept. of Computer Science
Columbia University
New York, NY 10027
kbhardwa j@cs.columbia.edu
Steven M. Nowick
Dept. of Computer Science
Columbia University
New York, NY 10027
nowick@cs.columbia.edu
latency and throughput [11],
[12],
[13]. There is also a
recent surge in industrial uptake of asynchronous NoCs:
(i) IBM’s TrueNorth neuromorphic chip integrates 4096
neurosynaptic cores, modeling 1 million neurons and 256
million synapses using a fully-asynchronous NoC [14], and
(ii) STMicroelectronics’ advanced GALS many-core system,
called STHORM, that uses a fully-asynchronous NoC to
connect 4 clusters, each with 16 synchronous processors [10].
Related work. There has been much research on handling multicast in synchronous NoCs [15], [16], [4], [5],[17].
These approaches are classiﬁed into two categories:
serial multicast
ABSTRACT
We propose a lightweight parallel multicast targeting an
asynchronous NoC with a variant Mesh-of-Trees topology.
A novel strategy, local speculation,
is introduced, where
a subset of switches are speculative and always broadcast. These switches are surrounded by non-speculative
switches, which throttle any redundant packets, restricting these packets to small regions. Speculative switches
have simpliﬁed designs, thereby improving network performance. A hybrid network architecture is proposed to
mix the speculative and non-speculative switches. For multicast benchmarks, signiﬁcant performance improvements
with small power savings are obtained by the new approach
over a tree-based non-speculative approach. Interestingly,
similar improvements are also shown for unicast. Finally,
another beneﬁt is to reduce the address ﬁeld size in multicast packets.
1.
INTRODUCTION
In today’s many-core era, on-chip networks have ma jor
impact on system-level power and performance. Networkson-chip (NoCs) have been an active area of research for the
last decade [1], [2]. Most of the NoC research has been devoted to improving performance, power and fault-tolerance
for unicast (i.e. one-to-one) traﬃc. However,
in recent
years, multicast (i.e. one-to-many) has also seen growing
interest, with several optimization strategies [3].
In multicast, the same packet is sent from a source to an
arbitrary subset of destinations. Multicast is widely-used
in various parallel computing applications: for example, in
cache coherency protocols to send write invalidates to multiple processors, in shared-operand networks for operand
delivery, and in multi-threaded applications for barrier synchronization [4]. Each of these applications cause signiﬁcant
multicast traﬃc in NoCs. For example, for the Token cache
coherence protocol, 52.4% of injected traﬃc is multicast [5].
There is also a growing interest in supporting multicast in
NoCs using emerging technologies, such as wireless [6] and
photonic [7]. Other emerging areas include large-scale neuromorphic chip multiprocessors [8] and the use of CDMA to
handle multicast [9]. Both these approaches support multicast in application-speciﬁc asynchronous NoCs.
Asynchronous NoCs are at the core of designing modern globally-asynchronous locally-synchronous (GALS) systems [10]. Asynchronous NoCs eliminate the need for a
global clock and are therefore free from associated overheads: clock skew, clock tree switching power and complex clock gating circuitry. Several recent examples have
shown signiﬁcant power and area reductions, compared to
the synchronous NoCs, while achieving similar or better
This work was partially supported by NSF Grants CCF1219013 and CCF-1527796.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC’16, June 05-09, 2016, Austin, TX, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4236-0/16/06. . . $15.00
DOI: http://dx.doi.org/10.1145/2897937.2897978
The ﬁrst contribution is a lightweight tree-based parallel
multicast using an asynchronous NoC. The target topology is MoT. To the best of our know ledge,
this is the
ﬁrst general-purpose asynchronous NoC to support multicast. This solution, although simple, is not very eﬃcient
in terms of performance. Further enhancements are therefore proposed for performance, while still maintaining design simplicity.
The second contribution is a novel strategy called local
speculation that achieves high-performance parallel multicast. In local speculation, a packet (unicast or multicast)
is always broadcast at a ﬁxed subset of speculative switches
in the network. To restrict the distance traveled by any
Figure 4: Unicast/multicast simulations: hybrid network
speculative nodes that always send packets on the right
path(s) and throttle any redundant packets from the specFigure 3: New fanout network architectures: (a)-(c) full range
for 8x8 MoT, (d) One possible hybrid network for 16x16 MoT
cation capability. In contrast, even with multicast, the existing fanin network will still direct all packets to their destinations. Hence, this paper focuses only on the fanout nodes.
The micro-architecture of a baseline fanout node is shown
in Figure 2. There are 5 main components: Input Channel
Monitor, Address Storage Unit, two Output Port Modules,
and Ack Module. The Input Channel Monitor detects the
arrival of each ﬂit of a packet. The Address Storage Unit
stores the address of the header ﬂit, which it holds until
after arrival of the tail ﬂit. The two Output Port modules,
which are normal ly opaque, manage routing and ﬂow control
of each output channel. Finally, the Ack Module observes
when either output channel transmits a ﬂit, in unicast trafﬁc, then completes handshaking on the input channel.
The fanout node has relatively simple operation [21].
When a packet arrives on the input channel, the header
is directed to inputs of both Output Port Modules. The
Input Channel Monitor detects the arrival of the ﬂit, enabling storing of its address in the Address Storage Unit.
The monitoring ﬂit-detect also partially-enables both Output Modules; the one receiving the correct address is then
activated, and the ﬂit is sent out on that output channel
along with a toggled Reqout. This signal enables two concurrent operations: closing of the Output Port Module (for
data protection) and enabling of the Ack Module to generate the acknowledge on the input channel. Finally, the
downstream node acknowledges (toggles the Ackin signal),
completing the handshaking on the output channel. Similar
operation occurs for the remaining body and tail ﬂits.
3. PROPOSED APPROACH: OVERVIEW
This section introduces multiple solutions to achieve efﬁcient parallel multicast, using new routing strategy and
network architectures. Protocol optimizations of the new
fanout nodes and ﬁve new networks based on these architectures and optimizations are also presented.
Simple tree-based multicast. The ﬁrst contribution
is a simple tree-based parallel multicast, applied for the ﬁrst
time to a general-purpose asynchronous NoC. Routing of a
unicast packet is the same as in the baseline network. The
fanout network architecture, in Figure 3(a), has all nonspeculative nodes. New fanout nodes are designed to handle
parallel replication, as described in Section 4(b).
Source routing is used to encode the address for every
fanout node on each path to the destination(s). The address
at each fanout node must encode 3 symbols: top route,
bottom route or both. Therefore, 2-bit encoding is used for
the address ﬁeld of each fanout node.
This basic tree-based multicast is simple but not eﬃcient
in terms of latency and throughput. The new fanout nodes
are slow due to expensive route computation and channel allocation protocols, required to handle a more complex set of
transmission modes. Another limitation is that the source
routing, as described above, leads to low packet coding eﬃciency, which does not scale with larger network sizes.
Local speculation-based multicast. A new strategy,
local speculation, is introduced for high-performance parallel multicast.
In local speculation, a subset of fanout
nodes are speculative and always broadcast a multicast
(or unicast) packet. These nodes are surrounded by nonarrives on the input channel, it is directed to both output
channels, along with the generation of Reqouts. These Reqouts perform two concurrent operations: close the Output
Port Modules for data protection, and enable Ack Module
to generate Ack. Finally, when the downstream nodes toggle
Ackin(s), handshaking is complete on the output channel(s).
(b) Unoptimized non-speculative fanout node.
Structure. The micro-architecture of the new node is
shown in Figure 5(b). Overall, the structure is similar to the
baseline fanout with identical key components. However, all
these units are now more complex, to support parallel replication for multicast and throttling of any misrouted packets.
As illustrated in Section 3, these nodes use a 2-bit source
routing address to support multicast. The address is passed
through the Input Channel Monitor, both as routing information for the Output Port Modules and also to notify the
Ack Module of any misrouted packets on the input channel. The Ack Module observes the output channels and
the input channel. Handshaking is completed on the input
channel for three cases:
if a ﬂit is sent out on exactly one
of the output channels, or both output channels, or if it is
a misrouted ﬂit. The Ack Module also notiﬁes the Output
Port Module(s) control unit, when a ﬂit has been sent on
the corresponding output channel(s).
Operation. Three packet types can arrive on a node’s input channel: unicast, multicast (going to one or two output
ports), and a misrouted packet from the previous node.
In case of a unicast packet, the header is ﬁrst directed to
both Output Port Modules. The Input Channel Monitor
detects the ﬂit arrival, enables storing of the address and
also partly enables both Output Port Modules. Depending
on the address, the monitor generates the top-route/botroute routing signals to enable the correct Output Port
Module(s). Generation of Reqout leads to three concurrent
operations: closing the Output Port Module for data protection, enabling Ack Module to generate Req0/1 sent control
signals, and completing handshaking on the input channel.
Req0/1 sent signals identify to the Output Port Modules if
the ﬂit on the input channel is new or stale; they disable
the Output Port Module, right after a ﬂit is sent out on the
output channel, hence avoiding any potential resampling.
Similar operations occur for body/tail ﬂits.
In case of multicast packet, if intended for exactly one direction, a similar protocol to the unicast packet is followed.
For multicast going to both outputs, the Input Channel
Monitor enables both output ports for routing. After Reqouts are generated on both output channels, in parallel, the
Ack Module completes handshaking on the input channel.
All internal operations (data protection, no resampling) are
similar to unicast, but now done for both Output Port Modules. Finally, for a misrouted packet on the input channel,
the Input Channel Monitor detects this packet, and enables
Ack Module to complete handshaking on the input channel.
(c) Optimized speculative fanout node. The idea of
this optimization is to speculate on the header ﬂit, then use
actual address information to switch to a non-speculative
operation for all body ﬂits, thereby saving power.
Structure. There are three main diﬀerences from the basic
speculative nodes: (i) the new Input Channel Monitor is
instrumented to detect the arrival of ﬂits, and the tail, on
the input channel, (ii) more complex Output Port Modules,
which can revert to non-speculative mode for body ﬂits, and
(iii) a new Ack Module that generates Ack for two diﬀerent
cases: body ﬂits which are routed correctly only on one
output channel, all other ﬂits. For the former, Ack is sent
after the ﬂit is routed on exactly one output channel; for the
latter, the protocol is same as the basic speculative nodes.
Operation. Once a header arrives,
it is speculatively
routed on both output channels.
Its address is used by
the Output Port Modules to identify the correct route (top,
bottom, or both), and to block the incorrect route for all
body ﬂits. These modules return to their default normally
Figure 5: Unoptimized fanout nodes: (a) speculative, (b) nonspeculative
ets intended for only one direction. Routing of the header
is used by the node to close the output port on the wrong
path before the trailing body ﬂits arrive. Therefore, no redundant copies are created for these body ﬂits. Arrival of
tail ﬂit is used by the node to return to always broadcast
state. These optimized nodes are called as power-optimized
speculative nodes, and are described in Section 4(c).
Latency and throughput of non-speculative nodes is optimized using channel pre-allocation. Routing of the header
is used to reserve the correct output channel(s) for the remainder of the packet (body/tail ﬂits). These ﬂits are then
fast forwarded through the output channel(s) after their arrival, optimizing latency of these ﬂits and improving overall
network latency and throughput. These optimized nodes
are called as performance-optimized non-speculative nodes,
and are described in Section 4(d).
Target parallel multicast networks. The above parallel multicast approaches are incorporated into ﬁve network
conﬁgurations, representing three distinct points in the design space of speculative architectures: (i) non-speculative,
(ii) hybrid, and (iii) almost ful ly speculative. The goal is
to explore the tradeoﬀs associated with varying degrees of
speculation and protocol optimizations.
In particular, the paper targets two non-speculative networks (BasicNonSpeculative, OptNonSpeculative),
two hybrid networks (BasicHybridSpeculative, OptHybridSpeculative), and one extreme case of a nearly fully
speculative network, with non-speculative nodes only at its
leaves (OptAllSpeculative). To support the design of
these networks, four distinct fanout nodes are introduced.
4. PROPOSED FANOUT NODE DESIGNS
This section presents the design and operation of the new
fanout nodes, which are the main building blocks of the new
parallel multicast networks. The basic new networks are
composed of the unoptimized fanout nodes, and the more
advanced new networks are composed of the power- and
performance-optimized fanout nodes, discussed in turn.
(a) Unoptimized speculative fanout node.
Structure. The node’s micro-architecture is shown in Figure 5(a). There are three main diﬀerences from the baseline
fanout: (i) drastically simpliﬁed design, due to elimination
of the Input Channel Monitor and Address Storage Unit; (ii)
new normally-transparent Output Port Modules; and (iii)
a new Ack Module. The node does no route computation,
therefore an Address Storage Unit is not needed. In addition, since the node simply broadcasts every ﬂit arriving on
the input channel, Output Port Modules are also considerably simpliﬁed and now only do ﬂow control with normallytransparent data registers. Finally, the Ack Module now
completes handshaking on the input channel only after a
ﬂit is sent on both output channels, hence a C-element is
used [12], as opposed to an XOR gate in baseline.
Operation. This node has a very simple operation, which
is the same for any type of packet and its ﬂits: always
broadcast. A packet on the input channel can be a correctly
routed unicast, or multicast going to either or both outputs,
or any misrouted packet from previous node. When a ﬂit
transparent state after the tail arrives. Therefore, the tail
also gets speculatively routed to both output channels.
(d) Optimized non-speculative fanout node. The
basic idea of optimization is to use the header to pre-allocate
the correct output channel(s) for body/tail. These ﬂits are
fast forwarded on their arrival, without route computation
and output channel allocation.
Structure and operation. There is only one key diﬀerence
in terms of more simpliﬁed Output Port Modules. Based on
the routing of the header, these modules now pre-allocate
the correct channel for trailing body/tail ﬂits. The routing
of the tail is used to release the channel.
5. EXPERIMENTAL RESULTS
This section presents the experimental framework for
evaluation of the new parallel multicast solutions, along
with node and network-level results on area, performance,
power, and addressing overhead.
5.1 Experimental Framework
Experimental case studies. Two distinct case studies
are used to evaluate the proposed parallel multicast solutions: (a) contribution trajectory, and (b) architectural design space exploration. The contribution tra jectory incrementally evaluates the eﬀectiveness of each contribution,
against a serial baseline: parallel multicast, local speculation and a hybrid network, and protocol optimizations.
Architectural design space exploration, on the other hand,
only evaluates the eﬀects of varying degrees of speculation
on the new parallel multicast networks. To isolate the focus,
only optimized networks are targeted, thereby eliminating
any interference from the optimization strategies.
The contribution tra jectory compares 4 networks: (i)
Baseline [21], only supporting serial multicast; (ii) BasicNonSpeculative, using simple tree-based parallel multicast;
(iii) BasicHybridSpeculative, using local speculation in a hybrid network; and (iv) OptHybridSpeculative, similar to the
previous one, but including protocol optimizations.
The architecture design space exploration compares
3 optimized new networks with varying degrees of speculation:
(i) OptNonSpeculative, with no speculation; (ii)
OptHybridSpeculative, with local speculation; and (iii) OptAl lSpeculative, with almost full speculation.
Experimental setup. Six diﬀerent 8x8 MoT networks
are implemented using FreePDK Nangate 45 nm technology.
Designs are technology-mapped and pre-layout. Six types of
nodes are implemented, as building blocks: ﬁve fanout and
one fanin. Nodes are mapped to the Nangate standard cell
library in the Cadence Virtuoso tool. Accurate gate-level
models are extracted using the Spectre simulator (typical
process corner), to determine rise/fall times for every I/O
path of each gate. Channel lengths and delays are borrowed
from a synchronous MoT chip [21] and scaled to 45 nm
technology. These extracted models of nodes and channels
are used to implement the networks in structural Verilog.
An asynchronous NoC simulator is used for both unicast
and multicast traﬃc. It includes a Programming Language
Interface (PLI) to connect a C-based traﬃc generator and
test environment to the technology-mapped network. A
ﬁxed packet size of 5 ﬂits is used. Injection of headers of
diﬀerent packets follows an exponential distribution. A procedure similar to [2] is followed to ensure long warmup and
measurement phases. Two steps are used to measure power:
(i) record and annotate precise switching activity of every
wire in the network over a benchmark run, and (ii) compute
total power using the Synopsys PrimeTime tool.
Benchmarks. Experiments are conducted on six synthetic benchmarks. There are 3 unicast benchmarks [2]:
1) Uniform random, 2) Bit permutation:shuﬄe, and 3)
Hotspot. There are 3 multicast benchmarks: 4) Multicast5
and 5) Multicast10, where all sources inject multicast traﬃc
at rates of 5% and 10%, respectively, to random subsets of
destinations, and otherwise do uniform random unicast, and
6) Multicast static, where 3 sources perform only random
multicast, and the others do only uniform random unicast.
5.2 Node- and Network-Level Results
(a) Node-level results. Area and latency of the four
new fanout nodes (Section 4) and Baseline fanout were evaluated. The unoptimized speculative nodes, due to their simplicity, have signiﬁcantly lower area and latency (247 µm2 ,
52 ps) than Baseline (342 µm2 , 263 ps). The more complex
unoptimized non-speculative nodes have only small overhead (406 µm2 , 299 ps) over Baseline. The optimized speculative nodes have moderate cost increases (373 µm2 , 120
ps) over unoptimized, but will provide substantial networklevel power savings (see below). Interestingly, the optimized
non-speculative nodes have slightly lower costs (366 µm2 ,
279 ps) than the unoptimized ones.
(b) Contribution tra jectory. This ﬁrst case study
explores the incremental impact of each key contribution:
parallel multicast, local speculation, and optimizations.
Network latency. Figure 6(a) shows the average network
latency results. We measure latency of each network at
25% of the saturation throughput of that network, up to
the arrival of all headers at destinations. This load is high
enough to show the impact of diﬀerent benchmarks, while
keeping the network largely uncongested. Moreover, long
warmup and measurement times are used, for example, for
Uniform Random/Multicast static benchmarks, warmup is
320 ns/640 ns, and measurement is 3200 ns/6400 ns with
injection of 2100/4000 ﬂits at each active source.
For multicast benchmarks, the simple tree-based parallel multicast network, BasicNonSpeculative, obtained signiﬁcant beneﬁts over the serial Baseline, from 39.1% (Multicast5) to 74.1% (Multicast static), highlighting the severe
overheads of the serial multicast approach. The BasicHybridSpeculative and OptHybridSpeculative show further improvements of 10.5-14.9% and 17.8-21.4%, respectively, over
the BasicNonSpeculative, illustrating the individual beneﬁts
of hybrid design and optimizations.
For unicast benchmarks, BasicNonSpeculative incurs a
small latency overhead over Baseline: since unicast is serial,
the added node complexity to support parallel multicast becomes an overhead. However, the two hybrid networks provide noticeable beneﬁts over BasicNonSpeculative, following
similar trends as observed with multicast benchmarks. Interestingly, these latter results show that local speculation
can signiﬁcantly accelerate unicast traﬃc due to very fast
speculative nodes.
Saturation throughput. Table 1 shows saturation throughput results. For multicast benchmarks, the new simple parallel network, BasicNonSpeculative, shows considerable beneﬁts over the serial Baseline, ranging from 14.8% (Multicast5) to 39.5% (Multicast static). The two hybrid networks exhibit additional
improvements up to 9.5% and
19.7%, respectively, over BasicNonSpeculative, demonstrating that local speculation, with accelerated packet transmission, provides a higher threshold for saturation.
For unicast benchmarks,
results are more complex.
Hotspot is highly-adversarial, with identical throughput for
every network. For Uniform random, the OptHybridSpeculative network showed substantial improvements (28.0%) over
BasicNonSpeculative. For Shuﬄe, two new networks show
moderate throughput degradation (BasicNonSpeculative,
BasicHybridSpeculative ) over the Baseline, while OptHybridSpeculative obtains 32.8% higher throughput than BasicNonSpeculative and 9.5% higher than Baseline.
Total network power. Table 1 shows power results for 4
benchmarks. An injection rate that is 25% saturation load
measured in Baseline, for a normalized comparison of energy per packet. Overall, as expected, Baseline has the
lowest power due to its low complexity and serial multicast approach. BasicNonSpeculative has moderate overhead
over Baseline (5.8-11.9%), due to more complex nodes. The
overhead increases signiﬁcantly for BasicHybridSpeculative
(a) Contribution tra jectory
(b) Architectural design space exploration
Figure 6: Network latency at 25% saturation load of respective networks
Schemes/benchmarks
Uniformrandom
Baseline
BasicNonSpeculative
BasicHybridSpeculative
OptHybridSpeculative
OptNonSpeculative
OptHybridSpeculative
OptAllSpeculative
1.26
1.25
1.42
1.60
1.52
1.60
1.65
1.48
1.22
1.25
1.62
Saturation Throughput (GF/s)
Total Network Power (mW)
Shuﬄe Hotspot MultiMultiMulticastUniformHotspot MultiMulticast5
cast10
static
random
cast5
cast10
Contribution tra jectory
1.28
1.28
1.29
12.6
1.47
1.63
1.80
14.1
1.61
1.73
1.87
15.6
1.76
1.84
1.96
13.9
Architecture design space exploration
1.72
1.82
1.93
13.1
1.76
1.84
1.96
13.9
1.78
1.84
1.96
16.1
0.29
0.29
0.29
0.29
3.8
4.2
4.5
4.1
3.9
4.1
4.6
14.7
16.0
17.4
15.7
15.0
15.7
17.8
17.1
18.1
19.4
17.6
17.0
17.6
19.5
1.57
1.62
1.70
0.29
0.29
0.29
Table 1: Saturation throughput and total network power results
Mesh-of-Trees based asynchronous NoCs. A new strategy, local speculation, is introduced, where ﬁxed speculative switches always broadcast, but redundant packets are
restricted to small regions. A hybrid network architecture is
proposed, mixing speculative and non-speculative switches.
For multicast, the network achieves 17.8-21.4% improvements in network latency with small power reductions over
a tree-based non-speculative approach. The approach is the
ﬁrst general-purpose multicast for asynchronous NoCs. For
future work, we plan to extend the approach to larger MoT
networks, alternative topologies (e.g. 2D-mesh), as well as
synchronous NoCs.
7. "
PICO - mitigating heterodyne crosstalk due to process variations and intermodulation effects in photonic NoCs.,"Photonic networks-on-chip (PNoCs) employ photonic waveguides with dense-wavelength-division-multiplexing (DWDM) for signal traversal and microring resonators (MRs) for signal modulation, to enable high bandwidth on-chip transfers. Unfortunately, DWDM increases susceptibility to intermodulation effects, which reduces signal-to-noise ratio (SNR) for photonic data transfers. Additionally, process variations induce variations in the width and thickness of MRs causing resonance wavelength shifts, which further reduces SNR, and creates communication errors. This paper proposes a novel framework (called PICO) for mitigating heterodyne crosstalk due to process variations and intermodulation effects in PNoC architectures. Experimental results indicate that our approach can improve the worst-case SNR by up to 4.4× and significantly enhance the reliability of DWDM-based PNoC architectures.","PICO: Mitigating Heterodyne Crosstalk Due to Process  
Variations and Intermodulation Effects in Photonic NoCs 
Sai Vineel Reddy Chittamuru, Ishan G Thakkar, Sudeep Pasricha 
Department of Electrical and Computer Engineering 
Colorado State University, Fort Collins, CO, U.S.A. 
{sai.chittamuru, ishan.thakkar, sudeep}@colostate.edu 
ABSTRACT 
Photonic networks-on-chip (PNoCs) employ photonic waveguides 
with dense-wavelength-division-multiplexing (DWDM) for signal 
traversal and microring resonators (MRs) for signal modulation, to 
enable high bandwidth on-chip transfers. Unfortunately, DWDM increases susceptibility to intermodulation effects, which reduces signal-to-noise ratio (SNR) for photonic data transfers. Additionally, 
process variations induce variations in the width and thickness of 
MRs causing resonance wavelength shifts, which further reduces 
SNR, and creates communication errors. This paper proposes a 
novel framework (called PICO) for mitigating heterodyne crosstalk 
due to process variations and intermodulation effects in PNoC architectures. Experimental results indicate that our approach can improve the worst-case SNR by up to 4.4× and significantly enhance 
the reliability of DWDM-based PNoC architectures. 
Categories and Subject Descriptors: C.4 [Performance of Systems]: Reliability, availability, and serviceability 
General Terms – Reliability, Performance, Experimentation 
Keywords – Process Variations, Crosstalk, Photonic NoCs 
1. INTRODUCTION 
Recent developments in the area of silicon photonics have enabled the integration of photonic components with CMOS circuits on 
a chip. Several photonic network-on-chip (PNoC) architectures have 
been proposed to date (e.g., [1]-[3]). These architectures employ onchip photonic links that use microring resonator (MR) modulators to 
modulate electrical signals onto photonic signals that travel through 
a silicon waveguide, and MR filter receivers that detect and drop 
photonic signals on to a photodetector to recover an electrical signal. 
Each MR has a unique set of (resonance) wavelengths that it can 
couple to and work correctly with. Typically, silicon waveguides are 
designed to support dense wavelength division multiplexing 
(DWDM), where a large number of wavelengths are multiplexed in 
the waveguide. The use of multiple MRs that are in resonance with 
these wavelengths enables high bandwidth parallel data transfers.  
Prior work indicates that heterodyne crosstalk is a major contributor of crosstalk noise in DWDM-based waveguides, which reduces 
photonic signal SNR and reliability in PNoCs [4]. Heterodyne crosstalk noise occurs at a detector MR when it picks up some non-resonant optical power from neighboring wavelengths. The strength of 
the heterodyne crosstalk noise at a detector MR depends on the following three attributes: (i) channel gap between the MR resonant 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request 
permissions from Permissions@acm.org. 
DAC '16, June 05-09, 2016, Austin, TX, USA 
© 2016 ACM. ISBN 978-1-4503-4236-0/16/06…$15.00 
DOI: http://dx.doi.org/10.1145/2897937.2898063 
wavelength and the adjacent wavelengths; (ii) Q-factors of neighboring detector MRs, and (iii) the strengths of the non-resonant signals at the detector. With increase in DWDM, the channel gap between two adjacent wavelengths decreases, which in turn increases 
heterodyne crosstalk in detector MRs. With decrease in Q-factors of 
MRs, the widths of the resonant passbands of MRs increases, increasing passband overlap among neighboring MRs, which in turn 
increases heterodyne crosstalk. The strengths of the non-resonant 
signals depend on the losses faced by the non-resonant signals 
throughout their path from the laser source to the MR detector.  
Intermodulation (IM) crosstalk has the biggest influence on the 
last attribute discussed above, causing loss of non-resonant signals 
in a DWDM waveguide [5]. IM crosstalk occurs when a modulator 
MR truncates and consequently modulates the passbands of the 
neighboring non-resonant signals. Thus the level of heterodyne 
crosstalk and resultant SNR at the detector depends on the amount 
of IM passband truncation at the modulator. This motivates mitigating the effects of IM passband truncation on heterodyne crosstalk by 
controlling the strengths of the non-resonant signals at the detector.  
Additionally, fabrication process variations (PV) induce variations in the width and thickness of MRs, which cause resonance 
wavelength shifts in MRs [6][8]. PV-induced resonance shifts reduce the channel gap between the resonances of the victim MRs and 
adjacent MRs, which increases crosstalk and worsens SNR. The 
worsening of SNR deteriorates the bit-error-rate (BER) in a waveguide. For example, a previous study shows that in a DWDM-based 
photonic interconnect, when PV-induced resonance shift is over 1/3 
of the channel gap, BER increases from 10-12 to 10-6 [9]. Techniques 
to counteract the PV-induced resonance shifts in MRs involve realigning the resonant wavelengths by using localized trimming [8] or 
thermal tuning [6]. Localized trimming is the more viable technique 
as it enables faster and finer grained control that is also not impacted 
by on-die thermal variations, unlike thermal tuning. However, our 
analysis has shown that localized trimming increases intrinsic optical signal loss in MRs and waveguides due to the free carrier absorption effect (FCA). This loss decreases Q-factor of MRs, which increases heterodyne crosstalk in MRs and reduces SNR. 
In this paper, we present a novel crosstalk mitigation framework 
called PICO to enable reliable communication in emerging PNoCbased multicore systems. PICO mitigates the effects of IM crosstalk 
by controlling signal loss of wavelengths in the waveguide and reduces trimming-induced crosstalk by intelligently reducing undesirable data value occurrences in a photonic waveguide based on the 
PV profile of MRs. Our framework has low overhead and is easily 
implementable in any existing DWDM-based PNoC without major 
modifications to the architecture. To the best of our knowledge, this 
is the first work that attempts to improve SNR in PNoCs considering 
both IM effects and PV in its MRs. Our novel contributions are:   
• We present device-level analytical models to capture the deleterious effects of localized trimming in MRs. Moreover, we extend this model for system-level heterodyne crosstalk analysis; 
• We propose a scheme for IM passband truncation-aware heterodyne crosstalk mitigation (IMCM) to improve worst-case 
 
 
 
 
 
 
SNR of MRs by controlling non-resonant signal power;  
• We propose a scheme for PV-aware heterodyne crosstalk mitigation (PVCM) to improve worst-case SNR of detector MRs 
by encoding data to avoid undesirable data occurrences; 
• We evaluate our proposed PICO (PVCM+ IMCM) framework 
by implementing it on the well-known Corona crossbar PNoC 
architecture [1], [10], and compare it with two encoding based 
heterodyne crosstalk mitigation mechanisms from [12] for realworld multi-threaded PARSEC benchmarks. 
2. BACKGROUND AND RELATED WORK 
DWDM-based PNoCs utilize several photonic devices such as 
microring resonators (MRs) as modulators and detectors, photonic 
waveguides, splitters, and trans-impedance amplifiers (TIAs). The 
reader is directed to [12] for more discussion on these devices.  
An important characteristic of photonic signal transmission in onchip waveguides is that it is inherently lossy, i.e., the light signal is 
subject to losses such as through-loss in MR modulators and detectors, modulating losses in modulator MRs, detection loss in detector 
MRs, propagation and bending loss in waveguides, and splitting loss 
in splitters. Such losses negatively impact SNR in waveguides. In 
addition to the optical signal loss, crosstalk noise of the constituent 
MRs also deteriorates SNR. Both modulators and detectors are susceptible to crosstalk noise in DWDM-based PNoCs.  
Crosstalk noise can be classified as homodyne or heterodyne. 
Homodyne crosstalk usually occurs in MRs used as optical injectors, 
when an injector MR couples optical power of the same wavelength 
from two different ports to a single output port. Heterodyne crosstalk 
occurs in detector and modulator MRs when an MR picks up some 
optical power from non-resonant signals. As discussed in [4], 
homodyne crosstalk may either contribute to the noise or cause 
fluctuation in the signal power, which makes the analysis and 
mitigation of homodyne crosstalk more complicated and beyond the 
scope of this work. Thus this work focuses on heterodyne crosstalk 
and propose solutions to mitigate it. In the rest of the paper, we use 
the term crosstalk to refer heterodyne crosstalk. 
A few prior works have analyzed crosstalk in PNoCs. The effect 
of crosstalk noise on SNR is shown to be negligible in WDM systems presented in [13] and [14], as these systems use only four 
WDM wavelengths per waveguide. In [5], IM effects are shown to 
be negligible for a WDM link operating at 10 Gb/s. However, in 
PNoC architectures that use DWDM (e.g., Corona [1] with 64 wavelength DWDM), there exists significant crosstalk noise. The damaging impact of crosstalk noise in the Corona PNoC is presented in 
[15], where worst-case SNR is estimated to be 14dB in data waveguides, which is insufficient for reliable data transfers. To mitigate 
the impact of crosstalk noise in DWDM based PNoCs, two encoding 
techniques (PCTM5B and PCTM6B) were presented in [12]. In [7] 
a technique was proposed to increase channel spacing between adjacent DWDM wavelengths, to mitigate crosstalk in MR detectors. 
However, none of these works considers the system-level impact of 
IM effects or PV on crosstalk in DWDM-based PNoCs. 
Fabrication-induced process variations (PV) impact the crosssection, i.e., width and height, of photonic devices such as MRs and 
waveguides.  A few prior works have explored the impact of PV on 
DWDM-based photonic links at the system-level [9][24]. In [9], a 
thermal tuning based approach is presented that adjusts chip temperature using dynamic voltage and frequency scaling (DVFS) to compensate for chip-wide PV-induced resonance shifts in MRs. In [24], 
a methodology to salvage network-bandwidth loss due to PV-drifts 
is proposed, which reorders MRs and trims them to nearby wavelengths. All of these PV-remedial techniques are network specific 
and ignore the harmful effects of PV remedies on crosstalk. Our proposed framework in this paper is different and novel as it considers 
the deleterious effects of IM crosstalk and PV-remedial techniques 
that increase crosstalk noise in detector MRs.  
(a) 
                                                        (b) 
Fig. 1: Impact of PV-induced resonance shifts on MR operation in 
DWDM-based waveguides (note: only PV-induced red resonance shifts 
are shown): (a) MR as active modulator modulating in resonance wavelength with PV-induced red resonance shifts (b) MR as active detector 
detecting its resonance wavelength with PV-induced red shifts. 
3. PV-AWARE CROSSTALK ANALYSIS 
3.1 Impact of Localized Trimming on Crosstalk 
An MR can be considered to be a circular photonic waveguide 
with a small diameter, not to be confused with the larger DWDMbased photonic waveguide for which MRs serve as modulators and 
detectors. Variations in MR dimensions due to PV cause a “shift” in 
the resonance wavelengths of MRs. Fig. 1 shows the impact of PV 
on crosstalk noise (as dotted/dashed lines) in MRs. From Fig. 1(a) it 
can be seen that PV-induced red shifts in MR modulators increase 
crosstalk noise in the waveguide and decrease signal strength of nonresonating wavelengths. Fig. 1(b) shows how PV-induced red shifts 
increase detected crosstalk noise and decrease detected signal power 
of resonance wavelengths in MR detectors, which in turn reduces 
SNR and photonic data communication reliability.  
As discussed earlier, the localized trimming method is essential 
to deal with PV-induced resonance red shifts in MRs. However, the 
use of this method in an MR alters its intrinsic optical properties, 
which leads to increased crosstalk noise and degraded performance 
in PNoCs that use these MRs. In this section, we discuss the effects 
of the localized trimming method on crosstalk and present analytical 
models to capture these effects in MRs. Further, we extend these 
models to generate system-level models for the Corona PNoC [1] in 
order to quantify signal and noise powers in the constituent MRs and 
DWDM waveguides of the Corona PNoC architecture. 
The localized trimming method injects extra free carriers in the 
circular MR waveguide to counteract the PV-induced resonance red 
shifts. The introduction of extra free carriers reduces the refractive 
index of the circular MR waveguide, which in turn induces a blue 
shift in resonance to counteract the PV-induced red shifts. However, 
the extra free carriers increase the absorption related optical loss in 
the MR due to the free carrier absorption effect (FCA) [22]. The increase in the optical loss results in a decrease of MR Q-factor, which 
increases MR insertion loss and crosstalk, as discussed in Section 1. 
change in refractive index ((cid:1986)(cid:1866)(cid:3046)(cid:3036) ) required to counteract this PV-inWe use a PV map (described in more detail in Section 4) to estimate PV-induced shifts in the resonance wavelengths of all the MRs 
across a chip. Then, for each MR device, we calculate the amount of 
duced wavelength shift using the following equation [21]: 
where, (cid:1986)(cid:2019)(cid:3045) is the PV-induced resonance shift that needs to be compensated for, (cid:2019)(cid:3045) is the target resonance wavelength of the MR, ng is 
and (cid:1985) is the confinement factor describing the overlap of the optical 
the group refractive index (ratio of speed of light to group velocity 
of all wavelengths traversing the waveguide) of the MR waveguide, 
of 450nm×250nm. The values of (cid:1985) and (cid:1866)(cid:3034) for these MR waveguides 
mode with the MR waveguide’s silicon core. We assume that the 
MR waveguides used in this study are similar to those reported in 
[22], fabricated using standard Si-SiO2 material with a cross section 
the refractive index change of (cid:1986)(cid:1866)(cid:3046)(cid:3036) at around 1.55µm wavelength 
are set to 0.7 and 4.2 respectively [22]. 
The required change in the free carrier concentration to induce 
(cid:1986)(cid:1866)(cid:3046)(cid:3036) = 	 (cid:3057)(cid:3090)(cid:3293)∗(cid:3041)(cid:3282)
(cid:3056)∗(cid:3090)(cid:3293) ,																							                      (1) 
 
 
 
 
 
 
(cid:1843) (cid:4593) = (cid:1843) + ∆(cid:1843) = 	
∆(cid:1866)(cid:3046)(cid:3036) = −8.8 × 10(cid:2879)(cid:2870)(cid:2870)∆(cid:1840)(cid:3032) − 8.5 × 10(cid:2879)(cid:2869)(cid:2876) (∆(cid:1840)(cid:3035) )(cid:2868).(cid:2876) ,														(2) 
∆(cid:2009)(cid:3046)(cid:3036) = −8.5 × 10(cid:2879)(cid:2869)(cid:2876)∆(cid:1840)(cid:3032) − 6.0 × 10(cid:2879)(cid:2869)(cid:2876)∆(cid:1840)(cid:3035) ,                 (3) 
(cid:3095)(cid:3041)(cid:3282)
 where, ∆(cid:1840)(cid:3032) and ∆(cid:1840)(cid:3035) are the change in free electron concentration 
can be quantified using the following equation [21]: 
in the absorption loss coefficient (∆(cid:2009)(cid:3046)(cid:3036) ) due to the change in free 
and the change in free hole concentration respectively. The change 
carrier concentration (owing to the FCA effect) can be quantified 
using the following equation [21]: 
cient. The relation between the Q-factor and ∆(cid:2009)(cid:3046)(cid:3036) , assuming critical 
The Q-factor of an MR depends on this absorption loss coeffi(cid:3090)(cid:3293) ((cid:3080)(cid:2878)∆(cid:3080)(cid:3294)(cid:3284) ),                              (4) 
coupling of MRs, is given by the following equation [22], where Q’ 
where, ∆(cid:1843) is the change in Q-factor and (cid:2009) is the original loss coefis the loaded Q-factor of the MR: 
ficient, which is a sum of three components: (i) intrinsic loss coefficient due to material loss and surface roughness; (ii) bending loss 
coefficient, which is a result of the curvature in the MR; and (iii) the 
which increases the absorption loss coefficient (positive ∆(cid:2009)(cid:3046)(cid:3036) ). As 
absorption effect factor that depends on the original free carrier concentration in the waveguide core. Typically, the localized trimming 
evident from Eq. (4), a positive value of ∆(cid:2009)(cid:3046)(cid:3036) results in a decrease of 
method injects excess concentration of free carriers into the MR, 
tion. This function is used to represent coupling factor (cid:2740) in Eq. (5) 
the Q-factor. This causes a broadening of the MR passband, which 
between wavelength (cid:2755)i and an MR with resonance wavelength (cid:2755)j. 
results in increased insertion loss and crosstalk power penalties.  
   We model the MR transmission spectrum using a Lorentzian func(Q’) decreases with localized trimming. This in turn increases (cid:2740) and 
factor (cid:2011) in Eq. (6) which is the factor by which signal power of a 
From Eq. (3) and (4), it can be inferred that an MR’s loaded Q-factor 
wavelength (cid:2755)i is reduced when it passes through an MR whose resonance wavelength is (cid:2755)j. Through loss of a wavelength in a waveguide when it passes through an MR is defined as (cid:2011) times the signal 
crosstalk noise. Further, using the same function, we determined loss 
power of all wavelengths received before the MR. 
(cid:2740) and loss factor (cid:2011) from this section to model worst case crosstalk 
In the next section, we use the derived values of coupling factor 
and SNR for the Corona PNoC, in the presence of process variations.  
We characterize crosstalk in DWDM-waveguides for the wellknown Corona PNoC enhanced with token-slot arbitration [1], [10]. 
In DWDM-based waveguides, data transmission requires modulating light using a group of MR modulators equal to the number of 
wavelengths supported by DWDM. Similarly, data detection at the 
receiver requires a group of detector MRs equal to the number of 
DWDM wavelengths. We present analytical equations to model 
worst-case crosstalk noise power, maximum power loss, and SNR in 
detector MR groups (similar equations are applicable to modulator 
MR groups). Before presenting actual equations, we provide notations for the parameters used in the equations, in Tables I and II.  
     The Corona PNoC is designed for a 256 core single-chip platform, 
where cores are grouped into 64 clusters, with 4 cores in each cluster. 
A photonic crossbar topology with 64 data channels is used for communication between clusters. Each channel consists of 4 multiplewrite-single-read 
(MWSR) waveguides with 64-wavelength 
DWDM in each waveguide. As modulation occurs on both positive 
and negative edges of the clock in Corona, 512 bits (cache-line size) 
can be modulated and inserted on 4 MWSR waveguides in a single 
cycle by a sender. A data channel starts at a cluster called ‘homecluster’, traverses other clusters (where modulators can modulate 
(cid:2004)(cid:3435)λ(cid:3036) , λ(cid:3037) , Q′	(cid:3439) = (1 + (2Q′(λ(cid:3036) − λ(cid:3037) )
λ(cid:3037)
)(cid:2870) )(cid:2879)(cid:2869) ,																								(5) 
(cid:2011)(cid:3435)λ(cid:3036) , λ(cid:3037) , Q′(cid:3439) = (1 + (2Q′(λ(cid:3036) − λ(cid:3037) )
λ(cid:3037)
)(cid:2879)(cid:2870) )(cid:2879)(cid:2869) ,																						(6) 
3.2 PV-Aware Crosstalk Models for Corona PNoC 
light and detectors can detect this light), and finally ends at the 
home-cluster again, at a set of detectors (optical termination). A 
power waveguide supplies optical power from an off-chip laser to 
each of the 64 data channels at its home-cluster, through a series of 
1X2 splitters. In each of the 64 home-clusters, optical power is distributed among 4 MWSR waveguides equally using a 1X4 splitter 
with splitting factor RS14. As all 1X2 splitters are present before the 
last (64th) channel, this channel suffers the highest signal power loss. 
Thus, the worst-case signal and crosstalk noise exists in the detector 
group of the 64th cluster node, and this node is defined as the worstcase power loss node (NWCPL) in the Corona PNoC. 
(cid:2755)j
RS12
RS14
RS16
LP 
LB 
LS12
LS14 
LS16 
Table I: Notations for photonic power loss, crosstalk coefficients [15] 
Notation
Parameter type 
Parameter value (in dB)
Propagation loss
-0.274 per cm
Bending loss
-0.005 per 90o
1X2 splitter power loss 
-0.2 
1X4 splitter power loss 
-0.2 
1X6 splitter power loss 
-0.2 
Table II: Other model parameter notations  
Notation
Crosstalk Coefficient 
Parameter Value
Q
Q-factor
9000 
L 
Photonic path length in cm 
B 
Number of bends in photonic path
Resonance wavelength of MR 
Splitting factor for 1X2 splitter 
Splitting factor for 1X4 splitter 
Splitting factor for 1X6 splitter 
noise power (Pnoise((cid:2755)j)) received at each detector with resonance 
For this NWCPL node, the signal power (Psignal((cid:2755)j)) and crosstalk 
wavelength (cid:2755)j are expressed in Eq. (7) and (8). PS((cid:2755)i,	(cid:2755)j) in Eq. (9) 
is the signal power of the (cid:2755)i wavelength received before the detector 
with resonance wavelength (cid:2755)j. K((cid:2755)i) in Eq. (11) represents signal 
power loss of (cid:2755)i before the detector group of NWCPL. (cid:2032)((cid:2755)i,	(cid:2755)j) in Eq. 
(10) represents signal power loss of (cid:2755)i before the detector with resonance wavelength (cid:2755)j within the detector group of NWCPL. Due to PV, 
crosstalk coupling factor ((cid:2740), Eq. (5)) increases with decrease in 
nance wavelength (cid:2755)j	of NWCPL as the ratio of Psignal((cid:2755)j) to Pnoise((cid:2755)j), 
in the detectors. We can define SNR((cid:2755)j) of the detector having resoloaded Q-factor (Q’, Eq. (4)), which in turn increases crosstalk noise 
as shown in Eq. (12).  
     				(cid:1842)(cid:3046)(cid:3036)(cid:3034)(cid:3041)(cid:3028)(cid:3039) (cid:3435)λ(cid:3037) (cid:3439) = (cid:2004)(cid:3435)λ(cid:3037) , λ(cid:3037) , Q′((cid:2874)(cid:2871)×(cid:2874)(cid:2872))(cid:2878)(cid:3037) 	(cid:3439)	(cid:1842)(cid:3020) (cid:3435)λ(cid:3037) , λ(cid:3037) (cid:3439),																								(7)	 
(cid:1842)(cid:3041)(cid:3042)(cid:3036)(cid:3046)(cid:3032) (cid:3435)λ(cid:3037) (cid:3439) = (cid:3533) (cid:2004)(cid:3435)λ(cid:3036) , λ(cid:3037) , Q′((cid:2874)(cid:2871)×(cid:2874)(cid:2872))(cid:2878)(cid:3037) (cid:3439) (cid:4672)(cid:1842)(cid:3020) (cid:3435)λ(cid:3036) , λ(cid:3037) (cid:3439)(cid:4673)
((cid:1861) ≠ (cid:1862)),					(8) 
 											(cid:1842)(cid:3020) (cid:3435)λ(cid:3036) , λ(cid:3037) (cid:3439) = (cid:1837)(λ(cid:3036) )(cid:2032)(cid:3435)λ(cid:3036) , λ(cid:3037) (cid:3439)	(cid:1842)(cid:3036)(cid:3041) ((cid:1861)),																																											(9) 
(cid:2032)(cid:3435)λ(cid:3036) , λ(cid:3037) (cid:3439) = 		(cid:3537) (cid:2011) (cid:4672)λ(cid:3036) , λ(cid:3038) , Q(cid:4593) ((cid:2874)(cid:2871)×(cid:2874)(cid:2872))(cid:2878)(cid:3038) (cid:4673)
,																												 (10) 
(cid:1845)(cid:1840)(cid:1844)(cid:3435)λ(cid:3037) (cid:3439) = 			(cid:1842)(cid:3046)(cid:3036)(cid:3034)(cid:3041)(cid:3028)(cid:3039) (cid:3435)λ(cid:3037) (cid:3439)
			(cid:1842)(cid:3015)(cid:3042)(cid:3036)(cid:3046)(cid:3032) (cid:3435)λ(cid:3037) (cid:3439) ,																																							 (12) 
(cid:1837)(λ(cid:3036) ) = (R(cid:3020)(cid:2869)(cid:2872))((cid:1838)(cid:3020)(cid:2869)(cid:2872) )(L(cid:3017) )(cid:3013) (L(cid:3003) )(cid:3003) ∏ ∏ (cid:2011) (cid:4672)λ(cid:3036) , λ(cid:3037) , Q(cid:4593) (cid:3435)((cid:3041)(cid:2879)(cid:2869))×(cid:2874)(cid:2872)(cid:3439)(cid:2878)(cid:3037) (cid:4673)
(cid:3041)(cid:3036)(cid:2880)(cid:2869)
((cid:3038)(cid:2879)(cid:2869))(cid:2996)(cid:3037)
(cid:3038)(cid:2880)(cid:2869)
(cid:2874)(cid:2871)(cid:3041)(cid:2880)(cid:2869)
(cid:2874)(cid:2872)(cid:3037)(cid:2880)(cid:2869)
, (11)                  
3.3. Modeling PV of MR Devices in Corona PNoC 
We adapt the VARIUS tool [23] similar to prior work [24] to 
model die-to-die (D2D) as well as within-die (WID) process variations in MRs. We consider photonic devices with a silicon (Si) core 
and silicon-dioxide (SiO2) cladding. VARIUS uses a normal distribution to characterize on-chip D2D and WID process variations. The 
key parameters are mean (µ), variance (σ2), and density (α) of a variable that follows the normal distribution. As wavelength variations 
are approximately linear to dimension variations of MRs, we assume 
they follow the same distribution. The mean (µ) of wavelength variation of an MR is its nominal resonance wavelength. We consider a 
DWDM wavelength range in the C and L bands [14], with a starting 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
wavelength of 1550nm and a channel spacing of 0.8nm. Hence, 
those wavelengths are the means for each MR modeled. The variance (σ2) of wavelength variation is determined based on laboratory 
fabrication data [6] and our target die size. We consider a 256-core 
chip with die size 400 mm2 at a 22nm process node. For this die size 
we consider a WID standard deviation (σWID) of 0.61nm [24] and 
D2D standard deviation (σD2D) of 1.01 nm [24]. We also consider a 
density (α) of 0.5 [24] for this die size. With these parameters, we 
use VARIUS to generate 100 process variation maps. Each process 
variation map contains over one million points indicating the PVinduced resonance shift of MRs. The total number of points picked 
from these maps equal the number of MRs in the Corona PNoC. 
the detector group. As the strength of the last ((cid:2755)n) wavelength signal 
is high, the incurred crosstalk noise is also high. 
5. IM-AWARE CROSSTALK MITIGATION 
Based on the observations in the previous section, we propose an 
IM passband truncation aware crosstalk mitigation (IMCM) scheme 
and detecting node. This extra MR is tuned near to the last ((cid:2755)n) 
to decrease crosstalk noise in MRs of DWDM based photonic links. 
In IMCM, to reduce signal strength of the last wavelength in the 
DWDM, we propose placing an additional MR at each modulating 
creases signal loss of this last ((cid:2755)n) wavelength and reduces its signal 
wavelength of DWDM with a tuning distance of half the channel gap 
(CG/2) of the DWDM (as shown in Fig. 2(c)). This extra MR instrength. Thus, it creates uniform signal loss across all wavelengths 
used in the DWDM. This extra MR (passband of this MR is shown 
with a dotted line in Fig. 2(c)) is always maintained in inactive mode 
and reduces the effects of IM crosstalk on the boundary wavelengths 
of DWDM by reducing their respective signal strengths. This mechanism reduces crosstalk in detecting MRs to improve SNR (and thus 
reduce BER). To implement the IMCM technique in the Corona 
PNoC, there is a need to increase the number of MRs in all modulating and detecting nodes by one on their MWSR and SWMR waveguides. The increase in MRs on the waveguides increases through 
loss and laser power. We account for this overhead in our analysis.  
Fig. 2: Transmission spectrum of MR groups with (a) high channel gap 
(CG) (b) low channel gap (CG); (C) IMCM at low channel gap. 
4. IM CROSSTALK ANALYSIS   
crease in loss factor ((cid:2011)). This (cid:2011) increases with a decrease in IM gap, 
Intermodulation (IM) crosstalk occurs when a resonance wavelength of an MR modulator is modulated by the neighboring MR 
modulators. As evident from Eq. (6), signal strength of wavelengths 
and inactive state.	Furthermore, this reduction in signal strength of 
in photonic waveguides of DWDM-based PNoCs decrease with inwhich is the gap between resonance wavelengths of an MR in active 
the resonance wavelength also depends on the channel gap (CG) between two adjacent wavelengths in the DWDM. Fig. 2 shows the 
transmission spectrum of MR groups with high and low CG.  A 
active MRs with wavelengths in the waveguide ((cid:2755)1-(cid:2755)n).	 This IM 
change from low DWDM (Fig. 2(a)) to higher DWDM (Fig. 2(b)) 
reduces the CG and IM gap, which in turn increases IM crosstalk as 
is evident from the intersection of the transmission spectrum of infirst ((cid:2755)1) and the last ((cid:2755)n) wavelengths of DWDM have the lowest 
crosstalk increases wavelength signal loss. 
All MR modulators at a node in a DWDM waveguide have neighbors on both sides except the first and the last modulators. So, the 
tor group, the first ((cid:2755)1) wavelength signal gets filtered and detected 
signal losses and highest signal strengths. Thus, the modulated set of 
DWDM wavelengths that travel along a photonic waveguide to the 
target detector node have varying signal strengths. At an MR detecing neighboring detectors. In contrast, the last ((cid:2755)n) wavelength, 
by the first detector. As a result, the signal strength of the first wavelength becomes negligible. This negligible signal strength of the first 
wavelength does not significantly add crosstalk noise in the succeedthe last detector in the detector group. So, the last ((cid:2755)n) wavelength 
which also has higher signal strength, gets filtered and detected by 
signal has to travel along all the detectors in the group of detector 
rings before being detected. On its way to the last detector, the last 
wavelength signal incurs crosstalk noise in all the detectors across 
Fig. 3: Overview of proposed PVCM technique 
6. PV-AWARE CROSSTALK MITIGATION 
We also propose a PV-aware trimming-induced crosstalk mitigation 
(PVCM) scheme, which is illustrated in Fig. 3. PV-induced red shifts 
can be realigned using localized trimming, but this process worsens 
crosstalk noise. From Eq. (5), crosstalk in MR detectors of DWDMbased PNoCs increases with increase in coupling factor (Φ) and increase in signal strength of an immediate non-resonating wavelength. This implies that the trimming-affected crosstalk in a detector can be reduced by reducing the signal strength of immediate nonresonating wavelengths. Therefore, our proposed PVCM technique 
decreases the signal strength of the immediate non-resonant wavemaximum PV-induced resonance red shift (Δ(cid:2755)max) in each MR 
length by modulating a zero (shielding bit) on it, which reduces 
crosstalk noise in the detector. The PVCM technique first divides 
detecting MRs into groups of 8 MRs each. Then, it determines the 
group. As discussed in [17], the PV-induced resonance shifts in MRs 
can be gauged in situ at system initialization by using a dithering 
signal to generate an anti-symmetric error signal that indicates the 
magnitude of PV-induced resonance shifts. The overhead of this insitu PV detection technique can be considered to be negligible [17]. 
In our analysis, we model and estimate PV in MRs using the VARIUS tool [23], a description of which was given in Section 3.3.  
induced resonance red shift (Δ(cid:2755)max) value for the group. If this value 
Once PV-induced red shifts of MRs are determined, we store inis greater than a threshold red shift value (Δ(cid:2755)th) for an MR group, we 
formation about whether to enable or disable encoding (i.e., injecting 
shield bits between data bits) for each MR group in a read-only 
memory (ROM) at the modulating node, based on the maximum PVthis MR group. MR groups with Δ(cid:2755)max < Δ(cid:2755)th are thus not impacted. 
Only MR-groups with Δ(cid:2755)max > Δ(cid:2755)th employ encoding.  
store a ‘1’ to enable PVCM, else we store a ‘0’ to disable PVCM for 
 
 
 
 
 
 
 
7. PICO FRAMEWORK: SENSITIVITY ANALYSIS 
We combine the IMCM scheme that mitigates the effects of IM 
crosstalk and the PVCM scheme that mitigates the PV-affected 
crosstalk in PNoCs into a holistic crosstalk mitigation framework 
called PICO. As the number of shield bits used in PICO increases, 
laser power and trimming power of PNoCs also increase. Thus, we 
need to limit the number of shield bits. We performed a sensitivity 
analysis using the Corona PNoC with varying number of shield bits 
across 100 process variation maps, we determined the value of Δ(cid:2755)th 
per detector node to quantify its effect on worst-case SNR. We analyzed worst case S"
Shift sprinting - fine-grained temperature-aware NoC-based MCSoC architecture in dark silicon age.,"Reliability is a critical feature of chip integration and unreliability can lead to performance, cost, and time-to-market penalties. Moreover, upcoming Many-Core System-on-Chips (MCSoCs), notably future generations of mobile devices, will suffer from high power densities due to the dark silicon problem. Thus, in this paper, a novel NoC-based MCSoC architecture, called Shift Sprinting, is introduced in order to reliably utilize dark silicon under the power budget constraint. By employing the concept of distributional sprinting, our proposed architecture provides Quality of Service (QoS) to efficiently run real-time streaming applications in mobile devices. Simulation results show meaningful gain in performance and reliability of the system compared to state-of-the-art works.","Shift Sprinting: Fine-Grained Temperature-Aware NoC-based 
MCSoC Architecture in Dark Silicon Age 
Amin Rezaei1, Dan Zhao1, Masoud Daneshtalab2, and Hongyi Wu1 
1 University of Louisiana at Lafayette (ULL), Lafayette, USA 
(me@aminrezaei.com, dzhao@cacs.louisiana.edu, wu@cacs.louisiana.edu) 
2 Royal Institute of Technology (KTH), Stockholm, Sweden 
(masdan@kth.se) 
ABSTRACT 
Reliability is a critical feature of chip integration and unreliability can 
lead to performance, cost, and time-to-market penalties. Moreover, 
upcoming Many-Core System-on-Chips (MCSoCs), notably future 
generations of mobile devices, will suffer from high power densities 
due to the dark silicon problem. Thus, in this paper, a novel NoC-based 
MCSoC architecture, called Shift Sprinting, is introduced in order to 
reliably utilize dark silicon under the power budget constraint. By 
employing the concept of distributional sprinting, our proposed 
architecture provides Quality of Service (QoS) to efficiently run realtime streaming applications in mobile devices. Simulation results show 
meaningful gain in performance and reliability of the system compared 
to state-of-the-art works. 
Keywords 
MCSoC; NoC; Dark Silicon; Sprinting; Reliability; Temperature 
1. INTRODUCTION 
As mobile devices, such as tablets and smart phones, get ever more 
sophisticated in their functionality, more internal heat-producing 
components are added to them. Keeping components cool is one of the 
most important challenges in mobile device industry and becomes even 
more severe by semiconductor 
technology scaling. Moreover, 
overheating causes significant reductions in the operating life of a 
device and uncertainties in reliability can lead to performance, cost, 
and time-to-market penalties [1]. By the same token, for upcoming 
Many-Core System-on-Chips (MCSoCs), dark silicon is anticipated to 
dominate most of the chip area since Dennard scaling [2] fails because 
of the voltage scaling limitations. Notably in mobile devices, which 
have limited cooling options, voltage scaling problem leads to a 
utilization wall [3] in which sustained chip performance is limited 
mainly by power rather than area. It means most of the cores will lie 
inactive (i.e. dark) in upcoming technologies. 
In this paper, a fine-grained NoC-based MCSoC architecture, called 
Shift Sprinting, is introduced in order to reliably utilize dark silicon 
under the power budget constraint. The key contributions are twofold:  
• Proposing a novel NoC architecture to gain high-performance by 
utilizing the intervals between cool-down periods of the cores 
• Presenting an innovative temperature-aware application running 
scheme to gain high-reliability by using simultaneous techniques of 
core sprinting, application migration, and power-gating 
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work 
owned by others than ACM must be honored. Abstracting with credit is 
permitted. To copy otherwise, or republish, to post on servers or to redistribute 
to lists, requires prior specific permission and/or a fee. Request permissions 
from Permissions@acm.org. 
DAC '16, June 05-09, 2016, Austin, TX, USA 
© 2016 ACM. ISBN 978-1-4503-4236-0/16/06…$15.00 
DOI: http://dx.doi.org/10.1145/2897937.2898090 
The rest of the paper is organized as follows. Section 2 reviews 
backgrounds and related works. Preliminaries and motivations of the 
proposed architecture are presented in Section 3. Shift sprinting 
architecture 
including core behavior model, system 
topology, 
application migration scheme, and controlling mechanism is proposed 
in Section 4. Experimental results and comparison with state-of-the arts 
architectures are presented in Section 5. Finally, conclusion and 
dedication are given in Section 6 and Section 7 respectively. 
2. BACKGROUNDS AND RELATED WORKS 
Due to the dark silicon problem, the threshold voltage cannot be scaled 
without exponentially increasing leakage, and as a result, the operating 
voltage should be kept roughly constant. This is an exponentially 
worsening problem that accumulates with each process generation [4]. 
Recent studies [5] have predicted that, on average, 52% of a chip’s area 
will stay dark for the 8nm technology node. The author in [4] has 
discussed four key solutions emerged as top contenders for thriving in 
the dark silicon age. Each class requires a careful understanding of the 
underlying tradeoffs and benefits. 
In addition, Computational Sprinting [6] is presented by using of 
phase-change materials to allow chips to exceed their sustainable 
thermal budget for sub-second durations, providing a short but 
substantial computational boost. Furthermore, NoC-Sprinting [7] is 
proposed, in which the chip selectively sprints to any intermediate 
stages instead of directly activates all the cores in response to shortburst computations. Moreover, a fine-grained voltage scaling [8] is 
proposed in order to allow on-chip voltage regulation.  
The mode-switching in computational sprinting lacks adaptability and 
only considers two states of single-core operation or all-core sprinting. 
However, based on the behaviors of running applications, some in between numbers of active cores may be sufficient for reaching the 
optimal performance speedup with less power consumption. On the 
other hand, NoC-sprinting lacks reliability and does not clearly 
consider the cool-down period requires for each core after sprinting 
phase. By assigning sprinting periods to all or some fixed cores of the 
system, both of the mentioned techniques are categorized in periodical 
sprinting class. However to address periodical sprinting problems, we 
introduce the concept of distributional sprinting by utilizing cool-down 
period to provide high-performance for media streaming while using 
the dark silicon to become with a reliable temperature-aware 
application running scheme. 
3. PRELIMINARIES AND MOTIVATIONS 
According to [9], smart-phone penetration is increased to 78% among 
people. Moreover, around 95% of the devices sold recently are smart phones. Nowadays, users are using their smart-phones not only for 
daily communication but also increasingly for media streaming. Thus , 
the Quality of Service (QoS) of real-time streaming applications will 
become more and more important for future generations of mobile 
devices. Furthermore, reliability issues are highly challengeable in the 
future mobile devices due to the limited cooling options. In short, 
designing reliable mobile devices to provide QoS-aware real-time 
streaming for the users is crucially important in dark silicon age. 
 
 
 
)
s
e
l
c
y
C
(
y
c
n
e
t
a
L
k
r
o
w
t
e
N
e
g
a
r
e
v
A
)
%
(
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
distributional sprinting utilizes cool-down periods and can provide 
appropriate QoS to the users. Moreover, By increasing the traffic 
injection rate, the weakness of periodical sprinting over distributional 
sprinting is fully observed.  
3.2 High-Reliability Demands 
Allowing periodical sprinting to some specific cores while let others 
stay at dark greatly increases the permanent failure probability of those 
highly active cores. Hence, distributing the sprinting periods through 
the whole system can reduce the core malfunction possibility. Figure 2 
demonstrates the failure probability of the system over time in both 
periodical sprinting and distributional sprinting in a 2×2 NoC-based 
MCSoC under the injection rate of 0.5����� . It is supposed that failure 
of a single core leads to the whole system failure. As can be seen, the 
failure probability of periodical sprinting is almost 1.5x as likely as 
distributional sprinting. 
Periodical Sprinting
Distributional Sprinting
75
60
45
30
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Injection Rate (×λfull)
Figure 1. Average network latency for 2×2 NoC-based MCSoC with 
periodical sprinting and distributional sprinting 
Periodical Sprinting
Distributional Sprinting
100
80
60
40
20
0
0.1
0.2
0.3
0.4
Time (×γidle)
0.5
0.6
0.7
0.8
0.9
1
Figure 2. Failure probability for 2×2 NoC-based MCSoC with periodical 
sprinting and distributional sprinting 
As a preliminary study, a 2×2 NoC-based MCSoC is simulated under 
uniform streaming traffic to show the necessity of high -performance 
and high-reliability demands of real-time streaming applications. By an 
optimistic assumption each sprinting period is considered to be equal to 
the cool-down period. Moreover each core in sprinting status is 
supposed to gain performance by a factor of 4 and to lose life-span by a 
factor of 2 compared to the core in nominal status. Maximum traffic 
injection rate and the average life-time of the system running in idle 
status are called ����� and ����� respectively. 
3.1 High-Performance Demands 
It is shown in [6] and [7] that for serving short burst computations, 
interleaving the status of the cores between nominal and sprinting 
along with cool-down intervals can be beneficial. Since in sprinting 
status the core is operating on higher than the Thermal Design Power 
(TDP) constraint, phase change of the core internal materials can be 
used to tolerate such situation. It is assumed that the core temperature 
stays constant for a specified time during the melting phase of the 
materials.  
However, applying short burst computations is not enough to fully 
support real time streaming applications (e.g. watching a live football 
match) because these applications require continues high-computation 
demands in order to provide QoS to the users. Figure 1 shows the 
average network latency for both periodical sprinting and distributional 
sprinting in a 2×2 NoC-based MCSoC. As can be seen, applying 
periodical sprinting to some fixed cores does not solve the problem 
neither since it still requires full cool-down intervals. On the other 
hand, migrating the running application to 
the dark cores in 
Figure 3. Core behavior model of shift sprinting 
4. SHIFT SPRINTING ARCHITECTURE 
In this section, by employing the concept of distributional sprinting, we 
propose a novel fine-grained temperature-aware NoC-based MCSoC 
architecture named Shift Sprinting, targeted at 
increasing both 
performance and reliability of the system in the upcoming age of dark 
silicon. 
4.1 Core Behavior Model 
The core behavior model of shift sprinting is depicted in Figure 3. Each 
core has four main states including Dark, Idle, Active, and Malfunction.  
Dark: In the future generations of MCSoCs, the common state of the 
core is dark. In this state the core is power-gated. 
Idle: After waking up the core, it goes to idle state. (i.e. the core is 
powered on but still no application is assigned to it.) Moreover, after 
the application departure, the core goes to warm status until it cools 
down and reaches the cold status. Then it goes back to dark state. 
Active: When an application arrives to the core, it goes to active state. 
In active state the status is interchangeable between nominal and 
sprinting. In nominal status, the core is operating under the TDP 
constraint. On the other hand, in sprinting status, the co re is operating 
on higher than the TDP for a temporary period in order to accelerate 
the process. 
Malfunction: In the case of fault happening, the core state is changed 
to malfunction. If the fault is temporary, after resolving the problem, 
the core can goes back into the normal cycle; otherwise, it comes to 
permanent failure. 
 
 
 
 
 
 
 
 
 
contrary, in sprinting phases, thermal capacitance of chosen cores is 
increased over short timescales to boost-up the process. (i.e. the 
sprinted cores operate on higher than the TDP .) Based on each 
application characteristics, the number of sprinted cores required to 
provide maximal performance speedup varies. 
In most of the available dynamic application mapping techniques in the 
literature, one core is already dedicated to the Central Manager (CM) 
[10]. In shift sprinting, CM is also used to globally control application 
migration process. In order to accelerate the application migration 
process, in addition to CM, there are three Sub-Managers (SMs) that 
are responsible for collecting information from the other cores. Each 
core sends and receives information from its nearest manager. For 
controlling the application migration process efficiently, the managers 
have created a virtual ring network [11]. As can be seen from Figure 4, 
CM is resided in one corner (i.e. core #1) and three SMs are resided in 
the other corners (i.e. core #5, core #21, and core#25); They formed a 
virtual ring network all together.  
Figure 5. Shift sprinting state diagram 
4.3 Application Migration Scheme 
Based on the behaviors of running applications, if the temperature 
reaches a certain threshold, the application migration (i.e. shifting 
between different phases) will start. Two different upper-bound 
thresholds are defined: One called Maximum Core Temperature 
(MCT) and the other called Maximum Overall Temperature (MOT). 
When the temperature of each core reaches the MCT, the shift happens 
from the current sprinting to the next sprinting phase. On the other 
hand, when the overall temperature of the system reaches the MOT, the 
shift happens from the current sprinting to the next nominal phase. For 
a lower-bound threshold, Average Overall Temperature (AOT) is 
defined. When the overall temperature of the system goes back to the 
AOT, the shift happens from the current nominal to the sprinting 
phase. Shift sprinting state diagram is shown in Figure 5. In dark 
silicon age, the required free cores are guaranteed to be available for 
application migration. With this scheme, instead of waiting for the 
cores to cool-down after each sprinting phase, the intervals between 
cool-down periods of the cores are utilized by migrating the running 
applications to the dark cores and maximizing the sprinting timelines.  
In order to lower the overhead of application migration process, shift 
sprinting is implemented based on a message passing interface called 
MMPI [12], 
in which application mapping 
is 
independent of 
application re-mapping. By changing the application mapping table, 
the application is remapped to another core. Then application state 
Figure 4. A 5×5 shift sprinting architecture 
4.2 System Topology 
Rather than abandon the benefits of transistor density scaling, some 
cores are transiently allowed to operate on higher than the TDP. The 
mobile platform trend shows that the future MCSoCs with the same die 
area as current mobile chips will have enough dark cores (i.e. on 
average 52% [5]) to support additional cores during sprinting [6]. 
Without loss of generality, the topology of shift sprinting is considered 
as the 2-D mesh NoC. As an example we have 8 different phases (i.e. 4 
sprinting and 4 nominal) in shift sprinting shown in Figure 4. The shift 
can happen between different phases whenever necessary. In nominal 
phases, a single core is operating under the TDP constraint. On the 
 
 
 
information is transferred; hence, the migrated application can restore 
execution on a different core. In this case, the application migration 
contributes less communication overhead because application state 
information excludes application code. 
Algorithm 1. Application migration algorithm 
4.4 Controlling Mechanism 
A cost function is needed for CM in order to determine the best 
destination core for application migration. When the threshold in one 
sprinting phase reaches MCT, the running application will migrate to 
the next sprinting phase. The destination cores in the next phase are 
chosen based on the least current temperature order. The system stays 
in this loop (i.e. inner loop of Figure 5) until it reaches MOT threshold. 
For MOT threshold, all the running applications will migrate to the 
next phase manager until the overall temperature of the system goes 
back to AOT. In this case the running applications in the manager 
spread through the dark cores of that phase based on the optimal 
number of sprinted cores required for each application. Then the 
sprinting phase is started again.  
Algorithm 1 shows shift sprinting application migration scheme.. It is 
assumed that only one application can be executed in a sprinted core at 
each time and the application itself knows the optimal number of 
required sprinted cores to provide maximal performance speedup. 
MCT is a static threshold relies on the core materials while both MOT 
and AOT are dynamic thresholds that depend highly on application 
behaviors. Note that obtaining the optimal values for these thresholds 
are beyond the scope of this paper. 
5. EXPERIMENTAL RESULTS 
Experiments are performed on a cycle-accurate many-core platform 
implemented in SystemC. A pruned version of an open source 
simulator for mesh-based NoCs called Noxim [13] is utilized as its 
communication architecture. For power and temperature simulations, 
power and thermal models taken from [14], [15] are integrated as 
libraries into the simulator. Some multi-threaded applications from the 
PARSEC [16] benchmark suite are used in the experiments. Maximum 
traffic injection rate and the average life-time of the system running in 
idle status are called ����� and ����� respectively. Three different 
network sizes of 16 (no dark silicon), 36 (55% dark silicon), and 64 
(75% dark silicon) cores are considered 
in 
the simulations. 
Comparisons are also made between shift sprinting (SS) and two stateof-the-arts architectures: computational sprinting (CS) [6] and NoCsprinting (NS) [7]. 
Figure 6. Execution time comparison between different sizes of SS 
Figure 7. Execution time comparison between different architectures 
Figure 8. Network latency comparison between different architectures 
5.1 Performance Evaluation 
Figure 6 shows the normalized execution time of different workloads 
in SS. In comparison with 16-core NoC (no dark silicon), the average 
performance improvement of 64-core NoC (75% dark silicon) is 36% 
and that of 36-core NoC (55% dark silicon) is 24%. As a result, even 
with increasing of dark silicon in upcoming MCSoCs, the performance 
0
0.2
0.4
0.6
0.8
1
Blackscholes
Facesim
Fluidanimate
Vips
X264
Average
N
r
o
m
a
i
l
e
z
d
E
c
e
x
u
i
t
n
o
T
i
m
e
SS-16 (no dark silicon)
SS-36 (55% dark silicon)
SS-64 (75% dark silicon)
0
0.2
0.4
0.6
0.8
1
Blackscholes
Facesim
Fluidanimate
Vips
X264
Average
N
r
o
m
a
i
l
e
z
d
E
c
e
x
u
i
t
n
o
T
i
m
e
SS-64 (75% dark silicon)
NS-64 (75% dark silicon)
CS-64 (75% dark silicon)
0
0.2
0.4
0.6
0.8
1
Blackscholes
Facesim
Fluidanimate
Vips
X264
Average
N
r
o
m
a
i
l
e
z
d
N
e
t
w
r
o
k
L
a
t
e
n
y
c
SS-64 (75% dark silicon)
NS-64 (75% dark silicon)
CS-64 (75% dark silicon)
���: maximum core temperature 
���: maximum overall temperature 
���: average overall temperature 
�: set of all the cores 
�: phase of the system (i. e. phases 1 to 4)  
�� : set of the dark cores in ��ℎ phase 
�� : , manager core in ��ℎ phase 
�� : temperature of the core �� ∈ � 
�: overall temperature of the system 
set the value of ��� 
initiate phase � 
while   true   do 
     set the values of ��� and ��� 
     if   � is in ��������� phase  then 
          if  � < ���   then 
               for   ∀ �� ∈ �   do 
                    if    �� > ���   then 
                          choose  �� ∈ ��+1 with min [�� ] 
                          migrate the running application from �� to �� 
                    end                            
               end 
               start ��������� phase in � + 1  
          else  // i.e. � ≥ ��� 
               for   ∀ �� ∈ �   do 
                     migrate the running application from �� to ��+1 
               end 
               start ������� phase in � + 1  
          end 
     else   // i.e. � is in ������� phase 
          if   � < ���   then 
               while  ∃ running application in ��  do 
                     choose multiple �� s ∈ �� with min [�� ]s 
                     migrate the running application from �� to chosen �� s   
                end                     
                start ��������� phase in � 
          end 
     end 
      set the next phase as � 
end 
 
 
 
 
 
 
 
 
 
 
 
 
of SS will still improve. This happens because SS utilize cool -down 
periods by activating dark cores.  
Figure 7 demonstrates the normalized executing time of different 
workloads with different architectures for 64-core NoC (75% dark 
silicon). The results show that SS considerably reduces the execution 
time compared to other approaches. It achieves 55% and 25% average 
performance improvement compared to CS and NS respectively. The 
performance gain is based on the higher overall sprinting periods 
achieved by utilizing cool-down periods. As can be seen from Figure 7, 
all the architectures performed quite the same under “Blackscholes” 
workload that is a non-streaming financial analysis application. This is 
because “Blackscholes” achieves the optimal performance speedup in 
CS and hence leave no space for power-gating in NS and neither 
power-gating nor application migration in SS. 
In addition, Figure 8 shows the normalized network latency of d ifferent 
workloads with different architectures for 64-core NoC (75% dark 
silicon) under the injection rate of 0.75����� . It can be seen that SS 
reduces the communication latency for all the applications (28% in 
average) in comparison with CS and for most of the applications (11% 
in average) in comparison with NS. Overall, SS performs quite well in 
media processing applications (e.g. Vips and X264). On the other hand, 
performance degradation of SS in the animation workloads (e.g. 
Facesim and Fluidanimate) compared with NS is due to application 
migration overhead. Although by applying MMPI [12], the application 
migration contributes less communication overhead, still more attempts 
are required to minimize this overhead by finding optimal thresholds 
and presenting low-overhead migration mechanisms. 
5.2 Power Consumption Measurement 
Figure 9 displays the normalized network power consumption of 
different workloads with different architectures for 64-core NoC (75% 
dark silicon) under the injection rate of 0.75����� . On average, SS 
saves 58% power compared to CS while consuming almost the same 
power as NS. This is due to the fact that both SS and NS can adopt 
network topology based on an intermediate number of sprinted cores 
and benefit from power-gating in dark cores (i.e. 75% of the cores) 
while CS 
fully actives 
the network and 
loses power-gating 
opportunities. 
As another evaluation parameter, Figure 10 depicts the normalized 
Energy Delay Product (EDP) of different workloads with different 
architectures for 64-core NoC (75% dark silicon). It can be seen that 
even with the overhead of application migration approach, the average 
EDP of SS in the media processing applications (e.g. Vips and X264) 
is less than the other architectures that make it a promising architecture 
for future MCSoC mobile devices. 
5.3 Thermal Analysis 
Figure 11 demonstrates the thermal analysis of SS under X264 
workload for 36-core NoC (55% dark silicon). The peak temperature is 
322.8K and the average temperature of the system is 298.9K, 304.3K, 
and 312.5 after one, two, and three consecutive sprinting respectively. 
Since SS uses simultaneous techniques of core sprinting, application 
migration, and power-gating to distribute the heat across the chip, it 
can efficiently avoid hot-spots in the system. 
Furthermore, Figure 12 displays the thermal analysis of different 
architectures under X264 workload for 36-core NoC (55% dark 
silicon) after four consecutive sprinting. As shown in Figure 12a, CS 
results in a hot-spot in the center of the chip. Moreover, since thermalaware floor-planning of NS, tries to physically separate logical 
connected cores, heat is distributed to the corners of the chip as 
depicted in Figure 12b. Such floor-planning proposal has three 
disadvantages: First, it requires additional overheads at design stage; 
Second, it is highly application-specific and is not suitable for dynamic 
workloads; Third, it leads to performance degradation due to longdistance communications between physically separated cores. On the 
other hand, as shown in Figure 12c, SS outperforms the other two 
architectures to efficiently distribute the heat across the chip. First, it 
does not add any temperature-aware floor-planning overhead to the 
system; Second, it does not rely on specific running app lications to 
avoid hot-sports; Third, there is no need to change the physical 
positions of the cores.  
SS-64 (75% dark silicon)
NS-64 (75% dark silicon)
CS-64 (75% dark silicon)
r
e
w
o
P
k
r
o
w
t
e
N
d
e
z
i
l
a
m
r
o
N
P
D
E
d
e
z
i
l
a
m
r
o
N
1
0.8
0.6
0.4
0.2
0
Blackscholes
Facesim
Fluidanimate
Vips
X264
Average
Figure 9. Network power comparison between different architectures 
1
0.8
0.6
0.4
0.2
0
SS-64 (75% dark silicon)
NS-64 (75% dark silicon)
CS-64 (75% dark silicon)
Blackscholes
Facesim
Fluidanimate
Vips
X264
Average
Figure 10. EDP comparison between different architectures 
5.4 Reliability Assessment 
The central hot-spot in CS and the angular hot-spots in NS greatly 
increase the permanent failure probability of those highly act ive cores 
due to frequent phase-change of the core internal materials. Figure 13 
demonstrates the failure probability of different architectures over time 
under X264 workload for 36-core NoC (55% dark silicon) under the 
injection rate of 0.5����� . It is assumed that failure of a single core 
leads to the whole system failure. It can be seen that fair core 
unitization in SS results in not only efficiently distributing the heat 
across the chip, but also increasing the reliability of the system. In 
other words, in SS the cores are aging almost evenly. On the contrary, 
there are some central aged cores in CS as well as some cornered aged 
ones in NS through time while the others are still young. The aged 
cores are increasingly subjected to failure than the young ones. This 
fact makes the requirement of fault-tolerant mechanisms inevitable in 
CS and NS. 
6. CONCLUSION  
Among all the challenges the mobile device industry faces, keeping 
components cool is the most important, since overheating causes 
significant reductions in the operating life of a device. Moreover, QoS 
of real-time streaming applications will become more and more 
important for future generations of mobile devices. 
In this paper, a novel fine-grained temperature-aware NoC-based 
MCSoC architecture, called shift sprinting was introduced in order to 
handle high-performance QoS-aware mobile demands by reliably 
utilizing dark silicon. Simulation results reported meaningful gain in 
performance and reliability of the system compared to state-of-the-art 
works. 
 
 
 
 
 
 
                                                             (a)                                                            (b)                                                           (c) 
Figure 11. Thermal distribution in SS-36 (55% dark silicon) under X264 workload after (a) one (b) two (c) three consecutive sprinting 
                                                             (a)                                                            (b)                                                           (c) 
Figure 12. Thermal distribution comparison between different architec tures under X264 workload (a) CS-36 (b) NS-36 (c) SS-36 (55% dark silicon) 
)
%
(
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
CS-36 (55% dark silicon)
NS-36 (55% dark silicon)
SS-36 (55% dark silicon)
100
80
60
40
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (×γidle)
Figure 13. Reliability comparison between different architectures 
7. DEDICATION 
This work is dedicated to my wonderful grandfather, Mr. Akbar 
Hemami, who passed away recently. I did not get a chance to see him 
again before his death nor to participate in his funeral; Because I was 
more than 7000 miles away working on this paper... 
8. "
Voltage-Frequency Island Partitioning for GALS-based Networks-on-Chip.,"Due to high levels of integration and complexity, the design of multi-core SoCs has become increasingly challenging. In particular, energy consumption and distributing a single global clock signal throughout a chip have become major design bottlenecks. To deal with these issues, a globally asynchronous, locally synchronous (GALS) design is considered for achieving low power consumption and modular design. Such a design style fits nicely with the concept of voltage-frequency islands (VFIs) which has been recently introduced for achieving fine-grain system-level power management. This paper proposes a design methodology for partitioning an NoC architecture into multiple VFIs and assigning supply and threshold voltage levels to each VFI Simulation results show about 40% savings for a real video application and demonstrate the effectiveness of our approach in reducing the overall system energy consumption. The results and functional correctness are validated using an FPGA prototype for an NoC with multiple VFIs.","8.1
Voltage-Frequency Island Partitioning for GALS-based   
Networks-on-Chip
Umit Y. Ogras, Radu Marculescu, Puru Choudhary, Diana Marculescu
Department of Electrical and Computer Engineering
Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: {uogras,radum,puruc,dianam}@ece.cmu.edu
ABSTRACT
Due to high levels of integration and complexity, the design of
multi-core SoCs has become increasingly challenging. In particular, energy consumption and distributing a single global clock signal throughout a chip have become major design bottlenecks. To
deal with these issues, a globally asynchronous, locally synchronous (GALS) design is considered for achieving low power consumption and modular design. Such a design style fits nicely with
the concept of voltage-frequency islands (VFIs) which has been
recently introduced for achieving fine-grain system-level power
management. This paper proposes a design methodology for partitioning an NoC architecture into multiple VFIs and assigning supply and threshold voltage levels to each VFI. Simulation results
show about 40% savings for a real video application and demonstrate the effectiveness of our approach in reducing the overall system energy consumption. The results and functional correctness are
validated using an FPGA prototype for an NoC with multiple VFIs.
Categories and Subject Descriptors
B.7 [Hardware]: Integrated circuits.
General Terms
Algorithms, Design.
Keywords
Voltage-frequency 
networks-on-chip.
island, GALS, Multi-processor systems,
1. INTRODUCTION
Recognized by the International Roadmap for Semiconductors
as the main bottlenecks in providing increased performance and
platform capabilities, the on-chip communication and power
management require a drastic departure from the classic design
methodologies [1]. Networks-on-Chip (NoC) communication
architectures have recently emerged as a promising solution for
on-chip scalable communication beyond the capabilities of classical bus-based and Point-to-Point (P2P) architectures [7][13].
Besides its advantages in terms of modularity, design re-use, and
performance, the NoC approach offers a matchless platform for
implementing the GALS paradigm [4] and makes clock distribution and timing closure problems more manageable. Given that
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA
Copyright 2007 ACM 978-1-59593-627-1/07/0006…5.00
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
Figure 1 A sample 2D Mesh network with 3 VFIs. Communication across different islands is achieved through mixed
clock/mixed voltage FIFOs.
for complex systems built at 65nm and below it is almost impossible to move signals across the die in a single clock cycle or in a
power efficient manner, it becomes obvious that a shift towards
global on-chip asynchronous communication is needed. In addition, a GALS-based design style fits nicely with the concept of
VFIs, which has been recently introduced for achieving finegrain system-level power management. The use of VFIs in the
NoC context is likely to provide better power-performance tradeoffs than its single voltage, single clock frequency counterpart,
while taking advantage of the natural partitioning and mapping
of applications onto the NoC platform. However, despite the
huge potential for energy savings when using VFIs, the NoC
design methodologies considered so far are limited to a single
voltage-clock domain [2,10,15]. On the other hand, studies that
do consider multiple VFIs assume that each module/core in the
design belongs to a different island and different islands are connected by P2P links [8,17]. 
To address these challenges (and unlike existing work), this
paper explores the design and optimization of novel NoC architectures partitioned into multiple VFIs which rely on a GALS
communication paradigm. In such a system, each voltage island
can work at its own speed, while the communication across different voltage islands is achieved through mixed clock/mixed
voltage FIFOs (see Figure 1). This provides the flexibility to
scale the frequency and voltage of various VFIs in order to minimize energy consumption. As a result, the advantages of both
NoC and VFI design styles can be exploited simultaneously.
The design of NoCs with multiple VFIs involves a number of
critical steps. First, the granularity (i.e., the number of different
VFIs) and chip partitioning into VFIs needs to be determined.
While an NoC architecture where each processing/storage element (PE) constitutes a separate VFI exhibits the largest potential
savings for energy consumption, this solution is very costly.
Indeed, the associated design complexity increases due to the
overhead in implementing the mixed-clock/mixed-voltage FIFOs
110
and voltage converters required for communication across different VFIs, as well the power distribution network needed to cover
multiple VFIs. Additionally, the VFI partitioning needs to be performed together with assigning the supply and threshold voltages
and the corresponding clock speeds to each VFI.
The novel contribution of this paper is a design methodology
for partitioning a given NoC architecture into multiple voltagefrequency domains, and assigning the supply and threshold voltages (hence the corresponding clock frequencies) to each domain
such that the total energy consumption is minimized under given
performance constraints. We present a unified approach for solving the VFI partitioning and voltage/speed assignment problems.
The basic idea is to start with an NoC configuration where each
PE belongs to a separate VFI characterized by given supply and
threshold voltages and local clock speed (i.e., having initially N
VFIs, for N PEs). This solution may achieve the minimum application energy consumption, but not necessarily the minimum
total energy consumption due to the additional overhead incurred
by implementing a large number of VFIs. Therefore, the proposed approach finds two candidate VFIs to merge such that the
decrease in the energy consumption is the largest among all possible merges, while performance constraints are still met. The
process is repeated until a single VFI implementation is obtained.
Consequently, for all possible VFIs (i.e., 1,2,...,N), we obtain the
partitioning and corresponding voltage level assignments such
that the total energy is minimized, subject to given performance
constraints. Finally, among all VFI partitionings determined by
the iterative process, the one providing the minimum energy consumption is selected as being the solution of the VFI partitioning
problem.
The remainder of this paper is organized as follows. Section 2
reviews the related work and highlights our major contribution.
Section 3 presents the problem formulation and solution to VFI
partitioning and static voltage-frequency assignment. Experimental results are included in Section 4. Finally, Section 5 concludes the paper.
2. RELATED WORK AND CONTRIBUTIONS
GALS-based systems consist of several synchronous IPs that
communicate with each other asynchronously [4]. There have
been many efforts to design low latency asynchronous communication mechanisms between synchronous blocks. Some of them
include design of mixed-clock FIFOs design [5], while others
include design of asynchronous wrappers [16]. The design style
based on multiple VFIs has been proposed in [12]. It fits very
well with the GALS design style, where the synchronous IPs in
the design have both different voltages and frequencies. 
Despite the natural fit between VFI design style and NoCs, the
existing design methodologies for NoCs are confined to single
voltage/single frequency domain [10,11,15]. There have been
several design efforts to combine the benefits of NoC interconnect mechanism with GALS-based design style. An FPGA prototype of a GALS-based NoC with two synchronous IPs is
presented in [18]. In [6], a method to reduce the wire propagation
delays in a GALS-based NoC is proposed. However, these studies assume that all the nodes in the network belong to different
clock domains, which may be a costly proposition. 
r
r
e
e
p
p
a
a
p
p
s
s
i
i
h
h
t
t
f
f
o
o
s
s
u
u
c
c
o
o
F
F
NoC Architecture
NoC Architecture
(topology, routing, etc.)
(topology, routing, etc.)
Application
Application
Scheduling
Scheduling
VFI Partitioning and 
VFI Partitioning and 
VFI Partitioning and 
Static Voltage-Frequency
Static Voltage-Frequency
Static Voltage-Frequency
Assignment
Assignment
Assignment
Interface Design for 
Interface Design for 
Interface Design for 
Voltage-Frequency Islands
Voltage-Frequency Islands
Voltage-Frequency Islands
Off-line
Off-line
Dynamic Voltage and 
Dynamic Voltage and 
Dynamic Voltage and 
Frequency Scaling (DVFS)
Frequency Scaling (DVFS)
Frequency Scaling (DVFS)
On-line
On-line
W
W
o
o
r
r
l
l
k
k
o
o
a
a
d
d
C
C
h
h
a
a
r
r
a
a
c
c
t
t
e
e
r
r
i
i
z
z
a
a
t
t
i
i
o
o
n
n
Figure 2 Overview of the proposed methodology.
In contrast to previous approaches, we address herein the VFI
partitioning and voltage/speed assignment problem for NoCs
with the objective of minimizing the overall energy consumption
and present a novel algorithm for solving it. To this end, our contributions in this paper are as follows: 
• A methodology for multi-clock/voltage domain NoC design
• An algorithm for VFI partitioning and supply/threshold voltage assignment
• Extensive evaluation of the proposed approach using realistic
benchmarks and an FPGA prototype.
3. VFI PARTITIONING AND STATIC 
VOLTAGE ASSIGNMENT PROBLEMS
3.1.  Basic Assumptions and Methodology Overview
We consider a tile-based implementation laid out as a grid, as
shown in Figure 1. Each tile contains a processing or storage element (referred to as PE) and a router. We employ wormhole flow
control and XY routing algorithm. Communication across different voltage-frequency islands is achieved through mixed-clock/
mixed-voltage FIFOs. 
The target application is first scheduled to the target NoC
architecture which consist of several heterogeneous PEs. In this
paper, we used Earliest Deadline First (EDF) and a heuristic
called Energy Aware Scheduling (EAS) to generate both computation and communication task schedules [11]. However, in general, the proposed approach can be used with arbitrary schedules.
As shown in Figure 2, given the target architecture, the driver
application and its schedule, the proposed methodology determines the VFI partitioning and the supply and threshold voltage
assignment to the VFIs. The voltages are assigned such that the
total energy spent in both computation and communication is
minimized, subject to performance constraints. The performance
constraints are imposed by the driver application as deadlines for
certain tasks and/or minimum throughput requirements. 
The voltage-frequency assignments computed using the proposed approach are static. For the applications with large workload variability, the operating voltage and frequency can be
further controlled dynamically around these static values. In this
paper, however, we restrict ourselves to the case of static voltage/
speed assignment, as summarized in Figure 2.
111
 
 
 
 
 
 
 
 
3.2.  Formulation of the Problems
3.2.1.  Energy and delay models
The set of tiles in the network is denoted by T ={1,..., N}. The
supply and threshold voltages for tile i∈T are given by Vi and Vti,
respectively. Using this notation, the sum of dynamic and static
energy consumptions associated with tile i∈T can be written as:
⎛
V t
-----–⎝
S t
⎞
⎠
EL b i t
ES b i t
,
(
)
=
E i V i V t i
R iC iV i
2 T i k iV i e
+
(1)
where Ri is the number of active cycles, Ci stands for the total
switched capacitance per cycle, Ti is the number of idle cycles,
ki is a design parameter, and St is a technology parameter [3].
We assume that the static component of the communication
energy component is included in the second term of Equation 1,
since each link and router belongs to a tile in the network. The
dynamic part of the communication energy consumption is found
using the bit energy metric [20] defined as:
=
+
E
(2)
Eb i t
EB b i t
+ S b i t
EL b i t
EB b i t
where 
, 
, and 
 represent the energy consumed by
the link, buffer and switch fabric, respectively. Assuming the
bit energy values are measured at VDD, the energy needed to
transmit one bit from tile src ∈ T to tile dst ∈ T is found as:
Eb i t s r c d s t
(
,
)
(
EL b i t
i( ) EBb i t
+
i( ) E
+ S b i t
i( )
2
) V i
2----------VD D
(3)
∑=
P∈
i
where P is the set of tiles on the path from tile src to tile dst. 
The clock period for each tile i is a function of its supply and
threshold voltage, and it can be expressed as:
K i V i
-------------------------(
)α
–
τ i V i V t i
(4)
=
(
)
,
V i V t i
where 1.2≤ α ≤1.5 is a technology parameter and Ki is a designspecific constant [19]. Thus, the operating frequency of a tile
and the VFI j it belongs to, is determined by the largest cycle
time of the comprising tiles, i.e.
⎫
⎬
⎭
⎧
⎨
⎩
1
-----------------------(
,
)
(5)
τ i V i V t i
f j m i n
≤
i
S j∈
where Sj is the set of tiles that belong to VFI j.
Finally, we assume each router is locally synchronous and communicates with its neighbors via mixed-clock/mixed-voltage
FIFOs. As such, the communication latency between tile src
and tile dst, while sending vol(src, dst) bits of data, is given by:
µ s
(
,
)
-----------------------------------v o l s r c d s t
f i
W
where W is the channel width and µs is the number of cycles it
takes to traverse a single router and the outgoing link. tfifo is the
latency of the FIFO buffers and it is found experimentally using
the prototype described in Section 4.3. The first term of
Equation 6 gives the latency for the header flits while passing
through the routers on path P, while the second term is the serialization latency.
t c o m m s r c d s t
(
,
∑=
t f i f o
(6)
P∈
+
)
i
3.2.2.  The static voltage/speed assignment problem
Assume that the target application consists of a set of tasks G.
For each task t∈G, the initial schedule specifies the deadline (dt),
start time (stt), the number of cycles required to execute the task
(xt), as well as the tile where the task will run on. 
When the network is partitioned into N VFIs, i = 1,..., N, (i.e.,
each tile belongs to a different island), the static voltage assignment problem can be formulated as follows:
Given an NoC architecture with N tiles, where each tile is a
separate VFI, a communication task graph describing the driver
application, and its schedule,
Assign the supply (Vi) and threshold (Vti) voltages, such that the
total application energy consumption is minimized; that is:
v o l i j,(
min 
(7)
(
)
)
,
)Eb i t i j,(
E i V i V t i
EA p p
∑=
i T∈∀
∑+
i T∈∀
∑
j T∈∀
–
≤
+
d t
s t t
(8)
t G∈∀
tC o m m
t
subject to deadline constraints expressed as:
x t
---f t
where xt / ft is the computation time and 
 is the communication delay encountered when task t needs to communicate
with a task mapped to a different tile. The number of cycles
required to execute the task xt is given by the schedule and
 is computed using Equation 6. The left hand side of
Equation 8 gives the sum of computation and communication
times, while the right-hand side gives the amount of time the
task should be completed without violating the schedule. 
tC o m m
t
tC o m m
t
3.2.3.  The voltage-frequency island partitioning problem
EV F I
Even though decoupling the supply and threshold voltages of
each tile results in a system with finest granularity for voltage/
frequency control, the overhead of designing a large number of
islands may undermine the potential for energy savings. In fact, it
may be possible to merge several islands with a negligible
increase in application energy consumption. In the extreme case,
all tiles can be conceivably merged into a single VFI. Between
the two extreme points, there exists a myriad of choices with
varying energy/cost trade-offs. In order to compare these
choices, we need to quantify the energy overhead of extra VFIs.
The energy overhead of adding one additional voltage-frequency island to an already existing design can be written as:
=
+
+
(9)
where EClkGen is the energy overhead of generating additional
clock signals and EVconv denotes the energy consumption of the
voltage level converters. Finally, EMixClkFifo is the overhead due
to the mixed-clock/mixed-voltage FIFOs used in interfaces.
Besides energy, additional VFIs exhibit area and implementation overheads, such as routing multiple power distribution networks. We assume that the maximum number of VFIs is given as
a constraint. Therefore, the VFI partitioning problem is formulated as follows: 
Given 
• An NoC architecture; 
• The scheduling of the driver application across the network;
• Maximum number of allowed VFIs;
EC l k G e n EV c o n v EM i x C l k F i f o
112
(a) Single VFI
(b) Two VFIs
(c) Three VFIs
Figure 3 Different voltage-frequency island partitionings and corresponding static voltage assignments for a 2×2 network. 
• Physical implementation constraints (e.g., certain tiles have to
the tiles brings ample opportunities for energy optimization.
operate at a given voltage level, or a subset of tiles have to
Hence, the application energy consumption becomes minimum
belong to the same VFI)
in this case. At the same time, a larger number of VFIs results in
Find the optimum number of VFIs (n ≤ N), VFI partitioning;
a more complex system and a larger energy overhead. 
and assign the supply and threshold voltages to each island,
The number of VFIs, hence system complexity, can be
such that the total energy consumption, i.e.,
decreased by merging some of the neighboring islands. Merging
islands brings a number of additional constraints to the problem
formulation. For instance, when tiles i and j are merged, constraints Vi = Vj and Vti = Vtj need to be added and thus, the voltage assignment problem in Section 3.2.2 is solved with these
additional constraints. Due to these additional constraints, the
application energy consumption goes up after the two islands are
merged. However, the total energy consumption given by
Equation 10 may decrease, if the increase in the application
energy consumption is smaller than the reduction in the energy
overhead due to merging two VFIs. 
is minimized, subject to performance constraints in Equation 8.
3.3.  Motivational Example
EV F I i( )
∑+
i
1=
(10)
ET o t a l
=
EA p p
n
As an example, we analyze the office-automation benchmark
[9]. The application is scheduled on a 2×2 network using the
EDF discipline, and the proposed approach is used for the VFI
partitioning and static voltage assignment. 
When the entire network consists of a single VFI, the supply
and threshold voltages are found to be 1V and 0.15V, respectively (see Figure 3(a)). The corresponding schedule and these
voltage assignments result in a 10.5mJ energy consumption.
When analyzing this design, we observe that one of the tasks has
a zero slack available, while others have a positive slack. When
the network consists of two islands, the task with a zero slack is
decoupled from the others. As shown in Figure 3(b), only one of
the tiles needs to operate at 1V, while the supply voltage of the
others is reduced to 0.8V. The energy consumption of this solution drops to 7.5mJ, which represents about 29% savings. 
The network can be further partitioned into three islands, as
shown in Figure 3(c). For this example, a finer partitioning granularity results in diminishing rate of returns. In addition, the
overhead of the extra island undermines the potential for energy
savings. In this example, the energy consumption of the threeand four-island networks is 7.6mJ and 7.8mJ, respectively.
Hence, the network partitioning shown in Figure 3(b) results in a
minimum energy consumption.
3.4.  Solution Methodology
We solve the VFI partitioning and static voltage assignment
problems in a unified manner; that is, the partitioning and voltage
assignment are performed simultaneously.
We start off with a VFI partitioning where all the PEs belong to
separate islands, as shown in the left most box in Figure 4. Then,
we solve the static voltage assignment problem in Section 3.2.2.
To solve Equation 7, we utilize a nonlinear inequality constrained problem solver based on gradient search available in
Matlab and find the supply and threshold voltages that minimize
the total energy consumption subject to performance constraints.
Naturally, decoupling the supply and threshold voltage levels of
113
For all pairs of 
For all pairs of 
neighboring islands 
neighboring islands 
(i ,j )
(i ,j )
Merge VFIs i and j
Merge VFIs i and j
Assign V and Vt
Assign V and Vt
to all tiles
to all tiles
Compute the 
Compute the 
energy consumption
energy consumption
Find the static 
Find the static 
voltages assuming 
voltages assuming 
there are N islands
there are N islands
Merge the pair of islands that
Merge the pair of islands that
provides the minimum energy
provides the minimum energy
Update the VFI 
Update the VFI 
configuration
configuration
Figure 4 Outline of the proposed VFI partitioning and static
voltage assignment methodology.
In the second step of the algorithm (i.e., the middle box in
Figure 4), the decrease in the total energy consumption as a result
of merging each pair of neighboring tiles is computed. Since
there are 2n(n-1) pairs of neighbors in a n×n grid, 2n(n-1) evaluations are performed during this stage. Then, the pair of islands
that results in the largest reduction of the total energy consumption is picked and merged permanently, as depicted in the right
most box in Figure 4. After the VFI configuration is updated, the
second step is executed again to find the next pair of candidate
tiles to merge. This process continues until all tiles are merged
and the network consists of a single island.
The proposed algorithm starts of with N VFIs and reaches a
single VFI at the end; as such, it evaluates all possible levels of
VFI granularity. Hence, the proposed algorithm can determine
the partitioning with the minimum overall energy consumption
and the corresponding supply and threshold voltage assignments.
4. EXPERIMENTAL RESULTS 
This section illustrates the proposed VFI partitioning methodology and demonstrates its effectiveness in minimizing the
energy consumption using real benchmarks. The first set of
benchmarks is chosen from a public benchmark suite [9], while
the second consists of a real video application. The energy
related parameters (e.g. energy consumption of a task running on
a certain PE) are derived from the benchmarks, while the technology parameters are taken from [14].
After the proposed algorithm is used to find the supply and
threshold voltage for each VFI, we map them conservatively to
the following discrete levels: Vsupply ={0.4V, 0.6V, 0.8V, 1.0V
1.2V}, VThold ={0.15V, 0.20V, 0.25V, 0.30, 0.35V}. Then, we
compute the total energy consumption using Equation 10. The
results reported hereafter are obtained for these discrete levels. 
(a) Single VFI
(b) Two VFI partitioning
(c) Three VFI partitioning
Figure 5 Different voltage-frequency island partitionings and corresponding static voltage assignments for benchmark 3 in Table 1. 
1.67mJ energy consumption when there is no VFI partitioning.
For this case, partitioning the network into two island drops to
0.34mJ. Finally, generating more VFIs does not decrease the
energy consumption further. For telecom benchmark, the energy
consumption of the partitioning with the proposed algorithm is
1.5mJ; this is more than 4× reduction compared to the 1-VFI case. 
For better visualization, we show the supply voltage levels
obtained for telecom benchmark in Figure 5. When there is a single voltage domain, all the tiles operate at 1V VDD and 0.15V
threshold voltage. However, when there are two VFIs, the supply
voltage of all the tiles except tile (2,3) can be lowered to 0.6V,
while the threshold voltages remain at 0.15V. When we increase
the number of VFIs to three, the tiles voltage on the lower left
corner is further reduced to 0.4V. 
The run-time of the algorithm ranges from a few tens of seconds to 30 minutes for the benchmarks reported in Table 1. In
general, the proposed approach does not guarantee the optimal
solution. However, we performed an exhaustive search for the
example with 2×2 network in Section 3.3, and found that the
energy consumption achieved with the proposed approach is
within 1% of the optimum solution. More thorough analysis of
the optimality is left as future work.
4.1.  Experiments with Realistic Benchmarks
Consumer, networking, auto-industry and telecom benchmark
applications are collected from E3S [9]. These benchmarks are
scheduled onto 3×3, 3×3, 4×4 and 5×5 mesh networks, respectively using the EAS scheme presented in [11]. Then, the proposed approach is used for VFI partitioning and static voltage
assignment. The second column of Table 1 (“1-VFI”) shows the
minimum energy consumption when the entire network consists
of a single island. The remaining columns show the energy consumption values obtained for the best partitioning with two and
three islands, respectively. The energy consumption of the partitioning picked by the algorithm is marked with an asterisk. 
Benchmark
Network 
Total Energy Consumption (mJ)
Size
1-VFI
2-VFI
3-VFI
Consumer
3×3
18.9
12.1*
12.2
3×3
Network
12.9
6.6*
6.7
Auto-industry
4×4
1.67
0.34*
0.40
Telecom
5×5
6.9
2.6
1.5*
Table 1: The reduction in the overall energy consumption
obtained as a result of the proposed algorithm.
For Consumer benchmark, the minimum energy consumption
is obtained when the network is partitioned into two voltage-frequency islands. With this configuration, the energy consumption
drops from 18.9mJ to 12.1mJ, which represents about 36%
improvement. As shown in Table 1, partitioning the network to a
finer granularity does not reduce the energy consumption further
due to the overhead of having the extra islands, which is about
1.7mJ. Similarly, a 2-VFI configuration achieves the minimum
energy for networking benchmark. Auto-industry benchmark has
4.2.  Experiments with a Real Video Application
We analyzed a video application consisting of MPEG2/MP3
encoder/decoder pairs [11]. The application is partitioned into a
set of tasks and scheduled onto a 4×4 mesh network using the
EDF scheme. Then, the proposed algorithm is used to find the
VFI partitioning with minimum energy consumption.
The proposed approach starts with 16 separate VFIs. Then, it
proceeds by merging the islands until a single island is obtained.
As shown in Figure 6, the total energy consumption is improved
until we reach two islands. For instance, when we move from 16
to 15 islands, the increase in the application energy consumption
is negligible. So, the total energy consumption reduces due to the
smaller overhead. Finally, a 2-VFI partitioning where the tasks of
the same application reside in different islands achieves the minimum energy consumption. This resulting partitioning results in
40% improvement compared to the 1-VFI case. 
4.3. Validation of VFI-based NoC via Prototyping
To further validate the simulation results for the VFI-based
NoC, we have prototyped a generic system on Virtex2Pro Xilinx
FPGA platform. A typical router in an NoC consists of a FIFO
and an output controller (OC) for each port, and an arbiter to
channel the traffic between the ports, as depicted in Figure 7. To
connect a node in a VFI with another node residing in a different
VFI, all data and control signals need to be converted from one
114
Figure 6 (a) The variation of total energy consumption as a
function of the number of voltage-frequency islands.
frequency/voltage domain to another. For this purpose, we
implemented mixed-clock/mixed-voltage 
interfaces using
FIFOs, which are natural candidates for converting the signals
from one VFI to the another, as shown in Figure 7. 
To support the simulation results, we implement a GALSbased NoC with a 4×4 mesh topology using Verilog HDL. Block
RAM-based mixed-clock FIFOs from the Xilinx library are used
in routers to transfer data between different clock domains. Our
design can be partitioned into as many as 16 VFIs. In our implementation, the signal conversion, both in terms of clock and voltage domains, occurs at FIFO interfaces. In this particular design,
the Delay Locked Loops (DLLs) present in the Xilinx FPGA
device are used to generate the individual clock signals. However, the voltage level conversion is not supported so multiple
voltage levels are not readily available for the Xilinx platforms. 
For experimental purposes, this implementation is configured
with 16 islands and simulated using the auto-industry benchmark
from E3S [9]. We first verify that no packets are lost in the VFI
interfaces. After that, we compute the total energy consumption
corresponding to single VFI and 2-VFI implementations, as in
Section 4.1. To compute the energy consumption values, we utilize the energy characterization of the on-chip routers reported in
[13]. The total energy consumption for single VFI operating at
1V is found as 109nJ. On the other hand, the total energy consumption of the 2-VFI partitioning found using the proposed
approach is 21.2nJ. Hence, we observe about 81% reduction in
the energy consumption. The energy consumption results
obtained using the FPGA prototype are different than that measured by simulation in Table 1 due to the differences in the target
platform and implementation details. Nevertheless, we note that
according to the simulation results, the relative improvement in
the energy consumption for the same benchmark is 80%, which
is very close to the result obtained using the actual prototype. 
5. CONCLUSIONS
In this paper, we addressed the design of GALS-based NoC
communication architectures with multiple voltage-frequency
domains. More precisely, we introduced a methodology for
multi-clock, multi-frequency domain NoC design, and presented
an algorithm for voltage-frequency island partitioning and supply/threshold voltage assignment. We showed that using VFIs in
the NoC context provides better power-performance trade-offs
than its single voltage, single clock frequency counterpart, while
Figure 7 Illustration of the interface between two different
voltage-frequency domains VFI1 and VFI2.
taking advantage of the natural partitioning and mapping of
applications onto the NoC platform.
As future work, we plan to complete our current FPGA prototype and demonstrate the energy savings through real measurements on several applications.
Acknowledgements: This research is supported by Marco
Gigascale Systems Research Center (GSRC) and in part by SRC
Contract 2004-HJ-1189.
6. "
Introducing the SuperGT Network-on-Chip; SuperGT QoS - more than just GT.,"Predictability of multi-processor systems-on-chip communication is critical and needs to be addressed by providing the right mix of soft and hard real-time guarantees. To this end, state-of-the-art packet-switched networks-on-chip (NoC) provide different levels of quality-of-service (QoS) such as best effort (BE) and guaranteed throughput (GT). Unfortunately, GT resources have to be reserved for the worst-case, resulting in over-allocated resources. We introduce the SuperGT NoC, a packet-switched NoC that, besides BE and GT, supports a new SuperGT QoS. A SuperGT connection combines guaranteed and non guaranteed traffic while maintaining in-order packet delivery. Time-slots are allocated to provide guarantees and extra BE resources are claimed by injecting data during free slots. Simulation results demonstrate the advantages of SuperGT over GT. Synthesis results of the SuperGT virtual channel manager show that the SuperGT router is an inexpensive enhancement to state-of-the-art packet-switched NoCs.","8.2
Introducing the SuperGT Network-on-Chip
SuperGT QoS: more than just GT
Th ´eodore Marescaux
IMEC vzw,
Kapeldreef 75,
3001 Leuven, Belgium
Theodore.Marescaux@imec.be
Henk Corporaal
TU Eindhoven,
Den Dolech 2,
5612 AZ Eindhoven, The Netherlands
H.Corporaal@tue.nl
ABSTRACT
Predictability of multi-processor systems-on-chip communication is critical and needs to be addressed by providing
the right mix of soft and hard real-time guarantees. To
this end, state-of-the-art packet-switched networks-on-chip
(NoC) provide diﬀerent levels of quality-of-service (QoS)
such as best eﬀort (BE) and guaranteed throughput (GT).
Unfortunately, GT resources have to be reserved for the
worst-case, resulting in over-allocated resources.
We introduce the SuperGT NoC, a packet-switched NoC
that, besides BE and GT, supports a new SuperGT QoS. A
SuperGT connection combines guaranteed and non guaranteed traﬃc while maintaining in-order packet delivery.
Time-slots are allocated to provide guarantees and extra BE
resources are claimed by injecting data during free slots.
Simulation results demonstrate the advantages of SuperGT
over GT. Synthesis results of the SuperGT virtual channel
manager show that the SuperGT router is an inexpensive
enhancement to state-of-the-art packet-switched NoCs.
Categories and Subject Descriptors
C.1.2 [Processor Architectures]: Multiprocessors
General Terms
Design, Performance
Keywords
MP-SoC, NoC, QoS
1.
INTRODUCTION
Many of the applications targeted at multi-processor systems-on-chip (MP-SoC), such as multi-media applications,
display a mixture of predictable streaming data and of less
predictable traﬃc such as cache misses or operating-system
(OS) communication. State-of-the-art NoCs, such as Æthereal, provide connection oriented guaranteed services, such
as guaranteed throughput (GT), with TDMA sharing of the
bandwidth and use BE services to ﬁll in the bandwidth reserved but not claimed by the GT QoS [3, 4].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright2007ACM978-1-59593-627-1/07/0006...$5.00.
116
A connection is deﬁned as the ensemble of a request path
and a response path connecting the network interfaces of
two communicating IP-blocks over the NoC. A connection
is assigned a particular QoS class and is typically established
for a duration much longer than a packet round-trip time.
Consider a system with a GT connection between an L1
cache and an L2 memory. As cache misses are not completely predictable it is necessary to largely over-allocate
bandwidth, possibly preventing other GT connections from
being allocated.
Instead, it would be desirable to reserve
enough bandwidth for the predictable traﬃc and absorb
peaks of less predictable traﬃc with additional non guaranteed bandwidth. In Figure 1, the oﬀered traﬃc load has
peaks higher than the amount of bandwidth reserved. With
a regular GT connection, throughput saturates hence increasing overall application latency.
The main contribution of this paper is SuperGT, a new
QoS class that allows the NoC to follow much closer the
oﬀered traﬃc load. The traﬃc source is still oﬀered guaranteed throughput for the amount reserved.
In addition,
traﬃc peaks over the reservations are transmitted in nonguaranteed (best-eﬀort) manner. Advanced virtual-channel
management is performed to guarantee in-order-packet delivery over a SuperGT connection.
This paper presents the SuperGT QoS and introduces the
routers and network interfaces of the SuperGT NoC. This
NoC supports three QoS classes: BE, GT and SuperGT.
Figure 1: SuperGT follows oﬀered load above reservations, GT saturates at reserved amount.
The rest of the paper is organized as follows. Section 2
introduces background information and related work. Section 3 details the concept and architecture of the SuperGT
NoC. Sections 4 and 5 present respectively performance results of the SuperGT NoC and synthesis results of its virtual
channel manager in 90nm standard cell technology. Finally,
Section 6 concludes.
2. BACKGROUND
This section introduces the related work on QoS for NoC
and discusses packet ordering.
2.1 Related work: QoS on NoCs
Bolotin and al. propose QNoC [2], an NoC that supports
four levels of QoS (signalization, real-time, read/write and
block-transfer) by employing pure packet-switching with four
prioritized virtual channels. All communication properties
have to be estimated at design-time in simulation to provide
the expected QoS levels, limiting the scope to applicationspeciﬁc NoCs with static conﬁgurations.
The MANGO NoC [1] is another asynchronous NoC that
mixes a BE router and a guaranteed services (GS) router to
provide both best-eﬀort and real-time guarantees. Connection oriented GS are provided by reserving virtual circuits
through the NoC. End-to-end ﬂow control is inherent. BE
services are connection-less and can be used to setup GS
connections at run-time. MANGO has a work-conserving
[9] scheduling discipline: the oﬀered application load on a
GS connection is immediately served.
The techniques to provide BE and GT that are based
on Time Division Multiple Access (TDMA) techniques for
synchronous NoCs, such as in the Nostrum NoC [5] and the
Æthereal NoC [3, 4, 7] are the closest to the one applied here.
They all have non-work-conserving scheduling disciplines.
The Sonics MXNoC performs (at the edge of the network) priority promotion and demotion of packets, based
on a token-bucket scheme. Arbitration inside the network
fabric, is based on epochs (a form of TDMA compatible
with asynchronous networks) [8]. Because of the proposed
priority switching mechanism, in-order packet delivery for
streaming connections cannot be guaranteed.
The SuperGT NoC contribution can be best understood
as a potential evolution of the GS-BE central programming
architecture of Æthereal (ÆGS−BE ). ÆGS−BE is a pure
packet-switched NoC, where both BE and GT1 packets have
headers and where time-slot tables (Figure 6) are only situated in the network interfaces [4].
2.2 Packet Delivery Order
Packet re-ordering at the level of the network interface is
impractical as it requires expensive re-ordering buﬀers and
greatly increases latency. Many NoCs try thus to avoid
out-of-order packed delivery. For instance, the Æthereal
ÆGS−BE guarantees in-order delivery for both GT and BE
services. Is it then possible to construct a service similar to
the proposed SuperGT by using a mix of GT and BE packets on the same connection2 between two IP-blocks, such
as an L1 cache and the L2? GT packets would be used to
provide the reserved bandwidth and BE packets to absorb
the oﬀered load that is superior to reservations. However,
as this section demonstrates, such a scheme would lead to
out-of-order packet delivery.
Let us assume a packet-switched router with two virtual channels, similar to ÆGS−BE . The ﬁrst virtual channel (VCGT ) is assigned the highest priority and is used for
hard real-time GT traﬃc, whereas the second one (VCBE )
of lower priority is used for BE traﬃc. Assuming that the
injection-time of ﬂits (ﬂits, or ﬂow-control units, are subunits of packets) on the VCGT respects a congestion-free
allocation schedule [3, 4], we can deduce two properties:
1GT provides throughput and latency guarantees, so the use
of the term GS is more appropriate. For simplicity, we do
not make the distinction and use the term GT.
2Using separate BE and GT connections clearly requires reordering buﬀers.
117
1. Any two ﬂits injected on VCGT never collide, as a consequence of the congestion-free allocation.
2. All ﬂits on VCGT have higher priority than ﬂits injected on VCBE . In case of a conﬂict between a GT
and a BE ﬂit, the GT ﬂit is guaranteed to have access
to the resources, the BE ﬂit is buﬀered locally.
The direct consequence of these properties is that GT ﬂits
are guaranteed to never be delayed on their path on the NoC,
thus ensuring hard real-time guarantees on their delivery
time. Note that in this conﬁguration of packet-switched
NoCs no time-slot tables are required at the routers, but
only at the GT network interface (NI) [6, 3].
Also, GT ﬂits may not be buﬀered on the routers. The
end consumer of the GT ﬂits should therefore be fast enough
to guarantee that all GT data is taken out of the NoC timely.
This condition is very diﬃcult to guarantee in realistic systems, hence techniques such as end-to-end credit-based ﬂowcontrol techniques are used to inform the producer NI of the
level of occupation of the FIFOs inside the consumer NI.
This allows the producer NI to control the amount of data
it may inject in the NoC and thus guarantee that the NoC
never gets congested on VCGT .
On a router, in the absence of GT packets on VCGT , the
bandwidth can be reused for BE traﬃc (of other connections) on VCBE . This happens automatically when interleaving virtual channels on classic packet-switched networks.
Given deterministic routing, packet-order delivery is guaranteed.
Assume now the network interface of the aforementioned
cache can inject BE packets as well as GT packets (on the
same connection) destined to the same L2 remote memory.
For instance, the NI sends ﬂits 2, 3, 4, 5, 6 on this hypothetical GT+BE connection (Figure 2(a)). The allocation of
time-slots ensures that ﬂits 3, 5 have GT priority and go
on VCGT on the underlying router, whereas ﬂits 2, 4, 6 are
injected on VCBE . Assume, the low priority 1-ﬂit packet
2 has lost contention to a low-priority packet on another
connection. Packet 2 is buﬀered locally long enough for
guaranteed 1-ﬂit packet 3 to take over, disrupting packet order. Special attention is thus required to provide in-order
packet-delivery on such a connection with reserved and nonreserved resources.
Figure 2: (a) packet order inversion on ÆGS−BE for
a mixed BE/GT QoS connection, (b) QoS levels in
SuperGT NoC.
3. SuperGT NoC
The SuperGT NoC is a packet-switched NoC that provides 3 QoS classes: BE, GT and SuperGT. Packets of all
three QoS classes have headers and are guaranteed to be
delivered in-order. Priority and ﬂit-type information are
transmitted out-of-band.
The SuperGT router is a packet-switched router with 2
virtual channels with priorities, featuring a SuperGT virtual channel, VCsGT , next to a high-priority-escape virtual
channel, VCescH i . Similarly to the ÆGS−BE architecture,
GT packets are assigned a high-priority (Hi ) and are sent
on VCescH i , whereas BE packets have a low priority (Lo )
and are sent on VCsGT .
SuperGT packets are special. Not only can the network
interface assign them either of the priority levels (Hi and
Lo ), but packet priority can be promoted or demoted by
routers along the path. Moreover, whereas VCescH i is only
used by SuperGT packets with Hi priority, VCsGT accommodates for both low and high priority traﬃc (Figure 2(b)).
Section 3.1 details the SuperGT virtual channel management in a router.
On a given SuperGT connection, ﬂits are injected with a
high priority (Hi ) during the allocated time-slots and with
a low priority (Lo ) during free time-slots. A contentionfree central time-slot allocation is performed for GT traﬃc
together with the Hi part of the SuperGT traﬃc.
3.1 SuperGT Virtual Channel Management
The SuperGT mechanism relies on a particular management of the VCescH i and VCsGT virtual channels and does
not depend on the type of queuing (input queuing, virtual
output queuing, etc.) nor on the switching method. For
simplicity, this paper describes a wormhole-switched, 2x2
input-queued SuperGT router (Figure 3). Every input port
on the SuperGT router has a SuperGT virtual channel manager (sgtVCM). Its role is to buﬀer ﬂits and keep in-order
packet delivery for all three QoS classes.
and simultaneously promote best-eﬀort Lo packets to realtime Hi priority. Priority switching is only performed on
VCsGT when ﬂits from a Lo priority packet are buﬀered on
the VCsGT FIFO and a Hi priority packet belonging to the
same connection enters the sgtVCM. The incoming ﬂit is
pushed at the tail of the VCsGT FIFO and its priority token
(1-bit out-of-band ﬁeld) is transferred to the ﬂit at the head.
This mechanism conserves both in-order packet delivery and
the number of priority tokens on a given connection, thus
its real-time properties.
Figure 4 details the priority switching process in time.
Assume a Lo packet {1, 2, 3} has lost contention on router
R + 2 (1 is a Head ﬂit, 2 is a Body ﬂit and 3 is a Tail ﬂit).
and the VCsGT FIFO can buﬀer two Lo ﬂits. Flits {1, 2}
The routers in the example implement wormhole switching
are therefore blocked at router R + 2 and ﬂit 3 is blocked at
router R + 1. A Hi packet {A, B , C } enters router R at time
T on the same connection as the previous packet {1, 2, 3}.
At time 2T on router R+ 1 the ﬁrst priority switching occurs
between the Head ﬂit A and the Tail ﬂit 3. At time-slot 3T
two priority switchings occur, the ﬁrst one at router R + 1
between the Body ﬂit B and the Head ﬂit A and the second
one at router R + 2 between the Tail ﬂit 3 and the Head ﬂit
1. The priority switching is pipelined, so that at router R+ 2
the Head ﬂit 1 is leaving during time-slot 3T and the Tail
ﬂit 3 is leaving at time-slot 5T . The packet {A, B , C } has
been completely demoted to Lo and is buﬀered on R + 1 and
R+ 2 (in place of the packet {1, 2, 3}) until the contention on
Lo on router R + 2 is won (or until a subsequent packet with
Hi priority on the same connection transfers its priority). A
minimum amount of bandwidth corresponding to the three
allocated slots is guaranteed.
Figure 4: Priority switching on VCsGT for packets
on the same connection.
The sgtVCM priority switching technique imposes a small
constraint on the link controller: the VCsGT FIFO may only
buﬀer (parts of ) Lo packets from a single given connection at
a time. Any (part of ) Lo packet from another connection is
to be buﬀered at the previous router until the current packet
is processed. In the example of Figure 4 this means that the
empty space on the VCsGT FIFO of router R + 1 can only
be used by Lo packets coming on the same connection as
Figure 3: 2x2 input-buﬀered SuperGT Router and
its two sgtVCM.
To provide in-order delivery on a SuperGT connection the
sgtVCM may have to demote guaranteed Hi packets to Lo
118
packet {1, 2, 3}. The reason to this constraint is simple: if
the Lo packet {Z } from another connection was allowed to
the following ﬂits: {Z, 3}. When the Hi packet {A, B , C }
be buﬀered on router R + 1 its VCsGT FIFO would contain
would enter router R + 1 on the same connection as ﬂit 3,
the priority switching mechanism would not make sense for
ﬂit {Z } as it is on a diﬀerent connection3 .
Since we have seen that priority switching only makes use
of VCsGT one may wonder what the purpose of VCescH i is.
Consider the situation of Figure 5 where Lo ﬂits of a given
SuperGT (or BE) connection ({1, 2, 3}) are being buﬀered
connection ({A, B , C }) enter the sgtVCM. As the connecon VCsGT and Hi ﬂits from a packet on a diﬀerent SuperGT
tions are diﬀerent, re-ordering does not make sense, so the
Hi packet must take over the Lo ﬂits buﬀered on VCsGT .
In this case only the Hi ﬂits are pushed on the other virtual
channel of the sgtVCM, the escape channel VCescH i , and
keep their priority tokens. Packet {1, 2, 3} stays in place
on VCsGT , buﬀered across routers R + 1 and R + 2 and
packet {A, B , C } uses the escape virtual channel VCescH i
to exit router R + 2 in the reserved time. Note that packet
{A, B , C } has totally left router R + 2 at time-slot 6T , exactly at the same time as packet {1, 2, 3} in the previous
example, where priority switching was needed.
triggered for these type of connections.
The sgtVCM contains a simple connection tracker module
to indicate whether the current packet belongs to the same
connection as the packet that occupies (or last occupied) the
VCsGT FIFO. The connection tracker checks the connection
ID ﬁeld (Section 3.2) in the header ﬂit of the entering packet
against the stored value of the previous one.
3.2 SuperGT Network Interface
The SuperGT network interface is responsible for driving the priority signal, besides creating and injecting ﬂits
into the NoC. The priority signal is set to Hi when a ﬂit
is injected during an allocated time-slot, giving this ﬂit a
priority token. The connection ID, required for connection
tracking, is simply composed by the NI by adding a few bits
to identify the traﬃc source to the routing information in
the header ﬂit (whether using source or destination routing).
To guarantee contention-free allocation in the system all
(GT and SuperGT) packets that potentially use the highpriority escape channel VCescH i need to free the switch
conﬁguration at any router R during their allocated slots
at R. This is ensured by having the network interface to
inject a Tail ﬂit (with payload) before or during its last
consecutively allocated time-slot to close any packet it had
opened. Figure 6 shows a NI managing a SuperGT connection with three consecutively allocated time-slots in the
time-slot wheel. A possible packetization policy for Hi packets is to send a Head ﬂit (H) in the ﬁrst allocated slot (1),
a Body ﬂit (B) in the second one (2) and a Tail ﬂit (T)
in the last consecutively allocated slot (3). We note this
packetization scheme {HH i,1 , BH i,2 , TH i,3 }. All other legal packetization for three consecutively allocated time-slots
are: {−, HH i,2 , TH i,3 }, {HH i,1 , TH i,2 , −}, {−, −, −} where −
is either an Atomic (combined Head +Tail ) ﬂit or no ﬂit.
Figure 5: Channel escape on VCescH i for packet
{A, B , C } from a diﬀerent connection.
The same sgtVCM supports also BE and GT connections.
Only SuperGT connections mix packets of Lo and Hi priorities and thus may require priority switching. As BE and
GT connections always keep the same priority level, the priority switching mechanism of the sgtVCM is simply never
3This constraint can be alleviated by duplicating the VCsGT
virtual channel to allow the management of several connections simultaneously (one VCsGT per connection).
119
Figure 6: Packetizations of size three in a SuperGT
network interface with three allocated slots.
The same exact packetization schemes can be used for a
GT connection. Unlike GT, when there is more data queued
than can be served in a time-wheel iteration (given the numline evictions as bursts of data onto the NoC. The memory
node is a slave on the NoC, to which it connects through a
similar bus-shell unit.
As Figure 7 shows, the four bus shells in the system are
attached to four network interfaces that are connected to a
single router, resulting in a star topology. The contention
on the L2 link is suﬃcient to illustrate the behavior of the
various QoSs. This system has been used to create two experimental setups. One uses GT and BE traﬃc (similar to
the ÆGS−BE ) whereas the other uses SuperGT and BE trafﬁc. The connections P0-L2 and P1-L2 are GT (SuperGT)
and have the same time-slot allocation for both experiments.
P1-L2 is allocated half the bandwidth of P0-L2. Connection
P2-L2 only supports BE traﬃc.
4.2 Simulation Results
All processors access data stored in L2 through caches.
Cache-misses and line evictions are converted to NoC transactions by the NI bus-shell adapter.
Processors P0 and P1 run a data parallel version of cavity
detection, a simple, but computationally intensive, imageﬁltering application from the medical domain.
Input and
output images (640x400 pixels) are stored in L2 (Figure 7).
Processor P2 performs FFTs on data also stored in L2
over a BE connection. In the SuperGT/BE setup, its traﬃc
competes with the Lo priority traﬃc on the SuperGT connections P0-L2 and P1-L2. To compare the eﬀect of BE load
over the Lo traﬃc of the SuperGT we ran two experiments,
one with a low BE load (FFT over 1024 points) and another
with a high load (FFT over 65535 points).
ber of allocated time-slots), a SuperGT NI may inject Lo
priority packets outside of the allocated time-slots, provided
two constraints are fulﬁlled.
First, to eﬀectively guarantee throughput, Lo packets are
only injected when there is suﬃcient data to create one (or
several) Lo packet(s) and one Hi packet (in the example of
Figure 6 six ﬂits are required). Even though packets are
served more frequently than for a GT connection, this packetization policy is clearly non-work-conserving. This constraint is required to push Lo packets (by transferring them
the Hi priority) from the same connection that are eventually buﬀered in a router along the path. It implies that
SuperGT has throughput guarantees at the granularity of a
time-slot wheel, whereas for GT, throughput is guaranteed
for an individual packet.
The second constraint requires a Hi packet to have the
same length as all Lo packets it is likely to push. This ensures the transfer of the correct number of priority tokens
during a possible priority switching. In its simplest form,
all packets for a given SuperGT connection need to have the
same size. More complex techniques need to ensure that all
Lo packets on a given SuperGT connection have been consumed. A possibility would be to use the end-to-end credit
based ﬂow-control scheme to track packet consumption.
4. SYSTEM SIMULATION
This section demonstrates, at a cycle-accurate level, the
SuperGT idea on a realistic example. We compare the performance advantages of the SuperGT QoS over a strict GT
using SystemC simulation transaction-level models of the
SuperGT NoC.
4.1 Simulation Model
To evaluate the performance of the SuperGT system, an
instance composed of three processor nodes and one memory
node has been created within NoCTurn , a custom SystemC
MP-SoC platform simulator. NoCTurn is a cycle-annotated
simulator where the NoC-level transaction is a ﬂit (the experiments use 3 phits per ﬂit). Each processor, a TIC62
Figure 8: Comparing SuperGT and GT QoS.
Figure 8(a) compares the average throughput on the request paths (P 0 → L2 and P 1 → L2) for both setups (the
network is loaded with the high BE load of P2)4 . The family
of 4 curves presents the same shape, an initial peak of trafﬁc (corresponding to L2 initialization performed by the TI
processors), followed by a slow ramping-up corresponding to
data transfers of the cavity application.
As expected for the GT experiment, the throughput of
P 0 → L2 is higher than P 1 → L2 as it has more bandwidth
allocated. For the SuperGT experiment, as Lo packets can
be injected additionally to the reserved bandwidth, not only
the throughput on both connections is higher than on their
4As the applications have a very high data reuse locality,
the traﬃc load they require from the NoC is low (though its
shape is realistic). To create substantial NoC load the ﬂit
clock (96 bits per ﬂit) has been slowed down to 120ns.
Figure 7: System simulation setup.
instruction set simulator, is connected to a local bus that
gives direct access to a private L1 memory and to a busshell adapter that allows access to remote memories through
the NoC (Figure 7). The bus-shell behaves as a 4-way setassociative write-back cache that transmits cache misses and
120
GT counterparts, but also the diﬀerence in time-slots allocated is absorbed. Indeed the throughput curves P 0 → L2
and P 1 → L2 are almost superimposed. Thanks to its guaranteed Hi packets and additional non-guaranteed Lo packets, the SuperGT QoS improves application throughput by
14.4% (resp. 35.6%) on the connection P 0 → L2 (resp.
P 1 → L2) with respect to the GT connections.
Figure 8(b) compares the average throughput on the best
eﬀort request path (P 2 → L2) for both setups and under
both BE traﬃc loads. The two top curves correspond to the
high BE traﬃc load and the two bottom ones to the low
BE traﬃc load. For both setups, the BE traﬃc competes
with high priority traﬃc from the reserved resources. In the
SuperGT versus BE setup, the BE traﬃc additionally competes with Lo packets from the SuperGT connections. This
eﬀect is visible as the BE curves in the SuperGT experiment
present a lower throughput than their GT counterparts.
Interestingly, one can see the eﬀect of a drop in application
throughput in the SuperGT connections. Indeed, an inﬂection point at t = 2.105ns only exists for the SuperGT/BE
setup (at both BE loads). This corresponds exactly to the
end of the initialization traﬃc of processors P0 and P1 (Figure 8(a)). As throughput drops on SuperGT paths P 0 → L2
and P 1 → L2, it increases on the P 2 → L2 path.
5. SYNTHESIS RESULTS
We have implemented the diﬀerentiating block of the SuperGT router, its virtual channel manager. The VCsGT and
VCescH i FIFOs are 34 bits wide (32-bit data and 2-bit for
ﬂit information) and there are 4 phits per ﬂit (due to restrictions of the FIFO model). The sgtVCM has been synthesized at gate level with Synopsys Physical Compiler,
in 90nm TSMC standard cell technology, worst case conditions. This example uses 21 bits for the connection ID
register (15 bits for source routing and 6 bits for NI ID).
Component
VCsGT (4-deep)
VCescH i (1-deep)
Control logic
Total
Comb. Non-comb. Total (µm2 )
6492
9894
16386
1846
2629
4475
378
480
858
8716
13003
21719
Table 1: sgtVCM area (TSMC STD Cell 90nm).
The total area of the sgtVCM is under 0.0217mm2 , out of
which 0.0209mm2 are for the standard cell implementation
of the FIFOs.
In a commercial design these could be replaced by custom or semi-custom implementations, that can
be a factor 3 to 5 less expensive. In essence, the overhead of
4% due to the logic of the sgtVCM is negligible compared to
the area of the FIFOs being managed, making the sgtVCM
an inexpensive addition to a regular packet-switched router
with 2 virtual channels.
For the sake of comparison, if the 6x6 ÆGS−BE router
with 2 virtual channels of [4] was to be extended with the
logic of 6 sgtVCMs its area would be increased by only 6.6%
(in 0.13µm technology the ÆGS−BE area is 0.175mm2 and
the logic of one sgtVCM 1917µm2 ). For a 6x6 mesh NoC
the total area increase is only of 7.2% (4.15 vs 4.45 mm2 ).
This unoptimized implementation of the sgtVCM can be
clocked at over 730M H z allowing the implementation of
high-performance SuperGT routers (the ÆGS−BE router implemented in 0.13um technology runs at 500M H z ).
6. CONCLUSION
This paper presents the SuperGT packet-switched router
and network interface. Thanks to advanced virtual channel
management the SuperGT NoC supports three QoS classes:
BE,GT and SuperGT. As for GT, the SuperGT QoS class
allows to reserve resources on a connection to guarantee
bandwidth. Additionally, on the same connection, nonguaranteed bandwidth can be granted while guaranteeing
in-order packet delivery.
Simulation results show throughput improvements up to
35.6% for the SuperGT QoS over the GT QoS. Synthesis
results of the SuperGT virtual channel manager, the critical block in the SuperGT router, show an area increase of
only 6.6% in a state of the art router of large arity. The
SuperGT VCM is an inexpensive enhancement to state-ofthe-art packet-switched networks-on-chip with diﬀerentiated
services. It adds the SuperGT QoS, beneﬁcial to many MPSoC applications.
7. "
Layered Switching for Networks on Chip.,"We present and evaluate a novel switching mechanism called layered switching. Conceptually, the layered switching implements wormhole on top of virtual cut-through switching. To show the feasibility of layered switching, as well as to confirm its advantages, we conducted an RTL implementation study based on a canonical wormhole architecture. Synthesis results show that our strategy suggests negligible degradation in hardware speed (1%) and area overhead (7%). Simulation results demonstrate that it achieves higher throughput than wormhole alone while significantly reducing the buffer space required at network nodes when compared with virtual cut-through.","8.3
Layered Switching for Networks on Chip
Zhonghai Lu
zhonghai@kth.se
Ming Liu
mingliu@kth.se
Axel Jantsch
axel@kth.se
Royal Institute of Technology, Sweden
ABSTRACT
We present and evaluate a novel switching mechanism called layered switching. Conceptually, the layered switching implements
wormhole on top of virtual cut-through switching. To show the feasibility of layered switching, as well as to conﬁrm its advantages,
we conducted an RTL implementation study based on a canonical wormhole architecture. Synthesis results show that our strategy suggests negligible degradation in hardware speed (1%) and
area overhead (7%). Simulation results demonstrate that it achieves
higher throughput than wormhole alone while signiﬁcantly reducing the buffer space required at network nodes when compared with
virtual cut-through.
Categories and Subject Descriptors:
B.4 [Hardware]: Input/Output Data Communications;
General Terms: Design, Theory, Performance
Keywords:
Network-on-Chip, System-on-Chip, Switching Technique
1.
INTRODUCTION
The communication platform is becoming increasingly crucial
for complex System-on-Chip (SoC) integration in the nanometer
regime [12]. Systems such as mobile platforms, personal handheld
sets and multimedia terminals require the on-chip interconnects to
handle a huge amount of trafﬁc, driving the traditional bus-based
architectures towards network-based architectures. As the Network
is fabricated on Chip (NoC), both high performance and small area
are basic requirements [4, 8, 9].
The switching scheme of an interconnection network determines
how packets ﬂow through each node and how to handle packet
blocking. It is the primary factor in dominating the network’s performance [3]. Store-and-forward switching (SAF), wormhole switching (WH) [2] and virtual cut-through switching (VCT) [6] are three
commonly used schemes. Whereas an SAF switch must receive
an entire packet before forwarding it downstream, a WH or VCT
switch starts to transmit portion of a packet, i.e., ﬂit1 , once the
1Virtual cut-through does not divide packets into ﬂits. We use the
division to allow a consistent comparison in the paper.
downstream buffer is available to hold the ﬂit. In this way, WH and
VCT make ﬂit transmission pipelined, resulting in lower latency
than SAF if the network is not saturated. WH and VCT are both
cut-through switching schemes. They mainly differ in how they
handle packet blocking. With WH, buffers and links are allocated
at the ﬂit-level and the switch buffering capacity is a multiple of a
ﬂit. If a packet is blocked, ﬂits of the packet are stalled in place.
With VCT, a switch, at which a packet is blocked, must receive
and store all ﬂits of the blocked packet. This enforces that buffers
and links are allocated at the packet-level and the buffering capacity in switches must be a multiple of a packet. VCT utilizes the
network’s bandwidth more efﬁciently, achieving higher throughput
than WH but requiring higher buffering capacity. From the performance perspective, implementing VCT for on-chip networks is
preferable. However, switch buffers take a large portion of area and
consume signiﬁcant power.
In the Æthereal WH switch [4], the
ﬂit buffers occupy about 30% of area even after being optimized
buffers consumes about 30% of total node power in a 4 × 4 torus
as hardware FIFOs. It is estimated in [15] that the access of ﬂit
network. From the cost and power perspective, implementing WH
for on-chip networks is more desirable.
We present a layered switching strategy which combines the
salient features of both WH and VCT. It performs better than WH,
and consumes less storage than VCT. The idea is to introduce a
data abstraction, group, between ﬂit and packet. In order to minimize communication overhead and hardware implementation cost,
we realize a logical group by which ﬂits of a packet are virtually
partitioned into groups of a uniform size. This abstraction enables
us to vertically lay WH on top of VCT. As virtual channels are allocated packet-by-packet, and buffers and links are allocated groupby-group, we say that WH is performed on packets and groups;
Due to the fact that buffers and links are allocated group-by-group
and link pipeline is conducted ﬂit-by-ﬂit, we say that VCT is performed on groups and ﬂits. The synergy of WH and VCT results in
a layered WH-VCT switching.
In the sequel, Section 2 briefs related work. In Section 3, we describe the operation of a canonical wormhole switch. Then we detail the layered switching strategy, compare it with WH and VCT,
present a hardware implementation study, and show synthesis results in Section 4. Simulation results are reported in Section 5.
Finally, we conclude the paper in Section 6.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4-8, 2007, San Diego, California, USA
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
2. RELATED WORK
Wormhole switching dates back to the Torus Routing Chip [2],
a fast switch in 1986 technology. Dally proposed Virtual Channel
(VC) ﬂow control in [1] to break the dependency between physical
channel (PC) allocation from VC allocation. Since VCs decouple
buffer resources from transmission resources, this scheme allows
122
active ﬂits to pass blocked ﬂits using link bandwidth that would
otherwise be left idle. Network throughput is thus improved.
Based on VC ﬂow control, much work has been done to further
enhance the network performance by optimizing wormhole switch
architectures. Typical optimizations are centered on the control
path.
In [14], routing and arbitration latency may be eliminated
by statically scheduling buffer and link resources. This leads to the
increasing size of routing memory, thus very high cost. In addition,
this static method can not handle dynamic trafﬁc. Flit-reservation
ﬂow control [10] allows to pre-schedule resources but scheduling
decisions are determined at run-time. Link and buffer usage is
scheduled by sending control ﬂits ahead of data ﬂits over independent channels. Scheduling decisions are stored in reservation
tables in association with each input and output PC. This scheme
improves performance but adds signiﬁcant complexity to the switch
architecture. Recently, techniques to hide routing and arbitration
latency are proposed in [9]. This is realized by pre-determining the
results of routing and arbitration decisions one cycle before they are
requested. All the above works implement purely WH and greatly
complicate the switch designs.
In contrast, our proposal is a novel switching strategy. Like hybrid switching [13], our mechanism bridges the performance gap
between WH and VCT. However, our approach is fundamentally
different from it. The hybrid switching combines WH and VCT,
performing network ﬂow control on either ﬂits like WH or packets
like VCT. It differentiates blocked packets by selectively performing either WH or VCT blocking policy on them. With WH, the
blocked packets are stalled in place. With VCT, the blocked packets
are buffered locally in the node where the packets lose arbitration.
Our switching scheme performs network ﬂow control on groups. It
does not differentiate blocked packets. The same two-level blocking policy applies to all blocked packets. Upon packet blocking,
both WH and VCT blocking polices are employed, but on different
data abstraction. The WH blocking policy is performed on groups,
i.e., all groups are stalled in place. The VCT blocking policy is performed on each individual group, i.e., ﬂits of an entire individual
group is to be received and buffered locally at the node where the
group is stalled. Besides, since the hybrid switching differentiates
packets, it requires additional informative bits in a packet. This is
not necessary with our switching scheme. In [5], Hu et al. proposed
a hybrid routing technique that judiciously switches between deterministic and adaptive routing according to the network congestion
conditions. Our work focuses on the switching strategy, thus is orthogonal to theirs. If needed, both works could be combined.
3. WORMHOLE SWITCH ARCHITECTURE
Figure 1 illustrates a canonical input-queuing wormhole switch
architecture [11]. The Æthereal NoC [4] adopts this architecture after carefully analyzing the performance and cost trade-offs between
different queuing strategies. The switch has p physical channels
(PCs or links) and v lanes (VCs) per PC. It conducts credit-based
link-level ﬂow control to coordinate packet delivery between adjacent switches to avoid buffer overﬂow and ﬂit loss.
As sketched in Figure 2, for each packet, a switch passes through
the following steps: routing S1 , lane allocation S2 , ﬂit scheduling
S3 , switch arbitration S4 , switch traversal S5 and lane release S6 .
In the routing step, the routing logic determines the routing path
over which the packet advances. Routing is performed only when
the head ﬂit of a packet becomes the earliest-come ﬂit in the lane.
After routing, the output PC is determined. In the step of lane allocation, the lane allocator tries to associate the lane the packet occupies with an available lane in the next hop, i.e., to make a unique
lane-to-lane association. Note that it is not necessarily required that
Credits out (1...p)
1
2
p
Lane allocator
(p,v)
Flits in
(1...p)
1
2
p
Routing
Lanes (1...v)
states
(1...v)
states
(1...v)
Flit FIFOs
states
(1...v)
Credits in
(1...p)
Downstream
lane status
(p,v)
Switch allocator
(p,v)
Flits out
(1...p)
mux
mux
mux
1
2
p
1
2
p
p−by−p crossbar
Figure 1: A canonical wormhole switch
N
N
S1
Y
S2
Y
S3
head flit
body/tail flit
S0
S6
tail flit
head/body flit
N
S4
Y
S5
S0
: Initial
S1
: Routing
S2
: VC allocation
S3
: Flit scheduling
S4
: Switch arbitration
S5
: Switch traversal
(and link release)
S6
: VC release
Y : Succeed
N : Fail
Figure 2: Sequence of steps based on ﬂits
there is an empty buffer in the requested lane in order for the lane
to be associated or allocated. But, if the buffer availability (at least
one buffer) is a pre-condition to complete VC allocation, the ﬂitscheduling step is not needed any more for the head ﬂit of a packet.
In such a case, a head ﬂit directly transits from step S2 to S4 , as
indicated by the dashed line in Figure 2. A lane-to-lane association fails when all requested lanes are already associated to other
lanes in directly connected switches, or the lane loses arbitration
in case multiple lanes in the switch request the same downstream
lane. If the lane-to-lane association succeeds, the packet enters the
ﬂit scheduling step. If there is a buffer available in the associated
lane, the lane enters the switch arbitration (S4 ), which can be done
with a two-level arbitration scheme. The ﬁrst level of arbitration is
performed on the lanes sharing the same input port to the crossbar.
The second level of arbitration is for the crossbar traversal to output links. If the lane wins the two levels of arbitration, the earliestcome ﬂit in the lane enters the step of switch traversal (S5 ). The
ﬂit is switched out to the next hop and the granted link is released.
Otherwise, the lane stays in the arbitration step. Once the tail ﬂit is
switched out, the lane-to-lane association is released (S6 ), thus the
allocated lane is available to be reused by other packets. Credits are
communicated between adjacent switches in order to keep track of
the status of downstream lanes, such as if a lane is free, and a count
of available buffers in the lane.
4. THE LAYERED WH-VCT SWITCHING
4.1 Operation
Wormhole switching allocates lanes at the packet level, i.e., packetby-packet, but packet buffering and link allocation are conducted at
123
the ﬂit level. This mismatch between lane allocation and buffer/link
usage causes three major drawbacks. First, ﬂits other than a head
ﬂit must again enter the ﬂit scheduling step, and contend for switch
arbitration and traversal. This incurs extra control cycles. Second,
since ﬂits of a packet may be distributed in nodes along the packet’s
routing path, they occupy allocated VCs and independently contend
for the link bandwidth along the path. This increases contention for
outgoing links, limiting the network’s maximum throughput [13].
Third, the spread-out ﬂits may occupy only a portion of allocated
lane buffers. The buffers between the ﬂits can not be used by any
other packets, making the buffer utilization less efﬁcient.
Group 1
Group 2
Group 3
h
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
t
packet−head
group−tail
group−head
group−tail
group−body
group−head packet−tail
Figure 3: Partitioning a packet into 3 groups
We are motivated to shorten the control cycles and make efﬁcient use of ﬂit buffers and links. To this end, we introduce a data
abstraction, group, between ﬂit and packet, and perform network
ﬂow control on groups instead of ﬂits. We partition ﬂits of a packet
same size. Speciﬁcally, we partition a packet of m ﬂits into (cid:1)m/d (cid:2)2
into groups according to the depth d of a VC. Groups thus have the
groups, and each group contains d ﬂits. If m/d is not an integer, we
have to pad the last group with extra ﬂits3 . As shown in Figure 3,
a packet of 12 ﬂits is virtually partitioned into three groups if d is
four, and one group contains four ﬂits. As groups are of the uniform
size, we can use a simple counter to implement the grouping (See
Section 4.3). No additional bits are needed to explicitly distinguish
the group-head, group-body and group-tail ﬂits. Therefore a group
is a logical group.
N
N
S1
Y
S2
Y
S3
packet−head
S0
group−head
group−body or tail
S6
group−tail
N
S4
Y
S5
S7
packet−tail
S0
: Initial
S1
: Routing
S2
: VC allocation
S3
: Group scheduling
S4
: Switch arbitration
S5
: Switch traversal
S6
: Link release
S7
: VC and link release
:
Succeed
:
Fail
Y
N
packet−head or group−head/body
Figure 4: Sequence of steps with grouping
The sequence of steps with the grouping scheme is depicted in
Figure 4. After a packet-head ﬂit performs routing, VC allocation, scheduling and switch arbitration, the rest of ﬂits in the ﬁrst
group directly transit to the switch traversal step S5 . For a grouphead ﬂit, the lane transits from the initial step S0 to the scheduling
step S3 . Since it is the scheduling step for one group, we call this
step group scheduling. After completing this step, the group-head
ﬂit enters step S4 (switch arbitration). By winning the switch arbitration, the group-head ﬂit is allocated with the requested link
and then goes to step S5 (switch traversal). This link allocation
2 (cid:1)x(cid:2) is the ceiling function returning the least integer that is not
less than x.
3We expect that, in NoC designs, it is possible to select m and d in
a way to minimize or avoid this overhead entirely.
will be inherited by the body and tail (either group-tail or packettail) ﬂits of the group, which will directly go from step S0 to S5 .
This shrinks the ﬂit-scheduling and switch arbitration cycles for
group-body, group-tail and packet-tail ﬂits, thus speeding up the ﬂit
traversal. After the group-tail ﬂit is switched out, the granted link
is released (S6 ). When the packet-tail ﬂit is switched out, both the
granted link and the allocated VC are released (S7 ). To complete
the group-scheduling step S3 , an entire group of free ﬂit buffers
must be detected and then allocated. This is a strong request on
the number of available buffers. However, it is in fact sufﬁcient to
detect one buffer available in the requested lane. The availability
of one buffer means either there are buffers for an entire group or
the group is in the process of being transmitted to the next switch
since links are allocated on a group basis. While the previous group
is switched out, the new group can be switched in, occupying the
lane buffers in a pipelined fashion. Thus the condition is satisﬁed
on-the-ﬂy with the one-buffer-availability.
The resulting switching mechanism implements a layered WH
and VCT switching. It performs WH on packets and groups since
VCs are allocated to packets, and buffers and links to groups. Meanwhile, it performs VCT on groups and ﬂits since buffers and links
are allocated to groups and the link pipeline is still conducted on
ﬂits. The layered switching requires a switch to hold entire group(s)
in case of packet blocking. The switch buffering capacity is a multiple of a group. This seems implying a higher buffering consumption than WH. However, this is not necessary. If we use the VC
depth d or a factor of d of a wormhole switch as the group size, the
layered switching does not incur extra buffers in switches.
4.2 Discussion
Switching
control level)
WH (Flit)
WH-VCT (Group)
VCT (Packet)
Link allocation
(Flow
VC
Packet
allocabuffertion
ing
Packet
Flit
Flit
Packet
Group
Group
Packet
Packet
Packet
Table 1: Flow control granularity
Link
pipeline
Flit
Flit
Flit
The granularity of network ﬂow control can be used to differentiate one switching technique from another [3]. WH with or without
VCs performs ﬂow control on the ﬂit level; VCT does this on the
packet level. The layered switching in fact realizes a group-level
ﬂow control. The group size g in ﬂits is in the range [1, mmax ],
where mmax is the maximum number of ﬂits of packets. Table 1
compares the three pipeline switching methods. All of them allocate VCs to packets and pipeline ﬂits on links. The unit for packet
buffering and link allocation reﬂects their ﬂow control granularity.
The group-level ﬂow control (WH-VCT) can be positioned between the ﬂit-level (WH) and the packet-level ﬂow control (VCT).
If g = 1, one group is one ﬂit, and the group-level ﬂow control
becomes the ﬂit-level one; if g = mmax , one group is one packet,
and the group-level ﬂow control resembles the packet-level one.
Note that a group is not simply a larger ﬂit. If this were the case,
a switch must always receive an entire group before forwarding it
downstream (in the SAF fashion). The link pipeline would be based
on groups. In our case, a switch sends portion of a group, i.e., ﬂit,
downstream as soon as enough buffering is available to hold the ﬂit
(in the VCT fashion). The link pipeline is still based on ﬂits.
4.3 An implementation study
To investigate the hardware implementation overhead of our proposal, we implemented a wormhole switch and a layered WH-VCT
switch at RTL by following the canonical switch model. Figure 5
124
o n e p h y s ic a l c h a n n e l
F l i t i n
F l i t
C r e d i t o u t
c r e d it
g e n e r a t o r
v la n e s
c o u n t e r
F S M
a r r a y
 ( * v )
g r a n t
t a il
c h e c k e r
r o u t in g
g r a n t
r e le a s e
r e q
r e q
r e le a s e
r e q
r e q
s in k _ a llo c a t o r
s in k _ s c h e d u le r
VC _ a llo c a t o r
VC _ s c h e d u le r
s w it c h _ a llo c a t o r
r e q
s in k la n e
s t a t u s
f o r w a r d la n e
s t a t u s
Figure 5: The implementation structure
F l i t o u t
to implement the ﬂit FIFOs in order to achieve better performance
and power efﬁciency. The switch speed is constrained by the control path. The WH switch can be clocked up to 396 MHz. As
a data cycle comprises two control cycles, the data operating frequency is 198 MHz. The WH switch has an estimated gate count of
41K gates. The control clock of the WH-VCT switch can operate
at 392 MHz, resulting in a data clock of 196 MHz. The WH-VCT
switch consumes 44K gates. About 20% of the areas is counted for
buffers. The frequency degradation is 1% and area overhead is 7%.
Note that this area overead is less signiﬁcant when compared with
a VCT switch. For a VCT switch, the lane depth d must be a multiple of a packet. Assume that d = 2 packets and a small packet size
m = 4 ﬂits, then the number of lane buffers is quadruple, resulting
in a gate count of about 64K for the VCT switch.
C r e d i t i n
5. SIMULATION RESULTS
sketches their implementation structure. For simplicity, only a single input and a single output are shown. The sink modules (sink allocator/scheduler/status) in Figure 5 are used for ﬂit ejection. They
shared sink queues instead of p · v sink queues as required by an
implement a cost-effective p-sink model that ejects ﬂits using p
ideal ﬂit ejection model [7].
Both designs share most of the modules in the structure. The
main differences lie in the FSMs and switch allocator. There are v
FSMs per PC, one for each lane. An FSM manages the states of a
lane, as described previously. With the grouping scheme, the FSMs
are added with a counter. Together with the ﬂit type information in
a ﬂit, this counter is used to distinguish different ﬂit types. Wormhole switching encapsulates a packet into a head ﬂit, body ﬂit(s)
and a tail tail. Single-ﬂit packets are also possible but irrelevant
to our discussion since it is meaningless to perform grouping on
them. In a ﬂit, there is a ﬂit type ﬁeld to indicate its ﬂit type. With
the grouping scheme, there are still physically three ﬂit types but
logically ﬁve ﬂit types, i.e., the packet-head, group-head, groupd ﬂits, the counter has a value range from 0 to d − 1. Starting with
body, group-tail and packet-tail ﬂits (See Figure 3). If a group has
0, the counter value is incremented whenever a ﬂit is switched out.
Once a group-tail or packet-tail ﬂit is sent, it is reset to 0. The
counter value 0 represents either a packet-head or group-head ﬂit.
The counter value d − 1 represents either a packet-tail or group-tail
ﬂit. Since group-head and group-tail ﬂits are physically body ﬂits,
this helps to distinguish a group-head from a packet-head ﬂit, and
a group-tail from a packet-tail ﬂit. The counter value(s) between 0
and d − 1 are group-body ﬂits. The switch allocator also uses the
counter values to distinguish ﬂits of different types. This simple
counter enables to virtually partition a packet into groups of ﬂits.
The implementations are not single-cycle models. Instead they are
cycle-true. With WH, it takes six cycles for a head ﬂit and four
cycles for a body or tail ﬂit to pass through the switch. With the
WH-VCT, it takes six cycles for a packet-head ﬂit, four cycles for a
group-head ﬂit, and one cycle for other types of ﬂits to pass through
the switch. This implies that the WH-VCT can also improve the
zero-load latency. Despite the high latency for ﬂit traversal of a
switch, ﬂits are pipelined on links, thus the network bandwidth can
be efﬁciently utilized, achieving high throughput.
We synthesized the designs for performance using 180nm technology. Both designs implement the dimension-order XY routing.
We set the ﬂit width W f l it = 32, the number of lanes per PC v = 4,
and the lane depth d = 2 ﬂits. Similarly to [5], we used registers
5.1 Experimental setup
To investigate the performance of the proposed technique, we
construct 4×4 mesh networks using the two switch designs. The
baseline network uses WH, the other the layered WH-VCT switching. The XY routing guarantees deadlock-free on the mesh [3].
The simulations were run with uniformly distributed trafﬁc. Fixedsize packets were injected synchronously to random destinations
except for themselves at a constant rate. Except otherwise noted,
contention for lanes and channel bandwidth were resolved using
round-robin. Each node injected 1500 packets into the network.
Simulation statistics were collected after all the packets were received. For each simulation, warm-up and cool-down cycles were
not included in the results, i.e., the results show the performance of
the network at a steady state. We investigated the packet latency
and network throughput. Latency of a packet is calculated from
the instant the packet is injected into the packet source queue to
that the packet is ejected from the network. This packet latency
has two components. One is the queuing time in the source queue.
The other is the network delivery time counting from the instant
the packet enters the network to that the packet is ejected from the
network. Throughput is deﬁned as the number of ﬂits received per
cycle per node in normalization with the network capacity, i.e., the
ideal throughput, which is 1 ﬂit/cycle/node for a mesh network under the uniform trafﬁc.
Test 1
Test 2
Test 3
Test 4
Test 5
W1 L1 W2 L2 W3 L3 W4 L4 W5 L5
4
8
8
v
m
d
g
n
A
RR
RR
RR
FR
RR
Table 2: Switch parameters in all 5 tests (v: the number of VCs per
PC; m: packet size in ﬂits; d : the VC depth; g: group size in ﬂits;
n: the number of groups; A: switch arbitration.)
16
4
4
2
4
2
2
4
4
4
8
1
8
2
8
4
8
4
We conducted ﬁve tests. The switch parameters are listed in
Table 2, where Wx and Lx mean Test x for Wormhole and the
Layered switching, respectively. We set the group size g to the VC
depth d , i.e., one VC can hold exactly one group of ﬂits. With the
layered switching, Test 1, 2 and 3 result in different numbers of
groups due to a different VC depth. They partition a packet of 8
ﬂits into four, two and one group(s), respectively. Test 3 with the
layered switching (L3) mimics virtual cut-throughput since its VC
125
depth equals to the packet size. Test 1, 2 and 3 use round-robin
(RR) for switch arbitration. To study the effect of the arbitration
mechanism, Test 4 uses ﬁxed-priority (FR) for switch arbitration.
Test 5 has a longer packet size of 16 ﬂits.
5.2 Basic comparison
0
0
0.1
0.2
0.3
0.4
0.5
Throughput (fraction of capacity)
0.6
0.7
0.8
25
50
75
100
125
150
175
200
225
250
A
e
v
r
e
g
a
l
a
t
y
c
n
e
W2: 4 buffers
 L2: 4 buffers, 2 groups
Figure 6: Basic performance comparison
Figure 6 draws the average latency in data cycles as a function of
the measured throughput for Test 2. With the same amount of storage, the layered switching (L2) saturates the network at a higher
throughput than WH (W2) due to more efﬁcient use of VC buffers
and link bandwidth. The graph also reﬂects a lower latency due to
shrinking ﬂit-scheduling and switch arbitration cycles for groupbody and tail ﬂits, as well as reducing VC and link contention.
Speciﬁcally, the layered switching (L2) reduces the minimum latency by 28% from 57 to 41 cycles. The network saturates at 72%
of capacity, 12% higher than 64% of its WH counterpart (W2).
As we shall see in Figure 8, this throughput is even higher than
the throughput (68%) in Test 3 with WH (W3: 8 buffers per VC),
which doubles the buffer capacity of Test 2.
0
0
50
100
150
200
250
Network delivery time
300
350
400
100
200
300
400
500
600
700
800
N
u
m
e
b
r
o
f
e
k
c
a
p
t
s
W2: 4 buffers
 L2: 4 buffers, 2 groups
Figure 7: Histogram of network delivery time
In Figure 7, we depict a distribution graph by the number of
packets with their experienced network delivery time. The packet
injection rate is 1 packet every 13 cycles (0.615 ﬂit/cycle/node). At
this point, both networks are highly utilized but not saturated. We
can observe that the layered switching (L2) moves the envelop of
the WH (W2) towards the origin along the X-axis, implying that
more packets enjoy lower network delivery time. In addition, the
number of packets experiencing high latency is reduced. Particularly, the layered switching shrinks the observed maximum delivery
time from 396 to 179 cycles, reducing it by 55%.
5.3 The effect of group size
0
0
0.1
0.2
0.3
0.4
0.5
Throughput (fraction of capacity)
0.6
0.7
0.8
25
50
75
100
125
150
175
200
225
250
A
e
v
r
e
g
a
l
a
t
y
c
n
e
W1: 2 buffers
 L1: 2 buffers, 4 groups
W2: 4 buffers
 L2: 4 buffers, 2 groups
W3: 8 buffers
 L3: 8 buffers, 1 group
Figure 8: Performance with group sizes
Test 1 and 3 are designed to observe the minimum and maximum improvement with the layered switching. Figure 8 shows the
results. As the number of partitioned groups decreases, the size of
a group is closer to a packet and the beneﬁt is generally increased.
With Test 1, 2, 3, the minimum latency improvements are 6%, 28%
and 35%, respectively; the throughput improvements are 5%, 12%
and 10%, respectively. The minimum improvement in both latency
and throughput occurs in Test 1, where the depth of a VC is two,
which is minimal to counter for the round-trip credit latency (2 cycles) so as to enable fully pipelining ﬂits [3]. The best latency improvement happens in Test 3 where one packet is partitioned into a
single group. The best throughput improvement occurs in Test 2,
not in Test 3. This is due to the fact that Test 3 with WH (W3)
is already approaching the bound of saturation throughput. Although the layered switching (L3) still improves this throughput,
the acceleration is slowed down. As a larger group size implies
higher buffer consumption, Test 2 addresses the tradeoff between
high performance and small buffer requirement.
5.4 The impact of arbitration mechanism
0
0
0.1
0.2
0.3
0.4
0.5
Throughput (fraction of capacity)
0.6
0.7
0.8
25
50
75
100
125
150
175
200
225
250
A
e
v
r
e
g
a
l
y
c
n
e
a
t
W4: 4 buffers, fixed−pri.
 L4: 4 buffers, 2 groups, fixed−pri.
Figure 9: Performance with prioritized arbitration
An important property for arbitration is fairness. The roundrobin arbitration for switch traversal is fair. We implemented also
a priority-based arbitration policy. This unfair policy sets a ﬁxed
priority for each VC of an input port. Flits situated in a VC with
a higher priority win switch arbitration to use the contended link.
Simulation results on latency are shown in Figure 9. With the unfair arbitration, the layered switching (L4) improves performance
126
 
 
 
 
 
 
 
 
 
 
 
 
 
similarly to the round-robin policy (W4). The latency and throughput are improved by 28% and 15%, respectively. We also looked at
the distribution graph of delivery time with the ﬁxed-priority arbitration. Similarly to the fair arbitration, the group-level switching
largely moves the graph envelop towards the origin along the Xaxis. This means that the layered switching improves the network
performance without bias towards switch arbitration policies.
5.5 The effect of packet size
W2: 4 buffers, 8 flits
 L2: 4 buffers, 2 groups, 8 flits
W5: 4 buffers, 16 flits
 L5: 4 buffers, 4 groups, 16 flits
250
225
200
175
150
125
100
75
50
25
y
c
n
e
t
a
l
e
g
a
r
e
v
A
0
0
0.1
0.2
0.3
0.4
0.5
Throughput (fraction of capacity)
0.6
0.7
0.8
Figure 10: Performance with longer packets
The previous experiments use packets of eight ﬂits. Test 5 uses
longer packets of 16 ﬂits. The results are depicted in Figure 10
together with Test 2 that uses the same amount of buffer storage.
The minimum-latency and throughput are improved by 34% and
11%, respectively. Enlarging the packet size without increasing ﬂit
buffers per VC leads to a larger number of groups per packet. In
this case, the group size is increased from two to four. The incremental latency improvement from 28% in Test 2 to 34% in Test
5 comes mainly from the increasing number of ﬂits enjoying fast
switching due to the deduction of ﬂit-scheduling and switch arbitration cycles. For the latency, with the layered switching, longer
packets beneﬁt more from the grouping scheme. But the improvement in throughput is not linear due to the network’s approaching
the throughput bound.
t
u
p
h
g
u
o
r
h
t
/
y
c
n
e
a
t
l
d
e
z
i
l
a
m
r
o
N
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
Latency with wormhole switching
Latency with layered switching
Throughput with wormhole switching
Throughput with layered switching
1
2
3
Test
4
5
Figure 11: Normalized latency and throughput
We summarize the results normalized with their corresponding
wormhole counterparts for the ﬁve tests in Figure 11. The maximum/minimum improvement in latency and throughput is 35%
(Test 3) /6% (Test 1) and 15% (Test 4) /5% (Test 1), respectively.
127
6. CONCLUSION
We have presented the concept of layered WH-VCT switching.
By virtually partitioning packets into groups of ﬂits, network ﬂow
control can be performed on groups but pipelining is still conducted
on ﬂits. It reduces the packet scheduling and switch arbitration cycles, and makes efﬁcient use of links and buffers. Therefore the
envelope of the latency-throughput graph is lowered and extended,
when compared with WH. For instance, with 4 VCs per input and
4 ﬂit buffers per VC, the layered switching scheme partitioning one
packet into two groups reduces the average packet latency by 28%,
and improves throughput by 12.5% from 64% to 72% of capacity,
which is even higher than the throughput (68%) of WH with doubled buffer capacity, i.e., 8 ﬂit buffers per VC. In comparison with
VCT, the layered switching requires signiﬁcantly less storage since
it buffers groups not packets upon packet blocking. The grouping
is logical and implemented by a simple counter in a switch, thus
the com"
Energy-Aware Synthesis of Networks-on-Chip Implemented with Voltage Islands.,"Voltage islands provide a very good opportunity for minimizing the energy consumption of core-based Networks-on-Chip (NoC) design by utilizing a unique supply voltage for the cores on each island. This paper addresses various complex design issues for NoC implementation with voltage islands. A novel design framework based on genetic algorithm is proposed to optimize both the computation and communication energy with the creation of voltage islands concurrently for the NoC using multiple supply voltages. The algorithm automatically performs tile mapping, routing path allocation, link speed assignment, voltage island partitioning and voltage assignment simultaneously. Experiments using both real-life and artificial benchmarks were performed and results show that, by using the proposed scheme, significant energy reduction is obtained.","Energy-Aware Synthesis of Networks-on-Chip Implemented 
with Voltage Islands 
8.4
Lap-Fai Leung  
Department of Electronic and Computer Engineering 
Hong Kong University of Science and Technology 
Clear Water Bay, Hong Kong SAR, China 
eefai@ece.ust.hk 
Chi-Ying Tsui  
Department of Electronic and Computer Engineering 
Hong Kong University of Science and Technology 
Clear Water Bay, Hong Kong SAR, China 
eetsui@ece.ust.hk 
ABSTRACT 
Voltage islands provide a very good opportunity for minimizing the 
energy consumption of core-based Networks-on-Chip (NoC) design 
by utilizing a unique supply voltage for the cores on each island. 
This paper addresses various complex design issues for NoC 
implementation with voltage islands.  A novel design framework 
based on genetic algorithm is proposed to optimize both the 
computation and communication energy with the creation of voltage 
islands concurrently for the NoC using multiple supply voltages. The 
algorithm automatically performs 
tile mapping, routing path 
allocation, link speed assignment, voltage island partitioning and 
voltage assignment simultaneously. Experiments using both real-life 
and artificial benchmarks were performed and results show that, by 
using the proposed scheme, significant energy reduction is obtained. 
Categories and Subject Descriptors 
C.5.4 
[COMPUTER SYSTEM 
Systems 
General Terms: Algorithms, Design, Performance 
Keywords: Network-on-Chip, Voltage Island, Routing 
IMPLEMENTATION]: VLSI 
1. INTRODUCTION 
Continuous improvement in the IC fabrication process technology 
allows billions of transistors and hundreds of computation resources 
or processor cores to be put on a single chip. System-on-chips (SoCs) 
with these capabilities require efficient communication architecture 
to offer scalable bandwidth and parallelism. Network-on-chip (NoC) 
with multi-Gbit/s bandwidth has emerged to be a viable solution to 
provide the necessary communication links among different on-chip 
processing cores [1]. For these complex NoC systems, power 
consumption becomes the most pressing design problem. Multi-Vdd 
is an effective method in reducing both the dynamic and leakage 
power for complex digital system. How to assign Vdd to different 
building blocks becomes an issue. To reduce the complexity of the 
power supply network and to minimize the number of level shifter 
required, voltage islands concept was proposed in [4] for core-based 
SoC systems. Here the chip is powered by several voltage sources 
and each voltage source powers a portion of the entire chip which is 
called the voltage island. The core in each island is usually adjacent 
to each other to simplify the power distribution. Voltage islands can 
be naturally extended to NoC design to reduce the energy 
consumption. For NoC design, both energy consumption of the 
processor cores and the communication links are significant.  
*This work was supported in part by Hong Kong RGC CERG
under Grant HKUST6256/04E. 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. 
DAC 2007, June 4–8, 2007, San Diego, California, USA 
Copyright 2007 ACM 978-1-59593-627-1/07/0006…5.00 
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
128
 With voltage islands, not only different cores and tasks can run at 
different optimal voltages to reduce the energy consumption, but 
also voltage scalable communication link can be used to reduce the 
energy consumption. However, introducing voltage islands makes 
the NoC synthesis even more complicated. The design issues such as 
tile mapping, routing path allocation, link speed assignment have to 
be re-considered when voltage islands are used. In particular, the 
design complexity grows significantly with the number of allowed 
islands. Thus, we need to group together the cores powered by the 
same voltage source in order to reduce the design complexity. How 
to partition the tiles/cores, how to group the adjacent cores to form 
voltage islands and how to assign the optimal voltage are the new 
design issues that need to be solved.  
 Recent researches on NoCs synthesis mainly addressed the interprocessor communication optimization problem and only focused on 
part of the NoCs synthesis design steps [4-7,11]. However, all the 
above works do not consider the effect of the computation timing on 
calculating the latency of the data transmission. Also, they do not 
consider the overall execution time and therefore, may not be 
suitable for applications that have hard deadline requirement. At the 
same time, they did not consider the use of voltage islands. Recently, 
research works have also been proposed for voltage scaling on 
communication links to reduce energy consumption. In [14], voltage 
schedule algorithms, which carry out joint dynamic voltage scaling 
for variable voltage processors and communication links, was 
proposed. This work assumed each processor and link can use 
different supply voltage. This assumption does not apply for voltageisland-based NoC systems as processors and links in the same 
voltage island are supplied by the same voltage. Partitioning and 
floor-planning are important for the physical implementation of the 
voltage islands. In [4], the problems of island partition creation, 
voltage level assignment and floor-planning for core-based SoC 
were tackled. In [2] a methodology was proposed to exploit nontrivial voltage island boundaries for optimal power versus design 
cost trade-off under performance requirement. These approaches 
worked on the floor-planning problem. For NoC design, we need to 
consider the grouping of tiles for voltage island partitioning. 
  In this work, we work on the design of NoC with voltage islands. 
We target at tile-based NoC architecture that supports variable 
voltage processor cores and voltage scalable communication links. 
Each processor and link is characterized with a set of discrete supply 
voltage values and the corresponding operating frequency. In 
particular, given a target application described by a communication 
dependence and computation graph (CDCG), a set of heterogeneous 
processor cores, a tile-based NoC architecture and the hard deadline 
of the application, we propose an energy efficient NoC synthesis 
algorithm which solves the following problems simultaneously: i) 
tile mapping to determine the corresponding tile that each processor 
should be mapped onto; ii) routing path allocation to obtain the legal 
routing path for each communication transaction; iii) physical 
voltage island creation and voltage assignment for each voltage 
island and communication link. The objective is to minimize total 
 
 
 
energy of the processor cores and communication links while 
satisfying the hard deadline of the applications. The overall design 
problem is very complicated and we propose a framework based on 
genetic algorithm.  
2. Problem Definition and System Models  
2.1 Problem Definition 
Given an application represented by a CDCG and the required 
deadlines, a set of heterogeneous processors to which each task in 
the CDCG has been allocated, a target N*N tile-based NoC 
architecture, and the maximum number of voltage islands, we 
synthesize the NoC design such that the total energy consumption of 
the task executions and communication transactions are minimized 
while the deadlines are satisfied.  
2.2 Task model and NoCs Architecture 
In this work, we target for NoC with heterogeneous processing 
elements (PEs). We assume the hard real-time applications are 
periodic, 
frame-based non-preemptive. The 
application 
is 
represented by a communication dependence and computation graph 
(CDCG) similar to that in [11] which is a directed graph, G(V,E). 
The CDCG contains M periodic tasks 
VTi ∈ and each task Ti has its 
own worst-case-execution-cycle (WCEC) Wi. We assume each task 
can only map onto a specific PE. In this case, the task allocation is 
just simply assigning a task to the corresponding PE. Each directed 
edge 
 between Ti and Tj in the CDCG characterizes the 
communication message. A PE can only execute a single task at any 
time. Each PE Pq is connected to a router and each router is 
connected to five links, including the 4 adjacent routers and its own 
PE. We assume a N*N tile-based NoC architecture. We denote the 
link from tile τp to tile τq as ep,q. The clock frequency of the PE and 
the bandwidth of each link ep,q is scaled with the corresponding 
supply voltage. Each edge Ei,j n the CDCG is associated with a value 
CE(Ei,j) which is the amount of data communicated between Ti and Tj. 
If Ti and Tj are allocated on the same PEs, CE(Ei,j) is zero. We also 
use Ζi,j to denote the set of links used by the routing path of 
communication transaction Ei,j.. Similar to the work in [11], 
wormhole routing scheme is used for the data communication.  
E
E ji ∈,
2.3 Energy Cost Model 
The dynamic energy consumption (Edyn) of executing task Ti is 
calculated as 
=
⋅
⋅
   (1) 
where Ceff is the effective switching capacitance, Wi is the number of 
clock cycles required for the execution and vi is the supply voltage 
of Ti. For processes that have significant leakage current, the total 
energy consumption Ei should also include the energy due to the 
active leakage current of the systems (Eleak) [14], which is given by
2
i
i
ff
dyn
vW
Ce
E
(
)
i
bs
J
VK
vk
i
g
leak
d
V
I
e
e
vK
L  
E
u
bs
i
=
⋅
⋅
⋅
⋅
+
⋅
⋅
(2) 
where Vbs is the body-bias voltage and IJu represents the body 
junction leakage current (a constant for a given technology). K3, K4 
and K5 are circuit technology dependent constants, Lg reflects the 
number of gates and di is total execution time of Ti, which is given 
⋅
⋅
by 
where Vth is the threshold voltage, k is a 
⋅
⋅
5
4
3
(
)α
th
i
i
i
i
vWk  
V-v
d
=
constant for a given technology process and 1≤α≤2. 
In [6], Hu et al. modeled the energy consumption of transferring 
one bit of data from tiles τp to τq  (
) at the maximum link speed 
as:      
E
 n
E
n 
1
E
   (3) 
q
bitE
τ
p ,τ
(
)
bit
bit
qp
L
hops
S
hops
,τ
τ
bit
⋅−
+
⋅
=
where nhops is the number of routers that the data passes through 
from τp to τq and 
bitSE and 
bitLE represent the energy consumed by 
the router and 
the 
link, respectively. For voltage scalable 
communication links, we modify the energy consumption of the 
communication by scaling with the corresponding supply voltage as 
each link for each transaction is running at different bandwidth and 
hence with different voltages. The total energy consumption Etotal 
including the computation and communication energy is equal to:  
   (4) 
(
)
(
)
∑
∑
∑
∑
∀
E
∈
Z
∀
e
∈∀
R
R
∀
T
⎫
⎪
⎬
⎪⎭
⎧
⎪
⎨
⎪⎩
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
⎤
⎥
⎦
⎡
⎢
⎣
⋅
⎤
+⎥
⎦
⎡
⎢
⎣
⋅
⋅
+
j
i
j
i
qp
,
bit
j
i
p
bit
i
2
q
p
L
2
p
S
j
i
E
i
min
v
V
 v
E
 v
V
E
EC
E
,
,
,
max
max
,
,
where vq is the supply voltage of Rq. The second term represents the 
energy of the communication transaction, which sums up for all the 
transactions, the router energy and the link energy scaled by the 
corresponding supply voltage which is the lowest voltage supply 
between two cores. If the transmitter and the receiver are on 
different voltage islands and hence run at different speed, the data 
transmission speed of the link cannot be greater than the minimum 
of the speed of the transmitter and the receiver. So we use the lower 
voltage of the two voltage islands as the supply voltage for the link.  
3. Design Methodology  
The voltage-island-based NoC synthesis 
includes solving 
the 
following sub-problems: i) tile mapping; ii) routing path allocation; 
iii) voltage island partitioning and iv) voltage assignment. Finding 
an optimal schedule for multi-processor system that consumes the 
least energy is known to be NP-hard [6]. Finding the optimum 
energy solution for the voltage-island-based NoC architecture is an 
even harder problem. In this work, we propose a design framework 
based on genetic algorithm. It is similar to the framework proposed 
in [5] which use genetic algorithm for tile mapping and routing to 
explore the whole solution space effectively. The main difference is 
that we propose an efficient inner-loop step to generate the voltage 
island partitioning and assign the voltage simultaneously for fast 
fitness evaluation. 
3.1  Tile Mapping  
In the genetic algorithm for tile mapping, we use an array of integer 
to encode the tile mapping solution. Traditional two-point crossover 
operation of the genetic algorithm will result in illegal solutions for 
our tile mapping problem because a processor can be mapped onto 
different tiles. In order to avoid the illegal mapping, similar to [5], 
we use cycle crossover [13], which is appropriate for the formulation 
where the encoding represents a sequence. In the mutation operation, 
two genes (a particular PE-tile matching pair) are selected with 
evenly distributed probability of 0.5% and they are exchanged to 
make a new individual. For each individual, we perform routing path 
allocation and voltage island partitioning steps and then evaluate the 
fitness value which is calculated using equation (4). 
3.2 Routing Path Allocation 
Routing path allocation is critical to the performance and the 
feasibility, especially for the system with heavy communication 
traffic. Here, we use genetic algorithm to explore the possibility of 
generating a better energy efficient routing path allocation. The 
genetic algorithm is similar to that proposed in [5]. We adopt the 
idea in [5] that we use a binary integer to indicate the directions of 
the routing path allocation. We use “0” to represent a path sending 
data along the X-direction and “1” to represent the data sending 
along the Y-direction. Since we only use minimum path routing, we 
can easily distinguish the correct directions, left or right along the Xdirection and up or down along the Y-direction. Typical approach 
using one-point crossover would generate illegal routing path 
allocation. In order to avoid illegal solution, we make sure the total 
number of “1” of each child individual is preserved. In this case, we 
129
  
      
 
  
 
 
 
 
 
 
 
 
 
 
 
 
     
 
 
 
  
select the crossover point such that the total number of “1” before 
the crossover point of the first parental gene is the same as that of 
the second parental gene. In the mutation operation, two genes (i.e. 
two corresponding routing directions) are selected randomly with 
evenly distributed probability of 0.5% and they are exchanged to 
make a new individual. 
3.3 Voltage Island Partitioning and Voltage 
Assignment   
Once we get a valid tile mapping and routing configuration, we need 
to generate the optimal voltage island partitioning and voltage 
assignment in order to calculate the fitness of the corresponding 
solution. In this work, we employ and compare three different 
voltage island topologies. The first one is a pre-defined squareshaped island. The shape of the island and the tiles that grouped to 
form an individual island are fixed. Once the voltage of a tile is 
assigned, the voltages of the other tiles in the same island are also 
assigned with the same value.  So for this topology, we only need to 
do the voltage assignment. The second and third topologies are 
adaptive rectangular shape and rectilinear shape, respectively. For 
the adaptive shape, the location and shape of the island are not fixed. 
It depends on the assigned voltage of the tiles. Thus it can increase 
the flexibility of voltage island partitioning to obtain a better energy 
efficient architecture. For the rectangular and rectilinear-shaped 
voltage islands, the tiles covered by the minimum bounding 
rectangle or spanning rectilinear shape have to use the same voltage 
when two tiles are assigned with the same voltage. Thus island 
partitioning and voltage assignment have to be done simultaneously.  
 We use a scaled voltage algorithm which is based on the Scaled 
Voltage Selection (SVS) proposed in [8], to assign the supply 
voltage for the tiles. In the SVS algorithm, the voltage is obtained by 
scaling down the delay of all the tasks by the ratio of the critical path 
(CP) delay obtained by a maximum speed schedule (we will discuss 
how to obtain this schedule later), over the deadline. The drawback 
of this approach is that the tasks on the non-critical paths do not take 
advantage of the available slack time. We improve this by iteratively 
recalculating the scaling ratio for all the tasks in the non-critical 
paths (NCPs). The algorithm first identifies the critical path of the 
maximum speed schedule and assigns the supply voltage for the 
critical path. In this work, we assume each core and link is 
characterized by a set of discrete voltage values (bounded by a given 
maximum and a minimum value) with 
the corresponding 
performance values [3]. So the assigned voltage for the PEs and the 
links are equal to the minimum voltage in the discrete set that results 
in a performance just satisfy the deadline requirement.  
 For the design with adaptive shape, we group those PEs on the 
critical path to form a single voltage island since using uniform 
slow-down on the critical path results in the minimum energy for the 
critical path [8]. The other PEs inside the minimum bounding 
rectangle (or the minimum spanning rectilinear shape) are also 
included in the same voltage island and assigned with the same 
voltage. In the next step, the algorithm identifies the next critical 
path (CP’) among all the tasks on the NCPs. The scaling ratio of 
those tasks and links on the CP’ that have not been assigned a 
voltage will be calculated and the corresponding voltage is obtained 
from the discrete voltage set. This iterative process is continued until 
all the PEs/tiles and links are allocated to a voltage island with an 
assigned voltage. At the end, the number of voltage islands may be 
larger than that specified by the user. We then carry out an island 
merging process to reduce the number of voltage islands. We 
identify two adjacent islands to merge to form a larger island. After 
merging, the high voltage of the two islands is used for the merged 
island to make sure the deadline will not be violated. Thus after 
merging, the energy consumption will be increased and the fitness 
S
S
S
S
Voltage Is land VI2
S
S
S
S
S
S
S
S
T5
PE15
T6
PE10
T3
PE7
T2
PE3
T4
PE4
S
S
S
S
T1
PE2
Voltage Is land VI1
T1
PE2
5Kb
5Kb
ET2=100
T2
PE3
10Kb
5Kb
ET1=200
ET3=200
T3
PE7
T5
PE15
5Kb
T6
PE10
ET5=50
ET6=100
V=1V
ET4=250
V=1 .2V
T4
PE4
Deadline4=950μs
(a) A CDCG       (b) Tile Mapping of the CDCG  
Figure 1. An example showing voltage island partitioning 
value in equation (4) will reflect this. Here, we stress that the 
merging does not increase the energy consumption of the final 
solution much because we always try to find a better tile mapping 
that require less merging (resulting in less energy consumption) by 
exploring the entire solution efficiently using genetic algorithm. 
Among all the possible island merging, we choose the merging of 
two islands that will result in the least increase in energy 
consumption after merging. The merging process iterates until the 
number of the voltage island is the same as that specified by the user. 
This voltage island partitioning algorithm is simple yet effective and 
is suitable to be used in the inner loop of the genetic algorithm. 
In some cases, non-critical tasks and critical tasks are grouped 
into the same voltage island and this may lead to increased energy 
consumption because some non-critical 
tasks are running at 
unnecessary higher speed. However, the genetic algorithm will try to 
obtain a better solution by keeping those PEs executing the tasks on 
the non-critical path away from the voltage island for the critical 
path and grouping them in separate voltage islands which can use 
lower supply voltages. By doing so, the fitness value is higher and 
the energy consumption is reduced. 
 Now the remaining issue is to get the maximum speed schedule. 
This is obtained by assuming all the PEs and links are operating at 
the maximum voltage and speed. For the task scheduling, we adopt 
the list scheduling algorithm which uses the ready time of each task 
to determine its priority. To get the ready time, we need to know the 
communication delay of each edge. However, we do not know the 
exact delay until we obtain the exact schedule but this requires the 
delay is known. Moreover, the communication delay at a link 
depends on how many communication loads share the link at the 
same time. So, we use the worst-case communication delay W(Eij) of 
each edge Eij to satisfy the hard real-time constraint. We can 
estimate the worst-case communication delay by assuming the link is 
shared by all the communication transactions assigned to the link at 
the same time and the effective available bandwidth for the edge is 
smaller. This is a conservative assumption as the communication 
transactions assigned to the same link may not overlap in time. To 
check whether two transactions overlap or not, we first sort the tasks 
and edges of the CDCG topologically. We then derive the ready time 
of the tasks and edges according to the topological order. If the 
ready time of an edge is earlier than the finish time of another edge 
that shares the same link, then we assume these two edges are 
Critical path CP1
T1
PE2
V1=1 .5V
V2=1 .5V
T2
PE3
T3
PE7
V3=1 .5V
V4=1 .5V
T4
PE4
Critical path CP 2
T5
PE15
T6
PE10
V1=1 .5V
T1
PE2
V2=1 .5V
T2
PE3
V6=1V
V5=1V
T5
PE15
T6
PE10
V(E6,3)=1V
T3
PE7
V3=1 .5V
V4=1 .5V
T4
PE4
(a) first critical path  (b) second critical path 
Figure 2. An example showing the ESVS algorithm 
130
 
 
 
overlapped. Otherwise they are not. If the edges do not overlap, the 
worst-case communication delay of an edge Eij is calculated from 
CE(Ei,j) and the maximum bandwidth of the link. Otherwise, the 
worst-case communication delay of an edge Eij is calculated from 
CE(Ei,j) and the effective available bandwidth which is proportional 
to the ratio of CE(Ei,j) over the sum of the CE of all the overlapped 
edges. Once the voltage islands are created and the supply voltage 
values are known, the fitness value of this solution is calculated 
according to equation (4). 
 We use the CDCG and the NoC system shown in Figure 1 as an 
example for illustration. We use the rectilinear-shaped voltage island 
partitioning and we set the maximum link speed as 100Mb/s. The 
worst-case execution time ET (in μs) for the computation task is 
shown in Figure 1a. The algorithm first identifies the first critical 
path CP1 as shown in Figure 2a. The total execution and 
communication time in CP1 is 950μs under maximum supply voltage 
(1.5V) and we have the scaling ratio SR1=950/950. The supply 
voltage for the tasks and edges in CP1 is 1.5V. Consequently, we can 
create the first voltage island VI1 as shown in Figure 2b with supply 
voltage 1.5V. In Figure 2b, we identify the next critical path CP2 
among all the remaining tasks, i.e. T5 and T6. Under the maximum 
speed schedule, the edge E63 should finish the data transmission at 
200μs and the start time of T3 at 1.5V is 400μs. This means that we 
can scale the supply voltage of the tasks and communication in CP2 
with the scaling ratio SR2 = 200/400. The supply voltage for the PEs 
that execute the tasks T5, T6 and links that is assigned for the edge 
E6,3 is equal to min(1.5V·SR2, 1V)=1V. Finally, we can create the 
second voltage island VI2 as shown in Figure 1(b). 
4. Experimental Results 
We carried out experiments using several real applications including 
video processing applications [12], industrial applications [10] and 
benchmarks applications from E3S benchmark suites [9]. For each 
benchmark, we mapped the tasks onto a 4x4 tile-based NoCs with 4 
voltage islands. The minimum and maximum value of the voltage 
supply levels are 1V and 1.5V, respectively, and the number of 
discrete voltage levels is 6. The genetic algorithm first initializes 
with 50 individuals and each loop runs for at least 1000 times and 
therefore we have examined as much as 2 million solutions. In the 
first experiment, we assumed a fixed pre-defined voltage island 
partition of which each island covers 2X2 cores and we consider 
only the dynamic energy consumption. We compared the energy 
consumption with a base-line design which uses random tile 
mapping, XY-routing and no voltage scaling. 100 random mapping 
was tried and the best solution was used as the base-line design. The 
results are shown in Table 1.  
In Table 1, the third column shows the normalized average energy 
consumption of the design that used random tile mapping, XY 
routing and voltage scaling. This shows the effectiveness of voltage 
scaling in reducing the energy consumption. The fourth column 
shows the normalized energy consumption of the design using 
random tile mapping, routing path allocation using genetic algorithm 
GA(RA) and voltage scaling. The fifth column shows the normalized 
energy consumption of the complete proposed design using genetic 
 Table 1. Results for real applications  
Application
Number of 
tasks
 CNC
MMS
Video Object 
Plane Decoder
Networking
Telecom
Auto_indust
Consumer
Average
64
40
16
13
30
24
12
Normalized overall energy consumption
Random
82%
96%
GA(RA)
75%
88%
GA(TM+RA)
58%
68%
89%
87%
73%
83%
91%
86%
83%
80%
68%
75%
86%
79%
67%
62%
56%
58%
68%
62%
BB+
70%
75%
74%
72%
67%
66%
75%
71%
131
 Table 2. Results for real applications with different topologies  
Application
Number of
tasks
 CNC
MMS
Video Object
Plane Decoder
Network ing
Telecom
Auto_indust
Consumer
Average
64
40
16
13
30
24
12
Normalized overall energy consumption of GA(TM+RA)
Predefined Square
58%
68%
Rectangle
52%
56%
Rectilinear
50%
53%
67%
62%
56%
58%
68%
62%
55%
53%
52%
51%
53%
53%
53%
52%
50%
49%
50%
51%
algorithm on both tile mapping and routing path allocation 
GA(TM+RA) as well as applying the voltage scaling. From Table 1, 
we also compared the proposed work with the branch-and-bound 
approach in [6] and we denote it as “BB”. Since BB does not 
consider voltage scaling, we improved BB by integrating the voltage 
assignment with it and we called this improved version as BB+. Our 
proposed method outperforms BB+ because BB+ only considers the 
dynamic energy consumption of the communication links and does 
not consider the dynamic energy consumption of the computational 
PEs. This results in putting the closely connected tasks together 
regardless of the characteristic of the computational tasks. In 
contrast, our approach considers both 
the effects of 
the 
computational and communication elements on 
the energy 
consumption.  
We also compared the energy reduction using different voltage 
island topologies and the results are shown in Table 2. We can see 
that 
the adaptive 
rectilinear-shaped voltage 
island 
topology 
outperforms the others because it gives the greatest flexibility in 
partitioning the voltage islands. Therefore, non-critical tasks can use 
a lower supply voltage and less energy consumption is obtained. The 
average run-time of our algorithm is about 10 minutes. 
5. "
The Case for Low-Power Photonic Networks on Chip.,"Packet-switched networks on chip (NoC) have been advocated as a natural communication mechanism among the processing cores in future chip multiprocessors (CMP). However, electronic NoCs do not directly address the power budget problem that limits the design of high-performance chips in nanometer technologies. We make the case for a hybrid approach to NoC design that combines a photonic transmission layer with an electronic control layer. A comparative power analysis with a fully-electronic NoC shows that large bandwidths can be exchanged at dramatically lower power consumption.","8.5
The Case for Low-Power Photonic Networks on Chip
Assaf Shacham
Columbia University
Dept. of Electrical Engineering
Keren Bergman
Columbia University
Dept. of Electrical Engineering
Luca P. Carloni
Columbia University
Dept. of Computer Science
assaf@ee.columbia.edu
bergman@ee.columbia.edu
luca@cs.columbia.edu
ABSTRACT
Packet-switched networks on chip (NoC) have been advocated as
a natural communication mechanism among the processing cores
in future chip multiprocessors (CMP). However, electronic NoCs
do not directly address the power budget problem that limits the
design of high-performance chips in nanometer technologies. We
make the case for a hybrid approach to NoC design that combines a photonic transmission layer with an electronic control
layer. A comparative power analysis with a ful ly-electronic NoC
shows that large bandwidths can be exchanged at dramatical ly
lower power consumption.
Categories and Subject Descriptors
C.1.2 [Processor Architectures]: Multiple Data Stream Architectures (Multiprocessors)—Interconnection architectures.
General Terms
Design, Performance.
Keywords
Network-on-Chip, Optical Communication.
1.
INTRODUCTION
The quest for both high performance and low power has
lead to a new emerging trend in high-performance microprocessors design with the arrival of the ﬁrst commercial
chips hosting multiple processing cores like the SUN Niagara, the IBM CELL, and the Intel Duo.
It is reasonable to expect that the number of these cores will continue to grow, leading to various generations of chip multiprocessors (CMP). Packet-switched micro-networks based
on regular scalable structures such as meshes or tori have
been proposed to implement on-chip global communication
in multicore processors [1, 2, 11]. These networks-on-chip
(NoC) are made of carefully-engineered links and represent
a shared medium that can provide enough bandwidth to
replace many traditional bus-based and/or point-to-point
links. A prototype chip with a NoC connecting 80 cores was
recently presented [15]. While NoCs potentially dissipate
less power than a set of equivalent point-to-point communication links [1, 2], a growing fraction of the on-chip power
dissipation is due to on-chip communications [6, 10, 15].
Leveraging the unique advantages of optical communication to construct photonic NoCs oﬀers a potentially disrupPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright2007ACM978-1-59593-627-1/07/0006...$5.00.
tive technology solution that can provide ultra-high throughput, minimal access latencies, and low power dissipation
that remains independent of capacity. An optical interconnection network that can capitalize on the capacity, transparency, and fundamentally low power consumption of silicon photonics could deliver performance-per-watt that is
simply not possible with all-electronic interconnects. Photonic channels can support large amounts of data traﬃc
across longer distances in a bandwidth-oriented design of
a network connecting processing cores and memories. Besides the power wal l, a photonic networks can address also
the memory wal l by allowing seamless delivery of oﬀ-chip
communication bandwidth with minimal additional power
consumption. Electronic technology can complement the
photonic network in overcoming some of the limitations inherent to photonics, namely processing and buﬀering.
The photonics opportunity is made possible now by recent
advances in nanoscale silicon photonics. High speed optical
modulators at data rates exceeding 12.5Gb/s [18] have been
reported. The integration of silicon photonic devices with
CMOS integrated circuits for chip-to-chip communication
recently became commercially available [4]. These remarkable achievements lead us to envision the integration of a
fully functional photonic NoC on a single die. In this paper
we present a novel architecture for a photonic NoC that uses
silicon photonic technologies to provide a low-power solution
for on-chip communication, and thus oﬀer unparalleled advantages in terms of performance-per-watt.
2. ARCHITECTURE OVERVIEW
Photonic technology oﬀers unique advantages in terms of
energy and bandwidth but lacks two necessary functions for
packet switching: buﬀering and processing are very diﬃcult to implement. Electronic NoCs, conversely, have many
advantages in ﬂexibility, abundant functionality and ample
buﬀering space, but their transmission bandwidth per line
is limited and their energy requirements are higher.
We propose a photonic NoC architecture that employs a
hybrid design, where an optical interconnection network is
used for bulk message transmission, and an electronic network is used for distributed control and short message exchanges. Both networks use the same 2-D planar topology
that maps well on CMP planar layout. Each core in the
CMP is equipped with a network interface, a gateway, whose
goal is to perform the necessary E/O and O/E conversions,
communicate with the control network and execute several
other related tasks like synchronization. Every photonic
message transmitted is preceded by an electronic control
packet (a path-setup packet) that is routed on the electronic
network, acquiring and setting-up a photonic path for the
message. Buﬀering of messages, which is impossible in the
photonic network, only takes place for the electronic pack132
Figure 1: Photonic switching element (PSE): (a) OFF
state: a passive waveguide crossover. (b) ON state: light
is coupled into rings and forced to turn.
ets during the path-setup phase. The photonic messages
are transmitted without buﬀering once the path has been
acquired. This approach has many similarities with optical
circuit switching, a technique used to establish long-lasting
connections between nodes in the optical Internet core [12].
The main advantage of using photonic paths relies on a
property of the photonic medium known as bit-rate transparency [12]: unlike routers based on CMOS technology that
must switch with every bit of transmitted data, leading to
a dynamic power dissipation that scales quadratically with
the bit rate [10], photonic switches switch on and oﬀ once
per message, and their energy dissipation does not depend
on the bit rate. This property facilitates the transmission
of very high bandwidth messages while avoiding the power
cost that is typically associated with them in traditional
electronic networks. Another attractive feature of optical
communications results from the low loss in optical waveguides: at the chip scale, the power dissipated on a photonic
link is completely independent of the transmission distance.
Energy dissipation remains essentially the same whether a
message travels between two cores that are 2mm or 2cm
apart. Furthermore, the employment of photonic hardware
for intrachip communication enables seamless integration of
optical interconnects for oﬀ-chip communications.
The rest of this section summarizes the main issues in the
architecture and the design of our hybrid photonic NoC. For
a more detailed presentation, the reader is referred to [14].
Building Blocks. The fundamental building block of
the photonic network is a broadband photonic switching element (PSE), based on a microring-resonator structure. The
switch is essentially a waveguide intersection, positioned between two ring resonators (Fig. 1). The rings have a certain
resonance frequency, derived from material and structural
properties. In the OFF state, when the resonant frequency
of the rings is diﬀerent from the wavelength (or wavelengths)
on which the optical data stream is modulated, the light
passes through the waveguide intersection uninterrupted, as
if it is a passive waveguide crossover (Fig. 1a). When the
switch is turned ON, by the injection of electrical current
into p-n contacts surrounding the rings, the resonance of
the rings shifts such that the transmitted light, now in resonance, is coupled into the rings making a right angle turn
(Fig. 1b), thus creating a switching action. PSEs and modulators based on the aforementioned eﬀect have been realized
in silicon and a switching time of 30ps has been experimentally demonstrated [18]. Their merit lies mainly in their extremely small footprint, approximately 12µm ring diameter,
and their low power consumption: less than 0.5mW , when
ON [18]. When the switches are OFF, they act as passive
devices and consume nearly no power. Ring-resonator based
switches exhibit good crosstalk properties (> 20dB ), and a
low insertion loss, approximately 1.5dB [17]. These switches
are typically narrow-band, but advanced research eﬀorts are
now undergoing to fabricate wideband structures capable of
Figure 2: In a 4×4 switch an electronic router controls
4 PSEs.
switching several wavelengths simultaneously, each modulated at tens of Gb/s. It is also reasonable to assume that
the loss ﬁgures can be improved with advances in fabrication
techniques. We use groups of four PSEs controlled by an
electronic router (ER) to form a 4×4 switch (Fig. 2). The
4×4 switches are interconnected by the inter-PSE waveguides, carrying the photonic data signals, and by metal lines
connecting the ERs. Control packets (e.g. path-setup) are
received in the electronic router, processed and sent to their
next hop, while the PSEs are switched ON and OFF accordingly. Once a path-setup packet completes its journey
through a sequence of electronic routers, a chain of PSEs
is ready to route the optical message. Owing to the small
footprint of the PSEs and the simplicity of the logic design
of the ER, which only handles small control packets, the
4×4 switch can have a very small area. We estimate it at
70µm × 70µm based on the size of the microring resonator
devices [18]. The 4×4 switch is internally blocking, a fact
that may aﬀect the NoC performance. Methods to address
this issues are discussed in [14].
Topology. We propose a photonic NoC for a chip multiprocessor (CMP) where a number of homogeneous processing cores are integrated as tiles on a single die. The communication requirements of such a system is best served by
a 2-D regular topology such as a mesh or a torus [11]. The
distributed control scheme of 2-D topologies also contribute
to improved scalability - an important property, considering
the expectations that the core-count in CMPs will continue
to increase in the near future. In this work we assume to
serve a 36-core system by a 6×6 2D mesh.
Network contention is a ma jor source of latency in the
path-setup procedure. The photonic NoC can be augmented
with additional paths so that the probability of contention is
lowered and the path-setup latency is reduced. Owing to the
small footprint of the switches, the simplicity of the routers,
and the fact that the PSEs only consume power when they
cause messages to turn, the power and area cost of adding
parallel paths is not large. Hence, path multiplicity can be
used as a cost-eﬀective method of improving performance
and reducing contention in the absence of traditional means
for contention resolution such as buﬀers.
Routing and Flow Control. Dimension order routing
is a simple routing algorithm for mesh and torus networks,
requiring minimal logic in the routers. We use XY dimension
order routing on the photonic NoC, with a slight modiﬁcation required to accommodate the injection/ejection rules of
the optical messages [14]. The ﬂow control technique in our
photonic NoC greatly diﬀers from common NoC ﬂow control methods due to the fundamental diﬀerences between
electronic and photonic technologies.
In particular memory elements (registers, SRAM...) cannot be used to buﬀer
messages or even to delay them while processing is done.
Electronic control packets are thus exchanged to acquire
photonic paths, and the data are only transmitted, with
133
very high bandwidth, once the path has been acquired. The
path-acquisition procedure requires the path-setup packet to
travel a number of electronic routers and undergo some processing in each hop. Contention may delay the packet, leading to a path-setup latency on the order of tens of nanoseconds. Once a path is acquired, the transmission latency of
the optical data is very short, depending only on the group
velocity of light in a silicon waveguide: approximately 6.6 ×
107 m/s, or 300ps for a 2 − cm path crossing a chip [7]. The
exchange of packets carrying small chunks of data can be
done on the electronic control network which is, in essence,
a low-bandwidth electronic NoC. These messages are not
expected to create congestion because of their small size,
Network Interfaces. Electronic/Optical (E/O) and Optical/Electronic (O/E) conversions are necessary in our photonic NoC, Each node includes a photonic network interface:
a gateway. Small footprint microring-resonator-based silicon
optical modulators with data rates up to 12.5Gb/s [18] as
well as SiGe photodetectors [5] have been reported recently
and become commercially available [4], to be used as photonic chip-to-chip interconnects. The laser sources can be
located oﬀ chip, externally coupled, as is typically the case
in oﬀ-chip optical communication systems [4]. The network
gateways also include the circuitry necessary for clock synchronization and recovery and serialization/deserialization.
Since electronic signals are fundamentally limited in their
bandwidth to a few GHz, larger data capacity is typically
provided by increasing the number of parallel wires. The
optical equivalent of this wire parallelism can be obtained
with a large number of simultaneously modulated wavelengths using wavelength division multiplexing (WDM) at
the gateways. The translating device, which can be implemented using microring resonator modulators, converts
directly between space-parallel electronics and wavelengthparallel photonics in a manner that conserves chip space as
the translator scales to very large data capacities [9]. Optical time division multiplexing (OTDM) can additionally be
used to multiplex the modulated data stream at each wavelength and achieve even higher transmission capacity [8].
The energy dissipated in these large parallel structures is
not small, but it is still smaller then the energy consumed
by the wide busses and buﬀers currently used in NoCs: the
E/O and O/E conversions in the gateway interfaces occur
once per node in the photonic NoC, compared to multiple
ports at each router in electronic equivalent NoCs [13].
3. HIGH LEVEL POWER ANALYSIS
The main motivation for the design of a photonic NoC
is the potential dramatic reduction in the power dissipated
on high-bandwidth intrachip communications. To evaluate
this power reduction we perform a comparative high-level
power analysis between two equivalent on-chip interconnection networks: a photonic NoC and a reference electronic
NoC. They are equivalent in the sense that must provide
the same bandwidth to the same number of cores. For our
case study, we assume a CMP with 36 processing cores,
each requiring a peak bandwidth of 800 Gb/s and an average bandwidth of 512Gb/s. These numbers match widely
accepted predictions on future on-chip bandwidth requirements in high-performance CMPs. We will see that in this
high-bandwidth realm, photonic technologies can oﬀer a dramatic reduction in the interconnect power. We assume a
uniform traﬃc model, a mesh topology and XY dimension
Clock Frequency [GHz]
ELINK [pJ/mm/bit]
EBU F F ER [pJ/bit]
ECROSSBAR [pJ/bit]
EST AT IC [pJ/bit]
65 nm 45 nm 32 nm
3.2
4
5
0.58
0.46
0.34
0.16
0.13
0.12
0.93
0.63
0.36
0.06
0.11
0.35
Table 1: Predictions for future technology nodes.
order routing. Of course, diﬀerent conditions can be used,
but as our gaol is to provide an equal comparison plane, this
choice provides a simple “apples-to-apples” comparison.
Electronic NoC. The reference electronic
network is a 6×6 mesh, where each router is integrated in
one processor tile and is connected to four tiles. A router
micro-architecture that has been widely proposed in the
NoC literature [2, 11]) is based on an input-queued crossbar
with a 4-ﬂit buﬀer on each input port. The router has ﬁve
I/O ports: one for the local processor and four for the network connections with the neighbour tiles (N, S, E, & W).
We estimate the power expended in an electronic NoC under a given load using the method developed by Eisley and
Peh in [3]: this assumes that whenever a ﬂit traverses a link
and the subsequent router, ﬁve operations are performed:
(1) reading from a buﬀer; (2) traversing the routers’ internal crossbar; (3) transmission across the inter-router link (4)
writing to a buﬀer in the subsequent router, and (5) triggering an arbitration decision. The energy required for a single
hop through a link and a router (EF LI T −HOP ) is the sum
of the energies spent in these operations.
Table 1 reports the values of the energy spent in these operations (buﬀer reading and writing energies are combined,
arbiter energy is neglected) that were obtained with the
Orion NoC simulator [16]. Orion account for the static
energy dissipated in the router and converts it to a per-bit
scale. EF LI T −HOP , the energy expended to transmit one ﬂit
across a link and a subsequent router, is computed based on
the energy estimates in Table 1 as well as the link length and
ﬂit-width which vary for diﬀerent technology nodes. The total energy expended in a clock cycle can be computed as
EN ET W ORK−CY CLE =
ULj · EF LI T −HOP
NLX
j=1
where ULj is the average number of ﬂits traversing link j per
clock cycle, an estimate on the utilization of link j . Then,
the power dissipated in the network is equal to
PN = EN ET W ORK−CY CLE · f
where f is the clock frequency. For the a 6×6 mesh under
uniform traﬃc using XY routing and an injection rate of
α = 0.625 the global average link utilization is ¯U = 0.75.
Hence, the energy expended in a clock cycle in the reference
electronic NoC (which has 120 links) is:
EN ET W ORK−CY CLE = 0.75 · 120 · EF LI T −HOP
and the total power dissipated is estimated as:
PE−N oC = EN ET W ORK−CY CLE · f
The results appear in Table 2. The main conclusion that
can be drawn from this analysis is that when a truly high
communication bandwidth is required for on-chip data exchange, even a dedicated, carefully designed NoC may not be
able to provide it within reasonable power constraints. Since
the electronic transmission is limited in bandwidth to a few
GHz at most, high transmission capacity require the use of
134
Flit width
Link length [mm]
EF LIT −HOP [pJ]
PE−N oC [W]
65 nm 45 nm 32 nm
256
208
168
3.33
2.33
1.67
788
406
235
227
146
106
Table 2: Power consumption of electronic NoC.
many parallel lines [2], which lead to high power dissipation
for transmission and buﬀering. Admittedly the above analysis is based on a simple circuit implementation, but even
if aggressive electronic circuit techniques such as low-swing
current mode signaling are employed, the overall NoC power
consumption that is necessary to meet the communication
bandwidth requirements in future CMPs will likely be too
high to manage within reasonable packaging constraints.
Photonic NoC. Since our photonic NoC is based on an
hybrid design (Sec 2), its power dissipation can be estimated
as the sum of three components: the photonic network, the
electronic control network, and the O/E E/O interfaces.
1. Transmission Network. Path multiplicity is a lowpower cost-eﬀective solution to compensate for the lack of
buﬀers in the photonic network. In this design we assume
a path multiplicity factor of 2, meaning a 12×12 photonic
mesh, comprised of 576 PSEs (144 4×4 switches), serves the
6×6 CMP. The power analysis of a photonic NoC is fundamentally diﬀerent from the electronic network analysis since
it mainly depends on the state of the PSEs: in the ON state,
when the message is forced to turn, the power dissipated is
less than 0.5mW [18], while in the OFF state, when a message proceeds undisturbed or when no message is forwarded,
there is no dissipation. Hence, the total power consumption in the network depends on the number of switches in
ON state, which can be estimated based on network statistics and traﬃc dynamics. We assume that in the photonic
NoC each message makes, at most, 4 turns. Assuming a
peak bandwidth of 960Gb/s and an injection rate of 0.6,
the average bandwidth is 576Gb/s. The average number of
messages in the network at any given time is calculated as
36 × 0.6 = 21.6. The average number of PSEs in the ON
state is about 86 in a 576-PSE NoC. Hence, the total power
consumption can be estimated as:
PP −N oC,transmission = 86 · 0.5mW = 43mW
dramatically lower than anything that can be approached
by an electronic NoC.
2. Control Network. The power analysis of the electronic
control network is based on the fact that this is essentially an
except for the larger dimensions (12×12 compared to 6×6).
electronic NoC, i,e, similar to our reference electronic NoC
We assume that each photonic message is accompanied by
two 32-bit control packets and the typical size of a message is
2 KBytes. Then, the total power consumed by the electronic
control network can be approximated as:
PP −N oC,control = PE−N oC · 2 ·
32
16384
· 2 = 0.82W
3. Network Interfaces. To generate the 960Gb/s peak
bandwidth we assume a modulation rate of 10Gb/s. The
modulated data streams are then grouped using ×12 OTDM
to ×8 WDM to form 960Gb/s messages. The OTDM and
WDM multiplexers are passive elements, so power is dissipated mainly in the 96 modulators and 96 receiver circuits
in each gateway. Since there is presently no equivalent to
the ITRS for the photonic technology, predictions on the
power consumption of photonic elements vary greatly. A
reasonable estimate for the energy dissipated by a modulator/detector pair, at 10Gb/s is 1.1pJ/bit today. We estimate that using silicon ring-resonator modulators and SiGe
detectors, the energy will decrease to about 0.11pJ/bit in
8-10 years. Consequently, the total power dissipated by 36
interfaces under the conditions described above is:
PP −N oC,gateways = 0.11pJ/bit × 36 × 576Gb/s = 2.3W
Hence, the estimated power consumed by the photonic NoC
to exchange data between 36 cores at an average bandwidth
of 576Gb/s is the sum of the three components: 3.2W.
Concluding Remarks. Although the power analysis
used here is rather simplistic and uses many assumptions
to ease the calculation and work around missing data, its
broader conclusion is unmistakable. The potential power
diﬀerence between photonics-based NoCs and their electronic
counterparts is immense. Even when one accounts for inaccuracies in our analysis and considers predicted future trends
the advantages oﬀered by photonics represent a clear leap
in terms of bandwidth-per-watt performance.
Acknowledgments
AS and KB acknowledge the support of the NSF under Grant CCF0523771 and the U.S. Dept. of Defense under subcontract B-12-664.
LC acknowledges the support of the NSF under Grant No. 0541278.
4. "
Interconnect and Communication Synthesis for Distributed Register-File Microarchitecture.,"Distributed register-file microarchitecture (DRFM), which comprises multiple uniform blocks (called islands), each containing a dedicated register file, functional unit(s) and data-routing logic, has been known as a very attractive architecture for implementing designs with platform-featured on-chip memory or register-file IP blocks. In comparison with the discrete-register-based architecture, DRFM offers an opportunity of reducing the cost of global (inter-island) connections by confining as many of the computations to the inside of the islands as possible. Consequently, for DRFM architecture, two important problems to be solved effectively in high-level synthesis are: (problem 1) scheduling and resource binding for minimising inter-island connections (IICs) and (problem 2) data transfer (i.e. communication) scheduling through the IICs for minimising access conflicts among data transfers. By solving problem 1, the design complexity because of the long interconnect delay is minimised, whereas by solving problem 2, the additional latency required to resolve the register-file access conflicts among the inter-island data transfers is minimised. This work proposes novel solutions to the two problems. Specifically, for problem 1, previous work solves it in two separate steps: (i) scheduling and (ii) then determining the IICs by resource binding to islands. However, in this algorithm called DFRM-int, the authors place primary importance on the cost of interconnections. Consequently, the authors minimise the cost of interconnections first to fully exploit the effects of scheduling on interconnects and then to schedule the operations later. For problem 2, previous work tries to solve the access conflicts by forwarding data directly to the destination island. However, in this algorithm called DFRM-com, the authors devise an efficient technique of exploring an extensive design space of data forwarding indirectly as well as directly to find a near-optimal solution. By applying this proposed synthesis approach DFRM-int + DFRM-com, the authors are able to further reduce the IICs by 17.9%, compared with that by the conventional DRFM approach, even completely eliminating register-file access conflicts without any increase of latency.","Interconnect and Communication Synthesis for Distributed
Register-File Microarchitecture
43.1
Kyoung-Hwan Lim
School of Electrical
Engineering and Computer
Science
Seoul National University,
Korea
khlim@ssl.snu.ac.kr
YongHwan Kim
School of Electrical
Engineering and Computer
Science
Seoul National University,
Korea
oceanic@ssl.snu.ac.kr
Taewhan Kim
School of Electrical
Engineering and Computer
Science
Seoul National University,
Korea
tkim@ssl.snu.ac.kr
ABSTRACT
Distributed register-ﬁle microarchitecture (DRFM) which comprises multiple uniform blocks (called islands), each containing a dedicated register ﬁle, functional unit(s) and datarouting logic, has been known as a very attractive architecture for implementing designs with platform-featured onchip memory or register-ﬁle IP blocks. In comparison with
the discrete-register based architecture, DRFM oﬀers a higher
degree of opportunity of reducing the cost of global (interisland) connections by conﬁning as many the computations
to the inside of the islands as possible. Consequently, for
DRFM architecture, two important problems to be solved
eﬀectively in high-level synthesis are: (problem 1) scheduling and resource binding for minimizing inter-island connections, and (problem 2) data transfer (i.e., communication)
scheduling through the inter-island connections for minimizing the access conﬂicts among the data transfers. This work
proposes novel solutions to the two problems. Speciﬁcally,
for problem 1 previous work solves it in two separate steps:
(i) scheduling and (ii) then determining the inter-island connections by resource binding to islands. However, in our algorithm called DFRM-int, we place primary importance on
the cost of interconnections. Consequently, we minimize the
cost of interconnections ﬁrst to ful ly exploit the eﬀects of
scheduling on interconnect and then to schedule the operations later. For problem 2, previous work tries to solve the
access conﬂicts by forwarding data directly to the destination island. However, in our algorithm called DFRM-com, we
devise an eﬃcient technique of exploring an extensive design
space of data forwarding indirectly as wel l as directly to ﬁnd
a near-optimal solution. By applying our proposed synthesis
approach DFRM-int+DFRM-com we are able to reduce the
inter-island connections by 18.1% more, compared to that
by the DRFM approach in [4], even completely eliminating
register-ﬁle access conﬂicts, which could never been resolved
by [4], without any latency increase.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA
Copyright 2007 ACM  978-1-59593-627-1/07/0006 ...$5.00.
Categories and Subject Descriptors
B.5.2 [Register-transfer-level Implementation]: Design
Aids—automatic synthesis
General Terms
Algorithms, Design
Keywords
Synthesis, communication, distributed register ﬁle
1.
INTRODUCTION
By the increasing development of integrated circuit technology, interconnects have become one of the most critical
factors that signiﬁcantly aﬀect the quality of the ﬁnal designs.
In particular, the long interconnects cause serious
eﬀects on circuit timing, power, and area.
It is reported
[13] that interconnects dissipate over the half of the total
dynamic power of the microprocessor in a 0.13um technology, and predicts that interconnects may consume up to 80%
of the dynamic power in future technologies. Furthermore,
several reports [15, 10] for FPGAs show that interconnects
occupy over 70% of the total area and around 80% of the
total power.
Recently, a distributed register-ﬁle based microarchitecture (DRFM) [4], which provides a platform consisting of
uniform blocks (called island), each having its own registerﬁle, functional resource(s), and data-routing logic, has been
attracted because it allows a high degree of localizing the
execution of register-transfer operations to the inside of the
islands, thus eliminating unnecessarily long data transfers.
In fact, many of high capacity FPGA devices such as Xilinx Virtex II [2] and Altera Stratix [1] contain register-ﬁle
IP blocks that are abundantly distributed on the chip for
storing data produced by their surrounding functional units.
Such a use of register-ﬁle not only improves the performance
but also reduces the area of multiplexing logic [4].
There are extensive research works on interconnect-aware
or interconnect-driven high-level synthesis [16, 6, 12, 11, 9, 7,
8, 3, 4]. Tarafdar [16] incorporated layout into the scheduling and binding of data transfers in high-level synthesis. The
binding process is based on a branch-and-bound heuristic.
Kim et al. [8] performed operation binding, placement, and
scheduling sequentially. The placement was driven by the
inter-clock slack time information obtained from the binding
results. Furthermore, Um and Kim [18] heuristically solved
765
the integrated problem of resource binding and layout by
using a linear programming formulation. Jang and Pangrle [6] proposed a layout estimation technique for binding,
and used it for selecting the most eﬀective binding. However, all the works assumed a general structure of architecture in which registers and functional units are freely
placed in any locations on the chip. The authors in [12]
synthesized interconnect (by data transfer scheduling) with
diﬀerent ob jective: minimizing the crosstalk eﬀect by exploiting data transfer scheduling. Lee and Hwang [11] proposed scheduling algorithms for minimizing the numbers of
memories and/or ports. On the other hand, the authors in
[9] proposed an interconnect minimization techniques during
variable binding, after the completion of operation binding,
to multi-port memories. The limitation of the two works in
[11, 9] is that they did not take into account resource binding on the minimization of interconnects. The work in [7, 8]
proposed a distributed-register architecture, in which individual registers are distributed so that each functional unit
can use its local dedicated register for storing the local data
values. Data transfers between diﬀerent functional units are
considered as global communications that may take multiple clock cycles.
In addition, the authors in [3] proposed
a regularly distributed register microarchitecture, which is
an improved architecture of that by [7, 8], and proposed a
synthesis method under the possibility of multicycle communication. However, these microarchitectures do not use
register ﬁles, which means that they did not use the onchip embedded memories for register-ﬁle implementation.
Recently, the authors in [4] proposed a new interconnect
synthesis algorithm for distributed register-ﬁle microarchitecture (DRFM). The input to the algorithm is a ‘scheduled’
dataﬂow graph (DFG). Thus, the eﬀectiveness of the algorithm is limited to the exploitation of resource binding only.
In addition, [4] proposed a data-forward scheduling algorithm to resolve the access conﬂicts of data transfers. However, the possible routes of data-forwarding are conﬁned to
the source and destination islands only, and never took into
account the possibility of data-forwarding through other intermediate islands indirectly.
In utilizing DRFM in high-level synthesis, two important problems are interconnect minimization problem, which
means to the scheduling and resource binding for minimum
inter-island connections, and communication optimization
problem, which means to the scheduling of data transfers
thru the inter-island connections for minimum access conﬂicts among the data transfers. This paper addresses these
two problems.
2. PRELIMINARIES AND OVERVIEW
Fig. 1(a) shows an architecture of computation islands of
DRFM [4]. It consists of three main components: a dedicated
register ﬁle (DRF), a functional unit pool (FUP), and input
buﬀering logic (IBL). The DRF with w-write ports and rread ports is used to store the data produced by the internal
FUP. On the other hand, the IBL is used to store the data
produced by the external FUPs. The inputs to FUP are
given from both the internal DRF and IBL, selected by the
multiplexing logic as shown in Fig. 1(a). In DRFM architecture, we assume that a FUP contains a single functional
unit, and a DRF is restricted to w=1.1
1The DRFM assumption corresponds to the most common
Island B
Island B
Input buffering 
logic (IBL)
Dedicated
register
file
(DRF)
MUX
Functional unit pool
(FUP)
ALU
MUL
Initial Partitioing with Feasible Schedule
step 1
Subproblem 1
Partitioning: Find Minimum IIC
Scheduling: Satisfy L
step 2
step 3
Subproblem 2
Commumication Scheduling
(Resolve read access conflicts)
step 4
(a) The internal structure of an island in a DRFM  
(b) Proposed  flow of our DRFM synthesis approach
Figure 1: (a) The structure of an island in DRFM
and (b) our proposed DRFM synthesis ﬂow.
The inputs to DRFM synthesis algorithm are an unscheduled dataﬂow graph (DFG), a DRFM, and latency L. Then,
the problem to solve is to schedule the operations in DFG
and bind them to the FUPs of DRFM with an ob jective
of minimizing the number of inter-island connections (IICs)
while satisfying the latency constraint.2 We tackle the problem by solving two subproblems shown in the design ﬂow
of Fig. 1(b). (Subproblem 1) The ﬁrst one is to minimize
the IIC number by binding operations to islands (i.e, their
FUPs) and scheduling their executions assuming there is
no register-ﬁle access conﬂict (i.e., r=∞); (Subproblem 2)
The second is, from the solution of the ﬁrst subproblem,
to resolve the access conﬂicts. We solve subproblem 1 by
formulating it into a partitioning problem combined with a
‘partial’ scheduling (in Step 2) and a subsequent scheduling
problem (in Step 3), and solve subproblem 2 by formulating
it into a max-ﬂow min-cost network ﬂow problem (in Step
4). In the following sections, we provide the details on the
steps.
3. THE PROPOSED DRFM SYNTHESIS ALGORITHM
3.1 The Interconnect Minimization Technique:
DRFM-int
Two ma jor factors that signiﬁcantly aﬀect the IIC number
are scheduling of operations and resource (island) binding.
In addition, the resource binding is directly determined by
the result of partitioning of operations to groups where each
group corresponds to an island in DRFM. Consequently, our
approach tries to partition and schedule operations in a way
to minimize the number of IICs.
In our approach, unlike
to the conventional method, which starts from a scheduled
DFG, we accept an unscheduled DFG and perform partitioning ﬁrst in an attempt to fully exploit the eﬀects of scheduling during partitioning on the minimization of IIC number.
Subsequently, we perform detailed scheduling based on the
annotated scheduling information obtained during the partitioning step. The following example gives a clean motivation
conﬁguration, but our approach can be easily extended to
general conﬁgurations.
2As veriﬁed from the experiments in [4], the minimization
of the inter-island connections is the right optimization goal
for the resource binding on DRFM.
766
of our approach.
Motivational example 1: Fig. 2 shows, for an input DFG, the
diﬀerence of the results produced by two synthesis ﬂows:
a conventional ﬂow in Fig. 2(a) and our proposed ﬂow in
Fig. 2(b). As shown in the comparison, the partitioning of
operations with a high ﬂexibility of scheduling in Fig. 2(b)
leads to a greater chance of ﬁnding solutions with less number of IICs than that by the IIC-unaware scheduling followed
by partitioning in Fig. 2(a). This is because the scheduling
done before partitioning limits the design space exploration
in partitioning for IIC minimization. Here, our key concern,
which will be discussed later, is how to (partially or fully)
predict possible schedules quickly during the partitioning
step, so that the latency constraint should be safely met in
the subsequent scheduling step.
1
4
7
10
8
2
3
5
6
9
1
4
7
10
8
2
3
5
6
9
1
2
3
4
4
7
10
8
2
3
5
6
9
1
2
3
4
1
Ia
Ib
Ic
IIC = 4
I d
1
4
7
10
8
2
3
5
6
9
1
4
10
3
8
2
7
5
6
9
4
7
3
6
9
1
2
3
4
1
I a
I b
Ic
IIC = 2
Id
5
10
8
2
scheduling
partitioning
scheduling
(a)
(b)
partitioning
Figure 2: An example showing the diﬀerence between our ﬂow and conventional synthesis ﬂow.
Our proposed technique, DRFM-int, performs the following three steps:
Initial Feasible Partitioning (Step 1 in Fig. 1(b)): In this step,
we generate a partitioned solution that guarantees to generate a schedule whose latency is less than or equal to the
latency limit L.3 A simple partitioning algorithm is to group
the operations in a critical path of DFG, and assign them to
an island. DFG is then updated by removing the assigned
operations. The process is then repeated for the updated
DFG. By doing this process the number of islands will be
equal to the size of the largest anti-chain in the initial DFG,
and the schedule length is equal to the length of the critical
path of the initial DFG.
Scheduling-aware Partitioning (Step 2 in Fig. 1(b)): Our partitioning algorithm is based on the framework of the Fiduccia and Matteyses (FM) partitioning algorithm [5]. To test
the scheduling feasibility for a partition, we employ an indexing mechanism which assigns a pair of numbers [si , sj ] to
each operations. The values of si and sj for an operation ok
are inherently used as lower and upper bounds of clock steps
at which ok can be scheduled under the resource (functional
unit) constraint of islands, respectively. Note that [si , sj ] is
much tighter than the clock step range by the ASAP and
ALAP schedulings because they do not take into account
the resource constraint at all.
Given a DFG, we include two special dummy nodes: source
node a and destination node b. We create an arc from a
to each node (i.e., operation in DFG) with no descendents,
3We call such partitioning a feasible partitioning.
and create an arc from each node with no successors to b, as
shown in Fig. 3(a). The procedure of computing the value
of si in [si , sj ] of each node in DFG is as follows. The si
logical order. First, [si , −] of source node a is set to [0, −],
values are computed from top to bottom of DFG in topoand [si , −] of all arcs outgoing from a are also assigned with
[0, −]. Suppose we now are in a stage to compute [si , −] of
a node in which the [si , −] values of all arcs incoming to the
node have already been determined.
1
3
5
7
2
4
[2   ]
[2   ]
9
6
8
a
b
[0   ]
[0   ]
[0   ]
[1   ]
[1   ]
[1   ]
[1   ]
[1   ]
[2   ]
[2   ]
[2   ]
[2   ]
[3   ]
[3   ]
[4 ]
[4   ]
[4   ]
[4   ]
[5   ]
[6   ]
[5   ]
[5   ]
[6   ]
1
3
5
7
2
4
9
6
8
a
b
[0 0]
[1 1]
[1 1]
[2 2]
[2 3]
[4 4]
[3 3]
[4 4]
[5 5]
[5 5]
[6 6]
1
3
5
7
2
4
9
6
8
a
a
a
b
[0 0]
[1 1]
[1 1]
[2 2]
[2 3]
[3 4]
[3 3]
[4 4]
[4 5]
[5 5]
[6 6]
1
3
5
7
2
4
9
6
8
[0 0]
[1 1]
[1 1]
[2 2]
[2 3]
[4 4]
[3 3]
[4 4]
[5 5]
[5 5]
[6 6]
Earliest index 
propagation
Latest index
propagation
Not changed. Stop
propagation.
Ok
1
1 1
3
Ok
3(m)
3(m)
3(m)
3
Ok
v 4
3
3
Ok
4(m)
4(m)
Final index of O k = 4 + 1 = 5
Latency constraint = 5
Latency constraint = 5
Moves
v1
 IB
v3
 IB
v5
 I
v7
 IB
v9
 IB
starting_idx ending_idx
[0 −1]
[7 6]
[0 0]
[6 6]
[0 0]
[6 6]
[0 0]
[6 6]
[0 0]
[6 6]
test
Not passed.
Passed.
Passed.
Passed.
Passed.
IIC
−
2
5
5
4
(b) Index merging example
(a) Calculation of earliest index
(c) Final indexes
b
(d) Initial partition
(f) Index propagation after moving op3 to island B
(e) The costs of the moves of nodes in (d) 
Selected
B
Figure 3: Example of scheduling annotation for a
partition.
The computation of si is derived from the corresponding si values of the incoming arcs internal to the island to
which the node is assigned. For example, suppose there are
four internal incoming arcs of node ok as shown in the leftmost ﬁgure of Fig. 3(b) where there are three arcs with si=1
and one arc with si=3.4 The three arcs with si=1, all combined together, imply that at least three clock steps starting
from clock step 1 are required before ok is executed. Consequently, value 3 assigned to the arcs, represents the earliest
(possible) clock step to be ﬁnished just before ok is to be executed, and ‘m’ in ‘3(m)’ is used to mean ‘merged’ arc. From
4We assume here for simplicity that each node is executed
exactly in one clock step.
767
this merged arcs shown in the third ﬁgure of Fig. 3(b), the
arc merging process is applied again to the two arcs with
si=3, resulting in the rightmost ﬁgure of Fig. 3(b). Formally, if there are l arcs with the same si values, the new
si of the merged arc is computed as: new si = l + si − 1.
The si of ok then becomes new si + 1. Note that for the
arc coming from a diﬀerent island, the merging process does
not happen because the corresponding node’s resource constraint has nothing to do with the si value of the external
arc. Fig. 3(a) shows all the computed [si , −] values of nodes
and arcs. The computation of [−, sj ] values can be done
reversely starting from the destination node b. Fig. 3(c)
shows the complete indexing of nodes. For example, node
o4 with [2, 3] means that it can be scheduled at a clock step
in between clock steps 2 and 3. For a trial of moving a
node from one partition to another, we compute the node
indexing [si , sj ] in O(n) where n is the number of nodes
in DFG. For example, suppose node o3 in Fig. 3(d) is attempted to be moved to other partition. Then, the [si , sj ]
indexes are updated forward and backward to reﬂect the inclusion/exclusion of o3 . Fig. 3(f ) shows the update of [si , sj ]
values. When the latency L is constrained to 5, the possible
moves of nodes with the indexes of source and destination
are summarized in the table of Fig. 3(e) where negative sj
value in the source and si value greater than 6 (=L+1) in
the destination are regarded as node moves that are not
likely to generate feasible schedules. From the list of feasible moves, we select the move that has the least number
of IICs, as indicated in the table, and perform the corresponding movement. The moved node is then locked and
the process is repeated until all the nodes are locked. Then,
the best partition is taken among the partition instances obtained during the iterations, and the process is performed
again by unlocking all the nodes. The process stops when
there is no further reduction of IIC number.
Scheduling (Step 3 in Fig. 1(b)): This step consists of two
sequential substeps: (i) ﬁnding a schedule satisfying latency
constraint and (ii) further reducing IIC number. Scheduling
according to the quantity of sj − si (i.e., the index range)
of nodes in DFG is straightforward. The nodes are sorted
in non-decreasing order. The nodes are then assigned to
clock steps in a brach-and-bound manner. Once a feasible
schedule is found, the process is stopped. Then, we perform
a local search-based iterative improvement by rescheduling
similar to the ‘vertical reﬁnement’ method in [4].
In this
stage, the schedule of operations at each partition can be
updated to reduce the IIC number further by maximizing
the sharing of IICs among the inter-island data transfers.
3.2 The Communication Optimization Technique: DRFM-com
Even though Step 3 of our algorithm generates a feasible
schedule, it does not always mean to generate a feasible one
when the read-port constraint is imposed. For example, if
the DRF in an island has 2-read port and Step 3 generates a
schedule in which three islands are to be executed in parallel
using data in another island I as inputs. In that cases, I
suﬀers the read access conﬂict, and thus requires one more
clock step to resolve the conﬂict. The ob jective of Step 4
(called DRFM-com) is to minimize the increase of additional
clock steps required to resolve the conﬂicts.
Motivational example 2: Fig. 4(a) shows an example of a
section of partitioned and scheduled DFG, in which four
islands and ﬁve clock steps are used. Note that operations
N6 , N7 , and N8 in islands B, C, and D are required to access
data from the register ﬁle in island A at clock step c5. In case
there are two read ports in the register ﬁle, it is not possible
to simultaneously access the register ﬁle from islands A, B,
and C. To resolve the conﬂict, a data-forwarding scheme
proposed in [4] can be applied. The scheme is to prefetch
data ‘directly’ to the destination island if there is an idle
clock step. However, for the example in Fig. 4 we cannot
ﬁnd any idle clock steps. Thus, the last choice is to insert one
more clock step in between c4 and c5, and apply the dataforwarding scheme. This results in IIC = 4 and 6 clock steps.
On the other hand, our DRFM-com applies a more general
data-forwarding scheme which checks all possible ‘indirect’
as well as ‘direct’ data-forwarding paths, and ﬁnds the most
economical one, if exists, in terms of IIC number. For the
example in Fig. 4(b), we can see the data forwarding path
which starts from N1 , goes to F2 in island D and ﬁnally goes
to N7 to the destination island C. Note that operation F2
allows to take the external data to input buﬀer and store it to
register ﬁle bypassing FUP. The selection of data-forwarding
path in Fig. 4(b) then makes N6 and N8 access the register
ﬁle in island A, and at the same time N7 access the register
ﬁle in island D, generating no access conﬂict. Consequently,
this results in 5 clock steps and IIC = 3 (IIC from A to C
path A→D→C).
will not be created due to the existing IICs in the routing
C1
N1
C2
N2
C3
N3
C4
N4
C1
N1
C2
N2
F1
F2
C3
N3
C4
N4
F3
C5
N5
N6
N7
N8
C5
N5
N6
N7
N8
Island A Island B
Island C
Island D
IslandA
Island B
Island C
Island D
Data flow causing read port conflict
Normal data flow
Data routing path, resolving the conflict
Normal data flow
Read port conflict at C5 cannot be resolved by [1]. 
IIC = 4
(a)
Read port conflict at C5 is resolved by DRFM−com 
IIC = 3
(b)
Figure 4: Access conﬂict and an extended dataforwarding scheme.
Network ﬂow based data routing (Step 4 in Fig. 1(b)): We formulate the problem of ﬁnding the minimum cost (IIC) data
routing path into a network ﬂow problem, and solve it optimally. We illustrate the process of network construction
using an example. From the initially partitioned and scheduled DFG in Fig. 4(a), we identify two groups of operation
nodes for the read access conﬂict: starting nodes and ending nodes In the example, the starting nodes are N1 , N3
and N4 and the ending nodes are N6 , N7 and N8 . Suppose
the number of starting nodes is k. Then, we create k separate networks, one for each pair of a starting and its ending.
For each network of pair (Ni , Nj ), we creat dummy nodes
(called ‘free’ node as denoted by ‘F-’ in Fig. 5(a)) for every
768
idle clock step in between the clock steps of Ni and Nj , and
add an arc from starting node Ni to every free node and
an arc from every free node to ending node Nj . Finally, we
crete a special node called super-starting node, as indicated
by double circle in Fig. 5(b), and create an arc from the
arc. Cost of zero is assigned to arc a → b if there exists
super-starting to the starting node. We assign cost to each
an inter-island connections from the island of a to the island of b, and cost of one is assigned otherwise. In addition,
cost of zero is assigned to the arc from the super-starting
to the starting node. The capacity of each arc is set to 1.
For example, Fig. 5(b) shows the networks of (N1 , N7 ) and
(N3 , N8 ) constructed for the conﬂict in Fig. 5(a). The remaining one with (N4 , N6 ) is not shown here because it does
not contain any free nodes. Then, for each network Gi we
ﬁnd a solution of maximum ﬂow of minimum cost eﬃciently
by applying the minimum cost path augmentation method
to Gi [17]. Note that the capacity of 1 and the introduction
of the super-starting allows the quantity of max-ﬂow to be
constrained to 1.
C1
N1
C2
N2
F1
F2
C3
N3
C4
N4
F3
C5
N5
N6
N7
N8
S
N1
c = 0
c = 0
c = 1
c = 0
F1
c = 1
c = 1
N7
F3
c = 1
Network of (N1,N7)
S
c = 0
N3
c = 0
F2
c = 0
Extra_IIC = 0
Island A Island B Island C Island D
F3
Extra_IIC = 1
Possible data routing arcs from N1 to N7
Possible data routing arcs from N3 to N8
Network of (N3,N8)
N8
c = 1
(a)
(b)
Figure 5: Network ﬂow based data-forwarding formulation.
The paths with heavy lines in Fig. 5(b) indicate the solutions
of ﬂow paths. Since the cost of the ﬂow path represents the
extra number of IICs required for the implementation of
the corresponding data routing, among the ﬂow paths we
select the one with the least cost. Consequently, the data
routing from island A to island D at clock step c2 (i.e.,
N1 → F2 ) and then from island D to island C at clock step
c5 (i.e. F2 → N7 ) will be performed, causing the other two
read accesses to island A to be performed at clock step c5.
Note that if there is still a read-port conﬂict, we repeat this
process for the updated network by assuming that the free
nodes on the chosen ﬂow path are now non-free operations.
4. EXPERIMENTAL RESULTS
We have implemented our proposed techniques DRFM-int
and DRFM-com in C++, ran on a PC equipped with 2GHz
AMD Athlon processor, and tested them on a set of Mediabench designs to assess how much the proposed techniques
are eﬀective. We evaluate our techniques in three-fold: (i)
checking the eﬀectiveness of DRFM-int on minimizing global
connections by fully exploiting the eﬀect of scheduling during partitioning (i.e., island assignment) (ii) checking the
eﬀectiveness of DRFM-com on resolving register ﬁle access
conﬂicts by the comprehensive (i.e., considering both direct
and indirect) data-forward scheduling, and (iii) checking the
combined eﬀectiveness of DRFM-int followed by DRFM-com.
• Assessing the eﬀectiveness of DRFM-int on minimizing interconnects: The tested designs, shown in the
ﬁrst column of Table 1, are DCT, DEM 3D blocks, MPEG,
Adpcm, JM 10.2, and AR ﬁlter. L in the second column
in Table 1 represents the latency constraint to be satisﬁed.
We assumed that each island of DRFM architecture has a
register ﬁle with 2-read and 1-write ports. The third and
fourth columns in Table 1 summarize the IIC numbers produced by the conventional approach FDS[14]+RB[4], which
uses the well-known force-directed scheduling [14] followed
by the resource (island) binding (i.e., partitioning operations) in [4], and our DRFM-int.
Table 1: Comparisons of the number of
interisland connections(IICs) for the design produced
by the Force-Directed scheduling[14] followed by
the partitioning (i.e., resource binding (RB) in [4]
(FDS[14]+RB[4] in Table 1) and our DRFM-int.
Benchmarks
(#ops)
DCT chen (87)
3D force1(53)
3D force2 (36)
MPEG calcid (27)
Adpcm dec (15)
JM itrans (58)
JM getb (32)
4AR (28)
Average
L
14
15
11
8
6
9
10
8
# of IICs
FDS[14]+RB[4]
DRFM-int
21
12
9
9
3
30
8
8
19
10
8
6
2
24
7
6
red.
9.5%
16.7%
11.1%
33.3%
33.3%
20.0%
12.5%
25.0%
20.2%
The comparision of the IIC numbers in Table 1 indicates
that the exploitation of scheduling by DRFM-int leads to a
signiﬁcant reduction of global connections, on average by
20.2%.
• Assessing the eﬀectiveness of DRFM-com on minimizing the increase of latency: Table 2 shows the numbers of read port conﬂicts that cannot be resolved by the
data-forwarding (DF) in [4] and DRFM-com for the designs
produced by the application of FDS[14] and RB[4]. As indicated in the columns marked “#cft” (representing the number of unresolved conﬂicts) in Table 2, DF[4] was not able to
resolve two conﬂicts in each of JM itrans and JM getb,
but DRFM-com resolved them completely. Furthermore,
DRFM-com even reduced the IIC number from 8 to 7 for
JM getb. Fig. 6 shows the results for JM itrans in Table 1.
Table 2: Comparisons of the number of unresolved conﬂicts for the designs produced by dataforwarding (DF) in [4] and our DRFM-com for the
designs by FDS[14] followed by the partitioning (i.e.,
resource binding (RB)) in [4].
Benchmarks
DCT chen
3D force1
3D force2
MPEG calcid
Adpcm dec
JM itrans
JM getb
4AR
FDS[14]+RB[4]
#IIC
#cft
21
0
12
0
9
0
9
0
3
0
30
2
8
2
8
0
DF[4]
DRFM-com
#IIC #cft #IIC #cft
30
2
31
0
8
2
7
0
769
Scheduled operation
Free nodes
F
Data flows causing read port conflicts
New data flows resolving the conflics
C2
C3
C4
C5
C6
C7
C8
1
13
F
14
F
3
11
12
27
42
18
1
13
F
14
3
11
12
27
42
18
F
F
F
F
F
F
F
F
F
F
F
F
F
F
26
28
45
25
26
28
45
25
(a) Unresolved conflicts by [1]
(b) Resolving the conflicts in (a) 
        by DRFM−com
Figure 6: A section of read-port conﬂicts unresolved
by the data-forwarding method DF in[4] and the
resolved data-forward scheduling produced by our
DRFM-com for design JM itrans.
• Assessing the eﬀectiveness of DRFM-int+DRFM-com
on minimizing interconnects and latency: Finally, Table 3 summarizes the numbers of IICs and unresolved conﬂicts for the conventional synthesis ﬂow FDS[14]+RB[4]+DF[4]
and our proposed synthesis ﬂow DRFM-int+DRFM-com. Clearly,
our approach outperforms the conventional approach in minimizing both the number of IICs (18.1% reduction on average) and the number of unresolved conﬂicts (actually resolving all conﬂicts without adding extra clock steps). The
amount of the IIC diﬀerence i.e., 2.1% (= 20.2% - 18.1%)
between Table 1 and Table 3 can interpreted as a rough
amount of extra IICs required for resolving the read-port
conﬂicts by DRFM-com.
Table 3: Comparisons of the numbers of IICs and
unresolved conﬂicts for the designs produced by
FDS (force-directed scheduling) followed by RB (resource binding) and DF (data-forwarding) in [4], and
our DRFM-int (binding and scheduling) and DRFMcom(data-forwarding) .
Benchmarks
DCT chen
3D force1
3D force2
MPEG calcid
Adpcm dec
JM itrans
JM getb
4AR
Average
FDS[14]+RB+DF[4]
#IIC
#cft
21
0
12
0
9
0
9
0
3
0
30
2
8
2
8
0
DRFM-(int+com)
#IIC
#cft
20
0
10
0
8
0
6
0
2
0
28
0
7
0
6
0
red.
4.8%
16.7%
11.2%
33.3%
33.3%
7.7%
12.5%
25.0%
18.1%
5. CONCLUSIONS
The distributed register-ﬁle microarchitecture entails potential advantage of using the distributed embedded memory blocks in current FPGAs. This work proposed a set
of novel solutions to two important problems in high-level
synthesis, that is, (problem 1) scheduling and resource binding for minimizing inter-island connections and (problem 2)
data-forward scheduling for minimizing the register ﬁle access conﬂicts. For solving problem 1 we placed primary importance on the cost of interconnections. Consequently, we
minimized the cost of interconnections ﬁrst to fully exploit
the eﬀects of scheduling on interconnect and then scheduled
the operations later. For solving problem 2, we devised an
eﬃcient technique for exploring extensive search space of
data forwarding indirectly as well as directly to ﬁnd a nearoptimal solution. By applying our proposed synthesis approach we were able to reduce the inter-island connections
by 18.1% further than that of the conventional approach
based on [4], even completely eliminating register ﬁle access
conﬂicts without any latency overhead.
6. ACKNOWLEDGEMENT
This work has been supported by Nano IP/SoC Promotion Group of Seoul R&BD Program in 2007, by the Ministry of Science and Technology / Korea Sceince and Engineering Foundation through the Advanced Information Technology Research Center (AITrc), and by ETRI SoC Industry
Promotion Center, Human Resource Development Pro ject
for IT SoC Architect.
7. "
Selective Band width and Resource Management in Scheduling for Dynamically Reconfigurable Architectures.,"Partial dynamic reconfiguration (often referred to as partial RTR) enables true on-demand computing. A dynamically invoked application is assigned resources such as data bandwidth, configurable logic, and the limited logic resources are customized during application execution with partial RTR. In this work, we present key theoretical principles for maximizing application performance when available bandwidth is limited. We exploit bandwidth very effectively by selecting a suitable clock frequency for each task and maximize performance with partial RTR by exploiting data-parallelism property of common image-processing tasks. Our theoretical principles are integrated in our scheduling strategy, SCHEDRTR. We present detailed application case studies on a cycle-accurate simulation platform that addresses microarchitectural concerns and includes detailed resource considerations of the Virtex XC2V3000 device. Our results demonstrate that applying SCHEDRTR to common image-filtering applications leads to 15-20% performance gain in scenarios with limited bandwidth, when compared to a sophisticated RTR scheduling strategy with data-parallelism but simpler bandwidth considerations.","43.2
Selective Bandwidth and Resource Management in
Scheduling for Dynamically Reconﬁgurable Architectures
Sudarshan Banerjee
∗
Elaheh Bozorgzadeh
Juanjo Noguera
†
Center for Embedded
Computer Systems
University of California, Irvine,
Ca, USA
banerjee@ics.uci.edu
Center for Embedded
Computer Systems
University of California, Irvine,
Ca, USA
eli@ics.uci.edu
Depar tment of Computer
Architecture
Technical University of
Catalonia
Barcelona, Spain
jnoguera@ac.upc.edu
Nikil Dutt
Center for Embedded
Computer Systems
University of California, Irvine,
Ca, USA
dutt@ics.uci.edu
ABSTRACT
Partial dynamic reconﬁguration (often referred to as partial RTR)
enables true on-demand computing. A dynamically invoked application is assigned resources such as data bandwidth, conﬁgurable
logic, and the limited logic resources are customized during application execution with partial RTR. In this work, we present key theoretical principles for maximizing application performance when
available bandwidth is limited. We exploit bandwidth very effectively by selecting a suitable clock frequency for each task and
maximize performance with partial RTR by exploiting data-parallelism
property of common image-processing tasks. Our theoretical principles are integrated in our scheduling strategy, SCHEDRTR. We
present detailed application case studies on a cycle-accurate simulation platform that addresses microarchitectural concerns and includes detailed resource considerations of the Virtex XC2V3000
device. Our results demonstrate that applying SCHEDRTR to common image-ﬁltering applications leads to 15-20% performance gain
in scenarios with limited bandwidth, when compared to a sophisticated RTR scheduling strategy with data-parallelism but simpler
bandwidth considerations.
Categories and Subject Descriptors: B.6.3 [B.8.3]
General Terms: Algorithms
Keywords: partial dynamic reconﬁguration, bandwidth, scheduling, data-parallelism, RTR
1.
INTRODUCTION
Run-time reconﬁguration (RTR) allows functionality of conﬁgurable logic to be changed (reconﬁgured) during application ex∗Sudarshan is currently with Liga Systems, Sunnyvale, Ca, USA
†Juanjo (juanjon@xilinx.com) is currently with Xilinx Research
Labs, Dublin, Ireland.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2 0 0 7 , June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
ecution – it is available on commercial architectures such as the
Xilinx Virtex devices (Virtex-II, Virtex-4, etc.). This capability
makes it feasible for multiple applications to share the same reconﬁgurable device– when an application is ready to execute, it
is allocated a set of resources (conﬁgurable logic, data bandwidth,
etc.) depending upon system workload, and partial RTR enables
customization of the limited logic resources to better satisfy application performance/power requirements. To truly beneﬁt from such
on-demand computing capability, we require application scheduling strategies capable of maximally exploiting limited bandwidth
and limited conﬁgurable logic by suitably addressing key concerns
such as the signiﬁcant reconﬁguration overhead on such architectures. We propose such a strategy in this work.
We speciﬁcally address the problem of maximizing performance
of typical image-processing application on a device with partial
RTR, given constraints on available bandwidth and available logic
resources.
(Our deﬁnition of the term bandwidth includes both
memory and communication bandwidth). Typical tasks in such applications, such as an IDCT task, are completely pipelined. As long
as sufﬁcient bandwidth is available, execution time of such tasks
can be very accurately estimated. This enables a static execution
schedule to be constructed before the application starts processing
data. A static scheduling strategy is able to take advantage of key
information such as task execution time, application structure, etc.,
to better maximize application performance (minimize execution
time) compared to simple dynamic strategies [10] that have no such
prior knowledge and simply wait for the current set of executing
task(s) to ﬁnish before scheduling dependent tasks. Our target applications are very well-suited for such static scheduling.
In this work, we effectively use key static information (such as
maximum possible clock frequency for executing a task) to maximize bandwidth utilization while minimizing application execution time with run-time data-parallelism. Common tasks such as
Quantize (used in JPEG/MPEG encoding) are data-parallel, i.e.,
multiple, concurrently executing task instances can share the task
workload (volume of data) to reduce task execution time – however, maximally exploiting such data-parallelism with partial RTR
constraints requires detailed bandwidth considerations.
We make the following contributions in this paper:
771
Memory
DYNAMIC TASKS
Memory controller
 TASK
 TASK
Logical shared memory
Logical shared memory
t
h
g
i
e
H
Task 1
Task 2
Static
Components
Dynamic
tasks
SHARED COMMUNICATION MEDIUM


CLB
UART
PPC
STATIC COMPONENTS
Width (CLB columns)
Width
Figure 1: System architecture
Figure 2: Target device model
Figure 3: System realization on target device
• We present key theoretical principles that enable a static scheduling strategy to maximally exploit available bandwidth and logic resources. Our theoretical principles lead to simultaneously selecting
(a) a suitable clock frequency for each task (instance), (b) number
of instances for each data-parallel task, and (c) workload of each
task instance.
• We demonstrate theoretically that our strategy of explicitly considering bandwidth (with partial RTR constraints) leads to better
exploitation of on-demand computing ability with shorter schedule
length compared to strategies with simpler implicit assumptions
(such as equal bandwidth for concurrently executing tasks).
• We follow-up with experimental results on a realistic platform
that considers detailed practical/microarchitectural issues of our
approach and provides accurate execution time data including bus
contention, SDRAM latency, etc., with cycle-accurate simulation.
Our theoretical principles are integrated in our scheduling strategy SCHEDRTR that additionally includes detailed placement, routing considerations necessary for mapping applications onto architectures with partial RTR capability. Previous work [2] has also attempted to exploit data-parallelism with partial RTR with suitable
task workload selection – however, their principles are not applicable when bandwidth is limited.
Detailed case studies on image-ﬁltering applications (Sobel ﬁltering, Laplace ﬁltering, etc.) demonstrate that our strategy frequently results in performance gain of 15-20% when available bandwidth is limited, as compared to prior work with simple implicit
bandwidth considerations (but detailed data-parallelism, partial RTR
considerations) Interestingly, our approach results in 10-20% performance gain while simultaneously requiring less bandwidth than
the fastest RTR-aware (but bandwidth-unaware) sequential execution schedule, i.e, when each individual task executes at its maximum possible clock frequency. Our results are based on very detailed resource considerations (routing, embedded memory, etc.) of
the Virtex XC2V3000 device.
2. PROBLEM DESCRIPTION
Our target system architecture is shown in Fig 1 – it is similar
to previous work such as [5]. The system consists of (a) static components and (b) dynamically reconﬁgured tasks. For static components such as UART, ""hard"" processors (PowerPC), the functionality is unchanged during application execution – this category includes key infrastructural components such as a shared communication medium, a memory controller, etc. Dynamic tasks are instantiated during application execution – the number of concurrently active dynamic tasks depends on task logic requirements
(task area) and hardware resource constraints (area, bandwidth
constraint). Once such a task(s) completes, corresponding resources
are reconﬁgured into one or more tasks with different functionality.
While such a system is very generic, each target device imposes
its own constraints on system realization. Placement (and routing)
data in this paper is obtained from a target device (Fig 2) capable
of column-based partial RTR and is essentially a rectangular grid
of conﬁgurable logic blocks (CLBs). Column-based partial RTR
requires that a dynamic task (from Fig 1) is realized on the device
as a set of adjacent CLB columns, and, the heights of all such tasks
are identical. Fig 3 shows realization of our system architecture
on such a device – part of the device is allocated for static comonents, while the remaining device logic is used for implementing
dynamic tasks. Given the constraints of system realization on such
a device, we believe that shared-bus based communication is more
practical compared to a system with network-on-chip based shared
communication [11].
Each CLB column in Fig 2 consists of a ﬁxed number of conﬁguration memory frames. Reconﬁguration delay for a task is directly proportional to its width (number of CLB columns) and can
be computed very simply from information on frame size, reconﬁguration port width, etc 1 . This reconﬁguration delay is quite
signiﬁcant on commercial architectures with column-based RTR
capability – as an example, reconﬁguration delay for a convolution
task on the XC2V3000 device is greater than task execution time
for a 256X256 image.
Thus, a dynamic task as described above can be represented by
a 5-tuple (ai , rti , et 0
i , f 0
i , bw0
i ), where ai is the task area, rti is the
reconﬁguration delay, f 0
is the maximum allowable task clock frequency (obtained from placement and routing data), et 0
is the task
execution time at clock frequency f 0
i , and bw0
is the task bandwidth requirement at frequency f 0
i .
Typical image-processing applications consist of a chain of n
tasks, {T1 ->T2 ->· · · ->Tn } [8], [5]. As an example, JPEG encoding application chain consists of 4 tasks: {RGB2YCrCb->DCT>Quantize->Huffman}. We assume a sequential execution model,
i.e., task Ti (i>1) starts execution only when its parent task Ti−1 has
ﬁnished and complete output data of Ti−1 is available in the logical
shared memory 2 – speciﬁcs of memory organization, including issues of mapping data to on-chip memory Vs off-chip memory, are
beyond the scope of this work. If hardware resources allow concurrent execution of multiple data-parallel instances of task Ti−1
to improve performance, the execution model enforces the rule that
all such instances of Ti−1 complete execution before Ti starts exei
i
i
1We assume a hardware write-only strategy for reconﬁguration.
Work such as [1] that use a read-modify-write strategy to process
reconﬁguration bitstreams are unable to accurately estimate the reconﬁguration time.
2 In this version of our work, we do not consider inter-task pipelining. While such pipelining requires less shared memory for buffering intermediate results, initial investigations indicate that in our
target scenarios of limited logic, overhead of setting up pipelining
for only a subset of tasks frequently leads to reduced performance.
772
cution. Common image-processing tasks such as RGB2Y CbCr are
data-parallel, i.e., multiple identical, independent instances of
such tasks can be instantiated d uring application execution to reduce execution time. (A few tasks such as Huffman encoding do
not have this property)
Our problem objective is to maximize performance (minimize
execution time) of such an application given a bandwidth constraint
BWC on the application execution and a limited amount of hardware logic AC for instantiating dynamically reconﬁgurable tasks –
the available logic AC is less than the amount of logic required to
simultaneously place all the tasks on the device.
3. RELATED WORK
The last few years have seen a very rapidly growing body of
work that focus on exploiting the powerful capabilities of partial
reconﬁguration for image-processing/multimedia applications [5],
[6], [3], [7], [1], etc. Our work is closely related to work such as [6],
[5], etc that focus on task graph scheduling with RTR-related constraints. However, to the best of our knowledge, we are not aware
of any work that explicitly considers bandwidth as a key resource
constraint in the scheduling formulation.
While work such as [6] simply focus on scheduling a given application task graph, our work includes more sophisticated application restructuring considerations for image-processing application
chains, similar to [5], [2]. Our system architecture is similar to [5].
However, their target device is a multicontext architecture with multiple concurrently active reconﬁguration processes. Commercially
available devices with partial RTR are single context architectures
where only a single reconﬁguration process is active at any instant.
(True multicontext architectures such as Morphosys [14] incur a
signiﬁcant area overhead.)
Our work is most closely related to [2]– our overall strategy of
performance improvement with partial RTR by exploiting dataparallelism borrows extensively from their work. Experimental
data demonstrates that our strategy generates superior (faster) execution schedules with limited bandwidth compared to a modiﬁed
version of this work (enhanced to explicitly consider bandwidth).
Last, but not the least, other (compiler-oriented) prior work in
transforming applications for better performance is typically unaware of partial RTR issues – equally importantly, such compiletime techniques [9] incur high execution overhead (explore a large
design space), since they are not intended for execution in an embedded environment.
4. APPROACH
4.1 Methodology
Ti
Application structure
Workload (image size)
Area Constraint
Bandwidth Constraint
AC
BWC
Synthesize 
SCHEDRTR 
Bitstream
Area
Max frequency
ai
f 0
i
Complete application execution
Placed schedule
Task Bitstreams
Compile-time activities
Execution-time activities
Figure 4: Application execution ﬂow
Our overall methodology is shown in Fig 4. Each individual task
Ti is synthesized with suitable placement and routing constraints
and the corresponding bitstream is stored in a library. This step
directly provides the key task parameters ai (area), f 0
i (maximum
clock frequency) – bw0
i (maximum bandwidth) and rti (reconﬁguration delay) are computed based on f 0
i and ai respectively.
For our target dynamically invoked applications, the area constraint AC and the bandwidth constraint BWC are run-time variables
available only when scheduling the application execution. The image size is another key run-time variable that enables accurate estimation of task execution time, required by any static scheduling
approach. Our scheduling approach SCHEDRTR has low execution overhead suitable for execution on a typical embedded processor (such as the PowerPC processor available on the Virtex devices). When an application is ready to execute, i.e., AC , BWC and
the image size are available, SCHEDRTR generates the application
execution schedule, and the application starts processing data.
SCHEDRTR generates a physically placed schedule necessary
for system realization on devices with partial RTR [12]– it includes
detailed routing considerations [4].
In the rest of this work, we
assume that scheduling a dynamically reconﬁgured task involves
a free-space management policy to locate a suitable free region to
place (reconﬁgure) the task, but we do not focus on policy details.
Before presenting theoretical principles behind SCHEDRTR, we
consider microarchitectural aspects of assigning a suitable clock
frequency to a dynamic task and implications on task bandwidth.
4.2 Bandwidth allocation and task frequency
assignment
m
u
i
d
e
M
n
o
i
t
a
c
i
n
u
m
m
o
C
d
e
r
a
h
S
Interface 
Clock Domain
Task Clock Domain
(Dedicated input)
c
i
g
o
l
e
c
a
f
r
e
t
n
I
c
i
g
o
l
s
s
e
c
c
a
y
r
o
m
e
M
Tx
Buffer
Rx
Buffer
Task
Logic
(DATA
(PREFETCH)
(TASK
(CORE)
Figure 5: Task microarchitecture
Dynamically reconﬁgurable tasks interface to the shared communication medium through a standard interface [7]. As an example, for bus-based communication such as AHB, PLB, etc., a
standard interface deﬁnes ﬁxed parameters such as data-width (32
bits, 64 bits, etc), interface clock frequency (133 MHz, 200 MHz,
etc). The task microarchitecture in Fig 5 enables separation of the
interface functionality from the task core logic and allocating task
bandwidth by assigning a suitable clock frequency to the core.
In Fig 5, the data prefetch receives(transmits) large blocks of
data from(to) memory. The task core processes smaller data units
that it receives from the dual-port RX/TX buffers. Since the three
pipeline stages of {task core input, core execution, task core output} need to be completely balanced, assigning clock frequency f i
to the core execution requires changes to the input (and output) data
rate. Thus, the dual-port buffers have one port clocked at the interface clock frequency and the other port clocked with the task core
clock frequency.
Consider the simple example of a convolution task that consumes one 16-bit data unit every cycle and produces one 16-bit
data unit every cycle. Assuming that the maximum allowable clock
773
 
 
 
 
 
 
 
 (Area Constraint)
E
1
  L0
 (Schedule Length)
  T
  AC
A
r
a
e
Time
Figure 6: Sequential execution
R
12
E
11
E
12
R
13
E
13
A
r
a
e
Time
  T
  T
  T
  T
  AC
  T
  L1
Figure 7: Data-parallel execution
E
11
E
13
R
12
R
13
E
12
A
r
a
e
Time
  T
  T
  T
C
  L2
  A
  T
  T
Figure 8: Simple task elimination (BWC = 2.5 * bw0
1 )
R
12
R
13
0
E
11
E
12
=
  f 1
*2.5/3
=
  f 1
*2.5/3
0
=
  f 1
*2.5/3
0
E
13
  f 12
A
r
a
e
Time
  T
  AC
  T
  L3
  T
  f 13
  T
  T
  f 11
Figure 9:
Identical bandwidths (BWC = 2.5 * bw0
1 )
frequency f 0
conv of the task core logic is 100 MHz, corresponding
bandwidth requirement bw0
conv = 400 MB/s. If we assign a reduced
frequency fconv = 60 MHz to the task core, basic pipelining considerations dictate that the bandwidth requirement bwconv is now 240
MB/s, i.e., bwconv = bw0
. One obvious but important consequence of reducing task bandwidth by assigning a suitable task
frequency is that the task execution time proportionately changes.
The task execution time et 0
i corresponding to the maximum clock
frequency increases to eti = et 0
when the task is assigned
frequency f i . This is a key trade-off in simultaneously trying to
address performance and bandwidth availability concerns.
One key issue in allocating bandwidth by assigning a distinct frequency to an individual task is the availability of enough clocking
resources. Current generations of devices with partial RTR capability such as our target XC2V3000 include upto twelve independent
DCMs (Digital Clock Managers) – each such clocking module is
capable of providing at least one independent clock (the CLKFX
and CLKFX180 outputs provide a frequency synthesizer that can
be any multiple or division of the frequency of the input clock
CLKIN [16]). While this capability is sufﬁcient for our current
applications, newer devices such as the Virtex-4 architecture additionally allow dynamic reconﬁguration of the clock synthesizers,
making our strategy applicable to larger applications (with more
tasks).
conv × fconv
f 0
conv
i × f 0
f i
i
4.3 SCHEDRTR
Next, we consider how a bandwidth constraint impacts performance improvement with run-time data-parallelism and partial RTR.
Detailed consideration of this problem forms the basis of our scheduling approach SCHEDRTR.
4.3.1 Bandwidth constraints on data-parallelism with
partial RTR
We ﬁrst brieﬂy review key principles of exploiting data-parallelism
with partial RTR, borrowed from [2]. Assuming that enough hardware logic is available, instantiating three copies of a data-parallel
task T1 leads to performance improvement (shorter schedule length)
as shown in Fig 6 and Fig 7. In Fig 6, T E
1 represents the execution
for task T1 . Every task, including T1 (the ﬁrst task in the chain)
needs to be reconﬁgured before execution– for ease of presentation, all our execution schedules (and corresponding equations for
schedule length) are shown starting with the execution of the ﬁrst
task T1 .
In Fig 7, T R
1i represents the reconﬁguration process for
the i-th instance of task T1 , while T E
1i represents the actual execution of the task instance T1i . The sequential reconﬁguration mechanism is a key constraint in commercial single-context architectures
– thus, at any time-instant, exactly one reconﬁguration process T R
is active. This sequentiality constraint and the large reconﬁguration
1i
delay rt1 signiﬁcantly affect the schedule length: instead of ideal
schedule length et 0
3 , the actual schedule length is:
L1 = 2 × rt1 + (et 0
1 − 3 × rt1 )/3 = (et 0
1 + 3 × rt1 )/3
Note that this is the shortest possible schedule with AC = 3 × ai
and each instance T1i assigned a different workload (volume of
data) depending upon its execution start time (in this schedule, before T13 starts execution, T12 has processed data (completed workload) for time rt1 , T11 has completed workload 2 × rt1 time and the
remaining workload (et 0
1 − 3 × rt1 ) is divided equally).
Next, we consider a bandwidth constraint BWC = 2.5 × bw0
1 on
the application execution. A simple solution that meets the bandwidth constraint BWC is to eliminate the offending task instance T13
and instantiate only two tasks as shown in Fig 8. The corresponding
schedule length is:
1
(1)
L2 = rt1 + (et 0
1 − rt1 )/2 = (et 0
1 + rt1 )/2
However, the schedule length is longer than in Eqn 1, and the
unused area (corresponding to T13 ) leads us to believe that a better
(faster) solution is possible that meets constraint BWC .
An alternate strategy would be to include the third instance T13
and divide the bandwidth equally between the three task instances.
Thus, we assign an identical clock frequency f1i = 2.5× f 0
to each
instance T1i , as in Fig 9. Following similar principles of maximizing performance as in Fig 7, the schedule length is:
(2)
1
3
L3 = (2 × et 0
1 + 5 × rt1 )/5
(3)
/2
E
12
E
11
R
13
0
1
E
13
R
12
=
  f
A
r
a
e
  T
  T
  T
C
  T
  A
  T
  L4
13
  f
Time
Figure 10: Different bandwidths: simple workload
/2
E
12
E
11
E
13
0
R
12
=
1  f
R
13
  f
Time
  T
  T
  T
C
  T
  A
  T
  L5
13
A
r
a
e
Figure 11: Different bandwidths: balanced workload
We next consider strategies where different instances of the same
task are assigned different bandwidths. In Fig 7, task instance T13
causes the bandwidth constraint to be exceeded. So, a simple approach would be to ensure bw13 = bw0
2 , i.e., assign clock frequency
of T13 , f13 = f 0
2 . If we assign the same workload to each instance
as in Fig 7, the corresponding schedule is shown in Fig 10 and the
schedule length is:
1
1
L4 = 2 × rt1 + 2 × (et 0
1 − 3 × rt1 )/3 = 2 × et 0
1 /3
(4)
774
  
  
  
  
  
  
Comparing the schedules in Fig 7 and Fig 10, we observe that
execution time of T13 in Fig 10 is doubled because it has been assigned half the bandwidth.
Finally consider Fig 11 where we adjust the individual task workloads given the bandwidth constraint BWC . Similar to Eqn 4, we
still assign f13 = f 0
2 , but we ensure that the three task instances
ﬁnish simultaneously, leading to maximum performance gain under bandwidth and area constraints. The corresponding schedule
length L5 is given by the following equations:
1
5. EXPERIMENTS
We conducted a wide range of experiments to validate our proposed approach. Our ﬁrst set of experiments consisted of detailed
case studies on real image processing applications– Fig 12 shows
the Blur application (unsharp masking). For better insight into how
our approach scales with varying device sizes and across a wide
range of applications, we also conducted a very large set of experiments on synthetic applications constructed with data from the real
applications – detailed results are available in [17].
L5 × 1 + (L5 − rt1 ) × 1 + (L5 − 2 × rt1 ) × 1/2 = et 0
1 ,
   Input Colour image 
   Input Colour image 
Or, L5 = (2 × et 0
1 + 4 × rt1 )/5
(5)
140 MHz
RGB2YCbCr
140 MHz
RGB2YCbCr-1
RGB2YCbCr-2
60 MHz
A close comparison of Eqn 5 with Eqn 2, Eqn 3, Eqn 4 conﬁrm
that Eqn 5 is the best solution with the shortest schedule length
that satisﬁes the bandwidth constraint BWC = 2.5 × bw0
1 . For the
comparison, it is important to remember that et 0
1 > 3 × rt1 for our
example, as otherwise the third task instance T13 would decrease
performance (even without any bandwidth constraint).
The following lemma formalizes the strategy in Fig 11 and proves
that it generates the theoretically best (shortest) schedule for a single data-parallel task.
L EMMA 1. For a single data-parallel task with parameters (ai ,
rti , et 0
i , f 0
i , bw0
i ), the shortest execution schedule lmin
with constraints (BWC , AC ) has the following properties:
• Number of data parallel instances nci = M IN (⌈ BWC
ncrt
i = M IN (MAX j (et 0
i > rti × j × j−1
• Each such instance Ti j ( j 6= nci ) is assigned bandwidth bwi, j =
bw0
i . Instance Ti,nc j is assigned bandwidth bwi,nc j = BWC − (nci −
1) × bw0
i .
• Each such instance Ti j is assigned data volume (lmin
i − rti × ( j −
1)) × f i, j
2 ), ⌊AC /ai ⌋)
⌉, ncrt
i ), where
bw0
f 0
i
i
i
Proof:Due to lack of space, detailed proof is in [17].
4.3.2
SCHEDRTR Heuristic Outline
The simple equations above provide the underlying principles
behind SCHEDRTR. However, precedence constraints make the
problem very hard– even with no data-parallelism considerations,
scheduling a task chain with partial RTR constraints is NP-complete
[2]. Such constraints introduce interactions between hardware resource requirements of parent and children tasks, leading to the
necessity of load-balancing considerations between execution of
parent tasks and reconﬁguration of children tasks.
SCHEDRTR is a greedy strategy that incorporates such load balancing similar to previous work. In each iteration it successively
adds a new instance of a data-parallel task while simultaneously
ensuring:
• Bandwidth constraints are satisﬁed
• Partial RTR constraints are satisﬁed
• Performance improves – this step includes parent-child executionreconﬁguration load balancing
However, unlike previous work, SCHEDRTR is capable of slowing down tasks by assigning a task clock frequency f i < f 0
i , the
maximum allowable clock frequency. The core principles in Section 4.3.1 are used to decide the number of instances of a dataparallel task, and assign a suitable clock frequency and data volume to each such instance. The detailed heuristic is presented in
our technical report [17].
775
110 MHz
 Blur
210 MHz
Sub
110 MHz
 Blur
197 MHz
Sub
210 MHz
Add
197 MHz
Add
140 MHz
YCbCr2RGB
140 MHz
YCbCr2RGB-1
YCbCr2RGB-2
60 MHz
   Filtered image 
   Filtered image 
Figure 12: Blur task
chain
Figure 13: Blur schedule, BWC =
600MB/s
5.1 Experimental setup with detailed XC2V3000
considerations
Our experimental setup is a cycle-accurate simulation platform
that includes a SDRAM memory controller, a AHB-compliant bus,
etc – each component is completely synthesizable, enabling us to
obtain accurate area, frequency information on any target device.
We use ModelSim SE with our platform to obtain very detailed
application execution time information including effect of bus contention, DRAM memory RAS/CAS latencies, etc. One key aspect
of our implementation is that the bus arbitration logic is a simple
FCFS (ﬁrst come ﬁrst serve) policy since all tasks are of equal priority.
For the experiments presented here, we targeted the XC2V3000
device. Thus, we synthesized each task Ti with suitable placement
and routing constraints on the XC2V3000, using Synplicity 8.1 and
Xilinx ISE 8.1. The synthesis process directly provided us with the
maximum clock frequency f 0
i and area ai for the task. We obtained
the reconﬁguration delay rti from the Virtex-II datasheet and the
task area ai . Finally, we computed the maximum bandwidth requirement bw0
i based on f 0
i and our knowledge of the task implementation, as explained in Sec 4.2.
5.2 Application Case Studies
For each application, we obtained the execution time Lseq with
the fastest possible sequential execution schedule – this includes
detailed conﬁguration prefetch considerations and other optimizations to minimize schedule length. The corresponding bandwidth
requirement is simply the maximum bandwidth of the individual
tasks in the chain, i.e., BWseq = MAX(bw0
bl ur ,...). Next,
we set a range of bandwidth constraints BWC and applied our proposed approach SCHEDRTR to obtain the schedule length Lbw
To evaluate the quality of solutions generated by our approach,
we additionally enhanced previous work [2] that focusses on dataparallelism with partial RTR considerations. Our modiﬁcations
enable this approach to generate schedules Lbw
ol d with an explicit
bandwidth constraint– however, it does not include our frequency
assignment technique.
rgb , bw0
f req .
OLD[2] NEW
Fastest
(Lseq , BWseq )
(18.8ms,
630MB/s)
(20.2ms,
630MB/s)
(35.57 ms,
630 MB/s)
BWC
(MB/s)
600
800
1300
600
800
1200
600
800
1200
Lbw
ol d
(ms)
19.04
18.8
14.84
20.44
20.2
16.23
35.85
35.57
23.22
CASE
Blur
512 X
512
Sobel
512 X
512
Laplace
768 X
768
Avg
Lbw
f req
(ms)
16.25
14.98
14.84
17.9
16.37
16.09
28.65
23.94
21.38
Perf
Gain
14.7%
20.3%
0%
12.4%
19%
1%
20.1%
32.7%
7.9%
14.2%
Table 1: Application case study
ol d - Lbw
f req )/Lbw
Table 1 presents the data for three of our experiments. For ease
of presentation, the ﬁrst column represents the data tuple (Lseq ,
BWseq ). The last column in the table represents the performance
improvement of our approach compared to work with simpler bandwidth considerations, i.e.,
Perf Gain = ((Lbw
ol d ) * 100
The last row represents the average gain over the presented experiments. One important aspect of our case study is that detailed
analysis of resource constraints (logic area, routing, on-chip memory (BRAM)) on our target platform indicate at most three concurrently executing task instances are feasible on the XC2V3000 –
thus, for the data in Table 1, best possible Lbw
f req is one-third of Lseq
for each application (if bandwidth was unlimited, and, there was no
reconﬁguration delay).
The data in Table 1 shows that for tight bandwidth constraints,
our approach often results in performance improvement between
15-20% compared to the approach with detailed considerations of
partial RTR constraints, but simpler bandwidth considerations. When
bandwidth ceases to be a constraint, expectedly the two approaches
generate identical results, as with the experiments, Blur 512X512
with BWC = 1300 MB/s, or Sobel 512X512 with BWC = 1200 MB/s.
One interesting aspect of our approach is that our frequency assignment strategy can result in performance improvement and consume
less bandwidth compared to the fastest sequential execution, as is
evident from the result corresponding to Blur 512 X512 with BWC
= 600 MB/s.
To better understand our results, consider Fig 13 that shows the
transformed application produced by SCHEDRTR with frequency
assignment and run-time data-parallelism – each task is labelled
with its assigned frequency f i . In Fig 12, each task is labelled with
the maximum allowable frequency f 0
i . Comparing Fig 12, Fig 13,
we observe that our approach slowed down the tasks {Sub, Add} to
match the bandwidth constraint. However, it was able to instantiate
two data-parallel instances for tasks {RGB2YCbCr, YCbCr2RGB}
while satisfying both bandwidth constraints and area constraints
on the application execution. Each such instance (RGB2YCbCr_1,
RGB2YCbCr_2, etc.) is assigned a distinct clock frequency.
Each of our case studies such as Laplace 768X768 is labelled
with the image size of the corresponding experiment. The image size is a run-time variable – it determines the volume of data
processed by a task and is directly proportional to task execution
time. The reconﬁguration delay becomes less important as volume
of data increases and thus applications that process larger image
sizes, such as 768X768 or 1024X1024 images, show more performance improvement as available bandwidth increases.
The detailed application case studies along with our synthetic
experiments (in [17]) conﬁrm the effectiveness of SCHEDRTR in
generating high-quality bandwidth-aware schedules.
6. CO"
Quantum-Like Effects in Network-on-Chip Buffers Behavior.,This paper proposes a paradigm shift in modeling and optimization of NoCs by identifying quantum-like effects in buffers behavior. The key idea of our proposed approach involves identifying a virtual random growing network (VRGN) which describes the NoC buffers evolution as a function of the packet injection rate.,"Quantum-Like Effects in Network-on-Chip Buffers Behavior†
Paul Bogdan and Radu Marculescu
Department of Electrical and Computer Engineering
Carnegie Mellon University, Pittsburgh, PA, USA
e-mail: {pbogdan,radum}@ece.cmu.edu
15.3
ABSTRACT
This paper proposes a paradigm shift in modeling and optimization
of NoCs by identifying quantum-like effects in buffers behavior.
The key idea of our proposed approach involves identifying a virtual random growing network (VRGN) which describes the NoC
buffers evolution as a function of the packet injection rate. 
Categories and Subject Descriptors
B.4 [Hardware]: Input/output and data communications.
General Terms
Algorithms, Performance, Design.
Keywords
Multi-processor systems, networks-on-chip, congestion control.
1. INTRODUCTION
Network-on-Chip (NoC) approach appears as a promising solution
to solve the complex on-chip communication problems but its performance is highly dependent on the amount of available buffering
resources. However, over provisioning the buffer resources is not
acceptable due to tight power and area constraints. The queuing
theory approaches to buffer allocation help to a certain extent
[1][2], but capturing the traffic variability is still a major problem. 
The present paper proposes a paradigm shift in NoC design based
on the analogy that each NoC buffer, at any point in time, is characterized by a particular energy level. Consequently, the flowing of
packets in the network looks similar to particles migrating between
different energy levels due to temperature variations in a thermodynamic system. In our analogy, the temperature of a physical system is replaced with the entropy difference in a communication
system and the buffers in NoCs are modeled as competitive systems. Indeed, most networks (e.g., social networks, WWW and
biological networks, etc.) exhibit competitive behaviors, such as
Fit-Get-Rich (FGR), which have been recently explained using statistical mechanics [4][5].
Mapping buffers behavior to a gas allows us to identify two phases
in network evolution. At the heart of this analogy lies a virtual random growing network (VRGN) which models the information flow
through the network. The VRGN topology is the by-product of a
self-organized stochastic process without any prior knowledge
about connectivity (i.e., congestion phase/location). To the best of
our knowledge, this is the first attempt to characterize the macro†This work was supported in part by Marco Gigascale Research Center (GSRC).
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
DAC 2007, June 4-8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006...$5.00.
scopic behavior of NoCs by establishing a connection between
traffic characteristics and quantum-like behaviors in the network.
2.  MODELING THE VRGN
Taking a statistical physics viewpoint, the buffer evolution in any
NoC can be mapped to a thermodynamic system. More precisely,
the traffic builds a virtual network (i.e. the VRGN) in which packets travel from sources to destinations, thus increasing or decreasing the nodes in and out degree in this virtual network. The nodes
(i.e., processing elements and buffers) from the real NoC are discovered during normal operation and become part of the proposed
VRGN. Using the fitness model in [3], each node can be characterized by two energy levels, where the packets that determine the
incoming and outgoing links of each node are represented as particles moving between various energy levels in a physical system.
To make things clearer, a schematic representation of the VRGN
evolution is given in Figure 1. The configuration of the energy levels on the right hand side gives a snapshot of VRGN state. The
numbers on the directed links of the left most graphs indicate the
number of packets exchanged between those nodes. The packets in
the real NoC correspond to particles on various energy levels in a
thermodynamic system. For example, in Figure 1(a), the in and out
degree of node 7 is 3 and 1, respectively. Within the infinitesimal
time interval (t,t+δt), node 7 may send one packet to node 8 (discovering this node) and thus increasing its out degree. This is illustrated in Figure 1(b) by a change on the E7 energy level.
IN
At time step t+δt
OUT
Figure 1: Schematic representation of Virtual Random Growing Network. 
The VRGN has the following properties: The number of nodes in
the VRGN increases (starting at time t=0) for each time step until it
reaches the total number of nodes in the real network (i.e., at each
time step, a new node is discovered as it receives or sends packets).
After discovering the entire network, the VRGN grows only due to
the number of edges acquired by each node. Competition for buffer
266
11
1
2
4
1
3
7
11
1
2
4
1
3
7
1
1
1
1
1
1
2
1
5
8
2
1
5
3
3
1
6
9
3
3
1
6
1
8
9
(a)
(b)
E1
E2
E3
E4
E5
E6
E7
E8
E9
E1
E2
E3
E4
E5
E6
E7
E8
E9
IN
OUT
At time step t
slots causes the edges be added to the network based on a preferential attachment mechanism [3]. Thus, each node is characterized by
two fitness functions, one for incoming links and another one for
outgoing links, and two energy levels. 
By establishing this isomorphism between the real topology of the
NoC and the virtual graph of traffic evolution, we emphasize the
role of the packet injection rate (λ) in the network. Its impact can
be seen at criticality (λc) when a transition from an FGR to a condensation phase takes place. We should point out that the maximum information transfer, in the absence of any optimization,
occurs precisely at this stage.
3. NOC BUFFERS BEHAVIOR AS POWER LAW
i
)N i o,
∂N i o,
Buffers in real NoCs are finite and therefore packet interaction
causes congestion. As such, besides the growing process mentioned earlier, a link rewiring process takes place in the VRGN. We
claim that the NoC traffic follows mixed statistics, i.e. Bose-Einstein (BE) statistics for the growing process before criticality, and
Fermi-Dirac (FD) statistics for the reverse process. Indeed,
Figure 2 shows that, as the packet injection rate increases, there is
a critical point (λc) beyond which the number of available buffers
decreases. In terms of quantum-like behavior, in the bosonic phase
each energy level can accommodate infinitely many particles,
while during the fermionic phase each energy level is constrained.   
In the proposed model for NoC behavior, at each time step, one
incoming or outgoing packet may be removed from any node and
re-attached to any other node due to packet re-routing. This corresponds to highly congested buffers where a few links are removed
since there is no available space to store them (i.e., Fermi phase).
We denote by Ni,o(Ei,Eo,t,ti) the number of nodes with input
energy level Εi (i.e. i-incoming links) and output energy level Εo
(i.e. o-outgoing links) that are in VRGN at time ti and which satisfies the following rate equation:
p iη
---------- iN i - 1 o, - i+ 1
[
(
M i
------------- =
∂ t
p i θ
p o θ
[
(
] [
(
--------- iN i - 1 o, - i + 1
----------- oN i o - 1
- o + 1
Z i
Zo
where pi/po are the probabilities of receiving/sending a packet,
ηi,ηo/θi,θo are the BE/FD fitness distributions, Mi,Mo/Zi,Zo are the
partition functions of BE/FD statistics, δ is the Kroneker symbol,
and π gives the dynamics of nodes discovering/acquiring process.
The first term in Equation 1 (right hand side) accounts for an
increase with one edge of vertices of in degree (i-1) and an equal
loss of vertices of in degree (i+1). The corresponding input rate is
piηi/Mi, with Mi being the partition function [5]. The second term
represents the gain of one unit to nodes of out degree (o-1) and the
corresponding loss to nodes of out degree (o+1). The third term is
a consequence of the rewiring process affecting the in degree. The
fourth term describes a similar action for nodes out-degree of size
(o-1) and o, respectively. The last term represents the probability π
of adding/discovering nodes in the real network.
Using Equation 1 above, it can be shown that the IN and OUT distributions of the in and out degrees, respectively, in the VRGN:
IN k( )
k
       OU T m(
) m
–≈
(2)
exhibit a power law as 
k m,
∞→
, where αin and αout depend on
pi, po, ηi, ηo and π. To support this claim, we performed several
simulations and observed 
that 
for different mesh sizes
p oη
------------ oN i o - 1
[
Mo
]+ δ i 1, δ o 1, π
α i n–≈
- o+ 1
(
)N i o,
)N i o,
)N i o,
α o u t
(1)
] +
] –
o
o
i
,
,
s
r
e
f
f
u
B
e
l
b
a
l
i
a
v
A
&
d
e
r
e
v
o
c
s
i
D
f
o
r
e
b
Bose-Einstein
Growing Phase
s
r
e
f
f
u
B
.
l
i
a
v
A
.
c
s
i
D
f
o
r
e
b
m
u
N
Fermi-Dirac
Falling Phase
Unbounded 
Buffer Space
Bounded Buffer Space
λc
λ-Packet Injection Rate
Bose-Einstein Phase Fermi-Dirac Phase
u
N
m
λc
λ- Packet Injection Rate
Figure 2: Phase transitions in VRGN.
(4×4...10×10), under adaptive routing and different traffic modes
(e.g, transpose, hotspot), the total number of packets (i.e.,
IN+OUT) passing through the network buffers exhibits a clustering
phase as the number of packets increases. This phenomenon can be
modeled as a power law. From a design perspective, it means that
the NoC buffers should be sized such that the network remains in
the FGR phase for a wide range of injection rates. 
The network exhibits a phase transition from FGR to a BE and
later to a FD condensation phase as the number of packets
increases over time. We assume that each node in the network is
characterized by a packet processing rate λproc. The difference
between the packet injection and the processing rates is a measure
of the disorder in the system; this is synonymous with the temperature of a physical system which determines the occupancy of
energy levels. Thus, we define the fitness functions based on system entropy measure H=|λ-λproc|δt:
–
β E i
–
E i
--------------------------------K S λ - λ p r o c δ t
E i
--------------------------------K S λ - λ p r o c δ t
β E i
(3)
η
=
e
=
e
            θ
=
e
=
e
Bose-Einstein Statistics
Fermi-Dirac Statistics
where KS is a Shannon-like constant and Ei is the ensemble energy. 
Conceivably, a buffer allocation algorithm would take as inputs the
node packet injection and processing rates, application execution
time, and find a solution for the FGR phase. The absence of such a
solution is a signature of congestion and so the input parameters
need to be adjusted accordingly. If by using the new parameters a
solution is found, then the algorithm returns the buffer size based
on a power law. Otherwise, more iterations are needed. 
4. CONCLUSION
We proposed a quantum-like approach to model the information
flow and buffers behavior in NoCs. In our model, the temperature
of a physical system is replaced by the NoC packet injection rate.
Analysis of the proposed model predicts that the buffer occupancy
follows a power law distribution. Given this observation, nonuniform buffer allocation should be done on different grounds than
classical queuing theory. This remains to be done as future work. 
5.  "
Interconnects in the Third Dimension - Design Challenges for 3D ICs.,"Despite generation upon generation of scaling, computer chips have until now remained essentially 2-dimensional. Improvements in on-chip wire delay and in the maximum number of I/O per chip have not been able to keep up with transistor performance growth; it has become steadily harder to hide the discrepancy. 3D chip technologies come in a number of flavors, but are expected to enable the extension of CMOS performance. Designing in three dimensions, however, forces the industry to look at formerly-two-dimensional integration issues quite differently, and requires the re-fitting of multiple existing EDA capabilities.","32.1
Interconnects in the Third Dimension: Design Challenges 
for 3D ICs 
Kerry Bernstein, Paul Andry, Jerome Cann*, Phil Emma, David Greenberg, Wilfried Haensch, 
Mike Ignatowski, Steve Koester, John Magerlein, Ruchir Puri, Albert Young 
IBM T.J. Watson Research Center, Route 134/ P.O. Box 218, Yorktown Heights, NY 10598 USA 
*IBM Microelectronics, 1000 River Road, Essex Junction, VT 05452 USA 
Email: kbernste@us.ibm.com
ABSTRACT 
Despite generation upon generation of scaling, computer chips 
have until now remained essentially 2-dimensional. Improvements 
in on-chip wire delay and in the maximum number of I/O per chip 
have not been able to keep up with transistor performance growth; 
it has become steadily harder to hide the discrepancy. 3D chip 
technologies come in a number of flavors, but are expected to 
enable the extension of CMOS performance. Designing in three 
dimensions, however, forces the industry to look at formerly-twodimensional integration issues quite differently, and requires the 
re-fitting of multiple existing EDA capabilities. 
Categories and Subject Descriptors 
B.4.3 
[Interconnects 
(Subsystems)]: Parallel 
I/O; C.1.2 
[Multiple Data Stream Architectures]: 
Interconnection 
Architectures 
General Terms 
Performance, Design, Standardization, Theory 
Keywords 
3D, Interconnect, Through-wafer via, Chip-stack, Silicon Carrier, 
Bandwidth, Latency, Hierarchical Memory 
1. What is 3D Interconnect? 
In general, 3D refers to a variety of technologies which provide 
for electrical connectivity between multiple active device planes. 
The technologies vary in the diameter and pitch of the vertical 
vias, their impedance, and how the planes are mounted in respect 
to each other. The first 3D structures appearing in the market were 
stacks of chips with progressively larger dimensions, which were 
then wire-bonded. Early integrated 3D structures comprised      
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
Permission to make digital or hard copies of all or par t of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or republish, to post on servers or to redistr ibute to lists, requires prior 
specific permission and/or a fee. 
DAC 2007, June 4–8, 2007, San Diego, California , USA 
Copyr ight 2007 ACM 978-1 -59593-627-1/07/0006…5.00  
562
separate dies connected with conventional C-4 solder bumps 
attached to large through-wafer vias, mounted either face-to-face 
or back-to-face. Advanced 3D technologies with vias under 1 
micron in diameter are now emerging, connecting wafer-bonded 
active layers. This range and their applications are shown in 
Figure 1. 
Figure 1 Technology range of vertical interconnects 
1.1 Chip Stack / Silicon Carrier Technology 
The key enabling technology element required for 3D integration 
is the through-silicon via (TSV).  Per Figure 1, the final 
application and 
the 3D process 
technology used for 
its 
implementation will dictate the physical parameters of the TSV 
including pitch, diameter or width, and via height.  Assuming 
interconnect density requirements can be met, the electrical 
requirements of the design must be fulfilled which include 
maximum allowable values for via resistance, capacitance and 
inductance.  A good deal of research has been performed to define 
the structure and fabrication process of TSVs [1][2].  A common 
approach employs a cylindrical through-via metallized with Cu 
electroplating. Copper can be processed using standard back-endof-the-line (BEOL) tools.  However, the dimensions of these 
through-vias are limited by the capabilities of the plasma vapor 
deposition (PVD) tools used to deposit the liner and Cu seed films 
necessary for electroplating.  In practice, via aspect ratios much 
beyond 10:1 become very difficult to electroplate using standard 
techniques.  Another approach known as a “vias last” process 
drives the via in from the backside of the wafer once the wafer has 
been ground to its final thickness.  Typically, this process requires 
some combination of wafer handling, backside lithography, low 
temperature insulation, RIE, PVD and electroplating.   
 
 
  
 
 
 
                   
 
through-silicon vias, which are used to provide I/O escapes from 
the bonded stack. 
Figure 3 Schematic illustration of face-to-face wafer-scale 
assembly 
enabled by 
copper-to-copper bonding 
of 
interconnects 
Because wafer-scale integration enables further processing of 
wafers after the join step, process flows can also be developed 
which first join wafers mechanically and then electrically at a later 
step.  One example of this in practice today enables very high 
densities of interlayer interconnections.  It uses a glass-handle 
wafer to stack silicon-on-insulator (SOI) top-circuit layer onto an 
underlying bottom-circuit wafer in a face-to-back orientation, as 
illustrated schematically in Figure 4. 
Figure 4 Schematic illustration showing steps used to stack a 
silicon-on-insulator (SOI) top-circuit layer onto an underlying 
bottom-circuit wafer in a face-to-back orientation 
2. 3D Advantages and Opportunities 
Vertical interconnects, in a number of embodiments have been 
shown to provide a number of distinct advantages to high 
performance chips. Each application has a different requirement 
for via pitch and count, electrical performance, and operating 
conditions. These requirements dictate which of the 3D schemes 
is most economically suited to the use. 
2.1 Wirelength Reduction 
3DI is commonly cited as a means of reducing lateral wire length. 
With the introduction of multiple active silicon layers, a 3DI 2-3 
micron tall vertical via with resistance of a few milliohms, 
inductance of under 1 pH, and capacitance of a couple fF replaces 
a lateral wire of tens to perhaps hundreds of microns. Figure 5 is a 
histogram of wirelengths from a recent 90nm-generation 
microprocessor subunit, assuming wirelength reduction factors 
ranging from 1X to 0.5X. As the wire lengths decrease, so does 
the demand for wire buffers, also shown in Figure 5. Overall, 
power dissipated in interconnects and buffers goes monotonically 
with the wire length-reduction factor, and is shown in Figure 6. 
For applications requiring modular designs, where heterogeneous 
integration of multiple die must be interconnected in a reworkable 
manner, chip-to-chip and chip-to-wafer 3D assemblies are useful.  
Chip stacking using thin die with TSVs and conventional pitch C4 
interconnects may dramatically increase density of memory 
products with only incremental changes to die, package and final 
assembly.  Further increases in density can be realized by 
decreasing the volume of the joining metallurgy and using 
thermo-compression 
techniques 
to reduce stacked 
inter-die 
spacing down to a few microns [4].  Since the Cu wiring density 
achieved using standard CMOS BEOL techniques is orders of 
magnitude higher than typical first levels packages, silicon 
carriers 
increase 
in-plane 
interconnection density between 
multiple die, assuming the I/O pitch of these die can be reduced 
simultaneously.  Bond and assembly of multiple chips on a silicon 
carrier have been undertaken using a variety of solder 
metallurgies on die having dense microbump arrays (25 micron 
diameter at 50 micron pitch) with excellent yield and reliability 
[3]. Such techniques may be extended down to pitches of tens of 
microns.  Planarity, solder volume and chip placement accuracy 
issues may limit further reductions.  Examples of chip stacks and 
silicon carrier assemblies are shown in Figure 2.  
1.2 Wafer Scale Integration 
Wafer-scale 
integration 
typically 
involves 
the 
joining and 
interconnection of two wafers that already have active circuits and 
devices.  The use of such wafer-to-wafer joining preserves the 
physical integrity of the wafer and therefore enables additional 
conventional semiconductor processing (typically back-end-ofline operations) after the wafer-joining step.  In wafer-scale 
integration, the wafers can be joined in either a face-to-face or 
face-to-back orientation; however, both types of builds typically 
require wafer thinning, aligned wafer-to-wafer bonding, and 
formation of wafer-to-wafer interconnect.  These processes are 
generally performed using materials that are compatible with 
future back-end-of-line processing. 
Wafer-scale integration can be used in combination with throughsilicon vias (TSVs).  As one example, face-to-face wafer-scale 
integration can be enabled by copper-to-copper bonding of  
a) 
b) 
Figure 2 a). a 3D stack comprising a full thickness chip, a 90 
micron thick Si carrier, and a ceramic package,  and b) SEM 
of a 2-chip stack showing TSVs, thin joining metallurgy and 
the resulting small inter-die gap. 
interconnects (Figure 3).  Copper metallurgy can be used to 
simultaneously form mechanical and electrical connections, and is 
compatible with additional back-end processing.  A full thickness 
top wafer is fabricated with both circuits and deep-via (deeper 
than the final top-wafer thickness target) structures.  This wafer is 
flipped over and aligned to the bottom wafer, so the copper 
patterns used at the join of the two wafers must incorporate 
mirroring in the design.  After aligned bonding, the top wafer is 
thinned from the backside to convert the deep-via structures into 
563
 
 
 
)
n
b
i
r
e
p
(
t
n
u
o
C
r
e
f
f
u
B
2000
1500
1000
500
1 .0X
0 .9500
0 .9000
0 .8500
0 .8000
0 .7500
0 .7000
0
0
5
10
15
Leng th Bi n (m ic rons)
T housands
20
25
30
r
f
f
s
s
d
e
n
a
u
s
B
u
o
a
h
o
T
T
t
l
45
40
35
30
25
20
1
0.95
0.9
0.85
0.8
0.75
3D A vg N et Lengt h R educti on Factor
0.7
specialized accelerator functions separate from the cores; security 
and network functions are two common operations addressed by 
specialized hardware.  These trends combine to significantly 
increase the need for larger cache capacity to support the many 
different threads running on a chip, as well as increasing the 
overall bandwidth requirements for the cache and memory 
systems.    
Figure 5  Histogram of wirelengths by length-reduction factor, 
and Buffer Count by Length-Reduction Factor 
10
1
0.1
)
W
(
r
e
w
o
P
0.01
1.00
W irePwr Sv gs (W )
Bu f Pwr Sv gs ( W )
0.70
0.90
0.80
0.95
0.85
0.75
Avg Net Leng t h R educ tion
Figure 6 Wire, Buffer Power Savings with Length-Reduction 
Factor 
2.2 Integration of Disparate Technologies 
Chip processing technologies specific to functions such as RF 
circuits or memories are often incompatible with fabrication steps 
needed to produce high performance logic devices. Past attempts 
to converge these onto a single monolith in 2D resulted in 
compromises in device mobility or device density. Converging 
these functions is a powerful means of reducing delay or power 
consumption, as the alternative is placing separate components on 
a system blade or planar. Power and delay associated with the 
resulting traces can make certain capabilities unfeasible. 3D 
interconnect enables 
the 
independent fabrication of 
these 
functions, and their subsequent integration in close electrical 
proximity. 
2.3 Bandwidth and Latency 
A more subtle but perhaps very compelling advantage of 3DI is 
associated with the availability of massive bandwidth.   
2.3.1 Architecture 
Processor chips have historically achieved a yearly average 
compound performance growth rate of 50% or more.  While much 
of this performance growth rate was due to faster cycle times and 
improved core architecture, a growing portion of the performance 
growth now comes from factors 
that 
increase bandwidth 
requirements at a faster rate.  Leading edge microprocessors are 
implementing an increasing number of cores per chip, as well as 
an increasing number of execution threads per core (see Figure 7).  
The Sun Niagara-2 will have 8 cores each supporting 8 execution 
threads for a total of 64 execution threads on a chip.  In addition, 
virtualization functions allowing multiple operating systems to 
run together on a single chip are becoming pervasive.  Processor 
chips are also implementing specialized hardware to increase the 
performance of various common operations.  This can take the 
form of specialized extensions to the core design, such as SSE 
(Streaming SIMD Extensions) from Intel, 3DNow! from AMD, 
and AltiVec for PowerPC processors.  It can also take the form of 
Figure 7 Thread growth in high performance processors 
2.3.2 Cache Management 
Bandwidth is used for moving “cache lines” between main storage 
and the various levels within the cache hierarchy. A cache line is a 
contiguous block of data; usually 128 or 256 bytes in servers. 
When a processor references a datum that is not in its cache, this 
event is said to be a “cache miss.” The system must the retrieve 
the line containing that datum, and deliver it to the processor. The 
processor stores the newly referenced cache line in its local cache, 
overwriting a previously held cache line. 
Miss events usually cause the processor to stall (at least, partly) 
until it receives the requested datum. The penalty incurred 
comprises what is called the “Leading Edge” (LE) and the effects 
of the “Trailing Edge” (TE) of the miss. 
The LE comprises the various latencies that are immediately 
apparent when considering what has to get done: logic cycles to 
do prioritization, directory lookups, and error checking; time-offlight wire delays; queuing delay incurred when the required 
resources are busy servicing a different miss, and the access time 
associated with the remote cache that sources the data. 
The TE is simply the number of cycles (processor cycles) required 
to move the entire line across the bus. It is purely a measure of 
bandwidth, and has nothing to do with latency. (Latency is 
accounted in the LE.)  
TE = (Line Size / Bus Width) X (FProcessor  / FBus)     (Eq. 1)                  
The first ratio in Equation 1 is the number of “packets” that are 
moved across the bus. The TE causes performance problems 
through five distinct kinds of interactions. For three of these 
interactions, the penalty induced is directly proportional to the TE. 
For the other two, the induced penalty is nonlinear: minimal if the 
bandwidth is sufficient and precipitously dreadful as certain 
thresholds are crossed [5]. 
A long-known heuristically observable property of caches is that 
the miss rate (the rate at which a processor generates misses) is 
564
 
 
 
 
 
 
 
 
proportional to the reciprocal of a root (frequently, the square 
root) of the capacity of the cache, and is workload dependent. The 
bus utilization is a product of the TE (which is the “service time” 
for a miss) and the miss rate. The nonlinearities in the TE effects 
arise when the utilization gets pushed too hard. 
Cache capacity and bandwidth can be thought of as “mutually 
fungible” entities. If the cache can be made larger, less bandwidth 
is required; if more bandwidth is made available, we can live with 
a smaller cache. It is always better to have more of both however: 
big caches with lots of bandwidth. 3D affords us this opportunity. 
Obviously, 3D enables more cache capacity directly. Cache 
“planes” can be readily stacked upon a system footprint. Less 
obvious is that within the 3D stack, much higher bandwidth 
(between planes) is achievable as well, much higher than would 
have been possible had the planes been laid out on a 2D package. 
To wire between chips on a 2D package requires a “Manhattan” x 
and y wiring having dimensions on the scale of the chip size. 
These wires can be quite long - several centimeters, or even 
inches. If areas to be connected between chips are spatially colocated, then when put into 3D the connections can be primarily 
vertical (in the z dimension). In this case, particular, busses 
between the adjacent layers in a cache hierarchy have the potential 
of requiring very little x or y displacement. Short busses like this 
(now perhaps only 10s or 100s of microns) run much faster, and at 
lower relative power. In addition, if specific 2D layouts can be 
anticipated (to minimize the x and y motion required in moving 
between planes), wiring blockages may be eliminated. This will 
enable a denser thru-via grid, which allows the busses to be very 
wide as well. This enables removal of much of the Trailing Edge, 
and its pernicious effects. 
2.4 Low Voltage and Power Savings 
The performance of many high performance processors, is limited 
not by raw capability but by the ability to supply sufficient power 
or remove the consequential heat.  One effective means of 
improving efficiency is by the use of reduced-voltage supplies. 
Since the signal swing in CMOS logic is determined by the supply 
voltage, reducing this voltage decreases the energy supplied by a 
logic gate to and discharged from the wiring and  
Figure 8 The relationship between CMOS active and passive 
power 
next-stage input capacitance. This reduces active energy per cycle. 
At the same time, threshold voltage increases as a consequence of 
565
drain-induced barrier 
lowering, decreasing 
leakage. Passive 
energy per cycle thus decreases for some range of decreasing 
supply voltage [6][7]. Figure 8 illustrates this relationship 
between both active and passive energy per cycle and supply 
voltage. 
Low-voltage operation comes at the cost of increased gate delay, 
however, due to the reduced on-current available to charge each 
gate output node. This increased gate delay degrades performance 
and allows leakage power to integrate over a longer period, 
eventually causing passive energy to rise again at sufficiently low 
supply values, as shown in Figure 8. 
Circuit architecture can capture the improved efficiency of lowvoltage 
operation while maintaining 
performance 
by 
compensating for delay. Parallelism can offset delay by 
increasing the number of circuits performing a given task, 
dividing the workload [8][9]. Since not all algorithms can be cast 
into a perfect parallel implementation, the percentage increase in 
circuit count required to maintain performance is typically larger 
than the percentage increase in delay, commonly represented by a 
power-law 
relationship with coefficient α. 
In a system 
characterized by an α value of 1.4, for example, circuit count must 
increase by a factor of 2.6 for each doubling of delay. The price of 
power efficiency constrained by constant performance is therefore 
an increase in chip area. Power-area tradeoffs can be difficult to 
accept in planar implementations. 3D integration offers options. 
Specific system blocks such as SRAM caches could be moved to 
their own layer, freeing area for increased logic count. In another, 
multiple cores with dedicated local memory could be stacked in 
multiple layers. 
A key obstacle toward realizing robust low-voltage design is 
process variability, particularly as it impacts threshold voltage. 
Such variability can be at least partly countered by adjusting 
threshold voltage dynamically through body bias (in bulk and 
partially-depleted SOI) or a backgate (in fully-depleted SOI). 
Implementing threshold-adjusting biases with fine granularity 
across a chip is a difficult design challenge, however, requiring 
generation of many individual voltages. 3D integration provides 
an elegant solution to this problem, enabling individual, adjustable 
voltage converters to be placed in their own layer directly above 
the zones of the logic chip where they are required. 
3. 3D Challenges and Solutions 
3.1 Effective Cooling of 3D Assemblies 
Thermal management in 3D stacks is critical for maintaining 
required reliability, performance, and power dissipation targets. 
Conventional 2D solutions, however, are insufficient to remove 
the heat associated with stacked or bonded layers.  
Cooling of 3D assemblies is difficult, both because the power per 
unit area is increased and because heat must be conducted through 
multiple chips, often with poor thermal interfaces. While it is 
possible to introduce coolants within a thick 3D structure to 
handle very high power levels [10], such an approach is complex 
and requires thick cooling structures within the stack to bring in a 
sufficient fluid volume. Thus every effort should be made to 
remove heat from the back of a chip stack as is currently done for 
single high-power chips.  
While calculating chip temperatures accurately requires detailed 
thermal modeling, rough estimates based on 
the 
thermal 
resistances from Table I show that removing heat from the back of 
a stack is feasible. While a microprocessor with a hot spot power 
 
density of 2 W/mm2 would give a prohibitive temperature drop of 
over 50 C per chip layer for a “typical” case, improved thermal 
design can manage this. Nevertheless, care must be taken that hot 
spots on different chips in a stack do not overlap. For DRAM 
chips with power densities in the range of 0.01 W/mm2, stacks of 
many chips may be possible through careful thermal design. 
Table 1 Approximate area-normalized thermal resistance for 
layers in stacked chip structures. 
Structure 
Thermal resistance 
(C-mm2/W) 
16 
PbSn solder balls 100 µm tall, 15% 
coverage 
Cu balls 20 µm tall, 20% coverage 
200 µm thick Si wafer 
10 µm thick SiO2 layer 
Total “typical” (solder balls) 
Total 
“improved” 
(Cu balls, 
dielectric) 
thin 
0.3 
1.6 
8 
>25 
~5 
3.2 Test Approaches 
The Automated Test Equipment (ATE) required to test advanced 
3DI wafers in the future will be no different than that needed for 
testing standard, non-bonded 2D wafers today.  However, for 
some 3DI bonding processes, the 3DI wafers will be aligned and 
bonded before metallization of the topmost layer, with the process 
repeated for each additional layer [11].  Without metallization of 
each IC layer prior to bonding, testing individual layers prior to 
bonding will not be possible.  Because testing of individual IC 
layers prior to bonding will not be possible, contact pads, ESD, 
I/O, and test structures on individual layers will not be necessary, 
and can be placed on a single dedicated layer, the Peripheral and 
Test Layer (PTL).  The debate remains open as to where the PTL 
should be placed in the stack, where the contact pads should be 
located (topmost layer or backside of substrate), and how the 
signals should be routed through the 3DI assembly.  If the PTL is 
placed first on the stack and the contact pads are located on the 
backside of the substrate, there will be significant opportunities 
for 
developing 
new 
test Front-End-Hardware 
(FEH), 
methodologies and processes 
for 
improving 3DI quality, 
throughput and yield.  Locating the contact pads on the backside 
of the substrate will enable the continuous monitoring of the 
bonding process as well as testing of the assembly prior to 
completion.  If the contact pads are located on the topmost layer, 
as is done with standard, non-bonded 2D wafers today, test will 
continue to be the last step in the process, and the FEH, 
methodologies, processes and value added by test will remain 
largely unchanged. 
3.3 EDA Enablements for 3D 
A fundamental shift in the technology has occurred beyond 90nm 
CMOS where the interconnect resistance has been increasing 
significantly to cause a repeater explosion problem. This problem 
translates into not only significant area overhead but also power, 
as repeaters are among the leakiest circuit topologies. 3D 
technology has the potential of easing the challenge of repeater 
explosion (Figure 9) 
Figure 9  Repeater explosion due to metal resistance increase 
with CMOS scaling 
In order to exploit the full potential of 3D technology, new 
challenges in the area of physical design [17][18], thermal 
analysis[15][16], system level design and analysis need to be 
addressed [13]. 3D interconnects have the potential of reducing 
critical paths delays significantly, which are typically between 
memory and the interfacing logic.  
New 
tools 
that consider 
thermally aware physical design 
implementations, most importantly at the architecture and SoC 
level are crucial to the success of 3D as thermal issues are 
exacerbated in 3D implementations [12]. To justify the cost and 
complexity overhead of 3D technology, it is essential to study the 
benefit of 3D early in the design cycle. This requires strong 
linkage between architecture level analysis tools and 3D physical 
planning tools. Most of the advantages of 3D will be utilized with 
new 
system architectures and physical 
implementations. 
Therefore, the tools to aid 3D implementation must also operate at 
the higher level in addition to the 3D place and route algorithms 
that have been proposed in the literature before. In fact, in our 
view, the benefits from 3D place and route will be limited since 
current 2D designs do a fairly good job of optimizing the critical 
path distance. There is a very strong need for 3D architectural and 
physical planning tools that operate in the domain of thermal, 
physical, and performance analysis in order to yield an optimized 
system implementation in 3D technology [19][20][21][22]. Most 
of the studies reporting huge benefits from 3D for wire length [12] 
do not adequately consider  
Figure 10 Sweet spot of 3D partitioning when considering 3D 
through via impact 
the physical impact of vertical vias. It is crucial to consider the 
impact of vertical vias on the physical design of ICs, from area, 
latency, and thermal impact point of view. Figure 10 shows that 
the sweet spot of partitioning for 3D implementation lies at the 
566
 
 
[15] 
[12] 
[16] 
[9] H. P. Hofstee, ""Power efficient processor architecture 
and the cell processor"" Proc. 11th Int. Symp. on HighPerformance Computer Architecture (San Francisco, 
CA), Feb. 2005, pp. 258-262, 2005.  
[10] B. Dang, et al., “Integrated Thermal-Fluidic I/O 
Interconnects for an On-Chip Microchannel Heat Sink”, 
Electron Device Letters, Vol. 27, pp. 117-119, Feb. 
2006.  
[11] K. Bernstein, et al., ""Introduction to 3D Integration"", 
ISSCC '06 Tutorial 3, February 2006.  
J. Cong, et al., “An Automated Design Flow for 3D 
Microarchitecture Evaluation”, Proc. of Asia Pacific 
DAC 2006, pp. 384-389, 2006. 
[13] A. Rahman, et al., “Wire length distribution of 3-D 
ICs”, Proc. of IEEE Intl. conference on interconnect 
technology 1999, pp. 671-678, 1999. 
[14] S. Das, et al., “Design Tools for 3-D ICs”, Proc. of 
Asia-Pacific DAC 2003, pp. 53-56, 2003. 
J. Cong, et al., “A thermal driven floorplanning 
algorithm for 3-D ICs”, Proc. of ICCAD 2004. pp. 306313, 2004. 
J. Cong, et al., “Thermal driven multi-level routing for 
3-D ICs”, Proc. of Asia-Pacific DAC 2005, pp. 121-126, 
2005. 
[17] C. Ababei, et al., “Placement and Routing in 3D ICs”, 
IEEE Design & Test, Nov-Dec. 2005, pp. 520-531, 
2005. 
[18] S.K.Lim, “Physical Design for 3D system on package”,  
IEEE Design & Test, Nov-Dec. 2005, pp. 532-539, 
2005. 
[19] G.L.Loi, et al., “A Thermally aware performance 
analysis of vertically integrated (3D) processor memory 
Hierarchy”,  Proceedings, 43rd Design Automation 
Conf. (DAC), pp. 991-996, 2006. 
[20] O.Ozturk, et al., “Optimal Topology Exploration for 
Application-Specific 3D Architectures”,    Proc. of Asia 
Pacific DAC 2006, pp. 390-395, 2006. 
J. Kim, et al., “A Novel Dimensionally-Decomposed 
Router for On-Chip Communication in 3D 
Architectures”, to appear in Proc of International 
Symposium on Computer Architecture (ISCA) 2007. 
[22] W.-L. Hung, et al., “Interconnect and Thermal-aware 
Floorplanning for 3D Microprocessors”,   International 
Symposium on Quality Electronic Design (ISQED), 
2006, pp. 98-104, 2006. 
[21] 
unit level (where a unit is a large logical entity such as floating 
point logic or Instruction decode logic etc) and beyond, when 
considering the via impact. 
In addition to 3D design and implementation tools, there are 
important challenging issues in 3D test and yield that must be 
addressed as well. It is well known that yield has a quadratic 
dependency on  die size, and a linear dependency on chip count at 
a given die size. 3D designs may incur some yield loss due to 
vertical vias, and may gain some yield due to density. One of the 
benefits of 3D is that this technology is compatible with the 
known-good-die practices, a known contributor to cost reduction 
and test simplification.  
4. "
Performance Modeling and Optimization for Single- and Multi-Wall Carbon Nanotube Interconnects.,"Based on physical circuit models the performances of signal and power interconnects at the local, semi-global and global levels are modeled at 100degC. For local signal interconnects, replacing copper wires with a typical aspect ratio of 2 by thin SWNT interconnects can lower power dissipation by 50%. This would also improve their speed by up to 50% by the end of the ITRS. Copper wires and large diameter MWNTs offer the lowest resistance for power distribution in the first and second interconnect levels, respectively. SWNT-bundles and MWNTs can be used to lower the delay of signal interconnects in semi-global levels. MWNTs with diameters of 50 nm and 100 nm can potentially increase the bandwidth density of global interconnects by up to 50% and 100%, respectively.","32.2
Performance Modeling and Optimization for Single- and 
Multi-Wall Carbon Nanotube Interconnects  
Azad Naeemi 
Georgia Institute of Technology 
791 Atlantic Dr. N.W. 
Atlanta, Georgia 30332 
Reza Sarvari 
Georgia Institute of Technology 
791 Atlantic Dr. N.W. 
Atlanta, Georgia 30332 
James D. Meindl 
Georgia Institute of Technology 
791 Atlantic Dr. N.W. 
Atlanta, Georgia 30332 
azad@ece.gatech.edu 
sarvari@ece.gatech.edu 
James.meindl@mirc.gatech.edu 
ABSTRACT 
Based on physical circuit models the performances of signal 
and power interconnects at the local, semi-global and global levels 
are modeled at 1000C. For local signal interconnects, replacing 
copper wires with a typical aspect ratio of 2 by thin SWNT 
interconnects can lower power dissipation by 50%. This would 
also improve their speed by up to 50% by the end of the ITRS. 
Copper wires and large diameter MWNTs offer the lowest 
resistance for power distribution 
in 
the first and second 
interconnect levels, respectively. SWNT-bundles and MWNTs can 
be used to lower the delay of signal interconnects in semi-global 
levels. MWNTs with diameters of 50nm and 100nm can 
potentially increase the bandwidth density of global interconnects 
by up to 50% and 100%, respectively.  
Categories and Subject Descriptors 
B.7.1 [Integrated Circuits]: Types and Design Styles 
General Terms-Design, Performance, Theory 
Keywords-Molecular electronics, quantum wires, inductance, 
crosstalk, 
repeaters, 
system analysis and design, system 
optimization  
1. INTRODUCTION 
Interconnects are going to be one of the major limitations for 
giga- and tera-scale electronic systems because of the delay they 
add to critical paths, the power they dissipate, the crosstalk noise 
and jitter they induce on one another, and also their vulnerability 
to electromigration. Interconnects constitute up to 70% of the total 
on-chip capacitance [1], and are the major source of power 
dissipation. To control the exacerbating problem of power 
dissipation that has become the main limitation for virtually all 
digital chips, it is critical to lower interconnect capacitance. The 
semiconductor industry has made major research and development 
investments in low-k dielectric materials in response to this 
urgency. However, low-k materials suffer from poor mechanical 
and thermal properties, and achieving dielectric constants below 2 
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
DAC’07, June 4–8, 2007, Anaheim, California, USA. 
Copyright 2007 ACM 978-1-59593-613-4/07/0003...$5.00. 
has proven to be very challenging [2].  
The RC delay of interconnects is also a major problem 
especially for semi-global and global interconnects. This can be 
exacerbated by size effects such as surface and grain boundary 
scatterings that may increase copper resistivity by up to 4 times 
for minimum size copper wires at the end of the ITRS. For highperformance chips in which the wiring pitch at each metal level 
can be optimized, size effects will have a limited impact on the 
design and performance of multi-level 
interconnects [3]. 
However, for low-cost applications, very few wiring pitches are 
normally used to avoid multiple recipes that are expensive to 
develop and maintain. Also, to minimize the number of metal 
levels, close to minimum size wires are normally used. Low-cost 
chips are hence going to suffer most from these size effects [4].   
Carbon nanotubes, both single- and multi-wall (SWNT and 
MWNT) are being investigated for a variety of nanoelectronic 
applications because of many unique properties they have [5]. 
Their extraordinary large electron mean free paths and resistance 
to electromigration have made them potential candidates for 
interconnects in gigascale systems [6-9]. A careful performance 
modeling for CNT interconnects is key to answer several 
important questions. It would enable quantifying the ultimate 
performance enhancement that CNT interconnects can potentially 
offer and help evaluate their viability as an alternative to 
copper/low k interconnects. It would also help identifying the 
most promising applications of CNT interconnects, and would 
provide guidelines on CNT technology development needed for 
various interconnect applications.   
This paper starts by a review of circuit models for single- and 
multi-wall CNTs and the impact of temperature on their 
resistances. Performances of CNT interconnects for signal and 
power distribution at the local, semi-global and global levels are 
then modeled and compared with those of copper interconnects. 
This is the first performance modeling for CNT interconnects that 
takes into account above-ambient temperatures on a chip, and 
looks at both signal and power interconnects. It is demonstrated 
that the simple replacement of copper wires with CNT-bundles of 
similar dimensions is going to offer a limited performance 
improvement for interconnects; therefore, fresh ideas are needed 
to take advantage of the novel properties of carbon nanotubes. 
Major performance improvements for both power and signal 
interconnects can be achieved by optimizing CNT interconnects 
at various levels and taking advantage of the fact that diameters 
of nanotubes can be controlled by chemistry. It is also shown that 
CNT interconnects are not likely to outperform copper wires for 
all purposes such as power distribution in the first interconnect 
level. A hybrid Cu/CNT system of interconnects would be 
therefore most desirable.  
568
  
 
 
 
 
 
 
2. CIRCUIT MODELS  
In this section, an equivalent circuit model is presented for a 
graphene cylinder that can be a SWNT or a shell in a MWNT.  
2.1 Number of Conduction Channels 
An ideal one dimensional conductor has one conduction 
channel, and it has a quantum conductance of G0=2e2/h=1/(12.9 
KΩ). Depending on the chirality of a graphene tube it can be 
metallic or semiconductor. In metallic tubes there are 2 subbands 
that pass the Fermi level, and in semiconductor tubes no subband 
crosses the Fermi level. The gap between the subbands in a 
graphene cylinder is inversely proportional to its diameter. For 
large enough diameters, the gap between the subbands becomes 
comparable to the thermal energy of electrons, and they occupy 
subbands according to the Fermi function. Assuming random 
chiralities, the average number of conduction channels for a shell 
can be approximated by [10] 
(cid:4)
N
chan shell
/
(
D aTD b D d T
)
/
2 / 3
D d T
/
where a is 2.04×10-4 nm-1K-1 , b is 0.425, and dT =1300 nm.K, 
whose values are determined by the thermal energy of electrons 
and the bandstructure of graphene tubes  [10].  
T
T
≈
≈
+
>
<
(1) 
2.2 Electron Scatterings 
Scatterings at the contacts give rise to the contact resistance 
that may be up to hundreds of Kilo Ohms if the contacts are poor. 
There have been many reports, however, indicating contact 
resistances in the order of a few Kilo Ohms or even hundreds of 
Ohms [11-13] that show that there should be no fundamental 
limitation in lowering contact resistance to the level that it can be 
ignored compared to the quantum resistance. 
Electrons also get scattered by defects and various types of 
phonons once their path length becomes comparable to or larger 
than the mean free path (MFP) associated with these scatterings. 
These scatterings are especially 
important 
in 
interconnect 
applications of carbon nanotubes since relatively long nanotubes 
are needed. The equivalent resistance of a conduction channel can 
be represented by the equivalent circuit in Figure 1 where [14] 
r
R
/
=(cid:65)
R T
/ 10
T D
is the resistance per unit length due to elastic scatterings by 
acoustic phonons [15]. In (2), R0 is the quantum resistance per 
conduction channel equal to 12.9KΩ. The resistance per unit 
length due to absorption of optical phonons is represented by 
≈
(
)
3
0
0
1
e
ac
≡
(2) 
R
0 exp(
−(cid:61)
K Tω
/
) /
abs
op
B
op
r
(cid:65) 
(3) 
where 
(cid:61)
0.18
eV
 is the energy of optical phonons, and 
D≈(cid:65)
20
 is the spontaneous scattering length for emitting an 
optical phonon for an electron with enough energy [15, 16]. The 
voltage dependent resistance per unit length rv =dV(x)/I0, where I0 
=25µA. The spontaneous scattering length for emitting an optical 
phonon for an electron with enough energy acquired either through 
electric field or phonon absorption is represented by a shunt 
resistance per unit 
length of rshunt=645Ω/D. For normal 
temperatures (<600 K) rshunt is more than 10 times larger than rabs, 
and can be ignored. It has been shown for interconnect application 
of nanotubes that the electric field is small, and rv has a small 
op
ω ≈
op
impact on the overall performance of circuits [17]. For 270 K< 
T< 420 K resistance per unit length can be approximated by a 
linear function with less than 10% error [14]: 
R
T
(
10
D T
Alternatively, one can use (4) and write the effective MFP as  
                                  (5) 
0
3
0
0
2
2),
100
abs
e
r
r
r
T
K
≡
+
≈
−
≡
(4)
3
0
10
T T
/
2
eff
D
=
−
(cid:65)
The scattering due to defects has not been considered here as 
the corresponding MFP in high-quality nanotubes is quite larger 
than the MFP of other scattering mechanisms [16].  
2.3 Kinetic and Magnetic Inductances 
Similar to any conductor, a graphene shell has a magnetic 
inductance that depends on its distance to its return path and 
other conductors. This magnetic inductance includes both self and 
mutual inductances. A quantum wire, however, has a small 
density of states that gives rise to another kind of inductance 
called kinetic inductance (lk in Fig. 1). It is proven that the kinetic 
inductance of a metallic SWNT with two conduction channels is 
4nH/µm [18]. The impedance associated with the kinetic 
inductance of a SWNT is smaller than the typical resistance of a 
SWNT unless the signal frequency rises to 200GHz. Since kinetic 
inductance and resistance scale proportionally with the number 
of nanotubes in a bundle, in virtually all practical purposes 
kinetic inductance can be ignored.   
2.4 Quantum and Electrostatic Capacitances  
To add electric charge to a quantum wire, one must add 
electrons to available states above the Fermi level (Pauli 
Exclusion Principle). A quantum capacitance can be defined in 
series with the electrostatic capacitance for each conduction 
channel [18]. This capacitance has a value in the order of 200 
aF/µm, and in virtually all cases it is larger than the electrostatic 
capacitance. In most practical cases it can therefore be ignored.  
3. SWNT-BUNDLES 
3.1 Conductivity 
It has been proven both theoretically and experimentally that 
the coupling between nanotubes in a bundle is weak [19, 20]. The 
resistance of a nanotube bundle is determined by the number of 
metallic nanotubes that are well connected and the quality of 
nanotubes and contacts. Resistivity of a SWNT-bundle is  
R
R
R
                    (6) 
(cid:65)
/
c
Q
Q
SWNT bundle
m
eff
n
L
σ −
⎛
⎜
⎜
⎝
⎞
⎟
⎟
⎠
+
=
+
RC1      R0 /2 
 dx  
   R0 /2  RC2
 re 
rshunt  
rv 
   lM        lk 
rshunt  
rabs 
rabs 
cQ  
cE 
Figure 1: The equivalent circuit model for conductance 
channel in a graphene tube as a SWNT or a shell in a 
MWNT.   
569
  
 
             
 
                         
 
                
 
                                    
   
Figure 2: Conductivity of Densely packed SWNT-bundles 
versus length. One third of nanotubes are assumed to be 
metallic, temperature is 1000C, and SWNTs are 0.34nm 
apart. Conductivities for bulk Cu and Cu lines with 
various dimensions are also shown (p=0.25, R=0.3). 
where nm is the number of metallic SWNTs per cross-sectional 
area of the bundle, Rc is the contact resistance, and RQ≡R0/2 since 
there are two conduction channels per every metallic SWNT. 
Statistically, 1/3 of SWNTs are metallic and the rest are 
semiconductor [5]. Conductivities of SWNT-bundles at 1000C are 
plotted versus length in Fig. 2 for various SWNT diameters. The 
spacing between nanotubes is considered to be 0.34nm [19].  
3.2 Capacitance and Inductance 
Ground, mutual and total capacitances per unit length of 
SWNT-bundles are plotted versus density of metallic nanotubes in 
Figure 3 using RAPHAEL field solver [21]. Two 10nm × 10nm 
bundles above a ground plane are modeled. The capacitances of 
perfectly smooth copper wires with the same cross-sectional 
dimensions are also shown. It is evident that SWNT-bundles have 
capacitances close to those of copper wires, and the change in 
capacitance as porosity increases is quite small. It, therefore, is 
safe to use the same capacitance values for copper and SWNTbundles unless SWNT-bundles are 
too porous. Likewise, 
inductance of SWNT-bundles can be considered to be same as 
those of copper wires [9, 22].     
4. MWNTS 
4.1 Conductivity of MWNTs 
The spacing between shells in a MWNT corresponds to the 
van-der-waals distance between graphene layers in graphite that 
with a good approximation is δ = 0.34 nm [5]. Total conductance 
of a MWNT is  
0
chan shell
/
0
(
D
L
/
(cid:65)
D
)
where 
(cid:65)
≡
(cid:65)
/
=
10 /(
T T
/
−
2)
. For Dmax > dT/T~4 nm, 
conductivity of a MWNT becomes equal to [10] 
)
G N
(cid:4)
(1
total
All shells
G
=
+
∑
(7) 
3
0
0
eff D
max
2
min
2
max
0
0
min
2
max
2
max 0
0
max
min
0
/
1
D
1
ln
aT
2
LG
2
totalG L A
L
(cid:65)
L
(cid:65)
D
D
D
D
D
aTL
(cid:65)
L
b
D
D
σ
≡
⎡
⎢
⎢
⎢
⎢
⎣
δ
⎤
⎥
⎥
⎥
⎥
⎦
⎛
⎜
⎞ ⎛
⎜
⎟ ⎜
⎠ ⎝
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠
+
⎛
⎜
⎝
⎞
⎟
⎠
⎛
⎜
⎝
⎞
⎟
⎠
=
−
+
−
−
−
+
(cid:65)
(8) 
For L < (cid:65)0b/aT conductivity decreases as diameter increases 
whereas for L > (cid:65)0b/aT it increases with diameter. This is an 
important point as it highlights the need for nanotubes with 
smallest possible diameters 
for short 
interconnects and 
nanotubes with largest possible diameters for long interconnects. 
For Dmax< dT/T~4 nm,  
max
0
0
min
2
max
2
max 0
max
min
0
1
D
ln
LG
3
L
(cid:65)
L
(cid:65)
that always increases as diameter decreases regardless of the 
length. Conductivities of MWNTs and SWNT-bundles at 1000C 
are plotted in Figure 4 versus length for various outer diameters. 
D
D
D
L
D
D
σ
δ
⎛
⎜
⎜
⎜
⎜
⎝
⎞
⎟
⎟
⎟
⎟
⎠
+
⎛
⎜
⎝
⎞
⎟
⎠
=
−
−
+
(cid:65)
  (9) 
4.2 Temperature Coefficient of Resistance  
Using the models presented in Section 2 for electron phonon 
scatterings and the number of conduction channels in a graphene 
shell, the TCR of CNTs with various diameters is plotted in     
Fig. 5 [14].  The results show that the TCR may vary from 
negative to positive values depending on the lengths and 
T=1000C 
Figure 3: Capacitance per unit length for SWNT-bundles 
versus density of metallic SWNTs. Solid points show the 
capacitance per unit length values for ideally smooth 
copper wires with the same cross-sectional dimensions. All 
cross-sectional dimensions 
(W=T=H=S) are chosen 
arbitrarily to be 10 nm for the ease of simulations. 
Figure 4: Conductivity of MWCNs with various 
diameters and bundles of densely-packed SWNTs 
versus length. SWNTs have random chiralities and are 
0.34nm apart.  
T=1000C 
570
  
 
 
                  
 
 
            
     
chan shell
/
diameters of nanotubes. For single- or few-wall nanotubes, shell 
diameter is small and 
N(cid:4)
 is independent of T as (1) shows. 
The TCR therefore starts from zero for very short ballistic 
SWNTs, increases with length, and asymptotically reaches its 
maximum value for lengths considerably larger than the MFP.  
For larger diameters, the number of conduction channels per 
shell increases as temperature rises which counteracts the decrease 
in MFP. For short lengths, the number of conduction channels is 
the dominant parameter determining the overall resistance, and 
increasing temperature lowers resistance. For long MWNTs, the 
MFP is more important, and resistance increases with temperature.  
5. CNT INTERCONNECTS 
5.1 Local Interconnects 
One of the major challenges for interconnects at the local 
levels is the contradicting requirements of signal and power 
interconnects. On the one hand, capacitance, not resistance, is the 
main concern for local signal interconnects. This is because a 
minimum-size interconnect must be roughly several hundred gate 
pitches long so that its resistance becomes comparable to that of its 
driver whereas it can have a capacitance larger than a typical logic 
gate if it is only ten or 20 gate pitches long [23]. Designers would 
like to use small aspect ratios for these signal interconnects to 
lower the lateral capacitances between adjacent interconnects and 
to minimize delay, crosstalk, power dissipation and also the 
dynamic delay variation due to various switching patterns. On the 
other hand, power interconnects carry large currents. Designers, 
therefore, would like to have large thicknesses to minimize their 
resistance and current density, and improve their immunity to 
power supply noise and electromigration.  
Signal and power interconnects are both routed in the first few 
metal levels, and they have to have the same thicknesses. This 
imposes major penalties in terms of delay, power dissipation and 
crosstalk for signal interconnects and in terms of wiring area for 
power interconnects. The thickness variation caused by chemical 
mechanical polishing (CMP) exacerbates the problem because 
even larger nominal thicknesses need to be used to compensate for 
the variation and to avoid low yield.  
Carbon nanotube technology may potentially revolutionize 
interconnect technology by allowing thickness optimization for 
each individual interconnect based on its functionality and 
requirements. Diameter of CNTs is determined by chemistry, i.e. 
×10-3 
,
e
c
n
a
t
s
i
s
e
)
R
K
/
f
(
1
o
t
n
T
e
∂
i
/
)
i
f
f
c
K
e
o
@
C
R
R
e
r
/
0
5
3
u
∂
t
(
a
r
e
p
m
e
T
the type of the catalyst material used for their growth, not the 
metal deposition time or CMP. By using thin SWNT signal 
interconnects one can lower the lateral capacitance between 
signal interconnects by a factor of 4 and lower power dissipation 
by 2 times and improve the speed of local interconnects by up to 
50% at the end of the ITRS as Fig. 6 shows [22].  
In the first metal level, nanotubes as short as one gate pitch 
would be needed to deliver power supply and ground to every 
gate. For such short lengths, copper wires are a better option as 
they offer a lower resistance as Fig. 3 shows. Since there would 
be no conflict of interests, thick copper power wires can be used 
with no penalty in power or delay for signal interconnects. In the 
second interconnect level, the wide power interconnects can be 
split into parallel sets of staggered MWNTs where each set is 
connected to every few power interconnects in the first level. In 
this manner, MWNTs as long as 280F can be used to distribute 
power in M2. [24]. The sheet resistances of copper and MWNTs 
with diameters of 50nm and 100nm are plotted versus technology 
generation in Fig. 7. At the end of the ITRS, MWNTs 100nm in 
diameter would offer a 3 times smaller sheet resistance compared 
to copper wires with a thickness of twice minimum feature size 
(2F). In practice, the advantage offered by MWNTs may be even 
larger than what Fig. 7 shows because wide copper wires are 
quite vulnerable to dishing caused by CMP, and their effective 
thickness is going to be smaller than their nominal value.   
5.2 Semi-global Interconnects 
Semi-global 
interconnects are 
longer 
than 
the 
local 
interconnects, and their delay is mainly determined by their RC 
product because of which they are normally routed in the metal 
levels above the local levels with cross-sectional dimensions 
larger than the minimum feature sizes. In some cases repeaters 
are also used to improve their speed as their delays become 
proportional to the square root of their RC products.   
SWNT-bundles or MWNTs can replace copper wires in the 
semi-global levels, and improve their latency because of their 
smaller resistivities. While for 
local 
interconnects contact 
resistance 
is 
important, semi-global 
interconnects are 
less 
sensitive to it as they are typically many mean free paths long.  
The improvement in interconnect RC product that CNTs may 
offer are plotted 
in Fig. 8 versus wire width assuming 
interconnects are long enough that the contact and quantum 
resistances can be ignored (
R
R
R L
). Density of 
(cid:65)
+
<
/
c
Q
Q
eff
RQ+Rc=10 KΩ 
1/3 Metallic 
T=1000C 
Nanotube Length, L (µm) 
Figure 5: The TCR at 350K versus length for a SWNT 
with a diameter of 1nm, MWNTs with various diameters 
and copper wires.  
Figure 6: The speed up offered by bi-layer SWCN 
interconnects versus length in gate pitch for various 
technology generations. The worst-case delay 
is 
considered, which happens when a signal line switches 
in opposite direction of its neighbors. 
571
  
 
 
 
 
 
 
 
 
 
)
(cid:0)
T=1000C 
/
Ω
(
T=1000C 
Figure 7: Sheet resistances of copper wires and 50nm and 
100 nm diameter MWNTs power interconnects versus 
technology year. MWNTs are assumed to be routed as 
staggered sets with each supply every 5 power rails in the 
first metal level (length-280F). For reference, sheet 
resistance of ideal copper wires with no size effects is also 
metallic SWNTs is the key parameter determining the speed 
enhancement that SWNT-bundles may offer. For minimum-size 
wires at the end of the ITRS, the RC product of semi-global 
interconnects can be improved by 2.5 and 7.5 times if metallic 
densities as high as 1/(4.5nm2) and (1/1.5nm2) can be achieved, 
respectively. The former density corresponds to a densely-packed 
bundle of SWNTs with random chiralities, and the latter 
corresponds to an all-metallic densely-packed SWNT-bundle. 
Low-cost ASIC chips normally use close to minimum feature size 
wire widths for the majority of metal levels to minimize the 
number of levels. Those designs would benefit most from 
replacing copper wires with SWNT-bundles.     
5.3 Global Interconnects 
Replacing copper wires with SWNT-bundles increases the 
conductivity; hence, lowers the delay. However, since global 
interconnects normally have large cross-sectional dimensions they 
often operate in the shallow RLC region. A large reduction in their 
resistance can push them deep into the RLC region where delay 
decreases slowly with lowering resistance. Furthermore, crosstalk 
noise can become very large as mutual inductance is a far reaching 
effect, and many aggressors can affect a victim line [25].  
Global interconnects are typically data busses between 
macrocells consisting of several hundred thousand to millions of 
gates. This allows greater flexibility in the design of the intermacrocell global interconnects. The optimal wire width is defined 
as the width at which bandwidth per unit width-reciprocal latency 
product is maximized [25]. Assuming optimal repeater insertion, 
the optimal wire width becomes proportional to the square root of 
resistivity. By using SWNT- or MWNT-bundles one can lower the 
optimal wire width and improve bandwidth density without 
entering the deep RLC region. The improvement in bandwidth 
density that SWNT-bundles may offer is plotted versus density of 
metallic nanotubes in Fig. 9. It is evident that there needs to be 
more than 1 metallic SWNT per 3nm2 cross-sectional area 
(nm=0.3nm-2) to improve the bandwidth density. Such large density 
is needed because global copper wires have large dimensions and 
their resistivity is close to bulk resistivity. MWNTs with diameters 
larger than 20nm can improve bandwidth density (Fig. 9). 
SWNT-bundles or MWNTs can be used in power distribution 
grids at the global levels if grid pitches are large enough. It is 
shown that SWNT-bundles can offer resistances smaller than those 
572
Figure 8: The relative RC delays of copper and CNT 
interconnects versus interconnect width. SWNT-bundles 
with two densities of metallic nanotubes and MWNTs 
with various diameters are considered.  
of copper wires in global power grids if the density of metallic 
SWNTs is larger than 1/(2.5nm2) [24]. MWNTs can outperform 
copper wires only if segment lengths larger than 20µm are 
needed, which would limit their use for power distribution at the 
global levels [24].      
6. CONCLUSIONS 
Based on physical circuit models the performances of signal 
and power interconnects at the local, semi-global and global 
levels are modeled assuming an operating temperature of 1000C. 
It is demonstrated that fresh ideas are needed to take advantage of 
the novel properties of CNT interconnects. At the local level, the 
key advantage offered by CNT interconnects is their potential in 
enabling independent optimization of the thicknesses of power 
and signal interconnects in the same interconnect levels. This 
would allow using thin SWNTs for short signal interconnects 
next to thick copper or MWNTs for power distribution. Thin 
SWNT interconnects offer 4× smaller lateral capacitances, 2.7× 
smaller worst-case capacitances, and 2× smaller average 
capacitances compared to copper wires with a typical aspect ratio 
of 2. By the end of the ITRS, the worst-case speed of local short 
interconnects can improve by up to 50% and lower their dynamic 
power dissipation by half if bi-layer SWNTs replace copper wires 
with an aspect ratio of 2. By the end of the ITRS, MWNTs 
T=1000C 
Figure 9:  Relative bandwidth densities of SWNT-bundles 
and copper interconnects versus the number of metallic 
SWNTs per unit cross-sectional area. The ratios of 
bandwidth densities of MWNT with various diameters 
and copper interconnects are also shown. 
  
 
 
100nm in diameter can offer up to 3× smaller sheet resistances for 
power distribution in the second interconnect level where they can 
be made long enough (280F). In the first level, thick copper wires 
are the best option for power distribution because resistivity of 
nanotubes so short is not likely to be less than that of Cu wires.  
At the semi-global levels, bundles of SWNTs and MWNTs can 
improve the RC product of interconnects. Density of metallic 
SWNTs is the key parameter determining the speed enhancement 
that SWNT-bundles may offer. For minimum size wires at the end 
of the ITRS, the RC product of semi-global interconnects can be 
improved by 2.5 and 7.5 times if metallic densities as high as 
1/(4.5nm2) and (1/1.5nm2) can be achieved, respectively. The 
former density corresponds to a densely-packed bundle of SWNTs 
with random chiralities, and the latter corresponds to an allmetallic densely-packed SWNT-bundle. For global interconnects 
MWNTs can offer up to 2 times improvement in bandwidth 
density with no penalties in delay or energy per bit.     
7. ACKNOWLEDGMENTS 
This work has been supported by the Interconnect Focus Center 
Research Program at Georgia Tech under MARCO contract 2003IT-674 and the SRC Center for Advanced Interconnect Science 
and Technology Program at the State University of New York at 
Albany (SUNY) under contract 1292.018. 
8. "
"Micro-Photonic Interconnects - Characteristics, Possibilities and Limitations.","Photonic interconnects have the long-term potential to reduce latency and crosstalk while increasing signalling bandwidth. This tutorial-style presentation for non-experts will provide a simple overview of photonics as it relates to the interconnect problem. We will touch upon the challenges faced by on-chip and chip-to-chip communications that motivate micro-photonic (optical) interconnects, and provide an introduction to the basics of optics. Topics such as the system impact of optical interconnects, impact on cache, chip-to-chip communication and global clock distribution will be examined. Practical feasibility issues and limitations related to power, speed, and technology hurdles will be evaluated. The talk will conclude with a look at some fascinating recent developments that bode well for the future of micro-photonic interconnects.","32.3
Micro-Photonic Interconnects: Characteristics,
Possibilities and Limitations
Jaijeet Roychowdhury
University of Minnesota, Twin Cities
jr@umn.edu
ABSTRACT
Photonic interconnects have the long-term potential to reduce latency and crosstalk while increasing signalling bandwidth. This
tutorial-style presentation for non-experts will provide a simple overview
of photonics as it relates to the interconnect problem. We will touch
upon the challenges faced by on-chip and chip-to-chip communications that motivate micro-photonic (optical) interconnects, and
provide an introduction to the basics of optics. Topics such as the
system impact of optical interconnects, impact on cache, chip-tochip communication and global clock distribution will be examined. Practical feasibility issues and limitations related to power,
speed, and technology hurdles will be evaluated. The talk will
conclude with a look at some fascinating recent developments that
bode well for the future of micro-photonic interconnects.
Categories and Subject Descriptors
B.4 [Interconnections]: Subsystems
General Terms
Design, Performance, Reliability
Keywords
Optics, Interconnect, Photonics
It is well recognized today that delays, crosstalk, dielectric losses,
and electromagnetic interference (EMI) limit the applicability of
copper interconnect as signalling speeds rise. Already, for chip-tochip bus connections, Cu faces the 1GHz parallel bus limit, prompting a move to high-speed serial-based links such as HyperTransport
and PCI Express. Moreover, dielectric losses and transmission line
reﬂections are expected to severely limit the usefulness of traditional Cu interconnect at speeds of around 10GHz, the so-called
10GHz Cu signalling limit. There is considerable interest, therefore, in alternative technologies that circumvent this limit.
In this context, optical ﬁber based interconnection technologies
have been a focus of renewed attention. Optical ﬁbers have, of
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007, June 4–8, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2007 San Diego, CA, USA
Copyright 2007 ACM 978-1-59593-627-1/07/00065.00 ...$5.00.
Figure 1: Optical ﬁber bundle (courtesy
Wikipedia).
course, been deployed extensively for several decades now for longhaul (intra/inter continental distances), metropolitan area and shorthaul data communications. Using ﬁbers for communication between chips on a board, or even on-chip, has many attractions but
also raises a number of questions regarding scientiﬁc, technical and
economic feasibility.
There are many potential advantages that
ﬁber optic links have
over Cu interconnect.
The primary one is their
enormous intrinsic data
bandwidth – even over
distances orders of magnitude larger than chip
and board scale, and
using only simple onoff modulation schemes,
raw speeds of 40Gbps
are easy to achieve with
ﬁbers. Another key
attraction is their relative immunity to electrical inﬂuences, i.e.,
crosstalk and interference due to parasitic capacitances and inductances. It is even possible to physically intersect light beams with
minimal crosstalk, a feature of interest for dramatically simplifying
routing and placement.
However, for photonic interconnect to be a practical and useful
solution in the chip-to-chip and on-chip domain, a number of factors need to be examined critically. Firstly, the present state of technology – not only for the interconnects themselves, but also for integrable light sources, integrated photonic detectors, and electricalto-optical/optical-to-electrical converter circuits – needs to be understood. Vertical Cavity Surface Emitting Lasers (VCSELs) are
the most feasible light source technology today for integrated applications, yet hurdles such as process incompatibilities, mechanical alignment problems and high power consumption remain. Integrating optical interconnections presents process and bend-angle
related challenges. The relatively high power consumed per bit per
second by VCSEL driver circuitry severely constrains the ubiquitous deployment of optical interconnect links. The wavelengths of
technologically and economically feasible integrated lasers (hence
the feature sizes of integrated optical components and systems) are
much larger than the nanoscale features in common use in today’s
commercial processes.
However, fundamentally promising technologies that may appear to be impractical or of limited utility at any given point in
time often do not remain so, typically due to a mix of many incremental developments and a few paradigm-changing breakthroughs.
574
Some recent developments that may fall in the latter category will
be surveyed in this talk.
An enabling technology that offers considerable promise for integrated optical interconnect is photonic crystals, also known as holey ﬁber. Patterned interconnect using photonic crystalline operational principles do not suffer from many of the process-related limitations of conventional photonic interconnect. Furthermore, they
offer sharp, wavelength-scale bend angles, as well as the feature
of physically intersecting optical interconnect that do not interfere.
The light-trapping properties of photonic crystals have a host of
other uses as well, including the fabrication of small cavities for
tiny on-chip lasers.
Making photonics co-exist with silicon-based technologies has
been a long-standing problem that has held back true integration
with modern VLSI circuitry. Recently, however, several breakthroughs have been reported in this area. Electro-optical converters
(Mach-Zehnder modulators) that are integrable in silicon technologies have been demonstrated, as have Raman-effect based (optically pumped) integrated lasers in silicon. An exciting recent development has been the demonstration of mass-producible hybrid,
electrically tunable lasers in wafer-bonded Si/InP technology. With
these advances and the promise of more to come, it may well be
that optical interconnect will evolve to become a practical solution
when signalling speeds rise to 10Gbps and above.
575
"
CAD Implications of New Interconnect Technologies.,"This paper looks at the CAD implications of possible new interconnect technologies. We consider three technologies in particular: three dimensional ICs, carbon nanotubes as a replacement for metal interconnects, and optical interconnections for longer range on-chip communication. Each of these requires new CAD support to be used effectively.","CAD Implications of New Interconnect Technologies
32.4
Louis K. Scheffer
Cadence
555 River Oaks Parkway
San Jose, CA
lou@cadence.com
ABSTRACT
This paper looks at the CAD implications of possible new
interconnect technologies. We consider three technologies
in particular: three dimensional ICs, carbon nanotubes as
a replacement for metal interconnects, and optical interconnections for longer range on-chip communication. Each of
these requires new CAD support to be used eﬀectively.
Categories and Subject Descriptors
B.7.1 [Integrated Circuits]: Types and Design Styles—
Advanced technologies, VLSI ; B.7.2 [Integrated Circuits]:
Design Aids—Placement and routing
General Terms
Algorithms, Design, Performance
Keywords
3D interconnect, Nanotubes, On-chip optical
1.
INTRODUCTION
Since interconnect is one of the main performance determinants of modern ICs, there are many proposals to improve
it. As of early 2007, some of the main proposals are
• 3D Interconnect
• Carbon nanotubes to replace wires and/or vias
• Optical interconnect for long on-chip connections
Each of these requires CAD changes to be eﬀective.
2. 3D CHIP CONSTRUCTION
Existing IC technology has a single surface for active devices, and a number of interconnect layers above it. If more
layers of active devices can be added, there is the possibility of much shorter connections. This could save power,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
Permission to make digital or hard copies of all or part of this work for
not made or distributed for proﬁt or commercial advantage and that copies
personal or classroom use is granted without fee provided that copies are
bear this notice and the full citation on the ﬁrst page. To copy otherwise, or
not made or distributed for proﬁt or commercial advantage and that copies
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
permission and/or a fee.
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
DAC 2007, June 4–8, 2007, San Diego, California, USA.
permission and/or a fee.
Copyright 2007 ACM 978-1-59593-627-1/07/0006 ...$5.00.
DAC 2007 June 4–8, 2007,San Diego, California, USA.
Copyright 2007 ACM ACM 978-1-59593-627-1/07/0006 ...$5.00.
increase speed, and add to density. Chips built of many different technologies could be combined, each optimized for a
function such as RF circuits or memories. On chip busses
might be much shorter, leading to higher bandwidths between processors and memory. There have been many studies of these and other beneﬁts that might be gained by such
technology. See for example [1][2][3][4].
3D chips may potentially be constructed in many ways.
Conventional chips can be stacked, or new techniques can
possibly grow active layers on top of existing interconnect.
In the terminology of 3D ICs, each layer is called a tier.
There are also many alternatives for CAD tools for 3D.
One possibility is to do 3D CAD as an analog of the existing 2D CAD - this could include 3D placement[5], routing[5],
congestion estimation[6], partitioning, FPGA tools[7], and
so on. Many of the needed algorithms are relatively straightforward extensions of existing 2D algorithms - placement by
many means (min-cut, analytic, force-directed, and simulated annealing) generalizes well to 3D 1 . Similarly, global
routing in 3D should be quite similar to 2D since global route
that includes layer assignment is already working with a 3D
structure. Detailed routing is currently unclear since there
is no consensus on the vertical cross-tier via structure, and
hence no-one has built a detailed router for this case yet.
However, no serious obstacles are apparent.
One important diﬀerence in 3D is that thermal issues become much more important. 2D chips are built on thin substrates, and the thermal path to the heatsink is therefore
short. In 3D, the heat generated in the interior tiers must
travel through the other tiers and interconnect to escape.
Therefore 3D algorithms, particularly placement, must be
sensitive to thermal concerns, and optimize them during design. Power distribution also becomes more diﬃcult, since
power can only be supplied to the outside of the stack, and
power driven placement may be more important as well.
From an industrial perspective, (in the author’s opinion)
full 3D design seems likely only for FPGAs. For ASICs,
only the partitioning and thermal analysis will be done in
3D. After this, conventional 2D tools will be used to design
each tier. There are several practical reasons in favor of this
approach:
• There is a large infrastructure for 2D design, and 2D
design is likely to persist for quite a while. Unless 3D
catches on in a big way (unlikely because of cost) it
won’t be economical to build 3D speciﬁc tools. One
previous problem, that 2D tools were not set up for
1There is a gold mine of potential PhD theses here.
576
576
IOs over the whole surface of the chip, is now addressed
fairly well since ﬂip-chip shares this characteristic with
3D.
• Designers currently partition 2D designs anyway, just
to get a handle on complexity, reduce interaction between groups, hit the sweet spot of DA tools, include
IP, and so on. As long as the designer is partitioning,
and cross-tier connections are expensive, then these
make good partitioning boundaries, too.
• If the design is partitioned into tiers, most engineering
changes (ECOs) will take place within a single tier. If
the design is fully 3D, ECOs could very well be spread
across many tiers, resulting in a much higher cost for
mask changes.
• IP will mostly be available in 2D form. IP providers
will not want to limit their market to those advanced
customers building 3D chips, and so will provide 2D
blocks.
Once a 3D IC is designed, it will need extraction, which
takes the speciﬁed geometries and computes the corresponding resistors, capacitors and inductors for the interconnections. In 3D, resistance and capacitance extraction will be
little changed, or at most a straightforward extrapolation of
current 2D techniques. This is because resistance and capacitance are local eﬀects, so within-tier and between-tier
components can be extracted independently.
Inductance
calculation, however, will be considerably changed, since inductance depends strongly on the return path for the signal
current. The return path may be quite far from the signal under consideration, and determining the return path
is a diﬃcult problem. Existing extractors use a number of
heuristics (such as [8]), which have been tuned for existing 2D chip architectures. In 3D the problem will be more
complex, since conductors on the adjacent tier may or may
not contribute to the return path, depending on the connections. At the very least, new heuristics will be needed, and
where there is signiﬁcant current ﬂow through the betweentier connections, a full 3D approach to inductance may be
required.
3. NANOTUBES REPLACING WIRES
Carbon nanotubes are one of the few materials that are
better electrical conductors than copper. They are potentially better in several ways - some nanotubes have higher
conductivity than copper, all are much more resistant to
electromigration, and some are more thermally conductive
than copper. See [9][10][11] for discussion of the electrical
properties of nanotubes. This naturally leads to the suggestion of replacing copper or aluminum conductors with
conductors made of nanotubes [12][13][14]. However, the
increased conductivity comes with a number of limitations.
First, it is only along the length of the tube, not across it.
This is not a serious problem for chip design, where the direction of current ﬂow is well known, though it may well be
a diﬃcult manufacturing problem.
One of the nice points of nanotubes is that they do not
suﬀer from electromigration, for all practical purposes. The
atoms are tightly bound into a lattice, and the electron wind
cannot move any of them. Also there are no grain boundaries. Carbon nano-tubes will melt before they suﬀer from
electromigration.
577
Figure 1: A multi-wall nanotube (MWNT) consists
of concentric nested nanotubes, as shown in this diagam, courtesy of A.V. Krasheninnikov. Single-wall
nanotube (SWNT) interconnects consist of parallel bundles of the smallest diameter nanotubes, like
those in the center of this MWNT.
Electrical conductivity in nanotubes is more complex than
conductivity in metals. The resistance of a nanotube is
not a linear function of its length - instead the resistance
is constant up to a certain length, then increases linearly
thereafter. The minimum resistance, present even for very
short nanotubes, is deﬁned by the electron states available
in its 1-D conductor. It appears half at each end, where the
1D conductor meets the 3D electron sea in the contact. In
the nanotube itself, there is no additional resistance (electron transport is ballistic) for lengths up to (roughly) the
mean free path. This mean free path is proportional to the
nanotube diameter, and is about 1.6µ for the smallest diameter tubes. Above this length, the resistance increases
linearly[15]. Furthermore, the temperature coeﬃcient of a
nanotube conductor can be either sign - higher temperatures
reduce the mean free path, increasing resistance, but also
can promote (in multi-wall tubes) more electrons into the
conduction band, reducing resistance. Finally, nanotubes
built with current techniques can be of one of three chiralities (basically where you end up after traversing around the
cylinder following the atoms). Only one of these conducts
- the other two are semiconductors, with very poor conduction compared to wires. The comparisons below all assume
random construction. If techniques for manufacturing only
the metallic type nanotubes can be perfected, the results
below could improve by a factor of 3. Although we have
no idea how to do this now, we can hope since nanotubes
are constructed by catalysts, and biological catalysts often
exhibit this type of speciﬁcity.
Given all this, how do nanotube conductors compare to
copper? This depends on both the width and length of the
wire. Copper conductors become worse at smaller widths,
from scattering oﬀ the grain boundaries and rough edges,
and from the higher percentage of low-conductivity cladding.
Even so, for short lengths where the unavoidable quantum
R dominates, a wire made of single-wall nanotubes is always
a worse conductor than a copper wire of the same cross section. As the length increases, a bundle of single wall nanotubes becomes slightly better than an equivalent area of
narrow copper, but still somewhat worse than bulk copper.
Figure 2: Long single wall carbon nanotubes are
better conductors than narrow copper wires of the
same area, but worse than wide wires or bulk copper. Very short wires are worse since contact resistance dominates. From [16].
Figure 3: Large diameter carbon nanotubes are better conductors than bulk copper if they are long
enough and big enough diameter. This is because
the mean free path is directly proportional to the
diameter of the tube. From [16].
This is shown in Fig. 2. Multi-wall carbon nanotubes are
even worse at short lengths than single wall nanotubes. This
is because there are fewer of them per unit area, so the unavoidable contact R hurts more. However, as the wire gets
longer, the ballistic transport dominates, especially in large
diameter tubes with their long mean free paths. A long
enough length of a large diameter nanotube may be many
times as conductive as copper. This is shown in Fig. 3.
Since conduction through nanotubes consists of relatively
few electrons travelling far and fast, there is another term
in the inductance, called kinetic inductance. This is due
(loosely) to the kinetic energy of the electrons[17]. Unlike a
conventional inductance, it does not depend on the distance
to a return path, and decreases linearly with the number of
parallel nanotubes. This eﬀect occurs in normal wires too,
but is negligable compared to the classical inductance from
the induced magnetic ﬁeld. In a single nanotube, the kinetic
inductance is the dominant one, to the point where only a
parallel bundle of nano-tubes makes any sense. Fortunately
nanotubes are thin, and so generally will be used in parallel
bundles, even at 22nm or 10nm technologies. In these cases
the kinetic inductance can in general be neglected[16].
Finally, the surface of the nanotube is built diﬀerently
than a metal surface, leading to slight diﬀerences in capacitance (a few percent).
With all of these eﬀects, plus diﬃculties in manufacturing, it is not currently clear if nanotubes will be a practical
alternative. See [18] for a pessimistic view, and [19] for an
optimistic view.
There are at least three ways nano-tubes may be used in
IC design. For short interconnects, a conventional copper interconnect with high aspect ratio can be replaced by a thin
sheet of nanotubes, as shown in Fig 4. This reduces the capacitance of wires, particularly the lateral capacitance. This
in turn implies better performance, less unwanted coupling,
and lower power dissipation. Such thin conductors cannot
be used with copper for two reasons - it is hard to fabricate
such shapes because of CMP problems, and even if they
could be built they would rapidly fail due to electromigration. Nanotubes may be capable of being grown as a thin
layer (and hence not need CMP) and do not suﬀer from electromigration, for all practical purposes. The capacitance of
a thin nanowire ribbon will be much lower than that of the
copper wire, perhaps as little as 30% as much, depending on
the aspect ratio of the copper. On the other hand, for such
thin conductors the resistance of nanotubes will be higher
than that of traditional thick copper, but short interconnects
(up to 100 gate pitches or so) are not dominated by wire R,
but by driver R, and gate and interconnect C. Therefore replacing short copper wires by a thin layer of nanotubes could
provide both lower power dissipation and increased performance[20]. Since a large fraction of power is dissipated due
to the capacitance of local nets (50% in the example of Intel
in [21]), this could help the system power considerably even
if restricted to local nets only. For longer lengths, the increased R becomes more important, and the thin nanowires
become slower than conventional copper wires, as shown in
Fig. 5.
The second way nano-tubes might be used is to replace
longer wires with better electrical conductors. In this cause
multi-walled nanotubes must be used, since only these nanotubes are better conductors than bulk copper[22]. This
means they will be fairly thick, and have a similar capacitance to a copper wire, but lower resistance. They may be
particularly useful as power supply wires, due to their lower
resistance and lack of electromigration problems.
Finally, vertical bundles of nano-tubes may be used to
replace vias. This has two advantages - they are essentially
immune to electromigration, and they are excellent thermal
conductors. This reduces the temperature gradients through
the interconnect stack. [23][24][25]. This might go very well
578
Figure 4: Proposed use of nanotubes to replace copper wires. From presentation associated with [16].
with the 3D interconnections above, which can utilize higher
vertical thermal conductivity.
3.1 CAD changes for nanotubes
The constant contact resistance of nanotubes means the
calculation of the resistance of a connection must change
and become length dependent. Also, since the length dependence of nanotube R depends on the mean free path,
this will need to be speciﬁed. Since the mix of nanotubes
used by each wiring layer may be diﬀerent, it will need to
be a parameter supplied by the foundry or user.
Nanotubes cannot turn tight corners, so at every corner
there will be a block of metal, and 2 metal-nanotube contacts. This will aﬀect the routers, probably resulting in
one layer of purely vertical wires and one for purely horizontal wires. This resembles a proposal for restricted design rules[26], which proposes this same restriction to make
lithography easier. Electromigration checking will also need
to change - through the nanotubes themselves are virtually
unaﬀected by electromigration, the normal metal used to
make the contacts can still deteriorate. New models and
new algorithms will be needed to check this.
Layer assignment may also be aﬀected.
If thin sheets
of nanotubes replace the lower layers, they will have much
higher resistance than the copper wires they might replace.
We may need to resurrect old algorithms from the days of
polysilicon that are very careful to route only short wires in
these lower metal layers. Alternatively, timing driven routing might accomplish the same ob jectives, but it would need
to be tested for use with such a wide range of resistances.
Finally, the impact and forms of process variation may
diﬀer for nanotube interconnect[27]. This must be taken
into account in any statistical analysis.
4. ON-CHIP OPTICAL INTERCONNECT
Optical interconnect has promise for long distance connections within a chip. The main disadvantage is that it requires electrical-optical conversion at the sender, then opticalelectrical conversion at the receiver. The advantages are
that a much greater distance can be covered without repeaters, and that many signals can share a route with wavelength division multiplexing, and that optical is very promising for inter-chip and inter-board communication as well.
579
Figure 5: Range of lengths at which a thin nanowire
ribbon is faster than a copper wire of conventional
thickness. From [16].
Optical signals can also cross without interacting. Optical
transmitters can be direct, where the transmitter generates
the light directly, or indirect, where the transmitter modulates an external light source. See [28] or [29] for overviews
of possible on-chip optical technology, and [30] for a comparison with copper interconnect.
The length of connection where optical makes sense is a
strong function of optical technology. The electrical technology is by comparison quite static. For example, the capacitance of a wire is roughly 200 ﬀ/mm, almost independent of scaling. The power P required to switch a wire is
P = C V 2 f , where C is the capacitance, V is the voltage
swing, and f is the frequency. For a 1V, 1GHz signal, for
example, this gives 0.18 mw/mm for the wire alone. Including the necessary repeaters might double this, giving 0.36
mw/mm. So if an optical transmitter/receiver pair takes
3.6mw, this starts to look good at 10mm. This break-even
distance will be inversely proportional to the number of transitions, so the activity of a signal must be considered as well
as its length.
Optical transmitters, receivers, and waveguides also take
up a fair amount of real estate. Direct transmitters require a
large buﬀer chain to generate the laser drive current, and indirect transmitters are larger yet - the smallest one reported
as of early 2007 was about 40 microns long. Receivers are
also large, also needing signiﬁcant amplifying circuitry. Any
speciﬁc ﬁgures for the sizes will need to be obtained from
the current technology - this is an area of much active research. On the other hand, optical waveguide cannot be
much narrower than a wavelength, or about 1µ (1000nm
wide), and this is unlikely to improve. However, by using
diﬀerent wavelengths, many signals can travel in the same
connection without interference. Some studies indicate this
is needed if optical is to achieve the same bandwidth density
as copper interconnections[31].
4.1 CAD changes for on-chip optical
Since the optical components needed to launch or receive
5. PROCESS VARIATION/CORRELATION
All these new technologies will probably share at least
one characteristic with existing methods - process variation.
This means that the constructed interconnects will not all
be exactly the same, and not exactly as desired. In addition,
the variations from nominal will almost surely be correlated,
with nearby components much more alike than components
spaced far away on the chip.
This problem is not well dealt with today, even with the
existing metal interconnects. The fab typically only states
a range of variation for identical wires in identical contexts
within a short distance of each other. Much more general
models of correlation are needed, that can ﬁnd the correlation between (for example) wires of diﬀerent widths on the
same layer, wires on diﬀerent layers, and wires far apart on
the chip. There have been many statistical timing papers
on dealing with correlation, but without data from the fabs
it is hard to get much traction on this issue.
Since these new interconnect technologies are not yet well
speciﬁed, we can only hope that the solution that is ﬁnally
adopted for the traditional metal interconnnect will apply
to these new interconnects.
6. CONCLUSIONS
There are several new technologies that have the potential to change the way we deal with interconnect on chips.
Three of the most likely are 3D chip construction, carbon
nano-tube interconnect, and optical interconnect. Each of
these requires CAD support before the advantages can be
used. Fortunately this does not look terribly diﬃcult. Nanotube interconnect is the easiest, where changes to electromigration checking and resistance calculations suﬃce. 3D is
probably the next hardest, though the considerable research
that has gone into 2D can help. The main new feature will
be mandatory thermal analysis and improved power distribution analysis. Finally optical interconnect would stress
CAD tools the most. Widespread adoption would require
changes in ﬂoorplanning, global and detailed routers, pin
assignment, congestion calculations, and many components
of today’s IC design ﬂows.
7. ACKNOWLEDGMENTS
The 2006 SRC interconnect forum had a number of excellent talks that presented progress in these areas. Azad
Naeemi kindly granted permission for many of the nanotube
pictures.
8. "
HyCUBE - A CGRA with Reconfigurable Single-cycle Multi-hop Interconnect.,"CGRAs are promising as accelerators due to their improved energy-efficiency compared to FPGAs. Existing CGRAs support reconfigurability for operations, but not communications because of the static neighbor-to-neighbor interconnect, leading to both performance loss and increased complexity of the compiler. In this paper, we introduce HyCUBE, a novel CGRA architecture with a reconfigurable interconnect providing single-cycle communications between distant FUs, resulting in a new formulation of the application mapping problem that leads to the design of an efficient compiler. HyCUBE achieves 1.5X and 3X better performance-per-watt compared to a CGRA with standard NoC and a CGRA with neighbor-to-neighbor connectivity, respectively.","HyCUBE: A CGRA with Reconﬁgurable Single-cycle
Multi-hop Interconnect
Manupa Karunaratne, Aditi Kulkarni Mohite, Tulika Mitra and Li-Shiuan Peh
{manupa,aditi,tulika,peh}@comp.nus.edu.sg
National University of Singapore
ABSTRACT
CGRAs are promising as accelerators due to their improved
energy-eﬃciency compared to FPGAs. Existing CGRAs
support reconﬁgurability for operations, but not communications because of the static neighbor-to-neighbor interconnect, leading to both performance loss and increased complexity of the compiler.
In this paper, we introduce HyCUBE, a novel CGRA architecture with a reconﬁgurable
interconnect providing single-cycle communications between
distant FUs, resulting in a new formulation of the application mapping problem that leads to the design of an efﬁcient compiler. HyCUBE achieves 1.5X and 3X better
performance-per-watt compared to a CGRA with standard
NoC and a CGRA with neighbor-to-neighbor connectivity,
respectively.
1.
INTRODUCTION
Accelerators provide signiﬁcantly improved power, performance characteristics compared to general-purpose processors. Application-speciﬁc ASIC accelerators, though optimal from the power-performance viewpoint, oﬀer little ﬂexibility.
In contrast, the reconﬁgurability of FPGAs along
with the recent development of high-level synthesis tools
have made them a popular choice as accelerators, especially
when time-to-market is of the essence. However, FPGAs
oﬀer poor power and area eﬃciency due to the overhead of
bit-level reconﬁgurability.
Coarse-Grained Reconﬁgurable Arrays (CGRAs) have emerged as a promising alternative accelerator, providing reconﬁgurability at word-level, thus realizing better eﬃciency
than FPGAs [5]. Samsung Reconﬁgurable Processor [9],
based on the CGRA architecture template ADRES [12], is
one such CGRA commercially available. A CGRA consists
of an array of functional units (FU), each including an ALU,
a register ﬁle, and a conﬁguration memory as shown in Fig
1. An FU is directly connected to its neighboring FUs. The
FUs share a data memory that can be directly accessed by a
subset of the FUs. The host processor handles data transfer
to/from the data memory in the beginning/end of execution
via DMA. CGRAs are ideal for acceleration of loop kernels. The operations (including memory operations) within
a loop are scheduled on the FUs, while data ﬂows are routed
between dependent operations, all at compile time. The operation schedule and routing information per loop iteration
are loaded into the conﬁguration memory prior to execution
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC ’17, June 18-22, 2017, Austin, TX, USA
© 2017 ACM. ISBN 978-1-4503-4927-7/17/06. . . $15.00
DOI: http://dx.doi.org/10.1145/3061639.3062262
Figure 1: A 4x4 CGRA connected in a 2D mesh
enabling per-cycle reconﬁguration of the operations executing on each FU and the schedule is repeated for the number
of iterations.
While CGRAs support reconﬁgurability through the programmable FUs, most CGRAs have static links connecting an FU only to its neighbors (Neighbor-to-Neighbor or
N2N connection). Thus neighbors can be reached within
a cycle, but any data transfer to a distant FU has to be
routed through intermediate FUs costing multiple cycles
and occupying the FU for communications, rendering it unavailable for compute. The design space explorations [2]
of ADRES [12] showed that optimal energy-eﬃciency can
be achieved through the addition of more interconnects. [15]
discussed the limitations of the connectivity among FUs that
resulted in the inability of the compiler to utilize distant FUs
for inter-dependent operations, leading to longer schedule
and lower resource utilization. It has been shown that utilization can be improved by mapping multiple loops to the
CGRA [16], but at the cost of sub-optimal performance of
individual loops. The N2N connection also makes the mapping of loops quite challenging for the compiler.
Indeed,
state-of-the-art CGRA compilers spend most of the eﬀort
in ﬁnding appropriate routes. The DRESC [13] compiler
for ADRES adopts a time-consuming simulated annealing
approach for routing. More recent works, such as GraphMinor [4], EPIMap [7] REGIMap [8], explore graph-based mapping that attempt to minimize routing costs and even introduce re-computation of the same operation multiple times
near each of its consumer FUs simply to overcome the limitations imposed by the static interconnect of conventional
CGRAs.
In this paper, we introduce a novel CGRA architecture,
HyCUBE1 that has a reconﬁgurable interconnect supporting single-cycle communications across distant FUs on the
chip. The reconﬁgurable interconnect leads to a new formulation of the application mapping problem that is eﬃciently
handled by our HyCUBE compiler. The contributions are:
• We propose a scalable novel CGRA architecture, HyCUBE
combining a low-power, compiler-scheduled NoC capable
1
We name our architecture HyCUBE as it is like the hypercube (a high dimensional cube) composed of large neighborhoods. HyCUBE enables large virtual
neighborhoods though.
(a) Example DFG
(b) 2x2 CGRA
(c) N2N CGRA schedule
(d) HyCUBE CGRA schedule
Figure 2: Mapping of DFG on N2N CGRA and HyCUBE.
of delivering data across multiple hops to multiple destinations within a single cycle, creating a virtual dynamic
neighborhood for the FUs.
• We provide a novel formulation of the application-mapping
problem on HyCUBE, expanding the conventional modulo routing resource graph (MRRG [13]) representing the
CGRA resources to include the notion of dynamic singlecycle paths and present a compiler that can generate nearoptimal mappings at drastically reduced compilation time.
• We implement HyCUBE architecture using TSMC 28nm
technology node. Experimental results using a range of
embedded and multimedia loop kernels show that HyCUBE is, on an average, 3X better in performance-perwatt with 137% performance gain and 60% energy savings
compared to a conventional CGRA.
2. MOTIVATING EXAMPLE
We ﬁrst present the advantages of HyCUBE compared
to the conventional CGRAs with N2N connections through
a motivating example kernel. The example also illustrates
the ma jor diﬀerences in application mapping between the
conventional CGRA and HyCUBE compiler. Fig 2a shows
the dataﬂow graph (DFG) corresponding to the loop body of
a kernel where the nodes represent operations and the edges
represent the dependency between two operations. Fig 2c
presents an optimal schedule of this kernel on a 2x2 CGRA
with only N2N connection shown in Fig 2b. The operation
nodes in white belong to the current loop iteration, while the
shaded nodes belong to the previous or next loop iterations.
Operation n1 is scheduled on FU: F0 in cycle 0. As F0 has
two neighbors, we only have three FUs available (F0, F1,
F2) to schedule the four dependent operations of n1 in cycle
1. Moreover, only two of these three FUs can be used to
schedule operations (n2, n3) because F1 (marked with (cid:114)) is
used to route the data from n1 to the remaining dependent
operations to be scheduled in cycle 2. As a consequence, n6
is shifted to cycle 3. In addition, F2 acts as a router in cycle
2 between n3 and n6, that are scheduled 2 cycles apart in
distant FUs. According to Fig 2c, the schedule is repeated
every 3 cycles, that is, a new loop iteration can be initiated
every 3 cycles leading to initiation interval II=3.
Figure 2d shows an optimal schedule of the same DFG
on HyCUBE that can reconﬁgure the interconnect at each
cycle to achieve a large and dynamic neighborhood. The
single-cycle multi-hop connectivity within HyCUBE enables
scheduling all the dependent operations of n1 in cycle 1. The
single-cycle, multi-hop (if necessary) path, corresponding
to each data-dependence edge, is indicated within brackets.
For example, [013] next to the edge n1 → n5 indicates the
single-cycle path from F0 (mapped with n1) to F3 (mapped
with n5) via F1. This leads to a more eﬃcient schedule with
II=2 without any additional routing nodes. Note that both
the edges n1 → n5 and n1 → n2 use the link between F0
and F1 in cycle 0 routing the same data. This is an example
of the multi-cast routing ability of HyCUBE interconnect.
3. HYCUBE ARCHITECTURE
HyCUBE is a CGRA architecture where the FUs are connected in a 2D mesh topology as shown in Fig 3. The main
feature of HyCUBE that distinguishes itself from previous
CGRA architectures is its network. The network allows any
FU to reach far-away FU (or multiple FUs) on chip within a
single cycle and is completely statically scheduled. In other
words, the compiler determines the conﬁguration of the network at each cycle. HyCUBE’s processing substrate consists
of two types of tiles. The leftmost column (Fig 3) contains
memory-operation capable tiles connected to a 4-port data
memory and the rest are compute-only tiles. All the tiles
comprise of an ALU, a conﬁguration memory, and a crossbar
switch. Additionally, the memory tiles contain a load-store
unit (LSU) for accessing the data memory.
Network. The heart of HyCUBE’s programmable interconnect is the crossbar switch. Each of the output of the
crossbar switch is driven by clockless repeaters that can be
conﬁgured to either let signals bypass asynchronously to the
next hop (N,E,W,S tile), or to stop and receive the incoming data. This enables data to be sent across multiple tiles
within a single cycle. In the event of scaling the substrate
dimensions, the crossbars are not scaled since they are connected only to the neighbours. Additionally, the crossbar
switch could be conﬁgured to connect the same input to
many outputs, allowing a single data to be distributed to
many distant destinations within the same clock cycle.
Clockless repeater links have been shown to be able to
traverse up to 11 hops (across tiles that are 1mm apart,
i.e., 11mm) within 1ns in a 32nm process in the SMART
NoC [10]. While HyCUBE’s interconnect similarly harnesses
clockless repeaters, it is not a dynamically routed NoC like
SMART, and is instead completely controlled by the compiler. As such, HyCUBE’s interconnect comprises of only
the crossbar switch and do not need any routing or ﬂowcontrol logic. It also only has a single register at each port
instead of buﬀer queues, with the register source selected at
compile time. This makes for an extremely lightweight interFigure 3: A 4x4 HyCUBE Architecture, with a single-cycle
multi-hop multi-cast path between tiles, scheduled completely at compile time.
been mapped) as the predicate input. This leads to execution and production of data with valid predicate for only
one of them, followed by a SELECT instruction (selecting
the result with the embedded valid predicate) that supplies
the valid data to any further dependent instructions. The
default value for the predicate input of ALU is always set to
true. If the predicate data is received (probably from branch
instruction executed anywhere in the substrate) from the
crossbar switch, the default value is overridden, supporting
partial predication in the architecture.
Conﬁguration Memory. As illustrated in the motivating
example, when loop kernels are mapped to a CGRA, the
same schedule is repeated after Initiation Interval (II) cycles. This is the number of cycles between the initiation
of two consecutive loop iterations in the fabric. Thus the
conﬁguration memory is required to store instructions (conﬁgurations) for II cycles. Each HyCUBE instruction encodes
control information for the ALU, LSU, crossbar switch and
register read/write enable signals based on the static schedule of the loop. The conﬁguration memory also holds constants required by the operations.
4. HYCUBE COMPILER
In this section, we present the HyCUBE compiler that
maps application kernels onto the architecture.
4.1 Mapping Problem Formulation
Modulo Routing Resource Graph (MRRG). Given a
DFG and a CGRA, the application mapping is performed
through modulo scheduling where a new loop iteration can
initiate execution every initiation interval (II). We ﬁrst determine the lower bound on II, denoted by Minimum II
(MII), as the maximum of the resource minimum II (ResMII)
and recurrence minimum II (RecMII) [17]. For each II value,
we create a time-extended (II cycles) resource graph of the
CGRA, known as Modulo Routing Resource Graph (MRRG)
[14]. As the schedule repeats after II cycles, the resources
at cycle II-1 have connectivity with the resources at cycle 0
connect that is compatible with the stringent power budget
of CGRAs. The conﬁguration for the switch is a part of the
HyCUBE instruction that is loaded each cycle from the conﬁguration memory of each tile. This instruction essentially
deﬁnes the interconnections between tiles dynamically on a
cycle-by-cycle basis. To the best of our knowledge, this is the
ﬁrst work that introduces a CGRA capable of reconﬁguring
its interconnect dynamically.
Registers. The single-cycle, multi-hop interconnect has the
additional beneﬁt that the register ﬁle (RF) per tile (Fig 1)
can be eliminated.
In typical CGRAs, the register ﬁle is
used to retain data in the current tile that will be consumed or routed away in future cycles. The addressing of
the register ﬁle per cycle for reads and writes adds control
overhead. Moreover, additional move operations need to be
inserted to transfer data in and out of the register ﬁle for
inter-dependent operations. However, in HyCUBE, the registers are moved directly into the incoming wires from each
direction North, East, West and South (N,E,W,S) and HyCUBE instructions control the reads and writes to registers
from/to each directional input in each cycle. The distribution of registers improve power and area eﬃciency.
For example (Fig 3), the LOAD operation scheduled on
tile 12, needs to send the output to dependent child operations: SUB and ADD, scheduled on tile 6 and tile 11,
respectively. Within a single cycle, the LOAD operation
fetches the data (from the data memory) that bypasses the
ALU output register RES, ejects from the crossbar towards
east, bypasses registers at the input ports of tile 13 and tile
14, takes a turn towards north and enters the crossbar of
tile 10 bypassing the register at the input port. From here,
the data multicasts towards north and east, bypasses the
registers of input ports and ﬁnally gets latched into input
operand registers of the ALU (I1 or I2) of tile 6 and tile 11.
If the dependent operation (ADD or SUB) is not scheduled
on the next cycle (because another operand for the operation is yet to arrive), the result of LOAD can be saved in
one of the registers that were bypassed at the input of crossbars. In the cycle prior to the execution of the dependent
operation, the data needs to be read from the register into
the input operand registers of the ALU (I1 or I2). The input
port associated with the register, remains disabled for any
other communication in the cycle that the register is read.
Similarly, the registers at input ports can be used to hold the
data if the outgoing link is not immediately available due to
contention, breaking up the single-cycle path into multiple
cycles, if necessary.
Predication. The loops with control divergence form a Control Data Flow Graph (CDFG), having both control and
data dependency edges. The control dependency edge is
treated as a data dependency edge when the dependent
instruction supports predication. HyCUBE supports such
predication without requiring an explicit predicate register
ﬁle. So the ALU has three input registers: predicate (P) and
two operands (I1 and I2). Each operand of HyCUBE is 33bits wide that includes an additional 1-bit predication signal.
An operation can be executed only if the predicate register
value is true and the embedded predicates of both input
operands are also true. For example, suppose an ADD and
SUB instruction are dependent upon a BRANCH instruction
along the true and false path, respectively. The BRANCH
output is routed to both the FUs (where ADD and SUB have
(a) 2x2 HyCUBE
(b) HyCUBE MRRG: High-level view
(c) Expanded view of the routing fabric with detailed
connections between FUs and routing links.
(d) Partial mapping of DFG (Fig 2a) on HyCUBE
MRRG.
Figure 4: MRRG of HyCUBE with single-cycle routing.
in the FUs as well as the data transfer through the routing
fabric can happen within a single cycle.
Problem Deﬁnition. Given a DFG D = (VD , ED ) and
a HyCUBE instance, the problem is to construct a minimally time-extended MRRG of the HyCUBE instance HI I =
(VH , EH ) that consists of two type of nodes: FUs (square
nodes, V F
H ) and links (oval nodes, V L
H ) for which there exists
a mapping φ = (φV , φE ) from D:
• Operation mapping φV : each node v ∈ VD should have
one-to-one mapping to a node φV (v) ∈ V F
H .
• Data dependence mapping φE : each edge epq ∈ ED (connecting nodes vp , vq ∈ VD ) should map to a set of links
(Spq ⊂ V L
H ) connecting φV (vp ) and φV (vq ).
Let us deﬁne vr as another child of vp and hence sibling of vq . When selecting a set of links Spq to connect
φV (vp ) and φV (vq ), Spq
5. EXPERIMENTAL EVALUATION
We implemented the 4x4 HyCUBE architecture in RTL
and mapped it onto TSMC 28nm process, using Synopsys
Design Compiler for synthesis and Cadence Encounter for
Place and Route of the design. The HyCUBE compiler is
implemented as a pass in LLVM 3.9 [11] that generates HyCUBE instruction streams for loop kernel after performing
the mapping based on the algorithm presented in Section
4. These instruction streams are used in our RTL simulations for estimating the power consumption. The representative loops (Table 1) are selected from MiBench [6] and
CortexSuite [18]. A comparison is done against two baseline
architectures: CGRA with a standard NoC (StdNoC), and
CGRA with N2N Connections (N2N).
StdNoC diﬀers from HyCUBE tile shown in Fig 5a in just
one aspect: The clockless repeaters are replaced by clocked
ones in the links; so data has to be latched at each hop,
taking one cycle per hop. The ALU is still freed from communications, as the compiler schedules the crossbar switch
and registers. Fig 5b shows the tile of a CGRA with N2N
connections that has an explicit register ﬁle for storage of intermediary data. All the architectures have three registers
in front of the ALU to hold predicate and input operands
(P,I1,I2) and are synthesized with 4KB data memory for
the entire chip and 256 Byte conﬁguration memory per tile
that can support I I <= 32 (Mappings that have II beyond
32, will need to be partitioned). We restrict the maximum
number of hops to 4 (adequate for most kernels) for 4x4 HyCUBE instance to limit the critical path delay(see Tab.2).
Performance. The previously introduced MRRG and the
conventional MRRG is used when compiling for HyCUBE
and N2N, respectively. The MRRG of HyCUBE had to
be modiﬁed slightly, by eliminating single-cycle connectivity
Table 1: Benchmark Characteristics
Benchmark
Nodes
adpcm dec
68
adpcm enc
88
aes enc
240
idctflt
140
sphnix hmm 43
texture syn
57
stitch
75
fft
85
Edges
93
132
291
187
70
75
102
111
Domain
Telecom
Telecom
Security
Video Compression
Speech Recognition
Image Processing
Image Processing
Signal Processing
(a)
(b)
Figure 5: (a)HyCUBE tile; (b)N2N tile
HyCUBE StdNoC N2N
I
I
/
I
I
M
1
0.5
0
Table 2: Critical Path Delays, Power and Area
Arch
N2N
StdNoC
HyCUBE
Crit.
Delay(ns)
0.8
1.11
1.42
Freq.
(MHz)
1250
901
704
Power
(mW)
145.88
148.38
115.60
Area
(mm2 )
0.49
0.64
0.64
between routing resources to create an MRRG for StdNoC.
As shown in Figure 6, HyCUBE has the best quality mapping for all the benchmarks and successfully achieves MII
(minimum possible II) for adpcm dec, idctf lt, texture syn
and f f t. The benchmarks aes, sphinx hmm and adpcm enc
fail to achieve the minimum II as they have higher node
count (FU constrained) or higher edge/node ratio (interconnect constrained). The benchmark stitch suﬀers as 28%
of its nodes are memory operations. HyCUBE is 1.64X and
4.2X better compared to StdNoC and N2N, respectively in
terms of average quality of mapping. Moreover, HyCUBE
delivers quality mappings with shorter compilation time (Fig
7) due to its reconﬁgurable interconnect compared to N2N.
StdNoC having an ﬁxed neighbourhood interconnect, performs better compared to N2N as the network is responsible
for data movement in lieu of FUs.
For further comparison of absolute performance, the different critical path timing of the three architectures are obtained after place and route PnR (Table 2). For HyCUBE,
the delay depends on the maximum number of hops unlike
the static delay for StdNoC and N2N. The critical path for
HyCUBE goes from from the output of the ALU → crossbar → links → crossbar of the tile maximum hops away,
before it is stored in a register. We restrict maximum number of hops to 4 to limit the critical path. The longer the
path, the higher the capacitance of the link that needs to
be driven from the repeater, stretching timing and limiting
maximum frequency. Extending the maximum number of
hops to 8, the entire 4x4 HyCUBE can be traversed in a
single cycle, but the critical path will stretch from 1.42 to
1.59ns, and most applications do not require such distant
communications. The link only contributes 10.8% of critical path delay, hence the maximum frequency at which HyCUBE and StdNoC operate do not diﬀer much. The critical
path deﬁnes the maximum frequency. Based on the critical path and II achieved for a loop L, T ime P er I teration
(L)= M appedI Iarch (L) ×C rit.Delayarch , which is the reciprocal of the throughput (loop iterations per second). Table 3 shows that HyCUBE and StdNoC are able to achieve
an average throughput improvement of 137% and 101% compared to N2N, respectively (when each architecture runs at
its maximum possible frequency).
Energy. HyCUBE consumes an average power of 115.60
mW at 704 MHz where crossbar switches and memories contribute to ma jority of the power (25.6% and 42.4%) while
occupying 24% and 25.28% of total chip area, respectively.
Further the ALUs, which account for 44.8% of chip area,
HyCUBE StdNoC N2N
104
103
102
101
a d p c m d e c
a d p c m e n c
a e s
t
l
f
i d c t
s p h i n x h m m
t e x t u r e
s y n
s t
t c h
i
t
f
f
)
s
(
e
m
i
t
n
u
R
r
e
l
i
p
m
o
a d p c m d e c
a d p c m e n c
a e s
t
l
f
i d c t
s p h i n x h m m
t e x t u r e
s y n
s t
t c h
i
t
f
f
C
Figure 6: Quality of Mapping (MII/Mapped II)
Figure 7: Compilation Time
Table 3: Throughput and Energy Comparison
Benchmark
adpcm enc
adpcm dec
aes
idct
sphinx hmm
stitch
texture syn
fft
Throughput w.r.t N2N
Energy w.r.t N2N
N2N
StdNoC
HyC
N2N
StdNoC
HyC
1
2.31
3
1
0.44
0.26
1
4.61
4.51
1
0.22
0.18
1
0.72
1.06
1
1.41
0.75
1
1.1
2
1
0.93
0.4
1
2.02
1.97
1
0.5
0.4
1
2.31
3
1
0.44
0.26
1
1.87
1.83
1
0.54
0.43
1
1.13
1.57
1
0.9
0.51
Mean
2.01
2.37 Mean
0.67
0.4
W
m
/
t
u
p
h
g
u
o
HyCUBE StdNoC N2N
6
4
2
r
h
T
a d p c m d e c
a d p c m e n c
a e s
t
l
f
i d c t
s p h i n x h m m
t e x t u r e
s y n
s t
t c h
i
t
f
f
Figure 8: Performance-per-watt w.r.t. N2N CGRA
only consume 20.8% of chip power. To compare the energy
consumption of an architecture for a loop L, we ﬁrst derive
the average power consumption (Parch ) of the architecture
running at its maximum frequency after performing PnR,
using Cadence Encounter, as shown in Table 2. Then energy per loop iteration is computed as Parch × T ime P er
I terationarch (L). According to Table 3, HyCUBE and
StdNoC are able to achieve a 60% and 33% average energy reduction compared to N2N, respectively, when running at their corresponding maximum frequency. The average performance-per-watt is computed based on throughput
(iterations per second) and the Parch . According to Figure
8, HyCUBE and StdNoC are 3X and 1.98X better compared
to N2N, while HyCUBE is 1.5X better compared to StdNoC
in terms of average performance-per-watt.
Comparison with other accelerators. To perform a highlevel comparison against existing commercial platforms, we
extracted published power and performance values of the
execution of 256-FFT on ARM-Cortex A5 [1], Xilinx Artix
7 [3], Samsung Reconﬁgurable Processor [9]. As HyCUBE
is mapped to TSMC 28nm process, all other performance
numbers are scaled (1.42 times per generation due to 0.7
scaling of feature size, with the power remaining the same
as core voltage is kept constant) for fair comparison. Fig
9b shows log(P erf ) = log(m) + log(P ower) lines and the
intercept of the lines indicates the logarithm of eﬃciency
(m=Perf/Power). The Xilinx Artix 7 FPGA oﬀers 10X per105
104
17.45 MIPS/mW
XilinxArtix7
63.15 MIPS/mW
HyCUBE
15.62 MIPS/mW
ARM-CortexA5
)
S
P
I
M
(
e
c
n
a
103
SRP
32.04 MIPS/mW
m
r
o
f
r
e
P
102
101
102
103
Power(mW)
104
Figure 9: (a) HyCUBE chip layout (b)Power-Performance
comparison with commercial processors and accelerators.
formance compared to the ARM at similar performanceper-watt. On the other hand, SRP is a commercial 3X3
CGRA with N2N connections designed to achieve better
power-eﬃciency (2X compared to ARM) but could only offer 30% performance compared to ARM. Given the reconﬁgurable single-cycle multi-hop interconnect, HyCUBE scales
very well for 4x4 CGRA and oﬀers 3X performance and 4X
performance-per-watt compared to ARM. It is clear that
CGRAs are a better choice compared to FPGAs in terms
of performance-per-watt but the improvements made with
regard to the interconnect of the HyCUBE enables it to improve performance while maintaining highest performanceper-watt.
6. CONCLUSION
CGRA is a promising technology to accelerate loops. However, current reconﬁgurability per cycle is for the operations
executing on functional units, but not for the connectivity between functional units. HyCUBE is a novel CGRA
architecture that incorporates a reconﬁgurable single-cycle
multi-hop interconnect between functional units. We introduce a novel resource abstraction model for HyCUBE that
includes single-cycle routing and leads to the design of an
eﬃcient compiler. We synthesize HyCUBE architecture in
28nm process and compile real loop kernels onto the architecture. The experimental results show that average power
eﬃciency of HyCUBE is 1.5X and 3X compared to a CGRA
with standard NoC and a N2N CGRA, respectively.
7. ACKNOWLEDGMENTS
This work was partially funded by the Singapore Ministry
of Education Academic Research Fund Tier 2 MOE2014-T22-129 and by the Singapore National Research Foundation
Research Fund NRF-RSS2016-005.
8. "
MOCA - an Inter/Intra-Chip Optical Network for Memory.,"The memory wall problem is due to the imbalanced developments and separation of processors and memories. It is becoming acute as more and more processor cores are integrated into a single chip and demand higher memory bandwidth through limited chip pins. Optical memory interconnection network (OMIN) promises high bandwidth, bandwidth density, and energy efficiency, and can potentially alleviate the memory wall problem. In this paper, we propose an optical inter/intra-chip processor-memory communication architecture, called MOCA. Experimental results and analysis show that MOCA can significantly improve system performance and energy efficiency. For example, comparing to Hybrid Memory Cube (HMC), MOCA can speedup application execution time by 2.6x, reduce communication latency by 75%, and improve energy efficiency by 3.4x for 256-core processors in 7 nm technology.","MOCA: an Inter/Intra-Chip Optical Network for Memory
Zhehui Wang
†
‡
†
†
†
†
, Zhengbin Pang
, Peng Yang
, Jiang Xu
, Xuanqi Chen
, Rafael K. V. Maeda
Luan H.K. Duong
, Haoran Li
, Zhe Wang
The Hong Kong University of Science and Technology
National University of Defense Technology
†
, Zhifei Wang
,
†
‡
†
†
†
ABSTRACT
The memory wall problem is due to the imbalanced developments and separation of processors and memories. It is
becoming acute as more and more processor cores are integrated into a single chip and demand higher memory bandwidth through limited chip pins. Optical memory interconnection network (OMIN) promises high bandwidth, bandwidth density, and energy eﬃciency, and can potentially alleviate the memory wall problem. In this paper, we propose
an optical inter/intra-chip processor-memory communication architecture, called MOCA. Experimental results and
analysis show that MOCA can signiﬁcantly improve system
performance and energy eﬃciency. For example, comparing to Hybrid Memory Cube (HMC), MOCA can speedup
application execution time by 2.6x, reduce communication
latency by 75%, and improve energy eﬃciency by 3.4x for
256-core processors in 7 nm technology.
1 Introduction
Over the past decade, the speed gap between the processor chip and the memory chip is continuously increasing,
which is known as the memory wall problem. This problem
becoming acute as more and more processor cores are integrated in one chip, demanding higher memory bandwidth.
The performance of traditional processor-memory communication system is constrained by the limited space for electrical pins on package substrate. The bandwidth provided
by the electrical buses cannot meet the demands of future
computing systems. Optical interconnect shows an order
of magnitude larger bandwidth density than electrical interconnect, and it is a promising candidate to alleviate the
memory wall problem [1] [2] [3] [4] [5] [6].
The traditional architecture of processor-memory network
is no longer an optimal choice for optical interconnect because it has diﬀerent properties from electrical interconnect. The new design will face the following challenges.
Firstly, with large bandwidth of optical interconnect, the
real bandwidth of the processor-memory network is mainly
constrained by the bandwidth demand of the memory system. In traditional design, it is much less than the potential
maximum bandwidth of optical interconnects. Secondly, the
serializer/deserializer (SERDES) used for electrical-optical
and optical-electrical (EO/OE) interfaces have a high deSupported by DAG11EG05S and FSGRF15EG04 of Hong Kong
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC ’17, June 18 - 22, 2017, Austin, TX, USA
c(cid:2) 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4927-7/17/06. . . $15.00
DOI: http://dx.doi.org/10.1145/3061639.3062286
mand of power. Previous investigations found that SERDES
can decrease the energy eﬃciency of the optical interconnects considerably [7]. Thirdly, when the network size is
increased with more and more processor cores and memory
banks, the communication latency in processor-memory network becomes very large, because of the increased hop count
in NoC and inter-chip network [8] [9].
The Optical Memory Interconnection Network (OMIN) is
becoming a promising choice for high performance computing systems. We propose MOCA (Memory Optical Communication Architecture) based on the properties of optical interconnects, which shows better performance, higher
bandwidth, lower latency, and higher energy eﬃciency. In
MOCA, we introduced the following technologies:
• An optimized memory organization, which is more efﬁcient in processing memory requests, and supports
large memory bandwidth demands.
• Wavelength-division multiplexing/time-division multiplexing (WDM/TDM) based optical interconnects and
• A uniﬁed inter/intra-chip processor-memory network,
low-power EO/OE interfaces.
including a low hop count NoC and a low hop count
inter-chip network.
The rest of the paper is organized as follows. Section 2
introduces modern memory system. Section 3 details the
design of MOCA. Section 4 analyzes and compares the energy eﬃciency, bandwidth, execution time, and latency of
the networks in MOCA and HMC. Finally, Section 5 concludes this work.
2 Background
Hybrid Memory Cube (HMC) is an entirely new category of memory system [10], delivering better system performance and larger bandwidth than traditional memory systems. In HMC system, multiple memory chips are connected
into a tree-based or chain-based memory network by serial
electrical interconnects [11]. Each parent memory chip has
at most three children chips, and the root memory chip is
directly connected to the processor chip. The requests from
the processor or responses from the memory are integrated
into packets. Any request or response can be sent or received in a diﬀerent order than the request or response was
made. Based on the address information integrated in the
packet, a switch inside each memory chip can transfer packets from the processor chip to one of the memory chips, or
from the memory chip to the processor chip. Packet in the
HMC network takes multiple hops of memory chips before
arriving to the destination.
Each serial electrical interconnect works independently.
Before it starts to work, it requires an additional amount of
time for initialization. A training sequence is sent from the
transmitter to the receiver, so that they can be synchronized.
After transmitting the packet, the transmitter still needs to
send bits continuously, keeping the transmitter and receiver
3URFHVVRU &KLS
0LFURUHVRQDWRU
2SWLFDO 6ZWLFK %R[
0HPRU\ &KLS 
«
(22( ,QWHUIDFH
«
2( Ȝ
Ȝ
Ȝ
Ȝ1
(2 Ȝ
Ȝ
Ȝ
Ȝ1
6
Z
W
L
K
F
(22( ,QWHUIDFH
0HPRU\
&RQWUROOHU
7UDQVDFWLRQ
(QJLQH
2SWLFDO :DYHJXLGH
2SWLFDO )LEHU
0HPRU\ 'LH
(OHFWULFDO ,QWHUFRQQHFW
«
5DQN
%DQN
%DQN
%DQN
«
«
«
«
/ ,
/ '
«
&RUH
0
R
Q
R
H
P
U

W
H
H
Q
R
J
Q
Q
R
G
Q
\
(
&
(
U
W
O
O
L
U
)
U
(22( ,QWHUIDFH
,QWHUIDFH 6ZLWFK
/ 6OLFH
«
&RUH
&RUH
&RUH
««
«
«
«
«
«
«
«
«
&RUH
&RUH
&RUH
&RUH
&RUH
&RUH
«
«
«
««
0HPRU\ &KLS 
0HPRU\ &KLS Q
«
5;
«
«
«
«
7;
«
«
««
«
2SWLFDO 'LH
/DVHU 6RXUFH
6SOLWWHU
2SWLFDO 3LQ
«
«
«
«
«
(OHFWULFDO 'LH
8SVWUHDP
'
R
Z
Q
VWU
H
D
P
5DQN
5DQN
5DQN
5DQN
Figure 1: MOCA overview. One processor chip is connected to n memory chips by optical interconnects. They are synchronized
by a common optical clock signal from the oﬀ-chip laser.
synchronized. To reduce the chances of long runs of repeated
0s or 1s, a pair of scrambler and descrambler is used on both
ends of each serial interconnect. Because the data inside the
chip are parallel while the oﬀ-chip interconnect is serial, a
pair of SERDES is implemented in the chip interfaces, which
convert on-chip parallel signals into serial signals, or convert
serial signals back to parallel signals.
3 MOCA Architecture
This section introduces MOCA, including the architecture
overview, the memory organization, the EO/OE interfaces,
and the inter/intra-chip network.
3.1 Architecture Overview
An optical inter/intra-chip processor-memory communication network, called MOCA, is proposed, which shows
high bandwidth, high energy eﬃciency, and low latency.
The overview is shown in Fig. 1, where one processor chip
is connected to multiple memory chips by optical interconnects. The processor cores and caches are connected by a
ring based optical NoC. When cache miss happens, a write
or read request is generated from one of the L2 slices. For
read requests, a packet containing command and address information is sent from the L2 slice to a interface switch by
the NoC, and then the interface switch forwards the packet
to a memory chip via the upstream optical interconnect.
The response from the memory chip is sent back to the interface switch via the downstream optical interconnect, and
then the interface switch forwards the packet to the corresponding L2 slice by the NoC. For write requests, a packet
containing data, command and address information is sent
to the memory chip by the NoC, the interface switch, and
the upstream optical interconnect.
In MOCA, multiple dies are implemented within one package using the 2.5D or 3D integration technologies. The processor chip includes one electrical die and one optical die,
and the memory chip includes one electrical die, one optical die, and multiple memory dies. In the 2.5D integration
technology, multiple dies are placed on the same interposer,
and they are connected by the metallic interconnects on the
interposer. In the 3D integration technology, multiple dies
are stacked on top of the package substrate, and they are
connected by through-silicon-vias.
The oﬀ-chip laser provides light sources to the processor
chip and all the memory chips. An optical ﬁber splitter converts light from one ﬁber to multiple ﬁbers. The processor
chip and the memory chips are synchronized by a common
optical clock signal from the oﬀ-chip laser. A clock source is
implemented near the laser, which modulates light with high
frequency clock signals. A clock module is implemented in
both the processor chip and the memory chip, which converts optical clock signals into electrical clock signals. To
increase the sampling reliability, the clocks of transmitter
and receiver are phase shifted.
3.2 Memory Organization
MOCA uses an optimized memory organization, which
is more eﬃcient in processing requests and supports larger
memory bandwidth demands than traditional memory organizations. There are two types of memory controllers in
the memory system. One is called front-end engine, located
on the processor chip, and the other is called transaction engine, located on the memory chip [12]. The front-end engine
is responsible for rescheduling all the request packets from
diﬀerent L2 slices, and forwarding packets to the memory
chip by the interface switch. There are multiple front-end
engines in MOCA, and each front-end engine is connected to
multiple memory chips. Inside each memory chip, there is
an electrical switch and multiple transaction engines. Based
on the address information, the packet will be forwarded
to the corresponding transaction engine by the electrical
switch. The transaction engine is responsible for process

«
;
5
2( ,QWHUIDFH
,
X
S
Q
W
7HUPLQDWRU
;
5
(2 ,QWHUIDFH
2( ,QWHUIDFH
2SWLFDO 3LQ
3'
3'«
:DYHOHQJWK Ȝ
3'
6
Z
W
L
F
K /$
7,$
«
;
5
;
5
«
«
«
«
«
3'
3'«
3'
«3'
3'«
3'
;
5
;
5
;
5
2
X
W
X
S
W
«
«
«
«
7
;
(2 ,QWHUIDFH
7
;
«
'
L
U
H
Y
U
7
;
«
7
;
7
;
«
«
«
«
«
««
7
;
7
;
7
;
7
;
«
«
«
2SWLFDO 3LQ
;
5
;
5
2
X
W
X
S
W
,
X
S
Q
W
2SWLFDO 3LQ
/
H
V
D
U
&
O
N
F
R
&
O
N
F
R
:DYHOHQJWK Ȝ
Ȝ1
Ȝ1
Ȝ
Ȝ
&
O
N
F
R
0
X
G
R
O
H
«
«
«
«
«
«
&/. 
&/.
&/.

0
&
O
N
F
R
0
X
G
R
O
H
&/. 
&/.
&/.

0
Figure 2: WDM/TDM based EO/OE interfaces. Time domain is divided into multiple slots. The voltage level of each clock
signal turns to high at its time slot, and turns to low at other time slots.
ing the packet, and generating command, address and data
information for memory banks. The response information
from the memory banks will be packed by the transactions
engine before sending to the corresponding L2 slice.
In MOCA, we increase the number of front-end engines in
the processor chip, and also increase the number of transaction engines mapped to one front-end engine, compared
with traditional memory systems. We move the transaction
engines to the memory chip, and leave only the front-end
engine in the processor chip. There are two reasons for this
design. First, with enough space and power budget on oﬀchip memory chips, the number of transaction engines in
MOCA can be much larger than that in traditional memory
system. This increases the level of parallelism, which boosts
the maximum bandwidth supported. Secondly, the transaction engines are physically closer to the memory banks,
which helps to reduce the turnaround latencies, an additional latency caused by the switching between read and
write operations.
3.3 EO/OE Interfaces
MOCA uses a mixed multiplexing technique to increase
the bandwidth of the optical interconnects. The ﬁrst is
WDM, where multiple optical signals with diﬀerent wavelengths are transmitted via one optical interconnect at the
same time. Each optical signal works independently. The
second is TDM, where the time domain of each wavelength
is further divided into multiple time slots with ﬁxed length.
Each slot carries an independent signal. The bandwidth
that a single optical interconnect can support is equivalent
to that of hundreds of traditional electrical interconnects.
A pair of low-power EO/OE interfaces are implemented
on both sides of the optical interconnect, as shown in Fig. 2.
One interconnect is for upstream signals and the other one is
for downstream signals. Implementing the WDM and TDM
requires modulating and switching the light sources, and this
is achieved by using on-chip microresonator (MR). Charging
or discharging the MR will change its resonance wavelength.
If we charge the MR at EO interface, light will be attenuated, and we use this property to modulate signals. If we
charge the MR at OE interface, the modulated optical light
will be redirected from the optical interconnect to photodetector (PD). The optical signals are converted to electrical
signals by PD, and the data is recovered by transimpendence
ampliﬁer (TIA) and limiting ampliﬁer (LA).
We increase the energy eﬃciency of EO/OE interfaces by
using the mixed WDM/TDM technique, and using optical components to serialize/deserialize signals. As shown
E 02&$(1R&
D +0&(1R&
(cid:258)
(cid:258)
(cid:258)
(cid:258)
F 02&$21R&
(cid:258)
(cid:258)
3URFHVVRU &KLS
(cid:258)
(cid:258)
(cid:258)
(cid:258)
(cid:258)
3URFHVVRU &KLS
3URFHVVRU &KLS
(cid:258)
0HPRU\ &KLS
)URQW(QG (QJLQH
(OHFWULFDO ,QWHUFRQQHFW
2SWLFDO ,QWHUFRQQHFW
6ZLWFK LQ (OHFWULFDO 1R&
6ZLWFK LQ 2SWLFDO 1R&
(cid:258)
(cid:258)
(cid:258)
(cid:258)
0HPRU\ &KLSV
0HPRU\ &KLSV
0HPRU\ &KLSV
(cid:258)
(cid:258)
0HPRU\ &KLSV
0HPRU\ &KLSV
0HPRU\ &KLSV
Figure 3: (a) The memory network used in HMC; (b)(c)
MOCA networks. The processor chip is partially shown.
in Fig. 2, time domain is divided into multiple slots. The
voltage level of each clock signal turns to high at its time
slot, and turns to low at other time slots. In EO interface,
each clock signal is used to enable one electrical input by
turns. This process serializes the parallel electrical inputs
into optical signal. In OE interface, each clock signal is used
to redirect the optical signal to a photodetector by turns.
This process deserializes the serial optical input to electrical signals. Clock signals in EO interfaces are synchronized
with clock signals in OE interfaces.
3.4
Inter/Intra-chip Network
MOCA uses a uniﬁed inter/intra-chip processor-memory
network to decrease the communication latency, including
a low hop count NoC and a low hop count inter-chip network [13]. The inter-chip network is shown in Fig. 3 (b)
and (c). Multiple memory chips are connected to one frontend engine in parallel by optical interconnects. Based on the
address information, the packet is transmitted from the processor chip to one memory chip via the front-end engine, and
the response is transmitted back to the processor chip via
the same front-end engine. Each front-end engine uses an
interleaved address mapping. In such a mechanism, contiguous memory operations are mapped to diﬀerent components,
reducing the average communication latencies.
By using the low hop count inter-chip network, packet
takes only one hop instead of few hops before arriving to
its destination. Hence, the oﬀ-chip memory access latency
in MOCA is substantially lower compared to the latency in
traditional memory network. Given the fact that optical pin
has an order of magnitude larger bandwidth densities than
electrical pin, the total area of optical pins used by MOCA is
comparable to the area used by HMC. For example, 256-core
processors needs area of 32 mm2 for optical pins [14].
In electrical NoC, communication between two cores normally go through multiple hops, resulting in large latencies.
Diﬀerent from electrical NoC, the optical NoC is a circuitswitched network, and a optical path is established before
the transmission of signals. In optical NoC, only one hop
is required for any communication between two cores. As
a result, the average communication latency in the optical NoC is much smaller than the communication latency
in the electrical NoC. In Fig. 3, we show three processormemory networks. Fig. 3 (a) shows HMC-ENoC, denoting
HMC using electrical NoC; (b) shows MOCA-ENoC, denoting MOCA using electrical NoC; (c) shows MOCA-ONoC,
denoting MOCA using optical NoC. The processor chip is
partially shown in the ﬁgure.
4 Analysis and Comparison
This section evaluates the execution time, bandwidth, latency, and energy consumption of MOCA-ENoC, MOCAONoC and HMC-ENoC.
4.1 Simulation Setup
We evaluate the performance of HMC and MOCA using
a cycle accurate simulator JADE [15] with COSMIC and
STREAM benchmarks [16]. The conﬁguration of the processor chip is shown in Table 1. A cycle accurate memory
simulator, DRAMsim2 [17], is intergraded in the JADE simulator. The conﬁguration of the memory chip is shown in Table 2. The timing and current parameters of the memory device are obtained from DRAMSpec [18], a parameter calculator for customized memory devices. The power consumption of the memory controllers, including front-end engines
and transaction engines, are collected from MCPAT [12], an
analysis tool for multi-core and many-core processors. The
power consumption of the memory devices, including both
the static and dynamic power consumption, are calculated
based on Micron’s memory model [19]. The characteristics
of both electrical and optical interconnects are analyzed by
OEIL [20]. The parameters of optical devices are collected
from [14] [21] [22] [23], and summarized in Table 3.
Table 1: Processor Chip Conﬁguration
Parameter
Value
Core
NoC Topology
L1 I/D Cache
L2 Cache
Cache Coherence
Cache Line Size
Technology
ARM Cores-v7 @3GHz
Ring
Private, 32KB/core
Shared, 256KB/Core
Directory-based MSI MOSI
64Byte
7nm E-Die, 65nm O-Die
Table 2: Memory Chip Conﬁguration
Parameter
Organization
Frequency
Memory Size
Schedule Policy
Page Policy
Technology
Value
2 Banks/Rank
× 8 Ranks/Transaction Engine
× 32 Transaction Engines/Chip
× 32 Chips/Front-End Engine
×1∼32 Front-End Engines
800MHz
8GB/Chip
FR-FCFS
Close-Page Policy
14nm E-Die, 65nm O-Die,
22nm M-Die
Table 3: Optical Device Parameter
Parameter
Waveguide propagation loss
Waveguide crossing loss
Fiber propagation loss
32-way splitter excess loss
Optical pin coupling loss
Receiver sensitivity
Laser power conversion eﬃciency
Laser power extinction ratio
Microresonator passing loss
Microresonator insertion loss
Microresonator heat tuning power
Value
1 dB/cm
0.1 dB
5×10
−6 dB/cm
4 dB
2 dB
-20 dBm
10 dB
10
0.2 dB
1 dB
0.05 mW
We evaluate networks with 32 to 256 processor cores.
There is one front-end engine (FEE) for every eight cores,
and all the front-end engines are assumed to be evenly distributed on the ring-based NoC. We assume the following
memory organization: each rank has two banks, each transaction engine is connected to eight ranks, each memory chip
has thirty-two transaction engines, and each front-end engine is connect to thirty-two memory chips.
4.2 Performance Evaluation
The normalized execution time of HMC-ENoC, MOCAENoC and MOCA-ONoC are plotted in Fig. 4. We simulate
the processor-memory network with 256 processor cores and
32 front-end engines. The total size of the last level cache
(LLC), the L2 cache, is assumed to be 32 MB, 64 MB and
128 MB. It shows that MOCA has larger execution speed
than HMC. For example, for LLC size of 128 MB, compared
with HMC-ENoC, the average execution speed of MOCAENoC is 15% larger, and the average execution speed of
MOCA-ONoC is 59% larger.
All the applications can be classiﬁed into memory intensive applications (ML, RSe, and RSd) and non-memory in0
0.5
1.0
1.5
2.0
128M 64M 32M
ML
128M 64M 32M
RSe
128M 64M 32M
RSd
128M 64M 32M
TURBO
128M 64M 32M
FFT
128M 64M 32M
US
128M 64M 32M
LDPC
HMC-ENoC
MOCA-ENoC
MOCA-ONoC
2.66
2.06
N
o
r
m
a
i
l
d
e
z
E
u
c
e
x
i
t
T
n
o
i
m
e
Figure 4: Normalized execution time when the LLC size is reduced from 128 MB to 32 MB.
0
0.2
0.4
0.6
0
10
20
30
40
50
Execution Time (µs)
60
25
50
75 100 125 150
Execution Time (µs)
256-Core 32-FEE
HMC-ENoC
MOCA-ENoC
MOCA-ONoC
M
e
m
o
r
y
B
d
n
a
w
i
h
d
t
(
B
T
s
p
)
0
0.2
0.4
0.6
0
15
30
45
60
75
Execution Time (µs)
90
40
80 120 160 200 240
Execution Time (µs)
128-Core 16-FEE
M
e
m
o
r
y
B
d
n
a
w
i
h
d
t
(
B
T
s
p
)
0
0.2
0.4
0.6
0
64-Core 8-FEE
M
e
m
o
r
y
B
d
n
a
w
i
d
t
h
(
B
T
s
p
)
0
1.0
2.0
3.0
0
32-Core 4-FEE
M
e
m
o
r
y
B
d
n
a
w
i
d
t
h
(
B
T
s
p
)
Figure 5: Delivered memory bandwidth.
tensive applications (TURBO, FFT, US and LDPC). For
memory intensive applications, when the LLC size is decreased, their execution time is almost unchanged. This
is because the memory intensive applications have low address localities. While running, the processor is always reading data in new addresses, and generating a lager amount
of memory requests. Hence, the LLC size is irrelevant to
the system performance. For non-memory intensive applications, when the LLC size is decreased, the execution time
is signiﬁcantly increased. This is because the non-memory
intensive applications have high address localities, only when
the LLC size is small, their performance becomes sensitive
to the network architecture.
The delivered memory bandwidth of HMC-ENoC, MOCAENoC and MOCA-ONoC are plotted in Fig. 5. We run
memory intensive applications on 32-core to 256-core systems. In this ﬁgure, when the memory bandwidth drops to
zero, all the tasks are completed. It shows that MOCA has
larger delivered memory bandwidths than HMC. In 256-core
system, compared with HMC-ENoC, the delivered bandwidth of MOCA-ENoC shows 53% increment, and the delivered bandwidth of MOCA-ONoC shows 162% increment.
The execution time using MOCA is also reduced. In 256core system, compared with HMC-ENoC, the execution time
of MOCA-ENoC shows 1.5x speedup, and the execution
time of MOCA-ONoC shows 2.6x speedup.
4.5
3
1.5
0
3
1.5
0
60
30
0
0
0.6 1.2 1.8 2.4 3.0 3.6
Latency (1000 cycles)
P
r
b
a
b
o
t
i
l
i
y
(
%
)
HMC-ENoC
MOCA-ENoC
MOCA-ONoC
0.3 0.6 0.9 1.2 1.5 1.8
Latency (1000 cycles)
256-Core 32-FEE
6
4
2
0
4
2
0
60
30
0
0
0.4 0.8 1.2 1.6 2.0 2.4
Latency (1000 cycles)
P
r
b
a
b
o
t
i
l
i
y
(
%
)
0.3 0.6 0.9 1.2 1.5 1.8
Latency (1000 cycles)
128-Core 16-FEE
6
4
2
0
4
2
0
60
30
0
0
P
r
b
a
b
o
t
i
l
i
y
(
%
)
64-Core 8-FEE
12
8
4
0
8
4
0
60
30
0
0
P
r
b
a
b
o
t
i
l
i
y
(
%
)
32-Core 4-FEE
Figure 6: Probability distribution of latencies.
The probability distribution of latencies in HMC-ENoC
MOCA-ENoC, and MOCA-ONoC are plotted in Fig. 6. We
run memory intensive applications on 32-core to 256-core
systems, and measure the latency to obtain data from the
external memory under cache misses. The communication
latency includes the latency on the NoC, the latency on the
inter-chip interconnect, and the latency on the memory chip.
The hop latency in the inter-chip network is assumed based
on the Intel QuickPath Interconnect (QPI) [24].
It shows
that MOCA has smaller average communication latencies
than HMC. In 256-core system, compared with HMC-ENoC,
the communication latency of MOCA-ENoC shows 28% reduction, and the communication latency of MOCA-ONoC
shows 75% reduction.
The execution time, the memory bandwidth, and the communication latency are closely related. If the latency of the
memory access is reduced, more instructions can be executed per unit time, resulting in larger memory requests
and larger memory bandwidth demands. Experiments show
that MOCA has faster execution speed, higher bandwidth
and smaller communication latency than HMC. This is because in HMC, packets take multiple hops in inter-chip network, while in MOCA, packets take only one hop, resulting
in smaller latencies. Experiments also show that the optical
NoC based MOCA has faster execution speed, higher bandwidth and smaller communication latency than that of the
 
 
 
 
 
 
 
 
 
 
 
 
 
 
200
150
)
t
i
HMC-ENoC
MOCA-ENoC
MOCA-ONoC
/
b
J
p
(
n
o
i
t
p
m
u
s
n
o
C
y
g
r
e
n
E
100
50
0
32-Core
64-Core
128-Core
256-Core
Inter-Chip Interconnects
Mem. Switch
Mem. Controller
Electrical NoC
Mem. Static
Optical NoC
Mem. Dynamic
Figure 7: Energy consumption breakdown.
electrical NoC based MOCA. This is because the average
hop count in the optical NoC is much smaller than that in
the electrical NoC, resulting in smaller latencies.
4.3 Energy Efﬁciency
The energy consumption breakdown of HMC-ENoC, MOCA-ENoC and MOCA-ONoC are plotted in Fig. 7. We run
memory intensive applications on 32-core to 256-core systems, and evaluate the energy consumptions of inter-chip
network (inter-chip interconnects and switches), NoC, memory controllers (front-end engines and transaction engines),
and memory chips (static and dynamic energy consumptions). It shows that MOCA-ENoC and MOCA-ONoC have
higher energy eﬃciencies than HMC-ENoC. In 256 core system, compared with HMC-ENoC, the energy eﬃciency of
MOCA-ENoC shows 1.6x improvement, and the energy eﬃciency of MOCA-ONoC shows 3.4x improvement. It can also
be observed that the ma jority of energy saving of MOCA
comes from NoC and inter-chip network.
MOCA shows higher energy eﬃciency than HMC mainly
because of the following two reasons. Firstly, the serial
electrical inter-chip interconnects use SERDES to convert
parallel signals into serial signals. For synchronization reasons, the SERDES implements many gates and ﬂip-ﬂops,
and these gates and ﬂip-ﬂops are designed to work at high
frequencies, consuming a lot of energy. Alternatively, in the
optical inter-chip interconnects, the parallel to serial signal
converting tasks are ﬁnished by optical components, and
such mechanism can save a lot of energy. Secondly, the energy consumptions of NoC and inter-chip network are proportional to their hop counts. In HMC, packets go through
a large number of hops in both NoC and inter-chip network, consuming a lot of energy. In MOCA, by using optical
NoC and an optimized optical inter-chip network, packets go
through a small number of hops, consuming much less energy than those in HMC.
5 Conclusions
In this work we propose MOCA, a processor-memory network architecture based on optical inter/intra-chip interconnects. Experiments show that MOCA has larger execution
speed, higher bandwidth, lower communication latency, and
higher energy eﬃciency than HMC. Compared with HMC,
the electrical NoC based MOCA shows 1.5x speedup in execution time, 28% reduction in communication latency, and
1.6x improvement on energy eﬃciency, while the optical NoC
based MOCA shows 2.6x speedup in execution time, 75%
reduction in communication latency, and 3.4x improvement
on energy consumption, for 256-core processors in 7nm technology. It shows a bright future of optical inter/intra-chip
interconnects in processor-memory networks.
6 "
Low-Power On-Chip Network Providing Guaranteed Services for Snoopy Coherent and Artificial Neural Network Systems.,"During the transition to packet-switched on-chip networks we lose the relative timing and ordering of requests, which are essential for shared memory coherency and the communication of spikes in hardware-based artificial neural networks. We present a bufferless network architecture that enforces a time-based sharing of multi-hop single-cycle paths, providing guaranteed services at low cost. We guarantee ordered delivery of requests, fixed network latency, and jitter-free neural spikes. In a 64-node network, we achieve a 84% lower latency and 7.5× higher throughput than SCORPIO. Full-system 36-core simulations show a 9% lower runtime than SCORPIO, with 39% lower power and 36% lower area.","(cid:70)i(cid:103)(cid:117)(cid:114)e (cid:50)
(cid:58) (cid:79)(cid:82)(cid:73) S(cid:110)ake (cid:82)i(cid:110)(cid:103) a(cid:110)(cid:100) (cid:82)(cid:111)(cid:117)(cid:116)e(cid:114) (cid:77)i(cid:99)(cid:114)(cid:111)a(cid:114)(cid:99)(cid:104)i(cid:116)e(cid:99)(cid:116)(cid:117)(cid:114)e
(cid:114)epea(cid:116)e(cid:100) (cid:108)i(cid:110)ks (cid:44) (cid:116)(cid:104)e (cid:3)i(cid:116) (cid:116)(cid:114)a(cid:118)e(cid:114)sa(cid:108) (cid:114)ea(cid:99)(cid:104)es a maxim(cid:117)m (cid:111)(cid:102) (cid:49)
(cid:51) (cid:104)(cid:111)ps i(cid:110) a
(cid:99)(cid:121)(cid:99)(cid:108)e a(cid:116) a (cid:99)(cid:108)(cid:111)(cid:99)k (cid:102)(cid:114)e(cid:113)(cid:117)e(cid:110)(cid:99)(cid:121) (cid:111)(cid:102) (cid:49)(cid:71)(cid:72)(cid:122)
(cid:46) (cid:91)(cid:49)
(cid:49)(cid:93)(cid:44) a(cid:110)(cid:100) is (cid:117)(cid:110)a(cid:98)(cid:108)e (cid:116)(cid:111) (cid:103)(cid:117)a(cid:114)a(cid:110)(cid:116)ee
a (cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116) (cid:119)i(cid:116)(cid:104)i(cid:110) a (cid:99)(cid:121)(cid:99)(cid:108)e(cid:46)
(cid:49) (cid:79)(cid:82)(cid:73) (cid:82)(cid:111)(cid:117)(cid:116)e(cid:114) (cid:77)i(cid:99)(cid:114)(cid:111)a(cid:114)(cid:99)(cid:104)i(cid:116)e(cid:99)(cid:116)(cid:117)(cid:114)e
(cid:65)(cid:116) (cid:116)(cid:104)e (cid:98)e(cid:103)i(cid:110)(cid:110)i(cid:110)(cid:103) (cid:111)(cid:102) a (cid:116)ime(cid:45)(cid:119)i(cid:110)(cid:100)(cid:111)(cid:119)(cid:44) a(cid:108)(cid:108) s(cid:111)(cid:117)(cid:114)(cid:99)e (cid:110)(cid:111)(cid:100)es (cid:119)i(cid:116)(cid:104) a (cid:3)i(cid:116)
(cid:119)ai(cid:116)i(cid:110)(cid:103) (cid:102)(cid:111)(cid:114) (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k i(cid:110)(cid:106)e(cid:99)(cid:116)i(cid:111)(cid:110)(cid:44) (cid:100)e(cid:116)e(cid:114)mi(cid:110)e (cid:108)(cid:111)(cid:99)a(cid:108)(cid:108)(cid:121) (cid:116)(cid:104)e a(cid:108)(cid:108)(cid:111)(cid:119)e(cid:100) i(cid:110)(cid:106)e(cid:99)(cid:45)
(cid:116)i(cid:111)(cid:110) S(cid:73)(cid:68)s (cid:102)(cid:111)(cid:114) (cid:116)(cid:104)is i(cid:110)(cid:116)e(cid:114)(cid:118)a(cid:108)(cid:46) (cid:73)(cid:102) a(cid:108)(cid:108)(cid:111)(cid:119)e(cid:100)(cid:44) (cid:116)(cid:104)e s(cid:111)(cid:117)(cid:114)(cid:99)e p(cid:114)(cid:111)(cid:99)ee(cid:100)s (cid:119)i(cid:116)(cid:104) (cid:116)(cid:104)e
(cid:3)i(cid:116) i(cid:110)(cid:106)e(cid:99)(cid:116)i(cid:111)(cid:110) i(cid:110)(cid:116)(cid:111) (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:46) (cid:84)(cid:104)e (cid:3)i(cid:116) e(cid:110)(cid:116)e(cid:114)s (cid:116)(cid:104)e (cid:114)(cid:111)(cid:117)(cid:116)e(cid:114)(cid:44) (cid:102)(cid:111)(cid:114)ks (cid:116)(cid:104)e (cid:3)i(cid:116) (cid:116)(cid:111)
(cid:98)e se(cid:110)(cid:116) (cid:116)(cid:111) (cid:116)(cid:104)e (cid:108)(cid:111)(cid:99)a(cid:108) N(cid:73)(cid:67)
(cid:44) a(cid:110)(cid:100) p(cid:114)(cid:111)(cid:99)ee(cid:100)s (cid:116)(cid:111) (cid:116)(cid:104)e (cid:110)ex(cid:116) (cid:114)(cid:111)(cid:117)(cid:116)e(cid:114) a(cid:108)(cid:111)(cid:110)(cid:103) (cid:116)(cid:104)e
m(cid:117)(cid:108)(cid:116)i(cid:45)(cid:104)(cid:111)p (cid:98)(cid:121)pass pa(cid:116)(cid:104)(cid:46) (cid:84)(cid:104)e (cid:3)i(cid:116) (cid:99)(cid:111)(cid:110)(cid:116)i(cid:110)(cid:117)es (cid:117)(cid:110)(cid:116)i(cid:108) i(cid:116) (cid:114)ea(cid:99)(cid:104)es
H P Cmax
(cid:104)(cid:111)ps a(cid:110)(cid:100) is (cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100) a(cid:116) (cid:116)(cid:104)a(cid:116) (cid:114)(cid:111)(cid:117)(cid:116)e(cid:114)(cid:46) (cid:84)(cid:104)e
e(cid:110)a(cid:98)(cid:108)e (cid:98)(cid:121)passi(cid:110)(cid:103)
si(cid:103)(cid:110)a(cid:108)s a(cid:114)e
se(cid:116) (cid:119)i(cid:116)(cid:104)(cid:111)(cid:117)(cid:116) a (cid:114)ese(cid:114)(cid:118)a(cid:116)i(cid:111)(cid:110) (cid:114)e(cid:113)(cid:117)es(cid:116)(cid:46) Ea(cid:99)(cid:104) (cid:110)(cid:111)(cid:100)e is a(cid:119)a(cid:114)e (cid:111)(cid:102) (cid:116)(cid:104)e a(cid:108)(cid:108)(cid:111)(cid:119)e(cid:100)
S(cid:73)(cid:68)s (cid:116)(cid:104)is (cid:116)ime(cid:45)(cid:119)i(cid:110)(cid:100)(cid:111)(cid:119) a(cid:110)(cid:100) (cid:119)(cid:104)e(cid:116)(cid:104)e(cid:114) i(cid:116) s(cid:104)(cid:111)(cid:117)(cid:108)(cid:100) a(cid:108)(cid:108)(cid:111)(cid:119) a (cid:98)(cid:121)pass(cid:44) (cid:108)a(cid:116)(cid:99)(cid:104)
(cid:116)(cid:104)e i(cid:110)(cid:99)(cid:111)mi(cid:110)(cid:103) (cid:3)i(cid:116) (cid:44) (cid:111)(cid:114) i(cid:110)(cid:106)e(cid:99)(cid:116) a (cid:3)i(cid:116) (cid:102)(cid:114)(cid:111)m (cid:116)(cid:104)e (cid:108)(cid:111)(cid:99)a(cid:108) N(cid:73)(cid:67)
(cid:70)i(cid:103)(cid:117)(cid:114)e (cid:50) s(cid:104)(cid:111)(cid:119)s (cid:116)(cid:104)e mi(cid:99)(cid:114)(cid:111)a(cid:114)(cid:99)(cid:104)i(cid:116)e(cid:99)(cid:116)(cid:117)(cid:114)e (cid:111)(cid:102) (cid:116)(cid:104)e (cid:79)(cid:82)(cid:73) (cid:114)(cid:111)(cid:117)(cid:116)e(cid:114)(cid:46) (cid:73)(cid:110) a(cid:100)(cid:100)i(cid:45)
(cid:116)i(cid:111)(cid:110) (cid:116)(cid:111) (cid:116)(cid:104)e si(cid:110)(cid:103)(cid:108)e i(cid:110)p(cid:117)(cid:116) a(cid:110)(cid:100) (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:108)i(cid:110)k (cid:111)(cid:102) (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:44) (cid:116)(cid:104)e N(cid:73)(cid:67) i(cid:110)p(cid:117)(cid:116)
(cid:108)i(cid:110)k (cid:102)(cid:111)(cid:114) i(cid:110)(cid:106)e(cid:99)(cid:116)i(cid:111)(cid:110) i(cid:110)(cid:116)(cid:111) (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k(cid:44) a(cid:110)(cid:100) N(cid:73)(cid:67) (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:108)i(cid:110)k (cid:102)(cid:111)(cid:114) e(cid:106)e(cid:99)(cid:116)i(cid:111)(cid:110)
(cid:102)(cid:114)(cid:111)m (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k is s(cid:104)(cid:111)(cid:119)(cid:110)(cid:46) (cid:87)(cid:104)e(cid:110) a (cid:3)i(cid:116) a(cid:114)(cid:114)i(cid:118)es a(cid:116) (cid:116)(cid:104)e (cid:114)(cid:111)(cid:117)(cid:116)e(cid:114)(cid:44) i(cid:116) ei(cid:116)(cid:104)e(cid:114)
(cid:98)(cid:121)passes (cid:116)(cid:104)(cid:114)(cid:111)(cid:117)(cid:103)(cid:104) (cid:116)(cid:111) (cid:116)(cid:104)e as(cid:121)(cid:110)(cid:99)(cid:104)(cid:114)(cid:111)(cid:110)(cid:111)(cid:117)s (cid:114)epea(cid:116)e(cid:114)s s(cid:104)(cid:111)(cid:119)(cid:110)(cid:44) (cid:111)(cid:114) (cid:98)(cid:117)(cid:102)(cid:102)e(cid:114)s i(cid:110)
(cid:116)(cid:104)e (cid:108)a(cid:116)(cid:99)(cid:104) (cid:102)(cid:111)(cid:114) (cid:111)(cid:110)e (cid:99)(cid:121)(cid:99)(cid:108)e(cid:46) (cid:84)(cid:104)(cid:114)ee (cid:99)(cid:111)(cid:110)(cid:116)(cid:114)(cid:111)(cid:108) si(cid:103)(cid:110)a(cid:108)s ma(cid:110)a(cid:103)e (cid:116)(cid:104)e N(cid:73)(cid:67) i(cid:110)(cid:106)e(cid:99)(cid:45)
I njEN (cid:44) BypEN (cid:44) RSEL (cid:46) (cid:87)(cid:104)e(cid:110) I njEN
is asse(cid:114)(cid:116)e(cid:100)(cid:44) (cid:116)(cid:104)e N(cid:73)(cid:67) is a(cid:98)(cid:108)e (cid:116)(cid:111) se(cid:110)(cid:100) (cid:116)(cid:104)e (cid:3)i(cid:116) i(cid:110)(cid:116)(cid:111) (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k a(cid:110)(cid:100) (cid:116)(cid:111) (cid:116)(cid:104)e
(cid:116)i(cid:111)(cid:110) a(cid:110)(cid:100) (cid:98)(cid:121)passi(cid:110)(cid:103) se(cid:116)(cid:117)p (cid:58)
m(cid:117)(cid:108)(cid:116)ip(cid:108)exe(cid:114)s (cid:119)(cid:104)e(cid:114)e (cid:116)(cid:104)e (cid:3)i(cid:116) is (cid:102)(cid:111)(cid:114)ke(cid:100) (cid:116)(cid:111) (cid:116)(cid:104)e (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:108)i(cid:110)ks(cid:46) Simi(cid:108)a(cid:114)(cid:108)(cid:121)
(cid:116)(cid:104)e i(cid:110)(cid:99)(cid:111)mi(cid:110)(cid:103) (cid:3)i(cid:116) (cid:102)(cid:114)(cid:111)m (cid:116)(cid:104)e (cid:87)es(cid:116) p(cid:111)(cid:114)(cid:116) is a(cid:98)(cid:108)e (cid:116)(cid:111) (cid:98)(cid:121)pass (cid:119)(cid:104)e(cid:110)
BypEN
is asse(cid:114)(cid:116)e(cid:100) a(cid:110)(cid:100) (cid:116)(cid:104)e m(cid:117)(cid:108)(cid:116)ip(cid:108)exe(cid:114) (cid:108)(cid:111)(cid:103)i(cid:99) (cid:102)(cid:111)(cid:114)ks (cid:116)(cid:104)e (cid:3)i(cid:116) (cid:116)(cid:111) (cid:116)(cid:104)e (cid:108)(cid:111)(cid:99)a(cid:108) N(cid:73)(cid:67)
a(cid:110)(cid:100) (cid:116)(cid:104)e Eas(cid:116) (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:100)i(cid:114)e(cid:99)(cid:116)i(cid:111)(cid:110)(cid:46) RSEL m(cid:117)s(cid:116) (cid:98)e se(cid:116) (cid:116)(cid:111) a(cid:108)(cid:108)(cid:111)(cid:119) ei(cid:116)(cid:104)e(cid:114) (cid:116)(cid:104)e
(cid:3)i(cid:116) (cid:102)(cid:114)(cid:111)m (cid:116)(cid:104)e N(cid:73)(cid:67) (cid:111)(cid:114) (cid:116)(cid:104)e
(cid:87)es(cid:116) i(cid:110)p(cid:117)(cid:116) (cid:116)(cid:111) (cid:98)e se(cid:110)(cid:116) (cid:116)(cid:104)(cid:114)(cid:111)(cid:117)(cid:103)(cid:104) (cid:116)(cid:111) (cid:99)(cid:111)(cid:110)(cid:116)i(cid:110)(cid:117)e
(cid:116)(cid:104)e (cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116)(cid:46) (cid:72)e(cid:110)(cid:99)e(cid:44) (cid:116)(cid:104)e si(cid:103)(cid:110)a(cid:108)s a(cid:114)e assi(cid:103)(cid:110)e(cid:100) (cid:116)(cid:111) e(cid:108)imi(cid:110)a(cid:116)e (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116)
(cid:108)i(cid:110)k (cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:116)i(cid:111)(cid:110)(cid:46)
(cid:50) (cid:87)a(cid:108)k(cid:116)(cid:104)(cid:114)(cid:111)(cid:117)(cid:103)(cid:104) Examp(cid:108)e
(cid:70)i(cid:103)(cid:117)(cid:114)e (cid:51) s(cid:104)(cid:111)(cid:119)s (cid:116)(cid:104)e (cid:98)(cid:117)(cid:102)(cid:102)e(cid:114)(cid:108)ess (cid:79)(cid:82)(cid:73) N(cid:111)(cid:67) a(cid:110)(cid:100) (cid:110)(cid:111)(cid:116)i(cid:2)(cid:99)a(cid:116)i(cid:111)(cid:110) (cid:110)e(cid:116)(cid:45)
(cid:119)(cid:111)(cid:114)k (cid:91)(cid:55)(cid:93)(cid:44) (cid:99)(cid:111)(cid:110)(cid:116)ai(cid:110)i(cid:110)(cid:103) e(cid:110)(cid:104)a(cid:110)(cid:99)e(cid:100) (cid:99)(cid:108)(cid:111)(cid:99)k(cid:45)(cid:108)ess (cid:108)i(cid:110)ks(cid:46) (cid:77)(cid:111)(cid:114)e (cid:116)(cid:104)a(cid:110) (cid:111)(cid:110)e
(cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116) (cid:3)i(cid:116) is a(cid:98)(cid:108)e (cid:116)(cid:111) sim(cid:117)(cid:108)(cid:116)a(cid:110)e(cid:111)(cid:117)s(cid:108)(cid:121) (cid:117)(cid:116)i(cid:108)i(cid:122)e (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:44) p(cid:114)(cid:111)(cid:118)i(cid:100)e(cid:100) (cid:116)(cid:104)e
(cid:3)i(cid:116)s (cid:100)(cid:111) (cid:110)(cid:111)(cid:116) (cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:100) (cid:102)(cid:111)(cid:114) (cid:116)(cid:104)e same (cid:108)i(cid:110)ks(cid:46) (cid:73)(cid:102) s(cid:111)(cid:117)(cid:114)(cid:99)e (cid:110)(cid:111)(cid:100)e (cid:48) is a(cid:98)(cid:108)e (cid:116)(cid:111)
i(cid:110)(cid:106)e(cid:99)(cid:116)(cid:44) i(cid:116) is a(cid:98)(cid:108)e (cid:116)(cid:111) (cid:116)(cid:114)a(cid:118)e(cid:114)se (cid:117)p (cid:116)(cid:111) H P Cmax (cid:104)(cid:111)ps a(cid:119)a(cid:121) (cid:98)e(cid:102)(cid:111)(cid:114)e (cid:98)ei(cid:110)(cid:103)
(cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100)(cid:46) (cid:87)i(cid:116)(cid:104) a H P Cmax (cid:111)(cid:102) (cid:52)
(cid:44) (cid:116)(cid:104)e (cid:3)i(cid:116) (cid:102)(cid:114)(cid:111)m
S ID = 0 (cid:99)
(cid:100)(cid:46) (cid:84)
(cid:101) 7(cid:44) (cid:119)
(cid:115)(cid:46) (cid:84)
ea(cid:99)(cid:104)(cid:46) (cid:84)(cid:104)(cid:117)s (cid:44)
(cid:3)i(cid:116)s (cid:102)(cid:114)(cid:111)m (cid:110)(cid:111)(cid:100)es (cid:55)
(cid:44) a(cid:110)(cid:100) (cid:49)
H P Cmax (cid:104)(cid:111)ps
(cid:51) a(cid:114)e a(cid:108)(cid:108)(cid:111)(cid:119)e(cid:100) (cid:116)(cid:111) i(cid:110)(cid:106)e(cid:99)(cid:116) a(cid:116)
(cid:116)(cid:104)e same (cid:116)ime as (cid:110)(cid:111)(cid:100)e (cid:48)
(cid:70)(cid:111)(cid:114) a s(cid:116)a(cid:116)i(cid:99) (cid:84)(cid:68)(cid:77) (cid:114)i(cid:110)(cid:103) a(cid:99)(cid:99)ess assi(cid:103)(cid:110)me(cid:110)(cid:116)(cid:44) (cid:117)p (cid:116)(cid:111)
N/H P Cmax
(cid:3)i(cid:116)s (cid:116)(cid:114)a(cid:118)e(cid:114)se (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k(cid:44) e(cid:118)e(cid:114)(cid:121) (cid:99)(cid:121)(cid:99)(cid:108)e(cid:44) (cid:119)i(cid:116)(cid:104)(cid:111)(cid:117)(cid:116) (cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:116)i(cid:111)(cid:110)(cid:46) (cid:84)(cid:104)e
s(cid:116)a(cid:116)i(cid:99) (cid:84)(cid:68)(cid:77) assi(cid:103)(cid:110)me(cid:110)(cid:116) (cid:99)(cid:121)(cid:99)(cid:108)es (cid:116)(cid:104)(cid:114)(cid:111)(cid:117)(cid:103)(cid:104) a(cid:108)(cid:108) (cid:110)(cid:111)(cid:110)(cid:45)(cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:100)i(cid:110)(cid:103) (cid:110)(cid:111)(cid:100)e
se(cid:116)s(cid:44) (cid:99)(cid:111)(cid:118)e(cid:114)i(cid:110)(cid:103) a(cid:108)(cid:108) (cid:116)(cid:104)e p(cid:111)(cid:116)e(cid:110)(cid:116)ia(cid:108) (cid:114)e(cid:113)(cid:117)es(cid:116) s(cid:111)(cid:117)(cid:114)(cid:99)es i(cid:110) (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k(cid:46) (cid:70)(cid:111)(cid:114)
i(cid:110)s(cid:116)a(cid:110)(cid:99)e(cid:44) a (cid:49)
(cid:45)(cid:110)(cid:111)(cid:100)e N(cid:111)(cid:67) (cid:104)as (cid:116)(cid:104)e (cid:102)(cid:111)(cid:108)(cid:108)(cid:111)(cid:119)i(cid:110)(cid:103) (cid:110)(cid:111)(cid:110)(cid:45)(cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:100)i(cid:110)(cid:103) se(cid:116)s (cid:111)(cid:102)
(cid:110)(cid:111)(cid:100)es(cid:58) (cid:91) {0, 7, 10, 13}(cid:44){1, 6, 11, 12}(cid:44){2, 5, 8, 15}(cid:44){3, 4, 9, 14}(cid:93)(cid:44)
(cid:105)(cid:99)(cid:104) (cid:97)(cid:99)(cid:99)(cid:111)
(cid:116) {(cid:111)
(cid:114) (cid:97)(cid:108)
(cid:111)(cid:100)(cid:101)(cid:115)
(cid:104)(cid:101) (cid:110)(cid:101)(cid:116)
(cid:107)(cid:46) (cid:68) (cid:117)
(cid:110)} (cid:116)
(cid:104)(cid:101) ×(cid:114)
(cid:109)(cid:101)(cid:45)
(cid:110)(cid:100)(cid:111)
(cid:119)(cid:44) (cid:116)
(cid:104)(cid:101) × (cid:114)
(cid:115)(cid:101)(cid:116)(cid:44) (cid:123)(cid:48)(cid:44)(cid:55)(cid:44)(cid:49)(cid:48)(cid:44)(cid:49)(cid:51) (cid:125)(cid:44) (cid:105)
(cid:115) }(cid:114)(cid:97)(cid:110)
(cid:116)(cid:101)(cid:100) (cid:97)(cid:99)(cid:99)(cid:101) (cid:115)
(cid:104)(cid:101) (cid:114)
(cid:110)}(cid:46) (cid:84)(cid:104)(cid:101)
(cid:115)(cid:101)(cid:99)(cid:111)
(cid:110)(cid:100) (cid:116)
(cid:109)(cid:101)(cid:45)(cid:119)
(cid:110)(cid:100)(cid:111)
(cid:119)(cid:44) (cid:116)
(cid:104)(cid:101) (cid:114)
(cid:110)} (cid:97)(cid:108)
(cid:123)(cid:49)(cid:44)(cid:54)(cid:44)(cid:49)(cid:49)(cid:44)(cid:49)(cid:50)(cid:125)
(cid:111)(cid:100)(cid:101)(cid:115)
(cid:106)(cid:101)(cid:99)(cid:116)(cid:46)
(cid:84)(cid:104)
(cid:111)(cid:99)(cid:101)(cid:115)
(cid:115) (cid:99)(cid:111)
(cid:117)(cid:101)(cid:115)
(cid:104)(cid:101)(cid:110)
(cid:116)(cid:99)(cid:104)
(cid:110)} (cid:116)
(cid:104)(cid:101) (cid:110)(cid:101)(cid:120)
(cid:109)(cid:101)(cid:45)(cid:119)
(cid:110)(cid:100)(cid:111)
(cid:119) (cid:97) (cid:110)(cid:100)
(cid:114)(cid:101)(cid:112)(cid:101)(cid:97)(cid:116)
(cid:115)(cid:46) (cid:65)(cid:116)
(cid:118)(cid:97)(cid:114)
(cid:105)(cid:101)(cid:100) (cid:116)
(cid:109)(cid:101)(cid:115) (cid:98)(cid:101){(cid:111)
(cid:114)(cid:101) (cid:84)(cid:48)(cid:44) (cid:99)(cid:111)
(cid:114)(cid:101) (cid:115) (cid:48)(cid:44) (cid:50)(cid:44) (cid:55)(cid:44) (cid:56)(cid:44) (cid:49)(cid:51)(cid:44) (cid:97)(cid:110)(cid:100) (cid:49)(cid:53) (cid:111)(cid:98)(cid:45)
(cid:115)(cid:101)(cid:114)
(cid:118)(cid:101) (cid:99)(cid:97)(cid:99)(cid:104)(cid:101) (cid:109)
(cid:115)(cid:101)(cid:115) (cid:97)(cid:110)(cid:100) (cid:115)(cid:101) (cid:110)(cid:100) (cid:98)(cid:114)
(cid:111)(cid:97)(cid:100)(cid:99)(cid:97)(cid:115)
(cid:116) (cid:99)(cid:111)
(cid:104)(cid:101) (cid:114)(cid:101)(cid:110)
(cid:114)(cid:101)(cid:113)
(cid:117)(cid:101)(cid:115)
(cid:104)(cid:101) (cid:78)(cid:73)(cid:67)(cid:46)
(cid:69)(cid:97)(cid:99)(cid:104)
(cid:111)(cid:100)(cid:101) (cid:99)(cid:111)
(cid:116)(cid:101)(cid:110)
(cid:121) (cid:100)(cid:101)(cid:116)(cid:101)(cid:114)
(cid:110)(cid:101)(cid:115)
(cid:111) (cid:97)(cid:108)
(cid:111)(cid:100)(cid:101)(cid:115)
{0, 7, 10, 13} (cid:116)
N (cid:99)y(cid:99)(cid:108)(cid:101)(cid:115)(cid:44) (cid:119)(cid:104)(cid:101)(cid:114)(cid:101)N (cid:105)(cid:115) (cid:116)(cid:104)(cid:101) n(cid:117)(cid:109)(cid:98)(cid:101)(cid:114) (cid:111)(cid:102) n(cid:111)(cid:100)(cid:101)(cid:115) (cid:46)
(cid:115)(cid:44) (cid:116)
(cid:69)(cid:97)(cid:99)(cid:104) (cid:98)(cid:114)(cid:111)(cid:97)(cid:100)(cid:99)(cid:97)(cid:115)(cid:116) (cid:99)(cid:111)(cid:104)(cid:101)(cid:114)(cid:101)n(cid:116) (cid:114)(cid:101)(cid:113)(cid:117)(cid:101)(cid:115)(cid:116) (cid:103)(cid:101)n(cid:101)(cid:114)(cid:97)(cid:116)(cid:101)(cid:100) (cid:119)(cid:97)(cid:105)(cid:116)(cid:115) (cid:105)n (cid:116)(cid:104)(cid:101) (cid:108)(cid:111)(cid:99)(cid:97)(cid:108) NIC
(cid:117)n(cid:116)(cid:105)(cid:108) (cid:116)(cid:104)(cid:101) (cid:116)(cid:105)(cid:109)(cid:101) (cid:119)(cid:105)n(cid:100)(cid:111)(cid:119) (cid:105)n (cid:119)(cid:104)(cid:105)(cid:99)(cid:104) (cid:105)(cid:116) (cid:105)(cid:115) (cid:97)(cid:108)(cid:108)(cid:111)(cid:119)(cid:101)(cid:100) (cid:116)(cid:111) (cid:105)nj(cid:101)(cid:99)(cid:116)(cid:46) T(cid:104)(cid:101) (cid:115)(cid:111)(cid:117)(cid:114)(cid:99)(cid:101)
ID(cid:115) (cid:40)SID(cid:41)
(cid:44) (cid:97)(cid:108)(cid:108)(cid:111)(cid:119)(cid:101)(cid:100) (cid:116)(cid:111) (cid:97)(cid:99)(cid:99)(cid:101)(cid:115)(cid:115) (cid:116)(cid:104)(cid:101) (cid:114)(cid:105)n(cid:103) (cid:99)(cid:104)(cid:97)n(cid:103)(cid:101)(cid:115) (cid:101)(cid:97)(cid:99)(cid:104) (cid:116)(cid:105)(cid:109)(cid:101)(cid:45)(cid:119)(cid:105)n(cid:100)(cid:111)(cid:119)(cid:44)
(cid:115)(cid:117)(cid:99)(cid:104) (cid:116)(cid:104)(cid:97)(cid:116) (cid:97)(cid:108)(cid:108) (cid:116)(cid:104)(cid:101) (cid:115)(cid:111)(cid:117)(cid:114)(cid:99)(cid:101)(cid:115) (cid:97)(cid:114)(cid:101) (cid:103)(cid:105)(cid:118)(cid:101)n (cid:97) (cid:99)(cid:104)(cid:97)n(cid:99)(cid:101) (cid:116)(cid:111) (cid:105)nj(cid:101)(cid:99)(cid:116) (cid:97) (cid:3)(cid:105)(cid:116) (cid:105)n(cid:116)(cid:111) (cid:116)(cid:104)(cid:101)
n(cid:101)(cid:116)(cid:119)(cid:111)(cid:114)k(cid:46) T(cid:104)(cid:101) (cid:99)(cid:108)(cid:111)(cid:99)k(cid:45)(cid:108)(cid:101)(cid:115)(cid:115) (cid:114)(cid:101)p(cid:101)(cid:97)(cid:116)(cid:101)(cid:100) (cid:108)(cid:105)nk(cid:115) (cid:97)(cid:114)(cid:101) (cid:117)(cid:116)(cid:105)(cid:108)(cid:105)(cid:122)(cid:101)(cid:100) (cid:116)(cid:111) (cid:101)n(cid:97)(cid:98)(cid:108)(cid:101) (cid:109)(cid:117)(cid:108)(cid:116)(cid:105)(cid:45)
(cid:104)(cid:111)p (cid:116)(cid:114)(cid:97)(cid:118)(cid:101)(cid:114)(cid:115)(cid:97)(cid:108) (cid:105)n (cid:97) (cid:115)(cid:105)n(cid:103)(cid:108)(cid:101) (cid:99)y(cid:99)(cid:108)(cid:101)(cid:46) T(cid:104)(cid:117)(cid:115) (cid:44) (cid:102)(cid:111)(cid:114) (cid:97)n N(cid:45)n(cid:111)(cid:100)(cid:101) n(cid:101)(cid:116)(cid:119)(cid:111)(cid:114)k(cid:44) (cid:116)(cid:104)(cid:101)
(cid:118)(cid:105)(cid:114)(cid:116)(cid:117)(cid:97)(cid:108) (cid:114)(cid:105)n(cid:103) (cid:108)(cid:97)(cid:116)(cid:101)n(cid:99)y (cid:102)(cid:111)(cid:114) (cid:97) (cid:98)(cid:114)(cid:111)(cid:97)(cid:100)(cid:99)(cid:97)(cid:115)(cid:116) (cid:105)(cid:115)N/H P Cmax (cid:99)(cid:121)(cid:99)(cid:108)es(cid:44) (cid:119)(cid:104)e(cid:114)e
H P Cmax is (cid:116)(cid:104)e maxim(cid:117)m (cid:110)(cid:117)m(cid:98)e(cid:114) (cid:111)(cid:102) (cid:104)(cid:111)ps (cid:116)(cid:114)a(cid:118)e(cid:114)se(cid:100) i(cid:110) a (cid:99)(cid:121)(cid:99)(cid:108)e(cid:46)
(cid:71)e(cid:110)e(cid:114)a(cid:108)(cid:108)(cid:121)
(cid:44) (cid:84)(cid:68)(cid:77)(cid:45)(cid:98)ase(cid:100) a(cid:114)(cid:98)i(cid:116)(cid:114)a(cid:116)i(cid:111)(cid:110) is pe(cid:114)(cid:102)(cid:111)(cid:114)me(cid:100) a(cid:116) (cid:116)(cid:104)e s(cid:111)(cid:117)(cid:114)(cid:99)es (cid:111)(cid:114)
i(cid:110)(cid:100)i(cid:118)i(cid:100)(cid:117)a(cid:108) (cid:110)(cid:111)(cid:100)es as (cid:116)(cid:104)e (cid:114)e(cid:113)(cid:117)es(cid:116)s a(cid:114)(cid:98)i(cid:116)(cid:114)a(cid:116)e (cid:102)(cid:111)(cid:114) a(cid:99)(cid:99)ess (cid:116)(cid:111) (cid:116)(cid:104)e (cid:118)i(cid:114)(cid:116)(cid:117)a(cid:108)
(cid:99)(cid:104)a(cid:110)(cid:110)e(cid:108)s (cid:44) (cid:108)i(cid:110)ks(cid:44) (cid:111)(cid:114) s(cid:119)i(cid:116)(cid:99)(cid:104)es(cid:46) (cid:72)(cid:111)(cid:119)e(cid:118)e(cid:114)(cid:44) i(cid:110) a (cid:98)(cid:117)(cid:102)(cid:102)e(cid:114)(cid:108)ess (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k (cid:116)(cid:104)e
(cid:102)ai(cid:108)e(cid:100) a(cid:99)(cid:99)ess (cid:116)(cid:111) (cid:100)esi(cid:114)e(cid:100) (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:108)i(cid:110)ks (cid:100)e(cid:103)(cid:114)a(cid:100)es pe(cid:114)(cid:102)(cid:111)(cid:114)ma(cid:110)(cid:99)e (cid:44) as (cid:3)i(cid:116)s
a(cid:114)e (cid:100)e(cid:3)e(cid:99)(cid:116)e(cid:100) i(cid:110) p(cid:111)(cid:116)e(cid:110)(cid:116)ia(cid:108)(cid:108) (cid:121) (cid:110)(cid:111)(cid:110)(cid:45)p(cid:114)(cid:111)(cid:103)(cid:114)essi(cid:118)e (cid:100)i(cid:114)e(cid:99)(cid:116)i(cid:111)(cid:110)s (cid:46) (cid:84)(cid:104)e (cid:84) (cid:68)(cid:77)(cid:45)
(cid:98)ase(cid:100) me(cid:99)(cid:104)a(cid:110)ism m(cid:117)s(cid:116) (cid:103)(cid:117)a(cid:114)a(cid:110)(cid:116)ee a(cid:110) e(cid:110)(cid:100)(cid:45)(cid:116)(cid:111)(cid:45)e(cid:110)(cid:100)(cid:44) (cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:116)i(cid:111)(cid:110)(cid:45)(cid:102)(cid:114)ee
pa(cid:116)(cid:104) (cid:102)(cid:111)(cid:114) e(cid:102)(cid:2)(cid:99)ie(cid:110)(cid:116) (cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116) (cid:99)(cid:111)mm(cid:117)(cid:110)i(cid:99)a(cid:116)i(cid:111)(cid:110)(cid:46) E(cid:118)e(cid:110) (cid:119)i(cid:116)(cid:104) (cid:99)(cid:108)(cid:111)(cid:99)k(cid:45)(cid:108)ess
(cid:51)
(cid:46)
(cid:46)
(cid:44)
(cid:51)
(cid:46)
(cid:97)
(cid:110)
(cid:114)
(cid:101)
(cid:97)
(cid:99)
(cid:104)
(cid:117)
(cid:112)
(cid:116)
(cid:111)
(cid:110)
(cid:111)
(cid:100)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:105)
(cid:116)
(cid:104)
(cid:97)
(cid:115)
(cid:116)
(cid:111)
(cid:98)
(cid:101)
(cid:108)
(cid:97)
(cid:116)
(cid:99)
(cid:104)
(cid:101)
(cid:104)
(cid:101)
(cid:114)
(cid:101)
(cid:115)
(cid:116)
(cid:111)
(cid:102)
(cid:116)
(cid:104)
(cid:101)
(cid:114)
(cid:105)
(cid:110)
(cid:103)
(cid:105)
(cid:115)
(cid:105)
(cid:100)
(cid:108)
(cid:101)
(cid:97)
(cid:110)
(cid:100)
(cid:99)
(cid:97)
(cid:110)
(cid:97)
(cid:99)
(cid:99)
(cid:111)
(cid:109)
(cid:109)
(cid:111)
(cid:100)
(cid:97)
(cid:116)
(cid:101)
(cid:97)
(cid:100)
(cid:100)
(cid:105)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:97)
(cid:108)
(cid:3)
(cid:105)
(cid:116)
(cid:111)
(cid:109)
(cid:97)
(cid:120)
(cid:105)
(cid:109)
(cid:105)
(cid:122)
(cid:101)
(cid:115)
(cid:105)
(cid:109)
(cid:117)
(cid:108)
(cid:116)
(cid:97)
(cid:110)
(cid:101)
(cid:111)
(cid:117)
(cid:115)
(cid:3)
(cid:105)
(cid:116)
(cid:105)
(cid:110)
(cid:106)
(cid:101)
(cid:99)
(cid:116)
(cid:105)
(cid:111)
(cid:110)
(cid:104)
(cid:101)
(cid:114)
(cid:105)
(cid:110)
(cid:103)
(cid:105)
(cid:115)
(cid:100)
(cid:101)
(cid:109)
(cid:97)
(cid:114)
(cid:99)
(cid:97)
(cid:116)
(cid:101)
(cid:100)
(cid:105)
(cid:110)
(cid:116)
(cid:111)
(cid:114)
(cid:101)
(cid:103)
(cid:105)
(cid:111)
(cid:110)
(cid:115)
(cid:111)
(cid:102)
(cid:44)
(cid:49)
(cid:48)
(cid:46)
(cid:54)
(cid:119)
(cid:104)
(cid:117)
(cid:110)
(cid:108)
(cid:110)
(cid:105)
(cid:110)
(cid:116)
(cid:119)
(cid:111)
(cid:114)
(cid:114)
(cid:105)
(cid:115)
(cid:116)
(cid:116)
(cid:105)
(cid:119)
(cid:105)
(cid:115)
(cid:116)
(cid:115)
(cid:116)
(cid:111)
(cid:116)
(cid:105)
(cid:105)
(cid:105)
(cid:105)
(cid:108)
(cid:111)
(cid:119)
(cid:115)
(cid:110)
(cid:116)
(cid:111)
(cid:105)
(cid:110)
(cid:105)
(cid:115)
(cid:112)
(cid:114)
(cid:110)
(cid:116)
(cid:105)
(cid:110)
(cid:119)
(cid:115)
(cid:119)
(cid:105)
(cid:105)
(cid:111)
(cid:116)
(cid:116)
(cid:116)
(cid:105)
(cid:105)
(cid:105)
(cid:105)
(cid:115)
(cid:116)
(cid:116)
(cid:115)
(cid:116)
(cid:111)
(cid:116)
(cid:110)
(cid:110)
(cid:115)
(cid:105)
(cid:115)
(cid:116)
(cid:108)
(cid:109)
(cid:105)
(cid:116)
(cid:108)
(cid:111)
(cid:119)
(cid:110)
(cid:111)
I njEN (cid:116)(cid:111) (cid:49)
a(cid:110)(cid:100) (cid:116)(cid:104)e RSEL (cid:116)(cid:111) (cid:49) s(cid:117)(cid:99)(cid:104) (cid:116)(cid:104)a(cid:116) (cid:116)(cid:104)e N(cid:73)(cid:67)
(cid:39)s (cid:3)i(cid:116) e(cid:110)(cid:116)e(cid:114)s (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k(cid:46) (cid:84)(cid:104)e
BypEN is se(cid:116) (cid:116)(cid:111) (cid:48) (cid:116)(cid:111) e(cid:108)imi(cid:110)a(cid:116)e (cid:111)(cid:117)(cid:116)p(cid:117)(cid:116) (cid:108)i(cid:110)k (cid:99)(cid:111)(cid:110)(cid:116)e(cid:110)(cid:116)i(cid:111)(cid:110) a(cid:110)(cid:100) e(cid:110)s(cid:117)(cid:114)e
(cid:116)(cid:104)e (cid:3)i(cid:116) is (cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100) (cid:116)(cid:111) (cid:98)e se(cid:110)(cid:116) (cid:116)(cid:104)e (cid:102)(cid:111)(cid:108)(cid:108)(cid:111)(cid:119)i(cid:110)(cid:103) (cid:99)(cid:121)(cid:99)(cid:108)e(cid:46)
(cid:65)(cid:108)(cid:108) (cid:111)(cid:116)(cid:104)e(cid:114) (cid:110)(cid:111)(cid:100)es a(cid:114)e
a(cid:119)a(cid:114)e (cid:116)(cid:104)a(cid:116) (cid:116)(cid:104)e(cid:121) a(cid:114)e (cid:110)(cid:111)(cid:116) a(cid:108)(cid:108)(cid:111)(cid:119)e(cid:100) (cid:116)(cid:111) i(cid:110)(cid:106)e(cid:99)(cid:116) (cid:116)(cid:104)is (cid:116)ime (cid:119)i(cid:110)(cid:100)(cid:111)(cid:119) a(cid:110)(cid:100) e(cid:110)(cid:45)
BypEN (cid:116)(cid:111) (cid:49)
(cid:68)(cid:117)(cid:114)i(cid:110)(cid:103) (cid:116)(cid:104)e (cid:116)ime(cid:45)(cid:119)i(cid:110)(cid:100)(cid:111)(cid:119)(cid:44)
H P Cmax (cid:104)(cid:111)ps pe(cid:114) (cid:99)(cid:121)(cid:99)(cid:108)e(cid:46)
(cid:44) (cid:77)(cid:50) is (cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100) a(cid:116) (cid:110)(cid:111)(cid:100)e (cid:49)
a(cid:98)(cid:108)e (cid:116)(cid:104)e (cid:98)(cid:121)pass pa(cid:116)(cid:104) (cid:102)(cid:111)(cid:114) (cid:3)i(cid:116)s (cid:111)(cid:110) (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:46) (cid:84)(cid:104)(cid:117)s(cid:44) se(cid:116)(cid:116)i(cid:110)(cid:103)
I njEN (cid:116)(cid:111) (cid:48)
(cid:44) a(cid:110)(cid:100) RSEL (cid:116)(cid:111) (cid:48)
(cid:116)(cid:104)e i(cid:110)(cid:106)e(cid:99)(cid:116)e(cid:100) (cid:3)i(cid:116)s (cid:116)(cid:114)a(cid:118)e(cid:114)se (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:44) (cid:116)aki(cid:110)(cid:103)
(cid:65)(cid:102)(cid:116)e(cid:114) (cid:111)(cid:110)e (cid:99)(cid:121)(cid:99)(cid:108)e(cid:44) (cid:77)(cid:49) is (cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100) a(cid:116) (cid:110)(cid:111)(cid:100)e (cid:55)
(cid:46) (cid:114)espe(cid:99)(cid:116)i(cid:118)e(cid:108)(cid:121)
(cid:110)(cid:111)(cid:100)e m(cid:117)s(cid:116) (cid:99)(cid:111)(cid:110)sis(cid:116)e(cid:110)(cid:116)(cid:108)(cid:121) (cid:111)(cid:114)(cid:100)e(cid:114) (cid:114)e(cid:113)(cid:117)es(cid:116)s s(cid:117)(cid:99)(cid:104) (cid:116)(cid:104)a(cid:116) a (cid:103)(cid:108)(cid:111)(cid:98)a(cid:108) (cid:111)(cid:114)(cid:100)e(cid:114) is
a(cid:99)(cid:104)ie(cid:118)e(cid:100) (cid:46)
Ea(cid:99)(cid:104) (cid:116)ime (cid:119)i(cid:110)(cid:100)(cid:111)(cid:119)(cid:44) (cid:110)(cid:111)(cid:116)i(cid:2)(cid:99)a(cid:116)i(cid:111)(cid:110)s i(cid:110)(cid:102)(cid:111)(cid:114)m (cid:111)(cid:102) (cid:114)e(cid:113)(cid:117)es(cid:116)s (cid:102)(cid:114)(cid:111)m (cid:99)e(cid:114)(cid:116)ai(cid:110)
s(cid:111)(cid:117)(cid:114)(cid:99)e (cid:110)(cid:111)(cid:100)es (cid:116)(cid:104)a(cid:116) a(cid:114)e expe(cid:99)(cid:116)e(cid:100) (cid:116)(cid:111) a(cid:114)(cid:114)i(cid:118)e (cid:118)ia (cid:116)(cid:104)e mai(cid:110) (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k(cid:46) (cid:70)(cid:111)(cid:108)(cid:45)
(cid:108)(cid:111)(cid:119)i(cid:110)(cid:103) (cid:99)(cid:111)(cid:110)sis(cid:116)e(cid:110)(cid:116) (cid:111)(cid:114)(cid:100)e(cid:114)i(cid:110)(cid:103) (cid:114)(cid:117)(cid:108)es(cid:44) e(cid:118)e(cid:114)(cid:121) (cid:110)(cid:111)(cid:100)e p(cid:114)i(cid:111)(cid:114)i(cid:116)i(cid:122)es (cid:116)(cid:104)e s(cid:111)(cid:117)(cid:114)(cid:99)e
(cid:73)(cid:68)s a(cid:110)(cid:100) p(cid:114)(cid:111)(cid:99)esses (cid:114)e(cid:113)(cid:117)es(cid:116)s i(cid:110) (cid:116)(cid:104)e p(cid:114)i(cid:111)(cid:114)i(cid:116)(cid:121) (cid:111)(cid:114)(cid:100)e(cid:114)(cid:46) (cid:84)(cid:104)e (cid:119)(cid:111)(cid:114)s(cid:116)(cid:45)(cid:99)ase is
(cid:119)(cid:104)e(cid:110) (cid:3)i(cid:116) a(cid:114)(cid:114)i(cid:118)es p(cid:114)i(cid:111)(cid:114) (cid:116)(cid:111) (cid:104)i(cid:103)(cid:104)e(cid:114) p(cid:114)i(cid:111)(cid:114)i(cid:116)(cid:121)
(cid:3)i(cid:116)s a(cid:110)(cid:100) is p(cid:114)i(cid:111)(cid:114)i(cid:116)i(cid:122)e(cid:100) (cid:108)as(cid:116)(cid:46)
(cid:84)(cid:104)(cid:117)s(cid:44) (cid:116)(cid:104)e (cid:3)i(cid:116) m(cid:117)s(cid:116) (cid:114)esi(cid:100)e i(cid:110) (cid:116)(cid:104)e (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k i(cid:110)(cid:116)e(cid:114)(cid:102)a(cid:99)e (cid:98)(cid:117)(cid:102)(cid:102)e(cid:114)s (cid:117)(cid:110)(cid:116)i(cid:108) a(cid:108)(cid:108)
(cid:111)(cid:116)(cid:104)e(cid:114) (cid:3)i(cid:116)s a(cid:114)e (cid:114)e(cid:99)ei(cid:118)e(cid:100) a(cid:110)(cid:100) p(cid:114)(cid:111)(cid:99)esse(cid:100)(cid:46)
(cid:121) (cid:99)(cid:111)(cid:110)s(cid:116)(cid:114)ai(cid:110)i(cid:110)(cid:103) (cid:116)(cid:104)e p(cid:111)(cid:116)e(cid:110)(cid:116)ia(cid:108)
(cid:110)(cid:117)m(cid:98)e(cid:114) (cid:111)(cid:102) (cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116) (cid:114)e(cid:113)(cid:117)es(cid:116)s ea(cid:99)(cid:104) (cid:99)(cid:121)(cid:99)(cid:108)e(cid:44) (cid:116)(cid:104)e e(cid:110)(cid:100)p(cid:111)i(cid:110)(cid:116) (cid:111)(cid:114)(cid:100)e(cid:114)i(cid:110)(cid:103) i(cid:110)
(cid:79)(cid:82)(cid:73) is (cid:118)e(cid:114)(cid:121) e(cid:102)(cid:2)(cid:99)ie(cid:110)(cid:116)(cid:46) Ea(cid:99)(cid:104) (cid:116)ime(cid:45)(cid:119)i(cid:110)(cid:100)(cid:111)(cid:119)(cid:44) (cid:117)p (cid:116)(cid:111) a maxim(cid:117)m (cid:111)(cid:102) (cid:102)(cid:111)(cid:117)(cid:114)
a(cid:110)(cid:100) (cid:77)(cid:51) is (cid:108)a(cid:116)(cid:99)(cid:104)e(cid:100) a(cid:116) (cid:110)(cid:111)(cid:100)e (cid:48)
(cid:46) (cid:84)(cid:104)is (cid:117)(cid:110)i(cid:100)i(cid:114)e(cid:99)(cid:116)i(cid:111)(cid:110)a(cid:108) s(cid:110)ake (cid:114)i(cid:110)(cid:103) e(cid:110)s(cid:117)(cid:114)es
(cid:98)(cid:114)(cid:111)a(cid:100)(cid:99)as(cid:116) (cid:114)e(cid:113)(cid:117)es(cid:116)s (cid:99)a(cid:110) a(cid:114)(cid:114)i(cid:118)e i(cid:110) a (cid:49)
(cid:45)(cid:110)(cid:111)(cid:100)e s(cid:121)s(cid:116)em (cid:44) (cid:119)i(cid:116)(cid:104)
H P Cmax
ex(cid:99)(cid:108)(cid:117)si(cid:118)e a(cid:99)(cid:99)ess (cid:116)(cid:111) e(cid:106)e(cid:99)(cid:116)i(cid:111)(cid:110) (cid:108)i(cid:110)ks(cid:44) sim(cid:117)(cid:108)(cid:116)a(cid:110)e(cid:111)(cid:117)s(cid:108)(cid:121) a(cid:108)(cid:108)(cid:111)(cid:119)i(cid:110)(cid:103) m(cid:117)(cid:108)(cid:116)ip(cid:108)e
(cid:111)(cid:102) (cid:52)
(cid:46) (cid:73)(cid:110) (cid:116)(cid:104)is s(cid:99)e(cid:110)a(cid:114)i(cid:111)(cid:44) (cid:116)(cid:104)e (cid:119)(cid:111)(cid:114)s(cid:116) (cid:99)ase (cid:111)(cid:114)(cid:100)e(cid:114)i(cid:110)(cid:103) (cid:108)a(cid:116)e(cid:110)(cid:99)(cid:121) a(cid:116) (cid:116)(cid:104)e e(cid:110)(cid:100)p(cid:111)i(cid:110)(cid:116)
(cid:3)i(cid:116)s (cid:116)(cid:111) (cid:116)(cid:114)a(cid:118)e(cid:114)se (cid:100)ema(cid:114)(cid:99)a(cid:116)e(cid:100) se(cid:99)(cid:116)i(cid:111)(cid:110)s (cid:111)(cid:102) (cid:116)(cid:104)e (cid:114)i(cid:110)(cid:103)(cid:46)
is (cid:54) (cid:99)(cid:121)(cid:99)(cid:108)es (cid:102)(cid:111)(cid:114) (cid:116)(cid:104)e (cid:79)(cid:82)(cid:73) (cid:110)e(cid:116)(cid:119)(cid:111)(cid:114)k (cid:58)
(cid:51) (cid:99)(cid:121)(cid:99)(cid:108)es (cid:119)ai(cid:116)i(cid:110)(cid:103) (cid:102)(cid:111)(cid:114) (cid:116)(cid:104)e (cid:104)i(cid:103)(cid:104)es(cid:116)
(cid:121)(cid:110)ami(cid:99) a(cid:110)(cid:100) (cid:68)is(cid:116)(cid:114)i(cid:98)(cid:117)(cid:116)e(cid:100) (cid:8"
Accelerating Graph Community Detection with Approximate Updates via an Energy-Efficient NoC.,"Community detection is an advanced graph operation that is used to reveal tightly-knit groups of vertices (aka. communities) in real-world networks. Given the intractability of the problem, efficient heuristics are used in practice. Yet, even the best of these state-of-the-art heuristics can become computationally demanding over large inputs and can generate workloads that exhibit inherent irregularity in data movement on manycore platforms. In this paper, we posit that effective acceleration of the graph community detection operation can be achieved by reducing the cost of data movement through a combined innovation at both software and hardware levels. More specifically, we first propose an efficient software-level parallelization of community detection that uses approximate updates to cleverly exploit a diminishing returns property of the algorithm. Secondly, as a way to augment this innovation at the software layer, we design an efficient Wireless Network on Chip (WiNoC) architecture that is suited to handle the irregular on-chip data movements exhibited by the community detection algorithm under both unicast- and broadcast-heavy cache coherence protocols. Experimental results show that our resulting WiNoC-enabled manycore platform achieves on average 52% savings in execution time, without compromising on the quality of the outputs, when compared to a traditional manycore platform designed with a wireline mesh NoC and running community detection without employing approximate updates.","Accelerating Graph Community Detection with Approximate 
Updates via an Energy-Efficient NoC
Karthi Duraisamy 
School of EECS, WSU 
Pullman , USA 
duraisam@eecs.wsu.edu 
Hao Lu 
Partha Pratim Pande 
Ananth Kalyanaraman 
School of EECS, WSU 
Pullman , USA 
hlu@eecs.wsu.edu 
School of EECS, WSU 
Pullman , USA 
pande@eecs.wsu.edu 
School of EECS, WSU 
Pullman , USA 
ananth@eecs.wsu.edu 
ABSTRACT 
Community detection is an advanced graph operation that is used 
to reveal tightly-knit groups of vertices (aka. communities) in 
real-world networks. Given the intractability of the problem, 
efficient heuristics are used in practice.  Yet, even the best of these 
state-of-the-art 
heuristics 
can 
become 
computationally 
demanding over large inputs and can generate workloads that 
exhibit inherent irregularity in data movement on manycore 
platforms. In this paper, we posit that effective acceleration of the 
graph community detection operation can be achieved by 
reducing the cost of data movement through a combined 
innovation at both software and hardware 
levels. More 
specifically, we 
first propose an efficient software-level 
parallelization of community detection that uses approximate 
updates to cleverly exploit a diminishing returns property of the 
algorithm. Secondly, as a way to augment this innovation at the 
software layer, we design an efficient Wireless Network on Chip 
(WiNoC) architecture that is suited to handle the irregular on-chip 
data movements exhibited by the community detection algorithm 
under both unicast- and broadcast-heavy cache coherence 
protocols. Experimental results show that our resulting WiNoCenabled manycore platform achieves on average 52% savings in 
execution time, without compromising on the quality of the 
outputs, when compared to a traditional manycore platform 
designed with a wireline mesh NoC and running community 
detection without employing approximate updates. 
CCS CONCEPTS 
•Theory of computation→ Graph algorithms analysis;
•Hardware → Network on chip
KEYWORDS 
Graph Community Detection, Wireless Network-on-Chip 
1. INTRODUCTION
In the era of big data, graph analytics is starting to take a centerstage. Data collected from numerous scientific, social and 
internet-scale applications render themselves in ways that suit 
graph-theoretic modeling and analysis. For instance, in social 
networks, users are represented as nodes and their interactions 
represented as edges. Such modeling opens the door for numerous 
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than ACM 
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, 
to post on servers or to redistribute to lists, requires prior specific permission and/or a 
fee. Request permissions from Permissions@acm.org . 
DAC '17, June 18-22, 2017, Austin, TX, USA © 2017  
ACM. ISBN 978-1-4503-4927-7/17/06…$15.00  
DOI: http://dx.doi.org/10.1145/3061639.3062194 
venues in pattern discovery and posing structural queries about 
the data. Community detection is an exemplar of such a discovery 
operation. Given a graph G(V,E), the goal of community detection 
is 
to 
identify 
tightly-knit subgroups of vertices 
(aka. 
“communities”), as illustrated by the example in Fig. 1. The 
community detection operation [1] reveals the natural divisions 
that typically exist in real-world networks, and finds application 
in a number of scientific domains including (but not limited to) 
social network analysis, collaboration networks, bioinformatics 
and electric power grid. Despite its broad application base and the 
recent developments 
in manycore processing , executing 
community detection over large real-world graphs remains 
challenging. Community detection algorithms typically have an 
iterative structure, where all vertices are scanned (in an arbitrary 
order) at every iteration and at each vertex, information from all 
neighboring vertices are queried to make local decisions. This 
imposes a significant overhead due to the repetitive need for 
communicating the latest of updates among neighbors, coupled 
with the fact that the neighbors need not necessarily be co-located 
in memory. Furthermore, as shown later in Sec. 5 .2, the on-chip 
traffic pattern that arise from community detection computation 
is long-range heavy and irregular in nature. 
To scale an advanced operation such as community detection 
to large networks, it is necessary to innovate at both the software 
and the hardware layers. More precisely, given that irregular data 
movement is the primary limiting factor in the performance of 
graph applications, we posit that reducing the cost and/or the 
volume of data movement will be critical in scaling up graph 
applications. Furthermore, in scaling up graph applications, the 
cost of energy consumption cannot be ignored, and exploring 
ways to reduce data movement and making it more effective will 
impact the energy cost of such applications. To this end, in this 
paper, we explore complementary strategies–at the software and 
hardware layers–as a way to derive a combined benefit in time 
and energy performance of the community detection operation on 
a single- chip manycore platform. More specifically, we present i) 
a way to reduce the intensity of data movement by deploying an 
approximate update technique at the software layer; and ii) an 
efficient design of a Network-on-Chip (NoC) that is aimed at 
reducing both the latency and energy consumption .  
2. RELATED WORKS
Implementing 
graph 
analytics 
on 
specialized 
parallel 
architectures has been a relatively recent pursued task. The 
Graph500 benchmark [4] has focused on evaluating modern day 
supercomputing platforms based on their performance for 
Breadth First Search (BFS)–a fundamental operation involving 
Fig. 
1: 
Illustrative 
example for community 
detection. Given a graph 
G(V,E), this figure shows 
a possible communitywise 
partitioning 
of 
vertices in V.  
graph traversal. A detailed study 
on using microarchitectures for 
graph analytics has been presented 
in [5]. In [6], the performance of 
Symmetric Multiprocessor (SMP) 
systems 
for 
large-scale graph 
analytics has been analyzed. There 
are 
several 
previous works 
exploring the efficiency of SMPs 
for applications with 
inherent 
irregularities 
[7][8][9]. Parallel 
implementations on multicore and 
manycore platforms for advanced 
graph operations such as graph 
community detection and graph coloring have been presented in 
(e.g., [10][11][12]). GPU implementation for executing community 
detection using label propagation has been already explored [13]. 
While GPUs have their performance advantages, codes need to be 
data-parallel to take full advantage of the platform. Graph 
applications, however, are 
latency-bound applications that 
inherently generate large volumes of irregular memory lookups 
due to edge dependencies. Consequently, manycore NoC 
architectures such as the one presented in this work are more 
naturally suited 
for optimized execution of such graph 
applications. Furthermore, a SIMD platform such as a GPU is illequipped to take advantage of heuristics such as early termination 
since they tend to generate variable work rates, and would require 
dynamic subset/data selection and load distribution capabilities. 
A recent study concluded that the small-world networkenabled WiNoC architectures are better suited for handling the 
irregular data movements exhibited by the graph-computation 
applications than the conventional wireline mesh and high-radix 
NoC architectures. However, this work [2], did not consider the 
impacts of any approximate update scheme and cache coherence 
protocols on the execution time and energy dissipation profile of 
the manycore chips running graph analytics. The work presented 
in this paper, represents the first of its kind in characterizing the 
role of these two software and hardware aspects in designing 
manycore-based platforms for community detection. 
3. COMMUNITY DETECTION
A scalable parallel implementation for executing community 
detection on conventional manycore architectures has been 
proposed recently [14]. This method, called Grappolo, implements 
a multi-phase, multi-iteration heuristic 
to maximize 
the 
modularity [15] of the output partitioning. In what follows, we 
describe the algorithm for the original version of Grappolo.   
3.1 Grappolo 
Given a graph G(V,E), with n vertices and m edges, Grappolo 
iteratively tries to improve the overall modularity by running 
multiple “phases”, and multiple iterations within each phase, until 
modularity gain becomes negligible. The steps of a phase are:  
1. Initially, each vertex is assigned to a community of its own.
2. Within each iteration the vertices are scanned in parallel and
for each vertex, a decision is made to determine whether or not to
migrate it to one of its neighboring communities (as defined by
Graphs Vertex 
Count 
ASJ 
Edge 
Count 
Execution Time (sec) Modularity 
FS 
FS+ET Gain 
FS 
FS+ET 
22,963 48,436  0.7608 0.3437 55% 0.66216 0.66001 
ASTRO 16,706 121,251 0.2998 0.1854 38% 0.73206 0.73216 
 COND 31,163 120,029 0.3174 0.11961 62% 0.76238 0.76397 
8,361 15,751 0.0535 0.0343 36% 0.84969 0.84667 
PGP 10,680 24,316 0.0829 0.0623 25% 0.88198 0.88282 
HEP 
Table 1: Evaluation of the approximate update scheme (via ET) on 
community detection’s execution time and quality (modularity). 
All runs were performed on a 32-core Intel multicore platform. 
the communities of its neighbors). For executing this step, the 
latest community information from all neighboring vertices are 
pulled by a given vertex. Locking is used to prevent concurrent 
and potentially conflicting updates from happening at the 
neighbors. In this model, locking needs to happen at two data 
structures – one at the level of a neighboring vertex and another 
at the level of the community that holds that vertex. This software 
version is labelled “fully synchronized update” version.  
3. The algorithm proceeds to subsequent iterations until the gain
achieved 
in modularity 
becomes negligible. Reaching
convergence by this criterion marks the end of current phase and
the algorithm constructs a compacted graph G’(V’,E’) by
collapsing every community detected in G into a single metavertex in G’, and creating edges with weights corresponding to
intra- and inter-community links in G.
4. Subsequently, the algorithm initiates the next phase on the
newly compacted graph G’, until no more appreciable modularity
gain is achieved (i.e., convergence).
One of the major contributors to the computational cost of the
above algorithm is its local neighborhood querying in step (2). The
fully synchronized update model ensures that the most recent
community information of a vertex is made available to any of its
vertex neighbors for its migration decision.  However, the cost of
such full synchronization manifests itself in a pattern of large
volume and potentially irregular data traffic. Therefore, this fully
synchronized version is expected to yield a fast convergence (i.e.,
less number of iterations due to latest information availability) but
at the possible expense of more time cost per iteration.
3.2 Approximating Updates 
As explained above, within each iteration of Grappolo algorithm 
using the fully synchronized update model , the computation time 
is dominated by the time taken to decide on a vertex migration 
(step 2). For 
this paper, we designed an alternative 
implementation of Grappolo that deviates from making precise 
updates. More specifically, unlike classical heuristics that mainly 
focus on reducing the runtime, our proposed scheme focuses on 
reducing the intensity of data lookups, thereby generating a 
potential to reduce both time and energy caused by the data 
Graphs 
No. of Edge Traversals and Community Lookups in Phase-1 
NET 
NCL 
FS 
FS+ET Reduction 
FS 
FS+ET Reduction 
ASJ 
871,848 307,696 
64.71% 
603,953 242,874 
59.79% 
ASTRO 2,667,522 812,821 
69.53% 
850,865 364,424 
57.17% 
COND 3,600,870 814,024 
77.39% 
1,601,311 474,852 
70.35% 
HEP 
283,518 104,532 
63.13% 
170,780 
75,148 
56.00% 
PGP 
437,688 161,352 
63.14% 
224,060 101,532 
54.69% 
Table 2: Evaluation of the reduction in number of edges traversals 
(NET) and number of community lookups (NCL), induced by ET.  
movements. The main idea hinges on the key observation of a 
diminishing returns property in quality as the iterations progress 
[14]. As shown by the results on real-world graphs (Fig. 2), the 
gain in modularity plateaus after a few initial iterations, because 
of significantly fewer community updates for vertices during the 
later iterations. We take advantage of this observation and include 
a counter, nElapsed, at every vertex to denote the number of 
iterations that have elapsed since its last successful migration (i.e., 
change in community). If the counter nElapsed exceeds a certain 
threshold , 
then we 
“terminate” 
that vertex–i.e., we 
(optimistically) stop considering 
that vertex during any 
subsequent iteration of that phase implying that the vertex is 
locked into that community. This heuristic has two performance 
advantages: a) it enables a faster convergence of modularity by 
reducing both the number of vertices that need to be processed at 
every iteration and the total number of iterations required, and b) 
it also reduces the volume of memory lookups generated from 
each vertex because terminated vertices will stop seeking 
information from their neighborhoods. However, this also runs 
the risk of possibly degrading the final quality achieved (measured 
by modularity). We tested with a number of graph inputs to study 
this precision vs. performance tradeoff. Specifically, we use a set 
of five DIMACS10 [16] clustering instance graph data sets, Hepth 
(HEP), Astro-ph 
(ASTRO), Comd-mat-2003 
(COND), 
PGPgiantcompo (PGP) and as-22july06 (ASJ). The vertex and edge 
counts for all the five considered graphs are listed in Table 1. For 
our experiments, we set =3. First, we compare the convergence 
rates (measured in the number of iterations) of the fully 
 
patterns. As shown later in 
Section 5.2, in case of the 
Directory protocol, about 
60% of 
total 
traffic 
is 
exchanged over NoC routers 
that are at least 5 inter-router 
hops apart (a hop is defined 
as the distance between two 
adjacent routers in a mesh 
NoC). 
Moreover, 
the 
Grappolo on-chip 
traffic 
patterns do not 
indicate 
presence 
of 
any 
high 
intensity 
data-transfers 
between specific pairs of 
cores and the data-exchanges are actually well distributed. On the 
other hand, memory accesses under the Hammer protocol induce 
broadcasts which must be distributed across all the NoC routers. 
Thus, the on-chip traffic exhibited by Grappolo in presence of 
Directory and Hammer coherence protocols is long-range-heavy. 
The wireless channel in WiNoC is a broadcast communication 
medium capable of establishing one-hop links even between 
physically distant nodes [3]. Hence, for both Directory and 
Hammer induced traffic, WiNoC provides an efficient solution. 
4.2 NoC Design 
The WiNoC architecture follows a small-world architecture [18].  
Primarily, we 
leverage 
the small-world property while 
constructing the NoC topology to design a low hop count on-chip 
interconnection network. Essentially, a small-world NoC 
incorporates mostly short-range links along with a few longrange shortcuts following the power-law distribution of the edges 
in a small-world graph [18][3]. The links are established such that 
the overall 
inter-router hop count 
is minimized. When 
implemented with conventional metal wires, the long-range 
shortcuts in the small-world interconnects are extremely costly in 
terms of power and delay. Hence, in WiNoC, power efficient 
wireless links are used to connect the routers that are separated 
by long distances [3]. We use the 3 mm-wave wireless channels 
operating in the 30, 60 and 90GHz range here. The on-chip 
wireless interfaces (WIs) are designed following [3]. Fig. 3 
illustrates a 64 core WiNoC architecture indicating the placement 
of the WIs, channel assignments and the wireline connectivity.  
4.2.1 Data-Transfer Protocols 
 In WiNoC for unicast packets, we follow the deadlock-free 
adaptive-layered shortest path routing. In this work, for Hammer 
broadcast packets, we follow a region based message distribution 
and acknowledgment (ACK) aggregation strategy. In this strategy, 
the whole system is first divided into multiple equal sized nonoverlapping regions (As an example, WiNoC illustrated in Fig. 3 
uses four regions). Upon injection, a broadcast message is first 
forwarded from the source node to the nearest WI in its own 
region and is then transmitted wirelessly. The message is then 
received simultaneously by the WIs in all other destination 
regions (WIs that are operating in the same channel as the 
transmitting WI). Finally, the message is distributed from each 
region destination WI to the final destination nodes using wireline 
links using a tree multicast mechanism [20]. In this mechanism, a 
destination WI first forwards the received broadcast message to a 
set of pre-defined intermediate nodes (Ex: nodes 13, 14, 21, 22 in 
Q2 of the NoC depicted in Fig. 3). The message is then replicated 
at these intermediate nodes and are finally forwarded to the final 
destination nodes using intra-region wireline links.   
In WiNoC, each broadcast message uses an additional field to 
indicate the state of the broadcast distribution (whether the packet 
is on the way to source WI starting from the source node or the 
message is undergoing a regional distribution starting from a 
destination WI). This state field can only be modified at the source 
node and the source region WI. With the use of this additional 
field and the cycle-free tree broadcast distribution, the WiNoC 
broadcast communication eliminates any possible deadlocks. 
For cache coherence-induced broadcasts, each packet is 
associated with a set of acknowledgment (ACK) messages that are 
transmitted from each destination back to the source [21].  In 
WiNoC, the acknowledgments are aggregated at the intermediate 
nodes. Initially, each destination forwards its acknowledgment 
message to the same intermediate node that forwarded the 
original multicast message. The ACKs are then aggregated at each 
region WIs and finally get forwarded towards the source node.  
The WiNoC uses a distributed MAC protocol as proposed in 
[19] to resolve the channel access contention among the WIs. 
5. EXPERIMENTAL RESULTS 
To evaluate the effectiveness of our WiNoC architecture for 
implementing community detection with approximate updates, 
we compare against a more traditional and an industry standard 
NoC, the mesh architecture. For this mesh NoC, we use dimension 
ordered XY routing for unicast packets and XY–tree broadcast 
mechanism for Hammer broadcasts. We also use ACK aggregation 
for the mesh following the method proposed in [21].  
5.1 Experimental Setup 
For all the following evaluations, we use the same set of five 
clustering instance graphs that were used in Section 3.2 (ASJ, 
ASTRO, COND, HEP, PGP). We use GEM5 full system simulator 
to obtain detailed processor and network information [22]. All the 
gem5 simulations were performed in the full-system mode where 
we leveraged the RUBY and GARNET modules of GEM5 to 
implement detailed memory and on-chip interconnection models. 
The FS and FS+ET software implementations of the community 
detection algorithm are parallel (using OpenMP) and are 64-way 
multithreaded for our GEM5 simulations. The simulated system is 
comprised of sixty-four x86-cores operating at 2.5GHz arranged 
in a 20mm×20mm die. The memory system is comprised of private 
64KB L1 instruction and data caches and a shared 16MB L2 cache 
(256KB distributed L2 per core). For both mesh and WiNoC, we 
use a generic three stage router capable of multiport multicast 
replicating [21]. Energy dissipation of the NoC routers, inclusive 
of the MAC block, was obtained from the synthesized netlist by 
 
the GEM5 simulations are incorporated into McPAT (Multicore 
Power, Area, and Timing) to determine the processor power [23]. 
5.2 Analysis of Traffic Patterns 
Before presenting the detailed performance evaluation, first we 
analyze the traffic patterns generated by Grappolo. Fig. 4 shows 
the percentage of the total on-chip traffic exchanged between the 
communicating routers that are separated by a certain number of 
mesh NoC hops (indicated by h), while executing the FS and 
FS+ET versions of Grappolo. As seen, both versions of Grappolo 
generate significant amount of long-range traffic with both 
Directory and Hammer. Around 50% of the total traffic under 
Directory protocol requires a minimum of 6 mesh hops while with 
Hammer protocol, about 62% of the total traffic requires at least 6 
hops. These long-range traffic patterns that arise form Grappolo 
computations can be attributed to shared-memory accesses 
caused by neighborhood graph lookups and vertex migration. 
Another contributor to long-range traffic is Hammer coherence 
induced broadcasts (constituting about 25% of all traffic), which 
can require up to 14 inter-router hops in a 64-core NoC.  
Fig. 5 shows the percentage of the traffic associated with each 
of the top six traffic hotspots in the system, for running Grappolo. 
With Directory, the top-six traffic hotspots are responsible for 
about 26% of the total traffic. This injection originates mostly from 
the master cores that handle the graph compaction– a largely 
serialized step owing to the need to gather vertices belonging to 
various communities. In case of Hammer, shared memory 
accesses induce broadcasts that need to be communicated with all 
the NoC routers. The broadcasts are then followed with the 
injection of ACK messages from each of the destination node, sent 
back to the source node. Because of these broadcasts and ACK 
messages, the impacts of the master cores are less pronounced in 
Hammer protocol and the volumes of messages injected by all the 
NoC routers remain fairly close to each other (Fig. 5).  
These long-range and hot spot traffic patterns are observed 
consistently over all the graphs considered and for both FS and 
FS+ET implementations. WiNoC is highly suitable to handle 
irregular traffic patterns involving long-range communication 
and traffic hotpots [2][3]. Hence, it is a natural candidate as the 
communication backbone for manycore platforms in this work .  
5.3 Execution Time Evaluation 
Fig. 6(a) compares the execution times of the mesh and WiNoC 
interconnected manycore systems running FS and FS+ET 
Grappolo versions with both Directory and Hammer protocols. 
All the values in Fig. 6(a) are normalized with respect to the 
execution times observed in the baseline system with mesh 
running FS version. As it can be observed from this figure, 
regardless of the NoC platform and cache coherence protocol 
employed, the use of early termination technique greatly benefits 
the execution of Grappolo. When compared to the FS version, the 
software implementation using approximate updates (i.e., FS+ET) 
achieves on an average of 40.7% and 39.1% execution time savings 
with the Hammer and Directory cache coherence, respectively.  
In order to clearly demonstrate the benefit of employing the 
WiNoC for Grappolo, in Fig. 6(b) we compare the runtimes of the 
FS+ET version of Grappolo in Mesh and WiNoC. The WiNoC 
achieves on an average of 22.4% and 15.96% reduction in execution 
times with Hammer and Directory protocols respectively, when 
compared to the mesh NoC. The reason for the variation in gain 
achieved for the two cache coherence protocols can be understood 
from the NoC latency plot shown in Fig. 7. When compared to the 
Directory protocol, the Hammer protocol induced on-chip 
messages give rise to a higher network latency in the baseline 
mesh NoC because of the fact that about 25% of the injections are 
broadcasts. Thus, in a mesh NoC, memory accesses cause higher 
penalties on the overall execution time with the Hammer protocol 
than with the Directory protocol. Hence, accelerating the memory 
Fig. 5: Percentage of traffic injected by the 
top-6 hotspots. Values are averages for all 5 
graphs considered in this work.   
0%
3%
6%
9%
FS
FS+ET
FS
FS+ET
Hammer
Directory
%
i
o
T
n
t
a
l
T
r
a
i
f
f
c
1st
2nd
3rd
4th
5th
6th
Fig. 7: Comparison of NoC Latency. 
0
5
10
15
20
25
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
Hammer
Directory
a
L
t
y
c
n
e
i
n
C
s
e
c
y
l
FS+ET_Mesh
FS+ET_WiNoC
Fig. 4: Distribution of the on-chip traffic based 
on communication distances. Parameter h in 
the legend indicates the value of hop counts 
in a XY routing based mesh. Shown values are 
averages for all the 5 graphs used.  
0%
12%
24%
36%
FS
FS+ET
Hammer
FS
FS+ET
Directory
%
i
o
T
n
t
a
l
T
r
a
i
f
f
c
h<=3
3<h<=5
5<h<=7
7<h<=9
h>9
 Fig. 6(a): Ex. Times of FS and FS+ET                     6(b): Demonstrating WiNoC Efficiency 
0
0.2
0.4
0.6
0.8
FS_Mesh FS+ET_Mesh FS_WiNoC FS+ET_WiNoC
1
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
Hammer
Directory
N
o
r
m
a
i
l
E
d
e
z
x
.
T
i
m
e
0.6
m
0.5
0.7
0.8
m
0.9
FS+ET_Mesh
1
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
Hammer
Directory
N
o
r
a
i
l
E
d
e
z
x
.
T
i
e
FS+ET_WiNoC
Fig. 8: Full System Energy Consumption 
0.2
0.4
0.6
0.8
FS_Mesh FS+ET_Mesh FS_WiNoC FS+ET_WiNoC
1
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
S
A
J
T
S
A
O
R
D
N
O
C
P
E
H
P
G
P
Hammer
Directory
N
o
r
m
a
i
l
e
n
E
d
e
z
r
y
g
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                          
 
 
 
 
 
 
 
 
approximate update scheme for the community detection 
algorithm that lowers the computation complexity and reduces 
the volume of on-chip memory accesses. Furthermore, we have 
designed an efficient wireless NoC architecture that accelerates 
the on-chip memory accesses under both unicast- and broadcast- 
heavy cache coherence protocols. For the input graphs and the 
cache coherence protocols considered in this work, the proposed 
manycore implementation provides on an average of 52% 
execution time savings and 53.9% lower energy consumption to 
perform community detection computation, over a baseline 
architecture using wireline mesh NoC and running the traditional 
community detection (without any approximate updates). The 
techniques proposed here have the potential to be extended to 
many other data-bound irregular graph applications as well.  
ACKNOWLEDGMENTS 
This work was partially supported by NSF grants CNS 1564014, 
CCF 1514269 and CCF 1162202 and DOE award DE-SC-0006516.  
7. "
"Task Mapping on SMART NoC - Contention Matters, Not the Distance.","On-chip communication is the bottleneck of system performance for NoC-based MPSoCs. SMART, a recently proposed NoC architecture, enables single-cycle multi-hop communications. In SMART NoCs, unconflicted messages can go through an express bypass and the communication efficiency is significantly improved, while conflicted messages have to be buffered for guaranteed delivery with extra delays. Therefore, that performance of SMART NoC may be seriously degraded when communication contention increases. In this paper, we present task mapping techniques to address this problem for SMART NoCs, with the consideration of communication contention, rather than inter-processor distance, by minimizing conflicts and thus maximizing bypass utilization. We first model the entire problem by ILP formulations to find the theoretically optimal solution, and further propose polynomial-time algorithms for contention-aware task mapping and message priority assignment. Communicating tasks can be mapped to distant processors in SMART NoCs as long as conflict-free communication paths can be established and bypass can be enabled. Evaluation results on real benchmarks show an average of 44.1% and 32.8% improvement in communication efficiency and application performance compared to state-of-the-art techniques. The proposed heuristic algorithms only introduce 1.9% performance difference compared to the ILP model and are more scalable to large-size NoCs.","Task Mapping on SMART NoC: Contention Matters, Not the
Distance
Lei Yang, Weichen Liu
(cid:3)
, Peng Chen, Nan Guan1 , Mengquan Li
College of Computer Science, Chongqing University, Chongqing, China.
1Department of Computing, Hong Kong Polytechnic University, Hong Kong.
ABSTRACT
On-chip communication is the bottleneck of system performance for NoC-based MPSoCs. SMART, a recently proposed NoC architecture, enables single-cycle multi-hop communications. In SMART NoCs, unconﬂicted messages can
go through an express bypass and the communication eﬃciency is signiﬁcantly improved, while conﬂicted messages
have to be buﬀered for guaranteed delivery with extra delays. Therefore, that performance of SMART NoC may
be seriously degraded when communication contention increases. In this paper, we present task mapping techniques
to address this problem for SMART NoCs, with the consideration of communication contention, rather than interprocessor distance, by minimizing conﬂicts and thus maximizing bypass utilization. We ﬁrst model the entire problem by ILP formulations to ﬁnd the theoretically optimal
solution, and further propose polynomial-time algorithms
for contention-aware task mapping and message priority assignment. Communicating tasks can be mapped to distant
processors in SMART NoCs as long as conﬂict-free communication paths can be established and bypass can be enabled. Evaluation results on real benchmarks show an average of 44:1% and 32:8% improvement in communication
eﬃciency and application performance compared to state-ofthe-art techniques. The proposed heuristic algorithms only
introduce 1:9% performance diﬀerence compared to the ILP
model and are more scalable to large-size NoCs.
1.
INTRODUCTION
Network-on-Chip (NoC) is a prevailing communication
paradigm for the next-generation MPSoC, which delivers
low latency, low power consumption and high scalability.
However, inter-processor communication, an integral part
of on-chip computing, has become the bottleneck of system
(cid:3)
Corresponding author: Weichen Liu. wliu@cqu.edu.cn This work
is partially supported by NSFC No. 61402060, National 863 Program 2015AA015304, and Chongqing High-Tech Research Program
cstc2015jcyjA40042, China.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and /or a fee. Request permissions from permissions@acm.org.
DAC ’17, June 18-22, 2017, Austin, TX, USA
c⃝ 2017 ACM. ISBN 978-1-4503-4927-7/17/06. . . $15.00
DOI: http://dx.doi.org/10.1145/3061639.3062323
performance, especially for communication-intensive applications due to long-distance routing overheads.
SMART (Single-cycle Multi-hop Asynchronous Repeated
Traversal ) [1, 2, 3] was recently proposed to tailor a genetic
fast NoC fabric, enabling new features for single-cycle multihop inter-processor communications by adding message bypassing channels. By electrical signals traversing multiple
hops via the shared crossbar and links asynchronously within
a cycle, SMART provides a long-distance message transmission mechanism with ultra-low latency. Corresponding
ﬂow-control and multi-hop setup techniques also have been
proposed, pushing the on-chip communication in NoCs to a
nearly ideal framework with higher performance [4, 5].
SMART NoC supports unconﬂicted messages to go through
the express bypassing channels in intermediate routers and
signiﬁcantly improve the on-chip communication eﬃciency,
while conﬂicted messages can still use the traditional NoC
for guaranteed delivery with extra delays. We observe that
the performance of a SMART NoC can seriously degrade
when communication contention increases, since these transmissions cannot utilize the one-cycle “hyperchannel” bypassing channels eﬀectively. Flits have to be stopped and buﬀered
for arbitration in conﬂict, which introduces extra overhead
for forwarding.
In the most extreme case, no beneﬁt can
be acquired while ﬂits are contended in each intermediate
router on the route. Therefore, the beneﬁt of SMART may
vanish without proper contention management.
In this paper, we address the above problem by developing
novel task mapping techniques. In existing NoC task mapping techniques, the most important factor to consider is the
inter-processor distances, which are not suitable to SMART
NoCs where contention, instead of distance, plays the most
important role to communication eﬃciency. We optimize application performance by minimizing message transmission
conﬂicts and thus maximizing the bypass utilization. More
speciﬁcally, we ﬁrst model the entire problem by Integer Linear Programming (ILP) formulations to ﬁnd theoretically
optimal solution, and propose polynomial-time algorithms
for communication contention-aware task mapping and message priority assignment. Instead of conventional clustered
task mapping on cores with short distance, communicating
tasks can be mapped to distant cores in SMART NoC as
long as conﬂict-free communication paths can be established
and bypass can be enabled. To the best of our knowledge,
this is the ﬁrst study on system-level application mapping
techniques that are necessarily designed to be compatible
with SMART NoC and boost the new hardware-software
system performance. Performance evaluation shows averFigure 1: Router with stages
(a) Lr = 3 (b) Lr = 2.
Figure 2: SMART router microarchitecture with bypass routing. (BWena :
Buf f er W rite enable; BMsel : Bypass M ux select; XBsel : C rossbar select).
agely 44:1% and 32:8% improvement in communication efﬁciency and total application performance. The proposed
algorithms only introduce 1:9% performance diﬀerence compared to ILP and are more scalable to large-size NoCs.
2. BACKGROUND AND RELATED WORK
A. Inter-Processor Communication In Regular NoC
In regular NoC-based MPSoCs, inter-processor communication is conducted by packet delivering through on-chip
routers and virtual links. Flits will be forwarded hop-by-hop
via Buﬀer Write (BW), Router Computation (RC), Switch
Al location (SA), VC Al location (VA), Switch Traversal (ST)
and Link Traversal (LT). Generally, the latency for packet
transmission on regular NoC is calculated as:
Lcomm = (Lr + Lw ) (cid:2) hpath + (Wf lit (cid:0) 1)=b:
It is linear with the route distance hpath from the source to
destination; Lw is the delay on the link between two routers;
Wf lit refers to the number of ﬂits; b is the bandwidth of link
channel. Lr is deﬁned as the number of stages in each router
(e.g., 3 or 2 stages in Fig. 1). With the increasing data injection on NoC especially for communication intensive applications, communication contention may occur, which will
signiﬁcantly degrade network transmission eﬃciency. We
denote LCf lit as the delay by ﬂits contention. Thus, communication latency will increase due to all competitive ﬂits.
Lcomm = (Lr+Lw )(cid:2)hpath+(Wf lit(cid:0)1)=b+
Over the last decades, research eﬀorts are made to improve the communication eﬃciency on regular NoC. NoC reconﬁguration [6], long-range links [7], virtual connection [8]
and skip-links [9], have built various NoCs for applicationspeciﬁc topologies, either to reduce latency in each hop or
directly interconnect long-distance routers. However, it is
time- and energy-costing for regular applications. Besides,
studies in [10, 11] have lowered router stage Lr , so that
low-load network latency between processors is expected to
equal the number of routers. Nevertheless, the communication latency cannot be continuously improved since the
router-stage has been developed to the limit one cycle.
B. Single-Cycle Bypass Routing In SMART NoC
SMART [2, 3, 4, 5] is proposed to tailor a fast NoC at
∑
8Cf lit
(LCf lit ):
Figure 3: Work steps on SMART NoC [3, 4].
run-time.
It is essentially composed of SMART crossbar,
router microarchitecture, arbitration policy and ﬂow control, enabling ﬂits bypass all pipeline stages in intermediate routers and traverse straight to destination or H P Cmax
(maximum hops per cycle) router in one cycle. To realize the communication fabric, single-cycle multi-millimeters
interconnect circuit presented as bypass, together with electrical signals, is integrated into regular router as illustrated
in Fig. 2. It is a low-swing clockless repeated link circuit embedded within router crossbars, breaking the currently optimal 1-cycle router stage and allowing 1-cycle transmission
for packets passing all the way to destination via bypass.
Unlike the baseline router, SA stage in SMART router
contains two steps: Switch Allocation Local (SA-L) and
Switch Allocation Global (SA-G). As shown in Fig. 3, in SAL stage, the start router chooses a winner from all buﬀered
ﬂits for each output port in ﬁrst cycle. Then, a SMART-hop
Setup Request (SSR), which carries the information about
the route, is broadcasted to downstream routers via dedicated repeated wires to establish bypass. Controlpath, including SSR broadcasting and SA-G arbitration, sets up signals (BWena , BMsel , X Bsel ) in each intermediate router. In
Datapath, ﬂits are traversed through ST and LT in a cycle.
Compared with the baseline router in Fig. 1, SMART
router provides a sing-cycle router for one-stage pipeline to
bypass all intermediate routers. However, ﬂits competition
in intermediate routers will dramatically increase the complexity of arbitration in SA-L stage. Thus, the pipeline of
SA-L and SA-G stages will be delayed as shown in Fig. 2.
Consequently, communication latency in intermediate routers
is prolonged due to bypass broken oﬀ.
Lcomm(cid:0)S = 2 (cid:2) (Lr + Lw ) + Nc (cid:2) (Lr + Lw )
+ (Wf lit (cid:0) 1)=b +
where 2 indicates the delay in source and destination routers.
Nc is the number of contentions. If Nc = 0, then
0 and ﬂits can be completely routed via bypass with the ideal
minimum overhead. Otherwise, communication latency increases linearly with the number of contentions. In the most
extreme case, no beneﬁt can be acquired while ﬂits are contended and should be stopped in each intermediate router.
Research eﬀorts are made to take the advantages of SMART
design in communication eﬃciency, including the proposed
bypass router for single-cycle data path [12], the circuitswitching design [13], and a specialized SSR network [14].
However, SMART NoC only supports unconﬂicted messages
to go through express bypassing channels to improve the
on-chip communication eﬃciency, while conﬂicted messages
should be stopped and buﬀered to use the traditional NoC
for guaranteed delivery with extra delays. That is to say,
the 1-cycle multi-hop bypass will be interrupted in intermediate routers, where multiple packets compete for the shared
LCf lit =
(LCf lit ):
∑
8Cf lit
∑
communication resource. This will signiﬁcantly degrade the
eﬀectiveness of SMART routers, especially for communication intensive applications on Mesh-based NoCs.
Traditional task mapping methods [15, 16, 17, 18, 19] are
not applicable for SMART NoC. Communicating tasks will
be mapped closely on processors with short distance, which
will induce more possibilities for communication contention.
Thus, not only the microarchitectural beneﬁts of single-cycle
bypass cannot be exerted, but also the desired eﬃciency of
SMART NoC will be greatly degraded. It becomes a new
bottleneck for performance improvement in SMART NoC,
and will be much worse with increasing complexity of applications in large-scale NoCs. In the following sections, we will
present task mapping techniques to address the problem for
SMART NoC by minimizing communication conﬂicts (not
inter-processor distance) and thus maximizing bypass utilization, through an ILP model and heuristic algorithms.
3. MOTIVATIONAL EXAMPLE
We continue the discussion on task mapping and scheduling on regular NoC and SMART NoC to illustrate the motivation. Formally, an application is commonly represented
by a Directed Acyclic Graph (DAG) as shown in Fig. 4,
G = (V ; E ), called task graph. V = fv1 ; v2 ; :::; vn g is a set of
task nodes, associated with execution time. E (cid:18) V (cid:2) V is a
set of edges, associated with the size of message.
Figure 4: Task graph; Execution time; Message.
As shown in Fig. 5, task mappings on 2-D mesh regular
NoC (3-stage router) and SMART NoC (1-cycle bypass) are
generated by the methods in Davare [16], Yu [17] and our
proposed strategy. On top of it, we obtain schedules with
communication through 3-stage for each hop and 1-cycle bypass in XY-routing, respectively, for each mapping method.
We can observe that, scheduling on SMART NoC by three
methods is better than their respective results on regular
NoC. Besides, mappings by Davare [16] and Yu [17] on either
regular NoC or SMART NoC have longer schedule length
than ours, respectively, due to communication conﬂicts. On
regular NoC, contended packets will be delayed and the resulted schedule length is stretched. Likewise, bypass cannot be successfully carried out in above two mappings when
conﬂict occurs, thus extra overhead is introduced. Our mapping with ﬁne-grained consideration of contention (detailed
in Section 4, 5) can obtain the shortest schedule length both
on regular NoC and SMART NoC, as shown in Fig. 5(c),
since there is no contention and all packet transmissions are
bypassing through intermediate routers in a single cycle.
This example indicates twofold inspirations: i) SMART
enables ultra-low latency via 1-cycle bypass through intermediate routers entirely, providing a fundamental platform
to make great progress in on-chip communication eﬃciency.
ii) In order to maximize the beneﬁts of SMART design and
enhance its eﬀectiveness in on-chip communication, the congestion induced overhead should be further minimized with
the maximized utilization of bypass routing. In this paper,
we present task mapping techniques with ﬁne-grained consideration of communication contention. Not to concern the
(a) Davare [16]: 2 conﬂicts on links R3 ! R0 and R5 ! R8
(b) Heng Yu [17]: 3 conﬂicts on links R1 ; R3 ! R4 and R4 ! R7
(c) Our contention-aware approach: shortest schedule length
Figure 5: Mapping and schedule on diﬀerent NoCs.
long-distance communication in SMART NoC, communication eﬃciency can be signiﬁcantly improved by conﬂict-free
bypass routing. On top of it, we propose a priority assignment strategy for run-time task and communication scheduling to further improve the entire application performance.
4.
ILP MODEL
We ﬁrst present an ILP model for task mapping and scheduling with on-chip communication in SMART NoC. Solutions
are composed of task mapping, tasks execution on processors and communication scenarios via SMART router.
In
Table 1, we have summarized the notations used for constructing the ILP formulation with detailed deﬁnition.
Table 1: Notations Used in ILP Formulations.
V , E , P
su /fu
N
τu;p
Wu;v
su;v /fu;v
hp;q
fapp
Notation De(cid:12)nition
The set of task nodes, edges and processors.
The execution time of task u on processor p.
Release/Completion time of task u.
The packet size on edge eu;v .
Start/Completion time of message on eu;v .
The Manhattan Distance from processor p to q .
Completion time of the entire application.
A large positive integer constant, and N > max{fu }.
Notation Boolean Variables
= 1, iﬀ task u is mapped to processor p.
= 1, iﬀ task u, v have execution overlap.
= 1, iﬀ u starts serially after task v completed.
= 1, iﬀ ei , ej have shared virtual link(s).
= 1, iﬀ ei , ej have transmission time overlap.
∧ βei ;ej = 1 & P rio(ei ) = high.
= 1, iﬀ αei ;ej
Mu;p
ou;v
ℓu;v
αei ;ej
βei ;ej
γei ;ej
(1) Basic task execution constraints: Each task is mapped
to a processor and invoked the precedence relations.
∑
p2P
fu = su +
∑
p2P
Mu;p = 1, ∀ u ∈ V .
(τu;p × Mu;p ), ∀ u ∈ V , ∀ p ∈ P .
sv ≥ fu , ∀ e(u, v) ∈ E .
(2) Basic message transmission constraints: As described in Section 2-A, the minimum message transmission
latency on regular NoC is formulated.
su;v ≥ fu , sv ≥ fu;v , ∀ u, v ∈ V .
fu;v ≥ su;v + 3 × (Mu;p + Mv;q − 1) × (hp;q + 1)+
(Wu;v − 1) − (1 − Mu;p ) × N − (1 − Mv;q ) × N ,
∀ u, v ∈ V , ∀ p, q ∈ P .
(3) Task overlap constraints: For tasks u; v 2 V which
are neither in each other’s transitive fan-out (TFO), the ﬁrst
constraint ensures that if task u ﬁnishes after v begins, the
corresponding variable ou;v is set to 1. The second constraint
ensures that no two tasks mapped on a same processor can
overlap. Thus, ov;u + ou;v = 2 iﬀ u; v have execution overlap.
For all other cases, the sum of ov;u and ou;v must be 1.
fu − sv ≤ N × ou;v , ∀ u, v ∈ V .
ov;u + ou;v + Mu;p + Mv;p ≤ 3, ∀ p ∈ P .
(4) Message overlap constraints: For two messages on
ei = (ui ; vi ); ej = (uj ; vj ) 2 E , communication overlap can
be formulated in the same way as task overlap constraints.
− sui ;vi
fuj ;vj
γei ;ej + γej ;ei + αei ;ej + βei ;ej
≤ N × γei ;ej , ∀ u, v ∈ V .
≤ 3, ∀ ei , ej ∈ E .
(5)
messages on edges ei ; ej 2 E , the following set of constraints
(5) Communication contention in time: For any two
deﬁnes time overlap of the two messages.
≤ fuj ;vj ) ∧ (fui ;vi
≥ suj ;vj ),
(sui ;vi
ℓvj ;ui /2 + ℓvi ;uj /2 − λ ≤ βei ;ej
≤ ℓvj ;ui /2 + ℓvi ;uj /2,
(su − fv )/N ≤ ℓu;v ≤ (su − fv )/N + 1,
(6)
∀ u, v ∈ V , ∀ ei , ej ∈ E .
where (cid:21) is a constant for linearization, and set as 0:75.
(6) Communication contention in space: For edges
ei = (ui ; vi ); ej = (uj ; vj ) 2 E , we denote the location of processors as coordinates, that u = (xu ; yu ) and v = (xv ; yv ).
We deﬁne (cid:11)ei ;ej = 1 to represent ei and ej have at least
one shared link. To accurately model it, the contended links
will be in x-coordinate or y -coordinate, when network is regarded as a two-dimensional coordinate axis. In one case,
messages sharing links in horizontal direction must have a
same y -coordinate of their sources in Equation (7). (Similar
model in vertical direction.)
αei ;ej
≥ [Mui ;p + Mvi ;q + Muj ;m + Mvj ;n − 4]
(7)
+⟨(yui = yuj ) ∧
{ [(xui > xuj ) ∧ [xui < xvj ) ∧ (xui < xvi )] ∨
[(xui < xuj ) ∧ (xuj < xvi ) ∧ (xuj < xvj )] ∨
[(xui > xuj ) ∧ (xuj > xvi ) ∧ (xvj < xuj )] ∨
[(xui < xuj ) ∧ (xui > xvj ) ∧ (xvi < xui )] }⟩.
fu;v ≥ su;v + 3 × (Mu;p + Mv;q − 1) × ei∑
γei ;ej + 1) + (Wu;v − 1), ∀ ei , ej ∈ E , ∀ p, q ∈ P .
(7) SMART communication delay constraints: Communication delay is upgraded as described in Section 2-B.
γei ;ej + 3 × 2
ei∑
ej 2E
+(
ej 2E
(8)
(1)
(2)
(3)
(4)
∑
∑
Figure 6: Communication through SMART routers.
where 3 (cid:2) 2 represents the delay in source and destination
routers as shown in Fig. 6. (
(cid:13)ei ;ej ) is the sum of conﬂicts,
and (
(cid:13)ei ;ej + 1) is the number of bypasses.
(8) Ob jective: Minimize application schedule length.
fapp ≥ fu , M in{fapp }, ∀ u ∈ V .
(9)
The model is established to illustrate mapping and scheduling with detailed communication scenarios for regular/SMART
NoCs. It is compatible with any deterministic routing algorithms as long as the transmission path can be decided at
design-time. Nevertheless, this model can incorporate other
routing algorithms by modifying Formula (7) accordingly to
address spacial contention. Besides, Formula (2)(8) can be
revised for distinct realization of router mechanisms.
5. HEURISTIC ALGORITHMS
Since task mapping problem is NP-hard and the design
space is exponentially increased, the running time of ILP
rises rapidly with the increasing size of applications. It is
not surprising that our technique has introduced a new dimension to detect contention in search space, which might be
huge since edges have a quadratic relation to task nodes. To
eﬀectively solve the complex issue in SMART NoC, we propose polynomial-time heuristic algorithms for contentionaware mapping and run-time scheduling.
A. Communication Contention-Aware Task Mapping
Long-distance packets transmission can be forwarded in
a single-cycle through bypass in SMART NoC if there is
no communication conﬂict. Thus, a contention-aware task
mapping in Algorithm 1 is proposed to provide a nearly
contention-free mapping and thus enhance the beneﬁts of
SMART NoC in communication eﬃciency. Without too
much concern about the distance between processors, dependent tasks can be mapped non-contiguous to evade spatial
conﬂict and explore more bypass routing in SMART NoC,
instead of traditional communication gathered mapping.
Figure 7: Contention-aware task mapping.
Generally, as the example shown in Fig. 7, if M(v2 !
P13 ), v0 will be mapped to the processors in the bottom box
(cid:20) yv2 . And v1 will be mapped to the processors
within the upper box with yv1 > yv2 . Similarly, if M(v3 !
where yv0
(cid:20) xv3 ,
P5 ), v4 will be mapped to the processors where xv4
and v5 to the processors where xv1 > xv3 .
B. Priority Assignment in Scheduling
Although the contention-aware task mapping can provide
more bypasses in SMART NoC, it is hard to completely eliminate all conﬂicts especially with the increasing complexity
of traﬃc ﬂow on network. With the ob jective to maximize
Figure 8: Results comparison of Application Schedule Length and Communication Latency in SMART NoCs by diﬀerent mapping methods.
Figure 9: Scalability of the
proposed heuristic algorithms.
Algorithm 1 Contention-aware Task Mapping
3:
4:
5:
Input: 1) Task Graph G = (V , E ); 2) SMART NoC: P = n × n.
Output: Mapping: M(V → P ).
1: Q ←Initial queue for v ∈ V as decreased out-degree;
2: if Q ̸= ϕ and (M(u → Pxu ;yu ) = 1) then
for v , w ∈ V do
if (eu;v = 1) ∧ (eu;w = 1) then
if [(xv ≥ xu ) ∧ (xw < xu ) = 1] ∨ [(yv ≥ yu ) ∧ (yw <
yu ) = 1] then
M(v → Pxv ;yv ); M(w → Pxw ;yw );
else
M(v → Pxv (cid:0)1;yv (cid:0)1 ); M(w → Pxw +1;yw +1 );
end if
end if
end for
12: end if
6:
7:
8:
9:
10:
11:
communication eﬃciency and minimize application schedule length, a priority assignment strategy (Algorithm 2) is
presented in addition to the mapping algorithm.
Run-time schedule is adaptively performed based on the
criticality of workloads.
It provides a priority assignment
policy for task execution on processors, ﬂits transmission in
SA-L stage, and SSR arbitration in SA-G stage. The priority
is calculated by downstream workloads referring to the sum
of computation and communication overhead.
f lit∑
f lit2CP (v)
vi∑
vi2CP (v)
P rio(SSR; f lit; task) =
Lf lit +
Lvi :
where Lf lit ; Lvi are deﬁned as transmission time of ﬂit and
execution time of vi in path C P (v). Thus, the winner in
task execution overlap on processors, or ﬂits contention and
SSR conﬂict in intermediate routers, will be assigned with a
higher priority to occupy the contended resources.
3:
Algorithm 2 Priority Assignment Strategy
Input: Application mapping M(V → P ).
Output: Tasks and communications schedule S che.
1: Initialize: S che = ϕ; QP ← Queue for u, M(u → P );
2: if P ∈ P is free and QP ̸= ϕ then
Select u with the highest priority; S che ← S che(u, su , fu );
4: end if
5: if fu ∧ (eu;v = 1) ∧ (eu;w = 1) then
if P rio(eu;v ) ≥ P rio(eu;w ) then
if P rio(SSReu;v ) = highest then
Bypass ← S che(eu;v , su;v , fu;v );
else
Buf f ered ← S che(eu;v , su;v , fu;v );
end if
end if
13: end if
6:
7:
8:
9:
10:
11:
12:
The time complexity of Algorithm 1 is proportional to the
number of processors and the number of tasks. The strategy
in Algorithm 2 is also polynomial, which is in an additional
linear growth with the size of messages on edges. Therefore,
these polynomial-time mapping and scheduling algorithms
will take negligible running time and be incomparably eﬃcient compared with the time-consuming ILP approach. The
combination of design-time contention-aware mapping with
run-time priority assignment policy has eﬀectively addressed
problems in SMART NoC, guaranteeing maximized bypass
for message transmission, and the best sequence for task execution and communication. The advantages of SMART NoC
are greatly enhanced by system level management strategy.
6. PERFORMANCE EVALUATION
A. Experimental Setup
In this section, we perform extensive sets of experiments to
evaluate the eﬀectiveness of the proposed ILP model (solved
by Gurobi ) and heuristic algorithms for communication efﬁciency and application performance in SMART NoCs.
We consider 3 (cid:2) 3, 4 (cid:2) 4, 6 (cid:2) 6, 8 (cid:2) 8 2-D Mesh-based
SMART NoCs as the target platforms. Processors, arranged
as a dual-lane proﬁle in logical interconnection in Fig. 5, are
homogeneous with same execution eﬃciency. We base our
experiment on well-known DSP-stone benchmarks [20] (IIR
ﬁlter, 8 Rlslattice, Diﬀ.Equ.Solver, 4 Stage Lattice, 7 Elf,
8 Stage Lattice without cycles), as well as industry standard
H.264 HDTV decoder derived as a 51 nodes task graph [21].
We build realistic simulator in Python to evaluate the proposed approaches on the target platforms, on which we do
contention-aware task mapping and priority assignment with
the simulated message transmission by SMART routers. We
use H P Cmax = 8 as conﬁrmed in [4] in our evaluation.
Experiments are run on a workstation with Intel (R)X eon
(R)E 5-2650 at 2:6 GH z and 64 GB memory.
Evaluation results on communication latency, application
performance and simulation time are compared with the
state-of-the-art techniques, including the task mapping methods proposed by N.Koziris [15], Davare [16] and Yu [17].
B. Evaluation Results
Since we have considered communication contention in
task mapping and priority assignment in scheduling, the
proposed ILP model and algorithms can achieve signiﬁcant
improvements both on communication eﬃciency and application performance. As the results shown in Fig. 8, ILP approach with accurate communication modeling can always
ﬁnd the optimal solutions, where on average 32:2%, 32:5%,
39:4% of application performance are achieved than state-ofthe-art approaches. The proposed heuristic algorithms can
also obtain averagely 30:1%, 31:4%, 36:9% improvement on
application performance than other three approaches. Besides, results generated by our algorithms are near-optimal,
which is on average 1:9% worse than the optimal results by
ILP approach. Communication latency is signiﬁcantly reduced by the proposed ILP and algorithms, due to the minTable 2: Design Space and Simulation Time of Benchmark Applications on 2-D SMART NoC Platforms.
Benchmarks
Design Space
Solutions
Simulation Time
3×3
4×4
6×6
8×8
3 × 3
4 × 4
6 × 6
8 × 8
Average
ILP Heur Yu [17]
ILP Heur Yu [17]
ILP Heur Yu [17]
ILP Heur Yu [17]
ILP
Heur Yu [17]
iir F
1:8E+11 1:8E+14 3:2E+16 7:9E+18 132
149
185
134
139
187
134
138
188
117
123
230
1mim
2s
2s
8 RLat 1:3E+18 7:5E+22 2:5E+25 2:1E+28 149
157
203
137
143
201
136
142
231
131
141
245
3mims
4s
5s
DSPd:e:Sol 9:8E+22 3:0E+26 4:3E+32 5:4E+39 187
189
230
183
188
275
183
187
277
181
182
281
1hr
11s
11s
Stone
4s Lat 6:4E+16 2:0E+24 3:1E+31 9:1E+36 189
201
283
201
209
331
197
205
341
175
188
381
5hrs
15s
16s
7 E lf
2:7E+22 8:7E+30 2:7E+36 2:5E+41 157
169
331
171
173
371
168
172
369
166
177
401
10hrs
18s
21s
8s Lat 1:2E+33 3:7E+40 4:1E+42 7:2E+45 216
222
307
213
219
381
215
221
377
215
216
357
17hrs
33s
34s
Decoder H.264
4:6E+48 2:5E+51 3:0E+58 1:3E+64
{
2247
2841
{
2131
2700
{
2117
2834
{
2108
2816 > 1day
55s
53s
imized communication contention and the maximized bypass
routing on SMART NoCs. Averagely 44:5%, 44:1%, 45:3%
and 41:3%, 42:7%, 44:8% of on-chip communication latency
are reduced, respectively, by ILP model and our algorithms
compared with that obtained by[15, 16, 17].
There are several observations from the results.
i) Our
proposed heuristic algorithms for contention-aware mapping
and priority assignment in scheduling can obtain near-optimal
solutions compared with the optimal results by ILP. Application performance is signiﬁcantly improved due to the
minimized communication contention in task mapping and
the priority-based task and communication scheduling.
ii)
Since the communication model in our approaches is much
more accurate to approximate the complex runtime communication scenarios in a ﬁne-granularity, communication
eﬃciency is much better than that in state-of-the-art approaches. Communication latency can be greatly reduced
beneﬁting from the maximized bypass routing for long-distance
message transmission in our methods.
iii) With increased
mesh size, we can obtain better results both of communication and application performance, since more processors
are available to be selected when task mapping, thus the
communication contention can be further reduced.
Then, we do another set of experiments to evaluate the
scalability of our algorithms. We have generated a group of
H.264 benchmark instances and executed them as independent applications with injection rate from 0:1 to 0:6. As the
results shown in Fig. 9, with the increasing injection, communication latency generated by the proposed algorithms on
of communication scenarios. Nevertheless, results in 8 (cid:2) 8
four SMART NoCs grows due to the increased complexity
SMART NoC are better than that in other three platforms,
since more optional processors in large-scale NoCs are available for contention circumvented task mapping. Even if processors are non-contiguous, long-distance transmission can
be performed in a single cycle via bypass. It veriﬁes that
the proposed contention-aware task mapping and priority
assignment approaches are scalable and eﬀective for largescale SMART NoCs in improving communication eﬃciency.
Finally, we record the running time of ILP, our algorithms
and Yu [17] in Table. 2. Since the solving time is closely related to design space, we have quantitatively analyzed the
complexity of solution space on account of the scales of applications and platforms. It is not surprising that the time
complexity of ILP model is exponentially increased, since
the growth of search space is exponential and ILP model
takes an average 30(cid:2) more time to ﬁnd the optimal soluhas introduced a new dimension for priority assignment. It
tion. We have integrated ILP modeling skills to improve its
eﬃciency. Nevertheless, it is worthwhile taking more eﬀorts
to explore ﬁne-grained communication optimization opportunities, not only due to the huge performance gain that
could be expected, but also because it is running at designtime for only once and then will probably be applied to millions of devices for run-time implementation. Meanwhile,
eﬃcient with negligible running time ((cid:20) 55s) compared with
our proposed polynomial-time algorithms are incomparably
the time-costing ILP method. Besides, ILP approach and
heuristic algorithms can respectively achieve at least 35:7%,
33:2% on application performance than Yu [17].
7. CONCLUSION
In this paper, we present system-level application mapping techniques, including an ILP model and the eﬃcient
algorithms, that are necessarily designed to be compatible
with SMART NoC and boost its hardware-software system
performance. Conﬂict-induced communication eﬃciency loss
and system performance degradation in SMART NoC are
minimized through the maximized bypass routing in designtime task mapping and run-time priority assignment strategy. Advantages of SMART NoC in quick inter-processor
communication are enhanced with signiﬁcant achievement in
entire performance compared to state-of-the-art approaches.
8. "
Cost-efficient buffer sizing in shared-memory 3D-MPSoCs using wide I/O interfaces.,"This paper addresses link-buffer capacity allocation in the design process of best-effort 3DNoCs holding hotspot memory ports. We show that in 3DSoCs with integrated wide I/O DRAMs, the congestion spreading is different from SoCs with external DRAMs: the bottlenecks are not anymore the external memory ports but the network links that become saturated and retro-propagate the congestion. The distribution of bottleneck links is directly affected by the traffic directed to the hot memory ports. Using an analytical performance evaluation method, we determine network link buffer capacities according to the given workload composed of regular and hotspot traffics.","Cost-Efficient Buffer Sizing in Shared-Memory 
3D-MPSoCs Using Wide I/O Interfaces 
Sahar Foroutan 
TIMA Laboratory 
 46 Avenue Félix Viallet 
 38031, Grenoble, France 
+334-76-57-43-02 
Abbas Sheibanyrad 
TIMA Laboratory 
 46 Avenue Félix Viallet 
 38031, Grenoble, France 
+334-76-57-48-64 
Frédéric Pétrot 
TIMA Laboratory 
 46 Avenue Félix Viallet 
 38031, Grenoble, France 
+334-76-57-48-70 
Sahar.Foroutan@imag.fr 
Abbas.Sheibanyrad@imag.fr 
Frederic.Petrot@imag.fr 
ABSTRACT  
This paper addresses link-buffer capacity allocation in the design 
process of best-effort 3DNoCs holding hotspot memory ports. We 
show that in 3DSoCs with integrated wide I/O DRAMs, the 
congestion spreading is different from SoCs with external 
DRAMs: the bottlenecks are not anymore the external memory 
ports but the network links that become saturated and retropropagate the congestion. The distribution of bottleneck links is 
directly affected by the traffic directed to the hot memory ports. 
Using an analytical performance evaluation method, we determine 
network link buffer capacities according to the given workload 
composed of regular and hotspot traffics. 
Categories and Subject Descriptors 
B.8.2 [Performance and Reliability]: Performance Analysis and 
Design Aids; B.3.3 [Memory Structures] Performance Analysis 
and Design Aids--Formal models; B.4.3 [Input/Output and Data 
Communications]: Interconnections (subsystems)--Topology (eg. 
bus, point-to-point) 
General Terms 
Design, Performance  
Keywords 
Multiprocessor Systems-on-Chip (MPSoCs), Networks-on-Chip 
(NoCs), Performance Analysis.   
1. INTRODUCTION 
In typical multi-core SoC (System-on-Chip)-based devices, many 
applications (e.g. HD video, image processing and 3D gaming) 
request data from shared memories leading to a drastically 
increasing demand on memory bandwidth. In current SoC 
architectures a significant part of the required shared memory is 
located outside the chip (i.e. off-chip DRAMs). Next-generation 
handheld devices with 3DHD video and 3D graphics will require 
a total memory bandwidth of 10’s of GB/s. It is even possible that 
these requirements reach 1 TB/s in the coming years [1]. Fast 
memory interfaces (e.g. DDR and LPDDR) have emerged to cope 
with the increasing demanding memory bandwidth. However, 
these standards have limitations when the bandwidth must be 
further increased, mainly due to high frequency in a noisy 
environment. Reducing the frequency is possible if the bus width 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. 
 DAC 2012, June 3-7, 2012, San Francisco, California, USA. 
Copyright 2012 ACM 978-1-4503-1199-1/12/06...$10.00. 
can be enlarged. But, current MPSoCs cannot afford this solution 
due to limitations on the number of external pads [2]. 
Emerging 3D-Integration [3], by enabling the stacking of dies 
fabricated in different technologies, allows the migration of dense 
dynamic memories inside chips. Using Through-Silicon-Vias 
(TSV), 3D-Integration provides a unique opportunity to propose 
new wide data bus interface, called Wide I/O, between the MPSoC 
and the stacked memory die. With wider and slower memory 
interfaces, Wide I/O improves the bandwidth and reduces power 
compared to existing memory interfaces [2, 4].  
In spite of all advantages of on-chip stacked-memories, hotspot 
dealing, as a common concern of MPSoCs with a centralized 
shared memory, remains but as shown in this paper in a different 
shape. Traditionally a hotspot is defined as a module receiving 
traffic from the majority of network sources with an aggregate 
rate exceeding the rate at which it can absorb data [5, 6]. When 
the arriving traffic persists indefinitely the hotspot causes a serious 
congestion situation in the network, called tree saturation [6]: 
when a memory buffer becomes saturated it forces that router’s 
link-level flow control to throttle back all the inputs feeding the 
memory module (since the network is lossless), that in turn cause 
the previous routers to fill their buffer and so on back to the traffic 
sources. An external shared  memory module is a hotspots and 
causes a serious bandwidth bottleneck in the system [7], while this 
concern is less important with Wide I/O 3D-DRAMS as their 
bandwidth is not less than the NoC bandwidth. This paper shows 
that with Wide I/O memories, as opposed to off-chip memories, 
the root of a tree saturation is not necessarily the memory port. 
Instead, because of NoC limited buffering capacity, network links 
saturate and become system bottlenecks. 
Several dynamic software or hardware methods (mostly for offchip networks) have been proposed to detect the hotspot 
contention at runtime and thus to avoid, prevent or remove the 
saturation tree as soon as detected (e.g. in [6, 8, 9]). Hardware 
techniques are more efficient since according to [8] the congestion 
tree is filled in less than 10 traversal times of the network – far too 
quickly for software to react in time to the problem. Most of the 
hardware techniques are too costly when applied to NoCs that are 
extremely cost-constrained architectures. Hardware combining [7] 
in which each router combines hotspot requests into a single one, 
Multipath networks  [7, 9, 10] in which extra paths are added to 
divert the regular traffic around the hotspot (requires large number 
of buffers, virtual channels, multiplexors, demultiplexors, etc.), 
Discard strategies [11] that forwards only one request to the hot 
memory and discards others, and the technique of  Deflecting 
packets away of loaded locations [12] (requires adaptive routing), 
are some of these techniques. 
This work is supported by Catrene Project CT105-3DIM3   
 
 
Among dynamic techniques, rate-regulation strategies have been 
widely implemented in both off-chip networks (e.g. [7]) and 
NoCs. (e.g. [2, 5]). Generally speaking, the hot memory is 
monitored dynamically and processors are notified (usually by 
using an end-to-end flow control mechanism) to stop or regulate 
the data injection if memory is at a congestion state. Since these 
strategies need to detect the congestion sate at runtime, they are 
usually expensive to implement and thus more appropriate for 
real-time applications that need guaranteed services. Also, in endto-end flow control strategies, the problem is not solved at the 
network layer; instead, it’s transferred to upper layers such as 
network-interface or application layers. While for the best-effort 
networks a simple and cost-efficient solution must be provided at 
the network layer.  
In this paper we do not intend to remove the hotspot problem 
dynamically. Instead, by properly sizing the NoC buffers we try to 
alleviate the impact of hotspot traffic on the network performance 
in terms of network average latency and saturation point. The 
proposed buffer sizing method is static and applied at design time. 
It addresses best-effort general purpose NoCs or 
those 
applications specific NoCs in which the traffic distribution (in 
terms of the average data rate transferred between different 
communicating IP pairs) is known at design time. Using the 
analytical method proposed in [13], we have developed a tool to 
automate the buffer sizing process. Hu et al. in [14] and Guz et al. 
in [15] take similar approaches in terms of using an analytical 
delay model as the heart of the buffer allocation method. However 
their buffer allocation algorithms are completely different from 
ours. They both use greedy algorithms while we distribute the 
buffering budget simply according to the distribution of the 
workload on network links (resulting in a less complex method).  
The rest of this paper is organized as follows. Section 2 discusses 
how the presence of hotspot nodes deteriorates the whole NoC 
performance. It also presents the assumptions of this work. The 
impact of 3D-specific wide-interfaces and NoC architectural 
parameters on the congestion is described in section 3. Section 4 
shows how the system bottlenecks are distributed over the 
network. In order to alleviate the system performance, in Section 
5 we propose a buffer sizing method to assigns non-uniform 
buffer capacities to network links. We also compare the NoC costperformance, before and after buffer sizing. Finally we summarize 
our contributions in section Error! ource not found.. 
2. PERFROMANCE DEGRADATION DUE TO 
HOTSPOTS 
The performance metric taken as reference of comparisons in this 
paper is the network saturation point as defined in the following.  
2.1 Saturation Point as a Performance Metric 
The network saturation point is the offered-load at which the 
network gets saturated and reaches its maximum sustainable 
throughput and thus cannot accept more offered-load from traffic 
sources. The term offered-load is used as the rate of flit injection 
by each core. Therefore the same unit of measurement for both 
offered-load and saturation point is used which is flits/cycle 
(shown in %). We use average-latency/offered-load curves to 
determine the network saturation point. When the network is 
saturated the average latency tends to infinity [16, 17]. Thus the 
offered-load at which the average latency grows infinitely equals 
to the NoC saturation point. The average latency is defined as the 
average latency of all possible paths of the network. 
To obtain the latency/load curves we use an ameliorated version 
of the analytical method proposed in [13] (see the appendix added 
to the end of this paper). The method is based on a router delay 
model that determines different delays inside a best-effort 
wormhole router (e.g. link acquisition and link transfer delays) 
within an acceptable accuracy. The model deals with the direct 
contentions between different flows coming to a router. It also 
handles the back pressure impact that happens in sequences of 
routers. For a given NoC (in terms of topology, routing, and link 
capacities) and its workload consisting of a regular traffic 
(determined by the traffic spatial distribution and data generation 
rate) and a hotspot traffic (determined by the number of hotspots, 
their location, and the fraction of each source load sent to each 
hotspot), the method provides an average delay analysis of the 
network layer. More precisely it provides average per-path latency 
(i.e. the latency of any desired path in the network), average router 
latency (the latency between any I/O ports of a router), average 
buffer utilizations and the probability of contention for accessing 
any shared link of the network. Using the method we obtain the 
latency/load curves and thus determine the network saturation 
point. The method is very fast and converges in less than one 
minute for experiments that require up to two days of Cycle-BitAccurate simulation. Therefore regarding the number of analyses 
and the dimension of the 3D-NoC used in our experiments, the 
use of the analytical method is more time-efficient than using 
simulations.    
2.2 Saturation-Point as a Function of HotspotFraction  
When the tree saturation occurs even the normal packets (i.e. 
packets whose destinations are not hotspot modules) are affected 
by congestion. This is more serious in best-effort networks where 
hotspot and non hotspot packets compete for the same resources. 
Hence the network as a whole suffers a catastrophic loss of 
performance. This conclusion applies to any network but hotspot 
effects in a wormhole network are more severe than in a storeand-forward one, as packets are blocked across multiple routers 
and buffering space 
is 
limited. To better understand 
the 
importance of the hotspot impact on the NoC performance, Figure 
1 shows the saturation points of a 3D-NoC obtained for different 
amounts of hotspot fraction (h). In this experiment (and other 
experiments of the paper) it is assumed that the 3D-SoC contains a 
NoC with a fully connected 3x5x5 3D-Mesh topology. Since the 
emergence of 3D-SoCs, other topologies have been proposed for 
3D interconnection that may exhibit better performance for 3D 
shared-memory MPSoCs (e.g. the network-in-memory architecture 
proposed in [18]). However, in this work we do not aim to 
investigate the impact of topology since the occurrence of tree 
saturation is independent from the network topology [8]. In general 
in any network holding at least one hotspot a tree saturation 
appears [8]. Only the shape of the tree may change form one 
topology to another. In the experiment of Figure 1 the NoC 
contains only one hotspot located on the center of its upper die. 
We assume that two kinds of traffic are simultaneously running 
namely hotspot and regular traffic. To generate hotspot traffic we 
use the model proposed in [6]. A new generated packet has a 
probability h, called the hotspot fraction, of being directed to the 
hotspot node and probability (1-h) of being directed, according to 
the regular traffic, to the other network nodes. Nodes generate 
traffic independently of each other following a Poisson process 
with a mean rate of λ flits/cycle (λ = offered-load), consisting of 
hotspot and regular portions of hλ and (1-h)λ, respectively. When 
i-th hotspot, h is equal to ∑ (cid:2190)(cid:2191)
there are more than one hotspot modules, h signifies the total 
hotspot fractions sent to all hotspots. For example when four 
hotspots exist and each node sends hi% of its offered load (λ) to the 
. The regular traffic is distributed 
uniformly in the following experiments, but it can follow any 
(cid:2781)(cid:2191)(cid:2880)(cid:2778)
spatial distribution determining 
the average data rate (λ) 
transferred between each communicating IP pairs. The hotspot 
memory controllers are assumed to absorb data flits at a rate equal 
or greater than the data arrival rate (the network bandwidth <= 
memory bandwidth). Regarding the high bandwidth of the new 
generation of stacked 3D-DRAM protocols (e.g. Wide I/O) this 
assumption is realistic. Similarly other destinations (non hotspot 
ones) are also assumed to behave as sinks. 
As it is shown in Figure 1, when the hotspot fraction increases the 
saturation point decreases considerably. For a hotspot fraction 
greater than 11% (i.e. each node sends only 11% of its total 
offered-load to the hot memory) the saturation point falls under 
10%. This is whereas the same network is capable to sustain up to 
45% of offered-load when the traffic is uniformly distributed 
(h=0). This example shows how drastically a hotspot module can 
deteriorate the overall NoC performance. 
Figure 1. The NoC saturation point determined by latency/load curves 
for various amounts of hotspot fraction     
3. THE EFFECTS OF SOC WORKLOAD AND 
ARCHITECTURAL PARAMETERS  
This section investigates the impact of the parameters related to 
the SoC workload (e.g. number and location of memory 
controllers shaping the hotspot traffic) and also its architectural 
parameters (e.g. routing algorithm) on the SoC performance.  
3.1 4-Port Wide I/O Memory Interface 
Memory vendors and JEDEC (JC-42.6) are currently developing 
standards for Wide I/O memory interface with TSV interconnects 
stacked on SoC [2, 4]. Wide I/O calls for a 512-bit wide data 
interface providing the total peak bandwidth of 12.8GB/second 
(with 200 MHz I/O bus clock). The chip architecture of a Wide 
I/O DRAM is made up of four independent memory partitions 
(channels or ranks) which are symmetric with respect to the chip 
center. Each channel has 4 Banks, with a density per channel 
between 256Mb and 8Gb. 
Although there are some works in the literature focusing on 
the location of memory controllers in on-chip many-core systems 
[19], few of them concern directly 3D-NoCs and Wide I/O 
memories. According to [2, 4] the Wide I/O memory controllers 
are located on the logic die and not on the DRAM die. To access 
the four channels either a single centralized memory controller is 
considered as a unique resource shared among the sub-systems 
(similar to Figure 2.A) or, four independent controllers manage 
parallel traffic streams to the four independent memory channels 
(like Figure 2.B). In this case each channel has its own 128-bit 
wide PHY interface working at 200MHz, thus providing 3.2 
GByte/s bandwidth which is approximately in the range of the 
link bandwidth of current NoCs [20]. In our experiments 
presented in the remaining of the paper we consider both cases.   
Wide I/O DRAM die 
Wide I/O DRAM die 
512 bit 
C
o
N
D
3
C
o
N
D
3
DRAM controller: one for the die 
DRAM controller: one per channel 
(A)  
(B) 
Figure 2. Wide I/O DRAM controller schemes: (A) one 512-bit wide 
controller for the DRAM die and (B) four separate 128-bit wide 
controllers for the four channels 
3.2 Hotspot and Architectural Parameters 
Figure 3 shows the saturation points (y-axis) obtained for different 
amounts of hotspot fraction in the 3D-NoC with one and four 
DRAM controllers located like Figure 2 (note that Figure 3 gives a 
2D-view of the x-y plane of the 3D curves similar to Figure 1 
when looking down from above). It is shown that having four 
memory controllers instead of one, results in a significant increase 
in saturation point. This is because the hotspot traffic is distributed 
more homogenously. As a result collision and congestion of the 
network decrease. Therefore the maximum sustainable throughput 
increases. However, even with four controllers the saturation point 
degrades considerably when the hotspot fraction rises. Low 
saturation points signify that such workloads (that mimic the 
behavior of applications requiring a huge amount of the shared 
memory bandwidth), are practically unfeasible on the NoCs in 
which the total buffering budget is uniformly distributed between 
network links (link have identical buffer length).  
An interesting point in Figure 3 is that the peak saturation point 
with four memory ports is achieved when the hotspot fraction is 
equal to 6% and not 0%. This signifies that in order to have a 
better performance, what is important is the lower contentions 
over the whole network, due to the combination of both hotspot 
and regular traffics. In this experiment the best case achieves 
when 6% of the load of each node is sent to the four hotspots and 
the rest sent uniformly to other destinations. 
Apart from the hotspot traffic parameters, the NoC architectural 
parameters are also of crucial importance on the propagation of 
congestion, and thus the network saturation-point. Figure 4 
compares latency/load curves related to different configurations of 
DRAM controllers and different routing algorithms. Three 
configurations are shown: 1) a single memory controller (1-port) 
located in the center of the upper die, 2) four memory controllers 
(4-port) located in the center of each corresponding partition of 
the upper die (Figure 2.B), and 3) four memory controllers on the 
four corners of the upper die. The dimension-ordered routing 
algorithms of XYZ (first X, then Y, and last Z) and ZXY are 
applied to each configuration. The hotspot fraction is set to 25%. 
As we can observe these parameters have influence on the 
network performance. When the controllers (either one or four) 
are nearer to the center of the NoC, the saturation point is higher 
with ZXY routing (the effect is more important for the case of one 
single memory controller), whereas, with four controllers located 
on the corner nodes, the highest saturation point is achieved when 
the XYZ routing is applied. 
 
 
 
 
Figure 3. Saturation point as a function of hot
tspot fraction 
not congested and they are not equ
ually loaded (because of the 
effect of the routing algorithm). Th
his testifies that the roots of 
saturation trees are these links, and 
not the memory ports. 
• With h=5% the workload is distr
ributed more uniformly and 
thus the buffers saturate in a more u
uniform way. Just before the 
NoC saturation point, the most load
ded buffers are those that are 
located on the middle die (shown b
by arrows) far from hotspots. 
More interesting, these links do
o not even belong to the 
saturation trees due to any of the f
four hotspots (regarding the 
ZXY routing).   
4.1 How to find Bottlenecks
s  
Since bottlenecks severely affect th
e network throughput and 
average latency, we would like to re
ecognize the regions of the 
network that are more sensitive to the a
accumulation of data flits. In 
order to do this we have implemented
d a tool that determines the 
distribution of traffic in the buffers of t
the network by obtaining the 
average buffer utilizations (also called
d average buffer occupancy) 
for all buffers (like Figure 5). The to
ol uses the aforementioned 
analytical method [13] (described in 
the appendix) to determine 
average buffer utilizations for a giv
en workload. It selects an 
operational offered load very close t
to the saturation point and 
obtains buffer utilizations and determin
nes the links almost saturated 
at this offered load. These buffers are
e the bottlenecks and cause 
retro-propagation of congestion to up
pstream buffers and thus a 
em.  
saturation tree is build behind any of th
Figure 4. The impact of routing algorithm and ho
tspot locations on 
the performance of a NoC containing one or four h
hotspots (h=25%)   
4. BOTTLENECK DISTRIBUTION  
In NoCs with external memory the hot memory
y port is the main 
bottleneck since it cannot absorb data with the r
request arrival rate 
(i.e. the network bandwidth > the memory band
dwidth). However, 
the new generation of 3D wide-interface DR
RAMs makes the 
memory interfaces to work with approximately th
he same rate as the 
NoC (even much faster if one 512-bit port is us
sed instead of four 
128-bit ports). In such systems even with a hig
gh rate of memory 
bandwidth demand, the memory port is not the
e main root of the 
tree saturation in the system. Instead, network 
contention due to 
high traffic toward the hotspot node causes som
me of the network 
links become congested and perform tree saturat
tions behind them. 
It means that the system hotspots are not necess
sarily the network 
bottlenecks. Instead, because of the network lim
mitations (e.g. high 
contentions and limited buffering capacity), an
ny of the network 
links can become a system bottleneck. Thus,
, let’s distinguish 
henceforth a hotspot from a bottleneck such that 
a hotspot signifies 
a hot destination (here a DRAM controller)
), and a network 
bottleneck is a congested buffer that starts to and
d thus throttles all 
upstream buffers such that a tree saturation appea
ars. 
The distribution of bottlenecks depends on th
he NoC topology, 
traffic shape, and routing algorithm. For exampl
le Figure 5 (A) and 
(B), presenting the spectrum of buffer-utilizatio
ons, show that the 
distribution of congested links can be complete
ely different in the 
same NoC when only one parameter changes
s. The same NoC 
configuration is considered in both figures: unifo
orm regular traffic, 
4 memory controllers on the centers of the upper 
die, ZXY routing, 
8-flit buffer per link, and 16-flit packets. The on
nly parameter that 
differs is the hotspot fraction that is 5% in (A) and
d 25% in (B).  
• With h=25% the hotspot traffic is too hi
igh. So the most 
congested buffers are the input ports of the 
routers connected 
to the 4 hotspots. Note that all of the hotspo
ots’ input links are 
(A)   h=5%
(B)  h=25% 
Figure 5. The distribution of congested b
buffers: In the same NoC (the
ZXY routing, buffer length=8 flits, Unif
form background traffic), the
congested buffer distribution is comp
letely different in (A) with
h=5%, and (B) h=25%.  
5. BUFFER SIZING  
As known, if the capacity of link buff
fers in a wormhole network 
increases the saturation point of the
e network increases. In a 
wormhole packet-switching network w
when a packet is traversing 
the network, all resources in the path b
etween the header (first flit) 
and the trailer (last flit) are allocated 
to the packet and no other 
packet can use those resources. This is
s the reason that contentions 
rises in the network as there are less fr
ree resources to be allocated 
to new packets. Now by a higher capa
acity for link buffers we let 
 
 
 
the packets occupy a shorter part of a path and thus the resources 
of the rest of the path are released. Reminding that more than 80% 
of the area of a router belongs to its buffers [20], due to cost 
constraints assigning large buffers to all network links is not 
practically feasible. On the other hand, as seen in Figure 5, the 
distribution of workload is not homogeneous in all links. For 
example with higher value of hotspot fraction most of the links 
have little buffer utilization and there are only few highly loaded 
buffers in the networks. Using large buffers (more than the need 
of links) is a waste of area and power. In the following we propose 
a cost-efficient buffer capacity allocation method that resizes 
buffers proportionally to their average occupancy:  
- 
Performing an initial analysis to determine the network 
saturation point. For this initial analysis the buffer length is 
assumed to be infinite (in practice a very large buffer, e.g. 200 
times the average packet length which is 3200-flit buffer in 
our experiments is assigned to each link).  
- Once the saturation point is known, determining the average 
buffer utilization for all network links at a load just before the 
NoC saturation point using infinite FIFOs (e.g. 3200 flits).  
- Determining the normalizing scale factor as follow: 
(cid:1845)(cid:1855)(cid:1853)(cid:1864)(cid:1857) (cid:3404) (cid:3014)(cid:3028)(cid:3051)(cid:3261)(cid:3280)(cid:3289)(cid:2879) (cid:3014)(cid:3036)(cid:3041)(cid:3261)(cid:3280)(cid:3289)
(cid:3003)(cid:3042)(cid:3047)(cid:3047)(cid:3039)(cid:3032)(cid:3041)(cid:3032)(cid:3030)(cid:3038)
where Bottleneck is the buffer utilization of the most 
congested link, and MaxLen is the largest buffer the designer 
intends to allocate to the bottleneck link and is given to the 
tool as an input parameter. MinLen is the buffer length 
(determined also by the system designer) allocated to buffers 
that are too slightly loaded (i.e. the buffers that do not receive 
important traffic but because of the architectural reasons and 
in order to absorb occasional bursts their existence is needed). 
- Optimizing the buffer length of each network link as follow: 
(cid:1828)(cid:1873)(cid:1858)(cid:1858)(cid:1857)(cid:1870) (cid:3404) (cid:1729)(cid:1847)(cid:1872)(cid:1861)(cid:1864)(cid:1861)(cid:1878)(cid:1853)(cid:1872)(cid:1861)(cid:1867)(cid:1866) (cid:3400) (cid:1845)(cid:1855)(cid:1853)(cid:1864)(cid:1857)(cid:1730) (cid:3397) (cid:1839)(cid:1861)(cid:1866)(cid:3013)(cid:3032)(cid:3041)  
controllers. For both sets we obtain the saturation point by setting 
the size of all buffers to 8 flits. See the curves of “F8 1-port” and 
“F8 4-port”. Then we replace all buffers with very large ones 
(3200-flit) to imitate the infinite link capacity and observe how the 
network saturation point improves (the most right hand curves in 
each sets). As can be seen the performance improvement is 
significant. For normalizing the buffer lengths we have chosen 
MinLen=4 flits and MaxLen=320 flits. Maybe the buffer length of 
320 flits seems too costly at the first glance but after the 
assignment of buffer lengths (according to their average utilization) 
most of the links are assigned with short buffers. For each buffer 
length between 4 and 320 flits Table 1 shows how many links are 
assigned with that buffer length. The average buffer length after 
optimizing  between 4 and 320 is 5.74 for 1-port memory and 
14.67 for 4-port memory and the corresponding curves in Figure 6 
are “FA5.74(4~320) 1-port” and “FA14.67(4~320) 4-port” . As can 
be seen the buffer sizing results in a significant gain in the network 
saturation that is quite near to the saturation point of the network 
with 3200-flit buffers.  
In order to show the importance of buffer sizing we perform the 
fourth analysis for each set, in which the buffer length is identical 
for all buffers and equal to the average of buffer lengths after 
optimization between 4 and 320 (i.e. equal to 6 for 1-port memory 
in the curve of “F6 1-port”, and 15 for four-port memory in the 
curve of “F15 4-port”). The poor saturation points of the last 
experiment emphasize the importance of distributing the link 
capacities with regard to their buffer utilizations at equilibrium, 
rather than distributing the same total amount of capacity 
uniformly between all links (i.e. identical length for all buffers).  
As a last point please note that the choice of the NoC architecture 
and workload was just an example for the experiments of this 
paper. Our method is completely generic for arbitrary topology, 
deterministic routing algorithms, and target applications that their 
traffic characteristics are determined at design time. 
Where Utilization = the average buffer utilization of each link 
We believe that distributing the buffering budget according to the 
average buffer utilization is one of the most straightforward and 
efficient ways for link capacity allocation. Although more complex 
methods like the methods proposed in [14, 15] can be used for this 
purpose, our experimental results show how proper buffer 
allocating at design time can help to mitigate the hotspot impacts, 
even when a simple and intuitive method is used. 
Besides, one can argue that buffer sizing methods need to iterate as 
removing bottlenecks can just move them somewhere else, or even 
results to spontaneously remove some others according to the link 
dependencies. It is true but please remember that the use of huge 
FIFOs, as described in the first step of our algorithm, is exactly for 
this reason. In fact using theoretically infinite FIFOs (e.g. 3200 flits 
in our experiments) does not allow the congestion to be retropropagated to upstream buffers and consequently there is no any 
dependency between congested links. Our experiments also proved 
that doing iterations does not change the value of normalized 
FIFOs significantly and we will have almost the same results. 
Figure 6 shows the saturation-point gain achieved by our buffer 
sizing tool. Two sets of curves are demonstrated: the first four 
curves from left correspond to 1-port memory controller and the 
next four correspond to the case of four separate memory 
5.1 Cost-Performance Analysis  
Table 2 and Table 3 present rough cost-performance comparisons 
between different configurations with one and four memory 
controllers respectively. The cost of a configuration signifies the 
normalized average buffer length and the performance of a 
configuration signifies the normalized network saturation point in 
the configuration. The cost and performance of the configuration 
with identical 8-flit buffers everywhere are assumed to be 1, thus 
Figure 6. Performance enhancement due to buffer sizing with h=25% 
Number of links  
Buffer Length 
4 
5 
6 7 8 9 12 13 14 17 18 19 20 21 22 23 26 36 39 42 43 48 58 59 97 98 128 131 318 319 320
(4-port memory) 
(1-port memory) 328 
Number of links  
- 124 46 46 12 46 10 - 4 3 4 13 1 4 2 1 - 1 3 2 2 4 2 2 
- 
4 
- 
- 
- 
- 2 
- 
- 
- 
- 
- 
- 
- 1 2 - 
- 
- 
- 
- 
- 
- 1 1 
- 
- 
1 
- 
3 
- 
1 
- 
2 
- 
1 
1 
Table 1. Buffer length distribution 
  
 
the reference of the comparisons. The most considerable point of 
these comparisons is the last line of Table 2 that shows with a 
reduction in the cost (about 29%), if the buffer capacities are 
distributed according to the buffer utilizations, we can obtain a gain 
of 78% in performance. 
In order 
to show 
the cost-performance analysis of all 
configurations and the impact of our buffer sizing method for XYZ 
and ZXY routings or location of hotspots, we present a normalized 
comparison summary of all configurations in Table 4. The 
reference configuration is that of one hotspot, XYZ routing, and 
identical 8-flit buffer length. Considering the best case of the table, 
it shows that if we split the memory controller in four separate 
parts located at four corners of the upper die, and if we distribute a 
normalized buffer capacities between 4 and 320 (averagely 13.80) 
flits according to our method, we can improve the performance by 
a factor of"
Attackboard - a novel dependency-aware traffic generator for exploring NoC design space.,"Network-on-chip (NoC) is very important for many applications, such as many-core architectures and application-specific usages. For exploring the design space, several approaches have been proposed with different considerations. In this paper, inspired by bloom filters, we propose Attackboard, a novel design for exploring the design space of NoC, which satisfies accuracy, space efficiency, and simplicity. To justify the usage of Attackboard, a parallel object detection program is used as the benchmark program to evaluate the performance of a specific NoC. By comparing the results with an execution-based simulator, it shows that Attackboard simultaneously achieves the requirements of fast speed, simplicity, and accuracy.","Attackboard: A Novel Dependency-Aware Trafﬁc Generator
for Exploring NoC Design Space
Yoshi Shih-Chieh Huang, Yu-Chi Chang, Tsung-Chan Tsai,
Yuan-Ying Chang, and Chung-Ta King
Depar tment of Computer Science, National Tsing Hua University, Hsinchu, Taiwan
{yoshi, yuchi, tctsai, elmo, king}@cs.nthu.edu.tw
ABSTRACT
Network-on-chip (NoC) is very important for many applications, such as many-core architectures and applicationspeciﬁc usages. For exploring the design space, several approaches have been proposed with diﬀerent considerations.
In this paper, inspired by bloom ﬁlters, we propose Attackboard, a novel design for exploring the design space of NoC,
which satisﬁes accuracy, space eﬃciency, and simplicity. To
justify the usage of Attackboard, a parallel ob ject detection program is used as the benchmark program to evaluate the performance of a speciﬁc NoC. By comparing the
results with an execution-based simulator, it shows that Attackboard simultaneously achieves the requirements of fast
speed, simplicity, and accuracy.
Categories and Subject Descriptors
B.4 [Input/output and data communications]: Processors
General Terms
Performance, Design
Keywords
Network-on-chip, Many-core, Dependency, Traﬃc generator, Table-driven
1.
INTRODUCTION
Network-on-chip (NoC) has become the de facto of the
substrate of many-core architecture due to its simplicity and
scalability. Exploring the design space of NoC is therefore
increasingly important. To study a novel NoC architecture,
architects usually exercise it with realistic workloads and
measure quantitatively how close the design ob jective is approached. Consequently, a model of the interested architecture needs to be established and evaluated ﬁrst, even when
the detailed implementations are not available yet.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2012, June 3-7, 2012, San Francisco, California, USA.
Copyright 2012 ACM ACM 978-1-4503-1199-1/12/06 ...$10.00.
Early-stage models are essential for architects working on
novel architectures, in order to evaluate the proposed architecture even before the detailed RTL or circuit design is
available. Existing early stage models for NoC architectures
come in diﬀerent ﬂavors, and cover a wide range of accuracy,
simulation speed, and ﬂexibility. One of the most conﬁdent
solution to early stage models are full-system simulators,
which have been success in ﬂexibility as it takes moderate
eﬀorts to develop and conﬁgure at early design stage. For the
mature full-system simulators including NoC, a wide range
of topologies, routing algorithms, and router architectures
can be modeled by changing the simulation parameters, such
as Simics with Garnet, GEMS, M5, SESC, and etc [3, 10, 5,
8]. Full-system simulators have been successfully deployed
to study various architectures, due to their high ﬂexibility
and accessibility. However, these simulators have relatively
lower simulation performance, and does not scale with the
progress of modern many-core architectures.
In contrast to simulate the full system, one another way
to address the simulation complexity is through trace-driven
simulator, which takes the trace log generated in execution as input. A second simulation run, usually with cycleaccuracy, is driven by the previously generated trace. Based
on similar methodology, a number of trace-driven simulators
for NoCs have been proposed, such as BookSim [1] and [9].
Comparing to full-system simulation, trace-driven simulation is simple and fast. However, the trace-driven simulation
is open-loop, which does not consider the backpressure from
NoC to the processing elements. Therefore its accuracy is
low and may not be tolerable for all studies.
As a result, improving the accuracy of trace-driven simulation has been addressed in state-of-the-art research. In
[11], dependencies among injected packets are inferred and
embedded into the raw trace logs. Embedding dependencies
into traces absolutely brings more conﬁdent results of evaluation. However, once dependencies are embedded into original trace logs, two following problems arise. First, it makes
the trace logs much complicated and require more storage
space, ranging from megabytes to gigabytes according to the
granularity. Second, each entry in trace-log becomes correlated and therefore need to be processed while evaluating
systems with dependency-aware traces.
In this paper, we propose Attackboard, a new methodology to help explore the design space of network-on-chip.
Attackboard strikes a balance between speed and accuracy,
while keeping the simplicity as trace-driven simulation. The
essence of Attackboard is multiple bloom ﬁlters. By taking
advantages of the property of denial for sure, permitting for
high conﬁdence, Attackboard1 can rebuild the application
behavior with a simple pattern-oriented table.
Since each attackboard is a dependency-driven table rather
than a time-driven table, redundant dependency patterns
only take one line in an attackboard. This property helps
reduce the size of table since a program is usually intrinsically repeating program because of loops. By keeping the
causality-domain instead of time-domain, the execution can
be logged with the pattern-oriented method.
Our evaluations show the accuracy and space overhead of
Attackboard by comparing with an execution-based simulator. To identify the contribution of this paper, we listed
them as follows:
(cid:15) A novel design with multiple bloom ﬁlters for NoC
evaluation is proposed.
(cid:15) Experiments for comparing Attackboard with an executionbased simulator are evaluated.
(cid:15) Analysis for the practical implementation is discussed.
The rest of paper is organized as follows. Related works
are given in Section 5. Next, Section 2 provides a formal
deﬁnition of Attackboard. In Section 3, we ﬁrstly start with
the overview of Attackboard, and followed by the detailed
operations and design options.
In Section 4, we evaluate
Attackboard by comparing with execution-based simulation.
Finally, conclusion is drawn in Section 6.
2. PROBLEM FORMULATION
Assume that there are N routers in an NoC. To each
router k , it contains its own attackboard, denoted as abk ,
which is a two-dimension table, and in which each row stands
for a unique dependency pattern, and each column represents the necessity of must having data from this router, i.e.,
a necessary predecessor. For example, if there is a 1 in the
cell of row i and column j , it means that for satisfying pattern i, router j must have already injected data to router k .
In contrast, if the number in the cell is 0, it means that this
pattern is not necessary to be after the injection of router j
to k .
Each router k contains a Current Status (CSk ) with length
N in terms of bit. By using CSk as the index to match an
entry in abk , once a pattern is satisﬁed, the corresponding injection is generated. This problem can be modeled as
a Multiple Bloom Filters problem. Each row represents a
bloom ﬁlter, and CSk is an entry which is trying to pass one
or more bloom ﬁlters.
3. SYSTEM DESIGN
3.1 Overview
There are mainly two stages in Attackboard. In the ﬁrst
stage, as Figure 1(a) shows, the dependencies of the injections from each source node are ﬁrstly discovered, and then
represented with a table structure. The basic idea of this
stage is to periodical ly capture the relationships among injections. Once the stage one is done, the characteristics of
the benchmarks are now represented by the attackboards.
1 In the following context, Attackboard with initial capital
stands for the whole design, and attackboard with the lower
case stands for the table structure.
Figure 1: The system ﬂow of Attackboard. (a) Execution logs are broken to attackboards and distributed to each router. (b) A router generates trafﬁc by feeding its current status to its attackboard.
In the second stage, as shown in Figure 1(b), while evaluating an NoC conﬁguration, each router only needs to look
up its own attackboard to generate traﬃc. Similarly, the
basic idea of this stage is to periodical ly generate the traﬃc
according to the indications of attackboards.
3.2 Stage 1 - Creation of Attackboard
For each send event, we observe the received traﬃc for an
interval. Suppose that a send event n is observed at cycle
x, and an interval size I is selected as our observing window
size, i.e., any traﬃc occurs between cycle I (cid:0) x to x would
be granted as a dependent receive event to the send event
n. As the representation between these dependent events
and send event n, a corresponding row is inserted into the
attackboard. For each dependency event, the relationship
would be marked as 1 in the corresponding cell in the table,
indicating if the source core of a dependent receive event had
communication with the core of the observed send event. For
example, in the scenario shown in Figure 2, the receive event
n (cid:0) 1 and n (cid:0) 2 would be granted as the dependent events
to send event n. And then a dependency pattern could be
built as < 1; 0; 1 >, which implies that core 0 and core 2 have
communicated with core 3 before the send event n occurs.
The attackboard is able to semantically reveal the program behavior, thus serve as a good evaluation tool to exploit the performance of NoCs. As previous works [11, 7], it
is important to collect correct dependency information for
attackboard so as to discover the semantic meaning of programs. The selection of dependency-extraction interval size
I is thus critical. The size of I decides how many dependent
(or independent) events of a send event could be seen, and
therefore strongly correlates to the accuracy of the recorded
dependency information.
Take the function MPI_Allreduce as an example. MPI_Allreduce
performs a reduce operation after core 0 collects data from
all other cores and then broadcast the results back to all
cores. Since the broadcast can only happen after core 0
receives all data from other cores, the corresponding attackboard of this operation should be like < 0; 1; 1; 1 > with
send events P1 (4); P2 (4); P3 (4). This attackboard indicates
that only after receiving from core 1, 2, 3, can core 0 sends
Figure 2: Recognizing the dependent receives of
send n.
data to core 1, 2, 3, each with data quantity 4, which is
exactly how MPI_Allreduce works. By using this small and
scalable attackboard, the semantic meaning of program behaviors can be well presented. In Section 4.1, we have an
experiment to discuss the choices of dependency extracting
interval.
3.3 Stage 2 - Usage of Attackboard
3.3.1 Initialization
The dependency status of each router k is initially set
as all 0s. Note that there is always an attackboard with
dependency pattern as all 0s, since the very ﬁrst send event
of the program depends on no other send events but the
startup of the execution. Therefore, the attackboard need
nothing to be propelled but the startup of the simulation.
After the ﬁrst send event generated by the attackboard, the
simulation would keep generating dependency-aware traﬃc
to cause the following injections as a chain reaction.
3.3.2 Entry Matching
As Figure 3 shows, each router k keeps its Current Status
(CSk ) which is caused by others. CSk is a bitwise vector with
length jN j (cid:0) 1, where N is the number of processors. For a
bit in CSk at column y , 0 means that currently processor k
has not received any packet from processor y . In contrast, 1
represents that processor k has received data from processor
y .
In the end of each interval I
, CSk is used as an index
to match its own attackboardk (abk ) to ﬁnd the matches.
As the matches are found, the recorded send events of this
dependency pattern entry will be generated. Note that the
generated traﬃc of an entry will be evenly distributed to the
incoming interval for avoiding all the send events injecting
at the starting point of the next interval. This is for simplifying the simulation without recording the computation
time before an injection.
′
Figure 3: A sample of an attackboard and the procedure of entry matching and traﬃc generating
3.4.1 Merging duplicated entries
During our gathering the dependency information, those
send events with the same dependency pattern would be
compressed into the same dependency pattern category. Upcoming send events would be appended to the end of the dependency pattern category, except for those send events with
same quantity and destination. The send events of same
quantity and destination would be granted as the repetition
of one send events, and thus only one send event would be
recorded. However, the payloads of two or more send events
may be diﬀerent. Denote a set S as set of the send events
and denote the corresponding payloads as payloadi ; i 2 S .
which have the same predecessors and injecting destination,
We proposed three solutions: First, by averaging the payloads and only record the averaged value as the constant
payload. That is,
. Second, record the payloads
as a sequence, i.e., in the cell of destination, keep the sequence as Pi (payload1 ; payload2 ; :::; payload|S | ). While generating traﬃc, the sequence acts as a circular queue. Third,
based on the second solution, instead of using the roundrobin strategy to select, diﬀerent payloads are tagged with
probability values according to the number of occurrences,
and the selection is based on the probability.
i2S payloadi
∑
|S |
3.4.2 Merging similar entries
For further minimizing each attackboard, similar entries
can be merged into one. For identifying those entries which
have similar patterns, eXclusive OR (XOR) operation can
calculate the Hamming distance of two entries. For those
entries which have closest Hamming distance are considered
to be merged ﬁrst. Once two entries are considered as high
and replaced by a new entry Ek = Ei (cid:1) Ej . For example, if
similarity, denoted as Ei and Ej , the two entries are removed
Ei =< 1; 0; 1; 1 > and Ej =< 1; 1; 0; 1 >, then it can be
replaced with Ek =< 1; 0; 0; 1 >. This operation relaxes the
condition of the necessity of predecessors, but the new entry
still keeps the coexisting predecessors.
4. EVALUATION
3.4 Table Minimization
Since the design of Attackboard are based on bitwise bloom
ﬁlters, the number of entries are proportional to the number
of processors, i.e., 2(N −1) , where N is the number of processors. For future many-core architecture, the number of
processors is expected to be thousands of cores in a chip.
Considering such a situation, we propose two methods to
reduce the size of attackboards.
The evaluation of NoC architectures usually involves performance of diﬀerent NoCs during the executions of real programs. If the average network delay of NoC A is more than
that of NoC B , we assert that the NoC B is more suitable
than NoC A for target programs. In the context of this evaluation standard, we would evaluate how close the average
network delay is by comparing the results of attackboard
with real benchmark executions on an execution-based simulator, since it is a representative of the semantic meaning
Table 1: Default simulation setup
Simulation Platform
Native processor element
Tilera TILE64 [4]
Native processor frequency
700 MHZ
4 (cid:2) 4 mesh network
Simulated topology
Routing algorithm
Dimension-order
Bandwidth
1 ﬂit/cycle per port
of real programs. The accuracy is considered higher if the
simulation results of attackboard is closer to the results of
real benchmark executions. Besides accuracy, the size of attackboard is also evaluated compared with communication
traces to meet the needs of easy distribution.
In the following paragraph, we evaluate Attackboard with
execution-based simulation in terms of accuracy, size, and
how representative the attackboard is for semantic meaning
of real programs. The parameters of the simulated environment are shown in Table 1. The utilization of Attackboard
involves two stages. At the ﬁrst stage, we run real benchmarks to generate Attackboard by retrieving dependency
information, and examine if attackboard could successfully
portray the semantic meaning of the behaviors of real programs. Afterwards, at the second stage, we would run the
simulation to generate dependency-aware traﬃc with Attackboard to examine the accuracy. Finally, we would compare the size of attackboard with trace.
4.1 Dependencies Extracting
We use an execution-based simulator to execute the instrumented programs and capture the dependencies for a
micro-benchmark from Intel MPI Benchmark (IMB) suites [2]
and a parallel ob ject detection program. The instrumentations are done by hand and therefore guaranteeing the truedependencies. Other strategies to automatically capture the
dependencies are discussed in recent works [7, 11, 13].
Note that the dependent events could be repetitive among
send events. That is, if an upcoming send event n + 1 occurs
right after send event n, the observing window of these two
send events might overlap, which results in the same dependency pattern, the reason is that these two send events are
just dependent on the same receive events simultaneously
(such as the MPI group communication as MPI_allgather,
MPI_alltoall, etc.) in real benchmarks. Furthermore, as a
dependency pattern driven traﬃc generation, Attackboard
would generate two send events once the receiving dependency pattern is satisﬁed, which is compatible with the semantic meaning of programs.
Figure 4 shows the derived attackboard of odd-even sort
and ob ject detection. In the odd-even sort part, the attackboards of core number 0, 1, and 15 are shown. The ﬁrst
entry of attackboards shows that core 0 would send data
(with quantity 4) to core 1 without depending on any send
events, and the core 1 would send data to core 2 in the same
situation. Meanwhile, the core 15 would only generate data
to core 14 after receiving data from core 14. This shows
the odd phase of the odd-even sort, while the even phase is
shown in the second dependency entry of core1, and the ﬁrst
entry of core 15. In the even phase, the core 1 would pass
data back to core 0, which describes the ”swap” operation in
the odd-even sort. And the second entry of core 0 conveys
that if the even phase ends, there should be another upcomFigure 4: Attackboards of odd-even sort and ob ject
detection.
ing odd phase. The attackboards of the unmentioned cores
also obey this communication behavior as discussed.
In the other hand, the ob ject detection part shows that,
initially core 0 would pass data to core 1, while core 1 passes
data to core 2. And after the receiving data from core 0, core
1 would send (handled) data back to core 0 and also pass
data to core 2 and core 3. Meanwhile, the core 15 would
send data to core 7 after receiving data from core 14. This
behavior indicates group communication. The ob ject detection algorithm here uses data parallelism to detect if ob ject
exists in a frame. To achieve data parallelism, some data
must be exchanged between cores to learn acknowledge of
other part of data. The attackboard also presents the behavior of data parallelism, which is compatible of the semantic
meaning of the programs of ob ject detection.
To conclude, it is feasible to use the derived attackboards
to reveal the semantic meaning of the corresponding programs.
4.2 Trafﬁc Generating
As discussed in section 4.1, the retrieval of Attackboard
involves the dependency extraction. The dependency extraction methodology has been proposed, with an interval
size I introduced. However, it may be questionable that if
a ﬁxed dependency-extraction interval size could really embrace all essential dependency information for diﬀerent send
events occurred in execution. Thus, during the generation of
dependency-aware traﬃc, instead of matching the recorded
dependency patterns of attackboard, we grant these dependency patterns as bloom ﬁlters. By using the characteristic
of bloom ﬁlter – denial for sure, permitting for high conﬁdence – we could generate the traﬃc only with essential
dependent send events, and still provide chances for those
unrecorded send events (which is also likely to be a predecessor, but somehow lost during creation of attackboard).
With the Attackboard composed of bloom ﬁlters, we could
then generate conﬁdent dependency-aware traﬃc. The trafﬁc is generated periodically. Every time the setup simulation
interval I
expires, an attackboard would be thoroughly examined. During the examination, we compare the current
status and the dependency entries of attackboard. Only the
1s in the dependency pattern entries matter. That is, if
we encounter a 0 in the dependency patterns, we simply
treat it as a ”don’t care”. For example, with current status
< 0; 1; 1 > comparing a dependency pattern < 0; 1; 0 >, the
generation of traﬃc is permitted. Contrarily, if a dependency pattern < 1; 1; 1 > is encountered, the traﬃc genera′
Figure 5: The space overhead of attackboards compared to trace ﬁles.
Figure 6: The accuracy of ob ject detection under
diﬀerent I and I
.
′
tion is not permitted.
Once a suitable dependency pattern is found, corresponding traﬃc would be generated.
If no suitable dependency
pattern is found, then no traﬃc would be generated. Since
the generated traﬃc would obey the form of dependency
information revealed from real benchmarks, the generated
traﬃc behaves like the real benchmarks.
As described, the attackboard would periodically generate
dependency-aware traﬃc with simulation interval I
. Since
the traﬃc injection rate is a critical factor to evaluate NoCs,
the selection of I
(i.e., how often Attackboard generates
traﬃc) plays an important role of attackbard simulation. An
appropriate I
could attackboard behave as real programs.
The selection of I
heavily depends on the NoC architectures, so a tuned I
for a simulating NoC architecture is
always needed in the attackboard simulation. In the following experiments, we would show that with the easily tuned
I
, the attackboard could be a representative of real benchmark. After the tuned I
is found, the average network delay
could then be generated with attackboard simulations.
′
′
′
′
′
′
′
4.3 Space Overhead
As shown in Figure 5, the space requirement of Attackboard is quite small. In the size comparison for ob ject detection, the size of attackboard is comparatively small than the
size of communication traces. Note that the improvement is
not big because of the communication events are relatively
lesser and execution time is shorter compared to other real
programs. A great improvement can be observed in the size
comparison for IMB, it is because the long execution time
and intensive communications, thus the size of communication traces is very large, and it would grow even larger if the
execution time is set longer. Compared with the large size
of IMB traces, attackboard is much smaller. Attackboard is
scalable, since Attackboard is a timeless table, so the size of
Attackboard would not grow linearly with time, but depends
on how many distinct dependency patterns exist in the programs. Further, the dependency pattern that Attackboard
records are in essence the semantic meaning of program, the
repetition of behaviors would be folded naturally with Attackboard, which makes Attackboard more scalable to use.
4.4 Case Study
The accuracy of Attackboard is evaluated by the comparison of average network delay. We compared Attackboard
with the average network delay of the execution of parallel
ob ject detection and IMB Broadcast. The accuracy is presented as the normalized average network delay with that of
execution of real benchmarks. Also, multiple I and I
are
tested to derive the best result.
Figure 6 and Figure 7 show the statistics of normalized
average network delay Attackboard achieved with diﬀerent
I and I
(in terms of cycle). The x-axis represents simulation interval length I
and the y-axis represents the normalized results. Diﬀerent curves in the ﬁgure represent an
attackboard with diﬀerent simulation interval length I . In
the following paragraphs, we discuss diﬀerent cases, respectively.
′
′
′
4.4.1 Parallel Object Detection
′
As we can see in Figure 6, there are two groups of the
curves. The ﬁrst group is the curve with I set as 100. The
curve is initially very high, and falls down slowly. After
I
is set larger than 1600, it would arrive a steady value
while still remain a comparatively high value. The reason
could be found obviously in the content of the attackboards.
There is only one dependency entry – all zeros – of the attackboard with I set as 100. This shows that the length
of dependency-extraction interval is too small, so that the
attackboard could not capture any useful dependency information for send events. Therefore, during the simulation
of Attackboard, the send events would inject every interval,
which causes severe congestion.
The second group is the curve with I set as 100, 1000,
5000, and 10000. The curves of second group would startup
high and then ﬂatten out afterwards.
It is because when
the length of I
is too small, congestion would occur since
the injection rate is too high. In contrast, after I
is fairly
set, the injection would then balance with routers’ receiving,
thus generate a steady average network delay value, which
matches the result of the real program. In this group, the
results almost ﬁt the ideal case.
′
′
4.4.2
Intel IMB Broadcast
Diﬀerent from the simulation behaviors of ob ject detection, the simulation results of attackboard of IMB broadcast is very centralized. As Figure 7 shows, the accuracy
(normalized average network delay) of the simulation results
ranges from 0.8 to 1.1, showing the high feasibility of the
6. CONCLUSION AND FUTURE WORKS
In this paper, we propose Attackboard, a new methodology to help explore the design space of network-on-chip.
Attackboard takes advantages of repetitive behaviors and
dependencies among injections. We use IMB and a parallel ob ject detection program to evaluate the performance of
Attackboard. The results show that Attackboard has high
accuracy as programs directly run on the execution-based
simulator. On the other hand, we also compare Attackboard
with trace-driven simulation in terms of space requirements.
The results show that the space overhead is much smaller
while keeping the accuracy.
The future works are twofold. First, the intervals for extracting and generating rely on empirical rule to ﬁnd. We
are developing an automatic process to ﬁnd the suitable intervals. Second, Attackboard currently is evaluated with
message passing programs. In our ongoing work, the traﬃc
of shared-memory programs will be included.
7. "
Towards graceful aging degradation in NoCs through an adaptive routing algorithm.,"Continuous technology scaling has made aging mechanisms such as Negative Bias Temperature Instability (NBTI) and electromigration primary concerns in Network-on-Chip (NoC) designs. In this paper, we model the effects of these aging mechanisms on NoC components such as routers and links using a novel reliability metric called Traffic Threshold per Epoch (TTpE). We observe a critical need of a robust aging-aware routing algorithm that not only reduces power-performance overheads caused due to aging degradation but also minimizes the stress experienced by heavily utilized routers and links. To solve this problem, we propose an aging-aware adaptive routing algorithm and a router microarchitecture that routes the packets along the paths which are both least congested and experience minimum aging stress. After an extensive experimental analysis using real workloads, we observe a 13%, 12.7% average overhead reduction in network latency and Energy-Delay-Product-Per-Flit (EDPPF) and a 10.4% improvement in performance using our aging-aware routing algorithm.","Towards Graceful Aging Degradation in NoCs Through an
Adaptive Routing Algorithm
Kshitij Bhardwaj
Koushik Chakrabor ty
Sanghamitra Roy
USU BRIDGE LAB
Electrical and Computer Engineering, Utah State University
kshitij.bhardwaj@aggiemail.usu.edu,{sanghamitra.roy, koushik.chakrabor ty}@usu.edu
ABSTRACT
Continuous technology scaling has made aging mechanisms such
as Negative Bias Temperature Instability (NBTI) and electromigration primary concerns in Network-on-Chip (NoC) designs. In
this paper, we model the effects of these aging mechanisms on
NoC components such as routers and links using a novel reliability metric called Trafﬁc Threshold per Epoch (TTpE). We observe
a critical need of a robust aging-aware routing algorithm that not
only reduces power-performance overheads caused due to aging
degradation but also minimizes the stress experienced by heavily
utilized routers and links. To solve this problem, we propose an
aging-aware adaptive routing algorithm and a router microarchitecture that routes the packets along the paths which are both least
congested and experience minimum aging stress. After an extensive experimental analysis using real workloads, we observe a 13%,
12.7% average overhead reduction in network latency and EnergyDelay-Product-Per-Flit (EDPPF) and a 10.4% improvement in performance using our aging-aware routing algorithm.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Design Studies, Fault Tolerance
General Terms
Reliability, Algorithms, Design
Keywords
NoC, Aging, NBTI, Electromigration, Routing algorithms
1.
INTRODUCTION
With the proliferation of on-chip cores allowed through rapid
technology scaling, Network-on-Chips (NoCs) are becoming a critical determinant of overall system power-performance characteristics. Consequently, the growing reliability challenges, which are
continuously reshaping the system design considerations, must now
be thoroughly analyzed in the context of NoC designs [14]. In this
work, we study two primary mechanisms responsible for circuit
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2012, June 3–7, San Francisco, California USA
Copyright 2012 ACM 978-1-4503-1199-1/12/06 ...$10.00.
wear-outs in an NoC design: Negative Bias Temperature Instability
(NBTI) and Electromigration.
An NoC architecture comprises two major components: NoC
router and link. A pipelined NoC router consists of both combinational logic structures (e.g. virtual channel allocation logic) and
storage-cell structures (e.g. virtual channels). Due to the presence
of these structures, NBTI is the major aging mechanism associated
with NoC routers [6]. NoC links, on the other hand, are implemented using repeated copper interconnects [8]. Therefore, NBTI
(repeaters) and electromigration (copper interconnects) are the two
primary aging problems associated with NoC links. Unfortunately,
previous works on NoCs have completely ignored the role of links
in their reliability analysis, focusing solely on the routers [10, 6].
We demonstrate that such limitations can grossly underestimate the
NoC lifetime by nearly a factor of 2.
In the context of reliability in NoCs, another critical design challenge stems from the asymmetric usage of NoC components. Mishra
et al. have shown this non-uniform pattern of router buffer and link
utilization [13]. They observed that the routers in the center of
the mesh are highly (75%) utilized, while the peripheral routers
have low (35%) utilization. Similarly, our experiments with multithreaded workloads on a 4 × 4 mesh indicates a wide disparity in
buffer utilization of different routers. Due to such asymmetric utilization, each router and link will also suffer from different amounts
of aging degradation. Therefore, there is a need for an aging-aware
routing algorithm for NoCs that considers the aspect of asymmetric
aging while routing packets, so as to improve the system reliability.
At the system level, solely improving reliability may hurt the
power-performance. For example, to alleviate the aging degradation on a heavily utilized path, it may be necessary to use an
alternate route. However, employing such an alternate route can
increase the network latency, thereby degrading the system level
power-performance. Thus, efﬁcient ways to improve the system
robustness requires design space exploration techniques that simultaneously optimize multiple objectives. Such an optimization problem must effectively model several NoC design aspects in the context of the overall system: (a) routing topology, (b) network trafﬁc
during the execution of real programs, (c) device level models capturing the effect of NBTI and electromigration, and (d) latency and
energy consumption of the routing policies. Ad-hoc analysis and
optimization of these complex objectives can lead to sub-optimal
solutions, with limited insight for future improvements.
To effectively model multiple device level aging characteristics,
system level asymmetric usage patterns and runtime trafﬁc variations, we introduce a speciﬁc reliability metric for NoC components: Trafﬁc Threshold per Epoch (TTpE) (Section 3). TTpE is
deﬁned as the amount of trafﬁc that a stressed link or router should
accept in a particular epoch1 during the runtime. The purpose of
this metric is two folds: (a) allow formal analysis of reliability
impact on an NoC, and (b) a means to dynamically analyze the
trafﬁc patterns and adapt the routing algorithms. Subsequently, we
present an adaptive aging-aware routing algorithm that a) reduces
the aging-induced power and performance overheads by routing
through paths that experience least aging effects and congestion,
and (b) minimizes the stress experienced by heavily utilized routers
and links by constraining them to meet their respective TTpEs for
different epochs throughout the total running time.
We make the following contributions in this paper:
• We show the effects of NBTI and electromigration on NoC links
and its signiﬁcance in reliability analysis and fault tolerance of
NoCs (Section 2).
• We formulate a comprehensive system-level aging model for NoC
routers and links. This model considers the effects of asymmetric
aging in routers and links during the program execution. Subsequently we show the impact of NBTI and electromigration on the
performance of an NoC-based multicore system (Section S2).
Our work integrates SPICE level process variation and aging
analysis, circuit-level statistical timing analysis, and full system
architectural simulation (Section 5).
• We propose an aging-aware adaptive routing algorithm and router
micro-architecture that not only mitigates the impact of aging on
the NoC’s power-performance characteristics but also minimizes
the stress experienced by heavily utilized routers and links. This
way, the algorithm is able to provide robustness at the systemlevel design of NoCs (Section 4).
• An extensive experimental analysis using the GARNET NoC
simulator [3] and real workloads (PARSEC benchmarks [2]) indicates an average of 13% and 12.17% reduction in the network
latency and Energy-Delay-Product-Per-Flit (EDPPF) [11] in a
typical NoC undergoing aging stress. We also obtain an average improvement of 10.4% in performance using our proposed
algorithm (Section 6).
2. MOTIVATION
In this section, we present a brief robustness analysis of an NoC
that not only considers impact of aging on routers but also considers
the effects of aging mechanisms such as NBTI and electromigration
on links.
To show the importance of considering the aging effects on links,
we perform an experiment using an NoC architecture that comprises two routers connected by two unidirectional links. We use
the ﬂexible numerical model of NBTI degradation based on reactiondiffusion [5] and wire resistance based electromigration stress model
in our calculations [15]. These models are discussed in more details in Section S2.2. We analyze the network latency under the
following schemes:
Table 1: Different Degradation Schemes
Scheme
A
B
C
D
Degradation in Routers
NBTI
NBTI
NBTI
NBTI
Degradation in Links
NONE
NBTI
Electromigration
NBTI and Electromigration
The above schemes are evaluated under tornado synthetic trafﬁc
pattern for three different injection rates: a) low (0.1 ﬂits/cycle), b)
medium (0.3 ﬂits/cycle), c) high (0.5 ﬂits/cycle).
9 1 To capture runtime trafﬁc variations, total time taken to route
the ﬂits is divided into equal intervals of time called epochs.
A
B
C
D
y
c
n
e
t
a
L
k
r
o
w
t
e
N
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0
1
2
3
4
Year
5
6
7
8
Figure 1: Latency variation with time (low injection rate).
A
B
C
D
y
c
n
e
t
a
L
k
r
o
w
t
e
N
5
4
3
2
1
0
0
1
2
3
4
Year
5
6
7
8
Figure 2: Latency variation with time (medium injection rate).
2.1 Analyzing Network Latency
Figures 1, 2 and 3 show the variation of network latency with
time for different injection rates. As is evident, D estimates the
highest network latency. The consideration of both NBTI and electromigration in scheme D degrades the link more, thereby causing this substantial increase. Network latency is least affected in
scheme A as there is no link degradation and the small increase is
only due to NBTI degradation in routers.
2.2 Effect on Fault-Tolerance of NoC
We assume that a network becomes faulty when the increase
in network latency exceeds a pre-deﬁned threshold (20% in our
study). Figure 4 shows the time taken for the network to become
faulty in the case of high injection rate trafﬁc. For example, under
scheme D, a network can be rendered faulty in almost three years.
However, using scheme A grossly over-estimates the time to failure
(almost six years). In reality, due to the combined effects of NBTI
and electromigration, the copper wires are likely to degrade beyond
the threshold by that time.
3. TTPE: NEW RELIABILITY METRIC FOR RUNTIME ADAPTATIONS IN NOCS
In an NoC, every router and link is utilized in different amounts,
some more than others. These variations stem from two key factors:
(a) asymmetric communication patterns between NoC nodes; (b)
runtime trafﬁc variation due to policies of the routing algorithm.
Section S1 illustrates this property through our rigorous analysis
with the GARNET NoC simulator.
Based on the above observations, we derive the system-level aging model to ﬁnd the relationship between router/link utilization
and the amount of stress experienced from NBTI/electromigration
degradation. For this purpose, we introduce a novel metric called
 
 
A
B
C
D
y
c
n
e
t
a
L
k
r
o
w
t
e
N
9
8
7
6
5
4
3
2
1
0
0
1
2
3
4
Year
5
6
7
8
Figure 3: Latency variation with time (high injection rate).
Threshold
A
B
C
D
e
s
a
e
r
c
n
I
y
c
n
e
t
a
L
3
2.5
2
1.5
1
0.5
0
0
1
2
3
4
Year
5
6
7
8
Figure 4: Time taken for the network to become faulty under
various aging models (high injection rate).
Trafﬁc Threshold per Epoch (TTpE), deﬁned as the fraction of the
nominal trafﬁc2 that a stressed router/link should accept during a
particular epoch. The signiﬁcance of using TTpE as a reliability
metric for an aging-stressed NoC design lies in the following facts:
• It determines an upper limit on the amount of trafﬁc that a router
or link should accept so as to keep the variation in network latency below a pre-deﬁned threshold for a particular aging period
(7 years in this work). If the limit imposed by TTpE is exceeded
in a router undergoing maximum degradation, it will be rendered
faulty.
• TTpEs are derived from continuous monitoring of the trafﬁc, and
are used to adapt the routing policies for every epoch to mitigate
the long-term degradation in the NoC.
The TTpEs vary over the runtime with different values during
different epochs for each stressed router and link. In order to calculate these thresholds, we ﬁrst proﬁle a congestion-aware routing algorithm to estimate the router/link utilization for every epoch
during runtime. Depending on the utilization, TTpEs are calculated for the stressed routers and links for every epoch using our
system-level aging model. TTpEs are then stored in each router in
the form of lookup tables so that the router can select the appropriate threshold depending on the epoch during runtime. These steps
are discussed in more details next.
3.1 TTpE calculation
The calculation of TTpE involves the following stages:
• Threshold calculation: This stage mainly involves proﬁling of
a state-of-the-art congestion-aware routing algorithm [7] to cal9 2Nominal trafﬁc is the trafﬁc across routers/links when they are
unstressed.
culate the TTpE of different stressed links and routers.
• Using TTpE Estimation in Routing: During this stage, we
build the trafﬁc threshold tables, which are then stored inside
each router.
We now discuss these stages in more detail.
3.1.1 Threshold Calculation
This stage can be further divided into two steps:
• Proﬁling: We ﬁrst proﬁle the congestion-aware routing algorithm that routes the ﬂits based on both local and global congestion information. The total time taken to route these ﬂits is then
divided into several epochs. The signiﬁcance of adding epochs
lies in the fact that an application’s communication characteristics may change during the runtime and therefore the trafﬁc must
be monitored continuously. This way we can keep track of the
link and the router utilization during runtime and take additional
measures if the utilization reaches TTpE for the epoch under consideration.
• TTpE Calculation: For each epoch, we ﬁnd the n most stressed
links and routers based on their utilization. The TTpEs for these
routers and links are calculated as follows:
1. Router TTpE: We use the aging model developed in Section S2.1 to calculate the TTpE for routers. This aging
model considers only NBTI degradation in routers.
2. Link TTpE: To calculate the TTpEs for links, we use the
NBTI and electromigration degradation model as described
in Section S2.2. Also we know that the trafﬁc across a
router can be controlled only by controlling the trafﬁc across
the links input to the router. Therefore, we transfer the
stress experienced by the router to the input links and calculate their TTpEs for the given epoch. For example, if a
stressed router R has four input links (l0...l3) then the utilization of the router is given by:
util(R) = util(l0) + util(l1) + util(l2) + util(l3) (1)
where util() estimates the utilization of a router or a link.
Using the above equation, we can calculate the amount of
trafﬁc each link should accept (or TTpE of each link) for
the router to meet its threshold. Therefore, there are two
kinds of stressed links: i) links that are directly under aging
stress, and ii) input links that experience stress because of
the stressed router.
3.1.2 Using TTpE Estimation in Routing
Here, we store the computed TTpEs for different epochs in the
form of lookup tables (SLset ) in each router. The router at runtime
can then select the appropriate TTpE depending on the epoch. During this stage, we also compute the routing tables for each router.
In order to minimize network latency and communication energy,
we select only the deadlock-free shortest paths for each ﬂow.
4. AGING-AWARE ADAPTIVE ROUTING
In this section, we present a robust aging-aware routing algorithm that not only reduces the stress experienced by heavily utilized NoC components but also minimizes the overall aging induced power-performance overheads. The algorithm involves the
following two stages (Table 2):
1. Congestion and aging-aware routing: For each ﬂow at runtime, the routing algorithm selects the best shortest path from
the routing table that i) suffers from least aging degradation
 
 
Table 2: Algorithm for Aging-aware adaptive routing
ALGORITHM Aging_Adaptive:
For each ﬂow,
1. Select the best shortest path from the routing
table which:
a) suffers from least delay variation due to
aging (scage is minimum).
b) is least congested based on global and local
congestion information (sccong is minimum).
2. For each stressed link in SLset of each epoch:
a) Check if the link meets its TTpE:
- If the link has already reached
its TTpE, keep the link idle for the
rest of the epoch (insert recovery cycles).
- If link utilization is safely below its
TTpE then there is no need for
inserting recovery cycles.
i.e. the path that suffers from least delay variation due to aging (1-a); and ii) is least congested (1-b). We give a higher
priority to a path that is least degraded as compared to a path
with the least congestion. For example, in case of an arbitrary ﬂow F , if the available number of deadlock-free shortest paths is four (path0....path3) then the algorithm maintains congestion and aging scores (sccong and scage ) for each
of these paths. These scores are calculated based on both
local and global aging and congestion information obtained
from different routers and links present in the paths. Now
if the scage is least for path0 but sccong is least for path3
then the algorithm selects path0 for routing. In a different
situation, if scage is same for all paths but sccong is least for
path3 then path3 is only selected for routing.
2. Honoring TTpE by employing recovery cycles: During
the execution of the routing algorithm, each stressed link in
SLset is checked to see if it meets its respective TTpE for
every epoch (2-a). There can be two possible cases: i) In
the epoch, if the link has already reached its TTpE then the
link must be kept idle for the rest of the epoch so that its utilization does not exceed its TTpE; and ii) If the link operates
safely inside its TTpE for that epoch, then there is no need
for inserting idle cycles. The physical signiﬁcance of inserting these idle cycles is that they provide additional time to
the links and routers to recover from the aging stress. Therefore, we call these additional idle cycles as recovery cycles.
This procedure also avoids unnecessary insertion of recovery cycles in the epoch and thus keeps the network latency in
check.
4.1 Aging-aware adaptive router microarchitecture
In order to implement the proposed routing algorithm, we extend
a congestion-aware router such that it computes the best route that
is both aging and congestion-aware. Moreover, this aging-aware
router must also ensure that the best allocated output link is operating under its respective TTpE. Figure 5 shows a detailed microarchitecture of the aging-aware adaptive router. Based on the functionality of each module of the router, we can divide the routing
unit of the aging-aware adaptive router into two different stages:
• Route Computation: This stage involves selection of the output
link towards the least stressed and least congested shortest path.
During this stage both the global and local congestion information from different routers is aggregated to calculate the congestion score (sccong ) of the paths. Similarly, based on the number
of stressed links in SLset for each epoch, an aging score (scage )
is calculated for each path. The Route Computation unit then selects the output link corresponding to the shortest path which has
the least scage and sccong . Note that the additional logic-based
circuitry is introduced in parallel paths rather than in sequence,
for example scage and sccong are calculated in parallel.
• Recovery Cycles: During this stage of the routing unit, utilization of the selected output link corresponding to the current
epoch is evaluated. If the link is stressed, its utilization is compared with the threshold for the epoch (TTpE), stored inside the
lookup tables (SLset ). In case the link has reached its TTpE then
this stage inserts recovery cycles for this link during the given
epoch. If the link has not reached its TTpE, then there is no need
for the recovery cycles. Also, an unstressed link can skip this
stage of the routing unit.
5. EXPERIMENTAL METHODOLOGY
Our experimental setup combines SPICE level analysis for process variation and NBTI aging, statistical timing analysis using
synthesized Verilog for NoC routers, and full system architectural
simulation. The effect of process variation and NBTI aging in basic
logic gates are performed through Synopsys HSPICE, using Predictive Technology Models (PTM), and long term degradation due to
NBTI [4]. On each of these gates, we run 10K Monte Carlo simulations to obtain respective statistical distributions of their performance characteristics. Using these gates at the 45nm technology,
we synthesize the NoC router RTL obtained from Stanford University’s open-source NoC router resources [1]. Subsequently, we
perform a statistical timing analysis to ﬁnd various critical paths in
the router, and their delay distributions under the combined effect
of process variation and NBTI aging.
For architectural simulation, we use the GARNET NoC simulator, embedded inside GEMS [12]. GARNET uses the ORION
power model [9] to calculate power consumptions of the routers
and the links. In our experiment, we consider a system with 16
processors in a 4 × 4 mesh topology. Each processor has a dual
issue 32 entry out-of-order issue window and a private L1 cache
(2-way, 32 KB, response latency: 3 cycles) and a shared L2 cache
(4-way, 2 MB, response latency: 15 cycles). For trafﬁc generation and system-level analysis, we use PARSEC benchmarks with
16 threads pinned to cores: Canneal (can), Dedup (ded), Facesim
(fac), Ferret (fer), Fluidanimate (ﬂu), Freqmine (fre) and Raystone
(ray).
6. EXPERIMENTAL RESULTS
To study the power-performance impact of aging on NoC designs, we conduct a set of experiments on a 4 × 4 NoC mesh shown
in Figure 10. Different schemes implemented for comparison are
discussed next.
6.1 Comparative Schemes
We use three different schemes to show the importance of a robust aging-aware adaptive routing algorithm:
• RCA-1D: In this scheme, we use a state-of-the-art congestionaware routing algorithm (Section S4) to route the ﬂits in an NoC
system comprising aging-stressed routers and links. Without aging awareness, this scheme continues to use heavily degraded
links/routers, thereby incurring power-performance overhead. Delay degradations in the stressed routers and links are modeled
according to Section S2.
Figure 5: Aging-aware adaptive router micro-architecture: In addition to the global congestion information in the form of sccong ,
the router also uses the aging information given by scage to select the best output route. The additional logic circuitry for this
route computation is implemented in parallel, e.g. scage and sccong are computed simultaneously.
• AGE-ADAP: Here we implement our proposed congestion and
aging-aware adaptive routing algorithm to route ﬂits in a stressed
NoC design. This scheme employs the step 1 of our proposed
routing algorithm (Section 4) but does not honor the TTpE limits
and therefore does not insert any recovery cycles in the epochs
during runtime. Delay variation in the routers and links are based
on the model in Section S2.
• AGE-ADAP-REC: This scheme extends AGE-ADAP such that
it inserts recovery cycles for stressed links/routers during an epoch
if the utilization has reached TTpE. Therefore, this scheme ensures that none of the stressed routers/links operates beyond their
calculated TTpE.
6.2 Robustness Evaluation
In order to show the robustness degradation in a 4 × 4 NoC mesh
that does not consider aging aware-routing (RCA-1D), we compare the reliability of this scheme with the AGE-ADAP and AGEADAP-REC schemes that model aging-awareness.
Figure 6 shows the reliabilities of all three schemes for an aging period of 7 years, calculated using the reliability’s dependence
on failure rate and TTpE (Section S3). As expected, RCA-1D
shows substantially higher failure rate compared to AGE-ADAP
and AGE-ADAP-REC, as its design does not adapt to the wear-out
degradation of NoC components. Also the trafﬁc utilization per
epoch of stressed routers and links is always above the TTpE for
RCA-1D which further reduces its reliability. As stressed routers
and links are well below their TTpE limits in AGE-ADAP-REC, its
reliability is even better than AGE-ADAP.
6.3 Overhead Analysis
6.3.1 Network Latency
Figure 7 shows the network latency for various schemes normalized to the RCA-1D scheme. As RCA-1D does not employ
any aging-awareness in its congestion-aware routing, it only selects
those paths which are least congested. However, frequent heavy
utilization in these paths results in large aging stress, which manAGE−ADAP−REC
AGE−ADAP
RCA−1D
y
t
i
l
i
b
a
i
l
e
R
h
s
e
M
1.2
1
0.8
0.6
0.4
0.2
0
0
1
2
3
4
Year
5
6
7
8
Figure 6: NoC Robustness Degradation Over Time.
RCA−1D
AGE−ADAP
AGE−ADAP−REC
y
c
t
n
e
a
L
d
e
z
i
l
a
m
r
o
N
1.1
1
0.9
0.8
0.7
0.6
can ded fac
fer
flu
fre
ray
Figure 7: Normalized network latency (lower is better).
 
 
F
P
P
D
E
d
e
z
i
l
a
m
r
o
N
1.1
1
0.9
0.8
0.7
0.6
RCA−1D
AGE−ADAP
AGE−ADAP−REC
can ded fac
fer
flu
fre
ray
Figure 8: Normalized EDPPF (lower is better).
higher performance overheads at the system-level. For these multithreaded benchmarks, we use fair speedup as a metric for performance as it provides a more accurate estimate (see Section S6).
Across different benchmarks, AGE-ADAP-REC shows 10.4% performance improvement over RCA-1D, demonstrating its effectiveness.
7. CONCLUSION
In this paper, we introduce a new reliability metric called Trafﬁc
Threshold per Epoch (TTpE) to model the effects of aging on NoC
routers and links. Using this metric, we propose an aging-aware
adaptive routing algorithm and router micro-architecture. Extensive experimental analysis incorporating power-performance impact of aging demonstrate 13%, 12.17% improvements in network
latencies and EDPPF for our algorithm, respectively. At the system level, our algorithm shows 10.4% performance improvement
for real workloads.
RCA−1D
AGE−ADAP
AGE−ADAP−REC
Acknowledgments
This work was supported in part by National Science Foundation
grant CNS-1117425 and Micron Research Foundation.
8. 
[1] Open Source NoC Router RTL. https://nocs.stanford.edu/
cgi- bin/trac.cgi/wiki/Resources/Router.
[2] PARSEC. http://parsec.cs.princeton.edu/.
[3] N. Agarwal, T. Krishna, L.-S. Peh, and N. K. Jha. Garnet: A detailed on-chip
network model inside a full-system simulator. pages 33–42, 2009.
[4] S. Bhardwaj, W. Wang, R. Vattikonda, Y. Cao, and S. Vrudhula. Predictive
modeling of the nbti effect for reliable design. In IEEE Custom Integrated
Circuits Conference, pages 189 –192, sept. 2006.
[5] T.-B. Chan, J. Sartori, P. Gupta, and R. Kumar. On the efﬁcacy of nbti
mitigation techniques. In Proc. of DATE, pages 1–6, 2011.
[6] X. Fu, T. Li, and J. A. B. Fortes. Architecting reliable multi-core
network-on-chip for small scale processing technology. In Proc. of DSN, pages
111–120, 2010.
[7] P. Gratz, B. Grot, and S. W. Keckler. Regional congestion awareness for load
balance in networks-on-chip. In HPCA, pages 203–214, 2008.
[8] C. Hernandez, F. Silla, and J. Duato. A methodology for the characterization of
process variation in noc links. In Proc. of DATE, pages 685–690, 2010.
[9] A. B. Kahng, B. Li, L.-S. Peh, and K. Samadi. Orion 2.0: A fast and accurate
noc power and area model for early-stage design space exploration. In DATE,
pages 423–428, 2009.
[10] A. K. Kodi, A. Sarathy, A. Louri, and J. M. Wang. Adaptive inter-router links
for low-power, area-efﬁcient and reliable network-on-chip (noc) architectures.
In Proc. of ASP-DAC, pages 1–6, 2009.
[11] B. Li, L.-S. Peh, and P. Patra. Impact of process and temperature variations on
network-on-chip design exploration. In NOCS, pages 117–126, 2008.
[12] M. M. K. Martin, D. J. Sorin, B. M. Beckmann, M. R. Marty, M. Xu, A. R.
Alameldeen, K. E. Moore, M. D. Hill, and D. A. Wood. Multifacetâ ˘A ´Zs general
execution-driven multiprocessor simulator (gems) toolset. SIGARCH Comput.
Archit. News, 33, 2005.
[13] A. K. Mishra, N. Vijaykrishnan, and C. R. Das. A case for heterogeneous
on-chip interconnects for cmps. In ISCA, pages 389–400, 2011.
[14] J. D. Owens, W. J. Dally, R. Ho, D. N. Jayasimha, S. W. Keckler, and L.-S. Peh.
Research challenges for on-chip interconnection networks. IEEE Micro,
27(5):96–108, 2007.
[15] M. Sun, M. G. Pecht, and D. Barbe. Lifetime rc time delay of on-chip copper
interconnect. In IEEE Tran. on Semiconductor Manufacturing, volume 15,
pages 253–259, 2002.
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
1.2
1.15
1.1
1.05
1
0.95
0.9
0.85
0.8
can ded fac
fer
flu
fre
ray
Figure 9: Normalized performance (higher is better).
ifests as performance degradation in routers and links along this
path. Consequently, packets transmitted along these paths suffer
from increased latency. On the other hand, AGE-ADAP and AGEADAP-REC route packets intelligently using paths that are least
stressed and least congested (Section 4) and therefore incur lower
overheads. Among AGE-ADAP and AGE-ADAP-REC, additional
recovery cycles in case of AGE-ADAP-REC to meet TTpEs for
each epoch leads to a higher latency as compared to AGE-ADAP.
On an average, AGE-ADAP-REC reduces the latency by 13% relative to RCA-1D.
6.3.2 Energy-Delay-Product-Per-Flit
To show the impact of aging stress on power-performance characteristics of NoC based multicore system, EDPPF is also evaluated for each of the schemes. Figure 8 shows the EDPPF for different benchmarks across various schemes normalized to the RCA-1D
scheme. AGE-ADAP and AGE-ADAP-REC are able to achieve reduced overhead as compared to RCA-1D. Due to additional recovery cycles, AGE-ADAP-REC incurs a higher EDPPF as compared
to AGE-ADAP. On an average, AGE-ADAP-REC reduces EDPPF
by 12.17% relative to RCA-1D.
6.3.3 Performance
Figure 9 shows the system performance for the schemes normalized to RCA-1D. Here also we observe that RCA-1D shows lower
performance as compared to AGE-ADAP and AGE-ADAP-REC
schemes. As RCA-1D is only congestion aware, it selects the least
congested paths over the least stressed ones and therefore incurs
 
 
)
e
l
c
y
c
/
s
t
i
l
f
(
n
o
i
t
a
z
i
l
i
t
U
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
Figure 10: Different router utilization in a 4 × 4 NoC
Mesh.
0
2
4
6
8
10
12
14
Epochs
Supplemental Materials
Figure 11: Buffer Utilization of router R1 for Different
Epochs.
S1. DISPARITY IN ROUTER UTILIZATION
Figures 10 and 11 show the disparity in NoC router utilization.
Figure 10 shows the variation in buffer utilization (ﬂits/cycle) in
a 4 × 4 NoC mesh. These utilizations were obtained using the
GARNET NoC simulator when trafﬁc is generated by the canneal
benchmark run. Similarly, the utilization of routers and links also
varies at the runtime. We conducted another experiment using the
same benchmark and NoC mesh to ﬁnd the runtime variation in
buffer utilization of router R1 (marked as 1 in Figure 10). We divided the total runtime into 10 different epochs of equal width. Figure 11 shows the different utilizations obtained for different epochs.
Therefore, we observe two kinds of variations in utilization: a) every router and link has different utilization (asymmetric); and b)
every router and link also experiences variation in utilization across
different time windows (runtime).
S2. MODELING AGING IMPACT ON NOC
ROUTERS AND LINKS
In this section, we present the system-level aging model that
models the impact of aging mechanisms such as NBTI and electromigration on NoC routers and links. Here, we also derive the
dependency of TTpE of stressed links and routers on the aginginduced delay variation.
S2.1 Modeling Delay Variations due to NBTI
Degradation in NoC Routers
Due to the presence of both combinational and storage circuitry
in NoC routers, the effects of NBTI on the performance of these
routers cannot be ignored [SR5]. In this section, we model these
effects using the ﬂexible numerical model of NBTI degradation
based on reaction-diffusion [SR6]. According to this model, Vth
shift is given by:
∆Vth =
qNit (t)
Cox
(2)
where q is the elementary charge, Nit (t) is the number of interface
traps per unit area at time t and Cox is the PMOS gate capacitance.
This model calculates Nit (t) using the different parameters mentioned in Table 3.
As the architectural level techniques such as dynamic voltage
scaling and activity and power management mechanisms are not
applicable for network-on-chip design, we can also use the device
level model [SR7] for NBTI modeling. We analyzed the reactiondiffusion model using the 65 nm technology parameters and obtained a similar degradation as the device level model.
To ﬁnd the TTpE of a stressed router, we use a similar a"
Explicit modeling of control and data for improved NoC router estimation.,"Networks-on-Chip (NoCs) are scalable fabrics for interconnection networks used in many-core architectures. ORION2.0 is a widely adopted NoC power and area estimation tool; however, its models for area, power and gate count can have large errors (up to 110% on average) versus actual implementation. In this work, we propose a new methodology that analyzes netlists of NoC routers that have been placed and routed by commercial tools, and then performs explicit modeling of control and data paths followed by regression analysis to create highly accurate gate count, area and power models for NoCs. When compared with actual implementations, our new models have average estimation errors of no more than 9.8% across microarchitecture and implementation parameters. We further describe modeling extensions that enable more detailed flit-level power estimation when integrated with simulation tools such as GARNET.","Explicit Modeling of Control and Data for Improved NoC Router
Estimation
Andrew B. Kahng†‡ , Bill Lin† and Siddhartha Nath‡
UC San Diego ECE† and CSE‡ Departments, La Jolla, CA 92093
abk@ucsd.edu, billlin@ece.ucsd.edu, sinath@cs.ucsd.edu
ABSTRACT
Networks-on-Chip (NoCs) are scalable fabrics for interconnection
networks used in many-core architectures. ORION2.0 is a widely
adopted NoC power and area estimation tool; however, its models
for area, power and gate count can have large errors (up to 110%
on average) versus actual implementation.
In this work, we propose a new methodology that analyzes netlists of NoC routers that
have been placed and routed by commercial tools, and then performs
explicit modeling of control and data paths followed by regression
analysis to create highly accurate gate count, area and power models
for NoCs. When compared with actual implementations, our new
models have average estimation errors of no more than 9.8% across
microarchitecture and implementation parameters. We further describe modeling extensions that enable more detailed ﬂit-level power
estimation when integrated with simulation tools such as GARNET.
Categories and Subject Descriptors
C.2 [Computer-Communications Networks]: Network Architecture and Design
General Terms
Algorithms, Design, Performance
Keywords
network-on-chip, ﬂit-level power modeling, parametric regression
1.
INTRODUCTION
Networks-on-Chip (NoCs) have proven to be a highly scalable
and low-latency interconnection fabric in the era of many-core architectures, as evidenced by in commercial chips such as the Intel
80-core [31], IBM Blue Gene [32] and Tilera TILE-Gx [33] processors. Because of their growing importance, NoC implementations
must be optimized for latency and power [7, 9, 11, 15, 20]. To aid
architects and designers in early design-space exploration, accurate
NoC power and area estimators are required. Previous approaches
to modeling are of two kinds, (1) based on templates at the architecture level, such as ORION2.0 [3], and (2) based on regression
analysis on post-P&R data, such as [2]. ORION2.0 is widely used
as a stand-alone tool as well as with full-system NoC simulators such
as GARNET [13].
Both template- and regression-based modeling approaches, however, are in need of improvement. ORION2.0 has large estimation
errors [2] for two fundamental reasons: (1) models are incomplete
because control path resources are not modeled, even though they
contribute signiﬁcantly to power and area, and (2) models are not
reﬁned using post-P&R power and area data. Kahng et al. [2] and
Jeong et al. [10] proposed non-parametric regression models to overcome the limitations in ORION2.0; [2] further concluded that parametric regression can be very inaccurate. In Figure 1(a), we show
power estimation errors at 65nm in ORION2.0 and the previous regression approach [2], as a function of the number of virtual channels in the router. The maximum errors are 185% and 75%. SimPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2012, June 3-7, 2012, San Francisco, California, USA.
Copyright 2012 ACM 978-1-4503-1199-1/12/06 ...$10.00.
ilarly, in Figure 1(b), we show power estimation errors at 90nm in
ORION2.0 and the previous regression approach [2] when the ﬂitwidth is changed.
Figure 1: Poor estimations by ORION2.0 [3] and previous regression approach [2].
NetMaker vs. ORION2.0 vs. regression at (a) 65nm. (b) 90nm.
In this work, we propose a new model, ORION_NEW, which improves the ORION2.0 models by explicitly modeling control and
data path resources. We perform parametric regression analysis with
post-P&R area and power data to reﬁne ORION_NEW models such
that the estimates for area and power are highly accurate across multiple router RTLs, microarchitectures and implementation parameters. We demonstrate that accurate parametric models lead to better minimization of error in least-squares regression, and the worstcase errors are signiﬁcantly better than the worst-case errors of nonparametric regression approaches [2]. We further describe modeling extensions that enable more detailed ﬂit-level power estimation
when integrated with simulation tools such as GARNET [13].
Our main contributions are as follows.
1. We explicitly model control and data paths to create
ORION_NEW models that are highly accurate and robust
across multiple router RTLs, and across microarchitecture and
implementation parameters.
2. We demonstrate that parametric regression with accurate models can signiﬁcantly reduce the worst-case error compared to
non-parametric regression approaches for NoC routers.
3. We are the ﬁrst to propose a detailed, efﬁcient and ﬁne-grained
ﬂit-level power estimation model that seamlessly integrates
with full-system NoC simulators.
The remainder of this paper is organized as follows. Section 2
presents related work. Section 3 describes the ORION_NEW model.
Section 4 describes our modeling methodology. Section 4.3 presents
our new ﬂit-level power estimation model. Section 5 presents experimental results to validate and compare ORION_NEW models
with ORION2.0 and the non-parametric regression approach in [2].
Section 6 concludes and outlines future work.
2. RELATED WORK
Previous works have focused primarily on two broad modeling
paradigms: (1) architecture-level models using templates for each
router component block (input and output buffer, crossbar, switch
and VC arbiter) and (2) RTL and gate-level simulation-driven models. For the ﬁrst approach, Patel et al. [4] propose a transistor countbased analytical model for NoC power. However, their models have
large errors because they do not consider any router microarchitecture parameter. ORION [7] and ORION2.0 [3] are architectural
models that use microarchitecture and technology parameters for the
router component blocks. However, from our experimental studies
as well as from [2], ORION2.0 estimates have very large errors.
The other approach is based on pre-layout (RTL or post-synthesis
gate-level) [6, 8, 5, 16] or post-layout [12, 1, 17, 14] simulations.
Banerjee et al. [1] report accurate power for a range of routers, but
do not present any analytical models for router power. Chan et al. [8]
develop cycle-accurate power models with reported average errors
up to 20%. Meloni et al. [14] and Lee et al. [16] perform parametric regression analysis on post-layout and RTL simulation results, respectively. Their models, however, are fairly coarse-grained
as they cannot explain how power dissipates in each router block
with change in load, microarchitecture or implementation parameters. Kahng et al. [2] use non-parametric regression to model NoCs.
Our methodology uses accurate parametric models along with
non-negative least-squares regression analysis to provide accurate
area and power estimates, with average error of no more than 9.8%
across microarchitecture and implementation parameters. Our models calculate area and power on a per-instance basis but avoid the
overhead of slow gate-level simulations.
Furthermore, we signiﬁcantly extend our models to achieve ﬂitlevel power estimations. Ye et al. [18] and Penolazzi et al. [17]
estimate power dissipation using bit-level model, and Penolazzi et
al. [17] propose a static bit-based model to estimate Nostrum NoC
power. However, each of these models is tied to a speciﬁc router implementation and cannot explain how different bit encodings affect
the power consumption in each block within the router. Our ﬂit-level
power estimation methodology estimates the power impact for each
component block and reports accurate power numbers across different bit encodings in ﬂits.
3. MODEL DESCRIPTION
We now describe the ORION_NEW modeling of each component
in a modern on-chip network router. We have developed these models by analyzing post-synthesis and post-P&R netlists of two RTL
generators, NetMaker [28] from Cambridge and the Open Source
NoC router from Stanford [29]. (Our methodology is described in
detail in Section 4.) Figure 2 shows the component blocks in a router,
i.e., input buffer, switch and VC (virtual channel) arbiter, crossbar
and output buffer [11]. We model instances (or gates) in each component block because our studies show that accurate estimations of
area and power are possible only if the instance modeling is accurate. The microarchitecture parameters used are #Ports (P), #VCs
(V), #Buffers (B) and Flit-width (F).
Modern router RTLs such as NetMaker and Stanford NoC use a
simpler and smaller crossbar implementation where each ﬂit bit is
controlled using a tri-state buffer, which can be modeled as a 2 : 1
MUX. Hence, the total number of such MUXes required are: P ×
P × F . This new model reduces the instance count by a factor of
[2l og2 P − 1] when compared to the multiplexer tree implementation.
3.3 Switch and VC Arbiter (SWVC) Model
This block is responsible to generate control signals for the crossbar such that a connection is established between input buffers to
output ports [19]. ORION2.0 adds an overhead of 30% to the arbiter
by default. Our analysis indicates that this overhead is not needed
with frequency ranges 400MH z-900MH z for process nodes 45nm to
130nm. Beyond this range of frequency a derating factor must be applied, which is discussed in Section 3.7. The ORION_NEW model
for switch and virtual channel arbiter is: 9 × (P × (P × (V 2 + 1) +
(V 2 − 1)). The constant factor 9 arises because six 2-input NOR
gates, two INVerters and one D-FlipFlop are used to generate one
grant signal on each path.
3.4
Input Buffer (InBUF) Model
This block holds the entire incoming payload of ﬂits at the input stage of the router for decode [19]. ORION2.0 models only the
buffer instances and does not take into account control signals which
are needed at this stage for decode such as FIFO select, buffer enable control signals and logic for housekeeping, such as the number
of free buffers available per VC, VC identiﬁcation tag per buffer,
etc. As a result, ORION2.0 underestimates the instances at the input
stage of the router.
In our new model, we model control signals and housekeeping
logic in addition to the actual FIFO buffers. Modern routers implement the same stage VC and SW allocation to optimize delay [11],
leading to doubling of input buffer resources. Hence, the number of
FIFO buffers are 2 × P ×V × B × F . The control signals for decoding
the housekeeping logic are modeled as: 180 × P × V + 2 × P2 × V ×
B + 3 × P × V × B + 5 × P2 × B + P2 + F × P + 15 × P (as analyzed
from the post-synthesis and post-P&R netlists). Each constant factor
in the model denotes the number of instances per path. For example,
the 180 factor accounts for instances to generate FIFO select signals
and ﬂags for each buffer in the P × V path. The smaller constant factors 2, 3, 5 account for instances for local ﬂags in the decode logic.
The factor 15 denotes the number of buffers in each FIFO select path
of an input port.
Figure 2: Router architecture [7].
3.1 New Model Elements
The new model explicitly accounts for control and data resources
in the router. The new modeling elements are:
1. Control resources such as FIFO select and decode logic signals in the input and output buffers.
2. Tri-state crossbar model.
3. Additional
input buffer resources for delay-optimized arbiters [11].
4. Output buffer model to store only head ﬂits.
5. Clock frequency dependent scaling.
3.2 Crossbar (XBAR) Model
This component block is responsible for connecting input ports to
output ports so that all ﬂit bits are transferred to output ports [19].
The ORION2.0 models for router crossbar consider two implementations, matrix [11] and multiplexer tree [3]. The multiplexer tree
is the smaller of these in terms of instance count and area and is
modeled as P × P × F multiplexers at each level of the tree. .
3.5 Output Buffer (OutBUF) Model
This block holds the head ﬂits between the switch and the channel
for a switch with output speedup [19]. ORION2.0 models the output
buffers in exactly the same way as input buffers; this is inaccurate for
modern routers that use hybrid output buffers, and leads to an overestimate of the instance count. The output buffers need to only store
enough ﬂits to match the speed between the switch and the channel. At the output, these buffers are used to stage the ﬂits between
the switch and channel when channel and switch speeds mismatch.
Instead of P × V × B × F used in ORION2.0, output buffers are proportional to P × V . There are several control signals per port and VC
associated with each buffer, which makes the overall instance count
grow as P × (80 × V + 25). The constant factor 80 accounts for the
instances used to generate ﬂow control credit signals for each VC,
while the constant factor 25 accounts for buffers and ﬂags.
3.6 Clock and Control Logic (CLKCTRL) Model
ORION2.0 does not accurately model clock buffers and control
logic routing resources as clock frequency scales. ORION_NEW
models these resources as 2% of the sum of instances in the SWVC,
InBUF and OutBUF component blocks.
3.7 Frequency Derating Model
As frequency changes, timing constraints change. To meet setup
time at higher frequencies, buffers are inserted leading to an overall
increase in instance count in the design. ORION2.0 scaling is agnostic to implementation parameters such as clock frequency. This
causes large errors in area and instance counts at higher frequencies
for component blocks such as SWVC, InBUF and OutBUF where
there are several logic signals which consume routing resources. The
number of instances in the crossbar does not vary much with frequency because there are no critical paths. So, we can ignore the
effects of frequency on the crossbar.
To derate for frequency, we ﬁnd the frequency below which the
instance counts change by less than 1%. In 65nm technology, this
is 400MH z for both NetMaker and Stanford NoC routers. We
derate instance counts based on this frequency as: ∆I nst ance =
∆Frequency × Const ant Fact or. The constant factor is dependent on
the amount of control logic versus FIFO for each component block.
In SWVC and InBUF, the cont rol /F IF O ≈ 1, so the constant factor
value is 1. In OutBUF, cont rol /F IF O ≈ 0.16, and a ﬁtted constant
factor of 0.03 is used to account for setup buffers.
4. ORION_NEW METHODOLOGY
In this section, we describe how we estimate power and area using
the two approaches described in Sections 4.1 and 4.2. We extend our
methodology to ﬂit-level power estimation in Section 4.3. We use:
• Multiple parametrized NoC RTL generators, NetMaker [28]
from Cambridge University and the Open Source NoC from
• Range of values of microarchitecture parameters, #Ports (P),
Stanford [29] to make the ORION_NEW models robust.
#VCs (V), #Buffers (B) and Flit-width (F) and implementation
parameters such as clock frequency and technology node.
• Operational parameters for power calculation: switching activity (TR) and static probability of 1’s in the input (SP).
• Multiple
commercial
tools, Synopsys DesignCompiler
(DC) [22] and Cadence RTL Compiler (RC) [21], with options to preserve module hierarchy after synthesis because we
analyze each router component block. We compare instance
counts, area and power reported by each tool to ensure that
for a given RTL these results do not vary by more than 10%.
• Cadence SOC Encounter (SOCE) [21] with die utilization of
0.75 and die aspect ratio of 1.0 to place and route the synthe• Synopsys PrimeTime-PX (PT-PX) [23] to run power analysis
sized router netlist.
of the post-P&R netlist, SPEF [26] and SDC [27].
• MATLAB [30] function lsqnonneg for regression analysis.
Table 1 summarizes these details.
Table 1: ORION_NEW Methodology: Tools and Parameters
Stage
RTL
µarch
Impl
Syn
Power
Tool
NetMaker
Stanford NoC
Ports; VCs;
BUFs; Flit-Width
Clock Freq
Tech Nodes
Synopsys DC
(v2009.06-SP2)
Cadence RC
(vEDI09.12)
Synopsys PT-PX
(v2009.06-SP2)
Regression
MATLAB
Options
ISLAY conﬁg
default
P = {5, 6, 8, 10}; V = {2, 3, 6, 9}
B = {8, 10, 15, 22}; F = {16, 24, 32, 64}
Freq = {400, 700, 1200, 2000} MHz
Switching Activity (TR) = {0.2, 0.4, 0.6, 0.8}
Static Prob of 1’s (SP) = {0, 0.25, 0.5, 0.75, 1.0}
45nm = OpenPDK45 from NCSU/OSU
65nm, 90nm, 130nm = TSMC GP, G, GHP resp.
compile_ultra -exact_map
-no_autoungroup -no_boundary_optimization
report_area -hierarchy; report_power -hierarchy
default synthesis ﬂow
set power_enable_analysis true
set power_analysis_mode averaged
set_switching_activity -toggle_count TR
-static_probability SP -type inputs
read_sdc router.sdc; read_parasitics router.spef
lsqnonneg
Figure 3 shows the ﬂow we use to develop ORION_NEW models
for each component block of the router. In Table 2, we summarize
the ORION_NEW instance count model of each component block.
Component
XBAR
SWVC
InBUF
OutBUF
CLKCTRL
Table 2: ORION_NEW model for Instances
Equation
P2 F
9(P2V 2 + P2 + PV − P)
180PV + 2PV BF + 2P2V B + 3PV B + 5P2 B + P2 + PF + 15P
25P + 80PV
0.02 × (SW V C + I nBU F + Out BU F )
There are two ways to estimate NoC area and power using the
ORION_NEW models as shown in Figure 4. The manual approach
is described in Section 4.1, and the regression analysis approach is
described in Section 4.2. The beneﬁts of each are described below.
Figure 3: High-level ﬂow used to arrive at ORION_NEW models.
• Both the approaches have minimum estimation error when the
router RTLs are modular so that instance count and area numbers per component block can be calculated.
• The manual approach requires knowledge of process node
and ﬁner implementation details such as (HP, LSTP, LOP) ×
(HVT, NVT, LVT) × (bc, wc) to correctly select a technology
library ﬁle. The regression analysis approach, on the other
hand, is agnostic of implementation details. It only depends
on a training set of data. More data points help the tool to
minimize the sum of square error.
• The manual approach leads to faster estimation since it only
involves technology library look-ups and plugging-in of library values into the ORION_NEW model. In contrast, the
regression analysis approach requires synthesis and P&R to be
performed on the router RTL for at least six data points. On
an Intel Core i3 2.4GH z processor, the runtime of the manual
approach when used with ORION2.0 code is less than 10ms,
whereas the regression analysis approach takes about 140ms,
when 64 test data points are used.
• It is extremely difﬁcult to capture ﬁne-grained implementation
details in ORION_NEW models, e.g., area and power contribution of wires after routing, and change in coupling capacitance and power after metal ﬁll. These missing details cause
estimation errors versus actual implementation when the manual approach is used. In order to reduce errors with respect to
implementation, the regression analysis approach with postP&R area and power is preferred.
Figure 4: High-level view of power and area estimation methodology using Manual
and Regression Analysis (LSQR) approaches.
4.1 Manual Approach to Estimate NoC Power and
Area
This approach uses ORION_NEW models along with the technology library ﬁle of the process node in which the router is going to be
fabricated. The key ingredients of this approach are:
• Microarchitecture parameters {P, V, B and F} and implemen• Cell areas, leakage, internal energy and load capacitance.
tation parameter (clock frequency).
• Switching activity.
ORION_NEW simpliﬁes design of a NoC, using only a few standard
cells. Instance count for each component block for a given set of
router microarchitecture parameters is calculated from Table 2. Cell
area is obtained from technology ﬁles. The area calculation, along
with TSMC standard-cell names in parentheses, is shown in Table 3.
Table 3: Area Models using Instance count
Component
XBAR
SWVC
InBUF
+ OutBUF
CLKCTRL
Logic (TSMC Cell Name)
MUX2 (MUX2D0)
6 NOR2, 2 INV, 1 DFF
(NR2D1, INVD1, DFQD1)
1 AOI, 1 DFF
(AOI22D1, DFQD1)
1 INV, 1 AOI
(INVD1, AOI22D1)
(cid:17)
Area
AreaMU X × X BARinst s
(cid:16) 6AreaNOR +2AreaINV +AreaDF F
(cid:16) AreaAOI +AreaDF F
9
(cid:16) AreaAOI +AreaINV
2
2
×(I n + Out )BU Finst s
×SW V Cinst s
(cid:17)
(cid:17)
×(CLKCT RL)inst s
Power has three components, that is, leakage, internal and switching. Leakage power is static power when the cell is not transitioning
between logic states. It is dependent on current state of the input
pins of the cell as well as process corner, voltage and temperature.
Switching and internal power together constitute dynamic power,
which varies with operating voltage, capacitive load and frequency
of operation. Switching power is the power consumed when a load
capacitance on a net is charged and discharged; internal power is the
power dissipated inside a cell and consists of short-circuit power and
switching power of internal nodes.
In ORION_NEW, toggle rate (TR) is equal to the input switching
activity for all nets in the crossbar, arbiters and buffer control logic.
We assume that buffer cells toggle at 25% of the input switching
activity, since multiple VCs do not require buffer contents to change
in every cycle.
Leakage power calculation: For leakage power, the model uses the
weighted average of the state-dependent leakage of the cells. Equations (1)-(4) are used to calculate the leakage power of each component block.
(1)
× SW V Cinst s
(2)
(cid:19)
Pl eak_X BAR = MU Xl eak × X BARinst s
(cid:18) 6NORl eak + 2INVl eak + DF Fl eak
(cid:18) AOIl eak + DF Fl eak
(cid:18) AOIl eak + INVl eak
(cid:19)
(cid:19)
9
2
× (I n + Out )BU Finst s
× (CLKCT RL)inst s
(3)
(4)
Pl eak_SW V C =
Pl eak_BU F =
Pl eak_CLKCT RL =
2
Internal power calculation: For internal power, table look-ups in
technology library ﬁles return the internal energy of given standard
cells with load capacitance of fanout pins and slew value of ≈ 5 ×
F O4 d el ay.1 Internal energy for a pin is the minimum of the rise and
fall energies. Equations (5)-(8) are used to calculate internal power
of each component block.
Pint _X BAR = MU Xint × T R × X BARinst s
(5)
Pint _SW V C = (6NORint + 2INVint + DF Fint ) × T R × SW V Cinst s (6)
Pint _BU F = (AOIint + 0.25DF Fint ) × T R × (I n + Out )BU Finst s (7)
Pint _CLKCT RL = (AOIint + INVint ) × T R × (CLKCT RL)inst s
(8)
Switching power calculation: For switching power, the load capacitance is calculated as the sum of the input capacitances of pins that
are driven by a net and the wire capacitance on the net. The wire
capacitance is approximately calculated as a constant factor times
the total pin capacitances. This constant factor is 1.4 at 65nm and is
assumed to decrease by 14% with for each successive process node
shrink. Equations (9)-(12) are used to calculate switching power of
each component block.
Psw_X BAR = X BARl oad × T R × X BARinst s
Psw_SW V C = SW V Cl oad × T R × SW V Cinst s
Psw_BU F = (I n + Out )BU Fl oad × T R × (I n + Out )BU Finst s
Psw_CLKCT RL = (CLKCT RL)l oad × T R × (CLKCT RL)inst s
(9)
(10)
(11)
(12)
1 The F O4 delay is the delay of a minimum-sized INV and is a standard proxy for
switching speed in a given process technology. The resulting slew time values are 80 −
100ps for 45nm and 65nm technologies.
Flow details: The steps below describe how total area and power
are estimated using the ORION_NEW models and equations above.
1. Choose microarchitecture parameters (P,V,B,F), clock frequency and average switching activity at inputs.
2. Use models in Table 2 to calculate the instance count of each
component block of the router.
3. Use models in Table 3 to calculate the area of each router component block. Total area is calculated as the sum of areas of
all blocks.
4. Obtain state-dependent leakage of cells from technology library ﬁles. Use Equations (1)-(4) to calculate leakage power
of each component block. Total router leakage power is calculated as the sum of leakage power of all component blocks.
5. Obtain internal energy of cells from technology library ﬁles.
Use Equations (5)-(8) to calculate internal power of each component block. Total internal power is calculated as the sum of
internal power of all component blocks.
6. Obtain input pin capacitances of cells from technology library
ﬁles. Use Equations (9)-(12) to calculate switching power of
each component block. Total switching power is calculated as
the sum of switching power of all component blocks.
7. The total power dissipated by the router is calculated as the
sum of total leakage, total internal and total switching power.
4.2 Regression Analysis Approach to Estimate
NoC Power and Area
As another approach to estimation of router area and power, we
use parametric regression to ﬁt parameters for cell area, leakage, internal energy and load capacitance into ORION_NEW models. This
approach requires instance counts, area, and total leakage, internal
and switching power of each component block of the router from
post-P&R tools. Options are set in synthesis to preserve module hierarchy and names. Constrained least-squares regression (LSQR) is
used to enforce non-negativity of coefﬁcients (cell area, leakage, internal energy, load capacitance). We use the MATLAB [30] function
lsqnonneg for this purpose, and tool options as given in Table 1.
Flow Details: LSQR is applied to ﬁt a model of post-P&R instance
count for each router component block. At least six data points are
needed in the training set because there are four microarchitecture
parameters and two implementation parameters (clock frequency
and toggle rate). Our parametric LSQR setup is as follows.
a1 · I nst smod el <com ponent> + a0 = I nst st ool <com ponent>
mod el <com ponent> + b0 = Areat ool <com ponent>
(13)
I nst sR
mod el <com ponent> is the reﬁned instance count of each component block after LSQR. The reﬁned instance count is used to ﬁt models of post-P&R area and power as follows:
b1 · I nst sR
(14)
In Equation (14), b1 is the ﬁtting coefﬁcient for cell area and the
coefﬁcient b0 accounts for the routing overhead.
We model leakage, internal and switching power as:
{c5 , d5 , e5 } · I nst sR
{c3 , d3 , e3 } · I nst sR
mod el SW V C +
mod el Out BU F +
{c1 , d1 , e1 } · I nst smod el CLKCT RL = {Pl eak t ool , Pint t ool , Psw t ool }
mod el X BAR + {c4 , d4 , e4 } · I nst sR
mod el I nBU F + {c2 , d2 , e2 } · I nst sR
(15)
where coefﬁcients {c5 , · · ·, c0 } are used to ﬁt cell leakage power, and
similarly {d5 , · · ·, d0 } and {e5 , · · ·, e0 } are respectively used to ﬁt
internal energy and load capacitance.
It is possible to skip the instance count reﬁnement step (Equation (14)) and directly perform LSQR for area and leakage, internal and switching power using the above equations. We observe
that average error can change by 3% in either direction by omitting the instance count reﬁnement step. Note that it is necessary to
perform per-component LSQR; if LSQR is performed for the entire
router’s area or power, large errors result because multiple components have the same parametric combination of (P,V,B,F). Failing to
separate these contributors to area or power results in large errors: at
65nm, we have experimentally observed worst-case errors of 296%
for power and 557% for area. Thus, it is important to preserve module hierarchy during synthesis in the ﬂow.2
4.3 Extension to Flit-Level Power Modeling
The
dynamic
power models
used
in ORION2.0
and
ORION_NEW do not consider bit encodings in a ﬂit, which
can lead to signiﬁcant errors in dynamic power estimation. As an
example, consider an 8-bit ﬂit with four bits as 1. This ﬂit can either
be 8b(cid:48) 11110000 or 8b(cid:48) 10101010. In the ﬁrst encoding, there is only
one toggle per ﬂit, whereas in the second encoding there are seven
toggles per ﬂit. Clearly, the second ﬂit will lead to higher dynamic
power than the ﬁrst one. To model this effect, we devise a ﬂow as
shown in Figure 5. Before using a testbench, the netlists must pass
an equivalence check using tools such as Synopsys Formality [24].
We inject different bit encodings in the input during simulation
over 10000 cycles and the resultant VCD (Value Change Dump) is
validated using a waveform analyzer such as Synopsys DVE [25]. A
satisfactory VCD is used as input to Synopsys PrimeTime-PX [23]
to obtain power values. Regression analysis is performed using
the tool-reported power values with the ORION_NEW estimates
to obtain an enhanced ORION_NEW model for ﬂit-level power
estimation. These models may be invoked by NoC full-system
simulators such as GARNET [13] to obtain very accurate estimates.
Figure 5: Methodology to enhance ORION_NEW dynamic power models with ﬂitlevel power estimation.
5. VALIDATION AND RESULTS
We set up experiments as described in Table 1 of Section 4.
We use parameters and tools for our experiments as listed in Table 1. We discuss the results in two parts - (1) ORION2.0 versus
ORION_NEW comparisons for area and power, and (2) impact of
results with our regression analysis approach versus the approach
used in prior work of [2]. We compare the results of our methodology with post-P&R instance count, power and area outcomes for
two router RTL generators, Netmaker [28] and Stanford NoC [29].
5.1 ORION2.0 versus ORION_NEW Comparisons
Since the instance count per component is at the core of the
ORION_NEW model, we compare ORION2.0 estimates of instance
(or gate) counts, as well as the ORION_NEW model estimates with
implementation (post-P&R) for each component block. Figures 6(a),
6(c) and 6(e) show the large errors in ORION2.0 in the crossbar, output buffer and input buffer respectively, and Figures 6(b), 6(d) and
6(f) show the signiﬁcant reduction in estimation error for these components with ORION_NEW models. ORION2.0 and ORION_NEW
are plotted in different graphs because of the large errors in instance
counts in ORION2.0.
ORION2.0 modeling of instance count for a component does not
consider implementation parameters such as clock frequency. As a
result, the instance count does not scale when frequency is changed,
even though at higher frequencies several buffers are inserted to
meet tight setup time constraints. ORION_NEW models apply a
frequency derating factor on the instance models for component
blocks as described in Section 3.7. Figures 7(a) and 7(b) show the
incorrect estimates by ORION2.0; by contrast, the estimates from
2Use of hierarchical synthesis in general leads to lower instance count, standard-cell
area, and total power as compared with ﬂat synthesis results. This comes at the cost of
frequency (timing slack), since ﬂat optimization across module boundaries can sometimes achieve better timing results. For our selection of microarchitecture and implementation parameters, hierarchical synthesis on average has 35% fewer instances,
48.8% less standard-cell area and 49.4% less total power – along with 8% less timing
slack – compared with ﬂat synthesis. The runtimes for hierarchical and ﬂat synthesis
are within 5% of each other.
Figure 6: (a) XBAR with #Ports: ORION2.0 vs.
Implementation.
(b) XBAR
with #Ports: ORION_NEW vs. Implementation. (c) Output Buffer with #VCs:
ORION2.0 vs. Implementation. (d) Output Buffer with #VCs: ORION_NEW vs.
Implementation. (e) Input Buffer with Flit-Width: ORION2.0 vs. Implementation.
(f) Input Buffer with Flit-Width: ORION_NEW vs. Implementation.
ORION_NEW are very close to actual implementation for output
and input buffer component blocks respectively.
Figure 7: (a) Output buffer with Clock Frequency: ORION2.0 vs. ORION_NEW.
(b) Input buffer with Clock Frequency: ORION2.0 vs. ORION_NEW.
Figure 8: Instance and Area error comparison of ORION2.0 vs. ORION_NEW.
Error% = ABS((TOOL - MODEL) / MODEL * 100).
Table 8 summarizes the error in estimates of ORION2.0 and
ORION_NEW when compared with NetMaker and Stanford NoC
router post-P&R area. Higher values of error among the two models are highlighted in red. Figures 9(a) and 9(b) plot the estimation
errors in power and area respectively at 45nm and 65nm technology nodes after applying the regression ﬁtting approach described
in Section 4.2. We see that ORION_NEW estimates are very close
to implementation (average error of 9.8% in estimating NetMaker
power at 45nm) and are robust across multiple microarchitecture and
implementation parameters as well as router RTLs.
Next, we analyze the impact of ﬂit-level power modeling as described in Section 4.3. To capture the effect of running simulations with input vectors having different bit encodings (shown in
Figure 5), we use options in Synopsys PrimeTime"
Approaching the theoretical limits of a mesh NoC with a 16-node chip prototype in 45nm SOI.,"In this paper, we present a case study of our chip prototype of a 16-node 4×4 mesh NoC fabricated in 45nm SOI CMOS that aims to simultaneously optimize energy-latency-throughput for unicasts, multicasts and broadcasts. We first define and analyze the theoretical limits of a mesh NoC in latency, throughput and energy, then describe how we approach these limits through a combination of microarchitecture and circuit techniques. Our 1.1V 1GHz NoC chip achieves 1-cycle router-and-link latency at each hop and energy-efficient router-level multicast support, delivering 892Gb/s (87.1% of the theoretical bandwidth limit) at 531.4mW for a mixed traffic of unicasts and broadcasts. Through this fabrication, we derive insights that help guide our research, and we believe, will also be useful to the NoC and multicore research community.","Approaching the Theoretical Limits of a Mesh NoC
with a 16-Node Chip Prototype in 45nm SOI∗
Sunghyun Park, Tushar Krishna, Chia-Hsin Chen, Bhavya Daya,
Anantha Chandrakasan, Li-Shiuan Peh
Massachusetts Institute of Technology, Cambridge, MA
ABSTRACT
In this paper, we present a case study of our chip prototype of a
16-node 4x4 mesh NoC fabricated in 45nm SOI CMOS that aims
to simultaneously optimize energy-latency-throughput for unicasts,
multicasts and broadcasts. We ﬁrst deﬁne and analyze the theoretical limits of a mesh NoC in latency, throughput and energy,
then describe how we approach these limits through a combination of microarchitecture and circuit techniques. Our 1.1V 1GHz
NoC chip achieves 1-cycle router-and-link latency at each hop and
energy-efﬁcient router-level multicast support, delivering 892Gb/s
(87.1% of the theoretical bandwidth limit) at 531.4mW for a mixed
trafﬁc of unicasts and broadcasts. Through this fabrication, we derive insights that help guide our research, and we believe, will also
be useful to the NoC and multicore research community.
Categories and Subject Descriptors
B.4 [Hardware]: Input/Output and Data Communications
General Terms
Design, Performance, Measurement
Keywords
Network-on-Chip, Theoretical Mesh Limits, Virtual Bypassing,
Multicast Optimization, Low-Swing Signaling, Chip Prototype
1.
INTRODUCTION
Moore’s law scaling and diminishing performance returns of complex uniprocessor chips have led to the advent of multicore processors with increasing core counts. Their scalability relies highly
on the on-chip communication fabric connecting the cores. An
ideal communication fabric would incur only metal-wire delay and
energy between the source and destination core. However, there
is insufﬁcient wiring for dedicated global point-to-point wires between all cores [8], and hence, packet-switched Networks-on-Chip
(NoCs) with routers that multiplex wires across trafﬁc ﬂows are becoming the de-facto communication fabric in multicore chips [5].
These routers, however, can impose considerable overhead. Latency wise, each router can take several pipeline stages to perform
∗The authors acknowledge the support of the Gigascale Systems
Research Center and Interconnect Focus Center, research centers
funded under the Focus Center Research Program (FCRP), a Semiconductor Research Corporation entity, and DARPA under Ubiquitous High-Performance Computing.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2012, June 3-7, 2012, San Francisco, California, USA.
Copyright 2012 ACM 978-1-4503-1199-1/12/06 ...$10.00.
the control decisions necessary to regulate the sharing of wires
across multiple ﬂows. Inefﬁciency in the control also frequently
leads to poor link utilization on NoCs. Buffers queues have been
used to improve ﬂow control and link utilization, but come with
overhead in energy consumption. Conventional wisdom is that
NoC design involves trading off latency, bandwidth and energy.
In this paper, we describe our design of a NoC mesh chip that
aims to simultaneously approach the theoretical latency, bandwidth
and energy limits of a mesh, for all kinds of trafﬁc (unicasts, multicasts and broadcasts). We ﬁrst derive such theoretical limits of
a mesh NoC for unicasts and broadcasts. This analysis closely
guided us in our design which leverages virtual bypassing to approach the theoretical latency limit of a single cycle per hop for
unicasts, multicasts and broadcasts. This, coupled with the speed
beneﬁts of low-swing signaling, enabled us to swiftly reuse buffers
and approach theoretical throughput without trading off energy or
latency. Finally, low-swing signaling applied to the datapath helps
us towards the theoretical energy limit.
Contributions. In this paper, we make the following contributions:
• We present a mesh NoC chip prototype that shows 48-55% latency beneﬁts, 2.1-2.2x throughput improvements and 31-38%
energy savings as compared with an equivalent textbook baseline NoC described in Section 3.1. To the best of our knowledge,
this is the ﬁrst mesh NoC chip with multicast support.
• We deﬁne the theoretical mesh limits for unicasts and broadcasts, in terms of latency, throughput and energy. We also characterize several prior chip prototypes’ performance relative to
these limits.
• We present lessons learnt from our prototyping experience:
– Virtual bypassing can enable 1GHz single-cycle router pipelines and 32% buffering energy savings with negligible
area overhead (5% only). It comes at the expense of a 21%
increased critical path, though this timing overhead can
be masked in multicore processors where cores limit the
clock frequency rather than routers. More critically, virtual
bypassing does not address non-data-dependent power.
– Low-swing signaling can substantially reduce datapath energy (3.2x less energy in 1mm links compared to a fullswing datapath) as well as realize high frequency singlecycle traversal per hop (5.4GHz with a 64bits 5×5 crossbar and 1mm links), but comes with increased process
variation vulnerability and area overhead.
– System-level NoC power modeling tools like ORION 2.0
[12] can be way off in absolute accuracy (∼5x of measured
chip power) but maintain relative accuracy. RTL-based
post-layout power simulations (post-layout) are much closer to measured power numbers, but post-layout timing
simulations are still off.
The rest of the paper is organized as follows: Section 2 deﬁnes
our baseline router, derives the theoretical limits of a mesh NoC,
and characterizes prior chips performance relative to these limits.
Section 3 describes our fabricated NoC prototype, while Section 4
details measurement results. Finally, we conclude in Section 5.
64b
Next Route
Computation
(NRC)
Header
Generation
Input Buffer
VC1
VC2
VC3
VC4
VC5
VC6
5b
Input Port (BW)
Credit Signals to Previous Routers
5b
5b
Outport Request (VC1)
Outport Request (VC2)
Outport Request (VC3)
Outport Request (VC4)
Outport Request (VC5)
Outport Request (VC6)
Round-robin circuit (mSA-I)
VC allocation (VA)
Credit Signals 
from Next Routers
5ports X 64b
64b
NIC
N
E
S
W
Output Port
(mSA-II)
5ports X 3b
64b 5X5 Crossbar
(ST)
Link
(LT)
Pipeline Stage 1
Pipeline Stage 2
Pipeline Stage 3
Pipeline Stage 4
Figure 1: Baseline router microarchitecture.
2. BACKGROUND AND RELATED WORK
2.1 Baseline Mesh NoC
The mesh [6] is the most popular NoC topology for a generalpurpose multicore processor, as it is scalable, is easy to layout, and
offers path diversity [7, 10, 11, 21, 23]. Each core in a multicore
processor communicates with other cores by sending and receiving
messages through a network interface controller (NIC) that connects the core to a router (hence the network). Before a message is
injected into the network, it is ﬁrst segmented into packets that are
then divided into ﬁxed-length ﬂits, short for ﬂow-control units. A
packet consists of a head ﬂit that contains the destination address,
body ﬂits, and a tail ﬂit that indicates the end of a packet. If the
amount of information the packet carries is little, single-ﬂit packets
are also possible, i.e. where a ﬂit is both the head and tail ﬂit. Because only the head ﬂit carries the destination information, all ﬂits
of a packet must follow the same route through the network.
Figure 1 shows the microarchitecture of an input-buffered virtual
channel router. Before an incoming ﬂit is forwarded to the next
router, it needs to go through several actions in order: buffer write
(BW), route computation (NRC) (only for head ﬂits), switch allocation (SA), virtual channel allocation (VA) (only for head ﬂits),
buffer read (BR), switch traversal (ST), and link traversal (LT). Out
of all these actions, only ST and LT actually move the ﬂits toward
the destination. Thus, we consider all other actions as overhead.
We will refer to this as the baseline router throughout the paper.
2.2 Latency, Throughput and Energy Limits
A mesh topology by itself imposes theoretical limits on latency,
throughput and energy (i.e. minimum latency and energy, and maximum throughput). We derive these theoretical bounds of a k × k
mesh NoC for two trafﬁc types, unicast and broadcast trafﬁc, as
shown in Table 1. Speciﬁcally, each NIC injects ﬂits into the network according to a Bernoulli process of rate R, to a random, uniformly distributed destination for unicasts, and from a random, uniformly distributed source to all nodes for broadcasts. All derived
bounds are for a complete action: from initiation at the source NIC,
till the ﬂit is received at all destination NIC(s). More details on the
derivation of the bounds is shown in Appendix A.
2.3 Related Work
There have been few chip prototypes with mesh NoCs as the
communication fabric between processor cores or nodes, as listed
in Table 2. Other NoCs, e.g. KAIST [2], Spidergon [4], Pleiades
[24], are targeted for heterogeneous topologies and architectures,
making it difﬁcult to characterize them against the theoretical mesh
limits. The prototypes range from full multicore processors to standalone NoCs. Of these, three chips were selected for comparison,
that differ signiﬁcantly with respect to targeted design goals and
optimizations: Intel Teraﬂops which is the precursor of the Intel IA32 NoC, Tilera TILE64 which is the successor of the MIT RAW,
and SWIFT, a NoC with low-swing signaling. Each processor is
described further in Appendix B.
Flit Size(cid:3)
Request(cid:3)
Packet Size(cid:3)
Response(cid:3)
Packet Size(cid:3)
64 bits(cid:3)
1 flit (coherence requests 
and acknowledges)
5 flits (cache data)(cid:3)
Router(cid:3)
Microarchitecture(cid:3)
10 (cid:59) 64b latches per port(cid:3)
(6VCs over 2MCs)(cid:3)
Bypass Router-(cid:3)
and-link Latency(cid:3)
Operating(cid:3)
Frequency(cid:3)
Power Supply(cid:3)
Voltage(cid:3)
1 cycle(cid:3)
1GHz(cid:3)
1.1V and 0.8V
Technology(cid:3)
45nm SOI CMOS(cid:3)
Figure 2: Die photo and overview of our fabricated 4×4 mesh NoC.
We calculated zero-load latency and channel load of these networks for both unicast-only and broadcast-only trafﬁc. Zero-load
latency is calculated by multiplying the average hop-count by the
number of pipeline stages to traverse a hop, with serialization latency added on to model pipelining of all ﬂits. We computed channel load based on an ﬂit injection rate per core of R, following
the methodology of [6]. The results are shown in the Table 2. We
can see that our proposed router optimizes for broadcast (multicast) trafﬁc and has much lower zero-load latency and channel load
compared to all other networks.
TILE64 attempts to optimize for all three metrics, by utilizing independent simple networks for different message types. The simple
router design, with no virtual channels, improves unicast zero-load
latency but broadcast trafﬁc latency is poor as its lack of multicast support forces the source NIC to duplicate k2
− 1 copies of
a broadcast ﬂit and send a copy to every destination NIC. This increases channel load by k2
− 1 times, causing contention at all
routers along the shared route, making it impossible to meet the
single-cycle per hop. TILE64’s static partitioning of trafﬁc across
5 networks may also lead to poor throughput when exercised with
realistic uniform trafﬁc. Similar effect on broadcast latency and
channel load is observed for the Teraﬂops and SWIFT NoCs as
none of these chip prototypes have multicast support. The SWIFT
NoC with a single-cycle pipeline for unicasts performs better on
zero-load latency, albeit at a lower operating frequency. The TeraFLOPS NoC has poor zero-load latency in terms of cycles due to
a 5-stage pipeline, which is aggravated with broadcasts.
In the rest of this paper, we will describe how we designed a NoC
chip speciﬁcally to approach the theoretical limits.
3. PROPOSED NOC CHIP DESIGN
This section describes the design of our chip prototype. Figure 2
shows our fabricated 16-node 4x4 NoC. The network is packetswitched, and all routers are connected to network interface circuits
(NICs) to generate and receive packets. Each router has 5 I/O ports:
North, East, South, West and NIC. Each input port has two message
classes (MCs), request and response, to avoid message-level deadlocks in cache-coherent multicores.
3.1 Overview of Proposed Router Pipeline
Our design essentially evolves the original textbook router pipeline (Fig. 1) into a strawman 4-stage router pipeline tailored for
multicasts so multicasts/broadcasts do not require multiple unicast
packets to be injected. Next, we add features pushing latency towards the theoretical limit of a single cycle per hop, throughput
towards the theoretical limit of maximum channel load, and energy
towards the theoretical limit of just datapath traversal.
In the ﬁrst pipeline stage, (1) ﬂits entering the router are ﬁrst
buffered (BW). (2) Each input port chooses one output port request
(mSA-I) out of the requests from all VCs at that input port with
a round-robin logic that guarantees fair and starvation-free arbitration. Since multicast ﬂits can request multiple output ports, the
request is a 5b vector. (3) The next router VC is selected (VA) for
each neighbor from a free VC queue at each output port. These
Table 1: Theoretical Limits of a k×k mesh NoC for unicast and broadcast trafﬁc.
Metric
Average Hop Count (Haverage )
Channel Load on each bisection link (Lbisection )
Channel Load on each ejection link (Lejection )
Theoretical Latency Limit
given by Haverage
Theoretical Throughput Limit
given by max{Lbisection , Lejection }
Theoretical Energy Limit
Exbar : energy of crossbar traversal
Elink : energy of link traversal
Unicasts
(one-to-one multicasts)
Broadcasts
(one-to-all multicasts)
2(k + 1)/3
k×R/4
R
2(k + 1)/3
R, for k <= 4
k×R/4, for k > 4
2(k + 1)/3×Exbar
+ Exbar
+ 2(k + 1)/3×Elink
(3k − 1)/2, for k even
(k − 1)(3k + 1)/2k , for k odd
k2
×R/4
k2
×R
(3k − 1)/2, for k even
(k − 1)(3k + 1)/2k , for k odd
k2
× R
k2
+ (k2
×Exbar
− 1)×Elink
Table 2: Comparison of mesh NoC chip prototypes
Clock frequency
Power supply
Power consumption
Latency Metrics
Delay per hop
Zero-load latency
(cycles)
Throughput Metrics
Channel width
Bisection bandwidth
Channel load
(R:injection rate/core)
Intel Teraﬂops [10]
8×10, 65nm
5GHz
1.1-1.2V
97W
1ns
30 (unicast)
120.5 (broadcast)
39b
1560Gb/s
64R (unicast)
4096R (broadcast)
Tilera TILE64 [23]
5 8×8, 90nm
750MHz
1.0V
15-22W
SWIFT [14]
2×2, 90nm
225MHz
1.2V
116.5mW
Modeled as 8×8 networks
1.3ns
8.9-17.8ns
9 (unicast)
12 (unicast)
77.5 (broadcast)
86 (broadcast)
Modeled as 8×8 networks
5×32b
937.5Gb/s
64R (unicast)
4096R (broadcast)
64b
112.5Gb/s
64R (unicast)
4096R (broadcast)
This work
4×4, 45nm SOI
1GHz
1.1V
427.3mW
4×4 network
1-3ns
6 (unicast)
11.5 (broadcast)
3.3 (unicast)
5.5 (broadcast)
64b
512Gb/s
64R (unicast)
64R (broadcast)
4×4 network
64b
256Gb/s
16R (unicast)
16R (broadcast)
try to pre-allocate the crossbar ahead of the actual ﬂit, thus hiding
mSA-II from the router delay. The lookahead takes priority over
requests from buffered ﬂits at the next router, and directly enters
mSA-II. If the lookahead wins an output port, this pre-allocation
allows the following ﬂit to bypass the ﬁrst two pipeline stages and
go into the third stage directly, reducing the router pipeline depth
from 4 to 2. Active pre-allocation by lookaheads enables incoming
ﬂits to bypass routers at all loads, in contrast to a naive approach of
bypassing only at low-loads when the input queues are empty.
Single-cycle ST+LT with low-swing circuits. We apply a lowswing signaling technique, which can reduce the charging / discharging delay and dynamic energy when driving capacitive parasitics [20], to the highly-capacitive datapath. As will be described
later in Section 3.4, the proposed low-swing circuits obtain higher
current driving ability (or lower linear drive resistance) even at
small Vds than the reduced-swing signaling generated by simply
lowering supply voltage, and hence, our low-swing datapath enables single-cycle ST+LT at higher clock frequency. Such singlecycle ST+LT can operate at up to 5.4GHz with 1mm 0.15um-width
0.30um-space links as demonstrated with measurement results in
Section 4.3.
These two optimizations achieve a single-cycle-per-hop delay
for unicasts and multicasts, exactly matching the theoretical latency
limits. The caveat is that in case of contention for the same output
port from multiple lookaheads, one of them will have to be buffered
and then forced to go through the 3-stage pipeline. In addition, critical path delay is stretched, which will be analyzed in Section 4.
3.3 Towards Theoretical Throughput Limits
We take two steps towards the throughput limit for both unicasts
and broadcasts (1) multicast support inside routers, and (2) singlecycle hop latency for fast buffer reuse.
Multicast support inside routers. We design a router that can
replicate ﬂits, allowing one multicast/broadcast ﬂit to be sent from
the source NIC, and get routed to all other routers in the network
via a tree. This allows a broadcast ﬂit to share bandwidth till it
does not require an explicit forking into different directions. This
dramatically reduces contention compared to the baseline design
where multiple ﬂits would have be sent as unicasts which are guaranteed to create contention at along the shared routes. We use a
dimension ordered XY-tree in our design as it is deadlock free, and
simpliﬁes the routing algorithm. The ability to replicate ﬂits in
the router is implemented in the form of our broadcast-optimized
Figure 3: Proposed router microarchitecture and pipeline.
3 operations are executed in parallel without decreasing operating
frequency as they are not dependent on each other.
In the second stage, output port requests for the next routers are computed
(NRC) for the winners of mSA-I, and concurrently, a matrix arbiter at each output port grants the crossbar ports to the input port
requests (mSA-II). Multicast requests get granted multiple output
ports. In the third stage ﬂits physically traverse the crossbar (ST)
and reach the next router through the link (LT) in the fourth stage.
At this point, our strawman router can simultaneously send a
broadcast packet to all 16 nodes of a NoC. The baseline textbook
router (Fig. 1), on the other hand, needs to generate multiple unicasts at each cycle to implement the broadcast packet and such unicasts takes 4 cycles per hop. The proposed design (Fig. 3) will
completely be described through the following subsections.
3.2 Towards Theoretical Latency Limits
We push our strawman towards the limit by adding two key features: (1) virtual bypassing [15–17] to remove/hide delays due to
buffering and arbitration and (2) low-swing circuits on the datapath
to achieve single cycle ST+LT without lowering clock frequency.
Single-stage pipeline with lookaheads. In stage 2 of the strawman, we add and generate 15b lookahead signals from the results of
NRC and mSA-II, and send them to the next router. The lookaheads
Figure 4: Proposed low-swing crossbar and link circuits.
crossbar and mSA-II (switch allocation for multiple output ports).
Single-cycle-per-hop latency. The number of buffers/VCs required at every input port to sustain a particular throughput depends
upon the buffer/VC turnaround time, i.e. the number of cycles for
which the buffer/VC is occupied. This is where our optimizations
for latency in Section 3.2 come in handy here since they reduce
the pipeline depth, thus reducing buffer turnaround time, thereby
increasing throughput given the same number of buffers. For our
single-cycle pipeline, the turnaround time for buffers/VCs is 3: one
cycle for ST+LT to the downstream router, one cycle for the free
VC/buffer signal to return from the downtsream router (if the ﬂit
successfully bypassed), and one cycle for it to be processed and
ready to be used for a new ﬂit. We thus choose 4 VCs in the request message class, each 1-ﬂit deep (since requests packets in our
design are 1-ﬂit wide) to satisfy VC turnaround time and sustain
high throughput for broadcasts. We chose 2 VCs in our response
message class, each 3-ﬂit deep, for the 5-ﬂit response packets. This
number was chosen to be less than the turnaround time to shorten
the critical path, and reduce the total buffers (which increase power
consumption). We thus chose a total of 6 VCs per port, with a total
of 10 buffers.
3.4 Towards Theoretical Energy Limits
Section 2 reveals a signiﬁcant energy gap between the baseline
router energy and the theoretical energy limit (which is just clocking and datapath energy, Exbar and Elink ). Such a gap is due to
buffering energy (Ebuf f ), arbitration logic energy (Earb ) and silicon leakage energy (Elkg ). Conventionally, these energy overheads are traded off against latency and throughput as follows: (1)
Fewer buffers reduce Ebuf f and Elkg , but stretch latency due to
contention and lower throughput. (2) Simple routers like wormhole
routers reduce Earb and Elkg , and increase operating frequency f ,
but these come at the expense of poorer latency and throughput.
Our proposed NoC ﬁrst includes multicast support so even broadcasts and multicasts can approach the theoretical energy limit. Then,
it incorporates two new features that permits different tradeoffs of
latency, throughput and energy. First, our multicast virtual bypassing reduces Ebuf f , while improving both latency and throughput.
The hidden cost lies in increased Earb and decreased f . As shown
in Section 4.1, the savings in Ebuf f outweigh the Earb overheads,
and operating frequency can still be in GHz. Second, our chip employs low-swing signaling to reduce dynamic energy in the datapath (Exbar and Elink ) which is unavoidable and part of the theoretical energy limit. Low-swing signaling provides an opportunity to
break the conventional trade-offs that achieve dynamic energy savings at the cost of latency and throughput penalties; In fact, lowFigure 5: Throughput-latency performance evaluation with mixed
trafﬁc at 1GHz.
swing optimizes both energy and latency. Its downsides lie in its
area overheads and reduced process variation immunity.
Figure 4 shows the circuit implementation of the low-swing crossbar directly connected to links with tri-state reduced-swing drivers
(RSDs). This crossbar enables low-swing signaling in the datapath
(crossbar vertical wires and links). The tri-state RSD disconnects
horizontal and vertical wires and only drives the corresponding vertical wire and link, thereby providing energy-efﬁcient multicasting
capability. With an additional supply voltage (LVDD), the 4-PMOS
stacked RSD design generates more reliable low-swing signaling
in the presence of wire capacitance and resistance variation than
equalized interconnects [9, 13, 18] where low-swing signaling is
obtained by wire channel attenuation. A delay cell aligns an input signal (which drives only a 1b crossbar) to an enable signal
(which drives all of 64 1bit crossbars). It reduces mismatch between charging and discharging time, thus decreasing inter-symbol
interference (ISI). The 64bits links are designed with 0.15um-width
0.30um-space fully shielded differential wires, to eliminate noise
coupling of crosstalk effects and supply voltage variation.
4. EVALUATION
In this section, we ﬁrst evaluate the measured energy-latencythroughput of our fabricated NoC against that of the baseline mesh
and theoretical limits deﬁned in Section 2. Armed with our chip
measurements, we then delve into three speciﬁc case studies on
virtual bypassing, low-swing signaling and power modeling and
estimation to dissect our design choices.
4.1 Energy-Latency-Throughput Performance
We measured average packet latency of our NoC as a function
of packet injection rate, with two different trafﬁc patterns: mixed
trafﬁc (50% broadcast request, 25% unicast request and 25% unicast response messages) and broadcast-only trafﬁc (100% broadcast request messages), at 1GHz operating frequency. For brevity,
Figure 5 only shows the results for mixed trafﬁc along with the
baseline performance and theoretical mesh limits. Here, we chose
a more aggressive baseline that has single-cycle ST+LT instead of
separate ST and LT stages shown in Fig. 1. Since even the fullswing baseline can support single-cycle ST+LT at 1GHz, this baseline is a fairer model of an equivalent unicast full-swing NoC. Except for the the single-cycle ST+LT, the baseline used in this section is identical as that described in Section 2.1. The theoretical
latency limits (cycles/packet) include two extra cycles for NIC-torouter and router-to-NIC traversals which are indispensable since
trafﬁc injects and ejects through the NICs. Theoretical throughput
(cid:36)(cid:3)
(cid:37)(cid:3)
(cid:39)(cid:3)
A: full-swing unicast network
B: low-swing unicast network
C: low-swing broadcast network 
   (w/ router-level broadcast support)
   (w/o multicast buffer bypass)
D: low-swing broadcast network
   (w/ router-level broadcast support)
    (w/ multicast buffer bypass)
(cid:38)(cid:3)
8
4
.
3
%
o
p
w
e
r
r
c
u
d
e
i
t
n
o
i
a
d
n
t
a
p
a
t
h
y
b
r
t
i
s
t
a
t
e
D
S
R
c
d
e
s
a
b
r
a
b
s
s
o
r
s
3
1
.
9
%
o
p
w
e
r
r
c
u
d
e
i
t
n
o
i
n
r
u
o
t
e
r
l
c
g
o
i
y
b
r
u
o
t
e
r
l
e
v
e
l
b
r
s
a
c
d
a
o
t
o
p
p
u
s
t
r
2
3
.
2
%
o
p
w
e
r
r
c
u
d
e
i
t
n
o
i
u
b
n
f
f
e
r
s
y
b
m
u
i
t
l
s
a
c
t
u
b
f
f
e
r
s
s
a
p
y
b
8
3
.
2
%
o
p
w
e
r
r
c
u
d
e
i
t
n
o
i
n
t
o
t
a
l
Clocking Circuit
Router logic and buffer
Data path (crossbar + link)    
137
67
494
425
288
Figure 6: Measured power reduction at 653Gb/s at 1GHz.
limits are calculated based on received ﬂits, then converted into
Gb/s to factor in the 1GHz clock frequency and 64-bit ﬂit size
(16×64b×1/1GHz=1024Gb/s). Simulation results were obtained
from pre-layout synthesis with sufﬁcient simulation cycles (104 cycles) to make scan-chain warmup (128 cycles) negligible.
For latency, our design enables 48.7% (mixed trafﬁc) and 55.1%
(broadcast-only) reductions before the network saturates1 as compared to the baseline. The low-load latency gap from the theoretical latency limit is 5.7 (6.3) cycles for mixed (broadcast) trafﬁc,
i.e. only 1.03 (1.14) cycles of contention latency per hop for mixed
(broadcast) trafﬁc. This can be further improved to 0.04 (0.05) cycles of contention latency per hop (obtained through RTL simulations) by removing the artifact in our chip whereby all NICS had
identical pseudo-random generators that caused contention which
lowers the amount of bypassing even at low injection rates.
Throughput wise, the fabricated NoC approaches the theoretical limits: 87% (mixed trafﬁc) and 91% (broadcast-only) of the
theoretical throughput limits. In addition, our NoC design has 2.1x
(mixed trafﬁc) and 2.2x (broadcast-only) higher saturation throughput than the baseline. In other words, the proposed NoC can obtain
the same throughput as the baseline with fewer buffers or VCs.
The throughput gap between the theoretical mesh and the fabricated chip is due to imperfect arbitration (like all prior chips, we
use separable allocators, mSA-I and mSA-II, to lower complexity)
and routing (XY routing can lead to imbalance in load).
Figure 6 shows the measured power reduction at 653Gb/s broadcast delivery at 1GHz at room temperature. The low-swing signaling enables 48.3% power reduction in the datapath. In addition,
the single-cycle multicast capability and virtual bypassing result
in 13.9% and 32.2% power reduction in router logics and buffers,
respectively. Overall, our chip prototype achieves 38.2% power
reduction compared to the baseline. To compare against the theoretical power limit, we performed a post-layout power simulation of a router in the middle of the mesh to further breakdown
data-dependent power from non-data-dependent components like
clocking. We then calculate the theoretical power limit to comprise
just clocking and a full-swing datapath: 5.6mW/router, at close to
zero-load injection rate (3/255). Compared to our NoC power consumption at the same low injection rate (13.2mW/router), our overhead comes largely from VC bookkeeping state (1.9mW/router)
and buffers (2.0mW/router), whereas the allocators (0.7mW/router)
and additional lookahead signals (0.2mW/router) contribute little
additional power. The data-dependent power (e.g. buffers, allocators) is due to our identical PRBS generators at NICs that limited bypassing at low loads and can be removed by virtual bypassing, but the non-data-dependent power (e.g. VC state) will remain.
Also, since our chip consumes nontrivial leakage power (76.7mW
measured, 18% of overall chip power consumption at 653Gb/s),
power gating will help to further close the gap, at the expense of a
decrease in operating frequency.
1To enable precise comparisons, we deﬁne the saturation point as
the injection rate at which NoC latency reaches 3 times the average
no-load latency; most multi-threaded applications run within this
range.
Pre-layout simulations
Baseline router design
Our virtual bypassed router design
Post-layout simulations
Baseline router design
Our virtual bypassed router design
Measured critical path
Our virtual bypassed router design
549ns
593ns (1.08x overhead)
658ns
793ns (1.21x overhead)
961ns (1/1.04GHz)
Table 3: Critical path analysis results.
Figure 7: Measured energy efﬁciency of the proposed low-swing
circuit on pseudo-random binary sequence data.
4.2 Virtual bypassing
Virtual bypassing of buffering to achieve single-cycle routers has
been proposed in various forms [3, 15–17] in research papers. The
aggressive folding of multiple pipeline stages into a single cycle
naturally raises the question of whether that comes at the expense of
router frequency f . While our chip is the ﬁrst prototype to demonstrate a single-cycle virtual bypassed router at GHz frequency, it
begs the question of how much f is affected. To quantify the timing overhead, we performed critical path analysis on pre- and postlayout netlists of the baseline and our design. Table 3 shows such
estimates along with the actual measured timing.
The critical paths of both the baseline and the proposed router
occur in the second pipeline stage where mSA-II is performed. The
overhead of lookaheads lengthens the critical path by 8% in prelayout simulations and 20% in post-layout simulations. It should
be pointed out though that if the operating frequency is limited by
the core rather than the NoC router, which is typically the case,
this 20% critical path overhead can be hidden. In the Intel 48 core
chip, nominal operation is 1GHz core and 2GHz router frequencies,
allowing any network overhead to be masked [11].
Also notable is the fact that while the critical path of the postlayout simulation is 793ns, the maximum frequency of our chip
prototype is 1.04GHz (i.e. the actual critical path is 961ns). This is
mainly due to nonideal factors (e.g. a contaminated clock, supply
voltage ﬂuctuation, unexpected temperature variations, and etc.)
whose effects cannot be exactly predicted in design phase.
4.3 Low-Swing Signaling
Low-swing signaling has demonstrated substantial energy gains
in domains such as off-chip interconnects and SRAMs. However,
in NoCs, there are few chip prototypes employing low-swing signaling [2, 14]. So a deep understanding of its trade-offs and its applicability to NoCs can be useful. To investigate such effects with
longer links (necessary in a multicore processor as cores are much
larger than routers), and at higher data rates than the network clock
frequency (which is limited by synthesized router logic), an identical low-swing crossbar with longer link wires (1mm and 2mm) is
separately implemented as shown in Figure 2.
Energy savings and 1-cycle ST+LT. The measured energy efﬁciency (Fig. 7) shows that the 300mV-swing tri-state RSD consumes up to 3.2x less energy as compared to a equivalent full-swing
repeater. Experimental results also demonstrates that the tri-state
RSD-based crossbar supports single-cycle ST+LT at up to 5.4GHz
and 2.6GHz clock frequency with 1mm and 2mm links, respectively. The tri-state RSDs enables a reduction in the total amount
of charge and delay required for data transitions, thereby resulting
in these energy and latency beneﬁts.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Synthesized full-swing crossbar
Proposed low-swing crossbar
Router with the full-swing crossbar
Router with the low-swing crossbar
26,840um2
83,200um2 (3.1x overhead)
227,230um2
318,600um2 (1.4x overhead)
Table 4: A"
High radix self-arbitrating switch fabric with multiple arbitration schemes and quality of service.,"A scalable architecture to design high radix switch fabric is presented. It uses circuit techniques to re-use existing input and output data buses and switching logic for fabric configuration and supporting multiple arbitration policies. In addition, it integrates a 4-level message-based priority arbitration for quality of service. Fine grain clock gating, tiled fabric topology and self-regenerating bit-line repeaters enable scaling the router to 8k wires. A 64×64(128b data) switch fabric fabricated in 45nm SOI CMOS spans 4.06mm
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 and achieves a throughput of 4.5Tb/s at 3.4Tb/s/W at 1.1V with a peak measured efficiency of 7.4Tb/s/W at 0.6V.","High Radix Self-Arbitrating Switch Fabric with Multiple 
Arbitration Schemes and Quality of Service 
Sudhir Satpathy, Reetuparna Das, Ronald Dreslinski, Trevor Mudge, Dennis Sylvester, David Blaauw 
University of Michigan, Ann Arbor 
sudhirks@umich.edu
ABSTRACT 
A scalable architecture to design high radix switch fabric is 
presented. It uses circuit techniques to re-use existing input and 
output data buses and switching logic for fabric configuration and 
supporting multiple arbitration policies. In addition, it integrates a 
4-level message-based priority arbitration for quality of service. 
Fine grain clock gating, tiled fabric topology and self-regenerating 
bit-line repeaters enable scaling the router to 8k wires. A 
64×64(128b data) switch fabric fabricated in 45nm SOI CMOS 
spans 4.06mm2 and achieves a throughput of 4.5Tb/s at 3.4Tb/s/W 
at 1.1V with a peak measured efficiency of 7.4Tb/s/W at 0.6V. 
Categories and Subject Descriptors 
B.4.0 [INPUT/OUTPUT and Data Communications]: General 
General Terms 
Algorithms, Design 
Keywords 
Switch Fabric, Radix, Arbitration, Quality of Service 
1. INTRODUCTION 
Technology scaling has made billion transistors design feasible on 
a single die. With transistors getting cheaper and faster, the core 
count in multi-processor systems has been steadily increasing 
[1,2]. High end servers [3], gigabit Ethernet routers [4,5] and 
multimedia processors [6,7] now serve workloads dealing with 
terabytes of data flow every second. Even medium throughput 
applications now prefer multi-core architectures over a single core 
implementation for better energy efficiency and fault tolerance 
[8]. These system need a network to communicate data among 
processing and storage elements in the chip. Although processing 
units are getting smaller and simpler, the dramatic rise of their 
number in a single die has resulted in the growing complexity of 
interconnect. As a result, the interconnect fabric has become a 
bottleneck in improving overall system efficiency. Thus, the 
design paradigm for multi-core chips is gradually shifting from a 
core-centric 
architecture 
towards 
an 
interconnect-centric 
architecture, where overall system performance is limited by the 
bandwidth of the interconnect fabric rather than the processing 
ability of any individual core. 
 A generic switch fabric comprises of three key modules: 1) A 
data routing module to transfer information among different IPs 
connected to the fabric. This could be a single or a collection of  
Permission to make digital or hard copies of all or part of this work for  
personal or classroom use is granted without fee provided that copies are  
not made or distributed for profit or commercial advantage and that copies  
bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee.  
DAC 2012, June 3-7, 2012, San Francisco, California, USA.  
Copyright 2012 ACM 978-1-4503-1199-1/12/06...$10.00 
shared buses 
for systems where processing units 
rarely 
communicate [9] or could be a fully connected crossbar [10]. 2) 
An arbiter that receives requests from processing units and 
configures the fabric to ensure data sent from a source reaches the 
appropriate destination. 3) A priority management module that 
monitors traffic flow pattern within the fabric and assists the 
arbiter to ensure fairness in resource allocation. The overall 
efficiency of the switch fabric relies on how efficiently each of 
these independent modules function and how seamlessly they 
communicate among one another. With a growing number of 
input and output ports, each module gets physically bigger and 
hence farther from the others. Beyond a size, the latency and 
energy overhead due to communication between these modules 
starts limiting overall fabric efficiency. Existing circuit techniques 
to build high radix switch fabrics rely on assembling together 
smaller switches that are usually 5×5 in dimension [11]. This 
approach has certain limitations: 1) The elementary switches are 
built using multiplexers that select bits rather than buses. Hence, 
as buses get wider, routing at the fabric input ports involves a lot 
of interleaving among the wires incurring additional area penalty. 
2) A switch with ports far exceeding 5 would require multiple of 
these switches to be connected in stages. Data has to traverse 
through multiple stages to reach its destination thereby increasing 
latency and energy dissipation. They would also require additional 
data storage elements in the data routing path for higher 
throughput resulting in further latency and power overhead. 3) 
Each source sends requests to the arbiter before it could access a 
destination, and eventually the arbiter sends an acknowledgement 
back to the source after setting up the routing path. Configuring 
all the switches along the routing path incurs latency. In systems 
that have well defined communication patterns and usually 
operate on massive sets of data, the latency and energy cost for 
configuring the fabric can be amortized by setting up the routing 
path ahead of time or by sending multiple chunks of data once the 
path is established. However, in generic multiprocessor systems 
most traffic patterns are not pre-defined. Hence, the fabric 
configuration cost becomes a bottleneck. 4) A non-blocking 
switch supports all possible permutations and hence can guarantee 
starvation free communication for all applications. However many 
applications (like FFT, LDPC, Color space conversion etc.) can be 
sped up significantly by incorporating multicast and broadcast 
features in the switch fabric [12].  
Arbitration policies have a noticeable impact on throughput and 
fairness of interconnection networks. Some arbitration policies 
like the greedy allocation policy tend to maximize network 
throughput at the cost of quality of service. At the other extreme, 
some complex schemes like probabilistic distance [13] based 
arbitration can guarantee quality of service at the price of more 
complex logic and hence additional arbitration latency and power 
overhead. Hence, adaptive and hybrid resource allocation schemes 
are preferred in general over static schemes because of their 
ability to mitigate congestion.  
 
 
 
 
 
In this paper, we propose a fabric architecture called swizzle 
switch network (SSN) to accomplish a variety of arbitration 
policies with very minimal overhead as shown in Fig. 1. For proof 
of concept, we fabricated a 64x64 SSN prototype with 128b data 
bus in 45nm SOI CMOS with the following key features: 1) A 
novel, single cycle 
least recently granted (LRG) priority 
arbitration technique that re-uses the already present input and 
output data buses and their drivers and sense amps. 2) An 
additional 4-level message-based priority arbitration for quality of 
service (QoS) with 2% logic and 3% wiring overhead. 3) A new 
bidirectional bit-line repeater that allows the router to scale to 
>8000 wires. These features result in a compact fabric (4.06mm2) 
with throughput gain of 2.1× over [5] at 3.4Tb/s/W efficiency 
which improves to 7.4Tb/s/W at 600mV. 
discharge of priority line m, suppressing access by input m. In 
contrast, input l stores a 0 at its m-th priority bit and hence does 
not suppress an access request from input m, meaning that l has 
lower priority that input m. 
Priority vectors need to be set consistently and indicate the same 
priority order. In Fig. 2, the 0 at bit m of input l must be mirrored 
with a 1 at bit l of input m. Furthermore, the priority bits need to 
be correctly updated after each arbitration cycle to implement 
LRG policy. We propose a new, simple mechanism to accomplish 
this. In Fig. 1, inputs l and m request the output channel in an 
arbitration cycle. Input m wins owing to its higher priority and its 
Figure 1. Swizzle Switch Network 
The rest of the paper is organized as follows: In section 2, we 
present the SSN fabric architecture. Section 3 explains the various 
arbitration policies that can be implemented in SSN. In section 4, 
we present SSN’s message based QoS arbitration scheme. 
Detailed circuit level implementation and design choices are then 
presented in section 5. Measurement results from SSN test 
prototype fabricated in 45nm have been described in section 6 
before conclusion in section 7. 
2. SSN ARCHITECTURE 
SSN is a matrix-type fabric as shown in Fig. 1 with input buses 
running horizontally and output buses vertically. When data is 
routed, the input and output buses transfer data traffic. During 
arbitration the input bus routes a multi-hot code indicating which 
output channel(s) are requested by that input, and the output bus is 
used for conflict detection and arbitration [14]. Each crosspoint 
stores a connectivity status bit indicating whether the input bus 
was granted access to the output channel. An n-1 bit priority 
vector is also stored to represent the priority of the input bus with 
respect to all other inputs for that output bus. Fig. 1 shows the 
priority vector at each crosspoint in a blow-up of a single output 
channel. Each input bus is assigned a un ique bit line from the 
channel as its priority line which, if high, indicates it as the 
winner in a particular arbitration cycle. Similarly, each bit in the 
priority vector at a crosspoint indicates whether the input bus at 
that crosspoint has higher or lower priority than the input bus 
associated with the priority line. For instance, in Fig. 2 priority 
line m corresponds to input bus m while the m-th priority bit of 
bus n is a 1, indicating that n has higher priority than m. When 
input n requests the output channel this high bit results in the 
Figure 2. Arbitration and priority update. 
connectivity status bit is set to 1. After data transfer, input m 
releases its channel during a channel release cycle. In this cycle, 
input m first resets all its priority bits. This guarantees that m now 
has lowest priority, as required by the LRG algorithm. At the 
same time, input m also lowers its priority line m, which is a 
signal to other crosspoints in the output channel to set their m-th 
priority bit. This ensures that all other input buses now have 
higher priority than m. Input buses with higher priority than m 
remain unchanged and only inputs with lower priority than input 
m are increased in their priority by exactly one level. This simple 
and fast update mechanism provably guarantees both consistency 
of all priority vectors and correct implementation of the LRG 
arbitration scheme, which enables efficient and deadlock-free 
routing. 
3. ARBITRATION SCHEMES 
The SSN is capable of supporting multiple arbitration schemes. 
The priority vectors at various crosspoints along a single output 
bus form a priority matrix. A priority matrix with 6 inputs 
(arranged top to down numerically from input1 to input6) is 
shown in Fig. 3. Here, the priority line connections are denoted as 
Xs. The priority matrix satisfies the following criteria: 1) The total 
number of 0s equals the total number of 1s. 2) Each row has a 
unique number of 1s which represents the corresponding input’s 
priority. 3) Each column has a unique number of 1s. 4) At any 
priority line connection (denoted as X in Fig. 3.), the sum of the 
number of 1s (or 0s) in its corresponding row and column must 
add to one less than the total number of inputs. 
3.1 Least Recently Granted (LRG) scheme 
Though priority lines can be randomly assigned to inputs without 
limiting the generality of the priority update schemes, a diagonal 
assignment as shown in Fig. 3 makes the priority matrix skew (or 
anti) symmetric and easy to understand. As shown in Fig. 3, the 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
For accomplishing an incremental Round Robin based update, we 
pick the row with the highest priority. This can be identified by a 
logical AND operation of all the priority bits. In this case input5 
has the highest priority. We reset all priority bits along the fifth 
row (denoted by R) and set all priority bits (denoted by S) along 
the fifth column (which is the priority line for input5) as shown in 
Fig. 5 top. By resetting bits in the fifth row, input5 now gets the 
least priority. All other inputs get upgraded by exactly one level. 
For accomplishing a decremental Round Robin based update, we 
pick the row with the lowest priority. This can be identified by a 
logical OR operation of all the priority bits. In this case input4 has 
the highest priority. We set all priority bits along the fourth row 
(denoted by S) and reset all priority bits (denoted by R) along the 
fourth column (which is the priority line for input4 ) as shown in 
Fig. 5 bot. By setting bits in the fourth row, input4 now gets the 
highest priority. All other inputs get downgraded by exactly one 
level. 
3.4 Priority swap and reversal 
Figure 6. Priority swap (top) and priority reversal (bottom) 
The priorities of 2 inputs can be swapped (without affecting 
priorities of other inputs) by swapping the priority bits in their 
corresponding rows and those in the columns corresponding to 
their priority lines as shown in Fig. 6 top. Here, we intend to swap 
the priorities of input3 and input4. In the physical realization of 
this technique, already existing word-lines will be used to swap 
priority bits between columns and bit-lines to swap priority bits 
between rows. In a single cycle, any two priorities can be 
swapped. 
The unique priority encoding scheme also allows reversing the 
priority of all inputs instantaneously by flipping all the priority 
bits as shown in Fig. 6 bottom. In physical circuit level 
implementation, rather than flipping all the bits, a multiplexer can 
be used to select the inverted priority. Hence, this functionality 
can be achieved without the expense of a clock cycle. The 
consistency of the new priority vectors is guaranteed because this 
transformation ensures that the priority matrix still satisfies all the 
criteria mentioned before. 
Fi
Figure 3. LRG based priority update 
3 LRG b
d
i
i
d
input corresponding to second row used the channel most 
recently. It is assigned a priority level 3. An LRG priority update 
is accomplished by resetting all priority bits along the second row 
(denoted by R) and by setting all priority bits (denoted by S) along 
the second column (which is the priority line for input2). Input2 is 
thus downgraded to have the least priority while all inputs with 
lower priorities get upgraded by exactly one level. In the rest of 
this section, the other priority update schemes will be explained 
using the priority matrix notation. 
3.2 Most Recently Granted (MRG) scheme 
Figure 4. MRG based priority update 
For accomplishing an MRG based update, we set all priority bits 
along the second row (denoted by S) and reset all priority bits 
(denoted by R) along the second column (which is the priority line 
for the input2) as shown in Fig. 4. By setting bits in the second 
row, input2 now gets the highest priority. Inputs that previously 
had higher priorities than input2 get downgraded by exactly one 
level. Inputs that previously had lower priority than input2 retain 
their old priorities. 
3.3 Round Robin schemes 
Figure 5. Incremental (top) and decremental (bottom) round 
Fi
5 I
l (t
)
d d
l (b
)
d
robin based priority updates 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3.4 Selective LRG and MRG 
In this scheme LRG update is applied to a selective section of 
inputs. In the standard LRG scheme, the input that used the output 
bus most recently is downgraded to have the least priority while 
all inputs with lower priorities get upgraded by exactly one level. 
In this case input6 with a priority level 4 used the channel most 
recently. However, in the selective LRG scheme, instead of 
downgrading input6 all the way down to 0, we intend to 
downgrade it to some intermediate priority (say priority level of 
input0 which is 1). To accomplish this, before setting/resetting 
priority bits we identify certain rows and columns that need to be 
frozen. In this case, all columns corresponding to priority bits that 
are high in the first row are frozen as shown in Fig. 7 top. 
Simultaneously, all rows corresponding to priority bits that are 
low in the first column (which is the priority line for input0) are 
also frozen. Following this the priority bits in the sixth row are 
reset (except the bits in frozen columns) and those in the sixth 
column are set (except the bits in frozen rows). This ensures that 
the new priority matrix is consistent and the intended priority 
update is achieved. 
4. QoS ARBITRATION 
In a 64×64 SSN it might take a message 64 cycles to win 
arbitration in the worst case when all inputs collide. To assist 
critical messages to reach destination early, SSN also features a 4level message-based QoS arbitration technique that allows only 
input buses with the highest message priority to arbitrate for the 
channel as shown in Fig. 8. A 2-bit message priority is decoded 
into a 4-bit thermometer code at the crosspoint, which is used to 
selectively discharge priority bit-lines comprising the QoS priority 
bus. A multiplexer samples one of those priority bit-lines using its 
own message priority and the input bus progresses to the LRG 
arbitration cycle if the monitored priority bit is not discharged. 
Using separate wires for QoS arbitration incurs 3% area overhead. 
However, the additional QoS arbitration cycle can be overlapped 
with the prior routing operation for the output bus, avoiding a 
latency penalty. 
ncy penalty. 
Figure 7. Selective LRG (top) and selective MRG (bot) 
In this scheme MRG update is applied to a selective section of 
inputs. In the standard MRG scheme, the input that used the 
output bus most recently is upgraded to have the highest priority 
while all inputs with higher priorities get downgraded by exactly 
one level. In this case input1 with a priority level 1 used the 
channel most recently. However, in the selective MRG scheme, 
instead of upgrading input1 all the way up to 5, we intend to 
upgrade it to some intermediate priority (say priority level of 
input6 which is 4). To accomplish this, before setting/resetting 
priority bits we identify certain rows and columns that need to be 
frozen. In this case, all columns corresponding to priority bits that 
are low in the sixth row are frozen as shown in Fig. 7 bot. 
Simultaneously, all rows corresponding to priority bits that are 
high in the sixth column (which is the priority line for input6) are 
also frozen. Following this the priority bits in the first row are set 
(except the bits in frozen columns) and those in the first column 
are reset (except the bits in frozen rows). This ensures that the 
new priority matrix is consistent and the intended priority update 
is achieved. 
Figure 8 QoS arbitration technique
Figure 8. QoS arbitration technique 
5. PROTOTYPE IMPLEMENTATION 
Figure 9a. Crosspoint circuit 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 9b. Priority storage latch 
Fig. 9 shows the SSN crosspoint circuit and the priority storage 
latch. Load_priority_b is an additional bit-line provided per 
channel that is discharged during the release cycle. This triggers 
the priority update mechanism. During a request/release cycle the 
channels are indexed using the lower 64 bits from the input bus. 
Crosspoints send acknowledgements over the upper 64 bits. 
Fig. 10 shows the test prototype fabricated in 45nm SOI CMOS 
with a 64×64 SSN as the communication network between the 
traffic generators and traffic analyzers. SSN is laid out using a 
semi custom design flow. A generic crosspoint cell is laid out as a 
parameterized cell. A skill script parses the generic crosspoint by 
taking in the x coordinate, y coordinate and the priority vector (at 
reset) as the arguments and generates crosspoint specific to each 
location. These crosspoints are then tiled using a compi ler that 
appropriately sizes the word line drivers and precharge transistors 
to generate the switch fabric. 
The SSN features 8448 word-lines and 8576 bit-lines spread 
across 4096 crosspoints. The integration of the LRG and QoS 
control within this fabric with very low overhead greatly improve 
SSN’s scalability and makes it possible to realize a fabric of this 
large size. In addition, new bi-directional repeaters (Fig. 11) are 
used for bit-lines that use a regenerative sensing element to 
improve delay despite high slew rates on long bit-lines. The 
proposed repeater uses a thyristor element to detect and amplify a 
transition on the bit-line. Once a transition is detected the repeater 
enters a self regeneration mode where it decouples itself from the 
slow transitioning bit-line. This allows the internal nodes in the 
thyristor to switch faster and reduces delay. The regeneration and 
self-decoupling mechanism improves bit-line delay by 32% and 
allows for a 50% smaller bit-line driver compared to a 
conventional repeater. Simulated fabric latency with increasing 
SSN size shows 1.6× performance improvements over an SSN 
with un-repeated bit-lines as shown in Fig. 11 due to the nearlinear latency increase with radix size rather than quadratic 
dependency without repeaters. Bit-lines are pre-charged within 
every 16×16 SSN macro. This improves pre-charge time by 59% 
over a similar sized lumped driver and results in more uniform 
current drawn from the power grid. Bit-line delay degrades more 
rapidly than word-line delay under voltage scaling. Hence, the bitcell aspect ratio (1:0.73) is chosen to shorten bit-lines, improving 
fabric latency at low Vdd. Fine grain clock gating reduces clock 
power by 94% at each crosspoint. A crosspoint is clocked only if 
its connectivity status is ON, a request is asserted, or an LRG 
priority update occurs. These events are registered in the positive 
clock phase, allowing gating of the negative (active) phase with 
2.3% delay penalty. Adjacent SSN input ports are driven from 
opposite directions, reducing routing congestion and local Ldi/dt 
drop when repeaters on the 2.5mm long word-lines switch.  
g
Figure 10. SSN die photo (top) and printed circuit board 
hosting SSN test prototype (bottom) 
Figure 11. Self-regenerating bit-line repeater improves SSN 
delay by 1.6× 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6. MEASUREMENT RESULTS 
Figure 12. Measured performance and power for 64×64 SSN 
The traffic generators in the test prototype can be tuned to 
produce traffic patterns with varying switching activity and 
collision patterns. The SSN is tested for functionality by 
streaming various data streams through it and verifying the 
signatures. Fig. 12 (top) shows the measured SSN’s operating 
frequency and the aggregate throughput at varying supply voltage. 
SSN’s power consumption at different operating frequencies is 
shown in Fig. 12 (bottom). At 1.1V, the SSN operates at 559MHZ 
with a throughput of 4.47Tb/s while consuming 1.32W. This 
translates into an efficiency of 3.4Tb/s/W which is which is 3.7× 
higher than [4] at similar bandwidth. The work in [4] uses an 8×8 
mesh topology based on 5×5 routers at each node to connect 64 
units, whereas the SSN uses a 64×64 single-stage fabric. The SSN 
is fully functional down to 550mV with a measured peak 
efficiency of 7.4Tb/s/W at 0.6V . 
7. CONCLUSION 
In this paper, we present a self-arbitrating fabric called SSN that 
leverages a novel priority encoding scheme that re-uses existing 
logic and interconnect resources in switch fabric to locally store 
priorities at 
router crosspoints 
resulting 
in a compact 
implementation. A 64×64 SSN with 128b data bus achieves a 
peak throughput 4.5Tb/s at an energy efficiency of 3.4Tb/s/W 
while spanning only 4.06mm2 in 45nm SOI CMOS. It features a 
single cycle least recently granted arbitration technique that reuses data buses and switching logic, a 4-level message based 
priority arbitration for quality of service and unique bidirectional 
bit-line repeaters to aid scalability. The unique priority encoding 
scheme also allows seamless implementation of many other 
arbitration policies in addition to LRG with very minimal 
overhead. 
8. ACKNOWLEDGEMENTS 
The authors gratefully acknowledge funding and support from 
ARM Ltd.  
9. "
A hybrid NoC design for cache coherence optimization for chip multiprocessors.,"On chip many-core systems, evolving from prior multi-pro cessor systems, are considered as a promising solution to the performance scalability and power consumption problems. The long communication distance between the traditional multi-processors makes directory-based cache coherence protocols better solutions compared to bus-based snooping protocols even with the overheads from indirections. However, much smaller distances between the CMPcores enhance the reachability of buses, revitalizing the applicability of snooping protocols for cache-to-cache transfers. In this work, we propose a hybrid NoC design to provide optimized support for cache coherency. In our design, on-chip links can be dynamically configured as either point-to-point links between NoC nodes or short buses to facilitate localized snooping. By taking advantage of the best of both worlds, bus-based snooping coherency and NoC-based directory coherency, our approach brings both power and performance benefits.","A Hybrid NoC Design for Cache Coherence Optimization
for Chip Multiprocessors ∗
Hui Zhao, Ohyoung Jang,
Wei Ding
Depar tment of Computer
Science and Engineering, The
Pennsylvania State University
{ hzz105, oyj5007,
wzd109}@cse.psu.edu
Yuanrui Zhang
Intel Inc.
{yuanrui.zhang}@intel.com
Mahmut Kandemir, Mary
Jane Irwin
Depar tment of Computer
Science and Engineering, The
Pennsylvania State University
{kandemir,
mji}@cse.psu.edu
ABSTRACT
On chip many-core systems, evolving from prior multi-pro
cessor systems, are considered as a promising solution to the
performance scalability and power consumption problems.
The long communication distance between the traditional
multi-processors makes directory-based cache coherence protocols better solutions compared to bus-based snooping protocols even with the overheads from indirections. However,
much smaller distances between the CMP cores enhance the
reachability of buses, revitalizing the applicability of snooping protocols for cache-to-cache transfers. In this work, we
propose a hybrid NoC design to provide optimized support
for cache coherency. In our design, on-chip links can be dynamically conﬁgured as either point-to-point links between
NoC nodes or short buses to facilitate localized snooping.
By taking advantage of the best of both worlds, bus-based
snooping coherency and NoC-based directory coherency, our
approach brings both power and performance beneﬁts.
Categories and Subject Descriptors
B.4.3 [Interconnections (Subsystems)]: Topology; C.1.2
[Multiple Data Stream Architectures (Multiprocessors)]: Interconnection architectures
General Terms
Design, Management, Performance
Keywords
Multi-core, NoC, Cache Coherence, Bus
1.
INTRODUCTION AND MOTIVATION
As modern fabrication technologies advance into deep submicron era, chip multiprocessors (CMPs) are moving from
multi-core to many-core architectures in order to fully take
advantage of the increasing number of transistors available
∗
This work is supported in part by NSF grants 1147388,
1152479, 1017882, 0963839, 0811687, and a grant from Microsoft.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
DAC 2012, June 3-7, 2012, San Francisco, California, USA.
Copyright 2012 ACM 978-1-4503-1199-1/12/06 ...$10.00
[1, 2]. Although in many aspects, many-core CMP systems
exhibit similarities to their predecessors, multi-processor systems, there are two ma jor diﬀerences between these two
types of systems. First, many-core systems are more constrained by the limited on-chip memories. Second, inter-core
communication latencies are greatly reduced because of the
short distance between on-chip cores. The ﬁrst diﬀerence
brings new challenges on how to design an eﬃcient memory
system with limited capacity. The second diﬀerence opens
up opportunities for reducing the memory access latencies.
A very important component of memory system design is
the cache coherence protocols. Cache coherence protocols
should not only ensure data access consistency, but should
also have low performance and energy overheads.
In this
direction, several techniques have been proposed, such as
tree-based coherence and token coherence [3, 4, 5].
In this paper, we investigate new schemes to optimize
cache coherence by taking advantage of the short communication distances in many-core CMPs. Speciﬁcally, we propose a novel network-on-chip (NoC) architecture that can
conﬁgure on-chip links into high-speed snooping buses in
order to support cache-to-cache data transfer. Conventional
multi-processor systems employ directory-based cache coherence protocols because bus-based snooping coherence is
not scalable to high core counts. Bus-based snooping is not
considered a good option for many-core CMPs due to the
similar scalability concerns. However, when an L1 access
misses in the directory-based coherence, directories have to
be accessed to obtain the sharer information, resulting in
extra delays and power overheads . We observe that, when
multiple cores are running threads of a same application,
there exist opportunities for a core to ﬁnd data sharers in
its neighborhood (nearby cores). To exploit such opportunities, we propose a scheme that connects several point-topoint links of the NoC together to form short-ranged snooping buses. When a core does not ﬁnd the requested data in
its L1 cache, it ﬁrst snoops opportunistically for a copy of
the data in the L1 caches of nearby cores. Indirections to the
directories are avoided if such snoops result in hits. Consequently, directories are accessed only when the snooping cannot ﬁnd a copy of the data. Our proposed snooping scheme
has the advantage of short latencies and low power overhead
because snooping messages are transferred on buses instead
of NoC routers.
Our other important observation is that the eﬀectiveness
of our local snooping scheme is closely related to the applicaL1 $
state
tag 0xabc
n0
S
n1
L1 $
state
0xabc
tag 0xabc
I
n2
L1 $
state
tag 0xabc
I
n3
L1 $
state
tag 0xabc
I
n0
L1 $
state
tag 0xabc
I
n1
L1 $
state
tag 0xabc
I
n2
L1 $
state
tag 0xabc
I
n3
L1 $
state
tag 0xabc
I
n20
directory
tag
0xabc
sharer
another
cluster
n20
directory
tag
0xabc
sharer
another
cluster
(a) snoop hit
(b) snoop miss
Figure 1: Snoop coherence in case of an L1 miss.
tion mappings. Fixed buses cannot exploit the communicadata from the bus and save it to its own L1 cache, concludtion locality if the data sharers are not mapped to the cores
ing a cache-to-cache data transfer.
If, on the other hand,
connected by a bus.
In order to decrease the dependence
none of the cores snooped has a valid copy of the sought
of snooping eﬀectiveness on the application mappings, we
cache line, then core 2 needs to send a request to directory
propose novel schemes to dynamically build snoop buses, in
(node 20), as in the directory-based protocols. Since our coan on-demand fashion, based on the locality of data sharers.
herence protocol involves both snooping and directory-based
We make the following contributions in this paper:
cache coherences, we need to pay extra attention to coordi∙ We propose a hybrid approach to cache coherence that
nate state changes both locally and globally. we create a
new state: shared-exclusive. This state is used to identify
employs re-conﬁgurable snooping buses to reduce the
eﬀect of application mappings on snooping eﬀectivethis locally shared and globally exclusive status. Had a core
∙ We propose dynamically constructing snooping buses
ness.
shared globally exclusive data locally, both this core and its
sharers need to change to shared-exclusive state. Otherwise,
by connecting on-chip point-to-point links together.
its state remains exclusive. In a similar manner, modiﬁed
This technique can reduce the hardware overhead withstates need to be distinguished between whether or not the
only copy in the directory’s view has actually been shared
out compromising on-chip bandwidth. To the best of
inside the snooping cluster. So, we also add a new state,
our knowledge, we are the ﬁrst one to propose such
called shared-modiﬁed to handle this situation.
∙ We provide a detailed design of the snooping bus, intechniques.
cluding the bus arbitrator and bus switch interfaces.
∙ We design clustering and bus-building algorithms to
group data sharing cores into local groups and build
short buses to facilitate the broadcasting of snooping
messages (presented in the supplemental section).
2.2 Writes and Invalidations
2. DESIGN OF CACHE COHERENCE
Our cache coherence proposal is built upon the principle
that, before sending requests to the directory, a core running parallel programs ﬁrst snoops cores in its vicinity for
shared data.
If a snooped core can provide the data requested, which we call a snoop hit, a cache-to-cache transfer
is performed between the data requester and provider. Such
cache-to-cache data transfers are made possible by connecting the point-to-point links of the NoCs to form snooping
buses. Our design of cache coherence involves two mechanisms: global directory-based protocol and local snooping
protocol. We optimistically group data-sharing cores that
are located in close distance to build a localized snooping
cluster. In such a cluster, snooping coherence is employed
to facilitate cache-to-cache data transfer, without involving
global (chip wide) directory. Globally, among diﬀerent clusters, cache coherence is maintained through structures similar to the traditional directories.
2.1 A Walkthrough Example
Figure 1 shows the detailed behavior of our coherence protocol in the case of an L1 miss. Node 2 issues a load to access
a cache line with a tag of 0xabc, but results in a miss. Instead of sending request to the directory node, it ﬁrst sends
a snooping query on the locally connected snoop bus. In this
case, three other cores are connected by the snooping bus in
the same local cluster with core 2: cores 0, 1 and 3. All other
cores search their L1 cache for the tag of the requested cache
line. If there is another core in the this cluster has a valid
copy of the cache line, for example core 0, then core 0 puts
the data on the bus. After a few cycles, core 2 can grab the
If an L1 core gets a write hit, the new state is decided
based on the cache line’s current state. For exclusive or
modiﬁed state, the operations needed are the same as in a
conventional directory based protocol. In the situation that
the returned state is shared (meaning there are shared data
copies outside its snooping cluster), an invalidation message
has to be sent to the directory to nullify all other possible
sharers. If the cache line state is shared-exclusive or sharedmodiﬁed, that means there are several copies of the cache
line in the same snooping cluster. However, globally, there
is no other copy of the data in any cores outside this core’s
snooping cluster. As a result, the core only needs to broadcast locally to invalidate other sharers in the same cluster
before modifying the data. In this case, no involvement of
the global directory is needed and an indirection to directory
is avoided.
In the only remaining case where an L1 write returns a
miss, our coherence protocol also tries to avoid indirections
to directories. Before going to the directory to fetch the
data, the requesting core ﬁrst broadcasts a message to local
cores in the same snooping cluster. If there is any core that
has the data in shared state, we invalidate local sharers and
access the directory to invalidate global sharers. However, if
there are other cores with the data in Modiﬁed or Exclusive
state, indicating that the only one copy of data of the whole
system is in the same cluster, then there is no need to go
to the directory to get this information. Instead, the core
can invalidate the other copy, and change its own state to
be modiﬁed. If there exist some cores in the same snooping
cluster with the state of shared-exclusive or shared-modiﬁed,
this means all the copies of the data line is in this cluster.
Therefore, the requester can invalidate all other local sharers
and change its own state to be modiﬁed and no message
needs to be sent to the directory.
0
4
8
1
5
9
12
13
2
6
10
14
3
7
11
15
Bus Switch/Router
NoC P2P Links
Snooping Bus
N
R
Controllable
Switch
W
NIC
S
Shared link
S
E
(a) Link sharing
between bus and
NoC.
Figure 2: Proposed hardware design
(b) Bus switch/router
design.
2.3 Organizing Local Clusters in the Directory
In the conventional MESI protocols, silent evictions are
performed of exclusive and shared lines in order to avoid
the bandwidth overhead of notifying the directory. In our
proposed protocol, when replacing an L1 line, silent evictions are still valid for shared, exclusive and shared-exclusive
states. However, for the shared-modiﬁed state, we need to
ensure that modiﬁed data are written to the next level cache
properly. Our policy for replacing cache lines with sharedmodiﬁed state is as follows: the cache ﬁrst needs to snoop
sharers in local cluster; if a sharer is found, then the line
can be evicted silently.
If there is no other cores holding
the line in the same cluster, i.e., this is the only core holding the modiﬁed data in the cluster, a writeback needs to
be performed just like replacing a cache line with modiﬁed
state.
In our design, we assume a cluster is a group of cores
that are running threads that belong to the same application. When a new application’s threads are mapped to CMP
cores, we create local clusters of cores running that application’s threads. Such cluster structures will exist until the
application ﬁnishes its execution. In traditional directorybased cache protocols, there is a bit map for each cache
line associated with one directory. If a core’s L1 cache has
the valid data of the cache line, the corresponding bit is set
to 1. In our protocols, on the other hand, since we group
cores that possibly share data into clusters, each cluster only
needs one representative in the directory. From the directory’s point of view, the cores inside the cluster behave like
just one core because all the copies of data are consistent
inside a cluster. That is, either there is only core having the
data inside the cluster, or all copies are exactly same.
3. CONSTRUCTING SNOOPING BUSES
FROM NOC LINKS
3.1 Reusing NoC links to build snooping buses
Buses are widely used in oﬀ-chip interconnections. However, with the growing discrepancy between wire delay and
gate delay, buses are not scalable to be employed in manycore systems. Consequently, for on-chip communications,
packet switched network-on-chips are considered a better
solution. However, the power consumptions of such NoC
networks can be very high as the routers are power hungry,
due to their inside components such as buﬀers and crossbars.
It has been observed that the on-chip routers can consume
up to 40% of the network power [10].
Our belief is that, even if buses are not suitable to be used
as global connections, there are still advantages to employ
them as local connections. When applied to a subarea of
the NoC network, buses have the advantages of short delays
and lower energy consumptions compared to router based
NoCs. If fact, there have been several previous works that
have reconsidered the application of buses to connect on-chip
cores [8, 10, 11].
In this work, we propose to use dynamical ly conﬁgured
buses to support localized cache snooping. The advantage
of doing so is three fold: ﬁrstly, buses have inherent ordering property which can support cache access coherency;
secondly, buses can provide simultaneous broadcasting without accessing the cores one after another in a sequential
manner; and thirdly, buses can reduce transaction delays
and consume less power compared to the routers.
In this
work, our baseline network is point-to-point link connected
router-based NoC. We expand the functionality of routers
by adding bus switches to them so that several links can
be connected together to form a short-ranged snooping bus.
At one time, the on-chip links can be used either to transfer
data packets between routers or to form a short snooping
bus. As shown Figure 2(a), cores 12, 13, 9, 10 and 6 are
committing a snooping bus transaction on the dotted lines.
At the same time, all other links in the NoC are available
to transmit packets between the routers. In a conventional
NoC, the links connecting routers are unidirectional, and as
a result, one router can both send and receive packets in one
direction at the same time. In our proposed design, when
links are connected to build snooping buses, all the wires
are used as bi-directional links. Consequently, the snooping
buses have bandwidth doubled compared to the point-topoint links which can oﬀset the delays incurred by increased
length of the buses. By reusing the on-chip links between
buses and NoC links in a time-division multiplexing manner, our design can reduce the hardware overhead without
compromising the NoC bandwidth.
3.2 Bus Switch Design
In order to connect NoC links to build snooping buses, we
need to extend the routers to provide bus switching functionality. Figure 2(b) shows our proposed link interconnection
interface design. There are two ma jor components: packet
routing component (R) and bus switching component (S).
The routing component is similar to the conventional NoC
routers consisting of buﬀers, routing logic and a crossbar.
The bus switching component is a programmable switch that
can connect each segment (an NoC link) to form a bus. Links
from each direction connected to our Routing/Bus Switching
unit are controlled by programmable switches. As shown in
Figure 2(b), links in the north and west directions are programmed to be used by a bus, whereas links in the south and
east directions can serve as NoC links. When links are used
as part of a bus, packets need to stay inside the router until
the bus transaction is over. This is similar to the scenario
in the conventional NoC routers where multiple packets are
competing for the same output link. Packets failed to be
granted the output link have to stay inside the buﬀer for
the next chance. Our scheme uses the same mechanism to
hold data packets when their output links are used by the
bus.
3.3 Bus Arbiter Design
Another essential component of a bus design is the arbiter. The reason buses can work as a mechanism to provide transaction ordering is because the users need to take
order to control a bus. Such arbitrations of bus control are
Arbitrer
bus grant n
bus grant 0
0
req
0
1
req n
2
n
bus request
busy
bus grant
0
1
2
n
(a) centralized arbitration
(b) decentralized arbitration
Figure 3: Traditional centralized bus arbiter design and our proposed decentralized bus arbiter design
implemented through bus arbiters. A typical arbitrator design is shown in Figure 3(a). Each core connected to a bus
ﬁrst send requests to the arbiter. The arbiter then makes
a decision based on some priority policies to grant the bus
control to only one requester.
It is easy to build such an
arbiter if the bus conﬁguration is ﬁxed. However, this type
of centralized arbiters do not ﬁt in our reconﬁgurable environment since the bus topology is changing from time to
time, based on the applications’ mappings.
We propose to employ de-centralized bus arbitration mechanisms [13]. As shown in Figure 3(b), there is no centralized
arbiters. Instead, the arbitration logic is distributed across
all bus switches and is also reconﬁgurable. The bus grant
input indicates that the bus can be granted to a requestor.
Bus busy indicates whether the bus is being used by some
device and the bus request line shows if another device has
made a request. Each device needs to make a request ﬁrst.
If the bus busy is negative, then the device negate its own
bus grant out signal and wait to see if its grant IN signal is
asserted. If so, the device can grab the bus and assert the
bus busy signal. The bus grant out signal also needs to be
asserted so other devices can have their grant in to be set in
order to compete in the next bus arbitration. More detailed
design description can be found in [13].
Similar to the data lines of the snooping buses, the bus arbitration control lines are segmented and can be programmed
through the bus switches. The ma jor arbitration lines are
the request, busy and grant lines, as illustrated in Figure 3.
Each data link is associated with a set of such segmented
control lines. At the time the data links are conﬁgured to
build connected buses, the corresponding control lines are
connected in the same way to build a separate arbitration
network. Then, each core can send its requests across the
arbitration logic in a decentralized manner. All the arbitration logics on a snooping bus work together to decide the
winner of the bus control. Some components in our proposed
scheme incur extra area overhead, such as the bus switches
and the bus arbitration logics. However, our area overhead
comes from simple logics and wires which is small compared to the rather complicated router structures (buﬀers
and crossbars).
4. EXPERIMENTAL EVALUATION
4.1 Experimentation Setup
We evaluate our proposed techniques using a trace-driven,
cycle-accurate CMP simulator that has a built in NoC network. We use GEMS [14] to generate traces from SPLASH
[15] and SPEC OMP [16] benchmarks and feed the traces
into our CMP simulator. Application threads are randomly
mapped to CMP cores. Our baseline architecture has 64
cores organized as a 2D 8 by 8 mesh. Each NoC node consists of a core, a private write-back L1 cache and a tiled L2
bank. The default memory hierarchy is a two-level directorybased MESI cache coherence protocol. Each router has two
pipeline stages with the input buﬀer depth of four. Our
NoC employs wormhole switching and virtual-channel ﬂow
Fixed bus with length of 4
Dynamic bus with length of 7
Fixed bus with length of 8
1
1
3
0
0
0
1
0
2
3
2
3
0
2
0
3
3
1
0
2
3
2
2
1
1
1
0
1
2
3
2
3
Dynamic bus with length of 4
(a) ﬁxed bus conﬁgu(b) dynamic bus conﬁgurarations
tions
Figure 4: Experimented bus conﬁgurations
control and use the deterministic X-Y routing algorithm to
route packets. Table 1 provides the main parameters of our
simulation platform.
We evaluate two types of snooping bus conﬁgurations.
The ﬁrst one is called ﬁxed bus conﬁguration, where the cores
connected by a bus is ﬁxed, no matter how the applications
are mapped to the NoC nodes. In the second conﬁguration,
called dynamic bus conﬁguration, we use our clustering and
bus-building algorithms (described in detail in the supplemental section) to dynamically conﬁgure buses that connect
cores. For each of the bus conﬁgurations we have, we experiment with diﬀerent bus lengths of 4 links and 8 links respectively. In the following discussion, we refer to these bus
conﬁgurations as Fix-4, Fix-8, Dynamic-4 and Dynamic-8
respectively. Figure 4 (a) shows two ﬁxed bus conﬁguations
with length of 4 and 8 respectively. Figure 4 (b) illustrates
two dynamically constructed buses proposed by our scheme:
the ﬁrst bus connects 5 cores running threads of application
0 and the second bus connects 3 cores running threads of
application 2.
We use CACTI 6.5 [17] to estimate the delay and power
values for the links. Additional loading due to multiple
senders and receivers is considered when we get these parameters. We use Orion [19] to obtain the router power.
Both our network and NoC structures run at a frequency of
3GHz. Table 2 gives our bus related parameter setups.
Processors SPARC 3 GHz processor, two-way out of order, 64entry instruction window
64 KB private cache, 4-way set associative, 128B
block size, 2-cycle latency, split I/D caches
shared L2 cache, with 1MB banks, 16-way set associative, 128B block size, 6-cycles latency, 32 MSHRs
4GB, 260 cycle oﬀ–chip access latency
2-stage pipeline, 128 bit ﬂits, 4 ﬂits per packet, X-Y
routing
L1
Cache
L2
Cache
Memory
NoC
Table 1: Baseline CMP conﬁguration.
Parameters
Length(mm)
Delay(ns)
Dynamic Energy(pJ)
Leakage Pwr(mw)
link Bus of 4 links Bus of 8 links
3.1
9.6
22.4
0.13
0.41
0.93
0.93
2.88
6.51
0.03
0.09
0.21
Table 2: Energy and delay of buses and links.
4.2 Results
Impact on Memory Latency. Figure 5 plots the impact of the localized snooping on L1 load miss latencies. We
observe that, on average, the Dynamic-4 conﬁguration can
reduce the load miss latency by about 10%. Dynamic-8 provides further improvements, lowering the load miss latencies
c
L
d
a
y
o
n
d
e
e
a
t
z
L
i
l
a
m
r
o
s
s
i
M
1.6 
1.4 
1.2 
1 
0.8 
0.6 
0.4 
baseline directory 
fixed-4 
fixed-8 
dynamic-4 
dynamic-8 
Figure 5: Normalized L1 cache miss latency compared to baseline MESI protocol.
N
e
t
a
R
t
i
H
p
o
o
n
S
C
P
I
d
e
z
i
l
a
m
r
o
N
80% 
60% 
40% 
20% 
0% 
1.2 
1.1 
1 
0.9 
0.8 
0.7 
Figure 6: Snoop hit rates with diﬀerent bus conﬁgurations(%).
Figure 7: IPC compared to a system using MESI protocol.
fixed-4 
fixed-8 
dynamic-4 
dynamic-8 
baseline directory 
fixed-4 
fixed-8 
dynamic-4 
dynamic-8 
baseline directory 
fixed-4 
fixed-8 
dynamic-4 
dynamic-8 
Figure 8: Normalized network traﬃc compared to baseline MESI protocol.
baseline directory
fixed-bus-config1
fixed-bus-config2
dynamic-bus-config1
dynamic-bus-config2
1.2 
1 
0.8 
0.6 
0.4 
1.2
1
0.8
0.6
c
i
f
f
z
r
d
e
a
T
a
k
i
l
r
m
r
o
o
w
t
N
N
e
n
o
i
t
y
p
g
r
m
e
s
u
n
n
o
E
C
Figure 9: Normalized network energy consumption compared to a system using MESI protocol.
by 20% on the average, with a maximum reduction of 30% in
the case of wupwise. These improvements are achieved by
avoiding unnecessary indirections to the directory, as discussed in Section 2. In most of the cases, Dynamic-8 incurs
lower load miss latency. This is due to the fact that a larger
number of cores are snooped by this conﬁguration.
Compared to the dynamically conﬁgured buses, the ﬁxed
buses experience larger miss latencies in most cases. In several cases, the snooping results for Fix-8 are even worse compared to Dynamic-4. This proves that our dynamic buses
perform better than ﬁxed buses when the application mapping is not optimal. In such situations, longer snooping distance does not necessarily guarantee more snoop hits, but
instead results in increased load latency due to more cycles
being spent on snooping transactions.
Snoop Hit Rates. Figure 6 shows the measured snoop
hit rates of the experimented snoop bus conﬁgurations. Our
dynamically conﬁgured buses can improve the snoop hit
rate compared to the ﬁxed buses by up to 50%. There
are some interesting cases where our Dynamic-4 conﬁguration has lower hit rate than the Fix-4 conﬁgurations, e.g., in
apsi, barnes and lu. This is because our clustering and busbuilding algorithms use local information to generate group
of sharers to be snooped together. Also, since links are not
reused between clusters, some nodes could not get connected
by the snooping buses in our dynamic scheme, even if this is
possible under a ﬁxed bus scheme. This is more pronounced
when the bus length is short, as is in the Dynamic-4 in our
case. However, in most cases, Dynamic-4 performs better
compared to Fix-4 in terms of the snoop hit rates. Note
that higher snoop hit rate does not necessarily indicate improvements in performance because longer buses also incur
more delay cycles.
Performance Improvements. Figure 7 shows the overall improvement in execution time of our proposed scheme
compared to the baseline MESI directory protocols. Our dynamic bus conﬁgurations deliver performance improvements
of up to 12%, with only barnes and apsi suﬀering a slight
slowdown when the bus length is short. The reason is that
sometimes short dynamic bus constructions suﬀer from gen 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
in [12, 11]. [11] designed reconﬁgurable bus-based networks
based on inter-core data sharing. Kim et.al [12] proposes
to reconﬁgure the network in order to suit for the speciﬁc
application characters.
In this paper, we proposed a novel hybrid NoC architecture that takes advantage of both snooping and directory
based cache coherence protocols. We ﬁrst investigated how
application mappings can aﬀect the performance of proximity snooping schemes. We then explained the design of a
localized bus-based snooping cache coherence protocol. We
also presented the design details of our conﬁgurable snooping buses using the on-chip links.
In order to reduce the
dependency of the local snooping on application mappings,
we further designed two algorithms to dynamically group
sharing cores into local clusters and build buses to connect
those cores. Our experiment results showed that the proposed techniques not only increase system performance but
they can also reduce energy consumption.
6. "
